hello and welcome to this video section 1.1 getting started in this section we're going to learn the vocabulary of Statistics the basic vocabulary at least we're going to distinguish between a population and a sample and we're going to distinguish between the two types of populations the target population and the sampled population so let's begin the goal of the branch of mathematics and these are the words from Hawks the goal of the branched mathematics called statistics is to provide information so that informed decisions can be made I disagree 100 with this statement statistics is not a branch of mathematics for sure we use numbers we use formulas but then so does chemistry and physics and they're not mathematics statistics is a science it proceeds forward based on inductive reasoning tries to learn about the larger whole based on your smaller sample mathematics on the other hand draws from the larger to bring down to the narrower deductive reasoning so statistics is actually a branch of science the goal of this text is to enable you to filter through the statistics you encounter so that you may be better prepared for the decisions that you make in daily life as befitting most things in statistics the word itself has a couple definitions first definition is that statistics is the science of gathering describing and analyzing data so it's the science of gathering describing analyzing data Hawks admits that it's a science we could also use the term statistics as the actual numerical descriptions of sample data in other words statistics are a function of your data a mathematical function of your data the target population is a particular group of Interest a sampled population is a group from which the sample is taken hopefully those two populations are the same they don't have to be if I would like to draw a conclusion about the population of Galesburg that's my target population if I contact People based on the numbers in the phone book the people in the phone book are the sampled population Target population and sampled population in that case are not the same hopefully however the sampled population is representative of the target population a sampling frame is a physical list of all members of the sampled population this may or may not actually exist in the example I just gave the phone book will be the sampling frame a sample is a subset of the population from which data are collected a subset of the target population no not from the target population from the sampled population from which the data are collected it's this sample that we're actually going to apply our statistics to so that we can learn about the target population now hopefully our sample is representative of the target population if the sample is not representative of the target population all the work that we're going to do is worth nothing the sample must be representative and we're going to see some ways of obtaining what are called quote representative samples in the future a census is a study from which the data are obtained from every member of the population every member of the population the United States is required to hold a census every 10 years that is not this type of census that we're talking about because the U.S Census Bureau is not able to contact every single resident of the United States it tries it does a good job of it but it never actually achieves their goal of a genuine census so now let's turn to an intralecture question these questions are you'll have to answer these questions in Moodle for section 11 is just to check that you are moving forward through these at a good pace so the first introelection question for this section what is the difference between a population and a sample what is the difference between a population in a sample remember you can pause this you've got that little double bar thing that you can push on pause it so you can do the answers in the Moodle right now I recommend that you actually write the question and the answer in your notes on the left hand side but regardless you can pause and I can go back to the lecture a variable is a value like four or a characteristic like blue that changes among members of the target population it's got to change among members of that population because if it is constant among all members that is if all members of the population have the same value such as blue there's really nothing to study it's just blue data are the counts measurements or observations gathered about a specific variable in a population in order to study it so data is what you actually mark down from your sample do not confuse a parameter with a sample statistic a parameter is a numerical description of a population characteristic whereas a sample statistic is a description of a sample characteristic so parameters are about the population statistics are about the sample P for population P for parameter s for sample s for statistic we want our sample statistics to be good estimates of the population parameters and that goal will actually dictate some of the formulas that we'll see in the future here's our second question what is the difference between a parameter and a statistic remember these go into your Moodle quiz but again I recommend on the left hand side of your notes write out the question and the answer and then later put it into Moodle again remember you can pause if I go too fast it is essential that you are mindful of the relationship between a population and a sample the next figure is the picture to help you visualize this relationship so that big blue oval is the target population that is the group we want to draw conclusions about the yellow oval is the sampled population notice the sampled population does not have to be a subset of the target population it hopefully is in fact it hopefully is the target population itself but it doesn't have to be but note that the sample does have to be a subset of the sampled population sample doesn't have to be a subset of the target population again we hope it does in fact we hope that the sample is representative of the target population but from the way that the data are collected it's not required strongly suggested hoped for that's the goal but it doesn't have to be so let's summarize the differences between a population and a sample population is about the whole group it's a group we want to know about characteristics of a population called parameters these parameters in reality are unknown we're trying to estimate those parameters those parameters are fixed they are about the entire population contrast that with a sample where the sample is part of the group is a group we know everything about because we've got it right in front of us we can measure anything we want in that group there is no Mysteries whatsoever characteristics of the sample are called statistics those statistics are always known because we can just measure it on this sample that we have in front of us and the statistics change with the sample a little bit ahead of the place on that example one one identifying population and Sample so please identify the population and the sample I will read through it both A and B I will expect you to hit pause and then I'll give you the answers I've got to identify the population and the sample so in this survey 359 college students at the University of Jackson were asked if they had tried the October flavor of the month at the campus coffee shop 83 of the student surveyed said yes I'm going to now give the answer to that go ahead and hit pause the population is going to be the University of Jackson students or the college students at the University of Jackson because that's the group we're trying to draw a conclusion about the sample is going to be those 359 college students we actually contacted a survey b a survey of 1125 households in the United States found that 24 subscribed to satellite radio identify the population and the sample hit pause the population is going to be all households in the United States all and the sample is going to be that 1125 that we actually contacted those are the solutions move on to example 1.2 identifying the population the sample the parameters the statistics Etc for each of the following reports identify the population the target to population each of these cases the sample and whether or not the Highlight value is a parameter or a statistic so population sample and whether it's a parameter or statistics statistic so here we go numero one after an airline security scare on Christmas Day 2009 the Gap organization interviewed 542 American Air Travelers about the increased security measures at airports the reports say that 78 percent that must be the highlighted part of American Air Travelers are in favor of the United States airports using full body Scan Imaging on airline passengers population the target population the sample and what does that pink purple whatever color it is highlighted number actually indicate go ahead and hit pause the population here is all Americans because we are try all American Air Travelers if you wish because we're trying to draw conclusions about American Air Travelers in the United States two what is the sample it's those 542 people American Air Travelers that the Gallup organization contacted and this 78 percent is going to be the statistic because it was measured off of that sample two Rasmussen reports also conducted a survey in response to the airport security scare on Christmas Day 2009 the national telephone survey of 1 000 adult Americans found that 59 of Americans surveyed favor racial profiling as a means of determining which passengers to search at airport security checkpoints remember Target population sample and what does that 59 represent go ahead and hit pause the population is going to be all Americans because we're trying to draw conclusions about the Americans two the sample is going to be the house 1 000 adult Americans contacted and at 59 again is going to be a sample statistic it's 59 of those 1 000 people two branches of statistics the branch of Statistics called descriptive statistics is the science that gathers sorts summarizes displays the data doesn't try to draw conclusions about the data it just tries to better understand the data itself and the first four five six seven chapters of this book are going to cover descriptive statistics inferential statistics as the science evolves using these descriptive statistics to estimate population parameters and that'll be the last half of the course so inferential statistics takes our sample and tries to draw conclusions about the population descriptive statistics on their hand just takes our sample and learns about our sample which brings us to our next question what is the difference between descriptive and inferential statistics remember you can hit pause there are two types of analyzes one is called exploratory and one is called confirmatory exploratory analysis uses data to estimate parameters this will be akin to the confidence intervals in the second half of the course confirmatory analysis uses statistics to test stated claims about reality and these stage claims about reality we're going to call hypotheses and this will relate to the hypothesis testing or the pvalues of the second half of the course so example 1.3 identifying descriptive and inferential statistics in a news report on the state of the media by Tom rosentile and Amy Mitchell and they write the following AOL had 900 journalists 500 of them at its local Patch News operation by the end of 2011 Bloomberg expects to have 150 journalists and analysts for its new Washington operation Bloomberg government so let's identify the descriptive and the inferential statistics used in this excerpt again remember hit the pause to answer that and then just go ahead and listen we talk and talk and talk and talk descriptive so the first descriptive is AOL had 900 journalists so we're describing how many journalists AOL had and we describing 500 of them at its local news Operation so those are descriptive the future by the end of 2011 Bloomberg expects to have so expects to have tells us this is a feature event it's going to expect to have 150 journalists so that will be the inferential statistic so descriptive of that data that we have inferential is about the population which also translates into things that are happening in the future oops we've got that solution and that's the end of the slideshow and that's the end of section one one so expect these videos to look very similar to this that's slideshow me talking over it giving you questions me adding to the slideshows maybe telling a joke or two didn't tell any jokes today I'm sorry but expect them in the future so I hope this was helpful if not as always please make sure you email me with any questions or leave the questions in moodle's discussion forum for questions for chapter one for learning module one that's it thank you very much I appreciate it hello and welcome to section 1.2 data classifications in this section we're going to be looking at how to classify data that is how to determine what types of variable it is it can be either classified as qualitative or quantitative qualitative it's also called categorical quantitative is called numeric it can be classified as discrete continuous or neither I guess the book loves to talk about the neither we're going to look at discrete or continuous and it also adds nominal ordinal interval or ratio so so by the end of this slideshow you should be able to know what all of those terms mean and you should be able to classify a variable or data as in each of those three ways so let's begin qualitative data also known as categorical data consists of labels or descriptions of traits so qualitative variable example would be eye color hair color um favorite music things like that where you can if you want put a number to it but it's not a meaningful number say my eye color is one doesn't tell you what my eye color is say my eye color is blue does tell you those are qualitative those are categorical data quantitative on the other hand also known as numeric actually does have a number that does make fundamental sense to it usually counts measurements I object to the book using the term measurement because counts are also measurements in their own way examples of quantitative data would be height weight age gas mileage we're talking about cars now those would be quantitative because there are numbers associated with them and those numbers are inherently meaningful so here's a graphic we got the qualitative side we've got the quantitative side again qualitative is categorical quantitative is numeric qualitative data or descriptions and labels whereas quantitative tends to be counts and measurements or measurements in general yeah so here's an example classifying data is qualitative or quantitative classify the following data as either qualitative or quantitative what are you assuming about the described variables so a shades of red paint in a home improvement store is that qualitative or quantitative hit pause that is qualitative probably it depends on how you're actually measuring the shades of red paint if you're doing in terms of the names such as fire engine red apple red baby red I don't know what color is red paint are how does it just see red then those would be qualitative but if you're doing in terms of the percent of the paint that is red that is or how many squirts of red dye you have to put in the paint to get that then it would be quantitative because it's the number of squirts of red in the paint interesting so the Ampro answered depends on what you're trying to actually measure hmm B rankings are the most popular pink colors for the season again think about whether this is qualitative or quantitative and what you're assuming and go ahead and hit pause the rankings are counts there's numbers one through however many paint colors there are so this is going to be quantitative I don't think that there's much you're assuming about it unless you're thinking hey what if we're talking about rankings in terms of loved versus hate it well in that case that's qualitative but if you're looking at maybe number of stars that would be quali quantitative if you're looking at number of people who purchased it that would be quantitative again it's how are you actually measuring this ranking huh so these questions actually are kind of difficult C amount of red primary dye necessary to make one gallon of each shade of red paint qualitative or quantitative hit pause this is quantitative because it's an amount of I don't think I can think of any way of of interpreting this to make it a qualitative because it's very clearly an amount of red primary dye okay so that that's going to be quantitative D number of paint choices available at several stores again qualitative or quantitative hit pause you're right I hope that's quantitative because you're looking at the words number of paint choices I suppose you could make this qualitative by saying a lot of paint choices not so many paint choices almost no paint choices that would be a qualitative way of measuring this but it's pretty explicit with the numbers of so that would be quantitative okay yeah that's not what I wanted to switch over to that's what I wanted to switch over to intra lecture question number one again this goes in your Moodle quiz is the variable number of toes a qualitative or a quantitative variable and again I recommend you write the question in your notes on the left hand side your answer below it so that you can go to Moodle after this lecture and put the answers in is the variable number of toes a qualitative or a quantitative numeric variable and back to the lecture now we're looking at continuous versus discrete data discrete data are quantitative data that can take on only particular values and those values are usually counts but they don't have to be counts they could be ratios of two counts I mean hey that would also give you discrete data continuous data are quantitative data that can be taken that can take on any value in a given interval and are usually called measurements again I personally don't like the term measurement because any measurement for statistician is something that you measure so eye color would be a measurement for me because I have to measure eye color but the way that the book wants to frame this measurements would be things that you have on a scale like a ruler scale continuous versus discrete so discrete are counts or ratios of counts continuous data is is takes on any value in a given interval so again we've got the qualitative and the quantitative ovals quantitative can be broken down into the discrete which is usually counts so ratios of counts and then continuous which are usually measurements notice discrete and continuous don't speak at all towards qualitative because qualitative data is neither discrete nor continuous here's another example to determine whether the following data are continuous or discrete temperatures in Fahrenheit of cities in South Carolina is this continuous or discrete hit the pause button now welcome back assuming you hit the pause button now temperatures in Fahrenheit are going to be continuous what's reported on TV will be discrete but the actual temperatures will be continuous B number of houses in various neighborhoods in a city continuous or discrete hit pause now welcome back numbers of houses tells me it's going to be discrete the key word there is numbers you're counting how many houses are in a various neighborhood see numbers of elliptical machines in every YMCA in your state numbers of elliptical machines in every YMCA in your States is that continuous or discrete hit the pause button you're absolutely correct that also is discrete it's got the word numbers of D Heights of doors continuous or discrete oh hit the pause button welcome back whatever I'm supposed to say that's continuous Heights are continuous in the words of the book it's a measurement realize that the actual height of the door is going to be continuous the variable called Heights of doors is continuous what you actually write down as the height of the door is going to be discrete because eventually you're going to have to stop writing down digits and that gets back to something interesting the difference between the variable and the data the variable is what you're measuring the data is essentially what you're writing down for those measurements the variables are what you're measuring such as temperature such as height such as age and the data is what you're actually writing down such as 97.8 degrees such as six foot five such as I I forget what the other example was but those are things that are written down variables can be discrete or continuous what you write down has to be discrete because you have to stop writing at some point which brings us to the next intraelection question is the variable grade point average discrete or continuous again I recommend you write the question over on the left side of your notes write down your answer right after it think about it go back over the last five ten minutes of this lecture so that you're absolutely certain what the difference between continuous and discrete is hit pause fast forward rewind as you need you've got total control over this lecture except for what I say I mean you could even slow this down so I sound like this or speed it up sorry that's not like this levels of measurement four levels of measurement nominal ordinal interval ratio if you learned French you can think of this in terms of Noir black Noir nominal ordinal interval ratio that may help you remember the four levels the level of measurement of a variable describes the amount of information that that variable contains the four levels are nominal ordinal interval and ratio nominal the values are just descriptions ordinal which is your for ordered nominal you've got the description plus you've got an inherent ordering to it interval level the differences between levels are identical so you can subtract and ratio level not only do you have differences between two levels being the same but you've also got the value of zero meaning an absence of so data at the nominal level of measurement are qualitative consisting of labels or names so variable eye color will be nominal level because what are the eye colors blue brown green Hazel and I'm sure I'm missing some colors blue brown green Hazel yeah I could also order them as brown blue green Hazel it would mean the same thing they're just names attached so it's nominal the word nominal comes from the Latin nominus meaning name suppose all students in a stats class were asked what pizza topping is their favorite explain why these data are at the nominal level of measurement piece of cake favorite favorite pizza topping is I don't know pineapple pepperoni this sausage olives I can't think of any more because I only use pepperoni and pineapple those are just names I mean it doesn't matter that they're the individual person's favorite I'm asking everybody for one pizza topping I can call it pineapple I can call it pineapple Apple Pine means the same thing B suppose instead that you wish to know that the number of students whose favorite pizza topping is sausage the number of students explain why this data value is not nominal well it's the number of so you're measuring the number of students counting the number of students nominal has to be categorical if you're counting things that's that's numeric so it can't be nominal data at the ordinal level of measurement are qualitative data that can be arranged in a meaningful order but calculations such as addition or division do not make sense so we've got some examples of nominal data eye color hair color but we can also look at some examples of ordinal data such as and having trouble coming up with one off the top of my head um well let's think through this it's it's got to be categorical and they've got to be in some sort of inherent meaningful order categorical but some inherent meaningful order to it how about socioeconomic status it's categorical low medium high but there is an inherent meaningful order to it low medium high the fact that you could also do it high medium low is irrelevant you've got an ordering to it so socioeconomic status would be an ordinal level variable notice ordinal level variables have additional information to them that nominal level variables don't have that is position I can say high SES is larger or greater than low SES I can't say blue eyes are larger or greater than brown eyes doesn't make sense example determine whether the data are nominal or no seat numbers on your concert tickets such as a23 and a24 go ahead and hit pause while you think about it I believe the answer to this will be it's ordinal although I haven't been to a concert 6080s I believe the a would be the row and 23 would be the seat number so seat number 23 is one seat closer to the aisle than seat number 824. since it is closer to the aisle since we do have some sort of ordering to it we've got an ordinal variable genres of Music performed at performed at the 2013 Grammys that would be nominal because it'd be I assume country and and rock and Jazz and doowop roll done at the Grammys I assume and there's no inherent ordering to that I could do it alphabetical order it would have just as much meaning as doing it in terms of the order that I just gave you so a is ordinal B is nominal data at the interval level of measurement are quantitative data that can be arranged in a meaningful order and such that differences between data entries are meaningful difference between levels are meaningful such as shoe sizes the difference between a 5 and a six in shoe size is exactly the same as the difference between eight and a 9 in shoe size they both differ by one shoe size but also by 1.3 inches I think temperature in degrees Fahrenheit is interval because going from 45 to 46 degrees is exactly the same as going from 95 to 96 degrees it's an increase of one degree Fahrenheit probably should say it's measured in degrees Fahrenheit birth years of your classmates are collected what level of measurement are these data well clearly it's going to be interval birth years 2000 I don't know what birth years you guys have 2001 2002 that differs by one year just like 1997 and 1998 different by one year the key for Interval level data is that subtraction and by extension addition actually makes sense and then the last level of measurement the one that has the most information in it is called the ratio level it's quantitative data it can be ordered this interval and the zero point indicates the lack of something so compare compare the year of your birth which we decided was interval from the last with your age which is ratio year the birth well a zero year of birth doesn't indicate a lack of anything just indicates it was two thousand some odd years ago but an age of zero would indicate you lack age now note that you don't have to be able to achieve a zero all that it means is that a value of 0 would indicate an absence or a lack of something so the height of a person would also be ratio because a height of zero would indicate a lack of height can't achieve a height of zero but if it were achievable it would indicate you lack height weight would be another example of a ratio level variable you can't achieve zero weight but a zero weight would indicate a lack of weight now it's called ratio level because ratios actually do mean something so comparing a person of age 10 to a person of age 20 that ratio actually does mean something the second person is twice as old as the first a four foot person versus an eight foot person that ratio of two actually does mean that the eight foot person is twice as tall as the four foot person compare that with your age of birth no just your year of birth sorry 1995 versus 2000 that doesn't indicate that your year of birth is 0.5 percent higher is just your five years later which is interval so the word ratio comes from the fact that ratios divisions actually make sense so example consider the ages in whole years of U.S presidents when they were inaugurated what level of measurement or these data clearly they're going to be ratio because it's in this section but why are they because it's ages in whole years an age of zero would indicate a lack of age doesn't mean you can achieve an age of zero just means that if 0 is in the age column in your spreadsheet you lack h so here are the nice little stair step you got qualitative quantitative qualitative includes nominal and ordinal nominal means names ordinal means ordered nominal quantitative is interval and ratio in interval zero is just a placeholder it's just such as zero degrees Fahrenheit if zero degrees Fahrenheit doesn't indicate a lack of temperature whereas in ratio zero does mean the absence of something so zero Kelvin would be a ratio level variable but zero I'm sorry uh measuring temperature in Kelvin would be ratio level but measuring degrees in Fahrenheit would just be interval level because a zero fahrenheit just means it's cold not that it lacks temperature zero Kelvin means it actually lacks temperature here's the third one give an example of a ratio level variable not provided in the slides or the text again I would write this off on the left write the answer beneath it so you can go into Moodle and answer the questions and pause of course if you need to so final example classifying data classify these as qualitative or quantitative discrete or continuous or neither and the level of edge of measurement normal ordinal interval ratio finishing times for runners in the Labor Day 10K colors contained in a box of crayons boiling points on sale let's see a scale for various caramel candies and the top 10 spring break destinations is ranked by MTV go ahead and hit pause before I give you the answers the answer for a finishing times for runners will be a ratio level it will be continuous and it will be quantitative colors contained in a box of crayons will be qualitative it will be neither because qualitative is neither discrete nor continuous and it will be nominal note that number of colors would be different but we're looking at just the colors themselves boiling points on Celsius that's going to be quantitative continuous and interval because zero Celsius does not indicate a lack of temperature and finally the top 10 Spring Break destinations is ranked by MTV notice these are the destinations themselves so it's going to be categorical or qualitative and hence neither discrete nor continuous lever measurement is going to be ordinal and that's it for section one two again don't hesitate to send me comments questions and post them in Moodle for the discussion section thank you much hello welcome to section 1.3 the process of a statistical study the objectives for this section are to describe the process of a statistical study that's probably rather clear identify the various types of studies but most importantly to understand the primary sampling schemes and it's this understanding the primary sampling schemes that is key for this section if all you take out of the section is those sampling schemes and an understanding of them you're doing great so here's the process of a statistical study notes that there are four steps Step One is determine the design of the study there are classes taught in experimental design by the way but you need to State the question we studied determine the target population and the variables and the determine the sampling methods you're going to use it's one C is it's it's a subset of one but it's probably the most important thing here because remember your sample must be representative of your target population otherwise all the statistical methods in the world are useless your sample must be representative so we need to study how to draw a representative sample two you're going to collect the data according to that sampling method three you're going to organize the data and four you're going to analyze the data yeah 4 is going to be the second half of the class 3 is going to be most of the rest of the first half of the class so example 111 neurologists want to study the effect of vitamin C on nerve disorders the goal of the study is to see if taking an intravenous dose of vitamin C will reduce the amount of nerve pain associated reported by patients so identify the population of Interest and the variables in the study again pause and welcome back the population of interest is all notice all populations of interests have the word all in there somewhere all patients period and the variables in the study will be I'm sorry all patients with nerve disorders and the variables in the study will be the amount of vitamin C and amount of nerve pain those are the obvious ones perhaps others will need to be taken eventually to take care of some intervening issues observational study versus an experiment and observational study observes the data that already exists so the statistician will sit there and just collect data won't try to influence the outcome of anything whereas an experiment generates data to help identify the cause and effect relationships um yeah the the book emphasizes that it's cause and effect it's easier to do cause and effect analysis when you've got an experiment but observational studies can hint that as well now that these are the proper definitions as used by scientists a statistician word for any quote theoretical data collection as an experiment this difference in terminology comes to the fact that statisticians experiment to better understand their field of study just like biologists experiment to better understand biology and physicists experiment to better understand physics statisticians experiment to better understand statistics so here's an example which type of study would you conduct observational or experiment a you want to determine the average age of college students across the nation B researcher wishes to determine if flu shots actually help prevent severe cases of the flu go ahead and hit pause the first one is an observational study it's second is an experiment notice in the second one you're actually trying to determine if the flu shots do something whereas in the first one you're just observing ages of students across the nation a representative sample has the same relevant characteristics as the population and does not favor one group from the population over another note that a sample could be representative for one characteristic of the population but not for another so here's an interesting question how do you know if a sample is representative of the population that we're going to come back to time and time and time again because it's such an important question remember all those statistics is based on that sample you draw and that sample has to be representative of the population so how do you know if your sample actually is represented the population we got a red star at the bottom so let's move on to the first intro lecture question one my sample is all females in this class is it a representative sample remember hit pause write the question over on the left side you probably should listen to what I'm saying before you hit pause write the question on the left side of your notes write the answer down below it so you can put it into your middle quiz and then hit pause I really do want to come back to this how do we know if a sample is representative of the population maybe you should write that on the left hand side as well and start coming up with answers to that question because you're going to be seeing it several times in this course here's five sampling techniques simple Ram sampling notice the book also has something called random sampling we're going to conflate that with simple REM sampling second type is stratified then clustered and systematic and then convenience never ever use convenient sampling except when I give you permission to in class but we're not in class so simple random sample every sample from the population has an equal chance of being selected keyword there is actually two keywords it's sample and equal chance so if I want to draw a sample of size 50 from everybody in the United States and they want to use simple random sampling that means that every possible combination of 50 people in the United States has an equal chance of being selected every possible combination of 50 people has an equal chance of being selected or we can bring it down to the class level let's say our our class is size 30 so our population is size 30. I want to sample from this population sample of size 5 simple random sampling would require that every possible combination of five people in this class would have an equal chance of being selected stratified sampling population is divided into subgroups called strata the grouping variable is correlated with a measurement variable and the sample is drawn from each stratum so in this example if I want to estimate the average GPA at Knox I probably would break it up into freshman sophomore junior seniors estimate the GPA and the Freshman estimate the GPA and the sophomores estimate the GPA and the changes estimate the GPA and the seniors and combine them together to get the estimated average and the reason I'd probably break it up into freshman sophomore junior seniors because GPA does seem to correlate with uh class contrast that with cluster sampling populations divided into subgroups called clusters not strata clusters the grouping variable is not correlated with the measurement variable and a sample is drawn from at least one of the Clusters so if we're going to go back and I want to determine the typical hair color at Knox breaking it up into freshman sophomore junior senior would not be useful in terms of stratified sampling but it would be useful in terms of cluster sampling so I highly doubt that level in school and hair color is correlated in this example we've got a humongous Rice Field one two three one two three four five six seven there's 21 subplots there I randomly select four of those subplots and estimate the amount of rice in each of them each of those subplots is is seems to be rather representative of the population as a whole systematic sampling every nth member of the population is selected so if I want to select 25 percent of the population I would select every fourth bottle in this example here I want to select every 10 percent of the population I'll select every tenth bottle if I want to get uh one percent of the population I'll select every hundredth the bottle but again it's it's systematic because it's selecting every nth member of the population as it comes down the conveyor belt or through the door convenient sampling it's just convenient for the researchers to select people will selfselect into the poll perhaps it's very unethical to use all of the web polls that you see on newspaper sites that say hey how would you vote in this case those are all convenient sampling and they are highly unethical because it is trying to give information to the reader and that information is about the population of interest but you're drawing a sample from a very skewed population or a very biased population itself so here's some examples a poster surveys 50 people in each of the Senators 12 voting precincts so this sounds like stratified um Senators 12 voting precincts so the voting Precinct is the stratum there's 12 of them surveying 50 from each I would assume that voting precincts are internally more consistent than the population as a whole because likeminded people tend to live near each other the quality control department is zero manufacturer marries a weight of every 10th box so this will be systematic a female student walks down the halls in a door I'm asking students how much money they'd spend from the food court this is convenience an educator chooses five of the school districts in the Chicago area and ask his household in those District how many school age children are in the district this would be cluster probably the book says this is cluster this may actually be stratified because a school district the distribution of of schoolaged children may not be the same across the school districts um one school district may have a higher proportion of children in each household than another school district if that's the case and this would be stratified to determine who'll win a hundred thousand dollar shopping spread them all manager draws the name out of a box of entries this will be simple round sampling technically this will be random sampling but we're conflating the two remember so it's simple random sampling there's the Red Box let's move on to question two this is a key one very very important what is the primary difference between cluster sampling and stratified sampling remember hit pause if you need to again write the question on the left hand side of your notebook and your answer underneath of it two types of observational studies is the crosssectional study in the longitudinal study crosssectional study data collected at a single point in time on a lot of members whereas in a longitudinal study it's over time on a few members a group of 220 patients is followed for 15 years in order to determine the okay right there I know this is going to be logic longitudinal because it's over time for a small group of of members be a gastroenterologist surveys 130 of his patients six months after okay this is going to be crosssectional because it's done at a single point in time six months after having the gastric bypass if the gastroenterologist surveyed those 130 people six months one year 18 months two years three years four years five years and that would be a longitudinal study similar terminology treatment is some condition that is applied to a group of subjects in an experiment the subjects or the participants are the people or things being studied in experiment the response variable is the variable in an experiment that responds to the treatment we'll also refer to this as the dependent variable the explanatory variable is the variable in experiment that causes the change or it explains why that change took place we're going to refer to this as an independent variable there's another set of variables that are called independent it's not the next slide I guess called control variables control variables are variables that we know affect the dependent variable but we really don't care about them in terms of our research so independent variables are broken up into exploratory variables or research variables and then those control variables so here are three principles of experimental design or one you got to randomize the control in treatment groups because the goal is the only difference you want between the control and the treatment group is to be the treatment so you got to randomly put people into okay you don't have to you should randomly put people into the two groups control and treatment and then apply the treatment to the treatment group uh control for outside effects on the response variable those would be the control variables replicate the experiment a significant number of times to see meaningful patterns and I do want to emphasize here the word replicate remember back earlier we asked how do we know if our sample is actually representative the answer is we don't we have to replicate our studies over and over and over again and then the hope is that we don't make the same mistakes and get an unrepresentative sample in each of those replications okay control group versus treatment group is this is the group of subjects which no treatments given whereas the treatment group gets the treatment and again the structure of the experiment has to be such that the only meaningful difference between the control group and the treatment group is the treatment and hence the importance of the randomization confounding variables are unmeasured factors other than the treatment variable that cause and effect on those subjects I had to get rid of confounding variables you measure them add them to the model and there's a red star so let's go back and see question three how do we know if there are confounding variables in a statistical study move those over write that over on the left hand side of your notes put the answer there so that you can answer the quiz in Moodle how do we know if there are confounding variables in Oh missing an s in a statistical study a placebo is a substance that appears identical to the actual treatment but contains no intrinsic beneficial elements um placebos are used to ensure that the only difference between the control and the treatment group is the treatment itself placebos will be given to the control group so the control group doesn't know that they're in the control group the placebo effect is response to the power of suggestion rather than the treatment itself that's why we have to give a placebo to the control group because the very Act of receiving medicine will affect you hence we don't know if the treatment actually improves the the people or just them thinking they receive treatment so we got to give placebos to the control group and the actual treatment to the treatment group so they all think hey I received something so the placebo effect is going to be constant for the entire group and again remember the emphasis here the only allowable difference meaningful difference between the control group and the treatment group is that the treatment group receives the treatment in a single blind experiment the experimenter pokes out one person's eyes oh wait no that's not what it is subjects do not know that they are in the control group or the treatment group whereas In a doubleblade experiment both the subject and the measurer doesn't know the measure is the person who measures the temperature of the patient measures whether or not the patient improved measures if the patient's foot fell off whatever whatever the treatment is supposed to fix the researcher a good researcher will move him or herself completely from this once the experiment is set up and is in motion researcher will just take the data that is collected and analyze it because the researcher needs to know everything but in a doubleblind experiment it's the subject and the person doing the measuring who don't know so subject person doing measuring and researcher three different people in the experiment three different roles in the experiments sometimes the subject and the the measurer are the same person so here's the lengthy example consider the study from example 111 in which a neurologist want to determine if taking the IV dose of vitamin C will reduce the amount of nerve pain reported by patients suppose that the study was narrowed to focus only on patients with the nerve disorder multiple sclerosis after study approval the neurologists solicit volunteers who are patients with MS and who are reporting nerve pain so Target population one I guess is all patients smaller Target population is patients with multiple sclerosis sampled population patients with multiple sclerosis who have nerve pain sampled population is the people in our after study approval the people in the study that I can pull from this doesn't specify what the sampled population is the sample is the actual 40 participants to one t in the control group and 20 in the treatment group a is the treatment group they're given the IV doses of vitamin C so the subjects are getting the doses somebody is measuring the nerve pain participants in group b are the control group they're given an IV dose of saline which is the placebo and somebody's measuring it the participants don't know which group they're in and the people measuring should not know which group they're in oh should not however the nurse is administrating the IVs are aware of the group assignments that's not necessarily a bad thing as long as the nurses are not measuring the nerve pain after predetermined length of time the amounts of pain reported by the separate groups are compared to determine if the IV dose reduces the amount of nerve pain interesting expandatory and response variables treatment which group is the treatment which is the control group what's the purpose of ministering saline to Group B is the single line single blind or doubleblind hit pause welcome back explanatory and response response variable is going to be the amount of nerve pain the explanatory is the vitamin C saline I'm sorry the vitamin C IV drip the treatment is the IV uh of vitamin C which group is the treatment group and which is a control group the treatment groups the one that receives the vitamin C the control group is one that receives Saline what is the purpose of menstruating saline to Group B it's to control for the placebo effect remember group a and Group B have to be exactly the same except that group a receives the treatment so giving the IV also makes the groups more similar is this a single blind or a doubleblind study it depends if the nurses are measuring the pain it's single blind if somebody else is measuring the pain that doesn't know who is when who is in which group then it's doubleblind last page IRB is a group of people who review the design of a stage to make sure that it is appropriate and that no unnecessary harm will come to the subjects involved Knox College has an Institutional review board it meets infrequently to handle student and faculty research plans or research designs informed consent involves completely disclosing to the persistence pins the goals and procedures involved in the study and obtaining their agreement to participate there is a lot of question out there whether informed consent is ethical um whether it's even possible in some cases in all cases where it's reasonable informed consent should be obtained now the question comes down to when is it not reasonable and it's not reasonable when the purpose of the experiment is is given away by the by telling the participants the goals and procedures because remember um the the Placebo and the placebo effect if you know the outcome of an experiment or if you know what the researcher is looking for in the experiment is a much higher probability the researchers are going to find it so unfortunately informed consent in those cases could actually destroy the experiment which means well is informed consent ethical in those cases I don't know it's a big question things to think about but that brings us to the end of this section that's the end of this chapter however I encourage you to go through section one four but I won't be lecturing through it it's important stuff we just have more important things to cover so thank you much hello and welcome to section 2.1 we're going to be creating frequency distributions today it's unclear at this point why you're creating frequency distributions tomorrow or in the next lecture you're going to see how we use these frequency distributions and help create Graphics that tell the story of the data and we're going to look at how to create ungrouped frequency distribution and create a grouped frequency distribution the ungrouped is going to lead to charts such as a pie chart and a bar chart whereas the grouped is going to lead to such things as a histogram or a cement leaf plot so today is the foundations and the next lecture will be for what we're actually going to be using this for throughout this lecture I'm going to point out some things such as while this is a lot of work for not that much and I want you to take that seriously and think okay a lot of work we're not getting too much out of it is there a way of making this easier and of course the answer is yes so frequency distributions a distribution is a way to describe the structure of a particular data set or population we're going to look at sample distributions now when we get to chapter 5 and 6 we'll be looking at population distributions so keep this in my effect on the left hand side of your notes put a little star right C chapter 5 and 6 and another star just to draw your attention to it that this is the start of looking at distributions here for the sample later for the population a frequency distribution is a display or a table of the values that occur in a data set and how often each value or range of values occurs so it's how often that's where the frequency comes in frequencies which is a little f are the number of data values in that category if we're talking about categorical data or range of values if we're talking about numeric data in a class we're going to call that as a category of data in a frequency distribution for categorical variables that class most often is going to be where the levels in the variable for numeric data it's going to be some range of possible values for the data in order to raise an ordered list of the data from largest to smallest or smallest to largest probability distribution is a theoretical distribution used to predict the probabilities of particular data values occurring in a population probability distribution If all we're going to talk about is probability distribution is going to be about the population if we're talking about the distribution or a sample we'll call it a sample distribution an ungrouped frequency distribution is a frequency distribution where each category or class represents a single value or level in the variable these are used for categorical variables whereas a grouped frequency distribution is a frequency distribution where the classes are ranges of possible values these will most likely be used in numeric data the ungrouped frequency distribution will lead to bar charts and pie charts in the future whereas a grouped frequency distribution will lead to histograms in the future so here's the steps in constructing a frequency distribution for ungrouped for categorical data so to create an ungrouped frequency distribution to determine the levels of the categorical variable and I do want to emphasize that you would use this only for categorical variables and count the number of observed values in each level okay number a level is a possible outcome of a categorical variable example the iCloud of my research students in this term are as follows blue brown brown blue brown brown brown green so the blue brown brown blue brown brown brown green is the raw data the observations the measurements I make of my students eye colors blue is the eye color of my first research student Brown is the eye color my second research student Etc the frequency distribution just looks at all the possible levels that we observe blue brown and green and then counts the number of number of observed values in each of those levels I got one blue two blue only two Brown I got one two three four five green I got just one so the frequency distribution is 251. for blue brown green ooh red star I think that's what I want to do so question one okay and I encourage you to write these questions and your answers in your notes on the left hand side what is the difference between a grouped and an ungrouped frequency distribution pause and we're back so that was all about the ungrouped or for the categorical variables now we're going to look at grouped which is almost always applied to numeric variables remember the main difference between the ungrouped and the grouped is that the ungrouped each Row in the table corresponds to one level in the variable whereas for the grouped each row is going to correspond to a range of values of that variable so the first step in creating this grouped frequency distribution is decide how many classes should be in the distribution now we've got rules of thumb none of them are good because they're rules of thumb the reality is you should choose many different numbers of classes so the first time through you should choose five and then maybe 10. and see what the information given by these distributions how that changes Maybe 20. but you're going to realize that if we do 5 10 20 12 and 18 we do the we got to do these steps five times and it takes a lot of effort to do just one so you're going to fall in love with the computer because the computer can do it just like that I don't know if you could hear me snapping my fingers but I'm snapping my fingers so the first step is to decide how many classes should be in the distribution once you've decided that two choose an appropriate class width class width is usually going to be the highest value minus the lowest value divided by the number of groups sometimes you'll round down to something logical for the lowest value round up to something logical for the upper value and then divide by the the number of classes sometimes you'll Skip One go directly to two and and physically determine that class width what the classes should be based on the the problem itself and we'll see that later in those cases easy lends itself to Natural divisions such as decades or years or hundreds of dollars three find the class limits the lower class limit and this is for each of the classes by the way the lower class limit is the smallest number that can belong to that class and the upper class limit is the largest number that can belong to that class so all the values in that class are greater than the lower class limit and lower than the upper class limit and now you just count the frequency of each class 4 is the easy part by the way you just determine the frequency count how many are in each of those classes so here's some terminology class width is the difference between the lower limit and the upper limit of two consecutive classes lowers the smallest number that can belong to that class upper class limits the largest that can belong to that class here's the example here are 20 television prices 3D TVs the first 3D TV price is one thousand five hundred ninety five dollars the second one was one thousand one hundred ninety nine dollars and sixteen eighty five Etc all the way up to 1999 great song but that has nothing to do with class limits first step is determine the number of classes if we follow these three steps determine the number of classes there's 20 data points how many classes should we do or we could use the problem itself to hint what the classes themselves should be um these are TV prices we got the lowest TV price here of uh 15.95 the highest of 19.99 these are in looks like class width should be probably be a hundred dollars that would make sense here so we do maybe 1400 to 1500 1500 to 1600 1600 to 1700 that would make sense remember the purpose of the graphics for you is so that you understand the distribution of the data for your reader or your client it's that they understand the story that the data is telling so to determine the class width we were told five by the way use five classes so maybe my idea doesn't quite work at least for this problem I think it would work for reality five classes so 19 this is the highest value the lowest value divided by 150 so about 81 would give us a class width okay 81 dollars class width I guess if we're going to follow this without thinking that would be a good thing to do but if we actually want to think 81 dollars doesn't really seem reasonable in telling the story of the data a hundred dollars would make sense because there's something magical about round numbers in fact doubly round numbers there's two roundnesses here and there's none here oh I guess the eight if you turn on side it's got two circles but this never mind so a hundred dollars would make sense then we gotta choose 100 width so we gotta say how low to how high um beginning at 1500 makes sense if we want to just follow the directions without thinking at all 15.95 would make sense is the first class starting point 1500 if you actually want to present this to a client would make much more sense so 1600 will be this class limit of the second 1700 1800 1900 2000 will be all the classes or the class boundaries otherwise known as the breaks should not be overlapped in the class boundaries really although in all reality it doesn't matter so here the 3D thinking that butt but they can't actually overlap so now we've got our classes 1500 15 99 1600 or 16.99 and now we just calculate the frequencies two five four five four notice that if you add up to five four five and four you should get 20 which is our sample size now we're going to find something called a class boundary it is the value lies halfway between the upper limit at one class and the lower limit of the next class so the class boundary here would be 15 99.50 1699 50 17 99 50 18 99 50. or 15.99.5 if you want and the purpose of the class boundaries is to make very clear that there is no overlap and that everything no overlap and that every possible value fits into one and only one of the classes the midpoint is the upper limit plus the lower limit divided by two these midpoints are used um for estimating the average value in each class we'll see it in the next chapter in dealing with grouped data lower upper divided by 2 gives us a class midpoint of 1549.5 although I don't know a statistician alive that wouldn't say 1550 would be the midpoint 1500 plus 1599 divided by 2 gives us this is the midpoint 1600 plus 16.99 divided by 2 gives us this is the midpoint those are for frequencies if we want to do for relative frequencies we just divide the frequency by the sample size so 2 divided by 25 divided by 24 divided by 25 divided by 24 divided by 20. notice that adding up all the frequencies gives us the sample size which we're always going to symbolize as a lowercase n lowercase f is the frequency the subscript of the I is for class I so F sub I is the frequency in the I class so this calculates the relative frequency we first add up all the frequencies to determine the sample size now we just divide each of the class frequencies the F sub I's by 20 so the first relative frequency is going to be F sub 1 divided by 20. second will be F sub 2 divided by 20. then F sub 3 divided by 20 Etc cumulative frequency is just the sum of the frequencies of a given class added to all lower classes so if you are able to order your data then you can do a cumulative frequency if your data cannot be ordered that is if it's nominal and you cannot do a cumulative frequency it doesn't make sense to talk about less than or equal to if there's no ordering so here we got the frequencies and this last column is the cumulative frequency so the cumulative frequency for the first one is always the frequency cumulative frequency for the second is going to be five plus two because it's whatever is here Plus what's above four plus five plus two five plus four plus five plus two four plus five plus four plus five plus two notice the cumulative frequency always ends with our sample size little n here's an example data collected on the numbers of miles that professors strive to work daily are listed below clearly not below it's on the next side to use these data to create a frequency distribution that includes the class boundaries midpoint relative frequency and cumulative frequency of each class we're told we have to use six classes so here's the data the low is one the high is 11.9 we're told six so if we just follow the directions without thinking we're going to come up with 1.8 as our class width so we'll do class boundaries as being 1 2.8 4.6 .4 8.2 11 12.9 I whatever they are I lost count already I'm just adding 1.8 where are we I'm just adding 1.8 to each of those notice that it may make sense to do it class width of 2 instead of 1.8 one it's a lot easier to do in your head and two it makes a lot more sense in terms of presenting the data so instead of 1.8 let's round this up to 2. so the boundaries will be 1 3 5 7 9 11 13. see much easier now the lower class limit is the limit of our first class one is the lowest so if we just want to follow this we can start with the one I would say let's start with zero that would kind of make sense so it'd be zero two four six eight ten twelve that would include all the data instead of one three five seven nine eleven thirteen which would also include all the data but maybe zero two four six eight Etc looks better to the client however we're going with one so one three five seven nine eleven three five seven nine eleven thirteen so we're backing It Off by point one now we do the frequencies there's our midpoints and again don't know any statistician that would say the midpoint is 1.95 every statistic I know would say the midpoint is two four six eight ten twelve and the class boundaries and the class would be one to three three to four four to five five three to five five to seven seven to nine but notice in each case you include the lower but not the upper frequency is three three four two four two we get that just by going back to the data itself and Counting relative frequency is our frequency so F sub 1 divided by our sample size of 18 F sub 2 divided by 18 F sub 3 divided by 18. cumulative frequency is the frequency in this group plus all those lower so be 3 6 10 12. 16 . and of course we could have a cumulative relative frequency which would be just each of these divided by 18. so here's the overview again notice there's a lot of work to this if we do it by hand and if we have a realistic data set and a lot of decisions we have to make and the the distribution itself depends on those decisions so we're going to want to do several of these hopefully not by hand so we got to decide on the number of classes I choose an appropriate class with find the class limits determine the frequency in each of the classes that's a lot of work for one frequency distribution and again remember this is for grouped frequency distributions being able to automate this would would make life so much easier and allow us to look at different uh the effect of different choices we make so instead of five we'd be able to figure out well what does the frequency distribution look like if we chose 10 classes what if we chose 50 classes what if we chose two classes what would it what would it look like and what stories do each of those frequency distributions tell us about the data here are the other characteristics we got the classes boundaries midpoints filter frequencies and cumulative frequencies and we got a red star it's good because that's the last page we got two questions question number two again put this over in your notebook on the left hand side write the question and your answer below it so you can transfer that into moodle's quiz what are the steps in constructing an ungrouped frequency distribution and then question number three what are the steps in constructing a grouped frequency distribution and a hint one of those two is going to be a lot easier than the other um and what I'm getting at in these two questions don't hit pause yet what I'm getting at least two questions is you'll see that the groups frequency distribution steps are exactly the same as the ungrouped except for you have to add a few things and taking your numeric variable and making it categorical take your numeric variable at making it categorical and that's what these classes are doing you're taking your numeric variable and you're classifying them you're making them categorical and that's it section 2 2 takes what we did today and makes pictures out of it okay makes Graphics out of it plus we get to see a lot of different types of graphics and we'll see how to do all of this really fast in r so hope this was helpful talk to you later hello and welcome to section 2.2 graphical displays of data recall in the last lecture that we looked at how to create frequency distributions both ungrouped and grouped today we're in this lecture we're going to look at turning those frequency distributions into Graphics that tell the story of the data I do want to emphasize the purpose of Graphics is to tell the story of the data the graphics that we see today are going to tell the story of the data to you mainly because they look pretty ugly but we're going to see in some of the scas the statistical Computing activities how to pretty up those Graphics so that they tell the story in a pleasing way to your audience remember the graphics serve two purposes that are really the same purpose the basic Graphics tell you the story of the data and then the the the publication quality Graphics tell your audience the story the data that you learned so we're going to look at those utilitarian ugly Graphics today and in SCA I believe it's sca2 we're going to see how to create nice looking Graphics that can help convey that story to your reader um we're going to look at how to create some bar charts we have a pie chart before that yeah but bar charts uh both uh univariate bar chart that is for one categorical variable but also some bivariate stacked and sidebyside bar charts look at histogram and it's neighbor stem and leaf plot and then line graph we're going to look at different shapes of the distributions remember these these Graphics are going to tell us about the data here's some rules and what graphs or Graphics should have they should stand alone without the original data that is in your paper you provide a graphic instead of providing all of the data the graphics have to have labels and values for both axes when appropriate a legend of source and date should be included the source and the data are usually in the caption however and in a paper the graphic must contain a number and a caption so the first figure that you include is going to be called Figure one and then you're going to give it a caption a brief description of what that graphic says the graphic must also be described in the pros and you're in your writing in the paper itself ooh red star so let's go to our first question what do all Graphics need and remember you should write this over on the left hand side of your notes the question and your answer and you should hit pause to remember to gain some time here because I'm going to hop back to the lecture so we'll start with pie charts probably the worst creation of statisticians a pie chart shows how large each category is in relation to the whole the whole is represented by the entire circle the parts are slices of that pie he used to describe qualitative categorical data it's pretty straightforward you create your your frequency distribution for it you calculate the relative frequencies and then you multiply each of those relative frequencies by 360 degrees or 2 pi if if you prefer here's the example calculate the class housing types for students in this in a statistics class four types of housing these this is the frequency distribution we learned about how to calculate that in the last lecture 2015 nine and five 49 is our sample size so now we create relative frequencies so the first relative frequency is 20 divided by 49 Second will be 15 over 49 there will be 9 over 49 the last one will be 5 over 49. and then we create the central angle measures by multiplying those relative frequencies by 360 degrees so we know that the apartment has to cover 147 degrees of that 360 degree circle the dorm has to cover 31 which is 110 degrees Etc and then we just create the histogram the pie chart drawing a circle a dot in the center and then measure that this 41 percent is 147 degrees and this 31 percent corresponds to 110 degrees and this 18 is 66 degrees Etc rather difficult to create you know reality it also suffers from the fact that slices have to be different colors and brighter colors tend to draw our attention more so the 10 percent slice the gold yellowish gold slice tends to look bigger than the 10 percent that is allotted to it furthermore it's kind of difficult without the 10 and 18 actually stated there to compare the size of this slice to the size of that slice our eyes our brains have trouble distinguishing central angles and the relative sizes of those so for all intents and purposes unless someone tells you you must make a pie chart avoid pie charts a much easier chart to create and better allaround chart to create is a bar chart which we'll see next but we got the R so let's go ahead and start r if I can figure out how to do that didn't do it that kind of did it so I'm going to just start r I've got our a link to our in several places this is one of them you will probably have two links on your desktop or one or two links on your desktop but it'll be here I've got several versions of our this is an old version of art but it works for me just started first thing always new script notice there's overlap between the two in PCS there's going to be that overlap in Max there may not be so I'm going to tile vertically notice I clicked on the console window first so the console window is over here the script window is over there so the first thing I'm going to do is load data these pound signs or hashtags indicate a comment once R comes across one of them it ignores everything else in that line so I use three to indep more notice I move things around I'm going to make this side bigger so you can see it I'm going to load data from the internet I'm going to read a CSV file from the internet function I'm going to use is read.csv it requires that you specify the path to that location what we're going to look at is the crime data set remember you've got the pause button so you can pause this and type it in um notice that if I just run this and by click run this that's control r on a PC or command enter on a Mac it's going to go to the Internet it's going to go to this particular URL and it's going to read it and it's going to spit it back out on over here on the console window so this is that data set it just printed off it didn't save it R doesn't know this data set now since we are going to be using this data set we need to save it into a variable by bad habit I call this variable DT DT for data a good habit and computer scientists would want to do this is to name it something descriptive so crime will be the crime data set or they may want to do crime data set if they like their camel case or Capital C crime data set I'm bad enough at typing that I just leave it as DT I run that notice what happens over on the left it just spits out DT equals read.csv of this URL no errors because I typed everything correctly and all this data set is now stored in the variable DT if I want to see it I'm going to type DT over on the console side to run a line on the console side you just have to enter and boom it spits out the entire data set um we got a variable repub if I want to look at that variable repub I can type repub here on the on the script side if I'm going to save it and use it later or I can just explore over on this side type repub type it in hit the enter key and get an error error object repub not found um there's going to be two reasons for that error one misspelling two you didn't attach the data set R only sees one variable at the moment that one variable is DT the entire data set and there are ways of peeking into the data set for R name the data set dollar sign says Peak into and then the variable name and that will give us all the Republican values or if we're going to be doing this a lot and we don't want to type DT frequently we can just attach the data run the attach and then repub now gives us the data values so if I don't attach just type in the name of the variable gives me an error I have to do the data set name dollar sign repub to get those values or if I just attach the entire data set I can type repub or any of these variables to access them actually if all I want is the names of the variables I can use the names function similarly if I want a summary of each of the variables I can type summary this is very useful because it tells us how R sees the variables themselves if it gives a frequency distribution RC's this variable as being a categorical variable is it if it gives us the six number summary RCS this as being a numeric variable so back to repub the minimum value is a negative 0.25 the maximum is 0.24 there's three missing values the median is 0.01 I mean this 0.005 but that isn't that doesn't matter because we want to look at how to do a pie chart pie chart let's do a pie chart of census four there's our census for variable first thing we need to do is tabulate the values that is get a frequency distribution for it we do that with the table function and then to create the pie chart we just use the pi function on that frequency distribution and as you can see it's kind of difficult for me to tell which of these two is bigger Midwest or west if I look at it long enough I can tell that it's West but it took me a long time to figure that out I had to stop and think about it those aren't pretty colors but this is just for you to understand the data itself so the pi function when it's applied to a frequency distribution gives us a pie chart and to create that frequency distribution it's the table function let's see if I can get back to this okay moving on from a pie chart we go to the barge graphs or the bar charts a bar graph is just like a pie chart except instead of pieces of a circle it's bars of a certain height the height of those bars represents the frequency of the data of the values in the data set it's for qualitative data operator charts are for nonmonal data it orders the bar Heights from lowest to highest or highest to lowest your choice stacked bar graph is for to looking at two different categorical variables um where the bars are stacked and then we've got the side by side it's where the bars are side by side clearly um of all of these if I've got the the bottom two or for for two categorical variables Prado is only phenomenal the bar is phenomenal and ordinal between the Stacked and the side by side I personally prefer the side by side it's a personal preference I don't know that there's any science behind it other than it's my personal preference so here we are creating a bar graph from our data that we've already found the frequency distribution for The Apartment bar is going to be 20 High the dorm bar is going to be 15 High the House Bar is going to be nine High the sorority fraternity bar is going to be 5 high because those are the frequencies that's all there is to this and now I don't have to squint and try to estimate which of these is bigger and which is smaller and by the way this happens to be a Pareto chart because it's going from largest to smallest or smallest to largest we've got the R so let's see how we can do a bar graph or a bar chart in r function is bar plot again it's applied to a frequency distribution and there's our bar plot the height of Midwest corresponds to the frequency of Midwest states height of Northeast to the frequency of northeast states Etc if I want to turn this into a Pareto I just have to sort and now we see that the South has the most frequent uh Northeast is the least frequent pretty straightforward by the way the space that I left here and here are optional those spaces are for me as the reader to better understand what I'm typing and to make sure I don't make mistakes if I don't include those spaces there's a good chance that I will forget the closing parentheses I'll run that line and I'll get a plus mark down here plus Mark indicates that R is waiting for some more input it's waiting for something for me usually it's a closing parenthesis or a closing brace in this case I know it's a closing parenthesis two ways of dealing with it if I know it's a closing parenthesis I can just highlight that parenthesis and run it it runs the line prints out the the bar chart and we get back to a less than sign if I wasn't sure what the problem was I click over on the console side and I'll hit the escape button and then enter and I get back to the lesson sign or maybe that's a greater than sign but the key is use spaces for you so that you can tell what you're typing here's how to do the prayer chart by hand you just order it and you get that notice this is a horizontal bar chart here's how to do horizontal and r h o r i z equals true there's a comma here is stands for horizontal there's true now we've got ourselves a horizontal Pareto chart here's stacked stacked bar charts are for looking at comparing two categorical variables um in this case one variable is the sample letter and the other is housing type so in this in the apartment housing type sample a came up with about 20 of students who were in the department and Sample B came up with 33 I'm sorry 13 because this the height from here to here is 13. here's how to do this uh the Stacked side by side looks exactly the same except the heights these are stacked on top of each other they're side by side hence the name side by side let's see how to do side by side in r first we got to figure out that frequency distribution we'll use the table command tables used for frequency distributions of categorical variables I will do census for that's a comma let's do it by what names do we have available here we go looking through these there's lots and lots of variables I'm going to do Dom Paul culture as my second so this is our two dimensional frequency distribution um this 5 here says that in our sample five states are in the Midwest and are individualistic this seven is seven states in the Midwest and are moralistic this zero is zero states in the midwest are traditionalistic seven western states are moralistic 15 southern states are traditionalistic so that's our frequency distribution and now we just apply our bar plot to this copy and paste is fantastic isn't it so here we are with this a stacked individualistic all together there's 17 more holistic altogether they're 17 traditionalistic altogether they're 17. um these colors correspond to the census 4 region see if this works specify Legend equals true so the lightest is the West so this would be the western states that are individualistic this will be the southern that are individualistic this is the Northeast that are individualistic this is the Midwest that are individualistic this will be the South that are traditionalistic and the West that are traditionalistic that's one way of looking at it the data we could switch the order and now the base is going to be the region of the country and the bar Heights are going to be based on the um dominant political culture so in the midwest individualistics about five moralistic is about seven Northeast South West which of those two is better this or this the answer is it depends on what story you're trying to tell or to learn about the data if you're trying to learn about the data both are important to do if you have trouble with the colors you can specify different colors I'm going to have to specify four colors here with call col stands for colors and pretty easy we can just do one through four that's a colon it indicates through so this one colon four is the numbers one through four one two three four and now we run this and we can see much more clearly what the colors are the West is the blue the south is the green the Northeast is the red the Midwest it's the black for this second there's only three I'm going to do colors four through seven four through six four five and six would be the colors we use ew probably should have guessed what the colors were blue cyan and magenta I guess it could be worse so those are side by side I'm yeah those are stacked to do side by side we just specify we spell correctly beside equals true so they get side by side we have to specify b side equals true and now we have side by side this may be helpful to look in the north Midwest and see that the moralistic outnumbers the individualistic in the Northeast the individualistic is about double the moralistic in the South strongly traditionalistic with just a couple individualistic and the West is much more spread out and again we could change the order but we have we now have four colors we need to deal with so the individualistics are kind of spread out over the four groups the moralistic it's totally missing the South and traditionalistic is Midwest and Northwest totally missing so again which of these two is better they both tell us a story about the data therefore they are both important frequency histogram or just a histogram is a bar graph of a frequency distribution of quantitative data frequency histogram is based on your groups distribution from last lecture a relative frequency histogram is also based on your grouped frequency distribution but it's the relative frequency distribution characteristics of histograms it's a bar graph of a frequency distribution horizontal axis is a real number line it's the values of your variable the vertical axis is going to be the frequency within each of those classes the width of the bars represents a class width the bars in the histogram should touch tell that to excel because Excel doesn't let them touch unless you know how to unless you know the tricks the height of each bar represents the frequency so here's the example remember this example from last time took a long time to come up with this table frequencies getting the graph is pretty straightforward though there's two in the first class five in the second class four and the third class five then four again this was the hard part creating bar charts off of it the easy part this is for the frequency histogram and doing it for the relative frequency you get exactly the same shape exactly the same shape you just got to call the vertical axis the relative frequency here's how to do a histogram and r we'll do it on the violent crime rate in 1990 the function is hist hist for histogram there's our basic histogram that was pretty fast by default in this case particularly R is going to have class widths of 500 there's going to be one two three four five classes there's a there's an algorithm it goes through to figure out the quote optimal but there's nothing optional about these you need to actually specify or I'm sorry you need to do multiple histograms so you get a better feel for what the data are so here's how you specify it's with the breaks option this will give us about 11 breaks um breaks are very are going to be close to the class boundaries it can be similar to the class boundaries you want more than 11 breaks how about 21 breaks again the key is 51 breaks the key is what story are these telling us and what's the best story for it to tell there's a thousand and one breaks there's five breaks there's two breaks notice that we could also or realize that we can also instead of just say the number of breaks we can specify the actual class boundaries themselves also with the brakes slot we're going to use the sequence function seq for sequence the lowest is zero the highest I think was 2500 and then the third slot here belongs to the number of classes you want I'm sorry ignore that it doesn't belong to the classes you want a number of classes it belongs to the class width so if I wanted my class widths to be 100 that's what it's going to look like if I want the class widths to be 200 oh I got an error down here some X not counted oh that's because this function will set up values I'm going to run that just that part at 0 200 400 600 I'm adding 200 each time until I get to 2400 because adding another 200 will put us Beyond and the Washington DC is actually sitting out there above 2400 it needs to be counted so 2600. and now it runs 250 10 . notice how quickly we're able to create these histograms we don't have to focus on the particular calculations we can choose our class widths and allow that to tell us the story of the data much more quickly in all of these the story of the data says hey look at this outlier it's far from all the others this happens to be the District of Columbia everything else seems to be clumped nice little bellshaped ish to it and that was true when it was a hundred for the class width again bell shaped not quite so smooth but a bellshaped nonetheless when we use 2500 when we use 200 I mean this actually looks like the best of them but it looks like the best of me because I still got that outlier and I've got a nice curve over on the left so notice how quickly we did one two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen Seventeen histograms if we were stuck doing these by hand you'd still be doing the first one stamen leaf plot is an old time histogram that's also for quantitative data there's a lot of steps to it again it takes a long time to do it you're going to want to do one of these at least once but the reality is once you can do a histogram there's no reason to do a stem and leaf plot so here we've got the ACT scores the stem is going to be the tens place so this stem will be a one the leaf will be eight the stem is two the leaf is three this damage two the leaf is four and then we just collect on the stems and write out the leaves so we got 18 19 18 17 or HCT scores 23 24 27 26 22 27 29 Etc are the ACT scores it's better to do these in order by the way so the data values are 17 18 18 19 20 21 22 23 24 24 25 Etc if you want and r function is stem and here's your stem and leaf plot here's the key decimal point is two to the right of it so this is going to be zero two decimal 2 to the right of this is a decimal place so that'll be 70 it'll be 130 130 140 160 160 170. because these are by twos 260 280 280 280 300 300 310 330 340 350 390 430 430 Etc this is 24.60 this is 1240 this is 10.50 1080. line graphs the last graph for the day is use the data our measurements over time it's just a scatter plot connected the dots where the horizontal axis is time the vertical axis is our variable of interest so here's the example with the Consumer Price Index CPI is a measure of the average change in the value over time for a quote basket of goods and services for a typical American it's an index calculated by the Bureau of Labor and statistics table below shows the actual values of the cpia from several years from 1920 to 2010 actually every decade so in 1920 the CPI was 20. so it costs 20 bucks for that basket of goods in the year 2000 it costs 172.20 for that same basket of goods technically not the same basket something items were switched out and put in over time but they were equivalent when they were switched out and replaced here's how here's the line graph notice the horizontal axis is the year vertical axis is the CPI and you can look it was pretty consistent until 1970 and then the price increased dramatically over the next 40 years let's see how to do this in r um first we got to specify the variable values this was the years happened to be a sequence from 1920 to 2010 every 10 years and the CPI itself is equal to C is a function that collects several values into one vector I'm going to be switching back and forth a lot 20 16.7 14 24.1 29.6 82.4 130.7 2 0.2 finally 218 point 18.1 those are the variables we got run them notice there's no errors over here and then we just do a plot X variable comes first that would be the year in this case the CPA the Y variable comes next which is a CPI and then this has to be a line and Dot got lines and dots one way of doing that is to specify type is equal to lowercase b and there's our line graph xaxis or X variable y variable and to get both lines and dots you would do type equals B if you want lines that will be a lowercase l that is a lowercase l I notice the dots go away but you can add them back with the points command now the dots are back you're going to learn how to make this graph look much more appealing but that is for a different slideshow so here's the summary for qualitative data that is for categorical data we've got pie charts and bar graphs we've got the Pareto chart if the data are nominal side by side bar and stacked bar when you've got two categorical variables that you're looking at for quantitative data you've got the histogram you've got the stamen leaf plot which if you turn your head sideways looks an awful lot like that histogram and then we've got the line graph if the horizontal axis or the X variable is time now we got our red star it's pretty good because that's the last page here let's see how bad I've messed it up aha we got it back so question number two and again I would recommend writing the question and your answer on the left hand side of your notes so question two which Graphics can only be used for a single categorical variable key is the word single and the word categorical go ahead and hit pause question number three which Graphics can only use for a single numeric variable single numeric and both question two and question three have multiple answers don't forget to submit these into Moodle and that's it I hope this was helpful take care hello and welcome to section 2.3 an additional section on analyzing crafts this is not computational in structure this is more of an interpretation section and a better understanding of what makes a good graph and in many ways more importantly what makes a bad graph so the objective is to identify misleading characteristics of a graph with the hope that you will avoid them yourself there we go so title access Source this is how to properly label a graph the middle part really is the only thing done in your statistical program the title and the source and the caption and the numbering is done in your word processing or typesetting program so the bar part this middle part is all that's done in your statistical program the title and the source stuff is done in your word processor um a Time series graph is a line graph that is used to display a variable whose values change over time we've seen this already we called it a line graph in the last section um this should ring a bell as something like panel data from chapter one it should kind of should be able to go back over your notes press pause go back over your notes and see what panel data was this is a way of describing panel data for one or a very small number of people a crosssectional graph is a way of displaying information collected at only one point in time again that should remind us of something we talked about in chapter one about crosssectional data a pictograph is a bar graph that uses pictures of objects instead of bars they look really spiffy but they're really dangerous so let's go ahead and we got a red star so let's go to our first question have you picked a graphs differ from bar charts so again over on your left side of your notes right how do you pick the graphs differ from bar charts answer that hit pause right now and you're back so let's move on so here's an example of how pictographs can be deceiving the one that you're going to see in the newspapers or the one on the right and the one on the left is the quote correct monthly housing there are three at least three things wrong with the the incorrect pictograph for one I'm going to draw your attention to the axis values and compare here two I'm going to say what is the area of the 94 versus the area of the 2006 House on the Left well the 94 I'm sorry the 2006 is double the 94. which corresponds to it actually increasing by double hint it's a ratio level variable so we can meaningfully talk about doubling and having whereas in the incorrect going from 94 to 2006 it doesn't double in size it quadruples in size because not only are you doubling the height but you're doubling the width to keep the right form so our eyes are saying oh my goodness it increased by a factor of four even though we try to focus on the scale on the left and it only increases by a factor of two our mind our our gut reaction to this is it increases by factor four so that's another way that this particular picture graph is deceiving I've given you two question number two what are three things misleading about this particular pictograph and I would recommend that again you right over on the left this question your answer also pause and go back and look at the that particular pictograph see if you can figure out another thing that's misleading about it scaling of graphs another important feature to look out for is that the graph is scaled properly if you stretch your strength shrink the scale on the yaxis the shape of the graph may change dramatically a line that raises gently on one scale may look very steep with a different scale make sure that you choose the correct scale and by correct I mean the scale that forces the graphic to show you this the story of the data in other words make sure that it represents the data well you don't want you to force the data to tell a story your graphic should allow the data to tell its story takes practice so here's an example looking at this quickly really doesn't it doesn't seem like there's anything wrong here consider the graph below the U.S federal minimum wage hour rates unadjusted for inflation what errors can you find in the graph how should they be fixed so the dates along the xaxis so this is a Time series plot also known from last time is a line graph vertical axis is the minimum hourly wage in dollars so as you over time things increase in terms of dollars and it looks pretty awesome because you're minimum hourly wage is increasing over time so what's wrong with this well the fact that in 1956 the dollar doesn't quite doesn't buy quite as much as a dollar did in 19 in 2008. so perhaps we should instead of unadjusting it we should adjust for inflation notice the xaxis does not have a consistent scale the ears are as few as one apart I didn't even notice that my goodness there's one A Part here there's six apart here there's five of Parts six five two four Etc so you should stretch this shrink this so that each inch along the horizontal axis corresponds to a single uh time limit so maybe each of these marks should be three years instead of something so change the x axis correct graph can be found in exercise 12 and the chapter 2 exercises but that's not the only thing maybe you should adjust this for inflation now we're going to talk about some shapes of graphs basic four basic shapes or uniform symmetric right skewed and left skewed the right skewed I will frequently called positively skewed and skewed to the leftover frequently called negatively skewed simply because I have trouble with left and right the the name outliers the data value that falls outside the quote normal shape of the graph or the typical shape of the graph we've already seen outliers back in section 22 the District of Columbia was an outlier in terms of the of the violent crime rate in 1990 its little bar in the histogram is far away from the rest of them so here's what a uniform distribution looks like the frequency of each class is relatively the same if you're dealing with sample data it's unlikely that you'll get bars all the exact same height even if the population has a uniform distribution to it symmetrical data lie evenly in both sides of the distribution whatever that means I want to talk about the skewed two skews then come back to the symmetrical that might make this a little bit more meaningful skewed to the right or positively skewed the key for skew is to locate the tail the tail is the side where the data just keeps lingering on and on as opposed to the in this case as opposed to the left side where it just kind of stops quickly the right side just keeps going on and on so there's a right tail here so this is skewed to the right or skewed positively skewed right because the tail is on the right skewed positively because the tail is on the positive side of the middle contrast that was skewed to the left again the data just keeps going on and on and on it stops rather quickly here so this is left skewed because the tail is on the left or it's negatively skewed because it's on the negative side of the middle with that said symmetric data have neither a right skew nor a left skewed notice here it kind of stops at about the same distance it it's not perfectly symmetrical in the mathematical sense the key and here's the key that I tell people it's in order to if you want to conclude that the data seems symmetrical it's try to determine which side gives you the tail if you have to stop and think a little bit about it like at least a second then it's symmetrical enough we're actually going to in the next chapter come up with a definition of symmetrical and the way determining if the data are quote symmetrical enough but for now just look and see okay are the data obviously skewed right or obviously skewed left and if the answer to both of those is no then it's symmetrical enough so let's describe the overall shape of this distribution the middle seems to be somewhere around here it's doesn't seem to have a tail on either side I mean if I squint it hard I could probably figure out a tail but because I'm not seeing the obvious tail I'm going to say this is symmetric the average is somewhere around 80. I don't see any outliers if there is one bar way way over here I'd say yeah there's an outlier if there's a bar way way over here I'd say yeah there's an outlier um so there's the smooth curve that they're talking about the average seems to be about 80. yeah symmetric no outliers red star so the third one how does one determine the tail of a distribution of a graphic go ahead and hit pause hopefully your answer is written in your notes so that you can later transfer them to the Moodle quiz and that's the end of this rather short chapter or a short section it actually is the end of this chapter so make sure you keep sending me those questions post those questions to the discussion board and I will talk to you later take care hello and welcome to section 3.1 in your Hawks book this is going to talk about measures of center a measure of center is a single statistic that tries to summarize your entire data set so it takes your data set boils it down to one single value um next lecture we're going to talk about how to determine the quality of that single value as a representation of your entire data set but today we're just going to look at how to summarize your data set with one single value and as you can imagine that's going to depend upon what your data actually are what level your data are so topics today calculate the mean median mode determine the most appropriate measure of center so we're moving on to the mean then clearly the median then eventually the mode definition the mean is the arithmetic mean of a variable is one of the most important statistics calculated and provided because it is one of the most important statistics calculated and provided in other words there's nothing special about the mean above and beyond the median of the mode except for four things that I could come up with I mean I racked my brain and these are the four reasons that we should be looking at the mean Carl Friedrich Gauss thought it was important the sample mean is easily calculated at least it's more easily calculated than the other two measures of center um the sample mean can be used to calculate missing data values that is if you have 10 data values you're missing one of those 10 but you have the mean you can figure out what that missing value is I think that might be helpful probably not and the sample mean has a nice distribution which we're going to learn in chapter 7 when we talk about the central limit theorem I think this last one is the best reason of all other than the fact that it's important because we've determined it's important you'll hear this talk again when we get to What's called the normal distribution we'll discover that the normal distribution is entirely the most important distribution in all of Statistics because it's the most important distribution in all of Statistics anyway moving on here's how you'd calculate it is the sample mean X is just going to be a generic variable the bar is what tells us it's the sample mean so X bar is going to be the sum of all the X values divided by the number of X values so this could be the first X Value Plus the second X Value Plus the Third x value dot plus the nth or the last x value divided by n in other words you add up all the data to divide by the number of data values this capital sigma and it's actually a stylized Sigma indicates the summation you're adding up everything to the right with a subscript of I so you're adding up all the i x values and dividing by the sample size contrast this with the population mean okay there's not much to contrast the sample mean deals with the sample population mean deals with the population the sample mean you've got an X bar population mean you've got a mu it's what a Greek cow says mu you're adding up all the values of the of the variable in the population divided by the population size for the sample mean you divide by the sample size for the population mean you divided by the population size again we're adding up all the X values divided by these population size nothing special so we got an example students from our previous stat toner classroom surveyed find out the average number of hours they sleep per night during the term here's a sample of their selfreported responses 56810469 I want to calculate the mean all we do is add them up and divide by seven y seven because there's seven data points so we're going to add them all up and divide by seven by the way 5 is x sub 1 6 is that sub 2 8 is x sub 3 10 is x sub 4 4 is x sub 5 6 is x sub 6. 9 is x sub seven so x sub we're adding up all the X's where I ranges from one through seven and dividing by n 48 over 7 is about 6.9 so the sample mean for the number of hours that the students reported sleeping per night during the semester is 6.9 hours wow that's pretty lazy embed 6.9 hours every day on average come on I'm kidding by the way um so here's how we do it in R we Define a variable sleep we could call this a if we wanted to or X if we wanted to I'm calling it sleep because sleep has some meaning there's the C function it combines everything to the right into a single Vector of values and then to calculate the mean of sleep we just calculate mean of parentheses sleep that's it for a small data set like this whether or not it's easier to calculate it by hand or using R that's up to you but the reality is most data sets that we care about are going to be of size 2 3 4 000 Maybe two three four million and we'll want the computer to do those calculations for us example 3.2 I alluded to this earlier let's go ahead and do it we're going to use the mean to find a missing data value Rutherford who is a famous physicist downloaded five new songs from the internet he knows that on average the songs cost a buck 23. if four of the songs cost about 29 each was the price of the fifth song he downloaded in other words we're missing that the the price of the fifth song but we got the mean and we got the value of all the others let's see if we can determine or bring back that last price so here's the solution we're just going to substitute in the values we know here's the function for the population mean times substituting the values we know Buck 23 was the average of those five values four of those values are a buck 29 we don't know the fifth doing some algebra we come up with that missing song was priced at 99 cents so the cost of Rutherford's fifth song was just 99 cents now this is one of the strengths and where the weaknesses to the sample mean or to the mean in general it's the same thing it equally depends on every single data point that's an observation that's a feature of the formula which could be a strength or weakness here is the strength because we're able to recover a missing value however if that missing value were a an outlier it the mean may not be representative of the data set as a whole sometimes we don't have the actual data but we have summarized data this would lead to us maybe perhaps wanting to calculate a weighted mean or the mean of the summarized data the weighted mean again this will be for the sample there's a bar on top of the variables so this this will be an X bar or a sample mean these are summations the W's represent the weights and the X's represent the observed values so the weighted mean is just going to be the sum of the values times the weights added up divided by the sum of the weights where might this be most important for you a lot of your classes are going to use weighted means for determining your final grade here's an example the syllabus my discrete mathematics class states that the final grade is determined by the midterm homeworks the discussions and the final grade sorry final exam midterm counts 40 towards the final homework 20 discussions 10 final exam 30 towards the final grade so we're waiting the midterm grade by the term exam grade by 40 percent we're awaiting the homework grade by 20 discussions by 10 final exam by 30 percent so there's two students in the class Bob and Virginia want to calculate their final grades or in this specific instance estimate their final grades below are their average grades in each of the categories for the midterm homework discussions and they've also guessed at what they might score in the final exam so midterm homework and discussions they know their grades the final they're going to estimate here it is for Bob he got an 83 in the midterm and 98 on the homework discussions you got 90 percent fantastic he thinks he's going to get an 87 on the final we'll see but he thinks he will if these numbers are correct then what is his final grade for the course this is for genius we'll come back to that later notice that Virginia did much worse than the homework and discussions she's going to think she's going to do much better in the tests so here it is for Bob first thing we need to do is determine which numbers are the values and which are the weights um the grade earned in each category is weighted by the percentage for that category so for instance Bob's test of 83 gets away to 40 percent so that 40 is going to be the weights and the 83 is going to be the values here's a nice little table so these are the scores that Bob got according to the syllabus these are the weights for each of those categories and technically the 87 he hasn't received yet he thinks he's going to get it he got an 83 on the midterm so a good chance of getting 87 at least on the final so to calculate the final grade down here we're just going to multiply the score by the weight we call that the contribution to the final score times weight score times weight Square Times weight add up this last column to get his final grade 83 times 40 percent is point is 33.2 98 times 20 is 19.6 and that should add up to 87.9 so if Bob is correct and he gets an 87 on the final he'll get an 80.87.9 percent for the course which is a B plus if we look at this in terms of mathematics we're adding up all the X's times the W's divided by the W's notice that the W's all add up to 1.4 plus 0.2 plus 0.1 plus 0.3 this top is just each of those values in the last column adding them up gives us an 87.9 there it is for Bob let's look at Virginia the X's are the genius grade in each category we got the the weights let's do this using r so the weights are 40 20 10 30. Virginia's actual grades are for 95 45 66 and she thinks she's getting a 90 on the final she got a 95 on the midterm there's a good chance she'll get a 90 on the final this formula and R corresponds the actual definition of the weighted mean so the sum of the values times the weights divided by the sum of the weights the sum of the values times the weights divided by the sum of the weights from this we decided we find out she Lambert within 80.6 for her final grade that is after we run it this line gives us 80.6 which is a B minus not bad considering she did so poorly on everything but the tests now as an aside from a teaching standpoint let's take a moment and consider the effect on her grade if her test score was low but her homework and quiz scores were high which should come out with the same grade in other words what lessons can you draw from this with respect to your own activity in the course hint hint hints and now extension here are some additional questions that we can answer for Virginia what's the highest grade she can earn what's the lowest grade she can earn and what does she need on her final to earn a 70 percent in the course which is the lowest passing grade well for the highest grade she can earn it occurs when she gets 100 on the final so you put a hundred percent in here you do these calculations and the highest that you can get is a b a low B but a b nonetheless the lowest grade she can earn happens when you put a zero in for the final grade and discover that lowest grade is a 53.6 percent which is an F now the likelihood she gets a zero on the final is is pretty low considering she got a 95 on the midterm so the next question is what does she need to earn on the final in order to pass the class 54.7 is the correct answer how did I determine that well I just kept changing this last number and rerunning this code until I got this last quantity to be 70 percent we got a red star which means we need to go to our intra lecture question one what is the largest number of means that a variable can have again write the question on the left hand side of your notes answer it below so you can put it put it into Moodle later and pause and we're back moving on to the median here's the definition of the median the gut definition first the median is the quote middle value in the sense that about half of the data is above it and about half is below it yeah that's a nice gut definition doesn't work mathematically because the word about so mathematically here's the actual definition of median uh the median of a set of data is a value notice it's a value not V value so it could be more than one median in a data set a value such that at least half of the data is at least this value greater than or equal to and at least half the data is at most this value less than or equal to kind of complicated definition so we go back to the gut definition of okay it's it's the middle value about half's above and about halfs below if we need to calculate it this is the mathematical definition if we want to understand what it actually means we use the get definition so here's how you actually calculate the median of a data set by hand you list the data in ascending order from lowest to highest in other words you're making an ordered array if the sample size is odd the median submittal value in that ordered array if it's even that it's the mean of the two middle values in that ordered array note that this implies at least two things one the median like the mean may not be a value in the data set and two the medium may only be applied to ordinal data if n is odd or if there's if the middle two values are the same level so let's calculate the median given the number of absences for samples of students in two different classes find the median for each sample first step is order one two three four five six seven there's seven data points here so it's going to be the middle data point after we order the data and for B there's eight values so it's going to be the middle of the the arithmetic mean of the two so we put it in order it's going to be six is the median we put an order seven and eight are the two middle values the mean of seven and eight to seven point five that's the median again the median like the mean does not have to be a possible value in the variable here's how we do it in R the function is median and all you have to do is give it the data not too hard ooh red star so let's move on to intra lecture question number two what is the largest number of medians that a variable can have again write it to the left in your notes answer below so that later you can transpose it into Moodle pause and back we are third one is mode the mode is the most observed value if you're dealing with data it's the most observed value if you're dealing with the population it's the most observed value here's how we calculate by hand you just do a frequency distribution remember that from chapter two a frequency distribution the data and find the value that occurs most frequently some terminology if there is only one value that occurs most often it's called unimodal Data where the variable is termed unimodal if exactly two values occur equally option it's bimodal data or the variable is bimodal if it's more than two values that occur equally often it's multimodal however if the data values only occur once or an equal number of times each we say there is no mode so here's some examples finding the mode for the first one I see two sixes but I see three sevens so seven is the mode of a and a is unimodal because there's only one mode for B I don't see anything occurring more than once so there is no mode C I see seven and two both occurring twice so the modes are seven and two and it is bimodal and for D everything occurs twice so there is no mode which is what these Solutions say doing this in R note that the mode is not really a helpful measure of center for most data that we deal with therefore most statistical programs don't have a function for the mode I've created one for you it's called modal that's the function modal and it just takes the data however you need to Source this file before modal is an appropriate function so run these two lines and you'll get the mode of this data set which should be seven will be seven that should be in fact I would recommend that this line would be one of the first that you run in every script for this course and you'll see it more and more frequently example three seven let's bring it all together look at the mean the median the mode for the data here's the data eight data points here we've calculated in R can we load in this data I'm sorry we load in this functionality here's the data the mean the median and the modal of that of that variable remember that the pound sign or the hashtag or the octathorpe or whatever you want to call this indicates a comment R ignores everything to the right of it so this is just a comment to remind me what the mean age is what the median age is and what the modal age is notice the mean is far away from the median and the mode it's probably due to this outlier everything else seems to be really close to 80 except for this 42 year old who retired we can see this on a graph horizontal graph this is actually a Dot Plot we've got our outlier at 42 and it's an outlier because all the rest of the data is so far away from it the mode happens here at 80 only because two people retired at age 80 the median is going to be pretty stable it's going to be really close to the middle of the data set and the mean is going to be really influenced by this outlier which brings us to a question of we've got three measures of center which one should we use for nominal data the mode should be used for ordinal data the mode should be used if it has to be the median should be used if it can be when can it be it's when the either the sample size is odd or the sample size is even and the two middle values are the same in other words we can use the the median for ordinal data when there's no need to actually take an average because average requires at least interval level data for numeric data the median should be used however if the numeric data is sufficiently symmetric the mean can be used and we like to use the mean because mathematically it behaves much more nicely than the median and really that is the only reason we want to use the mean now how do we determine if some data set is sufficiently symmetric we use What's called the Hildebrand rule the Hildebrand ratio is defined as the difference between the mean sample mean and the sample median divided by the sample standard deviation and you'll talk about the standard deviation next lecture if the size of the ratio is less than 20 less than 0.20 and the data are sufficiently submits sufficiently symmetric if H itself is greater than 0.2 then it is positively skewed if H is less than negative 0.2 then it is negatively skewed the variable age is not sufficiently symmetric the ratio is negative 0.312 we know that from running these three lines and actually if we have already run the first two lines in our R session we just need to run the Third since the data are not sufficiently symmetric we should not use the mean we should use the median as it is closer to most values AKA it's more typical of the data then is the mean at least that's what I'm supposed to tell you teaching introductory statistics the reality is you should give both values and interpret both values correctly sample 3 8 and that gives you data on these and just describing the variables which measure of center is appropriate Tshirt size is small medium large extra large well that's ordinal so at least mode if there's no incons no need to to take an average then the median would be appropriate salaries for a professional team of baseball players that would be median most likely um the star will be an outlier the rest of the players will tend to be at a little clump as it was back here the star will be not at the low end it'll be at the high end and everyone else will be clumped towards the middle so this would be a median see price of homes in a subdivision of similar homes since the homes are similar then everything's going to be clumped together so the mean would most likely be correct and Professor rankings from student evaluations of best average and worst that's ordinal so it's at least a mode hopefully we'd be able to use a median and we'd be able to use a meeting if we don't need to take a average of the middle two values oops so now we're looking at graphs a b and c a is the mode it is the most likely value B is going to be the median because about half of the shaded areas to the left and about half is to the right and C is going to be the mean notice that the mean is to the right of B because the data are right skewed if the data were left skewed then C would be to the left of B so here's the summary property is that the mean the median and the mode these are kind of important they just summarize what I've talked about for the last 20 minutes the mean is affected by outliers the median is not affected by outliers I mean there is a single value comedian there is usually a single value the way we calculate it but by the strict definition of the median it's a single value if n is odd and an infant number of values of n is even functions are mean median and modal remember you have to Source in that extra file in order to use modal so I guess that brings us to question three what is the largest number of modes that a variable can have again write this question on the left of your notes write your answer underneath of it so you can transfer that to the Moodle quiz and pause and we're back and we're done the key is not to be able to calculate these by hand you can get the computer to do that for you the key is much more difficult it's to be able to interpret these values what does it mean if the mean is 14 and the mode is 47. what does it mean if the mean is 14 and the median is 47. does it mean that the mean the median and mode are the same value what does it mean if the mean and the median are the same value and the mode the well there are two modes in the key here is to be able to interpret the values and you'll hear me say this over and over again as we move forward in this course the key is interpretation the computer can do the calculation for you you need to be able to interpret what the computer does and in many ways that's harder to do but it makes the calculations much more useful and so thank you and I will talk to you later hello and welcome to section 3.2 measures of dispersion purpose of this section is clearly to determine ways of measuring the dispersion of a data set what do we mean by dispersion we mean how spread out the data are hence these are also called measures of spread we're going to look at four measures of spread in this lecture postponing the fifth one the interquartile range until the next um so here's the gut definition of a measure spread it's a measure of how much we can expect a value of the data to differ from the appropriate measure of center so because of that that means that we've got measures of spread for the mode we cut measures the spread for the median we've got measures of spread for the mean we've got lots of measures of spread all of them trying to Define this and trying to measure the same thing the spread of the data or we can also think of spread and dispersion as being the uncertainty in your data value that is if I've got a data data set with a very small level of dispersion you're much more certain about where that individual value is going to be than if you've got a very very large disperse largely dispersed data set where you got a value and you have no idea if it's to the left tail to the right tail close to the middle Million Miles Away note that there are many many measures of dispersion how one Define spread or dispersion and what properties one wants in such a measure determine the formula that is being used so be aware of that I see a red star so let's pop over to the intellecture question number one what is the main purpose of a measure of dispersion AKA a measure of spread again I recommend writing on the left hand side of your notebook the question your answer underneath of that so that when you do go into Moodle to finish this quiz you'll have the answers right there and of course hit pause and we're back the first measure of spread and the weakest one that we've got is just the range the range of your data is just the largest value minus the smallest value so the range is a single number it's not two numbers it's a single number so if we're looking at Heights of students in this stat 200 class the range is going to be 13 inches 13.2 inches actually so a single number the difference between the highest value and the lowest value that's it since it's based on two specific values the range is going to be very unstable that is if I send three people out to draw a sample from the same population you're going to get different samples by the way then the ranges have a very strong chance of varying quite drastically between the two samples that's what I mean by unstable or it's not robust a much better measure of dispersion is called the variance this one is for the population variance the population variance is the average squared distance of the population values from the mean so I've got my cursor so x i is a data value or a value in the population mu recall is the population mean capital N recall is the population size and we're going to call Sigma squared the population variance that will be the symbol for the population variance and what we're doing is just adding up every value in the population minus the mean we're going to square that add them all up and divide by the population size similar to the population variance is the sample variance this is the one that's actually going to be useful for us notice that the formula looks exactly the same except for two three parts let's call this three parts part one is the symbol for sample variances and S squared for the population variance it was Sigma squared recall that population parameters tend to be Greek letters and Sample parameters tend to be Latin letters so it's s squared for the sample variance Sigma squared for the population variance that's one difference the second difference is you're subtracting off X bar which is the sample mean for the population variance you're subtracting off mu the population mean here we're subtracting off X bar the sample mean and two instead of dividing by n we're dividing by n minus one and we'll explain why a little bit later so again it's each data value minus the mean of the data squared added up divided by m minus 1. and that will give you the sample variance so let's calculate some variances very small simple toy data set here so let's assume the data represent the actual weight changes for a sample of fitness club members so for a we're going to calculate the sample variance and for B assume that the data represent the actual weight changes of every member so for B we're going to calculate the population variance of these five values so again notice the numbers are meaningless unless you add context to them does three two five six four is that the sample or is that the population you need to know by the way in this class in less stated otherwise that's going to be a sample so let's do the calculations here's the function here's the formula for the sample variance we've got the values x sub I which are just the three two five six four X bar is going to be 4 because the sample mean of 32564 is 4. n is 5. which means the N minus 1 is going to be four so all we have to do is take each of those X values subtract off 4 Square it add them all together and then divide by 4. and that's what we're doing here data values the deviations which is the data value minus the sample mean notice if we add up the deviations we get zero which is a good thing the squared deviations the x i minus the X bars yeah the bars there somewhere squared notice we add these up we don't get zero and now if we add these up one four one four zero gives us ten and now all we have to do is divide by little n minus one 10 divided by 5 minus 1 is 2.5 so the sample variance is 2.5 we can do this in r for example of size 5 you may not need to because it's very straightforward putting this table together and doing all the calculations but once we get into sample sizes of fifty hundred ten thousand you're going to want to be able to do this on a computer the function to calculate the sample variance is VAR VAR VAR for variance first line we Define a variable called weight we're going to put inside of weight five values a 3 a 2 a 5 a 6 and a 4. notice you've got the little C to combine all of these values into one variable and then we calculate the variance of weight that's it so now B remember was for a population variance we can do this the long way we would essentially just end up dividing by n five instead of n minus 1 or we can do this on a computer for a sample size 5 and not a big deal for a sample of size 5000 that is a big deal so again here's our data to calculate the population variance you could do the sample variance times four over five n minus 1 over n y n minus 1 over n well let's go back to the formulas there we go we want to calculate this number we've got this number notice the difference is the denominator so we're going to multiply this number the sample variance by n minus 1 here it'll cancel out the N minus ones n minus 1 over capital N so what you'll be left with is just the sum of the x i minus the mean squared over capital n so four over five if you're going to calculate population variance a lot you may want to create a function for it this is the first place that we see the extensibility of r will create a function called varpop the variance of the population I guess we're going to set it equal to this keyword called function we're going to give it just one variable X that'll be the data that's a brace an open brace and that's a closed brace down here and then the actual calculation is just x minus the mean of the x is squared and the average of those it's the average because you're dividing by the number capital n and you would use it as VAR pop of weights you would have to create this function in every script that you need the population variance in thing is we rarely have the entire population we tend to deal with just the sample populations hence the VAR is the sample variance so here's the big question where does the formula actually come from what does it tell us what does it do for us so we can think of variance as an average distance that the value is going to set Lie from the mean maybe they lay from the mean I think they lie from the mean so it's while it's not an actual average when we're thinking of the sample variance here and the variance is usually very close to the average and conceptually it's a good way to think about what this variance is actually calculating um so on your notes I'm sure you got the the variance formula there so to derive this formula for variance we're going to use a method similar to finding average distances or squared deviations from the mean so first we must know the actual deviation from the mean well that's just x i minus X bar that's a deviation that's how far that individual value is from the mean then we're going to square those to get a distance or technically a squared distance and then we're going to find the average of all those how do you find an average well you add them out divide by the sample size well we're not entirely dividing by the sample size we're dividing by the sample size minus one but essentially this is just the deviances squared so it's a squared distance and this part will just be an average squared distance from the data point to the mean so y n minus 1 and this is pretty important the fundamental purpose of sample statistics is to estimate a population parameter so the N minus 1 is there so that s squared is a good estimator of Sigma squared now that's the reason for n minus 1. and then mathematics is the other reason for that um laboratory activity d d as in dog looks at estimators and what makes a good estimator and what do we mean by a good estimator and at that point you'll see oh yeah unbiased estimators are all things equal are good things and dividing by n minus 1 gives us an unbiased estimator of Sigma squared the standard deviation is closely related to the variance standard deviation is also a measure of how much we might expect a typical member of the data to differ from the mean in words definition seems very very similar in fact it's probably exactly the same I like to copy and paste things the only difference is the formula and what we mean by how much we might expect it to differ Sigma squared is the variance Sigma is the standard deviation those are for the population by the way so the population standard deviation is just the square root of the population variance and the sample standard deviation is equal to the square root of the sample variance so the big question comes up if the sample standard deviation the sample variance or the the population standard deviation and the population variance really tell us the same information why are we pardon me why are we introducing the standard deviation and the reason is the units of the standard deviation are going to be exactly the same as the units of your data so if your data are inches your standard deviation units are going to be inches if your data are years your standard deviation data standard deviation units will be years for the variance it'll be the square of that unit which is much more difficult to see on a histogram I can see things that are in the same units of the data because the histogram is in the units of the data but see in terms of the square that's much more difficult to see thus standard deviation is much more interpretable and should be used in all of your papers instead of the variance which brings up a related question of why do we cover the variance first the reason we cover the variance first is because it's much more intuitive as to what it's trying to measure the square rooting of the variance just brings the the units down into what we're used to also mathematicians love to use the variance because variances add standard deviations do not add so mathematicians like the variance everybody else on the face of this planet love the standard deviations ooh we got a red star so question two why should one report the standard deviation instead of the variance again to write the question on the left hand side answer it underneath go ahead and pause and we're back so let's calculate the standard deviation so let's find the stand sample standard deviation of the data shown below yeah I challenge you to do to do this by hand here it is an R just take the data wrap it in the C function send it to the variable since there's no context to what these numbers actually mean we'll just say x is equal to those values and then the function to calculate the sample standard deviation is just SD that's it if you need to calculate the population standard deviation where you're going to have to go back to the function you created varpop calculate the population variance and then take the square root of that sqrt is the square root function but again statisticians rarely calculate population parameters they estimate them but they estimate them with the sample statistics hence SD is the sample standard deviation and VAR VAR is the sample variance here's a use or I guess we're going to say it's an application of the standard deviation um Financial people love to use variance and standard deviation because they indicate risk of an investment so we're looking into investing a portion of recent bonus into the stock market I guess Mark's doing it we're not while researching different companies he discovers the following standard deviations of one year of daily stock closing prices the standard deviation is a dollar two yardsmith it's 9.67 in other words the standard deviation of yardsmith is much larger than that of profacto so all things being equal there is much more variability in yardsmith than there is in profacto so if you want a nice stable stock investment you'll go with profacto instead of yard Smith which is what this solution says hence stable note that looking at standard deviations is just one component of evaluating market prices or youth or plans or things like that another measure of spread is called the coefficient variation the coefficient of variation is just the standard deviation divided by the mean times 100 percent if you're looking at the population coefficient of variation it'll be Sigma over mu if you're looking at the sample coefficient of variation will be S over X bar why do we introduce yet another coefficient variations because coefficient variation is used compared to versions for two variables hence our previous example which I don't want to say in front of anyone from Hawks this is completely useless because if profacto has a standard deviation of a dollar two but it trades at a dollar or that's an incredible change uh incredible width of it whereas yard Smith it's 967 but if it's trading at a thousand dollars a share that's really not much of a variation so while we do like to look at the standard deviations being a measure of risk really it should be the coefficient of variation which this gets to example 314 we got two graphs we're trying to figure out which has that greater standard deviation relative to its mean in other words which has a greater coefficient variation if we ignore the bottom numbers which of these two Graphics is more spread out and it's very clear that the price of U.S Farmland is much more variable if we ignore the bottom numbers much more variable than is the annual average rainfall and that's what the coefficient of variation really does it ignores the bottom numbers and just looks at the shape of the graphs or quantifies how spread out the graph is we can do the actual calculations find out the coefficient of variation for data set a is 28.9 percent and for B it's 36.7 percent that really doesn't surprise us because we just decided that b is much more spread out than a is and that's what again that's what the coefficient of variation is measuring it's how spread out is the data if we ignore the actual values at the bottom and dividing by that X bar allows us to ignore those values at the bottom so those are measures of of spread or measures of dispersion the last two topics for this section really don't sit well with that that idea of measures of center measures to spread they really probably should be on their own section but this is where Hawks is putting it and between the two of us good as places any to put them we're going to look at the empirical rule and chebyshev's Theorem the empirical rule says that when the data follow a bellshaped distribution an interesting pattern emerges in the data values about 68 of the data is within one standard deviation of the mean that is about 68 is between the values of mu minus Sigma and mu plus Sigma about 95 percent is within two standard deviations the mean that is about 95 is between mu minus two Sigma and mu plus two Sigma and almost all of it 99.7 within three standard deviations of the mean and that's all the empirical rule says notice it's when the data follow a bellshaped distribution these are good estimates if the data do not follow a bellshaped distribution these are not necessarily good estimates in fact they tend to be pretty poor so if the data follow a nice little bell shape here then six to eight percents within one standard deviation the mean 95 within two 99.7 within three and almost all is within four so here's an application of it distribution of Weights of newborn babies is bellshaped with a mean of 3 000 grams wow those are heavy babies three thousand grams and a standard deviation of 500 grams so we know that 68 percent of the babies are between three thousand minus five hundred and three thousand plus five hundred 95 within three thousand minus two times five hundred and three thousand plus two times five hundred and ninety nine point seven percent is within three thousand minus three times five hundred and three thousand plus three times five hundred so what the empirical rule says hey what percentage of the newborn babies weigh between two thousand and four thousand grams that's 95 percent because two thousand is two standard deviations below three thousand and four thousands to standard deviations above it so it's within two standard deviations of three thousand what percentage of newborn babies weigh less than 3 500 grams that one's not going to be so easy we do know that 68 are between 2500 and 500 I'm sorry twenty five hundred and thirty five hundred which tells us that 34 is between three thousand and thirty five hundred we also know that half is less than three thousand so we can use that calculation to get the answer and we can calculate the range of birth rates that would contain the middle 68 percent so it's between 2500 and 3 500. now here it is written out since we know the distribution the data is spell shaped we can apply the empirical rule we need to know how many standard deviations two thousand grams and four thousand grams are from the mean here are the calculations it's two below to two above according to the empirical rule approximately 95 of the values lie within two standard deviations the mean so it's 95 percent B takes a little bit more work how many standard deviations the weight of 3500 is away from the mean that's one above so it's one standard deviation above the mean says that 68 of the data values lie within one standard deviation the mean that means that 34 percent lie between the mean and one above and half is below 3000 because it's a bellshaped curve symmetric so the mean and the median are the same so if the mean is 3000 then half will be less than or equal to three thousand so the 34 and the 50 add together to give us an 84 percent of the newborn babies weigh less than 3 500 grams here's a picture of it 2500 to 3500 contains 68 percent since it's symmetric that means between 3000 and 3 500 we've got 34 percent and a systematic distribution so half of the data is less than three thousand so the part that's not yellow is just 50 plus 34 which gave us the 84 percent and then C also an easy calculation we know 68 of the data lie within one standard deviation the mean that's between 2500 and 3 500. and the last topic is chebyshev's theorem whereas the empirical rule is a nice rule of thumb if the data comes from a bellshaped distribution gets you really good estimates chebyshev's theorem is a mathematical theorem that is always correct it's not always helpful but it's always correct and the theorem statement is the proportion of data that lie within K standard deviations the mean is at least one minus 1 over K squared and K's got to be greater than one so if K is 2 then within two standard deviations the mean is at least threefourths could be more than that could be a lot more than that but it's at least threefourths and the proportion of the data within three standard deviations at the mean set k equal to three is about 89 percent so at least 89 of the data is within three standard deviations of the mean could be a lot more than that could be just 88.88889 percent so shabby chefs always works but it doesn't necessarily give us a good estimate empirical rule doesn't always work but when it does is applied to bellshaped data it does give us a good estimate so here we'll apply Chevy Chef's theorem suppose that in one small town the average household income is three thousand thirty four thousand two hundred dollars with a standard deviation of two thousand two hundred dollars what percentage of households earn between 27 6 and 40 800 dollars oh we just have to figure out how many standard Devi deviations both below the mean are 27 6 and how many both mean are forty thousand eight hundred that will tell us what the value of K is and then we need Chevy chev's theorem to determine the minimum or the at least as much so 6600 is the distance we know that the standard deviation is 2200 so that's negative three so 27 600 is three standard deviations below the mean similarly forty thousand eight hundred is three standard deviations above the mean thus K is three and we know that at least eighty eight points nine percent of the data is within those bounds could be a lot more can't be less it absolutely cannot be less than 88.9 percent so now let's compare the empirical rule and chebyshev here the difference is Shelby Chef's theorem always works that is it's always correct an empirical rule requires the distribution is bellshaped the empirical rule tends to give better estimates than does Chevy Chef's theorem Chevy chefs just gives a lower bound empirical rule tries to estimate the actual proportion it's going to be off if the data is not bell shaped but if the data are bellshaped it's going to be a really good estimate especially compared to championships theorem and while chebyshev's theorem always works it only serves as a lower bound hence the at least in the theorem statements and we got a red star so let's move on to question three which is more helpful the empirical rule or chebyshev's theorem again left hand side of your notes answered in your notes so that you can eventually put it into the Moodle quiz go ahead and pause and back and that's it for this chapter I'm sorry that's it for this section measures of dispersion as we move forward in this class the measure dispersion that's going to be most useful for statisticians for some odd reason is the variance or the standard deviation that's just how it is the range note we just got one small page dedicated to it then we kind of left it you can go ahead and ignore the range it's not helpful at all um empirical rule we're going to see pop up again in chapters six seven eight nine ten eleven 12. so you may want to spend a little bit of time learning the empirical rule shabby Chev we're not going to see it again after this chapter so well except on yeah we won't see it again on the chapter so now you know the most important things from this section but again I want you to focus on how to get the computer to calculate these and how to interpret them I also want you to figure out why measures of spread are important especially if we talked about measures of center last lecture how do those two relate how are they different why would one be more useful than the other those are important questions to answer in any class but especially in statistics so that's it I wish you a good day hello and welcome to section three three measures of relative position in some ways this should be section 2 2 and the measures of spread would be section three three because measures relative position are measures of some point in your data or in your distribution very similar to section 31 where we talked about a point in your data or in your distribution but the point for section three one was the mean the median or the mode here it's going to be some other point within your data and so topics from a Saturday you're going to be able to calculate percentiles sometimes these are called quantiles quantiles so you're going to be able to calculate the quartiles then the five number summary which is just all the quartiles together calculate the interquartile range the IQR which is also a measure of spread so that kind of ties into the last lecture we're going to be able to create a box plot and calculate some zscores and that Z scores may not sound too interesting now but those zscores will pop up again in chapters 6 7 8 9 10 11. 12. without zscores that understanding what zscores do we kind of lose a lot of this second half of the course okay so we'll start with percentiles definition of percentile is those hundred divisions in order to calculate a values relative position we can divide the data into equal parts and state in which part the value lies that kind of underscores all of these measures the relative position we may choose to divide the data up into any number of parts for percentiles we're dividing the data up into a hundred parts 100 cents and those divisions when you divide up into 100 Parts is called a percentile here's how you calculate percentiles we're going to locate the data value for the P percentile so you're going to be given the percentile such as the 45th percentile the 97th percentile the 1.75 eighth percentile those values are going to be the p n will be the sample size and this believe it or not is a lowercase l l for location and this is the formula to calculate the location notice it's the location not the data value so the location is just the sample size times the percentile over 100 well that percentile over 100 is just the proportion of the way through the data so it kind of makes sense that this would be a location spot um don't miss something nope so when using the formula to find the location for the percentile's value in the data set you must make sure of the following two rules if the formula results in a decimal value for l the location is the next larger whole number if the formula results in a whole number then the percentile's value is the arithmetic mean of the data value that is located at that location and at the next larger Hmm this kind of sounds vaguely like how we calculate the median which shouldn't surprise us because the median we're going to find out is the 50th percentile so here's an example to see how to do this car manufacturers studying the highway MPG for a wide range of makes and models of vehicles stem and leaf plots given in the next slides there's a lot of it there's 135 data points we need to find the 10th percentile and the 20th percentile so here's the data here's the second page of the data so we got one vehicle gets 12.1 percent one mile per gallon another vehicle gets 13.3 another is 14.1 then 15.5 and 15.6 so that's what statement leaf plot tells us so here's the solution first we need to notice that the data are in an ordered array that is the lowest is 12.1 mile per gallon and the highest is 35.9 miles per gallon there are 135 values so n is 135. we want to calculate the 10th percentile so p is 10 substituting the values we get L is equal to 13.5 notice 13.5 is not a whole number therefore the next larger we round that up to 14 and so the data point that is at position 14 will be the 10th percentile and that data value is 17.3 so let's go back to the data to see that one more time the 14th data value one two three four five six seven eight nine ten eleven twelve thirteen fourteen Seventeen point three is the 14th value therefore 17.3 is the tenth percentile in other words approximately 10 percent of the values in the data set are less than or equal to 17.3 more importantly if we gathered an infinite amount of data 10 of the data values would be less than or equal to about 17.3 for B we're looking at the 20th percentile we still have n is equal to 135 now we're given p is equal to 20 because it's 20th percentile we get 27 as value for l this is a whole number therefore we average the 27th and the 28th data values let's go back to the data 27th and 28th data values one two three four five six seven eight nine 20 19 20 21 22 23 24 25 26 27th is 19.2 28 is 19.3 therefore the 20th percentile would be the arithmetic mean of those two or 19.25 so approximately 20 of the values in the data set are less than or equal to 19.25 and now we need to figure out a way of calculating the percentile of a given data value so now we're given the data value and we need to determine its percentile so for instance we're given 18.9 miles per gallon we need to determine which percentile that corresponds to it's just inverts the previous uh previous equation L is the location of that data value n is the sample size so L Over N is actually the proportion of the way through the data that holds that value multiply by 100 that's the percent of the way through the data that holds the value and by gosh that's the percentile p so if the data from the previous example of the Nissan Xterra average 21.1 miles per gallon what is its percentile well here we go we got to figure out what position 21.1 miles per gallon is in the data set so let's scroll back 21.1 is way down here or way up here I'm not going to count that but I'll let you count it happens to be the 49th value so the value of 21.1 miles per gallon is 49 out of 135 of the way through the data 49 out of 135 is 0.36 multiply that by 100 so that 21.1 is about 36. is about the 20 30. is about the 36th percentile approximately 36 percent of the data values are less than or equal to that is the value 21.1 miles per gallon is in the 36 percentile of the data set oh we got a red star here we go this will be the intro lecture question the first one of this lecture question one what is the percentile again write the question on your notebook answer it so that you can transfer though that question and answer or just that answer to Moodle for the lecture quiz pause and we're back now we're going to look at quartiles recall that the percentiles divide the data of into a hundred whereas the quartiles are going to divide it up into fourths four for court first quartile is about 25 of the data is less than or equal to it second is 50 is less than or equal third quartile is three quarters or 75 of the data is less than or equal to in other words the first quartile is the 25th percentile the second quartile is the 50th percentile and the third quartile is the 75th percentile also recall that the second quartile which is also the 50th percentile is also the median here's a couple ways of calculating or shall I say estimating the quartiles we're going to use a percentile method which we just got through doing a couple examples on the percentile we're going to use the approximation method going this way approximation method is you find the median drop the median you find the median of the lower half to get q1 median of the upper half to get Q3 and look at how these values compare for a large data set the value is going to be very close we already know the data are in order from smallest to largest and we already know that n is 135 . here's the data set so using the percentile method we want to find the 25th percentile so p is 25. do these calculations to find that the location is 33.75 because that is not a whole number we just look at the 34th value which is 19.8 miles per gallon that will be the first quartile which is identical to the 25th percentile second quartile is the median or the 50th percentile thus n is equal to 135 p is equal to 50. that brings us to 67.5 round that up to 68. the median will be the 68th value which is 23.6 the third quartile is to the 75th percentile so p is 75 crack put into the formula 101 rounded up to 102 so it'll be the 102nd value which is 25.3 so the third quartile is 25.3 we can also do the approximation method divide the data in half we get the 68th position which is the median is 23.6 calculate that many times so the second quartile is 23.6 first quartile is going to be the median of the lower half which comes out to be 19.8 and the third quartile is the median of the upper half which is 25.3 they look they're very very close and for large data sets they are going to tend to be close and remember all the times I've said it's approximately this or about a certain percent of the data is less than or equal to when you've got data the best you can do is just approximates or about if you got the entire population you can get exact ly next example finding the quartiles of a data set so we got two data sets we got data set a and data set B we can use the approximation method there's the quartile I I'm sorry there's the median second quartile 70.5 q1 and Q3 can be estimated 65 and 78 . so three of the five number summary is going to be 65 70.5 and 78. using this second set of data can we start the median go through the exact same calculations to get estimated values so three of the five numbers for the B data set are 67 75 and 79. of course we can use R to do these rather quickly give it the data do summary of that data set and this will give you the first and third quartiles it'll give you the median it also gives you the Min and the max which are the other two numbers in the five number summary and it will also give you a mean now that these values are not the same as those received when the approximation method is used so which is correct they both are because remember medians don't have to be unique and that extends to all the quartiles and percentiles they don't have to be unique oops red star question two again write this over on the left hand side answer it beneath how do percentiles and quartiles differ hit pause and we're back five number summary and box plots five number summary consists of the five quartiles the minimum value which is q0 first quartile which is q1 second quartile which is Q2 third quartile Q3 and the maximum which is Q4 five number summaries may put these five numbers listed in order from smallest to largest why do we need the five number summary well it gives us a good feel for the distribution of the data so write the five number summary for the date an example of 3.2 we've done we've calculated the quartiles already all we have to do is figure out what the minimum 12.1 and the maximum 35.9 values are and there's our five number summary minimum q1 Q2 Q3 maximum we are going to illustrate the five number summary using a box plot technically it's called a box and whiskers plot box and whiskers plot this case it's a horizontal box and whiskers plot looks like this it specifies with the minimum value and the maximum values are those are the endpoints of the whiskers and the Box endpoints are q1 and Q3 and the median is indicated by a thick line inside that box the range between q1 and Q3 is called the interquartile range it's just Q3 minus q1 it's a single number about half the data occurs Inside the Box about a quarter of the data occurs above the box and about a quarter occurs below and this is what I just said here's the actual formula for the interquartile range it's Q3 minus q1 here's how we create the box plot we're going to begin with the five number summary we're going to determine a nice little scale horizontal axis that fits all those values nicely we're now going to Mark those five numbers on the on the graphic the minimum q1 Q2 Q3 the maximum we're going to draw the Box in q1 to Q3 the thick horizontal vertical line at the median and then we show the whiskers going out for the box to the Min and the max we can do this in r with the box plot command load in the data and just supply box plot to the data if you want something that looks like this graphic you have to specify horizontal equals true a vertical box plot is the default and the color is orange col for color and that will color the Box slot faster and a lot more accurate than doing it by hand we're going to interpret box plots now we have four sub basins we got the Upper Mississippi the Ohio Tennessee the Missouri and the Arkansas Red River sub basins notice this is not a typical box plot because the top whisker and the bottom whisker are at 90th percentile and the 10th percentile which we see over here we also see the mean as a DOT that will help us see the skew of the data set so for instance Missouri seems to be skewed up because the mean is above the median the Upper Mississippi and the Ohio Tennessee seem to be rather symmetric and the Arkansas Red River seems to be skewed up as well so here are the questions what did the top and bottom bars represent in these box plots according to the key they represent the 90th and the 10th percentiles which sub Basin had the highest median average highest median average the median is the solid horizontal line so it looks like it's the Missouri which set Basin had the lowest average spring lowest average so it looks like the lowest average is going to be the Ohio Tennessee if we're looking at the average it looks like that's going to be less than the lower than the Arkansas Red River if we're looking to medians however the Arkansas Red River is definitely below the Ohio Tennessee and which sub Basin had the largest interquartile range in other words which said Basin had the largest spread to the data that's clearly in Missouri because the box is the largest of all the other three last topic today is our zscores or our standard scores when we can compare two data values from two completely different populations by comparing their relative or their respective percentiles we could also determine how the values relate to the respective means of their data sets zscores do this latter it's called the standard score or the zscore again we're going to see the zscore pop up in chapter 6 7 8 9 10 11 12. the standard Square tells us how far a value is from the mean specifically how many standard deviations it is from the mean the formula for the population standard score that is if you are given the population mean and standard deviation it's just the value you have minus the population mean divided by the population standard deviation for the sample standard score it's x minus X bar the sample mean divided by S which is the sample standard deviation so it's always x minus a mean divided by a standard deviation example mean square of the math section of the SAT test is 500 with a standard deviation of when it's 50. what is the standard score for a student who scored 630 so 630 is X 500 is Mu and 150 is Sigma so the Z is just going to be 630 minus 500. divided by 150. so a person who scored 500 on the SAT now I'm sorry a person who scored 630 on an SAT has a zscore of 0.87 . in other words this student 's test score is about 0.87 standard deviations above the mean Jody scored an 87 on her calculus test and was bragging to her breasts friend about how well she'd done poor Jody she said that her class had a mean of 80 and a standard deviation of 5. therefore she had done better than the class average which is true Jody got an 87 though the average was 80 so definitely above her best friend Ashley was disappointed she'd scored only in 82 on her calculus test however the mean for her class was 73 with a standard deviation of six so who really did better on her test Jody or Ashley and we're going to do this compared to the rest of the class so Jody's zscore is 87 minus 80. divide by 5 and Ashley's will be 82 minus 73 divided by 6. so for Jody it's 1.4 Ashley it's 1.5 thus compared to everyone else in the class Ashley actually did better compare it to everybody else in the class Ashley actually did better Ashley's score was actually 1.5 standard deviations above average whereas Jody's score was only 1.4 standard deviations above average he and here we are calculating zscores using R let's go back to the A let's calculate the zscore corresponding to our first value now there is no zscore function native to R but sourcing this file will give us one we just give it the data and we specify zscore of the data and then in Brackets which data value do you want to get the zscore for if we want to get the first zscore we put a one in Brackets if we leave the brackets off completely and just have zscore of a then we get all the zscores the output for this tells us that the first value has a zscore of negative 1.47196 thus it is about 1.47 standard deviations below average about one half standard deviations below average below because the zscore is negative below average because all Z scores are done with respect to average if the zscore is zero then that person scored average yeah there's one more question question three I must miss the red star again write the question in your notes on the left answer it below when should one use a box plot instead of a histogram this one's going to take you some some thought think about what the histogram tells you think about what the box plot tells you think about the examples that we gave in this class in this lecture for box plots spend some time this is the last part of this lecture by the way spend some time thinking about the strengths of histograms the strengths of box plots and now when should a box plot be used instead of a histogram go ahead and hit pause but we are done and so that's the end of chapter three call in chapter three we're now summarizing our data using numbers in chapter two we summarized our data using Graphics chapter 3 it's using numbers we had some several we had several important numbers that we looked at the mean the median the mode standard deviation the interquartile range we also looked at measures of position the zscore don't forget the empirical rule and chebyshev's Theorem although you could forget Chevy Chef's theorem life would go on the empirical rule is actually very important as is zscore understanding both of those will help us when we get to chapter seven and that's it hopefully this was helpful if not drop me in line hello and welcome to section 4.1 this will be the first section of the chapter four purpose of chapter four is to introduce you to probability Theory the vast majority of chapter four is going to review um it's going to be review of stuff that you learned back in middle school and high school um there are no intralecture questions posted however there are quizzes in Moodle so the answers for the Moodle quizzes for each of these chapter four sections will be not in the lecture got it you write not in the lecture for each of those you get full credit if you don't write it you don't get full credit so the objectives for section four one identify the sample space of a probability event calculate basic probabilities determine if two of answer mutually exclusive and determinive to advance our independent mutually exclusive and independent are two things that you're going to be need to be very careful of if there's a lot of confusion between the two if two events are mutually exclusive then both cannot happen at the same time if two events are independent then having one happen doesn't affect the other so here's some terminology a probability experiment or a trial is a process with a result determined by chance such as flipping a coin each individual result that is possible for a probability experiment is an outcome so there are two outcomes for the coin flipping outcome one is ahead outcome two is a tail the sample space is the set of all possible outcomes for any given probability experiment therefore the sample space is heads and tails or heads comma tails an event is a subset of outcomes from a sample space so an event could be just hit an event could be just tail an event could be head or tail or an event here could be none of the above examples that probably experiments include 50 coin tossing a pair of die drawing a raffle ticket those are the basics in each of these examples there is more than one possible result and the result is determined at random example four one consider an experiment in which a coin is tossed and then a sixsided die is rolled so a we need to list the outcomes of the sample space for the experiment see coin toss then sixsided die is rolled okay list the outcomes in the event tossing a tail then rolling an odd number okay so here are the solutions a each outcome consists of a coin toss and a die roll for example you can get a head and then a three we're going to denote a head followed by 3 as H3 using this notation we've got 12 outcomes 12 possible outcomes and the set of those 12 possible outcomes is going to be the sample space you can get ahead and then a One a head and then a two a head then a three dot dot dot a tail then a five then a tail and a six it's 2 times 6 possible outcomes so the sample space is just those 12 possible outcomes so B we're going to choose the members of the sample space that fit the event quote tossing a tail then rolling an odd number tossing your tail and then rolling out number there's only three odd numbers one three and five so the three possible outcomes of this sample space are T1 T3 T5 next example let's consider the experiment in which a red sixsided die and a blue sixsided die are rolled together use a pattern to help list the outcomes in the sample space with the outcomes in the event the sum of the numbers rolled on the two dice equals six okay many patterns that we could use this is one that they're looking at why would we use a quote pattern here instead of listing all of them out there's 36 possible outcomes six for the first six for the second six times six gives us 36. then we'd have to list out 36 outcomes for the entire sample space here we can just give a pattern for what they're going to be or we can list them all out if we look if we'd like to have these drawings notice the sample space is one and one two and one three and one four and one five and one six and one Etc if all we care about is the sum of the uh numbers that come up then the sample space would be 2 3 4 5 6 7 8 9 10 11 12. because 2 through 12 is the sums I'll keep the sample space in mind note that rolling one on the red dying or two on the blue die is different there are only a two and a one if we care about the actual outcomes however if we care about just the sum then those two are going to be identical outcomes Part B I'm going to list the outcomes the event quote the sum of the numbers rolled on the two dice equals six so we're going to go back and look to see there's one here five one four two three three two four one five those all give us a sum of six so we could actually say the probability of getting a six when rolling two dice is six one two three four five sorry it's five one two three four five five out of 36. a tree diagram can be used to represent the outcomes of an experiment especially in the early phases when you're just trying to learn why it's 6 times 6 instead of six plus six the tree begins with the possible outcomes of the first stage and then branches for each additional possibility we're going to see tree diagrams in the future so it would be good to be able to write these out eventually number of possibilities in the bottom row the tree is equal to the number of outcomes in the sample space so let's consider a family with three children use a tree diagram to find the sample space for the gender of each child in regard to birth order so again ordering here does matter we're doing it oldest middle youngest so here it is the first child can either be a girl or a boy second can be girl boy or girl and boy and then girl boy girl boy girl boy girl boy so the bottom here GGG would indicate that all three children were girls ggp would be a girl girl followed by a boy gbg would be a girl than a boy then a girl notice that this looks like a tree especially if you turn it upside down and so using this tree diagram as a guide we can see there's going to be eight outcomes two times two times two three children so two to the power of three and now let's look at three methods for calculating the probability of an outcome this is also usually thought of in terms of three ways of understanding probability they're subjective the experimental and there's classical what we've used so far is classical what the course is going to be using the future will be mostly experimental sometimes it's called frequency probability or sometimes relative frequency probability in a feature course subjective probability will be renamed Bayesian probability and we'll see that subjective probability is actually the most useful of the three but that's for a a future course way down the line subjective probability according to Hawks is simply an educated guess regarding the chance an event will occur this is Hawks this is not reality but you won't understand the reality until you get into a future stat course and again subjective probability or Bayesian probability will be the most useful but we're not there yet experimental probability or relative frequency probability talks about using data to estimate a probability in an event so if e is an event then P of e the probability that e occurs is given by F over n where f is the frequency and N is the total number times the experiment is performed so if I want to estimate or find the probability of randomly selecting a sophomore from campus I could ask 500 people on campus if they're a sophomore and of those 500 295 said they are a sophomore and would be 500 the number of people I asked and therefore I would estimate the probability of a sophomore of being 295 or 500. and that's going to be the Cornerstone this experimental probability or this relative frequency probability will be the Cornerstone of the second half of the course mainly because I'm going to go the classical probability very quickly indicates a very the classical probability very quickly will become useless we don't know the actual probability of selecting a student who is a sophomore so we just have to estimate it and that estimation and the using of the estimated probabilities is this experimental probability the benefit or the theoretical background that we can use the experimental probability is called the law of large numbers as the sample size increases the proportion or the means of your sample approaches the means or the the proportions of the population and they have classical probability if all outcomes are equally likely and this is actually a very important requirement that the outcomes are equally likely then the probability of e is equal to the number of elements in the e divide by the number of elements in the sample space hence when we were talking about what's the probability of getting a 6 when rolling 2 Die by getting six I mean adding up the dice together of getting it was just five the number of ways of the dice adding up to six divided by 36 the possible number of outcomes example four is identifying the types of probability determine whether each probability is subjective experimental or classical again experimental is also also referred to as relative frequency or just as frequentest classical is also referred to as axiomatic ax iom a t i c probability of selecting the Queen of Spades out of a wellshuffled standard deck of cards is 1 over 52 that's clearly classical there's only one Queen of Spades out of the 52 cards in the deck we know this we don't have to estimate it we're not guessing at it Economist predicts a 20 percent chance that technology stocks will decrease in value over the next year from the information given that will be subjective although this economist may have used data to come up with that in which case it would be experimental police officer wishes to know the probability that a driver chosen at random will be driving under the influence on a Friday night so he records the number of drivers at a roadblock a number of drivers drinking with ba blood alcohol levels of the legal limit yet determines the probability is three percent that is very clearly a relative frequency or an experimental probability because it's based on the experiment that he performed we just discussed that Beck is allergic to peanuts for the next example poorbeck at a large dinner party one evening he notices that the cheesecake options on the dessert table contain the following flavors 10 slices of chocolate 12 slices of caramel 12 slices of peanut butter chocolate and eight slices of strawberry assuming that the desserts are served to a guest at random what's the probability that Beck's cheesecake contains peanuts it's 12 divided by 10 plus 12 plus 12 plus 8. and what's the probability that X dessert does not contain chocolate that's going to be 12 Plus 8 divided by 10 plus 12 plus 12 plus 8. in the first case it's because 12 of those the numerator 12 contain peanut butter and in the second case it's a numerator of 12 plus 8 does not contain chocolate however were I back I would need cheesecake at all because even a 28 chance it's not worth it consider a beginning Archer who only manages to hit the target half the time what's the probability that in three shots the Archer will hit the target all three times that's going to be 1 over 8 or 12.5 percent here's y each time she has a 50 chance of hitting the target so 50 chance to hit the first time fifty the second 50 the third to hit it all three times it's got to hit it the first and the second and the third time here's the tart here's the tree diagram notice that only one of these eight cases does she hit the target all three times so it's one over eighth or 12.5 percent consider a family with six boys what's the probability that the seventh child will also be a boy okay um families do to have a girl but reality States just the opposite if you've got six boys it's most more likely that you'll have a seventh boy however we're supposed to pretend that each time it's a fifty percent chance of getting a boy or a girl um so it's you got six boys followed by girl or six boys followed by a boy of these one out of the two um is a boy so 50 chance again this assumes that the outcomes are equally likely the reality is that if you've had six boys it's more likely that you'll have a seventh boy um it's not 50 chance and also even on your first child it's not a 50 chance that you'll have a boy it's actually a less than 50 chance that you'll have a boy that's a 49 point something percent chance but we're simplifying things here so we because we can multiply and by half very easily um in biology we learned that many diseases are genetic one example as such is Huntington's disease which causes a neurological disorder as person ages each person has two Huntington genes one inherited from each parent if the individual inherits a mutated Huntington genes from from either of his or her parents that person will develop the disease notice it's from either of the parents uh TV show House the character who Dr house calls 13. inherited the disease from her mother so if 13 has a child with a person who has two healthy Huntington G disease what's the probability your child will develop Huntington's quite simply the answer is going to be one half because gonna get a good Gene from the parent from the father and a bad Gene from her 50 percent and that's it for section four one remember go to Moodle take the quiz and for each of your answers write something like this was not asked in the lecture and that's it hope you all have a good day hello and welcome to section 4.5 where we're learning about the addition rules of probability in other words we're looking at probability of the Union of two events so we're going to use addition rules to calculate probability that's the objective for today there are three properties of probability um for any event e is going to be our generic event and the probability V is going to be between 0 and 1 inclusive for any sample space the generic sample spaces can be in a cursive S the probability of being an element of that sample space is one because remember the sample space is a set of all possible outcomes and three for an empty set the probability of empty set is zero so the probability of nothing happening is zero um the complement is a very important concept that you're going to be using in chapter five chapter four really only has two Concepts that are extremely important one is complement the other is going to be the concept of Independence Independence we're going to be using in chapter 11 but complement we're going to be using quite a bit in chapters five and six um the complement of event e has denoted is denoted as e to the power of C it's not really a to a power of it's just superscripted C in some sources this will be a prime in some sources it'll be bar over the e I like the C it's pretty nice it's the complement of e is the set of all outcomes that are not in e so describe the complement of each of the following events a red card out of a standard deck of cards the event is choose a red card so the complement will be choose a not red card which means the component is going to be choose a black card out of a standard deck of cards pardon me out of 31 students in your stat class 15 are out sick with the flu so the event e is being out sick with the flu the complement of e is going to be not being out sick with the flu so in this case e complement will be able to attend class or not being sick your area 91 percent of phone customers use phone South so the event is customer using phone South the complement will be customer not using phone South so be the complement e complement will be customer using some something other than phone South that's what all of the solutions tell us the complement rule for probability and this is the most important part is that the probability of an event plus the probability of its complement is one this is kind of like put a star next to it in your notes Circle it happy face around it because this we're going to be using a lot in chapter five for some problems and this is actually why we're going to use it on chapter five for some problems the probability of complement is much easier to calculate than the probability of the vent itself this is especially true when you're dealing with infinite sets or infinite sample spaces for these problems you can calculate the probability of the complement and subtract that value from one here's an example you're worried that there are there is a 35 chance you'll fail your upcoming test what's the probability that you will pass the test it's going to be the complement of failing so it's going to be 1 minus 35 percent if there is a five percent chance that none of the items on a scratch off will be a winner what's the probability that at least one who will win well at least one means the probability of a one or a zero I'm sorry at least one that's the probability of a one a two a three a four or five all the way up there so that's going to be 1 minus the probability of zero so here's the solution for the first 65 percent pretty straightforward the second is much more complicated probability of at least one winner is one minus the probability of no winners and we're told the probability of no winners is five percent which means that the probability of at least one is going to be 95 percent I do want to emphasize here make sure you understand that no winners and at least one winners are complementary events so pause until you understand that role of pair is standard sixsided dice what is the probability that neither die is a three that's gonna be pretty straightforward to calculate if you use complements if you do it Brute Force then you're going to have to list out all 36 possibilities and determine how many of those neither has a three or we can just use compliments so these are the outcomes that have a three remember e is not having a three so e complement will be having a three so there are 11 elements in E complements so the probability of being an e complement is 11 over 36. therefore the probability of being an e is 25 over 36. which is about 70 percent so you have about a seventy percent chance of rolling two dice and not getting a three on either one the original addition rules for probability this is the addition rule for probability there is a simplification to it that may be useful or may be allowed but this is the rule and the probability of given to events the probability of being in event e or in event F or both is just the probability of being an e plus the probability of being an F minus the probability of being both Cerise is looking for a new condo to rent her realtor provided with the following list of amenities for 17 available properties there's a list close to the subway with six seven were low maintenance fee five had green space two were newly renovated close to the subway and low maintenance were two so one and two there were two in that which meant that there were yeah um Green Space and Lira innovated was just one if cerise's realtor selects the first condo they visit at random what's probably the property is either close to a Subway or has a low maintenance fee close to assembly we'll call e low maintenance V we'll call F so let's verify that the realtor has accurately counted the total number of properties remember there were 17 properties six where if type one seven were a type 2 5 or type three two were type four which gave us 20 but we know that there is a overlap of three we were told what this overlap was to are both low maintenance and close to Subway and one is both newly renovated and green spaced so 17 individual properties we're going to use the or that tells us we'll be using the addition rule e or F that's equal to probability of e plus a probability of f minus the probability of both There are 16 that are close to a Subway seven that are low fee and we're also told that two are both so six plus seven minus 2 is 11. so about a 64 chance suppose that after a vote in the U.S Senate on a proposed health care bill the following table shows the breakdown of the votes by party 23 versus 21 43 versus 7 2 versus 4. if a lobbyist stops a random Senator after the vote what's the probability of the senator will be either a Republican or voted against the bill so either a Republican or evaluate against the bill that's just going to equal the probability of it being a Republican 50. plus the probability of voting against the bill 32 minus the seven that were counted twice divided by 100 or whatever Republican voted against the bill counted twice gives us a 75 chance the lobbyists will have dinner with the senator he wants roll a pair of dice what's the probability of rolling either a total less than four or a total equal to 10 total less than 4 will be e total equal to 10 would be f we want the probability of e or F that's just the probability of e plus the probability of f minus the probability of E and F less than 4 plus probability of 10 minus the probability of less than 4 and 10. well what's the probability of it being both less than 4 and equal to 10. well that's zero because this is the empty set and from our uh three requirements for probability that means that this probability is equal to zero and we just pay attention to the first two in other words less than four and ten are mutually exclusive events one can happen the other could happen but both cannot happen so there's three that are less than four there's three that are ten so it's going to be six out of 36. again the key these two events are mutually exclusive and that leads to a zero percent chance of both occurring if events E and F are exclusive and the addition rule is much easier and apparently Hawks didn't give us the formula it's just the probability of e plus the probability of f there's no need to subtract off anything because what you're subtracting off is just zero Caleb is very excited that it's finally time to purchase his first new car after much thought he's narrowed his choices down to four because it's taken him so long to make his mind his friends have started to bet on which car he will choose they've given each car a probability based on How likely they think Caleb is to choose that car Devin's betting that Caleb will choose either a Toyota or a Jeep find the probability that difference right and here's the probabilities that they've assigned probability of a Toyota or a Jeep it's just going to be the probability of a Toyota plus the probability of a Jeep minus the probability of a Toyota and a Jeep but that probability of both is zero because he's purchasing his first new car only one car so Devin has a 75 chance of correctly picking which car Caleb will buy example 416 we're going to extend the addition rule to more than just two events we got these probabilities and we're asked what's the probability that the driver will refuel at Shell Exxon or Chevron shell Exxon or Chevron these are mutually exclusive events therefore the probability is just going to be the sum of the individual probabilities the probability of the genuine is the sum of the probabilities if the events are mutually exclusive and that's the end of first chapter section 42 again don't forget to complete the quiz in Moodle and for each of the questions write something like this was not covered on the lecture and that's it take care hello and welcome to section 4.3 this is the third section in chapter four probability Theory here we're looking at the multiplication rules for probability which eventually will lead to conditional probability and Independence so we're going to use multiplication rules to calculate probability um multistage experiment is experiment with two or more steps or stages we've seen examples of this the roll the die and then flip a coin is a multistage experiment the flipping the coin three times is a multistage experiment in fact it's a threestage experiment an experiment performed with replacement refers to placing an object back into consideration before performing the next stage of the experiment those examples they just gave you were quote with replacement because you could end up with actually the first one with roll the die and then flip a coin it's not with experiment because once you roll the die you can't get a one two three four five or six for the second term we did have an earlier example of the probability of getting a Queen of Spades If all we're doing is drawing one card then we don't need to talk about with or without replacement but if we're talking about drawing two cards what's the probability that either one is a Queen of Spades well now we got to think about am I putting the first card back after I draw the second first card or do I keep that first card in my hand if I put the card back it's with replacement and the calculations tend to be a little bit easier when you're doing things with replacement if I hold on to that first card then it's without replacement and the calculations get a little bit more difficult not too much two events are independent if one event happening does not influence the probability of the other event happening um so that's independent multiplication rule for probability of independent events and I want to emphasize this is for independent events right now if enf are independent then the probability of E and F occurring is equal to probability of e times the probability of f that's it I suppose two cards from a standard deck I choose two cards from a standard deck with replacement that means I draw a card look at it put the card back reshuffle draw a second card what's the probability of choosing a king and then a queen well since I put the card back the outcome of the first doesn't impact the outcome of the second draw so these are going to be two independent events so it's just going to be the probability of choosing a king times the probability of choosing a queen so that's going to be 4 out of 52 times 4 out of 52 or 1 13 times 1 13 which is about 0.6 percent chance assume that a study by human resources has found that the probabilities of an employee being written up for the following infractions are the value shown in the following table so this is probability of being written up at work um and we're going to assume that each infraction is independent of the others this is given information to us we would have to know that they're independent we can't just look at the problem and understand hey they're independent we have to be told they're independent what's the probability that a given employee will be written up for being late to work taking unauthorized breaks and leaving early late to work unauthorized breaks leaving early so we're looking at the probability of late work and unauthorized breaks and leaving early so it's just the product of those three probabilities because these are independent events so about one percent chance set a different way about one percent of the of the people at that company get rid of for being late to work taking on authorized breaks and leaving early that seems rather seems rather High to me um an experiment performed without replace it means that the objects are not placed back into consideration that means that the two draws are going to be dependent most likely to events are dependent if one event happening affects the probability of the other event happening so we're looking back at our king and queen example we want to know the probability of drawing a king and then a queen if the cards are drawn without replacement so this is going to be a good one the situation is essentially the same as drawing two cards from a standard deck so instead instead of thinking let's draw one card and then draw the other you can think of this as just drawing two cards and you're asking what's the probability of the first card I drew being a king and the second being a queen by determining the probability of drawing a king from a snare deck of cards Begin by doing that it's just one out of 13. now let's assume that when I drew the first card it was a king so given the first card is a king what's the probability that the second one is a queen huh it's there's four Queens in the deck but the deck only has 51 cards left in it because I didn't put that King back there's only 51 cards left in the deck thus the probability of a queen given that the King was drawn first without replacement is 4 out of 51. so the probability of getting a king and then a queen is just gonna be the product of about 0.06 I'm Sorry by about point six percent now what we've actually done is we've started talking about conditional probability without telling you that we're talking about conditional probability here's the the key that this is conditional probability it's the word given conditional probability denoted by probability of f given e that vertical bar is read as quote given is a probability of event F occurring given that the event e occurs first is event E and F are independent then the probability of f given e is just probability of f which leads to a a nice definition of independence for us one card's already been chosen from a standard deck without replace and what's the probability of now choosing a second card and it being red given the first card was a diamond so we're asked what's the probability of being second being read given the first was a diamond red given diamonds just 25 over 51 because we didn't put the card back there's only 25 cards that are red when the first card was a diamond which is also Red by the way so now here's the actual multiplication rule for probability given to events E and F the probability of E and F occurring is just the probability of e times the probability of f given e which is identical to the probability of f times the probability of e given f these two formulas are exactly the same the one you use depends on the data that's given to you so it's a probability of choosing two face cards in a row we're going to assume the cards are chosen without replacement here we're dealing with dependent events so we'll use the multiplication rule and the first card is picked all 12 face cards are available at a 52 so the probability the first one is a face card is 12 out of 52. now given that the first one is a face card then there's only 11 left that are face cards out of the 52 out of the 51 cards remaining in the deck so the total probability will be 12 over 52 times 11 over 51. which is about a five percent chance assume that there are 17 men 24 women in the Rotary Club two members are chosen at random each year to serve on the scholarship committee what's the probability of choosing two members at random the first being a man and the second being a woman we're choosing two members the first choice will influence the probability of the second there's 41 people all together so the probability of a man and woman is just a probability of man times the probability of a woman given the first was a man probability of man is 17 out of 41. so given the first one was a man the probability of the second one being a woman is just 24 out of 40. it's 24 because there's 24 women in the Rotary Club it's 40 because there are 40 that are remaining after the first one was chosen so there's about a 1 4 chance we can also write conditional probability and this is sometimes referred to as the definition of conditional probability the probability of f given e is the probability of E and F divided by the probability of E if E and F are independent from the probability of E and F is just the probability of e times the probability of f probability of E's cancel out so we're given the probability of f given e is just equal to the probability of f and that's a very nice definition of Independence out of 300 applicants for a job 212 were female and 110 are female and have a graduate degree what's the probability that a randomly chosen applicant has a graduate degree given she's female so now we're told the person who God is female what's the probability that she has a graduate degree 212 female 110 female graduate degree so this probability is just going to be 110 over 212. if 152 of the applicants have graduate degrees what's the probability that a rambly chosen applicant is female given the applicant has a graduate degree 152 is the denominator 110 is the numerator so the probability is just going to be 110 over 152. he is female and graduate degree divided by female female and graduate degrees divided by graduate degree the emphasis here is that probability F given e is not necessarily the same as probability of e given F order matters with conditional probability the reason order matters is you're given different information probability F given e you're given that e is true and you need to calculate the probability of f or as in probability of e given F you're given f is true and you need to calculate a completely different probability probability of E and now for the fundamental counting principle for a multistage experiment with n stages where the first stage has K1 outcomes the second stage has K2 outcomes the third stage has K3 outcomes and so on the total number of possible outcomes is k1 times K2 times K3 times dot dot dot times kn we've already seen this um with the roller die then flip a coin K1 was 6 K2 was two the total number of outcomes was 12. 6 times 2. flipping the coin three times okay one was two k two is two k three was two two times two times two is eight there are eight possible outcomes rolling two dice K1 with six K two was six six and six is 36 there are 36 possible outcomes so we've already experienced this we're just giving you a nice name for it Kilby begins her first year in an online degree program in July the first semester she'll randomly be assigned to one section for each of four different core courses if there are eight English one sections 12 college algebra sections 11 American history sections and five phys Ed physical science sections how many different options are there for Kilby's schedule for her first semester oh it's just eight times twelve times eleven times five from the fundamental accounting principle fifty two eighty the governing board at a local charity Mission Stateville is electing a new vice president and secretary to replace outgoing board members if the board consists of 11 members who don't already hold an office how many different ways can the two positions be filled if no one may hold more than one office go ahead and hit pause to figure this out and you're back two slots to fill it's going to be without replacement because you can't hold both offices there are 11 choices for the first position 10 choices for the second that leaves 110 possible ways to elect the new officers example Robin is preparing an afternoon snack for her twins Matthew and Laney she wants to give each child one item she has the following snacks on hand carrots raisins crackers grapes apples yogurt and granola bars if she randomly chooses one snack for Matthew and one snack for Laney what's the probability each child gets the same snack as yesterday here's the solution probably want to hit pause to come up with the solution yourself before you read the solution here you need to count the number of ways in which Robin can randomly choose a snack for her twins again we can think of this as two slots to fill one for each twin seven possibilities for each child there's no requirement that the twins have different snacks or conversely have the same snack so the total number of ways she can prepare the snacks is seven times seven now we need to count the number of ways that she can choose the same afternoon snack as yesterday there's only one thing they had yesterday so the probability is going to be one divided by the 49 total number of snacks so about two percent chance and that's it thank you very much hello and welcome to section 4.4 this is the fourth section of chapter four we're going to look at combinations and permutations these are just functions that allow you to quickly calculate total number of possible outcomes um so the objective calculate numbers of permutations and combinations before we get to that we have to Define what a factorial is a factorial of n a positive integer denoted by n exclamation point or called n factorial is just the product of n times n minus 1 times n minus 2 all the way down to one so one factorial is equal to one two factorial is equal to two times one or two three factorials equal three times two times one which is six by agreement 0 factorial is equal to one you might want to be careful on that one so let's calculate the following factorial expressions a is 7 factorial it's just going to be seven times six times five times four times three times two times one B is 4 factorial over zero factorial that's numerator is going to be four times three times two times one denominator is going to be just one C 95 factorial over 93 factorial that's just going to be 95 times 94 y because 95 factorial is equal to 95 times 94 times 93 factorial and then you're dividing off by that 93 factorial so you're left with just 95 times 94. D the numerators can be 5 times 4 times 3 times 2 times 1 denominator well 5 minus 5 minus 3 is 2. so the denominator is just going to be 2 times 1. 2 factorial e numerator is going to be 6 times 5 times 4 times 3 times 2 times 1. denominator is going to be 2 times 1 times 4 times 3 times 2 times 1 because 6 minus 2 is 4 . so 7 factorial is 50 40. 4 factorial over 0 factorial is 24. 95 factorial divided by 93 factorial is 95 times 94. 5 factorial over 2 factorial is just 60. and 6 factorial over two factorial times four factorial is 15. now we're going to define the definite difference between a combination and a permutation a combination let's do with permutation first a permutation is a selection of objects from a group with the arrangement matters um combination the arrangement doesn't matter it's an example when you would use a permutation is you want to select a president vice president secretary from a group of people the actual positions are named and they matter combination would be I want to select a a group of three people from a larger class of 50. I'm just selecting three people in this group by not naming the positions it's just three so the arrangement matters that leads to permutations the arrangement is irrelevant that leads to a combination so the calculations are very similar ish and when order is not important the following formula is used to calculate the number of combinations um it's NCR or n choose r is just n factorial the larger number divided by R factorial times n minus r factorial so if I want to choose a group of three people out of our class of 30 and this 30 R is 3. so it'll be 30 factorial divided by 3 factorial times 27 factorial when the order is important the following formula is used to calculate the number of permutations so if I want to choose a president vice president and secretary from our class of 30. and it's again going to be 30 R again is going to be 3 but the number of permutations just n factorial over 27 factorial notice the number of combinations cannot be larger than the number of permutations and the number of combinations will be equal only when R is equal to 1 or 0. in other words when R is one or zero order is not important versus order is important that means the same thing because there's only one or no position to fill it's like choosing a president from our class of 30. whether or not the order matters within that one position it's not a question to ask because there's only that one position given a group of three friends how many ways can you arrange the way that they stand in line for the movies standing in line matters the order matters that so it'll be three factorial divided by three minus three factorial how many ways can I choose two of them to write in a car together I'm just choosing two I'm not saying one get shotgun the other doesn't I'm just choosing two so a is going to be permutations B will be combinations that'll be three choose two so there are six ways that they can stand in line here's the actual listing of those six ways as for the second part we're just choosing two from a group of three so it's called three choose two 3 factorial 2 factorial and then three minus 2 factorial so you can also think of this as the total number of way a total number of of people factorial divided by the number of people in the group you care about factorial times the number of people in the rest of the group we start with three friends we want two in the car this is what's not in the car and here are the possible options for who gets to ride class of 18 fifth graders is holding elections for class president vice president secretary how many different ways can the officers be elected since we've actually named these positions this is going to be permutations R is 3 n is 18. it's almost five thousand ways consider that a cafeteria is serving the following vegetables carrots green beans lima beans celery corn broccoli and spinach Bill what wishes to order a vegetable plate with three different vegetables how many ways can this plate be prepared well this is one where order doesn't matter this is going to be one two three four one two three four five six seven will be n 3 will be R so this will be seven choose three thirtyfive suppose that a little league baseball coach is randomly listing the nine starting baseball players in batting order for their second game at this level the batting order is randomly chosen to give all players an opportunity to experience different batting positions what's the probability that the order chosen for the second game is exactly the same as that of the first game since we're focusing on the order itself this will be permutations and and R are both nine and we're looking at the probability so the denominator is going to be n permute nine I'm sorry 9 permute nine and the numerator is going to be 1 because there is only one ordering that we had yesterday 9 9 is equal to 9 times 8 times 7 times 6 times 5 times 4 times 3 times 2 times 1 which is 9 factorial and zero factorial is one so this is nine factorial or nine permute nine so there are that many possible batting orders the probability of getting the exact one from yesterday is just one divided by that so pretty low probability Maya has a bag of 15 blocks Each of which is a different color including red blue and yellow I reaches into the bag and pulls out three blocks what's the probability of the block she has chosen or red blue and yellow from the way this is stated this looks like a combination because it doesn't say that the blocks have to be in that order that she first pulls out a red then a blue then a yellow just that there are red blue and yellow out of those 15 so it's going to be 15 choose 3. 455. how many combinations contain the red blue and yellow blocks order does not matter if there's only one way to choose those colors that's the probability that my choose is red blue and yellow is calculated and such just one over that last topic will be special permutations special permutations involve objects that are identical number of distinguishable permutations of n objects of which K1 are alike K2 are alike and so forth is given by this we've actually already dealt with this in terms of permutations we start out with n in our permutations and we divide by K that belong to the group we care about and N minus K belong to the group we didn't care about if we only have two groups then K2 is just going to be n minus K1 or n minus r make sure that your K1 K2 all the way as up to n Tennessee good example Mississippi is the other usual example how many different ways can you arrange the letters of the word Tennessee notice that the n's the E's the s's are all the same as each other there's one two three four five six seven eight nine letters in Tennessee there's one t four e's two n's and two s's which means the total number of ways is going to be 9 factorial divided by 1 factorial times 4 factorial times two factorial times two factorial 3780 and that's it again Mississippi is the other usual example how many ways can you arrange the letters in Mississippi if the I's the S's and the P's all look the same as the other eyes S's and P's definitely something to think about so that's the end of this section I think there's one more in the course I mean one more in the chapter enjoy hello and welcome to section 4.5 this will be the last section of chapter four and probability theory in this section we're going to combine the probability techniques of the first two and the counting techniques of the last sections into one nice little ball of of probability so we're going to use the basic counting rules to calculate probabilities in this section we'll look at counting problems that have more complicated Solutions than just one over the total number of possibilities a helpful trick for these problems look first to look for certain key terms the key terms are important because they Define the method to be used to find the correct answer words to look for include at least at most greater than less than between Etc so here we'll start out with a nice little example a group of 12 tourists is visiting London at one particular Museum a discounted admission is given to groups of at least 10. so a how many combinations of tourists can be made for the museum visit so that the group receives the discounted rate in other words we want to find the number of ways of having 10 and 11. and 12 chosen out of that group of 12. and B suppose that a group of tourists does get the discount what's probability was made up of 11 tourists in other words in a we got the total number of ways that you can get the discount and now we're given that they got the discount what's probability that it was made up of 11. so here's the solution keywords in this problem are at least so if at least 10 are required in the group that gets the discount has to be 10 11 or 12. now we calculate the number of combinations to get 10 number of combinations to get 11 and number of combinations to get 12. then add them to back add them together and notice that these are combinations because we aren't putting the tourists in any line we're just getting groups of those tourists so here's how to get the group of 10 tourists it's 12 shoes 10. group of 11. 12 choose 11. remember that 12 by the way and 12 choose 12 which is just one as we know the question applies that the group will get a discount if 11 a 10 11 or 12 so the number of ways is just the sum of those 66 plus 12 plus 1 so there is 79 ways that this group can get that discount now B we need to calculate this probability so we're given the group receive the discount what's the probability that it's a group of 11. so the numerator is going to be the number of ways of getting groups of 11 and the denominators are going to be the number of ways of getting the discount well we've already figured that ways of getting a discount is 79 and you figure out the ways of getting a group of 11 is 12. so the probability is going to be 12 over 79. Jack is setting a password on his computer he's told that his password must contain at least three but no more than five characters he may use either letters or numbers at least three but no more than five characters wow so a how many different possibilities are there for his password if each character can only be used once notice that this is without replacement and then suppose that Jack's computer randomly sets his password using all the restrictions given above what's the probability that his password is an arrangement of the letters in his name Jack so here's the solution for a it's got to be at least and no more those are some keywords there at least three no more than five so it's a number it's got to be three four or five so we gotta count the number of ways you can get three a number of ways you can get four and the number of ways you can get five the order of characters is important so this will be permutations there's 36 characters to choose from 26 letters 10 digits so here's the calculation for three it's 36 permute three for four characters it's 36 permute four and for five characters it's 36 permute five so the total number of possible passwords is just the sum of those which is about 46.7 million possibilities to find the probability of a randomly chosen password would include only the four letters from Jack's name find the uh we got we use the number from part A is the denominator that's the N of s that's our sample space I'm going to find the numerator the number of ways you can get the event calculate the number of permutations of four from a set of four four is Just 4 factorial which is 24 so the actual probability is that 24 divided by the total number of possible passwords so it's rather small now the way that they calculated this is never mind um it's n of e divided by n of s we don't need to know what e and s are except into calculating what n of e and N of s are Tina is packing your suitcase to go on a weekend trip she wants to pack three shirts two pairs of pants and two pairs of shoes she has nine shirts five pants and four pairs of shoes to choose from how many ways contain a pack or suitcase we do need to assume that everything matches which means that we are in the independent Realm this will be nine choose three times five choose two times four choose two because in the nine charts she's choosing three of the five pairs of pants she's using two and of the pairs of four pairs of shoes she's also choosing two so there's the number of ways that she can select her shirts number of ways that she can get her pants and every way she can get her shoes so the total number of ways from the fundamental accounting principle is just that product 84 times 10 times 6. I do want to emphasize the word and here implies multiply it's and being you have to choose this and choose those pants and choose those shoes in probability or indicates adding and indicates multiplying here's an interesting little drawing that might help see this shirts pants shoes they're independent of each other because we've specified everything matches there's 84 ways of getting her shirts 10 of her pants six of her shoes so by the fundamental accounting principle it's just the product of those an elementary school principal is putting together a committee of six teachers a committee of six teachers at the Springs Festival there are eight first grade nine second grade and seven third grade teachers at the school how many ways can the committee be formed notice we're not specifying that any that the committee has to contain a certain number of any of the grades there's eight 17 24 tshirts held together so the number of ways the committee can be formed is just 24 choose six how many ways can the committee be formed if there must be two teachers chosen from each grade it's going to be eight choose two times nine choose two times seven choose two for the same reason of Tina's suitcase and suppose the committee's chosen at random with no restrictions so we're in the case of a what's the probability that two teachers from each grade are represented so the denominator the bottom is going to be a and the top is going to be B because a is the number of possible outcomes and B is going to be the number of possible outcomes in the events 24 choose six eight choose two times nine choose two times seven choose two and then the probability will be just be this 21 168. divided by the total number n of s of 134 596. so even if you're not focusing on making sure that the committee consists of two first grader teachers two second grade teachers to third grade teachers there's still a 15.73 chance that it's going to happen naturally just out of randomness and that's it notice how this this section took all of our combinations and permutation stuff and used it to calculate probabilities of events and it comes back to n of e divided by n of s of course we are assuming Independence and all of those outcomes are equally likely but it's n of e Over N of s n of e Over N of s and that's it the end of chapter four the next lecture will be on specific probability distributions and things that we can do with discrete distributions so stay tuned hello and welcome to section five one this is the chapter or this section introduces the chapter that deals with discrete distributions this chapter is going to cover some named and therefore important discrete distributions this section is just looking at some basics of discrete distributions so by the end of the selection you should be able to understand the difference between discrete and continuous distributions or discrete and continuous random variables chapter 6 will deal with the continuous ones to know the purpose of the probability Mass function explain the three requirements for a function to be a probability Mass function that's that's an important one calculate probabilities using the probability Mass function determine sample space sample space also very important because it helps give you a better understanding of which distributions the data could actually come from and then calculate the expected value and variance of a distribution expected value and standard deviation will be the important ones the standard deviation is just the square root of the variance so let's start with the definition of what a random variable is a random variable is a variable whose numeric value is determined by the outcome of a probability experiment you can think of this as an outcome from the future some examples statistician's favorite flavor of ice cream you don't know that until you actually measure it ask the statistician a student's level of approval of a congressional decision you don't know that until you measure it AKA ask the student the Euro Knox college professor is born you don't know that until you ask until you perform that experiment so all of these are random variables Iran variables have or they follow probability distributions it's this fact that they follow probability distributions that allows us to understand the randomness of a random variable and I I want to say that just because it's random doesn't mean we don't know anything about the possible outcomes in fact we know a lot about those possible outcomes we don't know what the exact outcome is going to be but we know everything about it we know the expected value we know the uncertainty in that estimate we know the medians we know probabilities of specific outcomes so we know everything about that future outcome except for what that future outcome is going to be um these are the three requirements for a function to be a pmf all the probabilities have to be between zero and one this is true in general probabilities can't be greater than one can't be less than zero the sum of the probabilities of the sample space is one um cursive S is called the sample space it's the set of all values of x that can happen that have a nonzero probability of happening um so this notation this is summation over all the values of X that are in the sample space of that probability of the value the little X is going to be the value the big X is just going to be big X's these are random variables sometimes we'll have Big Y for a different random variable so the sum over the entire sample space of the probabilities will be one and the probability of a union is no more than the sum of the individual probabilities this comes from chapter four so the probability of a union B is less than or equal to probability of a plus the probability of B for events A and B they are equal if the intersection of A and B is empty it's less than if the intersection is not empty so let's create a probability Mass function for this experiment flip a coin three times count the number of heads flipped so the outcome or the random variable is going to be the number of heads flipped in those three coins from this we know that the sample space is going to be 0 1 2 and 3. how do we know that it's 0 1 2 and 3 well we're flipping the coin three times so the largest number of heads we can get is going to be three hits and the smallest will be zero and they're going to be counts this is discrete so we can expect it to be some sort of counts so the sample space is the set 0 1 2 3. second step is to determine the probability of each of those four outcomes we're going to rely on two assumptions I mean three the coin has two sides that'll be one two the coin is fair and the flips are independent coin is fair indicates that the probability of a hit is one half flips are independent means that the outcome of one flip is not going to influence the outcome of another so if these are true there are eight possible outcomes of the flips not of the random variable but of the flips you can get three tails and get Telltale head tail head tail head tail tail you can get head tail head tail head tail head head or you can get three hits the random variable is the number of heads so if you get Telltale tail you have zero if you get hit head you get three if you get tail head tail you get one probability of getting three tails is one half times onehalf times onehalf which is 1 8. so the probability of the outcome 0 is 1 8. similarly the probability of the outcome three is 1 8 because the only way to get three is head head onehalf times one half times onehalf these three outcomes from the flips give you the same random variable value of one because they all have one head probability of tail tail head is 1 8. probability of tail head tail is 1 8 probability of head tail tail is 1 8. so the probability of getting one head is 1 8 plus 1 8 plus 1 8. probability of getting two tails I'm sorry of getting two heads is just head head tail head tail head tattoo head 1 8 plus 1 8 plus 1 8 3 8. in other words there's three ways of arranging these outcomes so you get two heads times one half times onehalf times onehalf here's another way that we can represent this this will be graphically sample space is listed along the bottom the height of the bar corresponds to the probability of that specific outcome we could represent the probability Mass function in this way as well probability of the random variable equaling some value some specified value is 0.125 if this x is zero or three it's 0.375 if this little X is one or two and at zero otherwise notice this formula is also not unique we could also represent it as 3 choose x times 0.125 and keep this in mind when we get to the binomial distribution which will be in the next section remember that population parameter is a function of the population contrast this with a statistic being a sample of the data we want to use those statistics estimate the population usually and we'll do that in the second half of the course here we're going to look at ways of calculating those population parameters when we know what the distribution is some population parameters that we care most about will be the mean the standard deviation which is the square root of the variance and the median the definition of the expected value or the mean of a discrete random variable so the expected value of x where X is our random variable is just the sum over all values of X in the sample space of the quantity x times its probability recall from chapter 3 mu which is the population mean when we're dealing with distributions we can also use expected value of x just equal to the sum of x times 1 over n of x times well if each outcome in the population is equally likely to be observed then the probability of observing that person is just one over n and I suppose that should be a capital N expected value is just a long run average of those outcomes variance of a distribution is a measure of uncertainty in each outcome it's the opposite of the word precision I like the word uncertainty the variance of discrete random variable is given by that's a v of x we could also use Sigma squared just the sum over all the x's in the sample space of the quantity x minus mu squared times the probability of that outcome population variance remember was the same thing except one over n and this is perfect if every outcome is equally likely here we're dealing with unequally likely outcomes here's the wonderful definition the median notice that this explains why a hand waved saying it's uh it's about half is below and about half is above because dealing with the actual definition the median gives me a headache sometimes um when the distribution is continuous and we can just deal with the probability of X being less than or equal to the median that's tilde on top utilities are located to the left of the one as is just equal to 0.5 note that this implies that for a discrete distribution the median is not necessarily unique whereas for continuous distributions it will be here's an example three coins flipping through a coin three times the probability Mass function remember we came up with was this let's calculate the mean the variance and the median from definitions expected value of x is just the sum over all X that are in the sample space of the value times its probability there are four things in the sample space zero one two and three so it's zero times the probability of zero plus one times the probability of a one plus two times the probability of a two plus three times the probability of a three which gives us 1.5 so the expected number of heads on three flips of a fair coin is 1.5 doesn't surprise us variance same thing adding up overall X is in the sample space of the value minus the mean squared times the probability of the value so zero minus the mean squared times the probability of a zero plus one minus the mean squared times the probability of a one plus two minus the mean squared times the probability of a two plus three minus the mean squared times the probability of a three add them all up you get 0.75 standard deviation therefore is 0.866 that's the definition of the median I think the best way of using it is using cumulative sums of the probabilities so you order it from zero outcomes from lowest to highest start with the lowest outcome probability see if it's greater than or equal to 0.5 if it's not go to the next one and add the probability onto it so here x equals zero probability of that is 0.125 it's not greater than or equal to 0.5 so zero is not a median probability of a one was .375 so the cumulative probability at 1 is 0.125 plus 0.375 and that is greater than equal to 0.5 so 1 is a median note that since it comes out exactly equal to 0.5 the next value is also the median 2 is also a median and every number between one and two so 1.5 is a median 1.6 is a median here's the r code the X and P lines are going to be common to all the calculations it's the last one that shows you how to perform the calculation to get the the parameter so we've seen things like this P line so we're defining the variable p as equal to and we're using the C function to combine all of these values into one vector it's 0.125 0.375.375.125 so writing that second line P will now contain those four values the first line this is a new construct for us we're saying X is equal to the values from zero to three that's a colon so this will be 0 1 2 and 3. in some computer languages it'll you'll have instead of a colon three lower dots kind of like this to indicate zero through three but for R it's just a colon and now that we've got the sample space and the probability of each of those outcomes we can calculate the mean is just the sum of all those X's times their probabilities 1.5 for the variance is just the sum of the value minus the mean squared this is a carrot this is the thing on top of the six it's read as quote to the power of so this carrot 2 will mean to the power of 2 or squared times p added up and the standard deviation is just the square root of that function to take the square root in R is sqrt for the median we use the function cumulative sum or sum and we look for the first place where it gets to be 0.5 or greater and that first place remember the sample space is 0 1 2 3 0 1 2 3 0 1 so 1 is a median remember since this is exactly 0.5 then not only is one a median but so is 2 and every number between one and two example two ice hockey my stat 225 course we did a project where the students predicted the outcome of a future ice hockey game between the Portland Winterhawks and the Prince George cougars go Hawks I took all of their probabilities and averaged them for the entire class for the number of points scored by the Winterhawks so the class as a whole said the probability of them scoring zero points was 0.1 point was 0.1 two points is 0.2 3.6.4 and four points and five points or point one each so let's calculate the expected number of goals the uncertainty of that aka the variance and the median so here's a graph of that using the formula for the expected value we're adding up over all the x's in the sample space sample space zero one two three four five of the value times its probability and we get 2.6 so the expected number of goals to be made by the winter hawks is 2.6 you do not round that to the nearest integer the expected value is a long run average 's don't have to be in the sample space so the expected value is 2.6 it's not two it's not 3 it's 2.6 variance going to use the formula again plug and chug get 1.84 standard deviation therefore is 1.356 we could use the standard deviation with the empirical rule say that the probability of the Winterhawk scoring between 1.2 and 3.9 is about 68 percent we can actually calculate it exactly it's 60 percent the median start with zero probability is 0.1 that's not at least 0.5 add the probability of a one so we're at point two now that's not at least 0.5 I have the probability of two goals that's 0.4 that's not at least 0.5 add the probability of three goals a that comes up to be point a that is greater than or equal to 0.5 so 3 is a median in fact since the probability of three or less is greater than 0.5 we know that 3 is only is I'm sorry that 3 is the only median if these had added up to exactly 0.5 we'd know that 3 and 4 were both medians as well as everything between three and four but since it came out to be something greater than 0.5 we know that 3 is the only median we can do it in R quickly X Line This is the value 0 through 5. again there's that colon notation so run that first line X is now the vector 0 1 2 3 4 5. p this combined function the C function of 0.1.1.2.4.1.4 and the mean is just the sum of the X's times their probabilities 2.6 the variance is just the sum of the x minus mu squared times P's the standard deviation is just the square of that variance and the sum function r that's Q sum Q sum for x equals zero Q sum for x equals one for x equals two for x equals three boom x equals three is the median can sum is the cumulative sum of those probabilities so these values are probability of X being less than or equal to zero one two three four five so here's a summary of what we did today in this introduction to discrete distributions we looked at probability Mass functions and saw how they describe the probabilities of each outcome in the sample space we saw probabilities can be calculated by adding the individual probabilities this reflects chapter 4 very clearly the expected value is a weighted sum of the outcomes weighted by the probabilities expected value and mean are synonymous the variance is weight some of the distances between the outcomes and the mean again weighted by that probability and the median is the value again not necessarily in the sample space so it's just that at least half is less than or equal to it and at least half is greater than or equal to it in the future we're going to look at some named discrete distributions Bernoulli binomial on the next poisson later hypergeometric later I misspelled hypergeometric I guess we're going to use R to calculate those probabilities expected values if we wish it to find many uses for these probability distributions and modeling the real world events around us and then when we get to chapter six we'll repeat this for some continuous distributions and we're going to continue understanding how the data generating process helps us to better estimate the parameters the readings will be section 51 at Hawks appendix A1 and R for starters here are the intellecture questions not entirely intra lecture but we'll pretend question one what is a random variable again I would suggest in your notes write the question on the left hand side and answer it so that you can transfer that into moodle's quizzes what is a random variable question two is what is a sample space and question three is what is the difference between an expected value and a mean and that's it hello and welcome to section five two the binomial distribution in this lecture we're going to learn about two named distributions the Bernoulli distribution and the binomial you'll see that the Bernoulli distribution is just a special case of the binomial but the Bernoulli distribution does make a nice entrance into the binomial so here are today's objective again calculate expected value variance median probabilities associated with a Bernoulli random variable we're going to determine what random variable follows a Bernoulli binomial distribution using its definition and you do want to memorize the definition of a binomial random variable calculate the mean the variance of binomial determine if Bernoulli and binomial distributions are skewed and calculate probabilities from a binomial distribution and we're going to do a lot of that in R just to show you how easy it is to do so definition of a Bernoulli experiment a random variable with only two possible outcomes follows a Bernoulli distribution and we're going to define the success probability p so any random variable that has two possible outcomes is going to be a Bernoulli random variable some examples heads on a single flip the coin a correct choice and a true false question me stopping at Starbucks in the morning in a given single morning each of those two possible outcomes a success or a failure either I get ahead I get a correct choice and a true false I do stop at Starbucks or a failure the probability Mass function for the Bernoulli note that the sample space is 0 1 so and P is defined as the success probability so probability of X equaling 1 in other words having a success is p and we know from our rules the probability that if there's only two possible outcomes and the probability of one event is p the probability of the other is going to be one minus P the complement of it so the probability of a failure is one minus p um for reasons that will become clear when we get to the binomial distribution this can also be written as P to the power of X and one minus P to the power of one minus X let's calculate expected value and variance for Bernoulli's in general remember the definition the expected value is just the sum over all the X values in the sample space the quantity x times the probability of that X sample space is just 0 1 recall so this is zero times the probability of zero plus one times the probability of a one probability of a failure is one minus p probability of a success is p the expected value is p remember expected value just the long run average if my Bernoulli event is flipping a coin getting heads as a Fair coin then we would expect that I flipped that coin 10 billion times half of them will be ahead I'll get successes on half of them and that's what this expected value of x equal and P tells you similarly we can calculate the variance again it's sum over all the x's in the sample space if x minus mean squared times the probability of that X 0 minus p squared times the probability of a zero plus one minus p squared times the probability of a p algebra shows us that this is just P times 1 minus p you may see this is p times q and some sources Q would be the failure probability but let's keep it as 1 minus p so here's the pmf let's look at the median CDF function is what we're going to use for the calculator the median or call the CDF functions find as probability of the random variable being less than or equal to some value the cumulative part is the less than or equal to contrast that with the equals part cumulative will be for the less than or equal to CDF of a Bernoulli is zero when X is less than or equal to zero once you get to zero however once you get to that failure it hops up to 1 minus p stays at 1 minus P until you get to one then it's 1 minus P plus p from then on here's a that helps us calculate the median here's the CDF and we need to determine when that CDF is at least five at least 0.5 for the first time so the median is zero if success probability is less than a half it's one it's greater than a half and it's every number between 0 and 1 inclusive if p is equal to onehalf now the next two slides will show that this actually makes sense so here's the CDF for the Bernoulli when the success probability p is larger than one half this is a CDF because the vertical axis is the cumulative probability it's zero zero zero until you get to one to get to zero and then it pops up to one minus p stays at 1 minus P until you get to one that hops up to one so where is the first time that the cumulative probability is at least 0.5 right there one so the median when p is greater than 0.5 is 1. now when success probability is small when it's less than a half cdf000 popup to 1 minus P ooh we just popped up Beyond point five so zero is going to be our median so in other words when the success probability is large the median is one when it's small meeting zero make sure that that makes sense to you that should be very clear going from the Bernoulli to the binomial binomial random variable is defined statistically defined as the sum of n independent and identically distributed for newly random variables that's the definition of the binomial that actually comes down to this definition of five requirements and this you'll want to memorize by knowing random variable meets the following five requirements the number of Trials is known so we know what n is each trial has two possible outcomes that is each trial is a Bernoulli random variable itself the success probability p is constant from trial to trial identically distributed note that we don't have to know what p is we just have to know that it's constant for the trials are independent well it's up here trials are independent and the random variable or what we're measuring is the number of successes notice that one and five together tells us that sample space is 0 1 2 3 all the way up to n here's some examples no C's reflect the examples we had with the Bernoulli example uh so it's the times getting heads on N flips of a coin for Bernoulli it was just one flip of a coin times getting a six on N rolls of dice for Bernoulli it was one roll of a die time stopping at Starbucks and N mornings for the Bernoulli it was just one morning so again notice that the binomial random variable is just a generalization of the Bernoulli or you can think of the Bernoulli just as being a simplification or a special case of the binomial here's the probability Mass function for the binomial I like this version of it's in three parts this P to the power of X is the probability of getting X successes the 1 minus p to the power of n minus X is the probability of getting those n minus X failures and then the combinations part is just the number of ways that you can arrange those X successes in the end trials if we think back to section five one with the flipping the coin three times it was this combinations part that gave us the one the three the three and the one whereas the 1 8 part was just these two together binomial parameters the expected value of binomial is easily calculated from its statistical definition so this is the statistical definition the binomial let x sub I be a Bernoulli random variable define y as the sum of those X's this leads us to Y being a binomial with parameters n the number of bernoullis added together and P the success probability for each of those bernoullis notice there's no subscript on the P so this p is constant for all of those Bernoulli's here's the expected value expected value of y well we just substituted and what Y is this is y so we substitute it in we know that the expected value of a sum is just the sum of the expected values so we can switch the expected value and the summation we know the expected value of x sub I expect value for Bernoulli is just p and so we're n adding up the value p n times which is just n times p so the expected value of a binomial is just n times p as always stop and make sure that this works makes sense think of this in terms of flipping coins variance we can do the same thing variance of Y just substitute in what Y is the variance in the summation chain can switch positions because the X's are independent the variance of a Bernoulli is just P times 1 minus p we're adding up P times 1 minus p n times so there's our variance and a mathematical note this step requires Independence of the Bernoulli experiments which is a requirement of binomial so it works we'll do an example um I'll let you do the other four examples on your own and I really do suggest you do those work through them make sure they all make sense do them by hand and do them in r um so example one Fair coins let X be the number of heads flipped in 10 flips of a coin if the coin is fair then it's clear that X follows a binomial distribution with 10 trials and success probability of onehalf X is distributed as a binomial or X follows a binomial let's calculate these three things the probability of getting three heads probability getting three tails the probability of getting at most three heads or at least seven hits otherwise known as at least sorry at most three heads or at most three tails probability of getting three heads we want to calculate the probability that X is three this is a simple application of the probability Mass function here's the pmf we're given Little X is three n is 10 p is one half plug in chug get 0.1171875 10 choose 3 recall from chapter four it's just 10 factorial divided by 3 factorial divided by seven factorial which gives you 120. in R that's this one line notice three four things about I guess we're talking about a binomial distribution so the stem is binom we're asked to calculate the probability that X is equal to a value so that's what the prefix will be a d we're asked to calculate the probability X is equal to three so the first number is three then everything else defines that particular binomial distribution there are ten trials so size is equal to 10. and each trial has a probability of success of 0.5 so prob is 0.5 probability of getting exactly three tails if you get three tails we've got two options notice that this x is the number of heads flipped so we could redefine a new random variable for the distribution of tails or we can just note that if it's not a head it's a tail so the probability of three Tails suggests the probability of heads being seven again this simple application the probability Mass function plug and chug Little X is seven n is 10 p is one half probability is 0.1171875 before I go to the next slide what would this be an R binom because it's the binomial D because we're asked to calculate the probability that X is equal to something because we're asked to calculate probability X is equal to seven ten trials success probability of 0.5 this is a lot faster than this because I don't know 10 choose 7 in my head I'd have to use a calculator to do that finally we need to calculate the probability of getting at most three heads or at most three tails and most three heads are at least seven heads using the notation from chapter four this is probability of X being less than or equal to three Union X greater than equal to seven since there's no overlap between this event and this event probability of the Union just assuming the individual probabilities so to do this by hand we need to calculate the probability of X being less than or equal to three and add it to the probability of X greater than or equal to seven X less than or equal to three as X is zero x equals one x equals two x equals three and the event X greater than seven is just seven eight nine and ten have to calculate all those individual probabilities add them together and we get the answer if you do it by hand there's no shortcuts on this no special formula it's just you got to use this formula properly changed for each of these now if we want to do this in r it's pretty straightforward you would use the CDF function in r CDF remember is always less than or equal to so we've got to change our less than or equal to three Union greater than or equal to seven into just two less than or equal to's from chapter four we got the complements rule the probability of X being greater than or equal to seven is one minus the probability of X less than or equal to six which means the probability that we need to calculate is just part of X less than or equal to three plus this one minus probability of X less than or equal to six notice now all of our probabilities are cumulative they're all less than or equal to which makes things rather easy in R this would be P binom three and then the definition the binom plus one minus P binom six plus the trials and success probability notice we went from D binomial to p binom P is for cumulative probabilities D is for what I call Point probabilities P is for less than or equal D is for equal the interstate word problem remember I'm not going to do these I will encourage you to do these yourself I will require them as much as possible really not too much to say on these they're more activities for you illustrating calculations some examples from the life of my or time for my life as a consultant so here's what we learned from this slice deck definition the Bernoulli definition of the binomial expected value of a Bernoulli and a binomial distribution the variance of both of those applications a lot of examples got five examples again you should go through those how to use R to calculate the point probabilities the equals probabilities cumulative probability is less than or equal to for the equals probabilities you use d for the less than or equal to use p you'll want to download the all probabilities file we're going to do the same thing with the poisson in the future we're going to see that these distributions do describe real world events we're going to repeat all this with continuous distributions and we're going to realize that understanding the distribution of the data helps us to describe the underlying process better and that will allow us to estimate parameters of Interest covered two of these R functions in this slide deck but I'm going to give you four of them we looked at the D and the P versions the D for the equal the P for the less than or equal to both of these calculate probabilities you've seen the r version before you saw that in a lab R generates a random sample from that distribution so D and P give probabilities R gives a sample from the distribution and Q will give you the quantile or the percentile this will be useful for calculating calculating medians first quartiles 97.5 percentiles what have you so the Q will calculate a data value an X corresponding to a given probability and notice if you want to do Bernoulli random variables just make sure size is equal to one because really that's the only difference between a binomial and a Bernoulli R for starters appendix A2 and A3 will help Wikipedia's got a couple good pages on the Bernoulli and the binomial and so here are the intellectual questions not so intro lecture for this section but they're here again on the left hand side write the question underneath it write your answer so you can transfer that to your Moodle quiz what are the five requirements of a binomial distribution again you must memorize these what is the difference between a binomial and a Bernoulli and what is the formula for the expected value of a binomial random variable and that's it hello and welcome to section 5.3 the poisson distribution here we introduce a second discrete distribution and I'd like you to keep comparing it to the binomial to see what the similarities and the differences really are so by the end of this lecture you should be able to determine what random variables follow a poisson distribution by using the definition of a poisson calculate the probabilities from a poisson distribution both by hand and by R and calculate the expected value of variance median probabilities associated with that random variable um so here's the definition a random variable that is a count of successes over an area or a time period follows the poisson distribution with an average rate parameter of Lambda contrast this with the binomial distribution which was this count of successes over a certain number of Trials here it's over a time period so examples heads flipped in an hour as opposed to heads flipped in 10 flips which would be binomial here it's heads flipped in an hour number of dents on a car number of errors on a page number of terrorist attacks in a year number of bacteria in a swimming pool influenza cases in a week Wars in a year the binomial would be the number of states or countries at war in a year notice there's an upper bound to that um or binomial would be the the number of people in this class with influenza in a week that also has an upper bound notice in each of those cases you're you're measuring number of successes out of a total population of n bacteria swimming pool would be a poisson a number of swimming pools in Galesburg with bacteria would be binomial notice what I'm getting at here is that for a binomial it's the number of successes out of a number of Trials so the sample space is 0 through n whereas for poisson distribution it's not the number of successes over trials this number of successes in a time period or an area which case there is no upper bound therefore the sample space for poisson is 0 1 2 3 4 dot dot it can be proven AKA is beyond the scope of this course the probability Mass function for poisson is for probably Mass function it's got the equals part random variable is X the value is Little X is equal to e to the power of negative Lambda times Lambda to the power of x all over X factorial this e is the natural log base it's 2.7182 Etc Lambda is the average that's the parameter that we have to give it and X is the value that we want to find the probability of it can be shown that the expected value in the variance for a poisson are both Lambda that's kind of cool um we'll go with an example let X be the number of heads flipped in a minute I'm given that I average 24 flips per minute I've noticed I didn't say I am flipping the coin guaranteed 22 times 24 times in this minute I'm saying average 24 flips per minute if the coin is fair then it's clear that X follows a poisson distribution with Lambda equal to 12. probability of getting no heads probability of getting at most 20 heads and the probability of getting at least one head in the first six seconds probability of getting no heads in that minute so we're asking what's probably X is equal to zero we're given Lambda is equal to 12. so it's a plug and chug e to the negative 12 12 to the zero all over zero factorial recall from chapter four zero factorial is one so this is equal to point zero zero zero zero zero six one one four in other words if I actually do this experiment flip the coin for a minute and come up with zero heads then either I don't average 24 flips per minute or the coin is not fair or I should go out and buy a couple lottery tickets because this outcome is extremely rare on R this would be D plus zero Lambda equals 12. again D is for the probability of calculating the probabilities of equals here plus is for the poisson distribution zero we're calculating the probability X is equal to zero and we have to specify that Lambda is 12. because we're given Lambda is 12. part two what's the probability of getting at most 20 heads in that minute so probability of X being less than or equal to 20. of the random variable being less than or equal to 20. this is a quote simple application of the probability Mass function this is equal to the probability x equals zero plus probability x equals one plus the probability of x equals two plus the probability of x equals three okay I'll stop there until we get to 20 here's the pmf we're just adding up for all those various values of x from 0 to 20. easy way since this is a less than or equal to is to use the P form again the stem is pois for the poisson or calculate calculating the probability of X being less than or equal to 20. and Lambda is still 12. the probability of getting at least one head in the first six seconds at least notice X was flips heads in a minute we want to do this in SEC six seconds so we have to define a new random variable um six seconds is a tenth of a minute so the expected number of heads is just going to be 12 divided by 10 the number of heads in a minute divided by 10 because now we're in terms of six seconds plug and chug y greater than or equal to one so we're I guess this should be a y so we're summing up from Y is equal to one all the way up to Infinity this is going to take a long time if we do it this way but recall the complements rule probability of Y being greater than or equal to one is equal to one minus the probability of Y equaling 0. and this part here is the probability that Y is equal to zero one minus that it's y equals two we'll use a d for the equals and a zero so there's about a seventy percent chance that I get at least one head in SEC six seconds complements rule comes in really handy when dealing with poissons let X be the number of words in a year from the binomial slide deck we know that the average rate of wars in a year is one over 0.8462141 5 Section 5 1 gave us this this one divided that be 1.181734 so that's Lambda for one year I want to know what's probability that we have five years without a war if this is Lambda for one year the Lambda for five years will be just five times that boom and so probability of Y is equal to zero it's just .002715796 this is about 1 over 500 so we'd expect um a fiveyear P streak to happen about once every 500 years which seems to be borne out by reality real estate developer in Galesburg is looking to determine if it would be profitable to renovate the ferris building downtown into a boutique hotel to help determine this he has the student of mine who graduated last year to help estimate it John the student estimated the rate would be about 150 people per week the developer decided that a minimum of 130 per week would be needed to turn a profit so if John is right what proportional weeks will not be profitable so we're given Lambda is equal to 150 we want to know the probability of being 130 or less or I'm sorry less than 130. so this is a probability this is not a cumulative probability because accumulative be less than or equal to so we make it a less than or equal to by dropping the 130 by 1. it's now a cumulative probability so we use p uas because it's hipposan 129 and we're given that Lambda is equal to 150. so about four and a half percent of the weeks will not be profitable another example Galesburg crime there's actually a few examples in part of example four um reportedly the number of crimes in Galesburg was 96 last year so we're going to use that for our Lambda if there were four violent crimes last week is there significant evidence of an uptick in the crime rate okay so Lambda is 96 per year I got four violent crimes last week so we could make that Lambda in terms of weeks by dividing 96 by 52. so if the number of violent crimes in a week if x is that then we're asked to calculate probability of X being greater than or equal to four and interpreting that we're given X follows a poisson with Lambda equal to 96. this is crimes in a year divided by 52 so that would be crimes in a week this is the wrong direction it needs to be less than or equal to so this be one minus X less than or equal to three it's a less than or equal to so we can use the P form of poisson 3 Lambda is 96 out of 52. and we get 0.116 so what this means is if the crime rate stayed the same that is 96 per year then we'd observe four crimes a week about 11 of the time that's rather large really taking everything into consideration so it's not really evidence that the crime rate's gone up that isn't evidence crime rate may have gone up but this doesn't provide sufficient evidence for that so let's say there were 16 violent crimes last month notice part A it was four in a week so we'd expect 16 in a month if it's four in a week for four weeks so this really is in some ways the same question as last but we're holding that increase of crime for instead of over a week we're doing it over a month so really we're asking does increasing the sample size affect the probabilities so we're asked what's the probability and we're supposed to interpret the probability of X being greater than or equal to 16. given that Lambda is 96 over 12 because it's we're in terms of 12 months greater than or equal to 16 is 1 minus less than or equal to 15. we're now less than or equal to so we can use P form of poisson 15 Lambda and now we got probability of being .00823 that is very small in other words if the crime rate stayed the same we would expect to see 16 crimes in a month less than one percent of the time so this would constitute some evidence that the crime rate did go up this year now the good question that you're thinking is okay the last one it was Point 11 and this one's .008 0.11 it was relatively large interpreted as being relatively large .008 is relatively small the 0.11 doesn't give us sufficient evidence the .008 does where's the separation between doesn't give us the evidence and does give us the evidence I'm not answering that now but keep this in my effect over on the left hand side of your notes write something like what's the difference or what's the separation between not enough evidence and enough evidence Circle it make it stand out a little bit maybe put Alpha around it a few times just to so that we can refer back to that in the future so here's a summary of the poisson poisson distribution model is the number of successes or over a time period or an area a parameter of a poisson distribution is Lambda this Lambda is both the mean and the variance we know the probability Mass function of the poisson that allows us to calculate probabilities from a poisson random variable the future we're going to look at the hypergeometric notice the hypergeometric is spelled correctly here and also hypergeometric we've bumped into that before in in lab B hybrid or about ready to hyper geometric like the binomial model's number of successes given a number of Trials so you've got to figure out what is the key difference between the binomial and the hypergeometric and then chapter 6 will be the continuous distributions here are the r functions two of these we dealt with two of those are implied they all have the stem pois and they all require specifying Lambda because those are the keys to a poisson distribution the D form is for calculating probability of X equaling a value the P form is for the CDF calculating probability of X being less than or equal to a value the r generates random values from a poisson and the Q form calculates the quantiles so if I want to calculate the median I do Q Plus of 0.5 and specifying whatever value of Lambda it is if I want to calculate the 10th percentile I put in 0.1 for p uh the readings Hawks was Section 5 3 R for starters this is appendix A7 and as usual Wikipedia's got a pretty good page on the poisson distribution gives you a nice flavor of how far you can go with these probability distributions so here's the intra lecture question for five three our questions for five three question one what is the parameter of a poisson distribution it is the parameter for poisson hint it's called Lambda question two what's the difference between a binomial and a poisson variable I won't give a hint on that one except to say go back over your notes to make sure that you have this written down this is this is important and what's the formula for the expected value of a poisson that's also pretty easy one so of these three the sequence going to be the tough one and that brings us to the end of the poisson distribution take care hello and welcome to the last section of chapter five this will be the last named discrete distribution we'll be working with it's called the hypergeometric this feat is featured in lab B um so by the end of the lecture you should be able to determine which random variables follow hypergeometric distribution using its definition identify the three parameters of a hypergeometric distribution and calculate the expected value variance being probabilities associated with hypergeometric random variable hypergeometric is rather difficult to work with so don't expect too much work using the hypergeometric doing things by hand you'll see the probability Mass function and understand what I mean when you do see that the hypergeometric distribution describes the probability of obtaining X successes in K draws or trials without replacement from a finite population that contains exactly M successes and N failures see I told you it was kind of complicated um Keys two keys to this are it's without replacement so you're drawing and you're not putting it back and the population's finite if you are doing it with replacement or if the population is infinite then you've got a binomial random variable here so this is a binomial constrained to a finite population without replacement it's well when you get to the lab you'll understand that it's better but as you work through this uh slide deck you'll see why we tend to focus on the binomial examples number of Hearts drawn from a deck without replacement so I draw five cards what's the probability three are Hearts if I don't replace finite population Capital n's 50. two um and it's without replacement a number of sophomores counted in the Library without recounting somebody um number of Parolees that returned to the Hill Correctional Center in Galesburg um in each of these cases it's without replacement and the population is finite so key um so hyper geometric finite and no replacement now you'll you'll find out and I think the lab helps with this that if your population is sufficiently large but not infinite then the advantage of using the hypergeometric is very small it might as well stick with the binomial recall that the probability Mass function provides the probability of each element for the sample space for a hypergeometric random variable this is the sample space see notice automatically that we see that the hyper geometric is rather complicated to work with for the binomial it's just from 0 to n here it's the larger of zero and K minus n all the way up to the minimum of K and M how do we parameterize this there's lots of ways of parameterizing the hypergeometric that's another frustrating thing with binomial it's always n and p or n in pi for the hypergeometric there's at least a half dozen ways in fact Hawks uses a way that's different from either r or the forsberg text um the uh we can do it by successes and total or by successes and failures or by combination of the two but fundamentally the probability Mass function here at the bottom is just a number of ways to succeed times the number of ways to fail divided by the number of ways you can have those end trials and these are combinations so review of chapter four would not be unwise um so here's the probability Mass function it's m choose X n choose K minus X divided by M plus n over choose k X is the number of successes you care about m is the number of successes in the population in this case little n is the number of failures in the population so X is the number of successes in your sample K minus X will be the number of failures in your sample and then the denominator is just going to be the total population size divided by the total sample size and notice the switch here K is now the sample size not n and trust me this is one of the better parameterizations of the hypergeometric here's another one here K will be the population successes instead of a little m n will be the population size which means n minus K will be the population failures X will be the successes in your sample n minus X will be the six failures in your sample here n will be the sample size so n choose n will be the denominator they say the same things they're using different letters to represent what those things are but they're saying the same thing this is the number of ways of getting X successes from the population this is the number of ways of getting your specific number of failures in your population and this is the number of possible samples you can draw from that population same as it was for the previous expected value of a hypergeometric does not need to be memorized you've got this in your notes so make sure you know where you can locate it it's just k ick your sample size p the probability of a success in the population m is the number of successes n is the number of failures so n plus m is a population size so this M over n plus m is just the probability of a success in the population and that's the sample size so it's essentially n times P but we're using different letters here's the variance it's your sample size times the probability of a success in the population times the probability of a failure in the population times an adjustment Factor notice this is thinking back to binomial this would be n times P times 1 minus p times some other number notice this number is always going to be between 0 and 1. which means that the variance of a hyper geometric is always going to be smaller than that of a binomial that's important variance of a hypergeometric is always going to be less than or equal to for extremely large sample sizes that of a binomial as long as K is not equal to one if K is equal to one then there is absolutely no difference between a binomial and a hypergeometric go back to the definition of a hypergeometric to see why that's the case the big difference is with replacement or without if you're only drawing one thing it doesn't matter if you replace it or not it's the same probability this is another way of saying exactly what I did understand what the parameters represent don't get hung up on the formulas themselves understand that the expected value is just the sample size times the probability of a success in the population the variance is n times P times 1 minus P times some adjustment Factor so here's a couple examples okay here's a few examples I'll let X be the number of Spades drawn at a four car uh four draws from a deck of cards without replacement if the deck is fair and it's clear that X follows a hypergeometric with m equal 13 the number of successes in the population is 13. and the number of failures in the population is 39 and our sample size K is 4. success failure sample size so here are the probabilities or here are the questions what's the probability of getting one Spade what's the probability of getting at most three Spades and what's the expected number of Spades probability of eating one Spade plug and chug probably X is equal to one we're given m is 13 x is one n is 39 K is 4 4 minus 1 is 3. a little check here 13 plus 39 has to be 52. 1 plus 3 has to be 4. why is that a check this first refers to the number of ways of getting those successes the second is the number of ways of getting those failures and the total is just failures plus successes so the probability of getting one Spade is 0.438847539 here it is in r probability that X is equal to one so it's D and a 1 there this is a hyper geometric so the stem is hyper parameters m is 13 and 39K is four and by the way this is the parameterization that R uses mnk probability of getting at most three Spades this is a less than or equal to question less than or equal to three since it is a cumulative a less than or equal to we can use the P version 3 m n and K so the probability of getting at most three Spades is 0.9973589 in other words pretty good chance that you'll get at most three what's the expected number of Spades sample size times success is over trials so the expected number of Spades it's going to be one make sure this result makes sense especially you think about lab B a quarter of the deck is Spades I draw four so I'd expect one example two I wonder if stat 200 attracts third tier students at a greater rate than other courses so X will be the number of third years in stat 200 we know that the number of third year students at Knox is 356. we know the number of nonthirds years is 978 we got that from the registrar's website therefore those are population numbers in this stat course there's 41 students 18 or 30 years so we need to calculate the probability X is greater than or equal to 18. given m is 356 n is 978 and K is 41. this is in the wrong direction it's got to be a less than or equal to so using the complements rule this is one minus probability of X is less than or equal to 17. this is what it is in r 1 minus probability under hypergeometric of X being less than or equal to oops that should be a 17. so this number here should be 17 not 18. m n and K are given to us previously this is probably around .005 very small probability of this happening therefore it appears as though third years are over represented in this course or I'm wrong about m n and K I trust the registrar to give me these two numbers I know I can count to 41. therefore I'm going to conclude that third years are indeed over represented if we were to Pretend This was a binomial we get a probability of .00537 not much of a difference between these two the reason why there's not much of a difference between the two is because the population size is quote rather large it's thirteen hundred or so so once you get a large population hypergeometric and binomial are essentially the same thing I wonder if my math 121 attracts fourth year students at a lower rate than other courses let X be the number of fourth years in my math 121 course according to the registrar's website we know the number of fourth year is at Knox is 278 nonfourth years is 10 56. in my 121 there are 30 students three or fourth years so I need to calculate the probability of X being less than or equal to three where X follows a hypergeometric distribution with m n and k equal to 278 1056 and 30. P hyper P because it's less than or equal to we added at the 3 I mean it right this time MN and K given to us by the registrar's website and me being able to count the class this is 10 percent because the probability is not that small there does not seem to be much evidence that math 121 attracts fourth years at a lower rate than other courses 10 percent not that small again this goes back to um I think was the binomial slide deck where we started talking about okay what's the difference between no it was a poisson and the crimes what's the difference between not that much evidence and sufficient evidence so you might want to write in the in the margin again Arrow there of asking the question difference between sufficient evidence and not sufficient evidence by the way 0.099 versus 0.102 Hyper geometric versus binomial don't get me wrong the hyper geometric is the correct distribution and doing this by using R is pretty darn easy so there's no reason to use the binomial approximation but if you're doing it by hand or using some things that we will be doing in this class in the future that are predicated in the binomial as long as you're set plus size is large enough you can use those things those future things for both the binomial and the hypergeometric so the summary hybrid geometric models the number of successes out of a specific number of Trials when the population is finite and the elements cannot be selected multiple times there are multiple ways of parameterizing this distribution I gave you two of them but they all specify the sample size number of successes the failures in the population in some way you saw the pmfs of the hyper geometric you know the mean and the variance the hyper geometric again you should not spend your time memorizing those have them in your notes and make sure you're able to access them and use them the future today was or this lecture is the end of the discrete distributions the future will be continuous distributions and that'll start with chapter six we've got the four R functions all of them have the stem of hyper for the hypergeometric they all require you to specify M and K again the D is for the equals probabilities the P is for less than or equal the r is for generating a random sample from that distribution and the Q is for the quantile so if I want the median I'll put 0.5 for p and I'll be able to get the median if I want the third quartile I would put 0.75 in here for p and get the third quartile course readings Section 5 4 and Hawks this is appendix A6 in R for starters Wikipedia's got an interesting page on the hyper geometric again it'll give you a taste of where you can find the formulas if you forget them and it'll show you a lot of things that we can do with this probability stuff which brings us to the three interlection questions one what are the five requirements for a random variable to follow a binomial distribution yes this is for the binomial distribution you must know those five two what's the difference between a binomial and a hypergeometric variable and question three what's the difference between a binomial and a poisson random variable so a little hint here seems like we're focusing on the binomial as being the most important distribution in this chapter if it seems that way then good because it is but also you're seeing how close the poisson and the higher pot geometric are to the binomial when they are close and by extension when they're not close and that's it take care hello and welcome to chapter six in chapter six we're talking about continuous distributions contrast this with the discrete distributions in chapter five chapter 6 and Hawks goes directly to the normal distribution introduces some things that just kind of pop out of nowhere to help see where those things come from I'm introducing two other distributions the uniform distribution which will be this lecture and the exponential distribution which will be the next so by the end of this lecture you should be able to understand the difference between discrete and continuous random variables know the purpose of the probability density function notice this is the probability density function not the probability Mass function be able to prove that the density is not a probability you know the purpose of the cumulative distribution function CDF hint it's the same as the purpose of the CDF with discrete random variables be able to calculate probabilities using geometry or the CDF in fact we're going to figure out an easy way of calculating or creating the CDF for the uniform and then understand the uniform distribution it's two parameters it's sample space it's expected value its variance standard deviation uh median things like that so here's a definition of continuous random variable it's a random variable with a sample space consisting of an interval of values that means for continuous random variables there is always a value between two other values in it so there is no next two with continuous random variables for discrete random variables you could list them off there's a next two after one came two after two came three for a continuous random variable between any two values there's always going to be another value examples of continuous of actually continuous random variables student height age of a car time spent at a stoplight distance of golf ball goes those are all continuous measures there's also a category of random variables that are essentially continuous they're they're not continuous they're discrete but we can pretend they're continuous and not lose too much not create too much error in much the same way that when we were dealing with a hypergeometric distribution if the population was quote large enough we could use the simpler binomial and we wouldn't introduce too many errors such examples of quote near continuous random variables would be GPA definitely a discrete distribution but the distance between two levels of it is so small compared to the range that we can just pretend that it's continuous and not introduced too much by way of error annual salary my salary is paid down to the penny uh so the the the grid is a penny but my salary is in tens of thousands of dollars so that that close enough to being continuous that we can pretend it's continuous same with gross domestic product GDP per capita crime rate number of years of corn grown in Iowa that's definitely a discrete distribution but the distance between one ear and the next year that that that grid is so small compared to the number of ears grown that we it we can pretend that it's continuous and not lose too much what I'm getting at here is just like with the binomial there are benefits to using the binomial when you can Simplicity is is the best that even though the data are generated from hyper hypergeometric if this the population is large enough we can pretend it's binomial and not introduce too much error and then reap the reward of everything that comes from it being a binomial same thing here with these quote near continuous Ram variables they're not continuous they're discrete but they're so close to being continuous that we can pretend they're continuous and reap all the benefits of a continuous random variable and there are several um the first continuous random variable that we're going to talk about is the uniform distribution the uniform distribution is a continuous distribution that describes random variables whose likelihood of occurring is constant across a specified interval notice the word probability is missing from that definition when we start talking about continuous distributions we have to start talking about likelihoods of events we'll be able to retrieve probabilities in a lot of cases but we have to start talking about likelihoods if the ram variable X has a uniform distribution we're going to write X and there's that tilde is distributed as a uniform with parameters A and B A is the lower bound and B is the upper bound if x is a uniform a b distribution and then the sample space is is all values between a and b the expected value is just a plus b over 2. the variance is just the width of the interval squared over 12. standard deviation is just the width of the interval divided by the square root of 12 in other words it's the square root of the variance this is the probability density function for all X that's in the interval A to B the likelihood is 1 over B minus a or the density is 1 over B minus a outside the interval the density or the likelihood is zero it's a uniform distribution because each of the likelihoods is the same in a probability density function you're measuring the likelihood or the probability density for the uniform it's a constant between a and b the height is going to be 1 over B minus a Y is the height 1 over B minus a this is a graph of the likelihood let me be clear this is the graph of the density or the likelihood this is not a graph of probabilities we know it's not a graph of probabilities because if a is zero for instance and B is one half this width is going to be onehalf and the height is going to be two and you can't have probabilities that are greater than one so this is clearly not a probability to obtain a probability you just have to find the area corresponding to what you're trying to the event you're finding the probability of so probabilities are just areas of the PDF functions for most continuous distributions that means we have to use calculus for the uniform distribution we can use high school algebra or we can use sixth grade algebra it's just squares and rectangles note now we know why it has to be have a height of 1 over B minus a because the interval length is a to B it's the length is b minus a and we know that the probability of something in this interval happening has to be one so the probability that X is between A and B has to equal one it's a rectangle so B minus a which is this width times the height has to equal one because rectangles are width times Heights and therefore the height has to be one over B minus a um note that the PDF has two purposes one it's to help the researcher understand the probability for a continuous distribution and it's to help the researcher calculate probabilities of a continuous distribution and in red probabilities are areas under the density curve another thing that these PDFs can tell us is where the outcome is most likely here the the likelihood is flat therefore every value between A and B is equally likely um if there was a mound to this then those things near the those values around the highest around the peak that's the word Peak will be more likely so let's see how we do this there's only one stop light between home and school in the morning my home and my school it regularly Cycles among green 175 seconds green five seconds yellow and 180 seconds red there's only one stoplight it's at Maine and Academy it's it's a doozy given that I stop at the light in other words given that the lights red what's the probability that I wait at most 60 Seconds so let's define the random variable t as the time I spend waiting and I want to find the probability that I weighed at most 60 seconds that t is less than or equal to 60. because the time I wait does not depend on when I get there notice it's the only light if there were several lights I had to stop through then it wouldn't be a uniform distribution but because it's the only light it is a uniform distribution because there's a definite lower and upper bound the lowest that I will stop there will be for zero seconds and the highest that I'll stop there will be 180 seconds I'm definite upper and lower bound uniform distribution we now have that t the time I spend waiting at the light in seconds is distributed according to uniform with parameter 0 and 180. or with minimum zero and maximum 180 seconds and again we have to calculate probability T is less than or equal to 60 that I wait at most 60 Seconds the outside rectangle is the distribution of t specifically it's the probability density function of t the dark blue is the area that I want to calculate it's the probability T is less than or equal to 60 Seconds to find the probability it's just the area under the Curve of the density curve corresponding to this width Heights 1 over 180 the width of the dark blue is 60 so the probability of waiting at most 60 Seconds is just 60 over 180. this should ring some Bells from chapter four when you were talking about equally likely events because the PDF of the uniform is just a rectangle and because areas of PDFs are probabilities and I want to emphasize that again areas in PDFs are the probabilities we just need to calculate the region of T less than or equal to 60. height times width so 33.3 percent chance that I wait at most 60 Seconds for the uniform this is easy to do in our heads if we want to use r here's how we use r again we're looking for a probability of T being less than or equal to something so we use the P version the stem for the uniform is uni f we're looking for the probability T is less than or equal to 60. then we specify the parameters of the uniform Min equals zero Max equals 180. and the keywords are Min and Max Not A and B the CDF of a probability distribution recall is defined as a probability of the random variable is less than or equal to some value traditionally we give this a capital f for continuous distributions without knowledge of calculus this is frequently rather difficult to calculate however for uniform we can just rely on middle school geometry so we're back to our generic uniform distribution ranges from A to B which means the height is 1 over B minus a we're going to let X move between a and b and we need to calculate this area here that's pretty easy the area of this is just x minus a divided by B minus a in surprise you just calculated CDF function this is the CDF function so if I want to calculate the probability probability that X is less than or equal to 6. I'd put 6 in here for x 6 in here for x and I would have to know A and B from the definition of the uniform I want to be clear calculating CDF functions usually requires calculus usually requires integration but for those who've had calculus integration is just a fast way of finding areas and in this case we've got geometry to help us find the area the quantile function is the inverse of the CDF we've bumped into the quantile function several times in the past um it's the quantile function is a function of P the CDF is a function of X it's the same relationship here by the way CDF is a function of X that calculates p quantile function is a function of P that calculates X they're inverses so from the CDF we know that P is equal to x minus A over B minus a this is for the uniform all we have to do is solve for x so X is equal to P times the width plus a the starting point so this thing on the left is the quantile function you're given P you solve it for x the first line the thing on the right was the CDF you're given X to calculate p inverse functions so here's the quantile function we can use a capital Q to symbolize it the quantile function for the uniform is just P times the width plus a at the starting point so here's what we've learned in this slide deck actually let's go back a page so how can I use this quantile function I can use it to calculate the median remember the median is the 50th percentile so to calculate the median of this uniform distribution we put 0.5 in for p 0.5 in for p and solve if we know what b and a are we can come up with a number if we don't know what b and a are then we just get an expression I will leave it as an exercise for you to show that the median of a uniform distribution is equal to the mean of the uniform distribution all it takes is understanding what the quantile function is and a little simple algebra so here's what we learned in this slide deck actually before we get to this let's go to our intra lecture questions there's three of them again write the question over on the left hand side of your notes answer below it so you can transfer that into Moodle question one what is the difference between a discrete and a continuous random variable question two what is the definition of a uniform random variable and question three what are the two parameters of a uniform distribution remember you do have the pause button to use okay let's go into the uniform summary now here's what we learned in this slide deck continuous random variables describe different phenomena than discrete random variables they describe things that are continuous instead of counts in a discrete random variable you talk about the probability Mass function which actually does give you a probability in continuous you've talked about a probability density function which gives you a likelihood and you have to calculate areas in order to turn that likelihood into a probability probability is the area under the PDF curve the CDF is probability that the random variable is less than or equal to some value uniform distribution describes a random variable where all values are equally likely the mean of a uniform random variable is a plus b over 2. turns out that that's also the median and I gave you that as a fun exercise future we're going to look at the exponential and normal distributions we're going to practice calculating probabilities using formulas tables and r I'm going to continue thinking about the relationship between random variables around us and their distributions the key part for chapters four five and six is starting to look at the world around us and think in terms of probability distributions here's the for our functions notice the stem for each is u n i f for uniform for the density function the PDF it's just d uniform now we see what the D actually means for the cumulative probability it's p for a random sample from this distribution it's r and then the quantile function is q and while we were here we calculated the P the CDF we calculate the PDF as well and we calculate the quantile function by hand we realize it's not that difficult but let's be clear it's not that difficult for this distribution once you go beyond something as simple as the uniform it begins to get much more difficult and sometimes you can't even determine what the quantile function or the CDF function is it may not exist the readings are for started appendix B1 and B2 there's nothing in Hawks and uniform distribution continuous in Wikipedia now this is as far as you have to go if you've taken some calculus and you want to see how to use it I give you the calculus extra but if you don't know calculus skip over this and you will lose nothing so how does one directly calculate the expected value for continuous distribution you use calculus refer back to the discrete case for the expected value it was x times the probability of X added up over everything in the sample space in the continuous case you're integrating over the sample space of x times the probability density function so this is the definition of the expected value in the continuous case similarly this is the formula for the variance in the continuous case notice again there's an x minus mu squared and this f of x isn't a probability but it sure looks like that's where the probability was in the discrete case instead of adding your integrating there's another formula for the variance that's equivalent and sometimes it's easier to use sometimes it's not so for let's see how to use these formulas um expected value of x is the integral over the sample space of x times the density of DX sample space is all values between a and b of x times the density the density just one over B minus a DX 1 over B minus a is a constant so it can be pulled out integral of x DX is x squared over two x squared over two there's that one minus B one over B minus a and it's evaluated between B and A between x equals a and x equals B x equals B this is B squared over two times one minus B over a subtracting off the evaluation at the lower bound a squared which gives us this next line and now we can do some calculus not calculus algebra stuff B squared minus a squared is B minus a times B plus a why did I know to do that well there's a B minus a out here and I knew this was the difference of squares so the B minus a is cancel and I'm left with a plus b over 2. variance same idea I'm using the second formula for the variance calculating What's called the second moment just the integral over the sample space of x squared times f of x DX and then I'll subtract off the mean squared integral of x squared DX is X cubed over three times one over B minus a evaluating between B and A so that's B cubed minus a cubed over three and B cubed minus a cubed has a B minus a term to it B cubed minus a cubed who knows B minus a times B squared plus a B plus a squared and we got some canceling there going away I combine these two fractions a 1 3 and 1 4 common denominator is the twelve Ah that's where the 12 comes from common denominator is 12 so this becomes 4 over 12 and this is 3 over 12. there's four there's the three cancels distribute the four distribute the 3. combine like terms B squared minus two a B plus a squared we know is equal to B minus a squared it's amazing how much high school algebra does come in handy sometimes which gives us our formula for the variance that makes sense the B minus a squared I mean the the wider the interval the more uncertain we are in the outcome so it makes sense that the variance would depend on B minus a squared now we see where the 12 comes from comes from combining these two parts and that's it for the calculus extra again if you haven't had calculus you shouldn't have watched that and if you have had calculus you see why you've had calculus take care hello welcome to the second lecture of chapter six this one covers the exponential distribution and the previous one covered the uniform distribution the uniform distribution was very good at modeling stuff that had a definite lower and a definite upper bound and was continuous and you knew nothing else about it other than it had a definite lower a definite upper and was a continuous random variable an exponential distribution will be very useful when you've got a continuous random variable that has a definite lower bound of zero and no hard upper bound so it's bounded on one side examples of this would be wait times um so by the end of this lecture you should be able to determine which random variables may follow an exponential distribution specify the characteristics of an exponential distribution compare and contrast exponential with uniform calculate probabilities using the CDF and quantiles using the CDF so here's the characteristics it's a continuous distribution describes a random variable whose probability of occurring decreases with time it's frequently used to model quote time until something occurs when there is no upper bound we say x follows or has a distribution of an exponential the parameter for an exponential is Lambda Lambda is the rate of the thing happening sample space is from 0 to Infinity there is no upper bound there is a lower bound of zero but there's no upper bound expected value is 1 over Lambda the variance is one over Lambda squared which means that the standard deviation and the expected value of the exponential are identical one over Lambda the rate of something happening well if we remember our physics the rate of one divided by the rate is just the frequency so it makes sense that the expected value would be 1 over the rate because it's an average frequency of it happening this is what the exponential distribution looks like Smooth declining the height happens at Lambda when x equals zero the probability density function is Lambda times e to the power of negative Lambda X for X greater than zero and zero otherwise and again it's important probabilities are areas under the density curve notice there's no rectangles here for us to play with so calculating those areas is going to be a bit more difficult than it was in the uniform case let's go into two examples time between when I fill my bird feeder with seed and when chunky the squirrel starts eating from the feeder follows an exponential distribution with an average time of 10 seconds in other words if we Define t as the time in seconds until chunky raised the bird feeder T follows an exponential distribution with Lambda equal to one over ten Lambda is 1 over the mean the mean is one over Lambda Lambda is one over the mean we're given the average time is the mean time is 10 seconds so what's the probability that a weights at most a half minute before chowing down on some awesome seed at most a half minute so we're looking at the probability of T being less than or equal to 30. dark color is what we need to calculate remember again for continuous random variables areas under the PDF curve are the probabilities no rectangles to use can't use the tricks from last lecture are going to have to jump directly to the CDF function I leave it as a proof for those who enjoy calculus to prove that this is indeed the CDF for the exponential distribution what we did with the uniform remember is we created this ourselves for the exponential it's given to us and we use it for the normal which is the next distribution in the next lecture we realize there is no function and we have to use something else so we want to find the probability less than equal to 30 that's just the CDF at 30. Lambda is 0.1 x is 30. here's the CDF function 95 percent probability that chunky will wait no more than 30 seconds to raid the feeder this is a little bit more involved if we got R open might as well use R to do the calculations it's a less than or equal to so this is a p the stem for the exponential is EXP we want to find the probability T is less than or equal to 30. and we're told Lambda is equal to 1 10. we're not calling it Lambda here we're calling it rate so be aware that this has to be rate example two time I wait until the gold express bus comes follows an exponential distribution my average wait time is five minutes in this cold of winter it takes 10 minutes for Forest bite to start so given this what's the probability that I will have frostbite before the bus arrives in other words given that 1 over Lambda is five what's the probability that t is greater than 10. 10. this is not a CDF it has to be in the form of a less than or equal to but using the complements rule we can easily change this into from p is from T greater than 10 to 1 minus probability of T being less than or equal to 10. chapter 4 pops up every once in a while this is probably the most important thing from chapter four the compliments rule so now all you do who is calculate the probability is less than or equal to 10 that's the CDF at 10 given that Lambda is 20. I'm sorry 0.20 1 over 5 is 0.20 plug chug 13.5 percent chance that I get frostbite or it's 1 minus P EXP of 10 rate equals 0.2 again it's p because we're looking at the CDF function it's exp because it's an exponential distribution we can also calculate the median remember the median is the value X tilde such that the CDF at X tilde is one half on the left is the CDF function evaluated X tilde which is just 1 minus E to the negative Lambda X tilde 0.5 stays on the right now what we're doing now is solving for x tilde subtract 1 from both sides and then multiply through by negative one gets us here take the natural log of both sides gets us here divide by negative 0.2 gets us here remember the log of onehalf is negative log of 2. so the median is 3.466 minutes and we can show in general that the median is just mu the mean Times log 2. and this is the natural log of 2. or instead of doing all this calculation and R is just q q because we look at the quantile at 0.5 this will give us the median exp because it's an exponential and the rate is one over five it's one over mu we could also calculate the 90th percentile set the CDF equal to 0.9 and solve for x star or just put 0.9 in the quantile function and solve before we get to the summary let's go ahead and put in the intra lecture questions there's three of them as usual and again as usual write these on the left side of your notes answer them so you can transfer them into Moodle one give an example of a random variable that follows an exponential distribution something other than the two examples that we did today two what is the parameter of an exponential distribution and three what are the mean and standard deviation of an exponential distribution give me the formula for those I did say mean and standard deviation the reason I'm asking this is to set something in your mind so that you can contrast the exponential with the poisson because exponential poisson both use Lambda as the parameter um so here's a summary we reminded ourselves that probabilities area under the PDF curve the CDF function is X probability of X less than or equal to X in this case we needed calculus to find the CDF although the calculus extra is where we prove it I just threw the formula at you exponential distribution describes a white wait time random variable the time until something happens either being a variance of an exponential random variable or one over Lambda and one over Lambda squared respectively meaning that the standard deviation is going to be the square root of the variance in other words the standard deviation is one over Lambda last distribution is the normal distribution normal distribution is the key distribution from chapter six when we get to chapter seven and talk about the central limit theorem I'll drive this home as to why the normal distribution is the most important we cannot there is no formula to calculate the normal distribution probabilities we can either use the table at the end of the book no no no no no or we can use R to do it we're going to look at quantiles Otherwise Known percentiles and we're going to again understand the difference between discrete and continuous random variables here are the r functions again all of them have the stem exp for the expected value and all of them require that you specify what the rate is and the rate here is Lambda D for the density the little f of x p for the cumulative probability the capital f of x r to generate a random value Q for the quantile so the median we'd use the Q exp and then put in 0.5 for the 10th percentile we put in point one for the 99th percentile we put in 0.99 nothing in Hawks about this R for starters it's appendix B5 how Wikipedia's got the exponential distribution in the Wikipedia page you'll see that there's actually two parameterizations for the exponential the one that we're using in class which is Lambda and the one that the engineers tend to use Theta if memory serves me right that's kind of the end again if you are not a calculus person you can stop here if you are a calculus person you want to see the how to actually create that CDF function continue on so here's a proof of the exponential CDF remember the definition of the CDF it's the probability otherwise an area under this under the PDF curve other random variable being less than or equal to X for the exponential that means that you're integrating from zero to that x value F of T is the exponential PDF probability density function DT substitute F of T is just Lambda e to the negative Lambda t this Lambda doesn't depend upon T so it can be pulled out so we're integrating from zero to X of e to the negative Lambda T DT not too difficult the integral of e to the negative Lambda T is just negative one over Lambda times e to the minus Lambda t evaluated from T is zero to X this Lambda and this Lambda cancel so we're left with just a negative e to the negative Lambda t evaluated at X subtract off this thing evaluated at zero here it is evaluated at X subtract off evaluated at zero notice e to the zero is equal to one subtracting off a negative one means you're adding one no simplification the first term so we're left with CDF being one minus E to the negative Lambda X again this is for those who want to see that their calculus time was well spent that it's used a lot in statistics and probability Theory and I'm sure it's used other places but really I don't care it's all about the statistics and probability and how we're using the mathematics so I'm done I have a good one hello and welcome to chapter six from Hawks this lecture will cover check section six one through six four all four of those discuss the normal distribution because we spent some time building up things with uniform distribution the exponential distribution we'll be able to cover the normal distribution in one rather lengthy lecture but still just one lecture by the end of this lecture you should be able to discuss the differences between the uniform exponential and normal distributions know the expected value and the mean of a normally distributed random variable it's kind of tricky because remember the mean and the expected value are the same thing sketch the graph of a normal distribution realize that capital N normal and lowercase and normal mean different things the capital N normal refers to the distribution and lowercase and normal just means typical or not surprising to be able to calculate probabilities normal random variables and quantiles of normal random variables so here's the arc that we've been working on chapter five we looked at discrete distributions which included probability Mass functions pmf calculated mean the variances sample spaces for those discrete distributions the discrete distributions we looked at were the Bernoulli the binomial the poisson and the hypergeometric we also looked at generic discrete distributions in the first section of chapter five and chapter six looked at looks at continuous distributions I introduced the uniform distribution so you get an idea that probabilities for continuous distributions were actually areas under the PDF curve which meant the PDF curve was not probabilities they were probability densities or likelihoods we use the uniform to illustrate how to calculate the CDF the cumulative distribution function using simple high school geometry and then we moved on to look at how to calculate the mean the variance the sample space the median other quantiles on the uniform so that section on the uniform just laid the foundation for all the other important features of continuous distributions the second section was the exponential where we did the same thing but then we realized creating that CDF function that cumulative distribution function was not easy and in that case we had to use calculus to get it or It came it was given to us pop fully formed out of the sea foam your choice so we went from the uniform where we created it ourselves and got a good feel for how it was created to the exponential where we had to use Calculus if we wanted to see how it was created to this lecture it's normal and there is no CDF function which means that in the uniform well you really didn't have to pay attention to the CDF we could calculate those probabilities in our head exponential we need to use the CDF but it was a nice simple form that we could use for the normal we got to go to a table or to a computer and for much of this second half of the 19th century a lot of effort was spent trying to create those tables it was pretty easy in the early 19th century create CDF function for for values of we're going to see Z between negative 1 and positive one that was pretty easy to do once you got outside that negative one to positive one realm it got harder and harder and harder to get good estimates of those probabilities with the Advent of computers it's much easier I mean we could even do it and today we're going to look at the normal probability density function for it the CDF function for it the mean variance sample space median quantiles AKA percentiles same stuff we're going to see the function the PDF function we're going to realize okay we don't need to memorize that we're going to see that we can't write out the CDF function in any meaningful way but we'll find out ways of calculating the gaussian distribution apparently was named or no apparently about it it was named after Carl Friedrich Gauss who advocated for its use AKA created it um or at least we think he created those who stem from the German school of thought think he created it his first paper though was 1809. it was LaPlace who published a paper earlier than that using this distribution that's why the French descendants call this the Gauss LaPlace or just the LaPlace distribution but we're going to call it the normal distribution um gal said hey wait a minute LaPlace I was working on this way back in 1794. no evidence of it except gauss's word normal distribution arises from modeling observed Randomness in astronomical and geodetic data it arises in variables that have a specific expected value but demonstrate some minor random variation above and below so for instance this pop that I'm drinking now says that contains 100 milligrams of caffeine so I would not be surprised if the distribution of caffeine in these bottles followed a normal curve that is the mean would be a hundred and there would be some variation above and below because you can't get exactly 100 milligrams of caffeine every time there are two parameters to the gaussian distribution or to the normal distribution the mean and the standard deviation mu and sigma most important distribution in statistics because of the central limit theorem which we'll see in chapter seven so for the record if x a random variable follows or is distributed according to a normal distribution with mean mu and standard deviation Sigma then the PDF is lowercase f of x is this thing notice that the PDF depends on mu and sigma and some Sigma in a couple places however recall to chapter 3 when we looked at the standardized score we can subtract off mu divided by Sigma and we actually come up with another normal distribution it's a normal distribution that does not depend upon mu and sigma it's a standard normal distribution with mean zero standard deviation of one so the normal distribution has two parameters mu and sigma the standard normal distribution has no parameters and it's the standard normal that is tabulated in books here's a graph of the standard normal PDF notice that it goes off in both sides forever this should raise some memories of the empirical rule um you will see sometimes that the PDF is symbolized with a lowercase fee Greek letter Phi lowercase fee lowercase f because it's a PDF and the CDF will be at uppercase fee I don't think I have an uppercase fee on here here's the mean notice it's also the median normal distribution to symmetric so median median will be the same notice also it's the most likely value because the distribute or the the likelihood function the the PDF is highest there so this is the highest likely or the most likely value from this distribution so the normal distribution the mean median and mode are the same value here are the standard deviations at this point you should be able to say hey what I know what percent of the data or of this distribution is between negative 1 and positive 1. and what percent is that no no no no oh I'm sorry I miss heard you yeah 68 percent and what percent is between negative 2 and positive 2. yep about 95 percent and between negative 3 and positive 3 is 99.7 percent ish call the normal distribution is continuous but it's not rectangular and it can be proven that the CDF cannot be written out hence late 19th century early 20th century as well working on estimating those probabilities so you have to use a table or a computer table one calc tabulates this CDF computer calculations are easier to perform however they're more accurate and they are more precise the stem is Norm you have to specify the mean M and the standard deviation s P for the cumulative probability Q for the quantile r for generating random values pqr is the same as it has been with all the other probability distributions stem is Norm and you have to specify M and S so some examples intelligence quotient by Design the IQ measures in the United States follow the normal distribution with mu equal to 100 and sigma equal to 15. this is by Design This Is How They create this test What proportion of the U.S population has an IQ less than 90. in other words if what Rand variable do I use in other words if this is the distribution of IQs in the United States I want to find the area under the curve less than 90. the purple area I want to calculate the probability that X is less than or equal to 90. give that X follows a normal distribution with mean expected value of 100 and standard deviation of 15. here it is in r P for a cumulative probability Norm is the stem for the normal 90 is the value we're specifying m is equal to 100 and S is equal to 15. this comes out to be about a quarter of the population wait a minute I can hear you saying I thought this P only worked when it was less than or equal to what is going on here are you trying to pull something over on us the answer is no remember that X follows a continuous distribution in this case it's a normal distribution that means that the probability of X equaling 90 exactly is zero think back to the uniform distribution it's an area so it's the base times the height if the base consists of a single point then the width is zero and a width of zero times whatever height is going to give you a probability of zero so for continuous distributions less than or less than or equal to are going to be exactly the same what IQ value separates the lower 30 from the upper 70 percent so we're looking for an IQ value that tells me I'm going to be using a quantile I'm looking for the 30th quantile or the 30th percentile so I'm looking for this value here such that 30 percent is in purple Q for quantile we're looking at the 30th percentile so it's 0.3 here and we specify the mean and standard deviation 92.13399 so approximately 30 percent of Americans have an IQ score less than 92.13399 and approximately 70 percent of Americans that's everyone else have an IQ of more than 92.13399 30 it's pretty frequent still so in other words IQ of 90 and no big deal by Design IQs Etc what's the proportion of Americans that with an IQ score between we haven't dealt with any between calculations yet between 87 and 122. so we want this purple area between 87 and 122. for calculating cumulative probabilities it's got to be in the form of less than or equal twos so I can do less than or equal to 122 but that covers everything here not just what we want but notice like that this region is less than 122 minus less than 87. looking for this this is just the probability commutative probability at the upper minus the cumulative probability at the lower so approximately 75 percent of Americans have an IQ between 87 and 122. that's a lot and now we'll go for above we did a less than between now we'll do an above probability of a proportion of Americans have an IQ above 90. so this blue purple area complements rule this this purple area is just one minus the white area 1 minus the white area so and again about a three quarters of Americans have an IQ above 90. so let's do a learning check I'm going to ask some questions I'll pause you answer them on your own out loud because dogs watching you and you really do want to talk to it what type of random variable will have a normal distribution a random variable with a definite Target and some random variation added to it will tend to have a normal distribution what's the difference between capital and normal and lower end normal capital N normal is refers to the distribution itself lower end normal refers to the fact that something is not unexpected what is the sample space of a normal random variable all real values there is neither a lower bound nor an upper bound what is the expected value of a normal random variable very good that's mu it's one of the parameters what is the variance of a normal random variable yes Sigma squared it's the square of one of the other parameters remember there are two parameters mu and sigma the variance is just the square of Sigma what R function is used to calculate cumulative probability for normal yep cumulative probability so it starts with a P for normal random variables so it's p Norm and what our functions use to calculate the quantiles Q Norm Q for quantiles Norm for normal random variable the future we're going to keep working with the normal distribution simply because of the central limit theorem and we're going to use this normal distribution to estimate population parameters but this this last one will be the second half of the course here are the r functions none too surprising the stem is Norm then we got the DPR and Q notice when we were dealing with the discrete we used the d a lot now that we've moved on continuous we really don't use the D I use the D Norm to create those graphs but really you don't use the D Norm function at all you focus on the P norm and the Q norm and the r Norm here are some readings for you are for starters appendix B3 C1 and C2 and Hawk section six one to six four normal distribution in Wikipedia is a good one oh I haven't done the intra lecture quiz or questions here we go question one give an example of a random variable that follows a normal distribution question two what are the two parameters of a normal distribution and question three what are the mean and standard deviation of a normal distribution three again will be really easy two will be pretty easy one that'll take some thought but it'll be easy oops went too far so I guess I'm done hello and welcome to this last section of chapter six this is where we're going to use the normal distribution to approximate the binomial distribution notice this is rather interesting because the normal distribution is continuous and the binomial distribution is discrete so indeed we can approximate discrete distributions using these the The Continuous distributions so by the end of the selection you're going to be able to describe the normal and the binomial distributions you're going to see how the normal can be used to approximate the binomial you're going to calculate the approximate binomial probabilities and understanding why the binomial is useful even if we have a computer to calculate them exactly um so here's the arc we've examined recently discrete distributions of which the binomial is one and continuous distributions of which the normal is one we looked at probability Mass functions means variances sample spaces for the discrete we've looked at probability density functions means variance of sample spaces the CDF can be applied to both discrete and continuous as can median and quantiles today we're going to approximate one distribution with another so the first thing we have to do is explain what we mean by uh approximating one distribution with another short answer is that the cumulative probabilities are close that is if X1 X2 are different distributions that are approximately the same then this relationship holds for all values of little X of course there is a whole lot of detail hidden in that little tilde that I'm sorry that approximation sign um if X1 is approximately the same as X2 how close is close enough how approximate is necessary it's a question of precision and we leave that up to the scientist I've got the statistician scientist comes to me and say that says I need the probability speed within .003 of each other from that information I can say okay I need a sample size of 1300. or something like that um so we're going to approximate the binomial X1 with the normal X2 that is we'd like to determine some rules on NP mu and sigma to ensure that this relationship this approximation relationship is true now surprisingly it's not as difficult as it seems to get a good first order approximation what do we remember about the binomial we remember these five requirements and again you need to have these memorized the number of Trials is known each trial is a Bernoulli trial success probability is constant trials are independent and the random variable is the number of successes in those trials and this led to expected value and variance of NP and np1 minus p so from this it's next natural for us to see just how well this X2 distribution which is normal how well this approximates the binomial where the mean is and P and the variance is np1 minus p in other words we want to see how far apart those cdfs are so here's the CDF of a binomial 5.1 n is 5 p is 0.1 this is CDF here's the graph of a normal 0.5 and 0.45 so this normally is equating to the rule this will be the X2 distribution and this should illustrate how close they are midway between the numbers one two three four five it's right on or really really close the closer you are to one of the integers the worse off it is in fact here is a animated graphic of the difference in cdfs between the binomial and the normal sample size is up here um this is the value of x and the the height here for any value of x is binomial CDF minus normalcdf notice we have spikes at the integer values now I would love to press play but unfortunately this P this Adobe Acrobat type program isn't Adobe Acrobat so I'm going to have to open up this file separately the sample size is changing notice also the worst is getting smaller and smaller and smaller so here we are at about 70. it's the worst is 0.05 we're coming up on a hundred the worst looks to be about 0.045 let me just keep this running and notice that the error the worst error tends to go to zero as the sample size increases but notice that it doesn't go to zero too quickly here we are at the sample size of about 200 coming up on 200 we're going to see that's only about 0.03 where's the area is 0.03 and we start over again and I don't know how to stop that from continuing 0.03 as the difference between the normal binomial CDF that's 0.03 which is a probability so that's a large difference so if the correct probability is 0.57 then this error could be anywhere from or this would be anywhere from 0.54 to 0.60 and that's a big range big error introduced but as you saw it's a sample size increase that the worst absolute error went to zero so large sample size we can go ahead and say the normal and the binomial are close enough that also kind of leads to the physicist who tells me I need to be within 0.03 0.03 I can come back and say okay sample size needs to be at least 200. or the physicist comes to me and she says I want it to be within .003 I'll come back with okay sample size needs to be at least twenty thousand or whatever so one conclusion is the approximation increases as values of n increase we're going to see that again when we get to the central limit theorem in chapter seven um with additional exploration we could come to a second conclusion that the approximation is better for values of P close to 0.5 the example I gave you P was 0.1 so if p is close to 0.5 we don't need as large as sample size as we do if p p is 0.1 so combining these two observations leads to the following quote rule of thumb and this rule of thumbs tends to change from source to source um the normal distribution is sufficiently close to the binomial if both NP and N times 1 minus P are at least five I happen to think 15 or 20 is a better rule for that and if I need a lot of precision that I'll need NP to be at least 1000 or maybe two thousand and the approximation is improved by using quote a continuity correction of 0.5 added to or subtracted from the number value we'll have an example of that shortly this is not the best cracking but it's a good correction so we're going to use a con continuity correction factor to describe the area under the normal curve that approximates the probability that at least two people in a math class of 50 regularly cheat on their tests assume the number of people in the math class 50 who consistently cheated on their test has a binomial distribution with a mean of five and a standard deviation of 2.12 . so in other words if x is the number of students cheating in the class X follows the binomial of 50 with a p of 0.1 we need to calculate probability that X is greater than 2. so we're going to begin by converting the discrete number 2 into an interval by adding 0.5 and subtracting 0.5 from it I have discreet number two is changed to a continuous interval from point from 1.5 to 2.5 here's a picture of that so for the discrete we could just use two for the continuous because the probability of x equal to is zero just like x equal to any single number we need to change it into an interval so that 2 will be replaced by the interval from 1.5 to 2.5 now we're going to draw a normal curve with the mean of 5 and a standard deviation of 2.12 those were given to us and indicate the interval from 1.5 to 2.5 to represent the number two that's what this is the red curves the normal of mean five standard deviation of 2.12 and I've got the value of 2 in the double shaded or the Shaded and hashed now we're going to shade the area corresponding to the phrase quote at least two because we need to calculate the probability of at least two and that's the blue shaded area it's this hashed part which is 2 and above so in other words if x is binomial and Y is the normal approximation probability of X greater than 2 is about probability of y greater than or equal to 1.5 because it's everything blue if I were calculating the probability of X being less than 2 it would be y less than or equal to 2.5 use a normal distribution to estimate the probability of more than 55 girls being born in 100 births so exactly we need to calculate probability X is greater than 55. given X follows a binomial distribution out of 100 P of 0.5 using a normal approximation that means we're going to define a y variable to follow a normal distribution y follow a normal distribution with the expected value of 50 100 times 0.5 and variance of 100 times 0.5 times 1 minus 0.5 and we're going to calculate the probability that Y is greater than 40 uh I'm sorry that Y is greater than 54.5 there we go now we could calculate the exact answer of 13 percent 13 5 6 2 65. notice how close the approximation is though this is using the p binom and this is using the P Norm very close and the sample size is only a hundred I say it's very close we're off on the fourth digit fifth digit so if I need Precision beyond the fifth digit then I'm going to have to have a larger sample size if I did not use the continuity correction I'd be much further off after many hours of studying you believe you have a 90 or 90 probability of answering any given question correctly test is 50 true false questions oh that would be horrible if I gave you 50 true false questions for the test or would it now I wouldn't do that assuming that your estimate is the true probability that you will answer a question correctly let's estimate the probability that you'll miss no more than four questions so each of the 50 true false has a probability of 90 being right and we want to say missing no more than four questions so the probability of missing will be 0.1 50 questions this is the exact distribution of X this is the exact probability to calculate we can approximate this binomial with a normal y will follow a normal distribution of expected value 50 times 0.1 and variance of 50 times 0.1 times 0.9 and we'd calculate the probability that Y is less than or equal to 4.5 it's too far 40 percent the real answer is 43 percent so I'm off by two and a half percent many toothpaste commercials advertised at three out of four dentists recommend their brand of toothpaste using a normal distribution to estimate the probability that in a random survey of 400 dentists so we'll be using random sampling simple random sampling exactly 300 will recommend Brand X toothpaste we're going to assume the commercials are correct and therefore there's a 75 percent chance that any given dentist will recommend Brand X so if x is the number in that sample of 400 x follows exactly a binomial and a 400 P of 0.75 and we need to calculate exactly x equals 300. the approximation y will follow the normal distribution expected value of 400 times 0.75 variance of 400 times 0.75 times 0.25 and we need to calculate the probability that Y is between 299.5 and 300.5 and we get 0.046 0403 and the real answer is 0.046 O2 432 so the normal distribution with the continuity correction did a very good job here as well without the continuity correction this would be terrible in fact without the continuity correction the probability estimated would be zero because you'd be calculating the probability that y was equal to 300. so learning check what's meant by the normal distribution approximates the binomial correct their cdfs are quote close what are two requirements for the approximation to be quote good right n times p is at least five and N times 1 minus p is at least five again I would recommend higher numbers for those if I really really care about the results how do we make the approximation better yes larger sample size you'll find in statistics at a larger sample size solves a whole lot of problems from the economic standpoint however larger sample size causes problems because it costs time money other resources to collect that larger sample size what is the continuity correction why is it used very good continuity correction is used to help ensure that the normal approximation to the binomial is quote better continuity correction itself is to treat an equals as adding 0.5 and subtracting 0.5 and calculating that interval in the future we're going to explore the normal integrator detail we're going to use the central limit theorem which will be chapter 7 and discover that the binomial is not special the normal distribution is um here's a question that you may have we have a computer we can easily calculate these binomial probabilities exactly why do we still need to approximate a binomial with the normal and here here's the answer the examples that we're given today we're given where you could calculate exactly and approximately and compared just so you get a feel for how how good this approximation is when we get to trying to estimate a single proportion we're going to stick with the exact way of doing it using the binomial distribution and everything we know about the binomial however when we start to look at comparing two proportions or trying to estimate the difference between two proportions to population proportions we won't be able to use an exact distribution because we would need to find the distribution of the sum of two binomials and that doesn't exist however we do know the distribution of the sum of two normals that's the normal distribution so we would use we would have to use the normal approximation when we start comparing two proportions until we get there we can use the exact form now with that said some books and I think Hawks does this Hawks will use the normal approximation for even the one population parameter s population proportion estimating so there is reason to look at this approximately in the binomial not just it's the next section in the chapter here are the r functions we've seen these already d by Nom P by Nom and P Norm here's some readings appendix C and R for starters and Hawks section six five and then Wikipedia Central limit theorem appendix C and the central limit theorem and Wikipedia will give you a good background for chapter seven and this approximating the normal approximating the binomial distribution with the normal is actually a result of the central limit theorem so questions where are those questions there they go question one again I recommend writing these questions in the left hand side of your notes answering them below so you can transform the trans put them into Moodle quiz what does it mean for one distribution to approximate another question two what are the mean and the standard deviation of a binomial question three what are the mean and standard deviation of a normal distribution and that's it take care hello and welcome to section 7.1 where we introduce the central limit theorem Central limit theorem I would argue is the most important theorem in all of introductory statistics in fact I'd argue it's the most important theorem in all of Statistics it helps to explain that when we have data that comes from a continuous distribution and we're trying to estimate the population mean we don't really care that much about the distribution of the data we just quote pretend that the data comes from a normal distribution because the central limit theorem says Hey regardless of where the data comes from the sample means follow a normal distribution and it's the sample means that we're going to use to understand our population mean in the future so from this point forward when we're dealing with continuous data the sample means are going to follow a normal distribution or it's going to look like a normal distribution and we see that in lab C so by the end of this lecture you're going to be able to State the central limit theorem say the requirements for applying it and state its consequences so in recent Computing activities we've examined drawing random samples from a known distribution we've looked at calculating sample means from subsets of those distributions we've created histograms of the distributions of those sample means today we're going to examine the central limit theorem which kind of gives a mathematical explanation for everything that we've observed about those sample mean distributions so here's the statement of the central limit theorem let X be a random variable with a mean we're going to call that mean mu and finite variance Sigma squared and we're going to draw a random sample of size and from this distribution so that first paragraph is all about the data itself the data comes from some distribution with a mean and a variance second paragraph then the distribution of the sample sums converges to a normal distribution as n gets larger so the first paragraph said we don't care about the distribution of the data second paragraph says therefore the distribution of the sample sums converges to a normal distribution so the data could be exponential the data could be uniform the data could be binomial for All We Care it's the sample sums that are going to be normally distributed as n gets larger specifically this thing on the left is just how we represent sample sums we're adding up this is a big summation adding up over all n values in our sample those values it converges in distribution that's the D on top of the arrow means it converges in distribution to a normal with the expected value of n times mu and a variance of n times Sigma squared we've actually kind of seen something like this in the past but let's keep working forward the proof of this theorem is beyond the scope of this course I think the first time you get to see it is in math 321 happens at the end of math 321 after you've learned about these things called moment generating functions um so our first intra lecture question is here again on the left hand side of your notes write the question write the answers so that you can come back to it later remember you got the pause button so there are three main consequences um Central limit theorem tells us the following one it tells us that the sum of independent random variables is more normal than the distribution the variable itself of course if the data do come from a normal distribution you already start at normal so you don't have any place to go but if you start at uniform it takes a little bit to get to normal if you start exponential it takes a little bit longer to get to normal um recall that the binomial is the sum of independent or newly random variables we use that fact to prove things about the binomial specifically the expected value and the and the select the value in the variance poisson happens to be the sum of independent poissons we've alluded to that in the past but that means that as n increases the binomial becomes more normal and as Lambda increases the poisson becomes more normal it's because the poisson is just the sum of independent poissons and binomial is the sum of independent bernoullis kind of powerful right there a second consequence is because the sample mean is just the sample sum divided by constant the central limit theorem tells us that the distribution of sample means will tend towards normal and again if the data starts with the normal this tinting towards happens immediately if it starts with the uniform it takes like 10 to get there if it's an exponential it takes like 50 or so but eventually those sample means will become as close to normally distributed as if you want them to be you just have to get a larger n sometimes and the third the speed of convergence depends on how close the data are distributed to normal the closer they are to normal the faster thus the normal if the data actually do come from a normal distribution then the sample means are immediately normal the poisson is closer to normal than the since many of the binomials so you'd need a Lambda smaller than the and for binomial so poissons converge faster let's look at the next introduction question it's going to ask you about the uniform and the exponential oh no it won't well it will pretty soon so question two is State the second consequence of the central limit theorem and then I'm going to pause then I'm going to go directly to question three which talks about the third consequence since the speed of convergence depends on how closely the data are to normal as the missing L which of these two will converge the fastest if the data follows a uniform well that converge well the sample means converge faster to the normal than if the data follow an exponential remember to write down the question on the left answer it so that you can transfer it into Moodle um again the data follower uniform well the sample means converge to normal faster than if the data Fallen exponential pause moving back um so here's one of the big uses that is used sometimes let X follow a binomial distribution remember there are two parameters for binomial n and p we're going to use the central limit theorem to approximate the distribution of X we actually did this back in section I believe it was section six five but here we're going to see why it actually works or we're going to apply a mathematical reasoning for why it works remember the binomial is just the sum of N independent bernoullis that is if y sub I follows that Bernoulli then X being the sum of those y's follows a binomial of NP so according to the central limit theorem since it's the sum of independent distributions the x is going to be approximately normal with a mean and a variance that mean is just going to be the mean of this binomial well what's the mean of a binomial it's n times p and the variance is just going to be the variance of this binomial well what's variance of a binomial n times P times 1 minus p so this gets us directly to the approximation of the binomial with the normal in about one step more importantly for us okay never mind um we're going to let the X for another example follow a uniform a b a is the minimum value B is the maximum value we're going to use the central limit theorem to estimate the distribution the mean of a sample of size n remember that X bar the mean is just one over n times the sample total and we represent the sample total this way and so by the central limit theorem T follows approximately the dot on top of the tilde means approximately follows a normal distribution n times mu and N times Sigma squared where mu is the mean of the underlying distribution of the X distribution and sigma squared is the variance of that underlying distribution of the X distribution from our knowledge of the uniform we know that the mean of the uniform is just a plus b over 2 and the variance is B minus a squared over 2. so the sample sum is approximately normally distributed with expected value n times a plus b over 2 and variance of n times B minus a squared over two over twelve remember the question was originally about this distribution the sample mean and X bar is just one over n times t that means the that X bar approximately again dot on top means approximately follows a normal distribution expected value of mu n times mu divided by n and N times Sigma squared divided by N squared so the expected value of x bar is a plus b over two which happens to be mu and the variance of X bar is B minus a squared over 12n now notice something that we didn't notice before the expected value of x bar is Mu the variance of X bar is smaller than the variance of x so as n increases the variance decreases to zero which means these X bars that we actually measure from a sample The observed X bars are going to tend to be closer to the MU so in other words as n increases the Precision of our little X bar increases so the big question that comes in here is why is there an n in the denominator of the variance recall that the variance is find as the sum of x minus mu squared times the probability of that x value so the variance of a times x is just the sum of a times x minus a times mu squared times the probability of the x value factor out an a from both of these out front that gives us an a squared times x minus mu squared times the probability of the x value this is just a constant has nothing to do with the I pull it out so at a squared times the sum of the x minus mu squared times P of x so the variance of ax is just a squared times the variance of x in other words where does that squaring come from comes from the fact that the random variable itself is squared in a variance so when we found the variance of X bar that was just the variance of one over n times t pull the 1 over n out front it's 1 over N squared times the variance of t there's one over N squared this n times B minus a squared over 12 that's the variance of t the n on top cancels one of the ends of the bottom that gets us our variance of X bar so a learning check what is the main consequence for the central limit theorem I'll just pause for a little bit you can go ahead and hit the pause button to give yourself some more time the main consequence for the central limit theorem is that the sample means will become more and more normal what are the requirements for the central limit theorem the expected value and variance both have to be finite and the X's have to be independent why does this mean we should focus on the normal distribution as we move forward in the course when we care to estimate the population mean we're going to use the sample mean and since Central limit theorem tells us the sample means are going to eventually follow the normal distribution as long as n is large enough then we should just focus on this normal distribution R for starters this is appendix C3 Hawks this is section 7 1 um demov LaPlace theorem is interesting of course Wikipedia's got a really good entry on the central limit theorem and that's the N for Section seven one um the central limit theorem and by the way you will see the central limit theorem at least in the next two lectures but in undergirding all lectures from this point forward in this course so enjoy hello and welcome to section 7 2 last section we looked at the central limit theorem here we're going to be applying the central limit theorem to the distribution of sample means um so by the end of this lecture you should be able to State the central limit theorem and to apply the central limit theorem to the problem of sampling distribution of the mean or what most books just refer to as the sampling distribution so here's the arc the way what we've been talking about so recently we've examined drawing random samples from a known distribution we've calculated sample means from subsets to those distributions we've created histograms of those distributions we've seen that those histograms tell us that the sample mean distributions look pretty darn normal in the last slide deck we introduced the center limit theorem that says yes they do look pretty darn normal and that's not that's not an accident we've looked at Central limit theorem and its requirements and its consequences and the main consequence is the distribution the sample means tends towards normal as the sample size increases So today we're going to examine the center limit theorem in terms of what it tells us specifically about the distribution the sample mean in the next lecture it'll be about the distribution the sample proportion but if you think back to section 7 1 in fact if you think back to when we first introduced the binomial distribution we realize that the sample proportion and the sample mean are not that different at all mind blown so here's the statement of the central limit theorem again let X be a random variable with a finite mean and finite variance let us draw a random sample of size n from this distribution remember random sample means it's an independent sample each of those X values are independent of the others and again note that that first paragraph talks about the distribution of the data so the data have a mean and a variance and the data were generated from some sort of independent process that's all we need from the data the data is the prerequisite to the central limit theorem so then and here this part is the consequence of the central limit theorem the distribution of the sample sums converges to a normal distribution notice the only thing we need from the data from paragraph one is that it has a mean of variance in this from a random sample that tells us paragraph two that the distribution of the sample sums converges to a normal distribution so here's our first intra lecture question State the central limit theorem as I have stated it in this lecture that question is pretty familiar to us again write the question on the left hand side answer it on the left hand side of your notes therefore when you get to Moodle you can write it in nice and easily here's a corollary of it it's the distribution the sample mean so we're going to start with X being a random variable mean mu finite variant Sigma squared draw random sample size n from this distribution in other words that first paragraph notice is exactly the same as the first paragraph of the central limit theorem therefore those tell us that we can now apply the central limit theorem consequence of this is the distribution the sample mean is approximately normal here's some notation X bar is the sample mean sub n we're going to index it by n to indicate the sample size this converges in distribution to a normal expected value of mu variance of one over n times the variance of the original data here's the proof from the central limit theorem we know this is true we get this from that first paragraph this first paragraph same as in the central limit theorem so this is the consequence that first paragraph boom now we wonder about the distribution of X bar well X bar so that is just one over n times the sum of those X I's okay look at this sum of the x i is what distribution does that have normal boom expected value of some of the X sizes n mu expected value of one over n times the sum of the X I's is Mu the variance of the sum of the X I's is n Sigma squared the variance of one over n times those X Out of the sum of the X I's is one over n times Sigma squared why are we dividing by n instead of having just Sigma squared well set proof one this is y the expected value of x bar is Mu I'll find the expected value of x bar sub n substitutes pull out because expected value is a linear operation expected value of ax is a times the expected value of x in this case a is at one over n we know the expected value of T is just n mu the ends cancel that was easy set proof two with the variances if we want to find the variance of X bars of n substitutes 1 over n times t pull out that one over n it's 1 over N squared why is it now squared look back to the lecture on Section 7 1. because there's a square in the calculation of the variance of the random variable variance of T is just n Sigma squared that n and one of these cancel out so we're left with one over n times Sigma squared and that brings us to the conclusion of the Corollary and this is exactly what we just showed so let's have a couple intra lecture questions two how does the distribution the sample mean depend on the distribution of the data and might as well do question three now while we're at it since the speed of convergence depends on how closely the data are to normal I got the L in there finally I edited it which of these two will converge fastest in the uniform or the exponential yep question one and three are from the last slide deck question two it's kind of the Q is the most important part of this slide deck so let's go back to some examples example one let's say I draw a sample of size 14 from a population or that has a mean of 126 and a variance of 42 what's the approximate distribution the sample means notice I didn't tell you that X followed a normal distribution I didn't tell you X followed a uniform distribution I didn't tell you X followed an exponential distribution it doesn't matter I know X has a finite variance therefore I can apply the central limit theorem whether or not n equals 14 is large enough for this approximation to be good that's another question all I want is an approximate distribution the sample means so this is a straightforward application of the main corollary from this section approximate distribution the sample means X bar is approximately normally distributed with expected value mu of X bar being 126 the same as the expected value of the original distribution and variance of being one over n times 42. 42 is the original variance or I'm sorry the variance of the original distribution it's 1 over n times that Sigma squared 42 over 14 is 3 so X bar approximately normal mean of 126 variance of 3. example two I've been told that the average adult height for males in the United States has mean of 69 inches in standard deviation of 3 what's the probability of having the mean of a sample of size 2 being less than 65 inches so in the last example straightforward application we didn't do anything with the distribution of X bar here we're going to do something with that X bar we're going to ask what's the probability of observing an X bar being less than 65 inches um so we're asked ultimately what is the probability of having the mean being less than 65 we're asked to calculate this now if you want to do a sub 2 here that would be fantastic so we're asked to calculate the probability of X bar sub 2 being less than 65. that means we need to know the distribution of X bar sub 2. we have we're trying to solve a probability statement about X bar sub 2 we really need to know the distribution of X bar sub 2. so to calculate this we're going to use the central limit theorem X bar sub 2 is just is approximately normal expected value the same as the original distribution and variance equal to one over n times the original variance notice I told you the standard deviation was three therefore the variance is nine so we know X bar sub 2 approximately normal mean of 69 variance of 4.5 now we use that to answer our original question about the probability of X bar being less than 65. using R this is p Norm remember it's got to be a less than part 65 is from here m is equal to 69 s which for R needs to be the standard deviation that's just the square root of the variance the variance is 3 squared over two so this our code will get us 0.0297 in other words there's about a three percent chance that I will observe at average being less than 65. given that the mean of the population is 69 and the variance of the population is 9. in other words this property this this probability is small and therefore my beliefs about the original population are unlikely that's the probability of observing this event given our assumptions are correct is quite small so either I did not observe this event or my assumptions are not true so example one straightforward application of the central limit theorem example two okay now that we've got the distribution of X bar let's see how we can use this okay now my sample is size 10. same mean and variance of the adult height for males um but now I'm doing a sample of size 10. I want to know what's the probability of that sample of size 10 being less than 65. same statement it's probability of X bar being less than 65 except in this case n is now 10. so from the central limit theorem X bar follows an approximately normal distribution with mean of 69. from the original distribution and variance of one over n times the original variance and the original variance is 3 squared so X bar now follows this distribution notice the expected value the same the variance is smaller so as the sample size increases the variance of X bar gets smaller so again I want to calculate probability of X bar being less than 65 this is p Norm of 65. mu m is equal to 69 s standard deviation is just the square root of the variance of 0.9 this is .000124 so again very small probability meaning if everything I said is true then the probability of me observing this is small incredibly small it's one and ten to the negative fifth it's one in ten thousand since the probability of me observing this is so small I no longer really believe all of my assumptions maybe mu for the population isn't 69. maybe the variance of the population isn't nine maybe the distribution of the population is so nonnormal that the central limit theorem is not helping when n is equal to 10. but still there is a one in ten thousand chance that my assumptions are correct and I observe this event so it is possible but it really draws brings into really makes you doubt the assumptions fourth example crime uh the 2000 crime rate for the 50 states plus DC are given in the data file crime we need to find a 95 confidence Central confidence interval for the mean violent crime rate this is not the first time you've seen the word confidence interval um that would be one of the labs so here we're asked to calculate the 2.5th and the 97.5 percentiles of the sample means drawn from the 2000 violent crime rate why is it the 2.5th and 97.5th note the difference is 95 and no she got 2.5 percent below and 2.5 percent above so that gives us the central part so those two quantiles will give us the central the endpoints of that Central 95 confidence interval it's a confidence interval so it's going to be an interval on the means the confidence interval so it's going to be an interval on the means probably want to put a star there some googly eyes staring at it and big expression of wow one way of estimating this confidence interval is to play the corollary to a central limit theorem from the data we've got a mean of 441.55 and a standard deviation of 241.45 thus by the corollary Central limit theorem we have X bar is approximately normal expected value of 441.55 and variance of 1 over n times the variance of the data itself and so the endpoints for the 95 confidence interval of the 2.5 the 97.5 quantiles of that distribution which in R we can do this way let's look at all the parts that's a q q Norm is for the quantiles of a normal distribution these are the two percents that you want or the two proportions you want the 2.5th percentile and the 97.5 percentile got a put them together in a c function followed by a comma followed by how you're defining this distribution mean of 441.55 standard deviation of 241.45 divided by the square root of 51. I could have written this as the square root of 241.45 squared over 51 but the square root of a square you're just pulling that 241 through 45 out front so in other words this is really just s over square root of n so we're 95 confident that the actual mean is between 375 and 508. and that's finite crimes per 100 000. confidence interval is on the mean um review back what an observation interval is that's on all the observations you've seen recall back with a prediction interval it's on all the observations you will see in the future confidence interval is about the means the sample means we could also in this part is bootstrapping but also estimate the confidence interval from the data itself using a process that's called bootstrapping here's the code to do it and this will give us a 95 confidence interval from 380 to 510. I'm going to go through the line to this code very carefully shortly but notice that the confidence interval you get through bootstrapping is 385 10 but what we got from using a normal approximation was 375 to 508. in other words not too different Okay so five lines six lines here are the two key lines they're inside the for Loop actually this line is key and this line is key so let's look at the x equals line we're drawing a sample for the violent crime rate in 2000. it's going to be a random sample that's what the sample command does draws it and allows you and it does replace equals true which means it allows you to draw a state's crime rate more than once if replace equals true isn't there then this is just going to give you the same 51 values in a different order each time but the same 51 values and if it's the same 51 values the mean of those values will be the exactly the same each time so we need to replace equals true to do bootstrapping so this x equals line will draw a random sample from the violent crime rate in 2000 allowing for states to be chosen multiple times stores it in the variable X second line second important line we calculate the mean of that sample we're going to store it in the variable called MN MN is for mean but we're going to store it in this variable bracket I bracket that is done to allow us to remember or to allow R to remember each of those sample means so x equals that gives us the sample this line gives us the sample mean of that sample storing it in the variable m n in the ith position and bracket I bracket will mean that this is stored in the ith position we're going to do that 10 000 times that's what the for Loop does first time through I is equal to 1 because 1 colon 1e4 is one two three four all the way up to ten thousand first time through I is equal to 1 draws a sample calculates the mean of that Sample Stores it in the variable MN in the first position closing brace goes back up here I is now equal to 2. draws another sample calculates the mean of that stores in the variable M and in position two closing brace goes back up here I is now equal to three draws a ram sample calculates the mean stories in position three this is continued ten thousand times so at the end of this at the end of this loop at the end of these four lines the variable m n will con contain 10 000 values each of those ten thousand values will be a sample beam from this distribution the bottom line gives us the endpoints of that 95 confidence interval remember this is sample means so an interval on Sample means will be the confidence interval quantile function you give it the vector first and then you specify it's the percentiles you want so we've explained every single line there except the first in R when you're building a vector such as the way that we're doing it here you need to tell R to set aside memory for that vector this line tells our hey we're going to create the vector MN it's going to be a numeric Vector it's going to hold numbers and you just need to set aside memory so that we can build it and then this Loop will build it and then at the end we're going to use that MN I strongly suggest at this point you type this in you run it and you see that it actually does work your confidence interval will be slightly different why will your confidence interval be slightly different or why could it be slightly different because this is a random sample we're drawing a random sample so your random sample will be different from my random sample the fact that we're doing this ten thousand times means that the interval endpoints are probably going to be really really close if you want to make them even closer we do this a million times one E6 so here's the question why is there a difference between the two confidence intervals doing it bootstrap it's three five ten doing it using the normal approximations 375508 the reason it's different is here we're assuming that the sample means follow normal distribution and here we're just we're not making that assumption here we're saying okay data I don't care what distribution you have I don't care what the distribution of the sample means actually is I want to see the I want to have a good estimate for that confidence interval since the endpoints are so close to each other that tells me that this normal approximation is not a bad approximation when the sample size is 51. if the sample size were 5000 I would expect the this interval to be the same as this interval if the sample size were 5 I would not be surprised if this interval and this interval were very different they give you all of them I did give you all of them okay so learning check what's the main consequence of the central limit theorem you can hit pause come up with an answer and then I'll give you the answer the distribution of the sample means is approximately normal what is the sampling distribution for the mean sampling distribution for the mean is normal with expected value of mu and variance of Sigma squared over n how does the sampling distribution for the mean depend on the distribution of the data if the data are normal then the sampling distribution for the mean is exactly normal if it is not normal then the farther the data distribution from normal is the larger the sample size needs to be for this to be a good approximation what is bootstrapping and what is its greatest use bootstrapping is using the data under repeated sampling to estimate a population parameter and I would argue its greatest use is to estimate the confidence interval for the mean section 72 in Hawks appendix C3 and of course Central limit theorem you may want to also look at Wikipedia bootstrapping if you've got questions on bootstrapping and that's it see you in section 73 hello and welcome to the last section of chapter seven chapter seven covered the central limit theorem or covers the central limit theorem of two of its most important applications section seven one you were introduced the central limit theorem 72 you saw its application to sample means in this section you're going to see it's uh its application to sample proportions so by the end of this lecture you should be able to State the central limit theorem apply the central limit theorem to the problem of the sampling distribution of the proportion estimate the sampling distribution for the proportion and understand the relationship between the sample size and the Precision of the estimate we've been hinting at facts that there is a a strong relationship between sample size and precision for a long time here we're going to actually see it in the form or in the guise something we call power here's the Arc in recent computer activities the scas we've examined random draws a calculating sample means from subsets histograms from those we've seen that those histograms illustrate that the sample means look pretty normal and then we looked at the central limit theorem and saw oh that makes sense because the central limit theorem says sample means are going to tend to be normal especially if the sample size is large and the last slide deck we saw this application to the sample mean so boom today we're going to examine the central limit theorem in terms of what it tells us about the distribution the sample proportion aka the sampling distribution of the proportion so last time it was the mean this time it's the proportion so here's the the central limit theorem again the first paragraph is the premise these are things that must be met before you can apply the center limit theorem notice that the premises premises premises speak about the distribution of the data the consequence of the central limit theorem the second paragraph talks about the distribution of the sample totals so let X be a random variable with being mu finite variance Sigma squared let us draw a random sample in other words all the X's are independent of size n from this distribution notice we didn't specify that the data have to be normal or exponential or uniform or Gamma or beta or whatever we just said it has to have a mean and finite variance and the data have to be random draw from this distribution consequence then the distribution the sample sums converges to a normal distribution specifically T which is the sum of the X I's converges in distribution to a normal with expected value and mu and variance n Sigma squared which brings us to our first intralecture question State the central limit theorem as I have stated it in this lecture so again write this over on the left hand side answer it below this is really important because a lot of students in the past have thought Central limit theorem says the data become more normal that's not true the data central limit theorem doesn't tell us anything about the distribution of the data it requires that the data has a finite variance and a mean but it doesn't say therefore the distribution of the data becomes more normal no it says the distribution of the sums becomes more normal um there we are so there's the theorem statement here's the corollary for the sample proportion we saw one of these with the sample mean we're going to see it for the sample proportion let X follow a binomial distribution with parameter values n and p remember N is a sample size and P is the probability of success on each of the trials that the X be a random sample from n Bernoulli random variables notice again remember back to 7 1 I framed the binomial as being just the sum of independent Bernoulli's we're using that here then the distribution of the sample proportion which I'm going to call big p is which is defined as 1 over n times x the number of successes from that binomial or the number of successes from those and Bernoulli's so this is 1 over n times x I've seen one over n times x before I forget what that was converges in distribution to a normal with expected value Little P hmm and variance of P times 1 minus p over n so here we are for interlection question number two if I can find it what is the distribution of the sample proportion this is an important one also while you're here look at how this distribution compares to the distribution of the sample mean going back so here's the proof from the central limit theorem we know X follows in proximate normal distribution expected value NP variance np1 minus p that's straight out application the central limit theorem and P is the expected value of x and P1 minus p is the variance of x remember we want to find the distribution or the approximation distribution of P the sample proportions so expected value of P which is expected value of x over n pulling out that one over n because expectation is a linear operator we get expected value of x over n we know the expected value of x from a previous slide is just NP the ends cancel out we're left with p so the expected value of p is Little P so the expected value of your sample proportions is your population proportion normal distribution has an expected value and a variance so let's calculate the variance of your sample proportions again for step substitution we're going to factor out that n as an N squared check back to section seven one why we're factoring out an N squared has to do with the fact that the variance you square the random variable variance of X is np1 minus p this n and one of these cancels out P times 1 minus p over n therefore sample proportions are approximately normally distributed expected value of P variance of P times 1 minus p over n notice the expected value of p is Little P since the expected value of p is Little P this is an unbiased estimator and notice that as the sample size increases the variance also I'm sorry sample size increases the variance decreases in other words as n increases the Precision will increase as well the variance decreases Precision increases so there's our result some examples according to the U.S census 18 of Americans are below the poverty line if I randomly sample 10 people from the United States what's the probability that more than 20 of them are below the poverty line so notice I'm asking a probability question it's what is the probability that more than 20 percent are below the poverty line so I'm asked probability of P being greater than 20. I'm sorry 0.20 since we're doing a pro a uh trying to calculate a probability about the random variable P we need to know the distribution of P from the corollary to the central limit theorem that we covered today P approximate is approximately normal expected value of 0.18 because that's me that's p in the population and variance of P times 1 minus p over n so P big p is approximately normal mean at 0.18 variance to 0.01476 I need to calculate the probability that P is greater than 0.2 remember in order to use cumulative probabilities this has to be a less less than complements rule from chapter four says probability P being greater than something is one minus the probability of P being less than or equal to it we've got a less than or equal here so we can use the P Norm 0.2 0.18 and this is standard deviation so it's the square root of the variance that gives us a probability of 0.4346 that's not small not at all so it wouldn't shock me if more than 20 of my sample is below the poverty line assuming that the the population me uh population proportion is 0.18 then the probability of me observing more than 20 percent being under the poverty line of is 0.43 doesn't give me any evidence against my assumption of the 0.18 it's a coin flip essentially about half the time I'll be above 20 percent about half the time I went below same assumption of the population proportion is 18 percent I'm now going to sample a hundred people instead of what I did in the previous example increasing the sample size and I want to calculate the same probability probability more than 20 or below the poverty line so again we're asked to calculate the probability that P is greater than 0.2 we need to know the distribution of P from the central limit theorem it's 18 18 times 1 minus 18 over n and here is a hundred this gives us that distribution I need to calculate the probability P being being greater than 0.2 it's one minus probability of P being less than or equal to 0.2 and I get a probability of 0.3 so also not a small value it's about a third of the time I'll get a a sample proportion being greater than 20 percent if our assumption about the population is true that is if the population poverty rate indeed is 0.18 notice the probability did go down though the previous example when n was 10 this was 0.46 4.47 it's gone down it's 0.30 which makes sense because I collect more and more data I would expect my observations to average out being closer to the real average now my sample size is a thousand I'm going to ask a thousand people the first it was 10 then 100 now a thousand we're going to look at a 95 confidence interval for the sample proportion confidence interval is on those sample statistics not on observations that we've seen which is an observation interval not on observations we're going to see in the future which would be a prediction interval but on the sample statistic in this case sample proportion so with a thousand I would expect to see 95 of the time somewhere between 0.156 and 0.204 . so if reality is correct I'm sorry if my assumption about reality if 18 is correct and I collect a sample of size a thousand I expect 95 of the time the sample proportion is going to be between 15 and 20 percent how to get that I've got the proportions normally distributed so I'll use the norm stem I'm looking at the quantiles so I'll use Q Norm I'm doing the two point fifth percentile the 97.5 percentile and the rest just specifies that particular normal distribution now I'm going to ask a hundred thousand Americans and I want to know the probability that more than 20 of them are below the poverty line so I did ten hundred thousand now a hundred thousand and my confidence interval is 17.8 to 18.2 so I would expect 95 of the time my sample of a hundred thousand to be between 17.8 and 18 point got the two and the eight switched between 17.2 and 18.8 so if I fell out of the interval of 17.2 to 18.8 I'd say well there's something wrong with my assumption of 18 percent here's a graph of the probability that the observed sample proportion is greater than 0.2 as graphed against the sample size our first example is 10 n equals 10 our probability of observing it was way up here at about 0.46 I think the second n was a hundred and the probability we got was about 0.30 so as the sample size increases the probability of observing a extreme event decreases doesn't go away I mean here we are with n equal to 1000 we've still got a probability of about 0.08 so about eight percent of the time that sample of a thousand will give me a sample proportion greater than 0.2 we can also look at this in terms of confidence intervals as the sample size increases the upper confidence bound and the lower confidence bound get closer and closer together they never become the same even out here at n equal to a thousand there's still a sizeable gap between the two so this is where power comes in power is the ability to distinguish between a true and a false null hypothesis in other words we are assuming that the sample I'm sorry we're assuming that the population proportion is 0.18 power is the ability to say no it's not given our observations so if we observe P being greater than 0.2 the probability of us saying no that 0.18 is wrong gets bigger and bigger as sample size increases in terms of confidence intervals remember the width of the confidence interval is the Precision the smaller the width the higher the Precision as the sample size increases our Precision increases most of the benefit happens in the first couple hundred what this means is as the sample size increases we're going to observe sample proportions closer and closer to our population proportion we still get some above and some below but there will be much more concentrated around the sample proportion which brings us to intralecture question number three what is power when we get to chapter 10 we'll have a much better understanding of power but you gotta dip your toes in here so learning check what's the main consequence for the central limit theorem the main consequence for the central limit theorem is that that means a sample means and Sample proportions are much more normally distributed than the data itself what is sampling distribution for the proportion the sampling distribution proportion capital P is approximately normal with expected value of Little P and variance of P times 1 minus p over n would affect the sample size have for precision and estimating the sample proportion as the sample size increases the Precision increases as well that was section three you may also want to glance through appendix C3 and Central limit theorem you may also want to look at Power in statistics on Wikipedia and that's it for chapter seven in chapter 8 we're going to use all of this Central limit theorem stuff to start creating confidence intervals and in chapter 10 we're going to look at hypothesis testing um but in all reality we understand what confidence intervals are at this point but when we get to chapter eight we're going to get a much deeper appreciation for confidence intervals and that's it hello and welcome to the video lecture on chapter 8 where we introduce the theory of confidence intervals and how to calculate them by hand which we should never want to do because calculating things by hand introduces errors in many many places it's much better to learn how to do these with a computer which will be the next lecture learning how to do confidence intervals on a computer specifically using the r statistical environment um I guess we get to start by the end of this lecture you should be able to State what a confidence interval concerns what it is State the theory behind calculating those confidence intervals and understand that they represent a proportion not a probability but a proportion um so here's the entire Arc of the course or the story of the course thus far the second examination would have taken place right last week if we had a typical course so before that back when we were talking about probabilities we calculated three types of intervals uh We've calculated the observation intervals the confidence intervals and the prediction intervals and these three were actually intervals about different things even though it's all based on the data they all were intervals about different things the observation interval is intervals on observations that we have already had prediction interval is on observations that we will have in the future so a 95 prediction or role tells us that 95 in the future we will be within this range whereas a 95 observation interval tells us that of everything that we've observed so far 95 of those observations are in this range now contrast both of those with the confidence interval recall that the confidence interval was about the population I'm sorry about the the sample statistic so we had confidence intervals looking at the sample mean or confidence intervals looking at the sample proportion or confidence intervals looking at the sample variance and chapter 8 onward we're going to be using these confidence intervals in a much more mathematically balanced way um laying out the theory today we also examine the central limit theorem when we looked at the normal distribution a lot and I when we talked about the central limit theorem I said this tells us this this theorem is so important that it tells us that we can just focus on the normal distribution for a good first approximation anytime in the future so that's why the normal distribution is so important it's also why the central limit theorem is so important and we're going to use both of those today to create these this mathematical confidence interval um so today we'll look at confidence intervals that deal with population parameters and the confidence intervals we've dealt with in the past have looked at the sample statistics now we're going to do it for the population parameters and remember the goal of inferential statistics is to take our sample and draw conclusions about the population parameter so now we're going to take our sample and from that sample we're going to calculate a confidence interval and that confidence interval is going to tell us about reasonable values for this population parameter so let's start with the definition a confidence interval is a set of values that theoretically contain the population parameter given proportion of the time when the experiment is performed many many many many many many many times so it's a set of values usually it's an interval so we have a lower bound and an upper bound and we assume every value between those two lower and the upper constitute the interval it contains the population parameter that we're interested in a given proportion of the time so if we're talking about like a 95 percent confidence interval then theoretically the population parameter is about 95 percent of those confidence intervals when we do this experiment over and over and over when we do it for just once then we just have to hope that the population parameter is in the interval it and be confident at a certain level that it is in the interval and this is why it's important to replicate experiments over and over again because one confidence interval could be based on biased data or data that's not representative of the population and even we could use the best sampling method on the world and we would get a a set of a set of data a sample of that data that's not represented of the population we'd never know that if all we do this if all we do is do this once that's why replication is so important so that we know that whether or not our first sample was representative or not because if it's unlikely that the sample is unrepresentative then it's going to be doubly so if you have two samples and that they're both unrepresentative so note that it gives popular information about the population parameter it's a set of reasonable values for that parameter some population parameters we'll be looking at will be the mean the variance the proportion it's a set of reasonable values it's a function of the data you collect data and from that data you calculate the endpoints of that confidence interval and since there's a function of the data it's a random variable and it is a result of a probability distribution calculation and we'll see that later um so if LCL and UCL LCL for lower confidence limit and uclb upper confidence limit you have to say one minus Alpha times 100 percent confidence interval for parameter Theta and Theta could be mu or Sigma squared or p and then about 1 minus Alpha times 100 of the intervals under repeated experiments will contain that parameter by default we'll use Alpha of .05 which means that we'll be talking about 95 confidence intervals just by default which I say here the confidence level C is 1 minus our Alpha by default we're going to use Alpha of 0.05 in this course and the value of alpha will be reintroduced in chapter 10 when we talk about hypothesis testing it's going to be the theoretical or the nominal type 1 error rate and those words will make sense when we get to chapter 10 when we start talking about hypothesis testing note that Alpha is selected by the researcher therefore the confidence level is selected by the researcher uh so unless it's stated otherwise we're going to have Alpha 0.05 in this course so let's get to our first intro lecture question that interlection question will be Define the confidence interval and again I suggest writing down the question on the left hand side of your notebook answer it below that and this is especially important because understanding what a confidence interval is is extremely important for understanding what you can learn from your data okay so that's where we were let's move on to something called bootstrapping in a previous class we introduced bootstrapping it's in sca5b I believe and obtaining a confidence interval for a population parameter so let's look at the code again this is how you would load in data from this a path it's geography.csv these are the results of a previous class taking a geography quiz six is the highest possible score on the quiz zero is the lowest um load it in attach the data set if you want to you can do a do summary of DT to see what the mean and the median are see what the Min and the max are see what the first and the third quartile are you could also calculate the standard deviation doing SD you can do the IQR interquartile range using the IQR function then we performed these two lines multiple times the first line draws a random sample from the variable score that's the score that the students made on the quiz with replacement so each time through X is going to be a vector of scores on the second line you find the mean of that so this will calculate the sample mean of that particular sample of scores and store it into the variable St at the ith position this is a for Loop so it will be doing this these two lines or everything between the two braces 1 times 10 to the fourth times so at the end of running this Loop you're going to have 10 000 sample beans doing a histogram of those will show you the distribution of those sample means and during the quantiles from these two positions these two points or these two levels will give you the endpoints of a 95 confidence interval for the mean we've done this in the past here's what the histogram looks like notice the histogram looks rather normal considering that the actual data looked nothing like a normal distribution the distribution the sample means definitely does look normal that does not surprise us or it should not surprise us if we actually understand the central limit theorem because what does the central limit theorem tell us sample means will be much more normally distributed than the data itself and these are the endpoints of the 95 confidence interval so we're 95 confident that the population mean mu is between 1.63 and 2.57 1.63333 and 2.56667. again these are out of six so the mean is definitely going to be less than 50 percent because fifty percent is three and that's way way over here and just about all of the histogram is to the left of three so we're almost positive that the mean understanding of geography is less than 50 percent this is fine if all we have is the data and that's all we did here we just looked at the data we drew from the data we dealt with the data as being representative of the population we didn't say the data was normally distributed we didn't say the data followed a binomial distribution we just said here's the data if we can make an additional assumption about the data specifically that the data are normally distributed then we can go a little bit further we don't have to do bootstrapping we can just run a simple test so recall that for large n from the central limit theorem X bar is approximately normal with mean mu and variance Sigma squared over n where Sigma squared's the variance of the data or variance of the data generating process and mu is the expected value of the data generating process so this should look familiar from chapter 7 section 2. now if we apply the Z transform from back in chapter 3 we're going to subtract off mu from both sides and we're going to divide by the square root of Sigma squared over n we're going to get that X bar minus mu over root Sigma squared over n is approximately normal 0 1. this is why it's called the zscore this distribution normal 0 1 it's also called the Z distribution this is normalizing the X bar distribution so we have X bar minus mu over square root of Sigma squared over n is approximately normal it's approximately standard normal that means the expression on the left is between the values of negative 1.96 and positive not 1.96 about 95 of the time how do I know that because the 2.5 percentile and the 97.5 percentile of the standard normal are negative 1.96 and positive 1.96 . how do we know that Q Norm of 0.025 will give us the negative 1.96 and Q Norm of 0.975 will give us the 1.96 and so we would say that a theoretical 95 confidence interval I'm sorry a theoretical 95 interval for this quantity this thing on the on the left X bar minus mu over square root of Sigma squared over n is between negative 1.96 and 1.96 by definition so this bottom statement would be a definitional statement that is if I have X that's normally distributed with mean mu and variance Sigma squared and I draw a sample of size n from the X distribution and calculate this there's a 95 probability that it's going to be between negative 1.96 and positive 1.96 because 95 of the time it's between this quantity is between negative 1.96 and positive 1.96 about two and a half percent of the time it's above 1.96 and about two and a half percent of the time it's less than negative 1.96 . now we really don't care about the interval for this quantity we want it for Mu we want a confidence interval for Mu well all we have to do is solve this equation this equals negative 1.96 for Mu and here's what we get here's the quantity equal to negative 1.96 we're solving for Mu remember so we multiply both sides by the square root of Sigma squared over n and this is kind of important this quantity square root of Sigma squared over n is called the standard error now we got X bar minus mu is equal to negative 1.96 times the square standard error we subtract an X bar from both sides remember we're solving for this meal so we're subtracting X bar from both sides we'll f with the negative mu on the right multiply through by negative one we get that this is the upper Bound for Mu so this we're going to call the upper bound on the 95 confidence interval from U I leave it as an exercise to show that this with a negative sign here is going to be the lower bound how would you do that this quantity equal to 1.96 and so from U so we got it X bar minus 1.96 times the standard error and X bar plus 1.96 times the standard error of the two endpoints we can symbolize it in just one expression X bar plus and minus Z of alpha over 2 times the standard error if Alpha is .05 then this will be a 1.96 out front um so we would call this oddly enough we would call this the confidence interval from U technically this would just provide the two endpoints of the confidence interval from U foreign question at this point oops what is the distribution of the sample mean in this lecture so what did we require the distribution of the sample mean to be notice not the distribution of the sample we're looking at the distribution of the sample mean so what was that in this lecture and again write the question on the left side your answer below it okay so this confidence interval that we just calculated is for Mu and it works this confidence interval is from U and it works when you're estimating mu which makes sense if you're trying to estimate the variance you wouldn't use a confidence interval from mu if you're able to calculate X bar and N well if you can't calculate X bar and N from the data then you I don't know what to say this goes better calculating X bar from the data is pretty straightforward and as you're just counting the data size and if you know Sigma squared remember Sigma squared is the population variance um I don't know that we would ever know the population variance and then last but not least the data are generated from a normal process or n is large enough that the central limit theorem says at X bar is approximately normal and again the reality here for four is we just need X bar to be approximately normal at no point in the last few slides did I say x has to follow any specific distribution every distributional statement we made was that X bar was normal or approximately so call the going back to number three the you have to know Sigma squared the population variance recode the definition of the population variance in order to calculate it you need to know what mu is however since we're trying to estimate mu we don't want to know what mu is therefore how do we know what Sigma squared is and in general if we don't know Sigma squared then what we just did doesn't work so your question is your question to me is why did we do it if we never know Sigma squared and therefore what we just did doesn't doesn't do anything for us and the answer is this process that we just did gave us an opportunity to show how confidence intervals are constructed what they actually indicates and how they can be useful kind of shaky on that last one but now that we know the process in creating confidence intervals we can fix this quote error or try to figure out some way that we can get by without having to know Sigma squared and create a confidence interval from that using the same theory behind it I mean specifically since we can't use the Z procedure which we just did it's also called the Wald wald named after uh I think it was Benjamin Walt could have been Abraham Wald I probably should have looked that up we're going to use a different procedure if we know Sigma squared we can use this procedure we just did if we don't know Sigma squared we can't because we need to know Sigma squared to do this procedure instead of the Z procedure we'll use the T procedure here's the function that we will calculate it's X bar minus mu the numerator is the same as it was for the Z procedure denominator we're looking at s squared over n and the Z procedure that was Sigma squared here it's s squared s squared would be the sample variance since if we've got the sample we can calculate s squared and this quantity follows approximately a t distribution T distribution has a parameter called the degrees of freedom in for one sample cases the degrees of freedom is just equal to n minus 1. now with this said we can go through the same procedure we did before with the Z's to come up with the endpoints of a confidence interval using the T is X bar plus or minus this distributional multiplier times the standard error just in this case the standard error is the square root of s squared over n comparing this confidence interval to the one we calculated just earlier so here capital T sub Alpha over 2 is the alpha over second quantity over the T distribution with n minus 1 degrees of freedom table two by tradition table two is the T distribution table so in the back of the book there's probably a table two that you would use if you had to I'm this slide Compares with the Z distribution the standard normal distribution the blue distribution how it compares to the T distribution notice the T distribution is much wider than the standard normal there's more measurements out here in the Tails far away from zero in other words the T distribution is much more variable than the Z why does that make sense it's because when the Z distribution when we were using the Z distribution we only had one source of uncertainty that was in The X bar the denominator was a sigma squared a population variance with the T we've got more sources of uncertainty we've got the X bar but we've also got a s squared that's a random variable also so we've got two sources of randomness so we would expect the T distribution to be much wider than the Z now this confidence interval from U works if you're trying to estimate mu if you're able to calculate X bar and n and S squared which again those are from the data so you should be able to calculate them and if the data are generated from a normal process or if n is large enough again we're doing this for X bar has to be normally distributed and that happens in two ways if the X's are normally distributed or if n is large enough and the central limit theorem works uh let's do question three why do we use a t distribution when the population variance is unknown so that should be two or three sentences or maybe one or two sentences again write the question on the left hand side answer it below so we've got our procedure to estimate mu okay just one View as we go through this this section this this these two chapters eight and nine we're going to get more and more procedures to estimate things we're going to get a procedure to estimate P we're going to get a procedure to estimate Sigma squared we're going to get a procedure to estimate two mu's or the difference in two mu's the difference in two proportions that the ratio of two variances we're going to get procedures that work when the sample is not normal enough so make sure that you can match the hypoth the what you're trying to estimate the parameter with the correct procedure and the requirements for that procedure if we go back one who uh these are the requirements to the T distribution or I'm sorry these are the requirements of the ttest for the T procedure if you don't meet these then you've got to use something else here's a list of several others go to the all procedures.pdf handout that's located online that will give you all the procedures we'll be looking at and we're going to be able to even have a procedure for mutilda the the population median so in today into today's slide deck we cover the central limit theorem again we looked at the definition of confidence intervals we look for confidence intervals for Mu when Sigma squared is known uh from U when Sigma squared wasn't known and both of those cases X bar had to be approximately normal we looked at confidence intervals for other instances or I just mentioned them and most importantly the idea or the theory behind confidence intervals this is the key for today understand what confidence intervals tell us once you understand this then you can just hop on the computer and have the computer calculate those confidence intervals for you it's the theory that you need to understand in the next slide deck we're going to show how to do these confidence intervals calculations in r tomorrow we're going to review a little bit of today we're going to look at some processes to create confidence intervals for one and two means medians proportions variances but we're going to do it in r so instead of having to actually do those calculations by hand it's going to be one line of code and interpret the results so in the future we're going to see a lot of these procedures make sure you keep a separate sheet I would have it in the back of your notebook and for every procedure you should State how to do it in R what the procedure is for is it for a single mean is it for two proportions it is it a ratio of variances Etc what the requirements are otherwise known as the assumptions of it and then how to interpret so chapter 8 and 5 and then Wikipedia all of those will be helpful hope this was helpful I'll see you in the next lecture hello and welcome to chapter 10. this is the chapter where we introduce hypothesis testing and see several different types of tests so this lecture is going to be about the theory of hypothesis testing so this will leave the groundwork tie it into previous topics that we've discussed the next lecture will be how to actually do this using R so you don't have to worry about all these probability calculations so by the end of this lecture you should be able to identify the research null and alternative hypotheses the research hypothesis is given to you by the researcher the null on the alternative you have to come up with you have to calculate the pvalue for a given alternative hypothesis and again this slide deck will be about doing it by hand that'll let you know exactly what the pvalue means and what it doesn't mean which ties into the third objective the next lecture will be how to do this in R so all you have to do is interpret the pvalue that R gives you so previously we've calculated confidence intervals and we've interpreted confidence intervals today we're going to look at the other side of the statistical inference coin and looking at hypothesis testing and pvalues the key one of the key one of the key differences is in hypothesis testing you create the hypothesis before you collect the data and then you test that hypothesis with your data so a hypothesis comes first collect your data then test the hypothesis whereas in confidence intervals you walk into it with no idea what the parameter value is supposed to be and you estimate the parameter value from the data and only from the data so confidence intervals solely from the data hypothesis testing is from the data and from a claimed hypothesis so here's the theory the theory behind hypothesis testing is one state the research hypothesis and the null hypothesis two somehow determine how much the data support the hypothesis and by the hypothesis I mean the null hypothesis to do that you need to determine the parameter being tested turn the determine the appropriate statistic or estimator how to determine the distribution of that statistic under the null hypothesis determine How likely it is to observe that statistic if the null hypothesis is true and that last thing is called the pvalue and then finally interpret all of that specifically interpret that level of support so let's go through a few definitions we're going to talk about that the three hypotheses types um hypothesis is a testable Claim about reality that's all it is since it's a claim about reality it's going to concern some aspect of the population and this aspect of the population is going to be a parameter that's we've been dealing with parameters from the second chapter onward we're going to start testing hypotheses about that population parameter based on our sample now the usual parameters hypothesize about at this level are just the mean the proportion the variance but we can hypothesize about any aspects of the population and we're going to symbolize this generic parameter using the Greek letter Theta so Theta can represent the mean the proportion the variance since it's a claim about reality since this hypothesis is a claim about reality it separates all possible realities into those that are consistent with hypothesis and those that are incompatible with it so every single reality will either be consistent with a hypothesis or incompatible with it and we have to determine which reality we fall into if we fall into the reality that is consistent with hypothesis we say the data support the hypothesis if we fall into the reality where the the uh the we fall into the reality that is incompatible with hypothesis we reject the hypothesis we say that the data are not supporting it I'm the most absolutely without question the most important hypothesis is the one made by the researcher and that is the research hypothesis it's a testable Claim about reality that's hypothesis part made by the scientist or the researcher and that's the research part this is the one that the statistician must eventually come to a conclusion about we are going to create two additional hypotheses to help us with coming to a conclusion about the research hypothesis but but ultimately we have to come back to the scientists and say yes the data supports the research hypothesis or know the data do not support the research hypothesis because we're using statistics we create two statistics specific hypotheses these two hypotheses divide all possible realities into two groups those that are part of the research and those that are not up that are not a part of it um so here's a scary table pay attention solely to the First Column the column headed HR that's going to be the different types of research hypotheses recall that Theta is the parameter of Interest it could be mu it could be P could be Sigma squared it could be I can't think of it it could be Lambda remember Lambda from the poisson and the Lambda from the exponential it could be a could be B from the uniform but we're focusing solely on mu and P and sigma squared the mean the proportion and the variance so those thetas represent one of those three the value Theta 0 or the the simple Theta zero represents a hypothesized value claimed by the researcher the Theta naught is just a a number that the researcher thinks is interesting so now we got these six symbols between the Theta the parameter and the Theta not the value the relationship between Theta and Theta naught has to be one of these six Theta the hypothe the researcher could hypothesize that Theta is less than some value equal to some value greater than some value less than or equal to some value not equal to some value or greater than some value and again the research hypothesis is given to us by the stat by the scientist from that research hypothesis we statisticians create a null and an alternative now let's look at the null and alternative hypo columns starting with the null column notice the null only has three types of of symbols greater than or equal to equal to or less than or equal to greater than or equal to equal two or less than or equal to the null hypothesis must always have the equal to part and that's because we are going to create a probability distribution for the parameter or for the statistic based on that equals part so the null hypothesis always has to have the equals part contrast this with the alternative there is never an equals part in fact if you notice the null and the alternative are complete opposites if the null is greater than or equal to the alternative is less than if the null is equal to the alternative is not equal to if the null is less than or equal to the alternative is not less than or equal to otherwise known as greater than it's because the null and the alternative have to divide reality up into two parts one part that supports the research hypothesis and one part that doesn't now which part supports the research hypothesis the answer is well it depends what is the symbol in the research hypothesis If the symbol is less than that cannot be a null hypothesis so that's got to be the alternative If the symbol is equal to well that's one of the allowable symbols for the null so that's going to be the null and the alternative is the opposite if the research hypothesis uses greater than well that has to be the alternative because it can't be the null and the null is going to be the opposite less than or equal to will be the null greater than or equal to will be the null not equal to will be the alternative so this the HR column is given to us by the researcher and from that we create a null and an alternative hypothesis that split the world up into a a reality that supports the research hypothesis and a reality that does not we have to determine which reality we're in don't memorize the table understand it learn what it says about relationships these research hypotheses are given to us we just have to figure out what the null and the alternative is first step is to determine if the equals part is a part of the research hypothesis if it is then the research hypothesis and the null hypothesis are the same otherwise the research hypothesis and the alternative hypothesis are the same and then the null and the alternative are always opposites what do all the null hypotheses have in common already said that what's the relationship between the null and the alternative said that why does that relationship have to exist okay now we know what the hypotheses are the research the null the alternative now let's look at this thing called the pvalue the pvalue is the probability of observing a test statistic this extreme or more so given the null hypothesis is true some key points there it's a probability it relates to the test statistic it's a probability of observing such a test statistic that is this extreme or more so given the null hypothesis is true so we assume the null hypothesis is true and we calculate the pvalue based on that now this is the stat statistical definition of the pvalue this is the one that's usually in the books pvalue is the probability of observing data this extreme or more so given the null hypothesis is true while the first definition makes me feel all warm and fuzzy inside the second one actually ties it more closely to the data that you've observed and the one I really liked one that makes me feel all nice and warm and fuzzy inside is the last one it's not a mathematical definition but it's a gut level definition of what the pvalue actually tells us pvalue is the amount of support in the data for the null hypothesis and again it's for the null hypothesis in other words if the pvalue is large then there is a lot of support at the data for the null hypothesis if the pvalue is small then there is very little support in the data for the null hypothesis the third definition comes from the second definition and the second comes from the first and they're all equivalent assuming that your test is appropriate and you've collected the data well so now we know to look at the pvalue and to interpret the pvalue it's just a matter of the pvalue is how much support is in the data for that null hypothesis and remember we got to tie it eventually back to the research hypothesis okay now we've got two of the three parts of the hypothesis testing Theory we laid out earlier the last part is making a decision about a research hypothesis based on the data so the fundamental question is do the data sufficiently support the research hypothesis do the data sufficiently support the research hypothesis and note that this is a binary yes or no yes the data do know the data do not this goes back to the pvalue but we need to somehow change that pvalue of probability that ranges from zero to one we need to change that into a yes or no and to do that we just have to determine some sort of cutoff between what supports the research hypothesis and what does not this cut off boundary we're going to refer to as the alpha value the typical Alpha value is .05 my defaults .05 if the level of support is less than Alpha we reject the null hypothesis the data do not sufficiently support the null hypothesis if the day if the if the level support is greater than Alpha if the pvalue is greater than Alpha we don't reject the null hypothesis the data do not tell us that the null hypothesis is wrong notice that there is information in the pvalue that goes above and beyond just the yes or no answer so not only should we interpret the pvalue in terms of yes the data supported to know the data don't but we should also provide the pvalue because that will also tell us how much it supports it or how much it doesn't continue on to and continue on with decision Theory this Alpha value the alpha value is the type 1 error rate that's claimed by the statistician a type 1 error occurs in the researcher rejects a true null hypothesis so there is that null hypothesis if it's true then Alpha then the type 1 type 1 error occurs when we reject that true null hypothesis now the thing is we never know if a null hypothesis is true in real real life which can be a problem but then if we knew a null hypothesis was true in real life then why would we do statistics to test the null hypothesis I mean we'd already know that it was true the type 1 error rate is the proportion the type 1 error rate is a proportion of the time that the researcher commits a type 1 error so the rate part means it's the proportion of the times that the researcher commits that type 1 error note that the actual type 1 error rate may not be Alpha the alpha is claimed by the statistician but the test may have some flaws to it where the actual type 1 error rate is not Alpha and laboratory F explores this in much greater detail however we tend to just pretend that Alpha actually is the type 1 error rate how it's a lot of good pretending in statistics if there is a type 1 error then there must at least be a type 2 error otherwise why would we call it a type 1 error a type 2 error occurs in the research researcher fails to reject a false null hypothesis type 1 occurred when the researcher rejected a true null hypothesis a type 2 occurs when the researcher fails to reject a false null hypothesis or we could also think of it as a type 2 error occurs when the researcher rejects a true alternative although we never frame it in terms of rejecting a true alternative the type 1 error rate that slipped in the type 2 error rate is symbolized with beta just like the type 1 error rate is implies by Alpha the type 2 error rate symbolized by Beta both are errors and error rates so we would like to minimize both however is reducing one increases the other furthermore if we want to choose if we want to make either one of them zero then the other one goes all the way up to one so let's we're kind of stuck there we tradition in statistics is to set Alpha and then beta just happens as it happens beta is going to be a function of alpha as described here but it's also going to be a function the sample size it's also going to be a function of how wrong the null hypothesis is um and so if we want to reduce the type 2 error rate then increasing the sample size is guaranteed to do that but unfortunately as we've seen in the past increasing the sample size tends to be a little bit expensive at times here's an aside this is not statistical here well it is kind of statistical but it's if there's a type 1 error rate then there must be at least a type 2 error rate a terror error there are in fact a couple more these are nonstandard by the way but they do give us some insight into what can go wrong in a statistical analysis so we'll Define a Type 3 error which I want to introduce here but I only want to introduce this to you so that you think more through the process and where errors can pop in not so that you can replicate this on a test a type three error occurs in the researcher rejects a false null hypothesis that's good but for the wrong reason that's the error part is you're doing it for the wrong reason um type 3 so here are some causes of type 3 errors One is using the wrong test um two is aggregation bias in other words you're measuring at a lot at uh measuring something about groups and you're trying to draw conclusions about the individuals in the groups and there is ecological fallacy which is just about the opposite of that collinearity among predictors it just means you're using two independent variables that are highly correlated and you're not able to determine which is causing the the effect that you're looking at so the key to avoiding the type 3 error rate is to fully understand the statistical tests the data collection the relationship amongst the independent variables in other words the key is to understanding your model and your data the wrong test the wrong interpretation the wrong assumptions All Leads errors but also realize that type 3 errors can be eliminated it just requires that the statistician is careful type 1 and type 2 errors cannot be eliminated let's go through three simple the CCD examples I'd like to test if my coin is biased in favor of getting heads specifically using physical language my research hypothesis is HR for research hypothesis that's colon p is our parameter so it's a population proportion my research hypothesis is that P is greater than onehalf I would like to test if my coin is biased in favor of hits so P must be the probability of getting hits so this will be our research hypothesis going back to the table if this is our research hypothesis where will this also be the null or will this also be the alternative hypothesis the key is looking at the symbol in the middle there's no equals part so this will also be the alternative hypothesis since this is the alternative what is the null I recall that the null hypothesis is The Logical opposite what is the opposite of greater than correct it's less than or equal to and as always if we're going to test hypothesis we collect data in this case I flip the coin 100 times and I get 58 hits from the way the problem is set up we know that the number of heads is follows a binomial distribution N is a hundred it's the number of Trials the number of times I flip the coin p is 0.5 because that's my null that's the equals part of my null hypothesis that P is equal to 0.5 note that if the number of heads observed is too large then the alternative hypothesis is more likely to be correct if the number of observed heads is too large because we're research hypothesis is greater than onehalf so let's calculate the pvalue from the definition pvalue is a probability of observing data this extreme or more so given the null hypothesis is true so here's the pvalue it's the probability of of observing more than or equal to 58 heads because I observed 58 the pvalue is the probability of observing data this extreme or more so greater than or equal to 58 given the null hypothesis is true given the null hypothesis is true and this is the null hypothesis in distribution form and 100 p is 0.5 and guess what we know how to calculate this from back in chapter five we also know how to do it rather quickly it's a greater than or equal to 58 x is eight discrete so we actually do have to pay attention to the 58 this is 1 minus X less than or equal to 57. ends 100 piece 0.5 so the pvalue is .0660531 so how do we interpret that pvalue let's go ahead and look at the distribution of the number of heads so this is the probability Mass function for our null hypothesis the dark blue is the greater than or equal to 58 so the dark blue is going to be the pvalue and technically the sum of the area under the dark blue will be our pvalue since the pvalue is greater than our usual Alpha of 0.05 remember it's 0.06 something we do not reject the null hypothesis the data are not sufficiently against the null hypothesis therefore we actually cannot conclude anything about the research hypothesis P may be greater than 0.5 P may be less than 0.5 P may be exactly equal to 0.5 if you look at the confidence interval for p by doing the binomial procedure you'll see that the confidence interval includes 0.5 it includes some numbers less than 0.5 it includes some numbers greater than 0.5 and as such we know that that entire confidence interval is a set of reasonable values for p since it includes things above below and equal to 0.5 all of those are reasonable realities so we fail to reject the null hypothesis pvalue is too large example two cards I believe that my blackjack dealer is cheating if everything is fairer than I expect to have a blackjack 4.75 the time you can calculate that using chapter 4 stuff I have played 132 hands and got a blackjack only once do I have evidence that the black deck blackjack dealer is cheating in other words using statistical language my research hypothesis my claim about reality is that P is less than 4.75 percent why less than why would my blackjack dealer cheat and give me a higher probability of winning so less than will this also be the alternative or the null correct this will also be the alternative this has no equals part to it so what will the null hypothesis be very good greater than or equal to from the way the problem is set up we know the number of blackjacks X follows a binomial n of 132 the number of hands I've played P of 0.0475 which is 4.75 percent which is the equals part in the null hypothesis also we know that if the number of blackjacks is too small then the alternative hypothesis is more likely to be correct and we would reject the null so let's calculate the pvalue I observed one so the pvalue is going to be X less the probability of X being less than or equal to one given X follows this distribution so as P binom one size 132 prob 0.0475 that gives us a pvalue of .0123027 because the pvalue is so small I reject the null hypothesis I have evidence that the blackjack dealer is cheating notice that I say I have evidence I don't say this is proof statisticians deal in evidence not in proof so this will be the distribution or part of it continues on to the right a lot a distribution under the null hypothesis the pvalue will be the sum of these two heights notice that it is very small very low P naught is our claimed well oh yeah P naught is our it's the 4.75 percent sorry it threw me off there for a moment P naught is our 4.75 percent four point yeah 4.75 what we observed was way down here and remember the pvalue is what we observed or more so observed more so next example the D in CCD I believe that this dye is biased against getting a six specifically using statistical language my research hypothesis Claim about reality is hrp less than 1 6 which is 0.11 I'm sorry 0.1667 to test this I roll the die 100 times and get six a total of nine times because this is less than that's the alternative phenel would be greater than or equal to this is also binomial n of a hundred P of 0.1667 I need to calculate let's see I got a total of nine times so I need to calculate the probability of X being less than or equal to 9. the equal to is data this extreme and the less than is or more so we know how to calculate this this comes out to be 0.02124964 if we need to make a decision since this is less than our Alpha of 0.05 we reject the null hypothesis we have evidence that the dye is biased against the number six here's the probability Mass function for that particular binomial I observed 9 which is right here pvalue would be the sum of the heights for nine and less those are pretty straightforward those are the binomial examples let's look at some means examples these will go back to the Z distribution um some fast food restaurant in town claims that the weight of a quarter pounder hamburger before cooking is four ounces with a standard deviation of Sigma equals one ounce notice we are given the population standard deviation is one the research hypothesis or the claim bioacdonalds is Mu is equal to four since this is an equals this will also be the null hypothesis the alternative hypothesis will be not equal to that's no we haven't done or not equal to test this weigh a stack of 25 patties and find that the total weight is only 94 ounces we would expect it to be a hundred four times 25 but it's only 94. is there sufficient evidence that whack the nulls is incorrect so we're trying to calculate probability that t remember capital t is the statistic total or the sum of the observations probability that t is less than or equal to 94 or t is greater than or equal to 106. well the less than or equal to 94 is pretty obvious we observed 94 that's less than what we would expect therefore we'd have to calculate the probability of P being as extreme equals part or more so but remember the alternative is not equal to therefore being too high would also be as extreme or more so 94 is 6 less than what we would expect so the upper end will be six more than what we expect and then greater than equal to that 106. uh T follows a normal distribution with expected value of 100 and variance of one times square root of 25 and where this is for the sample total and the statement more or less comes from the central limit theorem in fact forget the more or less the statement does come from the central limit theorem specifically since I didn't tell you the actual distribution of the Patty weights one times the square root of 25 is just five so keep that in mind this is just two times probability of X being less than or equal to 94 given mu is equal to 100 and S is equal to 5. why is it two times this well notice we're only calculating the lower tail and we actually have to calculate both the lower and the upper but the normal distribution is symmetric so I can just double the lower gives me a pvalue of 0.23 because this pvalue is greater than Alpha we cannot reject the null hypothesis if we do a confidence interval we'll see that the confidence interval does include 100 as well as values above it and values below it so we don't actually know if mu is 4 or greater than 4 or less than four all values work according to our data which means we probably should go back there and collect some more data here's the distribution of t notice we observed 94 this area is less than or equal to 94. this is the as extreme or more so on the bottom end is this 106 so this will be the as extreme or more so on the upper end normal distribution is symmetric so this dark blue area will be exactly the same as this dark blue area so we can get away with just doubling this lower tail so the total area or double the lower is the pvalue uh McDonald's claims that the weight of a half pounder hamburger patty before cooking is eight ounces with a standard deviation of Sigma equals one again we know we're given Sigma the population standard deviation in symbols this will be mu is equal to eight because they're claiming that the average weight is eight ounces to test this we weigh a stack of 25 patties and find that the average now we're looking at the average is only 7.5 ounces sufficient evidence that wax Donald's incorrect since the research is equal that will also be the null alternative will be not equal to we observe 7.5 ounces so we need to calculate the probability of being less than or equals seven point ounces and greater than or equal to 8.5 ounces it because the alternative is not equal to here x bar follows a normal mean eight standard deviation one over five 1 over the square root of n and again this comes from the center limit theorem 2 times P Norm of 7.5 mean of eight standard deviation of 0.2 gives us a pvalue of .01241933 how should we interpret this result because the p because the pvalue is less than Alpha we reject the null hypothesis we have sufficient evidence that mu is less than eight here's the distribution of X bar we observed way down here whack Donald's claims way up here but we observe down here pvalue is this area plus this area because both are as extreme from U or more so we can just double the lower tail that will give us the pvalue that we saw it very small probability we're way out here in the tails whack the nulls claims at the weight of a pounder hamburger patty before cooking is at least 16 ounces with the standard deviation of Sigma equal one ounce and sybils this is HR mu that's the average is greater than or equal to 16 y greater than or equal to because it's at least that's what greater than or equal to means this will also be the null hypothesis because it has the equals part the alternative will be the opposite of this it'll be less than to test this we weigh a stack of 100 patties and find that the average weight is 15.9 ounces this sufficient evidence that whack the nulls is incorrect let's calculate the pvalue I want to calculate the pvalue this is just probability of X bar being less than or equal to 15.9 why only One Direction that's because the research hypothesis only has one direction notice we're calculating less than or equal to the alternative was less than and we know that X bar follows a normal mean of 16 standard deviation of one over ten because N is a hundred and this is a really good weekend for me here's how we calculate the pvalue comes out to be 0.1586553 this value is greater than Alpha therefore we cannot reject the null hypothesis there is not sufficient evidence that McDonald's is wrong I mean whack the nulls is wrong they could be wrong but we don't have the evidence that they're wrong because the pvalue is so high here's that distribution of X bar this is what we observed remember the alternative was less than because the null was greater than or equal to the alternative is less than so we shade less than what we observe this will be the pvalue one more example after this number of calories in a mcpork is at most 350. with the standard deviation of 50 calories and symbols this is less than or equal to at most is less than or equal to to test this we perform a calorimetry tests on a stack of 100 with porks what a waste of good food and find that the average number of calories is 343. note the alternative is greater than because this is less than or equal to this will be the null the or equal to part is key there the alternative will be the opposite so now we ask what's the pvalue is there sufficient evidence that whack Donald's is incorrect we need to calculate the probability of X bar being greater than or equal to 343 we observe 343 the alternative is greater than so the greater than part will be the or more so extreme and we got X bar follows a normal 350 because that's our claim with Sigma equal to 50. boom boom boom divided by the square root of n is just P Norm 343 55 gives us a pvalue 0.9192433 because the pvalue is greater than Alpha we cannot reject the null hypothesis there is no sufficient evidence that whack the nulls is wrong about its calorie statement they could be wrong we just don't have the evidence there is the distribution remember the alternative was greater than so we shade above what we observe and that's a lot of dark blue so the pvalue really looks really big so short summary we looked at stating hypotheses testing hypotheses calculating pvalue from the definition we looked at two R functions these are things that we've seen in the past download the all procedures PDF file that lists all the statistical procedures we're going to use in R we're going to see how to use those in the next lecture because in the next lecture we're going to review today and use R to perform the calculations really easy I guess that should be pvalues um again we're going to cover a lot of tests so keep separate sheets for each of the tests there's an example of a flow chart on how to test me a single mean on the module on the module 4 Page follow it create your own for the rest of them here are some readings are for starters chapter 4 Hawks learning chapter 10. of course Wikipedia hypothesis tests which brings us to the intraelectric questions there's three of them question one again write question one the question on the left hand side of your notes answer it below so you can easily transfer it into Moodle what is the relationship between the null and the alternative hypotheses question two what are the three allowed signs in a null hypothesis question three when is the research hypothesis the same as the alternative hypothesis and all three of these get back at a common problem in intro stats of trying to figure out okay I've got a research hypothesis where do I go from here you may want to review the table um or he may not so those are the three and that's the end of this again next lecture we're going to see how to do this in r thank you much hello and welcome to section 10 4 chapter 10 is all about hypothesis testing section 10 4 is about handling proportions one sample and two sample proportions so by the end of this lecture you should be able to understand the theory behind and test hypotheses about a single population proportion and the difference between two population proportions you should also better understand that the better understand what the pvalue means and how to test hypotheses and clearly specify why confidence intervals and pvalues both give important informations about the population parameter so one parameter procedures this will be about population proportion p on the parametric procedure is the binomial procedure the usual graphic is the binomial plots that seems to be a lot like what we did back with confidence intervals and one parameter uh one population proportion hmm requires the data being generated from the binomial distribution that really sounds familiar the r function is binom.tests X comma n x is the number of successes and is the number of Trials i s I guarantee we've seen this somewhere before haven't we note that this is not the procedure that Hawks covers they use something called the walled tests World DOT test it also takes X comma n so that should help you with the Hox assignments here's the tests Theory and we've actually covered this back when we introduced hypothesis testing it was a whole bunch of binomial stuff this is what led to the binomial test and we're trying to draw conclusions about a single population proportion we should use a test statistic based on the sample proportion or on what we observe the number of of successes if we do it based on the sample proportion we'll use the walled test if we do it on the number of successes we use the binomial test again the binomial test is the exact test and the wall test is the approximate test um we're given X follows a binomial distribution recall that the binomial has two parameters and NP um the number of observations serves as a particularly fine test statistic because we know it's distribution exactly it's binomial we only know the distribution of the proportions approximately x divided by n is only approximately normal and that only comes about because of the central limit theorem if we go all the way back to section six point five approximating the binomial with the normal we see oh there's a lot of error that can pop in there because it is a an approximation and the smaller the sample size the more the error the following are three examples showing how to perform these calculations I have a coin that I think is fair to test this F of it 10 times and count the number of heads in those 10 flips total of three heads actually came up is this sufficient evidence of the coin that's not fair so the claim is p is equal to onehalf that's the research hypothesis since it contains equal sign this is also the null hypothesis since the equals is the null the alternative will be not equal we're trying to make a conclusion about P where the data are generated from a binomial thus under the null hypothesis X follows the binomial n of tan P of 0.5 we observed x equals three pvalue is defined as the probability of observing data this extreme or more so given that the null hypothesis is true in other words the pvalue is the probability of X being less than or equal to three plus X greater than a probability of X greater than equal to seven data this extreme would be x equal three or or more so is less than three where'd the 7 come from well expected value of x which is n times p is five three is two less than five just as extreme would be two more than five and more extreme than that would be greater than two plus five there's the distribution of x distribution of the number of heads we observed three that's the red thing 7 is equally extreme less than or equal to 3 is or more so on the left and greater than or equal to seven is or more so on the right so the red Heights added up will give us our pvalue so the pvalue is 0.34375 simply using calculations that we did back in chapter five section two and in fact we did these calculations back in chapter five section two and we interpreted it correctly as evidence in favor of or against the claim we're just giving it some more terminology and some more meaning from the all procedures handout and the sca examples that we've looked at we know we can also do binom.test x equals three n equals 10 and P equals .050 line tells us that the pvalue is 0.3438 since this is greater than Alpha we fail to reject the null hypothesis the coin may be fair the coin may not be fair we have no evidence that can tell us either way a 95 confidence interval for the probability of a flip Landing head is between 6.7 percent and 65.2 percent that is an extremely wide confidence interval how possibly could we make it narrower right collect more data instead of flipping the coin 10 times flip the coin a hundred times ten hundred times ten thousand times the larger the sample size the narrower the confidence interval example two this goes back to a a lab that we did I contend that more than a quarter of students at Knox or Juniors to test this I randomly sample from the student body asking class here in my sample of 100 students 30 stated they were juniors here the claim is p is greater than 0.25 because I contend that more than a quarter so it's more than since it contains the greater than sign this will be the alternative the no will be the opposite of greater than which is not greater than otherwise known as less than or equal to that means the two statistical hypotheses are given below here's the distribution of the number of Juniors under the null hypothesis remember the null hypothesis contains the equals part of a quarter I asked 100 people so this would be the distribution of the number of Juniors that I experience pvalue will be the probability of remember I got 30 of them so this is what I observed right here since the alternative was greater than the pvalue is going to be the probability of X being greater than or equal to 30. the equal to is as extreme and the greater than is the or more so we get 0.1495 because the pvalue is greater than Alpha we fail to reject the null hypothesis we do not have sufficient evidence that the proportion of Genius is greater than a quarter in fact we can calculate the confidence interval directly and find that the proportion of generous is greater than 0.2249 based on this data or we can use a binomial test X is 30 because that's what we observe the number of successes N is a hundred that's the sample size p is 0.25 that's my value of Interest that's my Theta naught the alternative is greater because I claim that mu is I'm sorry I claim that P is greater than 0.25 if you get a pvalue of Point 1495 we get the confidence interval from 0.23 up so I'm 95 percent that the proportion of Genius is at least 0.2249232 one thing that we can definitely use this binomial test for is to check if a data set is representative on its face the data file is some college this is real data I had to change the name of the college to protect the innocent it was sent to me by the Registrar of some college to do some statistical analysis I did a little check sent it right back and said the data are not representative she said yes it is I said okay I'll go ahead and do the analysis and it'll be worthless but I will tell you it's worthless she kind of got upset with me um I was supposed to model the success the high enough GPA given some of the other variables let's perform a quick check to see if the data are reasonably representative of the population at that College so my provided sample consisted of 661 students she claimed a random sample from the College of which 22 were freshmen given that the proportion of freshmen at SCU some College University is 28 percent is the day or the data representative in terms of freshmen so here the claim is p is equal to 0.280 since it contains equal sign it's the null that means that the two hypotheses are equal and then not equal for the alternative under the null hypothesis the number of freshmen in my sample should follow this binomial distribution n of 661 P of 0.28 remember I observed 22. so I observed something down here something just as extreme is going to be located way up here or we can simply just double the probability of X being less than or equal to 22. probability of X being less than or equal to 22 is zero I'll go ahead and double that 2 times 0 is 0. we could also add zero to that um we could multiply by one if we want to stretch this out but it comes down the pvalue is zero since the pvalue is less than or Alpha 0.05 we reject the null hypothesis in favor of the alternative in terms of freshmen the sample is not represented of the population of SCU Simpson I'm trying to model the GPA it is very reasonable that freshman GPA will be different from sophomores Juniors and seniors therefore in order for my analysis to be worth the weight that it's to be worth its weight in gold I really do need more freshmen in my sample or we can just use power of r binom.tests x is 22 number of successes n is 661 p is 0.28 I got the 0.28 from there uh website got a pvalue of being less than 2.2 times 10 to the negative 16th this data would be representative if the proportion of freshmen at SCU is between two percent and five percent now that I talk about this I realize I never did get paid for this so yeah it's okay I didn't do anything other than say that they didn't give me the data next types of tests the two parameter procedures we're going to compare P1 minus P2 or we're going to compare P1 and P2 the parameter of interest will be P1 minus P2 this would be the proportions procedure or the two proportions procedure the graphic will be the binomial plot it requires that the expected number of successes is at least five in each group as some would say at least 10 in each group uh if no one's dying from you being wrong then at least 10 should be fine if you really do need to make sure that you're right then at least 50 or at least 100 in each group would be preferred the proportions test is based on the normality approximation to the binomial distribution therefore large sample size is required the r function is pardon me is prop.tests you give it the number of successes X and the number of Trials n since it's 2 um two proportions we're comparing you got to give it two sets of x's or two x's and two ends wrapped in C's they use something close to this but this procedure actually makes adjustments for the fact that the binomial distribution is discrete and the normal is not as such you'll need to use the wall test to perform Hawk's homework estimating P1 minus P2 so again when you're using proportion stuff with Hawks use the walled test the wall test will act will require both the X and the and just as laid out here with prop test um sample sizes are large the wall test and the proportions test will give you essentially the same answers it's only when the sample size is small that they're going to be different so here's the theory behind the proportions test actually here's the theory behind the walled test since we're trying to draw conclusions about the difference between two population proportions we're going to use a test statistic based on the difference into sample proportions in other words we're going to use sample proportion one minus sample proportion two we do know that X and Y both follow binomial distributions each with their own sample sizes each with their own probabilities PX and py one of the biggest problems is x minus y is not going to work because we don't know the distribution of x minus y it's not binomial um we but we do know that the sample proportion for x minus the sample proportion for y is a good estimator of p x minus py and the sample proportion is just the number of successes divided by the number of Trials so that's going to be the successes over trials for x minus successes over trial for y all we have to do now is figure out the distribution of this statistic if we're okay with approximation we know that X is approximately normal and Y is approximately normal which means that X over n x is approximately normal and Y over n y will be approximately normal and X over n x minus y over n y will also be approximately normal Central limit theorem is just so awesome I mean we can say all of that simply because of the central limit theorem and the approximation increases I mean the approximation gets better as the sample size is individually get better which is what we're saying here remember that the if x follows a binomial then it also approximately follows a normal with expected value of NP and variance of NP 1 minus p and we'll subscript all the X stuff with X and all the Y stuff with y standardizing or I'm sorry dividing by x and x and and Y will get us this as our normal distributions subtracting X over N X and Y over n y gets us its own normal distribution notice that this p x minus p y is what we're trying to estimate in other words our sample proportion for x minus the sample proportion for y is unbiased for our population proportion x minus population proportion y That's good here's the variance though and we'll do the typical standardization we're going to subtract off the expected value divide by the square root of the variance and that means that it will follow this this test statistic here will follow a normal distribution with mean zero standard deviation one and the standardization comes directly out of either chapter 2 or chapter 3 when we did the Z scores or the standardized scores if p x equals p y is our null hypothesis then that means that the second half just goes to zero or is equal to zero and this is the usual Z procedure version of the test we can do an equivalent test to this this isn't a side if we Square this thing on the left and that means we have to square this thing on the right squaring this thing on the left leads us to this squaring the thing on the right brings up a new distribution it's the chisquared distribution this probability statement and this probability statement are identical there is no information contained in one that's not contained in the other there is no difference in power there is no difference in Precision there is no difference in accuracy the two tests are mathematically equivalent so the one you use if you use this one you might as well use this one if you use this one go ahead and use this one they're the same the proof of that is left for later class 225 or 321. so let's see how to do this I'd like to determine the proportion of males who wear hats is the same as a proportion of females who wear hats notice now we're dealing with proportions in two separate populations proportion of hat wearers of males and proportion of hat wearers of females to test this I sample 100 males 100 females 10 males and 16 females were wearing hats so those are the hypotheses step one we're going to assemble the information we have P sub X is what we observe P sub y sample proportion I guess NX and Y we asked 100 males 100 females Alphas 0.05 which means our Z sub Alpha over two is plus or minus 1.96 this is the test statistic we have to calculate I'll give you a hint there's a fast way of doing this in R and you already know it but let's go ahead and churn through this so here's the formula for the test statistic this is the plug this is the Chug and we're chugging some more and more chugging and chugging we're done chugging so our test statistic is negative 1.26601 this is this distribution is our standard normal distribution this is what we observe negative 1.26601 it's right here that value is as extreme so it's positive 1.26601 or more so is the Shaded area on both sides so these two areas add up give you the pvalue because the normal is symmetric twice this area is the pvalue so we can calculate the pvalue it's 0.2053 because the pvalue is greater than Alpha we failed to reject the null hypothesis we do not have evidence that men wear hats at a rate different than women which is what this says there is no evidence to claim that they may they may not we just don't know we don't have the evidence um what I just described is the walled test doing this in R is just one line as opposed to doing all this calculation by hand all this fun calculation it's just one line This is the output 10 successes for males 16 successes for females out of 100 trials for men and 100 trials for females gives us back the data here's the pvalue 0.2071 greater than Alpha failed reject the null hypothesis no evidence of a difference and that's it how how much faster is this than all of this and I'll tell you this when I was typing this up you know how many times I had to go through this and make sure I didn't make a mistake in the calculations I'm not talking about in the typing unit I'm talking about the calculations I think I even made a mistake in the numerator here I think I made this as plus 0.06 so allowing the computer to do all of this in one line is just amazing so in today's slide deck we covered procedures for estimating a single population proportion and for estimating the difference in two population proportions if you're doing this in R I'm sorry if you're doing this for Hawks walled test works for both of these and you should use the wall test if you're doing Hawks if you're doing this for real binomial test for the first and prop test for the second in the future we're going to see many many many many many many many many many many many many many many more procedures more tests again create that section in your notebook dedicated to test the assumptions of the tests we looked at binom.test and prop.test wold.test behaves the exact same as either one of these except instead of binom.test you'll do Wald DOT test and instead of prop.test you'll do walt.test but the the parameters that you put over here are going to be the same here are some available scas to help you work through these SCA 12 and 22 the 2 means it's for proportions the one is the number of samples so this sca12 is for one sample proportions and SCA 22 is for two sample proportions and they're all located in the usual place um so now I'm going to ask the intralecture questions one of these will be very familiar question one when is the research hypothesis the same as the alternative I encourage you to go back and look it up again the rest of the the remaining two questions will be different but similar to each other for what types of hypotheses do we use the binomial tests and what I mean by that is I'll just give you the answer because it's fun the types of hypotheses for which we use binomial tests are those where we're trying to test hypotheses about one single population proportion and question three is what types hypotheses do we use the proportions test I can give you that answer too for hypotheses about comparing two population proportions I don't mind giving the answer sometimes chapter 8 and R for starters sections 10 4 and 11 4 in Hawks go ahead and read up on hypothesis tests in Wikipedia all procedures at PDF you should have that printed out and available to you when you're working on stats and that's the end handling proportions um hope this was helpful I will see you or talk to you later hello and welcome to chapter 10 where we cover lots and lots of tests for hypothesis testing here we're going to look at section 10 6 where we look at discrete distribution matching we introduce the chisquared goodness of fit test and by the end of this lecture you should be able to understand the theory behind and test hypotheses about just one thing comparing an observed categorical distribution to a hypothesized one also the usual better understand the pvalue and how to test hypotheses but this one is about goodness of fit tests so here's the parametric procedure it's the only procedure we've got the chisquared goodness of fit procedure the null hypothesis is that the data are generally generated by the hypothesized distribution so the research hypothesis is going to be in the null hypothesis is going to be that hypothesized distribution the alternative hypothesis will be the data are not generated by that hypothesized distribution the graphic is going to be the usual binomial plot notice that it's expanded to go from not one not two but K different values or different groups these will be successes the x's and the NS will be the number of Trials requires that the number of successes and expected number of successes in each group is at least five sometimes it's required to be 10 it really does come down to how precise or accurate do you want your values to be or your estimates to be um more data is always better as long as it's good data of course if your expected number of successes is not at least five in each group it's really not anything you can do except collect more data and if you can't collect more data then you've got to conclude whatever you can conclude from this test but you must specify the sample the sample size is too small to properly use this chisquared goodness of fit test therefore these results are Highly Questionable the r function is that tests what does stand for it's Chi Squared and notice again we got a DOT test thing we get a whole lot of DOT test things and the Dot Plot things we got a lot of Dot Plot things note that this function this DOT test function is not Hawks they use something close to this but the case not test function actually makes adjustment for the fact that the binomial distribution is discrete and the normal is not the hand calculations that we have do agree with Hawks however um so pay attention to the quote hand calculations and how to do them in R so all you have to do is just substitute a few change a few numbers and it'll be good for you so we'll start with a framing example that is a picture of a threesided die I would like to test if it is fair to do this I roll it 600 times and tabulate the observed frequency distribution in those 600 rolls I got 181s 215 twos and 205 threes note that I would expect to have 200 ones 200 twos and 203s if the die were Fair I didn't I got 181s 215 twos and 205 threes now the question is is the 180 215 and 205 just due to the random fluctuation in a fair die or are those values too far away from what we would expect if the diver fare and that's really what all of the statistical testing is coming down to it's what I expect versus what I observe and is what I observe too far away from what I expect given expected random fluctuation so we're given this information I'm just abstracting it The observed counts are 180 to 15 205 the expected counts are 200 200 200 now where did I get the 200 sample size is 600 I rolled it 600 times I want to test if the threesided die is fair if it is fair then I would expect the number of ones to equal the number of twos to equal the number of Threes each of them to be a third of six hundred note what the information above actually gives to us it gives us this a set of observed counts and a set of expected counts we're going to call the observed counts x's and the expected counts Muse and in this there are K groups that's what the k stands for in the example that we're working on K is equal to 3. so our goal is to create a test statistic that measures how far apart the observed is from the expected while creating well still having a distribution that we know and we're going to use Define we in this case as statisticians we in this class don't know this distribution yet although technically we have bumped into it once when we were doing onevar Dot tests so it can be shown but not in this class that the test statistic approximately follows a chisquared distribution with K minus 1 degrees of freedom the approximation gets better as U gets larger so that statement the TS equals the sum and the probability statement is something that is to be shown in a future course in other words it's beyond the scope of this course I like that phrase beyond the scope of this course uh TS represents test statistic it's the sum over all of the groups The observed minus the expected squared I divided by the expected y the squaring we're squaring it because if we don't Square then the sum of the x i minus the MU I is always going to give us zero so this squaring allows us to avoid the zero problem and to indicate that larger values of there are could be too high or too low they're just farther away from what you would expect so recall the observed counts were 182 15 205 the expecteds were 200 200 200 expected values again came from NP n is our number of Trials the p is the probability of success for each of those categories the probability of getting one is onethird so the expected number ones is 600 times onethird respective number twos is 600 times onethirds respect to number three is the 600 times onethird yes binomial mu is equal to NP so let's go ahead and do these calculations out for fun TS to test statistic is defined as this if we expand that summation that's what this means first time through I is equal to one so it's x sub 1 minus mu sub 1 squared over mu sub 1. plus because it's a summation x times through I is equal to two x sub 2 minus mu sub 2 squared divided by mu sub 2. third time through and again we're going to add on the third term which is x sub 3 minus mu sub 3 squared over mu sub 3. so this is the expansion of the summation and now all we're doing is just substituting in the values we've got x sub 1 is when a to the x sub 2 is 215 x sub 3 is 205 those are what we experienced or observed the expected values were 200 each now we do the calculations calculations calculations calculations so our test statistic is 3.250 this is the distribution of the chisquared distribution with two degrees of freedom this is what we observed for our test statistic 3.250 3.250 shaded area is 3.25 is the probability of having an a test statistic of 3.250 or greater so this shaded area is the pvalue and that pvalue actually comes out to be 0.1969 here we are with a pvalue we know how to interpret that pvalue so it's been the same since we introduced pvalues pvalue is greater than Alpha failed reject the null hypothesis we don't have evidence that the die is unfair pvalue greater than Alpha we failed to reject the null hypothesis we don't have evidence that the die is unfair pvalue greater than 0.05 we failed reject the null hypothesis we don't have evidence the die is unfair I think three is enough so there's the conclusion we don't have evidence that the die is unfair it could be unfair I don't know but the data doesn't tell us it's unfair note that the data doesn't tell us it's fair either the data leaves it up to a big question mark of we don't know could be fair may not be fair we don't have enough data to say so here's the way of getting it in uh for Hawks calculations OBS will be the observed counts EXP is the expected counts so these will need to change for your problem TS this is how you quickly calculate that test statistic that doesn't need to change just running TS will give you the value of the test statistic that won't need to change 1 minus P Kai's doesn't change of TS doesn't change but we do need to change the degrees of freedom this 2 needs to be K minus 1. the number of groups minus one so if we've got 10 groups here change that to a 9. if you've got two groups here change that to a one this will get you the Hawk's answers so we're going to test statistic of 3.25 a pvalue of 0.1969 here it is using r kaisk.test so you give it the observed distribution of counts and this is important this is a distribution of counts and you also give the hypothesized distribution but these are going to be probabilities so instead of 200 200 200 you give it onethird onethird onethird just so happens in this case you do get the same chisquared test statistic degrees of freedom and pvalue as if you did it by hand using the Hawks method example two my friend claims that the proportion of cars in the Knox campus that are American is the same as the proportion that our European and as the proportion that are Asian okay so to test this I went to the parking lot across burying from smack and counted the cars and their Origins and had a conversation with a couple people who were wondering what I was doing over there I on that day I there were 19 American 23 Asian and two European cars so the observed and expected values are 19 The observed is 1923 and 2. the expected is 44 thirds 44 30s 44 thirds those are expected counts why is it thirds that's because they're the same proportions so the proportion of Americans equals the proportion of Europeans equals a proportion of Asians onethird onethird onethird where the 44 come from there are 44 cars so let's go ahead and calculate this out three groups so the test statistic this is the formula which is expanded to this plug and chug and chug and chug are we done chugging there we go test statistic is 16.954 here's the distribution here the chisquared distribution with two degrees of freedom there are three groups so it's three minus one two degrees of freedom whereas the pvalue wait where's the test statistic test statistic was almost 17 so I guess test to sits way over here pvalue .0002 because the pvalue is less than Alpha we reject the null hypothesis the proportion of cars on campus is not the same for American Asian and European that's all we can say is it's not the same this is not the distribution because the null hypothesis in the research hypothesis for chisquared goodness fit test is this is the distribution the alternative would be this is not the distribution um what does this conclusion assume and it's time we start thinking about this again notice how I collected the data I went across the street from smack and went through the went through the parking lot for this to be an appropriate conclusion the data I collected has to be representative of the cars on campus cars in that one parking lot has to be representative of all the cars on campus the cluster sampling I did has to be appropriate remembering back to chapter one since it's cluster sampling I have to somehow assert or check that the proportion of cars that are American European Asian is independent of the parking lot huh how could we test that and I'll give you a hint we don't know yet because we would be testing for Independence between a categorical variable car type and a categorical variable parking lot so be aware that'll be a future test here is the code again three groups degrees of freedom is three minus one we can also do this with the test notice I did not specify comma P equals onethird onethird onethird here's why by default R assumes equal probabilities so if you are testing equal probabilities you don't actually have to specify that for r you can I encourage it because it increases the readability of your code but you don't have to in fact I strongly encourage it because it does make very explicit that you are testing equal probability amongst those three groups sample three Department of Mathematics claims that the proportion of its graduates who went to grad school is twice the proportion of any other postback path to test this department sent out a questionnaire to all of the alums who for whom they had current addresses here's a table of the results that includes the count and the expected we had 35 that we could get in touch with um notice we said twice the proportion of any other post back so 14 expected for grad school but seven for business seven for Education seven for unemployed by the way this is fake data we actually counted 14 grad school seven business 10 education five unemployed so this will be the vector of counts observed and this will be a vector of counts expected this is how we calculate it by hand notice we now have four groups here's the chisquared distribution with three degrees of freedom K is 4 K minus 1 is 3 degrees of freedom here's our value of the test statistic we got the Shaded region is our pvalue pvalue of 0.5874 because the pvalue is greater than Alpha we fail to reject the null hypothesis we do not have evidence that the data do not follow the distribution the claimed distribution the claim made by the math department that twice as many of its graduates go to grad school is in any other category is reasonable note that we did not prove that the math department is correct we just said their claim is reasonable given this data given additional data it may not be reasonable but given this particular data it is and then of course I'm going to ask what does this conclusion actually assume so I want to start tying back to chapter one more and more not just subtly but very explicitly how did the math department get this data is this data representative of all math majors in other words who would we have kept in touch with or who would have kept in touch with us is it equally likely that any graduate of the math department would have kept in touch with us or are certain types of students like grad students more likely to keep in touch with us or like education students more likely to keep in touch with us notice that we send out a questionnaire to all of the alums for whom we had current addresses Who Are we more likely to keep current addresses for who is more likely to fall off the grid this is how you do it for Hawks notice we got four groups so the degrees of freedom will be three notice I did the p is equal to C of 1477 divided by 35. I could have done CF 14 over 35 comma 7 over 35 comma 7 over 35 comma 7 over 35 this is a little bit faster one last example one of the initiatives of Knox College has become more representative of the US population this raises the question of whether we have succeeded in terms of numbers according to the fall 2019 domestic numbers the data are observed expected and the expected is due to its Compares aha let me start that again expected proportions are from the Census Bureau so we have 109 71 194 and 635 we'd expect 163.58 73.10 238.37 and 555.61 if we perfectly followed here's a graph notice what the the boxes with the lines are what we actually are and the dots are the population of the United States notice we are below average here we're dead on here we're below here and we're way above here by the way the boxes indicate confidence intervals the horizontal line is the proportions the sample proportions so here we are using r chisquare tests that's the observed counts this is proportions according to the Census Bureau I run it got an error the error is probabilities must sum to one what's the error well these probabilities don't add up to one why don't they add up to one well there's a couple possibilities one is I may have dropped a couple of the smaller groups two there could be rounding errors but for R to do this these probabilities have to add up to one so the question comes down to how do we fix it if we actually did leave out some of the smaller groups or if there is rounding how do we fix this R does have a parameter rescale dot p we set it equal to True it'll rescale these so that they do add up to 100 percent so this will fix the error gives us a pvalue that is significant is much much less than Alpha we have evidence that the distribution of races and ethnicities in Knox is does not match that in the population at large of the United States have her realize that this pvalue corresponds to a snapshot in time it doesn't compare where we were 10 years ago it just says where we were fall of 2019. so we weren't there we didn't match the distribution in the United States in 2019 but we actually are getting much closer to Ria to the population in the United States much closer so the pvalues are getting larger the test statistics are getting smaller and that's what this conclusion assumes so let's go ahead and do the introductor questions there we go huh this looks familiar question one when is the research hypothesis the same as the alternative hypothesis question two I like question two and question three so they're the same question essentially but I I want two examples question two is Give an example where you would use the chisquared goodness of fit test in your area of Interest so if your major is or will be political science give me a time when you would use the goodness fit test in political science if your area of interest is or will be biology give me an example where to use the goodness of fit test in biology or in physics if you're going to be a physicist or in uh I can't think of any other areas and so if you're going to be an anthropologist a sociologist that's question two is give one example question three is to give another example so question two and three are give examples of where you would actually use this chisquare goodness of it in your discipline and this actually could be in classes that you've taken in your discipline for instance in chemistry reaction rates be no that wouldn't work for instance chemistry we've had classes in chemistry you may have needed to use a chisquared goodness fit test so in today day Slide Attack ah let me start that slide again so in today's slide deck we covered procedures for testing if the observed categorical data came from the hypothesized distribution that's it I'm in the future we're going to look at the chisquare tests of Independence we're going to look at analysis of variance or you do linear regression those are the three remaining tests um all procedures take advantage of the sca's please take advantage of the scas for work for practice we only had one r function.test remember X and P are the numbers of successes and the probability of successes in each of the groups respectively SCA 32 is going to be very helpful here two for proportions which goodness fit test does talk about proportions and three more than two samples there's nothing in R for starters for this Hawks learning this is section 10 6. that brings us to the end um that's it um so bye hello and welcome to chapter 10 where we cover a lot of the hypothesis testing not all of it but a lot of it and here we're looking at categorical Independence section seven uh categorical Independence means that you're testing for Independence between two categorical variables um contrast this with testing Independence between two numeric variables which will be linear regression which will be in the future this lecture is about two categorical variables and that's really what we're doing still understanding the theory behind in hypotheses about determining if two categorical variables are independent and better understanding the pvalue and how to test hypotheses parametric procedure is the chisquared test of Independence it's called a chisquared test because the test statistic follows a chisquared distribution they're called ttests because things in the past that we've covered are called ttest because their test statistic follows a t distribution things in the past have been called ztests because their test statistics follow a z distribution we saw an F test at one point because the test statistic follows an F distribution here this is one of many chisquared tests and they're called chisquare tests because the test statistic follows a chisquared distribution the null hypothesis is that the two categorical variables are independent the graphics a matrix plot or a graphic is a matrix plot resign yourself to the fact that graphics for chisquare tests of Independence all look ugly and are difficult to interpret um this requires expected number of successes to be at least five in each cell uh or 10 in each cell or I mean the reason for this is It's a normal approximation of the binomial distribution and so larger sample sizes means that that normal approximation improves Central limit theorem Section 5 6 no 6 5 . this function is indeed what hawks covers so you won't have to change anything to to your hux homework with this however for R to give you Hawks results you'll need to use correct equals false in the function and I'll show you where to do that so let's go ahead and look at a framing example I would like to test if there's a relationship between whether a person has blue eyes and whether that person is a math natural science major yeah to do this I asked 100 people at Knox College so I'm dealing with one large sample of 100 people and I'm measuring two things on each person one if that person has blue eyes or not blue eyes and two if their major is in MNS or not in m s so here's the contingency table notice that each of these is a count so in my sample seven people had blue eyes and were M and S majors 20 people 7 plus 13 20 people in my sample had blue eyes the 52 people did not have blue eyes and were not math Natural Science majors 80 people 28 plus 52 did not have blue eyes 35 people were math Natural Science majors and whatever 13 plus 52 is I think that's 65 were not math Natural Science majors and adding them all together gives you a hundred the number of people that I asked so contingency table is a table of counts so let's us let's ask ourselves this question what does it mean for two categorical variables to be independent recall back to chapter four that really fun chapter where it covered Independence at one point that is exactly what we're going to be using to create our expected counts so we'll have observe counts expected counts and when you've got those two it's just a hop skip and a jump to your chisquare test um so what does it mean it means that the value of one does not affect the value of the other particularly for this it means that the probability of a math natural science major does not depend on whether you are blueeyed we can also frame this as the distribution of M and S Majors is the same for blueeyed people as it is for nonblueeyed people but you could also frame this as the distribution of blueeyedness is the same for mathematical science Majors as it is for nonm S majors let's check with the data for this interpretation here's the counts so what's the proportion of blue eyed people in this sample 7 plus 13 over 100 so 20 percent what's the proportion of blue eyed people who are mathematical science Majors it's seven divided by 35. this is the number of blueeyed people who are math Natural Science majors and this is the number of math Natural Science majors 20 it's a match so if I tell you I'm blueeyed that does not affect my knowledge about you being a math natural science major notice we could do the same thing proportion of blue eyes in the sample versus proportion of blueeyed people who are not math Natural Science Majors again it's 20 percent so if I tell you I'm blueeyed that should give you no information or that does give you no information about whether I am a math natural science major because the probability to do the same or whether I'm not a math natural science major probability is the same we can do this with nonblloid people proportion of nonbloid people in the sample is 20 plus 28 plus 52 which is 80 divided by our sample size of 180 percent proportion of nonbloid people who are m s Majors is also 80 28 divided by 35. so if I tell you I'm not blueeyed I'm not giving you any information about whether I'm a math natural science major X page or not because the proportions or the percents are the same so me giving you information about my ID eye color doesn't give you any information about my major in other words my eye color is independent of major now that we've seen a small example let's go ahead and generalize this here's our data variable one is a variable two is B variable one has two levels A1 and A2 the B variable has two levels B1 and B2 x 1 1 is the number of people the actual counts who were A1 and B1 X12 is the actual counts of people who are B1 and A2 x21 were the actual counts of people who were B2 and A1 and x22 is B2 and A2 count I'm going to scroll back to our data I only have to go back to here this would be a this would be X11 x 1 2 x 1 3 x 1 no I got that wrong X1 1 x 1 2 x 2 1 x 2 2 there we go the variable a is eye color the level A1 is blue level A2 is not blue the variable B is Major level B1 is math the natural science major level B2 is not math natural science major now we're going to create column and row sums we'll number the row sums R1 and R2 R1 will be the number of people who are B1 R2 will be the number of people who are B2 column sums will be the number of people who are A1 and C2 is a number of people who are A2 going back a slide so R1 will be 7 plus 28 there are 35 B ones R2 will be 13 plus 52 there's 65 b2s C1 is 7 plus 13 so there's 20 a ones and there's 80 a2s the total sample size will be the sum of everything here or it'll be the sum of the row sums or it'll be the sum of the column sums n's 100. so here's the data that we were working with here's abstraction of it in other words we've got our counts this is observed now we've got to figure out how to get expected if the two variables are perfectly independent and again refer back to section 4 3 the expected values would be although their expected values to be n times p so we got the ends common for all of these p is the probability of being in this cell if the two variables are independent that would be the probability of being in row one times the probability of being in column one if they're independent here this would be the probability of being in row two times the probability being in column one row two column two Row one column two those would be the probabilities multiplied by n to get NP which is the expected counts so this will be the table of expected counts you've got the table of observed accounts table of expected counts and we know what to do with that for a chisquared goodness for chisquared distribution observed minus expected squared over expected we're going to call this X2 instead of TS chisquared distribution as one parameter called the degrees of freedom for the goodness of fit test it was groups minus one for the test of Independence it's going to be rows minus 1 times columns minus one rows minus 1 times columns minus one in this example we've been working with there are two rows two columns the degrees of freedom will be one two minus one times two minus one example one of the actual examples we'd like to determine the proportion of males to wear hats is the same as a proportion of females who wear hats to test this I sample 100 males 100 females you know this is sounding familiar uh 10 males 16 females were wearing hats oh yeah we have seen this comparing two proportions let's also look at this in terms of a test of Independence Independence between gender and hat weariness so doing it the long way here's the table of observed values 16 females had a hat 84 did not 10 males had a hat 90 did not row sums are 100 and 100 columns sums are 26 and 174. if the two are independent then we would expect the proportions of females with Hats the proportion of males with Hats to be close to each other and close to 26 over 200 here's the table of expected values again in all the painful Glory n times R1 Over N times C1 over n times r 1 over n times C2 over n Etc so these are observed counts and these should be expected counts doing the math it actually comes out kind of nice it took a lot of effort to get it to come out so nice um so here's the test statistic value observed my suspected squared over expected we observed 16 we expected 13 we observed 84 we expected 87 observe 10 observed 90. we had a chisquared test statistic of 1.5915 we need to compare this test statistic to the chisquare distribution with one degree of freedom R is 2 C is 2. here's how we'd actually calculate that so the pvalue is one minus because it's greater than or equal to 1.5915 degrees of freedom is 1. gives us a pvalue of 0.2071 after all this fund calculation because remember we also had to determine the expected values here or we can use r if we add correct equals false then we'll get the Hawks result if we leave the correct equals false out we'll get a better result the first step is to create the Matrix of observed values The Matrix of observed values here's how we do that we use the function Matrix the first thing we give the Matrix function is the counts and then we specify how many columns here notice that the counts are going in by row let me go back 1610 84.90 16 10 84.90 so this line will actually give you the observed Matrix going back this thing right here as a matrix and then all we have to do is a chi got tests of that observed Matrix if you're doing it for Hawks we do comma correct equals false this first line that is the most difficult but it's just the Matrix function this the the values by row and then you specify the number of columns if we do the r we leave off the correct equals false here's the output for r got a pvalue 0.2931 degrees of freedom of one The observed value of 1.1052 because the pvalue is greater than Alpha we fail to reject the null hypothesis we do not have evidence that the hat wearing rate for males differs from the hat wearing rate for females similarly we could say we have no evidence that the gender ratio for hat wearers differs from that for nonhat wearers both are actually equivalent interpretations equivalent interpretations if one is true the other is true notice the difference between the two tests is minor when the sample size is large but we have 100 or so sample size pretty large so this is with the continuity correction if we did it with correct equals false we have it without example three I'd like to determine if females have a different grade distribution my stat tone of course is than males this is data from I forget what uh I think my first three years here so gender this is A's b c's D's and F in the in the class yes it is fake data don't worry um so if all my past students 57 were females who got A's 68 were males who got B's 40 were females who got these and 22 were females who got F's so I would like to determine if the grade distribution for females is this is different than that for males in other words what I want to determine is is the grade distribution dependent on gender is the grade this grades a categorical variable independent of gender categorical variable there are a lot of ways of interpreting this and they're all logically equivalent so to do this we just got to put get these numbers into a matrix 57 49 7868 Etc and do a test on it it will automatically tell us there are four degrees of freedom how did I get four degrees freedom two rows five columns R minus one times C minus 1 gives us four putting the Matrix in specify the number of columns is five we did it by column this is the Hawks result because I specified correct equals false pvalue is 0.9851 because the pvalue is greater than our usual Alpha of 0.05 we cannot reject the null hypothesis there is no evidence that the grade distribution differs between females and males similarly I could say hey I have this student the student got AC well guess what I didn't give you any information about the student's gender because information about the grade is independent of information about the gender so let's go to the intra lecture questions this looks familiar when is the research hypothesis the same as the alternative hypothesis should have this by now question two I want you to give an example where you would need to test independence of two categorical variables in your area of Interest two categorical variables and this is for our lecture today you would be able to use the chisquare test for Independence for this example question three is Give an example where would you need to use tests for independence of two numeric variables in your area of interest you do not know how to do this test yet this is not from today today was just too categorical independents in the future we'll learn how to do two numeric Independence but I want you to start thinking about okay where would I need to test for independence of two numeric variables so we learned how to test if two categorical variables are independent today future we got Anova and linear regression kaisk test performs a chisquare test of Independence m is a matrix the sca is number 41 there are no readings in our starters Hawks has section 10 7. and the usual don't forget the all procedures make sure you have a page for each of these and that's it hello and welcome to the analysis of variance this is the most important part of chapter 11. analysis the variance is used to test for independence of a categorical and a numeric variable that's one use of Anova the usual use of Anova is testing for equality of population means amongst more than two groups so by the end of this lecture you should be able to understand the theory behind testing the means of more than two populations and the independence between America and a categorical variable and again better understand pvalues and how to test hypotheses so let's do a frame example here I like to test if the average GPA of a student is the same for the four types of majors at Knox m s is math and natural science HSS is Humanities and social sciences hum is Humanities because it's history and social sciences and art is all the art stuff so to do this I asked 200 students at Knox College 50 of each major type and asked two questions one what is your major type two what is your GPA so note that we have one population students at Knox College and on every member of that population I asked two questions one what is your major type and two what is your GPA if we're looking at a relationship between the two we're actually testing for Independence between those two variables major type is categorical and the GPA is numeric so here's a box plot side by side box plot of the data I collected each dot represents a student that I talk to Green Dots or MNS majors horizontally corresponds to the reported GPA the dots are included as is or as are the box plots for each of the four major types so there's actually a few equivalent ways of looking at this question and as you've learned from your math courses throughout the years the ability to look at a single question from multiple standpoints is always a strength and that strength leads leads us to be able to in this case determine what the actual test should be so one way is do the means in each group significantly differ in other words is the mean for math and natural science which is probably located somewhere around here is that significantly different than the mean for history and social sciences which is probably down here somewhere and for humanities which looks like it's here somewhere and art which looks like it's here somewhere so the first way of looking at this is comparing the means of the individual groups second way of looking this is are the group and the GPA independent these two questions are logically identical are the group and GPA independent and the Third Way is does including the group identifier improve our ability to estimate a person's GPA and since we're looking at the mean the expected value what we're looking at is actually testing okay does the information of group improve our understanding of GPA thinking back to chapter four that's equivalent to saying our group and GPA dependent or independent think back to the category the conditional probability definition of Independence and it's this last one that gives us some insight into the test statistic what improving predictions implies is that we reduce the uncertainty in those predictions and reducing uncertainty means we reduce the variance and those predictions and this is the idea behind the analysis of variance procedure first thing you do is measure the variance of the original data to measure the variance that is left over after you include the model and by model I mean the group identifier and then you look at the ratio between the two from the explained to the Unexplained and it's this last ratio that's actually the test statistic now let's think about that for a second if the explained variance the variance is explained by the model are contained in the model or taken care of by the model is large compared to what's left over the Unexplained variance then the model is good because the remaining variance is small compared to what you started with the model is explaining a lot of the uncertainty in the dependent variable if on the other hand the explained variance is small in other words if the model doesn't explain much of that dependent variable then the model is is virtually worthless and it's that ratio which is an F ratio it's called an F ratio because the test statistic follows an F distribution it's this F ratio that leads us to a pvalue which leads us to an interpretation of those results so that was the theory let's go through the calculations by hand probably the only I think we get one more time when we do it by hand but so with that background let's calculate the test statistic and the pvalue using the Anova table and here's the blank Anova table this is a this is what an anova table looks like there's three sources the model what's remaining after you apply the model and then what's originally there so model is the model that you apply which is the independent variable of the grouping variable error is what remains after applying the model and the total is what you start with SS is sum of squares so this box will contain the sum of squares for the model this box will be the sum of squares that remains and this box will be the total sum of squares or the original sum of squares note that this box plus this box will give you this box this will be the degrees of freedom for the model degrees of freedom for the error and then the total degrees of freedom and the total degrees of freedom will be just the usual the total sample size minus one and again note that this box and this box will add to this box Ms stands for mean squared so this would be the mean squared error for the model sorry this will be the mean squared for the model this will be mean squared for the error the mean square is just the ratio of the sum of squares to the degrees of freedom I mean we saw that back in chapter two or chapter three when we were looking at the variance calculations it was the sum of the squares divided by the degrees of freedom we just didn't use term degrees of freedom back then it was n minus 1. this F ratio is going to be this box divided by this box and this pvalue will be a function of this F ratio according to the F distribution so let's fill it in and you get to see all the calculations the column marked SS contains the sum of squares for those three sources the sum of squares is just the sum of the deviation between the observation and the mean notice the structure of all of these is the same you're adding up over all the data values that's what the double summation is you're adding up within the group to the grand mean so this is what's explained by the model the the mean within the group difference with the grand mean or the mean of all the data values what's remaining is the variation within the group the data value to the group mean and this is the original this is the data values to the group mean back in the day this was x minus X bar squared add it up here we're just renaming x and x bar so again this is what's explained by the model this is the group mean minus the grand mean this is the what's left unexplained the variance within each of the groups the data value to that group mean and this is what you started with because each of these calculations require 50 sums differences in squares calculating by hand it's not really realistic so I'm just going to give you the answers for this data notice I didn't actually give you the data itself here's a sum of squares here's what's here's the error that's remaining here's the total notice the sum of squares for the model plus the sum of squares error is going to add up to the total the important reason for that is that means that the SS model or the the sum of squares for the model and the sum of squares for the error are independent of each other doesn't mean much for us in stat 200 but it will be important in later set courses call Mark DF contains the degrees of freedom for the three sources there are the parameters that reflect the amount of information contributed by each Source probably believe the best definition of degrees of freedom the degrees of freedom tool is just the total sample size minus one so it'd be 200 minus one because I asked 200 people the F model is the number of groups minus one there's four groups four different major types so DF model will be three and then the way I calculated is just DF total minus DF model that'll give you the if error 3 and 199 and the DF error will be 196. the mean squared will be the sum of squares divided by the degrees of freedom now this should look should refresh or should look very similar to what we got back in chapter two with the variance sample variance it was 1 over n minus 1 times the sum of x minus X bar squared calculating the same thing here this is an estimator of a variance just like s squared was an estimator of a variance so mean squared model is 2.6104 mean squared error is 0.4718 and this F statistic is going to be the mean squared model divided by the mean squared error as you can as you should guess by now the distribution of the F statistic IS F that's the name of a distribution do you know what the name that distribution is named after Fisher John Fisher one of The Luminaries of early statistics in the 20th century there is f it's 5.533 last box we'll calculate the pvalue pvalue is calculated in exactly the same way it's a probability of being of the test statistic being this extreme or more so given the null hypothesis is true if the null hypothesis is true F will be zero I mean if the null hypothesis is perfectly true this F ratio will be zero because there will be no uh this number will be zero this number would be zero remember the null hypothesis is there is no difference in the means or the null hypothesis is that the two variables are independent or the null hypothesis is the model explains none of the variation the model explains another variation this is going to be zero which means mean squared is going to be zero which means the F ratio will be zero over something which is zero so this is the probability of the distribution being greater than or equal to 5.53 which looks like this here's the F distribution for this particular problem pvalue is way out here it's this area that's hard to see 0.00115 how do we interpret the pvalue absolutely correct same as always compare it to Alpha if the pvalue is less than Alpha as in this case we reject the null hypothesis that means all of the following three things the two variables are not independent and that is GP average that is GPA and um GPA and major type those are not independent because p is too small it means the average GPA in the four groups is not the same and it means if you want to model the GPA if you want to explain the GPA then including the major type will help with that okay let's look at the rice yields I'll give you the data so we can see how all the calculations are done does rice variety influence the average yield amongst these four varieties here's the yield in each of the four plots for each of the four varieties so we got 16 plots in each plot we planted one of the four varieties of rice so there's the raw data here's a graph sure seems like variety D is much higher than the rest I don't know about variety a but it certainly seems as though at least one of the four varieties has a different average here's the blank Anova table let's calculate this all together just so we can see oh wow we really don't want to do this by hand so here are the formulas for SS model SS error and SS total from here so here's SS model we're adding up over all the data values the average in the group minus the grand mean squared but this actually translates to is this because there's 12 data points so we've got 12 terms from the data this is what we get the grand mean is 9981.9375 and the individual group means or variety means are those now we just plug and chug this term becomes 984.50 minus 991.9375 squared plus Etc substitution do a lot of calculations and we get 89 931. so the sum of squared for the model is 89 931. we can do the same thing with the error and total we could just calculate the total which is the variance of the data and subtract off the model to get the error degrees of freedom we've got 16 data points so DF total will be 16 minus 1. we've got four groups so DF model will be four minus one and then DF error will be 15 minus three mean squared is just the sum of squares divided by the degrees of freedom that 2 should not be there the F ratio is just the ratio of the mean squared of the model to the mean square root of the error so this would be about 30 divided by 4 should be about seven and a half this is the F ratio and it follows the F distribution that's why it's called an F statistic it follows the F distribution and now this pvalue is a probability of this distribution being greater than or equal to 7.212 . there are 7.212 this shaded area is the pvalue 0.00503 interpret the P bet value in the usual way compare it to Alpha if the pvalue is less than Alpha reject the null hypothesis in this case the null hypothesis is that the four means are equal or that the yield variable and the variety variable are independent or if we're trying to model the yield of wheat was it wheat or corn if we're trying to model the yield of whatever grain this is including the including the variety will help those three are equivalent the one you use depends on what you're trying to understand about rice I guess it's rice instead of corn or wheat now I'll do it in r three lines to get the data in it's the rice data set summary just to make sure that you loaded it in correctly and attach it that's what the box plot looks like after putting it up a little here's the code for the fancy box plot box plot line is actually way down here this will just get you a nice plain box plot here's the two lines of code to do all of the Anova calculations that you need to do we'll call it rice mod it's the model about rice the function is aov analysis of variance in parentheses this is the dependent variable that's the tilde to the left of the one and this is the independent variable the dependent variable here is the numeric variable and the independent variable is the grouping variable or the categorical variable this rice mod line doesn't do much of anything except everything inside it's the summary that takes it from what's inside R to actually put it in a form that is Meaningful for you notice it doesn't give the total stuff but we can figure out degrees of freedom total is 15. the sum of squares total is just the sum of those two there are four varieties so the degrees of freedom for variety is four there's a sum of squares the mean squared is the sum of squares over the degrees of freedom the F value is the ratio of the mean squared variety to the mean squared residuals which is often called error instead of residuals here's our pvalue notice we've got two stars on it two stars is down here so that pvalue is between .001 and .01 between U because we got the pvalue there and that's it just those two lines once you get the data in just those two lines interpret same conclusion Ronald Fisher introduced the Anova procedure in his 1925 book and to illustrate Anova he came up with this experiment collect a sample of pond water okay so you take your five gallon bucket and dip it in the pond and you've got a sample of pond water now divide that water in that same bucket amongst four different beakers now separate the beakers to ensure that there's no cross contamination and from each Beaker take four samples and counter record the number of amoeba present so you got this sample of pond water dip four beakers in that same sample so you would expect the amoeba concentration be the same in those four beakers and now from each speaker take four small samples and count and record the number of amoeba so you'd expect the averages in each of those four samples from each of the four beakers to be about the same so we would expect the pvalue of this experiment to be rather large data is available in the Fischer 38 data file this loads it attaches it this does the analysis of variance and the summary gives us these results we got a pvalue that's rather large notice that when the F value is small the pvalue is large and when the F value is large the pvalue is small it's because the larger the F value the more extreme your observations are if the null hypothesis is true and the null hypothesis is either all the means are equal or the the two variables the numeric variable and the grouping variable are independent or that grouping variable gives us no information about the numeric variable there's that F distribution there's the pvalue in Darker blue it's not small we didn't expect it to be small there's the conclusion and at this point we should be asking do these results make sense for every single analysis the results either make sense or we are learning something new or we did something wrong so here's the summary slides but I'm going to throw in the intro lecture questions here so the first introduction question does a large value of f correspond to a large pvalue or to a small pvalue does a large value of f correspond to a large pvalue or a small pvalue and again I would write the question your notes on the left answer below question two what is the null hypothesis in Anova notice I've given you three options for this give me any of those three and three here's a given example where you would need to test for the independence of one numeric variable and one categorical variable any example will work any good example will work so here's what we did in today's slide deck we covered Anova this procedure helps us determine three things if the mean of several groups are the same if a numeric and a categorical variable are independent and if knowing a group membership helps with estimation of the dependent variable all three of those are logically and statistically equivalent the way that you frame your results will depend on what the original research hypothesis and or research question are if the researcher is asking about the means of separate groups then that's the one you're going to use to interpret your Anova in the future we're going to do linear regression same reminders as always create that section in the notebook dedicated to the tests and the assumptions of that test take advantage of the scas and use the all procedures handout um there's some functions that we are hinting at the aov function we did in the summary function we did you've already seen the Shapiro test function the flickner test function is what we're going to use in the next set of slides notice that all we were able to conclude was the mains are all the same or at least one mean is different we weren't able to determine which mean was different next set of the next set of slides will cover that we looked at we could also conclude that the two variables are independent or they are dependent we weren't able to classify what type of dependents the next set of slides will help with that however the fligner test will need to be used to test one of the assumptions of Anova but that will be for next time and there are this the readings and that's the end and I hope this was fun or at least helpful