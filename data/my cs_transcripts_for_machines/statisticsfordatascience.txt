welcome to our first video of our statistics series this video is going to really introduce the idea of statistics and answer the question what is statistics and as we start studying statistics there is some key vocabulary that we need to be very comfortable with in order to study statistics accurately and communicate our results accurately so the first word we need to know in statistics is what we call the population the population is basically everything or everyone we usually are talking about people but not necessarily being studied maybe we're investigating college students the population would be all college students number two the second word we need to know is what we call a parameter and a parameter is some characteristic of the population if we're talking about all college students the parameter might be the average gpa that be a parameter of the population when we're talking about parameters just as a note for now we will usually use greek letters to represent parameters you'll see things like the greek letter mu or the greek letter sigma something in greek generally means we're talking about a parameter of the entire population this is different than if i was talking about a sample a sample is a portion of the larger population so if i went out and said we're studying college students and their average gpa the sample might be i took a sample of 200 college students that would be my sample and then with a sample we have these things called statistics hence the name of our course a statistic is a characteristic of the sample so this might be the average gpa of the 200 college students i interviewed and to differentiate samples and populations statistics and parameters with statistics we will use english letters these english letters would be something like what we'll call x bar or just the letter s so as we're looking at statistics of samples which hopefully will estimate the parameter of a population we're also interested in what are called variables and these are not the variables of algebra a variable is any characteristic of interest gathered from each item in the sample another way to think about the variable is it's really when i'm conducting a survey it's really the question that is being asked so for example with our college students we'd be asking them what is your gpa our variable is the gpa and then our last word for this part will be what we'll call the data or the actual values of the variables so a data for our example would be something like 2.38 a 3.47 a 4.0 that is the data so let's see if we can use this key vocabulary in an example see if we can identify the key vocabulary you want to know the average cost of statistics textbooks so you survey 25 textbooks and we're going to find and identify these key terms these same six key terms the population the parameter the sample the statistic the variable and the data first the population the population is everything that we're possibly interested in so we're talking about statistics textbooks the population would be all statistics textbooks and the parameter that we're interested in studying about those statistics textbook is the average cost the average cost but not just the average cost but the average cost of all statistics textbooks notice how i tie it back to the population because that's what the parameter describes that can be contrasted with our sample slightly different now our sample is the smaller group the subset we're looking at this is the 25 textbooks and then the statistic has to describe that sample it's our it's our characteristic of interest for the sample it is the average cost of and then tie it back to the sample the 25 textbooks now the variable that's the information i'm gathering the variables when i look at each textbook what am i recording or the question what am i asking with the variable i'm asking the cost of a statistics textbook and the data is the answer to that question or the actual values of the variables so it's the actual cost of the textbooks an example might be you find an expensive one for 235 dollars that is a piece of data to the variable and answer to the question all right now that we have some vocabulary let's move on to an example of actually summarizing data which is what statistics is all about with statistics we're often interested in what is called the frequency of an event or thing and the frequency is just how often a value occurs and often we'll organize these frequencies in what we call a frequency table and to set up a frequency table we have a little bit more vocabulary first is what we're going to call the relative frequency and the relative frequency is just the proportion of times a value occurs in other words it's the decimal equivalent of the frequency divided by the total and often with frequency we're interested in what's called the cumulative relative frequency and that is the sum of all previous entries so with that vocabulary the way we're going to set up the actual frequency table is frequency tables we'll generally have four columns one column for the data value a column for the frequency which i'll denote with just f a column for the relative frequency which i can denote with rf and then a column for the cumulative relative frequency crf and so then we fill in our data values maybe one two and so on actually let's just do one and two and then we'll do the frequency maybe the first the one appears three times and the two appears seven times so out of 10 the relative frequency is 3 out of 10 or 0.3 and for 2 the relative frequency is 7 out of 10 or 0.7 and the cumulative relative frequency will start to add all the previous values so 0.3 plus 0.7 is 1.0 for the last column interesting to note is the last entry should sum to 1. now maybe if you have a round off error it might be 1.00001 or 0.99999999 that's okay but generally speaking we hope that cumulative relative frequency should sum to 1. that's all to show us how to set up the table let's actually make a frequency table let's do an example here let's say a baker keeps track of how many free donut holes his customers eat 25 eat one donut hole 15 eat two donut holes seven eat three donut holes and three eat four donut holes let's make a frequency table we have the values of one two three and four are the number of donut holes that were eaten for the frequencies we know 25 eat one donut hole so that's our frequency 15 is the frequency for two donut holes seven for three and 3 eat 4. now if we want the relative frequency what we have to do is divide the frequency by the total so we need to know what the total is and so if we add these up we get a total of 50 customers so when we divide 25 by 50 we get a relative frequency of 0.5 when we do 15 divided by 50 we get a relative frequency of 0.3 7 divided by 50 we get a relative frequency of 0.1 4 and 3 divided by 50 we get a relative frequency of 0.06 and the fractions aren't needed so much as the actual decimal answers in our table and then once we have our relative frequency we can find the cumulative relative frequency by adding all the values before that so for 1 we only have the 0.5 but then for 2 we're going to add 0.3 so we add point three to get point eight and then we add the next value add point fourteen to get point ninetysix and then we add .06 to get 1.00 and that fills in our frequency table now that we have our frequency table we can answer some questions about this baker we could answer questions like what percent 8 between 2 and three donuts two and three donuts have a relative frequency of 0.3 and .4 so when we add those together 0.3 plus 0.14 the percent is 0.44 or as a percent 44 percent how about what percent eight more than three well i'll highlight in pink more than three just means four so that must be this last entry of .06 and so we can say .06 or six percent finally what percent ate at most three well i'll mark them in blue here at most three as everybody else i could add those all together but what you might notice is that everybody else excludes the six percent so it might be easier to say we've got a hundred percent as everybody exclude the six percent that are more than three and that leaves us with 94 percent eight at most three donut holes so the big thing we're doing today is we are looking at statistics vocabulary and organizing data in frequency tables and interpreting that information take a look at the homework assignment that goes with this section and in class we will investigate these frequency tables a little further statistics is all about the interpretation of data so we're going to take a look today at the question how is statistical data collected and first we have to really understand what we mean when we talk about data data can be measured on several different levels so let's take a look at the levels of measurement levels of measurement start with the most simple type of data and grow to the most complex type of data it's important we know what we're working with so we make sure we can do the correct mathematical operations of it and make sense of our conclusions the most foundational level of measurement is what is called nominal which basically just means we put things into categories you might say there are no numbers so for some examples of nominal categories you could look at color or maybe a yes no survey do you support this political issue or maybe some type of label or gender those are nominal categories now slightly more involved than a nominal category is something that we can actually put in order and say this one is more than that one which is more than that one we call this ordinal data and that's data that can be put in order however with that order there is no clear space between the data values in other words we can say a comes before b but we don't really care how much before b the space between a and b might be different than the space between b and c an example of ordinal data might be finishing place in a race there's a clear first second and third but the space between first second and third is not necessarily well defined that's ordinal or we might say the top five cooks in america cook one two three four and five that's ordinal we can put them in order but we don't necessarily know how much better each one is than the other now when we do clearly define the space between values we have what we call interval data interval data has a space between the numbers with meaning the space has meaning however there still is no zero point the space between uh the two numbers is well defined like in the example of temperature 30 degrees is 10 more than 20 degrees just like 90 degrees is 10 more than 80 degrees that space has meaning and while temperature does have a zero zero degrees doesn't mean the absent of temperature or no temperature it's just a point along the number line that has the same spacing as all the other temperatures now if we want to have a clearly defined zero that's when we have ratio data and that's where we have an absolute zero that means literally nothing or none an example of ratio data might be the length of a phone call or a test score or the number of children if we said we got a zero on a test or we have zero children that actually means nothing the lowest amount possible now these different levels of measurement can show up in different types of data let's scroll up it's very important we understand the two different types of data and the second one can actually be broken up into two subgroups the first type of data is what is called qualitative qualitative data which really focuses again on categories no numbers we're describing the qualities of something so for example we might be talking about the type of car or maybe some type of ethnic group those are the qualities of the data that's qualitative data the second type's the one we work with the most in statistics and that is what is called quantitative data and quantitative data is when we're looking at quantities quantitative data is either measured or counted really what we're talking about is numbers how many how much how far how long some examples of these might be the number of students or the distance to school quantitative data can even be divided up further quantitative data can be either discrete or leave a space continuous discrete data is data that is countable one two three four five when we count things we say we have discrete data generally we don't get decimals with discrete data examples might include the number of shoes a person owns you're not going to end up with decimal shoes you're not going to have 1.4 shoes discrete date is countable you count the number of shoes there are that's contrasted with continuous data which is measured which means every decimal and every decimal in between those decimals is possible measured data example might be the length of a phone call or maybe somebody's height that's continuous data it's measured always we can have decimals you can have half an inch you can have half of a half of an inch or a quarter you can have an eighth of an inch you can do all the decimals in between that is continuous data so that's the different levels of measurement we can do with different types of data but we haven't answered the question of how do we collect our data which is what we said we wanted to do at the beginning so let's take a look at actual sampling of data how do we collect it when we collect data in a sample we want it to be representative of the entire population in order for it to be representative of the entire op population we need the data to be random to avoid any bias in other words we want all options to be equally likely to be included in our sample so if random is best let's take a look at a few random sampling methods the first random sampling method is what we call simple random sampling simple random sampling is random selection methods such as random numbers or drawing out of a hat is the idea of i assign everybody a number and i pick a bunch of random numbers and those people are included in my survey an example is if i want to pick students we could assign students a number and pick random numbers to be included in the study a second type of random sampling is what is called stratified random sampling and this is when we divide the population into groups each group is called a strata and then select a proportionate number of each group polling is often done this way the idea is if we randomly survey 100 people or 200 people we don't want those 200 people to be all of the same political party otherwise we would get a biased sample so we guarantee it by saying if my state is 40 percent democrat 35 percent republican and 25 independent then using a random method we will select i said we wanted 200 people so maybe we double the percentages maybe we'd select 80 democrats 70 republicans and 50 independents to be included in my sample i have a proportionate representative of each group that matches the proportions of the state and so my sample still is random but i don't get the biased of only interviewing one party another type of random sampling also involves groups but it's called cluster random sampling again we're going to divide into groups but this time instead of selecting a proportionate number of each group we are going to randomly select entire groups so everybody in some groups are included and everybody in other groups are excluded an example of doing this might be a football stadium and it has different sections in the football stadium so we're instead of interviewing everybody in the stadium or random people through the stadium it's easier just to hit one section at a time so in the football stadium sections e and g are randomly selected and all fans from those sections are included cluster takes all people in each randomly chosen section while stratified takes a proportionate group out of each group the last random method we're going to use is called systematic random sampling the idea behind systematic random sampling is we start with a random item or person and choose every nth person after this that means like every fifth person or every 12th person or every 30th person an example of this might be if i pick a random phone book person phone number a random number in the phone book and then i choose every 50th person after this until i circle back to the beginning and go all the way through and back to the beginning and back to where i was taking every 50th person those are our four random sampling methods that you should be able to identify for this course there is a nonrandom method that is used quite a bit and so we should at least acknowledge the nonrandom method called convenience sampling which basically says use the results that are readily available as an example i would say i need to collect 50 data values for a survey so i'm just going to interview people nearby me or maybe people within driving distance maybe the friends on my facebook friend list something convenient and easy to get a hold of it's not really random and the problem with not being random is there are several drawbacks to not being random i could have bias results if i'm interviewing phone preference outside of the apple store i'm going to get more iphones that convenience sampling is going to work against me it may not be representative of the population if i only interview outside of a school because it's convenient near my house i'm going to get a lot more parents of young children than i will the general population and then the drawback of that is the results may not be useful outside of the sample i could conclude that such a percent of people prefer a certain type of music but when i'm with a different sample or a different population those results may not be useful so those are some drawbacks to convenience sampling we got to watch out for convenience sampling though it is used more often than it should be take a look at the homework assignment that practices with random sampling and also some of these different levels of measurement and types of data in class we're going to do more work with random sampling get really comfortable with the different types and i'll look forward to seeing you then now that we know the basic vocabulary of statistics and know how to collect data we're actually ready to start displaying some data our question for today is how do we display data visually and we're going to look at two main ways to display data the first way is going to be with what's called a histogram a histogram shows us frequencies over intervals and a histogram can really give us an idea of the shape of our data let's do a quick example here it's going to be easier to talk about a histogram with an example of a histogram so here's a scale two three four up the vertical axis the yaxis is always labeled frequency and then the xaxis is going to be some type of label of what we're actually looking at maybe we're looking at the number of tvs in a home and i'm going to actually start at negative 0.5 i'm going to come over here to 1.5 3.5 5.5 7.5 9.5 and we'll end at 11.5 first box we're going to make one tall the next one we're going to go up to three the next one we're going to start at 2 the next bar will start at one then we'll skip a space and put a bar at one i'm also going to give this a title let's title this the number of tvs and this picture is an example of what you would expect a histogram to look like a couple things that i want to note the bars on a histogram touch they come down on the interval numbers not in between but right on those numbers i labeled and they show frequency in a range in other words if i color the second bar green here what that second bar means is that there are three values or three people who reported having between 1.5 and 3.5 tvs between 1.5 and 3.5 are the numbers 2 and 3. so the numbers 2 and 3 all went into that bar now it's impossible to have 1.5 or 3.5 tvs and that's what brings up the second point here point b if possible and it's usually easiest with discrete data never have a bar come down on a value in other words if i'm talking about the number of tvs i don't want this to come down on four tvs if a bar came down on four tvs i wouldn't know if that four goes in the left bar or the right bar so instead i make sure my bars come down staggered from actual data values in this case by .5 and you also notice i took care to give a title to my graph and labels for the x and y axes title and labels are important so now that we kind of know what a histogram is and what it looks like showing frequencies over a range let's look at how to make a histogram first we need to decide on the number of bars once we decide the number of bars we'll use this nice little formula where we take the high number minus the low number and then divide by the number of bars and then whatever that number is we will round up and that will give us the bar width how wide each bar should be now it's important to note we always round up if it's 3.1 we'll still round up and the bars will be four in fact if it's three point zero we would still round up to have four bar bars of width four we always round up to the next number after this division next we need to decide on the starting value what i usually recommend we do to decide on the starting value is do 0.5 before the lowest number in my tvs example 0 is the lowest number of tvs so i went 0.5 before that so i'm staggered and the bars won't come down on that then it's helpful to make a frequency table like we saw in section one with ranges with the ranges that we found in parts a and b here and we can use that frequency table then to build the histogram so if that's the method let's see if we can go ahead and actually do that with an example the number of miles 20 students commute to work is below we are going to make a histogram with five bars to represent the data and our data here is going to be 4 6 6 7 11 13 18 18 18 21 24 26 27 35 36 36 42 43 45 and 49 so for our bar width we need to take the high minus the low divided by the number of bars so the high is 49 and the lowest four 49 minus four divided by the five bars is nine now it's exactly nine so i have to round that up to ten is my width always round up to the next whole number otherwise your last values won't be included in the last bar now we can decide on our start values or our ranges so for x values to start with our graph we're going to start half a unit before the low value of 4. so half before 4 is 3.5 and we're going to go up 10 to 13.5 the next range is going to start at that 13.5 and go up 10 to 23.5 then we have 23.5 to 33.5 and i'm running out of space so i'm going to scroll up a bit 33.5 to 43.5 and 43.5 to 53.5 those are our ranges now we just need to find the frequency inside each range 3.5 to 13.5 there's one two three four five six numbers 13.5 to 23.5 we only have four numbers then you see 23.5 to 33.5 there's three numbers 33.5 to 43.5 there's five numbers and 43.5 to 53.5 we see two numbers now that we have that frequency table we're ready to build our histogram starting at 3.5 my next tick mark is 10 later at 13.5 then 23.5 33.5 43.5 and 53.5 our frequencies go up to 6 1 2 3 4 5 6. the first bar is 6 tall the second bar touching it is four tall the next bar touching it is three tall the next bar touching it is five tall and the next bar touching it is too tall and now my bars show the frequency within each of those ranges now i still need a title we're talking about miles driven to work it's a good title our xaxis is in miles and then the yaxis going from one to six always shows my frequency and there's my histogram now with histograms it's important not just to be able to draw the histogram but we also need to be able to describe the shape of the histogram that we end up with so a couple notes here on some vocabulary you can use to describe the shape of a histogram and now with real world data it's never perfect histograms are never perfectly any of these shapes but they tend to be close to one of these shapes not always but generally a common shape we'll see is what's called the uniform shape where the bars are about the same so just a quick sketch here you might see bars that they're not quite exactly the same but they're pretty darn close we would say that histogram is uniform they're all about the same height on all the ranges another word we should know is what's called a normal shape the normal shape is taller in the middle and shorter on the edges what a normal shape looks like is it generally starts short and gets taller until the middle and then after the middle it starts getting shorter again it's also called the bell shaped curve where it goes up and then back down the opposite of the normal shape is what we would call the vshape and that is shorter in the middle and then taller on the edges so that's when we have a tall edge that gets shorter and then it comes back up and gets taller on the outside and you can almost see that vshape right on top of those bars in addition to the shape we can talk about its symmetry we say a graph is symmetrical if it's basically the same on both sides so if we go tall and then shorter and then shorter it's going to be the same thing on both sides you notice that's the same as the v shape it's also symmetrical the same on both sides and we can combine these different descriptive words together to come up with a detailed description of a histogram then we have this idea of skewedness skewed means it's not symmetrical and we describe the unsymmetrical part where the extra stuff is skewed right means it's not symmetrical because we have extra stuff for lack of a better term on the right that's where we're trying to be symmetrical right but then on the right side there's all this extra stuff and it kind of goes down a lot slower that extra stuff means it's skewed right and you might expect the last term then is skewed left where the extra stuff is on the left and that's where we've got all these extra little short bars on the left before it starts growing and giving us our what would be symmetrical shape so that's histograms we can draw them to show the shape of the data we can describe them as uniform normal v shapes skewed symmetrical really can help us visualize what our data set looks like a second thing we can do though to describe our data visually is to draw what's called a box plot and a box plot shows the spread of data with what is called the five number summary five number summary is made up of five pieces a b c d and e the two easiest to find are the minimum and maximum and then right in the middle of them is what is called we'll put it on c the median or the middle when the data is in order so the median cuts the middle of the data and then we've got the top half and the bottom half in the bottom half we're going to find what's called q1 or the first quartile which is the middle of the lower half and similarly q3 is called the third quartile which is the middle of the upper half so the quartiles and the medium really divide it into quarters now there's a little caveat with the quartiles in the median we said they're the middle values but if there are two middle values what we'll do is we will add them together and divide by two giving us the middle of the middle once we find that five number summary we'll usually represent it visually with what's called the box plot the box plot splits the data into quarters and to do that we put q1 and q3 as the edge of the box and then we draw whiskers out to the min and max values and finally we use a dotted line for the median and so what we end up with is there's some number line down the bottom and then floating above that number line is the box showing where the quartiles are q1 and q3 whiskers out to the minimum and maximum values and then a dotted line for the median and each part of the box represents a quarter or 25 of the data values the box then is the middle 50 the whiskers are the outside 50 percent and we can make some visual conclusions about how our data is spread out let's see if we can make a box plot going back to our example with the commute time that list of numbers again for us was 4 6 6 7 11 13 18 18 18 21 24 26 27 35 36 36 42 43 45 and 49 now if this data was not in order it would be essential as a first step to put the data in order fortunately ours already is in order we know there are 20 data values so 20 cut in half is 10. we're going to have 10 below and 10 above so 1 two three four five six seven eight nine ten cutting in half ten below and ten above sticks us right between twenty one and twenty four because it's right in the middle we add those numbers together and divide by 2 to get our median value which is 22.5 then we can go after our quartiles our quartiles are in the middle of the bottom half and the top half well the bottom half has 10 values so we're going to split 5 and 5 which sticks us right between 11 and 13. again because there's not one value in the middle we'll add those together and divide by 2. 11 plus 13 divided by 2 gives us our q1 equal to 12. for the upper quartile again we're going to have five data values on each side sticks us right between 36 and 36 when we add those together and divide by two the third quartile is 36. also include our minimum value of four and our maximum value of fortynine and we're ready to make our box plot showing the spread of our data we should make every attempt to make this box plot 2 scale so if i count by 6's starting at 3 we'll have 3 9 15 21 27 33 39 45 and 51 notice those are about the same size apart from each other my box is made from the quartiles at 12 and 36 connect our box the median is a dotted line at 22.5 the whiskers go to the minimum of four and the maximum of 49 and we have our box plot of course any graph needs a title so we can title this commute time and label the xaxis maybe time in minutes and we have our box plot now normally the box plot would all be one color but i did color coding to show where all the pieces came from in this example and just like we can describe the shape of our histograms we can also describe the shape of our box plots and there's basically three ideas here one is the idea of being spread out where we have a wide range of values this would be a big box plot covers a large range of values that means the data is not really close to each other not close to the median not not really close to anything it's all spread out the opposite of being spread out is to be clustered together where we have a small range of values and this is going to give us a really tiny box plot everything's really close to each other and often it's beneficial to split our description into the entire shape and the middle 50 percent or the box for example i could have a box plot that looks like this and we could say overall the data is spread out but the middle 50 percent is clustered together because the box is small compared to the rest of the data so that's what we're looking at today histograms and box plots practice making them practice interpreting them describing them take a look at some of them on the homework we'll work with these a little bit more in class and we will see you then in our previous section we took a look at how we could summarize data visually but quite often it's going to be useful to summarize data with numbers so that's going to be our question how do we summarize data numerically and there's two things that we'll try and summarize numerically the first one we're going to plant on for a minute here are the measures of center where is the middle of the data and depending on our context the measure of center will either be the mean the median or the mode let's start with the mean or what people typically call the average now if we're talking about a population mean we will always use a greek letter and that's going to be the greek letter mu but if we're talking about a sample mean we will use an english letter which we will notate with x bar and the formula for calculating the mean i'm going to use x bar but it works also for mu is equal to this symbol which we call sigma x over n and that symbol that funny looking thing means the sum what this means is sum up all the x's or sum up all the values and divide by n which is the sample size or population size if we're in the population context so for example if i had the numbers 1 3 3 4 4 4 5 5 and i wanted to find the mean of this sample the numerator says sum up all the x values or do 1 plus 3 plus 3 plus 4 plus 4 plus 4 plus 5 plus 5 and divide by the sample size and if i count here i see we've got a sample size of 8. so adding those all up we get 29 over 8 which gives us a mean of 3.625 or the average is 3.625 most students are familiar with that mean formula but one tweak we can do to it and we quite often have in statistics it's not two let's call this c because we're still under mean is if we have frequencies so we've already seen one formula for the mean the sum of the x's divided by the n but if we have frequencies given to us where we know how often each number is used the formula is going to tweak slightly x bar is going to be equal to the sum of the x's times the frequency divided by the sample size so if we have frequencies we should know this formula let's do the exact same example we just did but this time we're going to summarize those numbers by their frequencies so first i'll list out the numbers the numbers that showed up were 1 3 4 and 5 then i'm going to have another column that shows the frequency i had a single one there were two threes three fours and two fives what the sum says we need to do is first multiply the x's times the frequencies so when we multiply one times one is one three times two is six four times three is twelve and five times two is ten and then what we wanna do is sum this x times frequency table so 1 plus 6 plus 12 plus 10 is 29 that represents the sum of the x's times the frequencies and then we just have to divide by the sample size well the frequencies tell us how many things there are so my sample size is 1 plus 2 plus 3 plus 2 is equal to 8. so for my mean again we're doing 29 divided by 8 to get the exact same number of 3.625 but sometimes it's a lot quicker and easier if we have those frequencies given to us that's the first measure of center that measure of center of the average says if everybody was split up equally they'd have this many in common but quite often the problem with the mean is one large value or one small value can throw off the mean significantly which is why we might be interested more in the median or the middle number when they're first put in order this way one large or one small number won't have a dramatic impact on this measure of center so for example using our same data the one three three four four four five five the median is going to be the value right in the middle well if there's eight values the middle puts you between the four and the four and we know from our previous section when we made box plots we would add those and divide by two to get our median value which turns out here to be four and the big advantage of the median is that one extreme value does not impact the median as significantly as the mean it's a little more stable now the third measure of center and this is often used in categories or nominal data is what is called the mode the mode is the value that occurs the most often and again it's usually best for categories if we're talking about the color of cars in the parking lot we're not going to have an average of a blue point green car that doesn't make any sense but what we can do is say the most common frequent colored car is blue or gray or whatever that most frequent one is we can still look at mode in terms of numbers we seem to be using this example data of one three three four four four five so let's keep using it and what we see is the number four appears three times it is the one that occurs the most often so we will say that the mode is equal to four those are our measures of center the mean median and mode but the problem with just measuring the center is it only tells us where the middle value is it doesn't tell us kind of how all the rest of the data is behaving around the center is the data really spread out is it clustered close to the center what's happening with the rest of the data and that's why we also need some type of measure of spread it tells us more than just the middle it tells us how spread out the the other values are not just where the middle is but how's everybody else behaving around the middle and the reason this is important is we can look at data such as these three data sets i'm going to put up here and all of them all of the following have the same mean and median the first data set's going to be 1 1 1 5 9 9 9. both of that data set has a mean of 5 it also has a median of 5. but if i do another data set of 1 2 4 5 6 8 9 that data set has a mean of 5 and a median of 5. and if i do a data set of five five five five five five five that data set has a mean of five and a median of five there's no difference between these three data sets if i just look at the center but the numbers are spread out quite differently the first data set the blue one are spread out very far the green ones kind of spread out evenly and the red one has absolutely no spread in it at all this is why we need a measure of spread and the most basic measure of spread is what we call the range the range tells us how much space there is between the largest and the smallest number we take the large number and subtract the small number to see how far apart those extreme values are so for example with our data we've been playing with today of one three three four four four five five the range would be the big minus the small 5 minus 1 equals 4. and so there is a space of 4 between all of these values now there's a problem with the range though the problem with the range is one extreme value could greatly impact for example if there was also a 27 on this data set 271 which we sound like 26 there's a large range between the numbers when most of them are actually clustered quite closely together so this is why we need a different better measure of center and one measure of center that might be better is what we call the inter quartile range it's often abbreviated as iqr for inter quartile range and the interquartile range you could think about as the range of the middle 50 percent what we'll do is we'll take the q3 and subtract the q1 value subtract the quartiles and we see the range of the middle 50 percent or how spread out the middle 50 percent is and then we're no longer going to be impacted by an extreme outlier that's way too big or way too small for the rest of the data so for example with our data of 1 3 3 4 4 4 5 5. we already said the median was between 4 and 4. the first quartile then q1 is between three and three which is just three and the third quartile q3 is between four and five which is four point five this means our interquartile range is 4.5 minus 3 or 1.5 and that might be a little bit of a better measure of the spread because it's only looking at the middle 50 percent the extreme outliers are not going to impact the interquartile range however we still have a problem and that problem is this interquartile range formula only considers two values the q1 and the q3 it would be nice if we had some measure of spread that considered all the values and how spread out those are and this gives rise to the most important measure of spread the one we'll use a lot in this class called the standard deviation and the standard deviation attempts to measure what we call the average distance a point is from the mean how spread out is the data considering all the data values on average how far are they from the mean now with a population we will use a greek letter for the standard deviation and that's the greek letter sigma and with the sample we will use the english letter s to represent the sample the formula for the standard deviation kind of builds on this idea that we want the average distance from the mean so if i took any point and subtracted the mean that would give me the distance it is from the mean the problem is is some of these will be positive and some of these will be negative so if i add them up it actually adds up to 0. so to avoid the positive negative problem what we'll do is we'll square each of the values before we take the sum and add them all up then we'll divide by the sample size which turns out with standard deviation and when we derive the formula it's not exactly the sample size we divide by but we'll divide by n minus 1. and the reasons for the 1 are beyond the scope of this course so you just have to trust me to take an average distance from the mean with the standard deviation we're going to divide by n minus 1. the problem that we still have though is we squared everything so it's not really a true average so to undo the square we'll take the square root at the end of our formula and we will say s is equal to that square root and that is going to be an important formula for us in this course now i do have one little caveat turns out that the formula for a population standard deviation is slightly different than s the formula is we're not going to worry too much about that different formula for the population because generally we always have a sample we're always going to be taking sample standard deviations which is this formula we've looked at here so let's do an example and let's start with a smaller example and then we'll move to the bigger example that we've been seeing throughout this video we'll start with the example 11 13 14 and 14. and i'll give you a hint if you go through and calculate the mean of these values the mean is going to be equal to 13. what we'll do to get started is we'll list our values 11 13 14 14. and then we're going to have a column for every step along the way in this formula the first step says take those x values and subtract the mean subtract 13. so 11 minus i can even put a little thirteen here we're subtracting thirteen eleven minus thirteen is negative two thirteen minus thirteen is zero fourteen minus thirteen is one and fourteen minus thirteen is one then the formula says to square r values so the x minus x bar each of those values needs to be squared negative 2 squared is 4 0 squared is 1 1 squared is 1 and 1 squared is 1. finally the formula says to take the sum so the sum of x minus x bar squared is equal to 4 plus 1 plus 1 which is 6. now i'll jump to the standard deviation formula which says the square root of the sum which is six divided by one less than the sample size which is three six divided by three is two and the square root is one point forty one now similar to our formula with means if the data is given to us with frequencies rather than individual data numbers we need to do a slight adjustment to our formula so if we have frequencies the formula will slightly adjust to s equals the square root of the sum of x minus x bar squared but before we take the sum we have to multiply by the frequency and then divide by n minus 1. so let's take a look at an example where we do it with frequencies and let's use that data set that we've been using where we know the x values were 1 3 4 and 5. and the frequencies of that were one two the number four appeared three times and the number five appeared twice now we already know the x bar the mean is 3.625 we would have had to find that first if we didn't know that but since we do now we'll make another column for x minus x bar taking the x the number 1 minus 3.625 is negative 2.625 3 minus 3.625 is negative 0.625 4 minus 3.625 is 0.375 and 5 minus 3.625 is 1.375 next the formula says we need to square that x minus x bar we're going to square each of these numbers so 2.625 squared and i'm going to round to two decimal places is 6.89 0.625 squared is .39 0.375 squared is 0.14 and 1.375 squared is 1.89 now if we didn't have frequencies we would just add this column up but because we have frequencies we need to take this column the x minus x bar squared and multiply by those frequencies so the 6.89 needs to be multiplied by 1 to get 6.89 the point 39 needs to be multiplied by its frequency of 2 to get 0.78 0.14 times 3 is .42 and 1.89 times 2 is 3.78 and that's what we want to sum we want to get the sum of x minus x bar squared times the frequency and when we add up that column you should end up with 11.87 now we plug into our formula for s s is the square root of that sum we just found is 11.87 divided by one less than the sample size you could add the frequencies together to find out the sample size is eight or you might remember that because we've been working with this for quite a while one less than the sample size is seven so the square root of 11.87 divided by 7 is 1.30 so on average these points are about 1.30 units away from the mean of 3.625 gives us an idea of the middle and how spread out the data actually is now the standard deviation is actually quite nice because it gives us a way to compare data based on how many standard deviations we are from the mean standard deviations measure distance from the mean and in statistics we will use a very important variable to represent the number of standard deviations we are from the mean that variable is always going to be z z is the number of standard deviations from the mean and we actually have two formulas that use z they're both really the same formula the idea is z is the number of standard deviations from the mean so we'll take the value we're working with we want to know how many standard deviations x is from the mean well first we need to know the distance to the mean so we'll subtract the mean or x bar and then we'll divide by s are the number of standard deviations that is that's our main formula for z now sometimes we have the opposite information and we want to know what is three standard deviations from the mean we know z we want to be three standard deviations from the mean what value is that and so we can solve this equation for x and when we do we get x is equal to the mean plus the number of standard deviations times the standard deviation and so this formula it's really the same formula it's just been solved for x will tell us the number of standard deviations is what value let's do some examples let's say for our data one three three four four four five five we wanna know how many standard deviations from the mean is the median well we've already found all these important values we found the median is equal to 4. also earlier we found the mean was equal to 3.625 and also earlier we found the standard deviation is equal to 1.3 so if we want to find out how many standard deviations the median is from the mean the medians are x the means the x bar and the standard deviation is s so z the number of standard deviations four is we subtract the mean of 3.625 and divide by the standard deviation of 1.3 and we find the median is 0.288 standard deviations from the main or we could ask a similar question for the same data what value is 2 standard deviations below the mean we already know the number of standard deviations so 2 that's actually going to be our z the number of standard deviations below the mean and because we want to be below the mean we will use a negative number to make it below the mean so we're looking for the x this time what is that value of interest so x is equal to the mean x bar of 3.625 minus 2 because we have a negative 2 two standard deviations below the mean times the standard deviation of 1.3 and that gives us 1.025 now it turns out that we can say 95 of our data generally falls within two standard deviations of the mean if it's more than two standard deviations from the mean we say those values are unusual or extreme values we call those extreme values outliers and these outliers are values far removed from the rest of the data for example if i have the numbers 1 3 3 5 87 87 is far removed from the rest of those values it's an outlier and the outlier can either be large or small and we actually have two methods for calculating outliers and they generate very similar results so neither one is necessarily better than the other but i want to show you both of them the first one is called the interquartile range method and the idea behind this is we cannot be more 150 or 1.5 times the iqr from the edge of the box in the box and whisker plot the idea is anything below the first quartile minus 1.5 times the iqr or anything above the third quartile plus 1.5 times the iqr anything that's above or below those numbers becomes an outlier so for example let's say we've got the numbers 2 3 5 6 7 and 14. now the first chord the medians right in the middle but what we're interested in with the inner quartile range is the middle below the median the first quartile is three and the third quartile is seven so the interquartile range is seven minus three four four this means an outlier is anything below the first quartile 3 minus 1.5 times the interquartile range of four that gives us three minus six or negative three anything below negative three would be an outlier which there's nothing in this data set below negative three but also anything above the third quartile 7 plus 1.5 times the interquartile range 7 plus 6 is 13 anything above 13 becomes an outlier and you notice 14 is right on the edge there and so we will say based on that 14 is an out liar it lies outside of the majority of our data now that's the interquartile range method i said there's a second method it's based on the standard deviation the standard deviation method says that anything that is more than two standard deviations from the mean is an outlier which brings us back to that example that led into this discussion so actually let's first define this clearly let's do anything below x bar the mean minus two standard deviations or anything above x bar the mean plus two standard deviations is considered an outlier so for example we had our data of one three three four four four 4 5 5. and we found out already that the mean of this data is 3.625 and we also found out the standard deviation of this data is 1.3 so an outlier would be anything below the mean 3.625 minus 2 standard deviations 3.625 minus 2 times 1.3 is 1.025 and we'll recognize there that we do have a value below 1.125 or 1.025 it's just a little below but it is below here 1 is an outlier we also have to check above so we'll do the 3.625 plus 2 standard deviations or 2 times 1.3 which gives us 6.225 nothing above 6.225 so the only outlier we have here is the number one so we've covered quite a bit in this video we talked about measures of center the mean median and mode to estimate where the middle of the data is we talked about measures of spread to see how spread out we are around the mean or the center using the range the interquartile range and the most important ones the standard deviation and then we also looked at some uses of the standard deviation to see its distance from the mean finding out liars or identifying outliers so lots to take a look at on the assignment take a look and practice a few we will discuss them more in class we'll look forward to seeing you then our second unit is going to focus on probability and how to calculate various probabilities in different contexts so we're going to start out with basic probabilities and so that question we're going to answer is how do we calculate basic probabilities and as always we need to start off with some vocabulary to make sure we understand what we're talking about the first vocabulary word we need to know for probabilities is what is called the sample space the sample space is a list of all possible outcomes so for example if i was to flip a coin the sample space would be all possible outcomes i could get a heads or i could get a tails which begs the question what is an outcome so let's define an outcome really well an outcome is just the result of an experiment so an experiment might be flipping a coin an outcome would be just maybe heads or just tails and hence the sample space is all of the outcomes heads and tails when we're looking at the number of outcomes that occur in a sample space what we're really interested in finding is some type of probability or the chance that an event will occur and what we really end up with is a scale from zero to one where zero means the event certainly will not happen not going to happen and then the number one the maximum probability is it is certain to happen and then you could get any decimal in between so if i end up with like 0.5 that would be right in the middle so it's equally likely to happen or not happen so if the probability is .001 it's probably not going to happen but it could happen if the probability is 0.95 it probably will happen because it's closer to 1 but it might not happen the scale from 0 to 1 is our probability and there's two types of probabilities that we're going to be looking at the first is called the theoretical probability which basically says what we expect to happen or what should happen and the way we calculate a theoretical probability is we say that the probability of some event e is equal to the number of outcomes that we are looking for divided by the entire sample space or how many things occur could occur that's our basic probability formula so for example if i flip a coin and i want heads we would say the probability i get a heads is the number of outcomes there's only one outcome on a coin out of the sample space there are two possible outcomes on a coin heads or tails and then i would convert that to a decimal in this course we'll always use decimals for probabilities to get a probability of 0.05 of getting a heads that's theoretical probability what we expect to happen the other type of probability is called empirical probability and that is the chance of something happening based on our observations of some experiment what happened in an experiment so this is the example of maybe i flip a coin 500 times and i end up getting 257 heads because you know in actual practice the probabilities aren't perfect i'm going to get a few more heads or a few more tails it's not going to be exactly even and so in this case the probability of a heads as an empirical probability or an observed probability is 257 out of 500 which is 0.514 there's usually going to be a slight difference between the empirical probability and the theoretical probability but that difference can be made smaller using what's called the law of large numbers which basically says that the more trials i do the more trials done the closer the empirical probability is to the theoretical probability if i were to do a thousand trials this would be closer to 50 percent if i would do a million trials flipping a coin would get closer to 50 percent more trials the closer they're going to be now that's really basic probability but we do have some specific probability formulas to help us calculate some more involved situations and these three formulas we're going to look at are very closely related they really come as a group there's no one you should learn before the other because they're all so closely related so we'll do our best to define them one at a time when really they all come as a group the first is what we're going to call the conditional probability in a conditional probability we write it as the probability of b given a or with a vertical line between b and a and what that means is that is the probability of b given a has already occurred that's a conditional probability where we have some information and that's going to change the probability of b the formula for a conditional probability is the probability of b given a is equal to the probability that both occur a and b divided by the probability of the given information in this case the probability of a that conditional probability formula will be very important to us we'll do an example here in a minute let's go on to the second type of probability we need to know and that is the and probability the probability of a and b is the probability that both occur at the same time or together and the formula for the probability of a and b comes from the conditional probability if we multiply both sides of the conditional probability formula by the denominator the probability of a we end up with the probability of a times the probability of b given a has already occurred and that is the conditional i'm sorry that is the and probability formula the third formula we need to know is the or formula the probability of a or b occurring and that is the probability of a occurring or b occurring or both occurring one or the other or both and the formula for an or the probability of a or b is we're going to add the probabilities together the probability of a plus the probability of b the problem is this counts the and or the overlap twice it counts it in the probability of a and it counts it in the probability of b so we have to subtract off the overlap or subtract off the probability of a and b so it's not double counted it's only counted once and that gives us our formula for the or those are our three probability formulas the conditional probability is the probability of both divided by the given information the and probability with and we multiply the probabilities together given the first ones already occurred and with an or or probabilities we add them together subtracting off the overlap so let's do an example number four let's say we have three blue cards numbered one two and three and we also have let's write three out as a number since it's starting a sentence three blue cards numbered one two and three and i have two yellow cards numbered one two a we're going to find the probability actually let's just write as a probability statement we're going to find the probability that i get a blue card given the card is even with the conditional probability since i know the card is even we're not dealing with all five cards anymore we're just dealing with the even cards so we find the probability of both blue and even from the blue cards there is one that is blue and even so there's one of them out of the five cards divided by the probability of the given information the given information is that it's even there are two even numbers out of five now what's nice is generally those denominators will divide out and we're just left with one half or 0.5 so that means if i know i've got an even card the probability is 50 percent that it's going to be even that's conditional probabilities let's find the probability that i get a blue card and an even card now this is looking for the probability that both occurred at the same time blue and even of the blue cards only one of them is blue and even out of the total cards now we're looking at the total sample space of five and so the probability that's blue and even is 0.2 there's only a 20 probability that's both blue and even what about the probability that it's blue or even for blue or even now we're looking for how many are blue the first option there are three blue cards out of five plus how many are even there are two even cards out of five but then we need to subtract off the overlap the ones that are blue and even blue and even we know there's only one out of five that's both blue and even so three plus two minus one is four fifths which means we have a probability of point eight that it is blue or even i want to do one more example that kind of illustrates the and formula maybe a little bit better and that is finding the probability that if i draw two cards without with replacement actually maybe i should write this out probability i draw two blue cards without replacement what i'm really saying is what's the probability the first one is blue and what's the probability the second one is blue well for the first one to be blue the probability of the first event there are three out of five that are blue then we multiply by the second event the probability that i get a blue given the first one was already blue so now one of the blues is gone maybe the two is gone now there's only two blues left out of there's only four cards left and when i multiply that across we end up with point three is the probability we get two blues without replacement thirty percent so that's what we're looking for with that and formula the second part the and we adjust the probability to assume the first one already occurred those are our basic probability formulas but there are two vocabulary concepts that are related to those that i want to make sure we are familiar with so the first one of those two is what are called independent events when two events are independent what that means is one occurring does not change the probability of the other occurring the opposite of this would be dependent events and if we think about the blue cards that we drew without replacement the second probability was two fourths that second probability had changed from the first probability because one occurring changes the other one's chance of occurring because there's fewer cards left there's fewer blues now the way we show things are independent we can show them we can show this in one of three ways and it doesn't matter which one of these ways we use so we'll just pick the one that's most convenient for our context the first is we can show that the probability of a given that b has occurred if b is not going to affect its probability it should still be the same as just a occurring by itself because b has no impact on it the opposite is also true we could say the probability of b given a is going to be just equal to the probability of b because a occurring has no impact on it or the third method we can look at is the probability of a and b the probability of both of them occurring is equal to just the product of the individual probabilities because the given part doesn't change so for example let's say in a class twenty percent of students are lefthanded five percent of students are earning an a in the class good job to those five percent but only one percent of students are lefthanded and earning an a are these events independent is there is there a relationship between lefthandedness and earning an a well we'd have to look what's the probability that they're lefthanded we're told the probability they're lefthanded is point twenty or twenty percent the probability that they're earning an a of all students five percent are earning an a so the probability of earning an a is .05 we're also told the probability of the students who are lefthanded and earning an a both of them together is .01 well we can use either one of the three formulas for showing things are independent it's probably going to be easiest in this context to use the third because we have all of those pieces so the probability of a and b the probability of left and receiving an a should be equal to the probability of being lefthanded times the probability of receiving an a if they are in fact independent well the probability of a and l is 0.01 the probability of lefthanded is 0.2 the probability of an a is 0.05 and sure enough we get those are equal to each other because they're equal we'll say therefore they are independent events if it wasn't equal we would say the opposite or that they're dependent events so that's the first concept the idea of independent versus dependent the second concept that i want to wrap up with today is the idea of mutually exclusive two events are mutually exclusive that means both cannot occur at the same time essentially what we're saying is the probability of a and b is equal to zero an example of this would be if i were to roll a die a standard sixsided die and we're going to let o actually use the letter d because o is a bad letter for math d is going to represent an odd less than 4. b is going to represent a number bigger than 3. so what you see is d the odds less than 4 are the numbers 1 and 3. that's kind of the sample space of d and b the event b is everything bigger than 3 which is 4 5 and 6. these two have nothing in common you can't both be an odd less than four and a number bigger than three because we can't have both together we say they are mutually exclusive in other words the probability that we have an odd less than four and a number bigger than three is equal to zero that never happens so little vocabularies we wrap up with mutually exclusive both can't occur at the same time independent one occurring does not affect the other occurring but the big thing that we're looking at today are these probability formulas conditionals ands and ors take a look at the homework assignment to practice a few of these we will try a few more in class and answer any questions you might have then now that we're comfortable working with basic probabilities we're going to look at different ways we can organize our probabilities and information in today's video we're going to look at the question how do we organize probability information in a table specifically we're going to be in the context of what is called a contingency table which is basically just a table that lists results in relation to two variables these tables and this information will make calculating probabilities easier and what makes it easier is quite often we will add a column and row for totals so for example let's say we've done a survey and we're comparing whether or not people have speeding tickets in the last year or no speeding tickets in the last year and we're going to break this up into three groups the first group are going to be our younger drivers the under 21 drivers and then we're going to also look at the 21 to 25 year old drivers and then we'll also look at the over 25 drivers and the survey is conducted and there's 82 under 21s with the ticket 17 without a ticket in the past year for the 21 to 25s there were 39 with a speeding ticket and 27 without and for the over 25 there's 18 with a speeding ticket and 61 without now with this contingency table it's going to be helpful that we're going to add an extra row and an extra column if it's not there already that's going to give us the totals and these totals are going to make calculating individual probability questions much more efficient so if we total the under 21 we see we have 99 surveyed the 21 to 25 total that we get 66 surveyed totally over 25 we get 79 surveyed working across the rows 82 plus 39 plus 18 there's 139 people surveyed who got a speeding ticket in the last year the no tickets 17 plus 27 plus 61 is 105 and for the totals 99 plus 66 plus 79 gives us 244 people total in the survey and a good way to check that that total is correct is if we add the other combination 139 plus 105 that should also equal the 244 which it does and so that what we have there as that example is a contingency table now we're ready to find some probabilities off this contingency table for example if i want to know the probability that someone is 21 to 25 i can see very quickly on my contingency table that there are 66 people in the 21 to 25 range out of a total of 244 people and so when i divide 66 by 244 we can quickly get our probability of 0.2705 we could also do maybe the probability that someone has no tickets very similar i'd say well no tickets the total there is 105 out of the grand total which is 244 and when we divide 105 by 244 we get 0.4300 for our probability we can also do ands and we can do ors we can find let's combine these together the probability that someone's 21 through 25 and has no tickets well the 21 to 25 and have no tickets are when both of those occur together at the same time that's where they overlap here in the middle we have 27 people who are of no tickets and they're 21 to 25 out of the total of the whole group is still 244. and so when i divide 27 by 244 we get 0.1107 and we can change that to an or we can find the probability that someone's 21 to 25 or has no tickets and if you remember the or formula says we have to add the individual pieces and then subtract where they overlap so 21 to 25 there's 66 of them plus the no tickets there's 105 of them but we have to subtract where they overlap because these 27 where they overlap have been counted twice in both the column and the row so when we subtract off the 27 out of the 244 when we do that math on our calculator we get 0.5902 about a 59 percent probability they're one of those two we can even do given probabilities let's do the probability that we're in that 21 to 25 range let's get rid of these circles we don't need given we know the person has no tickets well with a given probability we are looking for both of them or the overlap divided by the given information so where they overlap 21 to 25 and no tickets they overlap with 27 but we're going to divide by the given information this time it's not the 244 because we've shrunk our sample size now we're just interested in those that have no tickets we're only interested in that 105. and so with the given information shrinking the sample size now the probability is 0.2571 we can switch that and see how that probability compares the probability they have no tickets given they're between 21 and 25 years old you might pause the video and see if you can figure this one out on your own with a given probability we need to find where they overlap divided by the probability of the given information they overlap again no tickets in 21 to 25 with these 27 individuals however now our sample space the given information is just the 21 to 25 years old and that's the 66. so we'll do 27 divided by the 66. to get our probability of 0.4091 and you can see how we move through each of these probabilities at a much greater accelerated pace when we have the contingency table to organize our data for us that's the nice thing about the contingency table one more thing i want to look at though is we have this vocabulary word from our previous video of independence so i want to know are being 21 to 25 and having no tickets independent does that mean being 21 to 25 has no impact on whether or not you had a ticket in the past year well we talked about there being three different formulas we could use in order to show this one of those three formulas says that a given probability should not change the probability if they are in fact independent in other words the probability they're being 21 to 25 given they have no tickets should be the same as the probability of just being 21 to 25 if they're independent because the tickets shouldn't impact that at all well we just found both of these pieces the probability of being 21 to 25 given we have no tickets is actually here in number five that was 0.2571 and the probability of being 21 to 25 we found in part one that's 0.2705 and we see that these guys are different therefore the probability has changed once we have given information and shrunk down the sample size that means these two variables are actually dependent on each other so all we're looking at today is organizing our probability information in a contingency table and taking a look at how that helps facilitate calculating the actual individual probabilities it also gives us an opportunity to practice more with and or and the given probabilities so take a look at these on the homework assignment come to class ready to discuss them and work with these contingency tables a little bit more today we're going to continue our work with representing probabilities visually using what are called tree diagrams and we'll also take a look at bayes theorem the question though that we're attempting to answer is how do we visually organize conditional probabilities how do we visually organize conditional probabilities and really the best way to organize a conditional probability is using what is called a tree diagram and a tree diagram basically has a branch for each outcome and then each branch produces a conditional outcome from that outcome from that outcome and then to find final probabilities we can multiply down the branches for final probabilities for example let's say we have an urn with five blue blue and three green marbles and you will draw out two marbles without replacement we are being asked to make a tree diagram to model the possible outcomes and probabilities so what we need to do is we're going to draw a branch on this tree for every decision point so the first decision point is the very first draw so this represents the first draw and off that first draw we could end up with a blue marble or we could end up with a green marble now you can see there's a total of eight marbles five plus three so the probability of getting a blue marble on the first draw is five out of eight and the probability of getting a green marble on the first draw is three out of eight but we're not done there because we're still going to draw another marble and that marble could be either blue or green and it doesn't matter which color we got first we're going to have the same possible outcomes it could be either blue or green on this second draw and so you can kind of see that if you follow a draw down if we want a green first and a blue second we'd go down the green line and the blue line and that represents green first and blue second or if i wanted a blue then a green we would go down the blue line first and then the green line second and we'd end up with the blue green combination but first let's fill in the individual probabilities because on the second draw things have changed if we go down the left side where the blue was drawn first and if we want a blue on the second draw there's no longer five blues left to pick from there are only four blues left to pick from and there are only seven marbles left so we only have a four and seven chance of getting a blue marble on the second draw given the first draw was blue similarly with the green option if we want blue then green tracking that down there are still three green marbles left but now the total is only seven marbles because one has been drawn out a blue one has been drawn out similarly on the right side of the tree diagram if green is drawn first and then i want a blue on the next draw there are five blues to pick from out of the seven marbles that are left but with the green there are only two marbles that are green left out of the seven marbles that are left now to get my final probabilities for each of these possible outcomes we just have to multiply down the chain so if i want a blue blue outcome what i'll do is i'll multiply the 5 8 times the 4 7 and when i multiply 5 8 times 4 7 we get a probability of 0.3571 now if i want blue then green we multiply down those probabilities to get the blue green possibility 5 8 times 3 7 is 0.2679 going down the next path we go green then blue so if we want a green marble then a blue marble that probability is 3 8 times 5 7 which turns out to also be 0.26 7 9. i think the other one was 2 6 7 9 also i wrote it down wrong the last track is going down the green and then the green so if we want two green marbles green then green we'll multiply the 8 times the 2 7 to get the 0.1071 and now we see all the different possible probabilities for drawing two marbles out of this urn without replacement and we can use that table to find things like the probability that both are the same color well both the same color would be a blue blue or a green green and then we subtract the overlap but there's certainly no overlap with blue blue and green green so we'll just add those together point and i should do this in red 0.3571 plus 0.1071 will give me the probability that i get two are the same color of point four six four two one more example what's the probability that i get at least one blue now i could do this a couple ways one way is i could say these first three options each have at least one blue but that's a lot more work than saying let's look at the complement and let's subtract off the only one that's left from the absolute probability of one we know all the probabilities are one so we subtract out the probability that doesn't satisfy what we want if we subtract off the 0.071 we'll end up with our probability of at least one blue of 0.8929 and that's how using a table can greatly facilitate our conditional probabilities and their various calculations but one very important application of using these tables is to facilitate what is called bayes theorem and bayes theorem is a way to find a conditional probability like the probability of b given a using known values of the probability of b oops sorry the probability of a given b in other words the given values are backwards from the given values that we want now bayes theorem officially says that the probability of b given a is equal to the probability of both the probability of a and b divided by the probability of the given information or the probability of a but we might not know the exact probability of a but we do know the probability of a given b and we can add to that the probability of a given it's not b in other words we combine all the a probabilities together to build that denominator and that last part that not b it may be several options so we might need to add three four five six things that are the not these but we have to basically add all the combinations of the probabilities of a given the other stuff this formula is not worth memorizing because it is always easier to do this on a tree the tree makes this easy and i'm going to show you what i mean with a couple examples let's say a marketing company predicts 90 percent of new products are profitable however the marketing firm isn't perfect only 70 of those products predicted to be profitable actually are in addition those predicted to be not profitable 20 percent actually are profitable so let's say we've got some product and it turns out that it was profitable that's given to us given a product was profitable what is the probability it was predicted to be profitable so was profitable what's the probability it actually was profitable let's make a tree to model this first the marketing firm looks at it and they say yes it will be profitable or not we are told the company predicts ninety percent of their products will be profitable therefore the complement 10 percent must not be profitable and that's actually not just profitable but that's a prediction of profitable and a prediction of not being profitable because then the next branch we're going to look at it actually is profitable or it's not profitable were they right or wrong it actually is profitable or it's not profitable we are told seventy percent of those that are predicted to be profitable actually are so if i go down that they're predicted to be profitable they actually are 70 are and the rest 30 are not of those that were predicted to be not profitable going down the nut line 20 percent actually are so 20 is on the right side and the rest 80 percent are not going down the lines multiplying point nine times point seven the first branch is a point sixty three percent probability point nine times point three there's a point twenty seven percent probability next branch point one times point two is point o two and point one times point eight is point o eight so for my actual probability statement it's asking what is the probability it was predicted to be profitable the probability that we predicted it was profitable given given it actually was profitable given it actually is profitable well we know conditional probabilities are the probability of both divided by the probability of the given information so the probability of both where it's predicted to be profitable and it actually is is this far left branch here that's 0.63 that is going to be our numerator the 0.63 the denominator then is going to be the probability of the given information the given information is that it is profitable the is occurs in two locations the is is the left side of both branches the is profitable both of those together make up my sample space so i take the given information and i add those pieces together 0.63 plus 0.02 and that gives me 0.63 at a point 65 which gives me a probability that it was predicted to be profitable given it actually was of 0.9692 and that is how bayes theorem helps us reverse the given probabilities in our table let's do one more of these bayes theorems with a little bit more detail in it let's say bob drives to work forty percent of the time he takes the bus fifty percent of the time he walks ten percent of the time so that's 100 of the time how he gets to work the probability he will be on time he is 90 if he drives 60 if he takes the bus and 30 percent if he walks if bob arrives late that's our given information bob has arrived late what is the probability he took the bus well let's make our tree diagram to summarize what happens he actually has three things that can happen initially he can drive he can take the bus or he can walk in fact we're given probabilities that he drives forty percent of the time takes the bus fifty percent of the time and walks ten percent of the time once he does that he will be either late or on time doesn't matter which method he takes he will be either late or on time he will be either late or on time we're told about the on time half if he walks or night if he drives sorry if he drives going down the drive he will be on time ninety percent of the time or point nine which means he's late the rest of the time ten percent the bus we're told he is on time with the bus sixty percent of the time which means he's late the rest of the time forty percent but if he walks he's on time thirty percent of the time and he's late the rest or seventy percent of the time multiplying down the chain then driving in light is .04 driving in on time is point 36 bus and light is .20 bus and on time is 0.30 walking in late is 0.07 walking and on time is 0.03 we are asked to find the probability that he took the bus given he arrived late i like to start off by marking all of the given information so given he arrived late late was our left branch late late late what we're looking for is the probability that he took the bus and was late so what we're looking for is the 0.20 now we're ready to build our probability the probability of both bus and late is that point 20. but divided by the probability of the given information the given information is that it was late we marked that as a 0.04 plus a point 20 plus a 0.07 so we have point 20 out of point 31 which when we divide those you get .6452 almost a 65 chance that bob took the bus given he's arriving late that's bayes theorem and that's how we can use tree diagrams to help us with conditional probabilities so take a look at a few of these on the homework we will discuss these more in class and i'll look forward to working with you then now that we're comfortable working with and finding probabilities we're ready to talk about discrete probability distributions and then later we'll talk about continuous probability distributions first i want to make sure we and we know what question we're trying to answer the question for today is what are discrete probability distributions and to set this up we're going to be working with this idea of finding what's called the probability distribution function often this is just abbreviated as pdf the probability distribution function and these probability distribution functions have two key characteristics the first is for this pdf each individual probability is between 0 and 1. in addition if we took all the probabilities and added them together they would sum to one now often we'll organize these probability distribution functions by what we call random variables and a random variable just describes the outcomes of an experiment experiment now a discrete probability distribution is used with countable or discrete outcomes for example if we let the random variable x represent the number of movies watched last week that's a discreet random variable because you only can watch a certain number of movies we can't do decimals you either watched it or you didn't so because this is a countable result it's a discrete result and we can collect discrete data a survey is conducted and it is found that forty percent of the respondents watched two movies last week 50 watched one movie and the rest watched no movies we can summarize the pdf or the probability distribution function in a table and in this table all we have to do is list the possible results i suppose countable wrong forgot the t in countable sorry about that all we have to do is list the possible results and their associated probabilities so x is the random variable people watched either 0 1 or 2 videos and then we'll make a column representing the probability that x occurred we're told that forty percent watched two movies so the probability is point four fifty percent watched one movie the probability is 0.5 we're not told what percent watched no movies but if we add what we have together we see we've covered 90 percent of the respondents so there's only 10 percent left to make it equal 100 percent and that must be the zero and this becomes our probably distribution table now what's nice about being able to organize all the countable possibilities is it allows us to consider the expected value which is also called the mean and the standard deviation of the probability distribution function or of the pdf and the formulas for finding these expected values and standard deviations are very similar to the formulas we had for means and standard deviations from chapter one but using the probabilities instead of the frequencies first the expected value the expected value is the long term average or mean of the pdf in other words if you ran lots and lots of experiments and took the average of the results what would that expected value or that average result b and the formula for the expected value or the mean is equal to the sum of all the x's times their individual probabilities this is a good formula to know so for our example where we had our number of movies being 0 1 and 2 and their individual probabilities being 0.1 0.5 and 0.4 we can find the average number of movies watched in this survey by making an extra column for x times p of x multiplying the x value times its probability zero times point one is zero one times point five is point five and two times point four is point eight and if i add those together that will give me the sum of the x's times the p of x's which is 1.3 this tells me that the average student or survey respondent watched 1.3 videos last week the expected value if i took a whole bunch of students over and over again i would expect the average to be 1.3 and again similar to how we found the standard deviation with frequencies we can find the standard deviation or the spread of our random variables using the formula that's sigma the standard deviation is equal to the square root of the sum of the difference between the mean and the value squared times the probability of the values again this would be a good formula to know how to use and basically like we did before with finding standard deviation we're going to make an extended table so again our values for the movies were 0 1 and 2. their individual probabilities were 0.1 0.5 and 0.4 and we're going to start to build this formula by adding columns the first part of the formula wants the difference between the mean and the value so our mean remember we just found out that the mean was equal to 1.3 we did that in a previous section so 1.0 minus 1.3 is negative 1.3 1 minus point three is negative point three and two minus one point three is positive point seven but then our formula says we take that difference and we square it make them all positive 1.3 squared is 1.69 0.3 squared is 0.09 and 0.7 squared is point 49. but then the formula says we need to take that square of the difference and multiply it by the individual probabilities so we'll take that green column times the second black column one point six nine times point one is point one six nine point five times .09 is point o four five and 0.4 times 0.49 is 0.196 and our formula says let's find that sum our formula wants the sum of the difference squared times the probability which is equal to 0.169 plus .045 plus 0.196 is point 41 and we're told the standard deviation is the square root of that value the square root of 0.41 which is equal to 0.6 so now we know our probability distribution function that table that we made has an average expected value of 1.3 and a standard deviation of 0.6403 so that's what we're looking at today is we're taking a look at how to work with these probability distribution functions specifically the discrete ones and then in our next few videos we'll look at some specific types of discrete probability distributions but for now take a look at practicing a few of these on the homework we'll discuss them further in class good luck this video is going to take a look at a special type of discrete distribution called the binomial distribution and the binomial distribution helps us calculate what is the probability of x successes out of in trials in other words i'm going to conduct a survey and maybe i want to see how many people support a particular political candidate i want to know what's the probability that 100 out of 200 of them or 50 percent support my candidate that is what we call a binomial distribution and a binomial distribution has the following characteristics first there is always a fixed number of trials i'm going to interview a certain number of people or i'm going to run a test a certain number of times and when i run that experiment or that survey there are really only two options the two options are success and we usually use x to represent the number of successes or failure and it's important to note as we define success and failure in our experiment there is no moral or good bad judgment to the word success and quite often success is a bad thing if i'm in quality control i might say a success is a defective part and we're looking at how many successes there are in that case success is a bad thing so there's no moral or ethical standard for success success is just what we are looking for so try to avoid pinning positive and negative emotions to that word with those two options of success and failure we'll often talk about p which is equal to the probability of a success and if x is the number of successes and we'll use n for the number of trials x divided by n would be the probability of that success then we also have this letter q which is the probability of failure the opposite and since there's only two options success and failure we can quickly calculate q to be equal to 1 minus the probability of a success it's the complement so the distribution itself the distribution for the binomial is when we're going to say that x tilde or little squiggly line b for binomial and then in parentheses we'll do n the number of trials and p the probability of success and we'll use this notation to represent how the x is distributed it's distributed as a binomial within trials and a p probability of success and if we are in the context of the binomial we have some shortcuts to help us calculate things like the expected value the expected value or the mean of the distribution is simply going to be the number of trials times the probability of success and that seems to make sense if i have 30 trials and a 10 percent chance of success i would expect to be successful 10 percent of 30 or 3 times we can also use a shortcut formula for the standard deviation of the binomial and the standard deviation or sigma is equal to the square root of n p q now we could go through the formulas for mean and standard deviation like we did with just the generic discrete probability distributions it just gives us the same answer so we might as well use this nice little shortcut and actually before we move on let's go ahead and highlight these three pieces because these three pieces are foundational to doing our binomial distribution let's look at using the binomial distribution and you notice i never gave the formula for how we actually calculate binomial probabilities that's because we're going to cheat and we're going to use the calculator to do all the work for us in the calculator the steps that you're going to push on the ti 83 or 84 calculator is you'll hit the second button and then you're going to hit the what's called the distribution button which is above the button that actually says vars so when you hit the second it gives you the command above the button the distribution function is what we want to use and we're going to use the binomial distribution so then we will scroll down and there's two options for the binomial the first option is called the binomial p d f and it opens a parenthesis that one gives us the probability of exactly x successes the one below it we'll also use is called the binomial cdf and the c in there stands for a cumulative distribution function that tells us the probability of up to and including x successes in other words with the cdf if i'm interested in the number three if i want three successes the pdf will tell me exactly three successes the cdf will tell me what's the total probability of three two one or zero successes all the probabilities up to and including that number now after the parentheses we do have to enter in the key information some calculators have some software that it'll prompt you for the information but if it doesn't prompt you you just need to know that the format for both of these is exactly the same we will use the binomial and i'm going to do a start because it could be the pdf or the c df both of them are the same then the first number you enter will be the number of trials comma the next will be the probability as a decimal comma then you'll do x or the number of successes and close the parentheses and that's how we can use the binomial distribution on the calculator and it's probably easiest to see with an example and according to the website citydata.com in moses lake seven point nine percent of workers carpool to work you're going to go out and you're going to conduct a sample of 41 workers first thing we want to do is identify what is the distribution for this situation the distribution of our x or our random variable is going to be randomly distributed as a binomial so we'll do a b because we're looking at the number of successes out of these 41 trials the first number is the number of trials 41 trials the second number is the probability of success which is 0.079 we do need to change that percentage into a decimal so of our 41 workers how many would you expect to carpool to work well when we're looking at how many would we expect we're talking about the expected value or the mean the mean we said is equal to the number times the probability or the 41 people you surveyed times the probability of 0.079 that gives us an expected value of 3.23 workers so maybe you'd round that down to three workers you'd expect about three maybe four out of your 41 workers to carpool to work let's scroll up we'll come back to the calculator strokes in a minute what is the standard deviation of our population well we have our formula for the standard deviation it's the square root of npq so we'll take the square root of n the number of trials is 41. p the probability is 0.079 times q well q is the probability of a failure or the probability of someone not carpooling to work well if 7.9 percent carpool to work we can do 1 minus 0.079 to get q is equal to 0.921 so q is 0.921 and when we multiply and take the square root we'll get a standard deviation of 1.727 but we still haven't calculated any probabilities so let's do two or three of these let's say what is the probability exactly 3 of 41 carpool to work in other words what's the probability that our x our number of successes is exactly equal to three well because i'm looking for a specific exact number that is going to be the pdf on our calculator so on our calculator we're going to do the binomial pdf and then we do the number of trials the probability of .079 comma the number of successes we want which is three let's go to the calculator and do that on my calculator we said the way we got the binomial pdf is we hit the second button and then above vars you see the word distribution now the binomial pdf is near the bottom so you can scroll down a bunch or if you scroll up once it'll take you to the bottom and you see options a and b are the binomial pdf and the binomial cdf i'll hit enter on the pdf now mine gives me the prompts so i just have to enter in the number of trials 41 p the probability of .079 the x value the number of successes i want which is 3 and then i can select paste and what that'll do is it'll automatically type in the 41 comma 0.079 comma 3 for me if you don't have those prompts you can just type those numbers in with commas in between them the commas right above the number seven and when i hit enter it's going to give me the probability that i get exactly three successes the probability is point two three zero four and we will always round probabilities to four decimal places point two three zero four what is the probability that in my survey less than the expected value carpool to work in other words we want the probability that x is less than the expected value which we found in part b was 3.293 well this is a discrete distribution you can't have .293 people saying yes they commute to work so what we're really saying is the probability that x is less than or equal to the number three and it's important to identify what it's equal to less than or equal to not just less than because in the cdf when we do the cumulative distribution we need to know what number to start at so now we're going to do the binomial cdf cumulative distribution which is going to add from 0 all the way up to 3. starting with 41 is the sample size 0.079 is the probability and we're going all the way up to 3. we'll hit second distribution to the bottom but this time selecting the binomial cdf i have 41 trials 0.079 is my probability and my x value is 3 and when i hit paste it's going to put those numbers in for me again if you don't have the prompts just put those numbers in separated by a comma and when i hit enter it's going to tell me the sum total of all the probabilities of 0 1 2 or 3 there is a 59.17 or a 0.5917 probability that if i ask 41 people i will get three or fewer commuters sorry carpoolers as they commute to work let's do one last example as we wrap this video up we want to know what is the probability more than four carpool so now i'm asking for the probability that x is greater than four the problem is the cdf counts down so this would be a lot of work to do to find them individually the probability of 5 plus the probability of 6 plus the probability of 7 all the way up to the probability of 41 that is a lot of work instead what we're going to do is we're going to use the complement we're going to find out the probability that we're not talking about and subtract that probability from one one minus the probability that x is now less than or equal to and we have to decide what number we're less than or equal to to count everything that's not included with the greater than four the blue greater than four does not include the number four so in the complement we do want to include the number four if our probability statement had equality if it said or equal to four then we would have to do the complement which is the opposite and to not include four and so we'd start at three so you really have to be careful to decide what you're going to include so now on my calculator we're going to do 1 minus we want less than or equal to 4 that's the binomial cdf cumulative distribution function so it goes up 2 with the 41.079 probability we're going to go up to and include the number four one minus second distribution up to the binomial cdf we still have 41 trials we still have 0.079 but now we want the x value to be 4. paste and when i hit enter 1 minus the binomial cdf gives me a probability of 0.2205 so there's a 22 percent chance that we would have more than 4 out of 41 workers carpooling to work that's the binomial distribution the calculations we'll have the calculator do for us but we do need to know how to set them up how to find the mean and the standard deviation and interpret what pieces are talking about what parts of the binomial so take a look at a few on your assignment come to class ready to discuss it further and we'll continue to investigate the binomial distribution a second type of distribution that we can work with is called the poisson distribution it's named after a mathematician named poisson but the question that the poisson distribution attempts to ask is very similar to the binomial the binomial one to know the probability of x successes in n trials poisson also wants to know what is the probability of x successes but instead of in a certain number of trials it wants to know in a certain amount of time in other words we'll say some event happens on average three times an hour what's the probability this hour it's going to happen five times and to answer that question we use the poisson distribution and some characteristics of the poisson distribution is that we are interested in some number of successes in a fixed interval usually those intervals are a certain amount of time a day a week a month a year but it can be any fixed interval like an editor might be interested in the number of errors an author makes per page and so a page is the fixed interval that functions as the time interval but usually we're talking in the context of time and we also need to know or we will have the average number of successes in that interval so the editor knows that an author makes on average three errors per page the distribution itself we will notate the distribution very similarly to how we did the binomial we'll use the x to say that's our random variable the tilde to tell us it's distributed but this time because it's a poisson distribution we'll use p to represent poisson and the only variable we need for the poisson distribution is the mean or the average number of successes in the interval as you might expect the expected value is the mean that's given to us that's no surprise but the standard deviation turns out to just be the square root of that average and so these formulas can help us shortcut that process as we attempt to actually use the poisson distribution and just like we did with the binomial distribution when we're using the poisson distribution we will actually have the calculator find the probabilities for us we just need to know how to use the calculator to get the information we want very similar to the binomial we'll hit the second button and then you're going to hit the vars button because above the vars we remember it says distribution and then we need to select the correct distribution as there are several in the calculator again near the bottom are going to be the two we'll use one is called poisson pdf and just like the binomial that gives us the probability of exactly x successes and then the other one is the poisson cdf opening a parentheses parenthesis that's the one that gives us the probability of up to and including x successes and again similar to the binomial the format we will use if you don't have the prompts on your calculator will be the poisson either c or p they both work the same df and then the first number you'll enter in is actually the average or the mean over that time interval the second number is the x value that you're interested in finding so let's see if we can use the poisson distribution in an example let's say a certain hospital baby unit has an average of four births per week and we're going to randomly choose a week a week is randomly chosen first things first let's describe the distribution the distribution is a poisson because we're talking about a time period an interval a span of time one week and in that one week there is an average of four births in that week that is the distribution now we can find the expected value and standard deviation of the number of births the hospital has per week well the expected value that's just mu or the average which is given to us the expected value is 4. the standard deviation or sigma is the square root of the average and the square root of four is two so average of four births with a standard deviation of two births is what we would expect to happen on any given week in the baby unit of the hospital so let's find some probabilities and see how likely various things are to occur i'm always interested in how often the average occurs so what is the probability exactly four babies are born this week what we're asking is what is the probability that our random variable x is exactly equal to 4. now because i want my probability to be exactly equal to 4 what i'm interested in finding is the pdf for the poisson exactly equal to 4 in that time interval so we will do the poisson pdf probability distribution function the first number is the average of 4 the second number says we want exactly 4 to occur so let's see what the calculator says when we pull it up very similar to the binomial we'll hit second and distribute distributions scroll down to the bottom and below the binomial you'll see the poisson distributions the poisson's pdf and the calculator if it gives you props will ask for lambda that's the greek letter lambda is the average so for lambda the average we say is four the x value we're interested in is four and when we paste you'll see it puts it in the calculator for me if you don't have the prompts again you can just type in four comma four and close the parentheses the comma's above the seven and we see the probability that we get exactly four even though that's the average the probability that actually occurs is only about 19 and a half percent point one nine five four when we round so that might beg the question what is the probability of fewer than four births this week the average of four only has a less than twenty percent probability what's the probability that x is fewer or smaller than four well the calculator can only do exactly equal to a number and down so first let's identify that we're really looking for the probability that x is less than or equal to not four because four is not included but less than or equal to three because we want to be less than or equal to three counting all the possibilities up to three zero plus one plus two plus three that's going to be our cdf or our cumulative distribution that adds up all the values up to that point the poisson cdf which has an average of 4 but we want to know up to the number 3. so we go to our calculator and hit second distribution down to the bottom you see poisson cdf the mean is four we want to go up to three and when we paste types the numbers in or you can type them in manually if you don't have the prompts the probability that we get less than four is point four three three five so there's a 43 percent chance we're actually less than the average number of births let's say the hospital can only handle about one baby a day one birth a day it's a small hospital a small baby unit so we want to know what is the probability seven or more births occur this week what is the probability that x is greater than or equal to because it includes seven it says seven or more bursts this week now the thing about the poisson distribution is there's not actually any maximum in theory there could be one thousand births this week at this hospital it could go up that high now if the average is four the probability is basically zero but it could potentially happen so when we're doing greater than seven this is actually impossible to calculate straightforward because we'd have to do seven plus 8 plus 9 plus 10 plus 11 and we have to go all the way to infinity we can't do that so just like we did with the binomial when we want greater than we're going to actually work with the complement we're going to do 1 minus the probability that x is less than or equal to the opposite set of numbers now if we're going from 7 up this time it includes the number seven so when we count down we don't want to include seven so we're going to start at six counting down so in the calculator we'll type in one minus the poisson cdf cumulative it's going to add those all up the average is 4 and we want to go up to 6 and then we'll subtract all of that off of 1 to get the complement so we'll type in 1 minus second distribution up to poisson cdf the average is still 4 but this time we're going up to 6. and when i paste it in and we do 1 minus that value we get a probability of 0.1107 0.1107 and so if they're not ready for uh seven or more births this week that's going to happen 11 percent of the time they might need to expand their baby unit in the hospital so that's the poisson distribution very very similar to the binomial the difference is now we're talking about the number of successes in a fixed interval usually a time interval so take a look at the homework assignment to practice a few of these and then when you come to class we'll continue working with poisson and that distribution we've spent several days discussing how to find probabilities of discrete distributions where the results are countable but probabilities with continuous distributions become a little bit more interesting that's going to be the question we look at is how do we find probabilities of continuous distributions because if it's continuous we've got to consider every possibility between 1 and 2 are an infinite number of decimals and we can't add the individual probabilities so instead we're going to steal a concept from calculus that says that the probability is simply the same thing as the area under a curve a really simple example of what i mean by this to help us visualize is probability distribution functions or density functions can be expressed as an equation maybe f of x equals one half of x that is a probability distribution function probability with results that are possible from 0 to 2. here's what i mean by that if we were to graph f of x equals one half of x we would know that has a y intercept of zero and a slope of rise 1 run 2 and so this line going from 0 to 2 represents the probability density distribution function or the density function and the probabilities are just going to be the area underneath that line remember that if we take the the probability of all of our possibilities they have to add up to 1. well if i were to calculate the area of this triangle as it turns out to be we know areas for triangle is one half times the base times the height well the base is two wide and the height is one high so one half times two times one equals 1.00 the total area of this triangle is 1.0 just like the total probability of all the possible outcomes is 1.00 so now we could find the probability that maybe our random variable x is less than one what we're really looking for is ones here in the middle if i were to shade the area less than one on this triangle we end up with this smaller triangle on the left side here well the probability that it's less than one is the area under the curve that is less than one which is still a triangle so the is the triangle area one half times the base well the base is one long times the height and if i were to draw this to scale you'd see the height was exactly 0.05 and so one half times 1 times .05 we'd end up with an area of 0.25 meaning there's a 25 percent probability that i'm between 0 and 1 on this triangle now this f of x equals one half x probability density function i made it up it's fake it doesn't model anything but it does show us kind of how this idea works that probability is the area under the curve the curve can be any shape as long as the total area under it is one because that's the total probability that is equal to one so rather than dealing with a fake triangle let's deal with a real probability density function one that we do use is called the uniform distribution and the uniform distribution is a distribution where all outcomes are equally likely i could get any number with basically equal probability the distribution itself has a special notation just like the poisson and the binomial did when we have a uniform distribution we will say x tilde where x is uniformly distributed u between a and b where a is the low number and b is the high number so if we're going anywhere from 5 to 10 with equal probability the numbers a and b would be 5 and 10. now the curve that we want to be underneath for a uniform is f of x equals the reciprocal of the difference or one over b minus a and when we do that what we'll end up with is a rectangle that goes from a to b where they're all equally likely to occur and that height on that line is how likely it's to occur that one over b minus a if i were to find the area of this rectangle because it's a rectangle area is base times height so the area is the base the distance between the two we have to subtract them is b minus a and the height that's the green height that we just calculated is one over b minus a and those b minus a's divide out so the total area is 1.0 so the curve the height of the rectangle for a uniform distribution is the reciprocal of the difference 1 over b minus a now just like the discrete distributions we can find the mean or the expected value and the formula for that with the uniform is just a plus b divided by 2 or the average of the extremes that's going to stick us right in the middle to find the mean turns out the standard deviation is equal to the square root of b minus a squared divided by 12. and it's always divided by 12 regardless of the numbers just works out that way so let's try an example of this and see if we can see this uniform distribution work out for example a plumber estimates that service calls are uniformly distributed between half an hour or 0.5 hours and eight hours first let's describe the distribution we said our random variable x was going to be distributed uniformly the smallest number possible is 0.5 the largest number possible is eight so our distribution is x tilde u point five eight we can easily then calculate the mean or expected value and standard deviation using the formulas from the uniform distribution the mean is the average 0.5 plus 8 divided by 2 which is 8.5 divided by 2 which is 4.25 hours so this plumber's average service call is about four and a quarter hours four hours fifteen minutes the standard deviation on those service calls is the square root of b minus a eight minus point five squared divided by 12. when we put that into our calculator we should end up with a standard deviation of 2.165 hours so now that we know his average call and or service call and and standard deviation let's actually calculate some probabilities and what you'll find with calculating probabilities on continuous distributions it's always easier to draw a picture of the situation so if we want to find the probability a call takes less than three hours we're going to draw a picture of the probability it's a uniform distribution so we know it's a rectangle from a low of 0.5 all the way up to 8. the height is that f of x equals one over b minus a or one over eight minus point five one over seven point five and we can leave that decimal in there that's okay now it's asking for us to find the probability that we're actually less than three remember probability is area so if i mark on my graph approximately where three is and we want to be less than 3 it's going to be the area of this rectangle off to the left so the probability that x is less than 3 is the area of the rectangle base times height the base is the distance from three to point five or three minus point five and the height is what we just found out the f of x the 1 over 7.5 and when i put this into my calculator 3 minus 0.5 in parentheses times 1 over 7.5 we end up with point three three three three or about a onethird probability that the service call takes less than three hours let's try another example let's find the probability a call takes between two and four hours again probabilities are always easier if you draw a picture so here's our uniform probability it doesn't have to be to scale but it does show the lowest point five the highest point eight the height is still one over seven point five but now i want to be between two and four hours so between two and four hours we're going to shade that area in between we're looking for the probability that two is less than x which is less than four which is just this rectangle the rectangle is base times height the base is the space from two to 4 or 4 minus 2 times the height which is 1 over 7.5 putting that in my calculator 4 minus 2 is 2 divided by 7.5 we get an area of 0.2667 when we round so there's about a 26 and twothirds percent probability that a call will take between two and four hours we can even find conditional probabilities in much the same way let's find the probability a call takes more than five hours given it was less than seven hours again we're going to draw a picture going from 0.5 to 8. with a height of 1 over 7.5 but now i want to be more than 5 given it was less than seven so we don't really care about this right side we just want to be less than seven and we want to know what's the probability that i'm more than five given that i'm less than 7. the probability that x is more than 5 given x is less than 7. well with a given probability we know we look at the probability of both divided by the probability of the given information so the probability of both would be between 5 and 7. so between 5 and 7 has a base of 7 minus 5 times a height of 1 over 7.5 that's the both and then we divide by the probability of the given information the given information is that we're less than seven so that probability's got a base going all the way down of seven minus 0.5 times a height of 1 over 7.5 which is kind of nice as often occurs with given probabilities part of it will divide out and so we're left with 7 minus five is two over seven minus point five is six point five and two divided by six point five is point three zero seven seven so there's just over a thirty percent probability the call took more than five hours given we knew it was less than seven hours another concept that we haven't spent much time with is the idea of what's called a percentile a percentile is the is the value where a certain percent is below the value you often see this with standardized test scores if you took a test and you scored in the 80th percentile that means your score was better than 80 percent of the participants so we could find for our example uh let's find the 80th percentile or what value has 80 percent below it let's draw a picture same picture it's a uniform distribution so it's just a big rectangle the height is still 1 over 7.5 and we're going from 0.5 to 8. but we want to know what value let's call it k for now will give us an area that's point eighty below it eighty percent is below it what value gives us that well to get there we're going to use our area formula the fact that we know that area of a rectangle is base times height the difference is this time we know the area we know the area is .80 the base is the distance from k down to 0.5 so we'll have k minus 0.5 is the base and the height we know is 1 over 7.5 all we really need to do now is solve this equation 4k and that solution will be our 80th percentile the value that has 80 percent of service calls below it first we can get rid of the fraction by multiplying both sides of the equation by 7.5 that's going to give us 6 equals k minus 0.5 add 0.5 to both sides and k is equal to 6.5 this means that 80 of service calls are less then 6.5 hours that's what it means to be the 80th percentile now as we wrap up there's one more example i want to show you it's kind of a special case and it seems counter intuitive we want to find what is the probability a call takes exactly five hours and the key word here is exactly because that means something very specific in probability it doesn't mean between five hours and five hours zero minutes and one second it means at the exact moment of five hours let's draw a picture to represent the exact moment of five hours going from point five to eight and a height of one over seven point five the exact moment of five hours occurs somewhere in the middle well the probability that x equals exactly five is going to be whatever the base is times the height the problem is the base of that rectangle since it's just a line right at 5 hours with no width the base goes from 5 to 5 with a height of one over seven point five but five minus five is zero and anything times zero is zero the probability that anything happens at exactly a specific moment in a continuous distribution is always equal to zero nothing ever happens at an exact moment that always happens over a span of time it could be between five hours and five hours in one second but that's a span of time at exactly a specific number that will never occur in a continuous distribution seems like a paradox but it's the way it works out we're focusing though today on finding probabilities off the uniform distribution and this idea with continuous distributions that probability is area under the curve so take a look at the assignment to practice a few of these and we will see you in class to continue working with the uniform distribution now that we've gotten familiar with continuous probability distributions we're going to move on to take a look at the most important continuous distribution in all of statistics and that is the normal distribution so the question today is simply what is the normal distribution the normal distribution is a probability density function that's got this beautiful equation of 1 over the standard deviation times the square root of 2 pi times e to the exponent of negative onehalf times x minus the mean divided by the standard deviation squared now what's nice about the normal distribution is you do not need to know that formula instead we're going to cheat and use a table to help us actually calculate what that formula is going to be equal to at various points but what you should know about the normal distribution is the shape of the normal distribution just like the uniform distribution is a clear shape of the rectangle the normal distribution has a clear bell shape to the curve the normal distribution if this is my xaxis is a bellshaped curve where the mean of the distribution falls right in the middle and we describe the distribution as x tilde in for normal and then we'll just state the mean and the standard deviation for the two arguments of the function now the normal distribution has slightly different shapes based on the standard deviation the smaller the standard deviation the taller and skinnier it is the larger the standard deviation the shorter and fatter it is but there's one special distribution which we call the standard normal distribution and when we're dealing with the standard normal we won't use x we'll use z so we know we're talking about the standard normal distribution which always has a mean of 0 and a standard deviation of one and what's nice about the standard normal distribution it has a table to help us find areas this is what the table looks like the table gives us values for z going down the first column you see the first two digits maybe 1.2 and then if i wanted 1.23 i'd go to the 3 on the next and where those 2 overlapped at the 0.303907 i can get my area under the curve based on those z values and we'll look more at using the table here in just a minute but what we need to know for now is there's a table to help us find areas and what we need to do quite often is we change between a regular normal curve which uses x values and the standard normal curve which uses z values so that we can use the table in order to find the areas the way we make that change is we use one of two equations we either use z equals the x value minus the mean divided by the standard deviation or if we solve for x in that same equation we end up with the mean plus the z value times the standard deviation and with these two definitions or these two formulas it's important that we keep track of what's an x and what's a z x has meaning in context x might be the height of the average person and so x we're looking at a 64 inch person z does not have context or meaning z is simply the number of standard deviations we are from the mean and once we have a z value then we're able to go to the standard normal to find areas off the table the table gives the area between a z value and the mean and of course the mean is zero in other words if i've got this standard normal curve here the mean is always in the middle of zero off to the side we've got a z value the table gives the area between that z value and the mean of zero so if we want to find probabilities let's scroll up a bit and give us a little bit more room if we want to find probabilities off this table we'll have to decide what pieces we're interested in first thing that we'll use is the fact that the curve is symmetric in other words the z table does not have any negative values on it fortunately the negative values behave like the positive values because the curve is symmetric each half the left and right side has an area of 0.5 and we use this one less often but the total area is 1.0 because it's a probability so let's see if we can figure out how to use this information and use our table in order to calculate probabilities under the standard normal curve let's do some examples for this example if you look up according to google the average act score is 20.8 with a standard deviation of 4.8 let's do some examples off of this information first let's describe the distribution for the distribution our variable x is normally distributed the mean is 20.8 and the standard deviation is 4.8 let's find out the probability a student scores higher than 30. now with all of these probability problems it will always be easier to draw a picture first so we'll draw a picture of our normal curve our little bellshaped curve we'll put the mean right in the middle the mean is an x value right now so i'm going to label my first row we're going to put x values on it and then in the second row we'll label what those equivalent z values are when we change them from x's disease that way we don't get them mixed up so the mean for our x value is 20.8 we want a student to score higher than 30 30 is off to the right and we want the area that's higher than 30 because the area is the probability well let's change those x values into z's we know the mean changes to zero but the 30 we need to use our formula for z z says take the x value subtract the mean and divide by the standard deviation we have an x value of 30 minus 20.8 divided by the standard deviation of 4.8 we end up with a z value of 1.92 so 30 changes into a z value of 1.92 we will always round our z values to two decimal digits because that's going to match our standard normal table so on our table we need to find 1.92 let's scroll down a bit to help us see it a little better 1.9 going down the column and then we want 1.92 and so when we combine those we end up with them overlapping at .4726 that .4726 is the area between my z value and the mean it's the white area kind of in the center we don't want the white area in the center we want the tail off to the right of it and this is where we use what we know about the normal distribution we know the entire right side is 0.5 so the probability that some student scores higher than 30 is going to be equal to the 0.5 minus that white area that's been cut out of 0.4726 which leaves us with .0274 there's just shy of a three percent chance that a random student will have scored higher than 30. let's try a few more of these so we can get really good at finding probabilities on this important normal distribution round number three let's find the probability a student scores less than 25. again we'll draw a picture the mean right in the middle of 20.8 that's an x value we want to be less than 25 which is somewhere over here to the right and we want to be less than so we want the smaller part or the left side all of that area in order to do that we need to calculate our z values we already know the mean will have a z value of zero what we don't know is the z value of the 25 so we'll subtract the mean of 20.8 we'll divide by the standard deviation of 4.8 and we get point 88 so now we have a z value of point 88 that corresponds with the x value of 25 0.88 is what we're going to look up in our table in the table we're looking for 0.88 so 0.88 when we go down and across we find out the probability there is 0.3106 0.3106 and that again is the area 0.3106 the area between the mean and that z value of 0.88 but this time we also want not just that little bit but the whole area to the side that left side we know is 0.5 so when we want the probability that x is less than 25 this time we need an extra 0.5 added to the 0.3106 that we just found to get 0.8106 or just over 81 percent of students score less than a 25 on the act so you can see how the picture helps in the difference between example two and example three example two we had to subtract from point five to get the area that we wanted example three we had to add to point five to get the area we wanted let's try another example to keep working on how the picture is going to help us calculate our probability let's find the probability a student scores between 15 and 23 on their a ct so we draw a picture the mean in the middle has an x value of 20.8 we want to be between 15 and 23. 15 is off to the left and 23 is off to the right we want the area between these two numbers so now when we convert to a z value we don't have to just change the mean to zero and one other value we have two other values that we need to convert to z values so let's do that for the 15 we take 15 minus the mean of 20.8 divided by the standard deviation of 4.8 that's going to give us a negative number negative 1.21 but that's okay because it's to the left of zero it makes sense that a number to the left of zero on the number line is negative that's what a negative z value is it just means we're to the left or smaller than the mean when we do the 23 we should get a positive number 23 divided by 0.8 divided by 4.8 and it's going to be positive because it's bigger than the mean this turns out to be 0.46 a z value of 0.46 we're going to look up both of these values in the normal table first looking up the 1.21 it's negative but that's okay because the curve is symmetrical so we'll just look up the positive version and it's going to have the same area on the other side 1.21 1.21 so when we do that we end up with this center area of 0.3869 0.3869 is the area between the mean and that z value 0.3869 we still need to look up the z value of point forty six so i'll do it in blue here point forty six when i go across we end up with an area of point seventeen seventy two so the area there is point seventeen seventytwo we want the area between those two numbers which includes both halves so when i want to find the probability that 15 is less than our score which is less than 23 we need to combine both those pieces together 0.3869 plus 0.1772 will give us a total area of 0.564 one there's just over a fifty six percent chance that a student will score between fifteen and twenty three so sometimes you see we have to add pieces together but that's not always the case either let's look at this example let's find the probability a student just one student scores between 18 and 20. now if we draw this picture with our mean in the center of 20.8 that's an x value but now you notice 15 18 and 20 18 and 20 are both smaller than our mean and we want the area between them of course to get that area we have to change them to a z value the mean has a z value of zero but we have to work to get the other two points so first for the 18 18 minus the mean of 20.8 divided by the standard deviation of 4.8 that's going to be equal to negative point 58 so the z value there is negative point 58. for the second one the 20 z is equal to 20 minus 20.8 divided by 4.8 is equal to negative point seventeen negative point seventeen and we want the area between those going to our normal distribution our first value was negative point five eight point five eight so we'll go over and down and we find our first area is point two one nine zero point two one nine zero so this first one point two one nine zero now it's important to note that's not just the shaded region that goes all the way to the main it doesn't stop at the 20. the 2190 goes all the way to the main does not stop at the 20. we still need to find the other piece which is the negative point seventeen doing this one in blue the negative point one seven going down and across we find an area of 0.675 so that's area of 0.6750675 sorry forgot the 0.0675 and now we're ready to answer the question what is the probability that 18 is less than our score which is less than 20. well the 20.90 goes all the way to the mean but we don't want the white space the 0.675 that goes on the right side of the main so we're going to cut out that white space we just need to subtract we have 0.2190 minus 0.0675 that's going to be 0.1515 just over a 15 probability a student will score between 18 and 20. let's do one last example but let's make this one a little different this time we're going to find the third quartile of act scores remember the third quartile that's the value that's over 75 percent what we're really asking to find is the 75th percentile this is different than we were doing before we're not saying what's the probability of this number we have the probability we have the probability 0.75 we're looking for the x and the z values that give us 0.75 so let's draw our picture we've got our mean of 20.8 that's an x when it's a z the mean is zero and we're looking for some value out here we'll call it k some value out there where the area below is a total of 75 percent well the table is only going to give us the space between k and the mean and actually let's label k down below we're going to make k a z value we'll use that k to find the x value we know the left side of the curve is 50 or 0.5 so the right side of the curve must be the remaining 0.25 of an area but we know the area of 0.25 when we're given the area do not go down the z values z's are not areas z's are a scale of the number of standard deviations we are from the mean we need to look inside the body of the table for the area that we're given we want 0.25 and if we kind of scan through our numbers you'll see 0.25 an area of 0.25 is right in between these two numbers between 24 86 and 25 17. now if it was closer to one i'd go with the one that's closer but it's really right in the middle so we'll call right in the middle 0.6 right in between 7 8 so we're going to call that 0.675 0.675 so that k value is 0.675 how do we find the x value then we have that other formula we haven't used yet that says x is equal to the mean plus the number of standard deviations times the standard deviation and that z is that z value we just found of 0.675 so if we plug in what we know the score x that's the 75th percentile or the third quartile is the mean of 20.8 plus 0.675 times the standard deviation of 4.8 the mean is 24 i'm sorry not the mean but the x value 24.04 the third quartile of act scores is about 24. that means a score of 24 is better than about 75 percent of all of the scores on the act the normal distribution is truly the most important distribution you know how to use in all of probability you need to know how to use the table how to find the left side the right side what's in the tail what's in the middle how do you find percentiles how do you use the table backwards you need to be very comfortable and familiar with the normal distribution and how it works as we move forward in our study of probability so take a look at the homework assignment practice a few of these important problems in class we're going to keep working with the normal distribution and i will look forward to seeing you then now that we've gotten really comfortable at working with the normal distribution and finding probabilities we're ready to actually get into working with samples now the heart of statistics is collecting a sample to make an estimate or a conclusion about a population so first we need to know how do we find probabilities with sample means the big difference here is in the previous chapter in chapter 2 we were finding the probability for one individual value now we're going to take several values find the mean and look at the probability of the mean of several values and this is what gives rise to what is called the central limit theorem and the central limit theorem basically in words says that the mean of a sample should be close to the mean of a population and not only that it should have a smaller standard deviation the idea is that if we have several values averaged together the extreme values are going to be averaged out and pulled back in towards the center which makes the standard deviation smaller in fact we can go one step further and say that the larger the sample the closer to the mean we become and the smaller the standard deviation is and that makes sense if i interview nearly everybody i will be probably pretty close to the actual mean i'm not going to be off by much which is why the standard deviation is going to be so small and as we're working with samples this idea of the standard deviation or the smaller standard deviation we call the standard error the standard error will use the symbol of either sigma sub x bar or you'll often see s sub e for standard error and that's the standard deviation of the sample means and the way we calculate the standard error or the standard deviation of the sample means is we will take the standard deviation of the entire population and we'll divide by the square root of the sample size or if it's a sample we'll say s for the sample standard deviation divided by the square root of the sample size and that's a key equation that we're going to use quite a bit today using this new standard error we can replace the standard deviation in our distribution of the mean when we're talking about means we're going to say that means are normally distributed with the same mean as the whole population but then we will use the standard error or s divided by the square root of n to represent our new standard deviation then we can go forward and calculate zscores and also probabilities in much the same way we did before now the zscore is equal to the mean of our sample minus the overall mean divided by the standard error and that's going to be the key new thing the central limit theorem gives us is that new standard error as we calculate our z values because we have a sample not just one value so let's look at an example where a sample is done a cell phone company finds that those who go over their data limit go over by an average of 2.2 gigabytes with a standard deviation of 0.4 gigabytes you conduct a survey of 80 customers first thing we want to know is we want to know what's the probability the average overage is above 2.3 gigabytes or what's the probability that x is greater than 2.3 actually x bar is greater than 2.3 that the average is more than 2.3 well the first thing we're going to need to do here is we're going to figure out what is the distribution of the mean the mean should has the same mean as the population 2.2 gigabytes but the standard deviation is smaller we take the 0.4 gigabytes and we have to divide by the square root of the sample size divide by the square root of 80. so we have 2.2 comma 0.4 divided by the square root of 80 is about point zero four five that .045 that is our new standard error so when we calculate our zscore we remember that z is x bar minus mu divided by the standard error our x bar we want to be greater than 2.3 so we'll take 2.3 we'll subtract the average of the population divided by our standard error because we have a sample not an individual of 0.045 and when we divide we get 2.22 so if we think about our normal distribution the mean of the populations at 2.2 we these are x values we want to be at 2.3 or bigger so we standardized into z values and the z value actually turned out to be 2.22 so that's what we're going to look at in our standard normal table in our standard normal table we've got 2.2 and another 2. so we see the probability there is 0.4868 but remember that is always the area between the z value and the mean we want the area in the tail so the probability that the mean is greater than 2.3 we know the entire right side is 0.5 subtract off the middle of 0.4868 and we get .0132 there's about a one and a third percent probability that if i interview 80 customers i'll get a mean bigger than 2.3 and that's the idea of the central limit theorem we're shrinking the standard deviation by dividing it by the square root of the sample size whenever we have a sample we need to divide by the square root of the sample size let's keep with this example one more i also want to see if we can find q1 or the 25th percentile so same problem with the cell phones where we've got these x values the mean x value is 2.2 gigs we want to find the x value where 25 percent or 0.25 is in that first tail well the table is going to give us the other half because the table always goes between our percentile or our z value and the mean so the table is going to be 0.5 minus a quarter or 0.5 minus 0.25 which is also 0.25 so we're going to look up the z that corresponds with an area of 0.25 notice we're talking about an area we do not know the z value so when we go to the table we want we're looking for an area of 0.25 we're looking inside the body of the table and .25 happens somewhere in the middle here between 0.67 and 0.68 so we're going to call that .675 the z value is 0.675 but notice it's to the left of 0 to the left of the mean so it actually has to be a negative .675 because it's to the left of zero it's smaller than zero we still need to convert that z value which has no context into a x value that does have context and very similar to how we did it back with the normal distribution our x bar is going to be equal to the mean plus the z value times the standard deviation which in this case is the standard error so x bar is equal to our mean of 2.2 minus a 0.675 because it's negative times the standard error which we calculated the standard error to be .045 0.045 and 2.2 minus 0.675 times .045 gives us a mean of 2.17 putting units on it gigabytes the 25th percentile of means of sample of size 80 is going to be 2.17 gigabytes now it's time for you to take a look at the homework assignment to try and do some problems using the central limit theorem where we have this new standard error the new standard error is triggered because we have a sample not just one individual data value see if you can work with a few and in class we will discuss them further and practice this whole central limit theorem a little bit more now that we've talked about this idea that samples adjust the standard deviation to become what the standard error is we're ready to do some inferential statistics inferential statistics take a look at the idea of how can the sample help us make an estimate or a conclusion about the entire population and as you might guess a sample and the population will have similar but different statistics or parameters and so the question we're going to start this discussion off with is how close is a sample proportion to a population proportion in other words if i take a sample of a hundred people and ask what pers what number of them or what percent of them eat breakfast in the morning i'll end up with a percentage of people in my sample who eat breakfast in the morning that's a sample statistic and i can use that to try and estimate what percent of the entire population or all people eat breakfast that is the population parameter however the two numbers aren't going to be exactly the same there will be some type of error involved in that proportion so that's what we're going to look at today how what is that error and how can we calculate it first i want to make sure we really understand this concept of proportion when we're talking about proportions what we're really saying is that the underlying distribution is binomial and if you remember from our probability unit binomial takes a look at x successes out of n trials that's what we're looking at so if we were asking about the breakfast example we're looking at how many people eat breakfast the successes out of how many people total we're interviewing and then from that success and trial concept we can calculate the population proportion or i'm sorry the sample proportion which we represent as p hat with the little triangle over the p we calculate that by taking the successes divided by the trials to get some type of percentage proportion or decimal and it turns out that proportions are normally distributed with a mean that's equal to the proportion and a standard error that's the square root of the proportion times the probability of failure divided by the sample size and again that q hat q is failure the opposite of successes so it's one minus the proportions this underlying distribution will allow us to estimate where the actual population parameter lies to do that we will find what's called a confidence interval a confidence interval is an interval maybe between 20 and 30 percent that's an interval based on the sample statistic where the population perimeter is likely to be located so it's going to be this range of numbers where we are quite confident the actual population parameter lies and the idea behind it is that our sample statistic is likely not perfect it's likely off by some error and so what we'll do to calculate this interval is we'll say okay let's take the sample proportion and we'll subtract the error and then we'll take the sample proportion and add the error and the population parameter is likely somewhere between those two numbers so we take our sample statistic and add and subtract the error and that should give us a range of numbers where the population proportion actually lies but how big is that error well we don't really know what we have to do is we have to say we're going to be comfortable with some level of confidence or some level of air that's allowed to occur and if we're okay with being wrong five percent of the time we'll make what's called a ninetyfive percent confidence interval if we're okay being wrong ten percent of the time we'll make what's called a 90 confidence interval so the confidence kind of tells us how often were we want to be correct and accepts a certain amount of error because we can never be 100 confident unless we interview everybody so we've got this idea of a confidence level and that's going to be the probability the interval contains the population parameter will have some type of confidence level let's say for example i want to have a 95 percent confidence level that means i want to be right 95 percent of the time but i could be wrong we have this alpha which is a greek letter the greek letter alpha is the probability we are wrong and so if we want to be 95 confident alpha is going to be 1.0 minus the 0.95 alpha is going to be 0.05 a 5 probability that we are wrong visually on the normal curve what that means is if my sample proportion comes in in the middle of the normal curve we're going to put a range the proportion minus the air and the proportion plus the error and we want to be somewhere in the middle we're claiming the population proportion is somewhere in that range so in that range that's my confidence level the 95 percent which means out in the tails is where i could be wrong if it actually falls out there well if there's five percent in the tails and there's two tails we could have 2.5 percent splitting it in half in each tail our goal is going to be to figure out what that error amount is that we have to add and subtract in order to get 2.5 percent in each tail that's what we're doing so with proportions to calculate the error we have this nice formula that the error is equal to what we'll call z sub alpha over 2 times the square root of p hat q hat over n this is a formula that we should be very comfortable using p hat q hat and n we should be familiar with because those all come from our sample we've already talked about those before this z sub alpha over 2 value that is the z value that gives the correct area in each tail so for this example up above where i wanted a 95 percent confidence interval that would be the z value that gave me 2.5 percent in each tail and we can look that up in the table backwards or we can consider some common z sub alpha over two values because really most confidence intervals come in one of three types we have confidence levels of either 90 percent 95 percent or 99 and the z sub alpha over two that goes with each of them with the ninety percent to get ten percent in the tails five percent in each tail we'll use one point six four five for the ninety five percent confidence interval like the example up above we'll use 1.960 and for a 99 confidence interval it turns out the z sub alpha over 2 is 2.576 you do not need to memorize these numbers but you should have them handy as you're doing your assignments and this and the practices and labs for our class okay i think we're in need of an example so that we can see this work out so we can see how we find out how big the error is between our sample statistic and population proportion and once we know the error how do we find a confidence interval that contains or likely contains the actual population parameter do an example a survey is done and 95 out of 174 voters support a particular candidate for senate the first thing we're going to do is we are going to construct a 90 percent confidence interval for the true proportion of voters who support the candidate can this candidate be 90 confident that that she or he has a majority of the voter support well first we need to know what is the proportion p hat that we're dealing with we have to calculate this the proportion is our number of successes out of the number of trials 95 out of 174 voters support the candidate that comes out to a proportion of 0.546 or 54 percent of the voters in the survey support this candidate looks pretty good q hat the probability of failure is always 1 minus the proportion so in this case 1 minus 0.546 which comes out to be 0.454 we also need a z sub alpha over 2 or in this case alpha is the probability of failure point 10 over 2 which gives us z of 0.5 because we want 10 in the tails oops not 0.5.05 5 in each tail and so we would need to find the z value that puts 5 area in each tail we can look that up on our big z table or that is one of the common ones that we have from our chart up above so we will use z sub 0.05 is equal to 1.645 those are the three pieces that we will need in order to build our confidence interval the error between our sample proportion and the population proportion is equal to the z sub alpha over two times the square root of p hat q hat over n z sub alpha over two is one point six four five times the square root of p hat which is 0.546 times q at which is 0.454 divided by n the sample size of 174 and when you do this on your calculator for some reason it's really common students forget to multiply by the 1.645 they just do the square root make sure you do the whole thing and you should end up with an error of point zero six let's round it to point zero six two this is how much my sample might be off at 90 confident we're 90 confident our sample might be off by about six percent so we will take the sample proportion and subtract the error to get the lower bound of the worst case scenario for this candidate and we'll do our proportion plus the air to get our upper bound to get our best case scenario for this candidate our proportion was 0.546 minus 0.062 the air and 0.546 plus 0.062 the air and when we subtract we end up with 0.484 and when we add we end up with 0.608 and this range is where the population parameter or the population proportion likely lies in between let's make a better way of saying that though let's look at how we interpret a confidence interval interpreting a confidence interval is almost as important as how we calculate it because our statistics don't mean anything unless we can put it in context of the situation so what we will say to interpret a confidence interval and this becomes a nice script for interpreting any confidence interval we do in this class is we will say we estimate with some percentage that would be your confidence level whatever your confidence level is with some percentage confidence that the true let's actually say that the true population whatever we're talking about we're going to put the parameter in context so we're talking about the population mean the population proportion the population standard deviation whatever we're talking about but then we'll put it in context of the situation we're describing is between blank and blank and those would be as you might expect the low number and the high number so for our proportion we're going to interpret the confidence interval from the example above and you can still see it at the top of your screen there in purple the confidence interval is 0.484 0.608 but what that means in context is that we can estimate notice how i follow the script here we estimate with and this one was a 90 percent confidence interval so 90 percent confidence that the true population and now i'm going to describe the parameter in context we're doing proportions here what proportions support this candidate for senate but the true population proportion who support the candidate for senate notice that puts it completely in context so we know what the problem was discussing is between and the low number let's go ahead and make it a percent 48.4 percent and 60.8 percent and that's how we will interpret that confidence interval we're 90 percent confident the true population proportion who support the candidate for senate is between 48.4 and 60.8 percent so it's not a guarantee this candidate is going to get a majority you might say it's pretty likely but we don't know where in between these numbers the actual proportions going to lie we just know it's going to be between those numbers or at least we're 90 percent confident it's between those numbers so you should be able to today build a confidence interval using the formulas for proportions that we talked about today and then just as important you should be able to interpret that confidence interval using the script we've provided here so you can go ahead and take a look at a couple of those and try them we'll look forward to discussing confidence intervals more in class and continuing to work with them in inferential statistics we'll see you in class a significant part of statistics is testing a claim to see if we can really believe it's true so that's going to be our question for the day is how do we test a claim and today we're going to specifically focus on a claim for a proportion the process we're going to talk about though does work for all sorts of claims but specifically today we're going to stay in the context of a proportion the way we test claims in statistics is what is called hypothesis testing and the idea behind hypothesis testing is it's this clear process we can do to test if a claim is true what we'll do is we'll first set up two hypotheses and they're going to be contradictory hypothesis either the first one or the second one is true the first one we'll call h sub zero that is what we will call the null hypothesis and it will always include it will always use equals some variable equals something and another thing about the null hypothesis is we will always assume the null hypothesis is true until proven otherwise if the null hypothesis is not true the other hypothesis is h sub a which we call the alternate hypothesis and this is often what we're trying to show to disprove a claim and this will either use greater than less than or not equal to kind of the alternative choice if it's not equal to a number it must be different than it and then once we've set up those two hypotheses we will run a sample or an experiment and then based on that experiment we will calculate the probability the null hypothesis hypothesis is true based on our sample this calculation that the null hypothesis is true based on our sample is what we will call the p value it's going to be very important to us the pvalue what is the probability the null hypothesis is true based on our sample once we know that probability we will compare it to the allimportant alpha alpha actually no period we'll just say or the smallest probability that we will still believe the null hypothesis is true so if our probability our pvalue is smaller than alpha that is too small of a probability to still believe the null hypothesis and so we will have to reject the null hypothesis in favor of the alternative because our pvalue was too small it was smaller than alpha the smallest probability we still believe the null hypothesis is true or the pvalue might be bigger than alpha there is a greater probability that it is actual the null hypothesis is true and then we won't reject the null hypothesis so step five is simply either two maybe a little more space we will either reject the null hypothesis and we do that if the pvalue is less than alpha because the probability was too small to still believe the null hypothesis or we will fail to reject the null hypothesis and that's the case where the pvalue is greater than alpha or it's just too big of a probability to believe that the null hypothesis is false a great example to kind of show how hypothesis testing works is to consider a trial in the united states we assume in a trial that a person is innocent until proven guilty and it's actually a perfect statistical hypothesis test let's say person a is accused of a crime the null hypothesis that everyone assumes is true until proven otherwise is that person a is innocent the alternative hypothesis what we try and prove or find enough evidence is that the person is guilty and what we always do is we assume innocent until proven that proof is the pvalue what is the probability that they're innocent given there's all this proof that they are guilty and not just proof that they're guilty because you never know for sure they're guilty we just prove they're guilty beyond a reasonable doubt and that reasonable doubt that is the alpha if you go beyond alpha if p value gets smaller than alpha the proof of innocent is so small we can no longer assume they're innocent and there's actually two conclusions that we can make if proven guilty if the pvalue is so small that they're innocent the probability of their innocent is so small it's beyond a reasonable doubt we reject the null hypothesis and conclude the defendant is guilty if not at least not beyond a reasonable doubt we will fail to reject the null hypothesis and conclude and this is where it gets interesting and it's very important in statistics we don't conclude they're innocent we conclude that they are not guilty the conclusion focuses on the alternative hypothesis what's key there is that we never conclude the null hypothesis h sub 0 is true we just failed to conclude the alternative we didn't say they were innocent we just said there is not enough evidence to say they're guilty and that's an important conclusion that applies to statistical conclusions as well we will never conclude the null hypothesis is true we will always conclude that the alternative hypothesis could not be proven or could be proven speaking of conclusions let's talk about how we want to make our conclusions similar to how we interpreted a confidence interval for a proportion when we make a conclusion it's really important we make that conclusion in context but we also are going to focus on the alternative hypothesis the h sub a and so a nice script we can follow is we will say there is or there is not depending on the context we will say not if we fail to reject because we did not get the alternative hypothesis like we wanted so there there is or there is not sufficient evidence to conclude whatever we can conclude the conclusion though is always going to be the alternative or the alternate hypothesis in context to the problem let's do two examples where we can really see what this hypothesis testing thing looks like example first for doing a hypothesis test specifically with proportions everything we've done so far actually applies to all hypothesis testing but specifically with proportions there's a few formulas we need for proportions first off we know that proportions are normally distributed with the proportion acting as the mean and the square root of p q over n acting as the standard error but as you calculate these values different than a confidence interval because a confidence interval focused on the sample and what we could learn from the sample we used p and q from the sample here we're focusing on a null and an alternate hypothesis so we're going to focus on the claim that the null hypothesis is true use the null hypothesis values and then to calculate our p value or our probabilities we will have z is equal to p hat the sample proportion minus p the hypothesized proportion divided by the standard error and remember the standard error is the square root of pq over n so let's try this let's say a phone company claims that 43 of smartphone users have an iphone but you doubt this claim so you conduct a survey of 83 smartphone users 44 of them use an iphone what can you conclude if alpha equals 0.05 in other words we're going to believe the claim of 43 until the probability dips below 5 percent that that claim is actually true well let's set this up our null hypothesis has to be that our proportion equals something and that's the claim that the proportion equals point 43 for the alternative hypothesis we can either say the proportion is greater than less than or not equal to there's no direction given in your doubt when you doubt the 43 is accurate you're not saying that it's greater or less than you just doubt that it's accurate so this is going to actually be not equal to 0.43 and when it's not equal to 0.43 we have what's called a 2 tailed test and what that means is we could reject the null hypothesis if the proportion is bigger or if the proportion is smaller either direction maybe it'd be easier to see if we drew a picture and we're going to annotate this picture as we go on here's the normal curve for the proportion the claim is that the mean the proportion is 0.43 but we doubt that's true we think it's either going to be lower somewhere in the red tail on the left or higher somewhere in the red tail on the right we don't know which side we just doubt it it's in both tails left and right the distribution then of the proportion just to review we know that the proportion of our sample will be normally distributed and again we're going to use the null hypothesis here around the claim of 0.43 with a standard error of 0.43 that's our p times q 1 minus 0.43 is 0.57 over my sample size and here we did a survey of 83 people so what that really means is our proportion is normally distributed with a mean of 0.43 and a standard error of 0.0543 so what is our sample proportion well our sample proportion is going to be x divided by n or the 44 out of 83 people who use the iphones and that's going to be point 53. run out of colors so we'll go back to blue actually one thing to put in since we know our proportions point 53 on my picture off to the right i'm going to put .53 that is the x value where the red shading starts we don't know the value on the left if our proportion had ended up being less we would have put the number on the left but because our proportion was more we put it on the right now we're ready to calculate our z value and z is our sample proportion minus the hypothesized proportion divided by the standard error so point 53 the sample minus the hypothesized point 43 divided by the standard error of 0.0543 the z value there is 1.84 so if we have x's on top we'll stick z's down below i should have left a little more space when it's a z we assume the mean is zero and our value on the table is 1.84 so let's go to the table and see what area goes with a z value of 1.84 on the table we're looking at one point eight four so if i scroll over and if i draw my line straight we see 1.84 corresponds to an area of 0.4671 but remember with that area of 0.4671 that is the area in white point four six seven one we are interested in the area in the tail to get the tail we have to subtract from point 0.5 that'll give us 0.0329 but what's important to know is because this is a twotailed test we have to consider the other tail as well is .0329 as well it's symmetrical which is nice so when we want to calculate the pvalue the pvalue is the total shaded area because this is a twotailed test we have to add them together the .0329 plus .0329 that gives us a pvalue of 0.01 and remember that p value is the probability our null hypothesis is true based on our survey in other words based on our survey there is a change it to a percentage 6.58 percent chance the proportion of iphone users is actually 43 percent what the null hypothesis claims now at first glance you might say that's not a very large percent but remember we did say up at the top here that alpha is 0.05 and that means we will continue to believe the null hypothesis is true as long as the probability does not dip below 5 so if the pvalue is six percent that's not below the five percent threshold we say that still is not quite enough evidence to kick out the null hypothesis so our decision is we will fail to reject the null hypothesis because there's just not quite enough evidence it was close but not quite enough evidence to reject the null hypothesis the reason for that really clearly stated is the pvalue the probability the null hypothesis is true is greater than that alpha that minimum threshold putting the numbers in there the pvalue is 0.0658 that is greater than the alpha of 0.05 so we failed to reject and we're ready to make our conclusion following our script then we will say that there is not because we failed to reject we'll say not sufficient evidence to conclude and the conclusion must be in context of the alternative hypothesis so we're going to state the alternative hypothesis that the proportion's not 43 percent of course we must put it in context so to conclude the proportion of iphone users is different than 43 now one thing you might notice is there's a couple of p's going on in here in problems like these and it's very important we keep them all straight we have a p value that's just the probability the null hypothesis is true we have a p in the null hypothesis that is the claim for the population proportion and we also have a p hat that is the sample proportion be very careful not to get the three p's mixed up quite often we'll see students compare the wrong p to alpha and they'll make the wrong conclusion as a result make sure you compare the p value to alpha to make a conclusion about your p based on your phat sounds confusing but practice a few to make sure you get them straight just to kind of make this interesting and this isn't always required but it often is with a hypothesis test is let's make a 95 confidence interval for the true proportion based on our sample so based on our sample p hat our sample we said was 53 percent which means q hat the opposite of that is going to be one minus that or 47 percent 0.47 and if we're doing a 95 percent confidence interval we should know the z sub alpha over 2 or z sub 0.05 over 2 or z sub 0.025 is equal to 1.96 and we found out in our previous lesson that the error is equal to that z value of 1.96 times the square root of pq over n but with the confidence interval notice i will use the sample data this is different than the hypothesis test where we use the hypothesis with the confidence interval we use the sample data of 0.53 times 0.47 divided by the sample size which was 83 and that will give me an error of 0.107 so my confidence interval then is the proportion .53 minus the error of 0.107 and the sample proportion of 0.53 plus the error of 0.107 giving me a confidence interval for the true population proportion to be between 0.423 and 0.637 or in context we're saying that we are 95 percent confident the true population proportion of iphone users always put it in context is between 42.3 percent and 63.7 percent and what you notice is with that confidence interval our null hypothesis said what we were assuming to be true the null hypothesis said the proportion was equal to 0.43 notice that 43 is within that confidence interval that's why we cannot reject it because it still is a valid possibility for the true population proportion of iphone users i want to do one more example i know this video is running a little bit longer than normal but it's really important that we're comfortable with these hypothesis testing so it has been claimed that 58.4 percent of web users prefer chrome however you believe the number is lower so you're going to test it you sample 152 web users and 74 of them use chrome with alpha equal to 0.01 this time we want to be very confident we're going to go all the way down to one percent error what can you conclude well like before let's start with our hypotheses the null hypothesis is that the proportion equals what they claim it equals the claim is that it equals 0.584 the alternative hypothesis is based on what you're trying to show and this time we're going to try and show that the actual number is lower that the proportion is less than 0.58 which means this time we really have a onetailed test or better said a lefttailed test meaning we're going to reject the null hypothesis if we end up far out into the left tail drawing a picture the hypothesized mean is at .584 we're going to reject if it's less than significantly to the left or in that left tail so we have our distribution we know that the proportion is normally distributed at 0.584 with the standard error of using the null hypothesis 0.584 times q which is 0.419 oops 416. sorry divided by the sample size of 152 which means it's normally distributed at 0.584 comma 0.0400 when we round if that's the distribution then we're going to compare it to the proportion or the phat that we get from our sample our sample said 74 out of 152 use chrome 74 out of 152 is 0.488 that's the value off to the left it's less than 0.487 where the shading starts is that far enough away that we can make a conclusion that it's actually less than well to do that we will go to our z value which is equal to our sample proportion of 0.487 minus the hypothesized proportion of 0.584 divided by the standard error of 0.0400 our z value is negative 2.43 so we've got our x's on our picture putting those all into z's the mean is 0 and the z value of negative 2.43 is where the shading starts let's go to our z table and i'm going to scroll down a bit to see 2.43 so 2.43 looks like this time we're going to have a z value of 0.49 i'm sorry an area of 0.49 that's the area in between of 0.4925 we need the area in the tail which is just 0.5 minus that or 0.0075 this time we don't need to worry about the other side because it is a onetailed test so my pvalue is just that shaded area the 0.0075 which means based on our survey there is a 0.75 chance the null hypothesis is true or the proportion of chrome users or people who prefer chrome probably would have been a better way to say that is 58.4 percent that is really really small pvalue in fact what's really important is when we compare that pvalue to our alpha of 0.01 it is actually less than that alpha of 0.01 which means that's too much evidence to the contrary so we will make a decision to reject the null hypothesis and the reason for that decision is because the pvalue the probability it's true is less than the alpha or specifically 0.0700 is less than the 0.01 minimum threshold so our conclusion if we reject the null hypothesis we will say that there is sufficient evidence to conclude and then we will state the alternative hypothesis in context focusing on being less than the 0.8.584 to conclude the proportion of web users who prefer chrome is less then 58.4 percent one last little thing as we wrap up our conversation on hypothesis testing is when all is said and done we make a conclusion based on a pvalue what's the probability that it's the null hypothesis is true do we reject it do we fail to reject it but when all is said and done at the end of the day in statistics we could be wrong and we really have no way of knowing if we're right or wrong we can be fairly confident 95 or 99 confidence but we could be wrong and there's two types of errors and statistics that we always try and minimize we call them type 1 and type 2 errors a type 1 error is when we reject the null hypothesis when it is true we should not have rejected it but we did the probability of that happening is actually the alpha that we're using in the problem the other type of error is what's called a type ii error and that's where we fail to reject the null hypothesis when we should have when it is false and the probability is not as evident for that we call that probability beta it comes up in more advanced statistics classes we're not going to spend a lot of time on that but you should be aware at this point of what a type 1 n type 2 error is in context so we did two examples today the first example was about iphones and we failed to reject the null hypothesis we could have committed a type 2 air where we conclude not reject or fail to reject when we should have or to put it in context and this is probably a better way to say it or we believe the proportion of iphone users is 43 percent because we failed to reject it but we should have when it is not it is not actually 43 we should have rejected that's a type 2 error the second example because we did reject could have been a type 1 error where we conclude reject when we should not have or to put that in context we currently after running that sample we believe the proportion who prefer chrome is less than 58.4 because that's what we concluded but it's not that would be a type 1 error where it's actually either equal to or possibly greater than 58.4 percent and we made the wrong conclusion those errors hopefully don't happen often to us but there's always a chance that they could happen because with hypothesis testing we're never sure of anything we could be wrong and that's what the type 1 and type 2 errors tell us is what does it mean if we're wrong so we covered a lot of stuff in this video today we introduced the concept of what a hypothesis test is and how hypothesis test works and then we did several hypothesis test examples in the context of proportions and then really briefly at the end we talked about we could be wrong the type 1 and type 2 error so take a look at the assignment if you want to try a few of these a little bit of a longer video but some of the next few are going to be much shorter to make up for it so we'll see you in class now that we've taken a look at how hypothesis testing works specifically in the context of one proportion we can extend it to hypothesis testing in many different situations and one of those situations involves having two proportions or two groups and trying to decide are the two groups the same or are the two groups different so our question for the day is how do we test a claim about two proportions we're going to have two groups and we want to know are these two groups the same is one group bigger is one group smaller what can we conclude about these two proportions and the idea of the hypothesis test is identical to the idea that we did with one proportion the only difference is we have a few different equations needed to find the test statistic first thing we need to know is what we're going to call the pooled proportion the pooled proportion is what the proportion would be if the groups weren't separate so if we weren't comparing men and women but we were just looking at people how many successes are there out of the total group the pooled proportion we represent with p sub c the pooled proportion is equal to the sum of the successes from the first group and the sum of the successes from the second group divided by the number in the first group plus the number in the second group and as a tip if you do this on your calculator you will probably need to put parentheses around the numerator and denominator to make sure that division happens in the correct order because a proportion should always be between 0 and 1. if your proportion's not between 0 and 1 check the order of operations so this is our first important formula finding the pooled proportion in order to test the two proportions if there was no separation of the groups the second formula you need to know is the distribution for two proportions when we compare two proportions we don't just compare them what we actually do is we take the first proportion of our sample and subtract the second proportion of the sample we're actually looking at the distribution of the difference between the proportions and they are normally distributed as you might expect and if they are the same we should have a difference of zero the standard error formula is a little bit more involved but not difficult it's the pooled proportion times one minus the pooled proportion which might be the pooled q times one over the first sample size plus one over the second sample size and so that is the second key thing we need specifically because it is going to help us find the standard error that big square root is the standard error that we need to calculate the test statistic with the normal distribution the test statistic is z and the test statistic here is going to be the difference in the proportions p hat a minus p hat b divided by the standard error so that's the third new equation we need other than that everything is identical to what we've seen before so we should be able to jump right in to an example where we will compare two proportions let's say a restaurant wants to know if teens are more likely to order dessert then adults knowing this information can help them plan their future marketing campaign so they contact a sample 84 adults 33 order dessert they contact a sample of 91 teens 46 order dessert can they conclude teens are more likely to order dessert if our alpha level equals point 10. notice this example is different than when we had one single mean when we had one single mean there was a global claim that said the proportion is equal to such and such a percent here we don't have any such global claim here we're doing a sample of two separate groups and then we're going through and comparing are these two separate groups different or are they the same so setting up our null hypothesis the null hypothesis is always going to have equality so the null hypothesis is that the proportion of the teens who order dessert is equal to the proportion of adults who order dessert notice i use a subscript on each of the p so i know which one is which t for teens and a for adults that way when i set up my alternative hypothesis we want to know are the teens more likely to order dessert they want to know if the proportion of teens is greater than the proportion of the adults what we have actually is a right tailed test because we are going to reject in the right tail drawing our little picture the mean the assumed difference the hypothesized difference is that there is zero difference between them the order of my hypothesis tells me the order of the subtraction we always subtract left to right so what we're really doing is the proportion of teens minus the proportion of the adults we're claiming the teens are bigger than the adults so we're taking a big number minus a small number it should give us a positive number and something in that right tail that's we're going to attempt to find let's uh calculate some of the pieces that we're going to need in order to solve this first the proportion of teens there are 46 out of 91 teens that order dessert that's going to be 0.505 the proportion of adults who order dessert that's going to be 33 out of 84 or 0.393 and then for our pooled proportion if there was only one group not two separate groups and we just interviewed people the number of successes would have been 46 plus 33 over the number of trials would be 91 plus 84 making sure i use parentheses so i don't get in trouble with order of operations the pooled proportion is 0.451 we can use that information let's not scroll too far keep that hypothesis on there we can use that information to find the distribution we know the difference in the proportion between the teens and the adults is normally distributed with a mean of 0 and a standard error equal to that big square root the pooled proportion of 0.451 times 1 minus the 0.451 times 1 over the first sample size of 91 plus 1 over the second sample size of 84 or normally distributed with a mean of 0 and a standard error if you put that in your calculator of 0.0752 now that we know the standard error in the distribution we can calculate the z value the test statistic let's label it test statistic the difference in the proportions we're going to do the same order as the hypothesis so we have to do the teens minus the adults 0.505 minus 0.393 over the standard error of 0.0752 now just so i can label it on my picture let's do the subtraction in the numerator 0.505 minus 0.393 the actual difference is 0.112 divided by the standard error of 0.0752 so on my picture the actual difference is 0.112 and when we divide by 0.0752 we get a z value of 1.49 so if we've got x's in the top rows these in the bottom row z's also have a mean of zero but now the z value is 1.49 that 1.49 is what we want to look up in the table to find our pvalue looking at our table then we have 1.49 so 1.49 that's going to give us an area of 0.4319 so our area of 0.4319 but the pvalue is the area in the tail so we subtract from 0.5 to get 0.0681 so our pvalue is .0681 which means the probability or given our sample i should say given our sample the probability the proportion of teens and adults who order dessert is the same is 6.81 percent probability they both order dessert at the same rate is 6.81 percent that's what the p value means we said we wanted our alpha to be point 10 which means we will believe the null hypothesis we will believe the proportions are equal as long as the probability is bigger than ten percent we got a probability of six percent so our decision is to reject the null hypothesis and the reason for that decision is our pvalue is less than our alpha there's a six point eightyone percent chance or uh let's not do it as a percent a point zero six eight one pvalue which is less than the alpha of point ten so we reject the null hypothesis and we're ready to make our big conclusion the script for the conclusion remains exactly the same because we rejected the null hypothesis got the alternative we were looking for we will say there is sufficient evidence to conclude and then we will go back and state the alternate hypothesis in context that teens are greater than adults the proportion of teens who order dessert is greater than the proportion of adults who order dessert the conclusion is in context it focuses on the alternative hypothesis you should feel like what we just did was very very similar to the test hypothesis testing for a single mean the only difference that we had to do was we had to find this pooled proportion and a new standard error to calculate our test statistic but the process of the hypothesis test remains the same regardless of what we're testing so take a look at practicing some of these we're going to do some of this in class we'll look forward to seeing you then just as we have done inferential statistics with proportions we can do many of the same things with means in fact we more often are working with means than proportions so let's answer the question first how do we find a confidence interval for a mean very similar in idea to find a confidence interval for a proportion but there's one key difference with the means is normally when we do a sample and we have a mean of the sample we do not know the standard deviation of the population we only know the standard deviation of the sample which means the normal distribution will be a little bit too tight or a little bit too small to calculate a reliable confidence interval because we're only estimating the standard deviation of the population with the standard deviation of the sample so if we have no standard deviation of the population we can no longer use the normal distribution we need a different distribution and the distribution we will use is called the student's t distribution or often you'll hear it just called the t distribution the student's t distribution is very similar to the normal distribution but it allows for greater flexibility as we will use the sample standard deviation to estimate the population standard deviation and that's never perfect it's probably close but it's never perfect and that's why we need that extra flexibility that the student t distribution gives us is it allows us to still make a confidence interval with a little bit extra flexibility and it turns out that and it makes sense as well the larger the sample size the less flexibility is needed and that makes sense as if we interview more and more people getting closer and closer to the population our estimate for the standard deviation is probably going to be more and more accurate and the more accurate we are the less flexibility we need it turns out that we have to adjust the student t distribution then based on the sample size and we call that estimation the degrees of freedom the degrees of freedom is often abbreviated df for degrees of freedom and it's very easily calculated as n minus 1. the degrees of freedom is n minus 1 one less than the sample size and it turns out that if the sample size is greater than 30 the student t is almost identical to the normal distribution and so when we have a sample size bigger than 30 we end up using normal values because they're so close together but if the num sample size is less than or equal to 30 then we will use the student t table for finding critical values this table shows us the amount of area that we're going to get in a single tail of the t distribution you'll notice the shape looks very similar but our degrees of freedom are going to determine the critical values that we need to calculate so what we'll do is we'll first find out the degrees of freedom of our problem maybe we've got 11 degrees of freedom then we'll figure out how much area we want in one tail maybe we want one percent in one tail the table would then give us the critical value that we can use to calculate a confidence interval one more thing you'll notice is the very last row of the table is labeled z because that's when we pass a sample size of 30 or 30 degrees of freedom and at that point the t distribution starts to look like the normal distribution and you should recognize several of the numbers in this row as the critical values we used with proportions those are the z values that give us the area we want in each tail so once we're past 30 degrees of freedom we'll just use those normal values well now that we're kind of familiar with this idea of the student t distribution that we have to use if we don't know the population standard deviation let's talk about how we can use that to make a confidence interval for means first off the distribution for the mean if we don't know the standard deviation we'll just say is t with the subscript that is the degrees of freedom and remember the degrees of freedom is equal to one less than the sample size so that's the distribution we're working with the t distribution so we need to calculate an error that exists between the population and the sample mean that error actually we'll do a colon that error is equal to our t sub alpha over 2 very similar to our z sub alpha over 2 but this time we'll use the t table and the correct number of degrees of freedom times the standard deviation of the sample divided by the square root of the sample size and once we know the error we can find the confidence interval and very similar to proportions with the confidence interval we will subtract and add the error to our statistic so we'll take our x bar and we'll subtract the error to get our low value and our x bar and we'll add the error to get our high value and that's the confidence interval using the error we just calculated these three pieces will work together to get us our confidence interval so let's try an example you are interested in the average cost of a smartphone so you take a sample of 16 smartphones and find a mean cost of 531 dollars with a standard deviation of eightythree dollars we are going to number one construct a ninety percent confidence interval a couple things we need to know to conduct this 90 confidence interval first our alpha the amount of area in both tails if it's a 90 confidence interval is going to be 0.10 so the alpha over 2 looking at just one tail is half of that or 0.05 for our t distribution we need to know the number of degrees of freedom the degrees of freedom is always one less than the sample size so we've got 16 smartphones minus one we have 15 degrees of freedom and so now we're ready to calculate our t value that is 0.05 in the tail and 15 degrees of freedom going down our degrees of freedom on the table we want 15 degrees of freedom and we want 0.05 area in that tail so we go down and across and we find a t value of 1.753 1.753 now we're ready to calculate the error the error is that t value 1.753 times the standard deviation of my sample size which was 83 dollars divided by the square root of my sample size which is 16. and putting that on my calculator we get an error of 36 dollars and 37 cents so if that's the maximum error between my sample mean and the population mean we just have to subtract and add it to my sample mean the 531 dollars minus the error of 36.37 and the 531 dollars plus the error of 36.37 gives me a 90 confidence interval of 494.63 up to 567.37 and very similar to how we constructed a confidence interval with proportions and then interpreted it we will also interpret the confidence interval for the means following almost the exact same script we estimate with 90 percent confidence the true population and then state the parameter in context mean smartphone cost is between 494 dollars and 63 cents and 567 and 37 cents so as we can see constructing a confidence interval with a mean is very similar to how we constructed a confidence interval with a proportion we've got a different distribution a slightly different formula for the error but the exact same idea so you should be able to try a few of these and we'll talk about confidence intervals a little bit more in class now that we've gotten comfortable with working with the t distribution and making a confidence interval for a mean we're ready to do a hypothesis test for a mean so the question we're going to answer is how do we conduct a hypothesis test for a mean and the process of a hypothesis test is always the same whether we're talking about one proportion or two proportions or in this case one mean the only difference is we have a few different formulas for the mean to help us calculate that important test statistic that will help create the pvalue first off for the mean the distribution because we don't know the population standard deviation the distribution of the mean is a t distribution with a subscript for the degrees of freedom one less than the sample size the way the standard error is calculated the standard error is equal to the standard deviation of the sample divided by the square root of n and then we'll use that standard error to calculate our test statistic and our test statistic is going to be t equal to the difference between the mean of the sample and the hypothesized mean divided by the standard error so these are the three pieces that will help us find the pvalue to conduct our hypothesis test now because the tdistribution has a slightly different number based on the degrees of freedom we need a slightly better way than just looking up a value on a table to calculate the pvalue in our t's the way we're going to do that is we're going to use our calculator first we have to set up the test and the way we set up the test is you're going to hit the stat button and then you will scroll over to tests then you will scroll down to the t test once you set it up you have to enter the stats from your study now the calculator has an option of entering the data or entering the stats we're going to enter the stats so if needed select stats so the calculator knows you're going to actually enter in the stats and the stats you're going to enter in first it'll ask for mu sub 0. that is the null hypothesis it's also going to ask you for x bar which is the sample mean it's also going to ask you for what they call sx which is the sample standard deviation then it'll ask you for n which is the sample size we know that one and finally it'll ask for mu which is the symbol in the alternate hypothesis whether that's less than greater than or a not equal to i'll show you how this process looks on the calculator but it really helps to have an example so let's do that let's build an example it is claimed the average page in a novel has 275 words per page to test this claim you sample 24 pages of a novel and you find the average page has 260 words with a standard deviation of 34 words do you believe the claim is true if alpha equals 0.05 well we start off every hypothesis test with the null hypothesis here we're talking about a population mean and the claim is that the mean is 275 words per page for the alternative hypothesis we're not really saying it's less than or greater than you just want to know is the claim true so what we say is mu is not equal to 275. and because we have that not equal we're really dealing with a two tailed test where we've got our normal distribution the hypothesized mean of 275 is in the middle but it actually turned out to be 260 which is less than it but because this is a twotailed test we're going to shade both sides those are our x values we're going to calculate t values off of the distribution the mean is distributed as a t distribution because we don't know the standard deviation of the population but we do know the degrees of freedom is one less than the sample size so the degrees of freedom is 23. let's go to our calculator then to calculate the t value and the pvalue again that keystroke that we're going to do is first you're going to hit the stat button which is right next to the arrow then we'll scroll over to test then we'll scroll down to the ttest we are going to input the actual statistics not the individual data values so we'll highlight statistics mu sub 0 is the null hypothesis at 275 x bar is the sample mean our sample was 260. sx is the standard deviation of the sample which was 34. and in the sample size was 24. we're going to make sure we highlight the alternate hypothesis symbol not equal to and scroll down to calculate when we do you'll see the calculator gives us several things but what we're interested in most is t and p t the test statistic is negative 2.16 and p the probability the null hypothesis is true given our sample is 0.0413 so let's record that our test statistic is t equals negative 2.16 we can add that to our picture to the left of zero and the p value is point zero four one three and what that p value means then that's the probability the null hypothesis is true so based on our sample the probability the average page in a novel having 275 words that probability is 4.13 percent and remember we said alpha was five percent five percent is the minimum probability where we will still believe the null hypothesis is true four percent is less than that so we can no longer believe the null hypothesis is true so we will make a decision to reject the null hypothesis and the reason for that decision is that the pvalue is less than the alpha the pvalue is 0.0413 which is less than the alpha of 0.05 that is too little probability there is overwhelming evidence to reject the null hypothesis and so we make our conclusion following our script we say that there is sufficient evidence to conclude and then we state the alternative hypothesis in context the alternative hypothesis was just not equal to or different than so there is sufficient evidence to conclude the average number of words per page in a novel is not or maybe we should say is different and then 475 words let's actually take this one step further and build a confidence interval for where we believe the actual mean number of words lies let's build a confidence interval and let's just do a 95 percent confidence interval now we know the distribution that we're dealing with we know the degrees of freedom off of that are one less than the sample size the degrees of freedom we said was 23 because the sample size is 24 pages so our degrees of freedom is 23. alpha the percent of chance that we're going to be wrong is 0.05 the opposite of the 95 percent so alpha over 2 is 0.025 so we're looking for a t sub 0.025 that has 23 degrees of freedom in our table looking at our table then we want 23 degrees of freedom we want 0.025 and a tail and so that tells us that the critical value we're using this time is 2.069 so t sub 0.025 is 2.069 we're ready to calculate the error that might exist between our sample mean and the actual population mean remember the error is t sub 0.025 times the standard deviation divided by the square root of the sample size so 2.069 times the standard deviation we said the standard deviation was 34 words divided by the square root of the sample size which was 24 we end up with an error of about 14.4 words so for our confidence interval we will subtract and add that error to the mean we got in our sample our sample mean was 260. we'll subtract the 14.4 to get a low number we'll add the 14.4 to get a high number and so our confidence interval is 245.6 through 274 and you notice the hypothesized mean of 275 is outside of that confidence interval which is related to why we ended up rejecting the null hypothesis let's go ahead and interpret it we can estimate with 95 percent confidence the population and we're talking about a mean and put it in context number of words per page in a novel is between 245.6 words and 274.4 words and now we have our confidence interval so hypothesis testing with a mean it should feel very similar to hypothesis testing with a proportion because the process of a hypothesis test is identical regardless of what we're studying the means are nice because we can use the calculator to make things a little bit shorter and quicker for us but the philosophy behind the hypothesis test is still exactly the same so try and take a look at a few of those we'll look forward to trying a few of these in class and answer any questions that you might have we'll see you then the next type of hypothesis test we're going to turn our attention to is a hypothesis test for two means as we attempt to answer the question how do we compare two means and just as this is the case with all the other different hypothesis tests the process is exactly the same the only difference is we have some formulas to help us set up our test statistic first the distribution for comparing two means when we're comparing two means we want to know if two separate groups have the same average is there a difference between the two groups or is one group higher or lower than the other group and when we're interested in comparing them what we're really comparing is the difference or the mean of the first group minus the mean of the second group and because we don't know those population standard deviations it's going to be a t distribution with a subscript that represents the degrees of freedom now the degrees of freedom is an ugly formula if you really want to know what it is you can look it up in your book it is in the section in your book preceding the practice assignments that we're doing for today it's ugly we are going to cheat and we will use a calculator we'll also use a calculator to find the t statistic but just so that we have it the standard error for two means is the square root of the first standard deviation squared divided by the first sample size plus the second standard deviation squared divided by the second sample size and then we use that standard error to calculate our test statistic t which is the difference in the means a and b divided by the standard error but as was the with the case with the t distribution with one mean it will be also the case with the t distribution with two means that we will use the calculator to do the hard calculations for us the setup is identical first you will hit the stat button then you will scroll over to tests and then you will scroll down but this time you're going to scroll down to select the two sample ttest once you're in the twosample ttest you will enter all the stats we have from our problem and again if the calculator is expecting you to enter the data we're not going to enter the data so you might need to highlight stats if needed when you get in there the first thing it's going to ask you is for x bar 1 that is the first sample mean then it will ask you for sx1 that is the first sample standard deviation and it will ask you for n1 which is the first sample size so we enter in all the information about the first group that we're going to compare to the second group as you might expect you'll see x bar 2 s x 2 and n 2 is for the second sample that we will compare it with then it will give us a mu which is the alternate hypothesis symbol we need to tell the calculator is this a onetailed test or a twotailed test in which direction it is and finally the last thing the calculator will ask us for is if we have pooled data and for our purposes the answer is always going to be no we are not pulling the data and i'll show you what this looks like on the calculator again but again it's going to be easiest to see it if we have an example to work with so for our example you want to know if there is a difference in gpa of online students and facetoface students so to determine this you survey 32 online students who have an average gpa of 3.45 with a standard deviation of 0.7 you also interview 41 facetoface students who have an average gpa of 3.67 with a standard deviation of 0.4 if alpha equals 0.10 can you conclude the groups are different we're comparing the average between the two groups not just a claim and testing against the claimed gpa we just want to know is there a difference between these two groups we have two means that we're comparing well the mean of the online students is hypothesized then to be the same as the mean of the facetoface students again i'm using subscripts to make it really clear which group i'm talking about the null hypothesis no difference they are equal the alternative hypothesis is going to be either one is greater or they're not equal to each other this example didn't give me any inkling of a direction they didn't say face to face or higher or gpa or online students have a lower gpa or anything like that so we are just looking to see if the online students are not equal to the facetoface students which means we have a two tailed test in other words we have our t distribution with our hypothesized difference we hypothesize that the difference between the gpas is zero and will reject on either tail whether it's higher or lower we will reject on either tail well the actual difference let's add that to our picture the actual difference in gpas and we have to subtract in the same order of the hypothesis so the online gpa has to come first the online gpa was 3.45 and we subtract the facetoface gpa of 3.67 and we end up with .22 negative 0.22 so negative point 22 is the x value we're looking for we need to figure out what the t values are well our distribution the difference between online students and facetoface students is a tdistribution but we don't really know the degrees of freedom because the it's got that ugly formula so we're going to go to our calculator again on the calculator the way we get to the test is we'll go to stat it's right next to the arrows we'll scroll over to test and we'll scroll down to the two sample ttest make sure stats is highlighted because that's what we have our first group the online students we said the online students have an average gpa of 3.45 with a standard deviation of 0.7 and we said that there are 32 of them the second group the facetoface students have an average gpa of 3.66 and a standard deviation of 0.4 and there were 41 of them we select an alternate hypothesis of not equals pooled is going to be no for our purposes and when we hit calculate you'll see the calculator gives us the three key pieces of information we need the degrees of freedom to finish out the distribution notice it's an ugly decimal that's very common with two samples and a t value and a p value let's copy that information over so the distribution had 46.5 degrees of freedom the t value that came off the calculator was negative 1.59 so negative 1.59 to the left of 0 and a pvalue equal to 0.1193 what does that pvalue of 0.1193 mean well remember the pvalue is the probability the null hypothesis is true given our survey so based on our survey the probability the average gpa is the same for online and face to face students is 11.93 percent and we compare that pvalue to the alpha which is the smallest probability where we would still believe the null hypothesis is true we have a greater probability so we're going to go ahead and say our decision because the probability the pvalue is bigger than alpha we will fail to reject and the reason for that is the pvalue is greater than alpha there's more evidence for the null hypothesis the pvalue is 0.11 0.1193 alpha's only 0.10 and so for our conclusion in context because we failed to reject we say there is not sufficient evidence to conclude and then we will state the alternate hypothesis in context that there is a difference between the means there is a difference between the mean gpa of online and face to face students again we should start to feel really familiar and comfortable with this process of going through a hypothesis test it's exactly the same regardless of if we're testing one or two means or proportions the process is exactly the same the only tweak that's different each time is actually calculating the distribution and the test statistic but we should be very good at the process of setting up and conducting the hypothesis test by now so you can take a look at those on the assignment we'll work out with them a little bit more in class and we'll look forward to seeing you then another hypothesis test we can do is hypothesis testing which with what are called matched pairs matched pairs are where we have before and after data and we are looking to see are things the same or was there some type of improvement or maybe we'll pair together couples or and see if there's a difference between you know the husband's score and the wife score or maybe we'll pair together twins to see if there's any difference between the twins or maybe we'll compare your left hand to your right hand but the data is paired together and we're looking to see if there's any type of difference that is matched pairs and so the question we're going to ask is how do we hypothesize test for improvement or maybe be better to say a difference has there been any change and this is that idea of matched pairs all the data comes in pairs and the pairs are matched together usually we have that before or after score and the way matched pairs work is we're going to do a ttest for one mean just like we did a ttest for one mean before the only difference is we will first find the difference for each pair and then we'll use those differences as our one variable to figure out whether or not there is a positive negative or no difference between the before after data so with that in mind we've got some equations that we need to know to run this test the distribution it's very similar to the t test distribution because it is a t test the only difference is we're going to put a little subscript of d on the x bar to represent the average distance is a t distribution with a subscript representing the degrees of freedom where the degrees of freedom is simply the sample size minus one and then we can calculate the standard error and the standard error is simply the standard deviation of the differences divided by the square root of the sample size and we can use that standard error to find our test statistic which very similar to the test statistic for a single mean is t is equal to the difference between the average difference and the hypothesized difference divided by the standard error and the hypothesized difference is usually zero we usually assume there's no difference between before and after scores and we're looking to see is there a difference or is it positive or greater than zero or is it negative or less than zero now like before with the ttest though we're going to save all that work with using our calculator to do a lot of manual crunching of the data for us so first thing we're going to have to do on our calculator is we're going to have to tell the calculator all of our data so here's how you enter the data into your calculator you're going to start by hitting the stat button and then you will select edit to edit a list and the list the edit feature will already be highlighted when you hit stats so you just really have to hit enter and then in l1 you're going to enter the before data or the first set of data and then in l2 you will enter the after data or the second set of data and then for l3 what you're going to do is you're going to scroll up and highlight l3 and do l2 minus l1 and the calculator will automatically subtract and find all the differences and fill list three with the differences now the way we get that is you'll hit the second button and you'll hit the number two which will give you l2 minus the second button and then you'll hit the one which will give you the l1 and now in l3 you will have a list of all of the differences between the before and after a positive difference means it got bigger a negative difference will mean it got smaller then you're ready to actually run the ttest so you can hit stat scroll over to tests and then scroll down to ttest and even though there's two sets of data before and after we're actually just working with the differences so it's just one ttest don't do the twosample ttest it's a single sample ttest that single sample is the difference between the before and after checking to see if the numbers went up or down so when you're running the ttest you have to enter in some information first thing you want to do is you want to highlight data because you're going you have entered the data into the calculator we don't have the summary statistics we actually enter the data this time and that is different than our last ttest then you'll see mu sub zero that is the hypothesized difference which is usually zero we usually hypothesize that there's no difference or that the difference equals zero then it'll ask for the list the list you want to be l3 where you have all those differences entered and the way we select l3 is you hit the second and then the number three will give that third list and finally we'll enter in mu which is the alternate hypothesis symbol are we looking for it to be smaller after the treatment bigger after the treatment or just not equal to zero after the treatment and as before it's this is a lot easier to see with an example so let's do an example where we check these matched pairs for some type of improvement a football coach wants to know if a strength class can help improve his players bench press weight the before and after data is below so we're going to have players we'll just mark the players with letters to protect their identity a b c and d and we'll have weights for before the class and after the class so before the class a bench 205 after a bench 295. that looks pretty good before the class b benched 241 after b benched 252. not as dramatic but still an increase c benched 338 before the class and 3 30 after the class c went down a bit and d benched 368 and afterwards benched 360. so the question is if alpha equals 0.05 can the coach conclude the class was helpful we're going to run a hypothesis test of matched pairs to see if there's a significant difference first the null hypothesis is that the average distance difference is equal to zero there's no difference the alternative hypothesis is that the average difference is going to be positive or greater than zero because the coach wants it to be helpful he wants the difference to be positive he wants it to be higher after than before which means we have a one tail test or better said a right tailed test if we were to draw a picture of this the hypothesized difference is zero somewhere over the right we hope to see improvement and we hope that right tail shows there was enough improvement out of the class well our distribution is that the average difference is a t distribution and the degrees of freedom is one less than the sample size there are four players one less than that is three it's very small degrees of freedom it's going to take quite a difference in order to say there's a difference so from here let's go to our calculator and see if it can help us find the differences to enter information into the calculator we're going to hit the stat button right next to the arrows edit is already highlighted if i already had numbers in my list let's say there were numbers already in here that i didn't want if you scroll up and highlight the list and hit the clear button and then enter it'll clear out the list so there's nothing left in the list the first list is for the before data so we'll enter in our data 205 241 338 and 368. the second list is our after data make sure they're entered in the same order 295 252 and 330 and 360. then list three is where we're going to put our differences so if we scroll up to highlight list three and then we're going to say take list two and subtract list one hit second and then the number two that'll grab list two minus second and then the number one and now i see list three is equal to two minus one when i hit enter i will see all the differences and that's the data we're going to use in our single mean ttest to run the ttest we'll hit stat scroll over to test scroll down to the ttest and this time we've actually entered the data into the calculator so we'll scroll over and highlight the data enter the hypothesized mean is zero for the list our data is in list three that's where the differences are so we'll hit second and the number three to give us list three leave the frequency alone for the alternate hypothesis we want to show that the mean is greater than zero so we need to select the greater than alternate hypothesis and then we're ready to actually calculate when we do that we get our t value of 0.91 and a pvalue of 0.2149 we're also told that the average difference is 21.25 so we can add that to our picture the average difference is 21.25 but when we convert that to a t value the t value was only 0.91 so the test statistic is that t equals 0.91 which gives us a pvalue an area for that tail of 0.2149 what that pvalue means is given our sample the probability the strength class made no difference in bench press weight because that's the null hypothesis is 21.49 percent there's a 21 and a half percent chance that the class had no effect on bench press weight and if we look at our alpha of 0.05 we see that is much bigger than the 0.05 alpha alpha is the minimum probability where we still believe the null hypothesis is true we are well beyond that probability much higher so our decision is to fail to reject the null hypothesis and the reason for that decision is the pvalue the probability that the null hypothesis is true is greater than alpha which is our decision break with numbers the 0.2149 is greater than the 0.05 and so for our final conclusion we will state that there is not sufficient evidence and then we state the alternative hypothesis in context to conclude the strength class increased bench press weights and so what you see is with the matched pair hypothesis test it really is exactly the same as a single mean hypothesis test with the t distribution the only difference is here we will focus exclusively on the differences so first we have to calculate that after minus before relationship to see how much things have improved or decreased after the treatment so that's how we do a hypothesis test for matched pairs take a look at a few of these on the assignment and we will see you in class to continue to work on matched pairs a little more quite often we read a claim that a certain population is distributed in certain way maybe we say that 20 percent fall in one category 40 fall in another category and 30 percent fall in another category the question we're going to take a look at today is how good that fit actually is to data we might collect so the official question here how do we test if data fits a claimed distribution and as we do this hypothesis test we have to run into a new distribution that tests how well data fits a claimed distribution and the new distribution is what we call the chi squared distribution and we use the greek letter chi with a little squared on it looks like an x with tails on the ends and this chisquared distribution a few characteristics of it it is a nonsymmetrical distribution and it is skewed right so unlike the normal and the t distribution which is perfectly symmetrical the chisquared is skewed right in fact the shape itself varies based on the degrees of freedom there is a different shape based on the degrees of freedom and another unique thing about the chisquared is that it is always greater than zero with the tdistribution and the normal distribution we found it's symmetrical around zero we had positive values and negative values representing if we were left or right of the mean the chisquared doesn't do that the chisquared actually starts at zero and depending on the degrees of freedom if there's only one degree of freedom one degree of freedom the graph looks something like this but if we increase the degrees of freedom to maybe three it's going to look something like this with a little hump skewed to the right and the more we increase the degrees of freedom that hump is going to move slightly over so this red line might represent 10 degrees of freedom and so you see the shape varies quite dramatically based on the number of degrees of freedom we have so let's look at how we can use the chisquared distribution to test a claimed distribution what we're going to do is the goodness of fit test or we've got some claim that a certain percent fall in various categories is that claim accurate does it does the data fit that distribution well is it a good distribution the test statistic we will use is chi squared is equal to the sum of the observed frequency minus the expected frequency squared divided by the expected frequency so we've got some new variables o is the observed frequency and e is the expected frequency and this chisquared value we're going to calculate by hand it's not too bad but it is one we should know how to do now with chi squared we have to know the degrees of freedom the degrees of freedom with the goodness of fit is one less than the number of categories and then for our null hypothesis and alternative hypothesis with the goodness of fit test we usually state it in a sentence rather than symbolically like we did with the means in the t distribution or the normal distribution so in words the null hypothesis is that the data fits the distribution the alternate hypothesis is that it's not equal to the distribution or the data does not fit the distribution and what's interesting about the chi squared because it starts at zero and is skewed right that alternate hypothesis is always going to be a right tailed test in fact with chi squared almost always we're working with a righttailed test there's only one context which we'll talk about in another video where we could have a twotailed test or a lefttailed test but in general chi squared is a right tailed test now we can use our calculator to help us find the area that's in that right tail so let's take a look at how the calculator can do that what we'll do on the calculator is we will find the area from the test statistic all the way out to infinity how much area is in that right tail the problem is our calculators can't do infinity so we're going to put a number in that's pretty darn close to infinity we're going to use 10 to the 99th power to represent infinity because there's going to be very little area that's past 10 to the 99th power that's a 1 with 99 zeros after it a 100 digit number that's pretty darn close to infinity so the calculator keystrokes we want to do is first you'll hit the second button then you'll select the distribution function which is really the vars button and then above it with the second feature you're getting distribution then you can scroll down to the chisquared cdf once you've selected the chisquared cdf what you'll do is you'll enter the test statistic to represent the minimum value we're finding the area of then you'll do a comma then you will do the infinity which is 10 raised to the 99th power then you will do a comma and then you'll type in the degrees of freedom we'll see this process work out when we do our example now let's say a researcher wants to verify a claim about her community she wants to verify the claim about her community that 40 percent of the residents speak spanish in the home 10 percent speak russian 45 percent speak english and five percent speak other languages so the researcher does a survey in a survey of 200 community members 71 speak spanish 23 speak russian 102 speak english and for speak another language if alpha equals 0.05 can the researcher conclude the claimed distribution is accurate we're going to go through the same process of a hypothesis test that we've gone through before we've just got a different test statistic but everything's exactly the same from there our null hypothesis we said is that the language put it in context but that the claim is accurate that the language spoken in the home matches the distribution of the claim the alternative hypothesis is that it does not match the claim that the language spoken in the home does not match the claim or the distribution of the claim i should say distribution because that's important and we said with the chisquared we are almost always dealing with a righttailed test we end up more in the right tail the more different we are from the distribution the distribution itself is a chisquared distribution and we'll do a little subscript for the number of degrees of freedom we've got spanish russian english and other four languages the degrees of freedom is always one less than the number of categories so we have three degrees of freedom and now we're ready to calculate the test statistic and this is where we're going to do our real work we've got spanish russian english and other the claimed proportion can't get it all on one page here but uh we've got 40 percent with spanish 10 with russian so spanish is 0.4 russians 0.1 english 45 0.45 others 5 0.05 the expected value we've done a survey of 200 community members so of those 200 we should expect 40 to speak spanish so we can calculate forty percent or point four times two hundred and that gives us we would expect eighty people in our survey to speak spanish for the russian we'd expect it to be ten percent ten percent of the two hundred ten point ten times two hundred is twenty for the english point four five times two hundred is ninety and for the other .05 times 200 is 10. so we use those percentages and the total sample size to calculate what we expect to happen but something different happened in our observed probabilities what we observed is 71 spoke spanish this is from our survey we observed 23 russians we observed 102 english and we observed four other languages so now what we can do is we can use that chisquared statistic is the observed minus the expected squared divided by the expected similar to how we did the standard deviation by hand back in chapter one we're going to just make a different column so that we can sum those observed minus expected squares over expected so first observed minus expected observed minus expected the observed comes first so 71 minus 80 is negative 9. 23 minus 20 is negative 3 102 minus 90 is 12. oops i'm sorry 23 minus 20 is a positive 3 and 4 minus 10 is negative 6. then we square those values the observed minus expected squared 9 squared is 81. 3 squared is 9. 12 squared is 144 and negative 6 squared is 36. now we take the observed minus the expected squared and we divide by the expected value the expected value column is that second column here so 81 divided by 80 is 1.0125 9 divided by 20 is 0.45 144 divided by 90 is 1.6 and 36 divided by 10 is 3.6 now we're ready to actually find that sum of the observed minus expected squared divided by the expected by adding up that last column 1.0125 plus 0.45 plus 1.6 plus 3.6 gives us 6.6625 that is our chi squared test statistic now that we have our test statistic we are ready to calculate a pvalue and we're going to do that pvalue by doing the chisquared cdf we're going to go from a low value of 6.6625 to a high value of infinity which we use 10 to the 99th power for infinity and we said we had three degrees of freedom so on our calculator to get the chisquared distribution we're going to hit second and then the vars button which gives us distribution and we'll scroll down we want the chisquared cdf it has to be the cdf make sure you don't do the pdf we're never going to use that one the cdf the lower limit is 6.6625 the upper limit is infinity which will use 10 to the 99th power and the degrees of freedom is 3. and when i hit paste you see it types those numbers in for me if you don't have the newest model of the ti 84 you might just have to enter in these numbers with commas separating them get the same thing though we end up with .0835 .0835 and what that p value tells us is the probability that null hypothesis is true given our data based on the survey there is 8.35 chance the proportion of people who speak various languages matches the claimed distribution eight percent chance that it matches the claim distribution but we said that alpha's 0.05 in other words we will believe the claim distribution all the way down to 5 we only have an 8 percent or we still have an 8 so we're still going to believe that null hypothesis our decision is to fail to reject the null hypothesis and the reason for that decision is because our pvalue is bigger than alpha there's enough evidence to still believe the null hypothesis specifically the pvalue was 0.0835 which is bigger than alpha which was 0.05 so we can make a conclusion in context focusing on the alternative hypothesis we say that there is not sufficient evidence to conclude the distribution of languages spoken in the home is different than the claimed distribution not enough evidence to say it's different so we'll have to be stuck believing that distribution is true that's the goodness of fit test the hypothesis test process still exactly identical we just have a new distribution of chi squared we have to do a little bit of arithmetic to calculate that chisquared value but it's not too bad so go ahead and take a look at a few if you want to practice some we'll talk about it more in class we will see you then another use for the chisquared distribution is testing to see if two variables are dependent or independent of each other so the question for today is how do we test if two variables are dependent or independent and to do this we will test for independence what we'll do to test for independence is we will collect frequency data and organize in rows and columns for example we might compare gender to we'll just do male female to whether or not you played sports in high school yes or no and this will generate a table where we'll enter in how many yeses how many no's for each gender we'll probably also have a column for totals and a row for totals and then off this contingency table we will see if playing sports is dependent or independent on gender once we have our frequency information we also need to calculate the expected frequencies and often i'll make a second table to do this of expected frequencies and the way we calculate the expected frequency is we will take the row total times the column total divided by the total of the entire survey and we'll have to do that for every single cell so first row first column first row second column second column second row find all the entries for their expected frequencies and then we can calculate the test statistic from here often i'll organize this in a third table because our chi squared is going to be equal to the observed value minus the expected value squared divided by the expected values and then we take the sum of all of those so we've got two equations that are going to be helpful to test for independence should know how to calculate the expected values and then from that calculated the chisquared now as we're testing for independence we need to know the degrees of freedom the degrees of freedom is equal to a product with independence it's the product of the number of rows minus one times the number of columns minus one also with independence similar to when we did goodness of fit the null hypothesis and the alternative hypothesis are generally in words not in symbols so the null hypothesis is generally going to be that the variables are independent and then the alternate hypothesis is that they're not independent or that they are dependent and then similar to the goodness of fit test that dependence is always a right tailed test so let's try an example and see if we can test for independence an example a restaurant wants to know if breakfast preference is dependent on gender so the following data is collected and they're going to compare male to female and some are going to prefer french toast some will prefer pancakes some prefer waffles and some prefer omelets and then we'll have a total row and a total column we find 47 males prefer french toast 35 prefer pancakes 28 prefer waffles and 53 prefer omelets if we add all of those together we'll get a total of 163 males were surveyed for the females 65 prefer french toast 59 prefer pancakes 55 prefer waffles 60 prefer omelets we interviewed a total of 239 females and if we total the individual breakfast choices we've got 112 who prefer french toast 94 who prefer pancakes 83 who prefer waffles and 113 who prefer omelets there's a total of 402 individuals in this survey the question is if alpha equals 0.05 can the restaurant conclude that breakfast preference is dependent on gender well let's set up our hypothesis test first we have our null hypothesis which states that breakfast preference is independent of gender breakfast presents preference does not change based on if you're male or female the alternative hypothesis is that there is some type of dependence that breakfast preference is dependent of gender or on gender as usual with the chi squared we have a righttailed test and our degrees of freedom to help us calculate the actual distribution is going to be the number of rows we've got two rows don't count the total one two rows minus one times the column don't count the total again one two three four columns minus one which gives us one times three or three degrees of freedom our distribution for our chisquared statistic is a chisquared with three degrees of freedom then let's see if we can calculate our test statistic to calculate the test statistic we first need to make another table of our expected values so for our expected values we'll do the same table male and female and for french toast give me a little more space here the way we calculate the expected value for our males with french toast is we will take the row of males and the column of french toast those totals so the row of males had a total of 163. the column had a total of 112. that's how we get the french toast male cell for expected 163 times 112 and then we'll divide by the total so we have 163 times the 112 divided by the total number of people which was 402 the expected value for french toast is 45.4 now we'll go over to pancakes pancakes is in the second column first row so in this case what we'll see is we want the pancakes for the males that's in the first row second column so those are the numbers we're going to multiply 163 times 94 and divide by the 402 the row times the column so we have 163 times 94 divided by the total of 402 we end up with 38.11 is the expected value for pancakes next is waffles with the waffles we want the third column in the first row so we're going to multiply 163 times 82 and divide by the total so we have 163 times the 80 3 is it 83 divided by the total of 402 and that's going to give us 33.65 for our expected value finally we've got the omelets similarly with the omelets the total for the omelet row for the males was 163. the total number of omelets was 113 and then we divide by the 402 to get 45.82 we'll do the same thing with the females this time the female row total was 239 so we're going to do 239 times the french toast total of 112 divided by the 402 the expected females who prefer french toast should be 66.59 with the pancakes 239 times the column total of 94 divided by the total total of 402 55.89 with the waffles the road total of 239 times the column total of 83 divided by the total total of 402 the expected value is 49.35 and finally with the omelets the row total is 239 the column total 113 divided by the total total of 402 the expected number of omelets for females is 67.18 so we've got the second table that demonstrates for us all of the expected values based on the row totals and the column totals next i'll make another table that does the observed minus the expected squared divided by the expected for our males females first with the french toast our first cell and i've got to do a lot of scrolling here hopefully you can see it all on one screen 47 is what we observed 45 actually go ahead and highlight it to emphasize what i'm looking at here highlighted in yellow here our first cell had an observed value of 47 an expected value of 45.41 so plugging that into our formula the observed value of 47 minus the expected value of 45.41 squared divided by the expected value of 45.41 that equals .06 then we'll do the pancake preference you'll see pancakes the observed value is 35. the expected value is 38.11 so we plug it into our formula 35 minus 38.11 squared divided by the expected value of 38.11 that gives us point twenty five waffles same thing we'll come up here we observed 28 waffles we expected 33.65 so we plug that into our formula 28 minus 33.6 squared divided by the expected value of 33.65 we get 0.95 and we'll keep going with all of our remaining cells for the omelets we observed 53 we expected 45.82 squared divided by the expected 45.82 that equals 1.13 doing the female row the observed french toast was 65. minus the expected 66.59 squared divided by the expected 66.59 equals 0.04 with the pancakes the observed value of 59 minus 55.89 squared divided by the 55.89 that equals 0.17 for the waffles the observed value was 55 minus the expected value of 49.35 squared divided by 49.35 equals 0.65 and finally the observed value for the omelets was 60 minus 67.18 squared divided by 67.18 that's going to give us point 77 so we're going to get really good at that observed minus expected squared formula the reason we need to do that is our chisquared test statistic is going to equal the sum of those observed minus expected squared divided by the expecteds we're going to actually add all eight of these numbers that we just found together and when we add all those numbers together you should get 4.02 that is our test statistic so it's a little cumbersome and tedious to calculate because you've got to go through cell by cell first we calculate the expected values by taking the row total times the column total divided by the total total then you need to find that observed minus expected squared divided by expected plugging those values in calculating them out and then finally we add them together to get our final chisquared value and now we're ready to go on with our hypothesis test everything should flow pretty quick from here we need to calculate a pvalue or the probability the null hypothesis is true to do that we'll do the chisquared on the calculator cdf the minimum value of 4.02 all the way to a maximum value of infinity 10 to the 99 and we said there are three degrees of freedom so let's do that we'll hit second the distribution or vars button we'll scroll down to select chi squared cdf the lower value we want is 4.02 the upper value is infinity 3 degrees of freedom again if you don't have the newer software you just have to separate them by commas and when we hit enter we find a probability of 0.2593 0.2 which means there is a 25.93 actually let's say based on our survey based on our survey there is a 25.93 percent chance that breakfast preference is independent of gender and we said we would believe the null hypothesis that they are actually independent as long as that probability does not go below alpha of five percent 25 percent is well above that and so we will make a decision to fail to reject the null hypothesis and the reason for that decision is the pvalue is greater than alpha or 0.2593 that's too much evidence overwhelming past the 0.05 we must still believe the null hypothesis is true so we make a conclusion our conclusion is that there is not always in context in the alternative hypothesis there is not sufficient evidence to conclude breakfast preference is dependent on gender and that is how we do a chisquared test for independence it takes a little bit of time to calculate the test statistic running through those calculations but it's not too difficult it just takes the time to run it out so go ahead and take a look at trying a few of these off the homework we'll do a few of these more in class as we look at it a bit further we'll see you then today we're going to take a look at another use of the chisquared distribution and that is testing a claim about a single variance our question is going to be how do we test a claim about a variance and when we're testing a variance a single variance we have a test statistic which is a chisquared statistic the formula with the variance for chi squared is the sample size minus 1 times the sample variance divided by the sample i'm sorry divided by the population variance and we need to be very careful or be very aware of what we have in the problem we use sigma and s to represent the standard deviation but when those pieces are squared and we either have sigma squared and s squared those represent what we call the variance and so the formula says s squared and sigma squared those values represent the variance if it's already been squared if it hasn't been squared we would have sigma and s the standard deviations which need to be squared in order to find the test statistic so we need to be very careful do we have s or s squared do we have sigma or sigma squared and not get those backwards also with chi squared we need to know the number of degrees of freedom we have the degrees of freedom are always one less than the sample size with the chi squared and something that's unique about testing a variance is that it can be either left right or two tailed test and this is very unique because normally with the chisquared we're dealing with a righttailed test but in the context of the variance it's the only time we're allowed to have either a left tail test or a twotailed test which doesn't happen as often so the best way to really attack this is to run through an example so let's say a customer wants to know how the cost of a list of school supplies varies from store to store a teacher claims the standard deviation is only dollars so to test this the customer surveys 43 stores and finds a mean of 84 dollars and a standard deviation of twelve dollars test whoops test if the standard deviation is less than the teacher's claim of 15 if alpha equals 0.05 so we've got a claim about the standard deviation the claim is the standard deviation is 15 but notice that's the standard deviation not the variance the variance is the standard deviation squared so when we set up our null hypothesis our null hypothesis will state that the variance or sigma squared is equal to the standard deviation squared or 15 squared which is 225. notice again we had to square the standard deviation to get the variance the alternate hypothesis we believe that the variance is actually less than 15 squared or less than 225. because we're interested in less than we actually have a lefttailed test so let's draw a little picture of our lefttailed test chisquared is skewed right and we're interested in being in the left tail now in the survey the standard deviation ended up being 12 compared to the actual mean of 15. is that enough to reject the null hypothesis well first we need to know the degrees of freedom the sample size minus one 43 minus one we've got 42 degrees of freedom and then we'll also calculate the test statistic the test statistic is chi squared is equal to n minus 1 42 minus 1 times s squared s i'll go ahead and actually write the formula here one more time so we can see it n minus 1 times s squared divided by sigma squared s is the standard deviation of the sample so for my sample the customer did a survey and found a standard deviation of 12. that is my sample standard deviation 12 squared to get the variance divided by sigma the claimed distribution of the population the claim standard deviation is 15. so we'll divide by 15 squared and when i do this on my calculator oops sorry the sample size was 43. 43 minus 1 times 12 squared divided by 15 squared we get a test statistic of 26.88 now that we have a test statistic we're ready to find the pvalue or the probability my null hypothesis is true the probability the standard deviation is actually 15. it's a chisquared cdf normally with chi squared we go from smallest to largest here the smallest value on the chisquared is going to be 0 to the largest value of 26.88 comma our degrees of freedom we said was 42. and let's see what the calculator gives us for that value to get the chisquared distribution we'll hit second vars so we get the distribution we'll scroll down to chisquared cdf going from 0 to 26.88 our degrees of freedom are 42 and we'll hit paste if you have the older version of the software you just enter those numbers in separated by commas like you see on the screen here and when we hit enter we find a probability of 0.0337 0.0337 that pvalue tells us that based on our sample the probability that the null hypothesis is true that the standard deviation of the cost of school supplies being 15 is 3.37 percent there's a three percent chance that that null hypothesis is true well if that's the case we're ready to make a decision the decision point is always compared to the alpha and we said alpha was going to be 0.05 five percent probability we'll still believe the null hypothesis we only have a three percent probability so we can no longer believe the null hypothesis so we will reject the null hypothesis and the reason for that decision is that the pvalue is less than the alpha value or that .0337 is less than the 0.05 and so our conclusion which focuses on the alternative hypothesis in context we can say that there is sufficient evidence to conclude the standard deviation of school supplies cost is less than 15 and that's all there is to testing a single variance it's actually the easiest chisquared problem to solve because it's very straightforward we don't have to find all those expected values that we do in other chisquared tests so we test a single variance with our new chisquared test statistic and then we run it through the same process of a hypothesis test that we've been seeing for several weeks now we should be very good at setting up these hypothesis tests so you can go and take a look at a few of these on your homework assignment we'll look at this more in detail in class and we will see you then now that we've taken a look at how a hypothesis test can be conducted for a claim about a single variance we can extend this to the next level and look at a hypothesis test on a claim of two variances and so the question we're going to answer is how do we compare two variances and similar to one variance we have to be careful if we're talking about standard deviation or variance because the standard deviation is the square root of the variance but different than the single variance is we need to introduce a new distribution that models a comparison of two variances and this new distribution is what we will call the f distribution the f distribution because this distribution is used as a fraction when comparing two variances similar to the chisquared distribution the fdistribution is not symmetrical in fact it is also skewed right also similar to the chi squared is its shape is different based i should say different shape based on degrees of freedom but what's really unique about the f distribution is it's a fraction of two variances in other words we're going to have a ratio or let's just go ahead and call it a fraction or ratio with two sets of degrees of freedom we say the numerator has degrees of freedom for the numerator is equal to the first sample size minus one and the denominator is going to be the degrees of freedom of the denominator is equal to the second sample size minus one and what's interesting is as the degrees of freedom get larger for both the denominator and the numerator the curve becomes more normal one last thing to talk about the f distribution is similar to the chi squared the f is always positive or always greater than i guess it could be equal to zero so that's kind of a brief introduction of the f distribution we're going to use our calculator to do most of the calculations with the f distribution what we're interested in is can we set up and carry out a hypothesis test on two variances the test statistic for two variances is simply the fraction the first variance divided by the second variance or the standard deviation squared divided by the standard deviation squared if both variances are equal if we have equal variances that tells us that f is going to equal to one we're dividing everything by itself if we have different variances f is closer to zero if the second variance is larger or infinity if the first variance is larger we'll use our calculator actually to do most of the work for the f statistic and so just really briefly how to do that first you're going to hit the stat button and then you can scroll over to test then you can scroll down to two samp f test and that's where we can access the twosample ftest for the comparing of our variances then to actually enter in our data you want to make sure that stats is highlighted and then you can enter the standard deviation notice i did not say the variance even though we're comparing variances with the f test the calculator wants us to enter in the standard deviation which is the square root of the variance so we can enter the standard deviation and our other data other than that the hypothesis test is going to work exactly like all the other hypothesis tests we've seen before so let's try an example and see if we can do a test of two variances quality control is interested in the variance of two machines making widgets the first make 32 widgets with a variance in the radius apparently there's a circle on these widgets of 4.1 millimeters the second makes 37 widgets with a variance in the radius of 3.7 millimeters at the alpha equals 0.05 level can quality control conclude the first machine has a higher variance we'll start by setting up our null and alternative hypothesis like always the null hypothesis always has equity so we're assuming with our null hypothesis that the variance of the first machine is equal to the variance of the second machine the alternative hypothesis is that the variance of the first machine is greater because we want the first machine to be higher than the variance of the second machine for our degrees of freedom the degrees of freedom of the numerator we always do our division in order as listed in the hypothesis test so we're going to do the variance oops the variance of the first machine divided by the variance of the second machine same order as they're in the hypothesis test so the first machine is our numerator the first machine made 32 widgets meaning the degrees of freedom of the numerator are 31. the degrees of freedom in the denominator the denominator being the second machine which made 37 widgets one less would be 36 for the degrees of freedom so our distribution is that we have an f statistic that is distributed as an f with 31 and 36 degrees of freedom and we can calculate our test statistic by just dividing those variances notice we're given the variances the variance is 4.1 and the variance is 3.7 they've already been squared so the first variance is 4.1 divided by the second variance of 3.7 that gives us 1.1081 so if we were to draw a picture of this situation the f distribution skewed right we have a test statistic right at 1.0181 we want to be greater so we go for the right tail how much area is in that right tail this is where we're going to go to our calculator to find our pvalue first we're going to hit stat scroll over to test and then we're going to scroll down for the two sample f test oh there it is two sample f test i'm going to enter in the statistics and it wants my first standard deviation not the variance the calculator is asking for the standard deviation which is the square root of the variance fortunately i can just type in the square root with the second and then the square root key is above the square diagonal from the seven and my first variance was 4.1 so if the variance is 4.1 the standard deviation is the square root of 4.1 and when i hit enter it's going to calculate that value for me the first sample size had 32 widgets in it the second machine had a variance of 3.7 so we'll take the square root of 3.7 to get the standard deviation of 1.92 and the second machine made 37 widgets for our alternate hypothesis we said that the first is greater than the second so we'll select greater than and go down and hit calculate notice it gives us the exact same f statistic that we found 1.1081 but what we're really interested in is it gives us a pvalue of 0.309 303809 so let me scroll a bit give me a little more space to work we have a pvalue of 0.3809 remember the pvalues the probability the null hypothesis is true so we will say the probability or based on our sample the probability both machines have the same variance that's the null hypothesis that they're equal is 38.09 percent and the alpha tells us the probability required to disprove that null hypothesis we have to drop below five percent we're well above the five percent so we're ready to make a decision which is to fail to reject the null hypothesis and the reason for that decision is that the pvalue is greater than alpha there's too much evidence and support of the null hypothesis specifically with numbers 0.3809 is greater than 0.05 and so for our final conclusion which is always written in the context of the alternative hypothesis i believe the alternative on there is that there is insufficient or there is not sufficient evidence to conclude the first machine has a higher variance than the second machine and that is how we can compare two variances so we have a new distribution the f distribution which is a fraction of the variances but the idea is still the same as our hypothesis test we've been doing we've got our null and alternative hypothesis the test statistic gives us an area a p value that we compare to alpha and make a conclusion whether or not we have sufficient evidence to believe the alternative hypothesis take a look at a few of these on your own if you'd like we'll talk about uh comparing two variances more in class and i'll look forward to seeing you then quite often in statistics we're concerned with comparing the mean from more than just two groups in the past we compared two groups with the ttest but here when we compare more than two groups we need a different statistical test and that's what we're going to look at today how do we compare the means of more than two groups this actually also works with two groups but the ttest is easier the ttest turns out to be a special case of this thing that is called the anova but the ttest is easier for two groups so we do the ttest but when there's more than two groups we use this test called the anova anova is actually an acronym it stands for analysis of variance and the idea behind the anova is we compare the variance between the groups to the variance within the groups and when we divide them we end up with an f statistic and so we can compare using the f ratio just like we did with two variances so a couple differences with the anova the hypothesis test for the hypothesis the null hypothesis is always the same that the first mean is equal to the second mean which is equal to the third mean which is equal to all the other means until you get to the very last mean basically all the means are the same and the alternative hypothesis is that at least one mean is different we don't know which mean but just one mean is different than the rest possibly all three are different from each other and we use the f test to compare which hypothesis we'll end up going with what we're doing is we are looking for a difference but not where the difference is turns out that once we decide that there actually is a difference between one of the means we have to do some followup statistical tests which are beyond the scope of this course to identify specifically where that difference is we might have an idea where it is but to get the exact difference we need followup tests that we're not going to cover in this course so for today all we're looking at is are they all the same or is there a difference somewhere we're not going to spend our time in this course with the complex calculations of the anova we're just going to look at actually carrying it out and having our calculator do the complex calculations how we're going to do this is we're going to put each group in its own list and the way we do that is we'll hit the stat button and then we'll select edit to edit our list and in list one we'll put the first group data list two we'll put the second group data list three the third group data until we get all of our groups and once all of our groups are actually listed in there we will run the test and we'll do that with the stat button we will scroll over to tests and then we will scroll down to the anova now the anova will not give us prompts on what information to enter in like a lot of the other statistical tests did so what we need to do is we're going to enter the lists separated by commas and the way we get the list is we'll hit second and then select the list number and we'll see how that all works out with our example let's go ahead and move to our example and see if we can compare this time this context we're going to compare three groups and see if they have the same mean or different means a university is comparing traditional students transfer students and nontraditional students by comparing gpa in the junior year here are their results the traditional students had gpas of 3.2 3.4 3.7 4.0 the transfer students had gpas of 3.1 2.7 2.9 3.2 and 4.0 and the nontraditional students had gpas of 3.4 2.2 2.7 and 2.8 we want to know if all three groups can be considered to have the same gpa or different gpas if alpha equals point 10 can the university conclude there is a difference in the groups let's scroll up and give ourselves a little room and start our hypothesis test the null hypothesis is that the mean of the traditional students is equal to the mean of the transfer students which is equal to the mean of the nontraditional students that they all have the same mean the alternate hypothesis is that at least possibly more one group have a different gpa let's go ahead and run this test on our calculator and see what hap first thing we need to do on the calculator is enter our data into stats and we'll select edit if there's extra stuff in this list you can highlight and hit clear enter and that'll delete the list or clear the list clear enter clear enter and so in list one i'm going to put the gpas 3.2 3.4 3.7 and 4.0 list 2 is my second group 3.1 2.7 2.9 3.2 and 4.0 the nontraditional group 3.4 2.2 2.7 and 2.8 now let's go ahead and run the anova we'll hit stat and this time going over to test and all the way down to the bottom or hitting up gets us straight to the bottom you'll see anova for the anova we enter in our list we have the three list so second one for the first list comma second two for the second list comma second three and we keep going based on how many lists we have we only had three lists and when we hit enter we get all sorts of information we've got our f statistic of 3.07 we've got our p value of .09 under factor you'll see we've got degrees of freedom equals two and under air we've got degrees of freedom equals ten the other numbers we're not going to concern ourselves with today but those degrees of freedom represent the numerator and the denominator so the factor is the numerator 2 the error is the denominator 10. they are in order which is nice so when we say our distribution we'll say f is distributed as an f statistic with 2 and 10 degrees of freedom and we just found out that f equals 3.07 and more specifically our pvalue that's the important one was .0912 but also go ahead and draw a picture of what's happening there's my f distribution my test statistic at 3.07 and we shade that tail which we now know has an area of 0.0912 now speaking of the pvalue what that pvalue means is based on our sample the probability all three groups of students have the same mean gpa in the junior year is nine point twelve percent we've got a nine percent chance that all three groups have the exact same probability the p value is the probability the null hypothesis is true which means we are ready to make a decision the decision is based on the alpha we said alpha's point 10 we're going to believe the null hypothesis is true until there's less than 10 percent chance it actually is true we we have a 9 chance that it's true so that passes our threshold so we will reject the null hypothesis and the reason for that decision is that the pvalue is less than the alpha or the 0.0912 is less than the point 10 we said was our decision breakpoint so our conclusion in context of the alternative hypothesis is that there is sufficient evidence because we rejected to conclude the mean gpa of traditional transfer and nontraditional students in the junior year is not the same again i'll notice this test doesn't tell us which group is different from the rest or possibly all three groups are different from each other it just tells us that they're not all the same so that's the anova it's another use of the f distribution there's actually several types of anova this is the most basic that we're going to look at in this course and you can see more anovas and more advanced statistics courses but for now we're comparing do multiple groups have the same mean we plug it into our calculator we get an f statistic and a p value and we should be able to conclude do they all have the same mean or is there a difference somewhere in the group try a few of these on the homework we'll talk about it more in class and we will see you then this video is going to look at the important concept a find a relationship between variables with what we call correlation and regression so that's where our question stems from our question is how do we test for a relationship between variables and the first thing we're going to start with is just getting a visual of how the two variables are related using what is called a scatter plot a scatter plot is basically just a graph of all of our data so it's best illustrated with an example let's say a researcher collects a sample the number of pages a person reads based on their age so we've got let's call the first column their age and the second column the pages that they read so we've got a 14 year old who read 40 pages there's a 21 year old who read 45 pages they asked a 33 year old who read 92 pages they asked a 45 year old who read pages and they asked a 63 year old who read 171 pages the idea of a scatter plot is if we call the first column the independent variable x and the thing that we think changes based on the independent variable or the dependent variable y we should be able to graph these to get a visual of how they relate to each other let's go up by tens on the xaxis 10 20 30 40 50 60 70 years and on the yaxis we'll go up by 30s 30 60 90 120 150 180. and so we'll make a point for each one of these and this is going to be what becomes our scatter plot so at 14 years old we'll go up to 40 pages which is right about where that blue dot is then we've got a 21 year old who goes 45 pages so maybe a little bit higher then there's a 33 year old who's going to go up to 92 pages a 45 year old who will go up to 167 pages and a 63 year old who will go up to 171 pages and so we kind of can see a relationship here but before we get into that let's make sure our scatter plot is complete because a good scatter plot will have titles and labels so the bottom represents the age going up represents the pages and we might say pages read by age for the title and what we can see is the dots aren't exactly in a straight line but they do kind of trend upwards it seems that as age goes up the number of pages goes up and so we've got this relationship starting to establish visually now our calculators can also make these scatter plots so i'm going to show you how to make this exact same scatter plot on the calculator so first i'll write out the instructions then we'll go ahead and do it first we have to enter the data and the way we enter the data is you'll start by hitting the stat button and then you'll select edit and then you can put x in list one and y in list two once you've entered the data you're ready to make the graph and the way we make the graph is first you'll have to hit the second button and then you'll have to hit the y equals button because above y equals the second feature is stat plot it's for graphing statistics and you're going to go into the first stat plot and make sure it is on make sure you select the scatter plot which is going to be the dots and then you want to select what list you want l1 and l2 once you're done with setting up the stat plot you'll hit the zoom button and you'll select zoom stat which will center you right on the statistics so let's look at doing that on our calculator first we have to enter in our data to do that we'll start by hitting the stat button selecting edit and there's already stuff in these lists to clear it out i'll highlight the list name and hit clear enter and that clears out the list clear enter and then i'll put in l1 my x's my x's were 14 21 33 45 and 63 and then l2 i'll put my y's 40 45 92 167 and 171. now that i've entered in the data to set up the graph we'll hit second and the stat plot button which is the y equals button option number one we're going to make our scatter plot first you want to make sure you've turned it on by selecting on there's lots of graphs you want to make sure the dotted one which stands for the scatter plot is selected and then l1 and l2 are my list we're ready to see it we'll hit the zoom button and near the bottom maybe not the bottom bottom there it is number nine on my calculator is zoom stat what that does is that centers my graph around my statistics and you see we end up with much the same graph we had before the two dots next to each other a dot a high dot and another dot over to the side and so now we've got our scatter plot on our calculator but sometimes we're interested in more than just the dots what we might be interested in is can we draw a line that kind of models through the center of the data a best fit close to the dots that is what we call our regression equation so let's take a look at the regression equation also known as the line of best fit also known as the least squares line basically a line that models those dots as close as possible well since it's a line we know the equation of the line is going to be y equals some yintercept plus the slope times x and in linear regression we call that y equals a plus bx a little different than algebra where you probably learn y equals mx plus b similar setup but now we use a for the yintercept and b for the slope the equations for a and b are quite complex so we're going to have the calculator do our work for us to calculate the equation and the way the calculator can do that is we actually run what's called the linear regression t test so we'll hit stat we'll scroll over to tests and we'll scroll down to the lynn rag t test we can also graph it on our scatter plot graph by hitting the y equals button and then typing in the equation so let's do that for our age versus pages example so pulling back up my calculator we're going to hit the stat button scroll over to test and near the bottom you will see len reg t test make sure you do lin reg t test not any of the other len reg stuff lin reg t test and hit enter l1 and l2 are x and y uh we're going to always select the not equals to button for our len reg t test and then if we hit calculate it gives us a lot of information we're going to come back to some of this information in just a minute but specifically what we're interested in right now if i scroll down we see a is negative 5.523 and b is 3.083 i'm going to go ahead and round those to two decimal digits so a is equal to negative 5.52 and b is equal to 3.08 which means if i put that into my equation y equals a which is negative 5.52 plus b 3.08 times x this equation models as close as possible my scatter plot if i hit y equals i can type that in negative 5.52 plus 3.08 x x buttons right next to the stat button and when now when i hit the graph button what i'll see is i have a line that goes right through the middle of my dots seems to model that quite well we have a line of best fit the nice part about the line of best fit is i can use it to estimate values i don't have let's estimate the number of pages read by a 30 year old if we go back to the original data we don't have 30 in here but age is my x value so if i plug in the age for x we should be able to get a good estimate for what that 30 year old is reading so y equals negative 5.52 plus 3.08 x x is the age my 30 year old and when i put that into my calculator i end up with 86.88 pages so round that baby to 87 pages we would expect this 30 year old to read one important thing to note about using the regression equation to estimate points is it only works within the domain of the problem it only works between the high and low values we can't estimate values outside of that range so for example this would be bad could we figure out a threeyearold well mathematically it would make sense if three is the age we'll just plug 3 in for the x and we get negative 5.52 plus 3.08 times our 3 year old and that's going to equal to 3.72 so do we conclude that a threeyearold is reading 3.72 pages probably not there's not a lot of threeyearolds that can read anything maybe their name the problem is is the threeyearold is outside of our data our data had a low of 14 and a high of 63. we're only going to estimate values between those numbers if we go outside of the data the model can very easily break down and so we want to be careful not to take the model further than it's designed to go all right i want to look at one more thing with regression and correlation and that's what we call the correlation coefficient and this is where the hypothesis test comes in though we won't do it nearly as formally as we have in other contexts the idea of the correlation coefficient is i have this red line on my graph that goes kind of through the blue dots but not perfect well how good of a model is that line of best fit is it close is it far off what can we know from that line this is what the correlation coefficient measures and we have a special variable r that we use for the collate correlation coefficient and it tells us two things about the graph it tells us the strength and direction first r is between negative one and positive one we say if r equals zero that tells us that there is no relation no relation between the x and the y the dots are completely random and our line of best fit just has to go straight through the middle but that doesn't even model it well there's no relationship everything's random if r equals positive one what that means is we have a perfect positive relation positive means we're going uphill so now we're going to have these dots going uphill and the line goes right through all the dots uphill perfectly that would make r equal to 1. similarly r equals to negative one means we have a perfect negative relation or we're going downhill so now there's a bunch of dots going downhill and the line goes right through the middle of all the dots going downhill now there's no such thing as perfect data so r very rarely is 0 1 or negative 1. usually it's somewhere in between r might be negative 0.78 or r might be positive 0.23 and the closer it is to 0 the less relationship we have and the closer to 1 the more likely we have a relationship and the way we determine if there's a relationship is we do a ttest it's called the lynn reg ttest linear regression ttest which gives a pvalue that tells if the relationship between the two variables is significant a pvalue less than alpha of 0.05 will mean that we have a significant relationship but r works hand in hand with this pvalue because r tells the strength of the relationship r could be positive or negative but what we'll say is if it's between 0 and 0.199999 the strength is considered to be very weak there might be a relationship because the p value tells us it's significant but it's very weak below 0.199999 from 0.2 to 0.399999 we say it's just a weak relationship 0.4 to 0.59999 is a moderate relationship but we like to see r bigger than 0.6 0.6 to 0.7999 means we have a strong relationship and on occasion we end up between 0.8 and 1.0 which would be a very strong relationship in addition to r telling us the strength of the relationship and p telling us whether or not the relationship is significant there's a third variable that we look at in analyzing correlation and regression and that is r squared which we get by squaring the r value r square tells the amount of variance actually not variance we'll say variation in the dependent variable that is explained by the independent variable how much of the changes in that dependent variable are explained by changes in the independent variable versus other factors this whole concept of the correlation coefficient is probably best seen by going back to our example so let's go back to our age versus pages example we can run the lynn rag ttest on our calculator to find these important values the tvalue the r the pvalue and the rsquared so going back to our calculator let's run it one more time i'll hit stat over to test and i'm going to do the len reg t test my numbers are already in there so when i hit calculate we see our test statistic the t is 5.09 we see that gives us a p value of 0.0146 and if i scroll down we will see an r value r is 0.9466 and r squared is 0.896 let me transfer all those values over here and we'll talk about what they mean so we had a t value of 5.09 when we round it which resulted in a p value of 0.00 also there was an r our correlation coefficient of 0.947 and r squared it says was 0.896 with linear regression the null hypothesis is always that the relationship which we represent with the greek letter rho looks like a p equals zero that means there's no relationship the alternative is that rho is not equal to zero or that there is a relationship and we can see in our case we're going to make a decision because the pvalue is less than 0.05 to reject the null and make a conclusion that there is significant evidence to conclude the alternative hypothesis which says there is a relationship there is a relationship between age and pages red but we can expand on that conclusion a bit and say because r is equal to 0.947 which if we scroll up in our notes we see that puts us in the very strong category between 0.8 and 1.0 we can say the relationship is very strong we can even go one step further and say because r squared equals 0.896 we can claim that 89.6 percent of the variation in the dependent variable or in pages red is explained by the independent variable or by age and so you see we don't end up with the traditional hypothesis test going through all the same exact steps we have before but the pieces are still there that we decided based on our pvalue that there is a significant relationship based on our r we can determine how strong the relationship is and based on our r squared we can say what percent of the variation is explained by the independent variable so that's what we're looking at today determining is there a relationship if there is a relationship what that relationship is based on the equation y equals a plus bx and also drawing the scatter plot so we can get a visual to see that relationship we'll look forward to seeing you in class so we can look at these scatter plots linear regression and correlation a bit further