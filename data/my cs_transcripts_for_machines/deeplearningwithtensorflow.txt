so the purpose of this course and the place that this course started from was we were noticing at the university that there were lots and lots of people applying for grant money wanting to start new projects getting interested in trying to solve new problems and there was this buzzword flying around of deep learning and machine learning and we realized that a lot of people wanted to take part in this new fad but didn't really know what it actually involved they would just write it down in their grant application and say well i'm going to do some deep learning as part of this and then when actually came to it they didn't really necessarily know the full extent of what was possible with it and so this course started from my attempt to educate and inform people about what deep learning really is how it works from the lowest levels and the kinds of problems that you might be able to use it to apply to so this really is an introduction to deep learning we're not going to be assuming any knowledge of neural networks any knowledge of how that kind of data science stuff works and we're trying to cover all of that if you came to my course yesterday applied data analysis in python and then we're going to be covering a few of the same topics but 90 of it is going to be completely separate so let's start from the basics this course is called introduction to deep learning but before we can get to deep learning we need to understand the context about how that fits into the larger scheme of things so machine learning is where i'm going to start and that's a lot of what we covered in the course yesterday but at its core machine learning is any technique which uses computers to discover patterns of information so it's what would traditionally have been called statistics but because using computers to do it it allows you to get greater reproducibility and most importantly for some of the stuff we're going to be talking about today much greater scale you can go to much larger amounts of data much larger depths of information you're trying to seek out and that's what differs machine learning from what would traditionally be called statistics is this scale of what you can do with it now machine learning sits inside the wider field of artificial intelligence artificial intelligence is a very poorly defined term it basically means anything where computers are trying to make decisions so that can be something as simple as trying to make a decision as to what the value of y should be given x that's a very very narrow form of artificial intelligence all the way up to selfdriving cars and potentially according to scifi anyway well beyond intergeneralized artificial intelligence but within artificial intelligence this is machine learning which is a better defined field where it's about statistics and data science and things like this but of course even machine learning being a narrower field than artificial intelligence is still incredibly broad and is full of loads and loads of different techniques and concepts in the course yesterday we covered things like kmeans clustering for doing uh unsupervised clustering algorithms as well as linear regression and correlation studies so these were some of the tools you'll apply to your data and what we're learning about today on a much larger scale is deep learning and neural networks around that so inside machine learning which remember is sitting inside artificial intelligence it's a whole bunch of different things yesterday one of those we looked at is a linear regression it is a very simple concept it got across you've got a scatter plot you draw a straight line through it that is nonetheless machine learning because you're using a computer to discover what the values of the gradient and the yintercept are going to be slightly more advanced technique is something like kmeans clustering it's trying to discover things about your data without you having to explicitly tell it where you want to end up there are hundreds maybe even thousands of different algorithms that sit inside machine learning variations therein obviously we don't have time to go through all of those possibly one of the most famous examples of machine learning algorithm particularly over the last 10 15 years or so are neural networks now neural networks have been around a long time they've decades and decades new networks have been around and at their core they're a relatively simple concept but over the decades as computers have got more powerful and the kind of questions that we want to scale up have got more difficult we've had to become clever about where how we apply neural networks to try and solve the problem and so for a long time neural networks were relatively constrained in the kinds of complicated problems they could solve but since about 2010 the last 10 years or so there's been a second wave or maybe even third or fourth wave depending on how you measure your history about how you can create new networks which are what is called deep now i'm going to come across shortly what i mean by a deep neural network but it's a specific structural shape of a neural network which has been managed to be solved by applying more advanced statistical techniques by very very clever mathematicians and it's this resurgence of neural networks which are deep which has led into the modern fad last five years or so perhaps in research of deep learning so deep learning is applying a deep neural network to some kind of problem so i've given you artificial intelligence i've drilled down into machine learning and from there we've drilled down into neural networks so this course is titled an introduction to deep learning but before i can really give you a sense of what deep learning is and why it's interesting we first need to make sure we understanding understand what a neural network is at their most basic elementary level how do they work because the terminology and the few bits of maths that i'm going to be putting in today are important to understand how the problem scales up and when you're wanting to use these things how you can solve the problems that you're going to come across when you're trying to get stuff done so a neural network is a collection of artificial neurons so the place we need to start before we think about a network is to think about the subset the things that make up that network the nodes that connect together and those are neurons and before i go any further i need to apologize and say that i am not a brain scientist i have no idea how the human or animal brain works so if anyone in the chat here does you might want to cover your ears for a second but at their core a neural network is modeled after a real animal brain a brain is constructed by having a whole bunch of neuron cells all connected together and all connected together but connected together in some kind of network and as inputs and impetuses come in from the outside world electrical signals are triggered off in some of the neurons and then depending on which neurons they're connected to and the conditions under which those following neurons are going to activate you get a kind of ripple effect going through with different scales and sections firing and other parts not and it's that complex interplay of all these different neurons which is what gives us intelligence and now the idea within your networks was to take a simplified model of how that works some kind of mathematical description and see if we can try and solve some of those same problems that the human brain can solve so from this point on you can forget all about the animal brain pretty much because everything i'm saying isn't really how nature works but it is how our mathematical model inspired model is going to work so an artificial neuron which is designed after the idea of what a real animal neuron is like has multiple inputs coming into it and it passes output out you can think of it as like being a little function with a bunch of inputs and one output at their most simplest the function that a newer one encompasses is adding together all the inputs coming into it and then putting the output out the far side it's slightly more complicated than that because it doesn't just take each input it takes the first input and multiplies it by a number adds that to the second input multiplied by a different number and adds that to the third input multiplied by a different number and so on and so on depending on how many inputs a particular neuron has neurons can have potentially zero inputs but realistically the minimum would be one they can have thousands of inputs but at its core it is this bit of maths here which is what is going on inside a newer one when you're passing information through the network it's the sum over the input values and their corresponding weights and notice that each input value x i has a w i each input has its own weight if there are 10 inputs to a neuron there are going to be 10 weights also bear in mind that every neuron is going to have its own set of weights so if there's another neuron somewhere else in the network its w1 will be different to this neurons w1 so you can imagine if you've got a network full of 100 different neurons and they're connected together you're going to end up with millions of these weights all connecting together and trying to give you some kind of realistic output the one other thing to be aware of in this simplified model i just described is there's an extra step which happens after this sum here is calculated p it then gets passed through an activation function now an activation function in nature will be something like don't trigger the output until the sum here is above a certain threshold but because we're doing maths here we can do absolutely any activation function we might want there are various reasons for choosing one activation function over another but often on the whole they'll do something like if the sum is negative then just ignore it and if it's positive pass it on through so before i go any further i'm going to ask if there are any questions about how this part of it works how a single newer one is interacting before we then think about how we connect it through into a network right so there's a question there from yan is there reason for using a linear combinations of inputs only i think because it is the easiest thing to do mathematically it is a simple function you can apply which gives you a an effective result you do want to pass it through an activation function because activation functions can give you extra bits of magic so if your whole network was just linear combinations of inputs and weights then if you add a whole bunch of linear sums together what you get at the end is a linear sum and if that was the case the most powerful thing a network would be able to do would be to evaluate some kind of linear relationship between the inputs and the outputs by having the activation function at the end that allows you to encapsulate nonlinearity in the thing you're trying to model so you can keep the core of your model simple and then allow for nonlinearity by passing it through this activation function and that gives you the space to explore more interesting models yvette asks so the weights associated with the neurons not the inputs yes that's the easiest way to think about it think about each neuron having a bunch of pipes coming into it and on each of those pipes it's got a valve which it can use to multiply the input or divide the input by so each neuron has inputs coming into it and it has its own uh value that is being multiplied by to decide how that neuron is going to treat that particular input sharia asks can we use nonlinear combination as well in in principle a neural network can do whatever you want there's no reason why a neural network has to have a simple function like that and in fact in reality there might be neural networks out there which have more complicated um summing functions inside there's no reason to think that it's necessarily this but at least conceptually it's the easiest way to think about it reality might be more messy but conceptually consider it as a sum over the inputs and the weights multiplied together reality might be more difficult but until you start having a full degree in neural networks and deep learning you won't have to worry about anything more complicated than that in order to get your job done at the end of the day here we want to learn enough to be able to use these things and understand how to fix them we don't necessarily for today at least need to understand the full mathematical description of how it all works okay so i'm going to carry on to the next slide now so as we go through do feel free to pipe up and ask any questions you might have even if it's on a slide that we've already been through that's fine i'm happy to reiterate over things because you might think oh yeah i understand this and then i mentioned something in two slides time and you realize i really didn't understand that i obviously misunderstood so do feel free to ask as we go so now we've got our description of a single neuron we're now going to see how we can connect them all together now again it is possible in principle to connect your network together however you want you could just turn it into a big net of things connected together in all sorts of directions but because we want our networks to be able to be understood by humans to some extent we want to be able to design them and understand what the different elements of it do there is a general structure which most neural networks follow now most simple neural networks anyway the complicated stuff comes from people breaking these rules and discovering new and interesting ways of structuring net networks but certainly for example today our network's all going to be this kind of structure which i'm going to describe now and that is you start on the left hand side of this picture on the input layer and this is where you take your values that you want to put into your model so uh an example that we used in the course yesterday was looking at the price of a house so you might go out and measure how many bedrooms a house has how large its garden is and how close it is to the nearest school for example so those are three quantitative measurements that you can make so those would be the values that you put into each of these three neurons on the left hand side now note that these three new ones on the lefthand side kind of don't have inputs that's because that's where the data is going to start from so the lefthand side is where we put our data in now each of these neurons on the lefthand side is connected to every one of the neurons in the next layer that doesn't necessarily have to be the case a new a neural network where everything is connected to every other one is called fully connected but it's perfectly reasonable to have some neurons on the first layer connected to only some on the next in whatever combination you see fit but you can see looking at this um second layer here the first one that's got two neurons on it it has each of them have three inputs coming into it and they have an output coming out now looking at the output coming out of the top neuron here you see it's connected to two neurons in the third layer that's not to say that this neuron has two outputs it's simply that that one single output is being passed on to both of the neurons in the following layer and each of those neurons in the following layer will have their own weight that's being applied to the output of the first mural on here this process just gets applied all the way through you work out the first layer you work out the sums and the activation function you pass into the second layer you add up the sums use the activation function you pass it through to the third until eventually you get through to the output layer when you get to the output layer this is your result this will give you some kind of number because at the end of the day we're doing maths it's all going to be numbers we'll end up with a number at the end and it's up to us to then interpret what this number means in the context of the question we're trying to ask but overall this is the structure that they all have they have an input layer on the left hand side i say left hand side it's all virtual but i think of it going left to right if you find it easier to think right to left that's perfectly fine input layer on the left hand side a single output layer on the right hand side and then some number of hidden layers in between in this example here we have two hidden layers each of which have two neurons in principle you can have any number of hidden layers you like with any number of neurons in each the thing that makes a deep neural network deep is having lots of hidden layers that's pretty much the definition of a deep neural network and we'll see later on why that's a useful thing to have great so i was asked if anyone's got any questions there's already one from sumet so how many layers and how many neurons should we have it depends on the problem you're trying to solve so i'll get on to how you go about designing your networks later so i'll go through this in a bit more detail but the number one rule i would suggest is find someone who knows what they're doing who has solved a similar problem to what you're trying to do and use the same structure as they have that is going to get you 90 percent of the way after that you can feel free to tweak it but by using preexisting preexisting and published network structures that's going to help you shuya asks how many layers is considered deep it's that's one of those questions where there's no real answer i would say as soon as you've got four or five you're getting to the point where networks would have struggled with in the past once you're at 100 layers you're definitely in the situation where you've got a deep neural network going much beyond that you're going to have to start bringing real firepower to be able to evaluate anything to do with the network but i would say rule of thumb you can start asking answering those really interesting deep learning questions by the time you get uh sort of six seven eight nine ten and beyond though as with all things the benchmark kind of shifts over time what might have been considered a deep neural network ten years ago might not be considered quite so deep these days and tim asked what's the precise definition of a layer here so there's no one universal definition of a layer which can capture every single different situation but under the structure of a network we have here where we take all of our neurons and we put them into boxes where they are lined up on top of each other such that in one layer in the in one box on the left hand side here they are only connected to neurons in the next layer and in this next box here these are only connected to neurons in the next box that allows you to have a an absolute definition of the ordering of the neurons which lets you decide absolutely what layer each neuron is in and therefore how many layers you have of course because these are just networks and you can in principle do whatever you want you could take the output of the third layer and connect it back to the input layer if you wanted and do something with that at which point it's harder to define what's a layer however you can do a lot of really interesting stuff without having to cycle through so the networks we're working today are what are called a cyclic ie they don't have cycles they are simply feed forward from the left hand side over to the right exactly tim that's a good way of putting it it's only it's at the neurons on layer n only have inputs from layer n minus one knitting asks more layers lead to overfitting absolutely we're going to cover one of the main techniques well actually two techniques which you can use to reduce the chances of overfitting as we go through okay i'm going to carry on so these are some of the questions you've been asking so it's good you've been um you've been uh asking intelligent questions before i've even got to it so a question you've been asking is what shape should it be so there is some art and some science to this there are there are rules of thumb if you've got a certain number of input layers if you've got 10 input parameters then your hidden layers should to an order of magnitude be about 10. there's no point in having an input with 10 inputs and then the first hidden layer being 100 million different neurons that's not going to extract any useful information if you've got 10 inputs you're not going to want more than 100 in the next layer realistically anyway this is one of the things which you can assess using mathematical techniques which we're not going to cover in this course because they're very complicated and also a certain amount of trial and error and data science applied to the shape of the network also as i think someone mentioned the chat the number of hidden layers kind of relates to how complicated a question you can answer every hidden layer provides a layer of abstraction so if you're trying to start from some very um generalized information and to pull out a really specific answer you're going to need lots and lots of hidden layers to work through the layers of questions that the network is implicitly answering so if you've got a hard question you're gonna need a deeper network if you've got an easy question you can get away with one hidden layer for simple things it doesn't need to be a complicated situation the flip side of this is the deeper the network and the larger the network the harder it is to train that means it's going to take longer to train you're going to be more risk from overfitting and we're going to see how this sort of balance comes through as we go through the course today the next question that you might have had sitting in the back of your head but you haven't worked out how to formalize it yet is i've been talking about the neurons and the inputs coming in and how they're going to be multiplied by some kind of weight and the question you hopefully have is how do you know what that weight should be because at its core there are two things which describe how the network is going to answer your question the first is the structure of it which neurons are connected to which how many layers how many neurons these kinds of things and the second part of the problem is the weights that are being applied on each individual neurons input so the first part we decide and we design upfront we describe a network we say this is the thing that's going to solve our problem but for the weights we have no way as a human of working out what these millions of different values should be and so this is where we use computers this is what makes it a machine learning algorithm because we are going to use a mathematical process on the computer to discover what the values of those weights should be for the particular problem that we're trying to solve this process is called training and it effectively works by showing it loads of examples and it ends up giving you something which can replicate the effect of those examples so question there from ali how can we decide how many layers we need again i would say that the number one way you should discover how many layers you need and the shape of your network is to think about the problem you're solving find someone else who solved a similar problem and start from their example if you're doing something where you're trying to identify um heart disease in uh ct scans of a heart tissue then you might start from someone else who's done the same thing with liver disease or something like that so it's something working with 3d data you're looking for certain features in the data you don't have to start from scratch design your network you start from someone else who solved a similar problem and has analyzed how it works and you start by using those if you can show that it performs well then you've succeeded and you can use that to do the things you want to do if it doesn't perform well then at that point you have to start talking to data scientists and experts and trying to work out how to go from there so to dive into that question there how do we go about training the neural networks what is the process by which the values of the weights get assigned now again there's lots of potential algorithms you could use here there's lots of different ways that you can decide to use a computer to work out what those weights should be but the main technique that's used is something called back propagation now again back propagation is a technique that was applied to neural networks quite a long time ago and in the preceding times in the time since there's been lots and lots of nuances and changes to it and replacement techniques but that propagation at its core as to how it works in principle is still pretty much how neural networks are trained even if there's lots and lots of clever hacks applied on top of it and so to do this training technique you need three different things pretty much you only need two of them but the third one isn't going to do very good data science without it so first of all you need a training data set you need a bunch of examples where you've got the inputs to it using the examples from before a bunch of houses where you've measured the size of the garden counter the number of bedrooms and measure the distance to the nearest school and for each of those examples you have a label assigned to it you've said how much is this house worth so that is what we mean when we say a labeled training data set for checking how well our network is performing we also need a labeled test or evaluation data set now it's important that your training data set and your test data set are distinct and disjoint to make sure you avoid overfitting now we're going to cover exactly how overfitting feeds in a little bit but we did cover it in our course yesterday as well but you want to make sure you have two distinct subsets of training data they can look the same but you should choose randomly about 80 percent of your data to be used for training and then keep aside about 20 to check how well your network is performing and for the back propagation algorithm to work you need to start off with some set of initial weights so now i'm going to go through and show you how these different things can be designed collected and evaluated before i do this two questions so prataps asking that the same output can be achieved by multiple combinations of inputs across layers how is precise connectivity and weights decided so you're right you can have repeated information existing inside network you might have two parts of your network which are deciding the same piece of information in which case you've got more neurons and weights than you really need there are techniques you can use to try and find correlations between weights as they progress and that gives you some sense as if you might be able to prune your network or reduce your network down if it's affecting the ability of your results to be accurate then you're going to want to do that pruning if it's not then you don't so the precise connectivity is hard to do connectivity is generally done on a course layer it's called course level although it's of course possible to apply specific techniques to prune parts of it as for weights we're going to show you now how this particular weights are decided and it does feed into that question you were just asking and yan asks is training always mandatory with neural networks realistically yes if you have an offtheshelf neural network which has been trained to identify the difference between cats and dogs you don't have to do any more training on it you can just use it but if you start with the empty structure you need to do something to work out what the values of the weights should be and to work out what the weight should be you have to train the network so starting with the initial weights lots of different opinions and techniques as you can use to start off your network with some kind of values but the easiest way to do and the conceptually the thing that we're going to do is setting all of your weights randomly there are ways to be smarter about it but for today let's just assume that they're all being set completely randomly so every neuron is having inputs coming into it and it's multiplying each of those numbers initially by a random number and we're going to train it to try and finetune those random numbers in the direction of something which is useful and then for the test and training test data sets so we're going to need two so we need one as i said to train the network on so we can learn about stuff and the other one is going to tell us how well it did it's important to separate your training and test data sets to avoid overfitting and to give a bit more information about why that's the case it's because on your network inside it has got tens hundreds millions potentially of different weights and if your data has fewer degrees of freedom than that then your network is able to learn potentially every single piece of information about your data that you're showing it when it's learning it'll be possible for every single newer one to each take their turn and remember one particular input into the network and so as you're going through it's going to start looking like your network is able to perfectly replicate the training data that you're showing it and that's because it's remembered every single little nuance up and down of the data you've been sharing it by keeping some of that data aside for training for testing at the end even if your network has learned every single little detail about your train data set it's not going to have learned any of the nuances of your testing data set your testing data set is therefore not going to have those same ups and downs and so when you show it to the network it's going to behave very very poorly it's not going to do a good job and so the dance you need to get is the balance between getting your training data set behaving well while also keeping your testing data set still also performing well the common split you generally see is about 80 for training and uh 20 for test but depending on how much data you have you can jiggle those numbers around and there are also more advanced techniques things like kfolds where you end up using all of your data for training but you split it into different subsets and you repeat it over and over again and then you aggregate it at the end in a clever mathematical way um says by disjoint do you mean statistically what i mean is you want of all the samples you've collected when you divide your set into two halves you don't want any overlap between the two they should be sampled from the same distribution is one way of thinking about it but they should be separate samples from that same distribution trey asks is it ideal to set weights randomly if there's repetition will it not be wasteful possibly yes you random setting your initial weight isn't necessarily the thing to do there are clever ways you can work out what your initial weight should be set to but as far as us as researchers using neural networks to solve problems we're going to leave that to the software to decide and they're going to do a better job than we would be able to decide and a vet suggests that you need a massive data set to do useful stuff you generally do you do need a lot of data to train in your network however if you're answering a simpler question and therefore you've got fewer neurons and fewer weights you can get away with less data we'll see an example in a minute where we only have around 150 examples and that's enough on that particular example to answer the question well on later examples you might need i i i generally think a thousand examples is a good kind of rule of thumb for neural network for most problems but having more than that is always going to help one of the problems with data science and your networks is having that data that's nice and clean and well prepared so i'm going to talk briefly about the matsy bit and this is probably one of the last bits of math on the screen so i'm not going to be using any maths that's beyond a level but if anyone wants a bit of clarification i'm happy to explain it in more detail but back propagation is the process by which we assign weights to the network it's an algorithm which we use to decide what those weights should be it's an iterative algorithm so we're going to apply this over and over again and over that time the weights are going to get closer and closer to that which the algorithm considers to be optimal in some way but before you can apply the algorithm you need to start off with your network structure so we've decided how many inputs we've got and outputs how many hidden layers and how many va neurons we have in each hidden layer we need to have some initial weights we need to have somewhere to start our iterative algorithm from and you need to have a training data set you need to have some examples you can show to it and those examples are going to inform how the weights progress as the algorithm is applied now there's lots and lots of different ways you can do the training but the core of how it's generally done these days is using backpropagation and this is a very simplified description of how backpropagation works so you need to start off by looking at the structure of your network looking at the inputs and at the outputs i need to look at every single weight in your network so every single input to a neuron is going to have its own individual weight you're going to have maybe thousands of potential weights and you need to work out how much do you need to change each weight by to affect the output of the whole network by a certain amount if i change a particular weight by a small amount is the output of the network going to only change by a small amount or is it going to change by a relatively large amount if a weight has a disproportionately large effect on the output of a network then the derivative of that weight is a large number if that weight only has a small effect on the eventual output of the network then it's said to have a small derivative and it's only going to have a smaller effect so working out this derivative independently for every single weight and assuming they are independent which largely works in the situations that we're talking about here this gives you a list of derivatives one for every single weight in your whole network every weight will have its own dn its own derivative so this is telling us how much you need to change that weight by to have a certain effect on the output note also that these derivatives are signed so it could be that if you make your weight slightly larger the output goes down and that would be a negative derivative for example so all this information is captured in a big list of derivatives that's stored alongside the network so that's the first step that happens once up front before you start doing any training because the networks we're looking at those derivatives don't change over time what you then do is start the training process and this is where you repeatedly do these steps over and over again and the core of how this works is you take a training entry which has for example three values the number of rooms the size of the garden and the distance near a school three numbers you put it in the left hand side of your network you then work out what the output to that layer is going to be pass it into the next layer that layer is then going to take if you take each of the inputs it's going to multiply them by its initially random weight it's going to do that all the way through right on the other side so the weights are initially set randomly and you're going to get out the far side a random number effectively you're going to get some kind of result it's going to be a numerical value between minus infinity and infinity but you're going to get a number out what you then need to do is look at how far away that random number you got is from the true number maybe the true value of the house was two hundred thousand pounds your network gave you a random number of zero 000 pounds you know you're there for 200 000 away so you then need to look at all of the weights and adjust them by how much they need to change to make the nut the result you've got out that network move in the direction of the correct answer now you don't move it all the way in one go you don't jump the result up by two hundred thousand you move it by a very small amount so that you can slowly move in the direction of something which is going to get you the right answer so after this you go through all of your weights and you update each of them by a small amount by looking at what their derivative is with respect to what the how wrong you were was scale that down a little bit to try and slow things down and then you do the same thing again you show it another example and this means that the more wrong the weights are the more they work towards the answer and as they get close they slow down and start propagating towards a settled result the idea being after this magic if you put in three values which represent a house it is going to give you back something which is going to make a good guess as to what the value of that house should be so i'm going to answer a few questions from chat before i move on nissan asks how's it going to stop and what's ideal you keep on running until your training data set is looking your testing data set sorry you keep on going until your test data set looks like it's giving you a good result at its core there's more nuance than that but that's the basic answer you keep going until your test data set your validation set is saying this is looking good this is answering the question quite well connor asks an insightful question in a network with a huge number of weights wouldn't analyzing them individually be an inefficient method of tweaking indeed so the way i'm describing it here is how you would do it if you were doing this on paper in reality uh modern techniques represent the network as a huge multidimensional tensor and you just do tensor arithmetic on it and it has the effect of doing this simplified maths and by doing this tensor arithmetic you get nice fast results because hardware can be designed to do tensor arithmetic very very quickly canal asks how should we determine the learning rate use the defaults of the software that you have in front of you they will generally have a default learning rate which works well if you find that it's behaving poorly as it's training you can tweak that learning rate and there are in fact algorithms and in fact we're going to be using one today where the learning rate adapts automatically as it goes through and roy white asks is this essentially manual optimization in a way yes but we're doing it using a computer and so it can do it a lot faster than we can you can imagine looking at each individual thing tweaking up and down seeing how it affects the answer tweaking the next one and so on and so on we can do it all in one go using the network and it becomes scalable and michael asks does this always converge no it doesn't there's no reason to assume it's always going to converge it's an extremely complicated mathematical object and even if it does converge it won't necessarily converge where you want it to at its core most neural network training methods are doing a form of gradient descent and you can imagine that in twodimensional way on a big plane covered in hills and valleys and you're trying to find the lowest point in that plane so you start by rolling a ball down a hill it's very possible in that situation to end up in a valley that's near where you started that is not as deep as a valley which is further away so that you've ended up in a local minimum and you haven't ended up in the true result so there are techniques you try and use using randomness and adaptive learning rates to try and avoid getting stuck in a local minimum so to reiterate what we were just talking about there here is a much uh cuter example than a house here is a lovely picture of a dog imagine you can extract some properties of this photograph of the dog maybe we add up all the pixels and look at their red green and blue values and so we end up with 20 000 input neurons for example one for each pixel however you want to describe the picture we start with a random network we put all the numbers going through so all the pixels end up in this input layer on the left hand side the three new ones we then pass it through to the next layer and the next layer the next layer of course we need a more complicated network than this but we would still get an answer with this network we run this example through and we get some kind of answer it's randomly weighted to start with and so it says something like 37 or 0.37 which we decide means is 30 set 37 dog and 63 cat we know that the truth in this example because all of our examples are labeled is that it is 100 dog and a zero percent cat and that means that we are 100 minus thirty seven percent are the 0.63 percent wrong and so we use that 0.63 to work out how much we should change all of our weights by it's a positive number so we should generally multiply all of our weights in that equation by a positive number and that's going to slowly tweak our network towards an answer we're going to do this thousands of times each time taking a really really small step and that's hopefully going to over time push the network into configuration where it's able to answer questions similar to that one that we started with uh alastair asks are there ways of assessing the rate of conversion as a proxy of the confidence that the conversion is generally a general rather than a local optima in principle you can but it could just be that you happen to be in a flat bit of the terrain at that particular time you could start off in hill country out in the distance then have to go through a large flat area before then ending up in a really valleyish area in the middle of the country so you can't necessarily assume that because it's converging slowly that you've ended up in the right place you have to use clever techniques you have to be careful to not end up assuming that you've ended you've solved the problem before you have and as lester says there you also start off in lots and lots of different initial places and you see if any of those are going to push you towards an answer i've been talking through so far effectively how you would do this by hand you can imagine that you could sit down with a bit of paper lovely picture of your man good job um you imagine you could sit down with a piece of paper draw your network out work out what the derivative of all the weights are and look at the numbers of the inputs do the maths do the inversion and keep going and keep going as i said the way this actually works is by doing this using very very large tensors using gpu accelerated hardware all this kind of thing because the scale to which you can get is not is not coverable obviously if you're doing it by hand you have to use computers which is why it's a classic machine learning algorithm again we also don't write the software by hand to describe the network and to do the back propagation and to do all these things we use software that exists already to do that work for us we're not going to spend five years trying to compete with google or facebook with their techniques we're gonna use software they provided which is shown by the whole industry to work really well now there are lots and lots of different pieces of software out there which can provide a way of describing a network of training a network and evaluating a network and some of the most popular ones are well the two most popular are pi torch and tensorflow so pytorch i believe was originally produced by facebook and tensorflow was produced by google they are both fantastic neural network libraries which do all the bells and whistles you'll need for any kind of complicated network in the course today we're going to be using tensorflow but um pytorch works brilliantly well so i'm not going to say anything bad against fighters there's also a package called karas which isn't itself containing any algorithms for doing the training it is a wrapper on top of the other packages to give a nicer way of describing the problems you're trying to solve it's a bunch of niceties and addons basically and so what we're going to be using today is keras's nice addon layer on top of tensorflow there's also a package called cafe 2 which isn't getting quite as much publicity these days but it and its derivatives are still thought of very well and they do a good job and finally psychic learn so yesterday in the applied data analysis course we use psychic learn for doing other machine learning algorithms it does have a neural network a whole set of neural network packages and modules built in but it's not going to be anywhere near as performant as things like pytorch or tensorflow it's not going to perform very well for your very very deep neural networks and it's not going to take advantage of the gpus in the way the other packages would but for exploring and playing around and getting started psychic learn works perfectly well and you get the advantage of being able to easily compare to other machine learning libraries so i said we're going to be using tensorflow and we're gonna be using a bit of a chaos layer on top of it so let's get on and actually see how we can use some code to describe train and evaluate on your networks with a a real example so the example we're going to use is a famous one if you've ever done a machine learning course before and by in this i include our course we gave yesterday you will have seen the ios example so this is a data set that was collected quite a way back and it is been used for decades for giving machine learning training and the data set at its core is information collected about three different species of iris flower so the three pictures here are three different species and we are going to try and design a network which based on measurements made of these flowers not made not based on photos but based on measurements made of the flowers we are going to try and decide which species the flower belongs to so these three are iris setoza i was verticolor and i was virginica that is the extent about which i know anything about these flowers however i do know about the data set that describes the flowers so there are 150 examples in the dataset and the link there i think takes you to the wikipedia page let's have a look yes it's famous enough that it's got a wikipedia page so if you're interested do have a look at that page later on and learn about the nuances of this data set so each flower that was measured by a particular person back in the day they went and measured four different properties of each flower they took a ruler they wrote down their notebook in a table and they measured these four things that we see in the table in front of us the length and the width of the sepal and the length and the width of the main petal and their idea was that based on just those four measurements they should be able to distinguish which species the flower belongs to and remember that we need to have this being a labeled data set we need to know what the truth is so that we can nudge our network in the right direction as it's training and so we also record the species so species 2 1 2 0 etc remember that we're dealing with maths here all of neural networks are maths and so anything that we come up with that describes something human or physical we have to convert that into something numerical in some way and so in this data set here we are using the number zero to represent the setoza one to represent the vertical and two to represent the virginica and so you can imagine that we want to try and train our network so that if we show it a set of sequels and petrol measurements which represent acetosa we want the network to output a number which is near to zero so now we've seen what the data set looks like we now think about how we can design a network which can take these first four columns now the inputs to a network are often referred to as the features we want to take something which can take these four features and give us out our label and do that consistently even for examples that it's never seen before okay so to see a bit more visual idea of how this works this picture was taken from the wikipedia page we have here a multiscatter plot of the four different features so along the uh along the rows we have the length width of the sepals and then the length and width of the petals and likewise down the columns we have the length and width of the sequels and the length and the width of the petals and so in each off diagonal we have a scatter plot of one of those features against the other so in the second row in the first column we have here a scatter plot of sepal length against sepal width and we have each of the dots in that colored by what the true label of that particular flower is and so you see here there is a fairly distinct cluster of the red which is the setoza whereas the blue and the green are relatively intermingled with each other so based purely on this scatter plot you would probably be able to make a good guess about whether a flower is a setoza or not but you wouldn't be able to easily distinguish between a verticolor and a virginica and if we go back to slides you'll see that you can probably guess which one's which the satosa is this one on the left and the other two flowers do look very similar to each other to my untrained eye i would struggle to tell those two apart however if we look in other projections some of them do have better separations but there's no one projection which has perfect separation between the data sets they all have at least some overlap between the blue and the green and i do apologize if you're colorblind um we i'll try and do a better plot for next time with better color blind friendly colors the idea however here is that even though no one single projection can distinguish the two we're hoping that by combining together all six projections because those are the four features um combined together all six of those different projections we can come up with some kind of complex multidimensional description of what's going on in a way you can imagine that the network is going to divide these things with kind of dividing areas in the different planes with certain probabilities and then some to get those probabilities at the end and give us an estimate of which flower species each example is that's not exactly how a neural network works um there are other machine learning techniques which effectively do that and they would also behave quite well on this this isn't the kind of problem that you need a neural network to solve this is something which can be solved with other relatively simple techniques but this is a good place to start when learning about how we can design a network and train it and go through that mechanical process so the code for this is all linked at this link here so on this page in the notes click on iris dot i pi nb i'll click on it as well to show you what it looks like it will take you to a page on google collab which looks like this i'll show you how to run through it in a bit so if you want to follow along and keep track of where i am feel free to have this page open in another tab but i'm going to have all the code samples on the slides and i'll talk through it bit by bit to explain what the different parts of the code are being used for so the code we're using today is using tensorflow and keras and it's being written in python but all the concepts and ideas i'm going through will apply for any other kind of programming language or tool so the same steps we're doing here we would have to do in pi torch we'd have to do if we're using r or we'd have to do it using julia or some other language so it's not necessary today about learning about the python it's learning about the steps you go through and as a side effect we're learning how to use tensorflow i'm going to jump into the question or two in chat before coming on this slide so tim asks how come the categorical output are incident coded rather than one hot encoded in reality tim they are one hot encoded it's just that in the original data set they are integer encoded when it comes to the network it is going to treat them automatically as being one hot and that's one of the features that kevas and tensorflow just handled for us we tell it it's categorical and it does that work marius asks when working with categorical features should we convert those into numerical features more or less yes and so um tim's and maurice's questions are related now i don't cover in detail what one hot encoding is here but let's just post it there in the chat but at its core you can imagine that we have three different features here we've got sorry three different labels here we've got is it flower one flower sorry is it flower zero flower one or flower two now when we're training a network it's going to be trying to aim for flower zero one or two if it's flower two and it's actually giving a value of three for example then we need to bring it down but also if it's below we need to bring it up and so it's got an ambiguity about if you're between two flowers which one it's going to be and other flowers even if it's likely probability they are necessarily further away because of the inevitable ordering of integers so what you can do instead is take those three different features and turn it into a threebit binary number so you have a zero or one followed by zero or one followed by a zero or one and you use the first digit to describe how much of it is the first species how much of the second speed is in the second digit and how much of the third species in the third digit so if it is a flower species zero the number would be one zero zero if it's a flower species one it would be zero one zero if it's a flower species two it would be zero zero one so the place where the one is tells you which category the particular measurement is in and that's why it's called one hot because the number one labels the hot place in that measurement and this allows you to find scalable probabilities for all of the different classes in a clever way but the nice thing about tensorflow and coax is they hide that for us we don't have to worry about binary digits it's just going to do the right thing for us so the first thing you need to do is load in our data so to be to cut to the chase with this psychic learn which isn't the package we're going to use to do our training with but it does provide us with some data loading facilities has a function called load iris and this gives us the table that we saw on the previous slide as a piece of code it's something which we can use and analyze and pass through the system so we are going to load into our x parameter the data of the load iris data set and the y value we are going to load the target so x is going to contain the values which were measured with a ruler it is going to be the length and width of the sepals and the length and width of the petals four columns of this data and then the y parameter is going to contain which species each of those samples relates to so we're keeping our training data and our labels associated with it in separate variables and that makes sure you don't accidentally leak some of your labels into your training set and skew the whole thing once we've got it loaded in the next thing we need to do is turn it into our training and test data set and just split those apart and again for this there is a psychic learn function called train test split which we give it our x and y and it gives us back our train and our test for x and our train and our test for y so this has done it randomly we can trust this to do it well pseudo randomly we can trust this to do our job for us by default this does a 25 split for testing and therefore a 75 split for training if we look at our x train value we see we have an array of lists or way of arrays twodimensional thing each sample is a row and each sample has four measurements one for each of those things we measured with a ruler on the flower so this is one sample you one flower this is the second flower and this is the third flower and these are the numbers that are going to be put into the input layer of our network these four numerical values if we look at the y thing and the first three that corresponded to it we see we have the number zero one and zero so that's saying this first sample here is flower zero flower species zero this here is flower species one this here is flower species zero if we look at the shape of these two things we see that our x data has 112 samples and four columns and our labels there's just 112 numbers it's just a onedimensional thing so our x is a table of data our y is a list of numbers and in general this kind of shape of things is going to help to ask is the capital letter for input data and lowercase outputs a convention there's a bunch of different conventions you often see x capital x being used because it's representing something which is a vector of samples so you've got something that's got multiple dimensions to it the fact that this is two dimensional is it's capital lettered same reason when you're doing a vector maths you sometimes do an arrow over it to sort of designate it as being a vector also sometimes use capital letters for matrices and this is effectively a matrix the conventions are a bit wobbly but it's more or less the convention that i'm using in the notes here and i think i'm consistent if i'm not please do ask this is our data we've got this for the training we've got equivalent shape stuff for the test data set we've got 112 training samples and therefore we've got 38 testing samples simply having your data in a twodimensional table isn't itself enough we need to do some prep work on it so that tensorflow understands what the data means we have our training sample but we want to be able to show them to the network repeatedly over and over again randomizing the order so that it doesn't do things like remembering the order of the samples for example it's not going to buy us towards remembering the ones at the beginning or the end more strongly so we're going to randomly shuffle them together and by doing over and over again each sample will contribute a small amount towards a result but at different points in the training and so hopefully you'll get a nice balanced result so the two bits of code here again the code details aren't important but it's good to know the kind of thing you need to do we take our data sets our training data set x and y and we turn it into what tensorflow calls a ten a data set the data set in tensorflow kind of encapsulates all the stuff that the network needs to know to train from it and that includes things like the fact that the data should be repeated over and over again those 112 examples should be 112 way through and then show them again and then again and then again they should all be shuffled together here we're shuffling them in batches of a thousand so the ordering that they were in the original sample isn't going to factor in and finally we do a thing called batching and this is another way that you get a more generalized smooth approximation of the answer and that is by showing it not just one example at a time as i was explaining we do before in fact we actually show it 32 examples all at once and because this thing's being represented as a big complicated tensor it just adds another dimension to the tensor which we end up inverting and multiplying and doing math stuff too and so we can show it multiple at once and effectively the average effect of this batch is what gets applied to the training weights and that smooths things out stops you ending up jumping around too much and makes it more likely you're going to find a generalized answer the numbers we use here 1000 or 32 you find them by tweaking the numbers messing around with it seeing what works we then do the same thing with our test data set except in our test data set we don't need to repeat it and we don't need to shuffle it because those things aren't affecting anything and we just batch it into batches of one because again we don't need to worry about this data set having any effect on the network it's only being used to measure it does batching affect the degree of overfitting it's possible by having larger batches you can reduce overfitting you'd have to fit the network for a lot longer to get the same overfitting effect but it's not the primary use of it the primary use of it is to smooth out the training and to stop you ending up in weird local minima too much exactly um we show it 32 examples and then based on the effect of those 32 samples we then update the weights all in one go i need to ask is it similar to crossvalidation in the with the test and train data sets this is effectively a form of crossvalidation we're doing it's not there are more details and nuances to advanced crossvalidation to really avoid the nittygritty of overfitting but this at its most basic is the first step towards doing crossvalidation to avoid overfitting and then carter asks does batching help the time taken for training it can do as long as the computer hardware you're using is able to hold four samples times 32 samples in a batch so four times 32 numbers in its little cash register to do the the maths to then it's going to make it faster if you make this number too large then it's not going to be able to fit in the memory of the machine and it's then not going to be able to train at all you'd often go the approach of making this number as large as possible such that it fits into the memory of the machine in the example we're doing here we've only got four features and so you're not really ever going to have problems but i've dealt with networks where you've got million features or something like that and then when you start having a batch of 100 or something you start potentially pushing into memory limits of some computers right so we've got our data set up we're all ready to go with that it is ready to be shown to the network it just needs a network to be shown to and so we need to design a network which is able to do this evaluation and the network we're going to design here is a very very similar structure to that which i showed in the earlier slides it's got a input layer at the far left an output layer at the far right and it has two hidden layers we use keras here because keras is what allows us to write layer sorry layer after layer after layer after layer just in a python list and so we start off with our input layer we say our input has four features and so we've got four newer ones in our input layer that's all we need to do to tell this how it's going to work based on this it's going to know to ask for our data set for things which have a dimensionality of four in one of the dimensions so a size of four in one of the dimensions then we're going to have a hidden layer both of these layers here are the hidden layers because they're between the input and the output and our input layer here is going to have 10 neurons in it followed by another hidden layer with 10 neurons in it and the final output of our network is going to be a layer which has three neurons on it we choose the number three here because there are three different categories that we want to put our iris samples into what this is actually going to do is instead of having one single output neuron which has a number and we try and push that number towards the result we actually end up with three different neurons each of which is going to represent the probability of a particular measurement being one of the particular flowers so if neuron one has a high value and neuron two and three you've got low values that's saying it's likely to be neuron one is the answer and we'll say that neuron one will decide in advance is flower zero for example we apply the soft max activation function to the output of those neurons because that is what turns it into a probability it basically works out the it normalizes it so the sum of them add up to one and so we end up with an effective probability being the output of each neuron of it being each particular species i mentioned activation functions earlier and it was more or less in passing and here you see we have to specify what the activation function is we have to say based on what gets added up and multiplied inside the newer one what function do we apply to that number before passing it on and if you don't know any better a good place to start is using the relu which is let's have a look at the wikipedia page for it it's the rectifier and this is a particular form of the rectifier the rectifier linear unit it's a function which looks like this it's the blue line there you see that if the output from the neuron which is x is negative it sets it to zero if the output is positive it just sets it to be the number now this particular activation function has some nice properties which mean that it works well at describing nonlinearity and things like that if in doubt go ahead and use the value or spend a few weeks reading the the literature out there to try and decide on a better activation function relu for most purposes is going to do the job quite well okay so i'm just going to check for any questions uh lester's answered a bunch of them so we may ask why do we need the comma after four for input neurons four comma that is a particular pythonism which is saying that this input function here takes arguments and the argument it takes has to be a list of dimensions in this case in python this is how we make a python tuple which only has one element inside it which is the number four nitin asks do we have to do feature selection before or does the model do it for us so if you want to do feature selection and think about what features are important and not you should do that before the network you can however query how the network is training to try and discover which features are interesting or not but i always recommend the best thing to do is to use your scientific intuition to think about the different features and decide which ones are important which ones are not look at correlations between them look at the effect that different ones have based on other machine learning models and decide which features are important you can also do various post processing to your features if you think that's a useful thing to do otherwise one of the nice things about neural networks in many situations is if you put in a useless input feature then it will just get pushed to the side all the weights coming out of that feature are going to get set to zero because they're going to have no effect on the output they're going to kind of get ignored now that's not necessarily a situation you want to be in but it does mean you can sometimes get away with it if you've just got a weak feature in your data set but obviously the more features you have the slower your network is to train so we've described our training data set and we've described our network so in principle we're ready to go now now we just need to describe how we're going to mesh the two of those things together so before we can actually train it we need to tell the algorithm how it's going to work and remember this is more or less a back propagation thing being applied but there was some unspoken details that went into that back propagation algorithm which i didn't cover at the time because uh we need a real example to actually understand what's going on so remember to forward that propagation about you show the the network a sample of data you push it through forward through the network and you see what result you get and based on how wrong that answer is you need to decide how much to change the weights by and you can see if you've got three different outputs and you've got a class you want to end up at you need some way of describing how wrong that is so if for example the three neurons on the output were 0.4 0.3 and 0.3 and the real answer is zero naively there's no obvious way to be able to say well i need a single numerical value to describe what how wrong i am so we need to come up with a mathematical function which can take the three neuron outputs and the place we want to be and give a number out of it for example we might want to get a number like 1.13 we need a single numerical value and this is what the loss function is for now there is an art choosing loss functions but mostly it comes down to what kind of problem are you trying to solve in our case we're trying to put things into categories so we want to use some kind of categorical loss function and in the situation where we are where we have multiple neurons as outputs with values and we have a single integer describing the class that we want to assign it to then tensorflow comes with a loss function called sparse categorical cross entropy if you really want to you can read the documentation page and the paper about it and learn about it but for our purposes it's good to know that it does the job well and in most similar cases it's going to do the job well almost every network i've trained in my career so far has used this fast categorical crossentropy loss function or a very similar variant on it so that's going to tell us how wrong we are so we can use that we can feed that into our back propagation now the other thing in the back propagation is the particular function that we use to accept all these arguments and decide how much to change the weight by on the earlier slide i had that uh sum over the differentials and then you shift the weight by that amount with a delta delta omega that was a simplified version of how gradient descent works of course when we actually come to doing the real job we have to have a specific real answer about how the back propagation is going to work and so we need to tell the network what algorithm should you use and here we are going to use the adam algorithm which does something very similar to how i described it before except it's a little bit smarter one of the things that the adam algorithm does is it has an adaptive learning rate so if going back to our example of being out on the planes with some mountains and hills and valleys and stuff around when it's on really flat land it will move more quickly it will move across flatland really quickly but as soon as it gets to the hills it's going to slow down to make sure it doesn't jump over any particular local minima in the in the whole domain the last thing we have is the metrics this isn't used to inform the training per se it's only used to tell us how the training is progressing like how well it's doing so we can keep an eye on it as it's going through so we've prepared all the pieces we have our data we have our network we've described how they're going to fit together then we just need to actually kick off the algorithm which is going to do the job and so we've created our model which was made up from these different layers and every model in tensorflow and keras has a fit function so we just call that fit function we show it our training data and it's going to go away and do the fitting that's pretty much all we have to do the nuance on top of what i just said is that as well as showing it the training data we also want to give you access to the test data so that while it's training it can be printing out onto the screen how the network at the point it's got to is working on the validation data set and so as it's printing these things out and it's training on the trained data set and evaluating in a side channel on the validation data set we should see the progression of the network improving for example one of the things that we asked it to tell us about if i go back to the previous slide we asked it to tell us about the accuracy that is of the samples that you have there how many are you getting right and how many are you getting wrong we want that number to be 100 and anything short of that is less good and zero is terrible so what we'll see as the network progresses is that the accuracy by looking at the training data set is going to improve over time and the accuracy on the validation data set is going to hopefully also improve over time the way that you spot whether your network is overfitting is whether the accuracy of your training data set keeps on getting better and better because it keeps on learning more and more nuances about the training data that it's seeing but then your validation data starts getting worse all of a sudden because your network is no longer generalizing well it's learning specifically about the training data set and so it's the combination of looking at the accuracy on the training data and the accuracy on the test data that tell you about overfitting and things like this the very last thing we need to do is tell us tell it how long to train for so we say for each big loop of training data you should do 150 look at 150 examples this is actually 150 batches so it's going to be 150 times 32 different flower examples it's going to look at and it's then it's going to evaluate against the test data and then it's going to do that 10 times it's going to do over and over again 10 times until it's finished now i've chosen these numbers because i know that by the time it's done that 10 times it's going to give a nice answer but in reality there are more advanced techniques to work out how long you should train your network for so when we run this function it's going to go off do the machine learning do the back propagation and it's going to output the information to the screen and it should only take a second or two with the data that we're looking at newton asks a question how is the model handled and balanced data set um and there are techniques there by choosing the right metrics is one of the main ways that you do so you choose a metric which rather than looking at accuracy which if you have a very imbalanced data set is going to tell you that you're often doing quite well because one of your classes only crops up one percent of the time so you have to think of a different metric to accuracy to describe it or you do some clever stuff to rebalance your data set i know this is something that lester who is working on a machine learning project at the moment is currently fighting with and it's not always very fun ume asks what steps for epoch's doing so steps for epoch is how many batches should it look at before it prints some statistics to the screen historically steps for epoch would be how many samples should i look at or other how many samples do i have and an epoch would be how many times should i look at the full data set when you're repeating and randomizing and batching your data those definitions fall apart a little bit so mostly you can think of it looking at all the batches one and a half thousand times 150 times 100 times 10. however i break it down into 150 and 10 so that it's going to print out summary statistics as it goes along so mostly that's a hack to make tensorflow describe what it's doing as it progresses so we call fit it goes away and does it and then as it's going through it's going to print this stuff and i'm going to just describe what you see on the screen and then we're actually going to run this stuff for ourselves so for example on epoch 10 the last epoch it's going to print out the value of the loss function remember that sparse categorical cross entropy thing that has a number we're trying to make that number small we're trying to reduce how much it's lost and we're also trying to measure the accuracy which will make the accuracy large we want it to be accurate when assessing the data that it's training over as well as the loss and accuracy on the training data set we're also looking at the loss and accuracy on the validation or the test data set so we want these numbers to be moving down at the same kind of rate as those two numbers they should have similar values and they should progress at a similar rate if they start diverging from each other that's a sign that you're probably over fitting on your data set and you need to reevaluate how you're balancing things up so this is telling us that at the end of this it's got a 75 accuracy once we've trained our network and we've got um everything all working then we have to actually use the model so this is after the model's trained it's learned everything the weights are set and they are fixed at this point the weights don't change anymore they are just going to be used we're no longer going to do any back propagation we no longer have any true labels we're comparing with we're no longer doing loss functions any of that stuff we have a fixed network which is designed to answer questions so we're going to pretend that we've since training the network gone out into the garden and measured some more flowers using my brain i know which one is which i know i've got one of each flowers and the four features of the first flower are those four four feet to the second foul are those and the four features are those we take this data this prediction this prediction this data we want to predict over these three samples and we pass it to the predict function of the model it takes some data that's the same shape as the training data and it's going to give us back some predictions this predictions is going to contain the values of the output neurons for each of the three samples that we've shown it we then do a bit of python magic where it takes those predictions loops over them find the one that has the maximum probability and then grabs the actual name of the flower out of the look up table which made zero be one flower one be the other flower and two of the other flower we run that bit of code and it gives us back its prediction of what the species of those three flowers are and if we look we see that it matches what we had at the beginning it's also worth noting that during this process while we up front wrote what the three flowers were we never actually used that in the prediction that was just for our human purposes it wasn't used by the code at all it's not cheating and looking the answer it's only looking at these data here umai asks where did it convert the binary number zero one or two like in y test it did that automatically by us describing the loss function as being the sparse categorical cross entropy that function there understands how to take an integer value like this and turn it into a one hot encoding which can be compared to the three output neurons this function is designed to kind of do that one hot thing automatically for us and it is just a function which you can call with the value zero and a list of numbers and it will give you back a loss value right enough talking for me let's actually go ahead and run it so uh go ahead and click on that link there when you've i'll go through this in a second so um just click on this link here if you're not logged into your google account already then you'll have to sign in and to do that you'll click on the sign in button up here in the top right once you're logged in go to run time up here at the top and click run all once you click that it's going to run all the code and it's going to run all the way through and while that's running i'm going to answer some questions it will warn you that this code is coming from github it is coming from my github account so if you trust me go ahead and run it if you don't trust me press cancel but i do hope that you trust me and you can see the code here and understand everything we've just done annika asks how would you look at the layers and see how the network makes decisions either planes separating the three species there are things you can apply on top of the data the network to try and do this tensorflow comes with some tools to do these kind of things but it is a whole whole field of research to try and understand what's going on to see the planes here for example you could just sample the face space and do a 3d plot and try and see these clusters but more advanced techniques would take more than this course to apply but the tensorflow documentation does have some information about this so this has now worked it has loaded the data it's done all the um shuffling of the data we've designed our network and my screen a little bit bigger that's much better and then it's on the training and so you see here we've started out and the loss function was off the first epoch quite large and the accuracy was quite small only 20 notice at the very beginning here while the training accuracy was 20 the validation accuracy was 40 and that's just random chance after the first iteration you could easily expect them to be the other way around what we want to see as we progress however is this accuracy increasing over time and that's it saying that based on the examples that it's being shown it is doing a better and better job of representing them but remember as well as that's increasing over time we want to make sure that the test data set the validation data set is also increasing and we see here it does and it quickly kind of starts aligning with the values 79 81 after a while the network has got to the point where the validation accuracy has topped out it's no longer getting any higher because 97 is probably of the 40 or so examples probably only one example that it's no longer getting it it's failing to um it's it's it's not able to resolve that one last example but it's doing all the rest of them quite well and from that point on it never gets any better on the validation data set we might need more validation examples to get a better idea of why but it's not necessarily a problem that the validation accuracy tops out what would matter was if this validation accuracy started going down we also note that the violation loss is still reducing which is a good sign that it's continuing to go down in concert with the uh loss function of the training data set so and right at the bottom you see that example and it says the sosa is 94 98 sure that it's correct the versacle is 97 sure that's correct and the virginity is 92 percent sure it's correct and so you wouldn't necessarily expect to see the exactly the same numbers as that because there's a random element but you should expect to see numbers in the 90s right i was going to go through a few questions here i've seen some things cropping up yan's asking about the role of the test and training data set so i'll go back to the beginning and briefly show what's going on here so we have our all our data we split it into training and testing and from that point on those two data sets are completely treated in independently our training data set gets shuffled and blocked up and our test data set just gets left how it is pretty much when we come to the fitting we show it the train data and the training data is what's going to be used during the back propagation process that's the one that's going to be having the loss function applied to it it's going to have the weights being recalculated from it and it's the one that's going to cause the network to collect towards a result so the network is only going to learn from the things that are in the trained data set it is never going to see for its training purposes anything from the test data set the only thing that the validation data test is used for is for calculating these numbers it's not used to inform the network in any way it's just used to give us this output so that we as humans can assess how well the network is working tim asks if you restart the window and start again it will by default in this case start again from scratch entirely or wipe all the weights and run it all if you run just the fit function again i think it will carry on from the previous weights and ozalpas asks can we access model metrics and model selection comparison yes so keras and tensorflow have a whole bunch of functions built in to extract information from these models for doing comparisons and whole kind of crossvalidation high performance tuning setups so that's all possible in tensorflow we're not going to cover it all in this course because it's a large topic we're trying to focus on as simple an example as i can come up with okay looks like i can update the notes and thank you lester for posting those links i'm going to close that page so that's the end of the ios example that is the arguably the simplest neural network you can think of designing and you see that even though it's a simple example there's still a lot of questions you have to ask and answer along the way you have to think about structure of data about tests and training data splits you've got to think about your loss functions and how you're going to repeat your data you've got to think about your optimization functions there's lots of choices you make along the way but the first thing to think about or to remember in that situation is you don't always have to make those decisions up front i'll say this again because i think it's worth reiterating the best place to start with the network is what someone else has created before you you can go online you can find predesigned and set up models in tensorflow to solve a problem you just import that code show your data and see how it performs that is going to get you a lot of the way there by understanding what we've gone through today that's going to give you the ability to tune and tweak what you're seeing to slowly start understand how to apply it to your particular situation so thomas is asking why the validation accuracy didn't keep increasing as the loss decreased on the validation data set and that's because it the accuracy is a course measure it's measuring of all the samples add up how many i got right and wrong and give that as a fraction the loss function as leicester mentioned earlier is differentiable it is smooth and so it can keep on increasing and so while the accuracy wasn't going up the degree to which it was sure about each of its answers would have kept on going up it might have once it got to that that threshold been saying i've got the right answer but i'm only 53 sure by the end it might have been saying i've got that same answer but now i'm 90 sure for example so that's why the loss can keep on improving while the accuracy doesn't necessarily change so i'm going to start covering this section here and then we're going to have a break and come back through the last example so this should only be maybe 10 minutes and then we'll have a break and so for this we're going to go through some image analysis stuff because we want to learn how we can apply neural networks to pictures as well as how we can apply neural networks to plain old numbers but before we can dive into applying neural networks to images we need to understand how computers treat images because images are more complex things than measurements made with a ruler so the flip side of this is that pictures are also much easier to collect it's much easier to grind your garden and take a photograph of a bunch of flowers than it is to go out with a ruler and measure all of their parameters it also requires less skill so you can collect larger data sets by using crowdsourcing and so on so i'm going to go through now and explain how image analysis techniques work so to get a sense from the class could people just post in the chat what kind of experience you have with image analysis using kernels and convolutions and things like that is it completely new to you or have you done some of this before i'm assuming it's completely new to you which is why i'm going to go through it all i'm seeing lots of news and nuns wonderful i'm sure there's gonna be some people in the room who've done this before and hopefully i'll be able to answer questions you have as well but let's start right from the basics so the idea of applying neural networks to pictures starts with the maths and computer science approach to dealing with pictures and so the place you have to start because we're going to be doing maths to this stuff because neural networks at their core are multiplications and sums and function applications we have to turn our picture into numbers i mean a consistent way of representing images as numbers and so i hope it you're comfortable with the idea of taking a gray set grayscale image just a black and white image and assigning for every pixel in that image a number and that number represents how bright that pixel is commonly these numbers are between 0 and 255 you've seen these sort of binary power of two numbers before so a pixel being 255 would be completely white and a pixel being zero would be completely black a number like 105 is slightly under halfway so that's a darkish gray but all the numbers on this little snippet here are a darkish gray the dots here are to represent that these pictures can be very large the numbers we have here is only a five by five sample in the top left corner but these pictures can be hundreds or thousands of pixels across in each dimension you know megapixels means millions of pixels which means millions of numbers which in our case corresponds to millions of input neurons and that's a lot of data you need to deal with so we have a grid of numbers sometimes people refer to these as matrixes matrices of numbers but don't think of these as mathematical matrices as you might have done in school or undergrad these are a grid it's a it's a grid of numbers and nothing more than that so once we've got our grid of numbers we need to decide describe some kind of mathematical process we can do to that grid and the way this is commonly done is by using a technique called kernel convolution and i'm going to explain the two parts of that term as i go through let's start with the first first part a kernel now a kernel in image analysis is a another smaller grid of numbers usually they're two by two three by three five by five that kind of size and they contain inside them a bunch of numbers and the magic of kernel convolution is depending on which numbers you put inside that kernel when you apply it to the image and we'll see how it gets applied in a moment when you apply it to the image different things happen so for example these particular numbers here will sharpen the image they'll make it less blurry you might have come across the sharpened mask or the unsharp mask if you've played around in photoshop or something like that a different selection of numbers with like one which didn't have the negatives here for example would blur the image you apply a particular small kernel to the image and it blurs it this is how all image blurring works in any kind of computer sense there's other kernels you can choose which do edge detection and edge detection is a very common computer vision technique for example working out outlines of things or it's also the core of what we're going to be seeing today now you might be wondering how can you know what particular numbers are going to give particular outputs and that's because some clever mathematicians have done that work for us they are all published and online you just choose the one that does the job for you if you want to blur the image you go on the wikipedia page you find the blurring kernel and you apply it that's all you have to do you don't have to think about what these numbers should do schweitz is analogous to normalization in a sense so um one of the things you might notice is that the sum of all the numbers in here add up to one and so after applying this kernel you expect it to have a sort of a normal effect normalization can be applied in lots of different ways so there might not be a an image kernel which can perform it but it's going to be a similar technique that's going to be done to it so the idea here is we have a set of predefined kernels designed by computer scientists and mathematicians which have effects which we can describe with words that's how these kernels are described the next question then is once we've got a kernel how can we use it to do something to our image and what we do is we take our image which here on the left hand side so you've got the purple section and the lighter blue out going into the distance we take our kernel matrix i say it's not again it's not really a matrix it's just a grid and we overlay it over those top three squares our kernel is three by three and so we overlay it over the top three by three grid and then we kind of shine a light through it we look at the top left number that's being shadowed and the top left number in the kernel matrix we multiply those two numbers together so 105 times zero and then we add that to the next two pairs that are over each other so 102 times 1 100 times zero and so on and so on we add up all of those total numbers and we put that in where the middle of that kernel is currently sitting you should ask why do we apply a kernel that's a tricky question so we apply a kernel because it has the effect that we want it to we apply a kernel because for example we want to blur our image and if we want to blur our image we choose a blurring kernel and we apply it to the image and we end up with a blurred image but at its core all kernels work the same regardless of what the numbers in the kernel itself are the process that's applied is the same and if we change the numbers the effect in the output image is also going to be different you'll end up with a blurred image or a sharpened image or an image which represents where the edges are for example so to take that picture and kind of turn it on its side a little bit we have the picture on the left hand side we have our kernel and we've done 105 times zero 102 times 1 100 times zero adding these up as i go 103 times minus 1 et cetera and so we see this in the sum at the bottom you see that's the calculation it's done and that gives us the number 89 and because this kernel was 3x3 and it was overlaid on the 3x3 purple area the result of that calculation goes in the middle of the kernel so the 99 gets replaced with an 89 so what we do after we applied the kernel the first time is we shuffle it across by just one square and we do the same thing again so it's overlapping with the original one but the middle of the kernel is now in the next space so now we're gonna do the same maths again but we've got different numbers so now the 103 gets maps to a 111 for example and we keep on doing that all the way along the row and on the next row and the next row and the next row filling up the whole image now what you might have noticed is that in that situation you're not filling in the edge values at all there's no way to represent what the number in the top lefthand corner should be because five of the numbers in the kernel won't have corresponding um image values to multiply against and so we have to make a choice how do we deal with that and that's just a choice that we make in our case here we can decide to just pad the edges with zeros that's perfectly valid you get slightly weird effects of the edge of the image but it's mathematically valid and it's consistent at least alternatively we could replace we could pad the outside with a repeat of the value so this top row would be well undefined but then it'll be 10502 197.96 repeating above the actual image there so depending on whether you want to zero pad or same pad it gives you a choice about how you want to deal with this edge condition you do this to an image by repeating that process over and over again obviously use computer if you use the right kernel for example there's a kernel that does sobel edge detection you start with the image on the left and you end up with the image on the right anywhere where the image is smooth just stays black anywhere where the sharp change ends up white so we've got an image now which shows where the edges are and that's a useful thing to have because edges are often where interesting stuff happens in pictures so we saw just before the break how at its core image analysis is done most image analysis algorithms have some elements of kernel convolution being done to them now the kernel is that little three by three matrix we saw and the convolution part is the bit that applies it to the picture and gives some kind of result so in principle it would be possible to take a whole bunch of kernels really carefully design them such that you can show them to an image one after another layering them on top of each other to get some kind of answer for example you could start off with one kernel which does the edge detection and then based on the result of that edge section you could have another kernel which says are there two edges next to each other here so they're parallel at which point you say okay the result of this is a image which represents where parallel edges are from that you have another kernel which looks for circles and then based on that you have another kernel which looks first circles next to each other and you start building up these questions getting more and more abstracted away from the picture but more and more towards human concepts like is this a person and at its core that's what convolutional neural networks are going to do now i alluded before to the fact that kernels in convolutional image analysis are designed by mathematicians and computer scientists and the example i just gave there i said if you choose your kernels really carefully you can do such and such a thing so what we're going to do is try and make those two things meet in the middle using your networks we're going to try and train a neural network to automatically discover what the values of the kernel should be we are going to design brand new kernels and a whole slew of them we're going to layer them on top of each other and through the magic of the back propagation stuff we saw before this is going to give us an answer which can based on a picture say is it a cat or a dog for example so the process is going to be the same we're going to be showing it examples checking how far away we are from the answer doing the same back propagation thing to the weights but the difference here is that the weights in the network are going to be representing the values in the kernel so going back to the previous section this matrix here for example has got nine values inside it you can imagine in our network that we have a weight associated with each of these numbers if this zero was a different number you can expect that the output of the network would be different and therefore this number here this weight has an effect on the output and therefore we can do the same thing we did before we work out what the derivative of that is and therefore work out how much this number in the top left of the kernel needs to be changed in order to make our answer more correct so it's the same idea as before but in this situation the weights are the values inside the convolution kernels the way we apply it ends up working exactly the same and that's the beauty of convolutional neural networks that the process is the same but the way that you treat those weights and the way you apply those weights you get a very different kind of idea a convolution on your network or a cnn is a classic example of a deep neural network they're deep because we're going to need lots of layers we need lots of layers because we need those levels of obstruction we need to start at one end with is there an edge here why is it the other end saying is this a cat there's lots of questions you have to ask along the way like where are the ears with respect to the nose what shape of the ears how do you describe an ear how do you find the edges that go into that where are those edges etcetera there's lots of questions you have to ask and so you need lots of layers in the situation where we're asking things about the location of something on an image with respect to the pixels around it on the image we have to use what are called convolutional layers now a convolutional layer doesn't connect every neuron in one layer with every neuron on the left on the next layer it only connects the neurons in one layer with the neurons in the next which map to the same part of the image so each neuron is representing a pixel b only connect to the neuron on the output with the neurons that are near to it on the previous layer as well as these convolutional layers which are the layers that are basically doing the same convolutional kernel thing we saw before just it's kind of encoded into a a neural network language we also have pooling layers and our pooling layer is a much simpler thing a pooling layer takes a picture which is a thousand by a thousand and makes it smaller so it takes a thousand by a thousand image and for example takes every two by two pixel and squishes it into a one by one pixel and therefore it goes from being one thousand by one thousand into five hundred by five hundred and we do that because by applying convolutional layers and then pooling layers we create layers of abstraction so in the output a particular neuron is having a larger area of effect on the earlier layers than it is on the later layers and so you get global information as well as local information the classic pooling algorithm is max pooling so based on a two by two little grid you find the largest of the four numbers inside there and that's the number that survives and goes on to the next layer and the third kind of layer we have in a cnn is the dense layers now dense layers are the same stuff as we saw before these are just the traditional neural network layers where everything is connected to everything and information just flows on through so here we see an example of what this is doing so we have an input image this feature maps blob here this is a layer this is one layer of the neural network now that layer has a bunch of feature maps each of those feature maps is encoding a kernel being applied to the image so by the first layer here we've got in this situation four different kernels being applied to the input image so if this input image was a 10 by 10 image then in the output we'd have four 10 by 10 images and each of those feature maps would be representing a different thing one of them might be looking for horizontal edges one might be looking for vertical edges one might be looking for dots that are by themselves who knows what they're actually looking for the point is each of the feature maps are likely to be different we then do this subsampling this pooling layer we make things smaller we do some more convolutions to make more feature maps with different algorithms being applied to them we then sub sample again to get a whole bunch of smaller feature maps which have loads and loads of different pieces of information inside them and at this point we treat each of these last very very small feature maps as inputs to a standard neural network like we had before by this point each of these can be represented as a single number we just put them in as the input layer to a traditional neural network so this is kind of in two halves the first part is finding sort of a graphical and spatial information about where things are in the image and then the second part takes where those things have been found and makes a decision based on that as to what the image is of we can do some amazing stuff with convolutional neural networks and i'm sure lester's got some examples you might post as we go through these sections one example i really like is image segmentation so here you can train a network so that it can identify objects in the image and it can overlay each image each pixel in that image with what kind of thing it is and this is how some selfdriving cars work so we can identify that as a blue the car over here is a car and it's been outlined with blue we've got the road we've got the arrows on the road we've got traffic lights we've got all sorts of things this is a convolutional neural network admittedly a complicated one but nonetheless a convolutional neural network you can also do things like style transfer now style transfer is a really fun thing to google for because since this example here there's been much better examples but this is a fun one nonetheless you show it an example of a picture on the left hand side by a particular artist you can then add things into that image just based on photographs and the network has learnt how this artist draws and so it's able to draw the things you've added in the style of the artist so you end up with this mcdonald's balloon being inserted right into a streetscape in new york in a style which looks consistent with the rest of the image and in this case the network has learned how the artist draws that's what the kernel convolutional layers are representing in a very complicated way nonetheless and finally again apologies to any people actually understand this is approximately how animal vision works so animal vision does largely work by looking for basic features it looks for horizontal and vertical lines simple patterns like that then it takes those simple features and your brain and your optical nerves and all this processing system in your head takes those things and consecutively applies more and more abstracted ideas on top of it until it ends up at the end with your brain able to identify this as a picture of a cat for example and most animal vision and brain systems work in a similar way to this although of course because these are evolved and not designed they are much more complex and complicated and also much cleverer in some ways so that's what convolutional neural networks are and how they work it is the same as we had before but we design a network so that it can calculate what the weights of these kernels should be and it's going to be going to say give me uh 60 different kernels work out what the weights in those kernels should be in order to answer this particular question and through the magic of that propagation it's going to manage to do that it really is quite amazing the thing that convolutional neural networks are for is for images that's at least 95 percent true so if you've got image analysis you almost always want a convolutional neural network of some kind if you're using convolution on your networks it's almost always because you're dealing with images they are designed for each other effectively so we have an example here where we're going to be dealing with this data set called the mnist data it's again a classic data set based on numbers i think written for automatic check processing in the us this is a data set which is freely available of 70 000 pictures of handwritten digits by all sorts of different people from the wild from actual real people writing checks to people each picture is 28 by 28 pixels so it's quite small and that's useful for us because remember what i said before that the number each pixel is going to end up being an input layer and then we're going to be doing our feature maps and so by starting with a small picture you manage to keep your network under control so we want to design a network where we can show it a picture one of these examples and it's going to say that's a 4. that's a zero that's a nine that's our task that we've got ahead of us so we're gonna go through the similar process that we did with the iris data set so again feel free to click on this link here it's going to take you to the um actually that one takes you to my github page but at the end there'll be a link to the code app where you can follow through the notes if you want to so we're going to do the same things we're going to specify the shape of the network we're going to say how it should be trained we're going to specify our training data set same process before this is process you always go through with your networks so design the network now we've had the question a few times about how do you know how many neurons how many layers how many how big the layer should be this layout of this network is a standard layout of a network for an image classification problem if you've got a relatively simple image classification problem this layout of a network is going to do a decent job as long as you haven't got 10 000 different categories as long as you're not trying to identify multiple objects in one image things like that then this is going to do a good job of classifying the images and the way it works is by starting with a convolutional layer now this is going to describe and design 16 different filters each filter is going to be a five by five kernel so a filter on a kernel is terminology means the same thing so you have 16 different kernels each of them are going to be five by five and it's going to make a new layer which is therefore the same size as the input but there's had this layer this this kernel applied to it but it's gonna do that 16 times all in parallel so if we had 10 input neurons we're going to have 160 neurons in the in the first layer we've actually got 28 by 28 input neurons and so we can end up with 28 times 28 times 16 neurons in the next layer um objects in the next layer then we have a pooling layer which makes our image half a size so it goes from 28 by 28 to 14 by 14 and then another convolutional layer and another pooling layer to do convolution pooling convolution and pooling that's a common sort of technique you get so you've got two layers of abstraction over the spatial information in the image then we finish off with a dense layer which combines together all of the outputs of the convolutions and tries to sort of answer ask the questions about how those different features that it's discovered relate to each other in the same way as we were evaluating how the four different features of the iris flowers related to each other to give us the answer we wanted the final output is the ten neurons which represent one for each digit so is it a zero one two three four five six seven eight or nine there's ten possible categories so we have ten neurons in the final image in the honest example we had three categories and so we had three neurons in the final image there's some details in here but i'm going to go through that as we go through so this structure works really well for mnist you could actually mist is a simpler problem you could get away with a smaller network for mnist but this is a nice generalizable classification network so you can use this in your work if you were doing classification stuff so we're going to do the same thing we did before we're going to build up our network we're using keras and we're going to use a sequential thing which means we're going to pass a list of layers they're going to start from the input and work their way through so the first layer is a convolutional layer it's twodimensional so it understands sort of built into these functions is the understanding of what images are and how they relate it's kind of wrapping that stuff up for us so all we have to tell it is that we want 16 different kernels 16 different filters each filter should be a five by five there was that thing before where at the edge we had to decide are we going to pad it with zeros or pad it with the same number here we tell it that we need to pad with the same and then again because this is eventually actually doing a mathematical operation of some kind we have to have an activation function and like before we use our our um our value function and that does the job well it lets us represent our nonlinearity in our model so we have 16 5x5 filters the layer will be 28 by 28 but we've got 16 filters so the overall size of this layer is now 12 and a half thousand so we now have 12 and a half thousand neurons in our next layer so a lot of neurons and each of those are going to have a whole bunch of weights they're each going to have 25 weights a lot of numbers to deal with already as you can see but nothing that modern computers can't handle we then into this list here we add a pooling layer and i've pulled out available here because we're going to be repeating this in a little bit and all we say here is we want a 2x2 pooling so it's going to make our image half as big in every direction and this one here is saying how much you want to jump along by each time and the same here is the same padding as we had before so we have now shrunk this down instead of being 28 by 28 by 16 it's now 14 by 14 by 16. so we've only got 3 000 neurons in the next layer then we do the same dance again we have another convolutional layer and another pooling layer so remember we have convolution pool convolution pool same bit of code only difference is at this time we have more filters we've more abstracted away so we're further away from the original image but we want to be able to represent more different types of things we're looking for particular curls and curves in the in the handwriting of the original image is what this layer here is going to represent the first convolutional layer is going to represent more things to do with where the lines are and if there's two lines next to each other this part here is going to be describing more precisely where different circles and curves and spaces are in the image as a whole and you see this number here is again getting smaller the layers are getting smaller as we go because we are abstracting away the information we are reducing down our face space so um each neuron have a different kernel so um it's a bit more complex than i was making out uh each there won't be the thing times 25 i misspoke when i said that each of the filters each of the 16 or 32 filters will have a um 5x5 filter associated with it so there'll be 32 5x5 filters and therefore there'll be that many weights so some of the weights are going to be sort of correlated with each other in some ways so it reduces the total number of weights you actually have to analyze which is helpful then we have our same thing as we had before this is the same set in the iris we have a dense layer now a dense layer means connect everything to everything 128 neurons so this is a number chosen which is big enough to be able to represent the variation in the images but small enough that it's going to be able to train quickly we end with a dense layer with 10 neurons and we have this soft max thing again to turn it into probabilities the only tricky part in all of this is this layer in here which i didn't mention before called dropout layer and what a dropout layer does is not actually a layer of neurons per se it's simply a thing that gets applied to the process and what this layer does is as the network is training once after each set of examples it will randomly delete 40 percent of the connections it won't set the weights to zero it just for this example will not use those inputs on the training and this is used to avoid overfitting it spreads around the information stops you getting stuck in local minima and make sure you end up with a more generalized model so you're artificially snipping connections and then reconnecting the next time so that no one connection becomes too powerful and too important so this dropout thing is often used to smooth the network out and allow it to be more general this bit of code here is exactly the same as we had last time we're doing the same classification task we're trying to put things into is it this thing this thing this thing on this thing is it one of these flowers is it one of these numbers same optimizers before and the same metrics so we are doing the same task as last time the way that it works the way the network's training is the same it's one of the nice things that once you've done this a few times you start seeing the commonalities and you stop having to worry about how you should make these decisions because you just use the same stuff as as last time which makes your life a lot easier so as i said before if you're doing a classification problem and the labels are the integer index of the class that you want to get to then sparse categorical cross entropy is the tensorflow solution to your problem use that one and you're going to be fine there's also a sparse i think binary cross entropy if you want to have a yes no answer at the end rather than a is it one of these 10 20 categories we've designed our network and we've said how it's going to be trained the order's a little bit different this time we are going to load in our data and for this tensorflow comes with a tensorflow dataset function which gives us our mnist data it splits it into test and train for us it shuffles it for us it does all the magic well much of the magic automatically for us so after this function's been called ds train and ds test that's our test and train data set they are both sequences of 128 by 128 by 1 matrices so it's a 3d matrix but one of the dimensions is only one big because it's only got gray it's not red green and blue and the values in each of those are the numbers from 0 to 255 so the same as we were seeing before there's a few things we do have to do with that data to make it you know suitable for our network to train one of them is that in most networks you end up working best if you have numbers between 0 and 1 rather numbers between 0 and 255 so we apply a function which divides every pixel by 255 and make sure it's a floating point number so that just turns them from zero to five five into zero to one then we do the same thing before we cache it and shuffle it and this kind of stuff details here don't matter too much this is just the process it's going through to get stuff in the right shape you would start with if you were doing this for yourself you'd start with these things and you play around with them and read the documentation and see where it takes you from there you then do the same thing to the test data set but simpler you need to normalize it but you don't have to do all the shuffling and stuff like we had last time the test data set analysis is a simpler process and then we call the fitting same as before we call the fit function and we give it the training data we're not giving the validation data this time because um we actually we do do so we do do underneath so the same process before but we're only going to do it through once so this notebook that i've got here on the screen that is exactly the same code as i've just gone through we set up the data we shuffle it all we do the normalization add all our layers together and call the fitting and you'll see here after the end of the fitting step the accuracy on the training data set is 94 percent and the accuracy on the validation data set is 98 so those are both large um but uh it's got there very very quickly with only one epoch it's already got a very good accuracy if we kept on training it could increase further but that would be something to have a playground with after session these collab notebooks will stay up for as long as you need them to the last thing we get printed out and i want to switch back to the notes of this is a table of predictions on certain images and what it thinks they are so let's have a look at what it looks like ah this is a good example uh training networks can take a long time sometimes so uh it's a good excuse to pretend you're working while your computer's doing all the hard work for you but it gives you a table that looks something like this it's got an image which is a number one uh it thinks it's a three because it's got it wrong we've given it a picture of a number two oh we've got that one right it thinks it's a two i'm not going to go any further because it's a bad way to look at data this is a much better way to look at data so in this table here down the lefthand side we have a bunch of images which once our network has finished training we are showing to the network and saying what do you think of this can you get anywhere with this kind of image and we'll see that it's done some of them quite well but many of them have done a terrible job at and this is on purpose we're going to get to solving this in a moment this isn't just a demonstration of how bad your networks are it is however a demonstration of how you've got to be careful about the difference between the data you train your network on and the data you apply it to so the data we had up at the beginning was all 28 by 28 pictures which were white ink on the black background now the only one that's whiting on a black background is number five and it's got that one successfully correct but the other images are mostly from a different source they're not handwritten they're different inverted contrast things like that and some of them it's got right but many of them it's got wrong so we've trained the network to identify white on black handwritten digits and it doesn't really behave well outside of that it gives an answer but it doesn't always give you the right answer so it's not reliable i would also importantly draw your attention to the very last row there we have that little dog we saw earlier so if we put a picture of a dog into this network it's going to give us back an answer it's not going to say none of the above it's going to say its best guess is that this looks like a number two maybe its nose has got the right kind of curve to it or something and so this is something you really need to be aware of you need to make sure when you're training your network it's been trained on the kind of data that it's going to be applied to there's a classic example which was mentioned in the course yesterday which i'm just going to post a picture to came up recently and this is the covid cat where a similar kind of situation network trained to look for kovind 19 if you show it a cat a bunch of the networks say that yes that's definitely got covered and even though they have no information really about what's going on inside they were trained on one thing and they are making spurious guesses about data outside of that so you have to be careful about the data you train it on and the environment in which you apply your network think about how general or not general it is jen asks is there a way to get a none of it answer with some networks you can have a go with that and that's what the um writers of the the publishers of that covered cat image have tried to do they've got a none of it example you've got to be quite careful about what you consider to be none of it and how that blends in with the categories you do care about it's a tricky problem to solve the important thing to do is think about it from the beginning think about the scope of data you're training on and think about how that relates to the um data that uh you're going to be using the network on regardless it's it's giving an answer for every single thing it's always going to tell us something and that's a common problem machine learning machine learning doesn't tend to be well designed to give you an i don't know answer it's a hard problem to solve but we can in this situation for these numbers do an attempt to improve that domain that we care about and i think the main difference between the numbers that worked well and those that didn't is that this was white on black and these were black on white the answer to this is doing something called data augmentation basically adding in more training data to represent a larger range of possibilities so the network learns a more general model of what the world looks like if you're doing something to identify dogs you don't just want really nice framed dogs with maybe the good camera you want pictures of dogs made with really bad cameras in dark conditions and snowy conditions all these different things you will have all of those the network can learn the full scope of the world in our case we've got some black on white on black images so we want to just put in some black on white images as well and that's hopefully going to make it able to understand the full scope of handwritten images handwritten digits sorry in general you're going to want to do data augmentation on any network it's a really really powerful thing to do if you can't collect more data then fake it by blurring rotating scaling the data that you do have in our case we are going to invert the images we're going to take our original data set and we're going to add on to it the same data set again but with black turn into white and white turn into black so we multiply it by 1 and then add one to it so that turns one to zero and zero into one and 0.5 stays the same so let's have a go and see what effect that has so go back to that notebook and at the very top in the top cell i made it super easy for you there's a variable that says invert equals false change that into invert equals true and go to runtime run all again and see what effect that has on it you do need to run all the cells again um so make sure it's true with a capsule t because that's what python expects then run time run all while that's running uh mia says this raises the issue of implicit bias absolutely and this is something i'm going to cover on my very last slide about being careful about the kind of things you train on and the kinds of questions that you're asking your computer to answer it's not as simple as throwing the numbers in and get a number out you have to think about the human in the loop for example so that's all run through it's currently fitting it's finished and it's doing the analysis and this is again going to print out our table of results and i'm going to skip that table of results and show you in a better format umea asks how to evaluate bias of a network well it depends on what kind of bias you're talking about one of the primary biases you get is sort of a selection bias where you've only trained your network on a subset of data and so it's only going to know things about that subset of data or sampling bias as well i suppose it could be as well so the way to do that is to apply try using your network on data that represents where it's actually going to be used and make sure it's performing well on that whole set but we see here it's worked well simply by showing it inverted color images it's able to generalize well across data which wasn't even part of the original data set the number one is an inverted image but two three and four are computer generated numbers they are you know printed to the screen they aren't hand written but it's still able to recognize them we trained it on handwriting it can recognize nonhandwriting which is perhaps surprising and it's not something you should rely on we should add into our training examples like these if we want it to perform well we see how see here however that it is performing reliably on these examples it's not only saying that there's a high probability of it being a two it's saying there's a low probability of it being anything else and that's a useful thing to see all the way through it's doing very well 97 is one of the lowest we get 71 there probably because of the low contrast on the number one so we want to add in maybe some more images with you know washed out colors and that's going to help the training the number nine here isn't handwritten or computer generated there's noise around the edge because this is actually a photograph of someone's door number and so it does a bad job it thinks it's a zero because he's getting confused again if we want it to recognize door numbers we need to train it on door numbers once again we see the dog has given us a wrong answer you see the probabilities are better spread out so you might not in this situation say well 23 is the highest and so therefore it's a number eight you might want to say none of them stand more than a certain amount amongst their peers and therefore it's an i don't know that's one of the ways you can say i don't know because there's no one conclusive answer but it's up to you to think about the thresholds and the statistics and the probabilities of these things to make a decision about where that threshold should be put but this hopefully shows you the power of data augmentation the stuff that you can do just by using the same data tweaking it a bit adding it into the mix you suddenly get a network which is way more powerful and way more general so in summary these are the things you need to think about when you're training the data so here we had on our first example we had the number nine the door number it doesn't represent the training data set so if we want it to we need to get some pictures that do we need to add them in if we want to make our network perform better we want to do more data augmentation we may want to rotate images around or flip them or blur them or change the colors or delete parts of them you mess around with them to represent lossy data and hope that the lossy data we apply is a good approximation of the kind of lottiness that exists in reality if you can get a larger base training data set if you can't if you don't if you if you want to do the best job then getting more actual data collecting real data is going to do the best job if you go out and collect more photographs and get more pictures of handwriting that is going to help you a lot if you can't get real data then data augmentation is going to help on top of that if it's taking a while to give you a good network then just train it for longer you can in general keep on training and it will keep on getting better but bear in mind keep an eye on the validation accuracy and the loss to make sure that it's not over fitting against the training data set the more data you have that you're training on the less likely it is to overfit as well so that's also a benefit you get and also there's loads of numbers that we've included in this process the batch size the learning rates the dropouts the kernel size all these things we put in there play around with those numbers make your network larger or smaller see how they affect things understand the effect that they have on your eventual result so the last thing i want to talk about is something which comes up a lot when you talk about machine learning i think in a research context it's very important as well because as researchers we have a sort of responsibility to be honest and good about how we're approaching things now machine learning and your networks in particular do have a bit of this black box problem you put numbers in you do an algorithm you get a number out and it always gives you some kind of answer and it's very easy to end up in a computer says no situation here where you're creating a network to decide whether someone is deserving of getting a loan for example so you put in the description of them to do with their social socioeconomic background and where they live and how what their income is and it gives a number like yes or no a result like this one no and you don't have any recourse there it's just a machine giving you an answer so you need to think about is that a an appropriate thing to do as a society but inevitably machine learning is becoming more important it's taking over everything it's cropping up all over the place and so you need to think about how you can do this stuff in an honest way now google have a set of ai principles and i've grabbed the few of them from here i also wouldn't say that necessarily google follow their own ai principles but it's nice to see them written down somewhere and we can choose to follow them so the first thing i think one really useful for researchers especially those that publicly funded is to be socially beneficial don't design a new network which is going to not give loans to people of color or that's going to make a missile kill someone or anything like this design stuff that is going to be good be good people be nice be helpful avoid creating or reinforcing unfair bias there's a classic example with the um microsoft connect their their xbox controller where they design some machine learning and networks to be able to identify people standing in positions they can dance around and control the machine when they took it on tour to all the game shows people from the public were coming along and having a go and it wasn't seeing them it wasn't seeing their faces and it turned out it was doing a bad job of seeing them because they hadn't trained the networks on people with a diverse set of skin colors they trained it on the people who were the engineers at that particular group in the company and they were mostly white and they'd failed to identify the broad spectrum of people that would be using the technology and that is an unfair bias for sure make sure stuff's designed for safety so have things which fail safely if it's unsure about something er on the side of asking a human and including people in the loop as well make sure you don't fall on the side of i'm not sure so let's not give them a loan and you know subject them to a lifetime of misery or whatever think about how it's going to be affecting the humans in the process be private with things it's easy when you're designing a network to accidentally have the eventual network containing personal information you might have a weight in there which accidentally represents the age of a particular person you've trained it on and there are techniques statistical techniques which you can use to extract that out so be really careful about the data that you're training it on and how you're going to incorporate privacy into that and as many of us are researchers or scientists uphold high standards of scientific excellence this means be honest about how you're training your data set be honest about the testing you've done on it make sure you honestly split your tests and training data sets make sure you've analyzed the results in a thorough and comprehensive way be good citizens and be good scientists is a big summary coming out of this but that's the end of the session today thank you all for your attention i know it's a lot of information in three hours and i really appreciate all the brilliant questions we've had so thank you all very much and i hopefully will see you again soon