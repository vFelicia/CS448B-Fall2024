hello and welcome to this free course in data science and machine learning for beginners made my AI Sciences Academy this easytounderstand course is dedicated for beginners who need to learn the A to Z fundamentals of data science and machine learning through a gradual and segmented approach ok here are the four main parts we will cover during this course we will start with an introduction part two data science and machine learning in this part we will answer some questions that you may ask like what is data science and machine learning why data science now and when I can apply data science and machine learning techniques in learning part 1 we will learn the preliminary to understand data science and machine learning in this part we will learn some vital concepts in data science and machine learning a learning bar we will start the machine learning part where I will explain to you how the machine learning models work we'll also discuss about regression classification and clustering in learning part three we will talk about how to evaluate the performance of the model and choose the best one based on some indicators in learning part four of this course we will discuss about some best practices in data science and machine learning at the end of this course you will receive a gift so please follow this course until the end this gift will help you on your learning journey so ready let's start you data science is not a straightforward easy to define field like most traditional fields it's rather a multidisciplinary field which means that it combines different areas such as computer science mathematics and statistics because data science can be applied and used in various applications and fields it requires domain expertise in each particular area ok for example if we use data science to develop a medical analysis application then we will need an expert in medicine to help define the system and interpret the results data scientists explore the data visualize it and calculate important statistics from it then depending on these steps and the nature of the problem itself they develop a machine learning model to identify the patterns so machine learning and deep learning are the subfields of data science so you might ask what is the difference between data science data analytics and big data well big data means huge volumes of various types of data we differentiate big data by its four V's which are the characteristics that are distinct from ordinary data they are volume velocity variety and veracity the sheer volume is the main characteristic that makes data big velocity is the frequency of incoming data that needs to be processed variety means different forms of data veracity refers to the trustworthiness of the data on the other hand data analytics is more about extracting information from the data by calculating statistical measures and visualizing the relationship between the different variables and how they are used to solve the problem it's like descriptive statistics why data science now that's an interesting question first there is currently plenty of data more than at any time before in history and it just keeps growing exponentially second now we have much better computers and computational power than ever before a task that can be finished in a few seconds nowadays would have required days with the computers that existed just a few years ago and finally we have more advanced algorithms for pattern recognition and machine learning than we did just a few years ago so in one sentence if you want to know why data science has become our focus right now is because we have a lot more data better algorithms and better hardware let us see where data science and machine learning is applied I want to say that data science is everywhere the number of data science applications is countless it's because we have data everywhere and there are dozens of algorithms developed each year to solve these tasks however we will talk about a few famous use cases of machine learning in data science in our daily lives as you can see machine learning and data science are currently being used in many fields such as healthcare finance transport social media ecommerce and virtual assistant apps among others in healthcare machine learning is currently used in disease diagnosis it provides higher accuracy compared to professional physicians it is also undergoing extensive research and drug discovery another application is robotic surgery where we have an AI robot helping and performing the surgery with a precision that is actually higher than that of the best surgeons in transport Tesla used machine learning algorithms in its selfdriving cars machine learning is also used for air traffic control in finance I worked for two years as the KPMG consultant in one of the French banks to develop different machine learning algorithms predicting customer defaults and bank capital requirements many banks are currently using machine learning powered software for fraud detection also banks are using machine learning for algorithmic trading in social media I think all social media platforms today use machine learning for both spam filtering and sentiment analysis Facebook also uses machine learning image recognition in ecommerce many online shopping websites such as Amazon eBay and udemy use machine learning for customer support targeted advertising and product recommendation machine learning is also used in virtual assistant apps many startups are founded based on the idea of developing a machine learning powered assistant in one particular field this assistant can be a chatbot for example which can intelligently reply and answer nearly any inquiries in that field these are just a few broad in general applications of data science and machine learning you can develop your own application in any field that you find interesting and have some experiences and by the end of this course you will be equipped with the knowledge necessary to create an app in the area of your choice let's talk just a little bit about the history of data science the term data science has been appearing in various contexts over the past thirty years but did not become an established term until relatively recently in its early usage it was used as a substitute for computer science since 1960 when Peter now are first mentioned the term how about the future of data science and data scientists we can clearly see that the future of data science is very bright another evidence for that is the cloud services that have appeared in the last two or three years being extremely cheap and fast they can help develop more advanced machine learning applications in all fields so it will now be surprising to see many tasks that we considered science fiction such as assistant robots and selfdriving cars already in use in our daily lives how about the future of data scientists let me share some statistics with you as you can see in the figure on your screen at the end of 2016 the percentage of jobs for data scientists was 474 percent larger than those for statisticians today in the USA the work of a data scientist is one of the high Penguin's learning data science maybe the best decision you've ever made in our learning company ai sciences is here to help you achieve this goal now how can you get the ultimate benefit from this course I highly recommend you take this course very seriously and follow it step by step we highly suggest that you go through the study materials that you used in your high school undergraduate and graduate programs and revised topics such as linear algebra calculus and statistics we will not go too deep into the math behind algorithms in this course but we will cover basic ideas logic and in some cases formulas to understand our main topic of interest better also try to finish every single project provided in the course on your own and then check the solution finally we encourage you to go through any further reading material that you will find it will give you an overview of what you can learn next after finishing the course okay in this lesson we will explore in detail some important terms in data science you ready let's go in the following lesson we will talk more about data and variables so what is the data data are basically collections of facts measurements observations numbers words etc that have been transformed into a form that can be processed by computers data are stored in columns and rows the convention is that each row represents one observation case or example and each column represents one feature or variable got it we also have two types of variables based on their value numerical and categorical variable if the value is a number and we can compute the mean for this variable we call it a numerical variable but if the but if the value is a factor or label and we can't compute its mean we have a categorical variable so we can also talk about dependent and independent variables a dependent variable is one that we need to predict and the independent variables or X variables will help us to predict a y variable it is essential to know that X variables need to be independent of each other and that is why we call them independent variables or predictors couple other important terms in data science are population and sample and data science the whole population is our target but due to a lack of resources a data scientist can't work in the entire population because of that we have to choose a representative sample of the data from the population the goal of machine learning algorithms is to find parameters that can do the mapping on the whole population based on the given sample as you can see we have our population and we will choose a sample by using the sampling technique now let's talk about outlier and missing data in data science an outlier is a data point that differs from other observations an outlier may occur due to variability in the measurement or it may indicate an experimental error outliers can alter the performance of many machine learning algorithms as we can see there in the image we can detect outliers by visualizing the data in this example employee number two and nineteen are outliers based on their profile how to deal with outliers we can drop them altogether cap them with the threshold assign new values based on the mean of the data set for example or apply a transformation on the data set itself okay one way to handle missing data is by dropping the observations another way to handle them is to use data imputation techniques so if we have a numeric feature we can replace the missing value with a mean median or mode or we can select random observations from the data set and replace its feature value in the observation that has missing values the last imputation technique can be performed by regressing the missing feature on the other features and making a prediction of the missing value if we have a categorical feature we can use the mode replacing or also we can use the KNN model to predict the feature that has a missing value if you still don't know much about the regression and KNN model that worries David will present them to you in the machine learning section that's it for this lesson in this class we will learn about the link between AI machine learning and deep learning dl you'll find out how a machine learns and the types of learning there are ready let's go first here's the link between artificial intelligence machine learning and deep learning let's clear the confusion between these three essential terms so what are they I ml and DL by looking at the image it is clear that ml is a subfield of AI and DL is a subfield of ML machine learning algorithms were developed with the goal of finding a useful prediction function among machine learning algorithms let's mention the artificial neural network an artificial neural network consists of a collection of neurons connected to each other in a specific way however the use of neural network was limited because of the lack of computational power and the lack of proper optimization algorithms for neural networks this is where deep learning came in it is a simpler algorithm that uses more neurons and layers to perform a lot of learning tasks like image recognition and natural language processing today deep learning is used in a lot of areas such as the hightech industry Tesla selfdriving cars and the Seattle Amazon store are constructed based on the deep learning algorithms combined with computer vision now let's see how a machine learning algorithm learns okay Before we jump into how a machine learns let us first try to understand how a human baby learns think for example of a oneyearold human baby a baby does not know the difference between an apple and an orange for him all fruit is the same orange Apple bananas cucumbers in his first phase of learning called phase one of learning in the figure he builds an intuition that oranges and apples are of one shape and bananas and cucumbers are of another shape once a baby is comfortable with the shapes of fruit he goes into learning phase two phase two of learning in the figure by introducing another property like color now he knows that a fruit that is round in shape and red in color means that it's an apple in a round shape and an orange color means it's an orange in the face 3 the baby will gather a lot of data as shown in the table with these two properties and based on these data in the future he will know the difference between fruits machine learning models learn the same way in machine learning the properties of the fruit such as shape and color are called the features the fruit type is called the label each instant of an input/output pair is called an observation so depending on the features and labels enter to a machine learning algorithm learning is classified into three main categories supervised learning unsupervised learning and reinforcement learning we also have semisupervised and instant based learning but in this lesson we're just going to focus on the main three learning types in supervised learning we train with labeled observations that means that for each observation of training data the input and output are known as you can see in the image we try to predict the output from the input by training our machine learning model classification is one example of supervised learning where the goal is to classify objects regression is another example where we try to understand the relationships among variables in a glance in supervised learning we have the Y and the X variables and we want to make the prediction of why okay now in unsupervised learning the trainer does not provide a labeled output in the learning data set the machine learning algorithm learns from unlabeled data and gathers information from it as you can see it's short in unsupervised learning we only have X variables and we need to gather the observations and groups based on the information given by X unsupervised learning is used mainly for clustering tasks where we organize the observation into clusters in reinforcement learning an agent learns by interacting with the environment so in this type of learning the agent performs an action in the environment this action takes the environment to a new state and gives a reward to the agent the reward can be negative or positive for multiple iterations and the rewards the agent learns based on his past experiences reinforcement learning is mainly used in skill acquisition tasks such as robot navigation or games okay to sum up we have three main types of learning supervised when we have X and y variables unsupervised when we only have X variables and reinforcement learning in this case the algorithm learns by rewards that it gets as a result of its action in an evolving environment now you know how a machine learns and what are the different types of learning I guess now you know a lot more about basic data science and machine learning terms in this lesson let me explain some important modeling terms to you alright you ready let's start in machine learning we always split our data into two so we have a training data set and a test data set the training data will help us to train our model and the test data set will help us evaluate our model performance and accuracy a typical machine learning model learns from the training data set and applies the learning to the test data set so if the model is able to make correct predictions on the test data set then the model is able to generalize the learning to any new data and then we can say we have a good model now what is an over fitted model when we obtain a model that works very well on the training data set but is not able to generalize to the test data sets we call such a model an over fitted model in this case the testing error is large because the model is very complex the model is under fitted when the training error is large because the model is too simple and just can't capture the true complexity of the data so you may wonder can we control these issues the answer is yes the solution for under fitting is either to increase the size of the data set or to increase the complexity of the model but the solution for overfitting is a bit trickier the first solution is to gather more data this solution is not always feasible the second solution is to use penalty terms in the model and that is called the regularization technique and the last one is to make crossvalidation okay in short you need to understand that our goal as data scientists is to come up with a model that is not too generalized and not too focused on training data this model is called the best fit model this is basically a tradeoff between an under fit and an over fit model like the middle image as you're already familiar with overfitting and underfitting the concept of bias and variance will be very easy to digest but before we start talking about bias and variance let us classify the type of models errors first we have the irreducible error which comes from the nature of the data itself for example the noise when you talk through your mobile phone the second kind of error is the reducible error we have to reducible errors the bias error and the variance error okay the bias error is the difference between the average prediction of our model and the correct value which we are trying to predict the bias error is high if the model is oversimplified the variance error is the variability of model predictions for the given data the variance error is high if the model is not generalizing well on new data okay look at the figure at the right the blue points represent how far we are from the minimum error which is represented by the small red circle in the case of low bias the blue points are not very far from the minimum error in the case of low variance the blue points are near each other of course we want our model outputs to be as close as possible to the minimum error which means low bias and low variance but it is impossible to have both low bias and low variance there is a tradeoff between bias and variance because as we decrease the model bias we make it more complex and we increase its variance and when we decrease the variance we increase the bias looking back to overfitting and underfitting we can say that when the model is under fitted it has low variance and high bias when the model is over fitted then it has high variance and low bias as you can see in machine learning it is important to make a biasvariance tradeoff and to choose a middle model in data science we have different types and qualities of data and we need to make them usable in our model number one feature extraction and feature engineering will help us transform raw data into features suitable for modeling and to feature selection will help us remove unnecessary features during the data processing step in this part of the course we will learn various machine learning models which are commonly used for prediction classification clustering etc as I already mentioned machine learning is broadly classified into supervised and unsupervised learning supervised learning means that the algorithms are supervised during the training phase so in other words in order to train these algorithms we need data that have labelled targets for example if a model is being created for predicting house prices then the historical data that is used to train the model should have a target column stating the price of a house unsupervised learning is when we don't have a target labeled data in this case the algorithm classifies objects based on some existing features supervised machine learning problems have two categories of learning regression and classification regression algorithms are used to identify a relationship between a dependent variable target and independent variables predictor / features so in regression the target is always a continuous variable and the predictors can be continuous or discrete in nature regression is best used for finding causal effect relationships between the variables forecasting time series modeling etc in regression analysis the model tries to fit a curve to the data points in such a manner that the difference between the data point and the curve is at a minimum on the other side we have classification models the classification models are used to predict the target that has discrete values for so for example class of fruits orange pineapple and lime or predicting whether a patient is suffering from cancer or not for this kind of use machine learning models collect insights from the historical labeled data and use these insights to predict the target class okay listen up don't forget this regression and classifications are all supervised learning models because we have a labeled output which we need to predict if this labeled output is continuous we use regression and if it's categorical we use classification now the most used unsupervised learning models are clustering and Association analysis clustering is the most important unsupervised learning model it deals with finding a structure in a collection of unlabeled data so a loose definition of clustering could be the process of organizing objects into groups whose members are similar in some way I will present clustering in more detail later Association analysis models discover relationships in large datasets hidden data relationships will be expressed as a collection of Association rules and frequent itemsets with Association analysis Association analysis isn't frequently used we will not focus on them in this course what are the machine learning models associated with each of these learning types I mean which models are dedicated to regression classification and clustering for the regression purpose we commonly use linear regression decision trees for regression support vector machines for regression SVR or neural networks for the classification purpose we use logistic regression decision trees for classification support vector machines classifier SVC nearest neighbor or neural networks in unsupervised learning we use the kmeans clustering so before we discuss these models in detail I need to explain the difference between two terms which can be confusing when you get started in machine learning the terms model parameter and model hyper parameter I'll talk more about these two terms in a few minutes so I think it will be better to explain the difference between these two terms now a model parameter is a configuration variable that is internal to the model and whose value can be estimated from data it's required by the model when making predictions and it is estimated or learned from data model parameters are key to machine learning algorithms they are the part of the model that is learned from historical training data so some examples of model parameters include the weights in an artificial neural network the coefficients in a linear regression or logistic regression on the other side a model hyper parameter is a configuration that is external to the model and whose value can't be estimated from data they are often only help estimate the model parameters and they are often specified by the practitioner linear regression is one of the most used supervised machine learning algorithms and now I'm gonna explain what it is exactly how it works and show you the logic behind it first of all what is linear regression let's say that two years ago I made $10,000 on my job and that last year I earned 20,000 with that in mind what do you think how much would I make this year if you answered $30,000 you just applied linear regression we make predictions all the time and most of them follow the logic of linear regression linear regression is a machine learning model which was designed to help you to specify a linear relationship to predict the numerical value of a dependent variable we will call it Y in this course for a given value of independent variables we will call them X by using a straight line called the regression line does that sound complicated it's not I'll show you we can say that linear regression will help us make a prediction based on some information prediction equals dependent variable Y some information equals independent variables X we use linear regression to answer the following questions is there a linear relationship between the two variables x and y which X variable contributes the most let's write this model as an equation it goes like this y equals B 0 plus B X plus e b0 is the value of y even if the value of x is 0 it's called the intercept B is a coefficient associated to X it will be a vector of coefficients e for error denotes all remaining information about why that hasn't been explained by the X variables of course the linear model is not perfect and it will not predict all the data accurately okay we have two types of linear regression the simple linear regression and the multiple linear regression in simple linear regression we use a single independent variable to predict the value of a dependent variable the model in this case will be y equals B 0 plus B 1 X 1 plus e a multiple linear regression we use two or more independent variables to predict the value of a dependent variable the difference between the two is the number of independent variables X y equals B 0 plus B 1 X 1 plus B 2 X 2 plus dot dot dot plus E to keep this lesson simple and to help you understand the rest of the course right now we will focus on the simple linear regression only the same thing will be replicated in multiple linear regression the only thing that will change is the number of X variables here are some examples of simple linear regression and multiple linear problems the estimation of the average student score based on the number of hours they have spent studying 30 hours please note that all the students who pass the exam will receive at least two points for their presence in this problem the dependent variable Y will be the average student score the independent variable is the hours of study and it equals 30 the intercept equals B 0 equals 1 the regression model we want for the score prediction is score equals B 0 plus B 1 times our study score equals B 0 plus B 1 times 30 in this model the b0 and b1 are coefficients these coefficients are what we need in order to make predictions about our score if we add the number of exercises that the student has completed to the model it will become a multiple linear regression model score equals b0 plus b1 times our study plus b2 times exercises and B plus for the prediction of the score we need to estimate three parameters B 0 B 1 and B 2 B 0 B 1 and B 2 parameters need to be estimated based on our historical data to estimate the linear regression coefficient we need to minimize the least squares or the sum of individual squared errors in other words that's the difference between the actual value and the prediction the error of the individual I is easily calculated as the difference between the real value of y I equals y hat i AI equals y I minus y hat I we square the error for two reasons one the prediction can be either above or below the true value resulting in a negative or positive difference respectively if we did not square the errors the sum of errors could decrease because of negative differences and not because the model is a good fit to squaring the errors penalize as large differences and so the minimizing the squared errors guarantees a better model let's look at a graph to understand it better in the graph the green dots represent the true data and the yellow line is a linear model the dotted red lines illustrate the errors between the predicted and the true values in practice we use the OLS algorithm it's ordinary least squares eater ative Li in each iteration the algorithm calculates the sum of the individual squared errors and in the next iteration the algorithm updates model parameters to shift the line from the previous position to reduce the squared error finally the best OLS estimators of the coefficients are in these equations X bar and y bar represent the mean now how to make a prediction we will use the existing data to estimate the values of B 0 B 1 through B K we can do that in Excel our Python etc after that we can make all the predictions we want please note that linear regression is considered to be one of the most straightforward machine learning techniques and an easy model to interpret but it has its disadvantages it will only work if the relationship between the dependent and independent variables is linear that's it now you know exactly what linear regression is and how it works now present the decision trees model a decision tree is like a series of ifelse conditions that lead to a decision it can be used for classification or regression use cases it works for both categorical and continuous input and output variables decision tree breaks down a data set into smaller and smaller subsets while at the same time an Associated decision tree is incrementally developed the final result is a tree with decision nodes so consider a scenario where we want to decide whether a loan should be approved for an applicant or not to decide we will ask the applicant a series of questions we might start off with whether the applicant has any other existing loans if the answer is yes then the next question might be whether he is a defaulter or not with this kind of series of questions we can narrow down the search and make a robust decision we build the model above by hand to make a decision about whether a loan application should be approved or not alternatively a machine learning model could perform supervised learning using the data set to arrive at a decision the decision tree model in machine learning can learn these decisions or conditions from the data set quickly and build the model okay decision trees have three types of nodes a decision node has two or more branches a leaf node represents a classification or decision we also have the topmost decision node in a tree which corresponds to the best predictor called root node how do decision trees work in practice well that's an interesting question in practice there are three important steps in the building of a decision tree the first one is splitting a decision tree can have a subbranch or a subtree as well and the process of creating a sub branch is called splitting have a look at the figure to visualize these notations so the root node has the full data set and at each decision though the test is conducted to split the data set decision nodes are executed to split the data until it reaches the leaf node a leaf node contains a single target value a single class or a single regression value a leaf node that contains only one target value is considered pure the second one is pruning or shortening of branches of the tree decision trees are prone to overfitting if the parameters are set to favor all pure leaf nodes which means that all data points in the training data set are correctly classified so to reduce overfitting the following strategies are commonly followed one pre pruning stopping the creation of the tree and this is achieved by limiting the maximum depth of the tree is so limiting the maximum number of leaf nodes or defining the minimum number of points required to split it further to post pruning just trimming the nodes that contain less information as you can see a pruned tree has less nodes and has less sparsity than a nun pruned decision tree then the last one is the tree selection in this step the models are looking for the smallest tree that fits the data usually this is the tree that yields the lowest cross validated error decision tree models work really well if the training data set is in a binary format but this is not a limitation for continuous features the decision condition can be applied in the form of greater than or less than a certain threshold for example X is greater than 0.7 a prediction on a new data point is made by traversing through the decision tree and checking at each decision node whether the condition is met or not one of the key factors in the decision tree is entropy so let me explain to you why it's important to carefully consider which feature will be used to split each node because decision trees with a different split node may result in different predictions and accuracy we can utilize a statistical method to identify the feature that should be selected as the root node the feature that has the most information gain should be selected as the root node so information gain measures how well a certain feature distinguishes among different target classifications information gain is measured in terms of the expected reduction in the entropy or impurity of the data the entropy of a set of probabilities is H of P in the formula where P is the probability of outcome event so if the sample is completely homogeneous the entropy is 0 and if the sample is an equally divided one it has an entropy of 1 to understand how entropy and information gain is calculated let's take a look at the following example okay a training data set has 500 observations of these 500 observations 300 are of positive class and the remaining 200 are of negative class positive class ratio equals 300 divided by 500 equals zero point 6 negative class ratio equals 200 divided by 500 equals 0.4 entropy of the target variable will be entropy equals minus 0.6 times log to 0.6 plus 0.4 times log to 0.4 equals 0.9 702 a feature X in the dataset is split as X is greater than 347 120 positive and 80 negative X is less than or equal to 347 240 positive and 60 negative entropy of X is greater than three hundred and forty seven equals E 1 equals negative 120 divided by 200 log two 120 divided by 200 minus 80 divided by 200 log two eighty divided by two hundred and ruvi of X less than or equal to three hundred and fortyseven equals e 2 equals negative 240 divided by 300 log two 240 divided by 300 minus 60 divided by 300 log 260 divided by 300 entropy of x equals two hundred divided by 500 times entropy one plus 300 divided by 500 times entropy two information gained for feature X equals entropy total minus entropy of X whichever feature in the note has the maximum information gain will be selected for the split we can also use other indicators like the misc classification rate the Gini index and Inter of Dakota miser 393 for calculating the information gain of a feature what are the hyper parameters of this model free routing parameters are the main hyper parameters of a decision tree model for example the maximum depth of the tree the maximum number of leaf nodes and minimum number of data points required to split the node further a combination of these hyper parameters can be used to build the decision tree which is generalized over the data set and provides good accuracy what are the advantages and the disadvantages of a decision tree decision trees are computationally cheap to use easy for humans to understand results and it can deal with irrelevant features its disadvantages are it is prone to overfitting and provides poor generalization performance sometimes it gives low prediction accuracy training in post pruning strategies are implemented in decision trees to reduce overfitting but still decision tree models tend to overfit so to overcome this data scientists use an advanced modeling technique called ensemble the idea of ensemble is to build many trees all of which predict well and overfit in their own way and average the results to reduce overfitting ensemble means assembling many machine learning models to create a more powerful and robust model these machine learning models also known as base estimators or base learner because they are combined together the most popular ensemble techniques are bagging and random forest bagging combines sampling techniques and aggregation to form an ensemble model and practice multiple samples are chosen randomly with replacement within the training data set let me explain how the sampling is made so suppose that a sample of 10 observations is drawn from a training data set of 100 observations these observations are then returned to the training data set before another sample is drawn so the next sample of 10 observations is drawn from a training data set of 100 observations in simple terms at any point all of the training data set observations will be available for a sample to be drawn this sampling technique is called the bootstrap for each of these samples a decision tree or other bass learners is created finally these decision trees or based learners are aggregated to achieve an efficient predictor typically the combined estimator is better than any one of the single decision trees question how does the algorithm choose the output at the end of this aggregation okay the output is chosen based on voting for classification or on an averaging for regression now how about the random forest model so you learned how bagging works random forests also uses the same bagging technique with a slight modification during bagging all features of the training data set are used on sampled data to create the decision trees or the base estimators because of this sampling techniques used in bagging the datasets in each sample are quite similar HBase estimator usually breaks at the same feature this results in quite similar base estimators this means that weak features will not be incorporated to avoid this in random forest model the samples are created with a subset of features selected randomly for each node in the decision tree this selection of a subset of features is repeated separately in each node so that each node in a tree can make a decision using a different subset of features this process of randomly selecting sampled data and a number of features for a split at each node ensures that all decision trees in the random forest are different similar to the decision tree the random forest also provides feature importance which is computed by aggregating feature importance over the trees in the forest typically the feature importance provided by random forests is more reliable than the one provided by a single tree the maximum number of features to split at each node determines how random each tree is and a smaller value reduces overfitting as a rule of thumb this parameter can be set to the square root of a number of features for classification and for regression use cases Criterion Gini entropy number of decision trees maximum number of features maximum depth of the decision tree minimum number of samples required at each leaf node minimum number of samples required to split a node and maximum number of leaf nodes in each decision tree are all hyper parameters of ensemble models this hyper parameter can be tuned to get a generalized and accurate machine learning model benefits of random forest okay what I can say is that ensemble models are very powerful and often work without parameter tuning but the bad news is it is difficult to understand thousands of trees and explain the decisionmaking process that is why this model is considered like a black box another disadvantage is that ensemble methods need more computing resources and take more time to learn from data ok they're machine learning model is boosting boosting is another model that makes a bass estimator like decision tree more powerful by making a sequential execution and each subsequent estimator focuses on the weakness of the previous estimator boosting incrementally builds an ensemble by training each model with the same data set but where the model coefficients of estimators are adjusted according to the error of the last prediction several weak models team up to produce a powerful ensemble model the main idea of boosting is to focus on the observations that are hard to predict boosting can reduce bias without incurring higher variance here are the popular boosting algorithms Aida boost and gradient boosting let me explain to you how they work Aida boost is adaptive boosting where more attention is given to the records that are not correctly predicted after each iteration weights of the wrongly predicted observations are increased so that these records will be picked up more in the next iteration to gain better accuracy gradient boosting is another popular boosting algorithm it works by sequentially adding the previous predictors under fitted predictions to the ensemble ensuring errors made previously are corrected here's what you need to know boosting does not introduce randomness to the decision trees or any based learners however it uses strong techniques to build accurate predictors in most cases the maximum depth for boosting models has kept a 5 models this makes the model faster and the model consumes less memory the hyper parameters for these models are the number of decision trees maximum depth and learning rate a lowered learning rate means more trees are required and a higher learning rate means less trees are required to build a model these hyper parameters should be tuned to get an optimized machine learning model boosting models are more sensitive to hyper parameters but once the hyper parameters are tuned properly these models provide very good accuracy and generalization so in general ensemble models are very powerful and widely used however like the bagging models it is difficult to understand thousands of trees and explain the decisionmaking process I just don't recommend it for dimensional sparse data they don't do very well in high dimensional data well the SVM's model or support vector machines support vector machines are one of the supervised learning algorithms mostly used for classification tasks however SVM algorithms can be used for regression as well a support vector machine for classification is called the support vector classifier SVC and for regression it's called the support vector regressor or SVR svms are based on the idea of finding a hyperplane that best divides a data set into two classes as shown in the image below you may ask what the hyperplane is okay take this example say we want to classify a task with only two features you can think of a hyperplane as a line that linearly separates and classifies our data into intuitively the further from the hyperplane our of data points lie the more confident we are that they have been correctly classified we therefore want our data points to be as far away from the hyperplane as possible while still being on the correct side of it so when new testing data is added whatever side of the hyperplane it lands on will decide the class that we will assign to it now how do we find the right hyperplane or in other words how do we best segregate the two classes within the data before we answer this question we need to understand what is a margin a margin is equal to the distance between the hyperplane and the nearest data point from either set now the goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set and giving a greater chance of new data being classified correctly as you can see in two dimensions it is very easy to classify using SVM but sometimes it's harder to identify clearly the hyperplane because the data is rarely ever as clean as our simple example above a data set will often look more like the jumbled balls which represent a linearly nonseparable data set look at this case in order to classify a data set like this one it's necessary to move away from a two dimension view of the data to a three dimension view explaining this is easiest with another simplified example imagine that our two sets of colored balls above are sitting on a sheet and this sheet is lifted suddenly launching the balls into the air while the balls are up in the air you use the sheet to separate them this lifting of the balls represents the mapping of data into a higher dimension this is also known as kernel okay because we are now in three dimensions our hyperplane can no longer be a line it must now be a plane as shown in the example the idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it so that's how SVM's work and produce the output so what are the benefits and the disadvantages of SVM's SVM's can give a great accuracy and can work well on smaller cleaner datasets nevertheless if you have a larger data set it isn't suited as the training time can be very high another machine learning algorithm is the K nearest neighbors K and n the K nearest neighbors knn algorithm belongs to the family of instancebased competitive learning and lazy learning algorithms it's a lazy learning algorithm because the calculation is delayed until a prediction is required it is called the localized model because only the data points that are near new data points are used for model calculation and for predicting classes of new data points so let me explain how this works what a prediction is required for an unseen data point the knn algorithm will search through the training data set for the k most similar neighbor the prediction attribute of the most similar data points is summarized and returned as the prediction for the unseen instance so in this model K is the number of neighbors we want to check to classify a new data point if K is greater than one the model uses voting to classify the new data point in short the class that is in the majority is assigned to the new data point for example okay so if the value of K is set to three the model will check the three nearest data points to classify the new data point the default value of K is one which means that the vanilla KNN model classifies the new data point according to the class of the nearest neighbor the yellow and purple circles represent the data points from the training data set and we want to predict the class for the red data point if the value of K is set to three then the model will check the three nearest data points inner circle and then classify the red data point if the value of K is set to six then the model will check for the nearest six data points outer circle and then classify the red data point in the figure above the red point will be classified as follows if K equals three Class B two votes for Class B and one vote for Class A if K equals six Class A two votes for Class B and four votes for Class A okay in a multiclass data set we count how many data points belong to each class and the class that is in the majority is predicted for the new data point so how does the algorithm choose the nearest points the KNN uses the distance to evaluate which point is near to the new data for continuous features Euclidean distance is calculated and for categorical features another distance called hamming distance is calculated the important hyper parameter for KN n is the number of neighbors it holds the value of K the default value of this parameter is 5 knearest neighbor has a lot of benefits among them its simplicity and flexibility it also works well with enough representative data the problem that we can face sometimes is that the can and algorithm is space consuming because for each prediction the calculation is done separately okay first of all logistic regression is among the most commonly used and best known algorithms that we can use to solve a classification problem it's named the logistic regression because of the logit function which is used in this method of classification other than that logistic regression is pretty much the same as linear regression the purpose of logistic regression is to detect a relationship between features and find the probability of a particular outcome in a way it extends the idea of linear regression to a situation where the outcome variable is categorical for example let's try to predict whether a student will pass or fail an exam the number of hours spent studying is given as a feature and the response variable has two values passed and failed okay in the equation below you can see that we need to predict the Y variable which can take two values 0 or 1 0 for failed and one for passed it turns out that it is virtually impossible to predict Y with the following model y equals B 0 plus B 1 X 1 plus dot dot B K X K that's because Y is a categorical value and B 0 plus B 1 X 1 plus dot B K XK will give a continual value as the result therefore instead of predicting this categorical variable we're going to predict the probability of the realization of y equals 1 B equals probability of y equals 1 in order to do that we need a link function the logit link function okay a link function is basically a function of the mean of the response variable Y that we use as the response instead of Y itself it means that when Y is categorical we use the logit of Y as the response in our regression equation instead of just Y the logit function is the natural log of the odds that y equals one of the categories for mathematical simplicity we're going to assume Y has only two categories encode them as 0 and 1 P is the probability that y equals 1 so for instance those X's could be specific hours spent studying number of completed exercises and the score in the first exams while P would be the probability that a student would pass an exam as in our first example linear regression versus logistic regression instead of linear regression the line between y and X the relationship between X and the probability P is a logistic distribution how to estimate the logistic regression coefficients at this point we don't know the coefficients B 0 B 1 B K of the model so we must estimate them in order to make predictions unlike the linear regression model logistic regression uses ordinary least square for parameter estimation the estimation is done by using maximum likelihood due to its more general nature and statistical features there can be an infinite set of regression coefficients the maximum of the log likelihood estimate is that set of regression coefficients for which the probability of getting the data we have observed is maximum in other terms we must make estimates for the coefficients that predictions are as close as possible to the originally observed value so how to make a prediction well the prediction is made using the original logistic function and the estimated coefficients from the maximum likelihood function with the observed data to compute the estimated probability of P if the probability of P is below 0.5 0 the predicted value of y is 0 otherwise it will be 1 in our example the student will fail based on the value of x in the figure we make a prediction by using a different set of X variables the first set gives us a red point with P equals 0.2 9 in the first case the value of y is equal to 0 as the predicted value of P is less than 0.5 0 in the second case the green point P equals zero point 9 0 the predicted value of y will be 1 compared to other models logistic regression is rather simple and efficient however it can't handle a large number of categorical variables successfully that's it now you know exactly what logistic regression is and how it works in our next tutorial you'll learn how to apply it in Python and our click on the video above to see this tutorial thanks for your time and hey if you like our videos please like it share it and subscribe to our Channel oh and click on the notification button so you can receive notifications for our next course enjoy machine learning first though let's see what a neuron is a neuron in the neural networks field is something that takes some input applies some logic and outputs the result we call it a function for example if we have F X equals y X is the input and Y is the output and F is a function to illustrate so let's say I'm trying to understand the relationship between the length of the video we produce on our channel and the time that people actually spend watching the video we collect data from some of our videos I mean we have the video duration let's call it X and the watching time let's call it Y and we imagine there is some relationship between them denoted by F after that I inform the Machine about the relationship I expect to see between these two variables I can choose a linear function between X and Y or a nonlinear function this function is what we call a neuron then we can predict the time people would spend watching a video lesson precisely based on our neuron and the video duration now let's see what a neural network is well in one sentence a neural network is a network of neurons it means that we have many neurons and all their inputs and outputs are intertwined and they feed each other in this figure you can see the difference between a neuron and a neural network as you can see a neuron is a basic unit of learning and a neural network is a bunch of interconnected neurons neural networks help us cluster and classify they helped a group data according to similarities among the example inputs and they classify data when they have the output variable in the existing data set to learn from it the questions you may ask at this point will probably be question 1 what kind of problems do neural networks solve neural networks could be applied for spam filtering fraud detection customer relationship management angry customers or happy customers image recognition selfdriving etc question two which functions will I use in each neuron we can use linear or nonlinear depending on the complexity of the problem question 3 what is the architecture of the network we have different types of neural networks a perceptron a recurrent neural network or RNN a convolutional neural network CNN etc now how we can run our neural network in the first place the neural network learn to recognize patterns just like a human we show them examples of correct inputs and outputs in the hope that when we give it a new example input that it's never seen before it will know how to give the correct output that's what we call training on existing data sets don't forget machine learning equals learning from examples let me present you the most basic neural network the perceptron and discuss how it processes inputs and produces an output so suppose we use our neural network for a froot image recognition I have two inputs for that purpose the color and the shape of some fruits and our data sets and a single binary output which is the fruit name once the machine has learned all these properties I can give it a new image of a fruit when it hasn't seen before and it will hopefully classify it correctly and be able to tell me whether it is an orange or a banana the perceptron learns from the existing data and knows which information will be most important in decision making to decide between multiple information it uses something called weights the weights are just numerical representations of these preferences a higher weight means our perceptron considers that input more important compared to other inputs so for our example let's deliberately set suitable weights for our two inputs two for the fruit shape and four for the fruit color now how does the perceptron calculate the output it simply multiplies the input with its respective weight and sums up all the values it gets from all the inputs let's consider that we have two shaped round and long if the shape is round the input one value is one and if it's not round the value is zero we'll repeat the same thing with the color red takes the value of one and the yellow color the value of zero based on this information if the fruit is round in red our perceptron would do the following calculation total equals round times shape weight plus red times color weight so total equals one times two plus one times four equals six this calculation is known as a linear combination now let's see what this value 6 means we first needed to find the threshold value because the perceptrons output is either 0 or 1 0 for a banana and 1 for an orange this output is determined like this if the value of the linear combination is higher than the threshold value then the output is 1 and if it is not the output is 0 so let's say the threshold value is 3 which means that if the calculation gives you a number less than 3 we have a banana but if it's equal to or more than 3 then we have an orange that's how perceptron works it uses a linear combination and produces the output in reality we set the weights to random values and then the network adjusts those weights based on the output errors it made using the previous weights that is called training the neural network in the mathematical language the perceptron algorithms work like this the output is equal to zero if the sum of the weight times the value of the variable is smaller than a threshold in the same way the output will be equal to one if the sum of the weight times the value of the variables is bigger than a threshold to make things just a little simpler for training the threshold is sometimes move to the other side of the inequality and replaced with what's known as the neurons bias now with bias we only need to make changes to the left side of the equation while the right side can remain constant at zero the left side of the equation is a function it is a function that transforms the values or states the conditions for the decision of the output neuron it is known as an activation function the formula above is just one of several activation functions and and the simplest one used in deep learning and it is called the Heaviside step function in reality we can also use other activation functions for example we can use the sigmoid function the tan function and the softmax functions each of them has a purpose and we'll present each of these functions in a special video dedicated just to that that's it for our first introductory video on neural network now you have a solid understanding of the basics of neural networks and how perception works in our next video in neural networks we will explain how they multi layers perceptron works and we will present the concept of hidden layers okay let's present an unsupervised machine learning model the clustering in general and the kmeans clustering in particular clustering models are used to analyze unlabeled data and get useful insights from it clustering is equal to grouping things means the data points which are similar in some way and different from other data points are grouped together each group of the data points is known as a cluster in the example data points are divided into four clusters based on the geometrical distance between two data points the data points which are close to each other are assigned to one cluster this approach of creating clusters is known as distance based clustering another approach of creating clusters is the conceptual clustering in this approach clusters are based on conceptual similarity so for example in a data set of oranges and apples two clusters will be created one for all apples and another one for all oranges this approach uses properties of data points to distinguish one data point from another and to group data points with similar properties together clustering is a very subjective area of discussion it is difficult to determine how one clustering is better than the other one machine learning algorithms learn the insights from the data and apply these insights to create clusters since we don't know those insights it is hard to say the clustering done by the machine learning model is best or not so the user must provide an appropriate criterion to get suitable clusters as an output from the machine learning model consider a data set that contains green color balls red color balls green color apples and red color apples now if we provide color as the clustering criteria to the machine learning model then it will create two clusters one of the green color balls and green color apples and the second one of course of red color balls and red color apples however if the criteria provided is edible then the machine learning model will create two clusters one of all apples and another one of all balls clustering algorithms can be applied in many industries marketing like finding groups of customers with similar behavior or customer segmentation biology for plants classification insurance fraud detection city planning earthquake studies document classification identifying crime localities cyber profiling criminals etc now let me explain to you how K means clustering works kmeans clustering is one of the most useful clustering algorithms it tries to find cluster centers that represent certain regions of the data kmeans clustering is a twostep process one assign each data point to the closest data center to set the cluster center as the mean of the data points assigned to the cluster the algorithm keeps on executing these two steps until the assignment of data points two clusters no longer changes once we decide how many clusters are required the number of clusters is passed to the algorithm so let's say for a given data set we want three clusters in the initiation step the algorithm randomly selects three cluster centers each data point is assigned to the cluster Center it is closest to we then update the cluster Center with the mean of the cluster the previous two steps are repeated until the assignment of data points two clusters no longer changes clustering is similar to classification the only difference is that classification use cases have labeled data I mean the classes are defined and clustering creates various clusters or classes from the data set one of the major benefits of the kmeans algorithm is the implementation kmeans is a relatively efficient method however we need to specify the number of clusters in advance and the results are sensitive to initialization and often terminates at a local optimum unfortunately there's no global theoretical method to find the optimal number of clusters a practical approach is to just compare the outcomes of multiple runs with different K and choose the best one based on a predefined criteria in general a large K probably decreases the error but increases the risk of overfitting you in the previous part of our course we learned the basics of data science and machine learning and the most important machine learning models in practice we can apply manually or automatically various models on a given data set before finalizing and choosing the right model and the more efficient one but how do we evaluate the performance of a model to determine the one model that is performing better in this part of the course we will answer this question by presenting some performance indicators to evaluate the model let's start with the R squared R squared is the most common way to evaluate the performance of a linear model it is a statistical measure of determining how close the data is to the fitted regression line it is a proportion of explained variance to total variance r2 equals explained variance over total variance you can see how it's represented graphically here typically the rsquared value lies between zero and one zero means the model does not explain any variability of the data around it's me one means the model explains all the variability of the data around it's me so the higher the value of rsquared the better the model fits the data and better the model explains the variance of the observed data around its meat however it is difficult to tell whether an increase in the rsquared value is a result of better model performance or more features in the model that's why in multiple linear regression it is better to choose the adjusted rsquare adjusted rsquared is a modified version of rsquared that has been adjusted for the number of features in the model adjusted rsquared penalize is the rsquared if the choice of the feature newly added to model he's not good so in classification scenarios having good rsquared or adjusted rsquared values is not important until both the classes are unidentified with similar accuracy this becomes a big issue when the data is not equally distributed for the targeted classes okay for example consider a data set with 98% observation of Class A and 2% observation of Class B the model can easily get 98% training accuracy by simply predicting that every sample belongs to Class A but the prediction accuracy for Class B is very poor to overcome this situation model performance should be evaluated for each class and then aggregated to get the overall performance of the model confusion matrix provides the right tools to get the individual and overall performance of a classification model so let's take this example with two class positive and negative classes in the figure we have the classifier the prediction outcome and actual values the total number of positive classes is P and the total number of negative classes is n TP true positive in the top left corner of the figure equals actual classes positive and predicted class is also positive this states how many positive classes correctly predicted this is also known as the power of the model FN false negative in the top right corner of the figure actual class is positive and predicted class is negative this states how many positive classes are predicted as negative this is also known as a type 2 error miss FP falls positive in the bottom left corner in the figure actual classes negative and predicted class is positive this states how many negative classes are predicted as positive this is also known as a type 1 error false alarm TN true negative in the bottom right corner of the figure actual class is negative and predicted class is negative this states how many negative classes are correctly predicted so correctly predicted classes is equal to TP plus TN incorrectly predicted classes is equal to FP plus FN actual positive classes is equal to P equals TP plus FN actual negative classes is equal to N equals FP plus TN so we compute the proportion of records that are correctly classified called the accuracy of the classification model and it is calculated as follows accuracy equals T P plus TN over TP plus TN plus FP plus FN so another important validation technique is the cross validation cross validation is a tool that utilizes the training data set in a better way to reduce overfitting and underfitting it is a model validation technique for assessing how the results of statistical analysis will generalize to an independent data set the purpose of using crossvalidation is to increase confidence in the model trained by the training data set so without cross validation our model may perform well on the training data set but their performance decreases when applied to the testing data set the testing data set is precious and should only be used once so the solution is to separate one small part of the training data set as a test of the trained model which is the validation data set okay here are two cross validation techniques kfold crossvalidation this involves splitting the training data set into K subsets of data also known as folds the machine learning model is trained on k1 subsets and then evaluated on the subset that was not used for training this process is repeated k times with a different subset reserved for evaluation and excluded from training each time so once the model has been executed for all training subsets the average of error in each run is calculated and represented as the crossvalidation error leave one out cross validation this is another technique used for cross validation it's a logical extreme of kfold crossvalidation where K equals n the number of observations so for each run only one observation is left with a validation data set this approach leads to higher variation and testing model effectiveness because testing is done against one observation only hence the estimation is highly influenced by the validation observation if the validation observation is an outlier it can lead to higher variation at this last part of the course I will give you some best practices in data science and machine learning okay you need to be aware that the data set that we will get for machine learning problems is not always as clean as we can expect we might have to clean the data set and transform it into a data set that can be used to build a model for this purpose we might have to go through multiple processes like data processing feature engineering and feature extraction feature scaling and selection this set of processes is a major part of a machine learning project because the model performance is based on the data on which the model is trained garbage in garbage out we already talked a little bit about feature engineering and feature extraction I'm going to give you some best practices that we can follow to tackle the data sets and machine learning use cases another important concept is one hot encoding machine learning cannot handle nonnumeric features by themselves so the question that you may ask is how do we transform the non numeric features let's take our example in the employee salary suppose that in the data set gender is maintained as a non numeric feature and this feature can have two values male or female for an organization and employees gender is meaningful information since it is a non numeric feature if we try to train a model on the gender feature then the model will not be able to interpret anything meaningful from it to make the gender column meaningful to the model we must transform it into a numeric column one hot encoding is one of the transformation techniques that can be used to transform categorical features into numeric categorical features so if the feature has only two categories then those two categories can be replaced by 0 & 1 in our employee data set example we can replace male with 0 and female with 1 now consider a scenario where the categorical column has more than two categories in other words in our data set gender have three values male female and unknown so what hot encoding can be extended to take care of this multi category features it creates a new feature for each category and for each observation the value one will be assigned to the newly created feature to which the category belongs to other newlycreated features will be set to zero for example in our employee gender example three new features will be created like in the table binning in a few machine learning scenarios continuous features cannot be used directly to train a model these features should be converted into categorical features and then one hot encoding should be applied to make these features important for the machine learning model and our employee data set example employee age ranges from 21 to 60 years these employees can be categorized into four age brackets 21 to 30 31 to 40 41 to 50 and 51 to 60 the technique of converting a continuous feature into multiple bins and creating a new feature from it is known as binning or bucket ization it's also known as quantization binning transforms a continuous feature into a categorical feature and categorical feature engineering might need to be performed before using this feature in modeling we can create a new age group feature and map each employee with one of these brackets for bidding we can use the domain expertise as well as a few statistical methods to correctly determine the number of Bin's / buckets and the boundaries for each bin so some of the common methods of bidding are one fixed width binning in this technique width is decided for each bin based on domain knowledge rules or constraints to quantile based binning this technique divides the data into Q equal partitions if q equals 4 then the parts are quartiles divide data into four equal partitions three two way ANOVA analysis of variance tests this is used to find similarity between the various data points of the feature these similar data points can be grouped together to partition the data set you first let's discuss what feature engineering is features are the independent variables don't forget we will use this term a lot feature feature equals variable once again independent variables or features are used to predict the dependent variable frequently these features have hidden information that the machine learning models cannot utilize to negate this situation in the data preprocessing stage we just applied the domain knowledge and create new informative features out of the existing features available in the data set these features should be created carefully though otherwise the model may over fit the set of features on which the model is supposed to run should be selected wisely because good features with good quality data can yield a less complex model with better results feature engineering is a recursive process that can be divided into the following steps understand data set brainstorm features create new features validate what impact these features have on the prediction result restart from step one until the desired accuracy and other metrics are achieved so for example consider a model designed to predict salary hikes for employees in an organization the data set contains employee ID geographical location date of birth employment start date career start date etc in this data set date of birth and career start date might not be useful for a data set if we derive two new features for example employee age and years of experience these two features could play a key role in salary hike prediction the process of identifying creating these derived features is called feature engineering in reality though feature engineering is an art and it comes with domain knowledge and experience feature scaling some machine learning algorithms do not perform well when all features in the data set are not on the same scale so coming back to our employee data set example salary may range from $40,000 to $200,000 and age ranges from 21 to 60 days two features do not have the same scale rescaling these features can improve the performance of some machine learning models now how we can make feature scaling we have two common techniques for feature scaling the first one is normalization this technique transformed the feature into a 0 to 1 scale the great thing about normalization is to reduce also the effective outliers the new value of the feature called exchanged is equal to the value of X minus the minimum x value and dividing by the maximum x value minus minimum x value exchanged equals X minus X min over X max minus X min the second one is standardization which transforms the features so that the mean of the distribution becomes zero and the variability becomes one standardization is not significantly affected by outliers look at the figures below they show how the transformed feature will be after implementing normalization and standardization well that's it for this free AI Sciences course hopefully this course has demystified the notion of data science and machine learning that is just the beginning so now that you are familiar with the logic behind the different types of learning you know how to create simple models it's time to move on but you will not be alone on that path we have more content courses and books to reinforce your efforts and guide you at every stage machine learning is a crucial development of today's world the concepts behind it have been around for more than a decade but the age of machine learning and related models such as artificial intelligence data science and more is now the change is just happening and it is fast you've made a great decision to start your journey into the world of machine learning with this free course today the knowledge and the ability to use machine learning is a competitive advantage tomorrow it will be a mere necessity machine learning techniques have already started to change the world of business by creating a new value for data the future will be even more exciting very soon most of the devices and apps that we use daily will be fueled by machine learning algorithms many of them already are now though you have a chance to become a part of this major development Congrats on your decision and don't forget to check out our courses ebooks and contents now as I promised you there is a gift for you it's an ebook available online where you will learn about machine learning you can find it at this link HTTP colon forward slash forward slash WWI Sciences dotnet forward slash gift eBook machine learning we highly recommend that you visit our website AI Sciences dotnet and subscribe to our email list you'll receive all of our books free in an eBook format and you will be informed about all our promotions and offers if you have any feedback please let us know by sending an email to review at AI Sciences net this feedback is highly valued and we really look forward to hearing from you if you enjoyed this video please like it share it subscribe to our Channel AI sciences and make sure you click on the notification button so you can receive a notification when our next course is ready see you at the next video