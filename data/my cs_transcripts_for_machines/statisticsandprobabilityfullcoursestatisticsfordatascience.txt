well it looks like it's time to get started so let's start with section one of chapter one discussing uh descriptive statistics and some basic terminology of statistics so all right let's uh get to work then so uh so in this chapter in this section we're first going to start out uh with some basic terminology uh let's start let's start out with the data data is what statistics is interested in we're going to say that data is basically a collection of facts we have generally a population or some group of interest for which we want to make some statements so if we were to collect data for the entire population we would have conducted a census uh let's let's have some examples there's the classic example of the united states because right now we're actually in a census the 2020 census being conducted by the census bureau and this is a massive undertaking by the u.s government mandated by the constitution to count every single individual currently living in the united states and with that get to get a count that is supposed to be a complete count not an estimate but a count this becomes the facts of how many people are currently living in the united states on a smaller scale there is this class and i presumably have the grades for this class so if i were interested in the grades of this class i could conduct a census by looking at the grades of every single student and with that i would have the entire data set the entire population where i've defined the population to be this class now that said you may not necessarily have access to the entire population for example in the case of the united states the census is an extremely expensive and complex undertaking that can only be done every 10 years and in the case of this class well i can conduct a census you yourself as a student may not be able to conduct a census so if you were interested in how your other fellow students were doing in the class you would be forced to collect a sample a sample is a subset of the population for which you managed to collect data so in the case of the class if you were to talk to some of your friends which may not be the greatest idea anyway but if you were to talk to some of your friends about uh how and ask them how they were doing in the class then you would have conducted this that you would have uh selected a sample uh whereas um uh in the united states we're regularly uh subjected to surveys where a subset of the american population is going to be examined and you're going to ask questions about them and in the case of the census bureau they have a lesser intensity project such as the american community survey which isn't aimed at getting the entire population i think every year they try to get one percent of the population which is big in and of itself all right so uh in a sample we have observations so maybe these are people but in principle they could be just about anything like they could be petri dishes they could be the price of a stock on a particular day something like that and uh with those observations we have variables so in the case of our little person over here who is an observation in our sample we may have tracked their age we'll say this person is 22 we may track their gender uh we'll say that this person uh identifies as male or maybe we are tracking i don't know their uh occupation and we'll just say that this person is a student all right so univariate data is a data where there's only one variable being tracked so in this case if all we did was track age then this would be a univariate data set but this is actually a multivariate data set because we're not only tracking age we're also tracking gender and occupation so that makes this uh data set multivariate presumably there's more than just this individual in our sample uh maybe there are some under other individuals too for whom we've collected some data in this case this would be a multivariate data set there is kind of a special case for the multivariate case which is the bivariate case and bivariate data is a data where there's two variables and when you have only two variables at least in this class we can start talking about ideas such as correlation how are two uh variables related to each other uh you can do this for general multivariate data too but that's kind of going uh beyond the scope of this course and even then you kind of start out with the bivariate case first so data itself the variables can come in different classes such as there can be categorical variables so an example of categorical would be gender and occupation these would be categorical variables because there are a finite number of categories that these could be in compare that to say age which we would consider quantitative because what's being tracked is a number presumably an unbounded number now you might think well what about say uh the roll of a dice if you're rolling a dice then there's only a finite number of numbers one through six this is pretty much a simplification more with categorical we're thinking with things where there may or may not be an ordering but we really don't want to view categorical data necessarily as numbers per se whereas quantitative are numbers right and there's also alternative formulations alternative breakdowns uh with maybe some more granularity on how you're going to break down data like for example you might say there's nominal data where it's just categories that have no relation amongst each other you might then progress to ordinal data where you do add a little bit more structure where one observation where one value could be considered greater than another maybe we could talk about a person's year in college in which case you would say that sophomore is less than in some sense junior which is less than a senior although you don't necessarily want to attach numbers to uh sophomore freshman sophomore junior senior and then you could have maybe interval where you're allowed to do addition and ratio where you're allowed to do division so uh but but yeah that's an alternative breakdown don't worry too much about it in modern statistics we depend very heavily on probability theory probability theory is a field of mathematics that describes the behavior of objects in the presence of uncertainty the mathematical study of probability we've known about probability as a species for a very long time at least since the greeks but it's only until around the time calculus was being developed and probability uh started to take a mathematical interest and then it was around the beginning of the 20th century where probability became its own proper uh field in mathematics so why do statisticians care about probability how's probability used in statistics well we have this relationship um we have a population and we have a sample so a sample is a subset of the population so the question is how is it that we get a sample from the population how when when we collect a random sample from the population where every individual in the sample is picked with some with equal likelihood how will that sample behave how will statistics computed in that sample such such as for example the sample mean or even then like going beyond mean like let's say we track the smallest age how will statistics like that behave the behavior of those statistics is determined by probability so probability describes how random samples from a population behave and as statisticians we develop probability models for our samples and then use those probability models to describe uh parameters of the population so this would be statistics so probability and statistics are working in inverse directions where probability describes how if you knew the population how will the sample behave whereas statistics is interested in given a sample how can we infer the properties of the population like for example important population parameters so okay all right so uh let's let's continue on uh how we define a population largely depends on our problem for example in the examples that i gave before about the united states and my class we would consider a study involving those populations in numerative studies since in principle there is the population exists the population exists in physical and temporal space so that means that there's no like you can actually find in principle every single individual in the united states and you can find every single individual in my class and a census is in fact possible to conduct now compare that instead to analytic studies where the population may not exist so in my previous example where i was talking about petri dishes as being a potential sample a petri dish for some bacterial culture is probably going to appear in an analytic study a lot of those biological studies are going to be analytic studies because the population isn't necessarily considered existing if or in a simpler case if all i wanted to do was determine whether a single dice that i have was fair in principle you can't really say that all of the possible roles for this dice exist in some physical temporal space i can keep rolling this dice forever so uh you can't really consider an enumerative study so you're just gonna say the population is all possible die rolls for this dice and in the case of a medical study you might say that the population is all humans past present and future and we're studying the behavior of some drug in humans we're we're thinking of humans as some biological as a human species as a biological entity uh not necessarily the current people who are living right now we would probably want to include future people too um all right so uh statistics is it depends very crucially on how the data is collected and we're not actually going to talk very much in this class about how to collect data correctly i'm just going to leave you a few words in this class we're going to assume that data was collected via a simple random sample so if data was in fact collected via a simple random sample then ins there is a sense in which every individual in in the population is equally likely to have been chosen uh the metaphor that i like to think of is we have a hat and that hat has little slips of paper and each of those slips of paper will identify uh individuals in our population we pull one of the slips of paper at random with equal likelihood from the hat and that gives us an individual in the population that will be included in our sample compare that instead to stratified random sampling which is a more complicated procedure and the procedure that the census bureau is actually going to use in these annual american community survey studies where in this case you actually divide up your population according to some known strata a strata is something that you automatically know for individuals in your population for example what state they reside in so you divide up your population into strata and those strata don't necessarily have equal size but you're going to pick a random sample from each strata a simple random sample as i described before and then make inferences this procedure the stratification procedure can increase uh the power or the ability of your statistical procedure without having to collect as much data so it's a nice procedure to have in hand it's the procedure that is being used in more complicated surveys but we're not going to discuss it here the procedures that we discuss here are not appropriate for stratified sampling uh so it's of course very easy to sample badly uh so one instance of bad sampling is convenience sampling where you're basically selecting individuals from the population based on how easy it is for you to get the data in the case of this class if you were interested in how and what the grades of this class were and you decide to ask some of your friends what their grades are that's a convenience sample it is not a random sample because in the end the sample is going to resemble you in some way it's going to be more a reflection of you than it is of the class so you may end up if your friends are like better students and that's because better students like to comingle then that could be a problem because you're not going to get an accurate view or let's say we're studying politics and you decide that you want to determine people's um political party affiliation so you stand outside of the marriott library on campus and you start asking students uh what party do you support in elections uh in that case you are not gonna get anyone from st george utah uh almost sure well i mean you might get a couple people but for the most part st george utah will be extremely underrepresented in your sample and that's going to be a problem now you don't have an accurate representation of the state of utah if that if the state of utah were in fact your population of interest and even if the university of utah itself for your population of interest not every student is going to be hanging out around the library so that'll be a problem uh or in the case of a voluntary sampling where like the classic example is a a tv host goes and tells his viewers to participate in a survey is like who do you like the democrats or the republicans and it's like oh my gosh all my viewers like the same party as i do and they agree with me no shocker so that would be a case of a very unreliable study um so yeah that's that's another thing but we're not really going to talk too much about this uh this chapter this section i mean there's a lot that can be said about how to appropriately sample and actually this is one area where current events i would like to say some more about it like the coronavirus uh what are some of the statistical issues surrounding the coronavirus because there's a lot of statistical issues involving the coronavirus and our understanding of it and its effect on the population and i'm thinking i'm going to leave that to a separate video but for now that is the end of this section and uh i'll see you in section two on pictorial tabular methods in descriptive statistics so all right best of luck to you hey students alright so next section is on uh making graphs and making tables for descriptive statistics all right so first let's describe the idea of a distribution when we're talking about random variables there's a couple things that we should keep track of there is what values a random variable could take and there's also how frequently those values are taken those ideas are are captured in the distribution of the data set so um in uh in in this section we're going to see some uh basic ways to understand a distribution of an observed data set and in later sections we're going to talk about the idea of the distribution of a population and how it could possibly uh extend the idea of distribution and one basic way that we could understand a distribution is using a table and we can also use visualizations visualizations can actually be very helpful in understanding a distribution because there are things that our eyes are very good at picking out and when we make pictures of our data set we pick those things out like for example where does the data tend to cluster how spread out is the data are there outliers in the data set influential observations and our eyes are actually very good at noticing those features so creating visualizations of data says can lead to us learning a lot that actually really cannot be learned by numbers on their own in fact there are some uh examples for example there's this thing known as um amscom's quartet maybe i should uh maybe i should very quickly uh have a look at anscombe's quartet because we can do that um very quick okay so and oh let's see wk and scum is it b uh quartet this is close enough it'll probably figure it out is it ants comb and i can't type oh okay so it has a b in it all right so there's this picture right here and this is known as enscom's quartet this is for a bivariate data set uh which is not what we're talking about right now but uh when working with bivariate data sets you want to track uh for example the mean of x and the mean of y and also if you were to come up with a best fitting line a regression line between the two variables uh what would the parameters of that line be and what is the correlation between the variables and numerically every single one of these data sets are exactly the same even though our eyes can see they are not the same at all these are clearly not the same data set and they have very different properties so making visualizations can help us learn a lot in fact this is a subject in and of itself you can take a class from the computer science department on just visualization um but we're not going to be going into that much depth and visualization today um what we learned today is pretty much enough to get you by in life unless you're doing some really serious work so from this point on in this video and future videos i use this notation to describe a data set and this data set has sample size n so the sample size is the number of observations in a data set um the indexing here we have x1 x2 xn the indexing the numbers themselves don't actually matter they're just a way for us to differentiate observations these observations may have the same values or different values uh it so there's it it actually doesn't really say all that much about the data set in particular i don't want you to look at this and think that this data set is now ordered this data set is not assumed to be ordered if the data set ever becomes ordered i will let you know but right now it isn't um i also would like to draw attention to the fact that this is in lower case and it's it's not a it's not a perfect rule i've often violated it uh but it is a hint that lowercase things and this will probably be the case in the in the in the videos for this class um it is generally the case that lower case numbers are constant uh and a data set is constant a data set is something that you observe and because you observed it is now constant whereas capitals often refer to random things which is not what we're going to talk about today so uh the first visualization method that we're going to talk about is very basic it's called a stem and leaf plot so we create a stem leaf plot using the following steps the first thing we're going to do given a data set is we're going to select the number of leading digits to be the stem values so we have a number maybe that number is 123.45 and we're going to select some number of digits maybe the first two digits and these will be the stem values the remaining step the remaining values uh let's use a different color for that uh where did red go the remaining values will be the leaf values which in principle you could put like the entire block 345 as a leaf value although a lot of people will just like round to just one digit uh because well you'll see for a second why someone might be inclined to do that um so after you make that selection you're going to draw a vertical line let's not use that color uh we're going to draw a vertical line and we're going to list the stem or possible stem values uh on the lefthand side of this line so we might have 12 11 10 and maybe 13 and 14 and on the on the right hand side of this line we would record the leaf values let's assume for a second just for simplicity that we didn't observe this part and all we saw was the three we might list one two three and this would be read off as a hundred and twenty three because according to part four we're somewhere in this display we're going to indicate the units of the stem so we might say in this case tens uh to indicate that the stem is uh tens place and beyond uh which means that everything to the right will be in the ones place or maybe we're doing a more advanced plot where maybe we actually did write three four five so this would be one two three point four five uh but we're not going to do that because uh i actually think that would be it kind of defeats the purpose of a stem and leaf plot because what we would instead do is list the leaf values for each observation our data set so maybe we saw 101 101 1 1 2 3 four so we have an observation a hundred and fourteen and another observation that's a hundred and thirteen and an observer uh another observation that's 112. uh or we'll also even throw in a nine so there's an observation that's 119. we've got 123 130 144 um and that and now we have a display and let's just throw in like one more number uh like right here uh now we have a display that that um actually later on we're going to see a histogram and say oh this actually looks a lot like a histogram but we do have this display that's indicating that that's telling us a lot about this data set for one thing it actually tells us the actual values in the data set that's one advantage of a stemandleaf plot when you have a stemandleaf plot you get to see the actual data values and that's one thing that's nice another thing that's nice about them is that they're very quick to create like i imagine the stem and leaf plot being created in the field like you are out there you are watching uh i don't know birds and tracking how many birds you're seeing or characteristics of birds and you actually get to write down uh as as you are recording your data you're also visualizing it so you can learn uh things about it because you can see like for example this data set uh seems to cluster in this 110s area uh we might be able to guess that the mean is somewhere around here that this is like where the center of the data set is and we get some sense of its spread all right so um here is another data set for us to practice with uh this is a subset of mcdonald's data on hint on heightened finger lengths and this right here is actu this part right here is actually our code so we could go into r and type this part in and uh use r to do some of this analysis so um and i'll actually show you a little later how this can get turned into like what r would do with this part uh but um we're just gonna take this and treat it as our data set and construct a standard leaf plot by hand so what are we going to do uh we first need to decide um where we're going to make the cutoff we're not going to make it in the ones place because if we put it in the ones place then there's going to be just five for our stem value and what you'll end up with is a plot uh that looks like this and it will be just like a giant skyscraper and you won't you won't learn anything so we don't want that so the next place that is available to us is the what is the tenths place and the tens place seems pretty good so i'm now going to make the line for the stem and i'm going to write let's see we've got five three five five we have a five two oh we also have a five oh so we'll put five zero five one five two five three five four uh and we go all the way up to five nine so five five five six five seven five eight five nine okay there's there's some dead spots on this super super cheap computer i bought it for 200 um at target across the street the which was the most convenient place to buy a laptop super cheap so you can't what you pay for and i'm especially learning that right now uh anyway um so uh we now have the stem values and now we can start recording our data set if you were to do this in a computer a computer would actually order the uh leaf values here they actually are ordered because nine comes after four which comes after three uh so a computer would actually order it i don't really see the point because the point is to understand the distribution as in in a visual way and ordering believes does not aid in that so we can go ahead and just start reading across in this data set to make our stem and leaf plot so we've got 5.55 so we'll put a 5 right here point three zero so we'll put zero there five six three five three zero five one three five zero five five three eight five nine six five two one and five three eight so in this case it actually turned out to be ordered and that's pure coincidence i wasn't planning on that but now we have a stemandleaf plot and if we scroll down and or if you're actually if you actually have these notes printed out oh one last thing we need to do is indicate what this line means and this line is the tenths place okay all right so if we were to continue on to the next page we would actually see some r co that would do this so this is the high data set from before this part right here the r function oh okay so this is the act the same height data set we had before the the r function stem creates stem and leaf plots and this is basically the same as we had before okay before i continue on i'm just going to double check to make sure i'm still streaming yeah we're still streaming okay all right um okay so next up for our visualizations we have a dot plot a dot plot represents each data point as a dot along a real number line uh so we can we would then be looking for clusters in dots to note patterns in our data set so in this example we're going to use the data from the previous example which has already been very nicely organized for us in a stem and leaf plot to create a dot plot let's see is there a nice easy way to zoom out oh very nice oh that's not what i wanted that's delete i need i'm new to this program uh and we're at the very bottom too that's not what i want either okay so all right so this this program is new to me my apologies um all right so all i want to do is zoom out a little bit okay that's good that's good um oh cool all right i just want to be able to see this uh nice stem and leaf plot that we created before when creating our dot plot so for our dot plot uh let's see we've got let's see so for the left point on the range we should pick 5.0 and uh for the right point of the range we should probably pick 6.0 because we get really close to 6. so in between we've got 5.5 and then we can just mark off in in a fifths so two three four five okay that's that's that's close enough uh if you want perfection ask a computer to do it all right so um so we've got an observation 55.05 so that's about right there 5.13 is about there 5.21 is about there and then we've got 5.30 um that is about there and that's also got another one right with it uh there's another observation uh oh that's actually could be a little bit to the left but we could also call that 5.38 so here's 5.30 for realsies this time we've got 5.55 5.63 and then 5.96 so then we end up with the stop plot that's also giving us a sense of where the data tends to lie but this time along a real number line so and and i i'm sure you can guess so far that the methods that we have been working with uh our visualization methods are pretty much devoted exclusively to uh quantitative data we'll see categorical in a second i'm pretty much going to say draw bar plot and that's it so here is our code for uh what i just did the r function for making dot plots is a strip chart so strip chart is what makes these things and i'm just passing some additional parameters to strip chart to uh make the function to make the plot how i like like for example this parameter right here controls how it's like what exactly we're drawing here are we drawing filled in circles or empty circles and so on so this you can think of this as point character because you're choosing the character of the point and 19 corresponds to a filled in circle uh we have a little bit of an offset and uh some other parameters that just help make the visualization look nice i would recommend go ahead going ahead firing up r playing around with these parameters seeing what happens uh stack for example uh means that when two points are about to collide with one another stack them on top of each other because it's not the only way to do things there are often very many ways to solve the same problem so all right uh continuing on so a quantitative variable we we need a further a refinement of quantitative variables and and we say that a quantitative variable is discrete if it's possible values are countable uh countable that could be for example one two three four five six or that could be one two three four five six seven eight nine blah blah blah up into the future uh it could be the integers so including negative numbers as well or including zero as well um and then so we would call any of those things countable there's also continuous variables where the possible values could come from a continuum so that includes 0 1 and anything in between so 0 one zero point five uh zero point zero one one zero one two five uh one divided by pi any one of those things are possible uh there's actually a nice little rule of thumb which is that uh discrete random variable discrete variables emerge when you count things like for example i count sheep so what am i going to get i'm going to get an integer in the end so if that's going to be the case then it's comfortable whereas continuous emerges from measurements so like with a ruler so you measure how long something is with a ruler and that will probably produce a continuous variable so continuing on uh some further notions when describing a distribution there is the frequency of a of a value and there's also the possible values it could take so frequency is how frequently we see a variable take a particular value and for discrete variables it is in fact possible and quite likely that we will see the same numbers repeatedly whereas if we're dealing with continuous you cannot necessarily assume that so the frequency for each value for a continuous variable would be rather uninteresting since you're probably going to see like going back to the data set we were just looking at admittedly uh for this uh height data set we did have some collisions but you can't really count on that you can imagine that if you were to increase the decimal position precision of the numbers that you're looking at you would um you would actually not have a collision you'd have two distinct numbers the only reason why we get 5.30 twice is because that's where we decided to round so in the case of continuous variables well with frequency well with discrete variables it's okay to just kind of consider the values in and of themselves and just say how many times did one show up how many times did two show up and so on for continuous variables we probably should do a procedure known as binning so binning is where we define a range in which a data point could possibly be and instead of tracking how frequently a certain value is taken we're going to track how frequently um a variable will fall into a bin right so you can imagine for instance that this is a this this is a continuous line between zero and one and we saw uh numbers uh in this region and they're just random numbers and we're not going to actually list out in a table how often we saw those individual numbers because most of the time the frequency for that number will be one instead what we'll do is we'll divide up this region and then count how many times uh numbers fell into those individual bins like for example we get we got twice here three times a year uh one two three four five six times here one two three four five six times here and then four times here and then we'd have like a more useful table um for understanding our distribution very closely related to the frequency is the relative frequency relative frequency is where you take the frequency and divide it by the sample size that's it so if you really want to see a formula for the relative frequency the relative frequency the relative frequency is equal to frequency divided by n a frequency distribution is a tabulation of the frequencies or the relative frequencies or in the case of continuous variables we'd probably also include the frequency of bins um something that i haven't really discussed so far is that actually how should we choose the bins like like like bidding is a choice i chose these bins but alternatively i could have chosen bins like this like here's one bin and here's another bin why didn't i do that so the actual bidding decision is it is in fact a decision you need to decide how many bins they are and what regions they're going to cover so in terms of the regions i think it's pretty safe to say the bin should be equal length if the bins are not equal length then what you're going to end up with is a difficult to understand chart or a difficult to understand table because people then have to pay much more attention to the bin length um and also when you're trying to visualize what's like this is leading up to histograms and we're going and the binge bin size is uh something you have to make when talking about histograms and when you um uh when you end up with bins of unequal length then the histogram like you there is ac there is a precise way to understand what's being visualized in the histogram but unfortunately people's eyes are not going to perceive it that way um and it's that you are making the histogram more difficult to understand so so just always make your bins equal length but the number of bins that you have and also what exact regions they cover because you can still for example take all of these bins shown here take all these bins and then shift them slightly to the left and then you have then you have a different visualization or you then have a very different description of your of your data set how exactly should you decide where the boundaries of your bins are and how many bins should you have these are all very important decisions that can have uh drastic implications for visualizations but i i really should move on to actually creating a frequency distribution which is a tabulation of frequencies or relative frequencies i should really move on before i start digging into that topic so let's make a frequency distribution for this soccer data set where we have a supposed statistically minded parent maybe that parent is you if you have a daughter or a son and they play soccer and you've and you've decided that you're going to uh track their little league soccer team scores during a regular season so here i have a data set i've and by the way what i've done here uh in terms of our code is creating an r vector so i have this list of values um in what's known as an r vector i'm not really going to talk about r in these videos i'm going to pretend and assume that you are watching my video series on how to use r uh but enough of that for now um i want to construct a frequency distribution for this data set so what are possible values in this data set uh well uh i should let's see uh we've got we've got a so on the left hand side of this line for this table so we're going to have um a value and we're going to have on the right hand side the frequency so what are possible values well we've got one and we've got nine and we've got numbers in between so we'll just go uh one two three four five six seven eight nine all right and then we need frequency so let's see how many times did 9 appear looks like 9 just appeared once all right so we got that covered uh six appeared uh so one two yeah that's it so six appeared twice uh five appeared three times and uh eight appeared twice uh and each each of the remaining numbers appears only once so one appeared once two appeared once uh three appeared once and four appeared once okay uh seven never appeared all right one second i catch up in my notes um so the sample size for this data set you can count there are 1 2 3 4 5 6 7 8 9 10 11 12 observations or alternatively what you could have done is um and in fact i'll go ahead and just kind of scroll down a little bit and down here i'm going to track the sum and yeah it adds up to 12. another thing i want to do is in this table track the relative frequency so we'll call this relative frequency and in the relative frequency i'm going to take the frequency and then divide by the sample size so 1 divided by 12. uh that is going to be a 0.05 or is that an 8 no that's an 8. 0.08 three and the three is a continuing uh digit all right so for the next one it's the same thing zero eight three uh and again for three zero point zero eight three okay and then four five uh no wait so let's see uh four also appeared that amount so zero point zero eight three uh four appeared three times so that's going to be zero point uh two five because three divided by uh 12 will be 0.25 3 is a quarter of 12. 6 will be 0.16 where the 6 is a continuing digit 0 0.16 and 0.083 okay and if you add these numbers up this is a way for you to check your sanity this should add up to one okay so moving on if you were interested in how to make a table like this in r the function for r in r for making such a table is well table so you would have given it the soccer data set and ask it to make a table and it will tabulate uh which observations were taken and how frequently they were taken notice that it did skip seven um which i don't know is that a feature you want i don't know uh i actually personally would rather not skip seven and you'll probably see why in a second um when working with continuous data as i was mentioning before there's this issue of binning uh deciding on the number of bins uh deciding um uh where the boundaries of the bins are and so forth but um so but uh once you've decided on the number of bins and there are some rules of thumb that you can use uh for how many bins that you should use like for example if the if the sample size is n you could perhaps choose the number of bins to be the square root of n actually there is some statistical arguments that the correct number of bins should be on the order of n to the power one fifth or the fifth root of n um there are some arguments for that but uh square root of n is also fine for now um at the end of the day whatever it is that r is going to use for its uh binning decisions is better than what you're going to do and it's going to have some uh nice theory backing it up so you probably should just trust r whatever r is doing is probably better than what than than the square root of n roll um so after you've decided on the number of bins you segment your number line so that you have that many equal length bins uh and uh depending on where in each data point falls assign it to a bin if it falls on a border between the bins assign it to the bin on the right so in other words bins are right inclusive that is a parameter some people prefer left inclusive bins does it really matter no it doesn't it doesn't really matter nobody really cares uh just just pick one right and be consistent either put it on the left band or the right bin just don't put it in both please and then construct a frequency distribution for the bims uh very quickly before i keep going okay we're still good all right um i i i'm very nervous about losing this video i'll just put it this way today i've recorded a number of videos that suddenly just went up into thin air because things went bad and i don't want that to happen again all right um so uh example four so using the data from example one uh you can scroll you can go back in time to see what that is this is the heights data set uh construct a frequency distribution and for what it's worth there were 10 observations in that data set the function length when given a vector will tell you the number of elements in that vector in r so since the number of elements is the number of data points we want to use length to determine the sample size okay continuing on i have made some decisions i'm going to decide that the number of bins i'm going to use is going to be uh about the square root of 10. the square root of 10 is between three and four so uh uh so if you wanted to use the square root of ten rule uh that would be um about uh we'll say for rounding up although apparently in my notes i decided that i was going to go with five and i don't want to change it so i'm gonna go with five why five why not five um it's it's it's it's whatever you want i i think the reason why i went with five uh i don't know i just did so um let's go ahead and zoom oops is that what i want yeah that's fine okay so uh i'm going to make a table so i on uh in this table i'm going to list the bins so i've got 5 to 5.2 and to indicate this is going to be right inclusive i'm going to put a little uh less than sign to say 5 2 less than 5.2 not including 5.2 and then 5.2 to uh 5.4 uh 5.4 hold on okay five point four two five point six five yeah i need to kind of get away from there okay uh 5.6 to 5.8 and then 5.82 uh less than six and we can just leave it at that all right and then we have the frequency for each of these bins so the frequency for these is going to be well there were two numbers between five and five point two uh five numbers between 5.2 and 5.4 and then one number in each of the remaining bins so uh if we were to do the relative frequency we're going to take each of those frequencies and divide it by 10. in which case it's you're just going to move the decimal point over one place so 0.2 0.5 0.1.1.1 okay all right once we have a frequency distribution such as this we can now construct what's known as a histogram which is a plot for visualizing the distribution of quantitative data so how do we construct a histogram first draw a number line and mark the location of the bins so for example we could do something like this and we're going to say that the bins are going to be about here um and then for just if you want to for discrete data you can center your bins on the corresponding values themselves um because you're not really doing any binning with the the discrete variables or you can imagine that your bins are exactly what they need to be uh to be touching each other and uh centered on the integer uh the corresponding integer and then for each of these classes or bins draw a bar extending from the number line so we have uh some yaxis that's tracking let's say the frequency so we have a yaxis that's checking the frequency we're going to draw a bar from the number line to the y value that corresponds to the frequency of that bin or the relative frequency if that is in fact what you're plotting on the y axis so it would end up with a plot maybe looking like this and that resulting plot is a histogram okay so uh going back to some of our examples let's draw a histogram for the data set in example three which was that soccer data set so for the soccer data set we had numbers between one through nine we'll go ahead and include zero as well for this data set so i'm just going to i'm going to start out by drawing the graph the xaxis corresponds to goals so goals in a game and the yaxis corresponds to the frequency so let's see the the frequencies never seem to go beyond three so we've got uh one two three okay and then possible integer values i'm going to go i'm going to go ahead and include zero because in principle this soccer team could go could score zero points so we'll go ahead and include zero but we'll also include at the very end we'll have nine and we've got one two three four five six seven eight nine okay so uh they there was one game where they scored one point uh one game where they scored two points one game where they scored three points uh and one game where they scored four points then they had three games where the team scored five points uh two games where they scored six points uh no games where they scored seven points uh two games where they scored eight points and one game where they scored nine points okay hmm new feature let's go ahead and see what happens uh when i no i don't think that works oh well um so there there be the histogram it's it's not a perfect picture but it's mine so if we wanted to down here is the r code for constructing a histogram so the r function is hist and uh i've given it some parameters to manually specify the breaks because i because otherwise um it would choose its own algorithm it would use its own procedures to come up with the breaks and i wanted to override that and this is one way to do it where i basically gave uh the um uh the function the uh break points so we have so the minimum of scott of soccer is going to be uh 1 so 1 minus 0.5 will be um why can't i do math it's 0.5 um and then you have and then i went one above the maximum score so that would be this number right here so this right here corresponds to 0.5 and this will be a 9.5 uh over at the right hand side and that's just telling it where i want those breaks and then it will infer that you have that everything in between is a bin uh but then you end up with a with a pot that is essentially what we came up with by hand okay uh next example for uh the data set in example one let's create a histogram okay so for that one i am going to use the relative frequencies instead which do you use at this point it doesn't really matter because the shape is going to be the same regardless of whether we divide by n or not and the interesting part when you're creating a histogram is looking at the shape of the resulting histogram so i actually really don't care although for what it's worth if you want to use this thing for more probabilistic inference you probably should pay more attention to whether you've got the frequency or the relative frequency um and if also the bins were not of um uh equal length then you would have to pay much more attention to the frequency versus relative frequency but also just take my advice and make your bins all equal length so uh we've got the relative frequency that is what we're going to draw this time and uh we've got the highest it will ever go is uh 0.5 in fact i don't think it ever reaches there so we've got right yeah it does it does reach 0.5 so and we'll just increment by 0.1 so 0.1.2 0.3.4.5 so this is 0.1 uh right here so possible bin values i said i decided that we were going to have five bins for some crazy reason i don't remember what it was uh and so let's see is that five it is now so we've got numbers between five and six and using the table that we came up with uh that time ago we're going to end up with a histogram uh looking something uh like like uh this so it goes up to here and then oh yeah so it goes to 0.2 and then 0.1 for these remaining bins and that's our histogram so if we were to continue along and look on the next page uh oh i think that's the reason why i chose 0.5 it's because it it's exactly what r thinks it should be and you should probably given the choice between what you think the number of fins should be and what are things it should be you should probably go with what r is good what r is doing um don't fully trust it but at this point you probably don't have the experience to have your own opinion so um yeah it came up with basically the same picture okay now we've been coming up with these nice plots and everything is great one second let's just uh satisfy my nervousness okay we're still good okay um i mean making these pictures is nice but why are we making these pictures well there are certain things that we are looking for when we're making visualizations like this for example is the data unimoda we're like one thing we're looking for is modality which is where does the data tend to cluster is the data unimodal where it only has one peak this would suggest it's clustering around one area or on the other hand is a bimodal or multimodal where you have multiple peaks so the unimodal case would be something that resembles this uh where so we'll call this unimodal bimodal would be a situation like this uh maybe think of it as a camel hump and multimodal uh is uh like all like you've got all sorts of different modes all sorts of different peaks so what would it mean if you had a unimo versus bimodal versus multimodal for starters the multimodal case the first thing that you should ask with multimodal is did i choose a bin that's too small because you can end up with situations if you choose your bin size to be too large when you're making a histogram uh if if at one extreme you can have a skyscraper where everything is in one bin and that is a chart that doesn't really tell you anything on the other hand you could have that basically a pancake where every single observation gets its own bin and that really doesn't tell you anything either that's basically a dot chart and you've kind of lost the point of the histogram so the first thing in the case of blue of this uh blue uh sort of histogram it's not really histogram because it's a smooth curve but whatever um in this in this case you should probably ask yourself whether you've chosen too few or too few bins uh it could happen that you have true genuinely multimodal data but the number of modes should be should not be too many because modality and having more than one mode is indicative of there being more than one actual population in your data set so for example if i say that this is tracking height of people i could genuinely have a bimodal data set because actually there isn't one population in this data set of people there's actually two populations men and women because men and women will cluster around different average heights so that's that's that's that's features that you're looking for unimodal indicates that your data set it probably consists of one population um and another thing that we're looking for when looking at stuff like histograms and by the way a lot of this discussion also applies to the strip the stem leaf plot and the dot plot in particular the seven leaf plot because you can argue that the standard leaf plot is actually a histogram it's just a histogram with partic with a particular bin choice but there's nothing really different about it um but anyway is the data positively skewed or negatively skewed or symmetric so a positively skewed data set the way i like to think of it is well let's first draw it out we have the positively skewed data set and then we have the negatively skewed data set which looks more like this and then we have the symmetric data set let's do a little bit better than that okay that's a little bit more symmetric okay so if you're if you're at all bothered by the terms positively skewed negatively skewed symmetric and you're wondering how can i tell the difference between positively skewed and negatively skewed here's a little rule for you draw a dinosaur draw a dinosaur and then ask where is his tail pointing in the case of the black dinosaur it's pointing towards the positive end so it's positively skewed all right now let's talk about the green dinosaur well let's uh draw the green dinosaur give him some legs give him some stuff on his back because he's like a stegosaurus so he's got these plates on his back where is his tail pointing oh it's pointing in the negative direction so it's negatively skewed right and you don't draw you don't draw a dinosaur for symmetric because i know we're not we're just not going to do that we're already silly enough but um why does it matter whether a data set is positively skewed or negatively skewed um it matters when we're thinking about the relationship between uh important statistics such as the mean and the median so the mean and the he the median are going to behave a certain way and have certain relationships depending on whether the data is positively skewed or negatively skewed uh positively skewed data sets what that basically means is that outliers there are outliers in this data set and when observations tend to be large they tend to be very large um negatively skewed data sets are data sets where when an observation is small is small it tends to be very small and i can think of specific data sets that fall into these types of categories like for example income tends to be positively skewed so you have most people in a certain range of incomes and then you have a few people who make much much more than that right so most people are around here but when you're rich you tend to be very rich and on the other hand for negatively skewed data sets i do have something in mind i have test scores test scores for me like there's a lot of ways test scores could actually appear statistically but it seems like most students tend to fall within a certain range and the students who really didn't get it uh when they fail they fail hard so it tends to be negatively skewed and symmetric symmetric is kind of this ideal case where you're just as equally likely to be above or below i think heights could be possibly uh i haven't really looked at a height distribution a long time uh but i think that heights could probably be a symmetric uh where it's like you're just as likely to be uh really tall or really short um at some degree so you're also looking in these uh plots for outliers for example a histogram that has an outlier you might have a histogram that looks something like this and then you have a point that's way out here and you would say that this point right here is a candidate's being outlier and you're also interested in how spread out the data is and by spread uh when we're talking about spread we're talking about the less spread out black distribution as opposed to the more spread out green distribution which of these cases are you actually looking at or at the very least you're just interested in what the range of the data is okay so that's it for visualizing qualitative quantitative data now let's talk about qualitative or categorical data how do you visualize that we're only going to advocate one method here and that's a bar plot in fact in my visualization class i was told if you don't know what visualization method to use use a bar plot because bar plots are actually very good visualizations never ever for the love of god draw a pie chart i know i know especially in like public policy and economics people love their pie charts for the love of god do not create a pie chart do you have any idea how many memes of ugly and stupid pie charts there are please do not drop our chart okay um continuing on uh to construct a bar plot list each possible value of the variable and how frequently that value is taken uh a lot like what we were doing before it's just instead of having numbers on the left hand side for like these bins instead of that you're just going to have the categories that your dataset could be in uh and then you're going to draw a horizontal line so let's see a cartoon bar plot that we're making so we have like category a category b category c so draw a horizontal line it could be a vertical line it doesn't really matter and along the axis marks possible values of the variables and then draw a bar for each category extending to the categories observed frequency so we'd end up with something that looks like this um uh it is worth mentioning bar plots and histograms are two different things and the difference is this axis here this xaxis with a histogram that axis is a number line and there is a very specific order and if you were to plot a point on that line it would mean something whereas with a bar plot this doesn't mean anything you could rearrange it if you wanted to and the bar plot would say exactly the same thing even in the case of ordinal data you could space it out there's all sorts of transformations that you could do and the plot says the exact same thing that is not the case for histograms so bar plots and histograms despite looking somewhat similar are certainly not the same thing okay so uh for our next example uh there is a data set showing the frequency of the class of passengers aboard the titanic who survived her sinking uh i have the titanic data set in r uh contains this data set but they're actually tracking a lot of things they don't just track the class they track male and female they track age they track survival and so on here i have restricted to using the supply function which i'm not going to talk about right now i have restricted it to the case of survivors for individual for certain classes i want to create a bar plot for the frequency of each class's survival so um i've already got the frequency distribution r is already given it to be very nicely so i'm going to say we've got first class second class third crap class class and a crew okay and then we're going to extend up we're going to say up here let's say that this is a 220 and right here we've got 110. we're gonna just eyeball this uh so for first class there were about 203 survivors so that's about here for crew there were 212 survivors so that's about here uh for second class there are about 118 survivors that's about here for third class about 178 so our bar plot should look something like this okay and in fact we can look at what r does we give the bar plot function in r this bar uh this data set and it will in fact make a very nice bar plot for us okay there is actually a visualization method that i haven't really discussed here and you know i'm on i'm making a video everything's quite nice i really don't see why i shouldn't show you this if you're familiar with r so let's see first are we still streaming yes we are okay so um there is another type of visualization called a density plot you cannot make a density plot by hand so i'm going to go ahead and i'm going to make a density plot and does this command still work this this is a new installation oh good everything's working uh maybe we should go back to um well let's see what's a data set that we could work with um rivers is fine this is the length of some north american rivers and there is a plot called the density plot remember that i was drawing some smooth curves and histograms do not look like smooth curves what i kind of was drawing was a density plot so i could create such a plot by typing in plot density and then give it the name of the data set if it's stored in a vector so in this case rivers and this is the resulting density plot and it's actually plotting a smooth curve uh let's see let's what's uh one of the data sets that we were looking at before um page up page up uh what is it was this okay okay so we had this height data set so let's uh go ahead and combine those two things uh center on this okay so height uh will be a vector consisting of the numbers 5.55 5.30 uh 5.03 5.30 5.13 5.05 uh what was that number 5.36 no that's 5.38 so we've got 5.38 5.96 and uh 5.21 and 5.38 okay so here's our data set and i'm just going to create a density plot for this data set um that's not it it's still rivers oh because i typed in reverse silly me um okay so what i want instead is height there that's better and it makes a smooth curve that kind of resembles actually it it yeah it certainly does resemble the histogram that we drew um except it's actually like in the histogram there is in fact when you look at that data set kind of a peak in this region um and that was completely masked by the histogram but the density plot was able to capture it interesting so but you have to make a plot like this an r you cannot make a plot like this by hand so just i'm just bringing it to your attention because these two these kind of plots do show up all right so uh that's it for section two of the book and uh i thank you for joining me and uh i will see and i hope that you watch the video for section three so have a nice day hey students all right so uh next section is on measures of location so up to this point we've talked about visual summaries and visual summaries are nice the thing is though we don't want to restrict ourselves just to visual summaries we would also like to be able to have numerical measures of data to understand distributions so we're going to start with measures of location measures of location tell us where a data set tends to be located along a number line so the first and most common measure you may have you probably have already seen a lot of these measures that we're going to talk about but the very first one we're going to talk about is the sample mean and for a data set consisting of observations x1 to xn the sample mean is just is defined as x bar which equals 1 divided by n times the sum from i equals 1 to n x i which if you're not familiar with this notation what this means is we would take our data set add up everything in the data set and then divide the resulting sum by n now the there's the thing called the sample proportion and in fact relative frequencies are sample proportions they're counting the proportion of observations in a sample that take a certain value the sample proportion is also a measure of location it loosely is like the proportion of observations in the sample that have a certain characteristic so we divide the sample into successes and failures we like to use that vocabulary of success and failure a success and the sample proportion will count the number of successes so we'll have p hat equals and very loosely we're just going to say this is the number of whatever we consider to be a success and divide this by the sample size now this could also be written as x over n which could also where x is this uh number of successes and then we could say let's suppose that x i is equal to one if the ith observation counts as a success and zero otherwise what then does it mean to count the number of successes to count the number of success successes is to go through each observation and then add one to a running count if that observation counts as a success and otherwise do nothing which is the same as adding zero so we could then say that the number of successes is equal to the sum from i equals one to n of this new x i variable that is counting the number of successes effectively and saying whether an observation is a success so we should say that this sum is equal to the number of successes and then we take this sum and divide it by n which is also for what it's worth the same as taking the sum and multiplying it by 1 over n so notice what i just wrote down i just wrote down the sample mean again which means that the sample proportion is the same as the sample mean of a sample that consists of ones for successes and zeros otherwise so it recognizing this is actually very important because in probability theory the mean or more more directly sums of variables or sums of random variables are very important so recognizing something as a sample mean means that any theorems from probability theory that involve the sample mean can be applied to that variable so this is actually quite important to recognize but that means that after this point we really don't have to say much more about sample proportions because the sample mean whenever we're talking about the sample mean we're also talking about sample proportions so let's get started with an example what is the average number of points your daughter's soccer team scores here is as a reminder the data set this is actually proper r code right here just to write the just to write down the variable name soccer and what will happen is r will then print out that data set so uh but we're just going to take that for granted for now and uh compute first let's go ahead and compute the sum of these numbers so the sum from i equals 1 to n x i which is basically this right here just means take all the all the numbers in this data set and add them up so we've got 9 plus 6 is 15 plus 5 is 20 plus 5 is 25 plus 5 is 30 plus 6 is 36 plus 2 is 38 plus 8 is 46 plus 3 is 49 plus 4 is 53 plus 8 is 51. plus oh it looks like i'm all right so i actually have something written down i i have i i have 62. i think i might i might have uh missed something in that account but it's going to add up to 62. okay so just take my word for it uh so this adds up to 62 uh n is equal to 12. so the sample size is equal to 12. and the sample mean then x bar will be 62 divided by 12 which equals 31 over 6 which is equal to 5.16 with a repeating six okay if we were to go to the next page in these notes we would see some r code that computes the sample mean for this data set the r function is mean so we ask so we say mean of the soccer data set and it will report to us the mean which is what we computed now let's suppose that uh r1 rn is the ordered version of this data set so x1 to xn uh is just so r1 to rn is x1 and xn but ordered remember from a previous video uh for section two on um that that for this notation uh x1 xn i do not necessarily imply any sort of ordering now i do for r1 to rn i'm going to imply an ordering uh the sample median is another measure for the location of the data set it is defined as the number that splits this data set in half so uh we can that is basically the definition um and from that we can come up with mathematical formula so we can say that x tilde that's the notation we will use for the sample median x tilde will be one of two possibilities first there is a case when there are an odd number of observations if there are an odd number of observations uh let's see let's zoom in so i can have a little bit more precise writing uh if there are odd number of observations after we order the data set the observation in the position n plus one divided by 2 will be the number that splits the data set in half so this will be what we're using if n is odd so as a so to think about this if we had 11 observations 11 plus 1 divided by 2 so that's 12 divided by 2 that's equal to 6. the sixth observation after you order the data set will be the median okay uh now suppose that there are an even number of observations we could potentially choose our the r so the um observation in the n over tooth position so if uh the sample size were 12 this would be the ordered observation the sixth ordered observation or we could potentially have the seventh ordered observation both of those are kind of dividing uh the center so what we'll do instead is we will take the midpoint between these two numbers which could potentially end up being the same number there's nothing that says that these two numbers are not the same but we're just going to average those two numbers take the midpoint in between them and admittedly if you had um a number line and you have some data over here and some data over here and you have these two uh observations as being potentially the median any number in between them could be defined as the median since i any of those numbers would divide the data set in half and in fact you may see alternative definitions of the median in r to take advantage of this fact um but for now it doesn't really matter if we had a lot of observations how exactly we define the median uh this is perfectly fine to just take the midpoint between uh the or the ordered observation in the position n over two and the order observation in the position n over two plus one so if we had uh twelve observations we would take the sixth and the seventh observations and average them to get the median okay let's see an example of computing the median find the median of the first 11 soccer games uh your daughter's team participated in i chose 11 just to have an even number to kind of no sorry odd number uh just for demonstration purposes um in this case uh i've our the the sort function in r uh this function will order your data set from smallest to largest uh given a vector it will order that vector from smallest to largest so we have sorted this data set we now have an order data set and i want to compute the median of that data set well first off there are n equals 11 observations in this data set that means that the observation in the middle will be m plus one over 2 which is 12 over 2 which equals 6. therefore the median will be the sixth ordered observation which is let's see one two three four five six si five so it will be five that will be the medium if we were to look at some r code there is a function an r function called median and given that data set by the way i didn't mention this before this right here uh this square bracket stuff this is a subsetting notation uh it basically translates to get the observations uh get all observations between the first and the eleventh in this data set okay and remember that soccer itself is not ordered so we're just getting observations x1 through x11 okay but i just feed that vector to the median function and it tells me that the median is 5. and by the way if you're wondering what this little one right here means uh that's just part of how r prints vectors if there were if this vector was quite long it would split over a number of lines and this would and this little one would just be tracking uh which observation you're looking at with each line so you wouldn't have so like if there was um uh 10 down so if there was like more numbers after this and then we had in square brackets 10 and saw the number six and numbers after that this would tell us that the sixth number and that that would tell us the tenth number and that back in that uh vector was six it's just something that's supposed to make reading uh what's in vectors um visually easier okay so uh i think i pressed something let's uh do that okay all right so continuing on and other measures of location are percentiles so we're going to say that the so this is the greek letter alpha and alpha is a number between 0 and 1 and we're going to include 0 and 1 as well the alpha times 100th percentile is the number such that roughly alpha times 100 percent of the data in the order data set lies to the left of that number uh so if we choose uh alpha equals 0.5 that would be uh the 50th percentile so roughly 50 of the data set lies to the left of that number which means that 50 lies to the right and what i just described is the median because the median is the observation that splits the data set in half which means that half of the data set is to the left or 50 and 50 is to the right so um the median actually counts as a percentile percentiles are a generalization of the notion of a median in fact there are other percentiles that we care about such as quartiles quartiles divide the data set up into quarters so for the first quartile roughly 25 percent of the data set rise to the left of that quartile and for the third quartile roughly 75 of the percent of the data set lies to the left of that quartile so um to visualize we would have um so we would have the first second and third quartiles and roughly 25 lies to the left of the first quartile and roughly 75 percent of the of the data set lies to the left of the third quartile and the second quartile is the quartile where roughly 50 percent lies to the left of that four quartile in other words the median again um there's you can also say that there is a zeroth quartile which corresponds to alpha equals zero alpha equals zero means that there is nothing really to the left of this observation so that would correspond to the sample minimum whereas alpha equals one means that about 100 percent of the data set lies to the left of that number that corresponds to the sample maximum there are actually a number of procedures for computing percentiles from data sets and i'm not going to repeat all of those procedures if anything i'm just going to list off the procedure that's easiest to do by hand because at the end of the day in the real world what you would do is ask r to get a percentile and r has its own algorithm for getting percentiles that's more complicated than what we're about to do and i really don't see the point in telling introductory students how to do that because you're it's it's more complicatedly intensive and like what's the point you get the if you're doing things by hand let's keep things simple um i should also probably mention um if you combine the zeroth first second third and fourth quartile fourth quartile is the maximum if you combine those together you end up with what's known as the five number summary of a data set so um here's a procedure for finding quartiles first find the median of the data set then split the data set into two data sets at the median and we're working with the order data set now so split it into do data sets at the medium if n is odd remove the data point that corresponds to the median and then to find the first quartile find the median of the lower data set and then to find the third quartile find the median of the upper data set so to visualize this procedure we have a data set find the median and then and this will split the data set into two then find the medians of the other two halves okay that will tell you what the first and third quartile are the minimum and the maximum are easy find the smallest and the largest numbers in the order data set okay so for our first example we're going to find the first and third quartiles of our of uh of this uh little girl's uh first 11 soccer games so uh let me get caught up in my notes for a second okay so to find these two quartiles how about we write down uh what that data set was just for our own purposes so we've got two three four uh five five five uh six six eight nine okay so the median was five and this is an odd number data set so we're going to delete that median six six oh oops there were two eights sorry about that so we're going to delete the median which is five and then we have split the data set in two so then we find the median of the first half there is an odd number of observations here so the median will be four thus the first quartile which i will call q1 that will be four and the median of the upper data set that will be eight so that means that the third quartile will be eight and if we wanted to we can kind of fill out the five number summary saying the second quartile which is the median this that number is going to be five the zeroth quartile which is the minimum well that's going to be 2. 2 is the smallest number and the 4th quartile which is the maximum well the largest number in the data set is 9 so that will be 9. okay okay next example find the 10th and 90th percentiles of the height data i have listed the data for you below in order so the data set oh yeah this is actually what i was getting to um regarding those little numbers in square brackets this is the ninth observation right so that means that this number right here is the tenth observation in the data set so that means that this data set has n equals 10 observations uh so 10th percentile so roughly 10 percent of the data set lies to the left of that number so uh 0.1 times 10 is equal to one so that's about uh so about one number lies there so we're going to choose 5.05 so uh 5.05 will be the 10th percentile which we will call uh we'll call that q hat point uh 10 and uh for the 90th percentile so 0.9 times 10 that's going to be 9 so that's observation number 9. so 90 of the data set lies including that number to the left of 5.63 so we'll say that 5.63 will be the 90th percentile which we'll call uh q q hat 90. okay and actually r has functions for computing uh quantiles another word for what we're talking about here are quantiles here i have asked r specifically to give me the 25th and 75th percentiles it is not using this procedure that i described it is using a different procedure for finding quantiles there's a number of different procedures for finding quantiles and percentiles and if your sample size is large nobody cares which one you use really um it really only matters at smaller sample sizes which procedure exactly you use and there are various different motivations um for different kinds of procedures there's various ways to solve the same problem and what i described is simple enough to do by hand so if you're going to do it by hand go ahead and use what i used this is probably uh whatever came up with this is probably much more complicated and if you're going to do if you need something more complicated then don't do it by hand that's hard to do it so you can read rs documentation to see what exactly is being done here it's using some sort of interpolation trick so here i've asked so the second parameter here this is a vector that contains the numbers 0.25 and 0.75 corresponding to the 25th and 75th percentiles or quantiles i'm not really sure what the difference between those two words is um here i ask for the 10th and 90th quantiles or percentiles and it gives me numbers these are all pretty close to what i came up with before okay uh next up uh let's kind of we we've come up with some measures of location and there's actually a number of different measures of location like for example i have seen a measure where you take the largest and the smallest observations so you could say that um so you could say that's r n m minus r one uh no plus r one so take the midpoint between the largest and the smallest observations that's actually another measure of location there's a number of different measures of location what i've shown here so far is fine but let's go ahead and start comparing these different methods for uh for measuring for describing the location of a data set so the sample mean x bar is known to be sensitive to outliers which means that outliers the data set have a profound effect on the sample mean so if you had for example a data set that consists of one one uh let's say one two three the average of that data set or the mean of that data set will be two and the median of that data set will also be two uh compare that to a data set that contains the numbers one to one thousand so uh the mean of that is going to be about 500 which is much larger than 2 what it was before compare that to the median the median of that data set 1 to 1000 is still 2 which means that the median is insensitive to outliers it basically doesn't care what they are all it cares about is the ordering of the observations so long as an outlet if you were to change the value of an outlier so long as it doesn't change the ordering of the data set the median will not change so as an example of this i'm going to compute the sample mean and the sample mean of the daughter's uh uh soccer games if i throw in a 12th soccer game and i'm going to consider a number of different scenarios where her 12th game was 1 point or one goal of four goals two goals and eighteen goals we're going to consider that and what i actually did here was um i created a vector that contains these uh that contains these so uh i'm not gonna talk too much more about this because i wanna focus on the math um so uh first off when i add up the 11 other games when i add those up i end up with a cumulative score of 61. so let's create a table for all of these potential outliers so in this table uh we're going to consider when the outlier when the 12th game is 1 when it's 4 when it's 2 and when it's 18 and we're going to have a column for the sum of the observations we're going to have a column for the sample mean and a column for the sample median so in the case where the 12th game is one this will add up to 62. and by the way i'm using the word outlier for one but one in this case would not be an outlier same with 4 and 2 but 18 certainly would be considered an outlier so if her 12th game is 1 then it adds up to 62. if her 12th game is 4 it adds up to 65. if her 12th game is 2 the game's cumulative score is going to be not 62. uh 63 and if it's 18 they all add up to 79. and then we're going to take these sums and divide them by 12. in the end we end up with uh in the first case we get a sample mean of 5.16 uh in the second case we get a sample mean of 5.416 uh repeating six uh in the third case we get 5.25 and in the fourth case we get 6.58 which is uh much different from what we had before uh hold on i think my pen might be okay my pen's back is it though might need to charge okay now the median in the first case uh so since uh so in the first case the median is still going to be five and in fact it's going to be five for the first three cases because the median was five and it didn't change the order of the data set if you go back and look at the original data set we're essentially in in these first three cases we shift all the numbers to uh the left which means that the median's gonna be uh the number to the left of what it was before which was five but if we chose or it's actually going to be the average of five and five which is still five but in the last case uh the median actually will be the average of five and six so it will be 5.5 so the median did change a little bit um in the last case but it's mostly because of where that outlier ended up it ended up on the right hand side of the data set or the right hand side of what the median was before and it could have been 273 and the median would be exactly the same but as we can see the median isn't changing really at all compare especially when you compare it to the mean and here is some r code that the idea of this code is i'm going through a loop adding this observation to a to a version of the data set and then uh computing the uh median and the mean of that data set the result will be an r matrix i take the transpose of that matrix because that's the version of the matrix that i prefer i'm giving the matrix some row names and column names do some rounding and this is a resulting matrix and it's pretty similar to what we had before in fact there is a relationship that we can say in general between the mean and the median depending on whether the data is negatively skewed positively skewed or symmetric if the data set is nic is a positively skewed so that would so the data set looks roughly like this then the me the median which is the point that devas divides the data set roughly in half will be less than the mean and that's because the mean is going to try to chase the outliers the outliers are going to be on the right hand side of the bulk of the data so in the case of negatively skewed data we're going to have the opposite relationship where the median since the outliers are going to be on the left hand side of the bulk of the data the mean is going to chase the smaller numbers so the me the median will tend to be greater than the mean and in the case of a symmetric data set the mean and the median should be approximately the same in real data i mean probabilistically when we talk about means and medians and probability uh they will be exactly the same when the date when the distribution of the data is symmetric but in real data that's never the case in real data they will just be close and what it means to be close is is like that's a judgment call right um so what would that some implications for that thinking back to some examples of positively skewed and negatively skewed data sets i said that a positively skewed data set is incomes this relationship means um this relationship means that the average income tends to be larger than the median income and economists generally prefer to use the median income for income distributions because it seems inappropriate to use the mean this kind of gets to the issue of i've given you these competing measures for means and for for me for measuring the location of a data set which one of these measures should you use i would say use the one that's appropriate which means um well here's one thing once you use the mean uh i would say you should use the mean when large observations are allowed to compensate for small ones let's say for example that you're gambling if you're gambling what you care about are your mean earnings and not your median earnings because it's okay for you to win nothing 99 times if you win a million or i guess it also depends on how how much this game is worth but let's say that you're playing a game that costs a dollar each time you play it's okay for you to win nothing 99 times if on the 100th time you win a million dollars that would be awesome for you whereas in that situation the median would be zero dollars and if you were judging by the median how well you were doing you would think you were actually doing poorly so if you're allowing large observations or small observations to to maybe uh replace or detract from the overall score then the mean is appropriate on the other hand the median in the case of uh social sciences we care more about what like what fifty percent of the obs of the population is experiencing and we don't necessarily want to allow uh like the very the the uh the um the great fortune of the wealthy to uh compensate for uh the great poverty of the very poor so in that situation the medium would be the preferred observation and of course if theoretically what you're trying to measure is the population median then you should use the sample median if you're trying to estimate the population mean you should use the sample mean and in which case you should not be using the opposite now there are some exceptions to this and we'll talk about this when we talk about probability and talk about um hypothesis testing there's notions such as most powerful tests like if your data set was symmetric and you knew it came from a normal distribution you should always use the mean even even if what you care about is the median uh but and the reason why that is is because for a normal distribution the two numbers are the same uh but um we're just going to leave that for now um so um and there so all of this was talking about uh sample results uh there are analogous population numbers too okay so um i just want to very quickly satisfy my nervousness okay we're still streaming okay um all right there is another number another measure for location called uh the trimmed mean so i'm going to comp i'm going to tell you about the trim bean and i'm even going to compute it for you but then i'm going to uh have some caveats about using it um you probably should not be using uh the trimmed mean and i'll explain that in a second but the idea of the trend mean is we have the stat we have the median and the median is not sensitive to outliers which is generally considered a blessing but it's not always a good thing because if it feels a little inappropriate to throw out so much data when you're computing the median because once you know the ordering of the data and you know which two which one or two numbers are in the middle then the other day the rest of the data doesn't matter and that feels a little extreme on the other hand you're a little bothered by the means sensitivity to outliers and furthermore on outliers you might think should we throw out outliers should we ignore outliers because that's actually kind of what the truth mean is suggesting that we should do with the trimmed mean what we're going to do is we're going to uh use only um uh we're going to throw out a certain percentage of the data like we're going to throw out 10 percent on the lefthand side and 10 on the righthand side of the data set so throw out the 10 10 percent of the smallest numbers and or the um so in this data set ten percent of the numbers that are the smallest numbers that the data set and ten percent of the numbers are the large larger numbers in the data set that would be the uh trend mean where alpha equals point one where you're trimming at ten 10 on each end um so the idea of the trend mean is throw out the outliers um so on this issue of whether you should throw out outliers you should actually think very carefully before you throw out outliers in general if you're competing a trimmed mean then that's kind of what you're doing but let's say you look at a data set and you see that your estimators are actually very influenced by a couple outliers and you're thinking maybe i should just throw those out i would first ask why are you throwing them out are you throwing it out because uh you think the number is erroneous because i in my own experience have seen numbers in data sets where it's like that's probably an error someone probably entered the wrong number so i'm going to throw it out um like when someone writes in the american community survey that someone's made a trillion dollars it's like that's probably not correct um you should just throw it out um if that's what's going on go ahead and throw it out because the reason why you're throwing it out is because of data contamination but if you're throwing it out just because it's causing bad behavior in your estimators that is probably inappropriate and you should instead try to model the outlier or just accept it rather than throw it out or think harder about why it is that you are using the mean rather than the median i would actually suggest that over throwing the outlier out but anyway um let's go ahead and compute uh the trimmed mean for the height data set so let's go ahead and rewrite that data set we've got uh i don't want blue okay so i've got for the height data set 5.05 you can go ahead and like skip ahead a little bit to skip me writing down numbers so 5.05 5.13 5.21 5.3 uh 5.3 this splits the data set in half so the next number is 5.38 and we've got 5.38 again 5.55 5.63 and 5.96 okay so there are 10 numbers in this data set if we're going to trim so if we're going to say alpha equals 0.1 so we're going to trim 10 of the data off on each end so trim ten percent at each end that means that we're going to end up trimming uh 0.1 times 10 where 10 is the sample size which is 1. we're going to trim off the smallest and the largest number in this data set so that would be that would be 5.05 and 5.96 and then take the average of the remaining data set so in that case x bar where we trim off uh 10 percent will be the average of the eight remaining numbers where we're sum from i equals two to nine uh the order data set um and that is going to end up being uh 42 so the sum is going to be 42.88 uh and then we divide that by eight and the result will be uh 5.36 so 5.36 feet because this data set is in feet we're we're talking about height okay and uh r can compute trimmed means in fact you can just use the mean function like we had before we're just going to pass it an additional parameter that tells the function to trim now i mentioned a little while back you actually probably should not be computing trimmed means and the reason why is because when we compute a median which i a a little side note i guess the median both the mean and the median count as particular trimmed means where the median is like the trend mean where you trim off 50 percent and the mean is where it is the trimmed beam where you turn off zero percent so the trend being kind of generalizes these other two statistics um but you probably should only use those other two statistics you should probably not use the trim mean i mean i guess if what you were doing is instead of taking off 10 on either end it's like always take off uh the two largest and two smallest numbers that could be appropriate um from a theoretical perspective uh so because basically as you increase the sample size the number of observations that you're trimming off becomes very small relative to the rest of the sample but trimming off ten percent at either end from a theoretical standing is a little odd and the reason why is that there is actually a population median that we are estimating when we are using a sample median and there is a population mean that we are estimating when we use a sample mean and both of those quantities are very well understood but when you're using a trend mean you are estimating neither of those things you're estimating some weird hybrid a monstrous monstrous population statistic that we don't necessarily understand you're estimating essentially the population version of a trimmed mean and it's questionable whether that's actually what you want it seems like the worst of both worlds in that case because no one can actually like why would we talk about the population except for the 10 largest and 10 smallest numbers like that doesn't really make a whole lot of sense so for that reason unless you are actually i would actually more advocate for like a fixed trimming where you take off the two largest and two smallest observations um although at that point you probably should just use the mean or use the median i probably would not use the trimmed mean so okay so that's it for this section in the next section we will be discussing uh measures of variability so um i will cut it off here and have a good day okay so this section is about measures of variability so last section we discussed measures of location let's start by justifying why we need measures of variability consider these three data sets and i'm going to construct a dot plot for each of these data sets so i've got uh three lines for my three dot plots data set one data set two data set three uh and in the these dot plots i'm going to uh start uh with one and it's ending the twelve and uh in between i've got six so so let's see i'm going to have we're going to keep these all on the same scale so 1 12 6 1 12. uh six and uh we'll go one two three four five six seven eight nine ten 12 okay that's one two three four oops five six okay that's that's that's that's a little inexcusable we're gonna have to try a little harder on that one uh one two three four 5 6 7 8 9 10 11 12. okay 1 2 3 4 5 6 7 8 9 10 11 12. okay so i've now got these three number lines and let's start with data set one so we've got numbers at uh so four five six seven eight and then we've got one at uh two five six seven ten and then finally we have one three six 9 9 and then 11. okay so look at these three dot plots now i want you to let's let's first start actually by uh computing the mean and the medium for each of these data sets so data set one four plus five plus six plus seven plus eight plus nine uh that is going to add up to 36 the second data set well we subtract 2 from 4 to get 2 but then add 2 to 8 to get 10. so that second one is also going to add up to 36. and for the third one you kind of are going to do the same trick so they all add up to 36 which then means that the sample mean is going to be 36 divided by there's six observations no actually there's five uh oh i'm sorry they don't add up to 36 they don't add up to 36. oh silly curtis silly curtis they added to 30 so the sampling would be 30 divided by 5 which equals 6 which is also equal to the median because you look at them because these data sets are ordered they have five observations so the third row is going to correspond to the median so that means that the mean and the median for these data sets are all the same and yet let's suppose now that i were to ask tell you that this was the waiting time for the train uh which of these data sets would you prefer to be the observed waiting times for the train probably the first one at least if you're like me because for myself i actually did not really like inconsistent trains i mean it's kind of cool that this train will there might be a oneminute waiting time uh for this train but there also could be an 11 minute waiting time for this train and one way or the other i would just love it if trains always showed up exactly six minutes um between which okay admittedly around here they generally do do that but um you don't really like a lot of variability in the wait time for the train because that makes the train unreliable that said this aspect of the data set is not being captured by uh our measures of location the sample mean and the sample median unfortunately so and the reason why is because the attribute that we're talking about is an attribute that doesn't have anything to do with the location these data sets are located at essentially the same place they it has to do instead with spread and we now have a pictographic method for understanding spread we can see that these data sets have different spread but we would like to have some numerical measures so very quickly i'm just going to say that i would prefer one because it's more consistent or less spread okay what we need is a measure of variability to describe how spread out a data set is uh how could we possibly do that well we might start by examining deviations which where we look at x i and subtract out the sample mean and and if we were to add these up together this might give us a measure for um how spread out the data set is here's the thing though when we try that um we're going to sum up from i equals 1 to n uh no that should be a 1. so from i equals 1 to n x i minus x bar and this is a sum sums are linear which means that i can now break up this sum into two sums and say that this is going to be a sum from i equals one to n x i minus um the sum from i equals one to n x bar but here's the thing about that ladder sum see this is actually adding up a constant n times and you probably remember from second grade what it means to add up the same number and times you end up with multiplication so this number is going to actually end up being n times x bar okay and this number also can be interpreted as n times x bar because it is the sum of the observations divided by the sample size and then multiplied by the sample size again so we end up with n x bar minus n x bar and that equals zero so what that means is that this quantity is always equal to zero always equal to zero i think i found a dead spot on my screen okay so that always adds up to zero uh which means i mean the issue is that these deviations they always have the same sign well okay the they they all have um they all right what i just said was literally false um they don't always have the same sign in fact uh you have opposite signs you have some positive some negative deviations and it turns out that uh the positive and negative deviations cancel each other out so you end up with a zero um so that didn't quite work although there was an interesting idea there um looking at the distance between an observation and the sample mean and one might be tempted to try this replace the parentheses with absolute values so you end up adding up the absolute value of x i minus x bar and um that now you don't have that issue of negatives and positives canceling each other out because everything will be positive and you'll end up with a positive number and that makes sense um the thing though is this is actually more difficult for a mathematical perspective to work with and the reason why is because it's involving absolute values and absolute values are not differentiable absolute values if you remember from calculus 1 they have a cusp of a sharp point and sharp points are not differentiable compare that instead to so this is like the absolute value of x compare that instead with the function x squared that should work i mean that that has the uh nice feature of being differentiable so what actually statisticians end up doing is they say we should add up the sum from i equals 1 to n x i minus x bar squared and this quantity is known as the sum of squared errors x i minus x bar a term that statisticians like to use for that is the error and um we're adding up the squared errors and there is in fact an interpretation for uh for the square part which is maybe you remember this is how i like to think of it i think that this formula kind of rhymes with x1 minus x2 squared plus y1 minus y2 do you remember that do you remember that from geometry class if you take the square root of this quantity you end up with the distance formula for euclidean distance for euclidean geometry and it actually kind of rhymes with that sum that i've drawn that i've uh shown uh up above so uh and in fact there is um a very deep connection between uh the sum of squared errors and euclidean geometry um but this quantity to me like it that seems like an appropriate way to think about distance and if we if we were to average this by by saying like this is one over n we would have an average squared distance now that's actually a good idea but um there's a better idea which is to divide instead of by n by n minus one now that might strike you as a little bit odd why is it that we're dividing by n minus one uh there's a few reasons for that some of which we'll talk about later in maybe chapter six but uh n minus 1 there's actually a term for this quantity and it's known in this in this context as the degrees of freedom and why are we dividing by the degrees of freedom rather than n i'm going to present to you a few arguments for why you'd want to do that for starters let's imagine that we had a data set of size 1. right so there's only one observation or data set what we're trying to measure right now is is a spread in the data set if we had a data set of size one is there really any way to estimate spread how can you determine the spread from a data set of one observation um that doesn't really make a whole lot of sense and it seems like something has fundamentally gone wrong in that situation and when you divide by n it's not going to reveal that something is wrong but when you divide by n minus 1 a sample size of 1 is explicitly forbidden because you cannot divide by 0. okay so that's one way to think that that's one way to think that maybe n minus one is more appropriate um and then uh uh secondly uh what we're actually doing with this number we actually call this we we've given this number a name as statisticians this is known as the variance the sample variance now maybe you recall from previous sections uh my saying that there is a sample meaning you can actually talk about a population mean and there's a sample median and you could talk about a population median and there is a sense in which the sample mean estimates the population mean and the media and the sample median estimates the population median so the sample variance should estimate the population variance there is in fact a population variance but here's the thing though about the population variance um our estimator if we were to divide by n would have a tendency to be too small i mean it would still be close to the population variance but you could be a little bit better by dividing by n minus 1 instead of n if you were to divide by n you'd actually be a little too small so we should divide by minus 1 instead there's a term called biasedness that we will discuss more in chapter six but long story short it turns out that when you divide by n minus one you have an unbiased estimator for the population variance whereas if you divide by n there is a very small bias now that said you can still divide by n and have a reasonable estimator it just will have that bias problem the bias gets really small as you increase the sample size but it is still there so why not just get rid of it um so these are some potential arguments for why you should be dividing by n minus one and later on in ch in that chapter we will actually uh i may actually show that if you compute the expected value and you divide by n minus one you get the sample variance but uh we're a long ways off from that so accept it that you pretty much have to divide by n minus one instead of and although it is still reasonable to think of this as um uh like an average squared distance oh yes another argument for why you should uh be dividing by n minus one uh when you have when you compute the variance there's something you have to do first you have to compute the sample mean you have to compute the sample mean first and there's a penalty that you have to pay for that the term degrees of freedom means that if you this is basically the number of observations in the data set that you are allowed to change um and where you can change those observations um freely and you could still end up with the same sample mean um it because it turns out for basically the reason that i showed up here uh this this this line of reasoning that um if you know n minus one of the observations and you know the sample mean then you know the nth observation that you didn't list out before so the sample mean contains information and the fact that you had to estimate a parameter before you could estimate the sample variance means that you need to divide that it means that there's in some sense a penalty to your sample size um so it's inappropriate to divide by n minus one you now need to do or to divide by n you now need to divide by n minus one all right now here's the thing about the sample variants that we don't like uh think about the units of these things let's say that we were talking about feet or going back to some examples or even for this uh soccer data set that i've seen in a few videos in the past where we're tracking the goals scored by a little league soccer team uh if we were talking about goals then this right here is a goal for a game so it's units or goals the sample mean is also in goals because you add up goals divided by something without units you end up with goals and you have goals minus goals so you still in that difference have goals but then you square and you end up with goals squared what the heck are skull are goals squared that's a unit that doesn't mean anything to us we don't like the fact that in the end the sample variance produces squared units we would rather have an s some measure of spread that is in the same units as the data set and there is such a measure called the sample standard deviation so the sample standard deviation is s which is equal to the square root of s squared so s squared is the sample variance s is the sampled standard deviation so we'll call that sd right i mean it's right there so uh the sample standard deviation so yes since you've taken the square root of the variance uh you now take the square root of gold squared and now end up the with the unit goals which is what you want so the sample standard deviation will be in the same units as the data set and we like that um furthermore it is still reasonable to think of the sample standard deviation as measuring the average distance of an observation from the mean or a typical distance all right so continue on uh there are in fact population analogs to these quantities uh such as the um such as uh the population variance and the population standard deviation and those will be discussions for a later chapter i believe that's chapter uh three so um now when you're computing the sample variance another way to write it if we write if we define s x x as the sum from y equals 1 to n of x i minus x bar squared we could say that the sample variance which is what you need to compute for the sample mean is equal to s x x divided by n minus 1. the thing though is a lot of people don't like to compute the deviations and then square them so compute the mean and then compute the deviations and by subtracting the mean from the observations and squaring them people don't seem to like to do that so when doing stuff by hand it's often easier to use this shortcut formula where you add up the observations squared and then subtract uh the mean squared multiplied with n and it is in fact possible to show and because this is the second time i'm recording this video i'm not going to show it because i'm tired um it is possible to show that these two quantities are the same and i would say i'm going to leave this as an exercise to you if you are curious if you're uh thinking you're probably going to take some more advanced stats classes why don't you take a second to show uh that these two quantities uh the uh some the sum of squared errors and the shortcut formula are the same um but i'm just going to leave it for now that these are in fact the same number so that gives that could help you potentially save some time when computing the sample variance by hand okay so uh let's start out let's now start looking at examples in example 14 we're going to compute the sample variance and sample standard deviation of the soccer game scores so here are the scores let's set again uh length is an r function uh length of so sock or in r is known as a vector and the length of the v of a vector will tell you how many objects are in that vector so um there's also an r function called summary which will give you uh some basic statistical summaries for a data set stored uh in a vector or actually summary is something we'll give you some basic statistical information about lots of things uh but that we're going to leave that for the r lab for now it's just giving us some basic statistics for an a vector and i'm now going to compute uh the sample variance of this uh data set okay so uh i like to create a table uh when computing this by hand if you don't want to watch me compute this by hand uh because it is kind of a tedious calculation if you don't want to watch it this is a part that you can skip over um all right anyway so um we have 12 observations in our data set so i'm going to start numbering off 1 2 3 4 5 6 just to track the observations 7 8 9 10 11 12. okay we have an observation and we have an observation uh hold on another dead spot we have an observation squared okay so uh observations in our data set we had uh nine six five five five uh six two eight uh three four eight one and then we're going to square each of these observations so we'll get 81 36 25 three times uh 36 for uh 64 9 16 64 and one okay um if you're out also like you kind of want to work on this by hand a little bit but you don't want to completely trivialize the problem by going to r and asking for the variance and standard deviation because it'll just give it to you uh maybe this would be something to work on in excel because i because what i'm basically doing is being a a human excel spreadsheet at the very bottom i'm going to sum up these two columns the first column sums up to 62 and the second column consisting of squares sums up to 386. so now i want to compute the sum of squared errors and that's sxx and we have our shortcut formula for computing that that's going to be 386 which is the sum of the squares of the observations minus 12 which is the sample size times the mean all right we need to compute the mean so the mean is going to be the sum of the observations which is 62 divided by 12. so in this parentheses i'm going to put 62 over 12 and square it and you plug into calculator and the number that you get for the sum of squared errors is 197 divided by 3 which is as a decimal number uh 65.6 where the six is repeating so the sample variance will be the sum of squared errors divided by as a reminder the sample size minus 1 which is going to be 65.6 divided by 12 minus 1 which is 11 which is equal to 5.99 where the 9 6 itself is repeating now this is nice but the thing is the sample variance is in the units of the sample variance is gold squared we don't like that we want to compute the standard deviation too because that's a more interpretable number so the standard deviation is going to be the square root of the variance which is going to be about uh 2.443 into three decimal places so you can think of this as your uh daughter's soccer team is varying around their average uh score of about five points but they're they're deviating from that by about two points so on average they'll be about two points away okay all right uh so in r the functions that are responsible for computing these quantities are var and sd r computes the variance and sd computes the standard deviation so var of the soccer data set is basically what i wrote down and the standard deviation it's basically the same thing too all right so um the sample mean so for the sample variance we actually have some nice properties that also translate into properties for uh the standard deviation actually before i continue on i'm going to double check because this scares me okay okay everything is good i'm scared okay um so uh some basic properties uh let's suppose for in this proposition uh let's suppose that we take our data set and then we shift everything by a constant to produce a new data set that's shifted by a constant it turns out that the set the uh sample variance for the new set data set will be the same as the sample variance for the old data set where you didn't shift uh that's a good thing what that means is basically this is in fact a measure of spread if it wasn't a measure of spread uh well basically if this was not the case if the sample variance changed uh by shifting the data then it doesn't seem really fair to uh call it a measure of of spread because it's also capturing location too but the fact that you don't have to uh the fact that it doesn't care about the location or in a way it doesn't actually care about the mean because and you can think of that as because it subtracts the means out the mean out for the data set uh the fact that it doesn't care means it is in fact a bona fide measure of spread and not measuring something else along with it the second proposition says that the variance uh if you were to rescale your data set by c uh the variance will scale by c squared and the standard deviation will scale by the absolute value of c so um the standard deviation is always going to be positive uh the variance will always be positive um so when you rescale the data set it's not going to the whether you multiply by a positive or a negative number doesn't actually matter and also it tells you that um if like you can think of this as saying something about uh unit conversions uh because remember that unit conversions uh generally and are multiplicative operations if you wanted to change the units of the standard deviation um you could do so by just multiplying the original standard deviation by whatever unit conversion formula you have and also this is basically telling us what i was saying before that the variance is in units squared but the standard deviation is in just the same units as the data set so these are good properties to be aware of um okay so the uh the sample variance and standard deviation uh these are one uh these are these are these are one class of uh estimator of spread they're not the only ones oh by the way you we did have this discussion when talking about measures of location about biasedness no no not biasedness um sensitivity to outliers it turns out that the sample mean and the sample standard the sample variance and sample standard deviation are also sensitive to outliers in fact they are more sensitive to outliers than the mean is so they care a great deal about outliers too um just throwing that out there and you can kind of tell by looking at those formulas since uh when you look at them they're basically means right they're averages or at least the variance looks like an average and the standard deviation is the square root of an average so uh if you end up having a very large error then that's going to make your variance very large so it's sensitive to outliers just mentioning that uh another measure for spread is known as the fourth spread or sometimes like in math 1070 we call the interquartile range it is the third quartile minus the first quartile and we're going to denote it in this class with fs this is another measure of dispersion so let's compute the fourth spread for the soccer game scores uh we already computed in uh in a previous uh video uh the third and first quartile for the soccer game for the soccer games so the fourth spread will be the third quartile minus the first quartile which actually turns out for this data set to be eight minus four which is equal to four okay so the fourth spread is in and of itself a measure of dispersion uh and one way statisticians might use the fourth spread is as a tool for outlier detection remember we care a great deal about outliers uh if there's an out we would like to have outlier detection tools because if there is an outlier in this data set we would like to investigate it further and decide how we should approach it and why the outlier is there outliers are very interesting aspects of data sets so we would like to be able to detect them so we might call an observation that is further than one and a half times the fourth spread from its nearest quartile a mild outlier so as an example uh let's suppose we have a data set our data set looks something like this here's kind of a a dot plot sketch of our data set and we've also got a couple observations over here so those two observations visually look like outliers what the for let's suppose that we have the uh oh i didn't realize that was the thing okay uh let's suppose that uh our first and third quartiles are here and here okay um if that is the case uh the fourth spread or the iqr is going to be the distance between uh those two quartiles so we'll call this q1 q3 uh the distance between those will be the fourth spread so according to this rule how you detect an outlier is you take this quantity and then increase it by uh one so one and a half and go beyond and you're gonna like see compare an observation to its nearest quartile so these are going to be close to the third quartile because they're above the third quartile um and you um compare see if those observations are one and a half times the iqr away from their nearest quartile and if they are beyond that range then they are candidates to be outliers so these are now starting to look like at least mild outliers but in fact if we were to uh double that quantity so three times the fourth spread um so that would look like this turns out that they are beyond three times the fourth spread as well so now these are looking like extreme outliers we could also do the same thing on the left hand side of the data set look for outliers that are you know numbers that are really small there's nothing that says that outliers have to always be large numbers they can also be really small numbers too uh and there's no outliers on the lefthand side of this data set i should point out it's tempting for students to think that what i just gave you as a definition for outliers the word outlier is intentionally vague because there's many different ways you could define an outlier this is one such definition or one such a criterion for deciding if something is an outlier this criterion would not work if we were to go into two dimensions it is a onedimensional onedimensional approach not a twodimensional approach and you can still have uh outliers in two dimensions or in bivariate data and uh um you can still have outliers there and they're going to behave like there's more possibilities the moment you've moved on to a plane as opposed to just a number line more ways for things to be outliers um so um i i'm i'm i'm hesitant to allow to just let students think that this is what an outlier is there's actually different ways to think about outliers we could come up with different definitions based off of our problem and how we want to approach it so so we could we could choose a procedure that is tailored to our problem to define outliers so that it's most useful to us so this is what we're using in this class but it is far from like what we'd always use okay and it's not really the definition of an outlier we would just say that an outlier is a point that that seems unusual to the other points that seems to be distant in some way from the other points or it doesn't seem to follow the same pattern as the rest of the data okay so moving on into example 16 use the fourth spread to detect outliers and soccer game scores what is the minimum score needed for a data point to be a mile outlier or an extreme outlier so the force spread as you may recall from above was four so 1.5 and i don't want that green that color all right so 1.5 times the fourth spread is going to be 4 times 1.5 which is going to be 6 and 3 times the fourth spread is going to be 12. so uh let's see what it would take for something to be considered at least a mild outlier to be mild you would have to possibly exceed the third quartile plus 1.5 times the fourth spread which is going to be eight plus six which is 14. there were no double digit scores in our soccer data set so that means that there were no outliers at least on the positive end and as for the negative end uh in order for something to be so small that it's an hour it would have to be less than q1 minus uh 1.5 times the force spread which is going to be uh that's going to be 4 minus 6 which is negative 2. well negative soccer scores are impossible so there's not going to be any mild outliers on the left hand side which means that there are no outliers in this data set now that's it let's let's go ahead and continue on just just for fun let's see what it would take for something to be an extreme outlier to be an extreme outlier you would have to exceed q3 plus 3fs which is 8 plus 12 which is uh 20. so in other words the other team didn't show up to be on the left hand side you'd uh to be an outlier on the lefthand side you have to be less than q1 minus three fs which is um uh four minus 12 uh which is negative which is negative eight and no way that's going to happen so there are no outliers in our data set okay so uh there's another visualization method that i would like to discuss that we weren't able to discuss in section two the reason why is because this is a box plot and box plots require uh the five number summary which is the minimum max so the five number summary is the minimum uh maximum median first and third quartiles so you need to have that in order to be able to compute a box plot and create a box plot that's why we didn't talk about before because we hadn't actually talked about those things uh so um so for a box plot we first compute those quantities uh on a number line which could this could be a vertical number line or it could be a horizontal number line if you want uh box plots oriented horizontally or oriented vertically either one is fine whatever whatever suits your needs do whatever you want okay but on a number line let's say something like this we're going to draw a box so we've got the first quartile and the third quartile we're going to draw a box whose ends are at those quartiles we're also going to draw a line in that box corresponding to the location of the median we will then draw what are known as whiskers that extend out to the maximum and to the minimum so the whiskers will stand out to extend out to the extrema of the data set um and that's that's a box plot now i should point out that r does not draw a box plus this way by default r does something different r will actually try to detect outliers and it will draw the whiskers out to the observations that are not outliers so the largest observation that is on outlier and the smallest observation that is not an outlier the outliers are treated differently they get their own points in the box plot kind of like with the dot plot uh that's more complicated to do and i'm not going to ask you to do that uh using just the five number summary and extending out to the maxima is fine for me and i feel like that if you were ever to draw a box plot by hand you should just keep things simple because you can it's not too hard to compute a five number summary if you especially if you had say um uh if you had like a stemandleaf plot but a box plot like competing the outliers is a bit much so a box plot on its own i mean it's okay but a box plot really shines when there are other box plots with it and when you have that you can now start drawing comparative box plots and when you have comparative box plots you can start to say things about the relationships amongst different groups that uh like if you want to compare different data sets you may have a data you may have two data sets for two similar but not the same populations uh like for example men and women men and women are both human uh but if you were to compare height you would probably want to differentiate between men and women so you could have a box plot for men's heights and you can have a box plot for women's heights and then you can make comparisons and you could compare in this case this looks like uh if if this were in fact talking about men's and women's heights this would be suggesting that men tend to be taller than women um so one thing that you could do when looking at a comparative box plot is compare look the location of the boxes you can also compare the spreads of the boxes so we for example if we saw one box plot that looks like this for one group and a box plot that looks like this for another group we might say that those two groups certainly have different spread okay so you can start making comparisons that are more easily made with with plots such as these and if you were to say try to overlap density plots or something um so comparative box plots are very nice because they allow you to at a glance compare two different groups and their distributions so let's go ahead and start creating some box plots uh in this example uh we're using a data set from r uh in this example we're studying we're studying the tooth growth of guinea pigs that were given a vitamin c supplement via orange juice at three different dosage levels uh here's a bunch of r code that takes the tooth growth data set and transforms it into a format that is um uh nice for this problem where uh it did not look like this before it go ahead and look at the tooth growth data it does not look like this at all for starters the tooth growth data set it also includes a group where the guinea pigs were given vitamin c like a vitamin c supplement directly rather than through via orange juice and we've completely excluded that group using this filter command these are known as pipes they are part of the dipler package um so we um filtered so that we were looking at only orange juice we selected the length and the dosage uh as the variables we were interested in and then did some other stuff so that the data came in a format that i liked which is where it's ordered from smallest to largest for each of these three groups and we have the half dosage group full dosage group and double dosage group okay so uh all right so and the data set is ordered which means it's going to be uh somewhat easy to compute a five number summary for each of these three groups so for example so for instance uh the last row is going to be the maximum for the three groups and the first row is going to be the minimum for the three groups as for the other quantities we also need the median so there are 10 observations for each group so the median is going to be the average of the fifth and the sixth rows so uh the medians after we compute those averages or or midpoints if you prefer the medians for the half dosage group uh its median is uh 12.25 for the full dosage group it's going to be 23.45 and for the double dosage group it's going to be 25.95 okay so those are the medians now we need to compute the first quartile remember what we do is we split the data set in half and then look at the median for the smaller data set or the lower data set this will be the first quartiles and the median for the third quarter data set remember that both these data sets have five observations each after we do the splitting so the median for the upper data set that will be the third quartile okay so we now have everything we need to start constructing our box plots okay so i'm going to construct these this by hand i don't want that let's make it black isn't that a song okay oh yeah i want a painted black yeah that's right that is a song all right um i want to paint it black uh anyway uh so um i have the half dosage group the full dosage group and a the double dosage group i'm going to have my box plot end at 31 up here and we're gonna start down here at eight so we're gonna go eight uh nine ten eleven twelve 13. okay uh 14 15 16 17 18. and then we go 1920. uh 21 22 23 24 25 uh 26 27 28 29 30 31 okay all right so we've got our scale uh so for our first group uh let's zoom out a little bit so for the first group the median of the minimum was 8.2 so minimum of 8.2 we'll put a dot right there uh then we go to the first quartile so that's going to be 9.7 that's about there um then we've got 12.25 that's about there uh q3 is 16.5 that's about 13. that's about there and finally the maximum is 21.5 so that is going to be about there okay so then uh we have a box the median and the whiskers all right so there's our first box plot all right so now for full dosage uh scrolling up a little bit so for full dosage the minimum is at uh 14.5 which is about there uh then we have a quartile at 20 which is about there uh the median's at 23.445 so that's about there uh q3 is at 25.8 so that's about there um and then uh the maximum is at 27.3 so that's about there okay so draw the box the line for the median and draw out the whiskers okay and finally for the double dosage group the minimum is at 22.5 which is about there uh the first quartile is at 24.5 which is about there uh the median is at 25.95 which is about there uh the third quartile is at 27.3 which is about uh there and the maximum is at uh 30.9 which is about there so draw the box draw the line for the median extend out the whiskers and there we go all right and now we have a box plot a comparative box plot and what can we see we can see that in fact increasing the dosage does seem to increase the tooth growth length we also see that there's much more spread in the half and full dose than there is for the double dose which is an interesting fact uh all right so moving on uh there is um an r function called box plot that can construct these box plots for you this is largely in agreement with what we drew um and as a reminder r doesn't by default uh produce box plots in the way that i just described where it draws whiskers out to the minimum and the maximum it does something a little bit different where it will um uh draw it out to the largest and smallest observations that are not outliers and then draw the outliers as their own individual points okay uh there's actually one more visualization that's similar to a box plot that i would like to discuss um i didn't discuss it in the lecture notes because you can't really draw it by hand uh but i've got a computer in front of me okay very quickly okay everything seems to be fine okay um so you can't really draw it by hand uh because oh what was that hey students let's get started with the chapter on probability probability is the mathematical study of randomness and uncertain outcomes so the subject in fact may be about as old as calculus at some level humans have known about probability for a very long time it's just it wasn't until around the time of probability that we saw some of the first uh semirigorous treatments of probability and then probability really became a serious mathematical subject around uh the beginning of the 20th 20th century uh when a mathematician by the name of kolmogorov rooted probability theory in in the in some real analysis theory so he developed a set of axioms that made it a rigorous uh mathematical subject in its own right and here we are today and statistics relies very heavily on probability we've seen in the previous chapter quantities such as the mean and the median we saw all these sample statistics we discussed the ideas of a sample and a sample's relationship to a population but it's hard you can't really say much more than that and really can't have a rigorous discussion about different uh sample statistics without having a probability theory to back it up so we're gonna start with that right now we're gonna start with section one on sample spaces and events so we start out with the idea of an experiment and experiment is an activity or process with an uncertain outcome examples of experiments including flipping a coin or flipping a coin until the coin lands heads up or you could have rolling a die a six sided die or a rolling two six sided die or you could even have something a bit more abstract such as uh the time in the morning that you wake up that can also be understood via probability theory so when we have an experiment that we have described narratively in a sense so i say i'm going to flip a coin or i'm going to flip a coin until it lands heads up after we have an experiment we need to describe the sample space which we are going to denote in this class with the letter s although i should point out that at least in my experience omega the greek letter omega is more common um notation for the sample space um but this is fine uh s is fine um so this will be the sample space is the set of all possible outcomes of the experiment the sample space is defined by the person who's developing this probability model so it basically you say what the sample space is and you're going to pick a sample space that seems appropriate to the phenomena that you wish to describe a set is very loosely defined as a collection of of objects uh actually this definition of set is bad um because it's possible using just the idea of a collection of objects to construct impossible sets uh sets that are like it's impossible in the sense of being contradictory to itself so uh there's this uh area of mathematics called axiomatic set theory that actually develops a rigorous notion of sets that largely allows for sets that we'd like to think of but honestly uh for our purposes this is f this is definitely overkill uh just thinking of set as a collection of objects is uh fine for us events are subsets of the sample space defining possible outcomes of an experiment we automatically get an event called or a subset called the empty set or the null event uh which is noted with this notation this is a set with no members it can be thought of as an event of as the event where nothing happens and that is precisely how you should think about it i might uh create a separate video describing what precisely the empty set is and kind of try to dispel some inclinations of students to try to assign some deeper meaning to what to the empty set it's like no no no the empty set is a set with nothing in it and you really cannot call it anything else it's more uh a necessity of the mathematics than it is anything that you can honestly interpret so let's get started uh with an example we're going to define a sample space for the experiment of flipping a coin we're going to list all possible events for this experiment let me just get caught up in my notes uh that i have a side here and uh all right so i'm going to say uh that this sample space uh which i'm going to call s uh what are going to be the possible outcomes of flipping a coin well despite what might be physically possible like i actually have seen coins uh not not necessarily like mint coins uh but things very coin like that end up landing on their side but that is not going to be allowed here there's only two possible outcomes heads and tails and notice that notice the curly braces often sets what we are talking about is a set generally sets are going to be denoted with uh curly braces another important fact about sets is that the objects in sets only appear once generally if you were so like for example this set is the same as h h uh t so at at some level there's uniqueness in a set you get imposed uniqueness so if you list heads twice it's the same set okay um and additionally the ordering of how i write stuff down in a set does not matter so i could have written tails heads and it would have been the exact same set here so but yeah now we have the sample space and this is what it is by definition i decided this is the sample space for my experiment and i decided this because i believe that this sample space is going to be the appropriate sample space uh for my problem so i'm saying that there's two possible outcomes of this experiment you the coin either lands heads up or it lands tails up uh and i so next i'm going to list some possible events for this experiment so an event is a subset of the sample space okay so what is one possible subset uh well one possible subset is the sec that contains only h right so only heads so this is the event or the subset where when you flip the coin it lands heads up and similarly we have the event where it lands tails up and some authors like to call um sets like these sets with only one element simple events because they have uh only one outcome even komogorov in his book on probability theory uh denoted uh had the notion of simple events where it has only one outcome corresponding to something that you would actually observe um i i i personally don't really care for the distinction myself uh but students might like it uh we can also have the event heads or tails so what's a what is a possible outcome for this well we get get heads or tails basically um and notice right here that this is the same as the sample space so i could have said the sample space is an event and generally that's true because what does it mean for a set to be a subset of another set what does it mean for for something to be a subset it means that every element in a set is present in another set or equivalently there are no elements in the subset that are not present in let's call it the parent set right so equivalently you cannot find an element in the subset that isn't present in this uh containing set okay so by that definition the sample space is a subset of itself since every element in the sample space is also present in the sample space so it seems almost tautologically true and yet at the same time it matters it matters a great deal that one set that you automatically get when you de one event you automatically get we need to find a sample space is the sample space itself and there is one more event that we have the moment we define the sample space the empty set that is also a subset of the sample space now it seems really weird because you ask yourself how is it that a set with nothing in it another way to write the sample space is like this where you write two curly braces but with nothing in between them because the empty set has nothing in it okay um so you ask yourself how is it that the sample space every element of the sample space is also every element of the empty set is also in the sample space it has no elements well exactly because by this alternative way to think about what it takes to be a subset there is nothing in the empty set that isn't present in the sample space because there's nothing in the empty set therefore you automatically get that you automatically get that the empty set is a subset of the sample space and therefore the empty space is an event now i i i'm kind of implying here that what it takes for something to be an event is that this set needs to be a subset of the sample space so in other words what it takes to be an event is that you simply be a subs of the sample space technically that is not true but the reason why it's not true is going well beyond the scope of this class and uh you might see a little bit of it in probability theory and it would become much more important if you were to take graduate level probability theory measure theoretic probability theory technically it is not true that every subset um of the sample space is an event that said it's really hard to imagine a subset that is a one so basically if you imagine the subset and you didn't actually try to break the theory if you imagine a subset it's probably an event so uh it's for now it's probably fine although i'll probably add a little more rigor to the notion of what it takes to be an event um or do i do that only in like a class devoted or probability theory i'm not really sure if i talk about in this class um we'll see we'll see we'll have to see as we go through the notes all right uh so that's that um by the way i should probably uh mention something uh here we might give names to these events like we might call this first event eh we might call this second uh simple event e t uh to say that one sub one set is a subset of another we can use the notation say uh e h uh is a subset uh and i like to put a line underneath um so to say that it could possibly be the same as uh the sample space so we have this or ah i mean a lot of people also will just write it like this uh so we set have that the event e h is a subset of the sample space e t is a subset of the sample space um s is a subset of itself and the empty set is a subset of the sample space okay so uh continuing on next example define a sample space for the experiment of rolling a sixsided die list three events uh based on this sample space okay so i'm going to say because i think this is the best way to understand this problem uh that it's going to consist of outcomes uh one two three four five six but i'm not going to write the numbers one two three four five six and i have and i have reasons for not writing the numbers the reason is there is nothing in probability theory that requires that your sets contain numbers nothing says that it just says some object very loosely defined so instead of writing numbers i'm going to write little dice faces so i have a dice face that has one pip a dice face that has two pips a dice face that has three pips a dice face that has four pips a dice face that has five pips and finally a dice face that has six pips okay that that looks like six pips all right so that's my sample space and now i'm going to list three events based on this sample space so one event uh one event is the empty set i'm just gonna say it right now the empty set is this upset so here we go we get one we get one event for free i've promised myself i'm not gonna do that here that's my preferred notation i'm not sure if it's the notation used in the book and not everybody uses the same notation that's something that you need to get used to in mathematics you need to pay attention to what notation someone is using because despite the fact that the books and sometimes the instructors make it look like there's one set of notation uh for a subject that's just not true and that's and that's including probability theory and statistics it's just not true that there is one set of notation and you need to pay attention to what someone is actually using to mean their stuff um anyway um so the empty set is a subset of the sample space the sample space is a subset of the sample space both of these are uh valid events okay but they're almost trivial at this point because these are events that you automatically get so what's something that's a little bit more interesting uh well we could have the event where you roll a four uh this is one of those simple events that you may have heard of so four is an event and it's a subset of the sample space uh what's another one that we could have well we could have the event where uh you have an even number of pips that's a valid event uh because in fact even though i've written this down in english and often it's useful to write down sets in english sentences what this actually translates into is a set with three elements you have the set containing the dice with a with two pips on its face uh four pips on its face and six pips on its face these are all uh come on you come on i said six pips don't make me a fool you stupid laptop some people there we go so this is also a subset of the sample space this is also an event um okay so um there we go i've given you four events in fact uh so you got a little bit more than what you paid for anyway uh example three define a sample space describing the event the experiment of flipping a coin until it lands heads up list five events from this sample space ooh so what does this look like what does this look like well i'm going to say here's my sample space and uh well what's one possibility uh flip a coin until it lands heads up well it could land heads on the first flip so you flip the coin it lands heads and then you stop uh you could then fl you could another outcome is you flip the coin and it lands tails so you haven't gotten heavy yet so you need to flip it again and then it lands up heads the second time so tails heads is another possible possibility uh tails tails heads is a third possibility tails tails tails heads is yet another possibility and so on this set has an infinite number of elements because in principle if i have if i have an outcome where uh where you've got so many tails and the last one is ahead it's possible to also have an outcome where you where in order where before you got to that point you flipped the coin you got tails once so for every outcome you can find a next outcome in a sense so since there's always going to be an x outcome this set must have an infinite number of members now there is actually an interesting wrinkle that co that came up in one of my lectures on this is there an outcome in this sample space that corresponds to flipping the coin and it never comes up with heads the answer is no the answer is in this probability model it is impossible for the coin to land to never get heads because i never described an element in this sample space where the coin just where you just flip the coin forever because you never get heads it is explicitly forbidden in this probability model since you cannot find an outcome corresponding to it so therefore since there is no such outcome where the coin never where you never stop flipping the coin it is literally impossible in this probability model it's a very subtle point it seems like you should have that outcome in this but in fact you don't um it it's just and the reason why being i didn't define this sample space to allow for that possibility since there isn't a possibility that corresponds to it it simply doesn't exist right although it seems a little unfair because we can imagine a universe in which someone flips a coin and they never get tails and they never get heads forever it does seem like it's a possibility but it is explicitly forbidden in this probability model since it is not in the universe of possibilities um and by the way it is different from something being impossible and improbable improbable will probably mean uh when you define a probability model for this the probability of something happening is zero which seems realistic to say for flipping a coin until it where you flip a coin forever and uh you never get heads it seems like it's reasonable to say it's improbable but not impossible but right now it is literally impossible since there is no outcome that corresponds to that we would have to add a separate element which if we really wanted to we could add maybe the infinity element to our probability model to represent the outcome where you flip the coin forever um and by the way none of this says anything about what the probability of these events or these outcomes are i have no notion of probably at this point this is all set theory right so it seems like you would say that the probably that you flipped the coin forever is zero but i've i have said nothing about probability so far now that said that is a complication that we are going to leave out we are not going to consider it any further uh we are just going to stick with this probability model uh maybe i would revisit uh this notion of flipping the coin forever uh in a later lecture but that that's it for now uh let's list five events from this sample space well what's one event one event is the sam what is the empty set why didn't i listen that because i'm bored well no not because i'm bored because i'm lazy um another one is um let's say uh you've you can maybe flip the coin exactly three times that were that would correspond to tails tails heads okay this is a possible event um that seems almost like a triviality we could have the event where you flip the coin um at uh at most let's say three times because i don't want to write too much all right at most three times so at most three flips what would this correspond to like this is in words but in fact i can translate in that into a collection of outcomes well you could have flipped the coin only once that's at that's that's no more than three uh you could have flipped twice that's no more than three but if you flip twice and that means that the first flip was tails and uh if you flip three times exactly then you got two tails and a head so we could have tails tails heads and this would be the event uh what's another event we could say um well another possible event would be uh in words um at least uh three flips that seems reasonable what would that translate into that would be the event where you flip it three times because it's at least three flips so tails tails heads and tails tails tails heads has at least three flips since it has four flips and tails tails tails tails heads has five flips so that counts and in fact there's an infinite number of such flips okay uh and one final uh possibility is an even number of flips that's that's possible what would that look like if we were actually using the elements uh of the set to describe it that would be the event where you have tails heads that has an even number of flips since it has two tails tails tails heads has an even number of flips uh tails tails tails tails tails heads that also counts since it has six flips and so on this event also has an infinite number of outcomes so you can see here it's perfectly possible to talk about a probability model that has an infinite number of outcomes um in fact such models are quite common uh and in fact this particular model where i'm flipping a coin until i get heads as an as an instructor i really like this model because uh it's it's um it's not too difficult at least in my opinion to understand what is going on the idea of flipping a coin until you get heads it's a perfectly reasonable thing to think about and yet at the same time despite its apparent simplicity it is actually a quite rich probability model and makes a lot of points about probability theory so i'll probably be revisiting this one it like it's it's simple but it can very easily get out of hand in a way uh you you can start it can get quite complicated when you start analyzing it uh probabilistically and the mathematics themselves like uh a um like students at this level can't understand it but it's also starting to push their knowledge a little bit and it starts requiring some trickier uh calculations to do all right i'm just going to check something that's not what i wanted yeah we're still streaming okay all right uh example four define a sample space describing the experiment of rolling two sixsided die simultaneously list three events from this sample space uh all right two six sided die simultaneously what would that look like well it's tempting to say that this sample space consists of the numbers two through twelve but that's actually not what we should use the reason why is probably what you're thinking is i'm adding the two the pips on the two dice together but i never said that i never said that there was going to be um addition of two pips so we're not going to do that because you can define a number of uh potential outcomes like maybe instead of uh combining the two pipes together you're taking the larger of the two pips something like that um or the smaller of the two pips so we don't want to define our our sample space that way um what's another thing that we probably should do well when developing such a probability model it's generally better to imagine that you are actually rolling two distinct dice reason why is because when you think of it that way you end up with more appropriate mathematics so it's actually better to think of this sample space uh i keep doing that it's better to keep it's better think of this sample space as consisting of rolling a red dye and a blue dye so that's what we're going to do and just uh to just for my own sanity i try to draw things out as a table so as an example we have a blue dye or we'll call it the left eye and we have a red dye so we have an outcome where the blue dye comes up with a one and the red dye comes up with a two we could also have an outcome where the blue die comes up with a one and the red die comes up with a two i think i might have said the wrong thing a second ago but whatever and we can have an outcome where the blue die comes up with a one and the red die comes up with a three and we would continue on with this i'm not gonna i'm not going to list out everything because i got better ways to spend my day i'm going to say that in this first row uh the last element is where the blue die comes up with the one and the red die comes up uh with a six all right uh so for the next row in the next row we'll have the blue die comes up with a two and the red die comes up with a one uh so we'll just say that uh everything in the blue second row the blue die will be a two so we'll just start out uh listing some of those outcomes and then let's uh and in the third row the blue die will be uh three and we in our very last row we would have the blue die is a six so six six six and finally six all right so i probably should still write down my red die so i'm going to write down my red die uh so we got one and one two two uh two three excuse me i have somewhat of a cough three three and uh six ah you failure you failure of a computer so six uh six and finally six ah for goodness sakes like i said really cheap computer six all right there we go i'm satisfied uh we'll just kind of tidy stuff up put some ellipses um i like things to be rather organized and while we're at it we'll put some commas too although i at this point i'm not really sure if the commas matter all that much but this is a set this is containing a bunch of stuff this is what our set contains um so just uh so how many how many elements are in this set um well this set contains 36 elements since you have six possibilities for the blue dice and six possibility for the red dice so there's 36 things in this sample space which using it this way comes up with more appropriate probability models because this is would be a model where uh if you were thinking about adding up the dice uh a sum of two would be less likely than a sum of three because there's three ways to get the dice to add up to three but only no there's two ways to get the dice to add up to three but there's only one way for the dice to add up to two uh so and that's and that's more appropriate for our probability model and we can start listing some events from this sample space i'm going to list one event uh one of one of them from this sample space uh and it will be the empty set because i'm i'm i'm tired uh e2 another uh event would be the sample space itself so anything happens uh again i'm just really lazy right now all right now i actually have to start writing down some real events well i mean those were real events but uh something that's a bit more interesting um let's see e3 what can we do for e3 well we do have the outcome where we roll um six for the blue die and uh one for the red die why why why this event because why not this event it is an event uh what's a fourth event let's let's start to get a little bit more creative we'll say that the fourth event will be um an even number of total pips total suggesting that you're adding the pips together so all right so let's uh translate this into something a little bit more mathy so you can have let's see there's a whole bunch of outcomes for one where you could have uh uh let's say that the uh oops all right we could say that the blue dice is a one and what's one outcome we have the red dice also be one uh we could have the blue dice b1 and the red dice b3 that has an even number of total pips we can just keep going on with this i'm not really sure off the top of my head i could compute it but i'm not really sure off the top of my head how many outcomes uh how many how many outcomes are actually in this uh sample space but it seems like it's going to be a number and i'm tired and i'm sure you don't want to watch me just list out stuff so uh we'll just end this with uh six and six so because at this point you probably get the idea plus i also don't honestly remember the last time anyone asked for this type of this type of event uh another of it so we could have for our fifth event um that the die add up to seven all right so what would be in this event well uh we could have that the blue dice is one and if the blue dice is one since they have to add up to seven that means that the red dice is going to be six uh what's another possible outcome well you could have that the blue dice is two and since the the oh no they don't add up to six they add up to seven yeah that's what i said so uh you could have that the blue dice is two in which case the red dice must be five uh you can have the blue dice be three in which case the red dice uh must be must be four and you keep going on like this uh until eventually you get to the very uh last element that if you were to continue writing down you would write down that element being when the blue dice is six and the red dice is one so how many elements are going to be in this uh event well it seems like for every blue die there's a corresponding and for every blue outcome between one and six there's a corresponding red outcome that would lead to the die setting of just adding up to seven so there must be six things in this event so yes um by the way for whatever for for what it's worth i'm not sure if i mentioned this later but i'll just mention it now uh we often use this notation uh like for example uh this we would put a set in between two what almost look like absolute value lines to mean the size of a set or more technically the cardinality of a set but for now when we're talking about um finite sets it's fine to talk about to say the size of a set meaning the number of elements in that set in which case the size of the sample space is 36 the size of the empty set the set with nothing in it since there's nothing in it its size is zero uh the size of um e5 would be six there's six elements in e5 and there's uh one element in e3 okay continuing on uh our next example define a sample space describing the experiment of waking up in the morning at a particular time where the time you wake up at the thought of as a real number and that's a critical point is the outcome of interest list three events from this sample space um i'm going to say that the sample space so we can think of it in terms of hours in a day but we're going to allow hours to be decimal points so an hour and a half would be 1.5 so we would say that midnight corresponds to zero hours and we're just going to say that you cannot reach the midnight of the next day so maybe you've seen this notation before when regarding uh intervals of the real line where a square bracket means that that number on that side is being included in the set and an open bracket or a parentheses on a side means that that number is not being included in that event uh so some more notation we say we use uh what almost looks like an e or an epsilon to say that something is a member of a set so for example zero is an element of the sample space uh 12 is an element of the sample space um but 24 uh well let's let's uh come back to 24 in a second 50 is certainly not an element of the sample space so we'll put a slash in that uh and 24 is not an element of the sample space either because i put an open what i call an open bracket or a parenthesis around the 24. uh so in this alternate so if i were thinking of 0 24 as a set this would be a set that doesn't include zero either alternatively uh just getting you more familiar with this notation uh we could describe a set that includes both of its endpoints 0 and 24 and by the way all of these are corresponding oh go away all of these by the way are corresponding to uh intervals of the real line so some some interval of the real line uh hopefully you're somewhat familiar with that notation uh yeah okay so uh continuing on this is our sample space let's uh erase all this stuff because this is someone evident aside um okay uh so uh three events from this sample space we could have an event where um uh we wake up at noon so that would probably correspond uh to the outcome where you wake up 12 hours from midnight um so this would be exactly 12 hours though exactly 12. not a second more not a millisecond more not a millisecond less and not a second less exactly 12 hours later which you think of that that matters it is exactly 12 and nothing it seems like like like our own intuition of the world is like if you wake up a millisecond after mid afternoon um you still woke up at noon i was like no that is not what i mean right here so it is a precise number and that precision should lead you to think well that's impossible it's it's or maybe not impossible but but really really really really hard to the point of being almost impossible and that is true when we develop for a probability model for this experiment we would probably assign a probability of zero to the event of waking up exactly 12 hours from midnight which is kind of a strange idea but i'm going to talk about that later video just understand that just to understand what exactly i'm saying here uh let's uh come up with another event uh this event corresponds to waking up between let's say instead that you wake up sometime between 8 and 12 hours now my language is a little imprecise here so i need to specify do i mean uh including eight hours uh including exactly eight hours um or am i saying more than eight hours so eight hours in a millisecond is okay but eight hours on the nose is wrong uh so we'll just say that we're going to exclude those endpoints why because i said so um so this excuse me uh this uh this event uh would be an event where you do in fact uh where you're where you can wake up um eight hours in a millisecond eight hours in a second or 12 hours less a second but you cannot wake up at exactly eight hours or exactly 12 hours and have this event actually have occurred okay um let's say for a third event um we're going to wake up um let's say that that this is uh waking up after 3 a.m so or um let's see uh maybe no earlier than three event at 3am so no earlier than 3am uh that suggests that we should include 3am in this in this uh in this event so uh so we should include 3am or 3 hours past midnight but all right so what should the end point on the other end be it seems like we can wake up any time after 3 a.m and that's fine so we're going to end at 24 since that's the last um so since those points are the last ones in this sample space okay let me get caught up all right moving on uh events once we have some events we can start uh manipulating these events in ways that create new events uh so we can start giving some uh operations on our set theory so for example let's just say that a and b are events uh and events in this situation they're also synonymous with sets so anything you know about set theory uh you can import that here i am actually when i'm talking about events i'm really talking about sets there those two terms in this class are used almost interchangeably so the complement of a which in this class we're denoting with a oops uh in this class we're denoting with a prime but that's not the only notation there's a bar or a complement this is actually my preferred notation uh that's but anyway uh in this class we're going to use a prime this is the set of outcomes of s the sample space not in uh the event a uh so it's all outcomes that could possibly happens that are not in a which so and in fact we the the the english words that we use for this event is not a that's that's a perfectly way to describe it there's the union of two sets uh which we would say a union b but i also like to just say a or b since this corresponds to the logical notion of or where something in one event can happen or something in another event could happen and by the way when we're using or in this context we're using it in the logical sense where um we can have an outcome in is in a only would count as happening in a or b an outcome in b only would count as have having occurred in the event a or b and an outcome that is both in a and in b counts as happening in the event a or b which is a little bit different from how the word or is used in english because it's quite often the case in english that or means exclusively or exclusive or or you can say you can have her your cake or eat it and like in that so the phrase have your cake or eat it it suggests that you can either have your cake or you can eat it but you can't have both right if you there was actually a it was actually a really long time until that phrase made sense so uh this might actually be something for um people whose native language is not english uh what they mean by have your cake and eat it too um you can have your cake in your hand or you can eat your cake but if you eat your cake it is no longer in your hand so that's what it means all right so just in case i think that that might be something that for nonnative speakers it's actually worth it to make that clarification but for a long time i also as someone who's spoken english all their life and was on the debate team and on the literary magazine did not get anyway um uh the intersection of two sets a and b or a intersect b so it's the set that contains only objects that appear in both a and b so in words we use a and b i'm going to go ahead and get started drawing some diagrams venn diagrams are a way to describe set theoretic relationships between uh different sets and venn diagrams you can construct venn diagrams for pretty much up to uh three events and venn diagrams make sense and very diagrams are very simple and the moment you try to go to four events event diagrams become impossible so we're just going to live in a world where there's only three events um so here's how you kind of draw a venn diagram at least for this class you draw circles and squares and stuff to denote sets often we draw a giant square and this square denotes the sample space so the square denotes the universe of possibilities and we denote a subset of this sample space otherwise known as an event with some other shape such as a circle or maybe i there are times where i will draw lines to try to divide it up but basically we're going to draw shapes and lines inside of this sample space that divide regions of the sample space from each other and those will denote subsets so often i'm going to draw circles to denote events so i'll often label one circle a and another circle b and how we draw these circles allows us to reason about relationships amongst events so here i have drawn a sample space with two events a and b and those events have some outcomes in in common since there is a region in which both set both of these circles intersect now there's also a situation where there are outcomes in a that are not in b so situations where a happens but b doesn't happen because you can kind of imagine if you really want a probability model that we're going to pick a random point from this set and we're going to ask where did this appear did this appear in in a did it appear in b did it appear in a or b um did it appear in a and b or did it not appear in either one um that would almost be a probability model so we can use diagrams such as this to start reasoning about uh set theoretic relationships um and in fact here's something to kind of think about well actually no i'm going to save that for the next section um but this is a perfect perfectly reasonable way to uh think about uh set theory at this level um okay so uh two so uh let's uh describe the situation uh a or b so a union b means an outcome that is either in a or in b or both so what i would do in a venn diagram is shade the region that corresponds to this event well maybe before i do that maybe before i go into that level of extra complication how about i first describe the event just a well visually using a venn diagram that would what i would do is i would shade the region a and nothing else and that would correspond to the event where a happens so i'm using shading up these circles uh to try and reason about what happens here uh i wonder if there's a let's use this highlighter tool i wonder what would happen if i use the highlighter uh it makes my computer lag a lot i'm not gonna use that um so um uh similarly we could describe another event where we just uh have b occur so we'll say the event b how would we shade this we're going to shade it like so just shade in the region that's enclosed by b and nothing else so this corresponds so this is the region that corresponds to the event b happening now let's go back to some of these other uh potential relationships uh such as a or b or a union b um hmm this seems like a fancy feature no no it doesn't work that way so i need to get rid of that red region all right so um here's a way to draw venn diagrams uh when you're trying to do um stuff so like let's take for example the union operation a union b to draw a union operation what i generally will do is i'll take a common color and i will shade in first uh a uh i don't want black i want blue i'll first shade in a the the set on the left side of the union relationship and then i will shade and be the element on the right hand side and a point that was shaded at all is a member of this set so um so this region that i've kind of enclosed in red uh corresponds that's that a or b since any point that got shaded by blue at all counts as being a member in this event okay uh let's draw another uh event let's uh illustrate for example a intersected with b so to draw intersection let's uh first draw our sets a and b to draw intersection you're going to subtract points uh you're going to subtract from uh uh or or actually what you would do is i like to think of it in terms of pieces of paper uh where you overlap two pace pieces of paper on top of each other and then uh cut the pieces of paper so that only the overlapping region is what's left so uh you could imagine here uh i draw uh something on a and i draw something on b and then i'm going to erase from the picture the region that what that was in b but not in a and i'm going to erase from the picture the region that is in a but not in b and what's left is going to be the region that corresponds to a intersected with b okay um so another important notion is disjointedness two sets are disjoint if they have no elements in common in that case a intersected with b is the empty set so to draw disjointedness this is what i would do here's my sample space i draw an event a and i draw an event b such that there is nothing in common between the two events all right i'm going to draw the intersection between these two events i'm done because there's nothing intersecting so the intersection between these two events is uh is the empty set to so this is one way if you really wanted to assign some sort of meaning to the empty set it would be that the empty set is a logical contradiction in a way since the empty set shows up and when you have events that have nothing in common which are almost logical contradictions uh another notion in probability theory no in a set theory is complementation how would i illustrate complementation so let's uh draw a sample space i have a i have b what is the so this is um a intersect with b uh equals the empty set that means disjoint uh what is a complement a complement is the region in the sample space that is an a that is in the sample space but it's not an a so that's going to correspond to shading everything that's outside of a so there are some parts of b that gets that get shaded but nothing that's in a okay so you shade everything except a uh some other important subset relationships um we have uh uh or so some other set relationships here is the relationship a is a subset of b what it means for one set to be a subset of another is that um all of the elements in a are also impres are also present in b or alternatively there is nothing that a that doesn't also exist in b so a subset relationship looks like so you have the set b and the set b contains the the set a so you would draw a as being in the interior of b another uh notion that i haven't really described above but let's go ahead and mention it is the notion of set subtraction we have a subtract the set b that corresponds to every element in a that is not in b so uh it this it is in fact possible to prove that a subtract b and you might actually be required to show this in your homework and the way you probably show it is by playing around with venn diagrams you can say this is a intersected with um the complement of b so a and not b so what does set subtraction look like as a venn diagram we have the set a we have the set b and we draw everything in a and we shade everything in a that is not in b so you kind of subtract out the set b it's as if you had these pieces of paper they put them you put uh the piece of paper the b piece of paper on top of the a piece of paper and then you cut out the part of the b of the a piece of paper that's in the b piece of paper excuse me okay uh so let's see an example use a venn diagram to illustrate uh a or b complement or a and b uh before i continue on let's just make sure i'm still streaming i'm still streaming um at some point i'll relax about that but today is not that day uh let's get started by just drawing uh the venn diagram oops i don't want red yet so here's my venn diagram for the sample space s i have a and i have b okay so i'm first going to illustrate uh the event a intersected with b i'm going to do so in blue so a intersective b that's the region that is in both a and b uh as for a or b complement you have the component you have the union of a or b which is kind of this uh figure eight looking region here and i want everything outside of that region because i want the complement of that region so i'm going to shade it everything that is outside of that region and this is what you end up with okay so that's one example next example consider three sets a b and c so how would i draw these in a venn diagram i would have my square denoting the universe of possibilities which is s and then i draw um three circles one for a one for b and one for c so what is the union of a b and c well it's going to be the set where uh if i shade a everything in a and then i shade everything that's in b and then i shade everything that's in c if the point ever got shaded it's going to be in that union all right next up we have a intersected with b intersected with c so let's draw our universe here's our sample space so we have a b and c so let's see uh let's take a point right here this point that i just drew is uh a point that is in b and it is in c but it is not an a so it's not going to be in the intersection uh this point right here is in b but it's in neither a or c but it it's also just not an a so it's not going to be in the intersection of all three you can just start reasoning about all of these points in a similar way and the conclusion that you're forced to reach is that this little sliver here is the only part that's going to be in all three sets so that corresponds to the intersection of all three all right uh next example this is a more complicated one we have a intersected with b or a intersective with c or b intersected with c what is that going to look like well let's get started i don't want red all right here's our sample space here is a here is b here is c all right let's start out by uh building this thing up with uh the intersection a and b right here is a and b here's a and c and here's b and c so this wind or pinwheel looking region uh is the only region that ever got shaded so that's going to correspond to the union of the three of the three intersections so this would in words this would be an event where at least two events happen this will correspond to all right uh so example eight describe the intersection complement and union of events described in examples one through five let me get caught up um so the point of this section is to go from these pictorial representation or not this section this example is to go from these pictorial representations of events to more algebraic representations so uh let's go with example one so for example one that was the example where we were flipping a coin we could get either heads or tails uh we could have uh the intersection of heads and tails uh of the two uh sets heads and tails these two sets have nothing in common they are therefore considered disjoint events so the intersection of these is the empty set since you would only restrict yourself to what is in common and they have nothing in common uh whereas the event heads or tails uh that event would correspond to uh the event heads with heads and tails uh which corresponds by the way to the sample space okay uh here's also some basic uh properties for you uh let's let's take an arbitrary uh set s no no no not s uh let's take an arbitrary event a a intersected with a sample space is equal to a since a is a subset of the sample space in fact in general if a is a subset of b then a intersected with b is going to be uh a and a union with b is going to be b and i'm just going to leave it at that i want you to think about why that's true uh go try and reason about it with a venn diagrams um so and in fact that's pretty much that pretty much says everything that i would want to say about uh the sample space and the empty set because uh since a is the subset of the sample space a intersected with s is going to be a and a unioned with s is going to be s likewise a intersect intersected with the empty set the empty set is a subset of a therefore the intersection of two is going to be the subset which is going to be the empty set on the other hand a union with the empty set is going to equal a since a contains the empty set all right uh continuing on with what i was saying before um the next example uh so example two uh so uh we have uh for example the sets um my notation in the notes is a little bit different from one i'm sure i wrote down earlier um so let's uh uh um i don't want to go all the way back there i'll just uh i'll just throw some stuff out uh we can have um for example uh so this was the one where we were rolling a die uh one we could say that the union of the set uh one with the die roll of one and the set with the die roll of uh so the intersection of these two sets this is also going to be the empty set since they have nothing in common but uh the intersection of the event with um no the union of these two sets is going to be uh is going to be um the event that contains both of these possible outcomes darn laptop so one and three and uh maybe less trivially um uh we could have uh we could have the um event uh let's say one three five so this would correspond to an odd number of pips and we're going to intersect that with the event where the number of pips doesn't exceed three um so that would be the event where you have one uh two and three oh okay so uh the intersection of these two events is going to be well let's see what outcomes do they have in common uh one amperes in both of them three appears in this set but it doesn't appear on the other one so let's see we've got one one appears in both uh three appears on only one no three actually appears in both of them so three appears in both of them five appears only and one and two only appears on one so whatever got underlined twice that is going to be in the intersection of the end of these two sets so we're going to end up with the set one and three now for a lot of these uh set relationships such as and and or you can kind of think logically using english like for example um uh maybe going to uh the fourth example where i had uh sets where um yeah i was rolling a two dice uh a red dye and a blue dye you could imagine a set where uh they summed to seven and the blue dice is at least three so maybe going to that so in the context of example four so in the context of oops in the context of example four so they sum to seven and they don't or no no no no uh the blue dice uh so blue dice is um uh let's say five so that would correspond to this to the event where uh you have um so let's see the blue dice is at least five so we could have the blue dice b5 stupid pen so the blue dice could be five or the blue dice could be six and in these two situations we know what the red dice is going to be in the first the red dice is going to be two and in the second the red dice is going to be one so this would be uh the corresponding uh resulting event after we do that intersection okay um i'm going to leave it at that there's because there's a lot of examples so for the rest of these maybe uh try going through uh some of the set the events that i listed down and figuring out if you intersect or complement these events so intersection complementation union uh all these things try going through those sets and uh seeing what you get okay but i've given you some examples to start working off of for now all right so i'm gonna leave that for this section and uh i will see you in section two where we go beyond just uh talking about some basic set theory and actually start saying what it takes to build a probability model all right so i will see you later hey students all right so let's move on now to the next section uh on starting probability theory so uh in order for us to uh we would like to be able to assign numbers to events from uh sample spaces to describe how likely those events are and in order to do so we need to develop a notion of probability so we'll start by defining what a probability measure is so we have so we have what's known as a probability measure p and this is a function it's a function that takes events as inputs and returns numbers between zero and one and satisfies the following three axioms and axioms are things that are true basically because we say so they're they're almost similar to assumptions maybe you remember axioms from geometry but the idea is that we have some starting things that we simply say they are true because they are almost selfevident first we say that every probability must be at least zero uh you cannot have negative probabilities second we say that the probability of the sample space is equal to one so the probability that anything happens is equal to one finally uh this is the weirdest looking axiom if we have a sequence of disjoint events uh so in other words for any uh i that's equal to j the intersection of uh any two such events is uh equal to the empty set and furthermore this is for a potentially infinite sequence of disjoint events we say that the probability of the union of those events is equal to the sum of their individual probabilities this is very wordy this is very technical there is a much easier way to understand what this axiom is saying if we decide well okay there's usually only one situation where you're going to understand where you're actually going to use this axiom most of the time which is that if a intersected with b is equal to the empty set so if a and b are two disjoint events then the probability of a or b is equal to the probability of a plus the probability of b so the probability of uh two events that have nothing in common uh if the probability that either one happens will be the sum of their individual probabilities okay so these are true because we say so and from this we get uh pretty much everything else that we believe should be true about uh probability or about how probabilities work so uh for starters uh we have uh that the probability of the empty set is equal to zero so this is not something that we said this is not something that we said must be true this is simply something that um this is actually a consequence of the assumptions that we have made up to this point so this is kind of a weird so we're going to show that this is in fact true using just these uh three um just these three assumptions not just these three axioms so only these three things all right um now that said we're going to have to use the fact that probably the sample space is equal to one what we could say is this the sample space the sample space is equal to the sample space unioned with the empty set and in order to use axiom three in the way we have written axiom three down we're going to have to say that the empty set is equal to uh the union from i equals one uh to well i equals one to infinity the empty set so in other words we need to create an infinite union of empty sets and it's certainly true that if you take an infinite uh collection of empty sets and you union them all together there's still not going to be anything in that union so you're still going to have the empty set furthermore the intersection of the empty set with the empty set is also going to be the empty set so technically the empty set is disjoint with itself so that means that this collection of empty sets this giant uh repeating of the empty set technically satisfies the condition of the conditions of the third axiom so then and and also it's true that the set the sample space uh intersected with the empty set is going to be the empty set because the empty set is a subset of the sample space therefore we can now apply this uh this uh this uh third axiom to prove the prob uh proposition because we know that one is equal to the probability of the sample space and the probability of the sample space is equal to the probability of the sample spaced uh unioned with the empty set which is equal to uh the probability of the sample space unioned with this infinite collection of uh empties of uh empty sets and then we apply that third axiom to say that uh this is going to equal to the probability of the sample space uh plus uh this uh plus um the uh the sum from uh i equals one to infinity uh the probability of the empty set and what that forces us to then say is that since uh the probability of the sample space is equal to one we're forced then to say that the sum from i equals one to infinity of the probability of the empty set is equal to 0 but that's only going to be possible if the probability of the empty set is itself equal to 0. because since probabilities uh cannot um be uh negative and oh and even just because of the fact that uh we're adding up something an infinite number of times and that's and that's the exact same thing and it adds up to zero so that means that each one of those must be equal to zero therefore this uh axiom this uh proposition is proven this one this is actually rather weird it's it's so surprising like there is a way in which you you you kind of just want to say uh you have one equals to the probability of the sample space which is equal to uh the probability of the sample space plus the sample space unioned with the empty set and then it turns into a sum of probabilities and uh that means that probably the empty set is equal to zero but we have to in order to be mathematically rigorous use these axioms in the way they were written down so we would actually have written the axiom in a way in which it wasn't written and therefore wouldn't be able to directly apply it so we had to be really tricky but the good news is that we only had to be tricky really once with this uh proposition just with this one because now that we have this proposition the others will not nearly uh will not be nearly as uh strange for example proposition three like this one this proposition if we had this already then that proposition that we just proved would actually be rather easy because uh the complement of the sample space is equal to the empty set uh the probability of the the sample space uh is equal to one and since the uh uh the the empty set is the complement of the sample space we can say if this proposition 3 were in fact true the probability of the empty set is equal to 1 minus the probability of the sample space which is 1 minus 1 which equals 0. right if we had proposition three then it would actually be rather easy to prove that the probably the empty set is zero but the thing though is in order to be able to prove uh proposition three we're going to have to uh probably use uh some of those uh other propositions so unfortunately uh it's it's um it's not quite that it's we we technically need to take a very long circuitous route in our proof before we can actually invoke this one so what we're going to say is that the probability of the sample space we can say that the sample space can be divided into uh into the set a unioned with the complement of a and uh a union a complement um well okay a intersected with a complement um is equal to the empty set so since a intersected with uh its complement is is the empty set that means that these two sets are going to be disjoint which means that we can then invoke uh that um that a second that third axiom and say that the probability of the sample space is equal to the probability of a union a complement which is equal to by that that other axiom the probability of a plus the probability of a complement and uh by the first axiom this is all equal to one therefore after you do a little bit of algebra where you subtract uh you subtract both from both sides the probability of a uh after you do that algebra you can now say that the probability of a complement is equal to one minus the probability of a and we're done that proposition has been proven uh next proposition we have that the probability of a is less than or equal to 1 for any event a let me get caught up in my notes over here all right uh so the probability of a complement since a complement is in fact an event by the first axiom uh we get to say that the probability of a complement is greater than or equal to zero which means that one minus the probability of a is going to be greater than or equal to zero and therefore it follows after oops after we add the probability of a to both sides after we do that uh we get to say that uh uh so since these two things end up cancelling out uh the probability of a is less than or equal to one so that proposition has been proven uh next one or is that it no that's not it uh the probability of a union b is equal to the probability of a plus the probability of b minus the probability of a intersected with b for any events a and b so uh this by the way is generalizing more or less that third axiom since that third axiom required disjoint events whereas here in this proposition we do not require disjoint events um we do not require just disjoint events and the penalty that we pay for that is that we have to subtract out the probability of the intersection here's the thing though uh this this makes perfect sense what's going on if we draw a venn diagram um it turns out that probability is part of this general class of mathematical objects or probability measures are part of this general class of mathematical objects known as measures and amongst that family of mathematical objects we include things such as measures of area or measures of length which means that it's actually rather appropriate and convenient to reason about probabilities the same way we reason about areas or lengths as we do in the real world so let's think instead about we have um a couple uh sets a and b we want to figure out what the area is in a and b like the that's enclosed within both of those we want to figure out that area um and how we're going to do so is uh we have a couple sheets we have an a sheet and a b sheet and we lay them down and we have some scissors so um we can measure the areas of these sheets so uh given that um we're able to measure the areas of these sheets how can we possibly compute the area that's enclosed in both a and b well what we would do is we would lay down the a sheet onto this diagram and we would have its area then we would lay down the b sheet on this diagram and we would have its area but the thing though is once we've done that we now have an overlap we have an overlapping area um so we've actually over counted the area um in a and b we the number is too large so what we need to do is subtract out uh the part the overlapping part of the a and b subtract out its area because otherwise we would have double counted it so we're going to uh subtract out that area um and uh or at least we'll like cut out a little slice of it um uh maybe just like the red slice part and leaving only the blue part left and and we're allowed to measure the area of the part that we cut out so after we uh subtract that the area that we removed we now have the area that's enclosed between uh that's closed within these two circles and that's an aerial way to understand why this formula here is in fact true because this is base because how i described it is basically what we're doing we have the area of the region a and the area of the region b and we subtract out once the area that's in between them since we accidentally well not accidentally since uh without doing so we would have double counted that part so we need to remove the double counting so how are now that's like a reasoning for what we're doing and now let's turn that into a series of mathematical statements and a proof here's a way to think about this region this venn diagram we can divide up in this venn diagram uh we can divide up uh this uh region so that we have uh the light blue region we have the green region and we have the red region we will call the blue region a and not b we will call the green region a and b and we will call the red region uh not a and b and it's clear that these three regions are um these three regions are disjoint you can see so visually but if you wanted a nonvisual way to reason why that is the case let's suppose an element is let's suppose that we pick a point in that is in the set a and not b can that set be an a and b well it must not because in order b in the part a and b it must be b b end b uh but it is not in b since it's an a and not b so it cannot be in there and uh similarly for uh not a and b and for the same reason if you're in a and not b you cannot also be in not a and b since you're in a and therefore you're not in not a i know this is the verbiage is getting really complicated but that is that is really the logic for why these things must be destroyed or you can just look at the picture and be satisfied um for that reason we can say that the i mean you can also look at this picture and see that when you take the union of these three uh events uh you have in fact the region a or b and there's also no double counting here since you counted each little part at this each division once so now we can say i'm going to zoom in uh some more we can say that the probability of a or b is equal to invoking that third axiom the probability of a and not b plus the probability of a and b plus the probability of um uh not a and b and uh let's see let's see what how should i proceed next well i do notice for starters that this part right here is equal to the probability of a which is uh clear from the picture because if you add the probability of a and not b and the probability of a and b then you've basically counted the probability of a so this must be the probability of a uh but the thing is though uh we need to somehow account for uh i mean we end up in the end with a subtraction of the probability of a and b so how are we going to do that huh well if we look at the probability of b we could say as an aside that the probability of b is equal to the probability for basically uh the reasons that i just wrote here that this is the probability of um a and b plus the probability of uh uh not a and b and then if you do some algebra you can then say that the probability of not a and b is equal to the probability of b minus uh the probability of uh a and b okay oh well look at that we now basically have what we need because we've identified a part as the probability of a and we can also identify the latter part in this sum as the probability of b uh minus the probability of a and b which is exactly the statement that we wanted to prove so therefore we're done and we have in fact proven this statement and notice that this statement does in fact include that third axiom since if since that third axiom was about disjoint events so um if we have a disjoint event we've proven in uh in our the first proposition that we proved in this section that the probability of the empty set is equal to zero and the subtraction of the probability of a and b is going to give you um so so that's going to uh so the intersection of a and b if and since in this imaginary scenario they're uh disjoint that's going to be the probably the empty set which is equal to zero and thus you get uh that uh that other axiom all right so uh oh oops i am uh i am doing something that i don't want to do what i want to do is zoom out okay so the next proposition i'm not going to bother to prove just because it's a lot of work although i may give you an argument for why it's true so proposition 6 now we have three events a b and c and the probability of the union of those three events is going to be the sum of the probabilities of the events minus the probabilities of intersections of two plus the probability of the intersection of all three all right so i'm not going to prove this but i'm going to give you an argument for why this must be true so here we have our sample space um we have the set a we have the set b and we have the set c it's basically the same reasoning as we used before um let's uh uh let's see so uh so what does it mean to add the probability of a probably be probably c so if we add those three probabilities so we're gonna add the probability of a because what we're trying to do is figure out in the area that is enclosed in all three circles uh but those circles are overlapping with each other so we need to figure out how to handle the overlaps so we're gonna color in the probability of a that's what we're told to do first uh then we're gonna color in the region and close by b that's what we need to do second and then we're going to enclose the region enclosed then we're going to color in the region and close by c because that's what we've been told to do yeah you can almost read this as a set of instructions on how to calculate an uh how to calculate um uh um uh an area so we see that we have done some double counting we've double counted here we've double counted here and we double counted here all right so we need to remove those double counts uh let's start with the um let's uh let's start with the double counting here we're going to subtract out uh the probability of a and b which will i will under i will understand that as subtracting out a little green sliver so how does this i'm wondering if this is doing something oh no it just was being laggy all right so um so i have subtracted out the green area from that sliver uh leaving a blue left and some red red left okay uh and then i need to i'm gonna cut out the uh red area that is intersecting and that is at the intersection of a and c okay so i'm going to cut that out uh leaving only blue uh um leaving only blue there uh let's see so we're going to have uh right here so and then we need to uh cut out the area that is in the intersection of b uh and so we now need to cut out the uh region that's in uh both b and c so we're going to subtract out uh the red area but the thing is that doesn't that doesn't uh that isn't enough we cannot just take out the red arrow because we need to take out everything that's in here so we also have to take out what that little blue sliver that's left as well so uh we're going to be left with green in that in that spot uh but the thing is in that little sliver uh that that is in the intersection of all three we have now cut out oops uh we have now cut out too much and that area is not being accounted for at all there's nothing left in there so we need to add that area back in in order to be able to have an accurate computation of the area and there we go so after we add it back in uh we now have the area so everything has been colored uh exactly once and uh we're able to compute the area of how much area we've colored uh in a sense so we have what we need so not exactly a proof because we need to uh do some do some of that a tricky algebra business but uh we're not gonna bother with that this should give you an idea of why it's true okay uh so the next example that all of that stuff was uh uh rather theoretical uh let's let's um start seeing how uh probability theory is actually going to be used so for the most part of the remainder of the section i'm going to be going through a number of illustrative examples uh we do talk a little bit more about uh theory mostly about what probability means uh but uh for the most part we can just talk about um examples so example nine reconsider the experiment of flipping a coin and assume that the coin is equally likely to land with each face facing up assign probabilities to all outcomes in the sample space so recall that the sample space for this experiment consists of the outcome heads and the outcome tails and we're assuming that all of the outcomes in this sample space are equally likely so that means that the probability of uh getting heads uh and i'm writing this in terms of a simple event but honestly this writing it this way often gets rather tedious so i'm often going to admit uh to omit the curly braces that are usually used to denote sets and just write whatever is in the set so we have the probability of h uh this is going to equal the probability of t because all outcomes are equally likely so the thing though is the probability the sample space is equal to the probability of the set containing only h unioned with the set containing only t since this uh so since the union of those two sets is in fact equal to the sample space and furthermore those two sets have nothing in common one has h one has t and they don't share anything so that means that they are disjoint events and therefore we can write this probability as a sum as the probability of h plus the probability of t and both of these are the same so we're going to say that this is equal to p so this is equal to uh p plus p which is two p and furthermore we know from uh axiom two that the probability of the sample space is equal to one so this is equal to one and that implies after you do some division by two that p is equal to one half which makes perfect sense right if you're saying that heads is just as likely as tails then the probability of getting heads is one one half perfectly intuitive right well you know uh probability is intuitive up until the point it isn't and when it stops being intuitive it really stops being intuitive and it becomes suddenly very very strange so it's simultaneously intuitive and unintuitive in fact i've heard someone say that there are fewer subjects with as many paradoxes they're not literally paradoxes but they contradict how humans think of the world very few subjects have as many paradoxes as probability because probability can get really weird really fast we have not reached that point yet but we probably will eventually example 10 do the same as example 9 but when rolling a single dice that is um we are trying to so we we're trying to assign probabilities to outcomes in a sample space when we say that those outcomes are equally likely so the sample space in this case is going to consist of die rolls um so we have uh an outcome one a two uh three four five and six okay so those are our six outcomes and we say that each one of them is equally likely so that means that the probability of rolling a one is the same as is uh the same as the probability of rolling a two and that's going to be the same as dot dot dot as this and the same as uh the probability of rolling a six all right so uh it's a six so all right there we go six it's a it's an oddly painted die all right so all of those are going to be the same and we're just going to say for convenience that this is equal to p all right so then we say that the probability of the sample space is equal to the probability of the result one uh unioned with the result uh two union with dot dot dot unioned with uh the uh outcome uh containing uh six there we go because that's equal to the sample space you basically written the sample space as a union of each of its elements and furthermore each of these sets again are disjoint so uh we can then write them as the sum of probabilities so say that this is equal to the probability of rolling a 1 plus the probability of rolling a 2 plus dot dot dot plus the probability of rolling a six and then we use the fact that we said from the beginning that all these probabilities are the same so this is p plus p plus p plus p plus p plus p so this is equal to six p but it's also equal to one because of axiom two that says that probably the sample space is equal to one therefore p must be equal to one over six and you can see where this is going in general if you have a sample space with a finite number of elements uh and you say each of those elements are equally likely then the probability of a single one of those elements in that sample space probably of drawing that element is going to be 1 divided by the size of the sample space or the number of elements number of unique elements in that sample space um so in fact what we're seeing here can be very easily generalized okay uh so example 11 we're now going to try and move away from equally likely outcomes and say that the dice from example 10 has been altered with weights now the probability of the dice rolling a 6 is twice as likely as rolling a 1 while all the other sides have the same probability of appearing as before therefore we want to know what the new probability model is so we're going to say that the probability of rolling a six so one two three four five six so the probability of this of rolling a six is equal to two times the probability of rolling a one and we're just going to say that's equal to two times p or uh we'll just we'll try to keep things a little bit different we'll call it q this time all right so we need to figure out q because q gives us the probability of rolling a one and if we figure out the probability of rolling a one we then instantly know rolling a six now remember these are the only two faces that have been altered all the other dice faces have the exact same probability as before so the probability of rolling a two is equal to the probability of rolling a three which is the probability of rolling a four which is probably ruling a five and all all of those are equal to one over six so that means um that the probability of rolling a one uh plus the probability of rolling a two plus dot dot dot plus the probability of rolling a 5 plus the probability of rolling a 6. come on cooperate you stupid screen i hate drawing the 6 because my screen is not very cooperative all right uh this is equal to one but uh what's different now is that all of these are equal to one over six so you end up adding one over six four times since it's two three four five there's four things there um the probability of rolling a six is equal to two q and the probability of rolling a one is equal to q okay so um collecting all of that information we now say um that we have a q plus 2q plus 4 over 6 which is 2 over 3 is equal to 1 which implies after you do some algebra that 3q is equal to 1 3 which then means that q is equal to ah no is equal to one over nine not nine because that's not even a probability uh that's that's that's one thing to keep in mind if you get a probability that's above one you have made a mistake so nine is not possible the problem so that means that the probability of rolling a one so yeah let's let's uh write this down now the probability of rolling a one in this new probability model is equal to oneninth and the probability of rolling a six in this new probability model is equal to two over nine and in fact this isn't this is consistent because one over nine plus two over nine is equal to three over nine which is one third there are the uh other dice faces when you add up their probabilities you have one over six one over six one over six one over six which is twothirds and so you have onethird plus twothirds which equals one so the probabilities still add up to one which means that we're fine this is a way for you to check uh whether this is one way for you to check that you're that when you do your probability calculations you have done so correctly make sure that all the probabilities add up to one if they don't add up to one you've made a mistake i remember once a student was working in a i had a student of mine and i gave her a quiz and she had to do some calculations with probabilities and she added up the probably sample space and it added up to something that was to some fraction it might have been uh maybe uh eight over nine or eight like 80 or 81 who knows but but i said this is very wrong and she's like well it needs to add up to one and she said well it's close to one and i said close to one is not one so if it doesn't add up to one it's just wrong it like close to one no caboose it's either one or it's not one if it doesn't add up to one it's wrong it's very wrong it always adds up to one so that's a way for you to check that you've done things correctly make sure that your probabilities add up to okay moving on example 12 reconsider the experiment of rolling two six sided die it is reasonable to assume that each outcome in s is equally likely this is the reason why instead of writing the numbers 2 through 12 or maybe just listing out without really caring about the ordering of the die or not really thinking about there being a red dye and a blue dye we we decided that we were actually going to assign an ordering to the dice it's so that we could basically work with problem example 12 and get a reasonable looking probability model because now if we assume that the two dice are distinct we can now say that everything in the sample space in that sample space with a red and blue die is equally likely and then get accurate probability calculations so uh getting back to this it is reasonable to assume that each outcome in the sample space s is equally likely what then is the probability of each outcome in s um well basically i already argued it uh to you before we'll say that omega is an element of s so this i'm saying this in general right um omega is the element is an element in in s and every element in s is equally likely i argued before without writing it down that the probability of drawing omega then will be 1 divided by the size of s so in this case for example 12 where you have two sixsided dice um the size of the sample space was 36 so the probability of a sink of a of a particular outcome of dice is going to be 1 over 36. now we can use this uh this model uh to find the probability of an event e where so i'll i'll just go ahead and write down in this case that um the size of the sample space s is equal to 36 so for this particular problem this is going to be 1 over 36 but in fact what i just wrote down here true what i just wrote down here is pretty much drew in general like um i'm i i'm thinking in the context of uh rolling a couple die but actually this is true in general when you say that s which is a finite sample space it has a finite number of elements um when everything in that sample space is equally likely um then the probability of an individual individual element is going to be one over the samples over the size of the sample space and it's not too hard why go ahead uh show that the uh probably the sample space when you do this is equal to one uh so use this model to find the probability of an event e where first e is the event where at least one die is a six so let's actually write down um let's write down uh what's in e so that would be we have a red dye and a blue dye at least one dye is six so that includes when the red dye is one and the blue dye uh is six we have uh the case when the red die is two and the blue die is six we have and we can keep going on like this until we eventually reach the case where uh the the red die is a five the red die is five and the blue die is six yeah damn it you cooperate all right that's six um so uh that's one set of outcomes and notice that there's uh that i've just listed down five elements okay uh so next up we will now make the red dye six so we've got one two three four five six it never wants to do the last one never ever wants to do the last one all right so one two three four five six and the blue die is one and uh just just repeat just keep uh carrying on with this logic until we get where the red dice is six and the blue dice is five and finally we have one last outcome where both the red dye and the blue dye are six so one two three four five six and uh one two three four five six all right so uh there are um five elements where the uh um where the red die is fixed at six and then there's one extra element where uh both of them are six so that means that the size of this event is going to be um 11. which then means when we're computing the probability of this event we could add up the probability of each one of these outcomes and each one of these outcomes has an equal likelihood and they all have an equal probability all those probabilities are 36 so you're gonna add up 36 and one over 36 11 times so this is going to add up to 11 over 36. and in fact it's once you have this uh this um assumption that all outcomes in this finite sample space are equally likely in general the probability of any event e is going to be the number of elements in e or the size of e divided by the size of the sample space so at this point all we need to do is decide how to determine how many elements are in our event in order to figure out that events probability all right so for let's let's use that now for uh this uh next problem where e is the set where is the event where the sum of the pips showing on the two die is uh five how many outcomes are there where the sum is going to be five let's start listing out uh possible things in this sample space so uh we've got i guess i've been writing the red dice first okay so okay so this is all right so i've been writing out the red dye first so let's suppose let's see the the two dice is five so can the red dice be one yeah sure why not uh the red dice can be one and if it's one well it must add up to five so that means that the blue dice must be four okay that's one possibility uh next possibility is when the blue dice is two and which is when the red dice is two sorry in which case the blue dice must be three uh we could have the case where the red dice is three in which case the blue dice must be two and finally we have the case where the red dice is four in which case the blue dice must be one and we can't go to five because it's not possible for the blue blue dice to roll a zero um so we're just gonna have to end it there uh we now have uh a sample an event and this event has four outcomes in it so that means that the size of the event is going to be a four in which case the probability of the event e is going to be uh 4 over 36 which is going to be uh 1 over 9. okay next part uh e is the event where the maximum of the two numbers showing on the dice is greater than two okay now here's the thing though um there's actually a lot of outcomes in this event so we could attempt to try and figure out how many outcomes there are where the larger of the two numbers is greater than 2. well let's see we could even have like three one three two three three three four three or five three six all the all the cases where the red dice is three and that's already six such outcomes and that's only for three so uh this could actually be quite difficult to compute except for the fact that it's actually much easier to compute the comp the probability of the complement of this event if we were to look at the complement of the event e that's going to be the event where the maximum um of the two dice is um not greater than two it so it's at most two okay uh where what are what are combinations of dice facing faces where the maximum is at most two well we have one situation where uh we have um oh yeah i said that the red dice is first so we have the situation where the red dice is one and the blue dice is one all right in that case the maximum of the two dice is going to be one and that is uh not greater than two so this is in fact in our event uh then we have the outcome where the red dice is two and the blue dice is one in that case the maximum would be two and that doesn't exceed two so this is in our event we also have uh the case where the red dice is one and the blue dice is two let's see all right there we go so the blue dice is a two and we also have the outcome where the red dice is two and the blue dice is two and the maximum of those in this case would be two and again that's uh at most two and we have to stop there because then because the the next thing we would do is make one of the dice at least three and in which case the maximum would be at least three so it's not long going to be in this event so therefore we have an event with four outcomes in it and the probability of the complement of the event e is going to be the number of things in the complement of e uh which is four divided by 36 which is equal to one over nine all right we're cooking because now we can use one of those propositions that said that the probability of the complement of an event is one minus the probability of the event so that means that the probability of e itself which is what we actually want to compute a probability of that's going to be 1 minus the probability of e complement because e is the complement of e complement so and that's going to be 1 minus 1 over 9 which is 8 over 9. there we go we've we've solved it and that's much easier than if we tried to do it directly and this is one reason why you care about uh these uh complementation rules because sometimes it's easier work to work with the complement of an event rather than the event itself if we had tried to work with that event uh we would have ended up with a sample space with uh or an event with um 32 elements or or a size of 32 so we would have ended up having to count 32 things i mean it's not like impossible to count 32 things but it's also a lot more work so so this is a very good trick to have and something that you should be looking out for when you're doing your own work you should be looking out for situations where the compliment is actually easier to work with than the actual event itself okay uh just real quick uh satisfy my nervousness all right we're still streaming all right so we shall continue example 13 reconsider the experiment of flipping a coin until heads is seen ooh this one this one's going to get rather involved what is one way to assign probabilities to all outcomes of this experiment so that we have a legal probability model justify your answer so how could we possibly do this because we no longer can say that each outcome in the sample space is equally likely because that only works when the sample space is finite but as i mentioned before this sample space is infinite um there's an infinite number of outcomes in the sample space since it's basically the integers in a way is you can map the sample space to the integers and kind of identify it with the integers or not the integers but the natural numbers and in fact i think that's what we should do we should uh try to view this sample space in terms of the natural numbers i'm going to want to zoom in for this one zooming in for me is a way to get to almost get more room on this piece of paper so we're going to say um we're going to define n of omega so omega is an element of this sample space so this is going to be one of those strings heads tails heads tails tails heads tails tails tails heads and so on so we'll say that um n of omega is going to equal the length of the string omega so for example uh n of the string t t h this is a string of length three it then follows that um using abusing notation a little bit because n is a function that doesn't take sets as inputs but let's suppose for a second that we were to put a set as the input this is actually pretty commonly done uh often when you have a function and you plug in uh its domain what you're talking what you're actually referring to is a set representing the range of that function so in this case uh the range of the function n is going to be the natural numbers it's going to be one two three four and so on these are all the possible outcomes it's going to be the natural numbers okay so um i'm going to suggest that what we should do for our probability assignment is say that the probability of an outcome omega is equal to uh onehalf to the power of uh n of omega so in other words it's going to be one half to the power of the length of the string omega so for example in the in this earlier case where we had t t h the probability of that outcome tth would be uh one half to the power of the length of the string tth or one half to the power of three which is one over eight okay all right then so this is a suggestion for what the probability should be but now we need to make sure that this is a valid probability measure so what is what needs to be true in order for this to be the case first off is the probability of um are all probabilities greater than or equal to zero under this method yes that is certainly true because there's no way that this function will produce negative numbers uh secondly uh is the probability of the sample space equal to one uh well that's actually something that we're probably going to have to check and then there's that third axiom about um the probability of a excuse me uh about the probability of unions of uh disjoint events and i'm not going to check that one because that one's actually getting rather complicated that's getting even more theoretical a bit too theoretical for this class but i personally think it's perfectly appropriate to check that under this probability model the probability of the sample space is equal to one um and in fact in my classes this is actually something that i love to ask questions about on quizzes i love to ask students that the probability of the sample space is equal to one under some probability model i love asking that so if you are in my class you should expect a question like that you should expect me to ask you to show that the probability of the sample space is one under some potential probability measure or alternatively very very similarly i might ask you to i might give you a probably measure but it depends on some unknown constant and i might ask you to compute what the constant is that causes this the probability measure to be a valid probability measure that is the probability of the sample space would be one under that measure all right so so this is something to look out for if you're actually taking classes from me uh but let's uh get back to the issue at hand i need to show that the probability of the sample space under this probability model is in fact equal to one so the probability of the sample space here is going to be the pr is going to end up being the sum um i i can basically view the sample space as consisting of you of as being the union of all of these of events uh con where each of these events contains uh one of these uh strings h t t h t t t t h something like that uh and this is actually a situation where that third form of where the way i wrote down axiom three uh before you really actually need it the way i wrote it down originally in like the body of the lecture notes rather than that footnote because we actually do in fact here have an infinite collection of events so i'm going to write this is going to be the sum over all the all of the omega that's in the sample space of the probability of that omega right so remember what this is actually doing is summing up the probability of h the probability of th the probability of t t h the probability of t t t h and so on all right uh continuing on uh we can then say that this is uh equal to um the sum over all omega in the sample space uh we have our probability assignment this is one half to the power of uh n of omega and at this point i'm just going to say that n of omega is equal to the length that sample is equal to length that string i can simplify what i'm writing down here a little bit by basically writing down uh what the image of n is uh under uh uh over the set s so i can now write equivalently that this is going to be the sum when uh n equals one to infinity of uh one half to the power n and now you should recognize from i think they talk about this in math 1010 at the university of utah intermediate algebra that this is a geometric cell this is a geometric sum and there is a formula for finding uh the value of of a geometric sum so recall this formula from your previous classes you have a a number r such that uh the the magnitude of r does not exceed one then the sum from uh n equals uh we'll say 1 to infinity of r to the power n is equal to r over 1 minus r remember that well we're going to use that here right now and we're going to say that this sum is equal to onehalf over one minus onehalf which is equal to onehalf divided by onehalf which is equal to one which is what we wanted to show we have now shown that the probability of the sample space under this probability measure is equal to one all right hopefully you have written down this formula if you don't remember it because i need to reclaim that space now that we have this we can now start answering some questions now that we know that this is valid probability model we can start using it uh so under this model what is the probability that the number of flips needed to see the first head uh exceeds four so what i'm asking for is the probability of um i'm gonna have to write this in a somewhat funny looking way go away stop stop bothering me i have to write this in a somewhat funny looking way i'm going to say that this is the probability of drawing an omega from the sample space uh or or drawing a sequence of flips such that um the length of that sequence is greater than four and this is another one of those situations where it's actually easier to work with the complement and say that this is equal to one minus the probability of drawing a string of flips such that the length of that string is less than or equal to four okay this is actually a finite set because we can actually list out the strings of flips in which uh in which the length of string is less than or equal to four we have heads we have tails heads we have tails tails heads and tails tails tails heads so this is one minus the probability no one minus uh the probability that you get heads on the first flip plus the probability that you get tails heads uh first tails then heads uh plus the probability they get tails and tails and heads and then you have the probability of three tails and a head and we can in fact figure out uh what each of those probabilities are the first probability is going to be one half the second is going to be one half to the power two or one fourth the third is going to be one half to the power of three or one eighth uh 1 8 and the last one is going to be one half to the power of 4 or 1 16 and long story short uh this is going to be equal to 1 minus 15 over 16 which is equal to 1 over 16. which is kind of funny it's a little funny to think about that why is it that it is equal to 1 over 16. hmm well here's kind of another way you could think about it um well it'll make more sense when you get when we start talking about uh independence but when thinking about independence what you end up doing is like we know that if a coin is equally likely to get heads and tails then we know that uh the probability of getting heads is one half so it's so what would then be the probability of getting four tails in a row um because that we must get at least four tails in a row in order for us to have to have at least four flips in order for us to get ahead so what we're actually asking for is the probability of getting four tails in a row so what i notice is well let's see it's if we were to think about um getting two tails in a row it seems like the probability of getting two tails in a row is one fourth and three tails in a row is one eighth um and then four tails of a row that's one over sixteenth hmm so that's an alternative way to think about what's going on here uh but there's an algebraic way to get it to an algebraic way to get the answer all right uh the second part what is the probability the number of the number of flips until the experiment ends in other words the last flip will be heads is between 3 and 20. this one is a little bit more painful but actually maybe we could use that trick well let's see uh we what we could potentially do is say um that the probability of observing a sequence of flips such that the length of that sequence um is exactly less than three well that's going to be uh so that's going to be the sequences where the length of the sequence is one or two so that's going to be heads or tails heads and that's going to be onehalf plus onefourth which is uh threefourths um now let's compute the probability of observing a sequence of flips where the length of that sequence is greater than 20. now this is not so far computing the probability of uh being between inclusively 3 and 20. but the reason why i'm computing these numbers is because i'm actually thinking i might want to use that complement trick again so i want to compute this probability where the length of the sequence is at least no is strictly greater than 20 and i could actually use the arguments that i was using above to argue that this is going to be a sum from n equals uh 21 to infinity of onehalf to the power n all right it's basically the same arguments that i was using before uh to compute the size of the sample space because this is going to be uh computing so you have the sum of the probably when you have 21 flips and then probably when you have 22 flips and 23 flips and so on and i could say what should i do next the thing is unfortunately i don't have a formula for when we're adding up starting at 21. i do have a formula when we add up starting at 1. so maybe what we could do is try to find a sneaky way to uh try adding up at 1 again what i could potentially do is say that n minus 20 plus 20 i could say that this is equal to the sum n minus 20 is equal to uh one because that's equivalent to saying that n is equal to 21 right certainly and then i could say this is one half uh to the power uh n minus 20 uh plus 20. hmm well i know i remember uh now that uh we can actually write this as uh we can write this as uh or the region the part enclosed in red as uh onehalf to the power n minus 20 multiplied with onehalf to the power 20. that's equivalent okay um so what can i use with that well i can now say that this is equal oops that this is equal to uh one half to the power oh i don't want red one half to the power 20 since that is effectively a constant that doesn't depend on n and i have a sum from n minus 20 equals one up to infinity of onehalf to the power n minus 20. uh but what i could do is say well n minus 20 uh you know what i'm just going to call that i i'm just going to call n minus 20 i'm just going to say that's i so replace all the n minus 20's with i so i equals 1 to infinity 1 half to the power i oh i know how to compute this i know how to compute this this is going to be uh one half times uh one half divided by one minus one half so actually this part right here i actually in fact this right here is pretty much a probability model uh this is summing over the sample space so this this part this blue part is equal to one so that means that this probability is equal to um is uh equal to one half to the power 20. think about that uh once again going back to this probability uh the probability of getting a sample of getting a string of at least 20 means that you must have flipped at least 20 tails and i've competed effectively the probability of flipping of getting 20 tails in a row oh all right now uh i'm still not done though uh let's uh start collecting this information um i now know uh let's see that uh so i'm gonna start erasing all this because i need to start reclaiming some space uh all right so uh this is equal to um uh one half to the power 20. all right so we can now say that the probability that uh of uh observing a sequence of flips such that three is less than or equal to the length of that sequence which is less than or equal to 20. i could work instead with the complement of this and say this is one minus the probability of drawing an omega such that either n of omega is less than three or n of omega is greater than 20. and here's the thing though i've basically written down two disjoint sets one set where n of omega is less than three or one set that is great where n of omega is greater than 20. um so so either a sequence ha is less than three flips or more than twenty twenty flips but it's not both so this is actually the union of two disjoint events and therefore i can say this is 1 minus the probability of drawing an omega such that n of omega is less than 3 plus the probability of drawing an omega such that uh the length of the string omega is greater than 20. and i have computed both of those probabilities so this is equal to uh 1 minus threefourths uh plus onehalf to the power uh 20 and i'm just going to leave it at that i'm not going to bother uh trying to simplify this because this is the correct answer so we're done at this point i'm not gonna go any further um i'm going to that said reclaim all this space and say um so hopefully you can rewind if you missed some of that and catch up on what you missed because i'm erasing this right now um and we're going to say that the probability of this event e is equal to 1 minus three fourths plus um uh one half to the power twenty all right uh next part what is oh yeah i don't want this this business all right uh what is the probability that an even number of flips is seen before the experiment ends hmm uh an even number of flips how could i possibly think about that uh so i want the probability of an e probability of an even number of uh flips and in short using some of the reasoning that i was using above i could say that this is the sum from of a onehalf how would i represent an even number i could say that an even number is 2 times a natural number that guarantees that the number the number is even so just take any natural number you want and multiply it by 2 and that's going to be an even number of fact that's going to cover all the even numbers and i'm going to sum this up from m equals 1 to infinity now what i do next well what i could do is recognize that i can basically take the m out and say that this is one half squared raised to the power m so that this is equal to the sum uh when m equals 1 to infinity of 1 4 to the power m oh i know how to sum that that's going to be since this is a geometric sum 1 4 over 1 minus 1 4 which is equal to 1 4 over 3 4 and those 1 4 parts cancel out so this ends up being onethird so the probability of an even number of flips is onethird which is kind of funny to think about if you think about it it seems like it like an even number of flips and an odd number of flips is equal equally likely but actually no they're not equally likely at all it's much more likely to have an odd number of flips since it's much more likely to get one one flip exactly okay um in fact getting one flip is twice as likely as getting two flips so by that reasoning it actually makes perfect sense okay uh moving on to the next part that was all a lot of work that was a lot of work okay uh example 14. in a small town uh 20 of the population is considered wealthy 30 of the population identifies as black and 5 of the population is wealthy and black select a random individual from this population everyone equally likely to be selected but i don't think that actually matters to this problem so much because of how i've laid it out what is the probability that an individual is wealthy and not black so let's see let me catch up in my notes here's how i like to think about this one with problems like this where you have uh let's let's start out by saying that we've got uh we've got a we've we've got a sample space but i'm not going to think too hard about what the sample space is for this one i'm just going to say we have an event b which uh corresponds to an individual being black or uh racially identifying as black uh w will be the event that a person identifies as no not identifies uh probably this is probably like some sort of a census business so the census bureau has maybe may have some uh more technical definition of wealthy like they make over ninety thousand is making over ninety thousand rendering you wealthy well you're certainly well off but um anyway that stuff aside we'll just say w that denotes the event that you are wealthy by some uh criterion and uh the probability of b that an individual is a is a black uh will be 0.3 and the probability of w that this individual is wealthy is going to be 0.2 okay and that's just true because the problem said so so now what we want to well actually we're also given another thing that the probability of an individual being both wealthy and black uh is going to be uh 0.05 oh by the way here's the thing to keep in mind um the probability of a and b is always going to be less than or equal to the probability of a or if you want you can replace the probability of a with probability of b it's just as true and that's because a and b is certainly a subset of a and if you think about um if you think about what about probabilities in terms of areas the area of a subset is certainly less than the less than or equal to the area of the thing it is it is uh nested within so you're always going to have a decrease in area it's not possible despite some human cognitive biases for the probability of a and b or something being true and something else also being true it is not possible for that to exceed the probability of the original thing is true i think people sometimes confuse uh intersection with conditional probability um that's probably what is going on when p when you start seeing those uh studies talk about stuff like that anyway uh we need to figure out uh what is the probability that this indiv that an individual selected from this uh from this uh village is wealthy and not black and here's how i like to solve problems like this and let's make it clearer all right here's how i like to solve problems like this i like to create a venn diagram and in that venn diagram i will start filling out regions so we'll have the wealthy circle and the black circle and we'll have this and we have the sample space uh and uh you start thinking about this like a puzzle like we know that the wealthy and black part is 0.05 and we're going to need to zoom in some more the wealthy and black part that is going to be 0.05 okay and then we have the wealthy part which is 0.2 but we can't point put 0.2 here because that 0.2 is also including that 0.05 so we need to put here is 0.15 so that when we add uh the this blue region and this red region those need to add up to 0.2 okay now for the black part um that needs to add up to 0.3 so that means that the part in that is not in the intersection is set to be 0.25 and i didn't write that very clearly um that needs to be point uh 0.25 okay which for what it's worth we now know the probability of w or b which would be 0.45 which means that the probability of being neither wealthy nor black is going to be 0.55 so that's something that i recommend when you encounter problems like these create a venn diagram and then fill out the diagram figure out the probability of every single little sliver in that venn diagram and then you can get any probability you want all right um and it will it will be like a genuine a generally useful chart for you not just for that individual problem because often these uh problems come in bunches all right so uh continuing on the probability of being wealthy and not black well actually that corresponds in our little chart up here to the blue region which is going to be 0.15 so this is going to be 0.15 okay what is the probability that the individual is neither wealthy nor black actually i already computed that i set it in words it's going to be 0.55 all right uh next example a ball contains a bag contains balls and blocks thirty percent of the bag's contents are balls an object is either red or blue and forty percent of the objects are red an object is made of either wood or plastic and sixty five percent of the objects are wooden ten percent of the objects are wooden balls five percent of the objects are red balls and twenty percent of the objects are red and plastic two percent of the objects are red plastic uh uh there's a typo here uh we're going to need to change blocks to balls okay reach into the bag and pick out an object at random each object equally likely to be selected what is the probability that the object selected is a ball red or wooden and this is inclusive by the way well this is another one of those problems where what i suggest you do is you create a venn diagram to represent the situation and fill out all of the little parts of that venn diagram so what does that look like here um all right so i've got my venn diagram this is my sample space uh right so i've got giant circles but i should probably be a bit more precise about uh let's start before i start filling out this chart uh let's create some notation and fill out what we already know in mathematical notation so we've got balls so b will be the event that we grab a ball and we'll say so objects are either red or blue so we'll say r is the event that an object pulled out of here is red so b complement is going to be a block and our complement is going to be a blue object and we say that so we can either have wood or plastic so we'll have a w correspond uh to the event that you get a wooden object all right uh let's start collecting some more information uh we have that the probability of uh drawing a stop it uh we have the probability of drawing a ball so the probability of the event b is equal to 0.3 the probability of getting a red object is a 0.4 the probability of a wooden object probably of a wooden object is going to be 0.65 uh the probability that an object is a wooden ball well that's w intersected with b since the object is both wooden and a ball that is going to be point one uh the probability of getting a red ball uh a red ball is going to be 0.05 the probability of getting uh something that's red and plastic so that's r and w complement that's going to be 0.2 and finally we have the probability of drawing a red uh plastic so w complement a block no no not a block it's no block it's a ball this is equal to 0.02 okay so so so so uh we need to start filling out this chart we're going to have uh we're going to have red here we're going to have balls here and we're going to have wooden objects over here uh i always recommend starting out with the smallest region first so the greatest intersection start out with that and then fill outwards so in this case the smallest region is red plastic balls uh where is that on our venn diagram uh you are in r you're in b but you're not in w so that actually corresponds to the blue region okay so the probability of the blue region according to the problem is 0.02 all right uh next up we could probably ask for uh what is the probability of being red and plastic uh red and plastic uh that corresponds to being um let's see so you're in the red region uh but you're not in the w region so that's going to correspond to this blue area and we know that the probability of being red and plastic is 0.2 but we've already got a 0.02 so that means that that in the other part we're going to be left with 0.18 so the probability of being a red plastic block is going to be 0.18 all right uh how about next uh we've got uh the probability of being a red ball so a problem the probability of being a red ball is going to be .05 what corresponds to red balls well this is the ball so we have the balls region and we have red stuff so that's going to be corresponding to the red region okay and that is 0.05 which means we've already got a 0.02 here it needs to add at 0.05 so that means that this little sliver in the middle is going to be 0.03 okay uh let's see we need next uh wooden balls so wooden balls corresponds to this green region uh the probability of being a wooden ball is point one so that we've already got a 0.03 so that means that we've got 0.07 in this other little sliver all right we're getting close to done uh what about uh let's see so we've got uh wooden balls red balls red plastic things uh what else have we got well we can we've got the probability of balls the probably balls is gonna be point uh 0.3 and that so balls correspond to this blue region and we've already got um 0.12 of the area accounted for the total area needs to be three point three so that means that for this uh remaining sliver uh we have point one eight for its area okay uh next up next up well we've got um the probability of red stuff and we know that the probability of red stuff is point four so point two so red stuff is going to be uh the region i've just encircled in red and so far 0.23 of that area is accounted for but point its total area is going to be 0.4 so that means that only so that means that when we're filling out uh this remaining blue sliver it must be 0.17 okay and for wooden stuff we know that the total area in the wooden region it's going to be 0.65 and 0.27 of that area is already accounted for so that means that the remaining area is going to be 0.38 all right and we are almost done we actually in fact know the probability of everything else if you were to add up all of those probabilities they would add up to 0.98 so that means that the remaining area that is outside of this region is 0.02 all right we have filled out that diagram and we are ready to answer some questions what is the probability that an object selected is a ball red or wooden so that's the probability of r or w or b and we have already figured that out that corresponds uh to the 0.02 so that is equal to 0.02 okay uh next up what is the probability that the object is a red wooden ball uh so the probability of being red of being red and wooden and a ball what is that going to be well we're going to go back to our diagram that we created we look for red wooden balls so you need to be in the red circle you need to be uh wooden so you need to be in the blue circle and you need to be a ball so you need to be in the green circle uh oh i guess that's a darker blue so that leaves us 0.03 so that leaves us 0.03 for that for uh that area so this is going to be 0.03 all right uh what is the probability let's go ahead and do some zooming out we no longer need such fine control when drawing uh what is the probability that an object is a blue plastic block well the probability of being a blue which is the complement of red uh not in a color sense not like when artists talk about complements that's a red is not a complement of blue but whatever in a probabilistic sense in our probability model uh so that's the reason that we've drawn here and it's actually a problem i'm gonna leave this to yourself but look up de morgan's laws uh learn about de morgan's laws uh for my students this is an assignment um i believe if i remember right but basically this is r union w union b complement which then means it's 1 minus the probability of r union w union b and that is a probability you've already figured out that's 1 minus 0.98 which is equal to 0.02 okay okay so that's it for the examples those are a lot of examples but i'm just gonna we're now wrapping up this section and i'm going to wrap up this section with a short discussion on the interpretation of probability now it turns out this is actually a rather large topic philosophers actually are debating uh what do probabilities actually mean and what is an appropriate definition of uh of uh of what a probability is because we have a mathematical notion of probability but that doesn't necessarily translate into a realworld notion of probability now i'm going to leave that discussion aside i might make an aside video about the interpretation of probability but in this class despite the limitations of this interpretation we're going to adopt the frequentest interpretation of probability which is interpreting probabilities as long run frequencies of events so in other words if we were to repeat an event an experiment many many many many times and each of those repetitions were independent of the past uh we would the sample proportion of times uh we would see that experiment occur would approach that probability so here's kind of and i here is a chart that kind of illustrates this idea we're going to have along the yaxis the yaxis ranges from 0 to 1 and somewhere along here we have the probability of some event uh e uh stop being uncooperative tablet all right so we have the probability of an event e okay and uh along the xaxis well it's not really the xaxis this time it will be the naxis and we're going to mark this axis off at integer values 1 2 3 4 and so on and um uh we are going to track uh p hat n which is the sample proportion of times the event e occurs and you remember how to compute sample proportions uh but here's kind of the thing about what we're doing here uh we are remembering pat the past so you could imagine that we uh are flipping a coin and we're tracking how many times we see heads in this uh experiment and uh uh so the first time you flip it you're gonna have either one or zero heads and uh the second time you're going to keep the results of that first flip but then recompute your proportion for a sample size of two uh for the third time you then flip the coin a third time and you keep the previous two results uh and then recompute the proportion using those previous two results and also the third flip you just did uh and so on keep doing this in a sequence so uh what could possibly happen so i'm going to put a little dashed line at the probability of e according to the frequentist interpretation of probability uh the probability is basically this longrun frequency where if you're tracking p hat n you'll start out at um like maybe zero or one depending on whether the event happened on the first trial or not and then you might track that it happens on the second trial and the third trial and the fourth trial and so on and what will happen is this line will get very very close to the probability of e um and that's how frequencies think about probabilities as a long run proportion as a limiting proportion um now the there is an unfortunate thing about this uh interpretation which that actually in mathematical probability theory that is not an assumption that's a theorem so this is not something that we assume is true this is something we prove is true so why are we assuming that something is true that we then prove is true that that seems like a circular reasoning and it's unfortunate but i'm going to leave those philosophical issues aside uh because at some level my own personal belief is that all of those technical philosophical issues aside at some level it's almost a limitation of human language uh and probability theory despite our difficulty in coming up with a definition a proper definition for it uh is a very real phenomenon so whatever the definition is it matches this frequentist notion uh i've actually got some r code here that you can look at uh this functions r is able of capable of generating random numbers and set seed is what's known as sets what's known as a random seed and make sure that when you're generating random numbers they actually are going to come up with the exact same results uh the same time so if you run this code it will produce the exact same results uh at least in principle uh different versions of software may cause different results but uh whatever so i set the sample size for my experiments uh i conduct a number of flips i track uh i accumulatively track the number of heads in these flips and then i plot these uh sample proportions and also draw a line at the theoretical probability of that event so this is the case when you were to if you were to flip a coin flip 15 times and keep a running proportion of how many times you saw heads this is what it looks like um and you can kind of see it seems to be converging around 0.5 if we repeat this experiment but this time when n is equal to 50 what we'll see is it kind of looks like it's getting even closer to something constant and then if we were to repeat this again but now uh the sample size is 500 it almost converges it almost converges to the truth now that said with probabilities it never means that you are guaranteed to see that many in any single sample you we did not say that uh which means that at the very end here you're not actually at one half you're pretty close to one half but you're not at one half but if you were to just keep going on like this and you were in fact possible for you to continue forever and god suddenly grants you the ability to live forever so long as you continue to flip this coin at the end of eternity you will have one half a flip uh half of your flips will be one half that's basically what we're saying all right so that's it for this video quite intense quite intense it's probably giving you some idea that probability can get rather out of hand um at least in terms of uh computational complexity uh but uh i've given you a number of useful examples and a number of useful techniques the thing though is those techniques like they're useful now and it's good for you to see them but you always kind of have to adapt to individual problems the next section is going back to the situation where you have a finite sample space we're going to talk about how you count elements from that sample space this part is both fun and frustrating because it's fun because we get to talk about things like poker problems and gambling and stuff like that but it's frustrating because counting is hard counting is really hard and also like i would love it to give you a magic formula that solves every and all counting problems and i will give you some formulas but those are kind of those are just tools that you kind of need to combine and the combination of those tools that's the hard part and i can't really give you a general principle for that every counting problem is kind of its own thing but enough of that we're we're calling it for now uh i will see you later in the future video on counting let's move on now to the section on counting techniques to start things off let's suppose that we have a burger shop that we're going to call and just off the top of our head bob's burgers and uh they're offering three types of bread we have white bread rye bread and sourdough a burger can come with or without cheese how many burgers are possible well let's see uh we have uh let's let's come up with some encoding for our possibilities uh we have white bread we have rye bread and we have a sourdough and a burger can come with or without cheese so we'll say we can have with cheese or we can have no cheese all right so given this encoding uh what are the possible burgers that we could have so we've got our possible burgers and we could have a burger with white bread and cheese a burger with rye bread and cheese and a burger with sourdough and cheese or a burger with white bread and no cheese a burger with rye bread and no cheese and a burger with sourdough and no cheat and no cheese so let's see how many burgers is that that's going to be six burgers uh so that's the way to do it by hand where you basically enumerate all the possibilities for uh how many burgers are going to be but the thing though is i mean that's only going to work for so long i mean you can you can enumerate stuff when it's possible to do and then literally count uh how many possibilities there are but there's also other ways to possibly represent what's going on here for example we could use what's known as a tree diagram with the t tree diagram we're going to create a tree that visualizes um the branching possibilities of our choices so for example at the first node we're going to choose the type of bread so we have white rye and sourdough and then at the second level of node we decide whether we're going to have cheese or no cheese so we have um for the upper branch cheese and for the lower branch no cheese and we're going to do this a few more times for the different types of bread that we could have chosen and we've got cheese no cheese cheese no cheese and then we're going to count how many no's there are on the end points and the number of nodes on the endpoints will correspond to the number of possible choices in this case there are six nodes so that means six possibilities and of course one thing that's nice about these types of tree diagrams is that we could allow for more flexibility in our possibilities such as perhaps for some reason bob has decided that he is not going to permit a sourdough bread without cheese in which case you just remove that possibility from from the from the tree diagram you remove that branch in which case there would now be five nodes in the branch so it's pretty general uh there's also something that we can use uh called the product rule uh proposition seven if there are n1 possibilities for choice one and two possibilities for choice two all the way to nk choice of possibilities for choice k then the total number of possible combinations is going to be the product of the number of possible choices we can make at each point so uh let's use the product rule to answer this question according to the product rule we could have three possible choices for the first node and two possible choices for the second node and thus the total number of possibilities will be three times two which is equal to six all right so we've seen so far uh six ways to basically answer this question uh now let's uh move on some more uh the sandwich shop deluxe deli offers four bread options where we have white sourdough whole wheat and rye five meat options turkey ham beet beef chicken or no meat six cheese options cheddar white cheddar swiss american pepper jack and no cheese with or without lettuce with or without tomatoes whether without bacon with without mayonnaise and with without mustard how many sandwiches are possible let's see uh how many decisions do we have to make first we need to decide on our bread option so we'll say that that's uh decision one so we'll say that n1 there are uh four possible options so four then we can decide uh our meat options and there's uh five meat options uh we have six cheese options so n3 will be six uh there are and then for the remaining options it's all binary so we've got with or without lettuce so that means that n4 equals two with or without tomatoes so n5 equals two with or without bacon so n6 equals 2 with or without mayonnaise so and 7 equals 2 and with or without mustard so n 8 equals two okay so based off this how many sandwiches are possible well according to the product rule we're just going to multiply all of those numbers together so we've got four times five times six times uh two five times so we'll say two to the fifth power so this when you multiply all this out you're going to end up with 3 840 possibilities for this sandwich shop okay so uh moving on here's here's the thing we are now uh very fully into the realm of combinatorics combinatorics is generally figuring out how many ways there are to do things and uh combi or how many combinations there are of things how large a finite size sets are when they're generated with certain rules and honestly combinatorics is pretty hard like for example i find combinatorics rather challenging um i remember once when i was in a probability class and i was in uh excuse me i was in office hours with a professor and a fellow student of mine just we were discussing combinatorics and uh my a fellow student of mine was just like is this what probability is or statistics is because if it is i i don't know if i want to do this and he was like no it's not uh you just kind of have to do this you have to learn this at some point but it's not what the bulk of probability is and i can understand why she said that because it can get pretty painful here's the thing about combinatorics uh what i'm about to do is give you some tools some counting tools for problem for counting type problems that are often reappearing but the thing is every single combinatorics problem is its own thing it's really hard to come up with general tools like what i'm coming up with here um there are certainly tools in the toolbox uh things that you kind of look out for when you're solving combinatorics type problems but you still need to think of it of each problem as its own thing and for this section i kind of have to come up with a number of examples and run through those examples to give you an idea of the thought process of combinatorics but it's really hard to teach that thought process uh without example since i can't just give you an algorithm that will solve every single covenantoric's problem every problem is its own beast and you're pretty much forced to think very carefully about uh the process by which a single combination has been formed uh also i should probably just mention uh if we have uh there's there's something that i didn't mention in these uh typed up notes but i call this the sum rule uh so uh if you're choosing uh from k uh k disjoint sets so sets each with uh each with um we'll say n sub i uh possibilities uh possible choices uh at one stage so you're gonna pick uh one of these sets and an item from that set uh then in this case uh there's going to be the sum from i equals 1 uh to k uh n sub i uh possible so possibilities so in other words if you need to choose an item from bin a or an item from bin b exclusively then the number of ways you can make that decision is going to be the sum of the items in those two bins not a particularly difficult concept but i'm going to just lay it out for you right now just so that you are aware of that and it's a it's a valid approach to solving cognitoric's problems and i would say that the the uh techniques that i'm about to show like these will these will get you through these are things that you look out for when solving combinatorics problems uh but uh they probably won't take you all the way all right so uh when we're solving some problems uh suppose that out of n possibilities we will be choosing k we have two essential questions to answer um are we choosing with or without replacement and does order matter depending on our answer to those questions we're going to have different solutions that are summarized below uh here uh if if the choice was where uh there's order and we're doing so with replacement then the number of possible outcomes is end of the power k uh ordered without replacement that's known as a permutation and here is a formula for a permutation uh n factorial by the way so this so uh n with an exclamation point means n factorial and that is n times n minus 1 times n minus 2 times dot dot dot times 3 times 2 times one that is n factorial and by convention zero factorial equals one huh that's kind of strange uh at least for me when i first saw that zero factorial equals one i found it rather surprising but actually it makes perfect sense for zero factorial to equal one and i might may explain why zero factorial is one in a separate video but we're just going to leave it at that so that is what n factorial is um and one way to think of what n factorial is it's it's the number of way to order number of ways to order n things so it's the number of permutations of n things um uh now in this case for uh ordered with without replacement uh we we all we're also calling that a permutation uh it's just a permutation of k things out of n so uh suppose that uh we are that we don't have replacement so remember what we're so think about what replacement means it means that uh so replacement means that if you choose uh an option you can choose it again so you're allowed to choose it again an example of of ordered with replacement is a sequence of heads and tails in coin flips where you may consider possibly you may not but if you're looking at a string there are different ways that heads and tails can manifest themselves in the string and furthermore if you get heads on the first flip you're still allowed to get hats on the second flip whereas if you don't have replacement you can get heads in the first flip but you can't get heads on the second flip so that would be without replacement in order matters that means uh you're that means that the order in which you see an item in a sequence matter so you're tracking that so headsets tails is different from head's tails heads whereas if order doesn't matter then headset's tails is essentially the same as heads tails heads because you don't care about the ordering of the items so that's what we mean by whether without replacement or with or without order okay so suppose that we're choosing items where order doesn't matter and uh we don't have replacement in that situation uh we have we're going to use the formula n choose k which is n factorial divided by k factorial uh and or the product of k factorial n minus k factorial i'm going to prove each one of these formulas in a second because i do believe that the proofs are enlightening on the combinatoric thought process and the combinatoric thought process is something where you just kind of have to get exposed to it a lot in order to be able to uh think that way yourself um finally we have the situation where uh you are you're choosing with replacement but order doesn't matter in which case you're going to have k plus n minus one choose n minus one okay so next up is uh the uh the proofs or the justifications for each of these formulas we're going and i'm going to uh prove this in um a sneaking fashion or a clockwise matter because often the formula after the formula in this clockwise order from before is going to be used in the next proof okay so uh let's get started uh we're going to start by showing by proving the formula and we in which case we have ordering and so and we also have replacement all right so ordered with replacement i'm going to zoom in because i'm going to need some uh i'm going to need more space all right so we want to come up with the formula um n to the power k well all right so we have k choices to make and since we have replacement we're going to use the product rule the product rule is kind of the underlying uh rule that we can use for all of these so starting out with the product rule we have k decisions to make i like to think of confidence of a combinatoric problems where you are describing any combinatoric problem how to construct an instance of an element of this set and when you are thinking about how to construct an element you construct a narrative for how to uh construct one of these elements and you're counting how many ways there are to make the decisions along the way and you're tracking how and you're tracking how many decisions you need to make and using the product rule the entire time so uh we can think of if we're doing ordering with replacement we might have for example we might have for example three spaces and we need to fill up those spaces with elements so we might have two possibilities for each space so at the first space we would have two possibilities and at the second space we'd have two possibilities and at the third space we have two possibilities and we care about the ordering and um we care about the ordering of what we put in in these spaces so the number of possibilities for filling up these three spaces would be 2 to the power 3 because we're going to multiply and we have to make a decision at each space and when you have to make a decision at each space you multiply the possible number of decisions that you needed to make so uh generalizing this idea uh we have uh we need to make a decision in the first slot the second thought the third slot all the way to the kth slot and for each of these and then and for each of these slots there were n possible things to choose one from because we're not taking we're not removing options as we go through our sequence of case slots so then by using the product rule according to the product rule we end up having to multiply um we have we end up having to multiply uh each of our possibilities or each of the possible options we can make at each slot but since all of those are the same since all of those are the same number you end up with multiplying n k times which is n to the power k so that gives us the first formula the next formula we need to come up with is ordering without replacement which gives us the number of permutations okay so we now next have oops that is definitely not what i wanted all right so next up we have ordered without replacement i'm going to go back for a second and i have in my head prototype problems for each of these four scenarios so ordering with replacement is like determining how many strings of heads and tails there are in a string of flips because the ordering in that situation would matter because your track because heads tails it's not the same as tails heads in a string and you have replacement because if you get heads on the first flip you can get heads again on the second flip um ordering without replacement to me my prototype problem for that is forming a list because the item that you put at the top of the list cannot be chosen for the sec for the next item in the list so you are removing items as you go through this list and maybe it's like a times top top 100 people or 100 people for the year list where the ordering matters and there's certainly more people than 100 so they might have this larger this larger master list of people they would consider to be candidates on their top 100 list so you're going to choose people to be in certain slots on this list and when you choose a person to be on the list you can't pick them again so that's kind of my prototype problem for ordered with without replacement for um not ordered and with replacement this is like uh poker problems because when you draw cards from a deck of cards you are allowed to reorder the cards in your hand so there is no ordering but once you draw a card you cannot draw it again so there isn't replacement and for the final situation my prototype problem here is choosing a dozen donuts when you have uh so many flavors of donuts because you are allowed to reorder the donuts in the box and in principle the donut shop has this almost infinite list for the number of or an almost infinite supply of donuts in this imaginary donut shop so there is replacement when you choose a flavor like when you choose a flavor you're allowed to choose as many donuts of that flavor as you want so a replacement is not an issue all right so you do have in fact replacement those are my prototype problems and those are good to keep in mind when we're going through and trying to think about what formulas work we should have and how we're going to prove these formulas and additionally uh it's kind of good to have these prototype problems in your mind when you're trying to solve common torque problems yourself all right so next up uh getting back to these proofs uh we need to show the formula for ordering without replacement uh so i had that formula for n factorial um so all right so how many ways are there to uh form a list of k things uh when you have n possibilities so we would need to pick the first item in the list and the number of ways that we could pick the first item in the list is n then we need to pick the the so there are n ways to pick the first item in the list then we need to pick the second item in the list so there are going to be n minus one ways to pick the second the reason why is that whatever we chose to be our our first element in the list cannot be chosen again to be the second element so we have n minus one ways to fill the second item then we have n minus two ways to pick the third the third item because now now in addition to not being able to pick what we chose for the first item in the list we also can't pick what we chose for the second item of the list so we're going to have n minus two way uh mis two choices now so mi it's two ways uh for the second item no no not the second the third and we just continue on with this uh with this process until we reach the kth slot uh and there will be uh for the kth item in the list there will be n minus k minus 1 ways to pick that item which by the way is equal to n minus k uh plus one right so so n minus k plus one ways to pick the cave item so then we're going to use the product rule and by the product rule we multiply all of these numbers together so we're going to get so the number of possibilities is going to be n times n minus 1 times dot dot times n minus k minus 1. and if we wanted to we could stop there this is in fact the formula but thing though is it's sometimes uh better to we we might prefer a more compact uh formula so we might say instead that um we have n n minus 1 n minus 2 uh dot dot dot and then we have n minus k plus 1 and then we could keep going multiplying and say we're going to multiply by n minus k and still you know still decreasing down but but we've now multiplied by n minus k and if we're going to multiply by n minus k we now need to divide by n minus k in order to keep things balanced and then we're going to multiply by n minus k minus 1. okay so we're going to need to divide by n minus k minus 1. and we're going to keep going with this process on both the top and the bottom until we're multi we multiply three two and one and what i've actually written down in doing this the top can be recognized uh so the top uh part of this uh fraction you you may recognize that as being uh i don't want that color you may recognize the top part as being n factorial and the bottom part of the fraction the denominator as being uh as being n minus k factorial hence uh producing the formula this will produce the formula uh n factorial over n minus k factorial which we may also just call p and k and that gives us our second formula okay uh next one very now what we now need to do is figure out how many ways there are to choose items that are not ordered and there is no replacement this gives us the formula that is often referred to in english as and choose k so the proof for this one is actually quite tricky where you start out by assuming that you know how to do it and then you get recover the formula in the end where you you pretend that you know the formula and after you pretend that you know the formula you then figure out something that you already know which in this case is the number of permutations of a list and once you have the number of permutations in the list you're able to recover uh the formula that you pretended that you knew but you actually actually didn't all right so that's that's rather convoluted let's get started with uh uh showing how we can get this formula so the proof again is kind of weird it's a really weird one but all of a sudden at the end we're going to have the result that we wanted so we have no order and we don't have replacement i keep touching that all right i keep touching it alright so not ordered without replacement so suppose uh n choose k is the number of ways to is a number of ways to do this so we know how so we know the number of ways to choose items when we don't care about order and we don't care about replacement and the number of ways to do so is n choose k okay bear with me i now want to know how many ways are there going to be to pick to pick uh k items out of n when replacement doesn't matter so as opposed to um doing so in order doesn't matter which we're assuming that we know uh that is what i want to do i want it what i want to do is calculate uh uh p and k or the number of of k length permutations of n objects okay so uh i want to compute that which by the way we have already computed that's that formula is actually already known to us it's up here but we're going to suggest that there is an alternative way to calculate this if somehow you knew the number of ways to pick k items out of n uh without without order so how would we do this how would we let's think about how we would construct a single permutation if what we had to do was pick items without order first so our first step if we were to attempt to form a single list of k items out of n when we can pick items without ordering them the first thing we would do is pick the number of items or pick what items will appear on the list without ordering them first so our first step in our process is to pick objects when order doesn't matter so you've decided basically what's going to appear on the list you just don't know in what slots it will appear and supposedly we know how to do so there's and choose k ways to do so so there are n choose k ways to pick the items that will appear on our list without ordering the items so first we pick a set of items that will appear on the list the next step then is to order those items so the second step is to order the k objects so how do we order the k objects well uh we picked the first so we pick uh one of the objects that we have selected to be the first item of the list uh there's k ways to do that then we pick another one of the objects that we haven't picked yet to be the second item on the list there's k minus ways to k minus one ways to do that keep doing so until you run out of items so there are uh k factorial ways to order the items oops oh darn it so there's k minus one ways to order the items going back to where i was or k factorial ways to order so now we're going to use the product rule and say that the number of ways to pick uh items to appear on our list is going to be the number of is going to be the number of decisions we have to make in step one which is the number of ways to pick the items to appear on the list uh there's n choose k ways to do that uh and then multiply that with the number of ways to order the items so there will be k factorial so that's the number of ways to form permutations but we also know that the number of ways to get permutations from what we did before is n factorial divided by n minus k factorial okay well what we actually were interested in this whole time was calculating this number that i just highlighted in red that's the number we actually want well how can we get that with division because now we have an algebraic relationship uh with which we can so that we can solve to get and choose k and it follows that n choose k is equal to n factorial divided by k factorial times n minus k factorial and we're done we computed what we actually wanted to compute so that was a little odd that was a little strange um here's some more ways to kind of justify uh this formula that we ended up with uh so the number of permutations is larger than the number of combinations because the because when you are sensitive to order you're going to end up with many more possibilities than if you're not sensitive to order so as a result you need to divide out uh by a factor that effectively removes uh all of the orderings that are that contain the same items that just in different and just in a different arrangement or to get the combinations and when it comes to computation like let's say for example three choose no uh five choose three um this formula for me at least when i was taking this class the way i think of it is i have five factorial on the bottom and on the bottom i'm going no i have five factorial in the top and on the bottom i'm going to have 3 factorial and whatever it takes to get 2 and the other number such that 3 and the other number adds up to 5. so i'm going to have 5 factorial divided by 3 factorial times 2 factorial okay so that's a way so the formula is actually not that hard to remember because it's like okay uh 10 choose 7. that's going to be 10 factorial divided by well we've got 7 factorial down there and we've also got something that adds up to ten okay three factorial so ten factorial divided by seven factorial times three factorial so the formula itself is not that hard to remember uh but you now have it okay uh continuing on so that was our third formula that i promised that we were going to prove uh so now we have one last formula to prove uh and that formula is let's see how much space we got uh hopefully we've got enough so the last formula we have uh not ordered and we have replacement so this is the donut shop situation okay so the donut shop uh situation uh here's like this this proof is again also rather tricky combinatorics often requires tricks honestly that that's that's one that's one reason why combinatorics is rather painful uh it feels like there are rather few unifying uh principles and combinatorics i know that there are some but it doesn't really seem that there's all that many so here's what we're going to do to um to solve this one uh what let's let's suppose that we're in a donut shop and we're going to choose say five donuts of three flavors uh how could we possibly do that uh we don't care about the ordering of the donuts because you put in the box and i mean no one's going to ask in what order the donuts were when you come home so what i like to do is imagine okay whatever we're going to do we're always going to arrange the donuts in the box right so so whatever arrangement they originally were we're going to put them into a fixed arrangement where we have one flavor first one flavor second and one flavor for flavor third so what we could end up doing to form a single box after we say that we're going to arrange to rearrange them at the very end is we're going to have uh we're going to uh put down all the donuts of a single flavor and we know that the donut that comes first is going to be of a certain flavor but we're also going to put dividers in our box to separate flavors so so the second flavor will come after the first divider and the third flavor will come after the second divider so and it is also possible to uh not have a donut of uh certain flavors so for example if we wanted a box of only the third flavor we put a divider in the first position of divider in the second position and then put donuts in all the remaining positions and with this encoding i have now encoded one of the one of the boxes of five donuts this is a box where you have only the third flavor and you can imagine what a box consisting of only the second flavor would look like so you'd have a divider at the beginning and a divider at the end and then in between you have your donuts so this is a box that has only donuts of the second flavor and maybe play around with this encoding of donut boxes but once we have this encoding it is now possible to calculate how many how many boxes we can achieve because what we can do is say how many ways are there to pick positions for dividers and positions for donuts how many ways are there to do that well we're going to have if in the if we have uh three flavors we're going to have two dividers and if we have five donuts we're gonna have positions four or five donuts so we're gonna end up with seven positions that we need to fill up uh we could just pick two of the seven positions to contain dividers and the remaining positions will contain donuts so let's see we could potentially pick numbers we could assign a number to each of the positions so we have positions one two three four five six seven and we pick numbers that represent positions that will contain dividers so in this case uh uh numbers four and six represent positions four and six so those will be uh so if we pick four and six which by the way is the same as picking six and four to contain dividers so in other words we don't care on the ordering of the numbers that we end up picking uh once we pick that we now know what our box of donuts is going to be because we've picked the positions for the dividers and therefore every other position will contain donuts okay all right then um so if that is the case how many boxes of donuts are there going to be um so in this case the number of possibilities which is n in this scenario is equal to three and out of that n we're going to be choosing uh k possibilities uh or we're going to be choosing k items or k donuts so for this problem k is equal to five okay so um how many ways were there to make this decision well it turns out uh there were um there were five plus three minus one choose three minus one ways to pick donuts in this fashion because we end up picking the or in other words this is uh uh this is seven choose two because we have seven slots and we pick two of those slots to contain dividers and the rest of the slots contain donuts hence we get the number of donuts all right so let's generalize this idea so if we have n possibilities we're going to have n minus 1 div n minus 1 plus k slots because we're going to have n minus 1 dividers and then also the case slots for the k things that we're going to end up picking so in general um uh so let's see uh so we're so we have um so we have uh n that no uh k plus n minus slots n minus one slots uh to fill with uh n minus one dividers which we call uh which we're going to call the the bar uh remaining slots so the remaining slots contain items uh so we need to pick uh so pick the positions let's see yeah so pick positions for dividers and order doesn't matter so since order doesn't matter there are going to be uh n plus uh no uh i like a different ordering uh there are going to be k plus n minus one choose n minus one ways to do so and we're done you end up with the formula and we're done so that was exhausting uh this is rather tricky types of mathematics that honestly it's it's frustrating because it often just requires knowing special tricks in order to solve a certain problem it just feels like all you're doing is coming up with a longer and longer list of tricks and it doesn't really feel like there's much of a unifying principle to them uh at least to me at least to me personally it's it seems also to me like there are some people out there some really smart mathematicians for which this type of thinking just for some reason just clicks and they and they are able to see an underlying principle i don't see it neces uh often uh but i often know enough combinatorics like this co this amount of combinatorics will get you very far in life uh in your statistics life that is uh you don't really need to know that much more than this all right uh so those were complicated proofs uh now we're gonna go through a series of examples to show uh how these techniques can be applied so uh suppose we're going to roll two sixsided dice and we assume each outcome is equally likely and we're going to say that the dice are different colors so uh if the red dice has a six and the blue dice has a one that's different from the red dice having a one a blue dice having a six those are two different pop those are two different outcomes how many possible outcomes are there and what about the situation when there's three six sided die uh so we're to first answer problem that i'm going to call one and the problem that i'm going to call two all right uh so the answer to one uh order matters and we're doing so with replacement because if we roll um a one for the red dice we're still allowed to roll a one for the blue dice so we end up with uh there are six possibilities and we're choosing two of them so we get 36. all right for the next option uh well it's just the same things now there's we're just choosing three instead of two so that's going to be 216. so this is a situation where order matters because the dices the dice are different colors and there is replacement because dice don't care what the other dice rolled all right uh next example a high school has 27 boys playing men's basketball in basketball there are five positions point guard shooting guard small forward power forward and center each assignment of player two position is unique how many teams can then be formed this is a this is a permutation type problem so here order matters because there's different positions and each of those positions are distinct but there is a replacement because a person playing point guard cannot also be center so position doesn't matter oh no position matters or order matters no sorry sorry no replacement no replacement i get it eventually all right so no replacement so that means we're going to be using that a second uh that second formula in the clock so we've got uh p uh there's a 27 possibilities we're going to order five of them so that's going to be 27 factorial divided by 27 minus 5 factorial which is 27 factorial over 22 factorial which also can be written as uh maybe more simply into the point 27 times 26 times 25 times 24. times 23 that's almost easier than remembering the formula just remembering that you decrement that many times and this multiplies out to uh 9 million six hundred and eighty seven thousand six hundred potential teams okay uh example nineteen when playing poker players draw five cards from a 52 card deck every card is distinct but the order of the draw does not matter you are allowed to reorder the cards in your hand how many hands are possible in this situation because you're allowed to reorder order doesn't matter and since order and also an additional ordering not mattering there is no replacement because if you draw an ace you can't if you draw an ace of spades you're not allowed to draw an ace of spades again so no replacement okay uh so let's see if that's the case then we're going to use that fourth third formula in the clock uh we have uh 52 possibilities for 52 cards we're going to choose five of them when we draw cards and that's going to be 52 factorial divided by 5 factorial times 47 factorial which is equal to 52 times 51 times 50 times 49 times 48 divided by 5 times 4 times 3 times two times one okay uh and we and we don't care about the one because it's times one that that doesn't really do anything um we can do some cancellation like for example the 50 and the five cancel down to a ten uh the 4 3 and 2 those met multiply the 24 so that reduces with the 48 rendering it a 2 so this is equal to 52 times 51 times 10 times 49 times 2 which you then go to your calculator and it will give you 2 million uh 500 and thousand uh 960 poker hands now r can do a lot of these calculations so for example 16 you can just say what is 6 to the power 2 and i'll tell you that the 36 and 4 6 to the power 3 that's 216. r does have a factorial function now be careful with some of these functions because you might end up with integer overflow you might end up with numbers that are so large that uh the computer cannot handle them so i would not just blindly use these functions because it is possible for these numbers to explode very rapidly in which case your calculations will end up being wrong but we do have a factorial function and we do have a choose function so here's example 17 and here's example 18. um uh so or is that that number might be wrong yeah that that that numbering's wrong my apologies uh so i guess at some point when i wrote this r code uh i uh or when i wrote these notes i must have deleted an example a long time ago but i did not change the comments in the r code this is why i need to be very careful with comments comments expire eventually they turn bad all right and you know what's worse than a note then no comment a misleading comment that's even worse um all right so example 20 you want to choose a dozen donuts from a donut shop there are eight different kinds of donuts how many boxes of a dozen donuts are possible well okay so in this situation uh we are choosing 12 donuts and there are eight possibilities so using that uh fourth formula in the clock we have uh 12 plus 8 minus 1 choose 8 minus 1 possibilities which is going to be 19 choose 7 which is equal to 19 factorial divided by 7 factorial times 12 factorial which is equal to 19 times 18 times 17 times 16 times 15 times 14 times 13 divided by 7 times 6 times five times four times three times two times one all right and uh let's do some cancellation to help make our life a little bit easier uh let's see the 7 and the 2 are going to cancel with the 14 we've got what else 4 and 16 will reduce the 16 down to 4 the 5 and the 3 will cancel out the 15 and the 6 cancels out with the 18 reducing it to 3 so in the end we're going to have this is 19 times 17 times 13 times three times four and then you go to your calculator and ask what that is and you'll get fifty thousand uh 388 potential boxes of donuts from this donut shop okay and here is some arco that will also compute that quantity okay uh so let's do some classic poker problems once people uh introduce combinatorics it's like the next thing you have to talk about are poker problems because poker problems are fun because poker is fun it's fun to talk about poker um so um now that said there is uh one potential was talking about poker problems is that unfortunately not everyone is familiar with the poker deck or the standard playing card deck as it's known in the englishspeaking world which for what it's worth the standard deck is technically the french deck and different european countries have different traditional playing card decks so like for example my advisor leo horvath the traditional deck that's used in hungary is not the french deck so he doesn't like personally poker problems because there's something that like they talk about a deck and he's not very familiar with that deck and it just goes against his intuition whereas i myself i grew up with this deck i grew up in america so i'm very familiar with uh with what is inside of a playing card deck that said if you're like an international student or something like that here is a description of what is inside of a playing card of a standard playing card deck or a a french deck the deck that's used in the english speaking world and often used in these probability textbooks because most of these probably authors are most most of these most of the authors of these uh probably books they may be in america but they're certainly speaking english they're probably using uh this deck so and here's some additional notation for uh this is a poker notation to describe what goes inside uh what cards are inside of a deck all right so but basically you've got four sweets and 13 possible faces uh all right for a total of 52 possible cards you should know how to do that by now because uh how do you form a single card you first pick a suit there's four suits possible then pick a face value there's 13 faces so 13 times 4 will be 52. all right so you've learned something today uh example 21 a poker hand is four of a kind if four cards have the same face value how many four of a kind hands exist how are we going to solve this problem well uh the trick that we're going to use again with these poker with these not just poker problems but most combinatoric problems what i suggest that you do is come up with a narrative like i just did right now for figuring out how many cards there are in a deck come up with a narrative for forming a single combination and then once you have that narrative figure out how many choices there were to make at each at each junction and multiply them together with the pop with the uh power rule and you'll get what you need um so first thing we're going to do to form a four of a kind hand is we're going to decide what card face value will be the four of a kind card so first we're going to pick uh the four of a kind card so we're going to say for example they're going to be four kings in this hand or uh four twos or four tens something like that so we need to pick a face value and there are 13 face values to be the four of a kind part right so there are 13 ways to pick the face value then we need to pick the remaining card because a poker hand has five cards we have picked four of those cards if we decided that we were going to use the ace we were going to use ace for the four of a kind card then we've automatically got the ace of spades the ace of hearts the ace of diamonds and the ace of clubs that's four cards now we need to pick the remaining card if we picked ace then we cannot pick the ace face value again so uh we've uh and in fact in that 52 card deck we have taken out four of the cards and put them in our hand leaving 48 cards remaining in our deck so that means that we have 48 cards to be the potential fifth card so pick the fifth card so 13 times 48 you multiply those two numbers together to get 624 poker hands with four of a kind uh next up a poker hand is considered full house if two cards have the same face value and three different cards have another common face value how many full house hands exist let me just list out for you an example of a full house hand you could have let's say the four of diamonds the four of spades so the four of spades and uh the four of uh hearts and uh so that's three cards with a common face value and then we need two cards with another face value we can't pick four for this so we're going to pick uh let's say king so we'll have the king of uh clubs and the king of uh diamonds so this would be a full house hand so let's see how can we form a full house hand in general well we're going to have two different face values in our full house hand but one face value will be the more numerous face value the three of a kind and the other face value will be the two of a kind so what i suggest we do is first pick the card that will be the three of a kind card so pick the card that will be the three of a kind card so pick the three of a kind face value and there's 13 face values for this choice so there's 13 ways to make that choice then we need to pick the suits because we picked for example four but there are four suits from which we can choose and we need to pick three of them so now we need to pick the suits and order doesn't matter in this situation we don't care about the ordering of the suits we just need to pick them so so we need to pick exactly three suits for all those three cards and since order doesn't matter for this choice we have four choose three ways to pick the suits and four choose three evaluates to four which is not surprising you know like alternatively and by the way this this thought process is quite useful instead of picking the three faces we're going to include we could have picked the one face no no no no not face i am so sorry i for the life of me i can't keep my word straight i'm always doing this uh so not face but suits uh instead of picking uh the three suits we're going to include we could pick the one suit we're going to exclude and there's four ways to pick the suit we're going to skip all right so there's an useful trick to keep in mind uh next up we need to pick the two of a kind card we need to pick the two of a kind face value so we can't pick what we chose before we like for example we couldn't choose 4 again so we need to pick one of the 12 remaining face values so there's 12 ways to make this choice and then after we pick the face of the two of a kind card we now need to pick the suits so we need to pick two suits and again uh we need to do so where we don't have replacement but order doesn't matter you can't have replacement because there are not two uh king of hearts in the deck there's only one king of heart so you pick so yeah you can't pick the same suit twice uh so um of the four suits we need to choose two of them to include so that's going to be 4 choose 2 4 choose 2 evaluates to 6. so in the end this is going to equal 13 times 12 times 6 times 4 which is equal to uh 3744 uh full house hands all right uh and here's some r code that's computing those quantities uh notice by the way this is a useful trick to to know to be aware of uh when doing uh when when using r notice that i wrapped the entire expression in parentheses if i didn't put the parentheses there nothing would have printed but when i wrap an entire expression in parentheses uh when i'm doing some variable assignment the variable the value of the variable that i just assigned gets printed which is really nice so if you remove these parentheses these parentheses the 624 would not have been printed but since i put the parentheses there it in addition to doing the assignment done here it prints the value of the variable that's a that's a nice trick all right so example 23 a flush is a poker hand where all cards belong to the same suit how many flush hands exist including what's called a straight flush a straight flush is a flush where the cards are also where you can order the car so that they're in sequence uh poker actually has a number of sequences but for example a hand where all the cards are spades and the cards are five six seven eight nine that is a straight flush since they're also in or since you can order them right and the straight flush is considered a different kind of hand in poker that's a straight flush is in fact the best possible poker hand so we actually should be accounting for straight flushes because generally when people say flush they are not including straight flush but we're just going to include straight flushes for now um we're going to allow that possibility so uh for a flush hand we're going to so in a flush hand all the cards have the same suit there are four possible suits so the first thing we need to do is pick the suit and there are four ways to pick the suit after we pick the suit we need to pick the face values so there are 13 possible face values we need to pick five of them to be in our hand so uh once you pick one face value you cannot pick it again because there are no two king of spades for example so there are 13 face values and we're going to choose five of them we don't care about the ordering and we're going to do so without replacement so you multiply those two numbers together uh this is going to be four times uh 1287 that's what that uh 13 choose 5 evaluates to and this is going to equal 5148 but this is also including the straight flush hands in the next problem i'm going to show you how we could potentially remove the straight flush hands because i'm actually going to ask that we that we do in fact remove them so a straight in poker is where cards can be arranged in sequence so for example we have five of uh five of spades six of clubs seven of clubs eight of hearts nine of hearts so this is a straight and the suit doesn't matter for a straight unless it's a straight flush if it's a straight flush the suit must be the same because it's also a flush so a straight flush is both a straight and a flush so it has a flush with all cards no it is uh so it is um uh i should change the that wording uh it is a straight it is a straight with all cars belong to the same suit and it is also the best possible poker hand uh hold on okay good all right uh how many straight flush hands exist we're going to compute that number now how many straight flushes exist so to get a straight flush so let's uh let's uh start some separations so first thing we're going to do is compute the straight flush all right so excuse me uh here by the way um is uh the uh the ordering of poker hands because poker poker face values have a ring uh have a ranking uh we have ace two three four five six seven eight nine and i'm gonna write x for ten the roman numeral x then we have uh uh jack queen uh king and also ace again because ace can be both low and high um and in fact the best literal possible poker hand is 10 jack queen king ice all of the same suit that is the best straight flush okay um so uh to have so to so for the flush part we we need to figure out how many ways there are for the flush part and how many ways there are to get the straight part so there are four ways to pick the flush part the part where you have the same suit so we're going to pick the suit for the flush part and then we have the straight part and the straight part for that what i would recommend you do is think about how many ways there are to pick the first card so pick the first card of the straight because if you know that the first card or the lowest card of the straight is ace since you there are five cards in hand that means that the other four cards are two three four five right so how many ways are there to pick the lowest card in the straight so let's see we got uh one two three four five six uh five six seven eight nine oh ten what do you know because the moment you get to ten the the uh the straight hand would be ten jack queen king ace and there is no straight that starts the jack because there is no you would not be able to get the fifth card since you can't circle around back to two so um so that means that there's ten ways to pick the lowest card in the straight so that means that the number of straight flush hands is going to be 4 times 10 which equals 40. all right and i asked also how many straights are possible but this time i'm not including straight flush because straight flush is considered different well uh the first thing we're going to do is pick the lowest card like we did above and then we're going to pick the suits so pick the suits of the cards so we decided there are 10 ways to pick the first card of to pick the first card of the straight and now to pick the suits well if i decide that the first card let's say the ace is going to be clubs and then i pick the next card which is going to be a 2 i can still pick clubs again because i took out the the ace of clubs but i didn't take out the two of clubs so i can still pick clubs for that second card so and i can still pick clubs for the third card and so on so basically i do have replacement but order does matter because uh picking the first car there to be the ace of clubs and the second car to be the two of hearts is different from the first car being the ace of hearts and the second card being the two of clubs those are two different hands so order does matter and also um uh you're doing so with replacement there are four possibilities we need to pick five of them so this will be four of the power five the thing though is once i've done this i am including picking the first card to be hearts the second card would be hard the third card to be hard the fourth card would be hearts and the fifth card to be hearts that's a straight flush i don't want to include straight flushes so how am i gonna remove the straight flushes i'm gonna subtract them out so this is straight flushes so just remove them remove them from the calculations subtract them out and you no longer need to worry about them anymore this is basically remember that sum rule that i very briefly mentioned this is basically that sum rule right where you say the total number of like straight flush number of straights including flushes is going to be the number of straight flush and straight nonflush hands so you add those together which means you can do some algebra to get subtraction too anyway uh in the end you calculate this and you get 10 200 uh straight hands excluding the straight flush okay uh we are almost done with this section we've had a long discussion about counting and i haven't really said anything about how this test of what this has to do with probability well this entire section was devoted to the case that you may have thought was the easy case where you have a set where your sample space has finite size your sample space has finite size and you decide that every element in that sample space is equally likely and you may have thought this is the easy case because in that situation it's actually rather easy to compute probabilities you can assign a very natural probability measure the probability of any event a which is a subset of this sample space will be the number of elements in that set a or in that event a divided by the size of the sample space which is a very nice natural probability measure but here's the thing though you need to compute then using these counting techniques the size of the sample space and the size of your set a and that's actually tricky because now you need to use these uh counting techniques and i personally don't think the counting techniques are all that easy all right um so based off of this once we have this natural probability measure we can use counting techniques if we need to to compute probabilities so we're going to use this to compute the probability of the poker hands that we were considering above so the size of pos of the sample space the sample space consists of possible poker hands we say that each of those poker hands are equally likely we do not care about the ordering of poker hands so there's going to be 52 choose five such poker hands which is going to be uh two million five hundred ninety eight uh thousand uh 960 possible poker hands all right so i want to compute the probability of each of the poker hands that we saw in the previous examples so for example 21 this was the four of a kind so the probability of a four of a kind hand is going to be the number of four of a kind hands which we computed to be to be 624 divided by the size of the sample space which is 2 million 198 thousand 960 poker hands which is going to be approximately 0.0002 uh for example 22 the probability of getting a full house is going to be uh let's see so probability of full house is going to be 3744 divided by two million five hundred ninety eight thousand nine hundred and sixty the number of full house hands divided by the total number of possible poker hands this is approximately zero point zero zero one four uh next up so uh for example 23. so for example 23 we're going to compute the probability of our socalled flush hand a flush that isn't actually of that includes straight flush so just the suit is the same uh how many so there were 5148 such hands possible we're going to divide that by the size of the sample space which is that 2 million ish number um and that's going to be approximately 0.002 and now for example 24 the probability of a straight flush uh that's going to be there were 40 straight flush hands divided by the size of the sample space this is going to be approximately 0.00002 really really really small and the probability of a straight so that's excluding the straight flush is going to be 10 200 divided by the size of the sample space which is approximately 0.004 there's always kind of this question of which is more likely a flush or a straight turns out the straight is twice as likely as the flush which is which to me is a little surprising it feels it feels intuitively like the straight is harder but it's actually not um and that's one that's a common trait of probability it it defies what you feel like should be true all right so that's it for this section and uh in the next success section we're going to talk about a conditional probability so uh i will see you then since somewhere using the idea of independence you can almost find it anywhere if you look hard enough so independence is an extremely important idea in probability i've heard some people say that independence is what keeps probability as a mathematical theory from just being a subset of analysis so we're going to talk about uh independence if you come away from this lecture not or come away from this class maybe more appropriately not knowing what independence is then that's that's bad that's really bad you must understand independence because it is used everywhere it's one of the most important ideas in probability theory so two events a and b are said to be independent if the probability of a given b is the probability of a so in some sense the information about the event b gives no information about whether a has happened since uh the probability of a knowing that b happened does not change so first off i've said only that the probability of a given b is the probability of a uh what about the probability of b given a and before i proceed i should probably uh make a quick note saying i'm going to assume that a and b are events that have probabilities on their own that are not necessarily zero just because it's easier mathematically to assume that their probabilities are not zero and you kind of have to uh treat the case where they are events with probability zero as a little as a as a separate thing but honestly for for the most part you get the information that you need with just pretending that just ignoring the the chance that uh they have probability zero okay so uh all right then let's uh compute the probability of b given a uh it seems like what should be the case is that if a is independent of b then b should be independent of a as well and in fact that turns out to be the case because we compute the probability of b given a and that's going to be the probability of a and b divided by the probability of a and we then say that the numerator is equal to the probability of a given b uh times the probability of b uh divided by the probability of a by the way uh if you're not recognizing this part uh recalling back to the previous lecture this is a bayes theorem prototype that all right that's some bad bad handwriting up there i cannot i cannot let that go i know that in some of these videos my handwriting isn't great but i cannot let that go so this is a bayes theorem prototype in that you're pretty much one step away from getting base theorem all you would need to do at this point is apply uh the law of total probability to the denominator uh down here in order to get bayes theorem but anyway that aside all right that was a a distraction um we now say that this is the probability of a given b times the probability of b uh divided by the probability of a but we now know that the probability of a given b because a is independent of b that's going to be the probability of a so this is the probability of a times the probability of b uh divided by the probability of b you can a probability of a sorry and you can probably see why we're assuming that these events don't have zero probability because we could end up with division by zero so uh we're going to cancel those out because they appear in the numerator and denominator and we get the probability of b in the end okay all right so that shows what we wanted to want to show so um this implies that if a is uh if a is independent of b so if a is independent of b uh that up that's going to imply that b is independent of a and vice versa all right uh it seems that it would seem to be the case that if a is independent of b and so um heuristically that knowing whether b happened gives you no information about whether a happened it should also be the true for the complement of a if you if um so knowing that b happened should not tell you whether a didn't happen either so this does in fact turn out to be the case because we compute the probability of a complement given b and that's going to be the probability of a complement given b uh we know from uh the previous section this is 1 minus the probability of a given b and the probability of a given b is going to be the probability of a so this is 1 minus the probability of a which we know from section 2 is equal to the probability of a complement so that means that the complement of a is also independent of b so there is an immediate consequence of this definition which is that the probability of a intersected with b is equal to the probability of a times the probability of b you can see that because you could uh potentially say that uh like you would start out before saying the probability of a that this is the probability of a given b but since a is independent of b that's just going to be the probability of a so that means that the probability of intersections turns into the product of probabilities and not only is this a consequence of how i've defined the definition oh i misspelled definition not only is this a consequence of how i uh defined independence independence of events this is actually taken in later courses in higher level probability courses or courses devoted to probability as the definition of independence because the two are essentially equivalent and furthermore what i've highlighted in blue here it's used even more frequently and and also uh it it's uh you have this issue of zero probability events and we don't have that issue here because there's no division taking place okay uh students often want to like i've been encouraging students to think about uh probabilities and and and uh sets and events in terms of venn diagrams and uh is it possible to get a venn diagram uh graphically representing independence it is although it's kind of hard and honestly not that enlightening i'm going to show it to you anyway though first off this is not what independence is this is not a picture of independence we have a we have b and we have our sample space that is not independence this is disjointedness and two events that are disjoint and have nonzero probability are certainly not independent of each other because if you knew that one event one of those two events happened you know that the other one didn't so disjointedness is not the same thing in fact it's almost the opposite thing or implies nonindependence so disjointness is not independence it's almost the opposite okay so uh i'm promising you to make a to show what independence kind of looks like as a venn diagram i can kind of torture a venn diagram so what i do is um i'm gonna need to zoom in a lot for this so to attempt to draw uh two sets that are independent of each other i uh draw the sample space as a square i divide it up into fourths and then i'm going to give the set a the upper left corner nothing special about that particular corner it just needs to be a corner and then i give the event b the middle quadrant it should be the same area roughly as a it's just taking up the middle area and the area inside of a and b relative to b is same is the same as a's area relative to the entire sample space so or or that is uh a is take a's area is a quarter of the sample space and a's area and b or a's intersection and b is a is a quarter of b so knowing that you fell into that so basically a corresponds to the probability of falling into the upper left hand corner quadrant and knowing that you are in this middle square does not really tell you whether you fell into that quadrant or not um that gives you basically no information on that so this is a sketch of what independent events might look like if you want to draw them as a venn diagram if this sketch does not make sense to you then don't worry about it just leave it alone because you know it's it's very much a tortured example you have to kind of work hard to get something that looks like this and i'm not super convinced that it's necessarily all that enlightening as to what independence means so if it doesn't make sense to you then just move on don't worry about it uh okay so the next example we're going to consider rolling a sixsided die and i'm going to show that the event a which is that the number uh of pips does not exceed four and the event b that the number of pips is even are independent events so let's uh go ahead and enumerate what falls into uh these two uh separate events we have the event a well not separate just different so the event a uh the number does not exceed four so that includes uh one pip two pips three pips and four pips all right uh the event b which is that you get an even number of pips is going to be two pips four pips uh come on i want the fourth pip i tried and six pips ugh not always cooperative all right um so let's compute the probability of a given b to show that two events are independent it is sufficient to just compute the probability of a given b and show that that equals the probability of a so the probability of a given b is going to be the probability of a and b divided by the probability of b all of these outcomes are equally likely so the intersection of the two sets is going to consist of two and four those are the two things the two sets have in common so this is going to be the dice with face two and the dice with face four why is it so uncooperative okay there we go uh probably two probably four and um divided by uh the probability of b okay and at this point we can basically just count how many elements are in these sets so uh for the intersection there's two outcomes in that set uh and uh for the probability of b there's three so that's going to be three over six so we end up with uh the six is cancelling out and the probability of a given b is equal to twothirds as a result but the probability of a there are four outcomes in a uh divided by the size of the sample space which is six which is also twothirds those two numbers are the same therefore these two events are independent of each other okay next up we suppose we have events a1 through a n these sets are said to be mutually independent if and i know that this definition is rather complicated for k less than or equal to n the probability of any subset of that collection of events will become the product of their individual probabilities so it is not sufficient to just check that for all of these events a1 through a n uh the probability of the intersection is the probability of probabilities it has to be true for not just the entire collection of events but any subcollection of that collection of events it they all must turn into product of probabilities if this is not true then they are not said to be mutually independent of each other um and here's kind of the reason why um when we were saying mutually independent it what we really kind of want to say is that uh none of these events give any information about any of the other events and so that means it you would want to say that you take any two events in this collection of events and they will be independent of each other you want to be able to say that but it is not sufficient to just check that the entire intersection the probably the entire intersection turns into the product of the respective probabilities and here is an example that shows uh why that is not in fact true um so uh this is an example from uh from actually uh this paper that i've highlighted in red use the diagram below for finding probabilities compute the probability of a and b and c that actually is just going to be that tiny little sliver so probably of a and b and c is equal to 0.04 and then we're going to find the product of the of the probabilities of the events a b excuse me and c uh so the probability of a so that's going to be the probability of the blue circle which is going to be 0.1 plus 0.06 plus 0.04 which is 0.2 so this will be 0.2 the probability of b is going to be the green circle so that's going to be 0.06 plus 0.04 that's 0.1 plus another 0.1 is 0.2 plus another 0.2 is 0.4 so this will be 0.4 and finally we've got the probability of c which is going to be this blue circle here so we've got uh 0.16 plus 0.04 is 0.2 plus another 0.2 is 0.4 plus 0.1 will be 0.5 so uh this probability the probability of c is equal to 0.5 so uh the probability of a times the probability of b times the probability of c is going to be 0.2 times 0.5 which is 0.1 times 0.4 which will be 0.04 and those two numbers are equal to each other so it's tempting to say uh that these events are independent of each other but then i ask you to compute the probability of a uh intersected with b so the probability of a intersected with b corresponds to this blue region that i've highlighted which is going to be 0.1 so that equals 0.1 but that does not equal the probability of a times the probability of b which is equal to 0.08 and you might and before you say well those numbers are close close doesn't mean a thing i don't care about close they need to be the same so uh yeah they are they are not independent events and as a result these events a b and c are uh not mutually independent could you say that there is some other notion of independence that you could apply i don't really know and i don't really care because i have seen notions maybe like pairwise independence as an alternative notion of independence and i've never seen it ever used in my own life or work it doesn't really seem like it leads to any sort of useful uh useful concepts so yeah that's that's that's that all right uh next example this example is supposed to motivate uh maybe you remember when i was uh talking about flipping a coin take it has this example supposed to motivate uh our assignment of probabilities in that situation so we're going to flip uh remove the eight part that i don't know why that was there i don't know why i have uh eight fair coins so flip fair coins until we get heads and furthermore each flip is independent okay so what is the probability of heads tails heads tail sales heads tails those stills heads uh what in general what would be the probability of a sequence of flips a sequence of n flips to have n minus one tails and a heads at the end so the probability of heads if this is a fair coin is going to be uh one half uh the probability of tails heads and this is by the way getting a little abusive with notation but the but the probability of tails heads is gonna be the probability of tails times the probability of heads since those two flips were independent of each other we are now on to chapter three on discrete random variables and probability distributions this chapter serves both as an introduction to random variables and also an introduction to discrete random variables in particular but many of the concepts that we see here are going to transform is are going to transfer over to the continuous context when we're dealing with continuous random variables so let's get started so in this section we're going to talk about some random variables a random variable which is sometimes abbreviated with the letters rb is a function taking values from the sample space s and associating numbers with them conventional notation for random variables uses capital letters from the end of the english alphabet with lowercase letters while lowercase letters are usually used to denote a nonrandom value or outcome so up to this point we have been using lowercase letters for uh data and when talking about random variables we're going to switch to uppercase variables or uppercase letters and the distinction does somewhat matter um honestly this is a rule that is very frequently broken although at the same while i say that in this class i'm not planning on breaking that rule all that much uh yeah i mean i i break it all the time in in my research work but this is a situation where it's probably going i'm probably going to stay rather true to it so there's a difference between a random variable whose outcome is unknown and a possible value that random variable could take so uh using the fact that random variables are actually functions like the term random variable is somewhat there's somewhat of a joke that random variables are neither random nor variables because they are uh because random variables are not really variables they're treated as functions and they're not random because if you know what outcome from the sample space you got you know the value of the random variable because the randomness comes not from the random variable x itself but rather from omega which is an outcome from the sample space so when uh so if omega is an outcome from the sample space the notation x of omega equals little x can be used to say the value of the random variable x when the when the outcome little omega occurs is little x so little omega that is a random outcome uh little x is uh nonrandom and x will equal little x when the random outcome omega occurs all right so the set of omega such that x of omega is equal to x is the event that an element of s is drawn that causes the random variable variable x to equal little x and the set of omega such that x of omega is in some other set a where that other set is often some such a subset of the real numbers is the event that an element of s is drawn that causes the random variable x to assume a value that is in the set a so technically when we want to talk about whether a random variable is in a it takes a value inside of a set or not this is the notation this is what we should be using for that notation we are asking for the probability that we draw an omega that causes x of omega to be in the set a but that is often rather tedious we would rather just say what is the probability that x is in a that's much simpler random variables are commonly classified as being either discrete or continuous discrete random variables or discrete real valued random variables take values in a finite or countably infinite or innumerable if you prefer a set with positive probability so examples of sets that random discrete random variables uh take their values in could be a finite set of numbers such as zero or one it could be the the integers it could be uh the natural numbers such as 0 1 2 3 4 5 6 and so on if it's innumerable then it could potentially be a discrete random variable if the set that it falls into is all any number between zero and one including fractions and including um uh rational numbers and irrational numbers any of those numbers then it's no longer discrete it's going to be considered continuous uh on the other hand continuous real value variables satisfy the following two properties first the random variables take values and intervals which are possibly infinite in length or disjoint unions of intervals of the real line with positive probability that's the first property that continuous random variable satisfied the second is that for any constant c and r the probability that that random variable is equal to c is equal to zero which is kind of a strange assumption you're saying that you know that this random variable this continuous random variable is going to equal some real number but you're saying the probability that it equals any real number is equal to zero well for starters that must be the case because uh this would be this is an infinite set so you need to have some way to assign probabilities and there's too many numbers in the real number system for uh numbers in general to have positive probability and secondly uh there's a way that my probability instructor put it he said if you like you know that someone's going to win the lottery you just know it's not going to be you so by that same logic you know that these random variables will fall inside of an interval you just don't know what number it will be you will never know what number it will be and uh it's highly highly highly unlikely that it will be that particular number you chose so unlikely that that probability is effectively zero uh perhaps the simplest nontrivial random variable is the bernoulli random variable if x is a bernoulli random variable then the probability that x equals one is equal to one minus probably that x equals zero which is p and we say uh that x follows a bernoulli distribution with parameter p that's that's what this we're saying so we have the random variable x and this notation means that it comes from some distribution with some parameter uh we're gonna leave that um alone that that uh verbiage about distributions alone for a minute that's going to be the subject of the next video uh so but basically we just say that x is a bernoulli random variable because it has these properties all right so the set s on which x of omega is defined could be really just about anything uh it's natural to think of bernoulli random variables as being equivalent to coin flips or possibly biased coin flips the thing though is you do not have to necessarily have coin flips for example you could have a probability space where there is a coin flip where if you draw heads this random variable will evaluate to the number one and if you draw tails or you flip and it lands tails up it will evaluate to zero that's one possibility to get a fair or balanced bernoulli random variable where the probability of getting one is 0.5 alternatively you could roll a fair die track whether the result of the die was even or odd and in the event of an even number of pips this random variable evaluates to one and the event of an odd number of pips this random variable evaluates to zero either one of those situations could be the case and it is statistically impossible to differentiate between these two possible setups for of random variables and the and and sample spaces so in that case it's almost as if once you know the distribution of the random variable you can pretty much forget whatever the original sample space was and any of the properties of that sample space you can now work in some you can work as if this random variable were the identity function and its sample space is going to be the real numbers or something uh maybe the number zero one so uh in that sense you almost forget all of that stuff that we talked about before um i mean i okay i guess you don't forget about it but you no longer care about the specifics of the sample space and the elements that you're drawing from the sample space the specifics no longer matter so we we get to talk about these things in a very general way all right so for the first example which of the following random variables are likely to be continu considered discrete and which are likely to be considered considered continuous describe the space of outcomes the random variable takes with positive probability so for first case flip a coin record one for heads and zero for tails if this is a situation the sample space for experiment would probably be the sample space consisting of heads and tails and the random variable x when given the outcome h is equal to is equal to one and the random variable x when taken the value tails is going to equal zero so the space of outcomes which i will denote by x of s where you almost feed the entire sample space into this function x is going to be the set uh 0 to 1. by the way the term for uh this is this is the image of the sample space s under the under the random variable x or the function x all right uh this random variable since the space in which its outcomes falls is generally like that's a finite space there's only two numbers in it so this random variable would be considered discrete okay uh next example roll a die record the number of pips showing so the sample space in this situation would be uh die rolls so we got one uh two three four five and six all right so that's the sample space uh x of one would be equal to one x of uh two or two pips would be equal to two and you'd kind of keep going on like this for uh other possibilities so you could say x when you have six pips on your face would be equal to six so this is the reason why we didn't want to write down the numbers one through six when talking about die rolls it makes it easier then to talk about random variables as being a translation from the number of pips showing on a dice face to numbers actual numbers and we would like to be able to make that translation random variables do not need to be defined on numeric spaces it doesn't have it they basically say nothing about whatever's in that original space they don't care at all what's in that original space and once you have random variable you get to work in the real numbers and that's really nice so the image of the sample space under x is going to be the numbers 1 2 3 4 5 6. and that suggests since since this is a finite set that this is a discrete random variable all right next example roll a die record one for an even number of pips and negative one for uh for an odd number of pips okay so the sample space is the same as before so we're gonna copy copy that sample space down so x of one is equal to x of three which is equal to x of five in all of these cases x is going to come out as negative ones and to have an odd number of pips uh in the case of two uh uh that's so x evaluated for the dice with two pips is going to be the same as when there's four pips which is going to be the same as when there's uh six pips in all of these situations the random variable x comes out as one so the image of the sample space under this random variable will be negative one and one and again this is a discrete random variable okay uh the time in minutes needed to complete a race uh it seems appropriate to say here that the sample space is going to be some uh positive real number which we'll put with the r plus which will be which is equal to the set uh zero to infinity including zero and in this case so it seems like the the the random variable is going to be the identity function where it takes one of the numbers from the space and just spits out that exact same number so in this case x of uh omega is equal to omega which is in uh the which is in the uh posit or nonnegative real numbers so that means that the image of the sample space under x is going to be the positive real numbers or the nonnegative real numbers and this is a continuous space so this is going to be a continuous random variable and basically what we're saying is uh the time it takes to complete the race can be any number from 0 to infinity uh not just integers but also including fractions and algebraic numbers and transcendental numbers every single possible real number and since the real numbers are an uncountable space that means that this random variable is going to be continuous uh for similar reasons number five the length in centimeters of a hair plucked from a person's head uh we could say um that the sample space is going to be uh hairs no no no no not x sorry the sample space will consist of hairs and the a random variable x from that sample space which i'm going to draw a hair okay that's the thing all right uh so x when given a hair uh gives you uh a number so the length of the hair and so it suggests that the image of the sample space under x is going to be the positive real numbers or the nonnegative real numbers again since hairs can be in principle of any length and it's an uncountable length so i mean idea all right it's it's going to be true that there's a finite number of hairs in the world and therefore a finite number of hair lengths so if we were being super super duper technical we would say actually this is a discrete random variable because there's only a finite number of possible hair lengths since there's only a finite number of pairs but that seems ridiculous that seems like a ridiculous model that seems like way too much complication continuous random variables are continuous because it's easier to work with continuous things than discrete things and you're probably going to agree with that when we start doing all the work for the discrete random variables and all the work for the continuous random variables and see oh it's actually not it's actually somewhat easier to work with continuous stuff so uh we're going to say that it's the real numbers in which case this is a continuous random variable okay uh next up roll two dice record the sum of the number of pep showing uh i'm not gonna bother writing out the sample space but i'm going to say that uh x could be like say if you gave x the numbers uh one and four it's going to add up the pips showing on those two dice and give you the result five so the image of x under the sample space is going to be the numbers from 2 to 12. and since this is a finite set this is going to be discrete okay next up flip a coin until h is seen and count the number of flips so the sample space consists of heads tails heads tails tails heads uh and so on so the si so the random variable x when given one of these uh strings let's say tails tails heads uh is going to evaluate to this to the length of the string which means that x when fed this sample space its image is going to be the natural numbers which are the numbers uh 1 2 3 and so on and this is a discrete space because the because there's an innumerable amount of numbers in this space and since it's innumerable that means it's going to be discrete so this is a discrete sample this is a discrete random variable by the way if you're not familiar with the word innumerable that means listable as then you can list it out and even though it will take you an infinite number of years to list out everything in this sample space or in this uh set there will you will hit everything in that set a in finite time so every possibility the same cannot be said for the real numbers by the way if even if given infinite number of years you will not hit every value if you started trying to list them out so that's actually a very deep result in set theory or i don't know about i don't know if deep is quite the right word for it but it's certainly an important result so uh we're just going to take it for granted here that that is the case example two consider an experiment of rolling two sixsided die define two random variables for this experiment are they continuous or discrete we can define multiple random variables for the same sample space and there are advantages to doing so because when doing so we can talk about notions such as correlation or study of the behavior different random variables in the same space consider different possibilities i'll talk about their joint distribution together stuff like that so as a reminder the sample space consists of dice faces so we could have for example one one uh one two and we would keep going on like this you've seen in previous videos how to do this until eventually we listed out 6 6. okay so this is our sample space what's one random variable we could define we could say that x the random variable x when given some outcome omega in the sample space is equal to the sum of the pips whereas if we gave uh no let's not call the other random variable x we'll call it y that y will be another random variable defined on this space and it will be the max of the pips so for example uh x of the dice face of the combination one and two will equal three because that's the sum whereas y of one and two will equal two because the maximum of the two of the number of pips is going to be two okay so that's it for this section this was a basic introduction into what one uh we are on to the next section in our chapter on uh discrete random variables and at this point you're probably thinking okay we got these things called discrete random variables uh all right what's the point why why do we have these things random variables what why do why does anyone care about this this seems like an extra complication on uh this uh on these probability spaces we we were still able to talk about probability we haven't really added really anything so far well random variables truly are and and an addition that makes things much better and allows you to say additional things about uh randomness uh once you have random variables you are now allowing for um concepts such as um uh you're allowing for concepts such as we have uh we we have a phenomena that is essentially uh like phenomena that is essentially the same the same except for a couple parameters like there's a few parameters that we need to figure out and once we know those parameters we basically know everything there is to know about this phenomena or there's concepts such as uh expectation or mean uh you in order to be able to talk about expected values you need to have random variables so once we've introduced random variables uh these things that take inc that take uh things in our sample space and turn it into numbers you now gain a lot of additional structure the first thing that we get that is an essential property of uh random variables is a probability distribution so a probability distribution for a random variable is a function that describes the probability that a random variable takes on certain values discrete random variables are determined completely by the probability mass function which is abbreviated pmf and the probably mass function for a random variable is p of x which is equal to the probability that that the random variable x capital x so remember that capital x is referring to a random variable this is a thing that hasn't taken a value at yet or we don't view it as being equal to anything per se at this moment uh but we're asking for the probability that this random variable when we actually evaluate it and get a number out of it is going to equal x i like to think of random variables as uh they will have a future value but they don't really have a value right now when we're asking for their probability and stuff so we're asking what could this thing be in the future so uh a coin flip is random before you make the flip if that makes sense after you've made the flip then it's no longer random because you get to see whether it was heads or tails which actually is um getting more again into the issue of what does probability actually mean like for example uh let's suppose that we flip a coin and it lands heads up and then we cover it up we don't get to look at it we never saw what the coin did we immediately the moment it lands on the ground uh covered up in a box in principle uh we would say that that under this uh frequentest notion of probability we should say that this random variable has a value and is no longer random we just don't know what it is whereas if you're adopting maybe the gambler's notion of random of randomness and probability you might still view it as being random where you can start placing a bet on whether it's heads or tails when you take the lid off of the uh when you when you take the box off of the of the coin and then actually observe it uh i'm not gonna talk anymore about that i've already recorded a half hour video about the interpretation of probability probability and you can watch that if you want to learn more uh but uh all right so i'm just saying this because i feel like students especially with this notation this notation especially when you're starting out can bother students and they're wondering what capital x is and what little x is and what's the difference between them and the difference is this is random and we don't actually know what it is whereas little x is something that is fixed and we know what little x is so um really when i'm writing this down little x is going to be substituted for a number like for example there's going to be p of 1 which is going to be the probability that capital x is equal to 1. like at some point we're going to make that substitution um so basically the little x right here is going to get substituted with a number but the capital x is never going to get substituted and that's always going to be viewed right now as being random and we don't really know what its value is going to be and we're just studying uh what its value could possibly end up being and how likely it will end up taking certain values uh sorry if i'm going on too long about this this is just something that i know that students at some level struggle with and i try over and over again to try and explain it and i'm never fully satisfied with my own explanation okay um the probably mass function one way to visualize a probably mass function is using a line graph where a line is placed on each point x of r that x takes a positive probability extends to the height representing p of x um okay so before i draw a visualization i'm going to say that we are totally allowed to say all right we've got inputs x and outcomes p of x where p of x is a function that gives us the probability this random variable will equal x and we can construct a table if we wanted like for example 0 1 2 3 4. we could construct a table of possible inputs to this function and for possible outputs we could say uh let's see uh what says something we could do uh we'll just say that all of these are equally likely so all of these are onefifths so this function puts out onefifth all the all the time well yeah should we always do that uh we might say this is twofifths and this is onetenth and this is onetenth there i think that's okay does this still add up to one this must add up to one by the way actually that's the thing we're going to talk about in the future let's see two onefifth plus onefifth is twofifths plus twofifths is fourfifths plus twotenths is another fifth so that does in fact add up to one which probably mass functions if you add them up if you add up all of their nonzero values then they must always add up to one always probably mass functions always add up to one if they don't add up to one then they're wrong they're not probably mass functions if you ever compute a probably mass function and it doesn't add up to one then it's not a probably mass function and i don't care if it's close it closes close as nothing because it's not one one is one all right um okay and and i suppose we're allowed to say all right let's suppose i threw in a fifth value then the probably mass function will be zero and presumably anything that's on this table if i didn't list it out then the s function is zero right so anything not written down is zero all right uh but then we can visualize a probability mass function using a line graph so we've got possible x values that this thing could take and we've got it's probably mass function uh it's it's probability at those points so we could have x equals zero one two three four uh and then for those let's let's see i've i've already got a lot of what i need so uh we'll do one tenth two tenths three tenths four tenths so zero is going to be two tenths so that's about here one is going to be two tenths again and then we got four tenths and then one tenth and one tenth this is a visualization of the probably mass function that i wrote on the right hand side of the page so i generally am like drawing a dot at the probability and then drawing a line up uh to the probability of the random variable equaling that value or something like that so yeah uh that's uh how you can visualize it there's also probably histograms which are very similar to line to the line graphs and very similar to the histograms that i was discussing uh several lecture videos ago where you could instead of having these uh lines uh draw i like these the the the table that i have is a lot like the the relative frequencies that i was discussing when discussing histograms so what you would do is draw something similar to the relative frequency of those uh of uh those um uh possible values so we got 0 1 2 3 4 and we draw a bar that is uh centered on that point so we got uh uh so we've got something going up to two tenths and two tenths again and then fourtenths and then two onetenths yeah so there we go uh so uh there's that uh this is another way to visualize the problem distribution if you if you prefer it um there's really no difference um in fact i would say that edward tufte would probably say that they are the same graph essentially it's just uh the one up here it should be preferred because it uses less ink okay uh continuing on uh let's see some examples uh a fair coin is flipped we define a random variable x uh when h occurs x evaluates to one and when t occurs x evaluates to 0 find the problem as function of x which is p of x visualize p of x through with a line graph and you're thinking how do we know what the probabilities are well we know because i said this is a fair coin and since i know that it's a fair coin i know that the probability of heads is one and probably a tails of zero okay so uh i'm going to start out actually with that tabular form so we've got x and p of x so we've got so possible values that x that the random variable x could take are one and zero so we're going to put zero and one as potential values for this random variable uh this random variable will equal zero if the coin comes tails up and since this is a fair coin the probability of getting tails so actually maybe i should be more verbose about this and say that the probability that x equals zero is equal to the probability of drawing an m and omega from the sample space an outcome from the sample space such that x when evaluated at that outcome equals zero i'm being very verbose about this uh what are such outcomes that causes us to evaluate to zero well we know from the definition of the problem that such outcomes are only tails so this is the probability that you get tails and the probability of tails because there's a fair coin is one half so that means that um x is so at z at zero the probably mass function will be one half and at one the probability mass function will also be one half because well a uh this random variable um well for starters this random variable is going to be one when the coin lands heads up and the probability of heads is one half that's one way to think about it and another thing to think about it is there's only two numbers that this random variable could take with positive probability zero and one zero it's probably mass function is one half so at one it must be whatever it takes to cause the probably mass function to add up to one so one minus one half meter is going to be one half and thus the other value is going to be one half uh so uh we we visualize this with a line graph uh we'll go ahead and make this one half uh what we end up having for our visualization of the probability mass function is we have lines uh extending up to uh one half all right and that's our visualization for it okay uh this by the way is a complete description of the problems function if we are willing to say that anywhere isn't anywhere we don't list uh the probably mass function evaluates to zero because like like if that should make complete sense to you because let's say what is going to be p at one half well that is the probability that x equals one half which equals the probability of drawing an outcome from the sample space that causes the random variable to evaluate to one half okay so we know that there are two outcomes in the sample space which are heads and tails and x is a random variable and therefore it is a function so you know that functions uh when given one input give you only one output only one output will come out so what does that mean here uh well we know what this function will be at heads which is one outcome in the sample space we know how this function will be at tails which is the other outcome in the sample space so what outcome causes this random variable to equal onehalf because neither of those caused the function to evaluate at onehalf so that means that the probability of drawing an outcome from the sample space that causes this to evaluate to onehalf is the probability of the empty set because the the set of all numbers or not numbers the set of all outcomes in our experiment that causes this random variable to evaluate to one half is the empty set because there is no such outcome so you end up computing the probability of the empty set the probability then of the empty set is zero so that would mean that anything that isn't listed here uh it's natural to say that the problem s function is zero okay okay okay uh moving along moving along there is an r package called discrete rv and this package allows for users to define random variables and work with them and i think this package is pedagogically useful but for serious work with these random variables i wouldn't recommend using it um i uh i've uh i actually just made a few minutes ago well it wasn't a few minutes ago it was more like a few hours ago i just made a few hours ago uh lecture videos for the lab for r uh are on my our introduction on introductory videos um functions for dealing with probability in r and dealing with a lot of random variables and classes of random variables families random variables and i never use this package because it's more for allowing students a laboratory to work with random variables in the notation that we're using in the lecture class or a notation very similar to it and not actually meant for serious work trying to compute trying to work with the cdfs and the pdfs and expectability and all that stuff of these random variables but it's kind of nice so for example in this situation i could define the random variable x and say that this is a random variable with the rv function its possible values are zero and one and the probability of getting those outcomes are each one half and it will print and make a nice uh output a printout for that basically summarizing what i just said and in addition to that when i tell r to plot this random variable it creates the plot of the probability mass function so that was all very nice uh next example let s be the sum of the number of pips rolled onto dice find p of s and plot it okay okay so let's uh come so let's uh form our table again so we've got s and we've got p of s okay so s are possible sums of the dice so what are some possible sums of the dice well uh the smallest it could be is two because that's what happens when you roll snake eyes or one and one so the poss the smallest possible sum is two and the largest one happens when you roll box cars which is both of them are six so that will be twelve so it's going to be everything in between so two three four five six 7 8 9 10 11 12. uh hold on hold on uh i didn't write that quite right 10 11 12. okay and now we need to figure out uh the probably mass function okay so we're saying we we could imagine that there is a sample space that's consisting of a dice of combinations of two dice like we got one one uh uh one two and two and going on and uh we got like six six we've already worked with this uh type of uh sample space in previous videos and i don't want to go into too much more detail into it because it can get kind of tedious so um we know that there are 36 outcomes in this uh sample space so and we're saying that everything is equally likely so therefore uh the probability of the event the dice add up to two is going to be the number of outcomes uh where the two dice add up to two divided by the size of the sample space so how many outcomes are there with the die set up to two well you can you can get uh snake eyes and that's it uh so there's only one outcome that corresponds to that and then we divide it by 36. uh how about three we could either roll one the left dice and two on the right dice or i guess let's uh uh readopt that uh blue dice red dice uh verbiage and we could say uh the blue dice rolls a one and the red dice rolls a two or we could have the blue dice roll a two and a red dice to rule a one and that's it otherwise it will not be it will not add up to three so that's two outcomes that correspond with this so we're gonna have 2 over 36 and for 4 we could either have 1 and 3 2 and 2 or 3 and 1 so that would be 3 over 36 and that's going to keep going so we would have 4 over 36 4 5 5 over 36 for 6 and 6 over 36 for 7. uh all right 12. there's only one outcome where you can get 12 and that's box cars 6 and 6. so this will be 1 over 36 for 11. you could get 5 and 6 or 6 and 5. so we'll have 2 over 36 and you can see the pattern it's going to become 3 over 36 for 10 uh 4 over 36 for nine and uh five over 36 for eight okay and now we're ready to create our little visualization so we got two three uh four five six seven eight nine ten eleven uh twelve uh that is an awful looking eleven i really did try harder but sometimes the screen doesn't want to cooperate all right and uh let's see for the yaxis we could go uh the highest you ever go is six over 36 so we could go one two three four five six there we go so six over 36 is at the top so for our probability so we'll go one two three four five six five four three two one okay and that's it that's our probably mass function so um using this discrete rv package uh we've got what we could do is create a random variable representing a single die so that's created here and then we could say all right s is the sum of independent and identically distributed copies of the random variable d that is getting into verbage that uh and terms and ideas that i haven't really talked about yet but basically you add up two independent dice which is kind of what's going on in this experiment and then i tell it to plot it and it makes a plot and that's a very good plot all right uh next up consider flipping a fair coin until heads is seen let n be the number of flips find a probably mass function describing the distribution of n and plot the first few values of the pmf and um we've actually talked about uh this type of random variable before a number of times in the previous chapter uh as i mentioned there it's one of my favorite random variables to refer to since on the one hand it's not like the setup is quite simple to understand you flip a coin until you get heads that doesn't that's not that doesn't take a great deal of imagination and yet you can still get a lot of richness out of it and the mathematics can get kind of involved so um okay so in this case what we ended up coming up with before and suggest what we should have now in fact maybe if you go back to that video and look at how it was uh showing that the probably the sample space under some probably measure um will in fact be one i actually kind of defined a prototype random variable n of omega uh that was uh measuring the length of the string and that was that's basically a random variable right there um i defined a random variable on that space so that i could compute show that the probability um of that space added up to one so yeah they're very useful things but basically we could just say that this is one half to the power n for when n is a natural number uh otherwise you would just assume that this thing is going to be zero so this is a natural probability mass function for this space we actually showed in that section that it adds up to one and uh yeah so okay so uh let's um uh plot this uh probably mass function we got possible values a half a fourth an eighth a sixteenth a thirty second so uh one two three four five so for one we go up to a half for two we go up to a fourth for three we go up to an eighth for four we go up to a sixteenth and for five we go up to thirty second and in principle this this graph goes on forever but we're just going to stop at 5. section is on expected values let's start by deciding defining what an expected value is the expected value of a discrete random variable uh which we are often dealing with e x is given below so the expected value of x is equal to the sum of x times the probability mass function of x wherever the problem mass function of x is nonzero this is just a more general way to write down what expected values are in principle most of the time the x over what you're summing are the integers so if you really wanted to most of the time you'll be okay thinking of expected values as this like um x equals let's say zero to infinity or another way we could do it is this is actually allowed uh x equals negative infinity to infinity um or if you know what that um your little x ranges from a to b you could say uh we could say x equals a to b there's all sorts of ways we could possibly rearrange this but really what matters is that you're summing up uh this expression x times the probability mass function of x wherever that probability mass function is nonzero so i'm just going to leave this as x such that probably mass function of x is greater than zero actually i'm just going to not even really say all that much over what x we're summing over i'm just going to say that you sum over x okay next uh expected value of x is viewed as the population mean which we're often denoting with the greek letter mu described in previous chapters we can always compute the expected values of functions of x oh no not always why did i say always like at the very end of this section i'm going to give you an example of an expected value that can't be computed but we can also compute i meant also also compute the expected value of functions of x functions of x that is um so that would be the expected value of h of x in a natural way by saying that the expected value of h of x is equal to the sum over x of h of x times the probability mass function at x and this formula should make some sense to you because you've actually seen it before or something very similar to it before remember when we remember when we were computing the sample mean the sample mean was 1 over n times the sum from i equals 1 to n x i which is also equal to the sum from i equals one to n uh x i times one over n and the one over n i mean if you add up one over n n times what number do you get you get one and also one over n is a number that's uh greater than zero so actually you could see this as a probably mass function for x i so you can imagine that you have your sample of observations and you're going to pull an observation at random with equal probability from your sample if it's pull from with equal probability then you're going to pull it with probability 1 over n so actually you've seen this expected value formula before it's just that this formula down here is a more general notion of of a sample mean or not really a sample mean but of mean so it allows for more situations it allows for infinite uh possibilities for x and so on the expected value is in some sense a best prediction for the value of x and uh this form uh no did i say sample mean i don't think i did expected value you can see this footnote for what sense in which it's the best prediction but it's hence the term expectation it's like if you had to guess what value this random variable is going to be uh you can use the expected value to do that and it will be uh correct in some sense so example 11 complete the expected value for some of the random variables that we've seen in previous sections so bernoulli random variables discrete uniform this discrete uniform random variable that represents a die roll and the geometric random variable with parameter p okay so let's get started um the first situation for a bernoulli random variable the expected value of x is equal to the sum of x p of x and the values of x for which p of x is possibly nonzero will be x equals zero to one so this will be um zero times p of zero which is one minus p this corresponds to the poly mass function at 0 plus 1 times p where p is the value of the problem as function at one i guess this uh notation is actually a little unfortunate because i've got a couple different p's so maybe i should switch out the notation for the probably mass function in this example just switch it with f that could uh probably make things a bit more clear so this still nevertheless in any situation is the probably mass function for x okay and well let's see that just goes to zero and that's going to equal p so that means that the expected value of x is equal to p which is nice and actually rather insightful it's saying that the expected value of a bernoulli random variable with parameter p is equal to the probability that that random variable is equal to one and that's actually a very useful fact that's rather useful it allows us to relate bernoulli random variables probabilities for events and expected values it gives us a way to what we could do if we wanted to relate expected values to probability of an event is create a bernoulli random variable that's equal to one when that event occurs and zero otherwise and then the expected value of that random variable would be the probability of that event occurring but i'm just going to leave that issue for now let's work with the next example so the expected value of s is equal to the sum of let's say s p of s again we're talking about uh a probability mass function when we're talking about p uh down here and let's uh think about what are things that this random variable could take with positive probability we'll end up summing from s equals one to six so that's going to be uh let's see p of s is always 1 over 6. so this is equal to 1 over 6 times the sum from s equals 1 to 6 of s and actually there's a formula for that hopefully you remember that from from your algebra classes in general you have the sum of uh s equals 1 to n of s this is a sum of an arithmetic series that's going to be n times n plus one divided by two okay which means that this sum is going to equal onesixth times six times seven divided by two those two sixes cancel out so this will equal 7 over 2 or 3.5 okay finally we have the expected value of what am i calling this geometric random variable i'm calling it n the expected value of n which is going to be the sum of n times p of n uh where n ranges from one to infinity all right this is gonna get much more complicated uh so because this is going to involve an infinite sum p of n is going to be we've got np 1 minus p to the power n minus 1. and we're uh summing from n equals 1 to infinity let's see the p here is a constant so we can bring that out we are not summing over p that means that we can say that this is equal to p times the sum from n equals 1 to infinity n times 1 minus p to the power n minus 1. you probably did not see a formula for this so what are we going to do we're going to get tricky we're going to get really tricky you've taken calculus presumably you've taken calculus 1 which includes differential calculus so you have seen this formula before the derivative with respect to x of x to the power n is equal to n x n minus one with some assumptions on n like for example that it's uh let's say at least one in which case that would hold in this situation if you're looking at one minus p as your thing as the thing you're differentiating hmm those two things look rather similar which means that actually we could be invoking some sort of differential or derivative in our sum and say instead that this is equal to p times the sum from n equals one to infinity the derivative with respect to p 1 minus p to the power n now you would notice that right now if you were to in fact take that derivative you would almost get what i wrote on the left over here that i've underlined in green you would almost get that except you'd be off by a sign as in you'd get negative something because you have to invoke the chain rule so what are we gonna do about that we're just gonna throw a negative out here and uh then it's certainly true although i'm going to mention that well okay it's actually true as written down there's no there's no caveats yet but there is going to be one in just a second because i'm going to say that this is equal to negative p times the derivative with respect to p the sum from n equals 1 to infinity 1 minus p to the power n okay you can't just bring derivatives out of sums like that or you can but there are conditions like you can't just look at any sum in the world and see derivatives inside the sum and say okay i could just slip the derivative out it's not that simple there are details there are conditions under which you can do this those conditions are satisfied here i actually don't remember them off the top my head um i just know you can you can go look at some calculus book or some analysis book it's probably going to be an analysis book uh or wikipedia wikipedia has pretty good math articles and it would tell you when you can switch because that's what you're doing you're switching a sum and a derivative it will tell you when you're allowed to do that and i'm just completely sweeping that under the rug and i'm fine with that because i don't care i got other stuff to do all right um the thing though is we know how to compute this sum you know that this sum is going to become uh 1 minus p divided by p so that means that this will become negative p oops wrong color negative p and then we've got the derivative with respect to p of um 1 minus p divided by p which we should probably uh simplify somewhat and say that this is equal to negative p and the derivative with respect to p of let's see one over p minus one yeah that's right and the derivat so then we take that derivative and say we've got negative p and on the inside after we take the derivative we'll get negative one over p squared those two negatives turn into positives we cancel out one of ps and this is equal to one over p there we go this actually has a very nice intuitive um interpretation which is that let's say that you're flipping a coin until you get heads how many times do you what is the expected number of flip first ones tails second one's heads makes pretty good sense to me or let's say that you have a bias coin and you flip this coin until you get heads and the probably that you get heads is uh 0.1 how many flips do you need 10 flips seems to make a pretty good sense at least to me it allows at least in my mind way for me to relate probability of an event happening with time and say that if that if you were to have an uh a sequence of independent re replications of this event this is about how long you have to wait until you see that event happen which is another way to think of or another way to reason about how rare that event would be like for example if there's like a 10 chance of an earthquake every year how many years is it going to take for you to see an earthquake 10 years on average uh something like that that's so i really like uh that formula interpretations like that okay moving on this is the same body of lecture notes that we've seen before so we have loaded up the discrete rv library in r when you whenever you see the r sections and there is an there's a discrete rv function called e that is for computing expected values so x was a random variable that we defined at some point that corresponds to the bernoulli random variable that we were talking about s corresponds to actually the sum of two dice so in this case it we should be doing seven over two or something like that um uh if we were actually talking about the same s but this is a different s uh this is a uh sum of two dice two independent dice by the way uh not just any two dice although i don't know how you make two nonindependent dice that would be really hard and is the what we're talking about and notice that the answer is approximate uh we know that in in for this end the p parameter was 2 so the number that results should be 2 but it's not 2. it's 1.9999999 so be aware of that it's giving us an approximate answer because it's only fine summing over a finite number of uh places but you get the idea you can tell that that is that it's essentially doing the right thing uh okay uh continuing on expectations are linear functions and being a linear function is an extremely important property um and it's for that reason that we can say expectations are integrals but that is a 60 40 idea right there instead we're going to talk about how the expected value of ax plus b is equal to a times the expected value of x plus b this is something that we can show uh watch the expected value of a x plus b uh is equal to this this by the way is uh we this is basically h of x where h of let's say s is equal to a s plus b so we can use some of those that uh expectation formula that we mentioned above uh towards the beginning of this lecture and say that this expectation is equal to the sum over x a x plus b times the probably mass function of x which is equal to the sum over x and then we'll factor all that stuff together we've got a x p of x plus b p of x and then we'll break up the sum and say that this is the sum over x a x p of x plus uh the sum over x scroll down scroll down uh the sum over x b p of x factor out the constants to say that this is a times the sum over x x p of x plus b times the sum over x p of x and we can recognize what some of those sums are for instance this sum is the expected value of x and this sum is the sum of the probability mass function um over everything where it's positive so this is going to sum to 1 and hence you get uh the result uh a times the expected value of x plus b hence it's a linear the variance of a random variable is given by we'll call it var of x which is equal to the expected value of x minus mu where mu is just the expected value of x we just don't want to write that again in there because it's it feels confusing so we just put a mu in there but basically it's the mean squared distance of x from its uh from its expected value so this actually does correspond very closely to the sample variance as well if you could think of the sample variance as divided by one over n instead of one over n minus one uh you could say we could argue as we did for how the sample mean is very similar to the population mean in terms of expected values and say that this is a sample variance too so yeah they so this is uh something to notice the greek letter that's used to represent the sample variance is sigma squared and like with the sample standard deviation you can get the population standard deviation by taking the square root of the variance it's just not as common to do so um all right so uh there is actually a handy formula for computing the variance that is often easier than computing it directly and in this formula you may recognize this from when we were working with the uh the uh did i say sample variance a second ago uh i meant the variance or the population variance but this formula resembles the formula for the sum of squared sum of squared errors uh that we had that we saw in chapter one where you have the mean of x squared minus the mean of x squared where hopefully you can tell from my inflection what's being squared okay so the variance of x is thought of as the population variance and is denoted by var x which is sigma squared and the population standard deviation is sigma which is the square root of sigma squared sometimes i'll write though the standard deviation of x because it's it's sometimes nice to do uh all right so our next example compute the variance and standard deviation of the random variables listed in example 11. so if that's the case let's start out with x we already have the expected value of x which was p the expected value of s which is uh seven halves and the expected value of n which is equal to one over p so as a reminder of what we have already so now let's compute the expected value of x squared which is equal to the sum from little x equals 0 to 1 um x squared uh f of x because that's what i'm calling the probably mass function which is equal to 0 squared times 1 minus p plus 1 squared times p which is equal to p again hence you get to say that the variance of x is going to be the expected value of x squared minus the mean of x squared which is equal to p squared minus p which we could be done right there but people often like to factor this into p times one minus p all right next example uh in the case of s so the expected value of s squared is going to be the sum from s equals 1 to 6 of s square times probably mass function at s which is 1 over 6 times the sum from s equals 1 to 6 s squared this is something that we have actually all right you might not have seen this a formula for the sum of squares uh in your previous algebra classes maybe you saw that maybe you didn't but there is in fact a formula for that which i'm gonna have to look up okay so you have that the sum yeah let's use a different color for this the sum from s equals 1 to n s squared is equal to n times n plus one times two n plus one divided by six okay so using that here we get to say that this is equal to 1 6 times 6 times 7 times 13 divided by six so those cancel and that's pretty much all we can cancel so we get to say that this is equal to 91 divided by 6. therefore the variance of s is going to be the expected value of s squared minus the mean of s squared which is equal to 91 over 6 minus 49 over four what is that number uh uh 35 over 12. and you should have that the variance is always a nonnegative number in fact the only time that the variance is ever equal to zero as if the random variable is degenerate that is if it's effectively a constant so it's unlikely that your variance is zero and uh and in the more general case it's impossible for your r for your variance to be negative so if you ever ended up with a negative variance then you've done something wrong all right for the final one and this one is where things get weird all right i'm i'm going to zoom in for this one because this one is where things get really tricky because we're now working with the geometric case and we need to compute the expected value of n squared okay and that is equal to uh the sum from n equals 1 to infinity n squared times the probability mass function at n which is equal to uh the sum from n equals 1 to infinity i'm going to go ahead and already do some simplification we get p n squared 1 minus p to the power n minus 1. now how on earth are we going to compute that well we're going to get we're going to get really tricky is what we're going to do so we're going to say that let's let's zoom in even more we're going to say and you're not all right this this is just so weird what's about to happen um according to my notes it's actually advantageous to keep the p inside uh so let's let's uh put the p back inside of this sum rather than factor it out uh so we got a sum from n equals one to infinity n squared p all right what's going to end up happening is we're going to end up um adding 1 no hold on subtracting 1 and then adding 1 again inside of that square and leave everything else the same and we're going to go do some calculations and at the very end of them the expected value go away the expected value of n squared is going to appear on both the left hand and the right hand side uh left hand and right hand side of an equal sign so after that happens the thing is though on the right hand side of that equal sign it's not going to be just the expected value of n squared it's going to be the expected value of n squared plus something times something and when you have a situation like that you're going to be able to solve for the expected value of n squared because you just have an algebraic relationship and you're just going to have to see it and and watch it happen in order to kind of understand it's just at the very end all of a sudden what you're going to need pops out this is one of those situations where it's a trick and you're going to see the trick and you might not understand the motivation for the trick but someone did that trick once and it seems to work all right so uh here we go this part right here is a perfect is a it's a square a perfect square so uh we can we can now write that part i i'm not going to write n equals 1 to infinity all the time that's going to get annoying so i'm just going to write a sum over n down here and say we've got n plus one no no no no not n plus one n minus one we've got n minus one squared plus two times n minus one plus one and then we've got p one minus p to the power n minus one yeah okay and then this is equal to this is equal to uh breaking up this part and breaking up the resulting sum we get the sum over n and we have n minus 1 squared p 1 minus p power n minus 1. plus 2 plus 2 times the sum over n uh n minus 1 p 1 minus p to the power n minus 1 plus the sum over n of p 1 minus p to the power n minus 1. okay now we can start uh recognizing some stuff uh the term on the very left hand side no not left hand right hand side this term this is equal to one because this is just the sum of the probably mass function this term is the expected value of n minus one okay and then we get to say that so far collecting our stuff um recognizing those substitutions is the sum over n uh n minus 1 squared p 1 minus p to the power n minus 1 plus 2 times the expected value of i'm going to write this as the expected value of n minus 1 because we have that linearity property that i proved a few mo a few minutes ago and then we have plus one all right and uh doing some even further simplification we're able to recognize that the expected value of n is equal to one over p so that means that this term uh that we're adding is going to be uh one over p minus one so we got plus 2 over p minus 2 plus 1 so this will be uh so that means that this is going to simplify into plus 2 over p minus 1. and then we're going to take this n minus 1 and say oh well that's a perfect square too so this will be n squared minus 2n plus 1. all right so we then get um oh wait actually we don't want to do that no we don't want to do that we do not want to do that uh we don't want to do that we want to do something even trickier what we're going to do instead all right let's write in again what i have what what i have been omitting this whole time that this is the sum from n equals 1 to infinity but we're going to reindex this we're going to reindex this and say well actually this is the same as saying uh n minus one equals zero to infinity all right and then replace all of those n minus ones with let's say j and say this is j equals 0 to infinity so we get j j squared and the thing is though we're uh the first term in this sum though is going to end up being zero because j because all right plug in j equals zero that means the first term is going to be zero because zero squared is zero zero times whatever is zero so that means that the first term is actually zero so we get to um replace j equals zero with j equals one because well when you start at zero you just add a zero term you're just adding zero to the sum so we get to start at one and this is now looking almost like almost like uh the expected value except for uh the power up here is wrong it should be j minus one to have the problem mass function but we've got j instead okay so we'll replace that with j minus one plus one okay and to account for the plus one that means that what we need to do is factor out a 1 minus p so all told we will have 1 minus p times the sum from j equals 1 to infinity j squared p 1 minus p to the power j minus 1 plus 2 over p minus 1. and that sum is what we started with this term right here is the expected value of n squared oh look at that so we get to say that this is going to be uh 1 minus p times the expected value of n squared plus two over p minus one and as a reminder at the very beginning of this long statement of equalities is the expected value of n squared oh look at that we can do some algebra now for instance we can subtract over uh we can subtract from both sides the expected value of n squared and say that this is going to suggest that uh what is uh 1 minus p minus 1 uh that's going to be negative p so we've got negative p times the expected value of n squared uh we will subtract two over p and add one to both sides minus two over p plus one to get uh that this is equal to negative two over p plus one and then divide both sides by negative p and now we get to say that this is equal to that uh the expected value of n squared is equal to 2 over p squared minus 1 over p and there it is that by the way is the expected value of n squared i'm just writing it down again because it's on my screen and because it was put on a separate line that's what we need to compute so it then follows that the variance of n is equal to 2 over p squared minus 1 over p minus 1 over p squared which is equal to 1 over p squared minus 1 over p which is equal to p minus 1 over p squared wait a minute that's hold it hold it hold it hold it hold it that's oh no not p minus one uh one minus p okay otherwise that would have been bad because i would have i would have just computed a negative variance so one minus p over p squared uh did i ask to uh did i did i did i say that we should compute the standard deviation too it does it's not it's not super hard to do you just take the square root but yeah i did ask for standard deviations so okay computing the standard deviations too like that's super easy you just take the square root of the variance so the standard deviation of x is equal to the square root of p times one minus p uh the expected the standard deviation of s is equal to uh the square root uh let's see uh one half times the square root of uh 35 over three and for n uh let's see the variance no the standard deviation of n is going to be the square root of 1 minus p divided by p that's an ugly looking p all right there we go and in fact there are functions for in this uh discrete rv library for computing variance and standard deviation they're going to be v oops v and sd respectively and you can see this function computing the variance and standard deviation remember that this is not the same as we were talking about before so we get to compute those things and we get pretty much what we had so all right uh proposition 10 the variance of ax plus b is equal to a squared times the variance of x and the standard deviation of ax plus b is a times the standard deviation of x or the absolute value of a times the standard deviation of x uh actually this would probably be better to write in terms of that sd notation to say that sd of a x plus b is equal to the absolute value of a times the standard deviation of x so uh let's go ahead let's see this uh let's let's go ahead and prove this so the variance of a x plus b so the variance of ax plus b is the expected value of a x plus b minus we should have the expected value of a x plus b here here's the thing though uh the b's are going to cancel because expectations are linear and the a's can that a can be factored out in front of that expectation so that means that a is going to be a common factor and therefore the a can be factored out completely if we just square it so we get to say that this is the expected value of a squared and then we have on the inside x minus the expected value of x which is equal to a squared expected value of x minus mu squared just remember that mu is the expected value of x and that's the variance so this is equal to a squared times the variance of x all right what do we then say for the standard deviation we say that the standard deviation of a x plus b is the square root is equal to the square root of the variance of a x plus b which is equal to after you do that uh algebra the absolute value of a times the standard deviation of x because you're just going to take the square root of what i've highlighted in blue just take the square root of that and you're good all right so there's that formula uh one final note there is nothing that says that expectations need to be finite or even exist there are random variables out there that do not have finite uh uh standard uh finite expectations and they may not even have like like an infinite expectation they might not have an expectation at all like there's just no way to define it like i guess technically an infinite expectation is considered undefined but it's like you can't but there's a sense in which it is defined like infinite just means arbitrarily large but even then even then you might not be able to say that it's even infinite it could be anything there are random variables out there that don't have expectations so uh let's actually see an example of this this one's a fun one uh this is what's known as the saint petersburg game consider a game where a fair coin is flipped until it lands heads up a player would earn a dollar if the game ends with one flip two dollars if it ends two flips four dollars if it ends with three flips eight dollars if it ends with four flips and so on so basically your winnings are doubling every time uh this game goes on the fair price of a game corresponds to the game's expected payout what then is the fair price to play this game and before i continue on i would like for you to think about how much you think this game is worth and how much you would be willing to pay for it how much would you be willing to play at pay to play this gambling game what do you think is a fair price you might be surprised so let's calculate it we're going to say that n is following a geometric distribution with parameter onehalf because that's how you should we're flipping a coin until we get heads and this geometric random variable it's a fair coin is what will model such an experiment so then what would be our winnings it would be two to the power um n minus one because if you get one flip that would be you should get one dollar so n minus one so that'll be one minus one so to the power zero which is one uh if we get if it took two flips that's two minus one in the power so that'll be two minus one uh so the power will be one so we get to the one which is equal to two and if we have three flips that's going to be two squared so we'll get four so this is in fact corresponding to what we think it should all right then what we were computing is the expected value of two to the power and minus one which we can make our lives a little bit easier by saying that this is one half times the expected value of two to the power n and we know how to compute the expected value of 2 to the power n so we'll say this is one half and we have the sum from n equals 1 to infinity to the power n and then we write down the probability mass function for the geometric random variable which is 2 to the power negative n or which is the same as that's the same thing as 1 half to the power n so n minus n that's going to be one so this is one half times the sum from n equals one to infinity one which is equal to infinity this game has infinite value you should pay a dollar to play this game you should pay ten dollars to play this game you should pay a hundred dollars to play this game you should go to the bank and take out a loan for a billion dollars and play this game because this game has infinite expected value any finite price is a bargain and yet no one in their right mind would ever do that no one thinks that this game is really worth anything people think this is a terrible game and why that is is somewhat remarkable it gets to the point that once you've earned a million dollars another million dollars doesn't seem that great i mean it's pretty good to go from one million dollars to two billion dollars that's nice same thing with one trillion dollars and two trillion dollars like you're gaining a trillion dollars when you go from one trillion dollars to two trillion dollars but it doesn't really feel like it like you already got everything you want at one trillion dollars the other trillion dollars is just gravy so basically the point is with this game the way to rationalize the paradox of this game the fact that it's expected value is infinite but no one wants to pay that is that people are not actually thinking about the winnings in terms of literal money they're thinking about in terms of the utility they get from that money and people know that the net the next trillion dollars is not as good as the first trillion dollars so in that case this game actually doesn't look very good when once you actually account for decreasing utility from your winnings that's how you resolve the paradox you resolve it with economics that's it for this video uh we have been talking about general ideas and random variable theory let's call it random variable theory that seems like a good word uh probably mass functions accumulate distribution functions all that stuff these are general things that all discrete random variables have and expectations uh excluding the cases where they don't exist they they are they're generally around two so now we're going to start looking at specific examples of common families of random variables that probabilists care about the first one being the binomial probability distribution a lot of the ideas also that we talked about here actually transfer over to the continuous case when we're talking about continuous random variables uh there they have analogs that are pretty similar what you do is you replace probably mass functions with probably density functions and you replace sums with integrals so that's what that's how you go from the discrete to the continuous case and but everything else applies everything else is the same variances expectations uh cdfs probably mass functions become probably density functions which are pretty similar so these are all basic ideas that you're going to see over and over again when talking about probability and from this point on we're going to for the remainder of this chapter we're going to be looking at specific examples because the advantage of talking about a family of distributions is once you have a family distributions you get to talk about you get to talk about it once and then you get to generalize to lots of different cases and it's like the expected value if you're able to recognize a random variable as being a particular case of a binomial then there's an expected value formula available to you and you don't need to compute it by hand you shouldn't compute it by hand because it's going to be a pain you could just use that formula and it's really easy same thing with a lot of these other random variables hypergeometric negative binomial there they would be a pain for you to do over and over again but because we've identified a family of random variables with common characteristics someone computed a formula and now you get to use that formula and that's really nice that's really nice okay so i'll just uh leave it at that uh we will end this uh the study of this section and i will see you later i will see you when we start talking about binomial random variables for our next section we are now discussing the binomial probability distribution so a binomial experiment is an experiment that satisfies the following requirements the experiment consists of n bernoulli trials that end in either success which we will denote s or failure with which we will denote f the trials are independent and for each trial the probability of s is 1 minus the probability of failure which is some number p between 0 and 1. in the case of p being 0 or 1 this random variable is degenerate or the resulting random variable i guess i haven't mentioned a random variable yet but that random variable would be degenerate because you'd either always get success or always get failure so we don't consider that situation uh so we can think of the outcome of an experience as a sequence of s and f such as s f s f which in that case the uh the duration of the experiment would be n equals five so the binomial random variable is the associated random variable with binomial experiments and what a binomial random variable does is it will count the number of successes in the experiment so x of omega is equal to the number of s in omega uh we should probably not be uh um i mean why did i write that as a set that doesn't make any sense it's not a set right it's just a number uh we will then say that x follows a binomial distribution with parameters n and p so for example given that sequence of s's and f's that we saw before the binomial random variable would evaluate to three so we denote the problem mass function of x with lowercase b although that's more notation for this class i don't really see notations that we use in this class elsewhere because people know what they're talking about when you're reading papers and stuff so they don't bother to come up with some special notation for it anyway here we have the uh probably mass function for binomial random variables this is zero for x that's not an integer from zero to n and for x between zero and n it can in fact be computed that the probability mass function for a binomial random variable with parameters n and p is equal to n choose x p to the power x times 1 minus p to the power n minus x so here's some further explanation of this formula in this situation there are x successes out of n trials okay the probability of each of those successes is p they are independent trials so you multiply p x times and you would multiply y minus p minus 6 times this is accounting for the probability of each of those failures that occurred and here's the thing that will get you the probability of let's say the sequence ssf that would get you the probability of getting that particular sequence but the thing is there's a number of sequences where you could have three successes until two failures for example you could have sss ff or the other way around like ff sss and so on so we need to pick the positions in which successes occur and failures occur or we'll just simply pick the position of successes and if we pick the position of successes we then know where all the failures occurred in the sample or in this string so we end up with n choose x meaning out of x position out of n positions choose the x positions where successes occur the cdf of this random variable x is given next the probability no i don't want black blue the probability that x is less than or equal to little x is equal to the cdf of the binomial random variable which is equal to the sum from i equals zero because you could potentially have zero successes in your sample so from i equals zero to x rounded down the probability mass function at i n p which is equal to the sum from i equals zero to x rounded down and choose i p to the power i 1 minus p to the power n minus i and it looks like all i did was write down sum over the probability mass function then that that is what i'm writing down i didn't simplify this any further there's not really a whole lot more that you can say with this formula there's no fun little algebraic simplifications that you get all you're just going to say is sum up over the probably mass function and for that reason you're with the exception of um uh cases like some strange n or p historically in this class i would have students use the tables that were provided in the back of the textbook to work with the probably mass function or no not the problem mass function the cumulative distribution function now seeing as i am teaching this class online at the moment i don't necessarily see why i shouldn't like i'm telling my students that they can use r for pretty much anything even on quizzes and even on tests so for that reason i'm just not going to bother with working with the textbook and using the tables in the back of the book in these videos instead i'm just going to use r to get the cdf for binomial random variables although there may be some situations where like we might have um the input x we might replace the input x with say one okay if it's one you don't necessarily have to use r maybe i'll tell you not to look up the number and not to use r and ask you to compute the cdf just because there's only two things you're going to end up having to compute only two things are going to get plugged in so but that's kind of where what we're working with right now i might recycle these videos in the future and if i do be aware all right so one thing that's nice in these uh upcoming sections is i'm not going to go through the trouble of computing expected values using that summation formula using like x times p of x the sum over all x with p of x is not zero i'm not gonna bother with that anymore i'm just going to tell you what the expected value for this random variable is it's np no it's just np it's np i was jumping ahead of my head to the variance the variance of this random variable x is equal to n times p times 1 minus p and the standard deviation of x is just the square root of the variance going to draw your attention to something one way you can view binomial random variables is as the sum of bernoulli random variables in fact you could probably play around oops i didn't want to race have a look at this probably mass function formula and show for me that if you choose an n equal to one the resulting probability mass function is the is the probably mass function of the bernoulli random variable that is in fact the case um so uh i so that's that's something to look into uh but okay i'm saying that binomial random variables are the sum of n bernoulli random variables the expected value of us oh hold on and independent bernoulli random variables that's critical if that's the case remember that the expected value of a bernoulli random variable was p and the expected value of a binomial random variable is n times p so you're saying in a sense that we add up p n times to get the expected value hmm intriguing and actually remember that the variance of a bernoulli random variable was p times 1 minus p well now we're adding up n of those and we get np1 minus p for the variance intriguing so that is something to notice and also these expected value well i don't know necessarily about the variance being something very easily interpreted but the expected value certainly is it's saying that if there's a probability of a success happening let's say that it's a let's say that the probably success is 0.1 and you do this experiment 10 times then you expect to see one success in your sample or if you do this experiment 100 times you expect to see 10 successes in your in your sample so it's actually a rather easily interpreted quantity this uh n times p quantity okay and i mention here that select values of bx and p are given in table 8.1 of the textbook but in this video i'm just going to use r okay uh moving on you flip a fair coin ten times all right so we should start filling out with numbers you flip a fair coin ten times there's going to be a binomial random variable showing up so fair coin suggests that the p parameter of this binomial random variable is going to be one half presumably what we're doing is counting the number of heads and if we're counting the number of heads then the resulting random variable is binomial and the end parameter of that binomial random variable will be 10. all right what is the probability you see exactly four heads do so without using a table the probability that this random variable x which is following a binomial binomial distribution with parameters n is 10 and p is one half so the probability that x is equal to 4 is going to be 10 choose 4 onehalf to the power 4 and one half to the power ten minus four okay you're probably noticing well okay we got one half to the power four one half to the power ten minus four so that's the same as 10 choose four one half to the power ten and that's just basically because one half is equal to one minus one half so maybe i should write one minus one half to be a little bit more clear uh like that that that's the reason why but if he had instead instead of one half we said the probability of getting heads is point one then this would be the then thinking about the problems function this way would have been more correct or would have been correct it's not more correct because the other one is incorrect all right so uh actually we're going to compute this thing by hand so we're going to say that 10 choose 4 is 10 factorial divided by 4 factorial times 6 factorial and we have 1 over 2 to the power 10 that's one half raised to the power 10 which is equal to 10 factorial divided by 4 factorial times 6 factorial we've got 10 times nine times eight times seven over four times three times two times one and then this is all multiplied by one half to the power 10 and the 4 and the 2 will cancel out with the 8 and the 3 will cancel out with a 9 leaving us a 3 so that gets us uh 210 over 1024 that's the tenth power of two uh which is equal to since there is a two in common uh 105 over 512 which is approximately equal to 0.2 okay so that's the answer to that one if x follows a binomial distribution with parameters 10 and 0.5 compute the probability that 4 is less than x which is less than or equal to 6. so we've actually got a couple ways we could do this let's do this without the table the probability that 4 is less than x which is less than or equal to 6 is equal to the probably mass function at five because you don't include four plus the probably mass function at six which is equal to uh uh it's going to be 10 choose five and we know we're just going to end up with one half to the power 10 in the end if in general if uh our parameter were not one half we should probably we should probably reason this way so we've got 10 choose five plus ten choose six i guess we switch to green uh one half to the power ten and that means what we need to compute now is ten choose five and ten two six we already know that one half to the power of the ten is uh uh one over a thousand twenty four so ten choose five that's going to be uh ten times nine times 8 times 7 times 6 divided by 5 times 4 times 3 times two times one and ten to six is equal to two hundred and ten and that's because 10 choose 6 is equal to 10 choose 4. i'm going to leave it up to you to figure that out i believe that was a problem in the exercise set but yeah that's the thing so now what we need to figure out is 10 choose 5. so we've got the 5 and the 2 canceling out with the 10 the 3 canceling out with the 9 reducing it to 3 uh and the 4 canceling out with the 8 reducing it to 2. so we've got in the numerator 3 times 2 times 7 times 6 and 3 times 2 times 7 times 6 is uh 252 i believe yeah so it's going to be yeah that's 252. so this quantity evaluates to 252. so we will get for their our probability uh 252 plus 210 which is uh which is going to be 462 divided by 1024. which that is that's around point uh five after you do some rounding uh we can also do some reducing of that fraction too now that said there was an alternative way we could have computed this quantity which was to say that this is equal to the cdf at six minus the cdf at 4. and then what's left what's left to do is get the cdf at six and four all right well let's get that i'm gonna have to boot up an r session all right so we've got p binom that's the function that is responsible for working with binomial random variables and we're going to give p bino what are we going to give it uh right so we're going to give it six our other parameters are size that's 10 and prob is equal to 0.5 minus p by nom which is going at 4 size equals 10 prob equals 0.5 yeah about 0.45 which rounds to about 0.5 which for what it's worth i said it was approximately 0.5 when we were doing stuff by hand and that was because we were rounding i knew i was rounding when i was uh when i was when i was saying that's about 0.5 so uh well let's go ahead and write down that more exam exact answer say that this is approximately 0.45 okay next example compute the probability that 2 is less than or equal to x which is less than or equal to four we could do this by summing up over the probability mass function but now i really don't want to do that i'm just i mean i've got better ways to spend my time so i'm going to instead say that this is equal to the cdf at 4 minus the cdf at uh two minus one remember we have to do the two minus one because we need to include the two in our region and the only way to do that is if we do two minus one okay and the other parameters are ten and one half and and you guys know what two minus one is this is equal to one which is going to be well let's compute this so p binom at 1 minus p by nom or other way around actually so we got four all right so 0.366 so approximately equal to 0.36 six what is the probability that you see more than seven heads strictly more so this is the probability that x is greater than seven that is well the converse event or the uh um the complementary event to the x being greater than seven is x is less than or equal to seven so this is going to be one minus the probability that x is less than or equal to seven since remember the probability of a complement is 1 minus the probability of a the complement of the set x is smaller than 7 is x is less than or equal to 7. so then we get that formula uh so this is going to be uh oops this is equal to one minus a cdf at seven with parameters ten and one half and now we need to compute that and we're going to turn to r for that so this is 1 minus the cdf at 7 size equals 10 prob equals 0.5 so the probability is about 0.05 let's say 0.055 so this is approximately equal to .055 now i should probably mention something else about how the software was working we could have done instead p by nom 7 size equals 10 prob equals 0.5 and there's an additional parameter that all of these uh p functions have which is lower dot tail lower dot tail let's set that equal to false that got us the same thing uh the p so basically these p bottom functions by default they're giving you the cdf but they can also give you one minus the cdf if you set lower tail equals false if you were to ask the developers for these functions they would say that rather than doing one minus the the cdf or one minus p binom or whatever you should use the lower tail equals false parameter the reason being that you're going to get less numerical error if you're using the lower tail equals false parameter because numerical error is very much a thing like we care whenever whenever we're using software to compute numbers we care about numerical error and it turns out that setting lower table tail equals false that results in less numerical error uh i i don't really know why i'm guessing it's because they can do some more optimizations or some other fancy uh numerical tricks thing is as an instructor i like to make sure that people are thinking like this expression or this relationship i really want students to understand that and it's very easy to just lose that under the easiness of this of this um of this uh of this function in this notation so i don't know how frequently i'm going to do that um and also for for whatever it's worth sometimes when i'm writing my own uh functions for probability i don't always include that parameter myself just because i can't i don't have it i know i don't really know what the developers are doing to make sure that lower tail goes false gives you more accurate answers uh so also by the way if if you're watching this in the future hello future person if you're watching this in the future and you're using a table because maybe because i told you to uh you're using a table for these calculations you don't have access to the probably that x is greater than seven you only have access to the probability that x is less than or equal to seven so being aware of um how this stuff is working uh or at least being aware of this relationship that i've underlined in blue still matters a great deal all right so continuing on compute the expected value of x the variance of x and the standard deviation of x all right so the expected value of x let's see all right the expected value of x is n times p which is 10 times one half which is equal to five all right simple enough the variance of x is n times p times 1 minus p which is 5 times one half which is five halves or two point five the standard deviation of x is going to be the square root of five halves and i don't know what that is off the top of my head so we can just leave it like that that's fine all right uh our functions that are doing this stuff well i think we just did it so some of this is completely redundant um here i actually created a random variable using discrete rv which presumably is loaded up uh so i create a random variable x to represent the x that we were talking about before and computing's expected value variance and standard deviation these are basically the same as what i had before i also plotted its probability mass function this is what it's probably a mass function looks like okay next example your manufacturer of widgets sends batches of witches and giant bins your company will accept a shipment of widget of widgets if no more than seven percent of widgets are defective the procedure for deciding whether a shipment is defective is to choose four widgets from the batch at random without replacement if more than one widget is defective the batch is rejected what's the probability of rejecting the batch if seven percent of widgets are defective model the process using a binomial random variable so we have so we're going to assume that there are in fact seven percent of widgets and actually the argument being used in this problem uh this problem is kind of suggesting the possibility well actually the procedure being described in this problem is basically a hypothesis test and uh we're gonna talk more about hypothesis testing later in a later chapter uh but basically what you do when you're working on the mathematics of a hypothesis test for computing pvalues and all that stuff you assume that the null hypothesis is true so the null hypothesis is that seven percent of the widgets are defective so um uh so in this case we assume that seven percent of widgets are defective in which case the distribution of the random variable x is going to be let's see how many widgets did they pull out for uh so n is equal to four and p is equal to 0.07 okay so also there's another wrinkle here the bin of widgets has a finite number of bims uh no no no not not fighting overpins has a finite number of widgets but this binomial random variable is not supposed to work in that situation see if there's a finite number of successes in this possible in this finite population then an implication of that is that you don't have independent successes and failures because if you pull a success out of the population you cannot pull that success again and presumably in this example when a when this uh when you're checking the widgets uh you pull out a widget you check it but you don't put it back in the bin it's for you to draw again no one ever does that so actually we don't have independent bernoulli trials so we don't have a sum of independent bernoullis we don't have independent trials which means that technically this random variable should not be a binomial random variable actually the random variable that is more accurate for this context is what's known as a hypergeometric random variable which we'll talk about in a later section thing though is if the sample if the population is large enough relative to how many successes there are in the sample like if there's a million widgets and seven percent of those widgets are defective then that means that about 70 000 defective widgets exist in the sample or in the population my my apologies uh in which case it the the numbers are so large that basically you can treat this as a binomial experiment anyway because the difference between the binomial random variable and this more accurate random variable the hypergeometric random variable those differences are negligible so you so you you get to you get to cheat you get to use the simpler binomial random variable as opposed to the more complicated more complicated hypergeometric okay so let's carry on then they want to know what is the probability of rejecting the batch if some percent are defective uh if more than one widget is defective okay with this problem we actually need to translate out because this is a word problem and by the way in stats classes i absolutely love to ask word problems so many word problems because statistics is so applied that it just feels inappropriate to not be asking word problems um it's it's such an applied subject that you have to be asking them okay so what corresponds to rejecting the batch you reject the batch if there's more than one defective widget in your sample so that's it so more than one that means greater than one and our random variable for tracking the number of defective widgets that we found is x and i guess all right a student might find it wait a minute wait a minute you're calling it defective widget a success yes it's mostly for the language all right so this is the probability we want to compute we want to compute the probability that x is greater than one which is equal to one minus the probability that x is less than or equal to one and at this point you could say all right let's go to r and compute this and i'm gonna say no we're not going to do that we're going to instead compute this thing by hand and say that this is going to be the pmf at uh 0 for parameters 4 and 0.07 plus the probability mass function at 1. oh i'm sorry i'm sorry uh i need to say that this is equal to 1 minus parentheses all that stuff okay all right there we go that's correct and now we need to compute each of those probability mass functions okay so the probably mass function at zero is going to be we've got four choose zero and four choose zero is one there's only one way to choose uh none of the four things in your in your little group and that's to choose all the one all the other ones there's only one way to do it so four to zero is one uh next up we've got uh what what have we got um oh yeah uh so .07 to the zeroth power and .93 to the fourth power b one for .07 is equal to four choose one which hopefully you know is four you can at least reason about i was like okay how many ways are there to pick one thing out of four we'll pick one of those four things all right there we go um so there we go uh so then we got .07 to the first power .93 to the third power and after this you go to a calculator to compute those numbers i think i actually computed them rather accurately in my notes so let's see what did i have okay so what i'm seeing here is that this is equal to so the so the top one is equal to point seven four that is an exact number i got a little carried away with the accuracy and the second one is point two two five two two and that means that we're going to have that this quantity here that ultimately is what we're trying to compute is equal to 1 minus 0.748052 plus 0.22522 which is equal to 1 minus 0.973272 3272 which is equal to 0.026 seven two eight all right uh next up oh yeah there it is excuse me my apologies i had to sneeze okay uh next example uh this one's fun i claim that i can make 80 of my free throw shots when playing basketball you plan to test me by having me shoot 20 baskets if i make fewer baskets in a specified amount you will call me a liar the threshold amount of baskets is chosen so that the probably make less than this amount given that i am in fact an 80 free throw shooter does not exceed five percent what is the threshold amount all right uh oh yeah additionally compute the mean and standard deviation of the number of shots i would make if my claim is true okay uh let's do this let's do the second part first because that's easier uh the first part is going to require a conversation so the expected value will say um s is following a binomial distribution uh with uh parameters i'm going to shoot 20 baskets yeah so n is 20 and p is 0.8 because all right this is again a hypothesis testing type problem in which case you're assuming that i am in fact an 80 free throw shooter so the expected value of not x because i decided not to do x um of s is going to be 0.8 times 20 which is equal to 16. so you expect me to make 16 of my baskets uh the did i ask for standard deviation yes i did so the variance of this random variable is going to be uh 20 times 0.8 times 0.2 which is equal to 3.2 and the standard deviation of s is equal to the square root of 3.2 which i don't know off the top of my head and i i'm i'm just i'm just not gonna bother okay all right so that was the easy part now for the hard part uh i have asked that you pick a threshold amount of baskets so that the probability i make less than this amount given that i am in fact an 80 free throw shooter does not exceed five percent all right so we will call this threshold amount we will give it a name uh the threshold amount we will call this we will call this quantity k all right i'm asking that i i'm asking you to find a k such that the probability that s is let's see so if i make fewer baskets so s is less than k this number needs to be at most 5 so this needs to be at least 0.05 well at most 0.05 and actually there's going to be a number of possible k's such that it's less than 0.05 but we're going to say that this is the largest possible k uh such that such that this probability is less than or equal to 0.05 uh k is a constant here we just don't know what it is uh s is a random variable uh let's go ahead and play around with this expression some more before continuing on this is saying that the probability that s is less than or equal to k minus 1 uh less than or equal to k minus one that's going to be less than or equal to 0.05 all right so we actually so by doing that i have the cdf on the left hand side of the inequality and 0.05 on the right hand side so we're go what we need to do now is basically a reverse lookup we're looking up a number such that the cdf is um less than or equal to 0.05 the largest number possible such that the cdf is less or equal 0.05 this is similar to the notion of quantile because a quo so you have a probability that a random variable is less than or equal to uh let's say 10 percent then the 10 quantile is that number so this is actually related to the notion of quantiles the unfortunate thing though is that what we're talking about are discrete random variables which adds this complication in that it's possible that the cdf in fact not just possible it's quite likely that the cdf never actually equals 0.05 if it were in fact equal for the cdf to equal 0.05 then we would say all right pick a k minus 1 such that cdf is equal to 0.05 the only thing though is that's not actually likely to be the case um it's likely that the cdf never actually reaches 0.05 in fact let's now go to r so let's look at so we got uh the cdf the cdf is so possible values for the standard variable are from 0 to 20. so we're going to ask for the cdf's values from for all numbers between 0 and 20. size is equal to 20 prob is equal to 0.8 ah that's it's not very helpful um all right i think that if we were looking at the textbook we would be rounding to uh three decimal places so uh so digits equals three all right yeah this is yeah we're gonna do this just because like what we have up here is scientific notation which may be more accurate but uh also it's hard to read so we're gonna round to three decimal places and work with this so this is the cdf uh and what we're looking for is uh we would go let's actually let's actually call give this vector a name so we'll call it cdf and we'll say names cdf will be 0 to 20. so now let's print out cdf okay all right so so so so so so um what we're looking for in this is a qua is a quantity where the cdf uh does so that so notice that the cdf is increasing as we increase the input to the cdf right our cdf is increasing but and we want to find the largest number that we can put into the cdf such that it doesn't exceed 0.05 and that number is 12 because at 12 the cdf is going to be 0.032 and at 13 the cdf is going to be 0.087 so it crosses that threshold at 12 suggesting the that k minus 1 suggesting that k minus 1 equals 12. and then we add 1 to both sides to suggest that k is equal to 13 that is if i score less than if i make less than 13 baskets you're going to call me a liar uh what is uh 13 divided by 20 that is uh 0.6 so that's 0.65 so if i make uh 65 or less than if i make less than 65 of my basket you're going to call me a liar the principle being the the logic being that that is such a rare amount it's so unlikely for you to score less than 60 65 of your baskets if you were in fact an eighty percent three free throw shooter that we're actually gonna say it's more likely that you're lying then you're actually telling the truth or at least it seems it seems unreasonable to continue to believe that you are still telling the truth so this is this so so that's how you would use like the cdf if we were if we if you were using the textbook you would use the cdf this way you would scan the cdf uh for some reason my mouse stopped working i don't know why this is a super cheap computer you would scan the cdf until eventually you crossed over that 0.05 threshold and then take whatever this what take whatever number got the cdf2 uh just below that threshold so if i had switched this to uh say all right it needs to be less than or equal to 0.10 then we would go to 13 we haven't crossed point 10 yet we would go to 14 but then we then we would cross so we would say all right the threshold amount is 13. uh let's say that i said instead 0.01 uh actually 11 is probably not what you would choose because you know that we're rounding here so you would go with 10. so because you know that actually well i don't know what what was a 11 so if we look back at the original vector that's a bit more accurate that doesn't have any rounding so one two three four five six seven eight nine ten eleven uh next one oh actually it's rounding up so you could choose 11. yeah 11 would be fine so yeah it doesn't always round down uh so that's what you would do you would just kind of reverse look from the uh the uh from the cdf especially if you're using the book or you could use q binom uh we'll put in 0.05 size equals 20 prob equals 0.8 you could have used uh okay what exactly is cubinom doing uh it might be doing something else it might be saying okay you exceed it because so we actually might need to do q by minus one uh let's look at the documentation for q by nom oh i think you just yeah you would just have to recognize all right so uh it's going to give you some details somewhere okay so it says right here the quantile is defined as the smallest value x such that f of x is greater than or equal to p which is actually different from how i just defined it so r's definition of what a quantile is for discrete random variables is uh different than what i just said so being aware of that you would actually have to take uh whatever q binong gave you and then do minus one to get the right answer okay but all right or i mean i i don't know i think there's so many different ways to possibly think about it and say okay the q binom that that r actually has is uh effectively uh giving you this quantity right away so you don't have to work with the cdf you that's another way you could possibly think about it all right so uh by the way what we're basically saying is that if you ended up shooting a number of baskets such that you ended up in uh 11 12 such that you ended up in this region it's so unlikely if you were in fact an 80 free throw sure to end up in this region i'm justified in saying you're a liar how unlikely is it well it's actually uh 3.2 percent uh it's so so so unlikely that we would just call you a liar because if you were in fact a three throw shooter you should probably be ending up in the other region and 80 free throw shooter my apologies i'm always confusing my words okay so that's it for uh the binomial random variable and in the next section we will be talking about the hypergeometric and negative binomial distributions all right so i will see you there uh we are now on to the last section of this chapter discussing the poisson process and the poisson probability distribution so x is said to follow a poisson poisson distribution with parameter mu or if the probability mass function of x is given by p of x parameterized by mu this is equal to e to the power negative mu mu to the power x over x factorial for uh x being a member of the set 0 1 2 and so on so in other words if if x is a whole number if x is a whole number then this is the probably mass function otherwise it's zero so the first question you may ask is this a valid pmf the answer is yes and why is that well let's sum the probably mass function from x uh sorry from x uh equals zero to infinity we got p of x parameterized by mu this is equal to the sum from x equals zero to infinity e negative mu mu to the x over x factorial the e to the negative mu part that is effectively a constant so that can be pulled out so we could say that this is e to the negative mu and then we have the sum from x equals zero to infinity mu to the x over x factorial and that part which i have highlighted in red that is e to the power mu y calculus 2. this is the if i remember right the taylor expansion of the function e to the power x so this is coming from calculus 2 but yeah that evaluates to e to the mu hence we get e negative mu e to the power of mu and those are that equals one and furthermore this probably mass function is positive everywhere so we get this is a valid pmf so uh poison random variables if x is following a poisson distribution with prion or mu then the expected value of x is equal to mu and also the variance of x is equal to mu so that means that we are parameterizing poisson random variables by their mean so in uh in your book if you're using the uh the dvor book table 8.2 contains the cdf of select poisson random variables or select poisson distributions uh in r the functions that are responsible for handling points on random variables random variables sorry our deploys handling polymath function cdf quantiles and random and random variants so in other words creating random instances of poisson random variables and they will be parameterized by their mean so the poisson distribution describes uh random variables that follow the poisson process so here is the intuition of poisson random variables they are tracking how many times within some how often a quoteunquote rare event occurs within a finite span of time so and which isn't at all clear from looking at this probably mass function that would in fact be the interpretation uh this is more of a limiting result this footnote that i have right here actually gives a little bit more justification as to why this is the case but you can think of it as all right we have some rare event uh me like i have a favorite example that was used by my uh 3070 instructor maybe i'll just pull up his webpage for you uh like i he he's got some he's got some interesting stuff on his on his web page that even now i kind of look back to it and like i think about it and i just kind of i kind of want to update it i think i think it could be updated but there's some interesting stuff there genuinely so math.utah.edu tilde treyberg uh his name was uh andres trebergs a professor at the u not actually a probability uh professor that's not his uh area of expertise although he's taught math 70 a few times and math the no matt 3070 sorry and matt 3080 a few times and like he he's interested in statistics but he is a uh what what is that area of study of his i think it's like analytic geometry i think that's what he studies so i wonder what happens if i just go straight here we could probably find from here his uh 3070 page yes i think this is i think this is it math 3070 fall 2013. math 3070 fall 2012 is the last that was the class that i took this was my web page this is where i went to find his stuff so yeah i i took matt 3070 in this class i think i'm pretty sure that's true all right if i scroll down yeah the supplementary materials oh yeah so here's a simple r uh which by john farzani this is the uh lab textbook for the r lab and uh several example problems he's got this one let's find it let's see prussian no no press poise ah let's see is this it i think this is it i think this is it uh are there horse kicks are people getting kicked by horses that's what i want to know i think this is i think this is it um i don't think so i don't think this is the horse kick example but the horse kick example is fun let's see oh why i want to close that webpage uh if we try horse come on there it is there it is the horse kick example ah yeah i love this one yeah so so have a look at this but basically it turned out that in the prussian army um you can model the number of people who died from horse kicks with poison random variables that poisson random variables actually do a good job of modeling stuff like that but i also had as potential examples i'm not entirely sure how the how accurate this is but you could maybe like maybe think of uh number of calls that a call center is getting in a day or the number of points that scored by a team in a game uh poison random variables so and also as being a poisson process a poisson process uh is a bit more general than that so a poisson process is a stochastic process stochastic processes are not the subject of this class beyond this little description i think another thing that could be considered a poisson process or that could be modeled by a poisson random variable is let's say you have some radioactive matter and the number of uh is it atoms i'm not sure i'm not very good at physics but some some particles are leaving the radioactive mass and the amount of particles that are leaving the mass uh over some period of time can be understood as a poisson random variable and also as a poisson process so i have this r code that's meant to simulate a poisson process and with a poisson process the time at which a particle or the time at which uh the pro the process jumps is random so it will so there are going to be random times at which the process increases its value by one so uh it will jump so you'll wait for a period of time and then suddenly the process will jump and basically something has left and then you'll wait a little longer and something else has happened maybe you could imagine this as being uh points in a game maybe a basketball game although i you could probably see why it's a little bit inaccurate to view points in basketball game as a poisson process because there's no way that you could well i guess this is possible if maybe if maybe you said both teams uh if you're counting like total points scored by either team i don't know it seems unreasonable though but you could but basically at random times this process is going to jump uh by one and the value of the process is going to increase and basically any fixed time the value of the process at a fixed time uh so you fix it at two at two or something and the value of the process at that time can be modeled by a poisson random variable so yeah that's uh hopefully that gives you some idea of what they're what they're describing how often some event occurs over a fixed period of time which in principle that event could be unbounded there's an infinite number of pos it could happen like there's no upper bound on how many times this event could happen it can happen an infinite number of times there's well okay maybe not literally infinite but any large number of times it's highly unlikely that it will be very large but there is no upper bound right so you're not gonna restrict it like the number of points that you could score in a basketball game like there's no there's no limit on that no one will just end the game well i don't know maybe but that's that seems that seems rather academic but yeah that's so this is going to be a random variable that is defined hello everyone we are now on the next chapter on continuous random variables and probability distributions continuous probability models are another major class of probability models in the previous chapter we saw models for discrete random variables with discrete random variables there was either a finite number of possible numbers this random variable could take or it was countably infinite for example the whole numbers were were possible in this situation not only are the number of possibilities infinite they are uncountably infinite so this these uh probably models allow for any real number within some range it could be within a range a to b or z like a to infinity or zero to infinity or negative infinity to infinity so any real number any of those could be possibly taken by the random variable so as a consequence of this we're going to change some of the notions that we had with discrete random variables but not by much for example the probability mass function is going to be replaced with the probability density function which is what we're going to be talking about right now and the cdf is still defined the same way but it's going to be computed a little bit differently instead of it being involving a sum it's going to involve an integral and expectations are also going to involve integrals basically whenever you would add numbers with discrete random variables you integrate with continuous random variables so this is where your calculus knowledge is going to be tested one nice thing though about continuous random variables though is that when you're working with continuous random variables uh the probability that the that that random variable is equal to any particular number is always zero which is the reason why we have to have uh integration and density functions so the probability that x is less than x is the probability that x is less than or equal to x and admittedly it is a little strange that the probability that this random variable is equal to a particular number is zero it's it's somewhat strange because when you go to your random number generator and ask this thing to produce a number it gives you a number but the probability that you got that number was zero so events that are happening with probably zero are happening all the time whenever you're working with continuous random variables but the way ferrous rasulaga one of my probability instructors researcher at the university of utah very good mathematician uh one way he put it was uh the you know at some level that someone's going to win the lottery you just know it's not going to be you so you get the probably that you win the lottery is zero and the probably that anyone wins the lottery is not zero that's the way he put it so uh let's uh move on to discussing probability density functions so these are the analog to the probability mass function for discrete random variables the pdf is a nonnet negative function which i'm calling f x such that for any two numbers a and b with a less than or equal to b the probability that a is less than or equal to x which is our random variable which is less than or equal to b is equal to the integral from a to b f of x dx naturally in order for f to be a valid pdf we must also have that the integral from no not a uh the integral from negative infinity to infinity of f of x dx well that's the probability that this random variable is between negative infinity and infinity and that's basically asking for what is the probability that this random variable is any real number we're basically asking what is the probability that x is a real number so naturally this must equal 1 because we know that x will be a random a real number and it will be finite so this is another relation we have to have now regarding this f itself might not be continuous everywhere and it's also possible that actually integrating from negative infinity to infinity is a bit much because actually this random variable is only positive on some on some interval of finite length and everywhere else it's zero so you're integrating for the most part zero and in fact we'll see one example of this one we are now discussing the ever famous maybe two famous sometimes infamous infamous not infamous that's not a word sometimes infamous normal distribution we say that a random variable x follows the normal distribution sometimes denoted like so x uh x follows a distribution n mu sigma so we say x follows a normal distribution with mean mu and standard deviation sigma if it has the pdf v of x parameterized by uh mu and sigma is equal to all right we should probably zoom in for this um it's a fairly a little complicated we have 1 over the square root of 2 pi sigma squared so this fraction multiplying with e to the power negative x minus mu squared divided by all in the power uh two sigma squared it's worth mentioning that often the normal distribution is parameterized not by its standard deviation but rather by uh its variance and you can kind of see why when you look when i've written this formula down yeah it's possible to take this sigma squared and pull it out in front of the square root uh but we can put it inside of the square root and then you have uh and then everything you've got sigma squares everywhere so you could just specify sigma squared directly and also it feels to statisticians to be or and probabilists my apologies to be more appropriate to parameterize by the variance rather than the standard deviation since it's often easier to work with the variance uh directly rather than the standard deviation and in addition to this you could say that parameterizing with the variance generalizes better when you start talking about multivariate versions of the normal distribution but this is fine for now like admittedly at at an introductory stats level it feels somewhat like since the standard deviation is the more uh natural measure of spread and the variance a little bit more alien you could argue that to these students it seems somewhat better to use the standard deviation rather than the variance but it's fine so this is the curve here is a sketch of the density curve for the normal distribution we have uh oops okay so here's kind of what it looks like uh we would have basically here is the mean here is the mean plus one standard deviation and here's the mean minus one standard deviation okay so we would have basically a curve that goes up there within one standard deviation is an inflection point so it will go from convex to concave at the inflection point and then it's going to be a symmetric curve and then go from concave back to convex all right and this is a simple sketch of what it may look like its peak occurs around the mean and we have our inflection points being within one standard deviation of the mean okay so that's a this is the whenever you hear the words the bell curve they are probably referring to the normal distribution be aware that the normal distribution is not the only bellshaped curve that is used in probability and statistics there are other bellshaped curves for it for instance there's the t distribution there's the koshi distribution i think i encountered a distribution recently that is meant to model uh when well okay it's not really a probability distribution though but it's kind of like one uh when someone gets the coronavir or no not cordovice when someone gets a virus during a pandemic type situation there's a curve for that that looks like the normal but isn't the normal i can't remember what it is though it's like the logistic curve i i don't know but yeah um uh this is usually when people are talking about the bell curve all in caps like capitalize and all that they're talking about the normal distribution here's an r plot of the density function of a standard normal curve and it's got that bell shape so the expected value of a random variable x following this distribution the variance and the standard deviation will be good given next uh these should not be shocking at all the expected value of x is equal to mu the variance of x is equal to sigma squared and the standard deviation of x is equal to sigma so normal random variables are specified by their mean and their variance okay you set the mean and the variance directly with normal random variables okay one property of the normal distribution is the 68 95 99.7 rule which i ain't going to sketch out for you this is basically a rule of thumb for how much of the distribution is within uh let's say uh within one standard deviation of the mean so we got mu plus sigma mu minus sigma within two standard deviations of the mean so mu plus two sigma mu minus two sigma and within three standard deviations of the mean so mu plus three sigma and uh mu minus three sigma okay so the peak of the curve happens at mu and we'll and let's see uh just kind of getting a sketch so uh the inflection points are going to happen with within uh one standard deviation all right so we could sketch out the curve to look something like this okay so uh what this rule says is let's uh start out within uh one standard deviation the area underneath the curve within one standard deviation is going to be 0.68 so within one standard deviation so mu plus or minus sigma all right uh let's go to within two standard deviations okay so within two standard deviations so the area underneath the curve within two standard deviations so the area underneath the curve within two standard deviations is going to be 0.95 so this will be mu plus or minus 2 sigma and then if we go to three standard deviations so let's uh have this going now let's have this going up and down horizontally is rough to draw okay on the other hand that's kind of a conflicting picture but you get the idea um within three standard deviations the area underneath the curve within three standard deviations is going to be 0.997 so this will be mu plus or minus three sigma okay the unfortunate thing about that is i just drew over uh the text but i can still read so uh let z follow a normal distribution with mean zero and standard deviation one we then say that the random variable z follows the standard normal distribution and this distribution is useful since we can relate an arbitrary normal random variable to the standard normal distribution and vice versa and we can do so like so uh as a so as a reminder z is following a standard normal uh mu is an arbitrary normal random variable no no no not mu sorry mu is not a random variable so x is an arbitrary normal random variable so any mean and sigma in this case what you get is that if you take a normal random variable subtract out its mean and then divide by its standard deviation the distribution of the resulting variable will be equal to the distribution of the random variable z that is it's following a standard normal distribution which should make sense what this operation does is shift the mean to zero and then what you do is you uh scale by sigma or scale but let's say scale by one over sigma so you both uh take your curve along the number line oops i didn't want to erase stuff all right so what you're doing is you're taking your standard normal curve which is located at mu you are shifting it to the left uh by by mu and then you are compressing the curve so let's let's draw like a new curve that's oops let's draw a new curve that's centered at zero and then you compress your curve so it has a standard deviation of one all right that's what that operation is doing and similarly we could say that if we take a our standard normal random variable z scale it by sigma and then add mu the resulting random variable will be equal in distribution to the random variable x which is following a normal distribution with mean mu and standard dev standard deviation sigma all right what this is doing it's basically doing the opposite we are scaling by sigma and then we are and then we are shifting the mean to mu so to sketch out what this is doing we start out with our standard normal random variable which is centered at zero we then take that random variable shift its mean by mu so we end up with something over here and then we scale out its uh standard deviation so we get a curve with possibly a different standard deviation it doesn't have to get bigger it could get smaller too but you get the idea of scaling okay so uh let capital phi of little z be the probability that a standard normal random variable is less than or equal to z this is the cdf of the standard normal distribution then if x is the probability of an arbitrary normal distribution or normally just of an arbitrary normally distributed random variable we have excuse me we have the falling relationship between f and phi f of x is equal to phi of x minus mu over sigma where phi is the cdf of a standard normal distribution which means since we can relate any normal distributions cdf to the cdf of the standard normal random variable this means that we only need to worry about tabulating values for phi of z for the standard normal distribution in order to work with any normal distribution which is what's done in table 8.3 of dvor's book and often many of these statistics and probability books will have tables of the cdf of the standard random variable and i even remember buying a study card for math 3070 for my set my stats class and that table came with a no and that card came with a very small table for c for working with the cdf of a standard normal random variable and the reason why you can do why they're doing that is because you can get all of the information you need for any normal random variable from that table which is very nice because you can have just you can have any real number mean and any positive real number standard deviation and have an infinite number of normal random variables but you only need to print one table because once you have the table for the standard normal then everything works out great and additionally let's consider for a second the pdf of a standard or of a normal random variable uh let's let's consider actually first the pdf of a standard normal uh fee which will we will call just fee of x that's going to be uh 1 over the square root of 2 pi um actually let's let's call this v of z it's going to be 1 over the square root of 2 pi e negative z squared over 2. and this is kind of a mess so let's clean that up okay you you know by now that cdfs are computed via integration because all probabilities are computed v integration when you're working with normal random variables what then is the antiderivative of this the answer is basically what you see the thing is uh you there is no closed form or elementary solution for the antiderivative of a normal random variable it doesn't exist it simply doesn't exist it's and i think in fact it's provable that you can't come up with an antiderivative for a normal random variable for a number of random variables pdf um in a in a in in any element in any elementary form so at the end of the day you're just kind of left with saying all right this is ranging from from uh negative infinity to uh i don't know x negative infinity to x you're just left with saying that this is equal to phi of x like that you're kind of left with this uh unsatisfying uh unsatisfying answer where you just say all right phi is this der is this integral and yet at the same time this is not a problem in fact nobody really cares nobody really needs to have an antiderivative for what i've written down in black nobody really needs it because we have numerical routines that can compute these integrals mathematically we can still work with it there's nothing that says that we can't work with this we have the fundamental theorem of calculus that is able that tells us how to take derivatives of this thing so we can take derivatives we can still study its properties we can study how how quickly it grows and decays and and stuff like that there's really no reason we need a simpler expression for the antiderivative of a normal random variable so we just say this is true by d this is true by definition and go about our business because whenever we need to actually compute what the cdf is we have techniques for doing it we can use all these numerical routines uh numerical routines maybe some monte carlo simulation something like that there's all sorts of things that we can do to compute the cdf and you only need to do it once for the standard normal curve in fact i think r actually internally when when working with the cdf of a uh stan of um of normal random variables is work it's working with an internal table uh that computes probabilities so um admittedly i'm teaching this right now in an online format in a context where students are not going to ever enter a testing center so they don't need to use the table and if i were teaching this in a regular semester i would teach students to use the table and it just feels like right now that's silly because they have access to r and they're not going to want to use the table and i've all never really had a problem with teaching the table from a pedagogical perspective because i feel like using the table was good practice for working with the basic properties of the normal distribution and now i don't really see a reason to use it i mean there is still that pedagogical reason but it's just completely swamped by the convenience of having r around so we're going to lose that i'm actually rather sad that this time i'm not really going to use the table that i'm just going to compute probabilities using r but hopefully you can still get the message and understand some of the properties that would be learned by working with the table such as the symmetry of the normal distribution or working with one minus and stuff like that okay if i ever teach this class again in person uh maybe what a maybe i would and if i were to use these lecture videos again maybe i would create a separate video for uh working with the table but i don't think i will do that now all right so um anyway let's compute the following we're now working with a standard normal random variable so remember that this is a random variable uh with uh a mean i can do better than that hold on okay so this is a random variable good grief good grief come on just some people just don't want to press the undo button good grief ugh this screen how i hate it okay um okay so remember that we are working with the standard normal distribution so it's a distribution centered at zero standard deviation one i'll just tell you that standard deviation is one i want to compute the probability that z is less than equal to zero so this is what the normal distribution looks like this is the area that i want to compute what is that area well i know the area under the entire curve is one because it's a pdf and we know that we're shading the area to the left of zero and the zero is the point of symmetry so that means half the area so if you were to fold the curve over on itself around zero it would have equal area to the left and to the right of zero which means that this must be half of the area underneath the curve so that means that this is going to be equal to 0.5 because 0 is the median of a standard normal distribution all right uh next what is the probability that z is less than or equal to 1.23 okay so what where she so what we're computing here here's a standard normal curve here's 1.23 we are computing the area underneath the curve and to the left of 1.23 okay and at this point i'm going to ask r what that area is so okay so i want a p norm by default p norm is working with a standard normal curve so we've got p norm of right uh 1.23 so that's going to be 0.8906 uh or we'll say 0.8907 so this will be this is equal to 0.89 okay uh let's see next up oh look at that some r code and uh it's basically confirming what we got via r so not shocking all right next up the probability that uh the center number and a variable is between negative 1.97 and 2.1 so this will be we've got the standard normal curve okay here's zero here's uh 2.1 here's negative 1.97 and we want the area in this region the area in between 2.1 and negative 1.97 so how are we going to do that well one thing we could do is say remembering that we are working with a cdf that this is going to be the area underneath the curve to the left of 2.1 minus the area underneath the curve to the right of negative 1.97 so you just subtract out the area from the left of negative 1.97 from the air that's to the left of 2.1 you can think of that as you have a piece of construction paper and this piece of construction paper can is the normal curve including the region underneath the normal curve that's a really long piece of construction paper since uh the normal curve extends from negative infinity to infinity right i never put any sort of bounds on this curve so i hope you notice that that this is a random variable that can take any number any real number between negative infinity and infinity um so we uh uh but you know we imagine that we have this uh maybe we clipped it off after a certain point and uh which is fine because after after a while the normal curve becomes minuscule so so minuscule because it it approaches zero very very quickly one way to interpret the 30 68 95 99.7 rule is saying that almost all of the curve is within three standard deviations and there's almost nothing outside of it so and that goes even more so for four standard deviations five standard deviations and so on there's almost nothing there um anyway we have we imagine that we have this piece of construction paper and we we clip off the area at 2.1 and have the area underneath the curve and to the left 2.1 and then we clip off the area underneath the curve to the right to the left of negative one point nine seven and what we end up with is the area that we want so we met or and if what we could end up doing is we start out by measuring the area underneath the curve uh the the area of our construction paper when we did our first clip when we clipped at 2.1 and we measured that area and then we clip again at negative 1.97 and measure the area of the part that we clipped off and then subtract that from our earlier calculation to get the area that's remaining uh uh for our construction paper so this will be uh fee uh remember this remember that capital fee is a cdf of a standard normal random variable so fiat 2.1 minus fee at negative 1.97 okay and then we need to compute this so p norm uh 2.1 minus p norm negative 1.97 and this is what we get so we get 0.9577 so we get oops so we get that this is equal to 0.9577 okay uh next example the probability that z is greater than or equal to 1.8 so this is the area underneath the normal curve that's to the right of 1.8 okay and we say that this is going to be well we could say that this is uh the area underneath the entire curve so here i've shaded the whole thing minus the area underneath the curve uh to the left of 1.8 going back to our construction paper analogy you have this piece of construction paper that has that's the the area underneath the entire curve uh and you clip off the uh the part at 1.8 and you're left with uh the part from negative infinity up to 1.8 so you lost the other part and you want to figure out the area of the other part well you knew the entire area was one so you measure the area of the part that's that you um the part to the left of uh to the left of 1.8 and subtract that from one to get uh to get the area that's underneath the curve and to the right of 1.8 so this would be 1 minus the cdf of the standard normal at 1.8 and we can go to r and compute this so 1 minus p norm at 1.8 and this is going to be 0.0359 so this is 0.0359 okay excellent uh the probability that it's greater than 5.2 so this is going to be approximately zero but i'll go ahead and compute this in r and say one minus p norm uh what was the number we're plugging in 5.2 okay 5.2 all right very very very small number uh not quite numerically zero because we can go to 16 decimal places but very very close um very small number and actually if you were using the table and if you look at most tables most tables don't go beyond four standard deviations they might even go beyond three or three and a half but you'll almost never see a table go beyond four standard deviations and then here we're asking for the area underneath the curve to the left of five standard deviations so uh if we were actually into the table which is uh the context in which these notes were written uh we would say what i would basically tell students is say this is approximately zero so don't even bother to look at the table just say this is approximately zero okay here's some arco that's doing all these calculations uh we also we also could have done some of that one minus stuff using the lower tail uh parameter let's see is that different from what we had no it's not different so that looks to be about the same um all right uh next example so iq scores are said to be normally distributed with mean 100 and standard deviation 15 like q be randomly selected be a randomly selected individual's iq score uh compute the probability that q is between 85 and 115. so in this case this was here's another thing um this is there's a lot of reasons why i really liked working with the table and one of the things that was great about working with the table was that it forced you to translate from a normal distribution to a standard normal distribution or any normal distribution to standard normal and thus i felt like it would force students to learn the relationship between any normal random variable and a standard normal random variable so what we could say here is that this is going to be the probability that 85 minus the so q is going to be according to this problem a normal random variable with being 100 and standard deviation 15. so this will be 85 minus 100 over 15 that's less than or equal to q minus 100 over 15 which is less than or equal to 115 minus 100 over 15. and this part right here is equal in distribution to a standard normal random variable z so we could say that this is equal to after you compute that lower and upper bound you'll find that this is the probability that negative one is less than or equal to z which is less than or equal to one which is going to be about 0.68 because of the 68 95 99.7 roll so you really wouldn't even have to go the oh okay so 0.68 is very much an approximation it's not exactly true uh but it's kind of close it's i think it's true if you round to two decimal places so um well i'm actually not really sure uh but yeah you do have this um and this is basically forcing us to convert to the standard normal case um the unfortunate thing though is that now i could just do this you could do p norm um uh 115 uh mean equals 100 and the other parameter is sd is equal to 15 minus p norm uh 85 mean equals 100 sd equals 15. and we get 0.6826 or 2 7. uh if we wanted to uh we could instead have written p norm 1 minus p norm negative 1 and they get the same number uh using basically that alternate form because you might not be converting you might not be converting to a standard random variable but r i'm pretty sure is so all right there's competing that uh this so the sad thing is that you can just do that and now i can't force you to uh use a table and uh and convert uh and uh convert to a standard normal random variable that's unfortunate okay uh so the probability that q is greater than 90. you know what i can't force to do it but i can still do it because i still have a point to make right so this is going to equal the probability that q minus 100 over 15 is greater than 90 minus 100 over 15 which is equal to uh 1 minus the cdf of a standard normal curve a standard normal random variable at uh at a 90 minus 100 over 15 this part becomes uh 10 over 15 or negative 10 over 15 which is negative twothirds so i would say uh this is going to be fee at i'll even round it i'll se i'll even convert it to a decimal number so negative zero 0.67 which is approximate but this is supposed to be negative twothirds and then i go to r and compute this and say this is one minus p norm uh negative two thirds which is point seven four seven five so this is approximately equal to 0.7475 all right next up the international society for philosophical inquiry requires potential members to have an iq of at least 135 in order to join the society this is one of those socalled genius societies based on this what proportion of the population is eligible for membership so this is the probability that an individual's iq is at least 135 which is equal to uh the probability that q minus 100 over 15 is greater than or equal to 135 uh hold on uh let's move this so this is equal to probability that q minus 100 over 15. is greater than or equal to 135 minus 100 over 15 which is equal to uh the probability that a well okay this is going to be uh one minus the cdf of a normal variable or one minus fee at so 135 minus 100 over 15 is 35 over 15 uh which is seven thirds which is about 2.33 okay so about 2.33 so 1 minus p norm 2.33 actually we'll just do seven divided by three so point zero zero nine eight so this is approximately point zero zero nine eight okay and here is some r code where i'm actually calling these mean functions sd function so mean sd parameters also using lower tail rather than doing one minus the cdf and so on all right uh so that was all trying to compute probabilities but sometimes we want to compute quantiles so we've got the notation z alpha which means that the cdf of at z alpha is equal to one minus alpha we can relate this back to general uh percentiles to find for arbitrary normal normally distributed random variables because these are the percentiles of um standard normal random variables okay so this is going to so we have in general a to p is going to be uh sigma z 1 minus p plus mu that's our percentile formula and z1 minus alpha can be found using table 8.3 using a reverse lookup but since you now have r there's really no point about discussing reverse lookups like this okay i mean i felt like they were good practice but we're now not going to be doing that anymore all right so uh what is z 0.5 that's the median of a standard normal z 0.5 is the area where the curve is split in two equal parts and that's going to be zero uh what is z 0.05 well uh what we could do is say all right let's go to p norm no no no no we're not using p norm anymore we're using cute arm so q norm uh 0.05 so uh when you could put in 0.99 no put put in 0.95 instead that would give us the same thing this is going to be 1 minus so this is 1 minus .05 or alternatively we could do q norm .05 lower dot tail equals false and that also works those all get us the same number so in the end though this is about 1.64 so this is so z 0.05 is about 1.64 okay what are the first and third quartiles of the standard normal distribution so what we're looking for is uh z so the first quartile will be z point seven five because remember we're doing up so the uh so here the uh subscript of the z is the upper tail area so this is the first quartile and the third quartile is 0.25 this is q3 all right so what are those going to be when we go to r and say q norm 0.75 dot tail equals false so we get negative zero point six seven so negative zero point six seven and this one well actually i'm not even going to bother computer because i know what it is it's 0.67 because we're working with a symmetric curve since we're working with a symmetric curve if we know one of those things that we know the other one because if the area underneath the curve to the left of z 7.75 is .25 the area underneath the curve to the right of point z 0.25 is also 0.25 so all we ever did was just flip over the flip over the y axis where x is equal to zero oh no let's not put a y there that's just confusing but my stupid undo button isn't working gosh why does stuff have to be so moody anyway um well since we're working with a symmetric distribution we actually have a property that i think gets written down uh later uh where did i write it down did i was i supposed to write it down up here where did i write it i know i wrote it down i know that i plan on talking about it at some point in these lecture notes oh i brought about on page 21 but basically i'll i'll i'll just cut to the chase and write it right now uh so z alpha is equal to negative z one minus alpha so basically uh you can just flip over the flip over the y uh the uh y axis so change the sign and work with one minus that area and you can get the same quantiles so but if it makes you feel better i'll go ahead and compute it and this is 0.25 yeah so what i did basically was exploit the symmetry of the standard normal distribution it's symmetry around zero our description of the random variable q from example 11. uh so using that answer the following questions mensa international requires individuals have an iq score that would place them in the top two percent of the population was the minimum iq score needed to be a member of mensa well that would be um so the standard deviation is 15. we've got z so top two percent so that's going to be 0.02 the upper tail area is 0.02 plus 100 and this is going to be let's see we've got 15 times q norm 0.02 lower dot tail equals false plus 100 130 or i guess he'd ran into 131 uh so we'll say 131. uh after rounding so 131 you need to have an iq of 131 in order to be a member of mensa uh there's an alternative way to do that though we could have instead we could have instead done q norm .02 mean equals 100 sd equals 15 lower dot tail equals false that would have also worked okay uh the part of the population with the lowest five percent of iq scores is considered to be intellectually disabled what is the highest iq score needed to be in this group okay so uh that means that we are looking at uh z 0.95 or actually we're asking for eta of 0.05 up here in this problem we were looking for eta of 0.98 okay so this is going to be 15z 0.95 plus 100 and then we go and compute that so in this case we got 0.95 now this is going to be 75.32 so we'll say about after rounding 75. so if you have a so if you have an iq score of 75 or lower you're considered intellectually disabled okay so right there's some r code that's doing the same thing so do the symmetry at the normal distribution we have the following useful identities for fee uh one second okay we have that the cdf at z is equal to one minus the cdf at negative z which what this is saying is uh if you were looking at so this is a standard normal distribution if you were looking at the area underneath the curve and to the left of z another way you could compute that quantity is look at the area underneath the curve and to the right of negative z which is what hap which is what you get when you flip over the uh y axis and then subtract that from 1 to get the to get the red area underneath the curve so the so the area above negative z um is going to be equal to the area below z or the area to the right of negative z is equal to the area to the left of z because of the symmetry of the curve and equivalently well as a consequence of this we have z alpha is equal to negative z 1 minus alpha that's our immediate consequence all right so as mentioned before fee can be used to approximate the cdf of other random variables so it turns out that the normal distribution can be used to approximate the uh cdf of other random variables or approximately other random variables basically which of course mattered more historically when we didn't have uh when we had to basically physically print out tables for random variables but you still want to be able to get probabilities for binomials with large parameters large n or uh poisson random variables with large mu but in this situation um like in the in the world in which we currently live that's less of an issue because software doesn't ask you how big n is it just works so um all right so well i guess it does literally ask you but uh it's not like you put in the wrong number and it will just not work um unless of course of course you put in something that's really big to the point that software can handle it but that's highly unlikely uh anyway uh still the fact that certain random variables can be approximated by normal random variables is not only important it's getting to fundamental theorems of statistics and probability uh that will be discussed in chapter five so let's say for example let's work it let's work with the binomial random variable when n the the sample size is large the cdf of a binomial random variable at x with parameters n and p and here we're assuming that n is somewhat large uh you should probably say well okay so a rule of thumb is that n times p is greater than or equal to 10 and n times 1 minus p is greater than equal to 10 that's one rule of thumb that they're using um we could probably say that if your p is between uh 0.1 and 0.9 then and a sample size of 40 is probably fine so this is going to be approximately equal to the cdf of a standard normal random variable evaluated at x minus the mean of the random variable which is np because that's the mean of a binomial divided by the standard deviation of a binomial which is n times p times 1 minus p and then in addition to this we do plus 0.5 the plus point five is what's known as a continuity correction uh it accounts for the fact that we are using a continuous random variable to uh approximate a discrete random variable if you didn't do this then you often end up with some numerical inaccuracy with the approximation the approximation is still at some level true but it's just off and you get better you get better approximate computations for say the cdf when you include this uh continuity correction i think the justification for why you add plus 0.5 is you could imagine that you have this uh probably mass histogram and your and your uh and the uh or cdf of the normal curve would be going through it basically at the left endpoints so you'd have something that's looking like this and if you shift everything over to the right yeah i think it's to the right when you add do plus or no it's to the left but when you shift the curve oops the undo button is not working when you shift the curve over a little bit you get like a better pass through of these uh histograms so it's like recentering so that it's centered evenly um on the uh on the uh probability histogram or this uh probably mass function understood as a histogram anyway uh let's let's work let's do an example a manufacturer will reject a batch of widgets if in a sample of 100 randomly selected widgets of the batch uh 15 or more are defective if 12 of the widgets in the batch are defective was the probability of rejecting the batch so the random variable in question is uh we'll call it s and it's following a binomial distribution uh the the parameter n is 100 and the parameter p for the probability of getting a defective widget is 0.12 okay so the approximating normal random variable follows a normal distribution with what is going to be in the mean well it's going to be the sample size times p so that's 12. and then we've got a standard deviation which is going to be the square root of a hundred times 0.12 times 0.88 okay so the standard deviation is about 3.25 so 3.25 this is the distribution of the approximating normal random variable so s's distribution is approximate so s is approximately equal in distribution to x so then when we want so okay reject the batch when is the batch rejected the batch is rejected when uh 15 or more widgets are defective so we're looking at the probability that s is greater than or equal to 15 and this is going to be uh 1 minus the cdf of this random variable at um uh hold on uh yeah at 14 uh and it's got parameters 100 and 0.2 and point 12. okay and this is according to our normal approximation approximately equal to uh one minus phi and we've got 14 minus 12 divided by 3.25 and then we add in the continuity correction 0.5 and what is this going to be equal to well we've got 1 minus p norm so we've got uh so 14 minus 12 plus 0.5 divided by 3.25 so 0.2209 and just for reference we could have alternatively computed p by nom and we would have used 14 size equals 100 prob equals 0.12 and we would have said lower dot tail equals false yeah so that's pretty close to what we uh got using the normal approximation okay all right so uh scrolling down oh did i oh something's different uh on the other hand is the r code wrong okay it looks like i might have made a mistake here so the probably that s is greater than or equal to 15 is probably that is one minus probably that s is less than or is strictly less than 15 which is one minus the probability that s is less than or equal to 14. so i think that my r code in these in this yeah i think that i did not put in yeah or hmm curious i don't know i i'm thinking actually that this might be wrong on a second look okay but you get the point at the very least and by the way i would suggest using the normal pro approximation at the very last step so right when you're about to compute something and that you don't know how to compute so like for example um i guess i should probably write down what steps i've kind of been emitting i can say this is the probability that s is no this is 1 minus the probability that s is less than 15 which is 1 minus the probability that s is less than or equal to 14 so here i um basically was still treating s as if it were a discrete random variable you should still do that uh you should like if i was treating this as a continuous random variable i would not be caring so much about whether i was working with less than or less than or equal to but you should you should still treat your random variable that you're approximating with a normal random variable as if it's discrete up until the final point when you need to compute something like the cdf okay so that's what i recommend all right uh the approximation works for poisson random variables as well when uh this lambda parameter is large or i think it was i don't know why i wrote lambda here i think that might be because the book's using lambda i'm not really sure why because of what i remember is that in chapter 3 i was using mu to write down poisson random variables okay but whatever um so in this case the approximating distribution for a poisson random variable uh let's suppose that um x follows a poisson distribution uh with mean parameter mu then we could approximate it with y which is following a normal distribution with mean mu and standard deviation square square root of mu because the standard deviation of a poisson random variable is square root is the square root of mu okay uh so suppose that x follows a poisson distribution with parameter 100 let's estimate the probability that x is less than or equal to 110 so uh the probability that x is less than or equal to 110 is approximately equal to fee at 110 minus 100 plus 0.05 that's the continuity correction divided by uh the square root of 100 which is 10. so this is going to be this is going to be the cdf at 10.5 divided by 10 which is 10.5 minus 10. i don't know divided by 10. oh yeah that's a 1.05 so that's going to be 0.8531 okay uh let's compare that to uh to what we would have had uh if we used the poisson distribution directly so 110 and uh lambda is equal to 100. oh very close very very close so a good approximation all right okay that's that concludes this section this is a very important section very very important because the normal distribution is a distribution that is appearing all over the place so and not just in this chapter but in later chapters too uh chapter five chapter seven chapter eight you're going to be using the normal distribution all the time it's going to be assumed that random variables are normally distributed so you need to get comfortable with this distribution all right so work on problems for this make sure you understand it if you don't understand it fix that and yeah it's it's your responsibility to fix it part of the way you fix it is by asking me what you don't understand right so um like part of how you fix not understanding something is getting help when you need it right from from whoever could possibly help you but you need to fix what you don't understand please please learn the normal distribution inside and out and you will be rewarded for it all right so that's it for uh this uh this section and i will see you in the next section when we talk about exponential random variables which we've already talked about quite a bit and also the gamma distribution which is an interesting distribution often shows up in applications and also in a more theoretical setting okay uh right so see you then