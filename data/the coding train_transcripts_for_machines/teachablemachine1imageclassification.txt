[WOODEN TRAIN WHISTLE] Hello, and welcome to a very special episode of the Coding Train. Today, I am so excited to demonstrate something to you. I am going to show you a project from Google Creative Lab called Teachable Machine. Now, you might say, Haha! I have heard of this Teachable Machine before, in fact you referenced it in some other videos you've made, that are floating over here right now. And in fact, I have, and in fact, I've talked a lot about the kind of thing that Teachable Machine does called Transfer Learning. But this video, I'm going to show you something that just launched today! Which is Teachable Machine 2.0. And Teachable Machine 2.0 allows you to train a machine learning model in the browser, you can train it on images, you can trade it on sounds, you can trade it on poses! And more to come in the future. And then you can save the model you trained and use it in your own project. So I'm going to make a few examples, images and sounds, train a model, demonstrate that, and then bring the train model into my P5 JS sketch with the ML5 JS library, and have some fun making a goofy project and giving you some starter code for you to make your own project. Let's get started by training an image classification model. And what do I need to create an image classification model? Props! I've brought a lot of props with me. I think this will make it fun. I should point out that a bunch of these things have green in them, and you can't see the color green, because I'm standing in front of a green screen. But if I show you the green screen, you can see the color green. To get started from Select a Project, I'm going to choose Image Project, because I want to start with images. Here, there are three steps, and I'm going to go through all of them, but the first step here is the data collection step. So what do I want? I want to collect a whole bunch of images of something, and provide a label for those images. Teachable Machine is going to assume you're at least going to have two kinds of images, otherwise why are you classifying images if you don't at least have two different kinds to distinguish between? And it's going to create automatic labels, like Class 1, or Classification 1. But it's much more fun to pick our own names, so I'm going to click here and switch this to unicorn. And we'll give it some images of this unicorn. So I'm gonna click Add Samples, Webcam, and get my unicorn placed nicely in front of the camera. So there we go, I've given it 413 example images of a unicorn. One thing I would like to mention in my excitement and enthusiasm for demonstrating this project, I collected a lot of images 400 plus for some of the categories and that's really quite unnecessary. It makes the training time take quite a bit longer, and with this transfer learning process that I've described, I could probably get the same exact performance and accuracy with many fewer images. So if you're following along, collect fewer images. The training will happen faster, it will be easier, then you could try it again with more images, with less images, just to experiment and see what works and what doesn't. But probably for getting started, I might start with, say, 25 to 50 images per category. Next of course is a train whistle. We'll call Class 2 "Train". Add samples. Now I definitely want to move on and show the ukulele and the rainbow as well, but let's at least see we've done two, let's see if it works. The next step is for me to click Train Model and actually train the model. So what happens when I press train the model? What is a model? What's the training process? What's going on? There is so much to say here. And let me give you a brief high level overview. Teachable Machine is using a technique called transfer learning. I've actually made several videos all about transfer learning with ML5 JS, and if you want, you could go back and look at those for a bit more of a deeper dive in how transfer learning works. But the short explanation is, there is a pretrained model. Somebody else already trained a model on many, many images, and this model is called MobileNet. And there's a paper written about it that you could read. And the training data for this model is called ImageNet, and there's a paper about that that you could read. And you could do a lot of research into what is this base what is this foundation model upon which we're going to add our images? MobileNet is a model that runs fast, works in the browser, and knows about 1,000 different kinds of things. But it doesn't know about unicorns or train whistles or rainbows, necessarily. Maybe it's in there, maybe it's not. But what we could do is take the fact that it's learned how to boil the essence of images down into a bunch of different numbers, to then retrain it with our own images. And this allows us to get something that works with just 400 images of unicorns and 400 images of train whistles. Because if you were training an image classification model from scratch without this base model, you probably would need a much larger data set. So now that I've given you that brief explanation of how it works, we can actually press Train Model and watch it go. So one thing to note is, it's telling me "don't switch tabs". Don't switch tabs! The reason why it tells you not to switch tabs and this is very important is it's training the model in here, in the browser. None of your images went anywhere. They have not been uploaded to any server, everything is happening right here locally in the browser. The model is training in the browser. Now, Teachable Machine gives you options to save your data, you could upload it to I think there's an integration with Google Drive, and once the model is trained, I am going to upload that model to a server. But this is one of the wonderful things about working with JavaScript in machine learning, is you can do it all right here from your local computer. You have to be connected to the internet to access the Teachable Machine website, but otherwise the actual data processing and training is happening right there in the browser. It's finished! So now I am [TRAIN WHISTLE] most definitely a train. Unicorn! Amazing. So this is a lot of fun, and we could keep going and add more classes and more training images, and I'm going to do that in a second. But the point of why I'm showing this to you is you can now take this model see that Export Model button? You can take that and bring it open to your own project. So I want to do that, but before I do that, let me just add two more classes, and give it some more training images. Turn this off, and let's add Rainbow. By the way, this is my daughter's drawing of a rainbow. Let's add one more, a ukulele. I've been struggling to collect images while I'm playing the ukulele, and you might run into this as a problem. So one of the things you can do is, you can click the Settings icon, and you can turn off the Hold to record. And if you do that, I can actually just give me a two second delay, and record for six seconds. So I think that'll be enough to demonstrate it, so I'm gonna hit Save Settings. I'm going to get my camera focused here on the ukulele, and then I am going to do Record six seconds. I have all of my data for one, two, three, four classes categories classifications labels. You can use any of those words. And I'm going to train the model. So here we go. (SINGING) Don't switch the tabs, don't switch the tabs! Don't We're trained and we're ready to test. Here I am, a train. [TRAIN WHISTLE] And here I am, a unicorn! Rainbow. (STRUMS UKELELE) Ukelele! Now that I've trained the model on these four categories, I'm ready to move into my own code, into my P5 ML5 project. But before I do that, I need to make sure I save the model. And to save the model the model is running right here in the browser, but I want to save the model by clicking this Export Model button. Once I press that button, I'm going to see this pop up, and I have a lot of different options. For example, I could export the model to work with TensorFlow, or with Tensorflow Lite, and you could click on the little i to find out more about those things. I couldn't possibly cover all the options right here in this video. But for me, I want to use it with Tensorflow.js. I am going to use it with a library called ML5 JS, but ML5 JS is built on top of Tensorflow JS, and so the compatible model for ML5 and Tensorflow JS is the same model. So I could locally download it again, this is important, if you really if you don't want anything related to your data, this is not uploading your data, it's [? uploading ?] the model trained on your data. It's a subtle difference, and a complex one to fully unpack. But if you don't want to upload anything, you could just locally download your model. It's going to be easier for me to work with it to upload it, so I'm going to choose Upload. You might actually see a slightly different URL, if the Google changes the way it's stored on their server, but whatever is here is the URL that you want. After you're done, if you have uploaded your model, you then get a permanent web page where you can play around and test your model still, even after the training process is done. So this is a really good tool for debugging your model, because I'm going to start coding with it, and if it starts acting weird, I could come back to this and sort of test it out right here in the browser. I am finally ready to start coding. So there's a couple different ways I could get started. You might remember actually, when I first uploaded the model, there were some code snippets there. There was native JavaScript and TensorFlow JS code. If you're an experienced JavaScript developer, that might be where you want to get started from. There was also actually a P5 and ML5 code snippet, that I could just go and copy paste right here into where I am, which is the P5 web editor. But I'm going to do it a little differently here to try to step you through the process. I'm starting with a template that I've created, that all it does is one thing. It captures video from the camera and displays it on the canvas. And I'm going to add all the bits of code to load the model, give the video to the model, get the result back, and display that back on the canvas as well. Now, if you've never programmed before, never in JavaScript, never in P5, you can still follow along with me, but I'll also refer you to some beginner tutorials that I have, that'll give you the fundamentals and the basics, which will help you understand all the bits and pieces that are going on in this video. But in terms of what's happening so far, we have a single variable for video, we connect to the capture device the webcam in Setup, and in the draw function right here, we draw the video on the canvas, and that's what I'm seeing right there. I should also mention that I recommend that you start with my template because it has the ML5 library imported along with the P5 library. Just in case you're not working in the P5 web editor from my template, you can import the ML5 library by following these Quickstart instructions on the ML5 JS web page. And you can see that my example has that script tag in its index of HTML file. But back to the code. Let's follow and fill in these steps. Step 1, load the model. The easiest way for me to do this with P5 is to write a new function called preload. And preload will load any important assets, images, data files, models, before the program starts in Setup. And I want to create another variable, I'll call it classifier, and in preload, I'll set the classifier equal to ML5.imageClassifer, and then the URL of my model. I'm going to copy it from the training page, and paste it right in here in the ML5 imageClassifier function. Let's run the sketch to make sure we don't have any errors. Looks good. You might see something like a 404 error 404 meaning File Not Found, if for some reason there's a mistake or error with your model URL. Next step, start classifying. All right, I'm going to want to have a function called classifyVideo. That function doesn't exist, so I've gotta write it, classifyVideo. And what I want to do in that function is call the ML5 classified function on the video. Am I just saying the same words over and over again? I kind of am. So how does that work? My variable that stores the model now is called classifier. So I can say classifier the function in ML5 to classify an image is called "classify". And then the first argument to the function, what I need to hand to it, is the image that I want to classify, which is the video. And then, now is sort of the important part. JavaScript, if you've worked with JavaScript before, handles things asynchronously and by that, I mean it takes some time for it to classify the image in the video, and it's going to report an event back to me when it's finished. And you need to handle that by giving it the name of a function that's going to get called as a callback. And sure, there are 10 other ways to do this JavaScript, but this is the way I'm going to demonstrate it here. So I'm going to say that that function is going to be called gotResults, and I'm going to add that in here. Let's run this again and see if we get an error. Oop! I did get an error. gotResult is not defined. It's looking for a callback to give me the results, but that function doesn't exist. So guess what? This is now step 3 oh, this should be step 4. Step 3, get the classification. I need to write a function called gotResults. And gotResults can receive one of two things. It can receive an error, like, I don't know what you just gave me, but that was not an image and I don't know how to classify that. Or it can actually receive some results, like, it's that unicorn friend of mine. So the order that those arguments need to go in is "error" first, "results" second. So I'm going to say error, then results. Now, if there is an error, I'm going to want to display that error, and one way to do that is by saying console.error, error. And then I don't want to continue. There was an error, I don't want to continue, I'm going to just say return, which will jump me out of that function. If there wasn't an error, I can do something with the results. Let's just look at them by saying console.log results. I've given myself some more room in the console, and I'm gonna run the sketch just to see what happens. All right, so look, this is what I got back. Object, object, object, object, object. We're done, right? What is in there? Let's take a look. I got four objects back kind of makes sense, right? There were four classes. The first one is a rainbow, and it thinks it's got an 87% confidence that what it sees in this image is a rainbow. Train, 12%, unicorn, very low percentage, ukulele, very low percentage. So what you get back from ML5 once you've passed it an image, is an ordered list of objects, each containing the label and confidence score, and the first one is always going to be the highest confidence, sorted by confidence. So there's a lot that we could do to visualize this and show all the labels and their confidence scores, but what I want to do is just look at the first results label. So I'm going to change this to results, index [0].label and run it again. Am I a rainbow? I am! I am a rainbow, it makes me so happy. But more importantly, what I want to do is display this label in the canvas. I'm going to create a global variable called label, and I'm just going to put some text in it, like waiting, just so there's something in there that I can see before it gets the result. I want to draw that on the canvas. So let's say text size 32, text align center, center. Then let me draw that label in the middle at the bottom of the window, and let's make sure the label the text is colored white so I can see it. Once I see the text, then I can just store that result in the label itself the label variable, that is and run this again, and then we should see we have a rainbow! I'm a train. You might notice, that's not changing. I'm not getting anything new. So we only called classify once. When the program starts, we said classify. Then we pass it the video, then we get the result, we add the result to the canvas, and what's next? Classify again. So this is something of a loop. This is a way of looping, to just call this function, get the result, call it again, get the result, call it again. So I'm going to add classifyVideo right here. I'm going to run this sketch. Unicorn! Ukulele! [TRAIN WHISTLE] It's like its default thing is a train, which is interesting. Like if there's nothing there, it's gotta pick something, and the nothing is the closest to the smallest thing with the least distinguishable features the train whistle. So this is pretty much done. And I could just stop here. But let's have a little bit more fun with this, and let me change it so that whichever one of these objects I show, I see the emoji for it. And in fact, I won't even bother to show the video anymore. In draw, let's first create a variable called emoji. And let's just give it something, like I'll put it in an emoji of a train. And then let's actually draw that same way, using the text function. Put it in the center, and say text size 256. Just guessing. And let's run this sketch. OK, great. We've got a nice big train emoji there. Now we can just check, what's the label? If label equals rainbow, emoji equals rainbow. We can fill in the rest with else/if statements. Let's throw caution to the wind and not show the video. And now, unicorn! Rainbow! (STRUMS UKELELE) And with nothing, we're back to the [TRAIN WHISTLE] train. Thank you so much for watching this video, I hope that you've enjoyed it. There's a lot more to say about this, I mean number one, what could you do next? First of all, try doing the same thing, but train the model on just the background image, when it sees nothing, to show something else. So in this case, I don't want the background to be the train, but I want to train a category that is background. So you could try that. So many other creative possibilities for how you could use this in an interactive create context. If you make something with it, please share it with me, you can write it in the comments or you can go to the page on the codingtrain.com, where you can submit your own project that you made with Teachable Machine P5 JS and ML5 JS. I'm going to come back and do at least one more video and probably a bunch more. The two things that I really would like to show you number one, I want to show you the sound classifier. So how do you do exactly this, but have it recognize the train whistle and the ukulele as different things? I will work on that in a separate video. I also want to show you an important detail here, and if I put the video back for a second, it's hard for you to see this because you're not standing in front of the computer. It looks to me like you're seeing me with the same orientation that I'm standing here right here. There's my hand, here's my hand. However, when I look at it, the image is not mirrored, meaning I look backwards when I see myself on the computer. When we were training the model with Teachable Machine, it actually flipped the image for you. It's not too hard, and in fact, there's an ML5 function that will allow you to flip the image. So I'm going to add that in a second video, which what I wanted to show you how to do is train a model that can be a game controller. So if, for example, what if I train a model where I hold up my right hand, and something moves to the right, and I train a model and hold up my left hand, and then have something move to the left. So I'm going to do that, I'm going to show you the sound classifier, in two separate videos. Make stuff, let me know what you think in the comments, and I look forward to all the wonderful creative rainbow unicorn train ukulele themed things that you make. Maybe you have other favorite objects and emojis use those. Those are mine. Those are mine, though. Goodbye! Thank you for watching. Mwah! [THEME MUSIC PLAYING]