well hello again it is me your friendly neighborhood coding person who makes lots of mistakes and always gets stuck even when said person thinks it's gonna be really simple and just take about 20 minutes and takes 4 hours later and then I haven't had dinner and I get very exhausted and I have to go home and then I feel depressed all weekend because my coding was terrible and I was so much better it could've been so much better come back now you might not be aware of this if you were watching but I live streamed already today for a prime within almost 3 hours so I started around 1030 1045 and I finished around 130 I did take a 20minute break in there because I had an important phone call I don't have any important phone calls as far as I know although where is my phone this is an interesting question came up here to this room and I usually take my phone out of my pocket then I because I have to put this microphone thing in my pocket you have just tuned into a livestream about a person who has lost their phone let's see it could be over here underneath my books and notes I have I took some more notes for today there's a there's an iPad here where I play some sound because this is weird this is a little distressing I did recently use the loo which is a term that I like to use and could I have left in it here hold on a second everybody let's make my phone play some loud noises I'll do it over here I'm gonna do that fine my phone I can't believe this is what I'm doing oh I out on silent so I have to log in to this like this is really what I'm doing right now I don't even know why ok thank goodness for one password and I'm sponsor could be a sponsor allow oh no wait no no just ok alright I'm about to play a very loud noise it won't be loud for you hopefully hopefully be loud for me you know what I think I just left it in my office downstairs I think I just plugged it into my office to charge and then when I came up here I didn't bring it with me that's it I think it's as simple as that we're just gonna check to make sure it's not in the hallway it's playing the sound now I'm gonna check to make sure it's not in the hallway I'm in the hallway just looking around in the hallway I left the door bub she's gonna check in this room here where I was there's no reason I would have taken it out of my pocket in here all right all right I'm feeling pretty confident that I left it I'm also looking like I'm talking to myself which I am I feel pretty confident that I left just looking downstairs which is fine oh oh oh dear okay hello again welcome to your regularly scheduled coding train now I was saying that this morning definitely in this building because it's at the same location that I am okay let me go back to so now this morning I did something which I felt was like very exciting and I need to turn this camera on it is on I think just switch to that camera oh I still have the cold where I talked about testdriven development with unit testing and continuous integration and I'm like saying this was like this big giant grin on my face because I'm so shocked that this is the thing that I actually just did because I I know that many viewers who might be work as their job doing actual software development this is a fundamental and key piece to your workflow many of you who volunteer or on opensource projects also collaborate through and and your projects that you're involved with run tests and run continuous integration processes but it is not something that I've had a lot of experience with in my be just made like some pretty rainbow colors on the screen twirl around and talk about objects and raise kind of world that living so I was excited to look at this workflow number one is I would like to use this channel as a way of helping people to contribute to open source projects so understanding something about unit tests and continuous integration is important as a key piece of a lot of projects and also I would like to start incorporating it into the community projects that I open up with this channel so one community project that I am working on is a simple and I'm calling it now a toy neural network library and I don't have anything open but if I open up atom it should open up to where I last left off and as you might recall part of that toy library is a toy just put the words boy in front of everything cuz just make it sound like I didn't I don't have to worry about that I've done it like incorrectly and it's like a problem my toy matrix library is this first attempt to start working with some tests so if you didn't watch this morning one of the things that I did is I added instead of just matrix such as I didn't matrix not test touch X this has a unit test which runs let's say the add function and make sure the correct output comes out and then I put all this on github where it now lives if I open up my browser here and I look for toy neural network and so here's the thing I there's been a bunch of pull requests here let's see hmm I think I want to go through the pull request later because what I want to do is work on the gradient descent problem which is the left this is why I don't know there's never a good time right I sort of felt like I need to find that day where I didn't have a busy week and I had all morning to read about gradient descent and sip some tea and relax do some deep breathing come up here and we're just like work through it and go have a nice relaxing weekend today wasn't that day I was like streaming I was having meetings and it won't go there but anyway as well give it a try so I anticipate that's going very poorly so I lowered the key to life is lowered expectations lower your expectations but I do have a at least even if I don't do a good job with maybe a tutorial about gradient descent and back propagation that at least get it working and that I can move forward and start to make some projects with the library itself and then also as I've mentioned before start using another library called deep learn is which is an open source deep learning library put out by some researchers at Google so I don't want to be stuck here anymore I wanna be stuck at the gradient descent step so we'll see can I instead of gradient descent this is gradient I'm gonna climb the gradient descent he'll get to the top roll down momentum is a thing right the gradient descent thing with momentum that's the thing that's the thing that I won't be able to explain in a way that makes sense either because this is gonna go very poorly okay so oh I didn't did I tweet no I did tweet so III have a feeling there's like fewer people watching because I think I used up all of my like livestream notifications on YouTube so anyway so that's what I plan to do I do want to look through these unit tests I mean these pull requests but I think those of you thank you let me just quickly thank make 1999 em dat sev and TR Nord Musa because lik Idon't tour sorry that I didn't pronounce any of those names correctly github user names I would say if you wouldn't mind I'm happy to for these to review these pull requests I wouldn't if you wouldn't mind a minute work on the code some more and push up new code so that might you know just to check your pull request again if there's any conflict space with some of the new stuff that I'm adding because if I go through and check everything to merge now I can go to lose time I want to get to the grade set I do want to ask a question though because I didn't notice here that this particular merge request moves all of the source files into a source folder and then the test file into a test folder and my understanding was that we just adjust the way that jest is structured is actually is to get away from this having to maintain totally duplicate directory structures and actually keep your tests with your code you can use directories but you're supposed to use like underscore underscore test in the directory where your sources so I'm not sure about this but if anybody like feels like they actually know what they're doing with jest and test different development want to like write a comment on this pull request give me some thoughts about that and please let me know all right so I'm gonna put the put the github away let me go to localhost there's nothing there probably I'm gonna go to terminal and where am i I'm gonna go to the desktop and and this is now github repository which was good I'm gonna run a local server so I will not be doing any by the way somebody said to be the testdriven development means you write the tests first well yeah first of all it's I mean I should be so happy that I'm using tests of the first place so I guess I have to get to a whole other level of actually writing the tests first but I'm you know in creative coding which is at all CODIS there's a little bit of a silly term and problematic term but I like to use it in a sentence and just sort of like quickly reference the kind of stuff that I do with graphics and animation and interaction but I do think there is an aspect of not knowing what you're making while you're making it so maybe you can't really do the tests until after you've figured out what you're making maybe okay like you hear that I have a cold it's great my backpropagation videos forever having a cold okay now what's happened here matrix is not to find huh barrel to load resource am I actually what is this nonsense what's going on here okay fine can't remember I'm reading the server okay let's just minimize this so I have this weird thing I don't know why why do I have this again because I had to figure out where I last left off oh yeah this is me trying to actually implement it and this is actually where I last left off after like these are the output errors and these are the hidden errors okay okay so this is what I want to be working with I'm going to temporarily oh yeah personalize it next redo gradient descent video about Delta wait formulas okay implement gradient descent in library talk about different activation functions X or M nest okay there's no way I'm getting through all this today but I'm still not using criteria but yeah so this is where I am did I already have this D sigmoid in there and already seen oh oh that's a neural network okay so I think what I want to do is just for clarity sake I'm gonna grep take this file out and put it somewhere else like on the desktop okay so now there is okay now and what's actually in sketch touch is just running that training function okay so no errors some boardings I don't care about and their network library and the training function okay okay now simple recommendation control a delete okay all right now let me go over here so I'm very sad to erase this but I think it's time I have completed the set of test driven development videos I did not make the one about having a bot tweet whenever your tests passed I would like to have that happen I will come back to that there was so much I didn't anticipate I really thought I was just making one or two like a two or three little short videos I think it turned into like four I'm gonna erase this with this eraser already I've been up here for a half an hour getting ready let's clean this a little bit with some water okay okay I think I'm about ready to get started I have some resources to point out to you about about this topic one of which is ml for a is it just let's just Google that ml for a I want the ml for a github I oh this is a website put together by Jean Cogan who if you're interested in machine learning knows a lot more about this than I do and under here there are instructional guides so many things Cara central openFrameworks demos classes code what I'm looking for is the recent one that was just published about how neural networks are trained ok so this is a resource I was just looking at this like a half an hour ago spending really good to get through all of it but this is a good supplemental resource that will go deeper than what I'm gonna do I would be remiss if I didn't mention the three blue one Brown videos and I'm gonna go to by the way I've just decided in this I had now have a YouTube red account for a Google account but don't you could email there I'll never see it I've never checked that I just like made a Google account just for being logged in while I'm in a live stream if I need to do stuff I mean I signed up for YouTube red because I know the ads don't show but I don't actually watch YouTube on this so it's very weird that I have this anyway that's just a little little behindthescenes tidbit for you where is the playlists essence a little absence calculus neural networks okay on to a reference this particular playlist and then there is the then there is the what's the other thing ah make your own so amazon.com slash shop slash the coding train there is the make your own neural network book that I will mention okay I've got a lot more books I want to add to this page so incidentally if you do I just mentioned it if you do your start your shopping on Amazon here it will support the channel so thank you for doing that for those of you who or buy stuff on Amazon and are able to okay let's see alright so alright I have to like Center myself so where I last left off if I recall was just working on the hidden errors and the output errors so looking at those and I don't think I think I can't do this anymore I think I'm gonna have to do the these steps cuz I'm gonna want to use some of this stuff so that's fine I have to figure out how to do the bias the nature of code offset is the nature of code on Amazon in fact it is I'll just go back to here and there is a link to here he actually says 999 which is I don't know why it says that that's for the Kindle you can get the Kindle book version for free if you just go to my website you're gonna get it on Amazon but um there's a print version I don't know why maybe I just i'd probably did that incorrectly where who my logged in ass hello I'm not logged in I need to fix that probably so that it links to the paperback version 65 customer reviews Wow okay hold sorry sorry okay oh no I needed that page because I'm gonna reference that book although I have a physical copy of that book but it's open to a certain page and I don't want to alright so I have some notes made some notes meeting notes I have this book actually put it under my pillow at night and I hope it's but this is the book that I found very helpful oh yeah and I should also reference Suraj who makes lots of wonderful videos about AI and machine learnings as well okay so these are the tools though these are the resources I've been using for like the vert the step by step building all the code for the neural network thing oh the book was over the mic oh maybe that's what I could use I'm trying to figure out there's like a thing that people do if you're a person who speaks with a microphone you can kind of like simulate a funny yelling thing and then move the microphone away and it sounds like you're shouting it from far away but five you shout it's right here so I guess I could do this alright all right so now what I need YouTube Schiffman mathematics of gradient descent dingdingdingdingding this is really a problem I think it's though just because I googled the exact title of my video most people probably don't Google mathematics ingredients then but something is really wrong with the world if this comes up it was a disaster what if I just do gradient descent yeah Oh somebody call YouTube and tell them their algorithm I mean this is good this is good good good good good good here you go we've got some solid educational content here this is a problem wooof where what's happened there we go it looks Suraj's here so this this I also would highly recommend Oh looks like formulas who you can hover over it and the video starts to like play was this made with the broke before broken arm or after broken arm I can't can't figure out it's mathematics curse seven months ago no that's gotta be I don't know who knows yeah it's got to be pre broken arm because I wasn't making any videos for anyway 20 minutes this stream a couple hours will you change the to the dark side of you there's a dark side of YouTube or you just mean my food my feet how do I do that I would love to anyway I don't know you're distracting me it's 420 and I got to get out of here by like 6 o'clock good you know I think you know anytime I'm livestreaming more than five hours in one given day it's nothing good is going to happen I did take a break but I'm we're on to like hour for here basically alright we're gonna go to this video and let's try to find a little like spot where I just end and have the formulas on the screen I'm very red look how red I am in there do I want the whiteboard image to finish with yeah let's do this this makes it look like I might have done something that made sense okay all right click your purple human icon then click dark theme woah dark theme [Laughter] doc I don't know it's I I like the dark theme for myself but I feel like it looks weird in my it's so dark there's no Rainbow theme oh it's an experience ideal for night saves eyes okay okay but I'm not gonna be on this page for very long all right um I have to start this video all right I'm so not prepared for this I'm so in the wrong screen all right let us begin you're now watching somebody pace in order to avoid having to start talking about this all right okay hello everyone here we are again hope deep breath I'm gonna talk in this video I'm trying to get this to the next step of gradient descent and I to be honest what I would I hope by the time that you're finished watching this video I'll have implemented gradient descent and I'll be working in this JavaScript neural network library that we are all building together as a happy internet family and I think what I'm gonna do is see my plan here is to just get loud in the hallway let me just start over please I some reason what I always need like the first video mmhmm okay like just kind of I should probably watch the end of my last backpropagation video but all right all right all right everybody here we go hello all right this is a good moment for me I think I'm excited to see if I can get through this video because if I can't implement this last piece of the train function in the neural network library then I'll have a working version of some kind of neural network library like thing that I can start to finally apply to some projects and it's my goal actually that in some of the future videos I make that make use of the library and eventually other more sophisticated machine learning deep learning libraries like deep learning is a new library which i think is now gonna be called ml5 which i'll talk about in another video that I'll be able really well I'm sad about it to make stuff and show you how to make stuff but I'm kind of working through this cuz it's just it's just something I have to do so I started I have to finish I'm just trying to like vamp till you stop move on and watch something else because I'm not sure you should continue so but let's let me try to actually been awhile since I recorded the previous video unfortunately might be watching them one after another let me try to like reset kind of where I think that we are so we have a something like a two layer network it has you know an input quote/unquote airboat layer it has the so these are the inputs this is the hidden and this is the output it is fully connected meaning every input is connected to every hidden and every output is connected to every every hidden is connected every input every output is connected every hidden and so now puts come out here the inputs come out here all of these connections have a weight so and we can consider them in a weight matrix and I guess I should put this stuff back in here wait if this is like input one two three this is hidden one this is weight one two one this is weight two one this so these all end up in a nice weight matrix these end up in weight matrix the outputs come out oh boy if it array like y1 y2 a vector the inputs come in in a vector x1 x2 x3 okay I'm getting back into this I didn't even draw like off the top wonderful so now the idea is we want to do training and the kind of training that I'm doing right now is called supervised learning where I have some known output I've some known inputs with known outputs inputs with target outputs I send the inputs in I feed them forward I get some actual guests output back and I have some sort of error which we can think of as the error equals the guess I'm the guess the target so I'm gonna say like why target okay so this is the error now I should have mentioned before that there are a variety of ways to train a neural network to the idea of training is adjusting the weights to get results that are more close to the actual I would say that again and by training I mean adjusting all of these weights like their knobs to try to actually get the matched target output when you send in the training data so one thing I should mention I really got to like even just take a minutes and make a video just about this but in most training situations you'll have training data test data and then of course there's the actual unknown data so we want to say we want to say like oh here's some labeled data I'm going to use that to train but I need to save some of that labeled data because what if I trained my neural network only to work with the training date but it doesn't actually work with any other data well I can determine that by giving it some saved some data that I didn't train it with but I know the answer to see how it dumps with that and that's how we can evaluate it so I just want to mention that this is gonna be the process now this idea of back propagation with gradient descent is one technique and it's a technique I'll put some links in this video's description you read about the history of it it's it's been around for a long time it's a big innovation in training neural networks but there is a lot of questions as to whether that's optimal best for the future etc there's different you know select weeks of these algorithms and I actually next probably after I get through this plan to use a genetic algorithm to evolve the weights of a neural network which opens up the door for a lot of kinds of projects that I excited to try to make so all that aside here we are what did we get so far we figured out in the previous video we calculated this error and then we calculated the hidden error which I'll just call each error right that's part of back propagation it's while we have this error how do we distribute this error all around so we can adjust all these weights the reason why we're doing this is because we actually did this already twice I've done this twice once with if you look at my videos about a single perceptron right which gets two inputs I forgot about the bias like I always I always remember the bias I'm biased against the bias I'll come back to that but the single perceptron which just takes in two inputs and one output well it doesn't say it's a single neuron you can take in more than two inputs but a single neuron with multiple inputs and an output I actually did this we did this idea of training this with gradient descent and we also did it I did it we I did it in a video about linear regression where if I have a bunch of points in a twodimensional space I can find the line that fits these points the best and I did this what's interesting here as though this app so for a linear Russian if you recall there's actually a mathematical formula to just compute the exact line of best fit called ordinary least squares I think I went through that in the previous as well but the idea here and so the idea is we're trying to figure out y equals MX plus B the formula for this line well basically what this is doing is this is exactly the same process that we needed here only we have these weights you know you can almost think of this as like M and this is B maybe or something if there's just one input and a bias we had to fit we had to fit all the stuff coming through here into a line well this is actually what we need to do with all of these all of these places right we basically need to do the exact same algorithm that we did for this one line to compute this is the weight and this is the Bice what we really have right is that y equals MX plus B we have y equals m1 X 1 plus M 2 X 2 plus M 3 can you see that X 3 plus M 4 X 4 but added up plus B we have this we basically have exactly the same problem but in a multidimensional space so I just need to figure out how do I adjust each one of these M one and two and three and four and B all of these weights inside of the matrix so the same training method I did for a linear regression gradient descent the same thing we did with the perceptron we now need to apply it here in this multidimensional space so what should I do next well first of all I forgot to thank a bunch of the resources errors usually target minus y I know why is that but I thought yeah that's what I thought its target will why but but I thought then last time people told me it was Y minus target all right fine hold on I some viewers rightly pointed out that when I did this previously and I guess the convention is target why the nice thing about this is when we look at what a cost function is if you look at a cost function for a machine learning system if you look at a if you look at a cost function for a machine learning system you'll see that the cost function is the sum of all the errors squared so if you do target wire y target you square it doesn't really matter but it is an important distinction probably you have to get it right of course otherwise you might start training in the wrong direction as you'll start to see as we do other stuff okay let me come over here and let me think so I'm gonna come back to this video in a second but let me first say there are three things one is this is a new resource that just came out it's not a new resource but a new page on the ml for a ml for a github do site and this is a site put together by Jean Cogan as a ton of machine learning resources videos examples demos etc it's amazing and there is a nice article here about how neural networks are trained with a lot more detail than I'm gonna get into here but you can see the same sort of idea of talking about linear regression a loss function adding more dimensions this is the idea this is what we're doing of course I highly recommend you watch the three blue one Brown series about rating descent back propagation and back propagation calculus this will give you a massive extreme set background for what I've grown attempt to do it's just kind of like let me just tape this in code like kind of squint the press the button hope it works so this is great I highly recommend this and then I also have been using the make your own neural network book which I could hold up and wave around for you by Terry Gross she'd and there's a link to it on the coding train Amazon shop along with some other books that I've been using for videos and then so this is where I what I wanted to do now is try to connect back to here this is from my previous video entitled the mathematics of gradient descent where I go through a long algorithm to arrive at a very simple result and that simple result is the following time out for a second I'm just sorry I want to pull up the code for that where would that be is it in rainbow code coding challenges gradient a linear regression where did I put the code for a linear regression with gradient descent oh it's with my intelligence and learning class where is that silly repository intelligence there it is mmm classification regression linear regression interactive regression live mmm this with gradient descent or with yeah Oh what I'm doing it like yeah okay target all right okay yeah there's also the perceptron but I mean I did the perceptron in processing which is probably mistake now they think about it because all this whole series is in JavaScript but okay so so all of this math boiled down to a simple formula which is in this case of y equals MX plus B what I'm looking to do is calculate the change in M and the change in B and so we can now write those formulas over here I'm gonna keep this oh I'm in the run it's fine we can now write those formulas over here so in the case of y equals MX plus B we need to calculate Delta M how do we want em to change based on the error and it how we want em to change is the learning rate by the way learning right if you look in textbooks and stuff and sometimes written with this um the reek letter alpha I believe is that how fun yeah learning rate times x times error and Delta B equals learning rate times error this is all done through some calculus in looking at the cost function looking at the derivative of the cost function the slope and how to walk around that cost function and find the bottom the minimum error how what values of M and B do we have to have the minimum error and that's what we want to do here what values of w w1 1 W 2 1 doubly 300 400 I have to have the minimum error and that error each individual error we've got to like back it up and pass around and chop it up okay so how do we take this and move this to a multidimensional scenario so let me timeout here for a second every time i time i timed out timed out have done I can't do it anymore let me look at my notes here yes I started doing like the partial derivative stuff I think what I want to just do is try to write out the formula first of all did I get those formulas correct this x times error times learning rate x times error times learning rate okay error times learning rate Eric Taylor okay great so what I want to do is okay hit into output not input to hidden hidden to output okay I'm sorry I'm okay do I have enough space on the board I'm gonna rewrite these this doesn't have to be just be an edit point here just say here I'm gonna rewrite these a little smaller further over to the left so I have more room to write out the formulas for the matrix version what'd I say it was learning rate times x times error Delta B equals learning rate times error okay so these are the formulas for the change in slope and change in by offset change and haven't change in B for y equals MX plus B so I have the same situation except I have a slightly different one I have like the output equals Sigma of like that whole you know weight matrix was it matrix product with the I guess it's with the inputs right I have this kind before but it's basically the same thing plus the bias plus the bias which is a vector so I have like kind of like basically the same thing but instead of like single dimension these are all matrices Multi multi dimensional so what I'm going to attempt to do now is I'm going to just write out a notation for these formulas using matrices and try to like compare craft a little bit and I'm not sure I get it right because i'm kinda i'm using a bunk combination of trying to keep consistent with my notation from before and some conventions but let's see I've got it written down here let's look at this so let's first just say I just want to figure out Delta for these weights so in other words you can think of what I'm doing is instead of just Delta n I want all of these weights so I want to say Delta all of the weights and the weights are from hidden to output the change in each weight IJ each row I column J I think that's what I've been using for me to remember what I used in the last video equals and it's gonna look very similar first of all learning rate same I always have a learning rate learning rates gonna basically tune like how big of a step are we gonna take and I don't like the way I kind of want to rewrite this just because to have it match the way I'm gonna write the matrix version error times X I'm gonna write this times the vector e right that's that's the that's the vector of output 1 minus target one output to our target one minus output one right that error vector that I've talked about along and then X in this case is kind of like the input the input to this neuron to each of this output layer so I'm gonna call that a in this form gonna call that like I could call it X what I'm call it H it's what's coming out of the hidden layer I'm gonna put that here and put that at the end H so these are the components that exactly match up here at times H right we have same way of learning rate we have the error right and by the way if I were doing this for this layer if I would say Delta W eights from I to J from input to hidden ih would equal do learning rate and then I would say times the hidden error right you remember that from the previous videos where I went through how to get the output error and the hidden error how we pass all that around and then this is times the input so this is the same exact formula but two different layers learning rate earths or hidden errors the hidden output or the input output sort of way to say they hidden output the hidden output is the input to the output the input is the input to the hidden so you can sort of see how this is this formula is looking at this part and this formula is looking at this part okay but I didn't put one other thing in here there's something funny that's gonna go in this spot right here I'm gonna pause for a second cuz I'm trying to see if nobody has complained yet rows as I and columns as J did ya Roberts Wayne says dancin using roses I and columns with J which i think is probably incorrect but that is how I've been doing it so is that do you recall is that how I did it in the previous videos I hate to have that inconsistency but I think that's how I've been doing you have the code in front of you okay great so in four okay so what goes here where here's here's the thing something that is quite different about the linear regression with gradient descent that I looked at before and the way these neural networks work is that there's this activation function and we want to kind of understand well when I just the wait what does that do does it push it which way in terms of the activation function and so the activation function being if I can only find a place to write this being sigmoid what we actually need to include here is the derivative of sigmoid right we need to look at what the change in what's a good way of saying this I'm trying to like sort of intuitively explain why the derivative of sigmoid makes it in right here maybe somebody will have a suggestion for that okay the chat to see if anybody has like a really nice way and then I need to find a derivative I'm trying to like not derive the formulas basically because I read through a bunch of different tutorials that do that I just think it's I'm never going to do a good job on this channel so I just want to sort of like put the farmers out there and implement them in code so I want to yeah is the slope of the gradient thank you yeah it's really the gradient of the slope of the gradient where I guess I'll just look at this page here No why am I looking for a page I'm just gonna write this out yeah so I so people are saying like oh you have to explain the chain rule and partial driven I did actually go through sponge of that stuff in previous videos and I don't want to just get into rehashing it again here I just feel like and I maybe I would come back again once I get further along to try to do some more derivation stuff but I feel like there are better resources out there that do this if we can just kind of agree on a notation and kind of understand the pieces of it and get implementing I think that's best so yeah cuz I'm just not gonna make like someday I will be a different person and make those tutorials by the way this morning I didn't have a sponsor for the coding train circle CI but we're back again according drink sponsored by water usually calculate the gradient and the deltas separately Delta M is the derivative of Y so you need the derivative of sigmoid yes yes yes thank you that's a good way of thinking about it all right so what goes here well if you recall what well a lot of what I used in this previous video where I went through the mathematics of gradient descent was this idea of a derivative the slope well the derivative you have to remember that Delta Y weight was such a nice little Delta and that's what I meant to say so the chain so so that change in a given weight you need the derivative of Y and we've got we get the output from the sigmoid so do we need this idea of the gradient we need to side you have the gradient which is the slope of the sigmoid function so we need the derivative of a sigmoid function okay like succinctly say it maybe you can plot the derivative sigmoid to show what it's doing that's a good idea but meaning is useful for understand a I would just say this succinctly in a way that makes sense the derivative sigmoid indicates it's confidence it can descend or ascend to confidence based on the learning right oh that's what I like that Medan gives me a great I know I I just don't even want to be in the frame right now leave it over there well I read the chat I like that's a nice way of saying it all right because ultimately this is exactly the same so far but in this case because it was linear the derivative of Y is 1 because it was linear and here I've got the sigmoid function so I need the derivative is that kind of it because in this case it really have like sigmoid around this we need a derivative of that which is yeah I think that's makes sense you need to show how W is connected to the error yes yes I showed that previously but writing it down is a good idea yes yes ok thank you ok the derivative of Y is n yeah yeah yeah but I've got that it's not M but uh yes yes well the derivative of Y with respect to the partial derivative with respect to M is X which is actually what's going on here I'm in the wrong stream all right but all right I have confused myself again I thought I had it first again so what goes here again these formulas are the same so far I have the learning rate I have the error this is one dimension but I have a single learning rate at the error which is a vector I have the input X and the stuff coming out of hidden is the input to the output so I have that here so what goes here well one thing you might remember if you looked at the mathematics of gradient descent is the derivative is kind of important and if I take the derivative here the derivative of Y its M with respect to X but the derivative of M sorry hold on I think what I want to say is I take the derivative of Y with respect to M right this says it's like did this pull for odd partial derivatives I just gonna end up with X which is kind of what's going on here but there's the sigmoid function here there's no sigmoid function here so what I really need is something else here that helps me I need to add in the derivative the sigmoid function and guess what we are so lucky so here's the thing sigmoid I should have mentioned it before sigmoid function is not a commonly used activation function anymore or it's not that sort of known to be an optimal one there's a zoo and for the French for rectified linear unit which and there's also tan H lots of other activation functions and I'll come back to some of those especially as we get into some more realworld examples but sort of in the classical historical sense I'm gonna stick with sigmoid and the nice thing about sigmoid is it's so easy to differentiate because sigmoid of X the derivative which you sometimes can write it's like s that little line thing tick what do you call that again of X the derivative of sigmoid equals why can I never remember this I think it's just sigmoid of X times 1 minus Sigma of X and I went off the board there but let me check to make sure that's right a prime prime it's called prime the tickets called prime yes hold on I'm gonna rewrite that let me just make sure I got that right sigmoid I'm over here by the way that's what I wrote right hold on let me try this again since I wrote off the board and also I was told this is called prime what I like to call tick it's called prime so we read this again the derivative of sigmoid sigmoid prime x equals asks the sigmoid function times 1 minus the sigmoid function I'm just checking so this is actually really easy and this is what goes right here so we need to we need to put in I have this here now what what is this by the way the sigmoid function this is the thing I've already calculated that's what's coming out of here no no wait wait wait sorry the sigmoid no no I want the sigmoid of the output which is here sorry the wat Y sorry the sigmoid function is something we've already calculated that is what's coming out of here so what I simply need is to multiply this by the output times 1 minus the output so this is the final formula what I need to do is I need to adjust each weight according to we can like look at the components of this I definitely need to adjust it according to the learning rate because that's just a thing I'm gonna say big steps little steps I need to adjust it according to the error and then this the derivative the sigmoid function the slope of the gradient kind of gives me some I think somebody in the chat used the word confidence right how what do I do what how how does how does sigmoid change if I move this direction or that direction and then according to the mathematics of gradient descent I need to also multiply by what was coming in so I think now and down here what this is going to be is it's the output of the hidden function so the hidden function times 1 minus hidden so I spent a very long time kind of just writing these formulas here and I and I'm also gonna have to maybe do some matrix transposition move stuff around so that it works but so let me actually just pause and end this video right here and in the next video what I'm going to do is go back and examine my matrix library go back and look at my neural network code and see if I can take these these formulas and get them working in the code let's see here I don't know what did I why did I mess up that was pretty poor oh you need to multiply this hey let me just pause yeah of course I should be doing this in Python no I should not be there's no reason at any whelmed whatsoever I should be doing what I'm doing oh I was a weak man in the chat wrote this is my final formula but was making a joke and I was like oh you have a final formula I'm waiting a book meeting I was waiting to see it okay all right so I'm gonna just kind of leave that as it was I mean it definitely could get edited together it was pretty awkward but this is not my best work I could always come back and try to do it again but that's why I want to create these into two different videos I just want to see this anybody have any real problems with this notation before I move on because it's possible that I would kind of try to do this explanation over again cuz I was most add videos me like reminding myself where I last left off which is probably unnecessary but I just want to get anybody have any comments on this notation before I move on oh yeah the bodies I forgot about the bias yeah I gotta put the bias in there I'll come back and do that uh what time is it five o'clock yeah target is the error yeah the cost is target minus y squared the sum of all that the eight should go to e H equals each time you need okay well not oh yeah so I need a parenthesis here let's hold on maybe I better like hold on let me let me collect things that I'm missing so that one missing parentheses bias I'm writing these hopefully where you couldn't see them but you can yes that should be a Hadamard products in there but where so this is the thing right isn't it this needs to these are the Hadamard products right i just wanna understand this greatly like well let's take the learning rate out of this this is a single vector this is a single vector so those are the Hadamard products and then this is like the transposed outputs this this has to be transposed that way I can get a matrix to change all the weights right I think that's right right matrix multiplication is wrong the eight should go to e don't worry Austin I'm locked locked lost well can I speak before II all right that's what we not talk about the derivative of lady cuz that's not that's not relevant to this video right now don't you have a book I do have a book let's look at the formulas in the book let's look at the notation eight should go before e where eight should go before e I have no idea where you're talking about oops oh this can run off alright so let's look here let me look at the formula in this book learning rate times e sub K which in my notation is J times Sigma which is of the output which is this times 1 minus Sigma or the output which is this times the output transposed the the output is the learning rate the sigmoid is the squashing activation function we saw before that times is multiplication as the normal element by element so that's the that's what I was trying to this is like I was trying to use an asterisk for element wise multiplication and then I was I guess I can use the dot for like the matrix multiplication this is slightly different though than what I and then what's written here how about random numbers output gradients equals output errors times output derivatives hidden errors equals output weights transpose product and Delta H before E except after C output error ALPA graves equals output errors times output derivatives hidden errors so I've got the up of gradients equals the errors times the derivatives right yes that's a nice way of thinking about it the errors times the derivatives the hidden errors times the derivatives final outputs minus final output errors times final outputs times 1 minus final outputs yep dot product with transpose the hidden outputs yeah this is right this is exactly the same and then here I like crowd look they just said I like how somebody said output gradients equals output errors times output derivatives so the gradient is the error times the derivative this is the error this is the derivative and then we have to adjust it in the same way we do here with the inputs and the learning rate so the gradient here is just the error is just the error is that right maybe I should come back to I think I want to slowly go back to where I didn't have anything here yet this this a way of thinking of the gradient right and the gradient here is the errors times the out the output errors times the output derivative then Delta is multiplied by the learning rate multiplied by the end deltas multiplied by the learning rate multiplied by the input yeah okay okay but is it right to say what I would like to know is it right to say that the gradient here is just the error whereas here because the derivative of is this is just a linear function the slope is continuous not continuous but it is the same always and here the derivative the gradient is the error times the derivative of the output column vector a multiplied by Rho vector V gives made sure the arrows by B columns yeah yeah okay H and I need to be transposed yeah these definitely need to be transposed all right so let me let me go back and try this again I hope you're saying I want you to be my teacher I hope you're talking about somebody else not me at this point all right Mattia please I hope we can make something of this because I don't know if I can bear to try this again but I'm gonna go back to where I didn't have anything here yet and these whoops were kind of a bit of more of a mess and I didn't talk about the transposing and yeah in the air back propagation yes okay the book make sure yeah really should really just read the book make your own neural network all right okay okay let's let me try this again okay Oh this wouldn't have been here and this also wasn't clearly this wasn't clear what this was so what goes here this is the question now this is where we're so happy to have this book make your up a neural network because we can just look up the formula in here but what's actually is gonna go here is the derivative of the output now let's think about this why don't I come in with your videos you might have remembered back from the mathematics of gradient descent that we had to take the derivative but in this case of Y in this case it's MX plus B it's very simple derivatives the linear function here we have this sigmoid thing we need to calculate the gradient and the gradient is the errors time the error of the output times the derivative of the output what happened well has the output change has the output change relative to the errors so in this case right we need to add in here the derivative of sinh Y now here's the wonderful thing sigmoid is the function right the derivative of sigmoid s prime X is simply equal to the sigmoid of x times 1 minus can you see what I'm writing is sigmoid of X so in this case we need to calculate this gradient the derivative of sigmoid and right here now one thing I should really clarify here is so learning rate is a scalar number error is a vector learning rate is a scalar number I'm gonna put an asterisk here err a hidden error is a vector so I need to multiply and the output is a vector so these are actually this is element wise multiplication of the out know I've already what's coming out of here out of output has already had the sigmoid pass through it so I just need to say the output plus 1 minus the output the output plus 1 minus the output now what's weird about this is now the H is really this right we have an exact same formula I have the the learning rate the error gradient here you could sort of consider this the gradient here is just the error the gradient here is the error times the derivative of the output and then if I'm multiplying I need to get a weight matrix so the input by the way is also a vector but I need to get a matrix so the interesting that happens if I use the matrix product here and transpose this vector right this is element wise multiplication this is just a scalar now here's the thing if I have the sorry if I have this error and the gradient essentially at a called gradient as a single column vector four columns if I multiply that by a the hidden output which is also a single column vector but transposed what am I going to get I'm actually going to get a four by four matrix pause for a second think about that right because I take the row and do the dot product I take the row into the dot product take the road to the dot product with the road to the dot product I see like lots of whoops did I write this in the wrong order when I'm doing when I'm doing matrix multiplication I need to have the same number of rows as columns right so the for this should have rows for this is one column this is one row this is four rows four columns it's right yeah yeah so I already did by the way kids Miller wondering I already did the whole error calculation stuff in the previous two videos like a few weeks ago so I'm just trying to do the deltas here and so I just want to make sure so what is this let me just as let me just take a minute to I just like don't have space on the whiteboard it looks only one I can't right it's looking right over here for a second right so what am I gonna get here first thing I'm going to do is do G 1 H 1 right and then here it's gonna be it's funny I can't do this matrix math all of a sudden okay first of all I can't do it because I'm afraid and people are watching me let me let's look let's go to here matrix multiplication not XYZ so can I make my own matrix there we go right let's look at this yeah I love it so helpful reset okay okay if you take a single column matrix and multiply it by a single row matrix as long as it has this has the same number of columns as this has rows let's let's look at what happens right there's a wonderful website that I use often when I get stuck it's called matrix multiplication XYZ and here what I can do is I can make I'm gonna make this exact right here's an arbitrary single column vector and then I'm gonna make a single row vector so a one column matrix a one row matrix I'm gonna hit multiply so this is I'm just gonna go to the end here we can this is a nice little animation that goes through everything that I did in previous matrix multiplications and you can see we end up with a four by four matrix so this is exactly what we need to do to be able to get the deltas for all the weights okay so this also has to be if I'm going here this has to be transposed and this should be element wise multiplication with the the I lost track here so I'm taking the output error and multiply by the derivative of the output here I'm taking the hidden error and multiplying it by the derivative of the hidden so that would be H plus h plus one so not plus O what I have plus here have had that wrong for so long times that's another element wise multiplication times one and this is a one not an I 1 minus H okay I think we've done it I'm gonna just check here my trusty guide but I highly recommend that you read instead of listing reads that are listening to me and I'm gonna look at this the change in weight matrix equals learning rate times the error times sigmoid of the output well I have sigmoid kind of builtin here I'm assuming the output of the sigmoid has already been done times 1 minus Sigma or 2 the output I'm sigmund I've been done with the dot product of the matrix multiplication of the hidden the output transpose well this is right my notation is slightly different okay so I think we're just about right now of course I forgot about the bias so I'm gonna have to basically do exactly the same thing with the bias bias is always simpler because I can just get rid of this so I'm assuming I could probably sort of do the same thing here so but I'll get to that in a bit let me at least try to implement this in the code and then I'll do that in the next video and then we'll come back to the bias okay I say that wrong I probably said that wrong I need to have the same number of rows as columns right so confusing it's confusing as I anyway right because if I take this out oh no that works seem to have columns as rows so these can looks as I said that wrong right like in my in my matrix library when I do multiply aids columns have to equal bees rows oh yeah I said that wrong chutes I said that wrong this is all that one column has to equal one row a number of rows and the number of columns can be other things because you're only gonna have a four by four darn I feel like I finally kind of got all this in some way that made sense but I kind of messed up that we'll see if how terrible that is okay alright let's just let's move on and let me try to implement the code okay suppress shift miss here trying to move on and start to implement this code so what I need to do is okay so what I've done successfully somewhat successfully in the code so far is compute the errors the output errors and the hidden errors and I've added I have a transpose function I have a matrix multi multiple I function so what I need to do is compute the deltas the gradients I need to figure out how what I need to do to change all these weights I should say that I did say something I think wrong in the previous video so I might as well at least say it here which is that in with matrix multiplication and this shouldn't be an asterisk here I don't know you know this you can think of the dot product or just an X for matrix multiplication I need to have be a this is matrix a and this is matrix B a the number of columns in a has to be the same as the number of rows in B so this could have as long as there's one column this could have three rows and this could have eight columns but this is one dimensional this way one dimensional this way that's gonna that's going to always give us in the end the correct dimensions for the weight matrix so I kind of Miss stated that in the previous video I don't know if you were worried about that I don't know if correcting it by now but that's enough so let's try to put this stuff in here now one thing that I think I really need to do unfortunately is that I had this lovely idea previously of like Oh what I need to do first to get the error right is I need to feed the inputs in get the output and compare it to the target right that's going to give me the errors and then I can compute the hidden error by doing the weighted percentage stuff that I did previously the issue is once they start wanting to do all this stuff it would be nice if I could remember all the parts that happen during the feedforward process so as much as I just wanted to call feedforward here I think it's actually gonna work better if I run the feedforward stuff here so I'm gonna actually grab all of this copy and I'm going to paste it right here so there's definitely some redundancy in the code this is gonna help me figure it out so I need to feed everything forward inputs hidden which then gets the bias passes through sigmoid so it's hidden this hidden matrix is left in the state of the values coming out of here good then I have the output be adding them the bias and the output is left in the state of it coming out now in the state of it coming out of here now Bri viously I had taken the outputs from the root from the feedforward function I don't want to do that anymore in fact I guess I'm being consistent I should call these outputs so now I have the outputs plus I have like all the other stuff that happened before if case I need to reuse it and I have the targets and I can have the output errors so now I need the gradients so what do I need for the gradient I'm gonna call this let gradient equal outputs okay so how am I gonna do this how you gotta do them it's like oh I don't have numb by this is where you really need dump I which is a Python library for doing matrix calculations so let me think about this I need to I need to take those outputs which have already been passed through sigmoid and multiply it times what I need to do but this won't work right I need to calculate this gradient as those outputs the derivative outputs times 1 minus outputs so I have to do this with my matrix library so one thing I just realizing here is I don't actually have element wise multiplication the multiply function multiplies two matrices with matrix multiplication the multiply function oh I do I do no no I do I do it the multiply function so the static multiply whew only multiplies is this good though the nonstatic can take a scalar or another matrix and this by the way this right here this is referred to as the hot think product which is elementwise oh boy so I do actually oddly have this here is this going to be useful so let's see where am I again I need I want to go back to train so what I need to do but first I need to do is map the outputs yeah I'm gonna need I'm gonna need a static version of that I don't I don't like what I've done here so whatever I do is I'm gonna go to my matrix library and I'm gonna have this function only be for the scalar product then I'm going to make a static function called Hadamard which is for element wise and that can be essentially this right here so I am going to I should have had this before I got into this video but I don't so I'm going to just do it in this video and I'm going to say a dot rose a dot columns B so a and I need to say I need to make a result is a new matrix a dot a dot rose B columns and the result each element of the result in this row column position is the the element wise multiplication and I should probably add a check here to make sure the number of rows and columns were the same but that can happen another time I'm going to say return results now remember I have this map function ah oh I'm so silly there's an easy way for me to do this Oh leave that there though I mean I sell my own there's an easy way for me to do this which is that right remember I have this like sigmoid function where did I put the sigmoid function is it so just in here sigmoid I am going to write a function called D sigmoid now here's the thing this isn't really the derivative of sigmoid because that would involve calculating sigmoid but this is I've already done sigmoid maybe I know what to call this exactly but I'll put Y in here to be more clear so I'm going to say Y times 1 minus y so what's a good work I don't know what a good function call this is but I'm just gonna call it a name because it's been called D Sigma because Sigma oi this output array is already this is what I need to do I could just say yeah I don't need to save it for any reason I can just say outputs map D sigmoid this what this is doing is it's saying out this is outputs equals outputs times 1 minus outputs all right so that is this piece right here now I need to element wise multiply it by the errors that's why I should have left it the way it was I regret all the changes I recently made weight cuz I don't need to save it once I'm done with that it doesn't get used anywhere else right am i right about that I think I'm right about that so let me go back let me undo a bunch of these things I'm gonna go back and now okay does anybody have a better idea of what would sort of more appropriately oh my goodness call this function here there is there are a lot of numpy like libraries or JavaScript I'm just trying not to use one I'm gonna replace what I'm doing eventually with one so let me get to the point where Here I am I'm gonna go back to where I said I'm in train I'm gonna go back to where I was here sigmoid prime maybe Optimus a boy okay I'm gonna go back to where I was here okay so I need to somehow get this gradient which is this piece right here the nice thing is I can actually use functionality that I have built into the matrix library so for example I have that map function so I can take every element of output and set it equal to output it that element times one minus itself so what I need is another function right much like I have sigmoid what I need is the derivative of sigmoid now this there's a little bit something strange that's gonna go on here let me just write this if I write a function called d sigmoid what I really mean is return sigmoid of X times 1 minus Sigma of X this technically speaking is the derivative of sigmoid but that's actually not what I want to do here because if you know if you're following along and where am I here in train I've already mapped the output through sigmoid so actually what I want is and I kind of like okay what's one called like fake dig see sigmoid but I'm just gonna put Y in here I'm gonna comment this out and kind of as if Y is I'm just changing the very ammonium Y has already been sigmoid and I'm gonna say return Y times 1 minus y so what I can do now to calculate this gradients is I can basically say outputs I kind of want to call it gradient but right outputs dot map and maybe I should make a copy of it or something d sigmoid all right so now I've taken outputs and I've set each element equal this now I need to elementwise multiply that by the error so I need to say outputs now here's the thing in my matrix library this is like I have this right here the multiply function currently if it gets a matrix it does what's what I was for the element wise multiplication which I'm referring to as the Hadamard product so otherwise so I guess what I mean I'm gonna keep this and I'm gonna say outputs dot multiply now remember when I use matrix top multiply that's the that's the multiply that's matrix multiplication this is Hadamard I should write a separate function called Hadamard yeah let's take this out and make this Hadamard alright let's just leave it I'll refactor that later what do i do what do I do I'll just leave it I'll just leave it it's fine multiply by output errors there we go so now I've done this piece and this piece now I need to multiply it by the learning rate do I even have I all I've been waiting my whole life just to get to the point where I could put the learning rate in the code because I feel like once you have the learning rate in the code I've kind of done so let's make that a variable this dot learning rate I don't know I'm just gonna set equal like point one right now and so now I also need to say outputs dot x learning rate and now what I need to do I have all of these these and now what I need to do I've done this whole piece here all I need to do is take what came out of hit and transpose it and do the matrix the matrix prana matrix multiplication between that matrix and this matrix and then I have all the delta weights and I can just adjust them you know I've got to talk about stochastic etc but let's just let me let me just we try to get through what I'm doing here all right I now I'm going to say what am I looking for I'm looking for this particular array right this particular vector I need to get let hidden T which is hidden transpose is matrix transpose hidden and then let deltas wait wait what am i calling these like the this dot weights I H so I'm going to say weights H Oh deltas equals matrix dot multiplied so this is calculus I'm going to put a comment in here calculate gradient and then calculate deltas calculate deltas matrix top multiply what do I want to multiply I got to do this in the right order I want the column vector which is the the gradients thing that I've been doing and the row vector which is the output so hit okay so I'm going to say multiply outputs so I hate that I've done this does the map function what I want to have is a you know what I want is I want a static version of the map function that will pull out make it new so let me do that really quickly so where's the map function map I made a static I made a static version already how exciting Oh life is good sometimes well that's so lucky so I want to do this let gradients equal the matrix dot map this solves all my problems outputs D sigmoid no I don't need alright so I want to map all the outputs with Sigma the derivative of sigmoid then multiplied by output errors multiplied by learning rate transpose the hidden output and then matrix multiply the gradients by the hidden output transposed and now wait H of this this dot wait H Oh add wait H Oh deltas so this is me just like taking going into just going out to saying I calculated those deltas change all the weights by those deltas so I don't know if this is this right let's think about this more later I need the bias well that's fine I'm gonna do the vice all right so that's good now I have to deal with the hidden layer this should be much easier now that I've done this once okay I need to calculate the hidden gradient which is matrix dot map the hiddens what came out of hidden pass through d d sigmoid then i need to take the hidden gradient and what did i do up here i multiplied by the output errors but have I calculated the hidden errors I have calculated the hidden errors somewhere I did back propagation hidden errors right there ah how lucky lucky me hidden gradient x hidden errors then hidden gradients x learning rate and that's this dot learning rate and did I forget that up here yeah this this dot learning rate right and then so this is calculate hidden gradient now oh my god calculate hidden Delta's alright you know that it's input to hidden input to hidden I'll just input to hidden deltas okay so that is just like I did up here the first thing I need to do is transpose input inputs T equals inputs dot transpose I know matrix dot transpose inputs matrix dot transpose inputs okay and then what did I call those deltas I called these weight hidden output deltas so let wait input hidden Delta 's equal matrix multiply the inputs no no nothing inputs the gradients times the transpose inputs so you hidden gradient times the what I call that inputs t transposed inputs and then I can just adjust those my goodness I have just less PII I'm speechless I without speech okay I think I might be done with this for the bias I'm gonna save that for I'm gonna add the bias in a separate video because I feel like I just need a break so let's think about this I might have made some mistakes but I think that I've gotten through this I think what I've done is I have figured out a way to use the train function to calculate the errors use back propagation to chop up and divide the error in a sine blame all over the place right I know the output errors I need to figure out the hidden layers errors then that's what I've got here those were the first two videos in this the third year I kind of talked through these formulas and now in this video I have used a math function to calculate the gradient the errors times the derivative of the output I adding the learning rate in I'm multiplying it by what's coming in transposed to get the weight deltas I've done that for both this layer this matrix and this matrix again at some point I really need to extract in you with this library to make it something useful I need to be able to have multiple hidden layers and this back propagation would happen in a loop but I'm doing this to separate distinct chunks just to understand them and then I'm gonna adjust all the weights so there's things that I haven't talked about yet number one is I've got to adjust the bias values and number two is when and where should I be doing this should I run through all of my training data and get like the kind of average error of everything and then adjust all the weights or should I each time adjust the weights for each record or should i do batches like send in these ten data points adjust send in these ten adjust and that has to do with stochastic gradient descent versus a batch gradient descent so I'm gonna get there let me take a break take a few deep breaths and make a separate video where I adjust the bias ease I might have made some something absolutely completely wrong so you you have to watch all the way through to find out if I have other things that I have to correct later which I may but I'm gonna change the biases and then I'm finally going to do I'm just gonna try to train it on the X or to see if I've done this correctly XOR would be a really simple problem so it's a good test problem for me to see if my code is mostly working correctly oh stop I have made but before I go there is one error that I can point out here this is input to hidden thank you okay see you in the next video what time is it 540 huh alright so the bias so here's the thing the the the make your neural network book doesn't include the bias and the other the ml for a tutorial that I read today okay I'll get this you know what I'm gonna leave that and I'll put it in the next video you don't have to add that into the end of this video the other the other I'm sorry I didn't change the camera the ml for a tutorial read just sort of says like oh it's easier to do this if you just make the bias as another weight and have it always be one which I've talked about in previous videos but wouldn't I if if I'm using the same logic where I'm extrapolating from here to these two if I extrapolate from here to these two don't I just is it just this without these two things is that all I need to adjust the bias somebody tell me like can't because certainly I don't need a matric the bias is a single column vector so can't I just take exactly this without without the matrix product of the transposed inputs and that the deltas for the bias somebody tell me if that's right that's my intuition I didn't run the code didn't run the code first I'll do that the beginning of the next video k week Minh says yes and I'm just going to assume that you're saying yes to the question I asked yes there 543 p.m. one minute ago must be yet okay great so that's gonna make my life easy because I can do that in fact I already have that in here right in theory that's this right I don't even need to I can just adjust the bias ease by the gradient by the great because it's multiplied by the like if this whole thing is the if this whole thing is the gradient that's the deltas for the biases right can you please type another yes all right so the things I need to fix are I did it actually I have to fix this and then I have to run the code to see what other syntax errors I have so all I need is one more yes I could move on now I'm waiting for Kate week Mons yes okay wheat wine is not typing usually I see like a quick Mon is typing by the way sometimes I'm speaking in I watch myself a lot at 2x I don't watch was up a lot 2x but if I have to go back and watch something I'll watch it at 2x so I always feel like I'm like my actual native personalities to talk like this was it he said yes to my previous question but I guess I'm gonna I'm gonna just go with it I think confidence thank you all right that is correct move on alright okay I'm here I am back again oh I'm so close to the end of this now I'm sure there's a lot of mistakes I mean so I made a really an error here which is I didn't even try to run my code and there was a big there's a big typo here already which is that this should be the is this and this might actually even be weights like I think I might not be like being careful about my right these these that's weights plural so that's totally wrong weights plural and this is not this is input to hidden and this is also input to hidden Delta's so that I should have said and I wonder if I made that mistake here like this stock weights that should be weights I got this right so let's try let's try running the code you know I have no idea put your good put odds on whether there's any errors I would put very things I give myself like 50 to one that there's no errors so let's go and just because if you recall I have in the sketch like I'm setting myself up for a very simple scenario of two inputs two inputs two hidden nodes and two outputs lying 99 typos so I should guess I should check there might even be a typo in line 99 well that's a comment so I don't know what that is alright alright let's just let's just hit refresh here identify our inputs has already been declared neural network digest line 59 so let's see what that is oh I forgot so I don't oh yeah oh this interesting so this should probably be called input array like I did with feedforward and in that sense this should probably be target array because and then targets equals matrix from array target array so let's just do that so that's important because I'm letting the enduser pass in the inputs and targets as simple arrays and internally in the library I'm converting those two matrix objects targets is not defined a neural network line 71 oh and this so this has to be let targets because it's a new variable here there's no errors okay weird all right so let's move on now this is really tricky I probably could have been much more thoughtful about how I'm doing this I'm but here's the thing I now need to add the Delta BOTS the deltas for the biases now if you've been following along here's how I've here's how I've connected all the stuff I made a video a long while back Oh yesteryears of days when it was just a onedimensional y equals MX plus B I made a video about gradient descent where Delta M is the learning rate times error times X and Delta B is just the learning rate times error well this is the analogous situation with matrices learning rate times error this gradient so to speak is this whole thing over here and the X is this thing over here and the same thing here this is the Delta the Delta I mean sorry the gradient which is this times the inputs which is X so the the deltas for the biases is actually not a matrix right the bias is a one column vector basically it won't make one column matrix which is a vector and so the Delta bias sees is actually just this part this part and this part and guess what I have already calculated those things so if you look at that here oops if I go back to my code right this is before where where is it right here where am i odd gradients right right here I am passing the outputs through the derivative I am multiplying elementwise with the errors and the learning rate this and then I have to do the transpose and get but this is it these gradients I could just say bias ease this dot bias and where am I output dot add gradients so let me actually will let me actually put this where did I go let me put this here so this is adjust the weights by deltas and now adjust the bias by its deltas which is just the gradients okay and then I should be able to do exactly the same thing with down here oops this will come after and what I want to do is adjust the hidden bias with the hidden gradient now I one thing that's probably inconsistent maybe I'm just gonna kind of leave it but if anybody as you this is a I'm hoping to put all this code in a repo which is already there she's it called like toy in neural network and so one thing I probably should be consistent is when I'm saying like weights versus weight or gradient versus gradients so that can be cleaned up later let's just see by goodness if I have any errors okay no errors sorry I check the chat to see if I have any errors no errors all right dare I do something next okay so here's the thing I mentioned before now how you collect and prepare your data is so important in terms of the ethics of what you're doing the scientific accuracy of what you're doing and I'm kind of glossing all over that just to make this toy neural network library but beyond just sort of like being thoughtful about collecting your data we've got to figure out like how do I even like do this and so there are a variety of techniques in terms of calculating the error over time and batches and then adjusting the weights versus but I think what I'm going to do it's called stochastic gradient descent let's Google that but let's just make sure I've got the right term stochastic gradient descent is known as incremental just is a iterative method for minimizing object that's written as a symbol so I think I'm correct in that what I'm doing is in other words what I'm gonna do with stochastic gradient descent which I think is what I did with my perceptron example and my linear regression example is that for every single record every single data point I'm gonna pass it in calculate the error back propagated and adjust one at a time instead of doing batches that you're but but be aware of this idea of batches because that's a core concept as you start to use other people's like real actual robust working deep learning libraries and examples and that sort of thing so I'm going to do this stochastic idea which then one of the reasons why I want to do that I know why switched over that one of the reasons why I want to do this stochastic idea is it's basically already what I've done so this training function simply takes a single set of inputs and a single set of outputs targets does all that it needs to do to it adjusts everything and finishes off so so let's do that let's do that I don't think let's let's try oh my goodness ah so let's let's prepare data set I'm gonna do this okay so I like I'm like terrified to erase this if I have to let me take a moment and erase this do my okay so this is my data set I'm gonna have this truck this is the this is the architecture I'm going to use I'm gonna have two inputs a hidden layer with two neurons an output layer with one and I wiII try that again I don't need to write so high on the board my elbow barely even goes straight okay so here's the architecture I'm going to use I'm gonna have an input input layer I'm just gonna say layer 2 inputs I'm gonna have a hidden layer with 2 2 nodes then I'm going to have an output with just one this is a nice architecture for trying to solve the XOR problem exclusive or I just want the simplest thing just to kind into some way of kind of debugging and validating that's something in my code is doing it correctly so what I'm going to do is so this it's going to look like this and so for my data set width is going to be this following 1 comma 0 gives me a 1 0 comma 1 gives me a 1 1 comma 1 gives me a 0 and 0 comma 0 gives me a 0 so this is the classic non linearly separable problem and I discussed with the perceptron a single perceptron can't do it this is now a multilayered perceptron with graty's assented back propagation in that code so I should be able to continuously feed it this training data sent now I said you need a training data set and a test data set but this is like such a simplistic problem there's only 2 4 possibilities and we interesting to look at you know visualizing it and letting these be continuous floating point values but let's look let me just do it up so I'm gonna put into my code the training data so I'm gonna say I'm gonna say let training data equal and have it be an array and each element of the array is gonna be an object inputs 0 0 targets 1 so I'm kind of I could put this in a JSON file or a spreadsheet but I'm gonna just do it like this hardcoded in just to make the point right and so now I have own ok so 0 1 1 0 0 0 is 0 0 0 is 1 1 is 0 ok so oh dear oh this should be a ah silly syntax error that I then copy/paste everywhere ok so this is now my training data it is an array with objects so now what I need to do is I am going to say 2 2 1 that's my neural network and I'm going to say let's see what do I need to do for data of training data that's a nice little loop through everything that's in here I'm gonna say neural network trained data inputs data targets and I probably I'm just gonna you know I could pick it randomly I'm gonna go through the data and I'm gonna suit it like I'm gonna do it like a hundred times in the same order which is probably a problem I should probably randomize the order let's just do it this way and see what's going on so let's do that and then I'm going to test things by saying guess equals neural network feedforward zero zero and actually you know I'm just going to do I'm going to say neural network fee for at zero zero print so this should give me everything in the console that I want so this is I haven't been to a mic being careful enough this is the inputs with the target this is the inputs with the target does the inputs of target doesn't miss a target I'm just going to train it with that like 100 times in the exact same order which is probably a terrible idea with stochastic gradient descent and then I'm just going to call v4 and I think I still have in my matrix library this print function which just like console dot tables the stuff out all right I don't know Oh what could possibly go wrong what could possibly go wrong feedforward print is not a function all right neural network oh you know what it gives me a nice little array I forgot I don't need to do this I could just consult I forgot that the library itself gives me a nice little array so let's just console.log I don't need that print thing so let's try this oh that doesn't look very good right I should be getting one one zero zero like we're close to it let's train it like so let's try training it like 10,000 times hey this is maybe interestingly sort of better one zero zero one one one zero zero so I'm definitely feeding in the all the proper inputs is my training data correct zero one gives 1 1 0 is 1 0 0 gives zero one one good so I think I really need to randomize order right I've really got to randomize the order so let's randomize the order and in fact what I'm gonna do forget about even randomizing the order I'm just gonna always pick a random one so I'm just gonna say let data and P five as a nice function if I just give it random training data it's gonna give me a random one the learning rate is something I actually should really be careful about there's put in point 1 so that's probably something I need to be more thoughtful about so let me so let me do it 50,000 times and let's see what happens in a random order so interestingly enough so I want to do this as a coding challenge I want to actually write an example that sort of like animates and visualizes it as it's learning I'm saying I have can't believe this work oh I can't believe I arrived here I mean I'm sure there's like problems and it's but at least it worked for this simple problem and but so somebody said I had a typo in the train function well I'm sure I do don't I'm so happy right now though so I need to do coding challenge which is actually the XOR problem and also animated as I'm going so I think I essentially do the same thing but draw sort of like a pixel space and actually iterate over it and I should see it you know the coroners would be the the full boolean that I'll explain this way do the coding challenge I can't even think anymore just got this to work this is not deep learned is I'm about like 5 I think I'm back propagation XOR I am I think it was like 86 was the back propagation paper history history history history history 86 so I am 86 96 2006 I'm like 32 years behind deep learn Dutch yes okay I can catch up a little faster you know a lot of years of research I mean I didn't do the research I implemented what much smarter people came up with for many years in a short period of time here I'm seeing lots of nice emojis in the chats great so it's 6 o'clock let's commit let's push this the github yeah oh and you know what let's get rid of oh oh that's deleted git add code is in state after last tutorial video after last backprop video and I am going to that's interesting it actually thinks that I renamed this file to that even though what I really did was delete it but anyway git push origin watch this watch this no no no no it's not gonna let me do that git push origin master back prop so I I today if you weren't around I did a I set the repo up with unit testing and continuous integration so you cannot push anything to master you can only push to other branches and my unit tests will run I mean there's basically no unit tests in there yet but I will add more no no no this is the old repo that I'm gonna delete now neural network where am I coding train toy yeah toy 20 role Network compare and pull request create pull request let's it with the base branch how is that even possible update branch it's running my tests okay so we're gonna merge then I'm not sure what happened I'm just to say git pull I'm afraid to do this master oh it's just the readme and the test that's all okay so readme and the test had some changes that was that was all okay yeah so if the get up stuff is confusing to you you can watch my holes this one get up and the new series that will come out next week about unit testing and continuous integration okay okay yes the files from this morning didn't upload so I have to remember to do that okay always the readme okay yeah Cody jumps so I think I think this should be famous last words a nice as much as it's sort of time for me to go at 6 o'clock it's not horrific ly like like it's been and I don't feel I have the energy or time to do a mist as a coding challenge with this library I think that can involve a lot of interesting stuff withdrawing it so I'll get to that next week but I do think I could do the XOR challenge so let me try that I don't I don't think and this would be great but I feel like I'm on like our five or six of streaming today and I think you know generally you know the smart thing to do would be to stop now but if I'm not gonna stop now let me just put the XOR thing together and that also I think before I do em mist it would be worth letting the library go through a bunch of like improvements over the next week maybe people will contribute to it more tests because that would be nice so that's that's kind of what I think so and also so let me refactor the repository a little bit also so so what I want to do let's think about this I want to make a folder called Lib and I want to put the neural network stuff in there then I want to make a folder called X or make up my call it examples and I'll make a folder in there called X or I will upload look this is its own coding challenge but I might as well do it in this repo right now and so sketch and index.html go in there which go in here and then what I'll need to do is make sure couple things one is the example code will need to reference Lib I think this is right so why is it asking me to do this so let's see does that fix it let's whoops so examples X or Lib what did I do wrong dot dot Lib taught Lib no I just have to go up one directory and then live right oh no two directories so I could be more thoughtful about how I do this but let's just do that there we go so this works okay so that's where I'm going to start with and I'm gonna so let me let's uh let's run the tests oh I mean NPM tests so that still is fine yeah all the normalizing of data and all of that I'm gonna have to do at some point okay you know the the I curb joke will never be overplayed I'll be sad when I finally do I turn because then it won't be a joke anymore so let me let me say refactoring directories and I'll merge it all in later okay so now so if you can tolerate this excuse me I am going to well happy livestream for almost six hours today I think I'm gonna start I'm gonna delete all this because the idea here is that this is now my idea for this was this this video could be a video that somebody could watch without having watched any of my other neural network videos they could just come in right here and use the library and understand kind of how it works so I'm gonna react splain some basic stuff and I'm gonna you just use the library and get renamed next time okay okay so so this is what I'm going to attempt to do all right so now let me up some water this is definitely it for today for me I can take next week off I made so much content um yeah so I probably I need to finesse the you know you can get yet yes I need to the randomized selection and the learning rate I but I'm gonna add make a slider for the learning rate I think so okay okay okay so you need this and then I need to go to a neural network well actually let me push this up there factor directories I guess I could make it so it as an admin I could just push it directly now I see why it might be useful but that's right I'm gonna live by my testing mantra now well I'm so excited to go home for this weekend just best okay um some initializations are slower to converge yeah that's what great one emerges okay all right here we go oh so excited for this and to be done oh this my iPad sound effects are about to the battery's gonna die yes I could plug it in I won't worry about that right now all right hello this this video needs may hold hello this video needs a ding a train whistle it eats everything because I have spent a very long time making a lot of video tutorials about writing from scratch all building from scratch of neural networking JavaScript and there are so many resources that I reference but I will probably this is probably the primary one make your own neural network by Tariq Rasheed anyway this is the first coding challenge that I'm gonna do with all that neural network code so you don't have to have watched any of those videos to watch this one I'm going to explain the parts that you need for this video and I'm just gonna make use of this toy neural network JavaScript library so this I've started this toy network job Duke Library people are contributing to it if you want to find out how this library was built up to watch like 10 to 12 short videos but you could also just be here right now so let's talk about I do and to be honest I am mostly making this video to test that library because I just finished the library and I just want to know that it works like I think it should work and so what I need is a simple problem with a very known result that I can apply to the neural network see if I get the correct results and feel like ok the library works I can start to maybe try to apply it to more interesting complex problems and this simple problem that I'm going to apply it to is a problem called XOR so I don't this is really just a pure sort of like mathematical exploration basically to see that the problem works but it will hopefully get at some pieces of things you need so what I need is for the neural network I need a data set the idea of the neural network is that I'm going to have some inputs the inputs are going to go into the neural network and then eventually some output is going to come out so a neural network can be used for a kind of classification problem for example here is an image of the cat please tell me that is a cat here is an image of a dog please tell me it's a dog and I will get to hopefully some videos about image classification and other applications of machine learning and with neural networks in the future but here my input is going to be something incredibly simple and it's going to be two inputs that are either true or false 0 or 1 because I want to solve the XOR problem so what reason why this problem is interesting so we used to be sort of yeah it is to me is that if I look at a truth table meaning I say true true here and false no it's been a very very long day this is like really my brain is completely melted if I look at a truth table here and I say true false true false the end problem for example I only get it true if when if in that this is a grid right would and I only get it true here right both things have to be true to get it true your true and false I still get a false false and false I still get a false false and true I still get a false or I'll get true if at least one is true so I'll get true true true and false here with both of those I can draw a line to separate let's just I was trying to not have to erase anything but I might as well so this is or right this is there was this is what or looks like and I can draw a line right here and I can separate all the truths from the false the reason why the X or problem is kind of interesting is that and I talked about this a bit in some of the other previous videos that you can go look at is that with X for exclusive or it is only true do we only get the true results if I have true false or false true to false is I'm gonna get a false and if both are true I'm also gonna get a false now try to draw one line to divide the truths from falsus you cannot I could draw like a curvy thing or I could let kind of draw two lines right here's all the truths this is not a linearly separable problem in the solution space of this problem it's a simple twodimensional space I cannot draw a line and so this is a kind of problem that you need a more sophisticated system I mean not necessarily you can imagine you really just need to but this is what I'm gonna use the neural network for and you can imagine building a circuit right where you have switches and how could you get the led only to turn on if one switches on and one switches off this is the same problem so you don't have so needed neural network to design that but that's what we're gonna do so the way this is gonna work is the inputs there's gonna be two inputs if I think about that it's like x1 x2 so what are the possible inputs and I'm going to express them as arrays the possible inputs are 1 comma 0 0 comma 1 1 comma 1 and 0 comma 0 can you see all that these are the four possible configurations of true and false true false false true true true false false those are angels those inputs are going to go in something called a hidden layer they're gonna get fed in to the hidden layer and again you can watch many many videos where I go through lots of details about this the inputs are connected this is fully connected every input is connected to every hidden node and each one of these little lines here each one of these connections has a weight which I'll just make as W then I'm going to have an output layer now in this case I only need a single output because what do I want the target output the desired output I want is a 1 here a 1 here a 0 here and a 0 here now why did I continue to put brackets around these why are these all arrays so in neural network systems you can have the system that I'm building here you can have inputs that have that are just there are a list of 5 things the outputs could be five things four things one things so in this case I have two inputs and one output but even if it's just one the system is gonna work is always gonna spit out the libraries aren't gonna spit out an array so these are both connected so if you are curious about the feedforward algorithm what's happening here is called feed for the inputs come in a weighted sum gets all multiplied and added together passed into this hidden layer an activation function activates and sends it out into here with a weighted sum and an activation function activates and sends it out and we get some result and what we're going to do is we're gonna call a function in the neural network library called train we're going to tell the neural network hey this labelled a this is labeled data this data corresponds with this correct output this input data corresponds with this correct output and the neural network internally when I give it the Train method is going to use a process called back propagation and it's going to look at the error it's going to say well if it got point two and the desired output was one that the error is 0.8 if it got point six and the desired output is zero it got an error of negative point six and so it's going to take that error and move it and train the system by twisting all these dials here and the problem with me doing this coding challenge I just spent like six hours building the library basically any way more complex than that but so my brains a little bit fried but hopefully you're saying this is the the setup that I'm going to use so I want to write the code that does this and visualizes and animates the system while I'm doing it okay here I am back alright so here's what I want to do I am now an in a p5 sketch you can find the code both with the neural network coding the neural network library repository and it'll also be with the regular coding train a repo with as a standalone example and so I'm going to add set up and I'm going to add draw and basically what I want to do is I need to have a neural network I'm skews n n is the variable and when I create a new neural network from this library I have to give it its architecture and I need to give it three things I need to say how many inputs are there how many outputs are there and also how many hidden nodes are there and by the way I could have does that you should try to try after this and try it with like six hidden nodes see if it works better works worse all right so it's two two and one so I'm going to say two two and one so the neural network itself with all the stuff is all happening now in that in the library everything is set up in the library now I need some training data so I'm going to make a variable called training data and I'm gonna make that an array I almost you know it I I kind of want to make it a JSON file now let's I'll just do it here because I wanted to make the point that you could be loading your data from somewhere else but I'm gonna make it an array and it's gonna have four objects inputs 0 0 and usually the word target is for the label this is training data so it has a known a known target so that is going to give me a 0 right and so now I need whoops I need to copy this object 1 2 3 times and I'm going to have did I get this right I think it looks right I'm gonna have 0 0 1 0 will give me a 1 0 1 will give me a 1 and 0 0 and 1 1 will give me a 0 so this is the training data now why do I feel like this syntax is wrong but I think it's right ok it's an array of objects with inputs targets inputs targets inputs targets ok what do I need to do now now I need to in the draw loop what I'm going to do I say background 0 let's make a canvas and I make the canvas 400 by 400 now let's just even let's run the code and just see that the canvas is there okay it's there move this over we make this a little bit smaller give me a little bit more room in the console okay okay okay now I'm gonna use the can for something you're saying it now here's the thing what I'm going to do I'm gonna do a technique called stochastic grip I'm gonna train it using a technique called stochastic gradient descent so the great descent algorithm is built into the library I've talked about in other videos but I'm gonna feed it one data point at a time and have a train based on that rather than do it in batches and other videos maybe I'll come back and look at batch gradient descent in that so gradual batch learning so what I'm gonna do is each time through draw I'm gonna say give me a data point a random data point from the training data then I am going to say then I'm gonna say neural network taught train send it the inputs and send it the targets so this is me just saying hey every time through draw pick a data point train pick a data point train pick a data point train take a data point drain pick is a two point train it's just like got lost in my thoughts all right so now here's the thing I can run this now it's running it's training it maybe I could even just like okay look at the neural network there it is it's got it's got all these biases and hidden nodes and inputs nodes and weights and matrices all that stuff is in the library and I could say feedforward which i think is the outlet of the function for just sending it some input data and getting the output like I could ask right now give me the results for zero zero it's not very good that's not right I did probably hasn't trained enough yet give me as well for one zero yeah so it probably needs some more time to train these things could be really really slow one thing that I might do just right now is well maybe I should do this like a hundred times per per cycle to draw this is still adjusting the weights with every data point but at least I'm doing it a hundred times faster now because draw is kind of slow let's see what so wouldn't it be nice oh look it's getting better it's going down to zero right we can see ah there we go let's try let's try neural network let's look at 1 comma 0 I'm getting a number so it's working it but it wouldn't it be nice if I could kind of watch the process of it learning so here's a way I could do that now first of all a separate coding challenge might be interesting to actually visualize a diagram of the neural network somebody in the chat mentioned tensorflow playground which is a tool that you can use to like try out different neural network ideas and visualize them but what I want to do here is I actually just want to visualize this truth table so if I think of the canvas as the truth table 0 1 0 1 I can actually you know there's no reason why I could I don't have to pass it in only these options in the neural network I could this could be a square 4 0 0 this could represent point 250.50 1.75 0 whoops I need another one 1 0 right so I can actually and then I could take that output which is a single number and map it to a grayscale color so what I should see if it mapping if 0 is black and one is white I should see right a big band here in the middle of a dark color with it fading out of a white color fading out to black around the edges where it gets to either 0 0 or 1 1 so let's see if I can make that happen so while it's training in addition to training it I'm going to say let me give me a like a variable called like resolution I'll just say it equals 10 columns equals with with divided by resolution and rows equals height divided by resolution let's say let's loop through all the columns way just by the way in the library used I for rows with the matrix tough but kind of I loop through all the rows and let's just first just to see what I what I'm that I'm drawing something that makes sense I want an x value which is I I times resolution I want a Y value which is J time's resolution and I want to draw a rectangle at XY with a size of resolution and I'm just gonna say random cut fill just right now random 255 and I probably if I'm being careful here I should probably use floor to make sure the number of columns and rows is always an integer cuz lots of weird things could happen okay so let's do this but missing parentheses sketch line 33 I'm staring at this you do not see a missing parentheses sketch line 33 oh my goodness it's been such a long day huh I really don't see a missing parentheses line 31 resolutions that's interesting that's giving me this weird error which is nothing to do with what the actual air is which is that should be resolution but I still have an error height resolution I I less than columns I plus plus what is going on letlet oh my goodness for 400 oh it's been such a long day I should never do these coding challenges it that after streaming for 6 hours okay fourth there we see okay so this is the space are going crazy YouTube compression algorithm probably going crazy so you can see all those random colors now what I want to do is I want to turn this into data so I want to say let input 1 equal map I mean this is sort of silly but I'm gonna do it anyway I actually I'm just gonna say I divided by columns let input 2 equal J divided by columns and now I'm going to say neural network feedforward input 1 input 2 I might want to say columns minus 1 here because I want the last one I is only going I want the last one to be equal to 1.0 know if this and this should be rows okay so I want to do that then I want to say output equals this then I want to say bright brightness I'm going to actually just say I'm gonna say let color equal output index zero it's an array but just one number in it times 255 and then I'm gonna fill here and here we go let's take a look at this let's watch the neural network hmm this doesn't seem right Oh give it some time give it some time whoa flickering flickering fleckeri oh you know what though I don't like these I don't like these I don't like these lines in here something weird is happening let's give it let's make it a higher resolution 40 and let's say no stroke come back to it let's refresh type oh I've got to have a type of somewhere well it's like the wrong something's wrong here four four four oh that's the type of from before I think it's actually working right let's think about it yeah yeah I should be getting true only if I I'm down here or at the top look that worked you can see it sort of thinking about it training it takes a while now what would be interesting is to watch it happen much more slowly let's make it ten times as slow it's really how long is it you think now here's another thing that's interesting a really important piece of neural networks is something called the learning rate now the learning rate at the moment is not really exposed in the library but there is a variable in the neural network library called learning rate and it would be nice to see how the learning the changing the learning rate actually affects the neural network from training training so what I'm going to do is I want to create a slider learning rate slider and I'm going to say learning rate slider equals create slider and I'm gonna give it a range from 0.01 20.1 I don't think higher than that really makes a lot of sense at the moment and I'm gonna let it have a step size of point zero one oh and a default sorry it's going to start at 0.05 and a step size of 0.1 so just if we look at this we can see there's a slider here and what I can do is I can say in draw neural network learning rate this is a variable called LR in the library there should probably be I should probably add something like set learning rate or something but right now I'm just gonna say so when you look at the code after watching this video there might be like a nicer function there but I'm just gonna set it to learning rate slider value so this is what the slider currently is so now in theory we should be able to play with that learning rate something doesn't feel right here point oh six point one yeah that's right and all the way down to the bottom point oh one so playing with this learning rate is an interesting parameter I can make it really high can we rule low that is something that you could really really alter the behavior of neural networks isn't not it is depending on how it is initialized it can really get stuck or it can really like train itself to get to the right answer very very quickly you can see with just one trade data set it's like one data set one data point per frame 30 frames per second you can see you can see that now I could really like maybe make the learning rate really small and it's coming like finesse itself but so these are the kinds of things I don't know maybe you have some more creative ideas let's let's go back to a slightly higher resolution let's look at this let's see if I increase the learning rate to try to get somewhere kind of closer then I can kind of refine with a slower learning rate we can see this is really working so the neural network library as far as I can tell is working this coding challenge is training using the library to visualize a twodimensional space of a truth table to to get the neural network to give us the result of the XOR problem so I don't know some things you might think about doing with this if you want to do your own variation of this is could you build a much nicer interface could you think of a color in a more interesting way what if you changed the way the neural network is structured is it going to be better or worse could you visualize the actual neural network itself and if you really again this video mostly exists because I wanted to test out the first version of that library I'm planning to do a lot more with that library as well as another library which I'll tell you about in a future video which is a simple JavaScript library for machine learning that's built on top of another library called deep learning is I would include links to all those things in this video's description I look forward to hearing your comments and thank you for watching oh yeah change the orientation whoa I'm in front of the slider something seems fishy wait hold on no it's still right this is still right I don't think so oh yeah no no oh you know what it is okay interesting this is very interesting hold on a second I'm back people are asking sometimes we see white on the edges and this black stripe down the middle sometimes if I can get lucky here I can get it to be try again so I the reason this is happening I think I should probably explain this is that there's two solutions to the problem so hold on so I don't know where I guess I have to come back and explain this oh yeah you know what just give me a second this is totally gonna be worth it all right I'm actually back for a second weird edit point they're probably because you'll reskin how you were getting different results each time well there's actually two solutions to the XOR problem and I brought up another example I will publish this code as well this is a processing version of the same thing that I did and actually you could do this as an exercise which is visualizing it in 3d so the corners are true and false and you can see two corners are optimized are being solved all the way down for true false false true and the other two corners true true false false but if you think about it what are you supposed to do in the middle there's no I mean that those there's no this doesn't actually make sense if I'm sending in 0.5 0.5 but if I'm thinking of that sebast solution space I can actually have the center of it kind of Bend upwards if you look at it in 3d or the center of it could it could be flipped it could be actually the complete flipped version of this with the kind of bending down work downwards so you'll see if I run this multiple times every once in a while I'm gonna get a white stripe along this diagonal versus a black stripe along this diagonal but the corners are always going to be white white black black because the corners should always train for those situations of 0 1 1 0 1 1 0 0 correctly so one thing that again just to let's see if I can get let's see if I can make this like really train a lot I'm gonna have to do a thousand cycles per frame and I'll do like a big learning rate and then a low learning rate let's see if I can get oh my sound board went and by the way there also shoot yeah sometimes I get stuck cuz they're like fighting to figure out which is the right solution it should like arbitrarily like there we go alright so hold on so let me try training it like 5,000 times per frame which it sounds crazy but it's really very little math that's going on in this tiny neural network and what you're gonna see hopefully is that you know it it's it's converged on this solution really quickly converged on that solution really quickly and sometimes it's gonna get actually a little bit stuck because I'm not really being very thoughtful about this there's probably a nice way and it really just depends on the neural network itself it starts with all of these weights being random values so you might by accident get like a really good set of weights that are already close to one of those solutions that's way it'll converge on that other solution so so I encourage you to and I can just sort of hit refresh a bunch of times and just sometimes it's going to get kind of stuff to think about how I could do some research of how I might get different solutions let's let's see if I can get this to train for the other way bent around the other way I'm going to run this processing code again really quick and so you can think about one exercise you might do is cat what are some ways that you can visualize this a solution space and what you know is there a way that you can get it to not get stuck etc etc okay so I'm gonna let this run I'm gonna go I'm gonna leave this coding challenge is complete thank you for watching I hope you come up with some creative ideas obviously the morano obviously but I you know this is really just a technical demonstration at this point hopefully we can make the neural network library better and I'm gonna you eventually use some other machine learning libraries like something called ml five which is not released yet or sort of is and I think it's called ml five I hydron is that that um but it's a new library that I'm working on with other folks here at ITP that's built on top of a library called deep learning is which is really amazing lots of really power a powerful robust machine learning deep learning library for the browser that runs on top of WebGL so much more to say about that another video thanks for watching oh oh my goodness the learning rate has been doing nothing this whole time let's just let's just pretend that's not the case no wonder I mean I can't there's nothing I can do about that but it doesn't matter because you know I was like in the end it's really like that's interesting to see like if it now I can actually really use the learning rate yeah it's not getting stuck anymore not like like it's doing get stuck I think the answer is can I get it stuck there we go got it stuck if I increase the learning rate or decrease it for a little while no it's really stuck so that's an interesting thing yourself so I don't think that really matters you know whatever I can redo this coding challenge it was no good I've made so many videos today yeah alright uh thank you everybody I'm done for today I will read some random numbers I don't have any sound I'm exhausted it's 645 I don't even have my phone up here I have some text messages which just came through oh yes if not too late but okay if too late all right so I've got to go I don't even think I have the energy to read random numbers I have to do it though right I hope my phone is downstairs in my office and not lost twenty nine thousand one hundred thirty seven eighteen thousand six hundred eighty eight nice boy really doing this in a perfunctory fashion alright everybody this was a lot of life streaming today I will I'm almost certain I won't be back until next Friday I don't know what time it will be next Friday but there will be another cut and I know I'm hoping next Friday to make improvements to this library and do like M NIST as an example with it okay thank you everyone and I will see you in the future so actually let me just quickly answer question dark go aqua is asking any forum for discussion so there is a slack channel for patrons of the coding train could sign up at page calm / coding train I believe there is a discord that I that I signed up for at one point that's a community discord there's a reddit there's the processing forum which is a great forum for asking questions when I'm processing or p5 and I'm open to people starting up other platforms for the community to engage with each other as well okay okay and I have to stop streaming it's very confusing okay goodbye goodbye I don't know what I'm gonna do next I have to just take a break from all this