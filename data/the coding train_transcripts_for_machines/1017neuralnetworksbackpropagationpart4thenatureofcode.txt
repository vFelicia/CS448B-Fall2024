okay suppress shift miss here trying to move on and start to implement this code so what I need to do is okay so what I've done successfully somewhat successfully in the code so far is compute the errors the output errors and the hidden errors and I've added I have a transpose function I have a matrix multiply function so what I need to do is compute the deltas the gradients I need to figure out how what I need to do to change all these weights I should say that I did say something I think wrong in the previous video so I might as well at least say it here which is that in with matrix multiplication and this shouldn't be an asterisk I don't know you know this you can think of the dot product or just an X for matrix multiplication I need to have be a this is matrix a and this is matrix B a the number of columns in a has to be the same as the number of rows in B so this could have as long as there's one column this could have three rows and this could have eight columns but this is one dimensional this way one dimensional this way and that's going to that's going to always give us in the end the correct dimensions for the weight matrix so I kind of miss stated that in the previous video I don't know if you were worried about that I don't know if correcting it by now but that's enough so let's try to put this stuff in here now one thing that I think I really need to do unfortunately is that I had this lovely idea previously of like Oh what I need to do first to get the error right is I need to feed the inputs in get the output and compare it to the target right that's going to give me the errors and then I can compute the hidden error by doing the weighted percentage stuff that I did previously the issue is once I start wanting to do all this stuff it would be nice if I could remember all the parts that happen during the feedforward process so as much as I just wanted to call feedforward here I think it's actually gonna work better if I run the feedforward stuff here so I'm gonna actually grab all of this copy it and I'm going to paste it right here so there's a little definitely some redundancy in the code this is going to help me figure it out so I need to feed everything forward inputs hidden which then gets the bias passes through sigmoid sidon this hidden matrix is left in the state of the values coming out of here good then I have the output be adding them the bias and the output is left in the state of it coming out now in the state of it coming out of here now Bri viously I had taken the outputs from the root from the feed forward function I don't want to do that anymore in fact I guess I'm being consistent I should call these outputs so now I have the outputs plus I have like all the other stuff that happened before if case I need to reuse it and I have the targets and I can have the output errors so now I need the gradients so what do I need for the gradient I'm gonna call this let gradient equal outputs okay so how am I going to do this how to do them it's like oh I don't have numb by this is where you really need numpy which is a Python library for doing matrix calculations so let me think about this I need to I need to take those outputs which have already been passed through sigmoid and multiply it times what I need to do but this won't work right I need to calculate this gradient as those outputs the derivative outputs times 1 minus outputs so I have to do this with my matrix library so I need to somehow get this gradient which is this piece right here the nice thing is I can actually use functionality that I have built into the matrix library so for example I have that map function so I can take every element of output and set it equal to output at that element times 1 minus itself so what I need is another function right much like I have sigmoid what I need is the derivative of sigmoid now this there's a little bit something strange that's gonna go on here let me just write this if I write a function called d sigmoid what I really mean is return sigmoid of x times 1 minus Sigma of X this technically speaking is the Rivet of sigmoid but that's actually not what I want to do here because if you know if you're following along and where am I here in train I've already mapped the output through sigmoid so actually what I want is and I kind of like oh I would call it like fake dig see sigmoid but I'm just gonna put Y in here and I'm going to comment this out and kind of as if Y is I'm just changing the very ammonium Y has already been sigmoid and I'm gonna say return Y times 1 minus y so what I can do now to calculate this gradients is I can basically say outputs I kind of want to call it gradient but right outputs dot map and maybe I should make a copy of it or something d sigmoid right so now I've taken outputs and I've set each element equal this now I need two element wise multiply that by the error so I need to say outputs now here's the thing in my matrix library this is like I have this right here the multiply function currently if it gets a matrix it does what's what I was for the element wise multiplication which I'm referring to as the Hadamard product so otherwise so I guess what I mean I'm gonna keep this and I'm gonna say outputs dot x output errors there we go so now I've done this piece and this piece now I need to multiply it by the learning rate do I even have I all I've been waiting my whole life just to get to the point where I could put the learning rate in the code because I feel like once you have the learning rate in the code I kind of done so let's make that a variable this dot learning rate I don't know I'm just gonna set equal like point one right now and so now I also need to say outputs dot multiply by learning rate and now what I need to do I've done this whole piece here all I need to do is take what came out of hit and transpose it and do the matrix the matrix product matrix multiplication between that matrix and this matrix and then I have all the Delta weights and I can just adjust them you know I've got to talk about stochastic etc but let's just let me let me just we try to get through what I'm doing here all right I now am going to say what am I looking for I'm looking for this particular array right this particular vector I need to get let hidden T which is hidden transpose is matrix transpose hidden and then let deltas wait wait what am i calling these like the this dot weights I H so I'm going to say weights H Oh deltas equals matrix dot multiplied so this is calculate I want to put a comment in here calculate gradient and then calculate deltas calculate deltas matrix top multiply what do I want to multiply I got to do this in the right order I want the column vector which is the the gradients thing that I've been doing and the row vector which is the output to hit ok so I'm going to say multiply outputs so I hate that I've done this does the map function what I want to have is a you know what I want is I want a static version of the map function that will pull out make it new so let me do that really quickly so where's the map function map I made a static I made a static version already how exciting Oh life is good sometimes well that's so lucky so I want to do this let gradients equal the matrix dot map this solves all my problems outputs D sigmoid no I don't need all right so I want to map all the outputs with Sigma the derivative of sigmoid then multiplied by output errors multiplied by learning rate transpose the hidden output and then matrix multiply the gradients by the hidden output transposed and now wait H of this this dot wait H Oh add wait H Oh deltas so this is me just like taking going into just going out to saying I calculated those deltas change all the weights by those deltas so I'm no if this is this right let's think about this more later I need the bias well that's fine I'm gonna do the vice all right so that's good now I have to deal with the hidden layer this should be much easier now that I've done this once okay I need to calculate the hidden gradient which is matrix dot map the hiddens what came out of hidden passed through d d sigmoid then i need to take the hidden gradient and what did i do up here i multiplied by the output errors but have i calculated the hidden errors i have calculated the hidden errors somewhere i did back propagation hidden errors right there ah how lucky lucky me hidden gradient x hidden errors then hidden gradients x learning rate and that's this dot learning rate and did I forget that up here yeah this this stop learning rate right and then so this is calculate hidden gradient now oh my god calculate hidden Delta's alright you know that it's input to hidden input to hidden I'll just input to hidden deltas okay so that is just like I did up here the first thing I need to do is transpose input inputs T equals inputs dot transpose I know matrix dot transpose inputs matrix dot transpose inputs okay and then what did I call those deltas I called these weight hidden output deltas so let wait input hidden Delta 's equal matrix multiply the inputs no no nothing inputs the gradients times the transpose inputs so you hidden gradient times the what I call that inputs t transposed inputs and then I can just adjust those my goodness I have just less PII I'm speechless I without speech okay I think I might be done with this for the bias I'm gonna save that for I'm gonna add the bias in a separate video because I feel like I just need a break so let's think about this I might have made some mistakes but I think that I've gotten through this I think what I've done is I have figured out a way to use the train function to calculate the errors use back propagation to chop up and divide the error in a sine blame all over the place right I know the output errors I need to figure out the hidden layers errors then that's what I've got here those were the first two videos in this the third year I kind of talked through these formulas and now in this video I have used a math function to calculate the gradient the errors times the derivative of the output I adding the learning rate in I'm multiplying it by what's coming in transposed to get the weight deltas I've done that for both this layer this matrix and this matrix again at some point I really need to extract in you with this library to make it something useful I need to be able have multiple hidden layers and this back propagation would happen in a loop but I'm doing this to separate distinct chunks just to understand them and then I'm gonna adjust all the weights so there's things that I haven't talked about yet number one is I've got to adjust the bias values and number two is when and where should I be doing this should I run through all of my training data and get like the kind of average error of everything and then adjust all the weights or should I each time adjust the weights for each record or should i do batches like send in these ten data points adjust send in these ten adjust and that has to do with stochastic gradient descent versus a batch gradient descent so I'm gonna get there let me take a break take a few deep breaths and make a separate video where I adjust the bias ease I might have made some something absolutely completely wrong so you you have to watch all the way through to find out if I have other things that I have to correct later which I may but I'm gonna change the biases and then I'm finally going to do I'm just gonna try to train it on the X or to see if I've done this correctly XOR would be a really simple problem so it's a good test problem for me to see if my code is mostly working correctly but before I go there is one error that I can point out here this is input to hidden thank you okay see you in the next video you