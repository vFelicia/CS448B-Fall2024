okay so maybe you've watched my videos so far about engrams and Markov chains and that sort of thing and you're thinking what what's the next step for you what could you possibly make so on the one hand I think the assignment are the exercise for this week it's really legitimate to not program anything new in fact a lot of using a Markov chain to generate text is about two things outside of the actual algorithm itself number one it's what source text are you picking and why are you picking that source text and number two is what are you presenting to a reader a viewer a user an audience and what medium are you presenting it in are you using a Markov chain to generate dialogue for a performance are using a Markov chain to generate tweets are you recreating you know news articles and the project about designing a news like a website that looks like real news but actually it's Markov generated news there are a lot of possibilities there using a Markov chain to generate text is not really a novel idea at all and you'll see I'll try to link in this video's description to lots of existing projects that use this technique that you can also find for experimentation now here's the thing though I have some additional examples that you know the videos that I made that you if you watch the Markov chain video previously i'm just implementing the sort of raw algorithm and doing this kind of simple not simple but doing this sort of basic implementation of it so one thing I want to do and that's this example that's trying to generate code or full code in Bo's that's kind of awesome fifth Matt's ratings shift so you can see there's some new names that I'm trying to think of for this channel now I do have some premade examples that if you go to the link to the A to Z github repository you'll see I want to highlight some of these number one and these are by the way this is a Markov chain example that does something much like my youtube channel naming and it just takes naming of names of lots of media art programs and creates name so till we get anything good sent lis program in arts and media arts encode media arts program technologies program creations in entrepreneurship anti discipline 'native technology program set technology people so you can see computer action that's pretty great so you can see this is the kind of thing for naming you can also look at my second example and try to find the differences right between generating short text this reads Mary Shelley's Frankenstein from Project Gutenberg and generates kind of an actual very very long text with paragraphs on everything written in the quote unquote style which isn't exactly accurate but uses a markup change a generated new text so you might look at those look at the difference between short text and long text both of those examples are generating text on the character by character basis so when I say a trigram I mean a collection of through a sequence of contiguous sequence of three characters and three characters but you can also do this by word so you can consider the tokens not as individual characters but as words a rainbow is rainbow is a is a meteorological so this is an example that instead of using sequences of characters using sequences of words and you can see the results here in a primary rainbow with inverse order of its colors reversed so at some point I could go back and do another coding challenge to implement it by word if people are interested in that but right now I might just refer you to this code example another thing you might think about is what are other tokens or things that happen in sequence in text for example this text is not just this sequence of characters or this sequence of words it's actually a sequence of parts of speech verb noun noun verb determinate adjective adverb getting that in the wrong order but what this actually does is evaluate the contiguous sequence of parts of speech use a Markov chain to chain to generate a sequence of parts of speech and then pick words that are parts of that parts of speech to generate the text and the results here you can see is the observer it reaches the meteorological set the raised section in a section and 40 to 42 degrees percent or more in the system between an arc and 50% excetera etc so you can take a look at that example as well another thing you might think about is mixing two texts within a Markov chain right what if I generate this uses if I take the slider all the way over here this is generating text using a Markov chain only from Midsummer Night's Dream this is now generating text only from my nature of codebook and now if I kind of put it like over here let's see if we can generate something good that's a little bit too Shakespearean let's let's let's give it a little more nature of code do it again unto his point numbers rather voice versions when program how do I knowledge about that we add one we can be the mouse location strong prevalence of the true for both X's and dispose of zero zero we still have this kind wanting but how to mic inside of length one then have our motion the close to your youth to one okay anyway I got a little too caught up in like pretending I'm like acting Shakespeare with my Markov chain but anyway so you can see that's the thing you might think about what if you're mixing two texts three texts how do you adjust the probabilities I'll give you a little hint behind the scenes if the probability if I want the probability of the Shakespeare text to be higher I just feed the Shakespeare text in multiple times again it's not the most efficient way of doing this but it works you could think about other ways that you might do that you could get you can use a Google sheet I have a mad libs generator that pulls data from a Google sheet could do the same thing with a Markov chain you could pull from an API I have an example that pulls from reddit and then also I should mention of course that even though I'm using my own Markov generating code based on code written in Python by Allison parish you could also use the rita library which has a markov generator built into it and I have a previous video that I should link to in this description all about the Rita library and encourage you to look at that and see its implementation so these are things that you could think about I made a little list here also that I'll reference so on the one hand just generate what's your own existing text what's your what's what's the order you're picking how are you presenting it you could think about just sort of visualizing the frequencies or the the possible pathways this web grant trigrams by Chris Harrison is a project I reference you can see this visualization of those of those engrams is something you might think about and then ice about mashing up to text the Benoa tree project is another useful reference that I'll have a link to and then the other thing that I think is really important to mention is even though this course and this set of videos are I'm focusing on this idea of text and the characters are words being States and this continuous sequence of states this Markov chain you the units of the Markov chain don't have to be text what if there are musical notes or rhythms or colors or vectors what other types of visual designs or sounds or music could you generate through a Markov chain so I hope you will think about making something with this you can share it with me at Schiffman on twitter you could share it in the comments here you can also subscribe to this channel Speight Rhian there's a link in the video's description if you would like to make a financial contribution of course that's optional but with that you can get membership to a slack channel where you can also share your assignments and get feedback from a community of folks in the slack channel so thank you so much for watching I look forward to seeing what you make from this and look forward to seeing you I don't actually see anybody was doing this from just a camera but it feels like I am so I look forward to seeing you in the next video okay bye bye