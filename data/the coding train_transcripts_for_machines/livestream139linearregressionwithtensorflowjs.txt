[Laughter] my event has started which means that it is me Dan ship and coming to you again for the second time today for the third time in two days this is probably really not so advisable there's a lot of reasons why I shouldn't be here but I am here and I am going to I have about one hour so I'm really gonna jump right in I have I think I might actually even call this dare I say a coding challenge think about this it's okay I don't know this is not the important thing to talk about this is the topic for today linear regression using tensor flow test I'll start going and running away which would be completely understandable I'm really doing this for a couple of reasons one number one is I have started a series of tensorflow chess tutorials just looking at some of the basic features of the API and I want to at least like gets into an example we're actually using this stuff for some purpose now what kind of you know this is really foundational knowledge it's kind of like a classic example of like the beginning stages of machine learning I have actually made some videos about this already in notably this linear regression with gradient descent video that I made just with plain JavaScript I also made this illadvised video about the mathematics of Oh underlying audio is not working all right well I won't worry about that we just checked the chat here okay this a video about the sort of mathematics trades it's and so these would be two pieces of background you're watching this live so you probably don't want to stop and go watch those but you could okay and so I think this is going to be helpful in term it's a vertical thank you I'm gonna be saying all this stuff again in a second because I'm gonna this but just bear with me please thank you you're very good thank you to old paddock whole pod car on Twitter who posted this code for an interactive simulation of linear aggression with aggression with tensorflow and p5.js this gave me the idea you can read in this thread oh great work I wonder if a video tutorial redoing my linear regression example in a similar fashion would be useful and then the reply was thanks yes and some other people replied and thought this would be useful as well so that's what I'm going to do this is foundational knowledge let me see we're we're aware does this appear we're we're aware under here this is all my neural networks and machine learning playlists we're looking for this one since session 6 so this is what I've done so far and the the thing that I really want to get to which is gonna come next week is the layers API and this is where I'm going to actually begin building some neural network examples and remaking some previous things I've done on this channel without tensorflow TAS but I think before we get there I want to just look at some other features of the attention largest library in the context of linear regression so that's the plan for today ok so I think I just want to get going so I need to reference all these things I guess I will just tab my way through them I'm gonna move this here and I haven't haven't implemented this I scanned over this github users code so I do have that a little bit in the back of my head but I have not tried to implement this myself I've got the chat going the slack channel going here for patrons and YouTube sponsors thank you so I'm gonna you'll be keeping me in check and I've got until about 530 p.m. I'm just trying to decide does this sit as 6.5 linear regression with gradient descent or or does this is this a coding challenge I'm seeing in the chat that a chat user says if you want to do interactive stuff with tfj s you should check out TF next frame okay hold on I was not aware of this so let me have a look at next frame returns a promise that resolves when a request animation frame has completed Oh fascinating Oh interesting so that's really interesting I think I'm gonna do this is right so what I'm doing is so incredibly well it's is comparable simple and I I can probably do it with pulling the yeah let's see how this goes let me try to build it and thank you for this note I'm gonna try to do it probably in a less correct way just get it kind of working let the threads kind of like and the amount of computation that I'm gonna do for linear regression is so simple that it even if it locks up for a second that it's gonna be fine let the animation keep going but this is absolutely something that I will need to pay attention to more later for sure okay does anyone can anyone help me with the pronunciation of this name cows tube old pod car calss tube old pop car any chance that that was kind of close okay you will be getting going in just a second I'm waiting for the live chat to maybe catch up to me somebody might help me pronounce this name correctly all right close enough I think well that's what its gonna have to be where's my marker here we go and I'm gonna get started I wish I could just call and pronounce it ah that's a good point yeah you could you know yeah let's move on I probably some fancy technological way we could make that happen everybody should stretch stretch is really this extra live stream towards the end of the day trying to do something with machine learning and since the flow Jess this really needs some space Mellon but I don't have any space Mellon with me so I'm just gonna have to try my best couse tub thank you coos tub ku stub old pot car okay thank you cows tub old pod ker okay um now people are just trolling me in the chat by saying it's pronounced with the actual spelling of the name I will do my best hello you are here watching another coding challenge and I you know this coding challenge maybe this should just fit in being one of my tutorial videos but I'm gonna make it a coding challenge because I bet his attempt to do it in one video and what I'm doing is recreating something that I've done before and some of my machine learning tutorials and it was it suggested er know if it was suggested exactly but a twitter user a cow's tube old pod car apologies if i pressing the name incorrectly created this interactive simulation of linear regret using tensorflow Jas and so this is very similar to something that I've done previously right I have this video linear regression with gradient descent where I just did this with plain JavaScript and then you could also look at this other video which I go through the mathematics of gradient descent a little bit but here's the thing going through the mathematics making this video where I implement the mathematics in JavaScript while useful and perhaps background for this video one of the exciting things about doing this with tensorflow Dutch ass is tensor photo chess has a nice API for optimizing loss functions with the gradient descent algorithm built into it so I could just do things so let's make a light to come back here but let's make a list oh hold on hold on the things that I'm going to need right I need to have okay so what's going on here what is linear regression first of all and of course you can compute the I was gonna do this as a coding challenge with no editing but so much for that alright so first of all what is linear regression anyway so let's say we have a space and I drew this as like a canvas but really I should be talking just about a generic kind of twodimensional Cartesian plane and in that plane there are a lot of points and I don't like the way I just drew that in that plane there are a lot of they're a bunch of points the idea of linear regression is to figure out can we fit oh this is a time for another colored marker can we fit a line into this twodimensional space that approximates all of these points as best we can and I can visually just kind of make myself do this like this so I can eyeball and say this line kind of gets close what we're trying to do is minimize all of these this is the most beautiful diagram I've ever made all of these distances of all of the points to the line the idea here then is that we can make some predictions right if this data if this xaxis you know represents height we might predict on the yaxis weight right you can think of kinds of data sets simple 2d data sets where there's a linear relationship between the 11 field of data and another field of data so if we pick a new height we can kind of make a guess approximately what that weight is going to be that's the idea of linear regression it's incredibly simple a lot of data isn't twodimensional a lot of data doesn't fit a line you know maybe a curve fits it better and this is more complex scenarios will come as we move forward and make more scenarios with complex polynomial equations our neural network based learning and other types of machine learning algorithms but this is a good place for us to start so what do we need we need a data set so we need a set of X's and Y's this is the data set right we need X's and Y's and I'm gonna create that data set through interactive clicking interactive clicking is the way I'm going to create that data set every with the mouse I need to have something called a loss function the loss function is a way of computing the error and there are a bunch of different loss functions and we'll see these as we as I use tensorflow DHS in more tutorials I can select different kinds of loss functions for different scenarios but in this scenario I'm going to use a simple basic one which I believe is called root mean squared error did I say that correctly is that the right name of it but the idea is that I want to look at all of those distances and I want to minimize them so that number if I add if I find all these distances the difference and by the way I totally uh I botched this time out vertical distance it's even in the chat so the good news is you can barely see it I'm looking over here on my monitor and I can't see it you can barely see it but I really botched what I was drawing there so let me fix that I don't think I'm gonna go back and redo this video I think I'm just gonna get a different pen color since you can't see it anyway and just fix that so a little editing point here okay I'm back as I started talking about the lost function I realized I really didn't draw I'm not actually looking for the the distance from the point to that line which would be perpendicular I'm looking for this vertical distance which is so this is what I'm trying to minimize right I'm trying to minimize and get a line that has and this that that is the least dilute the sum of all these distances is the smallest number minimizing the loss so I have a loss function I need that I also need I also need in tensorflow da chasse something called an optimizer and the optimizer is the thing that allows me to minimize the loss function and in order to do that I also need to have a learning rate so these are all I've actually missed something very important here but these are all the pieces I need the data I need to define a loss function I need to define an optimizer I say hey optimizer minimize the loss function with this learning rate so keep tweaking the pair parameters tweaking the parameters so that's the thing I forgot what are those parameters well the formula for a line is y equals MX plus B M is often referred to as the slope B as the offset what's the name for that what's the name of that formula for a line because why shouldn't do these live streams at the end of the day I was so so with it when I was talking about promises this morning wasn't I yintercept there we go yintercept ok I'm back I looked it up M is the slope and B referred to as the yintercept kind of like bias by the way if you've watched some other neural network tutorials this is like the thing we're doing with all the neurons oh it's also connected but we're just living in this very simple place so I need these parameters I need these variables because that's what's going to allow me to create the predictions that are on the line to compare with the actual points and compute the loss minimize it tweaking these values so tweak these values minimizing the loss this is what we're doing and I've done this before in great detail this is gonna be in less detail because tension flow dot yes it's gonna do a lot of this for us the thing that's a little extra complicated is we can't just work with arrays of numbers and variables in the way that we're used to in JavaScript and so this is what brings me to if you haven't looked at these particular videos that I've made already what's a tensor flow tensor what's a variable what's in operation how does the memory management stuff this is stuff we're gonna have to lean on while I build this example and this should be by the way an actual practical example of where I need a TF variable so I kind of in this video explained what a TF variable is we'll just kind of moved on and didn't use it for anything so hopefully this will show us that all right how about we write some code now I'm pausing for a second taking a look at the chat the route shouldn't be there I don't know what that means offset it's fine why intersect or intercept what's the route oh so what's the let me look this up on the what's the wire mess Oh in the in the back of my yeah let me fix that what's the why did I say root let me look at loss square Oh cuz I don't have to square root it mean squared yeah so I'm gonna just use mean squared error thank you Thank You Simon in the chat in the slack group makes a good point so maybe actually correct that really quickly I am NOT gonna be doing partial derivatives again and definitely not asked Kenneth so maybe I should go back and define that better root mean squared is oh okay so actually I match eclis appeared over here for a second because instead of I did make a little mistake here I mean root mean squared is a perfectly legitimate loss function but most linear regression with gradient descent examples will not bother with the root and the root refers to square root we just want the mean squared error which means like if I say that this value is y and this value is like the guests right I the error is guess minus y and squared and if I do that for all of these that's the mean an average them it's the mean so I can really sum them or mean them well it's gonna take care of that for us so we're just gonna use the mean squared error but that's the idea we take the differences there the reason why we have to square it well for a variety of reasons one it has to do with the derivative stuff that's in my other videos but also just because positive or negative with the it's the distance that the size of the error whether it's up or down which is key all right so I need to start the coding but the microphone is making me crazy I need to tape it to my ear so hold on I just need to get a little piece of tape it it really distracts me when the microphone starts yeah by the way who's Ricardo and the chat is mentioning the coding train hoodie this is a new coding train hoody that will be available later today all right am I good to start coding any last comments before I start coding this feels it's staying on my ear now okay and maybe next day next year next time I'll get the lapel mic back how are we for 51 we're doing pretty well too check mine I'm just gonna keep going all right I think I'm good I think we're ready to start writing some code all right where's my marker in case I need it over here all right so here's the kind of code we're gonna start with I'm using p5 so that I can draw stuff I'm making a canvas and the background is zero which means it's black and this is what I have so far so let's look at our list over here and let's first add the data set the X's and Y's so this is going to be easy because I just want to have X's be an array wise also be an array so and then whenever I click the mouse I want to say oh you know what I could make those vectors let's make them separate arrays I think we're gonna actually I know we're gonna we're don't want to do that for a variety of reasons we're gonna keep those at separate arrays so every time I click the mouse I'm going to say X's push Mouse X wise push Mouse Y ah okay here's this here's a little thing so this is our canvas right this is my drawing of the canvas I know I'm having like a DejaVu thing I totally talked about this in the other video the width is like 400 the height is 400 but I really want to think of this as I really want to think of zero zero down here and maybe one being over here oh and normalize everything between zero and one everything's just gonna work better if we do that so with Y pointing up so I'm gonna do a mapping so every Y value that's pixel value between zero and height I'm gonna map between 1 and 0 and every x value that's between 0 and width I'm going to map between 0 and 1 so let's do that so I'm going to say let X equal map mouse X which goes between 0 and width 2 between 0 and 1 let Y which is Mouse y between 0 and height and have that go between 1 and 0 and then push X and push Y so the other thing I want to do is I just want to UM you know I'm gonna add a draw loop and I want to draw all those points so I'm also now gonna say stroke 255 stroke wait for 4 let I equal what huh let a equal 0 is less than X dot link I plus plus and those are actually called X's X's dot length and what I'm gonna do is I'm going to say let pixel X equal map I really should make just like I probably have do this a lot so I should probably make a function that just like normalize and unnormal eyes or denormalize pix px equals map X's index I which goes between 0 and 1 back to 0 and width so this is the reverse py which maps Y which goes between 0 & 1 to height comma 0 and then I want to say a point P X py so I haven't done any I'm even I'm not even using tensorflow Jass yet I'm just kind of doing the stuff with p5 to draw things so let's see if getting the results that I want which is whatever I click I get the points they're perfect and I kind of want to see them a bit more that's like really make it bigger great that's like too big okay great so we can see those those are the points I'm clicking on okay pause pause edit edit pause pause at it I'm like drunk with power with the editing I just take a break it'll get edited later I used to say that and it didn't now it does I think was probably better when I didn't stop in the middle all right all right so what's next I need a loss function I need an optimizer ah oh I need these let's make these so cuz I'm looking for somewhere where I need to get some tensor flow Jas stuff working so what I need is I need to have M and B so let's figure that out so I'm going to create an M and a B and I'm not going to initialize them in setup here and I probably should be using Const in various places here just protect myself from reassigning something by accident but I'm gonna be looseygoosey and just use let you know these could be constant but anyway I'm not going to get into the whole let vs. constant it makes me crazy I'm gonna say up here M equals TF scalar random one so I'm going to use the p5 random function to give me a random number between 0 1 because I gotta start somewhere so this is kind of like initializing the weights of a neural network there is no neural network I'm just doing I'm just kind of optimizing this function y equals MX plus B but those are like weights and then B so I'm gonna initialize them randomly and scaler because it's a single number so go back to my tensor flow j ass intro videos and you'll see then B I'm gonna say the same thing ah but these are the things that have to change right the data never changes it's sort of fixed M and B change over time I need to tweak those which means they have to be variable they have to be able to change which means when I overhear I think what I write is TF variable I wrapped them in the TF variable so now I have M and B as TF variable right isn't it crazy like you see this kind of code you're like that looks like the craziest scariest thing but you realize like it's just like make a number and because we're in this like kind of lower level working on the GPU land I've got to be very like specific like this is a single number and it's going to be variable but really it's just a random number okay now what do we need to do we need to write I don't think I actually said this but I need to write a function called predict maybe which takes in all of the X's just the X's and gives me the Y predictions based on where the line is right because I need to compare the Y predictions to the actual Y values to get the mean squared error so let's write that function so I'm just gonna I'm putting these in like arbitrary places but I'm gonna write a function called predict and they're what I need to do is I need to have some X's and I need to return some wise I think that's the idea right yes so I don't want to just predict one value I want to predict a bunch so the X's here's the thing so if I call this function the X is if they're just a plain array I need to make it into a tensor so I'm gonna call it Const TF X's that might be a bad is tensor and this is a one D tensor tensor 1 d OT f tensor 1 D I think this will do it X's right and you turn it into a tensor and then I need to have the formula for a line so I need to say what which is y equals MX plus B so what I would be doing is I would be saying TF X's multiplied is it mu L or nu l T multiplied by M plus B right this is the idea if I'm getting just a plain array of numbers I turn them into a tensor then I apply the formula for a line and these are the predictions the wise I guess if I don't like my naming here I'm just gonna call this X and maybe I'll call this X's I don't know I have to think about I'll come back to this later okay so I have that now let's go back and look at the things that I need so I have this predict function I have a data set ah I need a loss function you need a loss function and I need a let's before we do the loss function let's create the optimizer and the learning rate so this is what's wonderful about working with tensorflow j/s when i say make the optimizer I just mean make it TF it exists it'll do this math for us so let's go to the this is not something that I covered in my other videos so let's go look for optimizer and I want an optimizer now there's all these different kinds of optimizers SGD stochastic a gradient descent this means the idea of slowly adjusting mnb toward two to minimize the loss function and I've covered this in more detail in the other videos so so I'm gonna click on that and I'm gonna look here and this is basically what I need to do i we got the code right here look there's even look at this oh my goodness there's like some stuff here we could really use so I'm gonna grab this and I'm gonna put this up here so I want to learning rate I'm gonna I'm gonna have a much bigger learning rate to start with and I want an optimizer so now I have a learning rate and an optimizer and the optimizer is doing stochastic gradient descent so I have learning rate optimizer now I need that loss function I need the loss function okay the loss function is something I'm going to write loss and actually let's go back to here so look at this so this is the fancy es6 way of writing a function but I'm gonna write it in a less fancy way and I'm gonna do this so what I want is I need the loss function I have some predictions and I have some labels so these are the predictions are the Y values I'm getting from the predict function the labels are the actual Y values that are part of this and by the way I'm gonna have to do memory management don't worry I'm if you're screaming at me that I haven't worked about memory management I'm gonna do that just gonna do that later so what I want to do is say return to predictions minus the labels that makes sense right because I said here when I said mean squared error is the predictions like the guests minus the labels which is the actual Y squared and so predictions minus the labels squared and then take the mean of them look at this all of these mathematical operations are inside of tensorflow digest and you can chain them so predictions is a tensor labels is a tensor all of these remember they're just gonna keep producing new tensors and I'm gonna have to tidy and clean all this stuff up or memory management but again I'll worry about that later so now I have the loss function all right well what is it that I want to do every time so let's say I think I'm actually like I have everything I have the loss function I have the data I have the optimizer I have a predict function I have a learning rate I can minimize Y oh this I haven't done so the training the actual training what does it mean to Train to train it means minimize the loss function with the optimizer and adjusting M and B based on that all right so let's see if we can make that I have a feeling that was in that page that I went to so maybe I could just copy it from there I'm kind of this is like very totally is I'm gonna happily cheat here who was seeing that example thank you very much thank you to float IGS documentation and so I'm gonna just put this in draw like every time through draw I'm gonna minimize so let's look at this oh look at this okay so this is a little different so first of all this is using nice fancy es6 arrow notation which I'm somewhat happy about but let me just write a function here called train and the idea of the Train function is to execute the loss with the predictions and the and the actual wise okay so here what I'm really doing is minimizing the Train um that's weird this isn't really no training would be doing this so this is a terrible name for this and actually this is silly for me to even name this it really makes sense for me to just make this an anonymous function and that what I'm minimizing is this right this is what I want to minimize the loss function but if I want to be nice and es6 like with my arrow notation which I think by the fact I'm using tensorflow j/s and you can watch my arrow notation function if this is if this is I can kind of get rid of a lot of the extra stuff here and this should be good so I just want to minimize the loss function now here's the thing these have to be tensors right the loss function requires predictions and labels they have to be tensors and if you remember my X's and Y's aren't tensors when I call the predict function with the X's it gives me back a tensor so that I can't believe I haven't run this code yet this is a terrible thing usually I try to run my code incrementally all the time I guess I forgotten to do that so probably people in the chatter telling me about mistakes I'm making so this is a tensor but this is still a plain array so what I need to do is say constant and I got a rethink to naming maybe something the chat has an idea for me I think what I actually should do I have an idea permit me a moment of refactoring X valve Y valve so I think when it's not a tensor I'm just gonna call it like X underscore valve because that's gonna help me remember so X valve Y valve and then whenever I say and this should be X whenever I say X s are YS that's really a tensor I guess I could have done T X s so here what I'm doing is predicting from the X valve and then Y s is TF tensor shoot somehow I went past a half an hour YS is TF dot tensor one D Y valve so I need to create that tensor and now I can minimize the loss with predicting from the xfiles and the Y boughs okay so this is good let's just run this see what errors we get okay predict dotsub dot squared is not a function at loss at optimizer minimize so what do I have wrong here in my loss function sub labels dot squared dot mean hmm and so let me let me put no loop in here because I just want to run this once and let me say I just want to like console.log oh actually I can just do YS dot print oh you know what it is there's nothing in the arrays at the beginning they have zero things in them so a couple things one is I could put something in it but I think I probably should just say I'm not I shouldn't do only if X dot length is greater than zero so this is definitely do I want to bother with do I want to bother with doing any of this if there's no values in there like calling predicting stuff with an empty array I think it's gonna cause problems that makes sense all right let's try this X is not to find X valve my naming okay Oh sketch 45 ah this is X valve and this is X valve Y valve okay that should be good all right let's try this mmhmm all right I think I have frozen the world up this is not square I being told in the chat breaking news due to deduce that this is actually dot square not dot squared that's not right operations square whoa this I might be able to use computes the mean squared error between two tensors so I could also use this probably but that's not what I'm looking for yeah oh it's square a square okay hold on so what's going on let's comment this out Oh like I put no loop I put no loop in there silly me okay everything's fine I put no loop when I was trying to console.log stuff there we go okay so things are going and there's no I don't have any like I could look at that's M m m dot print right so you can see it's changing it's actually like training it like the value of M is changing so everything's going and working the problem is I'm not seeing the results let's just check B and I haven't done any memory cleanup so if I say memory dot num tensors is that what it is now what is it again so let's look under memory management memory Oh memory num oh this TF so I know you can't see this but this is what I want I want to check how much I want to check and see like how if I have to have cleaned up stuff you can see I have 1147 tensor so I need to do the memory cleanup I don't know probably better practice would be for me to clean up as I'm going but I'm kind of gonna clean up at the end all right so let's I just want to click back here for a second oops no I'm just gonna click no loop to shut this off and let's go here so what do I need to do ah I need to visualize what's going on all right so how do I do that so I need to draw a line so the way that I would draw the line is first what I would do is all I really need is to give myself the x value of 0 and the x value of 1 get the two Y values and draw a line between those two points so if I were to say let X equal TF scalar this is silly for me to use the predict function why not why not let's use the predict function TF scales are zero so x 1.is TF scalar 0 y1 equals T F equals predict at 1 X 2 equals TF scalar 1 y 2 equals predict X 2 right so this should give me I mean it's a little bit silly for me to not just do this keep an extra copy of like M and B he has regular numbers but let's keep going with this will this work is it gonna be able to take a scalar and make a 1 D tensor I think so so let me just see here so let me do X 1 dot print y 1 dot print so let's see that tensor 1 D requires values to be a flat typed array hmm so I could y1 I could reshape yes good point minimize sorry sorry I started looking at the chat which I shouldn't do I'm gonna pause for a second for a little edit point Alex Pratt's Pharaoh writes you need to average the squared error I don't think I need to average it because I have the dot mean function that takes the average automatically I think so let me think what might be better is for me to just should I just I should just bite the bullet and get the M and B values back right yeah I could reshape this way oh no this is silly I can just do this hold on all right so one thing I could do is instead of making it a instead of making it a scalar tent I can make it a one D tensor that's what it wants and do the same thing here and I have to put it in as an array then but it's just one value oh this is so silly why am i doing x1 oh I could just do this right X is once again can I use X's yeah yeah yeah so I could do I could just do it with zero and one constant X's and then constant y z equals predict X's right so I could have both these points now then let's say X is dot print wise dot print let's look at that let's see if this works predict is not defined because my e key doesn't work I have to type it several times tensor one D requires values to be a flat typed array Oh silly silly me predict doesn't want a tensor oh it wants this so line X equal let me just this is a little bit silly but I'm gonna do this so I'm gonna make the oh oh but I don't need to know the X's tensor because yeah sorry everybody there we go my predict function I totally forgot already see this is just there's so many different ways you could do this like I could have I could enforce you to convert to a tensor before you pass it in to predict but I've just a lot of these decisions are completely arbitrary so you might have a better way of doing it but so I'm gonna do this so now I have the XS and the Y's and I don't even need to say XS print so we can see okay great so I'm getting these points k week mon in the chat makes an excellent point which is that i should think about actually mapping it between negative one and one with zero zero in the center that's not such a bad eye eeeh uh let me just keep going with this and then I'll maybe I'll change that after the fact because this should work anyway so now here's the thing here's the awkward thing in order for me all I need to do now is basically say this let x1 equal map X is 0 which goes between 0 & 1 1 between 0 & with and this is kind of silly could just multiply it times width but I'm gonna just go with the normalizing though all the full normalizing y1 equals map X sorry X 2 which map X's index 1 so this gives me X 1 which is just 0 and with now y1 and y2 I want to map YS the Y value is between and between height and zero because I'm flipping it the problem is and then I just want to say line x1 y1 x2 y2 so this is really all I need to do right I just want to get this sort of two points on the line and then draw a line between them this is fine because my X's are not tensors and I can use plain numbers right here x1 x2 but my y's and and here but my y's are tensors so for me to be able to I really need to get the values back and a way to do that is is with a function called data so I'm gonna say let line y equal wise dot data and I'm just gonna say Data Sync right now and let me comment all this out and let me console.log that and see if this comes so there's this is kind of a bad idea for a variety of reasons but I think it's gonna work ok so you can see I'm getting those numbers back as a float array so here's the thing this really requires not a callback but a promise and I'm so happy I just did a whole video series on promises I really should be saying data then and there's even something called TF next frame which allows me to sort of think about the asynchronous nature of pulling the data out of a tensor into a number that I can use in an animation these are key things I'm definitely gonna have to get to them but here's the thing this is just two numbers I think my animation can handle using data sync and maybe somebody from the tensorflow Jazz team is gonna want to say like actually this was not just a bad idea but like a really bad idea I'm not so sure but I think it's gonna let's just get it to work and see if this demonstrates the idea so now I'm gonna call this line why I should be able to say y1 y2 and I should get 0 & 1 from line Y and now we should see we should be done up line Y is not defined we're sketch that J's line 61 I think I just didn't hit save yeah oh I haven't clicked any points hey look at that oh hey look it's working alright for a couple things number one is let's say strokeweight and by the way we can now start to play with the learning rate III don't have to clean up the memory stuff I have to low can play with the learning rate like let's make this point zero one so you can see what happens with this lower learning rate I don't know if it's let's see is it really working well I shouldn't use such a low learning rate let's make it point five yeah it's definitely it's definitely happy okay so this is working linear regression created to say but I have a severe problem I am just filling the GPU with tensors and tensors and tensors and tensors and never cleaning them up so now it's my job to go through and find every place I'm creating a tensor and dispose of it so I can use TF tidy to do that automatically or it can just use the actual dispose function which I might be inclined to do it first all right so let's go through so here these I do not I always want to keep them m and B your variables that I need to keep throughout the course of this program loss do I just put tidy in here or should I let's predict so do I put tidy in here do I wrap tidy here what if I just put tidy here like what if I say TF dot tidy and put all of this will this do it and then here I also need well I'm here maybe what I'm gonna do is just dispose these there's no logic to what I'm doing but I'm just gonna dispose these manually oh and that's just the wise right line y is just y s is the own that the tents are here so this should tidy everything but hopefully not the variables that I need to keep rather than individually figuring out what to dispose of and down here I kind of know that this is my only tensor this by the way I should call this like line X just to be consistent with my variable naming you know I'm only using why that YS and XS notate variable name when I have something that's actually a tensor which helps me remember what I need to clean up and not let's see if this goes okay it's still running 221 no okay so I better there's fewer tensors but I haven't cleaned up everything so what could I be missing maybe the call to predict wouldn't tidy clean that up you know we need a I'm have to think about this for a little bit Ferenc asked just joined in as it being recorded and posted later on YouTube yes anybody see what tensors I think this is like the laziest thing ever there's got to be a good way to debug this so this definitely gets cleaned up the question does everything that's made inside of the loss the loss and the predict function get cleaned up it would seem not I'll hold on let's comment this out it's not working but no okay that's good to know okay hold on alright need to debug this somehow one thing I could do is trucks are commenting stuff out to see like where is the memory leak so one worry I have I really think Lawson predict those functions generate a lot of tensors I believe TF tidy should clean up anything but let's just for the sake of argument comment this out and now of course the learning is no longer happening and what I might as well do is console.log the amount of tensors not have to like ask for it whoops um what did I do wrong TF memory num tensors what is it how come I can't remember what it is num tensors no yes it's not a function it's just num tensors sorry everybody okay so and I need another parentheses here a little digression there all right all right so we can see it's growing so let's keep commenting stuff out to see like what's causing the memory leak let's comment out this whole area down here ah good news everybody the memory leak is in that part let's put this back just to be sure okay ah so the memory leak is definitely down here and I probably created oh my goodness oh my goodness no I'm not sure well let's put this back in I thought I saw it but then I didn't again so this is a tensor and I'm disposing it Oh predict aha the predict function makes other tensors and predict got cleaned up because it wasn't ID but I'm just manually disposing the Y's down there that's what it is so let me use tidy I guess so let me do this let me put this up here so this is really what I need to tidy so instead of disposing manually I kind of like the specific things manually the tidy thing kind of freaks me out but the problem with this is I have a scope issue which is that line Y right no matter what I do if I take this out here like this is going to tidy everything so I guess it's not the biggest deal at the moment at least for me to just put everything inside the tidy well that's the let's put everything inside the tidy for right now there's probably a way I could simplify that but this should work let's give this a try there we go there's only ever five tensors all the time so there's no more memory leak five tensors linear regression with gradient descent tensor flow das interactive here it is so what's what's left here so number one things that could be improved pause oh so I could say off all right so line why why I asked so I could actually just put a tidy here and have it returned okay so I wanted to talk through some things that could be improved but already me I am so me in the chat made a very good suggestion this is very awkward how I put everything in tidy so unnecessary let me take that out because predict returns something I can actually just put the tidy right here I don't know why I didn't think of that like I can actually just it's only this predict function that so I can actually put the tidy right here and I can use my fancy es6 arrow syntax right and the return is now assumed and then I can just say why is stuff disposed so this should work right tidies not going to tidy up the this y value but I can I can dispose that manually once I have the values so I think this will do the trick let me just take a look at this yeah so this I like better and there's probably other styles or ways you can do it the point is you've got to keep track of all the tensors you're making and dispose that okay pause edit point let me think what are some other things right I could put that lot let's use the data sink in there but I'm gonna keep that as a separate line all right ah interesting alright let's add a few more things Risa in the chat asks can you print the loss that would be really useful for us to see the loss I could even graph it so I'm sure there's a way for me to do that well I need to grab the loss somewhere so the loss I think the loss would come out here right I wonder if it would get returned let's just take a look at that oops Oh No hold on I mean I could call the loss function but maybe let's not the return is implied sounds better than assumed yes thank you that's a good point hello easily coding train sponsor outliers hold on what are some things that I can do to improve this though adding the loss I'm gonna leave that as an exercise so Matt you were at the Edit point where I was about to like add some improvements anything else tensor board to visualize the training graph is it it's 530 so I also have to go I do think I completed this I just want to check I just want to just check the reason why I have to go is I have to be home at a certain time and I just want to make sure it's not like an emergency for me to be home okay yeah all right so I've got like five or ten minutes I think I'm good try another optimizer all right so these are things but these are things I want I think I want this video to be over so you get the loss with a callback ah interesting all right print mean squared error all right all right so let me why don't you tidy the tensors inside the predict yeah that would have been another way to do it there's just so many different ways all right so here okay okay so I think we're gonna wrap this video up I'm getting all these great suggestions from the chat right you know I could have tidied this the tensors here and predict you know so number one is I'm um this code is gonna get posted to the coding train web encoding challenges make your improvements and add them as community contributions some things that I would love to see visualized graph the lost value I think there's a way to get the loss of sure there's a way to get the lost value out of that function that's one idea oh yes K week Mon suggested maybe trying some of the other optimizers so what happens if I go to the tensorflow chess documentation and like just use some of these other optimizers what are they what will they do do you get better or worse results can you make the learning rate somehow interactive adjust the learning rate you know I don't know if you could come up with any really clever visual ideas with this but anyway so but I think I'm good I think I have the basic idea so if you really want to dive as deeply as you can into linear regression with gradient descent you can go back and watch my other videos where I did this with just JavaScript in p5.js now you've seen it with JavaScript p5 yes and tensorflow yes so I look forward to your feedback and hearing more about it a more tensorflow digest videos to come and I want to get to some actual more practical things that you might want to do for interactive creative arts projects but I'm still in the weeds here of just trying to understand the nuts and bolts of how the library works so I hope you're enjoying that and I look forward to seeing you in future videos alright put on hold I am reading your comment all right so I am done for the day I have done a lot of livestreaming this week pat myself on the back at least 5 hours of live streaming this week which is pretty good hopefully that makes up for that week's that I've missed next week I think I might actually not be livestreaming on Fridays this summer I think I'm gonna be doing like Wednesdays and Thursdays during the day which is my New York time but I'm kind of gonna it's gonna be kind of gametime decision each week stay tuned so what's the what's the stuff you need to know about so if you go to the coding train comm and you click here to subscribe on YouTube it will take you to the channel hey yeah I totally want to subscribe I may not why not I'm so close to five hundred thousand which is nuts you can get fifty thousand subscribers in the next like ten minutes that would be exciting so good look it says I'm live now which is this but when I schedule a live stream okay I'm getting some important messages I really gotta go putting on my device try asking it now I missed it show us the hoodie you want so this hoodie I don't know if so it's the hoodie available yet oh the music's not working sorry about that everybody I forgot it turned it off the hoodie will soon be available on the store is a coating train that store envy calm so this is the URL for the coding train store for those of you who are interested but right now if you go here hoodie this is oh that's a different hoodie zip up hoodie I think is this the new one or not interesting question I think this I have to see let's click on this it's expensive yeah no this is the old one so you could get this but I'm going to change it this is a new American Apparel hoodie I like it better and it also has the coding train on the back although this was the sample so I'm gonna we're moving I'm gonna move it down to about here if anybody has any suggestions or ideas by the way all of this is fulfilled through a website called printful comm so in theory I have the capability to produce any like ton of like other things but it's just not my priority merchandise I don't really know what I'm doing with that but people were asking me about the hoodie all right yeah printful ships internationally it's not me who's sending it out it's done through a print on demand service it is marked up so there's like a shipping and a slight markup so if you if you buy the hoodie I probably make like four or five dollars or something I don't know what the exact amount is it sort of depends on what the there's always like all these weird taxes and shipping things but I don't you yeah it's actually no the patreon stuff I actually mail myself if you're funding through patreon or ever but the merchandise stuff just gets fulfilled through the printable store I don't have to do anything do you know uh then there the the ner engineer question do you know any reference how to link processing Java Adam yes so what you want to look at is commander processing command line so you want to look at command hold on don't where I know the camera went off everybody there we go I think somebody's already done this with like an atom plugin but if you go here to this wiki command line processing there is a command line tool that you can install and then you can have Adam like execute the sketch via a command line so you have to like configure Adam in a goofy way but this has been done I know it's done with sublime Adam editor processing gorg yeah so like yeah so I have a feeling yeah so you can see somebody's already done this already and figured it out so you have to install processing Java and this is so these are the instructions and then you have to add stuff to your path and that sort of thing okay let's look at the chat dan why do you know everything ask melon goggles it is absolutely an illusion I definitely do not know everything and I just know I just know like just enough to like get through the tutorial and then like if I were to go two steps further there'd be like a hundred things I don't know and you view all of you I'm sure seen my disastrous live streams where I get completely lost you if any of you will try to watch some of my tutorials where I tried to explain calculus stuff you would see that I don't know everything okay all right so um thank you for all the questions in the chat oh it's for the last question Justine asks what's up with the camera going off every 30 minutes have you ever watched the TV show Lost basically this is the what's the thing called the hatch I'm in the like Swan station and if I don't press the button every 30 minutes the world will end that's why I press the button every 30 minutes no the cameras have a these cameras are set to go to sleep and there are I've been many people have given me many suggestions to get them not to do that and I had not successfully been able to implement any of them so I that's that's kind of a goal I have for the summer for sure oh did I just spoil loft for people I'm terribly sorry the thing is that didn't really spoil anything a little bit loss was one of my favorite television shows I need some what am I trying to watch some stuff this summer but anyway watching I don't think you're gonna find me doing any blockchain tutorials sorry to dissapoint would you possibly do a time series prediction with lsdm recurrent neural networks the example of I don't know everything yeah but I gotta figure that out alright thanks everybody thanks for watching well well I guess my fingers are like a chef I don't know it's a sort of expression that I've adopted from somebody else if anybody knows what that reference is that's gonna that's crazy to me and did I like the lost ending not so much to be honest but it when I think back about that show it gives me fills me with such like amazing memories I was really interested this is the podcast so the ending I forgive you lost for that ending it was fine you did your best I couldn't have done any better that's for sure that was not my favorite season of loss sentiment analysis I definitely want to do yeah alright so there's going to be a lot of edited tutorials coming out just to recap there's I think five or six tutorials on promises async and await and now this coding challenge of linear regression with gradient descent I will see you all next week sometime stay tuned oh hi Cody garden Cody garden was live streaming at the same time as me earlier today Cody garden seems to live stream like 12 hours a day I don't know how that's possible is probably not that much but there's a lot of Cody Gardens live streams so everybody check out cutting garden a sponsor Cody drain oh I can't put on my music and do my goofy sponsor advertising again see everybody I'm gonna I'm gonna play out the outro now why not cuz it's the end of the day see you all later my microphone imagination creation you