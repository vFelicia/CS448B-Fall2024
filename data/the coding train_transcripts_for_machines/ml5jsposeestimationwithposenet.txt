hello and welcome to another beginner's guide to machine learning video tutorial in this video I am going to cover the pre trained model pose net and I'm going to look at what pose net is how to use it with the ml5 chess library with p5.js library and track your body in the browser in real time model as I mentioned that I'm looking at is called pose net with any machine learning model that you use the first question you probably want to ask is what are the inputs and what are the outputs and in this case the pose debt model is expecting an image as input and then as output it is going to give you an array of coordinates in addition to each of these XY coordinates it's going to give you a confidence score for each one and what do all these X Y coordinates correspond to they correspond to the key points on a pose net skeleton now the pose net skeleton isn't necessarily an anatomically correct skeleton it's just an arbitrary set of what is 17 points that you can see right over here from the nose all the way down to the right ankle that it is trying to estimate where those positions are on the human body and give you XY coordinates as well as how confident it is that it's correct about those points one other important question you should ask yourself and do some research about whenever you find yourself using a pre trained model out of the box something that somebody else trained is who trained that model why did they train that model what data was used to train that model and how is that data collected posed that is a bit of an odd case because the model itself the Train model is open source you can use it you can download it there's examples for it in tensorflow in tensorflow Jas and ml5 jazz but the actual code for training the model from what I understand or what I've been able to find is closed source so there aren't a lot of details a data set that's used often in training models around images is Coco are common objects in context and it has a lot of labelled images of people striking closes with their key points marked so I don't know for a fact whether coca was used exclusively for training PostNet whether it was used partially or not at all but your best bet for starting point for finding out as much as you can about the PostNet model is to go directly to the source the github repository for PostNet in fact there's opposed in 2.0 coming out I would also highly suggest you read the blog post realtime human pose estimation in the browser with tensorflow JS by dan Ovid and editing and illustrations from Irene Alvarado and Alexis gallo so there's a lot of excellent background information about how the model was trained and other relevant details if you want to learn more about the koko image data set I also would point you towards the humans of AI project by Phillip Schmitt which is an artwork an online exhibition that takes a critical look at the data in that data set itself if you found your way to this video most likely you're here because you're making interactive media projects and PostNet is a tool that you could use to do realtime body tracking very quickly and easily it's it's frankly pretty amazing that you can do this with just a webcam image so one way to get started which in my view is one of the easiest way is with the p5 web editor and the p5.js library which very so I have a sketch here which connects to the camera and just draws the image in a canvas I also want to make sure you have the ml 5gs library imported and that would be through a script tag in index.html once you've got all that set up we're ready to start coding so I'm gonna create a variable called pose net I'm gonna say pose net equals ml v dot pose net so all the ml 5 functions are initialized the same way by referencing the ml 5 library dot the name of the function in this case pose net now typically there's some arguments that go here and we can look up what those arguments are by going to the documentation page here we can see there are a few different ways to call the post net function I want to do it the simplest way possible I'm just going to give it the video element and a callback for when the model is loaded which I don't even know that I need I'll make sure there no errors and run this again and we can see pose net is ready so I know I've got my syntax right I've called the pose net function I've loaded the model the way post that works it's actually a bit different than everything else in the ml 5 library and it works based on event handlers so I want to set up a pose event by calling this method on on pose I want this function to execute whatever the pose net model detects a pose then call this function and give me the results of that pose I can add that right here and set up pose net on pose and then I'm gonna give it a callback called got poses and now presumably every single time it detects suppose it sees me it sees my skeleton it will log that to the console right here now that it's working I can see a bunch of objects being logged let's take a look at what's inside those objects the p5 console is very useful for your basic debugging in this case I really want to like dive deep into this object that I'm logging here the poses object so in this case I'm going to open up the actual developer console of the browser I could see a lot of stuff being logged here very very quickly I'm gonna pick any one of these and unfold it so I can see that I have an array and the first element of the array is a post there could be multiple poses that the model is detecting if there's more than one person in this case there's just one and I can look at this object it's got two properties opposed property and a skeleton property definitely want to come back to the skeleton property but let's start with the pose property it can unfold that and we can see oh my goodness look at all this stuff in here so first of all there's a score I mentioned that with each one of these XY positions of every key point there is a confidence score there is also a confidence score for the entire pose itself and because the camera seemed very little of me it's quite low just at 30 percent then I can actually access any one of those key points by its name nose left eye right eye all these all the way down once again to right and right ankle so let's actually something based on any of those key points will use my nose I wouldn't make the assumption that there's always only going to be a single person if there were multiple people I'd want to do this differently and I'm going to make if I'm gonna that stop I'm gonna make a variable called pose then I'm gonna say if it's found to pose and I can check that by just checking the length of the array if the length of the array is zero then pose equals poses index zero I'm gonna take the first pose from the array and store it into the global variable but actually if you remember the object in the array has two properties pose and skeleton so it seems there's a lot of redundant lingo here but I'm gonna say poses index 0 dot pose I this could be a good place to use the confidence score like only if it's like of a high confidence actually use it but I'm not I'm just gonna take any pose that it gives me then in the draw function I can draw something based on that pose so for example let me get myself a red nose so now if I run the sketch ah so I got an error so why did I get that error the reason why I got that error is it hasn't found to pose yet so there is no nose for it to draw so I should always check to make sure there is a valid pose first then draw that circle and there we go I now have a red dot always following my nose if you're following along pause the video and try to add two more points where your hands are now there isn't actually a hand key point it's a wrist key point but that'll probably work for our purposes I'll let you try that how did that go okay Ivan add it for you now let's see if this works woo this is working terribly okay I'm almost kind of getting it right and there we go but why is it working so poorly well first of all I'm barely showing him only showing it from my waist up and most likely the model was trained on fullbody images now I turn the camera to point at me over here and I'm further away and you can see how much more accurate this is because it sees so much more of my body I'm able to control where the risks are and get pretty good accurate tracking as I'm standing further away from the camera there are also some other interesting tricks we could try for example I could estimate distance from the camera by looking at how far apart are the eyes so for example here I'm storing the right eye and left eye location in separate variables and then calling the P 5 distance function to look at how far apart they are and then I could just take that distance and assign it to the size of the nose so as I get closer the nose gets bigger it you almost can't tell because it's sizing relative to my face but it gives it more of a realistic appearance of an actual clown nose that's attached by changing its size according to the proportions of what it's detecting in the face you might be asking yourself well what if I want to draw all the points all the points that it's tracking so for convenience I was referencing each point by name right eye left eye nose right wrist but there's actually a key points array that has all 17 points in it so I can use that to just loop through everything if that's what I want to do so I can loop through all the key points and get the X Y of each one and then I can draw a green circle at each location oops so that code didn't work because I forgot that each element each key point is more than just an X Y it's got the confidence score it's got the name of the part and a position so I need the key points index zeroes position X pose key points index I dot position X dot position dot Y now I believe this will work and here we go I only think I'm not seeing are my ankles oh it's not there we go I got kind of accurate there my pose okay so you can see I'm getting all the points of my body right now standing about probably six feet away from the camera there's one other aspect of this that I haven't shown you yet so if you've seen demos of pose net and some of the examples the points are connected with lines so on the one hand you could just memorize like always draw a line between the shoulder to the elbow and the elbow to the wrist but pose that what I presume is based on the confident scores will dynamically give you back which parts are connected to which parts and that's in the skeleton property of the object found in the array was returned to us so I could actually add a new global variable called skeleton so we've been good for Halloween skeleton equals and let me just stop this for a second poses index 0 dot skeleton I can loop over the skeleton and skeleton is actually a twodimensional array because in the second dimension it it holds the the two locations that are connected so I can say a equals skeleton index eye index 0 and B is index 1 and then I can just draw a line between the two of them I look at every skeleton point I get the two parts Part A Part B and just draw a line between the X's and Y's of each of those make it a kind of thicker line and give it the color white and let's see what this looks like and there we go that's pretty much everything you could do with the ml 5 pose net function so for you you might try to do something like make a googly eyes that's something I actually did in a previous video where I looked at an earlier version of pose net and you could also look at some of these other examples that demonstrate other aspects for example you can actually find the pose of a JPEG that you load rather than images from a webcam but what I want to do which I'm going to get to in a followup video to this is not take the outputs and draw something but rather take these outputs and feed them as training data into ml5 neural network what if I say hey every time I make this pose label that a Y and every time I make this pose label that an M a C and a you see where I'm going could I create a pose classifier I can use all of the X Y positions label them and train a classifier to make guesses as to my pose this is very similar to what I did with the teachable Machine image classifier the difference is with the image classifier soon as I move the camera to a different room with different lighting and a different background with a different person it's not going to be able to recognize the pose anymore because that was trained on the raw pixels this is actually just trained on the relative positions so in theory somebody around the same size as me swapping out it would recognize their pose and there's actually a way that I could just normalize all the data so that it would work for anybody's pose potentially so you can train your own pose classifier that are worked generically in a lot of different environments so if you make something with ml5 pose net or with pose net with another environment please share it with me I would love to check it out you can find the code for everything in this video in the link in this video's description I'll see you in the future Koda train ml5