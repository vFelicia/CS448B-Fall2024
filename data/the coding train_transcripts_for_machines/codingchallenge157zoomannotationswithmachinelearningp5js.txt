Hello, everyone, and welcome to a Coding Challenge. The first one since March of 2020, and what I am going to attempt to do today is make my own version of this project. This is not my idea. This came across my Twitter desk the other day from Cameron Hunter, who created a video lens that uses hand gestures to show comic book style messages instead in a video call. I don't know if you noticed in this year of 2020 lots of stuff is happening. Many of us, including myself, are spending a large amount of time in video calls, and I want to see if I can figure out a way using the tools of p5.js, ml5.js, teachable machine, and open broadcast studio, and Zoom as the video conference software of choice, not a sponsor. You could use anyone that you want, but I'm going to attempt to use Zoom and see if I can create my own overlays that are controlled to my own gestures. That's what's going to happen today. I have not attempted this at any point in the past, present, or future. So here we go. To be honest, there's probably not going to be a ton of coding in this. But I will be writing code, and mostly I'm going to be plugging in this piece of software, to this piece of software, and this piece of software. Lots of connectors, and tubes, and cables, and things, but all happening on the computer, because I'm too afraid to use that stuff in real physical life. So let me talk through what the components are. What I'm going to have ultimately is a video feed where, for example, when I raise my hand. And apparently I can't draw. That's me raising my hand. I might have a popup that says, question. A big popup so that the other people in the room can see that I'm raising my hand, and I have a question. So to do this, I'm going to use p5.js to connect to my camera and show my camera feed. Then I'm going to use a teachable machine to train a machine learning model to recognize my particular gestures. So just to point out you should be able to do this yourself at the end of this video, and you won't even have to write any of your own code. You could reuse my code, but almost certainly, well definitely, you will need to train your own model. So I'll talk about how to do that. So once I have that, and then I'm going to talk to the teachable machine model with ml5.js. So all of these components will be running in a web browser. This is all inside the web browser. Then I'm going to use a piece of software called OBS, or Open Broadcast Studio. It is the software I'm using right now to live stream to record to disk. It is software that I use for making YouTube videos. But what I can do with OBS one of the things I can do is I can capture everything that's in the browser. So the browser is going to be sent into OBS, and then this picture boy, this diagram is terrible you will see in OBS. Then from OBS I'm going to start something called a virtual camera. So I'm going to trick the computer into thinking that the open broadcast studio software is my webcam. So when I log into a Zoom meeting, and there is a grid of people. And this is me here. I will select the OBS virtual cam as my video input. So instead of picking the default if I'm on a Mac it's going to say FaceTime camera I'm going to switch to OBS virtual camera. This is not the only way to do this. As you can see in the project, I reference where I got this idea from. It's using something like Snap Lens studio something. I'm sure there are countless other ways you could approach this. I am hoping this is going to work. Again, I haven't tried this yet. So remains to be seen. All right let's do the first part, train the model. So I'm going to go to the teachable machine website. If you want to learn more about teachable machine, I have three videos that I've already made about how to create a model and use it with p5. And you can do stuff with audio. There's lots of stuff there. Google has a lot of resources. It's made by folks at the Google creative lab. I'm just going to click on to get started. I want to do an image project, because I want to train a model on me. I'm just going to do background. Background being me not doing anything, just me just kind of sitting in the meeting like this. Trying to make eye contact with people over a computer, or through a web. Have you ever tried? It doesn't work. How do you make the eye contact? And then the next category will be questioned. So I want to quick webcam. You can see I've already installed, by the way, OBS virtual camera on this computer. So you can see it's already available, but that's not what I want right now. I want the FaceTime camera, and I just want to record myself here. [MUSIC PLAYING] So that's probably good enough. And then I want to do the question. [MUSIC PLAYING] And I try to get about the same amount of samples, like 238. OK, and I'm going to train this model. I get to take a little break here. [MUSIC PLAYING] Remember the don't switch tabs song? (SINGING) Don't switch the tabs. Don't switch the tabs. Oh, it trained. OK, so let's see if it's working. Background, question, background, question. So this is perfect. So now I have a model train. It's very important. I mean even though I've talk about all of this in the teachable machine videos, and I'm trying not to do this now. It's important for me to emphasize this is not doing any kind of gesture recognition. This is pure image classification. So it's not going to work if I gave this model to you, and you're wearing a different shirt, you have different hair, you're not wearing glasses, your background is different. So this is the kind of thing that every person would have to customize, which I think is nice, because you can make up your own gestures and different things. And everybody moves in different ways, and does different types of things. So now export model. So I'm going to upload the model. Now I have URL right here, which I'm going to click Copy. Then I'm going to go over to the ml5 code example. So let's call this Zoom annotations. I don't know if that makes sense, just want to simplify this example. It actually has some extra stuff in it that I don't need, and let's run it. So this is the example that comes with ml5, and actually it's a pretrained model has two categories, daytime or nighttime. So it's hard to make a model that people can universally test and use, but this is one. So now in theory if I were to go and paste the URL. Remember, where did I get that from? Right from here. This sharable link of the model, I'm going to pasted in here. I've pasted it in, and I'm going to run the example again. So this will hopefully be fixed by the time you watch this video. I actually updated the examples. It needs to also have added the to the end of the URL I need to add plusmodel.json. That's what ml5 is looking for that particular JSON file that holds the information about the model. Background, question, background, question. All right, let's get at least a quick pop up of a question mark to appear when I say I have a question. Before I do that, let me make a couple of quick changes. I want to just make this a little bit bigger, and then I'm always going to have the video size match the canvas size. And then I actually don't want to draw the text label. That's not something I'm doing. So let me just make sure this works still, and we see it a little bit bigger. So I should be able to say now, if label equals question, draw something else. And I happen to have a thank you to Jason Haglund, who is that codingtrained illustrator, makes all of the characters. Let's try using this one. So I should be able to upload a file, and drag it here. It's kind of a weird name. Let me just rename this. I'm just going to call it a question.png. And then in the sketch I want to say question, and I'll preload that image. Preload a question.png. Then let us try to say image question 0, 0. So I'm not really being thoughtful about this. Let's just see if it appears. Whoops, what happened? Preload! No, the function name is Load Image. This is me trying to do stuff way too quickly and making weird mistakes. Guess what, folks? I named my labels with a capital Q. [MUSIC PLAYING] Capital first letter. OK. So one thing I really want to do which I feel like is important here is to kind of bounce this a little bit. So I feel like if it's going to appear, I don't want it to disappear immediately. I don't want it to flicker. So I'm just going to always have it fade out. So let me just I think I can use I mean I should make an object. There's all sorts of ways I could engineer this in a really thoughtful way. I'm going to make a variable called fade out equal to 255. And then whenever I detect a particular label I'll say fade out equals to 255. Bare with me. I going to have to think about how to do that with multiple things. But let me just get this to work right now. Fade out minus equals 5. Let's make minus equal 10. If fade out greater than zero, image question. And actually fade out should be zero. So you know what, this is why I need to wrap these two things together into an object or something. But let's just have question fade be at zero. And what I'm really doing is I'm setting question fade to 255 whenever I get that label. And then I'm always fading it out, and I'm only drawing it if I have a value greater than zero. And just for efficiency's sake, I'll also just fade it out when that's greater than zero. So it shouldn't flicker. Oh, sorry. That's actually like just leaving it there. I was actually going to fade it. So I was going to say tint, question, fade, which I realized I debounced it just by not even having it fade but Oh, Hello. That tint applies to everything. Well, that was weird. Maybe I need to do this. So I'm definitely fading. Oh, I'm just fading the brightness. There we go. That's why I was looking to do. So I want the brightness to always be a 255, but I want to fade the alpha. OK, so now let's see if we can get this into a Zoom meeting. Now obviously of course, we could spend hours making little animations and training different gestures. And after I get it all working, I want to spend a little time making a version of this with some animations and different things. But I just want to see if I can at least get this into a Zoom meeting. So the first thing I need to do is open up Open Broadcast Studio. So this is the website for OBS studio. It's open source. And so I already have that on my computer. I'm going to open it up. When you open it up, it should look blank just like this. It's an empty window. So I have a scene. That's what's down here. I can make multiple scenes, but I just want to use the default scene. Now I need a source. Ultimately, what I probably want to do and I'll see if I can get this working later is select a video capture device. I'm going to call this webcam, and I'm going to pick my webcam and have that be the main element of the scene. And then the other stuff would be an overlay over it, which I think I have an idea of how to do. But right now, I'm just going to try to get the raw browser window into this. So it won't be pixel perfect in terms of resolution and everything, but it will be a start. I'm going to take that out, and I'm going to say I think it's window capture. And I want to capture p5 web editor, and I think the best way for me to do this is to go to a share. Let's take the present mode and open a new window. Paste it in there. Let's see if it's working. Great. OK, so now in OBS I want to take the one that says Google Chrome webcam image classification. So I want to take that. Then I'm going to use the Option key to just crop around the part that I want. Again, this is silly. There's some better ways that I could do this probably. But I'm just going to crop around the part that I want, and then I will stretch it out. And now I've got it in OBS. And whenever I raise my hand, it's happening. You can see the frame rate is really low. I'm going to work on a way to fix this. The next thing that I want to do is start the OBS virtual camera. That's an option I have over here under tools, start virtual camera. That is not come by default with OBS studio. It's Dan from the future coming in to let some breaking news. [MUSIC PLAYING] Monitoring breaking news. In the latest version of Open Broadcast Studio, version 26, on Windows only. The virtual camera is built in. It looks like this. There's a button that you can press, start virtual camera. But this won't be available on Mac. Although, probably by the time you're watching this in the future, your future, it might be. The rest of this video will show you how to get the plugin extensions that you need to run a virtual camera, but if you're on Windows running OBS 26 you're good to go. Back to the past. And for Mac, there's this particular OBS virtual cam that you could just download and run the installer right here. And I'll include links to both of these for Mac and Windows in the video's description. And somebody's the chat telling me there's a way to use z4l2sink on Linux. I've started the virtual camera, and now I need to go into a Zoom meeting. Go into the meeting. So normally if I start my video, this is just me in the Zoom meeting with this video. That's the webcam. And now I should be able to go over here and change to OBS virtual camera. Shoot, I forgot that I opened it up here. There we go. Here we go. I'm in the Zoom meeting. Hi, everybody. Oh well this is such a great discussion class thingy. Oh, it's reversed. So because it's flipped, because Zoom is trying to make it a nice experience for me, I'm going to uncheck this mirror my video. And I've already mirrored it in my own software. And now, hi, I'm a question. So I want to think about ways to improve this. This is what I'm going to do. I want to make a 1280 by 720 canvas. I'm going to make the video as small as possible. So let's see, even if I do it like 160 by 120, and I'm going to hide it. I'm not going to hide it. I want to see that video separately, and then I don't want to draw the video. Instead, I just want to see the image pop up. So let's see if this works. So I've got my video at the bottom, and I'm able to get the question mark to pop up. Great. So now I'm going to go to the present to view again. I move the camera around, which I think is causing a bit of a problem. I'm going to have to retrain the model, which I'll do. Let me go back to here. This is working pretty well. And I want to change the background to a pure green, and you'll see why in a second. Now, this should run much faster. I don't need a high resolution video to do the classification, and I'm also not drawing it. So it's much less work for me not to actually draw the video. Just draw this, because now what I can do in open broadcast studio is I can actually just add my camera source directly. So I want to add the FaceTime camera. I'm going to stretch it out to take up the full, and you can see the frame rate is very fluid here. Then I'm going to add as another scene a window capture, and I'm going to get the browser. I want to grab this, which is here. I hit OK. I'm going to stretch this out, but I'm going to crop it around the Canvas. I'm going to fill the window basically with it. Then I'm going to add a filter in OBS. I'm going to go to Filters. I'm going to add an effect filter. I'm going to go to chroma key. And I want to chroma key the green out. The default is going to work really well. Have a perfect chroma key, because I drew the green itself. And now there we go, and I have no frame rate issues. Pops right up. It's time for my Zoom meeting. I'm so excited. It's time to be on Zoom for the 15th hour of the day. OBS I need to make sure I am running my virtual camera. I want to join my test meeting. Now I'm in the meeting. I turn on my video. Hello, friends. Welcome to class today on Zoom. If you have a question, please raise your hand. All right. This works. I'm so glad. This actually works quite well. Let's have a little bit more fun. Here are all the steps you need to do one at a time. First, collect images for any label that you want to define in teachable machine. I'm using question, yes, no, love, and funny, but obviously you can make up your own. And I'm also using one called background for a neutral position when I'm not doing anything, and I don't want to show any annotation. Train the model. Then upload the model. Copy paste the model URL into your code. You can find the code in the link in this video's description. Upload any images and animations you want to use to your p5.js sketch. Add an if statement for each label to either draw one of those images or any code that you want to write really. Once you're done coding, run your p5 sketch in present view. Now you can move on over to broadcast studio. Create a scene in OBS. Add a video capture device and select your camera. Then add window capture, and select the browser window that's running your p5.js sketch. Crop and resize, accordingly. If necessary, add a chroma key filter. Start the open broadcast studio virtual camera. Then move on over to Zoom, and select as your video source OBS virtual camera. Then fame and fortune await you as you entertain your friends and teachers in Zoom. All right. I hope you were able to follow those steps. I cannot wait to see if anybody actually takes this methodology and applies it to a real, live Zoom meeting. I, in fact, did just that. After I recorded this tutorial, I had a quick Zoom call with some friends of the coding training that I sent the steps. I'm going to show that to you right now, and you can sort of see what it looks like in real, live Zoom. During this meeting I thought, Of all these new ideas of things that I could try beyond just loading an image and displaying the image. The first thing I thought of is, OK. I'm programming in p5.js. There's lots of things I could do beyond just load an image and display it. I have the full range of possible things I can encode in p5. One of the things that I think is really fun to try is just any coding challenge I made with any generative art visualization, I can use as an overlay. So I could strike poses to set off heartshaped fireworks. What if every time I wear my party rock glasses, I also get a party rock hat, which follows my head using Poe's net? If you're watching this, I really hope you have your own creative idea for something you want to try, something you've already made in p5 that you want to have running live in real time in a Zoom meeting. Maybe you're visualizing data, it's just some sort of art, it's augmenting your body or your poses. You don't even need to show your video at all. You could just be showing other stuff in your Zoom. Anything you could do in p5.js with Open Broadcast Studio and a virtual camera, you can show in a video call. If you make something, I really want you to share it with me. Probably the easiest way for you to do it is on Twitter, @shiffman, but also if you go to the website page for this video it's a kind of a more permanent place you can submit a link to any documentation of what you've done. One important thing to note, and hopefully you're thinking about this already, is that if you're capturing a Zoom meeting. So for one, maybe just capture it with just you in it, and that's what you share with me. But if there are other participants, make sure you have their consent. You've asked for their permission before you share any images or snapshots of that Zoom meeting online. All right. That's all I've got for today. Thanks for watching this coding challenge. Go and bring some delight to somebody's day. I know you're going to be in a lot of video calls, but if you could take a break from that and safely get some fresh air. I highly recommend that too. And I will see you in a future coding challenge. Goodbye. [MUSIC PLAYING]