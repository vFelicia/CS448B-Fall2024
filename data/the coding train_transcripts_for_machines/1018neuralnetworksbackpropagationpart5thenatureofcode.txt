okay I'm here I am back again oh I'm so close to the end of this now I'm sure there's a lot of mistakes I mean so I made a really an error here which is I didn't even try to run my code and there was a big there's a big typo here already which is that this should be the is this good this might actually even make weights like I think I might not be like being careful about my right these these that's weights plural so that's totally wrong weights plural and this is not this is input to hidden and this is also input to hidden Delta's so that I should have said and I wonder if I made that mistake here like this stock weights that should be weights I got this right so let's try let's try running the code you know I have no idea we put you could put odds on whether there's any errors I would put very things I give myself like fifty to one that there's no errors so let's go and just because if you recall I have in the sketch like I'm setting myself up for a very simple scenario of two inputs two inputs two hidden nodes and two outputs lying 99 typos so I guess I should check there might even be a typo in line 99 oh that's a comment so I don't know what that is that alright alright let's just let's just hit refresh here identify our inputs has already been declared neural network digest line 59 so let's see what that is oh I forgot so I don't oh yeah oh this interesting so this should probably be called input array like I did with feedforward and in that sense this should probably be target array because and then targets equals matrix from array target array so let's just do that so that's important because I'm letting the enduser pass in the inputs and targets as simple arrays and internally in the library I'm converting those two matrix objects targets is not defined a neural network line 71 oh and there so this has to be let target's because it's a new variable here there's no errors okay weird all right so let's move on now this is really tricky I probably could have been much more thoughtful about how I'm doing this I'm but here's the thing I now need to add the Delta BOTS the deltas for the biases now if you've been following along here's how I've here's how I've connected all the stuff I made a video a long while back oh yesteryears of days when it was just a onedimensional y equals MX plus B I made a video about gradient descent where Delta M is the learning rate times error times X and Delta B is just the learning rate times error well this is the analogous situation with matrices learning rate times error this gradient so to speak is this whole thing over here and the X is this thing over here and the same thing here this is the Delta the Delta I mean sorry the gradient which is this times the inputs which is X so do the deltas for the bias YZ is actually not a matrix right the by C's is a one column vector basically I want me one column matrix which is a vector and so the delta bias these is actually just this part this part and this part and guess what I have already calculated those things so if you look at that here oops if I go back to my code right this is before where where is it right here where am i odd gradients right right here I am passing the outputs through the derivative I am multiplying elementwise with the errors and the learning rate this and then I have to do the transpose and get but this is it these gradients I could just say bias sees this dot bias and where am I output dot ad gradients so let me actually will let me actually put this oh where did I go let me put this here so this is adjust the weights by deltas and now adjust the bias by its deltas which is just the gradients okay and then I should be able to do exactly the same thing with down here oops this will come after and what I want to do is adjust the hidden bias with the hidden gradient now one thing that's probably inconsistent maybe I'm just gonna kind of leave it but if anybody as you this is a hoping to put all this code in a repo which is already there she's are called like toy neural network and so one thing I probably should be consistent is when I'm saying like weights versus weight or gradient versus gradients so that can be cleaned up later we just see if I have any errors no errors all right dare I do something next okay so here's the thing I mentioned before now how you collect and prepare your data is so important in terms of the ethics of what you're doing the scientific accuracy of what you're doing and I'm kind of glossing all over that just to make this toy neural network library but beyond just sort of like being thoughtful about collecting your data we've got to figure out like how do I even like do this and so there are a variety of techniques in terms of calculating the error over time and batches and then adjusting the weights versus but I think what I'm going to do it's called stochastic gradient descent let's Google that but let's just make sure I've got the right term stochastic gradient descent is known as incremental this is a iterative method for minimizing object that's written as a symbol so I think I'm correct in that what I'm doing is in other words what I'm gonna do is stochastic gradient descent which I think is what I did with my perceptron example and my linear regression example is that for every single record every single data point I'm gonna pass it in calculate the error back propagated and adjust one at a time instead of doing batches that you're but but be aware of this idea of batches because that's a core concept as you start to use other people's like real actual working deep learning libraries and examples and that sort of thing so I'd let us do this stochastic idea which but one of the reasons why I want to do that I don't know why switched over there one of the reasons why I want to do this stochastic idea is it's basically already what I've done so this training function simply takes a single set of inputs and a single set of outputs targets does all that it needs to do to it adjust everything and finishes off let's do that I don't let's let's try oh my goodness ah so let's let's prepare a dataset I'm gonna do this okay so I like I'm like terrified to erase this I have to let me take a moment and erase this okay so here's the architecture I'm going to use I'm gonna have an input input layer I'm just gonna say layer two inputs I'm gonna have a hidden layer with two to two nodes then I'm going to have an output with just one this is a nice architecture for trying to solve the XOR problem exclusive or I just want the simplest thing just to kind into some way of kind of debugging and validating that's something in my code is doing it correctly so what I'm going to do is so this it's going to look like this and so for my data set width is going to be this following 1 comma 0 gives me a 1 0 comma 1 gives me a 1 1 comma 1 gives me a 0 and 0 comma 0 gives me a 0 so this is the classic non linearly separable problem and I discussed with the perceptron a single perceptron can't do it this is now a multilayered perceptron with graty's assented back propagation in that code so I should be able to continuously feed it this training data set now I said you need a training data set and a test data set but this is like such a simplistic problem there's only two four possibilities and we interesting to look at you know visualizing it and letting these be continuous floating point values but let's look let me just do whatever so I'm gonna put into my code the training data so I'm gonna say I'm gonna say let training data equal and I have it be an array and each element of the array is gonna be an object inputs 0 0 targets 1 so I'm kind of I could put this in a JSON file or a spreadsheet but I'm gonna just do it like this hardcoded in just to make the point right and so now I have own okay so 0 1 1 0 0 0 is 0 0 0 is 1 1 is 0 ok so oh dear oh this should be a ah silly syntax error that I then copy/paste everywhere okay so this is now my training data it is an array with objects so now what I need to do is I am going to say 2 2 1 that's my neural network and I'm going to say let's see what do I need to do for data of training data that's a nice little loop through everything that's in here I'm gonna say neural network trained data inputs data targets and I probably I'm just gonna you know I could pick it randomly I'm gonna go through the data and I'm gonna suit it like I'm gonna do it like a hundred times in the same order which is probably a problem I should probably randomize the order let's just do it this way and see what's going on so let's do that and then I'm going to test things by saying guests equals neural network feedforward zero zero and actually you know I'm just going to do I'm going to say neural network fee for at zero zero print so this should give me everything in the console that I want so this is I haven't been to a mic being careful enough this is the inputs with the target this is the inputs with the target does the inputs the target isn't as a target I'm just going to train it with that like 100 times in the exact same order which is probably a terrible idea with stochastic gradient descent and then I'm just going to call feed for and I think I still have in my matrix library this print function which just like console dot tables the stuff out all right I don't know what could possibly go wrong what could possibly go wrong feedforward print is not a function all right neural network oh you know what it gives me a nice little array I forgot I don't need to do this I could just consult I forgot that the library itself gives me a nice little array so let's just console.log I don't need that print thing so let's try this that doesn't look very good right I should be getting one one zero zero like or close to it let's train it like so let's try training it like 10,000 times hey this is maybe interestingly sort of better one zero zero one one one zero zero so I'm definitely feeding in the all the proper inputs is my training data correct 0 1 gives 1 1 0 is 1 0 0 gives 0 1 1 so I think I really need to randomize the order right I've really got to randomize the order so let's randomize the order and in fact what I'm gonna do forget about even randomizing the order I'm just gonna always pick a random one so I'm just gonna say let data and p5 as a nice function if I just give it random training data it's gonna give me a random one oh the learning rate is something I actually should really be careful about in point one so that's probably something I need to be more thoughtful about so let me so let me do it fifty thousand times and let's see what happens in a random order so interestingly enough so I want to do this as a coding challenge I want to actually write an example that sort of like animates and visualizes it as it's learning I'm saying I have can't believe this worked oh my god I can't believe I arrived I mean I'm sure there's like problems and it's the but at least it worked for this simple problem and but so somebody said I had a typo in the train function well I'm sure I do don't I'm so happy right now though so I need to do coding challenge which is actually the XOR problem and also animated as I'm going so I think I would essentially do the same thing but draw sort of like a pixel space and actually iterate over it and I should see it you know the corners would be the the full boolean that I'll explain this when I do the coding challenge I can't even think anymore just got this to work