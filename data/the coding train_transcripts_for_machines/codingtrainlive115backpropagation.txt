good morning it's Friday time for a usual coding Train livestream I seem a little bit subdued it's because I'm well I'm a little bit worried about today I today is the day that I'm once again this month of January where are we in January we'll pass the middle is I'm dedicating this month of January to attempting to work through I'm starting looking at the chat to work through building a neural simple neural network library from scratch in JavaScript I spent some time working on some matrix math stuff building a simple feet working through the feedforward algorithm and today is the day to tackle a topic one particular technique for training the neural network for having the neural network learn and that technique is called it's not called quaternion but it is that it is something that does often caused me to run out of the room in fear and it's called back propagation now a couple things I want to mention number one is eventually I will be replacing a lot of the internals of this library that I'm building with a more sophisticated engine that's been highly optimized to do matrix math and also has a lot of libraries a lot of classes and objects and library functions for doing deep learning stuff called deep learning is so that's what that hopefully by the time we get into February that's where my focus will lie in looking at some more sophisticated models of deep learning systems to kind of do fun and creative stuff in the browser but I'm still I I want to use this time together in this live stream to kind of dive into the guts of how these systems work see what happens if we can not worry about optimizing or efficiency and just kind of build the pieces and parts and put them together get something to work so that we can start to understand how the algorithms work have a language a common language to speak to each other about about the pieces of these algorithms so we can ask the right questions later when we start to do projects and other types of things how's this sounding okay so I have done something somewhat shocking which is that I've prepared notes on that word note notes the plural of note meaning more than one note and not just more than one note or pages of notes page 1 page 2 page 3 page 4 page 5 and then even some printouts of some code that I wrote at one point earlier in my life with some mistakes in it that some of the code for the back propagation algorithm oh there's even yet another page on the pad that I didn't rip off so another page of the notes I don't know how much these notes are gonna help me I want to say that once again I should thank and I'm gonna do this multiple times today but some of my a lot of my knowledge has come from this book make your own neural network you can check the coding train Amazon shop for a link to purchase this book and other books that I recommend are used in preparation for a lot of my videos and so this is extraordinarily helpful I also want to mention but no written order book this I'm gonna need to refer to this let's put it over here I also want to mention three blue one brown and I've been watching the in particular let me pull this up here three blue one brown I have been watching the playlist let's go to playlists well there's a couple here but I've been watching this playlist on neural networks and sexually I talked a little about how I'm doing this fundraising and training for a half marathon actually watched well I did my training run at the gym because the weather is terrible here in New York watch videos three and four like a chapter three independent like multiple times a treadmill on or like a little iphone so it was probably not the most optimal way to experiencing this video especially when I felt like I wanted to pause to like contemplate for a second and then like you know bouncing around fortunately no one was harmed in the watching of said videos on a treadmill but these videos are extraordinary they even more so than this what is a neural network a video this gives you a really nice intuitive understanding through diagrams and all sorts of stuff about the backpropagation algorithm in you will also I would highly recommend if you want to dig even deeper into this the well the essence of linear algebra series is perfect for a lot of the matrix math stuff but in particular this essence of calculus playlist and in particular I'm looking for something about the CHEO a chain product rule the chain and product rule which are necessary for the backpropagation so I'm really struggling as to what to do here because there's no way that I am going to cover in depth what's here in three blue one Browns channel there is probably no way that I'm going to cover with the same level of depth what's in to make your own neural network book and by the way I should also mention I was looking at some of this code yesterday it looks extremely helpful but I haven't I haven't had the chance to dive really deep into it but oh yeah this to the this is a free online book by michael nielsen that looks like three blue one Brown used as a reference and if we could go to and so how the backpropagation algorithm works oh boy how about I just do a dramatic reading of this page instead of trying to do it myself so this I actually should I wish I'd spent some more time with this page because I think this would be extremely helpful but I don't think that I'm going to go as indepth as any of this content today we did that work could I actually like feed them through my ear so this is my plan this is my thinking as of right now play a lot of scary music so that I don't appear too serious on talking about back propagation this is my plan I am going to talk through how the algorithm works in generalities as it relates as it relates to this diagram that I went through in somewhat detail about the feedforward algorithm so what does it mean to suddenly have an error here and then correct these weights which are tied to the error and then propagate backwards those errors to adjust these weights and in a many more layer system just keep propagating back and back and back and back so I want to talk about that JIT in generalities then I want to look at how the weights are know how the error is kind of like portioned out and how that can be expressed with matrix math because I'm going to need that from our library and then I'm going to refer back to my videos on gradient descent and ultra justing weights based on gradient descent and the three blue one Brown videos just present the formulas that are sort of at the end of those videos and implement those in code so I don't think I'm going to derive all of the pieces of the back propagation algorithm today or ever so definitely because I would like to I would like like to turn to that like put in some filler videos I do think that other people are better equipped to make that content than I am so I'm gonna try to stick to where hopefully I can be of most help which is kind of figuring out how to implement the formulas in code rather than prove or derive them but I it's important to at least have a discussion of the understanding of those formulas this is my thinking I'm just kind of talking it through and then I can hopefully do some coding challenges because once I have the library built I can say like hey let's train it on this let's try to use let's do a coding challenge use this library on this particular problem so that'll be flightless like difference in some of my coding challenges of the past where there was sort of no code maybe there's a p5 library of course but we're build these quick quick machine learning examples using the library and then if people want to refer back to how the library was built they can do that okay so that's my thinking what what does everybody think about this I feel like I'm a newscaster now with my notes oh that was all the spooky piano I have thought that was a much longer effect okay so how are you fall feeling about this how are we feeling about this Marin Marin how did i do today Marin in the chat says sounds like a solid plan look what could possibly go wrong what could possibly go wrong me trying to explain what what do you I've actually learned which is kind of interesting and I might as well plug this while I'm here is I'm participating in this conference put on by the processing foundation and school for poetic computation what can i maybe called if I do I meant to type in SF PC learning to teach so I'm doing a short presentation at this at this conference this is it no that's from last year last year yeah 2018 so one thing I that I'm discovering is when I make mistakes in code and have to figure out and debug them that seems to be a useful technique for education meaning people watching they're like oh okay I make those mistakes oh I'm seeing how you figure it out well this is helpful to me or this is interesting compelling and I'm learning something whereas when I spend an hour trying to explain some math formula but have the formula wrong the whole time that's not so helpful so it's not like oh yes I'm seeing your process of how you get it wrong for a while and then you realize that later and kind of have to do it so what I'm discovering is a lot of these these sort of mathematical or algorithmic discussions especially if they have like complex notation require something called I believe it's called preparation okay I have here in the form of notice so I'm a trying trying that but most likely I have to admit I wouldn't be surprised if I just end up redoing trying to do I need a couple tries with this so I probably should do those tries not livestreaming in front of a live audience on YouTube but I don't when else am I going to do it right now so I thank you for being guinea pigs and and this is actually what happened most recently if I go to my channel and go to yeah this particular video this particular tutorial was made on Tuesday of this week though if you look at the live stream archives there's a live stream from the last Friday oh we could go today when I went through this exact algorithm but I made so many mistakes it wasn't worth trying to edit that together in some way that made sense it just made more sense to just do it again so with this gradient descent stuff I'm just kind of warfare warning in advance I'm kind of in my mind thinking of today as let's just give this a try and see how it goes now the other thing I should mention is Simon okay I will Simon is telling me that if I watch his livestream I'll learn better how to pronounce mitad okay now the other thing I should mention is I was going through all this and looking at it looking at it and I kind of have the formulas and has some code I think for all of the weight adjustments that I need to do but of course I kind of forgot about the biased part so look at that yet so let's see if we could do put the back propagation formula stuff in for adjusting the weights and then if any of you who know what you're doing more than me which is probably a lot of you maybe you can help me figure out how to put the bias stuff in okay and in regards to the camera that goes to black I will just mention to the people are talking about in the chat I'm gonna fix that soon you know sometime in the next year whose I the what the solution to that is to put this firmware on the cameras called magic lantern so I just have to basically like you know it's kind of like should I start live streaming on time and have two hours or should I try to fix this camera thing for an hour only have an hour left and school the way it works out so I'm not optimizing for fixing it but I will I will okay now as you know in the coding train especially when attempting things like back propagation it's always good to stay hydrated hello Brazil it's amazing people in Brazil are watching alright now let me let me compile some resources here it's already getting warm in this room okay so I want this playlist available to me and I want I guess I should verify yeah I guess I should get one of these check marks on my channel huh I want I also want this playlist available to me so this is I want to reference neural networks essence of calculus neural networks and deep learning free book and the coding trade Amazon shop which has this book here that now my own videos the ones that I think this relates to our this particular video Hey look somehow my subtitles are a closed captioning on and they are on in German it looks like who wrote these I put discovered recently that I can so as it says who caption the videos here okay here could be another video from Suraj has a lot of videos about this topic as well that are terrific then I also want the perceptron which is basically you know talked about a bunch of stuff multiple time either pity hood because this is this kind of like a video about the feedforward algorithm that's interesting fifteen minutes I forgot about this what's in these videos I think I made like duplicate content thought let's just kind of scan through this real quick oh I think this is me kind of like sketching out what I need own I stopped at the matrix where I need matrix math oh yeah so I did do this kind of twice there's the feedforward algorithm in a previous video that's so interesting so okay that's fine so what I'm looking for though is this one and maybe somewhere in here the training function is where I want to kind of pause for a second because it's basically the same code where do I have a nice if someone could find me a time code in this video where I have a lifestyle of the Train function I guess I can actually just open up the code on my computer might be more efficient sorry everybody I'm just like trying to get organized here perceptron sketchbook sir anybody see a perceptron probably coding challenge well maybe I should go to I should really talk about the new website stuff too but it's too many things coding challenges perceptron huh didn't I do it as a coding challenge why is it not numbered that's a mistake hold on what video is that oh that's not a coding challenge it's actually like a tutorial somehow it's here though maybe let's see no oh this is different yeah this this is kind of relevant here alright so that I can reference if I need to okay part one I wonder why why what's that someone give me the a too long didn't read answer why why it's two million views why why all right oh I really wish that the climate control system in this room could be fixed really make things better for me 1130 okay I got this we got this got this this this okay there's also like mathematics of gradient descent by the way just if I go click on videos that's nuts something is wrong with the universe is if my video shows up first that is a really a problem because I expect a lot of these other ones are much better but it prize to do with the fact that I've titled it that way math so if I just search for gradient descent mathematics yeah okay how about gradient descent algorithm just gradient descent there we go okay good good that's good oh I'm down there that's fine I'm happy with that look at these other ones first okay so this is actually this is actually the video that's most relevant all right okay Google knows it's me and so they're putting my videos up there cuz I watch them a lot that's the problem so but I'm not am I signed into Google hold on they probably even know they just know even if it's an incognito window I don't know I don't know Google I don't know Google Google just knows still does incognito window is like they just know right all right the Delta rule oh you should do a video on the Delta rule learning algorithm for single layer neural networks I think I did that already right though isn't that kind of like what my perceptron video is basically the Delta rule ah Tushar in the chat saying I have a question on your own the last video okay let me take a couple questions I'll do anything you think your procrastinate from trying to talk about gradient descent in a video so I'll take your question you guys still here like a weird okay hold on um do you guys hear our weird like echo like the timing of when you hear the sound that's being piped through the system versus the sound or my microphone is off like if I do this oh sure I wasn't looking at the chat I'm sure that question was asked I missed it which is a reason for me and maybe plug the patron for a second I do have a slack channel that I it'll be easier to keep an eye on and wear a nice friendly community of folks discuss different things that I'm doing the videos and provide some financial support for the channel so if you go to patreon.com/scishow de trein you'll get an invitation to the slack channel okay all right there is no echo all right all right all right all right let's we got is we've got to get this started with let's let's get rid of some let's let's let's catch up to where we are let's desktop let me get this going here look coops localhost there we go this is where we last left off with a feedforward neural network that will give me a random output now we want to put the back propagation algorithm into it if I open up atom I should have my code let me open up this will be another useful reference that I should bring in here and I think now I've got everything I want to talk about did I make the joke last time oh I think I did it wasn't it wasn't very good it wasn't very good though sorry I you know one thing that I shouldn't that I've also learned to do is sometimes I'm responding the stuff in the chat but not saying what it is which is very confusing if you're watching and you're not even seeing this chat so I'll try to be better about that Taryn asks why don't you prefer visual studio code maybe I do I'm just not as used to using it I keep meaning to set it up but I kind of have all my settings and everything and Adam vaguely the way I like them although I really should turn off all the autocomplete stuff they might know how to do that alright I think I've got to get going and I think we've got to start this like it's got a notification the coding train is live maybe I should go watch that that's interesting let's read some random numbers this will this will get me a little warmed up here Oh getting too old for this YouTube thing forty six thousand seven hundred and thirty seven eighty eight thousand and twenty seven thirty five thousand four hundred and ninety seven twenty two thousand eight hundred and sixty three sixty five thousand and eleven twelve thousand two hundred twentyeight sixty six thousand four hundred and thirty eleven thousand eight hundred and twentyfive 835 eighty one thousand eight hundred and thirty eight ninety four thousand four hundred and thirty one sixty three thousand nine hundred ninety five ninety one thousand one hundred and seventy one eighteen thousand two hundred and seventy three fortyseven thousand seven hundred and thirty seven hundred eighty four thirty nine thousand two hundred and eight thirty four thousand two hundred sixtyeight eighteen thousand ninety three seventeen thousand okay you know one thing that I should probably do really quickly in case in case you need to make a website where I read any possible random number will give you some footage for that real quick I'm kind of working on this on my own anyway but zero which I will be looking at the camera price you look at the camera zero one two three four five six seven eight nine 10 11 12 13 14 15 16 17 18 19 20 30 40 50 see 70 80 90 100 1 200 300 400 500 600 700 800 900 thousand yes 1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000 9,000 10,000 I think you could put together though the one two three four five six seven eight nine and the word thousand thousand and you could put together twenty thousand yeah one hundred thousand so I think I think you have enough right because that you can you've got one hundred one hundred thousand nobody one hundred ten thousand one hundred and ten no you need the and one hundred and ten thousand one hundred and no but these don't go up that high they only go up tonight that ninety nine ninety nine so you can get even ninety nine thousand one hundred thirtyfive yeah you're fine I think you have enough right you could probably make a bunch of video clips have a random where you picked and have me speak that random number right the and part means you want you want million all right fine million billion trillion all right whatever I missed I'll get you later am I being really quiet I didn't want the volume to be peaking too much okay glad that we worked it out oh oh I need my notes for sure like seriously I'll do anything I'll probably like agree to do you know that double pendulum simulation or something now he'll agree to do just about anything to avoid this backpropagation algorithm but we're gonna do it now all right all right Adam I need to command the settings and click packages search search for autocomplete um I guess I could so what do I want sometimes I like the autocomplete though which of these do I want to turn off I don't know now I'm afraid to disable it what if I've been using it all along and I didn't know that I've been using it well we'll leave then now we know where to look for this all right can anybody come up with any reason right now why shouldn't get started on the backpropagation thing okay oh yeah shoot nobody has a point what do you need two points for though is there's gonna be continuity errors point maybe you just need the audio though maybe you're just gonna use the audio which is fine all right I'll come back later at the end I will take your reading random numbers requests this stippling challenge re didn't I oh yeah the stippling challenge right all right all right here we go just need them house okay this cameras off I should yeah I know so many I'll do the reading I'll do that a sec I'll do some reading all the possible different numbers thing later I'll configure it in a way that's better just hold on let's let's let's let's actually do some let's try to get through this so my goal right now is let's see if I can get the backpropagation algorithm into the library by 1230 which is like 45 minutes or so and that gives me a half an hour slash an hour to try to at least solve maybe X or as a coding challenge that's my goal for yeah I need to like slow down here I'm gonna get to all this stuff eventually I'm just doing my best I got lots of videos you go back and watch those my hair is really really a mess today I wore a nice shirt for all of you I thought that might make the Grady the whole like backpropagation gradient descent thing a little bit less weird like it look like I know what I'm doing alright my glasses are very dirty really like I just I'm gonna start mmm eat me me me me me me me me me me me me me me me me me me okay here we are this video.i what I'm going to do with this video is I'm quite terrified at this moment I should say but I'm gonna start to talk about what it means to examine the output we fed in some input we did the feedforward algorithm we got some output I want to know look at that output and I want to say what do I think about that output really isn't good output is a bad output is it right on it's a little bit off and I'm going to this is a technique known as supervised learning I the teacher I'm now gonna say this output is incorrect please adjust all your settings to make this output more correct and I have done this before I have done this in a video about linear regression and gradient descent I have done this in a video where I made a simple perceptron where I did this same type of learning algorithm in fact I have done this in genetic algorithm examples or I'm not using the same exact technique but a different technique to sort of teach a system to do something so this is what I want to do you can think of all of these weights that are inside the network as these little knobs little settings and I just want to like always adjust the settings and so the key there's gonna be key terms that are gonna come up here like error what's the difference between the output and the known correct answer that's the error the cost well over a large training data set what's the sort of cumulative error that's the sort of cost the current cost of the neural network and then this term that's called back propagation so if the error is the thing that tells me how to tune these weights the error is right here it's gonna tell me how to tune these weights how do I tune these weights they're not connected to the error they're connected to the thing that's connected to the error so I need to propagate backwards feedforward is the process of moving all the data for through the network back propagation is the process of taking the error and basically like feeding backward to the error through the network now here's what we have here's where I have got to admit something this is probably I would say I'm trying to think of a topic that I've tackled in any my videos that's like harder this I can't think of one I don't know that I fully have a deep understanding of this I have implemented it before I spent a lot of time I prepared some notes I prepared some notes for the first time in my life which are right here that I'm going to use while I start to explain this but it's kind of unrealistic and I'm probably not the best person to go into all of the math so what I'm going to attempt to do is give a kind of just general overview of how the algorithm works look at how pieces of it actually work the math of it in particular as it relates to matrix matrices because I'm gonna need that understanding to implement it in code and then I'm gonna present some of the formulas to you that are the formulas for how you change the weights based on the error and then try to implement those formulas in code I'm sorry I put in a this is I I guess just probably be edited out but the permian should just stay in but I put in a order to try to fix the temperature in this room and I'm worried that something like someone's gonna just enter the room Fred fix the thermostat so I thought I heard like keys jangling I got worried so anyway okay so so this is my plan so probably take two or three videos but so my goal is really the implementation and I'm gonna provide to you a bunch of resources if you want to dive deeper into the math let me just mention those to you so number one make your own neural network by Tariq Rasheed this is a book that I was reading on the subway this morning you can actually get it on the kit on your Kindle app for a very inexpensive amount you can also find it here on an a lot books I Remmick recommend on my coding train amazon.com sheet slash shops mask coding training the three blue one brown video series what is a neural network what is back propagation really doing back propagation calculus you could pause right now and go watch these videos which will give you a deeper set of knowledge about the math that's that I'm going to use as well as this the essence of calculus right one of the things that's used in the math is the chain rule and the product rule so this is this this particular video might be useful to you as well as this particular online book which I found through the three blue one Brown videos neural networks and deep learning and I'd be remiss if I didn't also mention Suraj's YouTube channel that I've mentioned a lot on this video has a lot of different a lot of a lot of my videos a lot of different videos on similar content especially if you're interested in Python and using the tensorflow library and that kind of stuff okay so that's that now let me start I think I think we're ready to start timeout for a second alright so that was my brief little introduction and the next thing I'm going to do is I need to do some erasing so let's take a oh please do this no no I definitely no applauses I just want to have a freezeframe of this white board because I'm probably gonna erase it what do I want to keep I think in the end I think I just want to erase this I think I'm gonna erase this and start over okay so if you talk amongst yourselves I'm gonna erase the whiteboard this is just water in case you're wondering helps it erase a little faster this is whiteboard paint so the whiteboard is actually painted on to the wall which is kind of wonderful but there's a bit of a problem number one is it's very Glary so I can't it's very hard for me to figure out how to like this room and properly because if there's a lot of glare on the whiteboard and then also it's very hard to clean it's just so you can see here can you see how dirty it is I nothing would make me happier in life than to have a perfectly pristine white whiteboard that's totally clean this is a start so if I need to refer back to that notation okay let me check our various chats as they're going and alright I don't see anything like any emergency messages we get my notes we rightclick my notes I don't have a place for night he's like a little podium for notes let me just look at them for a second here okay this is where I'm starting okay okay okay God all right I've erased the whiteboard and I am now ready to start talking about the backpropagation algorithm so let's assume right now that this is my output neuron and just for the sake of simplicity at this moment let's just pretend this is one of the hidden neurons but let's just pretend that there's just one of them so there is some wait there's a also a bias but we'll come and come back to the bias at the very end so I'm gonna kind of do everything without the bias and then come back to the bias there's some weight which is connecting this hidden neuron to the output neuron now the input of the output of this neuron multiplied by the way to center here pasture the activation function and we get some sort of answer let's say that the answer that we get is point seven so this that can be referred to as the guess the output but I'm gonna call that I'm gonna call it the guess am i standing on the white board I'm dangerously close I erased might just pause for a second here let me let me put a little mark up here so really output is the very top all right point seven is the guest now in the road to I again one more time wait wait wait I'm seeing crazy stuff in the chat no all right that's fine that's nonsense that if something anything to do with me sometimes I think if the chat is being blasted with emojis it's like get my to get my attention but I'm now I'm only gonna soon that's ever the case if it's the slack Channel point seven is the guess now in the case of supervised learning where I have a prepared data set where I have sort of like known answers so I'm going to train the network to have these weights so that later on I can put in unknown data to get good results I would have some sort of answer and that answer could all that goes wrong today though I'm gonna be really happy I'm gonna have some sort of answer look I really like pushing the boundaries I don't know why I should just lower everything a little bit sorry machi I'm making this like annoying to edit for like no reason I would have some sort of answer known answer so I'm going to write that here and I'm gonna say the known answer is one I wanted this neuron with this particular input that came in I wanted to see the answer of one so this means now I also have an error and the error is calculated with a simple formula what is the desired output the answer the guess what's the difference between those two things so we can see is that the error is 0.3 so in my simple perceptron and in other videos that I've made I've then taken this error and used it as a way to basically nudgenudge that weights I wanted a 1 I only got point 7 I can make the white a little bit higher to get more stuff right I want more and look more stuff right I want that weight higher maybe the bias needs to be addressed but 0.7 I just need to increase that way to increase the weight in the direction of the error that's how this works now here's the next piece of this let's say however that instead of just one weight coming in here I have two weights because there are two hidden neurons H 1 and H 2 now we have a problem we have this error which I know I need to nudge weight 1 and weight but which one's really kind of responsible for the air there's a lot of blame placement going on here normally I would think like let's try not to blame anybody but this is the problem and I could just say like I don't know half of an error increase them both increase they're both the same amount but there's a key aspect of the way that the learning process works with gradient descent and back propagation is we really need to figure out who's responsible for the error so let me take the scenario these weights could actually be weights where this weight is zero point two and this weight is there point one well you could be made set we could now make the argument that this connection is more responsible for the error because it has a higher weight in fact it's twothirds responsible right this weight is double of this weight so in fact when we do these ninjas we take this error to nudge this one but we'll nudge it only by 33% and then we'll take this error and I know this is going to go out of your view but it's coming all the way back here and we'll judge that by an up let's see can you see this here 67% actually I just do it back here 67% so this is a key aspect of the bout and this is I've basically done this before so this is where we look for this Delta weight we adjust the weight based on that error and the inputs passing through okay so now little bit of pause for a second how's this going I'm getting good tips on how to fix my whiteboard issue this camera went off so I might as well fix that while I'm here all right no one's saying anything like is horribly wrong so I'm gonna keep going let me check my notes what was the next thing so step three is okay step three is adding the layer here okay but so so maybe this makes sense to you and if it does good here's the tricky aspect this is why there's something this is why this video is essentially about back propagation this is not the diagram of the neural network that I created in the previous videos in fact this hidden layer is connected to the inputs and all of those have weights and I I spent a lot of time worrying about the indices let's get those indices correct so this if this is input 1 and this is input 2 we usually call this sorry I usually call those X and this by the way the output we can refer to as Y it's usually what's often referred to as Y so if these are this is input 1 X 1 input 2 X 2 this weight is the weight from 1 to 1 this weight is the weight from 1 to 2 now that might look backwards to you it's the weight from 1 to 2 or kids connected between hidden to and in what but the reason why I'm numbering it that way is this is that it's going to be in the second row of our matrix this weight is the second row of our matrix and this is a weight from 2 to 1 so I write 1 2 and this is a weight from 2 to 2 so I write 2 2 so here's the thing if this error which is happening right here is used to tune these two weights proportionally based on their weights well how do I connect this error back to these weights how do they get tuned well it what I need just this is a what was Inc this to realize is this is like a little section of what could possibly be a much larger neural network right there could be many more layers this way and many more layers that way so these weights are just tuned based on whatever this neurons error is right here is at the end so I can actually calculate the error directly but here they're not connected directly to this error that sorry here they're not connected they're connected to these so what is the error coming out of here if I just had that if I had like hidden efore error error hidden 1 what is that equal if I and I knew if I knew what this was right if I knew the error coming out of here error hidden 1 then I could adjust these what because the air coming out of here adjust these weights this error just these weights and this error e to error hidden if I knew what that is then I could adjust the weights coming in to that so this is the idea of back propagation there's an error here it goes to here how many no but if I could calculate these errors then I could continue to go back here and then if there were more I would just calculate these errors and keep going this way so this is the real question how do I calculate that hidden air how do I calculate the error of a neuron anywhere within the network that's not necessarily directly connected to the output where that error is just a simple target out guess and it pause for a second okay I think this is all still making sense I'm gonna look at my notes I am now finished with my first page of notes thank you very much maybe I'm not going to need a second try I'll never need that page of notes again I look forward to the time where I forgotten how all this works again because it's like years later but I have this nice video that explains it I can go back to watch it all right all right okay so the way that I want to do this yeah yeah yeah all right I'm thinking about this sorry I'm Oh II won okay yeah yeah all right all right okay all right I'm gonna to figure this out I'm gonna pull just the section of this diagram out over here I'm gonna need to progressively over time make this diagram more and more complicated but right now I'm going to simplify for a second again so what I want to do is I just want to take actually it's sort of here already but I just want I don't want to pull it over here so this is the output and these are the two hiddens this is the weight of this is the weight of 0.2 and this is the weight of 0.1 and the error here is equal to 0.3 so what I want to and this is the hidden layer what remember what I'm trying to you know h1 and h2 what I'm trying to calculate is the error of hidden one and the error of hidden okay so the way that I do that is by taking a taking this error and getting a portion of it what portion should I get and I sort of said this already twothirds and onethird so if this is weight one and this is weight right there's a very simple scenario this is almost like back to that perceptron again then what I want to do is say the error of hidden one is weight 1 divided by weight 1 plus weight 2 times the error of the output e output the error of hidden one is a portion of that outputs error the error of hidden two is weight 2 divided by weight 1 plus weight 2 times that output error again we sort of said this already a realized I'm cut but this is fine it doesn't hurt to try to do this twice so these would actually be the errors here these are now the errors and I can just use the same gradient descent algorithm to to these weights but this actually is kind of a rare circumstance where you know everything's just connected to one thing so what I want to do now is I'm gonna make this diagram a little bit more complex and I'm gonna take the case of having more than one output neuron see how this works I'm gonna add another so let's say there's two outputs which means there's error one and error two right maybe this is trying to recognize a true we could actually recognize like true or false or something to recognize a zero or a one so in that case maybe the desired answer is you know it's is one and zero but we got we got point seven and point three is the outputs so here this error I made this to let's make this point for this error would be negative zero point four right and now what I need to do is have more connections here and once again this is and I'm this is for this matrix I've got kind of the same notation 1 1 2 1 1 2 2 2 ok now look at this now error 1 is one thing what's the error coming out of hidden error 1 sorry hidden error one is base so pause for second I got it in my head but I'm just gonna take a break shouldn't the air be squared yes yeah when I get to the yes right now I'm not worried about that it was asked in the chat room the air be squared when I calculate this sort of cost the total cost we're gonna square it yes you do square the error but and this case I'm just trying to look at how this gets divided up so if this is our diagram now with two outputs so there are two errors known errors that we can calculate in our supervised learning system we need to figure out still figure out hidden error one hidden air or two well this still stands right this is still a part of the error coming out of here it's the part that it's the part of that it's responsible for it's how much is it responsible for 0.3 and so in that case this should be air output 1 and I'm actually just going to if it's an error on the output I'm just gonna say e 1 so this is and this now is this weight let me let me let this weights portion and and this weights Portland portion right whoa sorry oh shoot try to click undo in my head I was just like undo I'm gonna look at my notes here I want to make sure I'm using being consistent about my 1 1 and 1/2 yeah I am that's what I that's what I've been using 1 1 and 1/2 right yeah okay sorry let me do this again this error coming out of hidden neuron 1 is the same as it was before it's still the portion of the error based on this weight in this way how much of like how much are we contributing to that point 3 air well this this one has a certain percentage in this one has a certain percentage and that is weight 1 1 divided by 1 1 plus 1/2 times that error now here's the thing that's just how much it's contributing to air 1 how much is it contributing to air we've got to calculate that as well and that is the portion of these two and I think you must make sure I have a lot of space so how much of weight 2 1 / weight to 1 plus weight to 2 times air 2 so if their multiplicity total cumulative error of hidden neuron 1 it's that can it's it's its portion of the connection from it to error 1 sum with all the other connections to air 1 and the portion of its connection to air 2 some with all of the connections to air 2 and I'm kind of it's that connected to air it's connected to the output but the output is producing that error so this is it's some right now I could do the same thing for this error it's it's portion that it contributes to this error which is now wait 1 2 right you can see that these are the same right this is its portion weight its that's where it's connected this is its portion that's where it's connected right it's it's its percentage portion of the total weights it's all connected to output 1 and then how is it connected to output wait its portion of the total of air of the error for output so this is now how those errors are computed so again if the mathematics of gradient descent tells us how to take an error to nudge weights then we calculated this error now we can we can take the error is coming out of the output these are our formulas for calculating the errors coming out of the hidden and those errors are the things that could then with gradient descent tell us how to nudge these weights then if we had more layers we could calculate these errors and keep going back that is back propagation that is how the hidden errors are calculated so this is the end of this sort of part one of just sort of like the basic idea of back propagation I'm going to check to see if there any question which I will answer at the beginning of the next video I'm gonna check to see if they're already questions or Corrections which I will mention at the beginning of the next video where I will also implement at least just this much in the code alright alright I'm not seeing any like thing that I got horribly wrong I actually think that might be fine it did prepare but the longer the more we get through today the less preparation I have that's the problem like I was preparing but then I kind of like ran out of time like I gave myself like an hour this poor to prepare only reading what how but back propagation is and I did watch those videos yesterday from 3 Brouillette brown but it's kind of you know I went to sleep everything i'lli'lli'll I this way while I sleep in all the knowledge it just leaks out if you look at the pillow in the morning just like all these like math symbols there it's amazing because just like put that back in so what I want to do oh I forgot I forgot actually there was more to this what I meant to do is like actually I meant to turn this into matrix math but that's fine I'll do that in the next video ok this is going great I never heard such feedback before amazing ok just wait to the power I get like here's a formula I forget it let's just implement it okay all right so all right all right okay I'm gonna keep going all right so in the previous video I went over how to calculate the error in a supervised learning system right the error is just the known answer the system's guess then what I did is talk about well how can I take that error and move it backwards through the system feedback words instead of feeding the data forward to the network feed the error backwards and the way that I can do that is by looking at like JIT each of these contribute to that error let's kind of like make this error like a portion of that error and these were the formulas that I went through right look at this this neuron hidden neuron is connected to this output this is the weight how much is this way two percentage of all the other neurons that are connected it's just two in this case so it's just the sum of these two but there could be many more so what I want to do in the code now is actually add this training function where I send in input data to the network with a known answer and have the training function calculate the error so calculate this basically I want to calculate this vector e1 e2 and I sent vector but this matrix one column matrix that's what I want to calculate that's going to be a really easy part so let's do that first alright so I actually I say in a previous video I sort of set up this function I think right this is this is the feedforward function where I just take the inputs and I feed them forwards and return the output here is a function where I take the inputs and get an answer now ultimately I should just be able to use the feedforward function right I might have to tweak this but on the one hand I should just be able to say I should just be I'll say let output equal this dot feedforward inputs no reason why I should be able to do that and and now what I need to do is calculate the error so the error is Error error equals the target so answer maybe I should call this target targets I'll call it target it might be plural right there these are the target output that I want targets outputs so let's call this outputs again the first example that I'm gonna build is gonna have just one output but I want the system to work for as many outputs as there are so in theory I should be able say let error equal math dot subtract targets comma outputs now this is where we get a little bit into the weeds here if I'm not math matrix I didn't even know if I've implemented this function in my matrix library so that's what I need to go and check but this is where we're kind of like oh my goodness if we're only we're using numpy we just use and it just works or maybe there's some other highly optimized JavaScript matrix math library that will do this for us I'm gonna get to all that yes deep learning is is coming but I'm right now I just want to do this in my code now here there's a little bit of awkwardness here this is an array so this comes out as an array I actually want that to be a matrix so it's a little bit awkward but I'm gonna say outputs equal outputs dot Oh matrix from array outputs so this is me convert convert array to matrix object and then the targets I probably the users gonna send that in as an array so I'm also going to say targets equals matrix from array targets so this is a bit of awkwardness because of the way that I develop my library that probably someday in the future I want to like refactor or rethink but I need to have those things be matrix objects for me it will do this matrix subtraction now let's check the let's check the matrix library I believe there's no subtract function there is an add function which adds to the alright so this is so silly the way that I'm doing this because I probably in a real world if I were really big thoughtful I would make a comprehensive matrix library that has every possibility that I'm just gonna put in there what I need so what I need is a static subtract function and I'm gonna have another I'm gonna call it other for the other matrix oh no no matrix a and matrix B so this should return a new matrix a minus B so a couple things one is if the rows and columns aren't the same I mean I want to do this element wise right basically if you've forgotten what's going on here basically I have I have a matrix like this which has these guesses 0.7 0.4 and I want to and I have the known answers 1 0 and I want to take this matrix minus this matrix to give me one that has 0.3 negative point 4 so let me add a function that does that it's too bad I didn't have it from before and I should do some error checking here like check if the rows and columns are the same I'll put that in later I'll put that in later got to kind of move along here some that'll be in there maybe when you look at the code someday but I'm just gonna say I'm gonna do a first I need to make a result which is a new matrix which is a new matrix that has the same number of rows and columns as either of them I think it call it rows and columns right so I need to make a new matrix as the same amount and then I'm gonna use my very subtle silly loop function that I use over and over again to loop over all the elements and just say the result dot data index I which is the row index J equals a data index I A B data index I index J right so this is me just going through and subtracting everything from one to the other and then I can return this result all right so I had this static subtract function is it silly to have a subtract function when I use of an odd function that then you know a die could be used for subtract by just saying like multiply the whole matrix by negative one but whatever I'm just doing I'm doing it as I'm doing it will refactor later all right so now let's think about this I'm going to comment this out and I'm going to say these are my inputs now I'm gonna have my targets and in this case my neural network is two to one so I'm gonna keep that my target is just one I want to get one now what I'm going to do is I want to say neural network instead of feedforward I want to say train with these inputs and these targets so let's just run the code and see if I get any errors then I'll debug the actual output so let's go to the browser where I've kind of got this code runs output is not defined sketch dot J S line 11 oh there is no output now hey no errors thank you I'm done I forget about the thermal Network stuff oh yeah I am more to do I'm afraid to keep going but I'm gonna keep going let's just look in the training function and console.log the inputs the targets oh let's just console.log the air actually I just do arrow dot print sorry so I can say outputs dot print because I have this print function and targets targets dot print and error dot print so I just want to look I just want to examine all these things are I got an error oh error dot print all right so okay something went wrong so this is what did I this is the output this is the target this should be the difference so what went wrong here within my subtract matrix dot subtract target's comma outputs let's look at this function something must be wrong here anybody sick of looking zeni buddy see it I J columns rows columns in that data Petros ADA columns return results this dot rows oh yes thank you alright here's the mistake this dot rows this columns that makes no sense I make I'm writing a static function that's not called on an instance of a matrix object I want to look through everything I've got to loop through everything in result or a and B they should all have the same number of rows and columns thank you to Rubin in the chat who pointed that out to me okay result dot Rose result columns all right now let's run this again great this looks right this is what the neural network produced this is what it this is the known output and this is the this is the error and just to just to take this one step further if I were to have two outputs I don't know I'm not and and have a second target this is matching what I've drawn over here we could see these are the guest outputs this is the target and these are the errors so we have now written into our code all of the pieces we need to get these two errors get the air the output errors the next step we need to do is calculate the hidden errors I need to calculate the hidden pairs so looking at the code I need another step here I need to now say let hidden errors equal and figure that out so what goes here what goes here this is the and I suppose if I'm being consistent I should say errors there right errors are the out pen and and if I'm being really consistent I should say output errors now what I need to do is calculate the hidden errors okay so let's go back to here and I want to do this with matrix math so this looks like hey that could be some matrix math going on here right this looks like a weighted sum or something dot product D like looking thing and here's the trick this looks good right but what's all this like fractions stuff well if you look at this this is the same as this this is the same as this these are really normalizing dividing these weights by the sum of all the weights is a way of normalizing everything so they all add up to 100% in the end we're gonna kind of like multiply everything by this learning rate constant anyway so we could say like make a big step or make a small step so that normalizing of it kind of doesn't matter I mean it's sort of important but we can also ignore it and that's kind of a trick here we're gonna just take out the we know we want the amount of the air to be proportional but the fact that we're multiplying by its weight it's going to be proportional we don't have to divide it by the Sun so we can actually take the bottom out and I'm gonna say wait one one here I'm gonna say wait one two here I'm gonna say wait to two here and I'm gonna say wait to one here and look at this by golly doesn't that look like some matrix math right that's got to be the result of some matrix product now I need more space on the whiteboard so how can I do is kind of condense this a little bit well one one times e 1 plus weight of 2 1 times e to make this take up less space and then wait one two times e1 plus wait what was that two times e2 now conveniently I have this matrix here and what matrix would I put here to get this right if I want the matrix product of these two matrices I need to put a row in here that's right the row this row W 1 1 W 2 1 the weighted sum the dot product of this row and this column is exactly this now let's take this one 1 2 and wait right take this dot product with here boom we've got this this matrix product the matrix product between these two matrices is that hidden errors coming out of the hidden layer this is really exciting but there's something really strange here it's like stare at this now all along I keep getting my indices wrong right I've been getting my histories wrong in these tutorials I had to do the tutorials over again and this might look wrong right because should not be wrong column row column row column row column this is Row one how eyes are there looks like I got it backwards well the fact is I did get the backwards I got that backwards on purpose this is actually this weight matrix transposed so these weights are stored in a matrix already in my code that matrix looks like this weight 1 1 wait 1 2 wait 2 1 wait 2 2 transpose this matrix to this and take the dot product with the heirs and boom I've got the hidden errors coming out ok let's do that this is working because it's square but am I gonna have an issue if it's not square I didn't notice this when I was working this out or anyway let's go put that into our code oops yes w superscript T okay all right the good news is oops right the good news is I believe in the matrix library I already wrote a function called transpose here's I should be consistent this function transpose returns a new matrix that's the transposition of the previous one and so I should actually make this static and it should require to receive a it should receive some matrix object and so it should be this so I just changed this to be a static function so that what I can do is I can say here back in the library what do I need to transpose I need to transpose the weights that are going from hidden to output that's the weights H of so I have this dot weights H oh so let hidden um weights H o transpose T for transpose hmm I could also just change the let who T these are the way naming I could use some work on the naming but these are the weights from hidden to output transposed equals matrix dot transpose this dot weights hidden output so the hidden errors now should be matrix multiply output way what did I say hey if we look over here matrix multiply that transpose matrix and those errors that transposed matrix and the output errors so this is calculate the hidden layer errors now if I were writing a proper library that could support multiple layers there'd be some kind of loop going on here because I keep doing this from layer to layer to layer but since I just have this two layer Network I'm just gonna do the output layers output errors and the hidden errors so that I can sort of get this back propagation thing going in just one step I'm getting a nice comment from the chat I was worrying about the dimensions of my matrices and Kate week Minh writes the dimensions work out when going backwards the transpose right of course because I was worried about the dimensions not working because you know the rows and columns have to match properly when you're doing matrix multiplication but since I'm now going backwards it actually makes sense that the matrices have to be transposed you can pause and think about that for a second but that does make sense now ok so I think I've come oh and I was also but you know if you're looking at this notation in like a text book or something you'll often see if I have like if you have a weight matrix that's W and maybe it's like W IJ for rows and columns you'll often see T in the superscript as and you can't see that write it over already over here right you know if this is the weight matrix W and you have columns and rows then you'll often see a superscript of T and that refers to this this matrix trans transposed and so maybe this would be you know weight h.o.t or something I don't know so my notation is as you all know is kind of poor but I try to do my best to explain it hopefully it will match up with other other notation that you see in that sort of thing okay so this is now done in terms of back propagation I've compute the error I've computed the I've propagated that error backwards to compute the hidden layers error and now I just need to add the part where I adjust all of the weights these weights based on this error and these weights based on this error and then we just move on and then we're done so I'm gonna do that in the next video I should warn you that the math for doing that is this which I'll discuss in generalities the math of gradient descent for finding these Delta weights is pretty complicated I have two videos that I've made where I kind of do something similar which I will reference as well as all of those three blue one Brown videos that go through the math in detail I'll probably start the next video just by presenting the formula and then implementing kind of talking through the pieces of the formula and then implementing that in code okay right people are telling me I don't need the IJ yeah yeah I don't need the IJ but anyway lower case T upper case T so now what I need to do is I need to where am I in my notes what time is it oh it's twelve thirty five remember items can be done with backpropagation by 1230 that didn't happen that's okay though even though it's search done alright alright so I think does everybody feel ok let's get a snapshot of the whiteboard as is I think I'm gonna erase this now because what what error do I have result equals new matrix this columns this dot row huh where where where where where do I need to change that sorry I'm seeing something in the chat that I got columns and rows incorrect somewhere I think I have that here in the static transpose no this is correct because but you might think this is incorrect but I have written my matrix library in such a way that the rows and columns come in first so if you're transposing it you send the columns in first and the rows second so I think that's correct I'll let and see if anybody I'm gonna give it a minute so what I need to do now is find the best notation for writing out the formula and I kinda this is where I kind of got stuck in my preparation for today although I have to say I can't use this on static ah oh I see thank you here got it um what's the best way I'll just I'll just do this at the beginning of the next video shoot as I didn't try to I would have gotten this error if I tried to run the code let's let's add something to the end yeah it's fine I'm just at the beginning yeah yeah I got I got it everybody I'm just trying to figure out where to know I haven't updated the wait Simon is asking have you update the weights no there's a whole big step here I mean kind of like I just did the sort of like beginning part so I the whole big step so I guess I'll just cop to this error at the beginning of the next video so what I'm trying to do I'm gonna erase the whiteboard I don't think this has any value anymore or at least this whole bottom part doesn't I can leave this diagram here maybe for reference so let me try doing that and let me try to find a form a way of noting the formula for updating the weights this time I need to try twice I'm not gonna so this is my this is my thinking and I'm going to kind of use my notes here a little bit I'm going to need help with everybody and well that's just the mouse oh it's like there's a black mark on the whiteboard so I I will correct this by the way at the beginning of the next video me just make sure the whiteboard is dry so my thinking is the any given weight equals its its weight plus a change in weight right and the so the thing we're trying to calculate is this another way of and the way that we're going to calculate this is by thinking about the way the the hat when we want to understand like two we want to adjust the weights we want to adjust the weights so you know we want to think about this as the change in error based on the change in weights so how does the right if I know how the error changes based on the how the I'm just sort of thinking this through based on how the weight changes right this is Nicole gradient descent thing then basically I'm saying Delta I want to change take the weight and minimize the error so adjust it based on the negative direction how the error changes based on the week and this error is something different it's actually the like cumulative cost so it's that's the kind of squares of all the errors let's see if I'm getting this about right and so that question becomes how do I know how the error changes based on the weights and that is the formula that I've kind of derived previously is the error times the so in this case right this is the output the derivative of the output based on the weight Oh talking this through I'm gonna come back and explain it let's see if everything I'm I'm gonna do this again I'm just kind of like talking this through into my head and so now what I need to do is figure this out and what this is is output is sigmoid right of the weighted sum how did I write this down I just wrote weighted sum of my notes we can think of the weighted sum of all the connections coming in but I could just call that like X for a second and so and the derivative of sigmoid is a known thing the derivative of sigmoid x equals sigmoid of x times 1 minus Sigma of X and it looks like a 6 and so and so basically if I put this here I kind of have the I kind of the formula for any given single weight let's look at this shot so so this I think is right that's where my notes end let me look at the code that I did so that's really like the gradient the good thing of this is would it be correct to refer to this kind of as the gradient and since I need to do that for everything how can I know take this with matrix math I know that what I do in my code is I I have I take the outputs transposed I need more room on the white board let me get this far so I think of what I want to do is try to get this far for a single weight then I'll erase and try to get further with turn this into matrix matrices anybody have any Corrections or notes or thoughts you want to add to what I haven't erased this and kind of go through it but actually like try to explain it a bit more as I do that I mean I could just include the final formula at the top maybe I should maybe I should write that down I don't see output equals matrix I don't see anybody talking about right now Jack who's never just talking amongst themselves all right so I think right we just we just need to figure it out for any one of these given weights how to adjust it based on the error and we've calculated all the error stuff already okay just looking at this all right I'm gonna yeah start with a single weight all right I'm looking in the chassis some people are typing in the slack patron group I might not get to to be honest the challenge today I think I can I think I can alright alright alright I'm just gonna move forward I'm gonna erase all this I'm gonna try to explain it again I forgot where I was I finished I just finished a video right okay all right oh the yes the error function meaning the the cost of all of the sum the sum squared of all the errors a squaring yeah yeah I do I do need to talk about that yes yes thank you okay I will I definitely will mention that all right so in this video I'm going to start to talk through learning learning with gradient descent we've looked at how to back propagate the error that's coming from the output back through the network now we need to look at how do we turn all the dial set but before I get to that I did make a sort of major mistake in the previous video where I was making this transpose function and turning it into static so the word this now means nothing so I need to make the resulting transpose matrix out of the one that's being passed in so just wanted to correct that right here the beginning of this video quickly okay now so there's a few things we need to talk about so first I want to talk about what is this thing called gradient descent now let me once again as I always do reference that other materials will go to this in more depth I made a video called the mathematics of gradient descent it don't really recommend that one is my top choice but you could go watch that and it'll also reference the one that do recommend which is the three blue one Brown video about gradient descent and there's a nice Siraj video about gradient descent to suit to I'll link to both of those in this video's description but the basic idea is that we need to have come up with a cost function what is the cost function well this is an error this is like an error vector right point three and negative point four what we want to do is we actually want to look at the cumulative error over time of many training examples so if we say like here's a hundred pieces of data with known labelled answers what's your cumulative error over all of them and the way that that's typically calculated is by the sum which is often used this nice uppercase Sigma is that uppercase the right thing to say with critters the sum the weight the sum of all of the you know targets minus output squared so this is often referred to as a cost function and you could graph this function like you could imagine it it's like a function like y equals x squared and if I had a function like y equals x squared it would look at this and if I wanted to minimize that function I would have to start moving I would want to like look at how do I go down how do I go down towards zero how do I walk down gradient descent and how do I know the direction of the slope of this function at any given point with a concept known as the derivative so again I would refer you to some videos from three blue one brown that go through the calculus what is a derivative what is great to send in more detail but this is the basic idea I want to push the weights I want to know what happens right when I change the weight how does that affect the error the change in the error based on the change in weights right I want to minimize minimize the error I want change the weight in the direction that changes the air this is kind of the notation for that the change in air L through the change of weights and you know the kinds of derivatives is more sophisticated than that but that's a way of thinking of it right now there's something else I want to mention about this ah yes so here's the thing this is a nice little like graph of y equal x squared and sort of notation over here but the thing that's really crazy about this is we aren't living into this would work if we have a single output this would kind of is what it would look like if there were two outputs then we might have this like Bowl perhaps I think if there were three outputs oh I've lost I can't think we're gonna get into crazy and dimensional spaces then you can't really imagine so that's one thing that's really confusing about this is these outputs could be many many so we really have to in a way tackle them one time like let's well how would it work with for just one and in essence we're gonna do the same thing to all of them by pushing them through some matrix math so that's ultimately what I'm gonna do I'm gonna explain a little bit further but eventually again I'm gonna be glossing over some details and mostly focus on trying to implement this in the code all right let me pause for a second I'm not seeing anyone yelling at me that I said anything horribly wrong so that's the thing that I wanted the error function thank you okay we come for reminding me about that that's what I wanted to start so where is the actual eraser now that we've talked about this or cost function how we're trying to minimize that function using gradient descent walking along the graph of that function towards the bottom let's talk about how we actually do that what's the thing we're actually trying to calculate well what we want to do as we've look at all this training data is we want to say how do I want to change a weight I want any given weight this W for any given weight just one weight in the system I want to the process that I'm gonna run in my code is to change that weight by some Delta weight that change in weight so here we are I should be able to just say if I could just figure out the formula for Delta weight I'm done all right well I did say that another sorry I did just a moment ago talked about the cost function and the idea of how does the error change when I adjust the weights well this is really another way of saying Delta weight if I said Delta weight is you know how does the error change great when I change the weight if it gets bigger when I change the weight I wanted to get I want the error to get smaller so I could kind of say Delta weight is negative how the how the Changez when I changed the weight so this is just another way of kind of writing the same thing people who know more about mathematics might be getting upset with me right now I will look forward to your comments all right so now let's think about this how do I calculate this so how do I know how the error changes when I change the weight well this is something that I have gone through in my gradient descent video what I can do here is actually just say looking at my notes oh I forgot about the learning rate sorry sorry sorry sorry I should add one more thing in here which is that this Delta weight can also be a factor I can also factor in this thing called a learning rate I'm gonna kind of fold this in at the end but the learning rate is just like I this is how much I should this is the direction that I need to move you know should just look like a tape it or let's try to move like a lot and so you've seen this again in the simple perceptron example this idea of a learning rate so that should get folded in here as well so basically what I'm doing here is I'm saying this is a formula that I've derived in other places this should equal I'm looking at my notes over here just a few pi get this right negative the error itself times how the output changes when I change a weight so I'm gonna call that do DW how does the output change when I change the weight well what is the output that's something we know we've been working on that right the output of any given neuron is the weighted sum of everything coming in pass through this sigmoid function and again sigmoid is kind of a historical activation function that's not used as much right now and later you know we can swap in other activation functions understand but they're one of the reasons why it's useful to use sigmoid is it makes doing this calculation kind of easy so if the output the function that's the output is equal to Sigma E and that's the Greek that's the lowercase Greek letters I'm trying to write this out maybe we should just write the word sigmoid I'm gonna write in this sig si G sigmoid of that whole weighted sum of everything let's just call that X for a second then if this is the output function how that output changes when I change a weight is the derivative of that function write out any given weight write that this is the weighted sum how they changed these weights what is the slope of that function in this crazy ndimensional space well it just so happens if we go back to Wikipedia sigmoid derivative lets derivative where is it on here it looks so it's so nice man that on here fail a little failed moment here let's find a different place I don't want to derive it just want to look at it yeah here it is that's a nice little oh wait hold on I'm getting some good questions maybe add a factor of 1/2 to make the derivative the error oh yeah yeah so wait where do I add that you mean added in here because this thing is the derivative air is actually there's R squared or something is that what you meant K week Minh I always just flop that off cuz it's just another constant in the system I think you should do a Eric in the chat rightz I think you should do a past propagating errors back through the whole network then update the weights it doesn't matter much now but if you did have more layers who would be able to update all the weights in parallel language permitting I think that's kind of what I did already I'm trying to do but I see what you're saying right because I've already I already did this step of passing the errors back and so now I just want to do update these weights and update these weights we already have these errors so update so I need to fill in the updates here and the updates here that's what I'm trying to do okay maybe I missed it in the stream Y is Delta wait in the first formula using the uppercase Delta but in the second formula upper case Delta wait equals to negative lower case Delta e over lower case Delta W I mean it's really the same yeah my notation is kind of bad but I may have said I really sort of mean this as shorthand this in a way in the cost function yeah yeah and the cost function okay okay so I'm just reading the chat for a second right I was looking for a nice derivative that of sigmoid anybody if anybody has a suggestion for a nice webpage that's gonna I mean I guess I should just use this one it's just like this is a little janky but yeah it's also it's driving it now I know what this thing my drift is just wanted to like find a webpage that confirms it for me maybe math world sigmoid derivative yeah there we go all right I'm just gonna yeah Reuben I did sort of mention that I believe during a bunch of times and I will continue to mention that all right WolframAlpha well from alpha what is the derivative of sigmoid and I cannot spell derivative all these web pages are making me sad I mean that's a nice way of putting it but okay I'm just gonna I'm just gonna I'm gonna I don't need it I don't need a diagram gosh darn it come back over here yeah I will yeah yeah no I'm definitely not doing the derivation I just wanted to put the confirm that I was correct so we could one of them Here I am now on a webpage which shows the derivative a sigmoid and this is what's really nice about it the derivative a sigmoid is actually sigmoid times 1 minus Sigma so that's what makes kind of working this out once I start to plug the math into my actual source code kind of easy so if I come back over here I can say this is times sigmoid of the output oh sorry times 1 minus Sigma of the output I don't know why I put so this is basically what I'm looking at here if I want to change any given weight I change it based on the derivative of sigmoid multiplied by the error and this is kind of this is the same formula that's appeared into several other of my previous videos and one that you can kind of dive deeper in with some of the links that I've included in this video's description the big question now is how do this is just I'm really just talking about the case of changing one weight how do I turn this into matrix math alright alright so now again this is where my notes have ended and I need a way of writing this with matrix math and the way that I'm gonna do that anybody who wants to help me with this I would gladly take your help oh I forgot about the 1/2 thing I will I'll bring that up I'll bring that up next let me see is there a notation that's used in okay this is the notation that is used in the tariq Rashid book so let me try to write that out dare I erase this top I think that's what I have to erase so let's look at the this notation Delta wait for and he uses JK I've been using I J so I'm gonna keep that I know that's JK is much better equals we have the learning rate I've talked about which times the error vector which is J right because it's a it's like one I'm iterating over the rows so it should be so confusing it's gonna say the error the error vector times sigmoid of the output vector sigmoid of the output vector this is the same thing that I did that I've got right here the same notation I'm just confirming that I'm right times 1 minus Sigma of the output question is if I'm thinking of this as rows columns and things are being transposed and as this is it what's the what's they in disk index it should go under E and under oh that's a question oh my goodness wrong camera wrong camera wrong camera wrong camera so long so long with the wrong camera and I wrote up let me do this again sorry everybody this is good I need multiple tries at this anyway I'm writing off the whiteboard so I'm looking at the notation that's in the tariq Rashid book and this is what's written JK equals the learning rate I'm changing some learning rate times e sub K times sigmoid of o sub K times 1 minus Sigma Y of K and again I'm going up to high times sorry sigmoid make sure you can see this of o sub K times sigmoid sorry times 1 minus Sigma of okay so this is the formula as written in oh and then this whole thing right this is why what did I miss I missed this whole thing now and I'm running out of space so I'll figure this out is the mate way and the matrix product right which my symbol for that is like a train the matrix product with the transposed output J so this is what's written into the tarik Rashid book the what I would like to do is try to match this with what I've done so far so in theory except I think ah Tariq one of the things that I noticed is the trick Rashid book I'm pretty sure does columns rows that's why I'm getting confused so my notation has been rows columns and this is a single column vector okay and then this is transposed transposed so it would be J there we go so this would match there and so right that's the bias derivative if I don't multiply it would you multiply it with multiply it with the activations as well so what's this step so why is it that when I explain this why is it that I went through this Delta wait and I didn't include I forgot to actually multiply it by the output here because oh is that because of the negative 1/2 thing and then I'm taking the 1/2 thing and I'm taking that out but with the bias it's just 1 and so that goes goes away this is where I this is where I this is comfortable for me and then I'm going to be able to turn this into code but I'm kind of confused as to I mean I get that this needs to be here but I'm trying to get what's a nice way of explaining how I missed putting this doing is the matrix product with these would be with oh but it always is multiplied by that well how come I missed it down here was that just my own mistake I'm gonna go check out some things that are being typed yeah we believe in the chat rightz why am i watching this I'm just learning to make the ellipse in j/s yeah go back to that don't get out while you can all right I'm just gonna I Dan if you already calculated this sigmoid in the outputs you can just do x times 1 minus X target is the derivative in the last layer predicted minus target there's a lot of interesting things being discussed in the chat all right I'm just going to and hold it I'm looking at this camera went off you need to do some more reading and preparing I mean I have to just be done for today and I will eris in the weights in now but I'm just I wish I had that gradient descent formula let's see here I think in this video I could find the part of this video right I'm doing it here basically going through exactly what I'm trying to go through now yeah oh no I'm just so close to finding the right point in this video I think actually this might help if I kind of match it up with the diagram in this video yes you are watching a livestream of a person looking back in a video tutorial that they previously made to try to find something to help understand the video tutorial that they're trying to make right now that is one day what that is exactly what you were doing and you should all just oh here it is down here please step away step away from the I did all this it went through this you know another way for me to do this is actually now that I'm realizing this is just alright alright so what I'm going to right err times X this is this is the equivalent of X this is obviously the error and the new thing here is the fact that I have sigmoid as the activation function right so in this the difference is in here there was no sigmoid function so the derivative was much simpler and doesn't show up right right can I can I make a argument can I can I can I stop where I am right now and connect this here with this video where I derive this formula for the change in weights the derivative is 1 in your video yes I think that's right all right so this is what I'm going to do I seek a week Mon is typing so I must wait K week one usually has something very wise to say shouldn't that be shouldn't that be the hidden layer activations we x yes the hidden layer activations I mean it depends where I am I got rid of my diagram right now I'm not doing the it would be multiplying by the hidden layer activations this is just like what I'm doing right now is generically for any connection and then I'm going to do the same for the output and the same for that the hidden layer but the output uses the error vector from the output and the hidden layers is the error vector the hidden errors so yeah all right two times target minus W index I times yeah exactly exactly exactly yeah I just erased that I know on purpose all right I'm gonna go in back into the video tutorial and we'll see what happens I just took a little break staring at this I'm having this moment it's like haven't I gone through all of this before and then I realized ah I mean I kind of referenced before I have it so I went back and looked so what I want to do right now is like this is where I sort of ended here in trying to sort of look at how the error changes based on how the when the weights change that I've talked about this in my mathematics of gradient descent and in this mathematics of gradient descent video there was no sigmoid function so ultimately what I have here is and this is sort of 18 minutes into this video which boils down to the chain and by the way one thing that I missed over here is is the the 2 or the 1/2 so nice when you take the derivative of something squared you get a 2 and so you divide by 2 or multiply 2 so I just sort of like take those out but because it's just a constant I'm gonna have a learning rate later anyway so but that's why you see the 2 over here but what you see here is change right the change in the weights this is M and B for a single and this is really for a single weight y equals MX plus B the bias I mean not to that yet is the error times the x times the learning rate and that's ultimately what I have here butbutbutbut solet's so I have this book so let me write this let me now write this out with meit sort of more matrix notation this is a makeyourown neural network and I'm going to basically use the exact and I highly recommend kind of reading this chapter along I'm going to use the basically the exact same you know what I want to calculate is the changing weights for every weight in the matrix of connections every row and every column and what I want that to be I got to be a little bit lower here this is based on what I kind of worked out here well I have a learning rate multiplied by the errors and the error vector right the error vector is a single column vector so I'm looking at all the different rooms time's the actual output oh uh sorry hold on oh if I come back to here this is basically sorry so let's just looking at this right now let's go back to my previous video that's basically the same thing as I've got right here I've got the change in weight is the error there times the learning rate but there's an X in there so the X what's the X over here well the X is actually that output so I'm gonna have to do the matrix product which I my symbol for that is a trade but I'm gonna use uses a dot right now with the actual output itself but the thing that's missing in this particular scenario is I didn't have this sigmoid function that's how I need to look which way do I go which way do I adjust based on those activations are coming from the sigmoid function so I need the derivative of the sigmoid function here so I also need to say sigmoid of the output that output is a single column vector times 1 minus Sigma of the output which is that one column vector and now I take the matrix product with the actual output itself now here's the thing this is something we've actually calculated already in my code I've died calculated the outputs I did that while I was feeding forward through the network so this is actually something that's already calculated and I just need to put it in here so this now if I can just take this Delta weights then I could say for every weight I J adjust it by its corresponding Delta weight I J so this is now the formula that I need to implement in my code I need to implement this in my code to calculate all of the Delta weights how do I change all of those weights for every single weight whether it's and this error and this output this could be the final output and the final error or this could be the hidden output and the hidden error same formulas gonna work and now of course I'll do the bias as well but here now I think I could put this into my code yes oh okay so and in fact I'm doing something really weird here so this is really I could simplify this even further right because the activation function is inherent it's already been calculated so I really should say just the output vector times 1 minus that output vector and this is going to have to be transposed so that the because of the way the matrix products worked out but let's let's get to that when I get into the code okay I should go back I think I should go I'm gonna maybe I should try explaining this again why is it trance this is definitely gonna require a second shot let me go into the code and if I have to redo this next week I will so I have to like watch this back and then figure out where my explanations went awry I can already take could already hear my ta telling me the last oh should be H I think oh yes yes thank you ah that's it that's what I've got wrong let me maybe this is salvageable yeah this is H thank you that's definitely H yeah why does it say Oh in the Oh Joe in the in the book here that's interesting okay let me try this whole explanation again I don't remember where it comes from differentiating the inner function which depends on the previous layer I either hidden okay some Oh squirty chair okay do I dare let me start over like almost to the last bit you just put this in the code and then whoa whoa oh I'm in the wrong I thought I was in the whiteboard because this screen has a like a shot of a whiteboard on it and we're almost done like I'm almost in the last bit where I could just put the code in oh it's 130 I have to go shoot let me let me see let me I'll come back I don't have time through this conference oh let's see if I can let me give me like let's just let me try this explanation one more time this is definitely have to come back and and continue this another time like next week but I'm so close to the end because if I once I just write that get that formula written correctly out there I could just put it in the code because basically yeah skip the conference time for it yeah I know I need to do a coding challenge I can't I'm not gonna have a coding challenge today because of back propagation back propagation just thinking for a second where I spend so much time like the momentum is totally died okay I'm gonna I'm gonna go back to where I referenced that previous video no that was fine so I reference that previous video I came over here bla bla bla bla bla and then I went back there okay alright so what I want to do now so where am I I had this formula I'm realized it's similar to my previous video oh no I'm just repeating myself but which is over here now what I need to do is take that formula that I've gone through and and let me actually just take it and write it directly from this book basically it's you know what I'm gonna do I'm gonna let's just implement the code shoot give me one more chance here so I'm reading the jack I believe there are people here still watching so after I reference this video so let's try to take this now and turn this into a matrix math basically remember what I want to do is I need to know the changer weight for every weight that's in our weight matrix and so this is exactly the same formula that I did in the gradient descent video it's basically what's down here but let's kind of rewrite it with some notation and thank you again to this book that I'm kind of also pulling from so if I want to know the change in weight for every weight in our row column matrix row I column J this equals referring back over here ok well first we need the learning rate times the error right the error now the error is a single column vector right so it's just rows now this could be the error of again these are the change in weights for any weight matrix anywhere in the network so this error could be the final output error or could be the hidden error and then I need to use the sigmoid function so the derivative of the sigmoid function that's the piece that's missing over here right oh shoot oh sorry and then I need to multiply the actual output itself from the previous what the previous layer whatever those I need to multiply by the inputs itself that's why that's why X is over here so let's just let's make the case what I'm doing I'm doing the weight adjustments for the final layer we're gonna so I need to multiply this by that and actually that's going to be the matrix product which I don't have a symbol for a good thing you can't see is I'm trying right or I need to buy those hidden outputs so I need to what's coming out of the hidden layer that's X so I'm gonna put that here and so now but I need to also figure out what direction to go I need the derivative I didn't have the sigmoid function in this video so it's not there in the formula but now what I need is this and guess what I already have this this is just the output itself right the so the derivative of this that's the output of the final output is the the sigmoid of the weighted sum of everything and so now the derivative is is o index I that single column vector output x times 1 minus o so this is actually the entire formula I need it's terrible terrible terrible me who has written this too high on the whiteboard I am hereby fired from ever making a youtube video again I'm gonna I'm gonna rewrite it a little bit lower because we've really got this now we've really got this Delta weight formula I need to this is just I should just show you this right I need to take the learning rate multiplied by the errors which I said was J but oh sorry I multiplied by the output multiplied by one the output and this whole thing now gets the matrix product of what was the X in this situation if this is the final output of the network then this becomes the hidden and this is the matrix product this is so this is the Delta wait look at this I love this now this is the Delta wait for hidden to output the Delta wait for input to hidden then equals the learning rate times the hidden errors times the hidden output times 1 minus they hit an output and that whole dot product with the input itself determine to call X so this is how this is how I adjust the weights this is why I need all my goodness this is why I spent all those other videos on just figure out what this hidden error is this is really the magic of back propagation right here so these are the formulas you know are these formulas didn't even there they gonna make sense to you especially with this massive video that I've put together probably not but we kind of have a ride to them but the thing but I haven't do these formulas make total complete sense to you well hopefully again the idea is to like look at this and kind of get an intuitive understanding right we know we need to adjust the weights based on the error the hidden error is kind of like a way of getting the portion of that that final error we know we need to adjust them in the direction of looking at the atom as well do these formulas really make any sense to you probably not a if you want to kind of unpack them further I think you could pause now and go back and kind of look it just watch my mathematical grading descent video look at the three blue one Brown videos but the pieces of this should really make sense right we know we want to change all the weights one of adjust those dials to get a better output that output is based on the error that error is back propagated to a hidden error if we're going back weights in the matrix the derivative of the sigmoid function is important because it tells us the direction of which style things are being activated and then of course we need the what's try to come before hidden comes before the final output and the input comes before the hidden this is exactly this is exactly what I've done in in the previous videos of what I had when we were looking at y equals MX plus B so we can connect that back to the gradient descent video okay so I think it's time I'm going to stop right now and I'm going to from here implement this in code alright so probably what I need to do that probably what I need to do is redo the explanation because I think I've really botched that but I think if I I think I'm gonna I think this is right enough that I can put this in the code and maybe we've actually got our neural network code I don't know what bag provocation means so I'm gonna I'm gonna have to return to this I've run out of time today but let's see if I can at least put this in my code I'm not even gonna do this as a video tutorial I'm gonna just come back to this last piece if so I'm gonna I think I need to do redo the videos I've realized it today redo the videos where I explain these won't be the final published ones the Delta weights how that's calculated but let's just see if this kind of matches up really quickly so if I'm doing this one here so here's the thing what I let me save this as old and then so I know where I last left it so a couple things I need I need D I need a function called D sigmoid and that is if I have let y equal sigmoid of X now I return Y times 1 minus y so this I'm going to need is the the derivative of sigmoid function I think this is going to like mess the problem is I want to use this stuff so I think instead of doing feedforward what I can actually do here is do this and then I was calling these outputs yeah this is the autocomplete that I guess I don't want and then I don't need this anymore so targets though I do need so what I just did is put the whole feet forward in here thing because I need to and then I need to calculate Delta the idea is to calculate Delta weights and so how do I do that what are the pieces that I need the pieces I need is are the air the air this this is the so I have this and now I need to multiply it by this and dark these that's an elementwise multiplication right so I actually want to say right this is map through sigmoid already right there so I want to say like I'll just call it D outputs for right now equals so I need to make a copy of it so I need to like did I have a mistake here yeah no that's fine I'm gonna come back I'm gonna come back to this next week I just I kind of wanted to put the code in here so the the derivative of the outputs if I can take suffice ik function called 1 minus yeah actually the outputs already have so the thing is I need a fake I need a like already sigmoid function which just does this right because it it's D sigmoid if I'm looking at the stuff that's already been sigmoid let's just put this up here again this is gonna be a mess I just want to put the code in here and then I'm gonna have a like some time to think about it I'll come back and do another live stream where I finish this oops so so I want to say matrix dot map the outputs right I want to because they've already been sigmoid map already sigmoid outputs already sigmoid so I'm gonna need a map function that's static to make a new vector where's map static map matrix and let result equal a new matrix matrix dot rows matrix Kyle's sorry I'm gonna do a lot of stuff without really explaining it right now fully just to try to function value and then return results so now I have this so I have this now I need this right and I have that but those get multiplied by each other just element wise so can I say D outputs that's the hata mard Hadamard how do you say that again Hadamard Hadamard product I'm gonna write a Hadamard function with the output errors right so this is the hata mart and so if i put that into the library tonight actually oh there is a mask a l'heure product that's the Hadamard so that should be called probably Hadamard but whatever it's called multiplied it's already in the library just cuz I have to go I'm kind of like race through this I can multiply by the learning rate there also should be how come this my library doesn't check to see if it's a just a single number oh no that's only the scalar product that multiply function is only doing the scalar product got it got it got it oh I had it previously doing that so I can adjust this adjust this to do Hadamard I need to do that I see I see so if n is an instance of a matrix now I should do the Hadamard product otherwise just do the scalar product which is different than the matrix product which also is called multiplies and the naming is terrible here but so that would be this so now okay so that works multiplied by the learning rate so I'm just gonna say I'm just gonna have that be a constant here right now I'm not a constant cuz let let learning rate equal 0.1 or something okay so this well this went off so I did this component this component at this point now I need to do the matrix product with but that has to be transposed so now what I need to do is say let this actually transpose no no where am i I'm here hmm yeah target on outputs now I need to just take this here I need to take these but hidden T equal matrix dot transpose hidden hidden T right right this is the matrix transposed and then a delta so this is really what I should really say is like sigmoid no ya d outputs yeah sorry sorry so now let Delta weights equal matrix dot multiply D outputs hidden transpose Delta weights dot print then all right Simon is asking why practice in a livestream I don't have a good reason just because I'm already livestreaming I need to practice and maybe this will help okay sorry for everybody sort of joining in the middle I've run out of time and I have to go so it's just trying to like see if I was in the right path I need to double back and redo the video explanation I need to watch my gradient descent video to connect it better and then kind of just present these formulas I think that makes more sense I can make this much shorter I just wanted to see if this would actually work in the code I don't have time to test it out and then so now we would say this dot weights which is hidden to output hidden to output dot add add Delta weights so this is change hidden to output weights gradient descent okay now alright so I think I'm going to finish up here I just want it so what I'm gonna do better than no practice exactly so I don't know if I've gotten this right I need to stop freeze frame here for myself sorry that this live stream is kind of ending abruptly and I didn't even do a coding challenge these are the formulas that I've arrived at for how to change the weights between hittin and output and how to change the weights between input and hit it I need to come back and redo my explanation I think everything looks fine all the way up until it's all the way up until the sort of I got to the point where I started started with this line right here so that is where I'm gonna return the next time I come back hopefully sometime next early next week to continue this live stream that's not finished I'm gonna start from here refer I'm gonna try to derive not derive derive is the wrong I'm definitely not going to derive I'm gonna start from here present these formulas kind of understand them as they relate to my previous gradient descent videos then I am going to then I am going to put that into the code here to see if I got it right so but I what I need to do I need to finish this code and then test it so I'm just gonna I'm gonna freeze frame this I will publish this somewhere let's publish this somewhere right now when the archive for this live stream gets posted which is typically sometime this weekend I will include a link to just like a Google Drive folder with code in progress so if anybody wants to like try to keep working on it or like check what I've got so far and refer to it you'll be able to I'm not going to put this code up in github exactly I mean I will eventually it aesthetically this is the same as what I already have in my this should be basically I'm redoing I think there are mistakes in here this exact function neural network prototype train this is a version that I made last spring and you can kind of see there's some similar stuff but I've kind of started over from scratch so this is the end for today it's 150 I I definitely have to be out of here I'm sorry that I didn't get as far as I wanted to the next steps are I knew this would be hard the next steps are redo my gradient descent video I'm just maybe I'll put this into some notes up here next to do redo gradient descent video about Delta wait formulas connect to mathematics of gradient descent the video ok then implement gradient descent in library somebody to talk about different activation functions then do X or coding challenge and then M NIST coding challenge alright so this is my this is my what I got to do next thank you everybody for being here today while I'm this is really it's I don't know I was gonna say this is kind of different than what I usually do but to be honest it's what I always do which is I'm trying to learn something yeah it's different in the sense that I do sometimes do videos about stuff that I I guess I know a little better or that a little simpler that I've been working with for 10 years or something this is like I don't really know what I'm doing so really trying to learn and get a sense of the mathematics of building a simple neural network and how to implement that with my own code so that I can later work from a higher level and feel comfortable with this or language of all this stuff so this is all I had time for today I would say that I'd come back and finish this later this afternoon but that's I know for a fact that it's not going to happen because I so that uh if anyone wants to join the patreon to discuss this board the slack group you're welcome to maybe you don't want to after watching this live stream but I'm gonna be busy for the rest of today and tomorrow with this processing foundation and SF PC learn to teach where I'm going to talk about how much of a disaster my video stuff is okay thanks sorry I don't have time to really answer questions and there will be some new videos continuing this stuff that get published and the coding challenges will come as well any last words I got a few donations from the chat that's very nice let me see if I can where do I check that here no this is where I send a super Joe can I see it in my live dashboard so I can thank the people by the way it's I look it's wonderful that people donate through the YouTube super chat my preference is the as a way of supporting the channel is through patreon but obviously not everyone can do that so that's wonderful where do where can I find them I don't it doesn't there's nowhere where I can see to find I know I can see them later once I finish and there's a place in my creator studio we can go back and see them but I'd love to be able to thank the people right now if anybody that's by the way those are you saying nice things to me in the chat but really appreciate it it actually like it does mean a lot it does help me I mean you shouldn't say if you don't mean it but if you did actually learn something if you think this isn't so terrible you say something nice about it that does help and I absolutely welcome I've gotten lots of wonderful constructive criticism on ways I can make the videos better and that kind of thing too so Socrates says I donated $5 Thank You Socrates and yeah okay so this is part one of this like this is part one of this week's episode it will be continued next week hopefully before next Friday but it might just be that next Friday I do the continuation of this we shall see I actually have a makeup class I have to teach next Friday the semester is starting so I'll figure it out I am I'm in it to win it I did it to lose it you have my random number video and blah blah blah all right thank you everybody for being with me today send me your feedback in the comments when this live stream gets archived at Schiffman at twitter the live stream will be archived within an hour I just leave it unlisted till effect till I'm Matt to help stance has a chance to like put in the description of the links to all the things so yeah I made it to learn it so I will come back later I'll be back next time definitely next Friday if not and next Friday will be a late one because I have a makeup class that I'm teaching here at NYU that goes until three o'clock in the afternoon about so I'm probably gonna be doing like a 4 p.m. or 5 p.m. Eastern Time live stream which is very late I know for all of you international depending on where you are obviously but international viewers all right thank you Socrates Etna Bruna Gino Zak and Kay weak bond I like to say your name with emphasis thank in it to win it hash tagging it to win it wilderness Dan this will be longer well the nernst and will be back alright alright it's so hard to know like right that the thing is like could I just try just write this formula up on the board and implemented the code and be like go watch those other videos that explain it more Shai explained it I'm kind of like being in the middle here which i think is a bad place to be but the best place to be I think is like I'm really gonna explain everything and it's gonna make sense or I'm gonna just present to you the these formulas that I'm going to implement and here are resources for you to dive deeper into those formulas kind of in the middle so I I'll come back and sort of sort that out all right this live stream is ending in three two one goodbye and good luck have a wonderful weekend give a friend a hug all that sort of stuff goodbye