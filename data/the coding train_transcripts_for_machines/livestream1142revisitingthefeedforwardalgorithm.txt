hello Wednesday people today's Wednesday is that right it get confused like I know when it's Friday because I'm here it's Friday but today is Wednesday I'm here is me Daniel Shipman for a bonus coding Train livestream before you get too excited about this bonus livestream here's the thing I had a regular breaker regularly scheduled livestream last Friday we could probably open it up here on my computer and find the YouTube video where it said neural networks continued and as I recall I spent some time improving the matrix library and then I talked a bit about a feedforward algorithm and then I started to implement that code for the feedforward algorithm in a neural network class and then I went home and I had a sip of some herbal tea and I read a book quietly and I don't know painted I don't I do at home what a chaos actually if really it's like running around and taking care of children but that's like what I like to imagine that I was doing but but and then at some point I had some conversations or reviewed some video it seemed apparent that even though it can oh yeah I got to talk about the haircut thing yeah okay even though often what is most useful I hear from my videos is is where a me is when I make mistakes and I have to debunk a problem and sort something out what I'm learning I believe and I'm not a hard person sure about this but my sense is that's particularly useful when I'm actually coding something and running into errors in the code and I have to go back and fix those errors and find those errors because that has some real pedagogical value however when I am taking the time to teach about a particular formula from a math algorithm or and diagram something on the whiteboard and for large portions of the video I have it completely wrong and then I go back and erase and fix it that edited down sometimes doesn't read as well so today's live bonus live session which a bonus is hardly a word to use now is just to simply revisit that tutorial about the feedforward algorithm we actually open it up so let me go here and find what I'm looking for so in case you're wondering what I'm referring to Here I am on the coding train YouTube page these two edited videos came out from this live stream and the rest of not yet because if I go to whoa you can really hear an echo in the music anyway that's another problem I need like a secret monitor in my ear or something so I can listen to the music that I play without having it play out through the speakers and into this microphone or I just need less delay cuz there's a delight wait actually I could probably any way alright wait I was doing something I'm gonna go to show more you can't even see what I'm doing I'm gonna go to show more and look here feed forward neural networks part 1 let's click there okay I'm gonna mute myself I'm gonna put myself at 2 times speed and I'm gonna leave this running behind me and I'm gonna keep talking so this is the part I'm gonna try not to look at it maybe this is very distracting for you so this oh there's one little problem you might notice wilderness Dan who's who is trapped trapped in a forest in a living amongst the trees for months with only with machine learning text books to read came back to civilization got a haircut so we'll understand also boosts really fast gestures so there's gonna be a little bit of a continuity problem because I can't recreate that outfit and hairstyle but a life will go on so yes if you're looking for I was trying to give a different way to say drunk Dan inebriated watch it 0.5 speed ok this is too distracting for me so this is my main goal for today is to revisit this video it is approximately 215 p.m. Eastern Time right now and I'm hoping this was going to take about an hour and then I will be on my way and we'll be back this Friday to continue and this fry if if all goes well today and I can get these other videos out this Friday I will do back propagation and training well that's the same thing I'm going to do the back propagation algorithm and then I will do hopefully some coding challenges that involve using the neural network library so the coding challenges will will be kind of standalone videos that make use of the library that's built so without having to code up all the neural network stuff in the coding challenge I can just make use of it that's my current thinking right now so I'll probably do it I mean I kind of don't want to but I'll probably do em NIST maybe X or an amnesty as these kind of basic demonstrations all right um what else can I say use a fake beer oh I wish I had a fake beard and wig to wear that would be awesome okay so whiteboard is here now one thing I would like to do because this is gonna be a video that heavily makes use of the whiteboard let me make some marks here to figure out that's the top you can see the tip of my finger the rightmost edge that you can see is about here so let me put something here okay that's the rightmost edge I don't have to worry about the leftmost edge because I'm not really gonna walk back here and then the bottom it's about here and let me put a mark here okay so I have mostly marked off where my area is and when I draw over here you can't see me which I guess is fine I will try to like stay here as my leftmost okay so that's good there any questions or let me let me actually pull up the books I'm in the wrong screen I was thinking of pulling up no I guess I guess I'm just gonna get going look at the chat anything getting oh I guess I could mention I don't need to mention all that stuff people in the chat are telling me that you can see the down work is that really a problem does this really ruin your ability to enjoy and learn from this video that you can see a red mark here on the bottom have no fear don't worry I've got you I hear you out there Internet I'm listening I will write the mark a little bit lower and just have a sense of it that it's there but really a little bit higher and maybe now that mark is invisible to you and you'll be able to go on with your day happy full of delight and without a care carefree and into the snow to frolic and be because there are no marks that you can see that might affect the way that you can absorb this material all right all right here we go I guess I feel we're just getting started but I'm just gonna get started I'm looking in the chat me ohhkay weakness 20 out a flaw in my design system it's not as bad as a flaw that leads one to my accident issue a missile alert but that I should put marks in the corners that might actually be helpful so let's let me find the corner it's about over here whoops note here here here this is the corner about there and then the corner down here is about here and I think I'm not gonna worry about the corners over here all right although I should give myself a height mark over here which is gonna be bright about there okay and the camera went off perfect timing I am soon going to investigate that magic lantern firmware stuff but I also I feel like this is part of my life now if I didn't have to press the button every 30 minutes like would I be I would i how would I get through the world and interact with the air in space around me anything I'll say is it's a so balmy 76 degrees this room I filed a work order to have the climate control system of this room looked at and I received confirmation back that the work order was complete and yet it's quite warm in this room so I'm gonna have to make another attempt and hopefully on Friday I'll have my nice cool room so I can tell bright light you know all my wonderful sweaters that I want to wear for you on the internet for I'll Isis being the tshirt you know etc etc okay ya know the camera turning off thing is terribly annoying I need to get that fixed I'm really just joking about how it's like a part of my battle that'll only be a good thing yeah I shouldn't get too stuck stuck in my ways alright it just keep me awake though alright so let's see how this goes hmm let's see I need to have this okay I think I'm just gonna begin I'm ready to do this why not no time like the present by the way this three blue one brown channel he's so good I mentioned that already it's so good it's so thoughtful and careful with all this design animation I've just it's amazing I I aspire someday to I guess what I do is just a totally different thing but yeah alright trying to decide which camera I should start with it's really gonna all be on the whiteboard I do need to have the matrix class available but do I want a reference I think I don't need to reference the continuity problem because it won't really become apparent this is sort of like a new so it makes sense that it would have gotten haircut but once you get to the next part it won't have made sense anymore I can always redo the next part too but I don't think I need to let's just go let me just do this forget about the continuity thing well address that later I'm standing in the wrong camera all right how did I start my previous one I started over on the whiteboard all right we'll do that all right this is a momentous occasion I got a haircut from the last video if you might have noticed I said I wasn't gonna mention it but I mentioned it and what I'm actually going to do now is describe we finished off I mean it's not finished we have to do more building this little matrix library that's gonna allow us to do some math stuff that we're gonna need when we implement the code for this particular video where I'm gonna describe the feedforward algorithm the neural network now I want to give thanks to two sources that I've used primarily in the sort of studying and preparation for this video number one is make your own neural network by tariq rashid you'll find a link to this a book at the coding train Amazon shop in this video's description and also I want to thank and reference the three blue one brown channel which has a playlist I figured what it's called but there's a video called what is a neural network if there's at the time of this recording there's about four videos those are amazing they're animated they're thoughtful they're careful but they're really for understanding get and having an intuition for how a neural network works and for seeing all the pieces of an algorithm I highly recommend those what I'm really attempting to do in my videos is sort of figure out a way to implement a lot of the stuff that's in this book and those videos in code so in this video in the next video I'm gonna start working on the code this video I'm really gonna talk through the algorithm in my own words as it applies to where I am in this playlist I've been way too much time introducing this video let's just get right to it okay so where I last left off before I started working on matrix math stuff I hadn't built this simple example of a percept and a perceptron the idea of a perceptron as a single neuron that receives inputs so we might have inputs something like x1 and x2 and those two values go into this perceptron they are they are processed and then some output is generated often referred to as Y so this is where this is the thing that I built last I talked about some of this in a previous video but I need to kind of just rehash it again to set the stage of where we are so let's say we're trying to solve a simple problem and like understanding logical and so the inputs that into this system this learning system this simple perceptron can be a true or false or true or false or we can also think of that as 0 or 1 right and we want this perceptron system this system to output a 1 for Y only if both the inputs are true so we could say if the if both the inputs are true then we want to get a 1 you know if if the inputs are true and false we should get a 0 if they're false and true we should also get a 0 if they're false and false we should also go to 0 so this is the kind of system now where I the thing that I talked about so this is great exciting wow we have a neuron it can receive inputs it can boom it could give me outputs all is right in the world the problem is most problems in life are more complex than this very simple scenario of two inputs each only a boolean possibility and and an output solving for a logical and so for example let's just make this slightly more complicated let's take this idea of something called X or exclusive or so if I were to switch this and I need an eraser here it is if I need to switch this to or this would now be the desired outputs for or right one thing is true that's what we've got hmmm exclusiveor means we will only output true if one thing is true and one thing is false so this for exclusive or will actually output a zero and if you I talked about this born in a previous video this is not a linearly separable problem we can't graph the solution space and draw a line right in the middle and say all the answers of all the answers for true on one side all the answers are false are on the other so this is where this idea of a multilayered perceptron comes in most problems as I get further and further through this playlist of doing more and more things cannot be solved with a single neuron this idea of a perceptron this is where what happens to solve a problem like X or if we add a second neuron and send those inputs the same inputs into that neuron and then set the output of that neuron into the output here now we have what's called a multilayered perceptron just pause for a second now now this is typically referred to as a 3 layer Network and one thing that I'm gonna do right now is even though the one of the first problems I'll try to solve once I build a neural network library is this XOR problem in order to demonstrate how the feedforward math works it's gonna be clearer I think if I have if I have a different number of inputs than the number of hidden nodes ahoh I said we go back go back having out time out time out tonight ok hold on hold on yeah everybody's telling me to layer so hold on I've seen this before and here's my feeling on this I said this previously yes this is kind of to layer maybe I should say because there's the hidden layer and the output layer the inputs aren't really a layer but I like to think of it am I wrong Len Len let me look up let me look this up I like to think of it as 3 layer and the inputs being like a special case layer but 3 layer Network okay how about 3 layer neural network what do huh see so that's the question this input layer so I think that it's reasonable for me to say 3 layer it is called a 2 layer Network okay so where are these sources coming from Oh to layer a what no ok you're right let's visit this page let's visit this page a 2 layer network one hidden layer for neurons of units and one output layer with two neurons right a 3 layer Network ok so I better say 2 layer well ok ok it's 5 layers yeah all right so it's a 3 oh no but somebody in the chat says it you can have this discussion yeah I'm gonna go back I want it I'm going to start this little section over after I took my break to blow my nose this is what's known as a twolayer network sometimes I'd like to think of it as three layers because I count the input as a layer itself input layer hidden let I said hidden so I don't want to say hidden yet what am I trying to die I lost my momentum here I'm gonna I'm gonna I'm gonna explain this one more time and I'm gonna keep going where would where did I really last leave off okay ah yes this is what's known as a multilayer perceptron we have this output layer this you could consider an input layer although the input functions differently than these other two this is a layer this is a layer and here's the thing this is called a hidden layer the reason why this I'm gonna say hidden is right here this is input right here and this is output now as I get further and further through many videos that I hope to make and examples I hope to make we're gonna see that there are many complex architectures for how you might diagram and design a neural network system this is a very very basic beginning point and typically this is a two layer Network hidden layer and output layer input isn't really considered a layer although I sometimes do it it's like close the 3 layer Network and we could get to something like long we could have a comment thread about which is precisely correct for this so it would start that common thread then I'll learn but but let's just call this raw 2 layer Network the reason why incidentally this is called a hidden layer it's because we as the operators of the neural network right we are going to give the neural network data we're gonna say hey take true and true or take true and false so we're really here interacting with the input we also want to see the output we want to take the output and use it in our project oh you gave me false thank you what's the answer hidden are the pieces that exist in between where the inputs come in the output comes out and this is kind of the magic as we'll see as I get further and further along and solving different kinds of problems how this works now the main purpose of this video even though I'm being very longwinded about it is to talk about the feedforward algorithm what happens to the data as it comes in passes through and exits through the output and in order to describe the math I what I want to do is add one additional input x3 so I'm gonna add that and that will also go into this hidden neuron and this hidden neuron so now you can see we have three inputs two hidden nodes and one output node and all of this is totally flexible you in a if you've ever looked at that classic hello world machine learning problem where you have this data set of handwritten images often the inputs are 784 for 784 pixels and the outputs there are 10 outputs because you have a probability for whether it's a 0 1 2 3 4 5 6 7 8 or 9 so the design of this how many inputs on the outputs how many hidden nodes how many hidden layers this is all food for thought but I just want to look at this very basic where is it the okay okay okay it's a 2 layer Network deep breathing deep breathing it's gonna be fine whether I say 2 or 3 life will go on now you might be wondering which weren't you just making all these videos about like matrix math how is that relevant here well it turns out that the math for what each one of these nodes do is something called a weighted sum what do I mean by that these are all connections each input connects each input connects to each hidden node each hidden node could connects to each output node these connections all have a weight and in fact if we call this like h1 and h2 right this is hidden one hidden we can have weights between one and one and one and two and two and one and two and two and three and one and three and two so those weights right if there are three inputs and two hiddens there's actually a weight matrix there are six weights there are six possible connections and I could write that like this I could say wait between sorry 1 & 2 yeah weights sorry this is where I practiced this so many times this is the part where dad messes everything up I don't think I messed it up yet so so far yet but I just wanted to think about this for a second it's called this row column I totally messed this up so so let me write out the weights in a matrix so if I have a matrix that's a row row column I can have Row 1 column 1 Row 1 column 2 Row 1 column 3 Row 2 Row 2 column 1 Row 2 column 2 Row 2 column 3 right so I have Row 1 Row 2 columns 1 2 3 so I so this is 1 this is a notation for writing all of these weights now let me now label the weights in this diagram and I'm going to do it in a particular way I'm going to say that this is a connection between hidden 1 and input 1 this year is a connection between hidden 1 and input 2 this is the connection we didn't want an input 3 and here I have a connection between hidden to an input 1 hit into an input 2 and hit into an input 3 you might want to pause this video just take a look at this and take a look at this and see if it makes sense for you the point of what I'm saying is what what should be intuitive or sort of like feel somewhat normal is like okay there's all these connections those connections 1 on 1 1 to 1 it what 1 and 1/2 and 1/2 there's connections but we're hidden one is connected to one two and three and hidden two is connected to one two and three right and so here are all the weights from inputs one two and three that are connected to hidden one one two three here all the weights that are connected to hidden two from inputs one two and three so so why am i spending all this time driving myself crazy and incidentally driving you crazy spending all this time trying to label these and do this matrix the reason is the value the math that I want to process here in the hidden node is the weighted sum of each input multiplied by the weight added together so let me give myself a little bit more space here on the white board and say that what I know that I want to end up with is the following I want to end up I got to give myself a little more space here hold on I should just pause here it's actually the right way yeah what I want what I know I would I want to end up with is this weighted sum so let's look at that weighted son first I want to take this weight weight one one oh give myself some more space I wrote it in the middle I want to I want to take this weight weight 1 1 and multiply it with this input weight 1 1 times X 1 plus what now this weight 1 2 and this input Plus this weight 1 2 times X 2 then I want to take this weight 1 3 and x 2 x 3 1 3 times X 3 you can see how this looks nice and neat the weight one one goes with x1 weight 1 2 goes with x2 weight 1 3 goes with x3 and we could do that same weighted sum for all the inputs and they're weighted connections to h2 to hidden to and that's going to be wait to 1 times x1 plus wait to two times x2 plus wait to three times x3 this is the result that we want and I could have just basically done all of this without thinking about all this notation and matrices with a for loop because I could just say like I have an array of inputs over array of hit and then I have an array of connections and then I'm gonna do some for loops to multiply all the input but the reason why I'm spending all of this time doing this is it look at something really interesting this might look familiar to you if you've been watching the previous 3 or 4 or 5 videos about matrices what is this really this is the matrix product of this weight matrix and a column of inputs if I put this right here x1 x2 x3 and I write a big X there for a matrix product or I could put a dot there may be a dot would be a better notation for matrix product we can see here that remember what that matrix product is it's this row multiplied is this the dot product of this row with this column 1 1 times X 1 1 2 times X 2 1 3 times X 3 all added together and then the next is this row times this column so it just works out perfectly this is why matrix math is so relevant and is used in all of neural network deep learning implementations because the way that the feedforward algorithm works and mind you I've there's a lot more to the feedforward oghren that I need to talk about I'm talking about the bias yet or the activation yet there's more but I just want to get now to this primary understanding of the inputs come in we take a weighted sum of all the connections between the inputs and that next layer and that can be done in a single operation if in our code the way we implement it is we store all the weights in a matrix and all the inputs in a matrix the weights are going to be the inputs are always going to be in a single column I'm actually that could change the bits of the design of your neural network but in this case that's how that's gonna work get me to pause for a second oh yeah dot and cross products or something else that's a good point okay so now the question becomes so so far I feel like this is an improvement off of what I did on Friday where I got all the notation wrong constantly and continuously I need to go through the activation stuff and the bias stuff the question is does it make sense to actually break this up into multiple videos like should I actually have a new should I wrap this video up and have the next one go through activation and bias or should I just keep going what should be the symbol of the train I'm just looking at the chat nice Luxan we're up to 77 degrees in here it's getting warmer and warmer with my moving around so much keep going oh yeah yeah yeah yes know what okay I'll keep going yeah I just look if anyone is anyone keeping track how much time do you think that was you know obviously there were like several minutes in there when I like pausing when it gets edited one video everyone's saying well alright Matt Matt Chu is asking how long do I think the rest will take you know 15 minutes yeah that's what so let's let me keep going I think it makes sense to be in one video I started 18 minutes ago there's probably at least three or three minutes of editing there all right I'm gonna keep going we can always yeah let me keep going I'm being told from the live chat that's going on that the dot really needs something else and a cross product is something else so we're just gonna put the our symbol for matrix product will be like a nice little you know steam engine train thing okay that aside what else do I need to do so we've established this beginning part of the feedforward algorithm the inputs come in the weighted sums get added all together inside these hidden notes now there are two big components that I've missed let me just write this over here one is bias and two is an activation function I kind of not sure which order to talk about these things but before I do either those actually I think I want to do is take this formula and break it down into something much smaller so what if I think of this weight matrix is just a big capital W then I take I need a simple I can't draw a train here the matrix product between this and the inputs right and so the hidden layer and so we can think of this as hidden is a column so that's really J right hidden J equals weights I rows and columns times inputs J and when I say times I mean matrix product so this is me rewriting this because this here if I give myself a little space is really like what I'm doing here is I'm calculating H 1 H 2 and H 2 whoa I got that wrong let me go it is a regular dot product all right hold on let's just go back to where I was here because it's not I I see this as a column but it's rude one column it's it's a 1 it's a 1 there in the column spot it's row yeah ok I was just doing so well I'm a flaw of a very flawed person okay there was a time I think it was a time my life or I was really good at this sort of stuff and I don't know I just left it left I'm trying my best okay all right but before I get to the bias in activation I think what I want to do is rewrite this what okay before I get to the bias and the activation what I want to do is rewrite this as a smaller formula so I want to consider the weight matrix for example just as the capital letter W and I can think of it as a matrix of I rows and J columns I am j are kind of terrible cuz they look so similar but the weight matrix I I rose and J columns now we're taking the dot the matrix product which I'm just gonna use a dot here I think that's going to be fine the matrix product preme this and the inputs now the inputs is a matrix but it's one column but I rose and the point of the reason why I'm doing this is because I'm trying to get the outputs what I want is I want to know what number to send out of here the data is flowing in we get this weighted sum what number flows out of there and the number that flows out of there I'm going to call the hidden it's really I'm trying to get the output of H h1 h2 so the outputs of the hidden layer just the inputs are the inputs of the inputs of the hidden layer the outputs of the hidden layer are the inputs to the Yaak put a lot of inputs and output there but the previous layer sends in the input so whatever comes out of the hidden goes in here into the output so hidden I equals the matrix product between the weight matrix and the inputs but there's more so one of the things that I'm forgetting and I can actually fold these two things in right in here the things that I'm forgetting are one the bias so if you recall when in some of the previous videos that I've done that looked at linear regression and the simple perceptron is that correct that I talked about bias in the linear regression video in some of the previous videos such as in in the previous videos where I went to the perceptron for example remembered us trying to like find this line and what points would be above it or below it and I've got to really deal with the problem all the inputs could be 0 if all the inputs are 0 then the weighted sum is always going to be 0 that can't be right so we need this bias we sometimes need to make it easier or harder for it to so to speak fire like we want to make we want to like bias the output in a given direction so one thing I would write here is I'd also say plus that bias so and that is a single column vector as well so really if I'm down here again and boy am i running out of space but that's ok I would do this matrix product and then for every single one of these hidden I would have a bias b1 + b2 right so there are weights and by it and by lights perfectly legitimate to ask the question right now huh like well what are the values of the weights what are the values of the biases this is what I'm going to get to like I just want to understand the algorithm the whole point of this sort of learning system that we're gonna create is to figure out how to tune all the values of all these weights and biases so that the outputs match up with what we think they should be the assistant needs to somehow adapt and learn and tune all those values to perform some sort of task and in a sense this is really just one big function that's why I narrow Network something called a universal function approximator it's just a function it's just the function that receives inputs and generates an output that's a function and we have to in theory like if we'd have enough of these hidden layers and nodes there's no inputs we couldn't match with some given set of outputs so anyway where'd he get to all of that but I'm back here so I need to add the bias inand then there's something else you might remember from the simple perceptron example that we had this activation function whatever this weighted sum plus the bias would be if it was a positive number we would turn that number into plus 1 if it was a negative number we would turn that number into negative 1 and this is something that's very typical of neural network based systems whatever these weighted stones come in as we wanted to like squash them into some known range and there are a variety of different mathematical functions that can do this and while this is not typically the sort of latest and greatest and most cuttingedge activation function the function that is we will find in a lot of textbooks and early implementations of neural networks is something called a sigmoid and sigmoid is a function that actually looks like this f of X equals 1 divided by 1 plus e to the negative x sure I got that right let's go look at the Wikipedia page for the sigmoid function yes I mean I didn't see your image about the bias thank you okay so hold on I'm coming over here to sigmoid let's go to those Siwon oh well I actually got it right okay whoops oh this camera went off all right people are telling me that index should be j4 hold on let me just look should be j4 B and H I don't think so I really thought about this right the row is I the column is J the column is 1 the row is 1 2 3 the row is I the column is J 1 1 1 2 1 3 am I wrong about that I'm in the wrong totally in the wrong camera right the row is I column is J so this is really row 1 2 3 I mean sure if I beat it an array because it is a flat array ultimately but I'm going to in my implementation actually have it be a column matrix because you had to bias the left and right side of the equation wait a sec oh yeah yeah yeah that's that's bad I will fix that that day but yeah okay Kay Kay we c'mon I would like to hear from Kay weak bond chat if it's okay that I'm using I am I get flamed in the comments like I will about my 3 layer Network oh yeah whoops this is kind of messed up thank you I'll fix that well you guys are way behind me it's all right how's the hold on sobut dexif yes see above iii don't understand above i underscore j and a summation yeah Thank You Simon I will draw the bias in at some point that's a good point I need to sum over J and it's i underscore J so because these are really this right so aren't I summing over I 1 2 3 J is constant but I mean oh because I'm taking the J of here and applying it to their I see I see I see notation is like the worst thing ever yeah yeah yeah yeah right right right apparently not would help yeah all right I'm just gonna keep I'm gonna I'm gonna do my best here to keep going on I'm gonna go okay the thing I was talking about I was coming over here to talk about sigmoid when I come back I'll fix some of that stuff okay okay so here we can see I did get the correct formula for the sigmoid function what is this number E E is called the natural number it's the base for the natural logarithm it's like 2.71 somethingorother since we're one of these magic numbers or Euler's number you can read up about it but though and to be honest the sigmoid function is barely used anymore in sort of modern deep learning research but I think it's a good starting point for us to look at and understand why and in other videos I will take a look at other activation functions but the reason why the sigmoid function is used is because it takes any input any number you pass it through the sigmoid function you will get a number between 0 and 1 and so this is perfect for a lot of scenarios it takes this input it squashes it so higher numbers are going to be much closer to 1 lower numbers are going to much a little closer to 0 and the bias is gonna can push things closer to 1 or closer to 0 this squashing of it works really well because you can kind of you know we could get us or a true or false 0 or 1 we can get a probability value between 0 & 1 we get to the output lots of possibilities so this is the sig and it's sigmoid function so oh so ok so I'm over here by the way I need to correct a couple things where's that eraser number one is I kind of did something horrible here because I made these equations not equal to each other anymore when I was talking about the bias so let me not put that over here so ok another thing I want to mention is I'm kind of fumbling here with the these index values we're really in what we're really doing is we're the matrix W is rows and columns so I and J the input is a single column vector but we're still iterating over I because the I of the weights gets multiplied with the each row which is now I of the of the input so you know the exact notation aside the point of the feed for one that the the truth of the feedforward algorithm is the inputs come in you take a weighted sum you add in the bias you take that weighted sum add the bias pass it through the activation function and that result feeds forward towards the network and go straight to the output and guess what the output does the output takes whoops the weights of all the connections between the hidden and the output so the matrix product of the hiddens with those weights plus its own biases these are different biases these are the biases for this particularly the output node and then passes that through the sigmoid function as well so this just happens every sing with every single layer here come the inputs weighted sum passes the activation function send that as output into the weighted son of the next node pass of the activation function send that as output etc etc so I was getting the question it was being point out that he didn't draw the bias anywhere into this anywhere into this particular diagram so a nice way to draw the bias into the diagram is to think of it as another input at each layer so for example if there were always an input that had a value of 1 that connected like this now we have bias 1 and bias 2 so these are just these are like weights that are getting weighted with an arbitrary input of 1 that's always coming in and the same thing with here now we have also another bias that's coming into the outlet so it each layer there is a sorry a 1 which which it's weight is like this bias there's only one so B 1 so you can think you could you'll often see I could probably pull up a much nicer diagram okay I highly recommend you go watch but you've just been watching the three blue one brown video all along if I'm being perfectly honest but this hopefully gives you a sense of what the pieces are the point of me doing since you could just skip ahead to the next video in the next video I'm going to start to have a matrix library which does this matrix math I can add two matrices together element wise I can perform a matrix product I can apply a function like the sigmoid function to every element of a matrix so I can do all of this I can start to write the code for a neural network library okay we have now arrived at the what is sort of the end of this video I'm going to pause and check and see what all the wonderful nice people who are generous enough to know who is the longlive have corrected me and see if I need to come back and offer any Corrections or answer any questions I'll be right back would it be clearer to write out the equation for the output layer as well shouldn't be too difficult with just two weights that's not a bad idea why don't you add the biases to the weight matrix instead of having an extra matrix I don't think that would work umm so Simon tiger writes a bias doesn't have a weight and that is correct but I like to think of it I mean I think a weight the bias these are just numbers they just get added in but it's the equivalent if I wanted to of having another input right it's equivalent of having another input of one and then adding another column and those would be the biases to the weight matrix so I could do that maybe I'll mention that so instead yeah I couldn't mention that all right you were using the same name for both weight matrices yes that's a big problem yeah I meant this to be like I'll call okay so let me let me let me let me do some of these Corrections okay so a couple Corrections that I should do is you know these are not I'm being very unclear about this these are not the same these are not the same these are not the same so this weights here these are really what I would call like the input to hidden weights so I'll call this like WI H the weights from and you're going to see this kind of variable naming when I start to write the code for this the weights from input to hidden and this is really the weights from hidden to output whoo WH oh so that that's one point of clarity that's important and also once again for the biases these are like the hidden by season these are the output biases so and you know again I'm kind of on some level making up my notation onthefly I'm sure if you were to read a properly thought out textbook or watch a three blue one brick on video all those some more thought some more precise notation but this is the way I'm think another thing that I'll mention here which was introduced if you know the biases you can kind of write it as this extra matrix that your column matrix that you're adding on but I could have in fact also just folded those into here right because if I added another input that's always one like I added this diagram here then I would just have I'm sorry not another row another column which would be the bias values so I could actually have b1 and b2 here that could also that could I could also express and put the bias ease in this matrix calculation as well I like to think of them separately to each one's own to each their own okay we should I say goodbye I don't know if I should say goodbye now let's probably check one we're done over that blue diamond superscript is common that would have been good change the O to H got it wrong change the o equals to H equals we're ah shoot no no I'm right no I'm wrong Oh oh dear oh boy all right snack let's do that here oh oh oh so sad hold on it's okay we're gonna back up we're gonna do that little addendum to the video again no one will notice and then I can use superscript you can have subscript for the index values superscript for the okay I totally do that wrong still not as bad as last time yeah all right okay let's now let me let me kind of like re let me come back into alright so there's definitely a few things wrong or misleading at least I would say about this that I've gotten some good comments from so that's all right number one is it's very important to realize that these two weight matrices and these two by C's are not the same I wrote them out I mean we have the hidden outputs and the outputs output that's what this is that's what this is so the hiddens outputs are actually the weight matrix between the inputs and the hidden so I could write that as like superscript up here WI H in a way this is the the hidden sout puts is the waitwaitwait matrix between the input and hidden with a matrix product of those inputs and this weight matrix is the weight matrix between the hidden and the output h0r whoo so and you'll actually see as I get into the next video I'll start using this kind of IH Rho kind of naming in some of the ver stuff the other thing is these are not the same biases this is the bias for that's connected with each hidden neuron and so I could say B H there and then this is the bias right that's connected with it's just one at one bias but it's connected with this output neuron and so I could put an O here so that would be more clear about that another point of clarification is there is a way to write this without having this having the bias as part of the weight matrix itself because there's no reason why I couldn't just consider the bias like I said as an extra input that always comes in with a value of one and then what I would need to do where's my eraser is I would need to add a fourth column here right because this weight matrix would have all of these connections 1/3 and 2/3 and then there would be the bias values bias 1 and bias 2 so if you see this this would actually end up now I could just add it over here if I follow through with this math plus b1 plus b2 because it would be multiplied by 1 which would just be that bias value so that's an important thing to say how do I possibly finish this video some people were asking what am i why don't I write out the matrix formula for for this uhhuh you know what maybe I should make this an exercise for you and I can maybe include a little link to a JPEG or something that has it but could you do this same exact diagram for the calculations that flow once the outputs come out of here through this layer so let me just just give me a second I'm gonna check see if there's any last comments or questions or important Corrections I'll be right back all right all right looking I'm looking anybody else have anything else it's all wrong okay better than last time at least this is good so it's gonna say on my like tombstone better than last time well at least that's what so I'm a big poster about the coding train it's a little bit better than last time okay okay okay it's important to mention the weight of the bias changes yes but the bias baked in the matrix do you still add the Bisons in there for us feels like the Chaz more importantly usual for this video just name the video chaos I just new teachers at least yeah we had we can we have this great tshirt idea what was that great tshirt idea it was Oh something like I watched your YouTube channel and all I got was this lousy I watched all of your neural network matrix math videos and all I got was this lousy 0.27 somethingsomething yeah back backpropagation is gonna come on Friday will not be doing that today no no no not today alright so I think I'm gonna wrap this up okay that in fact was the end thank you for watching this video to see something wise okay that in fact was the end thank you for watching this video I do want to mention that I I'm in the chat or a lot of really useful and important points and comments about how my notation and superscript versus subscript and I or J this is not perhaps the most conventional or correct notation so I apologize for that feel free to leave comments in the comments section it's particularly good ones if there's a really like one that sums it up all perfectly I'll in it right to the top but the point of this was for me to kind of like get through the basics of this I am now going to in the next video actually implement this in code and once I've done that we'll be ready to then look at the learning algorithm the training algorithm this thing called back propagation implement VAT and code then the neural network will be complete we can actually use it to solve something I hope so see you in a future video thank you acting all right now here's the thing I also need to was I not in the right screen there no I was all right all right I'm in the wrong screen now okay so what I need to do now is recorded a brief introduction to the next video see I think I already did the coding part and I think the coding part is a fine to go forward without me doing it again right I'm not sure you're still watching I don't even the problem is I don't remember what I did let's let me take a look at that while I'm here this is yeah this is like wildly different to it again please I think I should do I was excited for my it was very short but was fun alright I'm gonna I'm gonna leave all options on the table okay first I'm gonna just record a short intro into this video that explains why it doesn't match up then I'm gonna redo it again yeah note there give me any new stuff that's happening today I'm not I don't have the time to start doing the gradient descent and all of that because I and is the matrix I got to talk about the code I have not really published the code I got to think about how to do that give me a second everybody so remind me about that all right hello you're about to watch your video where I implement what I just did in the last video with code the only thing is in the last video I look like this and in the video are about to watch I its wilderness day it was in the wilderness reading textbooks about deep learning and also the whiteboards not gonna match exactly this is the whiteboard that I just completed where I really kind of like worked on and figured out all the notation in this diagram and then when I start to implement the code I might refer to this whiteboard which looks different I got to just redo the video you're never gonna see this this is this is no good we're gonna redo the video I mean let me just do it again I mean the only thing that I'm sad about now is they don't come the only thing I'm sad about now is I don't get to have like a nice shot of me like this I'm gonna talk about this anyway I'm going to talk about this anyway cuz you know people people should know the world should know what I went through to try to get these videos out but let me first go back to I have to reverse I have to take stuff out of the code right I don't think there's anything that I do did I do i think i made the from array function in that does anybody remember I think I made the from array function in that second video right so I think this has to go and the two array function I think those are the only two does anybody have a like a solid memory of this I think the matrix so this I'm going to recode everything here right this I'm gonna pletely rewrite the code and yeah Simon so I did it all as one live stream but my intention is to chop it up into several different videos so even though it was there so yeah it the two array functions weren't there okay thank you so I'm gonna go and put this somewhere just like make a backup of it put it somewhere then I'm going to what I'm going to do here is alright so this is what I had so let me just go back now yep oh right I made the joke the tshirt thing that was at the this video yeah yeah okay it said lots of matrix math and returned guests okay so that's what was here this tad did not have the from array or to array functions and so I will also put that in the little backup file that I made just in case just to have those there and there we go okay up to me yes I'll make this the same joke yes yes don't worry I apologize I will make the same joke it will not be as good because it won't be spontaneous I think you changed the randomize function to thank you that is such a good catch it was like it was something like this right because it was giving me random numbers between zero and ten that I recall as well thank you all right I think I'm ready to go I had the perfect freeze frame okay there you go all right oh thank you Simon now that's a pretty good idea actually I do like that improvement for the two array thing the only thing is if it won't work if it's a full okay okay all right yes I'm very good at picking the right freeze frame all right hello it's time I admit something to you which is that I actually recorded a live session where I tried to explain all of this fee Ford algorithm thing a while back when I and apparently this like wilderness hair so I workshopping my wilderness joke give you kitty let me do that again cuz by wilderness joke is kind of important all right it's got to make it in there well there's Dan okay I have to do a fivemile trading run for the New York City Marathon today I gotta get going I'll talk about that too in a second all right yeah I'm gonna I'm gonna start over here okay so if you watched the previous video first of all thank you oh my god camera went off good timing hello now if you watch the previous video hey boys thank you well I hope you're not too mad at me and I didn't file too many complaints in the comments there but in the previous video I talked through the feedforward algorithm they attempted to map it and graph it I attempted to get all the indices right to explain why we use matrix math for it all that sort of stuff now we got to get to something we got to get to the interesting part which is actually to use the neural network for something it's thinking me awhile to get there I will get there but I have to confess you thought that was bad I actually uh wilderness Dan over here tried in a live stream previously to go through all this and got all the index values and everything wrong was so bad it couldn't even like get it together to turn into an actual tutorial video that I'm making the playlist but it does exist if you're interested I got a haircut seem to fix a lot of things I look like a much more professional person you can go back and watch that previous those previous attempts so I'm actually done so this is my second try I'm just being honest here all right so what is it that I want to do in this particular video we can close all this stuff what I want to do is I have a matrix I have a matrix class that I've developed I have a neural network class that I've developed what I want to do is I want to be able to write code like the following timeout pause I just realized I forgot to delete this I want to be able to write code like the following I want to be able to go into my p5.js sketch or any javascript program and say something like let neural or let n equal a new neural network maybe it has maybe I want to solve X or so I want to have two inputs I just want to have one two hidden nodes and one output so I want to make a neural network object and I want to give it an architecture then I want to do something like I'm going to make an input like I want to send in true false 0 1 comma 0 I want to be able to say let output equal neural network feedforward that input and then I want to say console.log output so this is the code this is what I want to be able to do I want to be able to use the neural network on a kind of higher level and once I can do that then I can have inputs that are all different kinds of things where I'm doing data science or some kind of like learn to play flappy bird or whatever it is that I'm doing ok so let's do this what I have so far here is just I have that feedforward function it takes an input and returns that guess now there's no code there I need to fill in the code all that matrix math I need to take everything that I have here and I need to put that all into here so how do I do that all right well my neural network needs some more stuff for example all that it has right now is the number of input nodes which is eyes just sent into the number of hidden nodes I sent into and the number of output nose I sent in one well what I need is I need to keep track of those weights right I need to have weights and what do I need to have weights between we need to have the weights between input and hidden and the weights between hidden and output so those we've established our matrices weights between input and hidden is a new matrix and now it has a certain number of rows and columns it has a certain number of rows sorry columns based on the number of inputs and rows based on the number of hidden nodes so we're gonna say it has this dot hidden nodes is its number of rows and this dot input node is its number of columns so that's one wait mate ryx and another wait matrix is between hidden and output and that's going to have the number of rows which is the number of output nodes and the number of columns is the number of hidden nodes so I've created these weight matrices now oh you have an interesting question when we create a neural network how do we pick the weights again the whole point of this is we need to have some interesting wonderful exciting complicated weird algorithm for tuning the weights for finding the optimal weights for whatever type of application we're trying to build but to start we just want to give it random weights and did the whole field of research dedicated to figuring out like good ways to see the neural network with good kind of digging weight so that you can get the optimal weights more easily blah blah blah but for us right now I just want to give it random weights so I can actually just say we've already built a randomized function into the neural the matrix library so I can say randomized so this will randomize the weights and actually though if you recall I was reminded by this in the chat that my randomize function for some of my demonstrations I was actually picking a number between 0 and 10 and it would make much more sense to get a random number between maybe negative 1 and 1 to start at the weight so I could take math dot random which is 0 or 1 multiplied by 2 subtract 1 and I've gotten a random value between negative 1 and 1 okay so we're doing well what else do I need I should probably I should probably keep track of the bias the hidden bias and the output bias again as we saw how sort of talking about it could put that into the matrices but I'm gonna keep track of that separately it's a little easier for me so I'm gonna say this dot bias hidden equals a new matrix that has wha how many biases do I have I have o based on the number of hidden nodes right that those are the rows and then one column is it yeah because it's a one column vector and the bias or the output is based on how many output nodes so for every I need a biased value for every node in every layer so I need to bias values for each hidden node and one bias value for each the single output node but you know again my neural network library allows me to create neural networks with any number of hidden nodes any number of inputs any number outputs it is just has these two layers and the inputs but so at some point it might be worth expanding it so I got it multiple hidden layers that sort of thing but this is gonna work fine for now okay so what else do I need now ahhahaha there's something we're missing so for example let's just what am i okay what am i feeding in here feedforward input this is just an array but technically in order for me to be able to do the first stop right the first thing that I want to do is I want to say this dot hidden I'm sorry I'm gonna make this let hidden this will be the output right I want to compute the output of the hidden nodes that's going to be a onedimensional of one column matrix that is going to be that is going to be the matrix product between the input and this and the weights and the weight weight matrix between and actually I have to say the weights first sorry the weights remember the order in matrix multiplication really matters the weight input hidden with that input then I'm going to say hidden dot add this dot bias so I do the matrix product of the inputs and the weights then I add in the bias and then I'm gonna do activation function I won't do that just yet but even this is no good so far this is no good so far because this input what does this matrix multiply function expect matrix multiply in my library expects to matrix objects and this the way I've written my sketches I just made the inputs an array which is an incredibly convenient thing to do I don't want to make my endusers have to like figure out they just want me I'll send the inputs in in a simple array so one thing that I should do here I mean I could test I could check if it's already an instance of a matrix I could skip it but what I want to do is I want to say I'm going to call this input array I want to say inputs equals matrix from array input array and the reason why I'm writing this out is because I've thought of this already I need to be able to make a matrix object from that array so I want to be able to have a matrix object so that my matrix multiply function will work and I could create a different constructor that takes an array but I think what would make sense is for me to add right up here in the beginning right and a static function that's that's called from array it takes an array and what I want to do here is I want to say let may m equal a new matrix that has just that has a number of rows based on the arrays length and one column right I want to create a matrix that looks like this it has a number of rows based on the arrays length and one column and then all I need to do is say for let I equals 0 I is less than array length I plus plus and I'm going to say data oh sorry sorry m dot data index I index 0 equals equals array index I and then return n so this is just again oh I'm sure every lots of you could probably think of some type of array functionality that I could use really easily but this is creating that matrix and then just that matrix and then putting the input into it so just to be sure about this let me let me say m dot print all right let's take a look and I'm gonna just edit edit point for a second I know why this computer went to sleep all these notifications alright okay sorry I'm not running anything so I just need a second here no I want to go to desktop I lost mine okay no I changed it was this dotmatrix I've since changed it to this data so some of you missed a key point where I did that yeah yep yep yep okay okay let me just make sure this works so I have some errors here oh that actually nicely printed something out for me that I didn't realize it would so what I want to do is I want to say let a array equal a 1 comma 0 comma negative 5 and then I want to say matrix dot from array array and we should see there we go we've got a column a single column matrix 1 0 negative 5 so that function is working the way that I hoped it would so I can take out this debugging and I can keep this in here and now if I go back to this code I can say okay we get an input array we turn that into an input matrix and then this needs an S here we multiply we do the matrix product of the inputs with the weights we add in the bias we one more step left to go we need to do the activation function so we need the sigmoid function well it so happens that there isn't a magical sigmoid function that exists in JavaScript however there is in the math library a function called exp JavaScript math let's find this documentation page this X P Euler's number also known as Napier's constant does for us e to the X so I can write my own sigmoid function so I can just um and at some point I probably will allow the neural network library to use different activation functions but right now I'm just globally gonna write a function called sigmoid it's going to take one input X and it is going to return one divided by one plus mass of X somebody factcheck for me but I believe that is the sigmoid function and so the wonderful thing now is once I receive the inputs once I've done the matrix product of the weights and the inputs once I've added in the bias I can generate the outputs with the activation function just by saying hidden dot map sigmoid if you recall I wrote into my matrix library a function called map which just allows me to apply any arbitrary function to every element of that matrix so so there we go now we're done so this is this is all the code for generating the hidden outputs and this is the activation function now where are we in the diagram we've got the inputs we did the matrix product we added in the biases we passed the activation function now those outputs are coming in here so what do we need to do now take those hidden outputs and do the matrix product with these weights so we're gonna do exactly that same thing we're going to say let output equal matrix dot multiply this this dot weights now I need the weights between the hidden and the output H oh I need the hidden outputs which is just calling hidden then I need to add in those output biases did I call it bias oh yes I need to and by the way I just realized I never gave these any initial values so it would make sense for me to also randomize these so I'm going to randomize those biases so right so now I've got the weights times they hid the matrix product or the weights and the hidden outputs and added in the bias and then output dot map sigmoid and guess what returned output that's all I want that's done that's the last piece I want to send in the inputs right over here sorry in my actual code I want to be able to send in the inputs and then read out the output but here's the thing I sent in the inputs as a simple array then internally converted that into a matrix oh I could do all the math properly now before I get the outputs back I don't want you want to send out a matrix it's gonna be simpler if I just send out those outputs as an array so much like I wrote that from array function I am going to create a function called two array and that doesn't need to be static because I'm gonna take any given matrix object and return in a right now there are some back I probably transpose it and and do some fancy way or use the slice function I'm again I'm gonna do this just in a way that I know will work will catalyze refactor this later in the end I'm gonna refactor all this using somebody else's matrix library anyway so let me think about how I would do this first I want to make an array I'll make it empty and then really all I want to do is I want to loop through every element of the array every element of the matrix and what I want to do is I would want to say array dot push every element of the matrix and say return all right so this would actually take any twodimensional matrix and flatten it to one Mitchell array the question is though one thing we should ask ourselves here is it appropriate to have the columns as the inner loop so if I think about it if I wanted to take I mean obviously doesn't matter if it's this matrix because I just want to take all these and put it into an array but if it's this if I want to take like this array and flatten it do I want to go do do do do do do or do I want to go do do do do do do I think I want to go this way do do do do do I got distracted by my own singing of the iteration song is just dude alright sorry I'll get back to this I apologize I think I want to iterate through the columns on the inner loop which it looks like that's what I'm doing so I'm gonna keep it this way except for yeah okay so I'm gonna keep it this way and I'm gonna say here now in the neural network I can say return output to array okay everyone this is it this is the whole feedforward algorithm this is receiving the inputs generating the hidden outputs now generate whoops generating the output output and then sending it back to the color alright so this is the whole algorithm layer this is like layer 1 and layer 2 the hidden layer the output layer what have I missed we'll see ok um all right let's look at this now yeah it's probably no need to randomize the biases but that's I did it anyway ok hold on hold on timeout ok what do we do now let's try running the code I mean it's absurd but because this does nothing but let's actually let's just run this let's just see can I create a neural network give it some are very input and get some output can I get it with no mistakes it seems very unlikely that I'm gonna have no mistakes and this is the reason why I shouldn't use my drum roll effect but I'm gonna do it anyway okay here we go let's run this code it worked is a strange thing to say cuz relay to get any errors and probably one reason getting errors because there's actually my second try of building this out and you might be wondering like oh okay so I just watched like two and a half hours of like 8 videos in a row about matrices and neural networks and you just went for point five nine three one zero seven so you know and it we should probably get a tshirt that says I've watched all your YouTube videos and all I got was zero point five nine three one it was funnier when it was like a low I made this joke before in the previous attempt it was a lower number so now point two seven is a funnier number like great like like what's a funny I feel like though that's pretty good I feel like it's gotta even be lower the lower the number is the funnier there we go see now it's funny oh wait a second okay so the thing is we have no when you gotta start testing this we've got to come up with a server we're getting close we're what I want to do probably the first thing I'll do it's not that interesting but at least allow us to test whether this code works is can I train it to learn and operation or operation or X or exclusive or which is one that I've talked about that can't be solved with a single perceptron so can I feed it in one zero and train it to get the number one out can I feed it in zero one train to get the number one out but if I give it zero zero one one I get the number zero out so I need to attend that but I am missing a lot of functionality from this neural network class I have this feedforward algorithm function now what I need is to write a function called train and what I'm going to do with the train function is I'm going to give it some inputs and then I'm gonna give it a known answer so the idea here is like with feedforward I'm really just saying take these inputs and give me an output with training I'm gonna say take these inputs that I have a known labelled answer to and do something to yourself based on that and that's what I'll start getting to in the next video I have to talk about back propagation and gradient descent I have talked about this in previous videos will kind of return to that then I need to finish implementing this function and then I'll do a coding challenge where I try to solve XOR and do like a simple digit recognition or something with this particular neural network library okay thanks for watching umm just them going through this I'm trying it out and see if I can make this move happen to be things would be but all things all things will come eventually so thanks for watching and as always I don't know subscribe like that helps supposedly and because then YouTube's neural network I'm just copying three blue and brown stroke at this point you should watch three blue one Browns videos good and I learned so much just from like watching those and I'll be back in the next video it's a point thank you all right machi I hope I saved you a lot of editing time and save I think we're good I think we're good I think we're good for today did I forget to apply the weights to the biases I don't think I did I think I'm okay I think I did everything did I miss anything important a Bryan I've been waiting for back propagation propagation a ycu sigmoid and not hey you to French activation function from what I understand good question I mean I should use it but I'm not prepared to because I just kind of want to go through what the other let me make sure I talk about in the next video though but yeah I should come back to that okay it is 345 one of the reasons why I have to leave is I got to make at home but I'm also I guess I'll mention this here for those you're watching I am training for the New York City Half Marathon which I'm doing a fundraiser for crowd rise wonderful organization Lower East Side girls club if you liked watching this live stream today and feel like donating I am here we go I have to you have to do through my page so get the credit it's very important hold on I'm not here team see more Jane your shipment okay here we are the Lower East Side Girls Club is a wonderful organization that does afterschool programs they have a great makerspace they have a planetarium they do wonderful stuff with code stuff so my students have done workshops over there I've hung out over there it's a wonderful location place notforprofit on the Lower East Side if you are feel so inclined I am fundraising for them on running the New York City half marathon which I hope to do it under two hours I have run a half marathon in under two hours before it was like 1 hour and 59 minutes so I'm just gonna plug this for a second for any of you were watching here Thank You Alka for putting that link in the chat okay I just wanted to mention that since I'm not promoting anything else today I will promote that so yes thank you that's a good comment um alright so thanks everyone for tuning in I should be back on Friday at the regular time my intention on Friday if I am so lucky is to get all the way through solving X or simple goals so I want to do the back propagation step add that to the library and then I just want to make the X or and visualize that you know the mages of do you know visualize that no I don't know that they're to do some stuff with it so probably X or and M missed if I can okay whoops was that link no good thanks for trying Alka I can paste it in hold on because I am actually in the chat as well there we go okay and whoa I'm behind here all right okay so this is it for today it is 348 I gotta go so thank you everyone for watching send me your feedback at Schiffman on twitter like subscribe all that nonsense and yeah I'll be back on Friday if somebody can by the way find this song for me I think I found it at one point I think was on free music archive and and it had the word rainbow in it but also it could have been that I searched for it on I think zone I think free music archive could have been on soundcloud I'm not sure all right yes Ray Lewis stands for rectified to the newer unit that is correct all right my link didn't work either oh well let me at least show you what the link is I'll take us while I'm wasting time doing this I'll take a few questions from the chat alright let me take a couple questions in the chat when will the code hi yes good question so okay it's a couple questions so taco dude asks why don't you make it so you can have more than one hidden layer I absolutely should do that I'm starting simple building these out at some point I don't know how far it makes sense for me to go with this neural network library because eventually I'm gonna try to replace most of this with this deep learned j/s thing but c'est la vie but people ask about what to do about the code so here's the thing the code for this ostensibly is on github.com slash Schiffman neural network p5 this you can see like oh look here's the neural network code way that looks kind of the same but kind of different here's some matrix touch yeah oh boy looks pretty different it's not even es6 so I made this a long time ago not that long ago last commit there's somehow five days ago but really is like six six six months ago or so that I would like last did anything about this so I made this a long time ago using your an alder you know people will find you'll finally tweet at me I'll send you did I don't know how many of you really okay so I don't think I need to waste too much time about the URL but so hold on a sec we might so this is the code so I I want to update this and have this code match what I'm building but I'm also happy to accept pull requests here to optimize and fix this up so I think what I probably should do is have the raw code from the examples go in radan bow code and it would probably go under I would say if anyone wants to help me with this courses nature of code and it would be a video started numbering with and or intelligence and learning session for I don't know this never really got anywhere so maybe nature of code it should be like there should be a a subfolder here that has the code that goes along in steps with each and every video precisely and then a sort of finished version of it that's maybe a bit more optimized would end up here in this library so I would gladly work except any pull request and help in that direction that's kind of what I'm thinking of by right now I'm Simon in the slack channel and slack channel for patreon subscribers asked when do you are you going to do kmeans it was on my list that I kind of never got to it so I don't know I gotta have to come back to that at some point read random numbers someone's talking if somebody just donated oh my goodness well I'm deathless whoever just donated that I should give you a massive thank you that is so kind and nice of you Topher J thank you so much Topher J these random numbers I'm about to read are dedicated to Topher Jay thirty six thousand four hundred fifty three fourteen thousand nine hundred twenty eight twenty three thousand nine hundred and thirty one thirty two thousand ninety five thirty thousand seven hundred fifty seven fifty six thousand eight hundred and two thirty four thousand eight hundred eighty five eighty seven thousand one hundred sixty nine eleven thousand three hundred eighty six and ninety eight thousand six hundred and fourteen breaking is breaking is did the decoding train breaking news somebody I'll have that sound effect I'm being told that maybe Free Music Archive dot org slash music slash the underscore Colombians just like a rainbow you're only hearing this through my mic yeah that's definitely it there we go thank you very much I really I need to step up my music game I'm you know it's a tricky with sort of copyright stuff but I you know I kind of like Josh Scott Joplin would be good I think for me like it kind of Keystone Cops like aesthetic Maple Leaf Rag would probably be good so anyway yes okay so green thing on my desk what's the green thing on my desk I mean this thing it's just another laptop it's where I have a slack chat going I hope I didn't just violate anybody's privacy in the slack chat okay will you ever make a neural network that can play a game that you can make live for a coding challenge yes that's kind of my goal it's raining tacos no matter this guy tacos no no no my why do I know that song tacos and all right I've really got to go thanks everyone per burger bugeyed burger Bob I'm very sorry to say that I'm now finished with the live stream and it will be back on Friday well how long was this today two hours something like that alright um thanks for watching yeah and see you next time yes Elliot chill I will return steering but behaviour at some point