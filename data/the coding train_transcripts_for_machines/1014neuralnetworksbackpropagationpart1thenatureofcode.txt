okay here we are this video.i what I'm going to do in this video is I'm quite terrified at this moment I should say but I'm gonna start to talk about what it means to examine the output we fed in some input we did the feedforward algorithm we got some output I want to know look at that output and I want to say what do I think about that output really is it good output is a bad output is it right on is a little bit off and I'm going to this is a technique known as supervised learning I have a teacher I'm now gonna say this output is incorrect please adjust all your settings to make this output more correct and I have done this before I have done this in a video about linear regression and gradient descent I have done this in a video where I made a simple perceptron where I did this same type of learning algorithm in fact I've done this in genetic algorithm examples or I'm not using the same exact technique but a different technique to sort of teach a system to do something so this is what I want to do you can think of all of these weights that are inside the network as these little knobs little settings and I just want to like always adjust the settings and so the key there's gonna be key terms that are gonna come up here like error what's the difference between the output and the known correct answer that's the error the cost well over a large training data set what's the sort of cumulative error that's the sort of cost the current cost of the neural network and then this term that's called back propagation so if the error is the thing that tells me how to tune these weights the error is right here it's gonna tell me how to tune these weights how do I tune these weights they're not connected to the error they're connected to the thing that's connected to the error so I need to propagate backwards feedforward is the process of moving all the data forward through the network back propagation is the process of taking the error and basically like feeding backwards the error through the network now here's what we have so here's where I have got to admit something this is probably I would say to think of a topic that I've tackled in any my videos that's like harder this I can't think of one I don't know that I fully have a deep understanding of this I have implemented it before I spent a lot of time I prepared some notes I prepared some notes for the first time in my life which are right here that I'm going to use while I start to explain this but it's kind of unrealistic and I'm probably not the best person to go into all of the math so what I'm going to attempt to do is give a kind of just general overview of how the algorithm works look at how pieces of it actually work the math of it in particular as it relates to matrix matrices because I'm gonna need that understanding to implement it in code and then I'm gonna present some of the formulas to you that are the formulas for how you change the weights based on the error and then try to implement those formulas in code so this is my plan so probably take two or three videos but so my goal is really the implementation and I'm gonna provide to you a bunch of resources if you want to dive deeper into the math let me just mention those to you so number one make your own neural network by tariq rashid this is a book that i was reading on the subway this morning you can actually get it on the kitten kitten your Kindle app for a very inexpensive amount you can also find it here on an a lot books i Remmick recommend on my coding train amazon.com sheet slash shops mass coding train the three blue one brown video series what is a neural network what is back propagation really doing back propagation calculus you could pause right now and go watch these videos which would give you a much deeper set of knowledge about the math that's that I'm going to use as well as this the essence of calculus right one of the things that's used in the math is the chain rule and the product rule so this is this this particular video might be useful to you as well as this particular online book which I found through the three blue one Brown videos neural networks and deep learning and I'd be remiss if I didn't also mention Suraj's YouTube channel that I've mentioned a lot on this video has a lot of different a lot of a lot of my videos a lot of different videos on similar content especially if you're interested in Python and using the tensorflow library that kind of stuff okay so that's that now let me stir I think I think we're ready to start I've erased the whiteboard and I am now ready to start talking about the backpropagation algorithm so let's assume right now that this is my output neuron and just for the sake of simplicity at this moment let's just pretend this is one of the hidden neurons but let's just pretend that there's just one of them so there is some wait there's also a bias but we'll come and to come back to the bias at the very end so I'm gonna kind of do everything without the bias and then come back to the bias there's some weight which is connecting this hidden neuron to the output neuron now the input of the output of this neuron multiplied by the way it is sent through here pasture the activation function and we get some sort of answer let's say that the answer that we get is point seven so this that can be referred to as the guess the output but I'm gonna call that I'm gonna call it the guess now in the case of supervised learning where I have a prepared data set where I have sort of like known answers so I'm gonna train the network to have these weights so that later on I can put in unknown data to get good results I would have some sort of answer so I'm gonna write that here and I'm gonna say the known answer is one I wanted this neuron with this particular input that came in I wanted to see the answer of one so this means now I also have an error error and the error is calculated with a simple formula what is the desired output the answer the guess what's the difference between those two things so we can see is that the error is 0.3 so in my simple perceptron and in other videos that I've made I've then taken this error and used it as a way to basically nudge nudge that weight I wanted a one I only got point seven I can make the white a little bit higher to get more stuff right I want more a lot more stuff right I want that weight higher maybe the bias needs to be addressed but four point seven I just need to increase that way to increase the weight in the direction of the error that's how this works now here's the next piece of this let's say however that instead of just one weight coming in here I have two weights because there are two hidden neurons h1 and h2 now we have a problem we have this error which I know I need to nudge wait 1 and wait but which one's really kind of responsible for the error there's a lot of blame placing going on here normally I would think like let's try not to blame anybody but this is the problem and I could just say like I don't know take half of an error increase them both increase they're both the same amount but there's a key aspect of the way that the learning process works with gradient descent and back propagation is we really need to figure out who's responsible for the error so let me take the scenario these weights could actually be weights where this weight is zero point two and this weight is zero point one well you could be made set we could now make the argument that this connection is more responsible for the error because it has a higher weight in fact it's twothirds responsible right this weight is double of this weight so in fact when we do these ninjas we take this error to nudge this one but we'll nudge it only by 33% and then we'll take this error and I know this is going to go out of your view but it's coming all the way back here and won't touch that by an up let's see can you see this here 67% actually just do it back here 67% so this is a key aspect of the bout and this is I've basically done this before so this is where we look for this Delta weight we adjust the weight based on that error and the inputs passing through so maybe this makes sense to you and if it does good here's the tricky aspect this is why there's something this is why this video is essentially about back propagation this is not the diagram of the neural network that I created in the previous videos in fact this hidden layer is connected to the inputs and all of those have weights and I I spent a lot of time worrying about the indices let's get those indices correct so this if this is input 1 and this is input 2 we usually call it sorry I usually call those X and this by the way the output we can refer to as Y it's usually what's often referred to as Y so if these are this is input 1 X 1 input 2 X 2 this weight is the weight from 1 to 1 this weight is the weight from 1 to 2 now that might look backwards to you it's the weight from 1 to 2 or it's connected between hidden 2 and what but the reason why I'm numbering it that way this is that it's going to be in the second row of our matrix this weight is the second row of our matrix and this is a weight from 2 to 1 so I write 1/2 and this is a weight from 2 to 2 so I write 2 2 so here's the thing if this error which is happening right here is used to tune these two weights proportionally based on their weights well how do I connect this error back to these weights how do they get tuned well it what I need just this is a what wasn't good to realize is this is like a little section of what could possibly be a much larger neural network right there could be many more layers this way and many more layers that way so these weights are just tuned based on whatever this neurons error is right here's at the end so I can actually calculate the error directly but here they're not connected directly to this error that sorry here they're not connected they're connected to these so what is the error coming out of here if I just had that if I had like hidden efore error error hidden one what is that equal if I and I knew if I knew what this was right if I knew the error coming out of here error hidden 1 then I could adjust these weights because the air coming out of here adjusts these weights this error just these weights and this error e to error hidden if I knew what that is then I could adjust the weights coming in to that so this is the idea of back propagation there's an error here it goes to here how many no but if I could calculate these errors that I could continue to go back here and then if there were more I would just calculate these errors and keep going this way so this is the real question how do I calculate that hidden air how do I calculate the error of a neuron anywhere within the network that's not necessarily directly connected to the output where that error is just a simple target out guess to figure this out I'm gonna pull just the section of this diagram out over here I'm gonna need to progressively over time make this diagram more and more complicated but right now I'm going to simplify for a second again so what I want to do is I just want to take actually it's sort of here already but I just want I just want to pull it over here so this is the output and these are the two hiddens this is the weight of sir this is the weight of 0.2 and this is the weight of 0.1 and the error here is equal to 0.3 so what I want to and this is the hidden layer what remember what I'm trying to you know h1 and h2 what I'm trying to calculate is the error of hidden one and the error of hidden okay so the way that I do that is by taking a taking this error in getting a portion of it what portion should I get and I sort of said this already 2/3 and 1/3 so if this is weight one and this is weight right there's a very simple scenario this is almost like back to that perceptron again then what I want to do is say the error of hidden one is weight 1 divided by weight 1 plus weight 2 times the error of the output e output the air of Hin and 1 is a portion of that outputs error the error of hidden two is weight 2 divided by weight 1 plus weight 2 times that output error again we sort of said this already I realized I'm cut but this is fine it doesn't hurt tutor to do this twice so these would actually be the errors here these are now the errors and I can just use the same gradient descent algorithm to tune these weights but this actually is kind of a rare circumstance where you know everything's just connected to one thing so what I want to do now is I'm gonna make this diagram a little bit more complex and I'm gonna take the case of having more than one output neuron see how this works I'm gonna add another so let's say there's two outputs which means there's error 1 and error right maybe this is trying to recognize a true we could actually recognize like true or false or something to recognize a 0 or a 1 so in that case maybe the desired answer is you know it's is 1 & 0 but we got we got point 7 and point 3 as the outputs so here this error I mean this let's make this point for this error would be negative zero point four right and now what I need to do is have more connections here and once again this is and I'm this is for this matrix I've got kind of the same notation 1 1 2 1 1 2 2 2 so if this is our diagram now with two outputs so there are two errors known errors that we can calculate in our supervised learning system we need to figure out still figure out hidden error one hidden air or two well this still stands right this is still a part of the error coming out of here it's the part that it's the part of that it's responsible for it's how much is it responsible for 0.3 and so in that case this should be air output 1 and I'm actually just gonna if it's an error on the output I'm just gonna say e 1 this error coming out of hidden neuron 1 is the same as it was before it's still the portion of the error based on this weight in this way how much of like how much are we contributing to that point 3 air well this this one has a certain percentage and this one has a certain percentage and that is weight 1 1 divided by 1 1 plus 1/2 times that error now here's the thing that's just how much it's contributing to air 1 how much is it contributing to air we've got to calculate that as well and that is the portion of these two and I think you must make sure I have a lot of space so how much of weight to one divided by weight to 1 plus weight to 2 times air so if they're multiplicity total cumulative error of hitting neuron 1 it's that can it's it's it's portion of the connection from it to error 1 sum with all the other connections to air and the portion of its connection to air to some with all of the connections to air to and I'm kind of it's that connected to air it's connected to the output but the output is producing that error so this is it's some right now I could do the same thing for this error it's it's portion that it contributes to this error which is now wait one two right you can see that these are the same right this is its portion wait it's that's where it's connected this is its portion that's where it's connected right it's it's its percentage portion of the total weights all connected to output one and then how is it connected to output wait its portion of the total of air of the error for output so this is now how those errors are computed so again if the mathematics of gradient descent tells us how to take an error to nudge weights then we calculated this error now can we calculate the errors coming out of the output these are our formulas for calculating the errors coming out of the hidden and those errors are the things that could then with gradient descent tell us how to nudge these weights then if we had more layers we could calculate these errors and keep going back that is back propagation that is how the hidden errors are calculated so this is the end of this sort of part one of just for like the basic idea of back propagation I'm going to check to see if there are any questions or Corrections which I will mention at the beginning of the next video where I will also implement at least just this much in the code