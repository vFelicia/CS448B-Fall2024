hello welcome Good Friday to you afternoon here it is in New York City uh my name is Dan schiffman this is the coding train a YouTube thing that happens every once in a while where I uh come and talk about programming topics and various other types of things and often uh embarrass myself in a variety of other ways all while live streaming from Tish School of the Arts here at new new new New York University uh here in New York City uh it's a very hot day outside it is a cool Breezy 72 degrees Fahrenheit here in this room uh I hope that the microphone is working fine you can hear me okay that this music that the levels with me and the music are quite reasonable because I had a few technical things and change some things around so welcome I um have a few quick announcements to make do I have any announcements I just said that without thinking it through I should plan I've got a plan for these things actually hold on hold on where where where is it ah it's over here oh there is a Discord server somebody made one uh Z I made some notes I made some notes for today because topic that I want to cover today is linear algebra um as it relates to programming and matrices and code here's another book these are really just props these are actual linear algebra textbooks that I used at one point in my life but we're talking about okay let me do the math here divide by 12 carry the four uh I 25 years ago something like that so in any case uh but I'm going to return to that topic today now okay that stuff is going over there so what what oh a couple things I am taking a look at the YouTube chat over there on a screen and I see people saying has he already started hey you're doing linear algebra after calculus La how do you say lolz laws anyway LS lws anyway uh yes I am I don't I'm not unfortunately nothing on this channel has a particular is particularly thought out or planned with any like reasonable logic whatsoever I'm all about trying to figure stuff out pull from here pull from here piece this together piece this together make something creative see if we can get some code up and running compiling and running and doing something interesting so that's kind of my goal here um so I am seeing the YouTube chat over there I am looking also at a slack Channel which if you you would like to join the coding train Patron program making it sound like it's a thing you can go to patreon.com Trin that is a way of a crowdfunding the work that I'm doing here on YouTube uh and I have some benefits uh uh namely a slack Channel that I look at also during the uh during the live sessions okay so um bonjour to uh gaming with Izzy in the chat so it's summertime do I have any Summertime music is this Summertime music I don't know uh so I'm kind of on a little break from my usual schedule and it's actually been a bit more difficult to schedule these live sessions than I had imagined good news is I am here in town in New York City for a stretch of time from now until July 14th I will then be away for a couple weeks so just in case you're planning your summer around my ual which you really should not that's what's happening right now so I am hoping now to take this time from now until July 14th and really cover with some depth how to program a neural network from scratch in JavaScript and perhaps also in Java using processing most of my inspiration is coming from almost yeah I won't be doing this almost all of my inspiration is coming from this wonderful book that I purchased earlier this year called make your own neural network uh by Tariq Rashid uh I think believe who's based in London um this is a step by step with no prior knowledge needed of any maths or any programming actually um to build a neural network program in Python because reasonable people who live reasonable lives program their machine learning stuff in Python but I am not a reasonable person living a reasonable life and I will be doing this at least for now starting to do this in JavaScript and I have to say that the the ultimate goal here I know I'm going to uh end up just repeating myself again in a second is that I oh look it's already even open here I have actually already done this I have there's a GitHub repository uh neural network P5 which is a my watch is beeping at me I'm going to press this um this is a JavaScript library uh with code based on the code from that book uh and it actually has a few uh demos associated with it already I'm going to click on this one this is kind of a classic uh classic machine learning scenario of learning to recognize uh digits and I have a little P5 example where I can uh draw the letter three there which it thinks is the letter letter three the number three the character three the number three thinks it's a five maybe after some more training eventually it will determine that it's a three we'll see um so I have this example I also have uh another example which is uh using the same nural network code oh it's broken I knew it I changed something I got to fix this so I'll will fix that so anyway I've done this already I did this for a course that I taught here at NYU at ITP earlier this spring and now what I would want to do is unpack this process unpack the process that I undertook to make this Library and do it over a series of many videos so on the one hand you should probably just go and enjoy your summer vacation uh go to the beach read a book and uh come back when I finished with this and then kind of get your hands into like the using the neural network library making some creative projects but as an exercise to help myself learn more background and more depth about this topic and for those of you who might be interested I'm going to build all this stuff over the course of a bunch of videos um which I will start today okay so that is my main Spiel now now it's about 230 uh schedule wise oh oh yeah yeah I'm going next week O'Reilly AI Conference next week I will have a limited schedule is this am I ah right here because I will be attending this O'Reilly artificial intelligence conference where hopefully I might learn something that I can take with me and then bring to this YouTube channel but uh so if you if anybody watching will be at this conference please tweet me at shiffman uh so maybe I can say hello uh give you a sticker coding train sticker or a processing sticker or P5 sticker I guess I can try to remember to bring those with me um and say hello to you at this conference I'm particularly excited I'm going to a session on deep reinforcement learning which is really the the kind of a machine learning algorithm that I'm most interested in in terms of how it applies to creative animations and interactive systems so okay uh trua in the chat is asking about the Chrome extension tutorial so unfortunately I did say I so okay let me let me answer this for a second oh look I'm actually whoa that's weird my web browser is on the web page that I was just going to go to but it wasn't before how did it get there anyway I must have clicked on it subconsciously um so this is a syllabus for a course which is um kind of misnamed because it's not a beginner course at all but programming from a toz it was a course from last fall uh and I made a a lot of video tutorials actually if I go to shift.net teing A2Z uh that's not the right page uh oh whoa where is my uh a toz page if I go to here oh just SL a toz just if I just go to shiftman Donnet toz you will find the um whole set of P each with videos and notes on a variety of topics and if I go down here to Chrome extensions here are the examples and a few notes but I didn't actually make any of the videos yet so this was a topic last fall I made videos for this course all fall along if that's an expression to say and I never got to the Chrome extension videos and I kept saying I was going to get to it and then it became kind of like a running joke so I almost feel like I can't ever do it because then we won't have the running joke anymore but I really do need to do it at some point this will happen for sure in the fall because I'll be teaching this course again and want to fill out um stuff and actually I'm going to add some machine learning stuff to this course I want to do word to VC I want to look at uh recurrent neural networks and text generation there's something else some chatbot stuff so that's coming as well in the fall okay so now um I'm looking at the chat to see if anyone's got any questions for me ah okay so because this conference next week um I will not be live again until next Friday I mean that's what I usually do anyway um so today's a little bit of like getting back into the swing of things because I was kind of been my last live stream was a week and a half ago and then I'll be back next Friday and then I've got two weeks in July that I'm here in town with a stretch of a pretty flexible schedule and I hope to get a whole bunch of live streams in then my goal is to have four per month and so uh because I'm going to be away the last two weeks of July that I might do two per week those two weeks that's my kind of goal and so I'm hoping that by July 14th I will have rebuilt everything about this particular neural network library uh in a video on live talking about it figuring it out and taking hopefully your helpful suggestions so let me I the problem is I'm going to repeat a lot of this stuff in a second because I got to do sort of introduce one of the things that I do for those of you who haven't watched live before is the live stream gets edited into a bunch of tutorials videos that then get put in a whole bunch of different playlists depending on what course they go with so a lot of this introductory stuff I might end up repeating again I apologize for that but um now I even forgot what I was gonna say um so let's see here so I think I just want to get started okay so um couple good suggestions from the slack chat patreon group uh mincut writes you should make a calendar with all the conferences you attend a good idea except to be honest with you I go to very very few conferences I would love to travel and go to more things but I just have a lot of work commitments and family commitments so generally I'm just kind of here in New York City but uh but that's a good idea we'll try to kind of keep that up to date or announce if I'm doing any other workshops or things outside conferences outside of the stuff that I usually do here um Cod codo verly asks will there be a separate math video for this topic so this is a good question and I've been struggling with this and this is where I have arrived at at the moment so I am going to do I'm going to build a neural network code and I'm going to use Matrix math to do the weighted sums of all the connections and I will get into the details of what that means so I am going to do some separate videos that cover the Matrix math required so there's I'm going to pick and choose little nugget get from linear algebra and cover those and also write the code to implement those things then at some point and i' probably not today I'm going to get to the part of the neural network thing where you're training it and adjusting the weights with a process called back propagation so we're going to talk about what that process is write all the code for it but I don't think that I'm going to derive the calculus maths that's required for the formulas that are used I'm just going to use those formulas and point people to other references or maybe come later do a followup video to unpack some of that math in a bit more detail so that's my plan and I kind of had a similar plan point of view with the uh linear regression stuff I mean it's actually the same math so um a lot of that will apply but where I try to just sort of like make a video and use the formulas and then come back and make a followup video for those who would be interested in that so that's my plan uh is this going to be posted later oh yeah and Carl's so let me make some notes Here I forgot so I also the issue is as much as I want to okay so hold on uh linear regression uh batch gradi I forgot some things that I forgot to do are uh and Carl so couple somebody asked in the chat will this be posted later yes everything is posted later there'll even be edited versions of this later if you don't want to watch all the long winded stuff um batch gradient descent is something that I forgot to mention and cover as part of the linear regression tutorials and then there were some other additional things to the perceptron example that I wanted to do and I'm tempted to come back to the and everyone's asking for the f I did this to myself I wanted to do the fidget spinner uh code and I never did that and uh when I first mentioned it everybody had the what you know the reaction that one might expect which was like groan major groan I'm really could do a fidget spinner simulation video but then I got kind of excited about it and I talked about it and then some other people got excited about it and then I never did it so um and yes I'm definitely going to be doing neuroevolution is my happy place so one of the things we'll see as we build a neural network is that the whole point of one of the one of the uh one of the key pieces of working with neural networks is figuring out how to get the optimal weights of all these connections that are in that Network and so there are these methodologies for doing that uh there's the standard methodology which involves this gradient Ascent like back propagation algorithm tweaking according to an error to minimize the error changing weights to minimize the error but there is another method which involves an evolutionary approach to evolve the optimal weights and that's the method I love because it's just very more it's very intuitive and it's uh can understand it it relates to my genetic algorithm tutorials it doesn't involve all that sort of like calculus stuff so I'm I'm definitely planning to get to that uh yes I good so at least I'm getting some negative feedback about the fidget spinner which will keep me from doing that oh yeah and and and I have no I'm I I will sell out as soon as I possibly can right if I'm making the fidget spinner video will suddenly get me you know 100 million subscribers which it won't obviously uh and then I'll just do it but uh you know I have no qualms selling my soul yeah unfortunately if you're watching this live you cannot watch it at two 2x speed you have to watch it at 2x speed later but I could try to pretend that it's going at 2x speed because F to talk about that that's actually more like 4X speed uh what about polom regression oh my goodness o o there's too many things I didn't get to that either can I in one YouTube channel cover the entire known universe of knowledge no I cannot to be honest a lot I mean my the my goal with the channel is to create friendly and accessible tutorials for people who want to be creative and experiment with code in an informal way and so on the one hand you know I just don't it's not the goal of my channel to cover every sort of Statistics mathematics computer science algorithm from scratch in some sense that I'm going to show to you so you can memorize it and and redo it again again later I mean I'm not saying that doesn't have value and there aren't other channels that have that approach but my Approach is quite informal and loose and so Pol nomal aggression I don't know that I need that so much for where we're going because ultimately I think the creative examples will come from using neural networks to uh to um experiment with user interaction and other types of um interactive and animated possibilities okay all right I'm I'm reading this chat and it's just lovely to read all these nice messages um and people are having a discussion and I can't keep up with it at all uh I forgot the momentum so many things I forgot I can't do it all I can't trigonometry yes to mges I will be oh yeah I could do a fidget oh whoops I hit the Bell by accident I could do fidget spinner with querian if you're new here you know that if I ever say the word Quan I have to run away instantly it's terrifying they're terrifying all right I think what I'm going to do because I feel like I'm I want to get some momentum here and start the um neural network stuff I'm just walking over here to reset this camera um I think maybe I should hold off should I do so many requests I'm trying to decide I'll do the straw pole thing that's usually how it works best uh I kind of know the answer to this but let me just confirm here pick one finish off some details and things for perceptron uh actually uh just get started on a neural network so this is the multilayered perceptron you guys probably can't see this but I will so here we go um create poll so here is the straw pole address W 6 Z D3 e should I finish off some of these things that we didn't get to and like visualize and add some other stuff or let's just get started this is going to be a long process it won't finish won't be finished today but get started building this JavaScript neural network library this is why people can watch the edited verion or the archive because you could just skip over this part where I wait for the scrub Ball results and oh wait is this link not working oh oh no what happened what happened that was such a fail all right did let's try this again everybody is are people able to get the straw pole it's working okay what's funny is somebody in the chat said uh you know clicks link arrives at live stream sees guy dancing Le so this is good because if if somebody just joined this by accident and sort of sees the dancing isn't interested it's probably good to it's not going to like the rest of the video all right I think I know what this is going to be wow all right so what we're going to do today is just get started and we are going to build a neural network library in JavaScript and we should just all be aware that I'm not really an expert in this and I'm just doing this to learn it myself and give it a try and you're going to encourage me or not or whatever but that but I but I feel you out there I feel you watching how many people are watching 747 people that's terrifying it's absolutely terrifying okay all right so let's get started I really need some different music um okay so how to begin so I need a code editor okay um e okay I think I am ready now oh wait actually I want to oops there's something I want to do which is open this up okay so I should map this out a little bit um Okay so let's think about where where will these videos live this is a little tricky because there are two places right now I am here basically in uh week four of the intelligence and learning class but this these videos actually go along with chapter 10 of the nature of code book and there goes the camera so that music that I'm playing is by um Adam Blau who is a uh film and television composer based in Los Angeles uh he has a podcast that I'd love to plug uh called rarified air which is really terrific and I believe that is called Tory the dog said it to me as just some like extra uh extra spare music that I could use and Adam is the composer of the soon to be released coding Train theme song I know you can't wait for that um okay so I think this is not a coding challenge I will do some coding challenges with neural networks but I think this um and it would be interesting to try to do like a hey just program neural network in 20 minutes kind of thing but um I want to spend some time doing stuff step by step in a series of videos about building this neural network library so the thing that I want to start with is is yeah okay I have these slides from the perceptron boy I really um could have thought about this in a different way let me do an introduction video uh I'm really trying to I'm really trying to figure this out but I you know what I am not going to worry about it I am going to I am going to consider this to be a I got it I got it everybody we're good I figured it out this is a new playlist 10 new number 10 in uh nature of code uh it is going to look if I go to my channel and go to uh here so basically you can see here what this is going to be is 10 neural networks 10 neural networks and what I'm going to do now is make an intro video that sets the stage for what's coming and the perceptron videos will actually follow that intro video and then the neural network I mean the multilayered proc ctron Library building will follow that so I'm I'm making videos out of order so first I'm just going to do probably just like a couple minute introduction uh to the idea okay yeah I uh all right so I'm looking at the chat I'm seeing lots of stuff no you did not miss the train whistle there's the train whistle okay um all right here we go I need a moment to meditate shoulders hurts I need to stretch ah oh I think there's an issue where the way I have this set up the camera is actually right there but I'm kind of like standing over here so I have this like awkward neck craning thing to look at the the camera but we're just going to go all right so what time is it now oh my God 30 minutes in and I haven't really started doing anything yet but I was thinking this through and talking to you and you were watching apparently let's get let's get a move on hello welcome to a video that at this present time doesn't exist but when you are watching this video right there to the right of nine genetic algorithms will be the number 10 and we'll stay next to that neural networks so I am embarking on a journey uh to learn about neural networks what they are how you program them what are there what's kind of like math and stuff you need to know to make them work and then what kinds of creative and experimental outcomes can you have now it should be said that uh there are lots and lots of machine learning libraries out there uh there are lots of examples and resources for doing this uh I I want to hold on I'm still talking I don't know where where did I put that book ah it's over here I want to reference uh this book U make your own neural network by Tariq Rasheed which I used to develop a lot of the materials that I will be presenting to you and developing uh during this series of of videos and I should also say that um you know and this book this book has um all sorts as how to program your own network from scratch and without even knowing anything about programming in Python because as I might have said earlier today any reasonable person would start and make a video tutorial series about programming a neural network from scratch in Python but I don't really I'm not very reasonable or logical and I do make just constantly make mistakes with everything and here's a mistake that I'm going to make I'm going to do all this in JavaScript um and the reason for doing that is to have everything run in the browser on the web and also really for me to learn about how to do this stuff so I am going to build a set I'm going to build a a simple neural network library in JavaScript not to make something efficient not to make something robust but to learn about the mechanics of how all this stuff works because ultimately and you might want to just enjoy your summer or maybe you're watching this during the winter and get outside and do something else and not watch these videos and just skip ahead to like later cuz I'm going to do bunch of coding challenges and projects that involve that neural network library and also other neural network libraries namely something called tensorflow uh in future videos but these first videos of building the neural network library which I will do over a series uh or really just for me to to learn how to do this stuff and if you want to watch and sort of give me some good feedback and see if you can follow along and and and improve on what I'm doing and uh help me with it that would be great so okay what what hello am I just rambling here I am but why are we here so I'm going to go uh so the nature of code materials and this video sits in the nature of code playlist is all about looking at things in nature in our physical world and trying to unpack those things and understand the algorithms behind those things and see if we can convert those things those algorithms into code oh this is like going it's like Auto playing how do I stop that uh uh and uh turning those things into software to make animations and creative projects why not look at something really interesting in nature the brain so this is kind of a loose diagram of this idea of an actual biological neural network apparently I have one here struggling quite a bit these days uh where there are these entities called neurons and and they're connected to other neurons and there's a lot of you know mystery to this and a lot of recent research to your Neuroscience what I am focused on in this series of videos is what kinds of computational systems can be built inspired by the actual biological neural neural network biological brain and made into something called an artificial neural network and what kinds of applications and outcomes can we we can we create so what is the analog what is the neuron in by in our code how does it receive inputs how does it generate outputs so my brain does this it receives all these inputs you know from light in the room that travel through my retina and into the brain and the signals then produce outputs and allow me to catch something or read some words what how can that how can that process be simulated in software and what type types of outcomes can we generate and the very first thing that I'm going to do is look at the simplest possible neural network a net it's not even a network at all it has one neuron a processor neuron that receives two inputs and generates an output and that's called a perceptron so if you look at the next videos in this playlist I am going to build in processing uh perceptron example just to show the mechanics of how this works and to produce a sort of trivial example that doesn't necessarily have a very powerful outcome but gives us CU if we can build and understand how this single neuron receives inputs processes those and generates an output then we can start to connect those together to create more sophisticated systems that can begin to generate outputs based on more more complex outputs based on more complex inputs and this is kind of a fit sits right there in the world of machine learning this idea of I have some data that I want to make sense of that data is an input to a machine learning algorithm that algorithm is going to generate an an output so maybe the data is an image the machine learning algorithm is going to guess is it a cat or a dog or maybe that input is the specs of a house you know square footage uh number of bedrooms etc etc and this machine Learning System is going to generate an output a predicted price so there are lots of other machine learning algorithms besides just neural networkbased ones and I do have another video series that covers some of those but ultimately I want to learn how a neural network works so I can place it right there and start to make sense of data generate outputs from it so if you want to continue along the way this video series will work first there'll be a perceptron which is uh this thing then I'm going to talk after the perceptron done I'm going to talk about what the limitations of the perceptrons are and why it is that if we could can create a multilayered perceptron meaning many of these perceptrons all connected to each other what we can start to build uh and create afterwards so uh that's my rambling introduction that apparently you just watched because I mean maybe I'm that no one will ever watch this but but but probably somebody will and um I'll see you follow along I look forward to your feedback uh I hope this goes okay that's my that's a pretty good goal just okay is fine and I'll see you in these future videos as I keep going thanks for watching all right you know that's me that's this is my style uh okay so um looking at the chat so now fortunately so fortunately for all of you uh you don't have to now sit through me doing the perceptron because that is already oops H that oh my channel that is already um done and that is here well that's the followup but if I keep going this way we can see here it is the perceptron so there are two videos about the perceptron at some point I might come back and fit some little pieces in there um people are giving me great suggestions like yeah in the chat but now uh I code please everybody is very very focused on me getting to the point of things I I I I would tend to agree okay so the next thing that I want to do now is I want to talk about all right this is very hard this is difficult here all right so I want to talk about first hold on uh trying to see the thing that I need to reference I probably should have followed this along more closely I'm sorry I'm I'm looking at my notes Here Yeah okay um okay so there's this and there's also I have these notes okay okay okay so the next thing is to discuss why the perceptron okay all right let's see how this marker does so first of all um oh this is such a good marker that makes me so happy can you read that I'm going to go walk over to my monitor and see if I can even read that looks like the focus is kind of reasonable it's a little bit small um yasu or yasus to everybody from Greece in the chat um um I guess I just need to write bigger but uh that's good okay people are saying it's okay okay so where's my Eraser all right okay here we are uh I'm going to begin now a a little bit large would be fine okay font size plus three okay okay uh here we go hi again so maybe you just watched my previous videos about uh coding a perceptron and now I want to ask the question why not just stop here so okay so we had this like very simple scenario right where where we have a canvas and it has a whole bunch of points in that Canvas OR cartisian plane whatever we want to call it and we drew a line in between and we were trying to classify some points that are on one side of the line and some other points that are on another side of the line so that was a scenario where we had the single perceptron the sort of like processing unit we can call it the neuron or the processor and it received inputs it had like x0 and X1 were like the X and Y coordinates of the point it also had this thing called a bias and then it generated an output each one of these inputs was connected to the processor with a weight you know weight one weight two or whatever weight weight weight and the processor creates a weighted sum of all the inputs multiplied by the weights that weighted sum is passed through an activation fun function to generate the output so why isn't this good enough now let's first think about what what's so what's the limit here so the idea is that what if I want any number of inputs to generate any number of outputs that's the essence of what I want to do in a lot of different machine learning applications let's take a very classic classification uh algorithm which is to say okay well what if I have a handwritten digit like the number eight and I have all of the pixels of this digit and I want those to be the inputs to this perceptron and I want the output to tell me uh a set of probabilities as to which digit it is so the output should look something like you know there's a 0.1 chance it's a zero there's a point 2 chance it's a one there's a0 one chance it's a two 0 3 4 5 6 7 oh and is like a 9 chance it's an eight and a 0.05 chance it's a uh 10 and I don't think I got those to add up to one but you get the idea so the idea here is that we want to be able to have some type of processing unit that can take an arbitrary amount of inputs like maybe this is a 28 by 28 pixel image so there's 784 grayscale values and instead those are coming into the processor which is weighted and summ and all this stuff and we get an output that has some arbitrary amounts of probabilities to help us guess eight that this is an eight this model why couldn't I just have a whole bunch more inputs and then a whole bunch more outputs but still have one single processing unit and the reason why I can't is uh stems from an article I don't know sorry a book that was published in 1969 by Marvin Minsky and Seymour paper paper called perceptrons you know AI uh luminaries here I don't know if I click on this link where it goes to Amazon maybe oh MIT press so in this book Minsky and paper T edit time out for a second how do you pronounce Seymour paper's last name is it payal paper paper paper do I pronounce the T can somebody help me with this somebody said qu Tor the dog while I wait for somebody to help me with pronunciation paper with a t or no T paper with it's pronounced it's pronounced gift that was a good one oh come on I don't want daily hacks I'm not interested in your deal I want to know how to pronounce paper nobody will tell nobody will tell me I'm going to just mispronounce it I'm looking for at least the slack Channel I can rely on you patrons somebody must know look at this I really got the chat going crazy here PayPal yeah my PayPal is Daniel shiftman Donnet that's also of my email my Bitcoin address is uh nobody has any idea that's fine I will just suffer through the comments that I will get in the video at a later time check this video oh my God boy that the chat is going crazy I've never seen anything like it the t is pronounced thank you a AA you have come through for me thank you very much I are totally forgot what I was talking about okay in the book perceptron Marvin Minsky and see more paper point out that a simple perceptron the thing that I built in the previous two videos can only solve linearly separable problems so what does that mean anyway why should you care about that so let's think about this this over here is a linearly separable problem meaning I need to classify this stuff and if I were to visualize all that stuff I can draw a line in between this part of the DAT this stuff that's to this class and this stuff that's with this class the stuff itself is separable by a line in three dimensions I could put a plane and that would be linearly separable because I can kind of divide the space in half and and and and understand it that way the problem is most interesting problems are not linearly separable you know there might be some data which clusters all here in the center that is of one class but anything outside of it is of another class and I can't draw one line to separate that stuff and you might be even thinking but that's you know still so much you could do so much with linearly separable stuff well here I'm going to show you right now a particular problem I'm looking for an erase sir I'm walking around like a crazy person I'm going to show you a particular problem called xor so let's erase all this and making the case for why we need to go a step further and start to whoops I'm making the case for why we need to go a step further oh I just had an idea I'll come back to later I'm making the case for why we need to go to a go go a step further and make something called a multilayered perceptron and I'm going to lay out that case for you right now so you might be familiar you might remember me from my videos on condition conditional statements and Boolean Expressions well in those videos I talked about operations like and and or which in computer programming syntax are often written you know double Amper sand or two pip the idea being that if I were to make a truth table true true false false so what I'm doing now is I'm showing you a truth table I have two elements I'm saying what if I say a and the B so if a is true whoa whoa whoa this makes no sense what I've drawn here because I am losing my brain cells slowly over time with every passing day it's very sad true false true false true and true yields true if I am hungry and I am thirsty I shall go and have lunch right true and true yields true true and false is false false and true is false false and false is false right if I have a Boolean Expression A and B I need both of those things to be true in order for me to get true interestingly enough this is a linearly separable problem I can draw a line right here and true is on one side and false is on the other side this means I could train a perceptron to receive two inputs true and false you know that are true or false oh I'm like way off the screen here that's not a screen that's a hold on how's that let's let let me go backwards for a second and redo this part this means this is I'm fing I'm going to get to the coding I swear I I don't know if I'm going to get to it today to be perfectly honest with you but I I've got all this stuff that I want to talk through I don't know if it's a good idea I'm giving it a try this means this is a this is a linearly separable problem which means I could create a perceptron that perceptron is going to have two inputs there are going to be Boolean values true or false true or false and I could train this perceptron to give me an output which if two truths come in I should get a true if one false at a true comes in I should get a false two falses come in I should get a false great or I could do the same thing what does or change into if I'm going to do or let me erase this dotted line and or now ah all of these become true because with an or operation A or B I only need one of these to be true in order to get true but if both are false I get false and guess what still a linearly separable problem and is linearly separable or is linearly separable we could have a perceptron learn to do both of those things now hold on a second there is another Boolean operator which you may you might not have heard of until this video which would be really kind of exciting for me it would make me very happy if somebody watching this has never heard of this before it is called xor can you see what I'm writing here X or the X stands for or exclusive exclusive it's exclusive or which means it's only true if one is true and one is false it's not true both are false this or that both of those things are false I'm still false but if both are true it's also false so this is exclusive or let me erase all this exclusive or means if one is one is true and one is false it's true if one is true is one is false is true if both are true it's false if both are false it's false this is exclusive or a very simple Boolean operation however I I triple dog dare with the cherry on top you to draw a single line through here to divide the falses and the truths I cannot I can draw if this is not a linearly separable problem this is the point of all this like rambling I could draw two lines one here and now I have all the trues in here and the false is outside of there this means a single perceptron the simplest cannot solve cannot solve the a simple operation like this so this is what Minsky and paper talked about in the book perceptrons well this is like an interesting idea conceptually it kind of seems very exciting but if it can't solve xor what are we supposed to do with this the answer to this is and you might have already thought of this yourself it's not too but believe I I I kind of missed a little piece of my diagram here right let's say this is a perceptron that knows how to solve and and this is a perceptron that knows how to solve or what if I took those same inputs and sent them into both and then I got the output here so this output would give me the result of and and this output would give me the result of or well what is xor really xor is actually or but not and right so if I can solve something and is linearly separable not and is also linearly separable so what I want then is for both of these outputs actually to go into another perceptron that would then be and so if this perceptron can solve not and and this perum can solve or and those output can come into here then this would be the result of both or is true and not and is true which is actually this these are the only two things where or is true but not and not but not and and so the idea here is that more complex problems that are not linearly separable can be solved by linking multiple perceptrons together and this is the idea of a multilayered perceptron we have multiple layers and this is still a very simple diagram you could think of this almost as like if you were designing a circuit right if you decide whether electricity should flow and this were like a um these were switches you know how could you get a bunch of how could you have an LED turn on with exclusive or you would actually wire the circuit basically in exactly this way um so this is the idea here so what I am would like to do in the next so at some point I would like to make a video where I actually just kind of build take that previous percepton example and just take it a few steps farther to do exactly this but what I'm going to do actually in the next videos is diagram out this structure of a multilayered perceptron how the inputs how the outputs work how the feed forward algorithm Works where the inputs come in get multiplied by weights get summed together and generate an output and build a simple JavaScript library that has all the pieces of that neural network system in it um okay so I hope that this video kind of gives you a nice followup from the perceptron and a sense of why this is important important and I'm not sure if I'm done yet I'm going to go check the live chat and see if there any questions or important things that I missed and then this video will be over time out okay all right what did I do okay I'm oh I'm I gotta turn this camera on okay so now is a brief moment where uh you can um point out things that I got horribly wrong that I should make sure I correct or ask some followup questions that might be important I see there's some chat going on here but I I think that has uh nothing to do with what I um so are we good are we good did that make sense did I get that about right I don't know if K weekman is watching I don't think so that's my uh sanity check um yeah I didn't mention hidden I probably should have uh mentioned that um um like this is technically the this is technically the hidden layer these are the inputs this diagram is terrible because this should be down here so maybe what I'm going to do is I'm going to connect you know that's what I'm actually going to do is maybe I'll do a quick redrawing of this so that it matches what people are used to um uh doing great oh thank you got to know doing great uh Place circuit scramble explains this thing perfectly that's interesting um Tom this is a coding train tshirt that you can get at coding train. storenvy.com uh proof for xor is not and and or that's right right when I said it is both or and not and that's correct okay so I think I'm good I'm not seeing anyone telling me something that I've done um I'm not seeing anything uh uh that I've done that's sort of like horribly out of about my watch not the shirt this is a Fitbit I don't know it's the Fitbit whichever one looks like this um okay so let me do let me get a couple things um I would like to show so for those of you interested by the way this is a viewer um K weekman uh GitHub is K weekman who created a learning xor with a neural net example um and it visualizes it and kind of visualizes the connections and that sort of thing so I'm going to show this uh I'm going to read I'm going to fix the diagram here and mention hidden and then that's not a Wikipedia page by the way um I this is this page here is part of my um uh core syllabus it has a bunch of references here Daniel it be nice to know if the nonlinearity of the problem affects the number of neurons to use in the hidden layer so yeah so we'll get to this I mean yes yes and no it'd be really nice if I could just like open a door over here and like an actual machine learning expert come out and answer some of these harder questions but um uh let's let's let me get a little further and sort of come back to that I mean the complexity of the the number of hidden layers affects the number of parameters that you get to tweak which is uh the level of complexity that you can kind of apply to a problem so certainly um Okay so um okay I'm back so there was one question which is important like oh what I heard some somebody in the chat asked what about the hidden layer and so this is jumping ahead a little bit because I'm going to get to this in more detail in the next video there's a the way that I drew this diagram is pretty awkward let me try to fix this up for a second imagine there were two inputs and I actually Drew those as if they were neurons and I know I'm out of the frame but I'm still here um and these inputs were connected to each of these perceptrons each was connected and each was weighted so this is actually what's now known as a thre layer Network there is the input layer this is the hidden layer and the reason why it's okay well actually let me go this is the output layer right that's obvious right this is the input those are the inputs the TRU and the falses this is the output lay layer that should give us a a result are we still true or are we false um and then the hidden layer are the neurons that sit in between the inputs and the outputs and they're called hidden because as a kind of user of the system we don't necessarily see them a user of the system is feeding in data and looking at the output the hidden layer in a sense is where the magic happens the hidden layer is what allows one to get around this sort of linearly separable question so the more hidden layers the more neurons the more amount of complexity in a way that the system the more weights the more parameters that need to be tweaked and we'll see that as we start to build the neural network library the way that I want that library to be set up I want to say I want to make a network with 10 inputs three outputs one hidden layer with 15 like hidden neurons something like that but there could be multiple hidden layers and eventually as I get further and further down this road if I keep going we'll see that there are all sorts of other styles of how the network can be configured and set up and whether the output feeds back into the input that's something called a recurrent network convolutional network is if some this kind of like um set of image processing operations almost happens early on before as one of the layers so there's a lot of stuff in the grand scheme of things to get to but this is the fundamental building blocks uh so okay so I'm in the next video I'm going to start building the library and to be to be honest I think what I need to do no no no no yeah I'm next video I'm going to set up the basic skeleton of the neural network library and look at all the pieces that we need and then I'm G to have to keep going and look at some uh Matrix math that's going to be fun okay uh see you soon goodbye I'm g walk over here all right okay it is now 330 and I've been streaming for an hour um I'm checking out the chat which is nice to see okay so I think it's time to write some actual code um okay so what I want to do is oh I forgot to show this yeah I'll come back to it because maybe um I maybe maybe there'll be a time to come back to it um but I do want to build I want to do this kind of as a coding challenge but it'll make more sense to do it once I have the library I think actually in a way um okay so what I want to do now is go to here okay I can close this I can close this is there anything yeah okay um and here whoops uh let's get rid of this of this okay so I think now I'm ready to start writing the code and um and let me just look I'm going to look look at how I set it up here to do it the same way so I want to create this is how I'm going to set it up I want to create a Network that has a certain number of inputs so I'm going to have the library only create a thre layer Network and so it's made with a certain number of inputs a certain number of outputs and a certain number of hidden neurons okay and the viewers are dropping I'm not surprised you know this I I'm kind of you know I I feel like what I'm doing here is getting away from is moving a bit a skew or a scance a side adjacent to the sort of core Mission and and kind of stuff that I like to do on my channel but trying to take this twoe period in the summer to see if I can blast through this material and and hopefully get a sense of it and and begin to do some more interesting stuff applying it later um Alexander right maybe you should have an outline for every Live code session that would allow for a more smoother presentation duly noted duly noted there's nothing smooth about this whatsoever okay let me keep going I have a mental outline but I I'm I'm just so scatterbrained okay um so this is what we're going to do I'm not going to get very far with this but that's okay all right so let me see here okay um all right so here we go I'm going to move on now welcome back I'm going to actually write some code in this video um not that much so what I'm doing now welcome I made a few introductory videos covered some background uh about uh neural networks and why they exist and where I'm trying to go with this and in this video I'm going to actually begin to write the code for a simple JavaScript neural network library now I've actually already done this it exists here at this repository uh GitHub shiftman neuralnetwork P5 I'm designing this library to be used with a set of p5js examples with a a library a JavaScript library called P5 although ultimately this Library stands alone on itself by itself you don't have to use it with just P5 so before I can write the code let me come over here to the Whiteboard and this is where I last left off talking about how the general structure of a neural network library Works uh a neural network system works and so what I need to do here is when in the code I create a neural network I want to create three things I want to create an input layer I want to create a hidden layer and I want to create an output layer so when I create a new the way I want to design this library is I want to say new neural network and I want to give it can you see this I think you can I want to give it three arguments the number of input neurons let's just use the word neurons the number of hidden neurons and the number of output neurons so I'm doing something which I typically don't do which is usually I like to have a specific problem that I'm trying to solve and like write the code for that problem and in here the problem that I want to solve is I want to make a generic kind of useful library that could be used in a bunch of different contexts so I don't know what those numbers are going to be uh I don't know what the data is I'm just kind of working on the skeleton the structure of the library before I start to apply it to things so let's just make up some numbers let's say there's going to be three input neurons four hidden neurons and two output neurons what this means now is in a feed forward neural network there are three inputs we could imagine again I'm using the kind of classic example of guessing the price of a house this could be number of bedrooms number of bathrooms square footage so those are like three parameters of a house these will connect to 1 2 3 four hidden neurons so this is the input layer this is the hidden layer and then I'm kind of running out of space here there will be two outputs and then this is the output layer so this is the configuration the idea of a uh and so what I'm building here is what's known as this is a multilayered perceptron these are individual perceptron units essentially that are have multiple layers and it also is an another important term that I want to add here is I want to create a fully connected Network and now there are variations to this that we might see in future examples but the idea of a fully connected network is that every input is connected to every hidden every hidden is connected to every output but I so I can draw those connections and it's not so many that I you know if I were doing some kind of post production I would speed this up but I'm going to just draw this web of all these connections so every input is connected to every hidden and every hidden is connected to every output whoa oops ah ah I messed this up but I'll get it eventually there we go right so you can see that every every node is connected to every node in the next layer so the idea is that those three inputs come in the data feeds forward and those two outputs come out so this is the structure now we have to get into a lot of details here well how do I keep track of all of these connections how do I actually do the loops to like do all the sums of everything and how do I read the outputs I'm going to get to all that but this is the overall structure so let's go back to the code and now let's actually try to like write a little bit of this Library very very little so where am I going here okay so this is my code there's nothing yet I'm going to create a new file and I'm going to call this nn. JS so this is now going to be my so here's the thing ultimately I want this to be like a proper JavaScript library but ultimately what is a JavaScript library but a file with some JavaScript in it so I might later as this gets more sophisticated optimize it and use some sort of like build process or break it up into multiple files but right now I just want to kind of like get the pieces going so I am going to I'm also going to use es5 syntax this is the trajectory that I've been on soon in future videos I'm going to start adopting some es6 syntax But ultimately maybe this Library I'll do a followup and come back and kind of I'm going to do a lot of things maybe not in the most optimal or efficient way but hopefully in the most easy to understand and fall away so I want to create a Constructor function called ner Network okay and I should also mention again while we're here that I built this Library already and when I built it I based just about everything out of this book called make your own neural network by Tariq Rasheed and so while I'm doing this now kind of a bit more on the Fly I'm sure everything that's in my brain ultimately came from here and probably some other sources too okay so what do I want to do the core thing that I want to do is I want to create the N neural network with a certain number of input nodes number Hidden number of output so I'm going to add those as arguments here I'm going to say um number of input number of hidden number of output I'm going to create a neural network with three arguments and then I'm going to say uh input nodes I think I'm going to be longwinded about this equals number of input and so I'm going to create three uh hidden nodes is this argument and uh output nodes is this argument is that an O yes it is okay so this is we've actually written some code the idea being that what I want to do is say things like VAR brain and brain is a new neural network that has three inputs with three hidden and one output right this is the idea so I need to figure out what shape and shape using the word shape very specifically does the data come in that's how many input nodes I want what shape is the output that I want am I looking for a single output am I trying to look for a range of outputs that's how many outputs I want then how many hidden neurons do I want well that's kind of an open question well maybe I want as many as I could possibly fit in was the program running reasonably fast but it sort of depends on the complexity of the problem and we'll come back to that later and I should also note that I this is a simp oversimplification of how neural network architectures can be this is by definition a three layer Network and this library is only going to allow for a three layer Network an input a single hidden and a sing and an output but as something you might think about for the future how would you write the code to have multiple hidden layers because a lot of neural networkbased Learning Systems need multiple hidden layers to be able to perform optimally but for now I'm going to keep things very simple okay I'm gonna pause for a second because I'm kind of thinking um I'm I'm seeing some interesting questions um and um so there's an interesting discussion going on is what's the relation number of input hidden and output so the the it's not that the hidden is arbitrary but it it can be kind of any number but you know if you're just going to have one it's not going to do going to work very well and so you know one thing that you might do is just kind of like well however many inputs let's just make the hidden layer the same number that could be like a good starting point I would say um Okay so the thing that we're going to need very quickly and I need to refill my water here is we're going to need the linear algebra stuff yeah um so I'm trying to I think I think I'll take this video a little bit further and then I have to stop and explain so here's the thing let me talk this through without this being part of the official tutorials ultimately what I'm going to do with each one of the actually why do I why do I even bother saying like let me talk let me just talk this through and this will be part of the video I don't know where I came over here I don't know where I was uh oh linear algebra yeah I don't even want to use that word but it's it's what we're going to do is actually quite it's not so hard to figure out okay what is the next step written we did write some code thankfully wrote some code now we got to stop again the next step is the feed forward process okay the feed forward process the way that the feed forward process works is that we receive these inputs oh there's so much to do so many pieces to this puzzle I'm excited to get through it all though so let's just say for example we're looking at this hidden neuron do you remember from the perceptron video videos maybe you didn't watch those so let's talk about it the idea is that we need to do something called a weighted sum so let's pretend this is the house prediction thing and this was the number of bedrooms three this is the number of bathrooms you know this is the number the square feet so each one of these connections right the data is going to flow in the data comes in here the number three comes in here and then look at this there's four outgoing connections each one of those connections has a weight to it now ultimately the whole point of doing this learning neural network based Learning System is we want to tweak those weights we want to train the brain train the neural network to have optimal weights to get good results results that make sense and that training process is something that I'm going to get to I don't how many videos down the road from now but not too far away these weight weights will have typically to start one way of thinking about them is they're going to just have random values between 1 and one and there's a wide variety of techniques and strategies for initializing random weights or not just random to a neural network but for right now good way for us to get started they all have random weights so even though I'm looking at each one of these flowing out slightly better way for me to look at this with you is actually just look at all the connections flowing in so this particular hidden neuron has three connections flowing in a three a and the input values are 3 2 and 1,000 each one of those has a weight so let's pretend this is like 0.5 U let's say this is like uh .5 and this particular weight is one so I'm making using very very simple numbers the idea is that each hidden neuron does something called a weighted sum so so it takes the input multiplied by the weight and adds that to the other input multiplied by the weight and adds that to the other input multiply by the weight so we could actually do this 3 * .5 is 1.5 plus 2 * .5 is 1 plus 1,00 * 1 is plus 1,000 so this value now is a 100.5 now we can see there's a huge flaw here which is that the fact that square footage is kind of a big number and number of bedrooms and number of bathrooms are small numbers means this kind of way of summing it is going to produce some like odd results this the square footage is going to be weighted so heavily just by the fact that it's bigger numbers so a lot of time in working with a machine learning or neural network based system we need to do some type of cleaning or normalizing of the data and we might do something where we you know we sample this down so that they you know we actually do the number of bedrooms between 0 and five as a value between 0 and one and number of bathrooms always as a value between 0 one and square footage this would actually turn into 0.1 like because the range is between Z and 10,000 square feet or something so we would do some kind of normalization of these values but this is again further down the road when we start to apply the library in an actual project once this weighted sum is complete the result of that weighted sum gets sent out through the outgoing connections but it gets passed through an activation function so I'm going to come back to the activation function this is something we did with a perceptron and that's going to be a separate video where we look at different activation functions and how they work right now I want to focus on this weighted sum so I could keep going here I could create some type of array of I could create an object that's like each one of these nodes or neurons is an object object then I could iterate over I could have connection objects so there's a bunch of different approaches I could take but the classic and standard approach is actually to look at storing all of these weighted Connections in something called a matrix which is really just like a spreadsheet a grid of numbers looking at the inputs as an array and doing some type of math that basically takes take that array of inputs multiply it by that Matrix of weights and generate the outputs of this hidden layer so this is so give me a second here I'm going to erase I'm gon to I'm gonna I'm going to make the case for this with a simpler scenario okay so um so I'm going to erase this this can get edited out and uh let's see here you know a lot of times when I us to do these videos I would do the videos at the end of the day after I taught the material with in a classroom earlier in the day and I would sort of figure out in my head how to condense things down maybe through the magic of editing things will get condensed down but okay so um I'm gonna let's see let me just let's go back to this um simpler diagram okay okay okay I'm back I erased what I had and I drew I'm drawing a simpler diagram here now so let's look at this diagram which just has fewer connections it's going to be easier for us to unpack so we can think of these inputs as x0 and X1 let's not even worry about the output right now these are the inputs this is the hidden layer right hidden layer so let's think about this and and actually let me change these numbers to X1 and X2 you know sometimes I like to count from zero sometimes I like to count from one I don't know why but I I I feel like in this case let's let's call it one and two so this is really like hidden one hidden two so each one of these connections right each one of these weights you could say here this is a weight that goes from one to one this is a weight that goes from one to two this is a weight that goes from two to one and this is a weight right here that goes from two to two so notice how there are two inputs two hidden neurons four weights in other words the weights and I'm going to draw I'm going to kind of use start to use Matrix not a little bit the weights can be expressed like this 1 1 1 2 2 1 2 two row I want those to be row column hold on let me think about this for a second time out time out time out this second uh because maybe my numbering should actually be row row yeah row I did it right I did it right never mind okay okay so this is a way of expressing the weights and a way of expressing the inputs I could write it like this X1 X2 okay so I'm making the case that I have two inputs and I have four weights and I could write it out like a matrix of numbers a 2X two Matrix and this is essentially a 2X one Matrix whenever I'm going to get more into matrices in the next video or am I in that video already I don't even remember I don't know where I am in my world but uh typically when we talk about a matrix a grid of numbers we reference it rows by columns 2 by two 2 by one okay so let me just show you something remember this we need a weighted sum here this weighted sum is X1 times weight 1 one plus X2 times weight 21 okay that's the weighted sum for this neuron or node the weighted sum for this neuron or node is X1 times the weight from 1 to 2 and X2 times the weight of 2 to two plus X2 times the weight of 2 to two it so happens I could take these two results I could call this like uh H1 and call this H2 and I could say let me actually say I could I could basically say this times this equals H1 H2 so this is the actual math the way that we described it look at both inputs coming in multiplied by their weights and summed look at both inputs coming in multiply by their weights and summed these are the the it written out but it so just happens that this exact math writing it like this and producing this outcome is exactly the math that is part of a field of study called linear algebra linear algebra involves manipulating vectors and matrices a vector being a onedimensional list of values a matrix being a twodimensional list of values the inputs are always always onedimensional the outputs are always onedimensional the weights are always can always be expressed as twod dimensionals it's every input connected to every hidden you can think of it very much like pixels every row and every column so this is where I need to stop and what I want to do is do a few videos that cover this notation and math with a bit more detail writing a little JavaScript simple JavaScript Matrix library and ultimately once we done that we can come back here and see how if we have that Library written we can then use it to do the math between the inputs and the hidden and the hiddens to the output and ultimately later we're also going to go backwards through the network to tweak values and and train it and that's we're also going to use the same Matrix math so this is why we need or why we don't need because we could kind of do it without it but while it's useful to work with this idea of linear algebra and I should note once again that if we were doing this in something like python using a library like something called nump we would get all of this stuff for free and there are JavaScript Matrix libraries and might but I'm going to kind of unpack some of this and and write a lot of the code from scratch just to have a sense of how it's working because why not okay I'll see you in the next video where I look at this a bit more in a bit more detail thanks very much okay uh how we doing here what did I get wrong uh oh I'm wrong looks like I got something wrong shoot X1 * w11 X2 * w21 X1 * this plus this X2 * this plus this I don't see it being wrong let me go back and look at the chat I'm pretty sure I got it right oh whoops sorry I didn't change the camera I'm pretty sure I got it right the way that u i mean I'm going to cover this in the next video but the way that you do this is essentially uh these two um sorry I I lost my train of thought uh I basically take I do the dot product of like this vector and this Vector so that would be w11 * X1 plus W12 * X2 w11 * X1 w21 * X2 and then I would do that for this and this W2 uh oh this is wrong maybe wait wa wait wait when I looked at it a second ago it was right oh maybe I've just did I write this in the wrong way and this should have been 21 up here ah shoot yeah this should be shoot I miswrote I wrote this wrong I knew this would happen uh and then this should be um one two I have to go back we have to go back we have to go back we have to go back hold let me check the chat uh uh no did I have it right the first time Matrix is correct okay hold on I I I I I did it to myself hold on was it correct oh why I'm look at the chat here formula is wrong the problem is you guys are behind me so you guys are uh yep the par below does not match the Matrix so here's the thing this is correct the way that I've let me go and um let me look at a a let me look at the way I mean I I I these definitely don't match these match and the way I thought of it writing it this match matches but I I'm pretty sure that this should actually be down here so let me look at how it's the notation is used in um in this book um yeah yeah yeah yeah yeah yeah I just shoot I messed up so I don't know if you guys can see this but um Tariq this book uses very similar notation and so I have to I'm trying to decide whether so I just I just want to um make this one two and this 2 one whether I want to I hate the fact that this is going to be wrong throughout the whole video and then I'm going to correct it at the end but I think if I go back I don't I don't want to like redo this whole section I wish I could watch this back right now actually I can sort of by stopping because like where where could where could I erase the board from yeah I'll just mention that it's wrong and I'll fix it in the next video it wasn't so long in the video lost is right um no nothing the this um I I just ended up notating it in this is one I I was actually asking myself this question I decided that I was right and then I realized I was wrong because I'm thinking of this row column row column really column row column row um this should all right so just trying to decide if I want to like rerecord this tutorial with the correct value Val is there or just issue a correction at the end of a video I need to do another straw poll I I I just I wish I had a sense of where where I was where because I think I'll just issue a correction um oh yeah okay good idea so um Topher in the chat is just suggesting you know what I can do is when to make the edited version of this we can also I mean this is more work for MATA but um I know this is technically possible um we could maybe just put an actual annotation in the video like an overlay so I'm just going to issue a correction right now okay um so hopefully as you were watching the video you saw a little annotation um this is actually incorrect I mean everything about this math is correct this matches this right the weighted sum is X1 * weight one from 1 to 1 X2 * weight from 2 to 1 but actually the notation that I the way I wrote this Matrix as we go as I go into the next video where I actually look at how the Matrix math works this really should be written as one two and this should really be written as 21 the reason why that is is this should be X1 * w11 plus X2 * w21 which is written right here so that Matrix mathod I'm going to go in more detail in the next video we take this row and multiply it by this column and this row and multiply it by this column and you can see that's what these two things are okay so thanks for bearing with me I there's a lot of little pieces but I am going to get back into the code so in the next video I'm not very confident about the order I'm doing all this in but it's just the way that I'm going to choose to build it and so in the ne again I'm saying this again the next video I'm going to look at the Matrix math again and then write a generic Library that does that math and then come back and put it back into the neural network itself okay so see you in the next video thanks um okay so um I think I got it is that correction good put some thumbs up the notation on The Matrix is always because there's so much going so much chatter going on I can't tell what people are referring to what time is it an hour and 40 minutes um formula is wrong not the Matrix can we agree that what I have here right now is correct before I move on I want to make sure there's nothing else that's wrong here to me I feel I'm going to look at the my uh tariq's book again good job everything is good some people say no I forgot your notation is bad no it's correct all right let me look here boy I'm getting a lot of mixed signals here let me look and see how Tariq notates it um so I'm going to look I'm looking at this input one time weight 1 one plus input 2 time weight 2 1 input 1 * weight one 2 plus input 2 * weight 2 two okay and then the matrices are 1 1 2 1 1 2 22 1 2 so I now have just confirmed that my notation matches exactly this notation the one thing that I've done that's kind of incorrect is swapping the order of these I really should be saying weight one * X1 plus weight 1 1 * X1 weight 2 1 * X2 I kind of wrote it out this way but that's a minor detail because it's the math is equivalent but maybe the standard would be to write this multiplied by this so I'm feeling based on the fact that I have now looked at this particular book and see that what I did matches I'm feeling more confident about it oh somebody is saying Matrix notation is different for Europe and America oh whoa oh no don't tell me I got to use the metric Matrix system this is so interesting is that really true let's Google this Matrix notation Europe versus America Two dimensions are read by 2 by3 that's how I do it two rows and three columns one one one two 1 1 three ah so yeah so the awkward thing here is so this is the unfortunate awkward thing these are mapped to this but you wouldn't normally I think notate that's why I wrote it the other way the first time around you wouldn't normally notate it that way because you would sort of do in this notation you would say x by y or column by row but these aren't actual these aren't actually the column and row numbers they're the weight mapping so I I think I'm a little bit off in the weeds here um I'm going to leave this as is and when I do the generic linear algebra videos I'll try to use this the more St it won't be tied to the neural network stuff I can kind of think about it in a different way I'm going to go read the chat again one more time yeah the don't differ so far classic R someone is fooling with me all that also could be possible that people are trolling me because yeah it's the transpose uh all right okay I'm seeing some very interesting discussion but I am going to to um move on um and I guess I could add a little addendum there and maybe I'll just for the sake of argument uh I'm going to just record that saying that in one sentence to tack it on could be edited in I guess somehow if necessary let me um one more thing I should point out about now that I fixed this notation that's a bit awkward here is that there are different styles for notating Matrix you know and again I'm going to use the convention of rows by columns so this is a 2X two row column but you would notice here that typically these would then be row column row column so this would be one one one one2 that's why I Had It reversed the first place but because I'm taking it from these weights these aren't actually the row column numbers oh my head hurts already um but it's all going to work out it's all going to be fine uh just bear with me um um in this sort of notation snafu okay the important thing is that this actually matches the way I want to describe the weights and the and those weighted sums and I will we're going to double back and everything should hopefully as I get through more explanation stuff will start to make more sense okay um I don't think that should be edited in I don't think yes that should not be added let's just leave it as is people in the comments will complain but I'm I I had the whatever I'm GNA keep going yes I know about transposing matricies use please use square brackets um okay oh my God the chat is going crazy oh my God good job uh all right all right all right okay okay okay I'm I'm I'm I'm let me just say for the sake of argument I'm sorry I'm going to move on [Laughter] okay this is now going to be erased all right now now we come to the point where I actually took some notes you might be shocked to hear this okay do I have the stamina and energy to keep going oh hold on hold on Jedi was writing something in the chat I can't you guys are write the hidden nodes are the rows and the input nodes are the columns exactly everyone knows how to troll me now I'm GNA keep going oh the fact that I wasn't using square brackets don't you worry I'll use square brackets now so I should use I should do square brackets okay hello here I am so I'm trying I'm moving along here through this journey of trying to program this neural network library again I might suggest skip ahead find some videos where I'm just using the library but I'm I'm doing this I'm exposing this process of a person struggling to make sense of the world but for this video I did actually make some notes um and I want to reference actually a there's a nice um a medium post about kind of what linear algebra you need to know for for deep learning that I will uh show you on my laptop in a second and and link to it in the video where I read that post this morning and helped me kind of gather my thoughts for this particular set of video so what I've done so far is I've established that we need this idea of linear algebra in order to perform some of the math in the neural network library that I'm building so what I want to do is take a break let me start over let me not start over that's fine it's fine so what I want to do is take a break from the neural network stuff itself and look at the linear algebra stuff in a vacuum and yes finally actually hopefully write some code because I want to talk through the math and implement the math in code in a generic way and then apply that to the neural network we're g to get through this everybody okay so what are the core so I have I I have some props wait time out so unnecessary but since I brought these up here I found my old linear algebra textbooks from 20 some plus 25 some amount of years ago so I brought these as props I was reading them this morning but here's the thing this is not a course in linear algebra there's actually some great linear algebra videos on KH Academy um probably there are some other ones out there I will link to additional resources in the description of this video I want to do is cover the aspects of linear algebra that are necessary or relevant to the neural network stuff um um and kind of leave out the rest so I'm going to give that an attempt and see how it goes and write code along with it and you'll let me know how that goes okay so here's the thing there are two key Concepts in linear algebra there's the idea of a vector and there's the idea of a matrix now a vector is actually something that I've spent a lot of time in previous videos in this nature of code playlist talking about the idea of a twodimensional Vector an entity magnitude and direction in a twodimensional space we use this Vector for forces and velocity and all sorts of physics simulation all sorts of stuff but ultimately this Vector is just an X and A Y that two dimensional Vector from and of course could be a z if it were a threedimensional Vector for all the computer graphics and animation physics simulation stuff I've done in previous videos we could think though about we can we can consider a vector as just an N dimensional list of values and I could make the notation like this and I could say x0 X1 X2 X3 X4 X5 so this is a fivedimensional vector there you go so this is the idea of a vector now one thing I should note is that you will see a variety of different kinds of notation um you might see them am I still you might see things written like this XY you might see it written like this XY different textbooks different styles I'm going to use this square bracket notation for the algorithms and examples I'm going to demonstrate in this video and in future videos okay so that's the idea of a vector now if you also recall we can do math with vectors and there are a few different kinds of operations there's the idea of a scalar operation like let's say I have the vector two 2 three and I multiply that by the number two I could take this scalar value the single value and multiply it by each component of the vector and I would now have 4 six there also are operations that are referred to as element wise this is the kind of operation that I did over and over again if I had a velocity vector and a position Vector so if I had a position Vector that was something like you know 2 three and then I had a velocity Vector that was you know 15 I could add elementwise add these together so the first element add to the this the the first elements get added together so 2 +1 is 1 the second two elements get added together 3 + 5 is 8 so these are element wise operations now in addition to that there is also something reference referred to as Vector multiplication and there's like the dot product and the cross product there's like the Hadar how do you say that hodaru anyway there's so I don't so I'm kind of reminding you of some things and I I have a bunch of videos on the dot product the dot product I use in videos to look at the angle between two vectors there's a path finding example we really needed the dot product to figure out how to get a moving agent to follow a path and the way the dot product works is we take two vectors and get a single scalar value so you can see these scalar operations a vector by times a single number we get a vector these element wise operations a vector plus a vector we get a vector the dotproduct and the reason why I'm going through this is I'm going to use this again again once I get to Matrix Matrix is where the new stuff is the dot product if I have 2 three uh I just use these same values 1 5 the way that we do this is we take the first value we wait time out for a second I'm so used to looking at this in without actual numbers we take X like let me just look up the formula I'm like oh and I I'm going off this I I'm going off the window so let me correct this um let me try to oops so um just take a break for a second if I I had a b and c d the dotproduct would be a time D + B * C is that right losing my mind here is that right I it's like been such a long day and I'm doing so many things uh no I got it wrong hold on it's F wait I I I wrote this down A1 oh oh no no no no it's the X's plus the Y's a * C plus b * I I over complicated this in my head of course I'm such a sorry about that I know I I I got like confused for a second it's it's simpler than I think yeah yeah I don't know why I I was thinking about how I do yeah like here's all my excuses for I had that wrong but really I just kind of get confused a lot okay because this is none of this is fresh in my mind I'm like just pulling this out of a old body of knowledge I guess and uh it's usually easier just to look stuff up okay okay let's so the way that the dot product works is we actually take the if these were X and Y values we take the x's and multiply them together and the Y's and multiply them together and add them together it's kind of like that weighted sum thing that I was doing earlier in the sort of neural network in the perceptron stuff so I would take 2 * 1 which is 2 plus 3 * 5 which is 15 and I would get uh 13 so that is the dot product so this is linear algebra now if I wanted to implement all this stuff in code I could actually come back over here oops hold on don't see that I have the dot product Wikipedia page up or anything um so let me actually let me go to where I want to go um I want to go to the uh github.com I'm going to do is go to Source math P5 do vector and you know um oh my God what a just want to find the dot product function oh this.x yeah yeah yeah sorry sorry okay ah sorry I forgot to switch the camera here I am um okay okay okay let me come back so I could take the next step and I could start to write code for all these operations for vectors but I'm not going to bother with that because ultimately what I need for the neural network library is the Matrix stuff but I starting with the vector stuff because it's all going to translate uh it's it's all going to it's going to be analogous but I should point out that this is all in if you're in p5js for example there's P5 vector. JS the source for the P5 is all on GitHub and you can actually find all of these operations here's the dotproduct function you know if I look for the uh add function here's you know adding two vectors together so you can start to actually go and unpack for these 2D and 3D vectors um how that math works in the source code but now what I want to do is redo this but not for vectors but for matricies so the idea here is what I want to now do is I want to understand well what if I'm storing numbers in a mat and why would I do that well there are so many reasons pixels live in a matrix data in a spreadsheet is in a matrix the weights of Connections in a neural networks can be in a neural network can be stored in a matrix so there are so many scenarios in programming where the numbers that we're working with are stored in a matrix and we could think of that like a twodimensional array um that we want to perform these kind of mathematical operations very very often so what is a matrix a matrix instead of a a linear list of values is a twodimensional grid of values and I could think of it like this A B C D E F and this would be a 2 by three Matrix typically we refer to matrix by the number of rows and the number of columns two rows three columns so in that sense we can redo all of these mathematical operations with a matrix so let's do these one at a time and then also write the code actually yeah anybody have a sense of like how long this particular video starting from the linear algebra stuff has been does anybody note the time where I started trying to decide like maybe do I want to keep going and do the code also in this video or maybe I should just describe describe actually describe these maybe I should come back and do this one in a separate video yeah yeah yeah let me see if anybody in the chat has told me about how long they think we've been going six hours it's no no not the whole live stream just since I started writing linear algebra up here so not all the other stuff I did 1 by two ah K weekman is here to correct everything shouldn't be a 1 by two a vector is a uh a vector is a one by is a 2 by one typically is usually how this camera went off um 15 minutes around 30 minutes okay that's reasonable I'm going to keep going then um okay okay I lost my train of thought so so let's look at these kinds of mathematical operations now with a matrix so I could do a scalar and this should be an a I don't know scalar operation so let's say I have the Matrix 2 uh 3 4 uh n and if I were to multiply that by the number two an scalar operation would just double all of these values so this would give me then the Matrix 4 68 18 okay so let's actually let's pause for a second I'm not really going to pause and let's before we get to these other operations let's start to write some code okay so what I want to do is have a live library that allows me to create a matrix of values and then perform a scalar operation let's go write the code for that now I should point out that what I'm doing the nature of what I'm doing is kind of ridiculous because there is uh math. JS this is an extensive math library that includes an entire Matrix implementation there is also GPU dos which is a GPU accelerated JavaScript library for doing Matrix operations and you know uh talk about GPU stuff that a little while later but um there's also I think uh matrix. JS there's P5 as a matrix implementation but um I am going to write my own just to kind of understand how it works and then later as part of this Library I probably want to swap it out to have something more efficient that's going to actually you know opt do these Matrix operations optimally but so let's create a new file I'm going to call it matrix. JS and I'm going to write a Constructor function and I'm going to call that a matrix and the Constructor should get a n a certain amount of rows and columns and I should say this. rows equals rows it's been so long since I typed this dot it feels good this do calls equals columns okay so the idea being that I want to be able to say varm is a new Matrix 3x two something like that right that's the idea here I want to be able to just generate a matrix okay so for example I can do this just here in the console now oh let's actually go to index.html and add in the neural network library and the Matrix library now and I should be able to say varm equals a new Matrix 3 comma 2 and I can see there we go I have a matrix object with three rows and two columns okay now we got to come up with a way of at least initializing the values and this is this is 2x3 and I said 3x two but whatever so let's initialize all the values as zero so how do I do that well ultimately I need to have a variable and maybe I'll just actually call it Matrix I could call it values I don't know what to call it I'm going to call it Matrix equals an array now there are all sorts of sophisticated JavaScript ways you know I'm only ever going to put floating Point numbers in these I can have fixed size to allocate the memory in some optimal way but I'm just going to live in the breeze po this in the most kind easiest loosest friendliest way and then we can always come back and optimize to use some more efficient and optimal data structures later so what do I want to do I first want to have um a certain amount of uh columns time out for a second uh I want to see how did I do this I'm just curious in the library that I made here I just want to look at my implementation no I did Rose yeah because I did it Rose so interesting because we typically this is that back to that is rows first or columns first o o the bane of my existence I got to just go back to like generative art coding challenges I is row we are triggered yeah no kidding don't worry I'm now correcting that okay so what I first want to do again the traditional way to think about a matrix is rows by columns so I'm going to start with a loop through the number of rows and I'm going to say every single um row is also an array and then I am going to Loop through all of the columns and I have a j here an I here by accident and say then every single row column location is a value and let's just initialize them all at zero whoops so this is me now making a matrix of values everything with zero so let's go back to the browser oops let me mat you a little edit point there because I want it to be on this page let's go back to the browser and let's refresh the page and create that Matrix again and I should now see Matrix has three three rows and two columns and then it has an array each one of these rows has two values 0 Z 0 0 0 0 so this is now we can see the data is actually stored in there so I've got the beginnings of a matrix Library nothing about this is optimal or efficient but I have a library an object that stores the number of rows and the number of columns and creates a twodimensional array fill the zeros okay so now what I'm going to do so we kind of now we have the ability of a library to create this Matrix the next thing that I want to do is add a function that performs a scalar operation so for example let's add a function that's called multiply which is the wording of this is a little bit tricky because ultimately Vector matrix multiplication can mean a lot of different things but just for right now I'm going to write a function matrix. prototype that's part of the Matrix object all Matrix objects I'm going to call it I guess I could call it scale let's just call it scale for right now um equals a function that's going to receive a single value n and what do I want to do I want to I'm going to do this a lot Loop through every single row Loop through every single column colum and say this. Matrix i j times equals that value let's call I'm going to call this multiply and then I'm going to add quickly add another one for another scalar operation called add and I'm going to uh say plus equals so again this is this idea I've written two functions these are scalar functions I just want to take a single value and multiply every value in the matrix by that value or I want to take a single value and add it to every single value in The Matrix that's what these two functions can do so let's now come back here once again oh I've got a syntax error I guess I have an extra Clos curly bracket so I'm going to create that 3x2 Matrix again I'm going to say add five now let's look at it and I should see the values in it should be all fives right now again we're not really seeing the Nuance of this because there're not different values but it started as zeros and then I added fives to it and now I could say m. multiply 3 oops oh I called it multiply and if I look at M again now and I start to look at those values we can see all the values are 5 so what do I have so far I have a simple Matrix implementation that allows me to initialize a grid of numbers by rows and columns and perform scalar operations I can multiply or I can um add so I'm going to pause here and in the next video I'm going to do elementwise operations and then we're going to start to look at other Vector multiplication which is really no longer the dot product but we I'll talk about sorry matrix multiplication so I'm going to kind of break these out into separate videos and I'm going to show you some interesting things about building a JavaScript library where I can actually determine what's coming in I can reuse the multiply uh and the add function um to determine am I adding a scaler or am I adding a whole other Matrix so I'm going to get to that in the next video okay thanks o I am running out of steam here oh boy this is how I feel right now where am I TimeWise oh two hours and 15 minutes toxic desire asked I'm sorry how long are we going to be here I wanted to at least get to the end of this uh but um can I yeah but it is it really worth the stress rebuilding existing libraries from scratch I don't know I feel like people are interested in learning this stuff and seeing the process of making it but um it's a good question I am more generally of the I I I suffer from the when I do video tutorials that I have kind of the theoretical infinite time and so I just like well I might as well fill in as many details as I can whereas if I were doing uh teaching a course and it just meets like a couple hours each week and I want to get to Applications I would kind of talk through the stuff in more generalities and show the library and that kind of thing so um but but uh fortunately a lot of people in the chat are enjoying this so much coffee shot um coffee shot for me yeah okay well is it's not just a matter of me running out of steam I uh have a endof year school picnic to attend I have no idea if it's does anybody know if it's raining outside in New York City right now because that picnic might not be I can actually look at my let's see uh I can try to look at the I mean I basically like a room with no windows uh I could try to look at the weather on my phone it doesn't say that it's raining right now it just has a 50% chance so I think that picnic is on yeah this is going to be a series I I wanted to I feel like I wanted to make it to a good end point today which was all the way through element wise and matrix multiplication let me at least let but I but I but I don't know if that's realistic um oh wait wait you can use table array to visualize in console this is a really good tip so are you saying if I do table m. Matrix how do I do that uh I guess I don't know so I if anybody has a tip for me there okay oh I forgot about the what is it called that hadam or hadam hadamard why is it called that sure product Oh French ma mathematician jacqu hadamard or German mathematician isai Shore okay these are really easy to do okay uh console. table thank you console. oh did I oh look at that oh my God my whole life has changed wow my whole life has just changed in this moment I did not know you could do that that is amazing I don't know I feel so happy that's like the greatest thing that happened to me in a long time I'm so tired it takes a lot of energy to do these you know I think also doing this stuff like Friday afternoon after a long busy week um hey let me try to get let me try to get a little bit further along I'm gonna actually open this video with that who who suggested that first I want to like thank the person I don't know who it was yeah Karma points okay okay let's keep going gelito was it gelito okay I think it was galito hopefully I got that right hi this you you don't know what you're about to watch this is ostensibly just like a random video in the middle of a very long series of building a neural network and I'm kind of now doing matrices and I already started it and this is really just a video about doing elementwise operations with a matrix and adding that to the Matrix little Matrix library that I'm building but guess what I'm opening this video with something really exciting that I just learned that I never knew thank you gelito from the YouTube chat for pointing this out but what let let me set the stage of where I am we're building this Matrix Library the idea is to be able to store numbers in a grid and perform different mathematical operations with them and we're going to ultimately use this library to do weighted sums in an neural network and right now I'm about to add an elementwise operation but I just did the basics of creating the Matrix and um the basics of creating the Matrix and multiplying or adding a value to it okay so now let's review I could say varm is a new Matrix and it's a 3X two and then I previously was looking at it like this and kind of going like this and trying to like look at the values in it but I learned that I can say console. table and then path in an array look at this console. path in an array my whole life is Chang in an instance realizing that now I can have this nice little tabular View and so I can just say uh multiply or I can do add five and then I can look at it again and I can see that there's fives in there and I could try to do other stuff and there's so many things I'm going to need to do to like check if it's working this is going to make it so much better I had to fake my reaction right now because I when I really first learned that I was genuinely oh that I'm still genuinely excited about it okay uh but for everybody watching live had to watch me get excited about it twice apolog for that okay so uh you watching this video right now let's add the next piece so what's interesting here is one of the things I wanted to do right is just say let me for example add this I multiplied but add the number two to each one of these values but what if I had another Matrix you know 3 1 4 3 and what if I wanted to add this Matrix to this Matrix element wise what element wise means is if I have two matrices A B C D and I have e fgh h i get a resulting Matrix that has a + e b + F C + g d + h i just take these two values and add them together these two values add them together these two values add them together these two values add them together now this will only work the way that I've described it to you if these Matrix matrices have the same dimensions the same number of rows and the same number of columns now there is there's something in Python the numpy library which is the core uh you know Matrix math library in Python has I forgot what it's called what's that thing called where it like time out for a second I'm going to edit this video right here and come back with the answer to what is that um what's that thing in Python what's that thing in Python where it um um it like can can actually do element wise with slightly different dimensions where was I reading this um it was actually on the um that blog post deep learning uh linear algebra I really should be thanking this blog post so this is u i got to remember to linear algebra cheat sheet for deep learning by Brendan Fortuner thank you so much this has really helped me um somewhere down towards the bottom here uh um am I losing my mind here broadcasting Elemental are relaxed via mechanism called broadcasting um yeah yeah yeah yeah okay thank you um okay where was I back here okay I'm back it's called broadcasting in numpy so but we're we're going to live in a simpler world where we for this we have to have the dimensions match exactly so what I want to do now is I want to keep those multiply and add functions I want them to be the same function but I want those functions to be able to receive a single number and add that single number to all of the values or receive another m Matrix and add all those Val add the values of of and add those values element wise so let's go back and add this now there's some things that I need to do for example I first why don't I at least uh write a function called uh randomize and what this function will do and you're going to see this and everything is just give each value a random value so I am going to um this I'm going to do something rather silly right now where I'm just going to say math. floor math.random time 10 so I'm not using the p5js random and floor functions writing this library because I want this library to be able to be used outside of the p5js library so I have to actually just use the native JavaScript random and floor functions so I should be able to now oops syntax error line 16 oh this should say equals function I should be able to say uh here's a new Matrix m. randomize and then let's look at its values and you can see there we go 1 1 18 38 1 14 those are random values so now if I were to say m. multiply by two and look at it again we can see there we go 216 616 28 great so now at least we can experiment and use different values now here's the thing look at this function matrix. prototype. add equals function n the argument coming in is n a single value but what if n isn't a single value what if it was actually a matrix so actually what I can do here is say if n is an instance of Matrix let's see is that right instance of of what does that mean I'm trying to determine what the type of n is so I can look at that here right m m instance of Matrix true M instance of what's another JavaScript object array false right so it's an if what I'm basically saying is here the add function receives an argument that argument might be a matrix it might be something else if it is a matrix what I want to do is add all the values elementwise otherwise now I should probably check like is it actually just a single number but I'm kind of going to assume here that there's only two possible ways any reasonable person would call this function either with a matrix or a single number so if it's a matrix add the values element wise every i j should get added to the corresponding i j otherwise um otherwise just add the single value to every single value so let's now see if this element wise works it gives myself some more space here so I'm going to make a matrix that is I'm going to call it M1 and I'm going to say M2 is also a 3x2 matrix I'm going to randomize M1 I'm going to randomize M2 then I'm going to say console I'm going to look at them both table M1 oh whoops sorry let me clear this console table M1 do the actual array in there console. table M2 Matrix so we can see here these are my two matrices 6372 2 07 04 3173 let's double the values in M2 just to see that that works or let's add to the value sorry what I did this with ADD right so I'm going to say M2 do add one let's add one to every value in M2 and let me I'm going to make this font a little smaller hopefully you can still see it let's look at M1 and M2 so we can see yep every value in M2 increased by one now if I add M1 to M2 I should get a matrix that has 78 right 6 + 1 3 + 5 7 + 4 2 + 2 let's say M1 do add M2 let's do that ah okay ah H cannot read property zero of undefined what did I get wrong matrix. JS 29 what's wrong here n oh you know what I forgot probably a lot of you in the chat noticed this or if you watch this you notice this The Matrix object has inside of itself a variable that actually stores the values called Matrix and maybe I should call that something else I'm not so sure about this this has to be n dot matrix right if this is an instance of the Matrix object I want to add this matx matx values to the N Matrix values so unfortunately I'm going to have to redo all this I have one Matrix I have two matrices now I'm going to add one to the second one and then I'm going to add M2 to M1 okay didn't get an error now let's look oh wait I didn't randomize them they won't have the same values it's going to be zero at least okay give me a second here oh no M2 is going to have one in it so let's uh let's just let's randomize M1 let's randomize M2 let's add one to M2 takes a long time just to like get back to my test should prob just write this code into like a code example it would be much nicer that way uh I'll do that in the next video now let me look at all them uh console. m1m Matrix console. table M two. Matrix okay 1 + 5 is 6 6 + 5 is 11 so let's see what we get M M1 add M2 okay console. table M1 I no no no dot matrix I think that's right if I scroll back up oh I I cleared it I can't scroll back up someone will have to confirm the math but I think we have successfully now written a function into our library that can do either a scalar operation or an element operation element wise operation and it's the same function and if I go back to the library I could do this same thing with multiply however I'm going to leave that I'm going to just do that on my own time I'm going to leave that as an exercise for you so if you're following along and building this library with me now go and write the same code to make multiply work both scalar and element wise and we're the p to resin stall so to speak the thing that's the most important thing that I haven't gotten to is actual matrix multiplication that isn't uh element wise and this by the way this element wise matrix multiplication is referred to is commonly known as the hard Maru no oh no that's that's on Twitter hard on Twitter does amazing work but what's that product had hadamard product let's go to the Wikipedia page so that's what this is called hadamard also is the sure product uh um that's the element wise multiplication but matrix multiplication itself is actually going to work in a completely different way and is be going to become the fundamental piece of how we look at inputs and weights between layers in a neural network and multiply and add all those things together say that again it's going to be the fundamental piece of how we look at inputs and weights and how we multiply those things and add them all together in a neural network so so this is where we're building up to so in the next video I am going to look at uh matrix multiplication the sort of core piece and we're going to put that into our library there's some other things we got to look at transposing a matrix is something we'll need and a few other things too and then we'll be back into the Neal network uh and starting to put those pieces together there's a lot of a lot of elements to this a lot of videos but uh thanks for staying with me in this journey process thing and hopefully I'm doing okay see you soon oh okay uh hadamard thank you forgot to randomize okay um boy everybody is talking about gelito in the chat is that there's an actual person named gelito or did I make that up I'm in the wrong camera all right everybody I'm very sorry that this is as far as I got today um I don't even remember where I started I got to look at my phone I apologize for doing that in the middle of a live stream but I got to get to this picnic oh um um let's see okay hold on I'm send gotta send some text messages here hold on I'll play the this dot S as always I always forget the this dot this dot this this going to do this this to do this going to do this to do this this do this do this do I'm going to do this do this do going to this this do this do this do actual jugl I don't want break never forget okay I'm going to make a I'm going to make a prediction I remember distinctly when I've looked at my live dashboard a while ago at the beginning there were 747 people watching because I remember 747 like the like the airplane so I gone off the deep end here into Never Never Land of matrices and linear algebra and all this stuff I've left the core I think the core audience of this YouTube channel behind and uh I gonna guess that there are 300 people watching right now I will look 685 that's kind of amazing um okay uh let me go back let me check the slack Channel all right so everybody um I'm sorry that this is where I have to wrap up um I um this has been a 2our and 36 minute live stream uh let me mention to you so what's what's coming um what's coming is next Friday I will be back to continue this so I'm just going to keep going I'm going to do the matrix multiplication I think things will pick up speed at some point then I'm going to put that matrix multiplication into the neural network code I got to do the training and the back propagation and all this stuff o ve but that's going to come next week uh next week I will be at if any the O'Reilly AI conference here in New York so if anybody happens to be watching is going to this conference uh send me a tweet at shiftman so I can say Hai and give say heli which is coding train for hello say hello and um give you a sticker um if you so that's that um I will take a short few minutes to answer a few questions it is so hot in here and and I'll be back next Friday the galito stuff is really kind of unbelievable I I wish I could follow the chat if someone could like summarize that for me like what what happened what went wrong uh and uh gelito if I Google gelito what do I get I really should not oh that is a thing okay I don't want to go any further I don't want to go any further with this what I'm doing okay um oh I'm not giving a talk at the conference I'm just I'm just trying to absorb and and go to some sessions and going to some tutorials and stuff um so um okay so let me see um oh boy there's no way I'm going to be a look at that right s multiply and S add e multiply that would be right okay so gelito is a real YouTube user I thought that maybe I was just like like told a fake name and there was some kind of joke so is so so I'm fine you guys have your own thing I have my thing which was actually thanking a real person whose YouTube name is jolito got it okay okay um let me see if um I'm going to put on this Goodbye song and I will take a few questions uh I'm looking at the slack chat um that's kind of uh if you are want to support what I'm doing patreon.com/crashcourse terribly mind you uh and then a many years later I ended up at a program called ITP which is where I teach right now and I learned about creative coding and lots of other interactive media stuff there at this program can I make a first person uh 3D style first person game um that's not something close on my radar right now that'll be interesting to kind of look at systems like that but um uh I do I would like to do more 3D stuff and particularly get more guests for 3D stuff where am I from I grew up in Baltimore Maryland do you plan on making searching sorting algorithm visualizations interesting uh yeah that would be a great topic boy that would be a really nice topic to do and I could actually probably do those in like quick short videos where's my coating train hat it's me your friendly neighborhood train conductor need the microphone to still be all aboard I really shouldn't have the math. JS in the background for this allo Cho I'm losing my mind can I balance this on my head or my chin oh that hurts my neck no I don't think so um yeah uh okay yes this is my engineer costume all right uh yes the renaming the functions I did see that um I think I think I don't think there's any questions left CU people are just talking about gelito so I think I'm going to be saying goodbye uh I see some people are typing in the um oh what live yeah having subtraction as its own function hold on so okay so k u there's been a discussion actually in the um in the slack channel in the slack live chat about what operations I have left to do so I need to do the um element wise for multiply to add that in I need to rethink the naming because what should I actually call the matrix multiplication function and you know versus element wise function so I think they need to have name I need to do the matrix multiplication I need to do the transpose um we can actually you can actually see all this stuff um is in the um you because I I did this a month ago or so already um so I can actually just look at that here and I'm kind of doing it from scratch again now so transpose copy add multiply what did I do I guess I called it a map is something we're going to need but I'll add that dot is what I called it is that technically a good thing to call it I'm not so sure so that's what I called it okay so that's yeah so that's pretty much what I have left to do and so one person in the chat asked about well I could have two different functions like s ad and E add like for scaler ad or element wise ad but I like the I mean so that would be a reasonable way but I I do kind of like the solution of reusing the same function and having the function kind of autod detect what's coming in um and so the other question was added was asked well should I have a subtract function and you could make the argument yes to have a subtract function but you could also just use add with a negative value and would have the same result but I I think it could have some utility especially for the element wise subtraction otherwise you have to like multip You' have to First Take a matrix and multiply by negative one and then add it that sort of thing all right thank you everybody for tuning in today uh thank you for bearing with me um I hope to get back to kind of just more creative examples and different generative algorithms and quick quick games and that kind of stuff uh more guests so but I want to try to see if I can get through this neural network stuff and um just make for people who are interested and um uh your feedback and your thoughts are highly appreciated and encouraged uh uh criticism and everything as well okay so um thank you guys I'm going to turn this button off I will be back next Friday and most likely a same sort of timing more a little bit later probably in the 300 p.m. eastern time which is I guess like 7 or 800 p.m. Greenwich meantime um okay so um see you all I don't have a great I don't do a great job of having a fixed schedule oh this is this computer is about to die it's not plugged in um this is actually those of you who want a little inside baseball here this computer has green green paper on it so that you don't see it uh but you can see I'm kind of like cut off by it if I walk over this way all right um so um see you all next Friday unless something magical happens I have time to do earlier but hopefully uh in between July 3rd and July 14th I'm going to have uh twice a week live stream so I'll update you about those schedule times and get through all this neural network learning stuff okay um thanks very much and I will see you next Friday goodbye