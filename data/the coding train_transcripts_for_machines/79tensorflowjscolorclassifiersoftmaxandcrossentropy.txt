alright I'm back in part 471 of building a color classifier now what am I gonna do here in the previous video I created the architecture of my model a hidden layer and output layer a subsea tension fluid is sequential model to dense layers activation functions units etc now at the end of the last video the next thing I need to do is define an optimization function and then compile the model well I really botched that is what there's three things I need to do optimization function loss function and compile the model and so I kind of conflated optimization and loss I'm optimizing against the loss but the optimizer that I want to make is I can use Const I guess here I get a very inconsistent about winning using converses let maybe I'll go back and clean up that code at some point I'm gonna say I can get it from TF train stochastic gradient descent and I can create a learning rate which I'm going to say is like 0.2 so this so one thing to do is create an optimization function right there are different options and we can try other options stochastic gradient descent is the one that I basically used in almost all of my examples and covered in detail in my how to build a neural network from scratch series and the idea of graded descent is walking along trying to go down the graph of the loss function to minimize that loss so what is the loss function that I want well if I'm gonna say model dot compile I believe this is a whoops this is a function that I'm going to write with a configuration option and one of the things when I compile the model I need to specify up to optimizer optimizer now this is very awkward that I just called this up here but that's fine and then the other thing that I just specified is a loss function mean squared error so this is typically what I have done in previous examples if you look at my EXOR coding challenge but this is now going to change and the reason is because I am using an activation function called softmax so let's talk about what softmax is softmax question mark ok so remember the output that we want from the neural network is a probability distribution right what's an example of what an output might look like it might look like this there's nine values 0.1 0.1 0.2 0 zero zero zero point seven zero zero right ohho my math is off zero point six right these all add up to a hundred percent this is the idea we're what this is saying is this particular RGB color has a 60% chance of being you know blueish if that's the particularly ball that matches with zero one two three four five index number six a 10% chance of being reddish a 10% chance of being purplish and a 2% chance of being greenish this is what we want now the training data is encoded like this and maybe we can actually look at it right next to it maybe this is what the training data looks like 0 0 1 0 0 0 0 0 0 a 1 hot encoded vector because actually the correct label for that color is greenish so I need a loss function sorry good cat across entropy and soft backs are linked together they're used together so that's why I just can't remember which one I'm explaining but I need a loss function to give me the error between this probability distribution and this probability distribution but I need my neural network to generate a probability distribution in the first place activation function as you might recall is something that squashes any number into some range it's one way of thinking about it the sigmoid function if we to graph that sigmoid function it looks like a boy can never do this something like this oh boy that's a terrible graph of it you can look it up on Wikipedia something more like this right and this the top is one the bottom is zero so any number given to sigmoid results in a number between zero and one softmax is an activation function that not only squashes the values that are coming in to these outputs between zero and one but guarantees that they all add up to one now you might say to yourself that's easy that's very easy to do we do this all the time with normalizing data I could just find I could just take all of the outputs add them all up and then divide each one by the sum of the total right because let's say somewhere I have these numbers two two one five right I can add all these up and they're going to add up well look at that they added up to ten let me divide by ten I have point to 0.2 0.1 0.5 so this we could do this sort of like divided by the sum as our activation function in but that's but but this is not going to give us an AK or an accurate probability distribution that we want for this scenario and softmax is another way of doing the same thing with more that that sort of expands the difference this one makes this one much more likely expands the difference between these different values so the way that softmax works is we actually do the following you know that I gotta find it aracely's know that natural number e for natural log 2.7 something I think well what if I said and took E squared e squared e to the 1 power e to the 5th power what if I took all of these what if I took all of these and then added them all up and made that I'll call that the e sum and then just took each one of these values and / e summed that is softmax nutshell you'd only thing I'm gonna do I'm gonna have a like a tan tangent video that you can go and watch now or I'm actually gonna write the code for the softmax function I think we'll explain it better only it's worth doing that in this video but I'm gonna I'm going to do that in a separate video so look for that in the delete it look for a link to that in this video's description just to like just to be sure that I'm right about this we can now go here and this makes it look like oh my god this is like the craziest scariest thing in the world but you can see it right here the softmax function for a vector of values Z means take every value e to that Z index J power divided by the sum of all of those values and so that and you can see here the probability theory the output of the softmax function can be used to represent a categorical distribution a probability distribution over K different possible outcomes alright so again in a separate video I'm gonna write the code for softmax and actually it's right there intensive load KS also as functions for doing it and I'm gonna compare what those outcomes look like versus just summing and dividing but I'm gonna move on and say so if I've established that softmax is what I'm using as the activation function for the last layer the output layer the question then becomes what loss function should I use how do I calculate the error between the node the target outputs with the training data and what the what the model generated during the training process so again mean squared error would work here but I am gonna change that sounds like two categorical crossentropy why am i using that so first of all what is entropy entropy is a term that refers to like the chaos associated with the system's you can think about probability distribution is like being very chaotic or more or less chaotic so what the cross entropy function is a loss function designed to compare to probability distributions and look at how much chaos there is in between then the cross entropy between them and the math of it is you know mean squared error is like subtract take this one this one and then do like the square root square it then do the square root or make it up to the square root and add them all together mean squared error I've talked about that you can look it up it's a pretty simple mathematical function cross entropy if we look at it we get we could build that I could build this in a separate video which might be worth doing as well is really just the if if I have two probability distributions P and Q I'm looking at the mine negative the sum of one probability distribution times the log of the other probability distribution so again you can research what cross entropy how the math behind it works more in more detail and maybe I'll do a video about that for those who are interested but at the moment the important thing to do where am i over here the important thing to realize is that softmax is an activation function for generating a probability distribution and cross entropy is a loss function that works well for comparing two probability distributions so for a classification problem those are the two things we want to use so we've done that oh I think I'm done with this video let me just uh let me just kind of like run this code oh wait we got it unknown loss ah okay I think this is lowercase e okay there we go so so now we're done what is the next step what am I gonna do in the next video it is now time for me to call model dot fit model dot fit is actually the function I will call with the exes and the Y's that I've prepared in a previous video to train the model right I really only got two steps left and I'm sure there's gonna be lots of other stuff that are forgetting about right now I want to train the model then I want to use the model to give me a label for a new color that the user is going to specify okay so in the next video I'm going to actually add model dot fit see you then you