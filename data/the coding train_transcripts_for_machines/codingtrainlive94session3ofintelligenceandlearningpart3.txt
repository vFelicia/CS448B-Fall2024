helloo do you hear that why I believe that's the sound of the coding train pulling into the station hello good evening afternoon and welcome to another episode of the coding train with me dan Shipman this is a YouTube channel where I make coding tutorials and a variety of other things actually I don't do any other things I'm only do the coding skirt look my thing no tutorials is the right word I try to be here every week on Fridays typically I was here yesterday I do a terrible job at scheduling and keeping it just in time maybe someday I'll get better at that because I hear as a content creator in this world that we live in a 2017 it is very important to have regularly scheduled that's okay so today is actually a followup from yesterday if you weren't here yesterday with a thing that I covered is something called linear regression and today I'm gonna do linear regression again I don't know I don't know I don't know about these sound effects okay so yesterday I did something called linear regression we're on this train we're on a train we've got a destination that destination is machine learning station and ultimately I want to get to neural network based machine learning systems and I'm going to build a neural network from scratch and probably JavaScript but maybe in prospect I've decided yet on this channel but I'm trying to do a bunch of tutorials to set up a foundation for making that easier and linear regression looking at linear regression from a statistical approach and now looking at it with something called gradient descent gradient descent is a learning technique that is a fundamental piece of neural network based machine learning systems so today we're really not I it's not my intention that what I'm to show you will yield beautiful generative algorithmic artworks but rather will demonstrate a particular technique that I'm going to apply again later and also hopefully will give you a background for when you're reading blog post or papers or books that involve machine learning based systems and those systems talk about gradient descent and derivative this and calculus that a partial derivative is and chain rule that this hopefully the today's session will give you a kind of a background in that sort of stuff okay so I'm looking here to see what's going on in the chats I can see yep there's a link there we go the rainbow on the tshirt hasn't the same colors as the wallpaper it should oh oh it's different this is a slightly different design now if you're wondering oh look at that tshirt where could I get one of those tshirts like that I want one I don't know if that's what you're wondering you can go to I'm such a shill a sellout coding trained Storenvy comm actually I think the one that I've got here is I ordered a sample of a color that I didn't make available one thing that I'm discovering is the light weight I don't know why I'm talking about tshirts but the light weight tshirts with the sort of this is I think Heather blue or something I like those better I don't know how you feel about them but anyway you can you can get your own tshirt here okay I can add more colors I'll think about that on my own time um okay um I heard a noise it's it's like Friday afternoon here it's a holiday weekend here in the United States I can't imagine that there's anybody roaming in the hallways and I will soon be on my way to enjoy a nice holiday weekend hopefully before I do that I have to torture myself by attempting to talk about gradient descent now um calculus or something like that so before I get started I would like to read my what will now be my weekly quote I did one yesterday every live session I'll read you a quote from this book in her own word Rhodes this I mentioned this yesterday but I'll mention it again red burns was the founder of the interactive telecommunications program it's where I teach at Tisch School of the Arts at NYU she was she led the program for over 30 years and in her first class that she used to teach called the applications class though and anybody everybody just called it Reds class she had a slide presentation so this book is a compilation of quotes from her slide presentation I will read the one that I read yesterday so that you get the continuity as you come together depend on each other that was yesterday's and now today's is you will find yourself wearing the illfitting clothes of someone else's world dining on the strange food of someone else's thought okay so that's my read Burns quote from today hopefully getting us in the mood and the right frame of mind in the relaxed meditative state that is high to do some coding calculus linear algebra okay so what's going to be for no sound a chart some say no sound I don't believe that that's a one person saying this sound alright so now I'm just playing weird sounds for no reason at all um Oh is somebody here at the coding train I go over York it seems that the door oh there's nobody at the door but someday that would be great right if we had somebody at the door who would come in and say hi and then start talking about something you're over there okay come on and come back over here I think I'm just gonna dive in um dive in you know what I don't actually know how to dive I know how to swim but I don't know how to dive and I think I should take lessons and learn to dive alright so let's see how we're gonna get started with this I am going to first thing I'm going to do here is um clean the whiteboard I don't think that I need I should have done this clearly I should have done this in advance but I did not we're trying to think do I need any of this from yesterday maybe I did maybe I'm not going to clean it just yet and I will do that in the middle of the video at first so I think that what is it this is good this is like a total world record for me seven minutes in I'm going to start with the lesson thank you for somebody screenshotting that in case I need it okay understand the chat rightz finally finally some proper math so let me just make a good point here uh I think if you said finally some math you're the right on the right track there will be nothing proper about this whatsoever I have to say that I am NOT this is not something that I have a deep knowledge of this is not something that I consider myself an expert on this is something that I've been learning and reading about and playing around with over the last few months and so I'm going to attempt to impart some perspective and attempt at this and I have a feeling that there's gonna be a lot of stopping starting and hopefully some helpful suggestions and corrections from the various chats that are going on here oh I didn't change the camera thank you sorry I have a bad I think I probably should do the whole sensor thing that just knows where I am okay so sorry about that um was I going to say so I'm going to get corrections bla bla bla there's something else that I want to say we're on camera math is talking camera okay you guys are all behind I fixed it I fixed it already okay so let me go into session three and linear regression so this one is now okay if I raise this up is that a problem a linear regression I didn't publish a the code from yesterday ordinary least squares and linear regression gradient descent and I get a web browser as always I always forget the dis stop this stop this stop this duck and then GT this stop this stop underneath this dis stop this dot this dot and then using this stock will stop using aqua stop just stop this starts on this table this dot just gotta get to the stop this stop the stock starts on this stop the stock just I never forget to this stop just stop this stock this done and then retain this stock the stock when you lose this this stock the stock just died and you're doing this stop the stop is is not this stop it stop it stop and they're using this stock the stock is use this stock is stock this time and then do this stop this time this dock is never forget this stuff I'm gonna do this this dot this dot this dot this dot song never forget the Vince dot but somebody composed that song for me I'm just giving myself a little time to be silent for a second while I attempt to okay great so I now have this is the example from yesterday uh uh fidget spinner trust me I own there's a lot of fidget spinners in my household right now the bit it's a bit of a problem ok so this is what I did yesterday this is the example from yesterday where I am fitting a line let's see if I move this over a little bit actually it's fine it's fine where it is actually whoops nope give ourselves more room here just trying to get everything set up and let's go to a gradient descent we're gonna redo this so shoot hold on a row I'm just trying to get the code kind of ready I wonder if what I should do I wonder if it would be useful for folks I forgot to upload the code from yesterday to github let's see if I can manage to do that really quickly now so if people want to get it so github.com coding train rainbow code am I logged in I'm logged it that's a good sign this goes under nature of code oh boy Wow I don't even I need I don't know where I've got by the way anybody who wants to volunteer to help me organize and rethink this github repository possibly turning it into a a website where people can find listings of all the videos and all the code welcome contributions to thought there's a couple github issue threads people discussing that that said right now I think that this code goes under nature of code because I think technically speaking what what playlist am I in let's look at the numbering here I think I need to make a new I think I need to make a new playlist let's see here I'm sorry I'm like wasting everybody's time here intelligence and learning session 3 that's what this is where I am so the videos would be intelligence of learning three point something alright that's fine we are going to this is very confusing how to do this but I am going to go into courses and I'm going to can I make a new folder right on github create new file can I do this intelligence learning readme whoops slash Rio I can read meat readme MD code foreign intelligence and learning tutorials let's commit that file then again I might redo this later but now what I'm going to do is upload and what I'm going to do is grab from yesterday this ordinary leastsquares folder and so now I did commit sorry I didn't commit so close so close ordinary leastsquares and I'm a hit commit it's amazing what you can do with get and github just using their website right now you know okay so if you want to get the code from yesterday to follow along with what I'm doing you would go to this github repo under courses under oh I put it in the wrong place I'll fix this later right now it's right here in linear regression ordinary leastsquares so I got a number everything and put it in the right place but at least it's there right now you can go ahead and grab it it probably should just made it a gist but I did it okay so that's there now I want to do is get this code put it here remember when it was like seven minutes and I said I was about to start actually coding right okay so we are going to do this now great okay so here's the thing I'm gonna mention a couple books but I might mention these when I gets okay so first of all if you haven't watched how I say this every time but if this is your first time watching me live this is a live stream everything that I do today will be edited sometimes out of order into a set of smaller video tutorials that will get uploaded later so sometimes unfortunately there's a bit of redundancy I'll talk something through and then say it again but that's what probably gonna happen these are two books that I wanted to recommend one is make your own neural network by tariq rashid this has been helpful to me it has a really nice appendix also in the back that's kind of like a little introduction to calculus it's a little bit absurd to you know explain and talk about what calculus is in you know just like 10 to 20 pages it's even more ridiculous to do that in you know two three minutes of video that I might do but this I would also recommend for for anybody this is a book called calculus made easy this book is actually from what is the original edition looking for it originally published in 1910 it had subsequent editions in 1914 and 1946 and this is a update revision and republication of it in from I believe 1998 and it's a really it just has like all these like beautiful little diagrams in it maybe I can flip to a random page and do a little reading from it because it's just so I really find actually the first chapter on differentiating is kind of nice let's see what I can find here partial drip okay so partial differentiation is something that we need chapter two I love the fact that okay plus we're going to read this chapter one is titled to deliver you from the preliminary terrors page 39 the bedtime reading I'm going to read this to my children tonight at that time considering how many fools can calculate it is surprising that it should be thought either a difficult course the camera went up it's like a good place when I little stick was totally ruined there tries to get considering how many fools can calculate it is surprising that it should be thought either a difficult or tedious task for any other fool to learn how to master the same tricks so this is the prologue I don't wanna read the prologue I wanted to read to deliver you from prolific wait I need some like ominous bedtime music like I don't have any okay the preliminary terror which chokes off most high school students from even attempting to learn how to calculate can be abolished once and for all by simply stating what is the meaning in the common sense terms of the two principal symbols that are used in calculating the dreadful symbols are d which really means a little bit of the s' DX means a little bit of X or do you mean little bit of you ordinary mathematicians think of it more polite to say an element so instead of a little bit of just as you please but you will find that these little bits or elements may be considered to be infinitely small okay so clear yeah all right so the other thing that's very exciting look at this I developed this like massive technological leap forward in my live streaming capabilities yesterday when I discovered that I could use not one but two whiteboard colors and now I have purple and blue and green so I'm hoping to make use of all these different markers today now and now here we are we are ready to begin let's where's my water this is a very strange water that I'm drinking as I did have some coffee in this mug earlier if coffee was all gone but I didn't like completely rinse it out and I just fills it up with water through there's like this faint air of Oda cafe I guess it's cafe um that sort of still in here okay it's 308 p.m. I have until I have until I would say I'm hoping to finish around 430 p.m. but I have a little 15 to 30 minute grace period if I need more time but I I'm pretty committed today to getting through this was my list from yesterday and I did these two things I'm gonna jump to 4 first I think it's important for me to attempt to do that I guess rip off the bandaid actually had a bandaid on earlier so it's already off rip off the bandaid and do a gradient descent ok can you put the code on something like Dropbox instead of github I can't do that right now I'm not sort of set up to do that but I'm not opposed to doing something like that another time if that's helpful to people ok I'm looking at the chat I don't see anything I'm the wrong camera again this monitor that I have to keep track of where I am it's really not I think I need to put up a big one just up there in the wall where I'm looking straight ahead if I look up right now does it appear as if I'm looking at you or can you really tell that I'm looking up because here now I'm really looking at you if I look like slightly above anyway I'll figure this out later on my own time actually I don't never in this room unless I hit the streaming buttons I have to figure everything out while I'm live I don't know what is islets let's get a nice hold on let's get a nice little grip let's get a nice linear regression going here with some points kind of would like it to be more angled ok this is perfect yes I know I oh okay okay I better not look at the chat every once in a while somebody writes a mean comment to the chat I'm really just trying my best it is the Internet after all okay here we go hello okay so this is now another video in my series about linear regression now why are you watching these videos I'm not really sure to be honest but the the topic here and the skills here I hope are laying a foundation for what I'm going to get to in future videos which is building a neural network based machine learning system so at the top of this video why are we making another video about linear regression so what I did in the previous two videos is I created a p5.js sketch which implements linear regression using the ordinary leastsquares method so a statistical approach there are a whole bunch of data points into the space and I try to fit a line that that fits to that data as best as possible so that I could predict new data points in this space and you can see as I start to click around how the line is fit sort of changes now I also discussed a little bit of long does linear regression make sense based on your data and these are big important questions in working your data science and machine learning but right now we're just trying to focus on the techniques now one thing you'll notice is here is i refresh this page I make I click twice I get a line instantly because I am actually calculating the perfect exact best fit line according to the least squares method but someday we will have a data set that's not 2dimensional someday we will have a data set that has hundreds that's a big data set that's a many many dimensional and in this case there isn't going to be an easy statistical approach that could just be done to fit to create a model that fits the data perfectly with a single calculation on the so this is this is the problem that machine learning neural network based deep learning based systems are here to solve to figure out a way to create model to fit a given data set and one technique for doing that which is different than say ordinary leastsquares is to use a technique called gradient descent and what gradient ascent descent essentially does it says let me make a guess I'm just going to put the line here and I'm gonna see how is that line good or not so good that's so good let me try shifting it a little closer to the data let me try shifting it a little closer to data let me shift it let me shift it so making lots of little small nudges and tweaks into what that line is doing I think I have a better way of explaining that so I'm going to come over here to the whiteboard I'm going to do a little magic here which is that I'm going to stand right over here and I'm going to snap my fingers and the moment I snap my fingers the whiteboard behind me is going to be erased now for all of your watching that's live that clearly didn't happen but I'm setting up something I'm gonna race it I thought it was going to use this to explain something but I realized that was unnecessary so I don't know talk amongst yourself for a little bit I'm gonna I'm going to use this whiteboard cleaner I guess I should play some music or something I guess I could maybe get the kitten song going unicorns and rainbows and explode else is there yes kids thank you very much kittens and rainbows and cupcakes notice that look what I get I really I'll get my son Cody Creek okay uh here we go now what do I need to do I need to do I have a marker in my hand I can't even remember no I didn't because I just walked over here I was standing right about here Wow Oh does that work dorks this is kind of interesting okay so um let's I'm going to make this same kind of diagram that I've made a few times now and I'm going to we're going to like really simplify it so we have this idea of this two dimensional space there is some piece of data we'll call it X for example the temperature that it is outside today and we're trying to predict some outcome maybe based on that temperature yesterday we talked call that Y we heard about that of the sales of ice cream I still actually a dataset that was interesting about like the frequency at which crickets chirp according to the temperature outside that's the data set you can find online somewhere use so we have this idea maybe there's some existing data points based on ice cream store that we have studied and I can graph that data so the idea here is that we have our machine learning recipe we are going to take I know I'm out of the frame here we're going to take one of our inputs called X feed it into the machine learning recipe and the machine learning recipe is going to give us a prediction Y so we have known data and we can if we had new input data we could make a guess take a break for a second is this where I want to be going here uh yeah I think so I'm just thinking about this for a second hydrate someone gave me that I knew the camera was about to shut off what is life going to be like someday if the cameras don't shut off for third some highlights I don't think I'll be able to function okay okay I feel like I'm off here in where the camera view is you see if I can turn it a little bit this way because I like to be able to stand more over here okay great this is better now okay okay so yesterday my machine learning recipe was the ordinary leastsquares method meaning I was able to do a statistical analysis of all this data and create the line of best fit and then if I had a new input you know X value of suchandsuch I could look up its corresponding spot on the line and that would be the Y output this is a function the machine learning recipe is essentially solving for M and B in the equation of a line so that's what I did yesterday today's technique I want to demonstrate the technique known as gradient descent so the idea of gradient descent is okay so boy so much to say where should I start where should I end I really have no idea so one thing I'll mention is that the math required for gradient descent typically involve calculus and they involve two concepts from calculus one called a partial derivative which if you don't know calculus or what a derivative is well how can you be expected what a partial derivative is as well as a something called the chain rule and I think what I'm going to do is I'm going to walk through this entire system and how it works and explain it without diving deeper into the math um but I will want to make a followup video where I discuss some of those pieces in a bit more detail so so okay so but here's a way that you could think of gradient descent thats related to stuff that i have done in previous videos and in my book nature of code where i reference the work of Craig Reynolds steering behaviors so think about this for a second this is this is great let's say you have a twodimensional space and you have a vehicle that is moving around an agent that is moving around this space and the vehicle has a particular velocity expressed as a vector or an arrow in this case now what if the goal of this vehicle is to reach this target well we could say that this vehicle has a desired velocity its desired velocity is to move at maximum speed from its current location to towards the target so this is a vector which is its desired velocity and if you and you can think about the difference like the its current but this vehicles current velocity is like its guess I don't know where I should go I'm gonna try going this way oh but really I should go this way well if I'm going this way but I really should go this way what if I just turn a little bit towards the target what if I were to just steer a little bit in that direction and this is what gradient descent does you can think of desired as the known output the correct output what if I feed in one of these data points right and I say look at this particular X Y pair let me feed it in let me try to get a guess just sometimes I think written is why a tick I think but I'm going to say what if I get Y I must say Y guess the error is the difference between what I guessed it would be minus what it actually should be right if I start with an XY pair this is the error and you'll notice if you look at Craig Reynolds steering behaviors and all of these animate systems that I that I implemented from that work you'll see there's a formula in it steering equals desired minus velocity so you know I put I put it get in you know I kind of do the reverse here because this is really the equivalent of desire but the point is the difference between the way that I should go and the way that I am going that's the error the difference between what my machine learning recipe what my model currently thinks the output should be compared to the known output that is the error and steering Ike if I adjust my velocity if I steer towards the desired I'm going to get a better model I'm going to move towards the target if I use this error to tweak the parameters of the machine learning recipe I'm going to make my model I'm going to have better M and B values for the next time and I could do this over and over and over and over again and this is we've been talking about this supervised learning I can take the known data send it in get a guess look at the error tweak the knobs send the next data point and get a guess look at the error tweak the knobs I can do this over and over and over again and I can just start with random values for M and B so I don't know what it B I'm going to just put a line here and then I can start moving the line around according to the error as I go through all the data so this is what we're trying to do okay I'm going to pause for a second and see if see if anybody's got any comments or anything in the chat they want to like mention any Corrections that I'm going to move on so I think what I want to do next I think I'm going to start just adjusting the code yeah okay uh all right so change camera Oh Suraj is watching oh no don't don't watch it's gonna be all be wrong I'm going to delete this from the internet we square the difference as well to keep it positive and preserve the magnitude ah some of the squared errors yes this is a good point so I should mention this okay so actually a good point thank you ah thank the Internet thank god that's my thank you sir thank you sorry you guys should watch Suraj's channel especially for all of you are like how come you're not doing Python Java Python Python Python well I'm someday but go watch Suraj Jenna lots of Python there um so I forget to mention something which is important especially once I start getting the math I don't know if I like to say maths but now I worry that I'm I said this yesterday that I'm sounding pretentious it's a bit of a problem okay so we I'm going to come back into my tutorial okay um okay so I should mention about right cuz I should mention about squaring the error the thing about it is yeah actually I don't I'm gonna come back to that later because when I do the follow up I'm going to so there's something else that I need to mention which is that there's stochastic gradient descent and batch gradient descent so I could adjust the knobs so to speak for each data point one at a time or I could process all the data look at the sort collective error and then adjust the knobs I don't know I don't know what to do here I should probably look at the chat again but I I think I'm going to come back to the squaring of this when I dive deeper into it yeah okay all right let's see here all right so I'm gonna I'm gonna keep going I'm gonna go because what I'm gonna do is I'm boom yeah there's cuz there's the hole right there's the hole error graph thing and square you know you change this parameter how does it look at this slow but I think I'm going to come back to that I think I'm going to leave that out of this sort of initial pass where's my eraser I'm not sure I'm very unsure about this but I think what I'm going to do is I'm just going to dive in and start programming it and then I'll come back and maybe follow up with looking at sort of like a graph of the error and talking about slope because I think I'm going to save the derivative stuff for like a follow up to if people want to go a little further into it everybody okay with this plan oxfordshire about it okay anyway so now I'm back thank you for the thank you to Matt you who helps to edit a lot of this stuff after the fact so I'm going to pretend it pretend I'm going to walk back and start adding to the soup of the cook okay so there's more to how the math behind this stuff works and how we look at the overall error and there's some stuff that involves the derivative and the slope of the graph of the error and I'm gonna I think I'm going to come back to some of that stuff in a second video where I go a bit further into some of the math here but what I'm actually going to do is just start showing you how to set up to do gradient descent in the code itself so let me come over here so this as you saw before this is the example from yesterday that's using the ordinary leastsquares method so what I'm going to do now is I am going to this so I had this function linear regression and this linear regression function calculates the slope of the line and the yintercept and and B according to or nearly squares so what I'm going to do is I'm just going to completely get rid of this so now nothing happens there so I can click and the first guess of the line I just plugged in some values that typically speaking I think what's probably typically done is these values are initialized at zero these are like weights so to speak and ultimately you can see these are analogous to the weights of connections in a neural network but this M and B values I could start with them randomly I could pick something and hardcode it I could get that the both be zero I think I'm gonna stick with actually one comma zero just to sort of see because then at least I can see that the line is there so now what I want to do is I want to look at with the existing data points I want to look at the error and I want to adjust N and B in the direction of the error so let's see how that goes so I'm going to call this now gradient descent and so in the draw function I think I want to call this now gradient descent I hear a noise in the hallway who may it be I won't go over there to check its middle making a video it's okay um it's just very distracting gradient descent I'm a person in a world in a world where I'm programming linear regression with gradient descent I've lost my mind this is gonna get edited out I have to go away oh oh I hurt my knee also so I can't really bend it okay um I'm back a little digression there that I had to edit out thanks for uh thanks for tuning in ok so where I am is that I'm changing the name of the function to gradient descent and what I want to do is I'm going to just look through all of the data so let's just first look through all the data and okay so for each data set I have the Y is data index i dot y so we can get the X and the y and I can actually calculate a guess so my guess is M times X plus B right this is my machine learning recipe I am taking the input data X I am multiplying it by n I am adding B and that is my guess so now my error equals my error equals y minus the guess and I think technically speaking I think I should be saying guess why now you'll you may recall that in the ordinary leastsquares method I would always square the error because I want to get rid of the sort of positive or negative aspect of it in this case and again I'm going to go a little further into this in the next video I actually want the positive or negative direction of the error because I want to know which way in essence to tune the M and B values to get a better result so the issue here is now that and this is what's known as stochastic gradient descent so I want to make an appoint that's available I want to make a change to M and B so I need to calculate how should I change M and how should I change B so really what I'm saying is M equals M plus some amount of change B equals B plus some amount of change and we can in this case kind of say this is a one way to think about and understand it I have this error whose respondent who is to blame here is it um is it UB who's in charge here what's the what's going on I got to figure this out so in essence we could say if I adjust those values according to the error maybe if I tried it again I would get a better result and in this case B can be adjusted directly by the error because it's just the yintercept should I move it up or down and M which is the slope can be adjusted by the error but according to according to also the input value itself so this is how you can kind of intuitively understand it I want to adjust those values according to the error the slope also relates to what the input actually was the yintercept just the air itself now so I'm missing a whole bunch of steps and a bit a few pieces of explanation here but let's just run this and see what happens so first I always have to click ok well first of all I got it error uncaught reference error and is not defined in gradient descent where did I have n Oh B equals B plus error yeah I don't know what n is so you can see like okay well I don't know where that line went it was there for a second and it just went far away so here's the thing if I come back to my analogy from the steering one of the things in the steering behavior examples from nature of code and Craig Reynolds examples is that there was a variable called maximum force know if you can see that maximum force because one thing you might think about it here so well how powerful I know what the error is between the way I'm going and where I want to go how powerful is my ability to turn well maybe I've able to turn at like imp with infinite power and that could be good but not so good because if I try to like push myself I might end up going all the way down this way then I'm like oh my god going in the wrong direction and then they end up going all the way up in the other direction maybe I just won't really want to be able to make little adjustments because it's the wrong way I want to just make a slight adjustment I don't want to overshoot the target this target being I want to find the parameters I want to find the weights the M and B values to minimize the error so so I don't want to overshoot what that minimum that that optimal value is and so that is where a variable sometimes called alpha but most commonly called learning rate comes in so I could have a variable called learning rate usually this is a small number something to really reduce the size of that error so in this case I would say well let me take this change in the value of the slope and multiply it by the learning rate and let me change take this for B and multiply it by the learning rate okay so now I'm going to try this again with a learning rate of point zero zero one hey that does it look right come back to me okay so let's think about what might be wrong the problem is I this is my like music for consider your thinking and then I just decide like I'm doing my ticket like and then I don't say I'm like often thinking about what I'm gonna have dinner these are like sickened rain tomorrow and have time to go jogging every what's right one chapter of Harry Potter my on again you know I pulled it behind okay so I was also waiting for somebody in the chat to give me the correction and I most likely I have a positive or negative problem here which is that probably and moving in the wrong direction so whenever that happens I could just switch one of these two I could also just say it oh yeah actually over here I wrote guess why and that's really what I that's what I wrote here no I want why guess I knew it was always the same so hopefully you're not watching this but in this case here right steering if I want to move towards the target the error is the desired the known result the velocity and so this should really be if I want to move in that direction why why guess and so let me come back over here and I'm going to do that camera went off this doover we come back over here and let me change that to I changed it already wait how did I do that okay I must have done it before then I went to explain it so let's try this looks pretty good right now here's the thing let's start let's start with M at zero because that was a little bit weird what just happened there so over time something is definitely wrong here and it might just be that my learning rate isn't good let's try a little bit of uh I'll have to think about this oh yeah why this is better now I'm I'm gonna come back yeah I guess I guess I'm doing this right it's really about the learning right here I guess all right if I click here I'm going to that line is going to stay and if I click here yeah okay hold on so let me go back to this Thank You much yeah this one's definitely need to be aggressively edited okay let's go so okay so let me look I let me put M and B back to zero let me put em in B back to zero hit refresh here and so let's see um so we can see interestingly enough this isn't the correct correct line because the line should really go through those two points you know I think I've got an issue here with the learning rate so you can see how it was kind of like moving to the right spot but then it's still making very very small small changes only have two points not a lot of data it's not a lot of time for it to change I probably just need kind of a larger higher learning rate here just for this demonstration let's make it at 0.05 and we can see now it's kind of moving much more quickly and it's starting to turn albeit very slowly but you can see as it's slowly slowly turning approaching the correct the correct or the optimal spot for this line and as you can see if I were to click again and click again try to you know click a lot up here and a lot down here ultimately eventually I should start getting the line of best fit now so there are you know there's some strategies that in theory you should really need to adjust the learning rate over time but there is a technique and a lot of machine learning systems that you will say see that you can call annealing I think that's the right word where you kind of start with a high learning rate and then slowly over time reduce it so you can kind of get some big Corrections at the beginning and then find some some smaller Corrections so but pause here for a second error equals I'm looking at the chat um to see if I'm missing anything because this I you know I think simulated yeah I have a feeling this is going to perform much better once I do the stochastic I'm sorry the batch gradient descent there's probably an issue with let me see my I have some examples already prebaked let me see how those let me just make sure to see if there's anything I'm really missing here so this was my example that I made yeah this is kind of the same interestingly enough so this looks similar to what I just demonstrated right now right it's behaving the same way let me look at what values I used in this particular example where is this these are the examples from my course from a while ago why did that not open correctly that's annoying there we go so let me go into this code I just want to compare this code before I come back into the actual video I have like a learning rate slider so this is is this doing the batch right now sorry this is hard to see yeah so this is actually doing the batch delta b yeah why why guess + equals Delta B so this is a different example which I'm going to do next the bigger the error the bigger the learning rate yeah well okay so wait put two dots right above each other I'm coding along and it's not adjusting let's try that so so you mean if I do this yeah that's not good so let's so hold on let me go let me close this I just want to make sure my example that I made previously didn't do anything so let's give it a very high learning rate I'm just wasting what it's going to do here you know I think we got it close enough okay the the thing about this is I'm less concerned about uh you know there unless here's the thing I'm not looking in this particular example to demonstrate the optimal way to fit line to 2d data I just want to look at the idea of linear regression with a statistical approach and we grade in two cent to set a foundation of knowledge for what I'm going to do in future videos so um oh wait collinear knees dad's regression anyway so there you go should work like wait that's not a that's not a uh yeah so that's yeah exactly thank you okay so where am i where was I in this tutorial I was kind of like checking I think I talked about the learning rate I changed it to 0.5 and I was kind of done ok um okay so some folks in the chat we're asking about like okay well it's sort of performing weirdly if I put a lot of like points above and below but if I put you know points to the right and left it's kind of it fits the line very nicely you can see the Toxic now I'm doing above and below again so here's the thing um collinear T meaning like a lot of vertical points is not really good this this isn't real this data doesn't really make sense for linear regression if I were trying to make predictions so we're not necessarily going to get a good line and part of what I'm doing again is not to demonstrate the optimal way to do linear regression but to demonstrate the technique known as grading a set descent of making small adjustments to weights to parameters to the slope and yintercept based on an error based on the supervised learning process so this is a start to that you could stop here and I highly recommend that you do because what I'm going to do in the next video I don't really know how it's going to go to be honest but I'm going to try to look a little bit more closely as to why this works out the way that it works how do I know how to change em and be how do I know exactly how to change em and B to minimize the error I said kind of well the error kind of gives us the direction in which to change this has to do with calculus it has to do with comparing how changing one variable affects another variable so if I change m how does that change the error and can I look at the slope of a graph perhaps to see how to move along that graph to minimize that error so this is what I'm going to cover a bit more in the next video I'm not going to really I'm gonna actually also change this is I said I think I said this stochastic gradient descent meaning I'm adjusting the weights I'm adjusting the M and B values with every data point but I could also look at the sort of error in totality and then adjust the weight all at once at the end of one cycle through all of the data and that's known as batch gradient descent so I'm going to do I'm going to do is explain a bit more about the math here and then I'm going to do it and change the code to batch gradient descent in the next video it might be many parts to be honest with you but I don't know what how it's gonna go maybe this videos not going to exist the next one you can look see if it's there because I don't know if I should really make it okay see you soon thanks for watching this okay umm yeah alpha writes in the chat I'm just learning how to program so all of these videos are kind of overwhelming I do apologize I'm kind of on this you know I wanted to make a set of videos that go along with a bunch of topics for this course that I taught this semester some of them are not as useful perhaps what is useful or as exciting or just replace all as some of the other stuff that I've done terms of making games or generative algorithms and some of them also do require a depth of knowledge or experience with certain coding topics that are beyond what some of you who are watching are if you're beginners and new to code so I don't know how to best handle this what I'm hoping to do this summer is kind of like take a week off from machine learning and go back to just some simpler more creative projects and then go back to machine learning so we'll see what I do next week but feedback thoughts suggestions all that is quite welcome okay please do something with this sound to be a bit louder so hopefully I don't know so it's personal to say so how was that so let me do a little inquiry here little poll I could do a strop oh really quick I'm just gonna a formal anecdotal poll if you're watching this and had never learned about gradient descent before did that make sense with this helpful or maybe you learned about it before did this add something I'm just I don't know I guess I'm looking for a little feedback here okay so I'm going to think about how much time I have left today those needleless parentheses make me uncomfortable I totally agree these parentheses are awful oh I should get rid of them I'm going to get rid of them now okay thank you uh Mike in the chat who writes it was helpful learn something new helpful okay good Dawid exact could you get this into 3d and teach a bit about tourniquets okay um all right so now what I'm going to attempt to do is talk about let's let's let's make a plan for this everybody so here are the things I need to know what calculus is I need to know what a derivative is I need to know what the chain rule is and what a partial derivative is try making videos on that stuff and then with all those pieces we can actually like sort of derive this formula why not right okay so I think I need to erase this whiteboard this is a terrible idea I should have left it where it was I should just say I'll put links links in the video's description to more detail about the math behind getting those values let's let's try it all right let's try it what's in what's a succinctly to say actually this is a good metaphor a metaphor is not the writer analogy here to look think about something move like a runner or car moving for speed position to think about that as as kind of a foundation for the what a derivative is and what calculus is calculus like I think right couldn't I say that it's the study of how changing one variable affects another variable or is that actually too narrow because that's really only thinking about derivative as opposed to integration but integration just being the inverse there throw three blue one Brown that is a great that is a great that's a great YouTube channel actually I watched some of those videos okay okay so here we go where's my calculus book if the chat of Jim Wright's got a calculus exam next week would very much enjoy your teaching it oh boy this is not going to help I am not qualified to help anybody with their calculus exam I think you don't have to do this video about couch boots could we get three blue one brown for a guest tutorial that would be great somebody maybe put that out into the universe on Twitter or something we can make that happen okay all right I'm going to I'm going to against my better judgment I'm going to I'm going to talk a little bit about calculus this is a terrible idea chapter three look just don't pick don't like me I'm just gonna be reading this book chapter three chapter one wait no no one chapter three chapter two I love this book is so nice hmmm constants and variables fixed value called constants burials okay okay okay all right all right all right all right all right all right all right here we go all right here we go okay here we go everybody hello are you watching this video if you are please turn off your computer but go to something else this is gonna be a problem because I'm gonna do something I have no way qualified to do but I just did send tutorials about linear regression 1 using ordinary leastsquares method which is a statistical approach another using a technique known as gradient descent which I here I've been told behind the scenes uses something called calculus this is a book that I really do love called calculus made easy it's from 1910 this is a reprint of it as a nice foreword by Martin Gardner and I recommend I recommend this book I've been reading it it's been a delight to delight has wonderful little drawings in it and it's it's a completely absurd that I might even attempt to say that oh I'm gonna make a 5 15 20 30 minute video and I'm going to explain calculus or something like that so I'm going to kind of narrow my focus and talk in generalities and then in some specifics and try to give you a sense of the the aspects of calculus which by the way calculus calculate those words are similar doesn't need to be some scary weird thing uh that I couldn't possibly ever do and how it relates as for any of you who are interested in a bit more behind the scenes for this linear regression with gradient descent stuff but really I don't know am I ever gonna use this again in any of my other videos are you gonna use this I don't know maybe it'll be interesting maybe you should go practice piano which is the lovely thing to do if you play the piano I guess if you don't maybe you should take lessons okay so let's think about this so I'm going to start with I here's so I have a lot of videos and I think this is actually a good place to start from that deal with motion and animation so let's think about motion on a compute in a computer window I might draw an ellipse and that lips would have a given X Y value but let's just simplify our world for a moment and think about only I don't have no plan for what I'm do just wait just get your wondering it's look because it kind of sounds like I have a plan I don't have a plan let's think I'd simplify this and think of just as it only has an X value so this circle is going to move each time through a draw loop each frame of animation X is going to change so I could if I wanted to create some sort of graph where the xaxis and now don't get confused here there's an x point there's an XY plane here now I'm creating a graph of the next y this is confusing so let's adjust this a little bit I'm looking for an eraser to make this let's see you can see now that I don't have a plan let's actually just call this um let's pretend this is forget about the computer window that this is a runner I can't draw I'm wearing a hat for some apparent reason and this Runner starts at 0 meters and is going to move along this bottom of this place where this runner is running so now this is going to be a graph where the y axis is distance and the x axis is time this is going to relate to conscious think okay so the first moment in time the runner is at a distance of zero then one second later the runner is at one meter and two seconds later the runner is at 2 meters and three seconds later the runner is at three meters so I could say this is a graph of the runners distance as it relates to time and calculus or at least a calculus and a key aspect of calculus known as the derivative often written as for example do this is bags this is called distance hmm can I make this X let's make this X so we're going to think of graphing X as it relates to time and so this is the X ax is up here I might write DX over DT calculus is the study part of calculus is the study of how one variable changes when another variable changes as time goes forward how does X Change how does X Change according to how time changes now this is relevant because what I did in the previous video and I want to like tie this together if this isn't just a general video as I was looking at how does the error change when I change the parameter of mission one of my enemy machine learning system right so what how does D error change relative to D wait if I change a weight what does that do to the error so if I can learn about how to study how certain variables affect other variables I can apply that to a machine learning system where I want to change weights to minimize the error okay so this graph being a graph of X as it relates to time we could actually we could write this as a function so time sorry I'm kind of taking a timeout for a second I don't I've got this like I don't love that I called this X and this is the yaxis look at the chat see if anybody has any helpful suggestions to me make distance s for displacement that's a good idea okay hold on I think he started wrong not talking about how derivative it could be explained concretely that was the last hope I'm in the wrong frame I'm very I'm looking at the chat waiting for some really a key piece of information that I want too oh it's 4 o'clock already out of keep moving here all right so I'm not seeing anybody I'm not seeing I'm not seeing anybody shouting at me and that have done anything oh yeah so ok so I'm going to get into ok um I need some more water here you got to stay hydrated especially if you're going to attempt to talk about calculus would it be maybe I should look at the Wikipedia page for calculus that might help me keep going it's you that we're here to see okay all right all right so I'm going to keep going now um okay okay so I kinda I don't love what I've done here because I'm you know mixing all my axes and variables so I'm now going to simplify I'm going to change this up again one more time and I'm going to consider the runners distance as the Y value and time as the x value whoa hold on a second time out did I write this correctly hold on if this was X and this was T and let's um let's say that these are 10 20 is this what I'm talking about I'm not talking about DT over yay dy over deep because if I change it then I would have dy over DX ah losing my mind here because what I what I want to get into the next thing I do is I'm gonna talk about acceleration and then I'm going to look at like an exponential curve I lost my train of thought here it's right okay I'm right okay yeah I kind of f of X would be a good way of doing this I'm going to leave this it's fine it's fine that the axes are kind of screwy it's going to be okay okay so actually I want to what I want to do here so hot I want to change the unit's a little bit because I think this will make it a little bit more interesting so let's say that at one second so let me sorry I lost my eraser let's actually change this a little bit so let's I'm going to try to actually get some units of measurement here so 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 so let's say that the runner is running 2 meters per second right DX DT now I 2 meters per second this is a kind of leading to objection leading the witness here but so at 1 second the runner is at 2 meters at 2 seconds the runner is at 4 at 3 seconds the runner is at 6 so this is what the graph is going to look like like it I can't it should it should actually look like it's point my scale is off but you'll see the idea here so what is the formula the formula for this line is x equals 2 times T I can always determine what the position of the runner is according to what time so at 10 seconds the runners at 20 meters at 100 seconds the runner is at 200 meters so in this case the change in X relative to the change in time is 2 and it is that for any single point it is always that value now here's the thing what if the runner is not running at a constant speed what if the velocity changes over time as well so one of the things if you watch my other videos about motion and animation you'll see that ok velocity describes the rate of change for position so velocity is essentially the derivative of position acceleration being the rate of change describes how velocity changes over time and so that's why by the way when you look at a physics engine they'll often be on using some integration technique I might be using something called Euler integration or Verl a verlet verlet integration relay integration that's because what happens in a physics engine is I start with an object's acceleration and I want to then look at the velocity based on that and then look at the position based on that so I'm going in the reverse so integration being the reverse of the derivative but in this in our case for what we're looking to do in terms of figuring out it minimize the error according to different weights that are changing derivative is the key word here so if this is the function and really X is a function of T the change in X relative to the change in time the derivative of this is to always to the rate of change as T changes forever east as T okay not it out nothing wrong one's gonna time I was looking at the chat ok essence of calculus YouTube but I should just forget about all of these I shouldn't be making my own video here so many better videos on YouTube about this I think I gave enough disclaimers at the beginning ok the derivative the X the X changes the camera just went off over there let me fix that okay how X changes relative to time that is okay so now here's the thing this is a very unrealistic scenario nothing is going to move at a perfect constant velocity but so let's just let's come up with a simpler let's come up with another scenario what if the graph of the runners position over time looks something like this so this might look familiar to you this parabola this exponential curve you know I try to approximate a drawing of something like y equals x squared so again I've kind of unfortunately named all my variables in a pretty terrible way here but let's look at this so let's try to understand what does how do I calculate so I'm going to come back over here and say in this case maybe x equals T squared the position of the runner as we're rates to time is the number of seconds squared so in this case the runner is accelerating at at zero so you know it's at zero seconds the runners at zero at one second the runner is at one at two seconds the runner is suddenly at four at three seconds the runner is suddenly at nine so the runner is speeding up as fast as it's running so in this case how do I know how X changes according to time so let's take a look though I don't I don't want to be here I want to run away and I now I gotta I went down this path and I'm thinking about this I gotta just keep going it's fine okay you can edit that out because you're wondering I mean you can't it'll be on the internet forever okay so let's look at let's look at a given moment in time let's say over here so this is I don't remember we're in 0 1 2 3 4 5 6 seconds so it's 6 seconds the runner and I obviously didn't draw this correctly for to this exact graph but that 6 seconds we know the runner is a not trying to scale would be at 36 meters it's a very fast runner I guess 36 meters in 6 seconds it's not realistic I don't think so ok so now we could kind of what we can do is I could say like I could sort of look at this and say well what if I changed time by 2 seconds where would the runner be well the runner would be then at 8 seconds the runner would be at 64 the runner would be at 64 meters if I change time by 2 seconds so in this sense I can have I can I can say that what what happened here I changed by two seconds and did this I'm thinking about this I'm thinking about how do I want to describe this you where's my music when I need it but but maybe I need to hydrate is it really necessary to explain high school math I'm reading this chat this is fascinating I think I just got to speed this along here I actually got the thing is I want I'm going further into this than I intended and so go speed this along here so how much did it change by okay so I could say okay sorry so I could kind of so I could ask the question when I changed by two seconds how much did distance change and you 64 minus 36 is 28 is that right I think it's 28 right so it changed by 28 I know this is this makes sense well the thing is I went way too far out here the point of what I'm doing here is that if I head against the wall this is also I should recommend yeah I get when people are watching this but I though the must have all turned this up this is also a nice book because it has it actually has a kind of explanation that's very similar what I'm talking about here in terms of incline and extent and slope and so actually I should really be talking about this in terms of thinking about going a little bit that way and a little bit that way I got a I got a back up here and we're going to edit this stuff out because and simplify things here okay so let me back up a little bit this is going to be a miracle if this ever gets edited to a video that actually makes sense for anybody okay so if I have this exponential graph where at the value of x is equal to x squared the way that I can understand how X changes according to a according to how time changes is by looking at well let me what if I went a little bit over this way and I went a little bit over this way nice okay well here's a point here and then here's a point here and now I draw this line you can kind of see that this line is describing in a way what you know what the change is right at this point as I move a little bit ahead in time or a little bit behind in time how does how's that distance change well in essence this is called the tangent to the curve right if I were to take these points and make them successively smaller and smaller and smaller and smaller and this is kind of a part of calculus is you know well what if we think about this in terms of like infinitesimally small things I'm going to get a line that is tangent to this curve and the slope of that line describes how the distance changes as you move a little bit ahead in time or a little bit behind in time at that moment and so in this case the way just I can see here like okay well let me actually write this out so the change in X over time is actually two times T for this particular graph right the slope of this line is two then it's 4 then it's 6 then it's 8 then it's 10 that it's 12 all right that's the slope of the line and in fact this is I forgot what this rule is called but in order to if you have any graph if you have any function sorry you can take the derivative that function by looking at the exponent exponent subtract subtracting 1 from it and then taking that exponent and multiplying it so if I have this function you know y equals 4x to the third power the derivative is 4 times 3x to the second power or 12x squared somebody factcheck debug this okay did I get that right power rule Thank You chat that was goes Inc Y o zy and Kane is seeing the chat tells me that this is known as the power rule so this is the first piece of the puzzle we need we need to understand that calculus allows us to look at how a given variable changes according to another variable and if we have a function that describes the relationship between those two variables we can look at that derivative and calculate that derivative in generalities using the power rule that's step one any questions ok I'm going to pause here in the next video I'm going to look at the chain rule and partial derivatives which we're also going to need in the gradient descent problem did I make it okay so oh I'm over here now alright so uh I thought that was there's something else going on in the chat that I'm not following why talk about calculus why talking about calculus magic recess when the topic title is linear regression well because I attempted to do linear regression with gradient descent and I was going to try to derive the formula for adjusting the weights in gradient descent and you kind of need calculus for that but I'm kind of regretting it okay um okay so that seemed to be I'm not seeing any major complaints in the chat I guess I'm going to keep this video I'm going to move on now and like skip over like a universe of information and talk about the chain roll boy do I even know what the chain rule is the chain rule okay okay I know what the chain rule is oh and I need in partial derivatives okay okay Horry says I didn't understand anything oh I'm sorry Horry I'm sorry that makes me very sad okay I'm looking at the chat still okay this is you know one thing that I one thing with my other videos is that I always like typically I if when I'm making a video tutorial I mean coding challenges aside typically what I do is I've kind of like use of material that I've taught in a classroom setting like a bunch of times and what that involves is number one preparing for class I don't really prepare for these videos number two it involves kind of like watching it and then getting feedback and kind of doing it again and then by the time I get up here to make a video I'm sort of more in the frame of mind of knowing what I want to accomplish but anyway this is it is what it is okay so I am now going to there's the chat okay so I'm gonna I'm gonna come back and okay okay welcome back I can't believe that you're if you're here watching this but um so you know I kind of in the last video it wasn't my best work but I discussed a bit about kind of calculus and what a derivative is some people might have helped that might have helped some people it might not have helped other people I will also I think include in these videos links to additional resources that might be better than mine but what so you know I'm skipping over a universe of information here and stuff that we could do and kind of jumping right into some other arbitrary piece but um what I established in the previous video and let me use this eraser here if that I have we have something called the power rule the power rule says that if I have a function f of X equals something like X to the third power plus four x squared plus three X plus two I can get the derivative of this function with the power rule so the power rule says that I take the exponent multiply it and then subtract one to the exponent so I have 3x squared plus 8x plus in this case one subtract one and get zero and then multiply one by three three and then this I get nothing I get zero so I just get rid of the constant so this would be the derivative of this particular function using the power rule power rule there are two so this is part of what we need for the gradient descent algorithm that I'm going to get to again in a couple of videos but there are two other rules that we need and in this video I don't know which order to talk about them in I think in this video I will talk about the chain rule picking the one that I'm much less comfortable with so the chain rule involves in both chain rule and partial derivatives involve systems with multiple variables so in this case this function has a single variable so what if I said to you f of X equals 4x squared but wait wait hold on time out so if I want to do the chain rule what I want to say is something somebody correct me if I'm wrong I want to have another function that depends on this function right hold on a sec there's a nice explanation of it in let me just see what example I'm going to look in in here for a second this is a Tyreke Rasheed's book that has a nice little appendix about calculus I'm pretty sure it had a chain rule example right why oh okay and so if I have one function equals and then if I were to say this doesn't make sense to me oh yeah I see I kind of doing this what if I then said because I could say like this if I said y equals 4x squared and I said like x equals something like Z to the third power plus you know Z or something like that so that oh I'm looking in this I could see you in the edited version okay hold up wait well then we come back uh D of F so hold on I'm gonna let me gently see if I can get I mean see if I can map this out then I'm going to like erase it and and do it as if I know what I'm talking about so what I would say I want now to say how does how does y change relative to Z right this is what I would use the chain rule for and I would say a Y changes relative to Z is a change in Y relative to the change in X times the change in X relative to the change in Z is this correct I think this is the chain rule okay so I'm pretty so many fact check me I'm gonna race this and do it again I think this is the chain rule the question is is my notation kind of awkward here should I have notated this in a different way like should I but I think this is fine okay yes correct okay thank you everybody looks good okay so where's my eraser okay okay I can't remember what I wrote here maybe I did something did I have this to start I can't remember okay maybe I'll just go back so okay so let's say I have a function y equals 4x squared now I know that the change in Y relative to the change in X the derivative of this function is 8x that is the power rule but what if this function actually depends on another function like what if I had a function that says x equals y to the third power plus 2y oh no no not Y sorry ah another variable I don't know what I'm doing here what if X is dependent on another variable right plus so X is dependent on Z X equals Z to the third power plus 2 times Z well according to the cheat according to the power rule the change in X relative to the change in Z is 3 Z squared 3 Z squared plus 2 and this is also the power rule but what if I wanted to ask how does y change relative to Z right Y is dependent on X but X is dependent on Z I could just like plug Z in here and work out all the math I think but I can also use something called the chain rule the change role states that with a function that depends on another function I can I can separate this out into two different parts I can say dy over DZ equals the change in Y relative to x times the change in X relative to the change in Z pause did I get that right is that right is that what I had before is this for ten euros oh god I do have a viewer in the Netherlands who is a sevenyearold boy who's done amazing things I hope that I don't know if this is gonna be at all helpful and it's done some wonderful stuff I've been amazed it's beyond me encoding already I think I get this right any another fact check yes okay and you can and you can see how this could kind of works out because you can almost like cancel those out and you have dy over D Z so if I were just to follow this along I now have 8x times 3z squared plus 2 which would he'll give me 8 X Z squared plus 16 X if I did the math there correctly so this would be the chain rule which would give me the derivative of the dy the derivative of Y relative to Z is that is that the right way to describe it if Y depends on X and X depends on Z so this is the chain rule okay so now we've got what is the derivative with the power rule we've got what if I have one function that depends on another function that this depends on another function I could do a derivative with the chain rule I'm setting up things that we need and there's one more thing that I need to talk about which is what is a partial derivative once I talk about what a partial derivatives then we can go back to the gradient descent math and workout and derive that exact formula ty used in that previous video which seems like a lifetime ago okay next video will be partial derivative yeah I'm getting the comment which is that this is much smoother than your derivative explanation which is interesting because I think I felt this need with the derivative explanation to kind of somehow summarize all that is its calculus in one video and I became overwhelmed by that and I kind of messed it up I'm just seeing if anybody wants to is I'm just reading the chat to make sure there's no like wildly wild what anything that I've done like way off base but so far and it's 430 I don't have a lot of time left so let's see if I can do this partial derivative thing this is what I felt okay so um okay so let me let's erase some of this where's that eraser okay so if Y is a function of X and X is a function of Z I just want to state the chain rule then dy DZ equals dy DX times DX DZ this is the correct way to state the chain rule okay the power rule Harrell states that y equals some constant times X to the N dy/dx Oh a wait yeah I don't I thought it C times n times X to the N minus 1 this is a good way to write it power rule I don't like my notation first of all I think I want to use asterisks since that's sort of like the coding way I need to put one here any corrections to this because I'm going to add partial derivative next partially out of frame thank you why I think you know I'm just going to do is I'm just going to say edge what what if I just say X to the N then I can say n times X to the N minus 1 and that should be in the frame yes how am i doing okay I'm look write f of X not Y and then say DF here is that better I've also seen right this can you write this right which also indicates derivative use a use F okay yeah all right maybe at the chain rule not always using F FX play yeah okay it's better to have two names for the functions yeah maybe yeah so in other words my notation is kind of bad here because it's not consistent yeah because there's like up but I'm taking the chat very seriously if I everybody hasn't completely different uh everybody has a completely different approach I think I think I better be consistent here and you know what I'm gonna do even though I feel like this is a little bit less mathematical notation so I'm gonna stick to this this is it's a little bit awkward what I wrote here right is this really is you can't see me I'm in the wrong frame this is the thing that I'm this is what I'm worried about being most problematic the way that I wrote this here is this something that a mathematician would never write it this way that's my question xeo x equals G of Z that makes sense yeah right a different function that's a good point okay thank you okay here we go oh boy okay thank you everybody okay hi so I don't know by some miracle I I made a video where I attempted to describe something about a derivative I think that one wasn't that great then I need another video which actually felt a little bit better but about the chain rule so I've kind of summarized how the power rule is a way to take a function and compute the derivative by taking the exponent multiplying it subtracting one by the exponent the chain rule is a way to say if Y depends on X and X depends on Z I can look at the relationship between y and z by chaining the two derivatives of this function in this function so those are two pieces that I've done so far the last piece that I need for the gradient descent algorithm is something called a partial derivative now I think this is actually gonna be somewhat easier to do to explain and then you know hopefully it all makes sense we come back and look at the derivation of the linear regression with gradient descent formula but let's say what if you know in in many cases you have a function with multiple variables so I could say I have a function f of x and y and it equals you know 3x squared plus 2xy plus y to the third power plus um you know I don't know 9 x squared Y so this is some crazy function that I wrote I don't know what the use point of it is but what if what I want to do is look at and let's say this is Z Z is a function of X and and what what I want to do it and by the way there's I've been struggling with this you know if I have a function like f of X equals x squared I can another notation that you might see is this f tik X I don't know what the technical term for this is but this is another way of writing the derivative of the function f is 2x so but I could also say look what I've done here with y I could say the derivative of the function is dy over DX and in this case the reason why this is important is because what if I want to look at how Z changes only relative to X or only relative to Y this is what's known as a partial derivative and then notation is written instead of I'm going to just do this over here right this would be kind of the regular way of writing it I don't fold or if it if it's not partial dy over DX a partial derivative is written with D but in a slightly different style like this I don't know where this notation comes from maybe somebody to check and tell me and I can say it in a second or put a link to it but if I want to know how Z changes relative to X only that's a partial derivative or how Z changes relative to Y only ok I'm pausing for a second how am i doing are people people are freaking out in the chat yeah out of frame I got to fix that so um I I don't know I don't know what's going on the checks I'm bill to follow but if you're watching this and you're kind of like oh do I need to know this or this is annoying or I'm confused first let me say sorry I'm trying my best dear this is an experiment to see if it makes sense they even cover this stuff and this is not something again that I have a deep knowledge of and I also I need to get this back into the frame here but but I will say is that you don't actually need to do all of this math yourself if ultimately what you're going to do is use a machine learning library like say tensorflow to train and calculate all the weights of our model and in essence I would say that you know I would probably say that when I frame the whole set of videos I'm going to make these are videos that you could skip but this will give you a background into how the math works which might gives you give you a sense of some of the terminology language ideas that might be in various papers and blog posts and books and things that you might read as you're learning and exploring this material so don't worry everybody calm down keep calm and carry on so to speak but I'm going to keep going with this because I'm just about done okay so first thing that I need to do is first thing I need to do is get this more into frame so here I'm going to come back thank you machiya this is going to be so much more work than usual okay so sorry I kind of drew this out of frame so let me let me draw this a bit smaller so partial derivative of Z relative to X partial derivative in the frame I get lazy of Z relative to why did I keep that in the frame I think I did okay so how do we do this the way that we do this this partial derivative is calculated with the power rule the same exact way that you would do normally and with treating Y simply as a constant so whatever you would do to a constant you would do you doing the exact same way so what do I mean by that so this would now equal this partial derivative would be so this like this this little section I can do just with the regular power rule so 2 times 3 is 6 so 6 X 2 minus 1 is 1 6 X plus now this is tricky so 2 X Y well I want the derivative relative to X another way of writing this is 2 y X now think about this if I had 5 X the derivative of 5x would just be 5 in this case 2 times y is a constant I want to complete as a constant so the derivative of 2y xr2 XY is just 2 y so 6 X plus 2y plus now here's a tricky one y cube do you think oh well it's a constant so it stays is Y cubed but it's not what if I have just a value like I don't have this anymore like 5 the derivative of that would be 0 so this goes away and then this is the same thing this is now the equivalent of 9 times y times x squared so that is 2 times 9 18 times 18 times 18 Y X right because it's right if I just had a constant 9 x squared I would have 18 X so 9y x squared is 18 times y x so this is now the partial derivative relative to X I treat Y as if it's a constant and I have this the derivative now as an exercise I might say stop watching this video pause this video and try to do the partial derivative of Z relative to Y now I have no room here the whiteboard to do it because I've done a terrible job of organizing myself so I will just put the answer to that in the video's description and you can check it because this is the end so now we have the pieces I know that I've skipped a million details this isn't really a proper and this is the proper anything frankly but this isn't a proper calculus lesson I don't even know what I'm talking about at the time in these videos but I'm trying to set the stage for at least the terminology and the pieces of the puzzle so what when I get into the next video I'm going to go back to gradient descent this thing where I calculated the error and then nudged the slope or the yintercept of this formula for a line according to that error well why is it that I did it that way I'm going to show you why using the power rule the chain rule and partial derivatives so hopefully this will give you some background for that I'm gonna do that in the next video you know you don't you could just go do something else like but that's what I'm going to do so let me know your answer to the partial derivative here and then I will see you maybe in the next video bye okay did I by the way so first of all uh is that all this legible it is did I oh its prime not tick so this is not tick this is prime why is it prime so I got to correct that hopefully get flamed we're going to insert this we do a little insertion so the chat is telling me that the proper term is not tick for this but it's F prime sorry about that okay little flip that'll get edited in okay F okay oops this camera went off I probably should leave I really yeah I'm making these videos but you know for me the thing that I'm really interested in is the application of stuff and some creative projects but F yeah dub the word over F prime okay um okay so let me see here I don't think I have time for the next piece doing 15 minutes before I really have to go so the question is could I do this in 15 minutes um let me think about this mmm cuz it really would be nice to finish this off today I don't want to come back to it for sure okay let me try so what I need to do now Oh oh my god so this is horrible I really got myself into a situation this is correct by the way so if this is wrong a big trouble so hopefully somebody tell me this is right or it was wrong somebody would told me so I've gotten myself into a situation where what I need to do now is show the formula for calculating the error based on the weights and then look at how the derivative shows us which way to go to change the odd I really get myself into the situation where this is what I'm teaching this is a good exercise for me I should understand and learn this stuff if I'm going to mostly I'm doing this because I want to understand and try to learn this stuff so that I feel like I have a bit more background in it if I'm just kind of like hacking together some tensorflow stuff but do I have time to do this today should I come back on Tuesday leave this as is maybe come back fresh I think I might need to come back fresh seeing ya f f DZ DX is correct thank you I got lucky I did study this stuff but why did take multivariable calculus that would be right now let's see when would I've taken that character one twenty three twenty four years ago something like that you know then I had wasted a lot of time in New York City with odd jobs and ends that went to AI TPS or learned about programming and then like a week ago I thought let me read about all that multivariable calculus stuff it's a gradient descent again because maybe that's useful okay um all right okay so I think I think I probably I think I shouldn't rush this and try to like do the next thing in ten minutes because it's just going to be a fail I'm hoping that maybe I can actually just come and complete this on Tuesday you know now that it's summer I strangely enough have the flexibility to record much more often and I feel like I need this to be completed cuz then once I'm done with this I can do the perceptron I can do the neural network and I don't have to go back and explain gradient descent I can just use it and if anybody wants you they could go and watch these other videos is it not called multi variable calculus anymore I am 43 I might as well and people ask that a lot in the chat but I might as well if you can find it online pretty easily I think I have a Wikipedia page by the way let's take a look somebody made one is like a class assignment at some other school maybe you guys watching I can't how many people are watching this right now has got to be like 381 people are still watching this that's got to be a mistake you know the camera this room I have like a soft light makes me look much younger ya see the Wikipedia page actually has looking it has my birthday on it but I don't know Internet there's three today you want people watching you can add something brush it off this page for me that would be nice it could be like a picture on it something like inside coding train humor somehow that's an easter egg see if you can do that I do act acting like I'm 25 is very generous thank you guma in the chat okay all right so um thank you all for watching this today so let's recap what do I have so far that I've done new this week really just linear regression linear regression linear regression so I did a Tommy how many edited videos are going to come out soon what is linear regression linear regression for ordinary leastsquares linear regression with gradient descent I know what I could do in ten minutes I know what I could do in ten minutes okay people are really talking about my age I shouldn't said anything too much maybe I maybe I'm actually wrong about my age it's a state of mind man yeah uh maybe what I should do is do I dare dare no no I'm going to come back I'm going to do this stuff next I'm going to answer some questions I also wanted to use this library which will do the regression for you and I want to go through this library and show you how this works I will do this yeah so the chat K week one is asking your Wikipedia page says you studied philosophy has that about had been a help in your further studies well interestingly enough I did study philosophy and the focus first I didn't really pay attention it didn't really make good use of my undergraduate education I would say I regret I take it's a wonderful luxury to and wonderful thing education so anyway but that aside the thing that I focused on was a logic paradoxes and so I think that a lot of this are boolean logic set theoretical paradox kind of like stuff that I was studying this is a long time ago unbeknownst to me at the time became incredibly useful as a way of thinking as applied to coding so coding I think is a learning to program learning to think through a problem in a logical way I think definitely relates to a lot of the stuff that I learned about and studied in terms of philosophy but I never really made that connection okay I am watching this Sasha I'm watching this even after two whiskies and three beers I should go maybe a beers in the cards for me this evening to celebrate the end of a week the weekend is coming up two whiskies and three beers you would have to hospitalize me I think that happened but yeah okay can I teach csharp can I teach Arduino IDE I'm not an expert or which by the way clearly is not stopping for me to teach anything not knowing it is kind of the is the criteria for me attempting to teach it apparently but csharp Arduino are not things that I'm looking at doing anytime soon but I do want to get back to having more guests tutorials so I would like to have more physical computing related guest tutorials tutorials maybe using things like unity which I know csharp is a part of so those things are and I think I need to it's on my todo list but to schedule is it June yet I was kind of hoping that June would be the month of guest tutorials so I need to start scheduling people to come in and do guest tutorials i pronounced alright looking at these questions I'm going to play my goodbye way so I'm gonna be off this has been a two hour and five minute livestream I've never tried aerial ask have you ever tried the Elm foreign language I have not I didn't know what that is cigar says smiley emoji smile you smile you smell oh I know I know this chat I have the chat on this monitor on the side and like a really big font so I can read it from faraway Scrolls by this like most almost all other questions how about a Python I do want to do actually I kind of I kind of want to do some intro to Python tutorials because I don't really know Python i hack away at it sometimes when I need to get a Python script working and I thought I would try to do once I get I have this idea that once I get to in my list here week five CNN tensorflow so I'm gonna do all the neural network stuff without Python and then I'm going to use tensorflow and I'm gonna use tensorflow with Python I was thinking of like a series of like teach myself to program in Python so I might just like a few basic Python tutorials I would love to do shaders I'd love to get a guest do shaders I didn't use multiple colors that's why today didn't go well H I I do have HTML job Jerrica boasts 14 asks no juice says delay test responded this ASAP that's specially respond to it do it HTML tutorial I have a whole playlist of kind of like what is HTML and some basic to Charles about HTML in the context of using the p5 chess Dom member yes I can juggle could also play piano a little bit and I was thinking maybe I should write a few like parody computer science programming songs and come in and play a few for you I've been learning some Nick Cave Nick Cave is playing tonight at the King's Theatre in Brooklyn I'm not going but I bought some tickets as a gift for somebody else I'm excited for that Nick Cave is also playing at the Beacon Theatre on your boys side in the K stand unlike concerts too loud too proud it ok um so thanks everybody for tuning in I I don't know if I hope this isn't too much of a digression kind of going off on this linear regression stuff and and can a 13 year old learn coding of course a 13 year old can learn coding a 10 year old could learn coding can learn coding my girl can learn coding concepts and talking my baby I do I do think that for children experiencing the physical world experiencing learning to how to be part of a community and be with people these are sort of like the primary things to learn an experience as in the development of a child this is a nonscientific this is you know but so I do question in terms of all this I've learned to code and you know code with Elsa and on encode with Star Wars and Lee or four years old and play in your code on your iPad app you know I don't have anything necessarily against that so to speak but I do think that well I one thing I'm excited about sort of offline collaborative activities that teach kids ways of thinking of code is sort of a kind of thing that interested but certainly I do think for a thirteenyearold at that age you know using something like scratch even something like p5.js and JavaScript these are platforms that absolutely can be productive for programming ah mica Erickson in the chat and I have the same birthday wonderful okay I'm out in about 20 seconds I'm sorry that this is I know people are really excited about the tensorflow stuff and the neural network stuff I am gonna get to it I wish I was getting to it faster maybe it was a mistake to have all this lead up to it but it's what I'm attempting and trying to do so that's what it is okay um by the way I didn't really even start coding Hilario for 28 or something so I don't remember the exact age but I have to look it up okay thanks everybody um if you're interested in supporting the work that I do you are welcome to join the slack community patreon.com slash coding train you can also buy some merchandise that Cody train store Envy com somebody asked if they could donate with Bitcoin and I'm looking into thing if I could make that possible for people and I have a coin base account but I don't have a merchant account I couldn't really figure out how to do it wants to help me with that please let me know what else do I want to say like subscribe at Schiffman on twitter feedback constructive criticism all of that is welcome I hope you're enjoying everything and I don't know for sure I'm definitely coming back next week on Thursday or Friday I'm not free on Monday and I'm not free on Wednesday if I have time to just come back for a short bit I really want to get through this linear finish off this regression stuff if I can do that on Tuesday I will so just stay tuned at Schiffman on twitter subscribe here's one thing you can do if you want an alert whenever I'm live if you subscribe on YouTube and then click this alarm bell it will you'll get an alert so I suggest doing that and sometimes yeah so that's that's about it okay thanks everybody have a wonderful weekend be safe be with your family and love and friends and loved ones give somebody a hug see you in the next train I'm going to take the train where's my eye it's time I don't know if you know this but I actually have some toy trains in this video so here we go goodbye I'm leaving do the awkward thing is the button for me to stop the YouTube stream is over here so I have to come back by the way look at this well what is this strange invisible laptop look at this it is a strange magical laptop that is invisible it's actually just a laptop with green paper this is where I cheat this is where I have all my secret code on it now I just have the slack chat on here but a lot of people write I could see you looking to the side you're steep you're just typing your code from the other thing which first of all how do they go be cheating that would be just being prepared and going through a lesson that I previously prepared that would be a good thing okay I don't know people want some green screen stuff that they can do some weird things with no I don't know what I'm doing your work I got to go I got to go see you guys have a wonderful weekend and I will see you in the future