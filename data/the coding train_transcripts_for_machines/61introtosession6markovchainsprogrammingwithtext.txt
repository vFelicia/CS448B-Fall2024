hello welcome to session six or week six I don't know you doing this every day every week every month this could be year six for you if you wanted it to be but this is programming from A to Z it's a set of tutorials a kind of online course you could follow all about programming and algorithms with text text language text words letters all that sort of stuff so today in this week's session the focus will be about the focus that I want to have is Markov chains well what's a Markov chain what smart what's the deal what's going on so in the last session I focused on something the sort of topic of text analysis so in last week the idea was really exclusively about reading text in and analyze it analyzing it counting how many times different words appear trying to think about how you might do sentiment analysis what happens when a computer program reads in text I want to turn now towards what happens when a computer program writes its own text and there of course are many many ways that you could do this in next week's session I'm gonna look at something called a contextfree grammar in other sessions I hope to look at some machine learning techniques for generating text as well as other just kind of creative ideas for ways to mix and match and have a program put together text as if it's writing it now one thing I should say about this week's topic of Markov chains is it by definition requires a source text from which to generate text so this is something you'll see in a lot of these algorithms for generating text they also include a reading text component so now a Markov chain is not something exclusive to the idea of text and in fact a Markov chain really just describes a sequence or a chain of states like I am happy I am sad I am running and I'm like typically on any given day be sad and then I start running and then I feel happy and that's kind of my sequence so what with a Markov chain looking at how certain states are sequence and the probability of a given state following another state we can use we can evaluate sort of existing data right I can look at like you know what's the weather like today what's the weather like tomorrow it's a leatherlike the next day over a year and try to use that to either predict the new weather or to recreate a simulation of weather based on the sequence of states so this is something I'll look at more in more detail in the next video but there's a piece of that which is if I want to apply this idea of a Markov chain to text what I want is for the characters are words of a piece of text to be States for example the state is I the next state is M the next state is feeling the next is like dancing so those are states and the you know whenever I say m i usually say feeling and then whenever you say feeling I usually say like you know but I might say the other day I I'm feeling like eating some kale salad or something like that too so this is this is like quite possibly the worst explanation of our cupcakes ever the Internet but you skip to the next video where I'm sure it will make a lot of sense but a piece of evaluating this fiscal properties of characters and words how they appear next to each other is this idea of an Engram so this will also be a piece of the example that I built today of how do we look at a body of text and look at this idea of engrams now Google has this massive treasure trove repository of text corpuses of text from you know 1800 all the way up until present day and you can search for the frequency of certain engrams so these are what it might be called by grams meaning to computer science creative code creative writing we could think of other ones like I am and I could search for any of these diagrams and their frequency in text and we can see that computer you can see first of all computer science started to appear more and more frequently and starting the 1960s you can see a sort of more consistent amount of i.m you can see creative writing creative code down here so this is a way of looking at how and we can look at trigrams engrams with an order write an order of for the order referring to the number and you can do creative projects with so here's a great project by Chris Harrison called web trigrams visualizing Google's trigram data and we can see here if we just look at this PDF and I'm going to zoom into it so you can see you can see here that that in this project the Chris is visualizing all the words that tend to follow he and then from there all the words that tend to follow that so another I think this is these are some nice examples to look at and you can kind of hear imagine now I have a I am not I was not I do not I can a so you can sort of see the frequencies of these sequences and if you can evaluate those frequencies you can use those frequencies as probabilities from which to generate new text so here's an example of a project made by Alison parish the interactive telecommunications program is a program at ITP it's called is a program at IUP it is it's where I teach and we have courses every spring and what Allison did is read all of the courses into a program look at all the statistical properties of all the characters and words and how they appear next to each other and use that to generate new new courses so I'm gonna let's let's find one that that looks good the Anthropology is a virtual design workshop MIDI and cinematic objects this course constraints of weekly sessions beyond exercises and inspire the possible so this sounds great I think I'll take that course this is another example of a project called King James programming and these are post generated by a Markov chain that has mixed several different input texts the King James Bible structure and interpretation of computer programs and some of Eric s Raymond writings run by Michael Walker so you can see here these are different things that are generated from that exercise 3.67 addresses why we want a local variable rather than a simple map as in the days of Herod the king okay so there's a lot of possibilities for how you might use Markov chains to for a creative output and on the one hand this is nothing new this has been done and done and done and done again this idea of reading in a source text evaluate the probabilities on a character or word based level I'll talk about that as I implement the code in the next video and then text generating out of that you could make a Twitter bot that generates text based on a Markov chain so I I think there's value and hopefully you might enjoy exploring the idea and you might even just take the examples that I provide and find your own source text in but I think for you to think about what is the reason why you might do this where might it fit into an existing project hero walkietalkie and and come up with some creative possibilities so in the next video I'm going to focus on the code how to write the code to implement a Markov generator I'm gonna go through it entirely from scratch and then I'm gonna show you a few additional examples and then come back with some exercise ideas for things that you might want to try doing yourself after watching these videos and then I hope you'll share them with me on Twitter at Schiffman or you can subscribe to the patreon too to post the post your work in slack as well okay so I will see you guys in the next video