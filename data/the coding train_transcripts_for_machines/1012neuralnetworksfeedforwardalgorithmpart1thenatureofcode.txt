alright this is a momentous occasion I got a haircut from the last video if you might have noticed that I wasn't gonna mention it but I mentioned it and what I'm actually going to do now is describe we've finished off I mean it's not finished we're have to do more building this little matrix library that's gonna allow us to do some math stuff that we're gonna need when we implement the code for this particular video where I'm gonna describe the feedforward algorithm of neural network now I want to give thanks to two sources that I've used primarily in the sort of studying and preparation for this video number one is make your own neural network by tariq rashid you'll find a link to this a book at the coding train Amazon shop in this video's description and also I want to thank and reference the three blue one brown channel which has a playlist I forgot what it's called but there's a video called what is a neural network if there's at the time of this recording there's about four videos those are amazing they're animated they're thoughtful they're careful but they're really for understanding again and having an intuition for how a neural network works and for seeing all the pieces of an algorithm I highly recommend those what I'm really attempting to do in my videos is sort of figure out a way to implement a lot of the stuff that's in this book and those videos in code so in this video in the next video I'm gonna start working on the code this video I'm really gonna talk through the algorithm in my own words as it applies to where I am in this playlist I've been way too much time introducing this video let's just get right to it ok so where I last left off before I started working on matrix math stuff I had made built this simple example of a perceptron and a perceptron the idea of a perceptron as a single neuron that receives inputs so we might have inputs something like X 1 and X 2 and those two values go into this perceptron they are they are processed and then some output is generated often referred to as Y so this is where this is the thing that I built last I talked about some of this in a previous video but I need to kind of just rehash it again to set the stage of where we are so let's say we're trying to solve a simple problem and like understanding logical and so the inputs that into this system this learning system this simple perceptron can be a true or false or true or false or we can also think of that as zero or one right and we want this perceptron system this system to output a 1 for y only if both the inputs are true so we could say if the if both the inputs are true then we want to get a 1 you know if if the inputs are true and false we should get a 0 if they're false and true we should also get a 0 if they're false and false we should also go to 0 so this is the kind of system now where I the thing that I talked about so this is great exciting Wow we have a neuron it can receive inputs it can boom it could give me outputs all is right in the world the problem is most problems in life are more complex than this very simple scenario of two inputs each only a boolean possibility and and an output solving for a logical and so for example let's just make this slightly more complicated let's take this idea of something called X or exclusive or so if I were to switch this to or this would now be the desired outputs for or right one thing is true that's what we've got hmm exclusive or means we will only output true if one thing is true and one thing is false so this for exclusive or will actually output a 0 and if you I talked about this more in a previous video this is not a linearly separable problem we can't graph the solution space and draw a line right in the middle and say all the answers of all the answers for true on one side all the answers are false are on the other so this is where this idea of a multilayered perceptron comes in most problems as I get further and further through this playlist of doing more and more things cannot be solved with a single neuron this idea of a perceptron this is where what happens to solve a problem like XOR if we add a second neuron and send those inputs the same inputs into that neuron and then set the output of that neuron into the output here now we have what's called a multilayered perceptron we have this output layer this you could consider an input layer although the input functions differently than these other two this is a layer this is a layer and here's the thing this is called a hidden layer the reason why this so I'm gonna say hidden is right here this is input right here and this is output now as I get further and further through many videos that I hope to make and examples I hope to make we're gonna see that there are many complex architectures for how you might diagram and design a neural network system this is a very very basic beginning point and typically this is a two layer Network a hidden layer and output layer input isn't really considered a layer although I sometimes do it so I close the three layer network or we could get to something like long we could have a comment thread about which is precisely correct for this so we'd start that common thread then I'll learn but but let's just call this rut two layer Network the reason why incidentally this is called a hidden layer it's because we as the operators of the neural network right we are going to give the neural network data we're gonna say hey take true and true or take true and false so we're really here interacting with the input we also want to see the output we want to take the output and use it in our project oh you gave me false thank you what's the answer hidden are the pieces that exist in between where the inputs come in the output comes out and this is kind of the magic as we'll see as I get further and further along and solving different kinds of problems how this works now the main purpose of this video even though I'm being very longwinded about it is to talk about the feedforward algorithm what happens to the data as it comes in passes through and exits through the output and in order to describe the math I what I want to do is add one additional input X 3 so I'm going to add that and that will also go into this hidden neuron and this hidden rod so now you can see we have three inputs two hidden nodes and one output node and all of this is totally flexible you in a if you've ever looked at that classic you know for hello world machine learning problem where you have this data set of handwritten images often the inputs are 784 for 784 pixels and the outputs there are 10 outputs because you have a probability for whether it's a 0 1 2 3 4 5 6 7 8 or 9 so the design of this how many inputs on the outputs how many hidden nodes how many hidden layers this is all food for thought but I just want to look at this very basic where is it 2 ok ok ok it's a deal to layer Network deep breathing deep breathing it's gonna be fine whether I say 2 or 3 life will go on now you might be wondering which weren't you just making all these videos about like matrix math how is that relevant here well it turns out that the math for what each one of these nodes do is something called a weighted sum what do I mean by that these are all connections each input connects each input connects to each hidden node each hidden node could connects to each output node these connections all have a weight and in fact if we call this like h1 and h2 right this is hidden one hidden we can have weights between one and one and one and two and two and one and two and two and three and one and three and two so those weights right if there are three inputs and two hiddens there's actually a weight matrix there are six weights there are six possible connections and I could write that like this so let me write out the weights in a matrix so if I have a matrix that rohroh column I can have Row 1 column 1 Row 1 column 2 Row 1 column 3 Row 2 Row 2 column 1 Row 2 column 2 Row 2 column 3 right so I have Row 1 Row 2 columns 1 2 3 so I so this is 1 this is a notation for writing all of these weights now let me now label the weights in this diagram and I'm going to do it in a particular way I'm going to say that this is a connection between hidden 1 and input 1 this year is a connection between hidden 1 and input 2 this is a connection we'd hidden 1 input 3 and here I have a connection between hidden 2 and input 1 hit into an input 2 and hit into an input 3 you might want to pause this video just take a look at this and take a look at this and see if it makes sense for you the point of what I'm saying is what what should be intuitive or sort of like feel somewhat normal is like okay there's all these connections those connections one on one one to one it oh sorry one on one two and one there's connections but we're hidden one is connected to one two and three and hidden two is connected to one two and three right and so here are all the weights from inputs one two and three that are connected to hidden one one two three here all the weights that are connected to hidden two from inputs one two and three so so why am i spending all this time driving myself crazy and incidentally driving you crazy spending all this time trying to label these and do this matrix the reason is the value the math that I want to process here in the hidden node is the weighted sum of each input multiplied by the weight added together so let me get myself a little bit more space here on the whiteboard what I want what I know I would I want to end up with is this weighted sum so let's look at that weighted sum first I want to take this wait wait 1 1 and multiply it with this input wait 1 1 times X 1 plus what now this wait 1 2 and this input plus this wait 1 2 times X 2 then I want to take this wait 1 3 and x 2 x 3 1 3 times X 3 you can see how this looks nice and neat the weight one one goes with X 1 wait 1 2 goes with X 2 wait 1 3 goes with X 3 and we could do that same weighted sum for all the inputs and their weighted connections to H 2 2 hidden and that's going to be weight 1 times x1 plus weight 2 times x2 plus weight 3 times x3 this is the result that we want and I could have just basically done all of this without thinking about all this notation and matrices with a for loop because I could just say like I have an array of inputs I have array of hidden and then I have an array of connections and then I'm gonna do some 4 loops to multiply all the inputs but the reason why I'm spending all of this time doing this is it look at something really interesting this might look familiar to you if you've been watching the previous 3 or 4 or 5 videos about matrices what is this really this is the matrix product of this weight matrix and a column of inputs if I put this right here X 1 X 2 X 3 and I write a big X therefore a matrix product or I could put a dot there may be a dot would be a better notation for matrix product we can see here that remember what that matrix product is it's this row multiplied is this the dot product of this row with this column 1 1 times X 1 1 2 times X 2 1 3 times X 3 all added together and then the next is this row times this come so it just works out perfectly this is why matrix math is so relevant and is used in all of neural network deep learning implementations because the way that the feedforward algorithm works and mind you I've there's a lot more to the feedforward Algrim that I need to talk about I'm talking about the bias yet or the activation yet there's more but I just want to get now to this primary understanding of the inputs come in we take a weighted sum of all the connections between the inputs and that next layer and that can be done in a single operation if in our code the way we implement it is we store all the weights in a matrix and all the inputs in a matrix the weights are going to be the inputs are always going to be in a single column I'm actually that could change the design of your neural network but in this case that's how that's gonna work I'm being told from the live chat that's going on that the dot really needs something else and a cross product is something else so we're just going to put the our symbol for matrix product will be like a nice little you know steam engine train thing okay that aside what else do I need to do so we've established this beginning part of the feedforward algorithm the inputs come in the weighted sums get added all together inside these hidden notes now there are two big components that I've missed let me just write this over here one is bias and two is an activation function I kind of not sure which order to talk about these things before I get to the bias and the activation what I want to do is is rewrite this as a smaller formula so I want to consider the weight matrix for example just as the capital letter W and I can think of it as a matrix of I rows and J columns I ain't a are kind of terrible cuz they look so similar but the weight matrix I I rose and J columns now we're taking the dot the matrix product which I'm just gonna use a dot here I think that's going to be fine the matrix product pre miss and the inputs now the inputs is a matrix but it's one column but I rose and the point of the reason why I'm doing this is because I'm trying to get the outputs what I want is I want to know what number to send out of here the data is flowing in we get this weighted sum what number flows out of there and the number that flows out of there I'm going to call the hidden it's really I'm trying to get the output of H h1 h2 so the outputs of the hidden layer just the inputs are the inputs of the inputs of the hidden layer the outputs of the hidden layer are the inputs to the island so a lot of inputs and output there but the previous layer sends in the input so whatever comes out of the hidden goes in here into the output so hidden I equals the matrix product between the weight matrix and the inputs but there's more so one of the things that I'm forgetting I can actually fold these two things in right in here the things that I'm forgetting are one the bias in the previous videos where I went through the perceptron for example remember I was trying to like find this line and what points would be above it or below it and I've got to really deal with the problem that all the inputs could be 0 if all the inputs are 0 then the weighted sum is always going to be 0 that can't be right so we need this bias we sometimes need to make it easier or harder for it to so to speak fire like we want to make we want to like bias the output in a given direction so one thing I would write here is I would also say plus that bias so and that is a single column vector as well so really if I'm down here again and boy am i running out of space but that's ok I would do this matrix product and then for every single one of these hidden I would have a bias B 1 and B 2 right so there are weights and by it and by weight it's perfectly legitimate to ask the question right now huh like well what are the values of the weights what are the values of the bias these this is what I'm going to get to like I just want to understand the algorithm the whole point of this sort of learning system that we're going to create is to figure out how to tune all the values of all these weights and biases so that the outputs match up with what we think they should be the assistant needs to somehow adapt and learn and tune all those values to perform some sort of task and in a sense this is really just one big function that's why our own Network something called a universal function approximator it's just a function that receives inputs and generates an output that's a function and we have to in theory like if we'd have enough of these hidden layers and nodes there's no inputs we couldn't match with some given set of outputs so anyway where'd he get to all of that but I'm back here so I need to add the bias inand then there's something else you might remember from the simple perceptron example that we had this activation function whatever this weighted sum plus the bias would be if it was a positive number we would turn that number into plus 1 if it was a negative number we would turn that number into negative 1 and this is something that's very typical of neural network based systems whatever these weighted sums come in as we wanted to like squash them into some known range and there are a variety of different mathematical functions that can do this and while this is not typically the sort of latest and greatest and most cuttingedge activation function the function that is we will find in a lot of textbooks and early implementations of neural networks is something called a sigmoid and sigmoid is a function that actually looks like this f of X equals 1 divided by 1 plus e to the negative X percent sure I got that right let's go look at the Wikipedia page for the sigmoid function ok so here we can see I did get the correct formula for the sigmoid function what is this number e E is called the natural number it's the base for the natural logarithm it's like 2.71 somethingorother so for one of these magic numbers or Euler's number you can read up about it but there and to be honest the sigmoid function is barely used anymore in sort of modern deep learning research but I think it's a good starting point for us to look at and understand why and in other videos I will take a look at other activation functions but the reason why the sigmoid function is used is because it takes any input any number you pass it through the sigmoid function you will get a number between 0 and 1 and so this is perfect for a lot of scenarios it takes this input it squashes it so higher numbers are going to be much closer to one lower numbers are going to much a little closer to zero and the bias is going to can push things closer to one or closer to zero this squashing of it works really well because you can kind of you know we could get us or a true or false zero or one we can get a probability value between zero and one we get to the output lots of possibilities so this is the sig and it's sigmoid function so oh so okay so I'm over here by the way I need to correct a couple things where's that eraser number one is I kind of did something horrible here because I made these equations not equal to each other anymore when I was talking about the bias so let me not put that over here so okay another thing I want to mention is I'm kind of fumbling here with the these index values you were really in what we're really doing is we're the matrix W is rows and columns so I and J the input is a single column vector but we're still iterating over I because the I of the weights gets multiplied with the each row which is now I of the of the input so you know the exact notation aside the point of the feed for one of the the truth of the feedforward algorithm is the inputs come in you take a weighted sum you add in the bias you take that weighted sum add the bias pass it through the activation function and that result feeds forward towards the network and go straight to the output and guess what the output does the output takes whoops the weights of all the connections between the hidden and the output so the matrix product of the hiddens with those weights plus its own biases these are different biases these are the biases for this particularly the output node and then passes that through the sigmoid function as well so this just happens every sing with every single layer here come the inputs weighted sum passes the activation function send that as output into the weighted sum of the next node pass of the activation function send output etc etc so I was getting the question and was being point out that I didn't draw the bias anywhere into this anywhere into this particular diagram so a nice way to draw the bias into the diagram is to think of it as another input at each layer so for example if there were always an input that had a value of 1 that connected like this now we have bias 1 and bias 2 so these are just these are like weights that are getting weighted with an arbitrary input of 1 that's always coming in and the same thing with here now we have also another bias that's coming in to the output so it each layer there is a sorry a 1 which which it's weight is like this bias there's only one so B 1 so you can think you could you'll often see I could probably pull up a much nicer diagram again I highly recommend you go watch that sure just been watching the three blue one Brown video all along if I'm being perfectly honest but this hopefully gives you a sense of what the pieces are the point of me doing this in a sense you could just skip ahead to the next video in the next video I'm going to start to have a matrix library which does this matrix math I can add two matrices together element wise I can perform a matrix product I can apply a function like the sigmoid function to every element of a matrix so I can do all of this I can start to write the code for a neural network library okay we have now arrived at the what is sort of the end of this video I'm going to pause and check and see what all the wonderful nice people who are generous enough to all who is along live have corrected me and see if I need to come back and offer any Corrections or answer any questions I'll be right back alright so there's definitely a few things wrong or misleading at least I would say about this that I've gotten some good comments from so that's all right number one is it's very important to realize that these two weight matrices and these two by C's are not the same I wrote them out I mean we have the hidden outputs and the outputs output that's what this is that's what this is so the hiddens outputs are actually the weight matrix between the inputs and the hidden so I could write that as like superscript up here WI H in a way this is the the hidden tout puts is the weight B weight matrix between the input and hidden with a matrix product of those inputs and this weight matrix is the weight matrix between the hidden and the output H 0 or whoo so and you'll actually see as I get into the next video I'll start using this kind of AI H or H Oh kind of naming in some of the variable stuff the other thing is these are not the same biases this is the bias for that's connected with each hidden neuron and so I could say B H there and then this is the bias right that's connected with it's just one one bias but it's connected with this output neuron and so I could put an oh here so that would be more clear about that another point of clarification is there is a way to write this without having this having the bias as part of the weight matrix itself because there's no reason why I couldn't just consider the bias like I said as an extra input that always comes in with a value of 1 and then what I would need to do where's my eraser is I would need to add a fourth column here right because this weight matrix would have all of these connections 1/3 and 2/3 and then there would be the bias values bias one and bias two so if you see this this would actually end up now I could just add it over here if I follow through with this math plus b1 plus b2 because it would be multiplied by 1 which would just be that bias value so that's an important thing to thing how do i possibly finish this video some people we're asking what and I why don't I write out the matrix formula for for this ah you know what maybe I should make this an exercise for you and I can maybe include a little link to a JPEG or something that has it but could you do this same exact diagram for the calculations that flow once the outputs come out of here through this layer so let me just and just give me a second I'm gonna check see if there's any last comments or questions or important Corrections I'll be right back okay that in fact was the end thank you for watching this video I do want to mention that I I'm in the chat or a lot of really useful and important points and comments about how my notation and superscript versus subscript and I or J this is not perhaps the most conventional or correct notation so I apologize for that feel free to leave comments in the comments section particularly good ones if there's a really like one that sums it up all perfectly I'll pin it right to the top but the point of this was for me to kind of like get through the basics of this I am now going to in the next video actually implement this in code and once I've done that we'll be ready to then look at the learning algorithm the training algorithm this thing called back propagation implement vadym code then the neural network will be complete we can actually use it to solve something I hope so see you in a future video thank you acting