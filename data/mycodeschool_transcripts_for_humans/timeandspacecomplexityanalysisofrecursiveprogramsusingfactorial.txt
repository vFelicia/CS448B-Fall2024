With timestamps:

00:00 - In this lesson we will see how to analyze
the complexity of recursive programs and we
00:06 - will do so by picking up a very simple example
of factorial of a positive integer.
00:12 - Now when we talk about complexity analysis
of programs we basically mean two things.
00:18 - First is time complexity, which basically
is a measurement of how the time taken by
00:24 - a program grows with the input.
00:27 - And another thing is space complexity which
is the measurement of how the space or memory
00:32 - consumed by the program grows with input.
00:36 - So let's pick up a simple example of recursive
implementation of factorial of a positive
00:41 - integer and try to analyze it.
00:44 - So here in the left, I've written a recursive
implementation to calculate the factorial
00:48 - of n.
00:49 - If n is 0, we simply return 1.
00:51 - Else we make a recursive call to calculate
the factorial of n-1 multiply it with n and
00:57 - return the output.
00:58 - Now let's say time taken to calculate factorial
of n is equal to T(n) When we try to analyze
01:04 - time complexity of a program we make an assumption
that each simple operation costs us one unit
01:11 - of time.
01:12 - So, if we see for n greater than 0 we first
compare it with 0 which is one simple operation.
01:19 - So this has got cost of 1 unit.Then we have
a multiplication operation here that we perform
01:25 - after we get a return from factorial of n-1.
01:29 - So this is one unit of time.
01:31 - And we perform a subtraction to calculate
n minus 1, which is another unit of cost.
01:37 - So T(n) is nothing but the time taken to calculate
the factorial of n minus 1, which is T(n-1)
01:43 - plus 3 units of cost for the simple operations.
01:48 - And this is true for all n greater than 0.
01:51 - For n equal to 0, T(0) is equal to 1 because
we only make a comparison here and simply
01:58 - return.
01:59 - Now, we have T(n) equal to T(n minus 1) plus
3.
02:04 - Let's try to reduce this expression T(n) in
terms of our known value T(0).
02:10 - Now T(n minus 1) can be written as T(n-2)
plus 3, so overall the expression will be
02:16 - T(n minus 2)plus 6 and this can be further
reduced to T(n-3) plus 9 because T(n-2) is
02:24 - nothing but T(n-3) + 3.
02:26 - So if I have to reduce this by a generic k,
then this is equal to T(n-k) plus 3 into k
02:34 - or 3k.
02:36 - Now we want to express this in terms of T(0).
02:39 - So in that case n minus k will be equal to
0 or k will be equal to n.
02:46 - So this expression would finally reduce as
T(n) = T(0)plus 3n which is nothing but 3n
02:54 - plus 1 because T(0) is 1 So we can see here
that the time taken by this particular program
03:04 - for an input n is directly proportional to
n and we can also say that this is big-O(n)
03:10 - in terms of time complexity.
03:12 - Or this is an order of n algorithm.
03:16 - Now there are a couple of different methods
to calculate time complexity of a recursive
03:21 - program.
03:22 - This is one of the methods in which we try
to reduce T(n) in terms of its base conditions.
03:28 - Now let us try to analyze space complexity
of this program.
03:32 - And we will again this time go by an example
Let's say we want to calculate factorial of
03:40 - 5 so we make a call to Factorial(5).
03:42 - I will write F(5) here as a short cut for
Factorial(5).
03:46 - ow F(5) goes on to calculate F(4) recursively.
03:53 - What happens at this stage is that the computer
says "Hey, I will save the state of this particular
03:59 - function call F(5) which means saving all
its local variables and its current state
04:04 - of execution in the memory and go ahead and
calculate F(4) first.
04:09 - And once I'm done with F(4) I'll come back
to F(5)" Let's say this is computer's memory
04:14 - on the right and these partitions are some
unit in which the memory is divided.
04:21 - So computer saves the state of F(5) in the
memory and goes ahead to calculate F(4)
04:29 - Now F(4) again makes a call to F(3).
04:32 - So F(4) again is saved in the memory.
04:35 - F(3) again makes call to F(2).
04:37 - F(2) again makes a call to F(1).
04:41 - So all these states of the functions are getting
stacked in the memory.
04:46 - And this goes on until we make a call to F(0).
04:51 - And F(0) does not make any further recursive
call.
04:54 - So we can see here that in our actual implementation,
even though we are not declaring any variable
04:59 - or assigning any memory explicitly, all these
states of these functions are getting saved,
05:06 - or stacked in the memory and behind the scenes,
our program is consuming all this space.
05:12 - We often say that an implicit stack is growing
in the memory when recursion is executing.
05:20 - And what is the maximum size of this stack?
05:22 - If this structure, which is called the recursion
tree has a maximum depth of n.
05:32 - Maximum depth of the tree can be defined as
the longest path in the tree.
05:38 - So if this is level 0 and this is level 1
and this is level 2, 3, 4 and 5 and each arrow
05:45 - here is one unit of length in the path then
the maximum path/maximum depth is from L0
05:52 - to L5 and it is equal to 5 units.
05:55 - And in this case the maximum depth is equal
to 5.
06:01 - And if you draw this recursion tree for any
generic n, then the max depth is equal to
06:06 - n.
06:07 - So in this particular case, the space required
by the program is directly proportional to
06:13 - the input n.
06:15 - So we say that this is an order of n algorithm
in terms of space complexity.
06:21 - So now the stack grows to a maximum of n units
in this particular example.
06:26 - So when we are calculating F(5), it grows
to a maximum of 5 units.
06:31 - And as soon as we reach F(0), F(0) simply
returns 1 and it does not grow any further.
06:37 - And now F(1) resumes.
06:39 - F(1) finishes and it is also remove or popped
from the stack.
06:44 - And now F(2) resumes and F(2) is also removed
from the stack once it finishes.
06:49 - So we're trying to show you a simulation of
how this stack is growing and decreasing as
06:54 - this recursive call is finishing.
06:56 - Now F(3) finishes and F(4) resumes.
06:59 - Finally F(4) finishes and F(5) resumes.
07:05 - Now this F(5) finally finishes and returns
to its caller, maybe the main method.
07:16 - Now I'll also remove this F(5) from the memory
here because this also finishes and returns.
07:21 - There are also lots of other things corresponding
to this program execution like the state of
07:26 - the main method and other function calls that
get saved in the memory.
07:31 - But we only tried to show you the memory used
by this particular recursive call.
07:36 - So we saw that the maximum space taken by
this program for an input n is n units.
07:44 - So the space taken is proportional to n.
07:50 - And that's why this is an order of n algorithm
in terms of space complexity.
07:54 - In another lesson, we will try to analyze
the complexity of Fibonacci sequence.
08:01 - Thanks for watching!

Cleaned transcript:

In this lesson we will see how to analyze the complexity of recursive programs and we will do so by picking up a very simple example of factorial of a positive integer. Now when we talk about complexity analysis of programs we basically mean two things. First is time complexity, which basically is a measurement of how the time taken by a program grows with the input. And another thing is space complexity which is the measurement of how the space or memory consumed by the program grows with input. So let's pick up a simple example of recursive implementation of factorial of a positive integer and try to analyze it. So here in the left, I've written a recursive implementation to calculate the factorial of n. If n is 0, we simply return 1. Else we make a recursive call to calculate the factorial of n1 multiply it with n and return the output. Now let's say time taken to calculate factorial of n is equal to T(n) When we try to analyze time complexity of a program we make an assumption that each simple operation costs us one unit of time. So, if we see for n greater than 0 we first compare it with 0 which is one simple operation. So this has got cost of 1 unit.Then we have a multiplication operation here that we perform after we get a return from factorial of n1. So this is one unit of time. And we perform a subtraction to calculate n minus 1, which is another unit of cost. So T(n) is nothing but the time taken to calculate the factorial of n minus 1, which is T(n1) plus 3 units of cost for the simple operations. And this is true for all n greater than 0. For n equal to 0, T(0) is equal to 1 because we only make a comparison here and simply return. Now, we have T(n) equal to T(n minus 1) plus 3. Let's try to reduce this expression T(n) in terms of our known value T(0). Now T(n minus 1) can be written as T(n2) plus 3, so overall the expression will be T(n minus 2)plus 6 and this can be further reduced to T(n3) plus 9 because T(n2) is nothing but T(n3) + 3. So if I have to reduce this by a generic k, then this is equal to T(nk) plus 3 into k or 3k. Now we want to express this in terms of T(0). So in that case n minus k will be equal to 0 or k will be equal to n. So this expression would finally reduce as T(n) = T(0)plus 3n which is nothing but 3n plus 1 because T(0) is 1 So we can see here that the time taken by this particular program for an input n is directly proportional to n and we can also say that this is bigO(n) in terms of time complexity. Or this is an order of n algorithm. Now there are a couple of different methods to calculate time complexity of a recursive program. This is one of the methods in which we try to reduce T(n) in terms of its base conditions. Now let us try to analyze space complexity of this program. And we will again this time go by an example Let's say we want to calculate factorial of 5 so we make a call to Factorial(5). I will write F(5) here as a short cut for Factorial(5). ow F(5) goes on to calculate F(4) recursively. What happens at this stage is that the computer says "Hey, I will save the state of this particular function call F(5) which means saving all its local variables and its current state of execution in the memory and go ahead and calculate F(4) first. And once I'm done with F(4) I'll come back to F(5)" Let's say this is computer's memory on the right and these partitions are some unit in which the memory is divided. So computer saves the state of F(5) in the memory and goes ahead to calculate F(4) Now F(4) again makes a call to F(3). So F(4) again is saved in the memory. F(3) again makes call to F(2). F(2) again makes a call to F(1). So all these states of the functions are getting stacked in the memory. And this goes on until we make a call to F(0). And F(0) does not make any further recursive call. So we can see here that in our actual implementation, even though we are not declaring any variable or assigning any memory explicitly, all these states of these functions are getting saved, or stacked in the memory and behind the scenes, our program is consuming all this space. We often say that an implicit stack is growing in the memory when recursion is executing. And what is the maximum size of this stack? If this structure, which is called the recursion tree has a maximum depth of n. Maximum depth of the tree can be defined as the longest path in the tree. So if this is level 0 and this is level 1 and this is level 2, 3, 4 and 5 and each arrow here is one unit of length in the path then the maximum path/maximum depth is from L0 to L5 and it is equal to 5 units. And in this case the maximum depth is equal to 5. And if you draw this recursion tree for any generic n, then the max depth is equal to n. So in this particular case, the space required by the program is directly proportional to the input n. So we say that this is an order of n algorithm in terms of space complexity. So now the stack grows to a maximum of n units in this particular example. So when we are calculating F(5), it grows to a maximum of 5 units. And as soon as we reach F(0), F(0) simply returns 1 and it does not grow any further. And now F(1) resumes. F(1) finishes and it is also remove or popped from the stack. And now F(2) resumes and F(2) is also removed from the stack once it finishes. So we're trying to show you a simulation of how this stack is growing and decreasing as this recursive call is finishing. Now F(3) finishes and F(4) resumes. Finally F(4) finishes and F(5) resumes. Now this F(5) finally finishes and returns to its caller, maybe the main method. Now I'll also remove this F(5) from the memory here because this also finishes and returns. There are also lots of other things corresponding to this program execution like the state of the main method and other function calls that get saved in the memory. But we only tried to show you the memory used by this particular recursive call. So we saw that the maximum space taken by this program for an input n is n units. So the space taken is proportional to n. And that's why this is an order of n algorithm in terms of space complexity. In another lesson, we will try to analyze the complexity of Fibonacci sequence. Thanks for watching!
