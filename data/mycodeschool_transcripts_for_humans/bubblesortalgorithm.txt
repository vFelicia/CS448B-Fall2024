With timestamps:

00:00 - In our previous lesson, in this series on sorting
algorithms, we had discussed Selection Sort
00:05 - algorithm. Now in this lesson, we are going
to discuss another sorting algorithm, named
00:10 - Bubble Sort. So, once again, let's say we
have a list of integers, given to us in the
00:15 - form of an array something like this. Let's
name this array A, we have six elements in
00:21 - the array. So, we have indices from 0 to 5.
Now, we want to sort this array, so, we want
00:26 - to rearrange the elements in the array in
increasing order. What we are going to do
00:31 - in this algorithm is we are going to scan
the array from left to right multiple times.
00:38 - We will call each scan 1 pass on the array.
It's like we will first look at the 0th element
00:44 - and then look at the 1th element and then
the element at index 2 and so on. Of course
00:52 - we will not scan the array for no reason.
What we will do is when we are scanning the
00:57 - array and we are at a particular position,
we will compare the element at that particular
01:01 - position with the adjacent element at the
next position. So, if we are at 0th position,
01:08 - then we will compare the element at 0th position
with the element at 1th position, and if the
01:13 - element at the current position is greater
than the element at the next position, we
01:18 - will swap the two elements. In this case,
2 is not greater than 7, so, we will not swap
01:24 - the two elements. We will just move on to
the next position which will be position 1.
01:30 - Once again we will compare the element with
its next element and if it is greater, we
01:36 - will swap the two elements. In this case 7
is greater than 4, so, we will swap the position
01:42 - of these two elements. So, 7 will move to
index 2 and 4 will move to index 1. And now
01:49 - we will move to index 2, the number at index
2 at this stage will be 7. Once again we will
01:56 - look at the next element, it's 1. 7 is again
greater than 1, so we will swap and now, we
02:04 - will move to index 3. Once again, at this
stage the number at index three is 7, we will
02:10 - compare it with 5 and we need to swap again.
So, 7 will go to index 4 and 5 will move to
02:16 - index 3 and now we will go to index 4. As,
you can see, this whole process is pushing
02:22 - number 7, which is the largest in the array
towards higher indices at each step. When
02:27 - we are position 4, once again 7 is greater
than three, so we will swap. There is no next
02:33 - element for index 5 and at this stage we are
done with one pass on the array and what has
02:40 - happened after this one pass is that 7 which
is the largest in the array is at its appropriate
02:47 - position. It deserves to be at index 5 in
the sorted array and that's where it is. So,
02:52 - 7 has kind of bubbled up to the right most
position in the array, with this whole logic
02:58 - of swapping the adjacent elements. This was
our initial array and after first pass we
03:03 - have gone to a state like this. If I quickly
have to write pseudo code for a pass, it will
03:09 - be running a loop something like this. We
will run a loop from 0 to n-1. Let's say n
03:15 - is the number of elements in an array and
if A[i], the element at position i, is greater
03:23 - than element at position i+1, we will swap
the two elements. There is one minor bug here,
03:31 - if i=n-1, it will be the last index, so there
will be no element after that. So, we will
03:38 - not be able to access A[i+1], so we will run
this loop only till n-2. We don't want to
03:44 - access an index that will be out of the bound
of the array, out of the range of the array.
03:50 - For i=n-1, A[i+1] would have been out of range.
Okay, so this is the state of the array after
03:57 - one pass. What if we perform another pass
and once again keep comparing the adjacent
04:03 - elements and performing swaps. If we will
do so, now the second largest element in this
04:08 - example will land up at index 4. The second
largest in the array is number 5. So, after
04:17 - 2nd pass our array will be in a state like
this. With every pass on the array, the array
04:23 - will be divided into two parts, one part will
be the sorted part and another part will be
04:29 - unsorted part. After 2 passes, the part of
the array from index 4 till 5 is sorted and
04:35 - the part of the array from index 0 till 3
is unsorted. With each pass, the largest element
04:41 - in the unsorted half will bubble up to the
highest index in the unsorted half. So, in
04:47 - 3rd pass, number 4 should bubble up to position
3, at index 3. And while scanning, once we
04:55 - reach to the part where we are sorted, there
will be no swapping. We can actually avoid
05:00 - going to the sorted part. It will only improve
our algorithm. After pass 3, our array will
05:06 - be looking like this. In fact we are already
sorted. In general, if we will conduct n-1
05:13 - such passes, for an array of size n. We can
say something like for k, 1 to n-1, or we
05:18 - could say for k, 0 to n-2, after n-1 passes,
we are guaranteed to be sorted. So, this is
05:26 - our pseudo code for bubble sort algorithm.
Given an array and the number of elements
05:32 - in the array, this function Bubble Sort will
sort the elements in the array in increasing
05:38 - order. Let's now try to analyze the time complexity
of this algorithm. The running time of this
05:43 - algorithm will be the running time of these
statements inside the nested loop. Let us
05:49 - assume that these statements will take constant
time c in the worst case. These statements
05:55 - will execute in constant time. Now, the first
loop will run exactly n-1 times, and the second
06:01 - loop will also run exactly n-1 times, so the
total time taken as a function of n will be
06:08 - (n-1)* (n-1)* c, which will evaluate to c*
(n^2) - 2cn +1. Whenever we have a polynomial
06:18 - expression for time, then we say that the
time complexity is big-Oh of the highest order
06:25 - term in the polynomial. The highest order
term here is n^2, we just remove the constants.
06:29 - And, we say that this running time will fall
into the class big-Oh of highest order term,
06:35 - in this case it will be n^2.If you do not
know about big-Oh notation, or how to calculate
06:41 - running time of algorithms, we have a whole
series on time complexity analysis, you can
06:46 - find a link to it in the description of this
video. Big-Oh of n^2 is not the best running
06:52 - time for a sorting algorithm. In fact this
running time is bad. Bubble sort is a slow
06:58 - sorting algorithm, it's as good as Selection
Sort but both Bubble sort and Selection sort
07:03 - are slow sorting algorithms. We can do couple
of things in this algorithm to improve the
07:08 - time complexity, at least for some scenarios.
The first thing that we can do is, we need
07:14 - not run this second loop till n-2, all the
time. As we had discussed earlier, at any
07:20 - stage during the sorting, the array will have
some part as sorted and some part as unsorted.
07:26 - There is no point passing through the sorted
part because there will be no swapping in
07:30 - that part. For first pass we can run this
inner loop till n-2, for the second pass we
07:35 - can run this inner loop till n-3 and we will
be good. For the third pass, we can only run
07:42 - till n-4 and so on. So, in general we can
run the loop till n-k-1, so when k is 1, we
07:49 - can run till n-2, when k is 2, we will run
till n-3 and so on. This is some improvement,
07:56 - but in this case also, if you will calculate
the time expression, it will be some polynomial
08:00 - of the form an^2 + bn + c, so complexity will
still be big-Oh of n^2. We can do something
08:08 - else to improve this algorithm further. If
you remember the example that we had picked
08:12 - up, it was sorted after 3 passes only and
4th and 5th pass was only redundant. If the
08:19 - list is already sorted, there would be no
swaps. So, if we go through a pass without
08:24 - swapping anything, then definitely at that
stage the list is already sorted. So, I will
08:30 - do something like this in this algorithm,
I will take a variable and name it flag and
08:37 - set it to 0, before making a pass and once
this condition A[i]>A[i+1] is true for any
08:46 - i, then we will have to swap and we will set
this flag as 1. And now, when we will come
08:54 - out of this loop after a pass, if flag is
0, then there has been not even one swap,
09:01 - so we do not need to conduct any more passes,
so we can break out of this loop, the outer
09:07 - loop and this way we will be able to avoid
making redundant passes once the array is
09:12 - sorted. Now, with this modification, if we
input an already sorted array, to the function
09:18 - bubble sort then this particular loop will
execute only once to figure out that it's
09:23 - already sorted. So, the time taken if we were
considering this as taking some constant time
09:30 - c, in the worst case, the time taken will
be c*(n-1) only. So, this will be the best
09:35 - case for our algorithm. Our algorithm will
be O(n) in the best case. In the average case,
09:41 - somewhere mid-way, after making let's say
n/2 passes, we will exit the inner loop. If
09:49 - we will deduce the time complexity expression,
it will still come something like an^2 + bn
09:56 - +c kind of expression, so in average case
also, this will be Big-Oh(n^2). And in the
10:02 - worst case, the inner loop will also run n-1
times, and we will be Big-Oh(n^2). Bubble
10:11 - Sort is in place and stable sorting algorithm
and we just deduced it's time complexity which
10:17 - is Big-Oh(n^2) in the worst case. This is
it for this lesson. Thanks for watching!

Cleaned transcript:

In our previous lesson, in this series on sorting algorithms, we had discussed Selection Sort algorithm. Now in this lesson, we are going to discuss another sorting algorithm, named Bubble Sort. So, once again, let's say we have a list of integers, given to us in the form of an array something like this. Let's name this array A, we have six elements in the array. So, we have indices from 0 to 5. Now, we want to sort this array, so, we want to rearrange the elements in the array in increasing order. What we are going to do in this algorithm is we are going to scan the array from left to right multiple times. We will call each scan 1 pass on the array. It's like we will first look at the 0th element and then look at the 1th element and then the element at index 2 and so on. Of course we will not scan the array for no reason. What we will do is when we are scanning the array and we are at a particular position, we will compare the element at that particular position with the adjacent element at the next position. So, if we are at 0th position, then we will compare the element at 0th position with the element at 1th position, and if the element at the current position is greater than the element at the next position, we will swap the two elements. In this case, 2 is not greater than 7, so, we will not swap the two elements. We will just move on to the next position which will be position 1. Once again we will compare the element with its next element and if it is greater, we will swap the two elements. In this case 7 is greater than 4, so, we will swap the position of these two elements. So, 7 will move to index 2 and 4 will move to index 1. And now we will move to index 2, the number at index 2 at this stage will be 7. Once again we will look at the next element, it's 1. 7 is again greater than 1, so we will swap and now, we will move to index 3. Once again, at this stage the number at index three is 7, we will compare it with 5 and we need to swap again. So, 7 will go to index 4 and 5 will move to index 3 and now we will go to index 4. As, you can see, this whole process is pushing number 7, which is the largest in the array towards higher indices at each step. When we are position 4, once again 7 is greater than three, so we will swap. There is no next element for index 5 and at this stage we are done with one pass on the array and what has happened after this one pass is that 7 which is the largest in the array is at its appropriate position. It deserves to be at index 5 in the sorted array and that's where it is. So, 7 has kind of bubbled up to the right most position in the array, with this whole logic of swapping the adjacent elements. This was our initial array and after first pass we have gone to a state like this. If I quickly have to write pseudo code for a pass, it will be running a loop something like this. We will run a loop from 0 to n1. Let's say n is the number of elements in an array and if A[i], the element at position i, is greater than element at position i+1, we will swap the two elements. There is one minor bug here, if i=n1, it will be the last index, so there will be no element after that. So, we will not be able to access A[i+1], so we will run this loop only till n2. We don't want to access an index that will be out of the bound of the array, out of the range of the array. For i=n1, A[i+1] would have been out of range. Okay, so this is the state of the array after one pass. What if we perform another pass and once again keep comparing the adjacent elements and performing swaps. If we will do so, now the second largest element in this example will land up at index 4. The second largest in the array is number 5. So, after 2nd pass our array will be in a state like this. With every pass on the array, the array will be divided into two parts, one part will be the sorted part and another part will be unsorted part. After 2 passes, the part of the array from index 4 till 5 is sorted and the part of the array from index 0 till 3 is unsorted. With each pass, the largest element in the unsorted half will bubble up to the highest index in the unsorted half. So, in 3rd pass, number 4 should bubble up to position 3, at index 3. And while scanning, once we reach to the part where we are sorted, there will be no swapping. We can actually avoid going to the sorted part. It will only improve our algorithm. After pass 3, our array will be looking like this. In fact we are already sorted. In general, if we will conduct n1 such passes, for an array of size n. We can say something like for k, 1 to n1, or we could say for k, 0 to n2, after n1 passes, we are guaranteed to be sorted. So, this is our pseudo code for bubble sort algorithm. Given an array and the number of elements in the array, this function Bubble Sort will sort the elements in the array in increasing order. Let's now try to analyze the time complexity of this algorithm. The running time of this algorithm will be the running time of these statements inside the nested loop. Let us assume that these statements will take constant time c in the worst case. These statements will execute in constant time. Now, the first loop will run exactly n1 times, and the second loop will also run exactly n1 times, so the total time taken as a function of n will be (n1)* (n1)* c, which will evaluate to c* (n^2) 2cn +1. Whenever we have a polynomial expression for time, then we say that the time complexity is bigOh of the highest order term in the polynomial. The highest order term here is n^2, we just remove the constants. And, we say that this running time will fall into the class bigOh of highest order term, in this case it will be n^2.If you do not know about bigOh notation, or how to calculate running time of algorithms, we have a whole series on time complexity analysis, you can find a link to it in the description of this video. BigOh of n^2 is not the best running time for a sorting algorithm. In fact this running time is bad. Bubble sort is a slow sorting algorithm, it's as good as Selection Sort but both Bubble sort and Selection sort are slow sorting algorithms. We can do couple of things in this algorithm to improve the time complexity, at least for some scenarios. The first thing that we can do is, we need not run this second loop till n2, all the time. As we had discussed earlier, at any stage during the sorting, the array will have some part as sorted and some part as unsorted. There is no point passing through the sorted part because there will be no swapping in that part. For first pass we can run this inner loop till n2, for the second pass we can run this inner loop till n3 and we will be good. For the third pass, we can only run till n4 and so on. So, in general we can run the loop till nk1, so when k is 1, we can run till n2, when k is 2, we will run till n3 and so on. This is some improvement, but in this case also, if you will calculate the time expression, it will be some polynomial of the form an^2 + bn + c, so complexity will still be bigOh of n^2. We can do something else to improve this algorithm further. If you remember the example that we had picked up, it was sorted after 3 passes only and 4th and 5th pass was only redundant. If the list is already sorted, there would be no swaps. So, if we go through a pass without swapping anything, then definitely at that stage the list is already sorted. So, I will do something like this in this algorithm, I will take a variable and name it flag and set it to 0, before making a pass and once this condition A[i]>A[i+1] is true for any i, then we will have to swap and we will set this flag as 1. And now, when we will come out of this loop after a pass, if flag is 0, then there has been not even one swap, so we do not need to conduct any more passes, so we can break out of this loop, the outer loop and this way we will be able to avoid making redundant passes once the array is sorted. Now, with this modification, if we input an already sorted array, to the function bubble sort then this particular loop will execute only once to figure out that it's already sorted. So, the time taken if we were considering this as taking some constant time c, in the worst case, the time taken will be c*(n1) only. So, this will be the best case for our algorithm. Our algorithm will be O(n) in the best case. In the average case, somewhere midway, after making let's say n/2 passes, we will exit the inner loop. If we will deduce the time complexity expression, it will still come something like an^2 + bn +c kind of expression, so in average case also, this will be BigOh(n^2). And in the worst case, the inner loop will also run n1 times, and we will be BigOh(n^2). Bubble Sort is in place and stable sorting algorithm and we just deduced it's time complexity which is BigOh(n^2) in the worst case. This is it for this lesson. Thanks for watching!
