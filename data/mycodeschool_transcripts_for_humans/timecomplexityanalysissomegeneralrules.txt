With timestamps:

00:00 - So, in the previous lessons we learnt how to calculate the  running time of an algorithm
00:05 - and we also learnt about asymptotic notations as a way to classify these 
00:09 - running times
00:11 - into some generic classes
00:13 - now in this lesson
00:14 - we will learn about some techniques and some rules using which
00:18 - we can avoid doing all these complex calculations
00:21 - and still derive the time complexity expression in terms of
00:26 - big-oh or theta notation
00:29 - before we start discussing these techniques
00:32 - i will make two statements
00:34 - and the statements are that we analyze time complexity
00:38 - in most cases for (a)
00:41 - very large input size
00:44 - and (b)
00:45 - worst-case scenario
00:48 - what can be a worst case scenario apart
from very large input size is something
00:52 - that we will discuss
00:54 - later in some examples
00:57 - now if we have a function defining the
running time of an algorithm like
01:00 - T(n)  equal to
01:02 - n^3 + 3n^2 + 4n + 2
01:05 - then for very large input sizes
01:08 - these lower order terms like 3n^2 + 4n + 2 will become
01:12 - insignificant
01:14 - in comparison to the n^3 term, so this will
01:17 - be almost equal to n^3 for very large values of n
01:21 - Or we can also say that
01:23 - thing like
01:24 - when n tends to infinity
01:27 - and that's why we say that this function 
after some time will not grow
01:31 - any faster
01:33 - than function cn^3
01:36 - where c is some constant and we
01:37 - say the same thing as
01:39 - this particular function is
01:41 - big-oh of  n^3
01:44 - so the first
01:45 - rule that i want to state is that if you want to calculate the big-oh
01:48 - notation from a polynomial expression like this, then
01:52 - you need to drop all the lower order terms, and
01:57 - you also need to drop the constant multiplier
02:00 - and you will get the
02:01 - big-oh
02:02 - expression
02:03 - for example
02:04 - say if T(n) is equal to
02:06 - 17n^4 + 3n^3 + 4n + 8,  then you first
02:12 - need to drop the lower order terms
02:14 - and just keep the
02:15 - highest power which is n to the power 4
02:18 - and then you also need  to drop this
02:20 - constant multiplier seventeen
02:22 - so this will be big-oh of
02:24 - n to the power 4l
02:27 - and we may also sometimes encounter
02:30 - time expressions like
02:32 - T(n) equal to 
02:33 - say 16n + log(n)
02:36 - and for higher values of n log n will
become insignificant
02:41 - and you can always
02:42 - try to deduce a mathematical proof but
this will be equal to
02:47 - big-oh of n
02:48 - so you just
02:49 - ignored the
02:50 - lower order term
02:52 - which will be insignificant in comparison ton. log n will be
02:55 - insignificant in comprising to n and we
drop the constant 16
03:00 - now let use see some other rules
03:02 - and the rule is that we can calculate
the running time
03:08 - of an algorithm by summing up that
running time of
03:13 - the fragments in the program
03:14 - and we will discuss this using some
examples shortly
03:18 - what we will  basically teach you is that
03:20 - by looking at a code you shouldfirst look at
03:23 - that running time
03:24 - for different fragments
03:26 - and then calculate the overall running time
03:29 - if a fragment of code is a group of
simple statements
03:32 - like some declaration
03:34 - for example we declare int a and then we say 
that a =5 then we say
03:39 - that a
03:41 - should be incremented
03:43 - then all of these are simple expressions
03:45 - so fragment of code
03:46 - with all these simple expressions will
always run in a constant time
03:50 - so the time complexity is O(1)
03:54 - now if we have a loop, say for example
03:57 - and let's say
03:58 - this is a single loop and inside the loop we
have simple statements
04:02 - and no complex function call
04:04 - or another loop
04:05 - so the running time of
04:07 - a single loop
04:09 - would be
04:10 - the number of times the loop runs
04:13 - multiplied by
04:14 - the running time of the statements
inside, so in this case
04:17 - this loop runs
04:18 - n times
04:20 - and the simple statements take constant
time so the total time taken is
04:23 - definitely proportional to n, so
04:26 - the time complexity of this fragment, a
fragment like this would be big-oh
04:30 - big-oh of n
04:32 - and I will  create some space for another
example
04:35 - now let's say we have a fragment which is
a nested loop
04:39 - now in this case
04:41 - again the outer loop
04:42 - will run n times
04:44 - and corresponding to each
04:45 - run of the outer loop, the inner loop will also run n times
04:49 - so the simple statements
04:51 - will be actually executed
04:53 - n cross n times
04:55 - and the running time of this particular code
04:59 - will be some constant times
05:01 - n square
05:02 - so clearly
05:03 - you can say that this is big-oh of n^2,
 this fragment is O(n^2)
05:07 - in terms of
05:08 - Time Complexity
05:10 - similarly if there were
05:12 - three nested loops, then in that case
the complexity would have been big-oh
05:16 - of
05:17 - n cube
05:18 - Now, let's assume we have a program like
this
05:21 - now in this case we have a fragment
which is just a bunch of simple
05:25 - statements
05:27 - we have a fragment which is
05:29 - uh... single loop
05:31 - and we have a fragment which is
05:33 - nested double loop and we have picked up
05:36 - this example from our previous slide
05:39 - now the running time of the first fragment as we saw
05:42 - was
05:43 - O(1), so this is O(1) and
05:45 - this is O(n)
05:48 - and this is O(n^2) and the overall running time of this particular function
05:53 - would be sum of the running time of all
these fragments, so this is
05:57 - O(1) + O(n) + O(n^2)
06:01 - now once again
06:02 - when we analyze time complexity, we want to analyze it for very large input
06:06 - sizes
06:06 - so for very large input sizes
06:09 - for very large values of n
06:11 - this part will again become insignificant
06:14 - so, the time taken is actually O(n^2), so
06:19 - Thus in a typical program
06:21 - the maximum running time, the fragment
which has the maximum running time
06:25 - actually
06:26 - decides the
06:27 - overall running time of the program
06:30 - now let's say
06:31 - we have a function like this where we have
some conditional statements, so the
06:35 - program is like
06:36 - if some condition is true
06:39 - then we have a 
06:41 - single loop
06:42 - which have
06:43 - which will have a  time complexity O(n)
06:46 - and if this condition is not true
06:48 - we have a nested double loop
06:50 - which will have a time complexity of O(n^2)
06:54 - now if control of the program goes to
06:56 - this particular part
06:58 - It will execute with a complexity O(n)
07:01 - but if the control goes to the else part
07:04 - it will execute
07:06 - with a time complexity of O(n^2)
07:09 - as i had mentioned earlier, when we try to analyze time complexity
07:13 - we always try to analyze it in the 
worst case
07:15 - so in this case
07:17 - if we are not lucky then
07:19 - we will go into
07:20 - that else condition
07:22 - the control of the program will go into
the else condition and the
07:25 - complexity would be O(n^2)
07:28 - so that's why we say  that the time
complexity for this particular program
07:32 - O(n^2),  because that's how
07:35 - it is in the worst case scenario
07:37 - so in case of the conditional
statements, you do not simply add up
07:41 - the fragments or try to
07:43 - calculate the time taken
07:46 - as the some of the two
07:48 - running times
07:49 - but you
07:50 - pick up the maximum of the two
07:52 - so we have this rule for conditional
statements
07:55 - that pick complexity of the condition
which is the worst-case
08:00 - so in this lesson we saw some rules to
analyze the time complexity of some very
08:03 - basic programs
08:05 - in next couple of lessons
08:07 - we will see different
08:08 - time complexity functions
08:10 - and their comparison
08:11 - and, we will also see
08:13 - how-to
08:14 - analyze the time complexity of
08:16 - recursive programs
08:17 - So, thanks for watching !

Cleaned transcript:

So, in the previous lessons we learnt how to calculate the running time of an algorithm and we also learnt about asymptotic notations as a way to classify these running times into some generic classes now in this lesson we will learn about some techniques and some rules using which we can avoid doing all these complex calculations and still derive the time complexity expression in terms of bigoh or theta notation before we start discussing these techniques i will make two statements and the statements are that we analyze time complexity in most cases for (a) very large input size and (b) worstcase scenario what can be a worst case scenario apart from very large input size is something that we will discuss later in some examples now if we have a function defining the running time of an algorithm like T(n) equal to n^3 + 3n^2 + 4n + 2 then for very large input sizes these lower order terms like 3n^2 + 4n + 2 will become insignificant in comparison to the n^3 term, so this will be almost equal to n^3 for very large values of n Or we can also say that thing like when n tends to infinity and that's why we say that this function after some time will not grow any faster than function cn^3 where c is some constant and we say the same thing as this particular function is bigoh of n^3 so the first rule that i want to state is that if you want to calculate the bigoh notation from a polynomial expression like this, then you need to drop all the lower order terms, and you also need to drop the constant multiplier and you will get the bigoh expression for example say if T(n) is equal to 17n^4 + 3n^3 + 4n + 8, then you first need to drop the lower order terms and just keep the highest power which is n to the power 4 and then you also need to drop this constant multiplier seventeen so this will be bigoh of n to the power 4l and we may also sometimes encounter time expressions like T(n) equal to say 16n + log(n) and for higher values of n log n will become insignificant and you can always try to deduce a mathematical proof but this will be equal to bigoh of n so you just ignored the lower order term which will be insignificant in comparison ton. log n will be insignificant in comprising to n and we drop the constant 16 now let use see some other rules and the rule is that we can calculate the running time of an algorithm by summing up that running time of the fragments in the program and we will discuss this using some examples shortly what we will basically teach you is that by looking at a code you shouldfirst look at that running time for different fragments and then calculate the overall running time if a fragment of code is a group of simple statements like some declaration for example we declare int a and then we say that a =5 then we say that a should be incremented then all of these are simple expressions so fragment of code with all these simple expressions will always run in a constant time so the time complexity is O(1) now if we have a loop, say for example and let's say this is a single loop and inside the loop we have simple statements and no complex function call or another loop so the running time of a single loop would be the number of times the loop runs multiplied by the running time of the statements inside, so in this case this loop runs n times and the simple statements take constant time so the total time taken is definitely proportional to n, so the time complexity of this fragment, a fragment like this would be bigoh bigoh of n and I will create some space for another example now let's say we have a fragment which is a nested loop now in this case again the outer loop will run n times and corresponding to each run of the outer loop, the inner loop will also run n times so the simple statements will be actually executed n cross n times and the running time of this particular code will be some constant times n square so clearly you can say that this is bigoh of n^2, this fragment is O(n^2) in terms of Time Complexity similarly if there were three nested loops, then in that case the complexity would have been bigoh of n cube Now, let's assume we have a program like this now in this case we have a fragment which is just a bunch of simple statements we have a fragment which is uh... single loop and we have a fragment which is nested double loop and we have picked up this example from our previous slide now the running time of the first fragment as we saw was O(1), so this is O(1) and this is O(n) and this is O(n^2) and the overall running time of this particular function would be sum of the running time of all these fragments, so this is O(1) + O(n) + O(n^2) now once again when we analyze time complexity, we want to analyze it for very large input sizes so for very large input sizes for very large values of n this part will again become insignificant so, the time taken is actually O(n^2), so Thus in a typical program the maximum running time, the fragment which has the maximum running time actually decides the overall running time of the program now let's say we have a function like this where we have some conditional statements, so the program is like if some condition is true then we have a single loop which have which will have a time complexity O(n) and if this condition is not true we have a nested double loop which will have a time complexity of O(n^2) now if control of the program goes to this particular part It will execute with a complexity O(n) but if the control goes to the else part it will execute with a time complexity of O(n^2) as i had mentioned earlier, when we try to analyze time complexity we always try to analyze it in the worst case so in this case if we are not lucky then we will go into that else condition the control of the program will go into the else condition and the complexity would be O(n^2) so that's why we say that the time complexity for this particular program O(n^2), because that's how it is in the worst case scenario so in case of the conditional statements, you do not simply add up the fragments or try to calculate the time taken as the some of the two running times but you pick up the maximum of the two so we have this rule for conditional statements that pick complexity of the condition which is the worstcase so in this lesson we saw some rules to analyze the time complexity of some very basic programs in next couple of lessons we will see different time complexity functions and their comparison and, we will also see howto analyze the time complexity of recursive programs So, thanks for watching !
