00:00 - hey this is andrew brown your cloud
00:01 - instructor at exam pro and i'm bringing
00:03 - you another complete study course and
00:04 - this time it's the azure data
00:05 - fundamentals certification made
00:07 - available to you on free code camp so
00:09 - this course is designed to help you pass
00:11 - exam and achieve microsoft issued
00:12 - certification and the way we're going to
00:14 - do that is by going through lots of
00:15 - lecture content getting some hands-on
00:17 - experience with follow-alongs and on the
00:19 - daily exam i have these really great
00:20 - cheat sheets that are going to help you
00:22 - uh pass for sure so the great thing is
00:24 - at the end of it you'll be able to get
00:26 - that certification and show that on your
00:28 - resume or linkedin that you have the
00:29 - azure knowledge so you can go get that
00:31 - data job or that promotion you've been
00:33 - looking for just to introduce myself i'm
00:36 - previously the cto of multiple edtech
00:37 - companies 15 years experience with five
00:39 - years specializing in the cloud i'm an
00:41 - aws community hero and i've published
00:43 - many many
00:44 - free cloud certification courses and i
00:47 - love star trek and coconut water i just
00:49 - want to take a moment here to tell you
00:50 - that this video cloud certification
00:52 - course is made possible by viewers like
00:54 - you and i appreciate your support and
00:56 - thank you if you want to help support
00:59 - more free cloud courses just like this
01:01 - one the best ways to buy your extra
01:02 - study materials at exam pro dot co
01:04 - forward slash dp hyphen 900 to get study
01:07 - notes flash cards quizlets deliverable
01:09 - cheat sheets practice exams
01:11 - you can ask questions and get some
01:13 - support from our cloud engineers and
01:15 - just so you know if you want to keep up
01:16 - to date with upcoming courses you can
01:18 - follow me on twitter at andrew brown
01:20 - that's uh and what you can do is once
01:22 - you do that you can tell me you passed
01:24 - the exam you can tell me what you'd like
01:25 - to see next because a lot of times the
01:27 - next course i make is based on the
01:28 - feedback i get from you so let's jump
01:30 - into the course now
01:34 - [Music]
01:37 - hey this is andrew brown from exam pro
01:39 - and we are at the start of our journey
01:40 - asking the most important question first
01:42 - which is what is the dp900 so the azure
01:44 - data fundamental certification is for
01:47 - those seeking a data related role such
01:48 - as data analysts data engineer or data
01:51 - scientist and the certification will
01:53 - demonstrate a person can define and
01:54 - understand coordinated concepts hadoop
01:56 - workloads apache spark workloads mssql
01:59 - databases nosql databases data lakes
02:01 - data warehouses elts and big data
02:04 - analytics and more the certification is
02:06 - generally referred to by its course code
02:08 - the dp900 and it's the natural path for
02:10 - the azure data engineer or the azure
02:13 - data analyst certifications this is an
02:16 - easy course to pass and great for those
02:18 - new to cloud or data related technology
02:20 - so let's look at our big old road map
02:23 - here and what i'm going to do is pull
02:25 - out my laser pointer so you can see
02:26 - where i am and we'll get some lines in
02:28 - here because i want to show you some of
02:30 - the paths you can take with the
02:31 - certification so we're over here with
02:33 - the dp900 and this is at the
02:35 - fundamentals level right and so a lot of
02:37 - times what people will do is they will
02:39 - have the az-900 oh they'll take that
02:41 - first if they're 100 new to cloud so i
02:43 - see a lot of people they start here and
02:45 - if you're if you've never used azure
02:47 - before it's a great starting point and
02:49 - then they'll move on to the dp900 or
02:51 - sometimes what they'll do is they'll
02:52 - move on to the ai 900 and then onto the
02:55 - db dp900 or they'll take the dp900 and
02:58 - then the ai fundamentals but the reason
03:00 - you want to take this data fundamentals
03:02 - certification is primarily because you
03:05 - you're very likely want to take the data
03:07 - engineer certification or the data
03:09 - analyst certification which is at the
03:10 - associate track so this one it's
03:12 - basically uh the dp900 but actually
03:15 - knowing how to implement everything and
03:18 - then the data analyst is really focused
03:20 - on power bi so just knowing how to use
03:22 - uh power bi uh to uh to its maximum
03:25 - extent okay
03:27 - now a lot of people that are going for
03:29 - the data scientist or ai engineer track
03:31 - will take the dp900 because you do need
03:33 - foundational knowledge about data to
03:36 - understand these roles
03:38 - and so you know it's just if you are
03:40 - going for the data scientist you
03:42 - probably want to add the dp900 to your
03:44 - track but if you've taken the az900 and
03:46 - the az-104 you might want to skip over
03:49 - this it's just up to you but yeah
03:51 - there's a lot of ways that you can go
03:52 - about this okay
03:54 - and we'll just move on uh here so how
03:56 - long to study to pass the dp900 so if
03:58 - you have one year experience with azure
04:00 - you're looking at five hours of study if
04:02 - you've passed the az 900 you're looking
04:04 - at 10 hours if you're new to cloud
04:06 - you're looking at 15 hours of study for
04:08 - new people i'm saying 30 minutes a day
04:11 - for 14 days the course content itself is
04:14 - not very long but the thing is you have
04:15 - to factor in that you have to do um
04:18 - you have to do practice exams and you
04:20 - also uh just have to put that knowledge
04:22 - into practice by using the console so
04:25 - even though you know the course
04:26 - content's not long there's additional
04:28 - time you have to do there where would
04:30 - you take this exam well you can do it in
04:32 - in-person test center or online from the
04:33 - convenience of your own home so there
04:35 - are two options you got psi and pearson
04:38 - vue and uh they both actually do in
04:40 - person and online and so just so you
04:43 - know if you ever heard the term proctor
04:45 - a lot of times we talk about online
04:46 - exams being proctored exams because
04:48 - there's a person or supervisor that's
04:50 - watching you take the exam online okay
04:53 - so what does it take to pass this exam
04:55 - well you got to watch the lecture videos
04:57 - you got to do hands-ons and follow
04:59 - alongs and it helps to do paid practice
05:02 - exams that simulate the real exam this
05:04 - is pretty easy certification so you
05:06 - could probably get away with um
05:08 - [Music]
05:13 - okay so what i did was i just googled
05:14 - dp900 exam guide and so i made it to the
05:17 - microsoft site if you scroll on down
05:18 - you're looking for that skills measure
05:20 - download that pdf open it up it will be
05:22 - over here now it's very common to see
05:24 - this red text here azure loves to update
05:26 - their exams even once a month they'll do
05:28 - this to me and people ask me are your
05:30 - exams out to date i'm like no they're
05:31 - just making my new changes if they do
05:33 - make major changes what they'll do is
05:35 - actually release a new version they'll
05:36 - call it the dp901 that's when you should
05:39 - be concerned about changes uh but uh
05:42 - yeah they do it frequently so what
05:43 - you'll do is if you get if you see red
05:45 - text you gotta scroll down to the real
05:46 - section because this is the old one and
05:48 - this is the new one where they make
05:49 - minor changes okay
05:51 - and what we'll do is work our way down
05:53 - here and and take a look here so
05:55 - describe core data concepts so batch and
05:58 - streaming relational data we have data
06:00 - linux concepts so visualizations bi
06:03 - types of bar like types of charts
06:05 - they're talking specifically about power
06:07 - bi because there's a lot of different
06:08 - kinds of visualizations describe
06:10 - analytic techniques so descriptive
06:11 - diagnostic predictive prescription
06:13 - cognitive most places don't describe
06:16 - cognitive so they add that additional
06:17 - one there elt and etl so um uh extract
06:22 - elt is more common for cloud so that's
06:24 - the one you really want to understand
06:26 - describe concepts of data processing
06:29 - onto how to work with relational data so
06:31 - describe what a relational workload is
06:32 - describe the structures within a
06:34 - database if you've ever worked with any
06:35 - kind of database you already know them
06:36 - tables indexes views columns et cetera
06:39 - we have described relational data
06:41 - services so they do p-a-s-i-a-s and
06:43 - s-a-s they're specifically talking about
06:46 - in that comparison you'll see that like
06:48 - in the az 900 uh those three though i
06:51 - think they might have removed them as of
06:52 - recent but um they're specifically
06:55 - talking about the azure sql family so
06:57 - down below here you have azure sql and
06:59 - underneath it has a bunch of
07:01 - variants so like sql database sql manage
07:04 - instance and virtual machines and these
07:05 - sit
07:06 - in the pas and iis and you have to know
07:08 - that okay
07:10 - describe azure synapse analytics
07:11 - describe database for postgres mario by
07:14 - sql that's the open source sql databases
07:17 - uh so then we have relational data so uh
07:20 - provisioning deployment of relational
07:21 - data
07:22 - deployment with the portal azure
07:24 - resource templates powershell cli you
07:27 - know what i don't ever see these on the
07:28 - exam you know but um you know they they
07:31 - have this in here okay
07:33 - identify security components now they
07:34 - say firewall they're actually talking
07:36 - about when you have a database because
07:38 - there's actually azure firewall but then
07:40 - there's um a a server firewall built
07:43 - into azure sql and that's what they're
07:45 - talking about there authentication like
07:46 - how to connect because there's a few
07:48 - different ways you can do that
07:49 - connectivity from on-prem to azure vnets
07:52 - etc identity query tools so azure data
07:54 - studio sql studio management sql sql cmd
07:59 - utilities and things like that describe
08:00 - query techniques for using uh sql so
08:03 - compare ddls with dmls there's actually
08:06 - a lot more types of um
08:08 - data language files for sql so we do
08:11 - them all just because it's you know it's
08:14 - the proper way to do it but they only
08:15 - care about these two
08:16 - query relational data in sql database uh
08:19 - azure disk postgres azure database for
08:21 - mysql we'll go on down to this section
08:23 - here non-relational data workloads so
08:25 - describe the non-relational data
08:26 - describe non-relational and nosql data
08:28 - recommend correct data stored determine
08:31 - when to use non-relational data and
08:32 - they're all talking about um
08:35 - because like non-relational data is
08:36 - mostly cosmodb a cosmodb like has a
08:39 - bunch of different sql engines there so
08:40 - that's going to help you understand that
08:41 - like graph and document key value store
08:44 - things like that describe non-relational
08:46 - data so we have table blob files cosmodb
08:50 - identify basic management tasks for
08:52 - non-relational data so provisioning
08:54 - deployment of non-relational data
08:55 - services describe method of deployment
08:57 - in uh azure portal et cetera et cetera
09:00 - again i don't see these a lot on the
09:02 - exam so i don't know why this is in here
09:04 - identify data security components um so
09:07 - it's the same thing as before it's just
09:08 - for non-relational data basic
09:10 - connectivity issues v-nets etc identify
09:12 - management tools for non-relational data
09:15 - um
09:16 - describe analytical workloads so
09:18 - transactional data the difference
09:19 - between transactional analytical and
09:21 - we're talking about olap and oltp okay
09:25 - difference between batch and real time
09:27 - uh warehouses data warehouse solutions
09:30 - describe modern data warehouses so here
09:32 - we're talking about data bricks hadoop
09:34 - systems
09:35 - synapses is kind of like a data lake
09:37 - house and then actually azure data lake
09:40 - and it's storage medium
09:42 - down below we have data ingestion
09:44 - so loading data azure data factory um hd
09:47 - insights databricks etc and then down
09:50 - with below we have a whole section on
09:51 - power bi what i'm surprised is they
09:53 - don't have much like on azure um
09:56 - streaming analytics and event hub
09:58 - because a lot of these have to consume
09:59 - from there so that's something i think
10:01 - that should be on the example they don't
10:02 - have it in here but yeah it's uh it's
10:04 - not a super hard exam it's mostly just
10:06 - describe describe identify see so you're
10:08 - all going to be in great shape and i
10:09 - hope that helps you out and we're on to
10:11 - the actual course now
10:12 - [Music]
10:16 - hey this is andrew brown from exam pro
10:18 - and we are taking a quick look at all
10:20 - the core data related azure services
10:21 - that we are likely to encounter through
10:23 - this course so let's get to it the first
10:25 - starting with azure storage accounts and
10:26 - this is an umbrella service for various
10:28 - storage types such as tables files and
10:29 - blobs we have azure blob storage and
10:32 - this is a
10:33 - data store that stores things as objects
10:36 - instead of files and the advantage here
10:37 - is that you get distributed storage
10:39 - these objects can span multiple machines
10:41 - for unstructured data you have azure
10:43 - tables which is a key value no school
10:45 - data store more like a database but it's
10:47 - under azure storage accounts and it's
10:49 - intended for its simpler projects you
10:51 - have azure files and this is a managed
10:53 - file share for nfs or smb so if you need
10:56 - a file share or file system that you
10:57 - need to mount to multiple virtual
10:59 - machines or workstation
11:00 - this is what you would use you have
11:02 - azure storage explorer this is a
11:05 - standalone application you download to
11:06 - your windows linux or mac machine that
11:09 - easily allows you to explore
11:11 - the various services above then you have
11:14 - azure synapse
11:15 - analytics this is a data warehouse and
11:17 - unified analytics platform the service
11:19 - used to be called something like azure
11:21 - warehouse but they added analytics on
11:23 - top of it kind of making it into a lake
11:25 - house service and so that's what it is
11:27 - now we have cosmodb this is a fully
11:30 - managed nosql database service that can
11:32 - host various nosql engines such as azure
11:35 - tables documents key value and graph
11:37 - when you use cosmodb it's going to have
11:39 - a core mode and that pretty much is its
11:42 - documents engine so a lot of times when
11:44 - we talk about cosmodb we just think of
11:46 - it as a documents
11:48 - documents database but it can actually
11:50 - have a variety underneath you have azure
11:53 - data lake store generation two we won't
11:55 - talk about gen 1 because it's just not
11:57 - really in use anymore but this is a
11:58 - centralized data repository for big data
12:00 - blob storage designed for vast amounts
12:02 - of data it actually is just azure blob
12:04 - storage with an additional layer of
12:06 - management you have azure data analytics
12:09 - this is a big data as a service you can
12:11 - write usql to return data from your
12:14 - azure data lake then you have azure data
12:16 - box which isn't really covering the exam
12:18 - but i'm including here because i think
12:20 - it's a great addition so you can import
12:22 - and export terabytes of data via hard
12:24 - drive you mail into the azure data
12:26 - center onto our next page here we have
12:29 - sql server for azure virtual machines
12:31 - this is when you need an sql server
12:33 - where you're migrating an existing sql
12:35 - from your on-premise data center onto
12:37 - azure but you can't afford to make any
12:39 - changes so you're literally taking the
12:41 - vm and lifting and then shifting it onto
12:43 - azure but you get to have access to the
12:46 - virtual machine underneath so you can
12:48 - control the os access
12:50 - layer and also if you already have an
12:53 - existing license it's a great solution
12:54 - for that as well if you are doing a lift
12:56 - and shift but you don't need to manage
12:58 - the virtual machine and you want azure
13:00 - to do all the work for you you have sql
13:02 - managed instances then you have azure
13:04 - sql which is the fully managed mssql
13:07 - database then you have azure databases
13:09 - for mariodb postgres and mysql you have
13:12 - azure cache for redis now this is an
13:15 - in-memory data store for returning data
13:16 - extremely fast but is also extremely
13:18 - volatile and this isn't covered on the
13:21 - exam but i like to include it because i
13:23 - think it's just one of the data services
13:25 - that's important you have microsoft
13:27 - office 365 sharepoint not really covered
13:29 - on the exam but you will hear it
13:31 - mentioned throughout
13:33 - the course content and you know i think
13:35 - that if you haven't had exposure to you
13:37 - should know what it is it is a shared
13:38 - file system for organizations the
13:40 - company owns all the files and applies
13:42 - fine-grained role-based access controls
13:44 - you have azure data bricks this is a
13:46 - third-party provider partnered with
13:48 - azure specializing in
13:50 - apache spark to provide very fast etl
13:52 - jobs as well as ml and streaming
13:55 - you have microsoft power bi this is a
13:58 - business intelligence tool used to
13:59 - create dashboards and interactive
14:01 - reports to empower business decisions we
14:03 - have hdinsights this is a fully managed
14:05 - hadoop system that can run many open
14:07 - source big data engines for doing data
14:09 - transformations for streaming etl elt
14:13 - we have azure data studio this is an ide
14:16 - that looks very much like visual studio
14:19 - code but designed around data related
14:20 - tasks across platforms similar to sss
14:24 - ssis
14:26 - but broader data workloads you have
14:28 - azure data factory a managed etl elt
14:31 - pipeline builder easily build
14:33 - transformation pipelines via a web
14:34 - interface within azure and then you have
14:37 - sql
14:38 - server integration services ssis it's a
14:41 - standalone windows app to prepare data
14:42 - for sql workloads via transformation
14:44 - pipelines there's probably a bunch of
14:46 - other little services or tools that we
14:49 - don't have in this list but don't worry
14:50 - we'll cover them throughout the course
14:52 - just remember these ones that we went
14:53 - over here today
14:59 - hey this is andrew brown from exam pro
15:00 - and we're taking a look at the types of
15:02 - cloud computing for azure data related
15:04 - services starting at the top of our
15:06 - pyramid is software as a service and
15:08 - it's a product that is run and managed
15:09 - by a service provider so you do not
15:11 - worry about how the service is
15:12 - maintained it just works and remains
15:14 - available and this is specifically
15:15 - designed for customers so uh it's not
15:18 - particularly azure services but it'll be
15:20 - like microsoft based services like power
15:22 - bi or the office 365 suite is going to
15:26 - be software as a service going down to
15:28 - platform as a service this focuses on
15:30 - deployment and management of your apps
15:32 - so you do not worry about provisioning
15:34 - and configuring or understanding the
15:35 - hardware or os layer and this is
15:38 - specifically for developers so we would
15:39 - put hdinsights azure sql cosmodb
15:43 - managed sql
15:45 - all right and at the bottom we have
15:47 - infrastructure as a service these are
15:48 - the basic building blocks for cloud it
15:50 - it provides access to networking
15:52 - features computers and data storage
15:53 - space you do not worry about the it
15:56 - staff data centers and hardware and
15:58 - underneath here we would have this would
15:59 - be for admins but we'd have azure disks
16:01 - virtual machines sql vms and honestly
16:05 - you know like when you look at aws and
16:07 - azure
16:08 - these kind of
16:09 - categories are defined a little bit
16:11 - differently so
16:12 - you know like manage sql i would
16:14 - probably put that infrastructure as a
16:15 - service but
16:16 - azure says that it goes into the mid
16:18 - tier i really want to pull up a
16:20 - particular document here that i think is
16:21 - important
16:22 - because this is all about the azure sql
16:24 - family and they're specifically
16:26 - categorizing these in particular so you
16:28 - go down below here and when we're
16:30 - looking i have them on here but when you
16:32 - look here i have sql vm down below so
16:35 - that would be considered infrastructure
16:36 - as a service you have managed sql where
16:38 - they put in the middle but they
16:40 - categorize it as platform as a service
16:42 - and then you have it's covered up here
16:45 - we have azure sql database which is
16:46 - platform as a service
16:48 - i'm showing you this because they might
16:50 - ask you this question on the exam uh and
16:52 - so i just wanted to point that out to
16:54 - you there but there you go
16:56 - [Music]
17:00 - okay so let's take a quick look at three
17:02 - azure data roles that azure wants you to
17:05 - know about specifically related to data
17:07 - services when i say roles here i don't
17:09 - mean like azure permissions i actually
17:12 - mean like jobs that people would do
17:14 - within azure
17:16 - and let's just take a look at it here so
17:18 - the first one is database administrator
17:20 - this is somebody that would configure
17:21 - and maintain a database such as azure
17:24 - data services or sql servers and they
17:26 - would be responsible for database
17:28 - management management security or
17:30 - granting users access backups monitoring
17:32 - performance and common tools that they
17:34 - would use would be azure data studio sql
17:37 - management studio azure portal the cli
17:40 - the next role would be data engineer and
17:42 - that would be to design and implement
17:44 - data tasks related to the transfer and
17:46 - storage of big data responsibilities
17:48 - here would be database pipelines in
17:49 - process data ingestion storage prepare
17:52 - data for analytics prepare data for
17:54 - analytics processing and common tools
17:57 - that they would use would be azure
17:58 - synapses studio sql azure cli and the
18:02 - last role here we have is data analyst
18:05 - so this is analyzes business data to
18:07 - reveal important information so you have
18:09 - provide insights into data visual
18:11 - reporting modeling data for analysis
18:14 - combines data for visualization analysis
18:16 - common tools here are power bi desktop
18:19 - power bi portal power bi service and
18:22 - power bi report builder so i just want
18:25 - you to know that there's definitely a
18:26 - lot more roles than just these three
18:28 - here but this kind of helps you narrow
18:30 - down what this entire dp900 is focused
18:33 - on which are these three kind of rules
18:35 - here but what we'll do is we'll jump
18:37 - into these common tools and just talk
18:38 - about them a little bit more in detail
18:40 - uh next here okay
18:42 - [Music]
18:46 - okay so we're taking a look here at
18:48 - database administrator common tools the
18:50 - first being azure data studios this
18:52 - allows you to connect to azure sql
18:54 - data warehouses post postgres sql sql
18:57 - servers big data cluster on premise i
18:59 - say azure sql data warehouse it must be
19:01 - azure synopsis analytics i probably just
19:04 - wrote that incorrectly there but various
19:06 - libraries and extensions along with
19:07 - automation tools a graphical interface
19:09 - for managing on-premise and cloud-based
19:11 - data services runs on windows max and
19:14 - linux
19:15 - possible replacement for ssms but still
19:18 - lacks some of those features if you
19:20 - launch the service it looks a lot like
19:22 - visual studio code because it probably
19:24 - is but it's specifically for data
19:26 - related tasks so if you're used to
19:27 - visual studio code you're going to be at
19:29 - home with the service you have sql
19:31 - server management studio ssms and it's
19:34 - an automation tooling for running sql
19:36 - commands or common database operations
19:37 - it has a graphical interface for
19:39 - managing on-premise and cloud-based data
19:41 - services but it only runs on windows so
19:44 - if you're on mac or linux you're going
19:45 - to be using azure data studio
19:48 - and if you're on windows you might just
19:49 - have both of these installed because
19:51 - there's just some things you can do in
19:52 - ssms that are just a lot easier than
19:54 - azure data studio but it's more mature
19:57 - than azure data studio so you know it's
19:59 - just going to be the features are going
20:00 - to be a lot more richer in partic in
20:02 - particular for sql you have azure portal
20:05 - and cli so here you can manage sql
20:07 - database configuration so you can create
20:08 - delete resize the number of cores uh you
20:11 - can manage and provision other data
20:14 - azure data services automate the
20:15 - creating updating or modifying resources
20:17 - via the azure resource manager templates
20:20 - which is infrastructure as code so those
20:22 - are the three
20:24 - major ones that a database administrator
20:26 - is going to be working with
20:27 - [Music]
20:31 - now let's take a look at data
20:32 - engineering common tools so at the top
20:34 - we have azure synapses studio so you
20:36 - know azure synapses analytics when you
20:38 - click into it there you launch a studio
20:40 - and this allows you to manage things
20:42 - like your data factories
20:44 - your warehouses sql pools spark pools
20:48 - things like that you're gonna have to
20:49 - know really really no sql there's tsql
20:52 - usql
20:54 - synapsis sql there's all sorts of sqls
20:58 - within azure so it's definitely
20:59 - something you want to learn for the
21:00 - azure cli you'll have to be able to use
21:03 - it to then
21:05 - execute sql commands because once you
21:06 - connect to an sql server via the cli
21:10 - you're just going to be writing sql from
21:11 - there
21:12 - i added these they weren't in the common
21:14 - tools list prior but i just added them
21:16 - now because i thought they were useful
21:17 - so hd insights which would have for
21:19 - streaming data via apache kafka or
21:22 - apache spark or applying etl jobs via
21:24 - hive pig and apache spark as your data
21:26 - bricks also because here you could be
21:29 - creating an apache spark
21:31 - cluster and using that to do etls or
21:33 - streaming jobs to your data warehouses
21:34 - your data lakes and of course you'd be
21:36 - working with blob storage and data likes
21:39 - as well so they should be on the list
21:40 - here
21:41 - but just there you go
21:43 - [Music]
21:47 - all right taking a look here at data
21:48 - analysts for common tools we have power
21:50 - bi desktop this is a standalone
21:51 - application for data visualization you
21:53 - can do data modeling here connect to
21:55 - many data sources and create interactive
21:57 - reports then you have power bi portal or
22:00 - also known as the power bi service and
22:02 - really this is just intended for
22:03 - creating interactive dashboards you can
22:04 - definitely do other things here but this
22:06 - is what i know it for then you have
22:08 - power bi builder report
22:10 - or report builder this is another
22:12 - standalone application and this allows
22:14 - you to create paginated reports which
22:15 - are just printable reports definitely
22:17 - there are more tools
22:19 - than just these three for uh data
22:21 - analysts but this is what azure wants
22:23 - you to know uh so there you go
22:25 - [Music]
22:29 - hey this is andrew brown from exam pro
22:31 - and we are looking at the data overview
22:34 - so the idea here is that we're going to
22:36 - be covering a lot of fundamental
22:38 - not necessarily as your specific data
22:40 - related knowledge that you need to know
22:42 - to really understand how to use azure
22:44 - data related services uh so i'll just
22:46 - give you a quick overview here and then
22:47 - we'll dive deeper into all these things
22:49 - so the first thing is data so that's
22:50 - units of information you have data
22:52 - documents these are types of abstract
22:54 - groupings of data you have data sets
22:56 - these are unstructured logical grouping
22:57 - of data when you structure your data now
22:59 - it's called structured data and then you
23:02 - have data types these are single units
23:04 - of data that are intended to be used in
23:06 - a particular way then you have a bunch
23:08 - of loose concepts so you have batch and
23:10 - streaming so this is how do we move our
23:11 - data around we have relational
23:13 - non-relational
23:14 - databases or data so how do we access
23:17 - query and search our data you have data
23:19 - modeling how do we prepare and design
23:20 - our data schema versus schemas how do we
23:23 - structure our data for search data
23:25 - integrity and data corruption how do we
23:27 - trust our data normalization and
23:29 - denormalization how do we trade
23:31 - quality versus speed and i'm sure we
23:33 - cover a lot more than just this list i
23:35 - just didn't feel like
23:36 - being exhausted here and put every
23:38 - little thing here so many things that
23:40 - have the word data in it but let's jump
23:42 - into it
23:43 - [Music]
23:46 - so the first question we should be
23:48 - asking ourselves is what is data so data
23:51 - is units of information that could be in
23:53 - the form of numbers text machine code
23:55 - images videos audio or even in a
23:58 - physical form like handwriting maybe if
24:01 - you're in the future it's crystals i
24:02 - don't know and so just some images here
24:05 - or
24:06 - examples of graphics so here we have an
24:08 - image here is a bunch of binary code or
24:11 - machine code
24:12 - we have a book
24:14 - here we have uh
24:16 - like audio so we have like audio
24:18 - spectrum here uh and then you have
24:20 - mathematical formulas so i'm sure at
24:22 - this point you know what uh data is but
24:24 - just in case uh you know you need to
24:26 - just broaden your your thoughts of what
24:27 - data is it really
24:29 - could be everything including physical
24:30 - stuff
24:35 - so what is a data document a data
24:38 - document defines the collective form in
24:40 - which data exists so common types of
24:42 - data documents would be data sets which
24:45 - is a logical grouping of data databases
24:47 - which is structured data that can be
24:48 - quickly accessed and searched data
24:51 - stores this is unstructured or
24:52 - semi-structured data
24:54 - for housing data data warehouses
24:57 - structured or semi-structured data for
24:59 - creating reports and analytics notebooks
25:01 - data that is arranged in pages and
25:03 - designed for easy consumption so just to
25:06 - give you some examples if we're looking
25:07 - at data sets we might talk about the
25:09 - mnist data set
25:11 - for azure sql that would be a database
25:14 - for a data store such as a data lake we
25:16 - have azure data lake
25:18 - for a data warehouse we have azure
25:20 - synapsis analytics and for notebooks we
25:23 - might talk about jupyter notebooks but
25:25 - this could also include you know an
25:26 - actual handwritten notebook so there you
25:29 - go
25:29 - [Music]
25:34 - all right so what is a data set well a
25:36 - data set is a logical grouping of units
25:38 - of data that are generally closely
25:39 - related or share the same data structure
25:42 - and so i say data structure there but i
25:44 - just want you to know that just because
25:45 - something has a data structure doesn't
25:46 - mean it's structured data it can be
25:48 - semi-structured like json objects or xml
25:51 - files
25:52 - but generally data sets are unstructured
25:54 - or structured they are publicly there
25:56 - are publicly available data sets that
25:58 - are used for learning statistics data
26:00 - analytics and machine learning the most
26:02 - popular one being the msns database i
26:04 - can't tell you how many times i've seen
26:05 - this data set
26:07 - but it's images of handwritten digits
26:08 - used to test classification clustering
26:10 - and image processing algorithms commonly
26:12 - used when learning how to build computer
26:15 - vision ml models to translate
26:16 - handwritten text into digital text and
26:19 - here's an example of that data set there
26:21 - another
26:22 - very popular data set is the commons
26:24 - objects in context the coco data set i
26:27 - believe microsoft had a hand in this one
26:29 - and it's a data set which contains many
26:30 - common images in a json file a coco
26:33 - format that identify objects or segments
26:36 - within an image so there again it has
26:37 - json files that means that it's
26:39 - semi-structured not necessarily
26:40 - structured data here is an example of
26:43 - the data set you can see that there are
26:44 - images and they have borders drawn
26:47 - around things that are trying to
26:48 - identify in the images such as objects
26:50 - and segmentation
26:53 - recognizing context super pixel stuff
26:55 - segmentation no idea what that means uh
26:58 - 329 000 images and it has a lot of
27:00 - labels and a bunch of other things in
27:03 - this data set
27:04 - another interesting one would be the
27:06 - imdb review data set where it has 25 000
27:09 - highly polar movie reviews meaning
27:11 - people really like the movie or they
27:13 - really dislike them i pulled out an
27:14 - example here of pluto nash which at the
27:16 - time was highly uh liked and disliked
27:19 - there was a huge split between this
27:20 - movie mostly i think people disliked it
27:23 - but apparently it's kind of split this
27:25 - is great for customer seg
27:28 - sentiment analysis again you'll probably
27:29 - be using ml for this but the idea is to
27:32 - say did people like the movies like or
27:34 - did they like it hate it uh we're sad
27:36 - about it that's kind of like customer
27:38 - sentiment right how did they feel
27:40 - some other data sets we have the free
27:42 - music archive this is a data set of
27:44 - musical uh music
27:45 - tracks so you have a hundred thousand
27:48 - tracks across 163 genres you have the uh
27:52 - li i guess this is library but lib re
27:54 - speech a data set of a thousand hours of
27:57 - english speech you think they use an
27:58 - english word to describe that but no
28:00 - there are many more data sets online
28:03 - some are paid some you have to extract
28:05 - via an apis uh some you have to scrape
28:08 - the data yourself
28:09 - um but just taking a big look here
28:11 - of the full list we got crunchbase
28:13 - glassdoor
28:14 - fbi
28:16 - google trends data hub
28:18 - world health organization it's across
28:20 - the board right so
28:22 - you know there's a lot of things that
28:24 - you can get data online a lot of times
28:26 - you are just creating your own data sets
28:27 - but it's good to know that there's a lot
28:29 - of stuff out there
28:30 - [Music]
28:34 - so what is a data type it's a single
28:36 - unit of data that tells the compiler or
28:38 - interpreter so a computer program how
28:41 - data is intended to be used and the
28:43 - variety of data types will greatly vary
28:45 - based on the computer program a better
28:47 - way of thinking of it instead of saying
28:48 - computer program just say programming
28:50 - language because that's where you're
28:52 - going to be encountering data types
28:54 - so let's take a look at the most common
28:55 - data types the first being numeric data
28:58 - types these are data types involving
29:00 - mathematical numbers the most common
29:02 - being integer which is a whole number
29:04 - could be negative or positive a lot of
29:06 - programming languages will have a
29:08 - variety of these that have different
29:09 - ranges like int 32 into 64 what have you
29:13 - then you have floats or sometimes also
29:16 - known as decimals this is a number that
29:17 - has a decimal so 1.5 0.0 can be a
29:21 - negative number as well here it is
29:22 - example of an end and a float in python
29:26 - we'll take a look at text data types
29:28 - this is a data type that contains a
29:30 - readable or non-readable letters so
29:33 - there are characters so characters is a
29:34 - single letter so it could be a to z it
29:37 - could be a digit it could be a blank
29:39 - space punctuation special characters
29:42 - then you have a string and a string is a
29:45 - sequence of characters that can be word
29:47 - sentences or paragraphs they don't
29:48 - necessarily have to be words but you
29:50 - know that's usually what you're using
29:51 - them for so here's an example of a
29:53 - character and a string then you have
29:55 - composite data types and these contain
29:58 - cells of data that can be accessed via
29:59 - an index
30:00 - or a key so for example we have an array
30:03 - so that is a group of elements that
30:04 - contains the same data type that can be
30:06 - accessed via the index and position you
30:09 - have a hash commonly known as the
30:10 - dictionary and if you're using python
30:12 - you're probably known as the dictionary
30:13 - i know it's a hash because i like ruby
30:15 - so that's how i know it and it's a group
30:17 - of elements where a key can be used to
30:19 - retrieve a a value
30:21 - and the thing is is that composites are
30:23 - they overlap with data structures so
30:25 - when we talk about data structures you
30:27 - might say hey is an array in hash a data
30:29 - structure and yes it is but yes it's
30:31 - also a data type it just depends on the
30:33 - programming language and the constraints
30:35 - around those okay so i know those get a
30:37 - little bit confusing i just wanted to
30:38 - point that out but here's an example of
30:40 - again this is um python so we have an
30:43 - array and
30:45 - down below we have a python dictionary
30:47 - and if you use
30:48 - json or javascript yes uh you know a
30:50 - json object basically is a hash
30:53 - we've got one more here so we have
30:55 - binary data types this these are
30:56 - represented by a series of bits uh or
30:59 - bytes which either are zero or one so
31:02 - off and on so here's an example in
31:04 - python how to set up a byte um for
31:07 - billion values if we have true or false
31:10 - some languages represent billions as a
31:12 - zero or one so a lot of times when
31:14 - you're using like mysql true and false i
31:16 - think is zero and one in there sometimes
31:18 - it's a t or an f when you're using um
31:21 - a post sql it's going to be a t or an f
31:24 - uh and sometimes it's just true or false
31:26 - so in python it's just capital t true
31:28 - and actually i think they have a
31:29 - lowercase tree which is a different kind
31:30 - of object it's a bit confusing but i'm
31:32 - just saying there's some variants there
31:34 - then you have enumeration data types or
31:36 - sometimes known as enums and these are a
31:38 - group of constants unchangeable
31:40 - variables so for example diamond spade
31:42 - hearts and clubs they're all related
31:44 - because it's card groups
31:47 - so the idea here is the data type uh
31:49 - you know it could be also a data
31:51 - structure again it varies on the
31:52 - language just like composite types but
31:54 - here on the right hand side we have a
31:55 - shake and it's wrapped in a class and
31:58 - below this again this is python that's
32:00 - how you do in python and so the shape
32:02 - could be vanilla chocolate cookies and
32:03 - mint a lot of times the nums will map to
32:06 - an integer value or something so but not
32:09 - always the case um but yeah that's uh
32:12 - that type there and there you go that's
32:13 - the common data types
32:14 - [Music]
32:19 - okay so let's take a look at schema
32:20 - versus schema list so what is a schema a
32:23 - schema in terms of a database is a
32:24 - formal language which describes the
32:26 - structure of data a blueprint of a
32:28 - database and a schema can be
32:30 - can define many different data
32:31 - structures that serve different purposes
32:34 - of a database so
32:36 - different data structures in a
32:37 - relational database could be things like
32:38 - tables fields relationships views
32:40 - indexes packages procedures functions
32:42 - mxl schemas
32:44 - cues triggers types sequences
32:46 - materialized views cinnamons cinnamons
32:49 - synonyms can't say that word database
32:51 - links and directories gonna highly vary
32:54 - based on the database that you're using
32:56 - and i'm just going to show you an
32:57 - example of a schema so here is actually
33:00 - part of my schema for the xampro app and
33:03 - so this is a ruby on rail schema that
33:05 - defines the structure for a relational
33:07 - database and it's written in a dsl
33:09 - called ruby
33:10 - but the thing is is that this is going
33:12 - to highly vary uh based on again what
33:14 - you're using but just notice that you
33:16 - can see things like creating a table
33:18 - creating indexes creating columns for
33:20 - the database things like that
33:22 - adding extensions uh schema-less uh is
33:25 - just kind of it's still schema but the
33:28 - idea here is the primary cell of a
33:29 - database can accept many types so just
33:32 - going back here for a moment notice here
33:34 - that we have very particular notice here
33:37 - that we have very particular data types
33:38 - like integer string and stuff like that
33:40 - the idea here is with schema list that
33:43 - that data type is a lot more flexible
33:45 - and the idea there is it allows you to
33:46 - forego
33:47 - the upfront data modeling that you
33:49 - normally would have to do which is a lot
33:50 - of work and so that's one of the
33:52 - advantages of schema lists common
33:54 - stimulus databases would be key value
33:56 - document columns
33:58 - and then the subcategory of wide columns
34:00 - and graph and
34:02 - not a lot of information here but we
34:03 - will describe it in more detail when we
34:05 - talk about
34:06 - nosql databases okay
34:08 - [Music]
34:12 - all right let's talk about query and
34:14 - querying because those are terms that
34:16 - you're going to need to know because
34:17 - you're going to be doing quite a bit of
34:19 - it if you are going to have a career as
34:21 - a
34:21 - anything in data right like a data
34:23 - analyst so what is a query a query is a
34:25 - request for data results also known as
34:28 - reads
34:29 - uh or to perform operations such as
34:31 - inserting updating or deleting also
34:32 - known as writes and so a query can
34:35 - perform maintenance operations on the
34:36 - data and it's not always restricted to
34:38 - just working with the data that resides
34:40 - within the database and you'll see this
34:42 - where there's like there's commands to
34:45 - do analysis on your database and other
34:48 - things like that but here's an example
34:50 - of a query so what is a data result well
34:54 - results are data results is the results
34:57 - of the data returned from a query so
34:59 - here you generally will see tabular data
35:01 - that's usually what people want back but
35:03 - you know you can get back json or xml it
35:06 - really just depends on the database what
35:08 - is querying so this is the act of
35:09 - performing a query so the idea is that
35:12 - you write your query above and it's
35:14 - being sent as an as a request could be
35:16 - through an sdk cli the shell an api lots
35:20 - of ways for it to get there and then the
35:22 - idea is that the query is going to
35:23 - return the the data results so what is a
35:26 - query language that is a scripting
35:28 - language or programming language
35:29 - designed as the format to submit a
35:31 - request or actions to the database noble
35:34 - query languages is sql graph sql
35:37 - cousteau xpath gremlin and there's a lot
35:41 - of them but you know those are ones that
35:43 - stand out to me right now and just to
35:45 - note up here this is sql okay so i just
35:48 - didn't want to make a big old line this
35:50 - way here but this is sql here okay
35:53 - [Music]
35:57 - all right so let's compare batch and
35:59 - stream processing so batch processing is
36:01 - when you send batches a collection of
36:03 - data to be processed and batches are
36:05 - generally scheduled so you might say
36:07 - every day at 1 pm but you can also just
36:09 - queue up a batch whenever you feel like
36:10 - it batches are not real-time meaning
36:12 - that all the data is sent and then you
36:14 - wait until the batch is back to see the
36:16 - results batch processing is ideal for
36:19 - very large processing workloads batch
36:21 - processing is generally more cost
36:23 - efficient than stream processing and so
36:27 - here we just kind of have a
36:28 - representation so here we have our data
36:29 - we've broken into batches or collections
36:31 - we pass it to something like an etl
36:34 - engine
36:35 - and it will transfer the data and then
36:36 - we'll insert into our database data
36:38 - warehouse data store data lake house
36:41 - wherever you want to put it then we have
36:43 - stream processing so this is when you
36:45 - process data as soon as it arrives
36:47 - you'll have producers which will send
36:48 - data to a stream and consumers which
36:51 - will pull data from a stream a lot of
36:53 - times the stream will look like a
36:54 - pipeline and data can be held in that
36:56 - stream for a period of time so you have
36:58 - a better reusability of data if you need
37:01 - it for multiple
37:03 - consumers
37:05 - stream processing is good for real-time
37:06 - analytics real-time processing like
37:08 - streaming videos anything that has to do
37:10 - with real time if you need it right away
37:12 - it for that purpose it is much more
37:14 - expensive than batch processing um and
37:17 - here's a visual representation where we
37:18 - have bits of our data they go into our
37:20 - stream pipeline that can be held there
37:22 - for a while
37:23 - and sometimes
37:25 - minor operations will be performed on it
37:27 - but consumers will pull the data and do
37:28 - what they want with that data if we want
37:30 - to contextualize these things in terms
37:32 - of services on azure the idea is you'd
37:35 - have your data sources and you'd ingest
37:37 - them into something like azure stream
37:39 - analytics or i didn't really make this
37:41 - graphic very good but the idea is that
37:43 - you go into stream analytics or maybe
37:45 - you go into hdinsights or maybe you go
37:49 - into azure synapse analytics or one of
37:53 - these intermediate steps and then
37:54 - eventually you go to power bi to make
37:55 - your visualization reports
37:57 - for
37:58 - stream processing
38:00 - you could use event hub so event hub is
38:03 - a uh a single topic uh streaming service
38:07 - and you could ingest that into azure
38:08 - stream analytics
38:10 - um it's funny because this is the stream
38:12 - analytics icon this is actually the
38:13 - hadoop icon up here it's got it mixed up
38:16 - but anyway you go into stream analytics
38:19 - and you can insert that into cosmodb and
38:21 - then maybe pull cosmodb into power bi
38:24 - but yeah that is the difference between
38:26 - the two okay
38:29 - [Music]
38:33 - all right let's talk about relational
38:34 - data and this has to do with tables and
38:36 - relationships between other tables so
38:39 - let's talk about what a table is it's a
38:40 - logical grouping of rows and columns so
38:42 - an excel spreadsheet is actually tabular
38:45 - data tabular data just means the data
38:47 - that makes use of table data structures
38:50 - okay then you have views which look a
38:52 - lot like tables except they are the
38:54 - result set so a table when you do a
38:56 - query you're returning back data and
38:59 - you're storing that queries data in
39:01 - memory and basically it's a temporary or
39:04 - virtual table
39:05 - then you have materialized views and
39:07 - it's the same thing as a view except the
39:09 - difference here instead of being stored
39:11 - in memory it's stored on disk but again
39:13 - it's the results of a table so it's a
39:15 - virtual table
39:16 - then you have indexes and this is a copy
39:19 - of your data sorted by one or more
39:21 - multiple columns for faster reads at the
39:23 - cost of storage so think of it kind of
39:25 - like a virtual table but it does include
39:27 - all the columns and it's it's just so
39:29 - it's just to help you understand what
39:31 - order to retrieve data
39:34 - you have constraints these are rules
39:36 - applied to rights that can ensure data
39:38 - integrity so like if you have a database
39:40 - and you want to make sure that there are
39:42 - no duplicate records you put a
39:43 - constraint for no duplicates things like
39:45 - that
39:46 - you have triggers this is a function
39:47 - that is is a trigger on a specific
39:50 - database event this is really useful
39:51 - let's say after you insert a
39:54 - a column in your database you want to
39:56 - have a uuid you'd have a function that
39:58 - would generate out the uuid
40:01 - then you have primary keys so one or
40:03 - more multiple columns that uniquely
40:04 - identify a table in the row the most
40:06 - common primary key is id
40:09 - a foreign key so a column which holds
40:11 - the value of a primary key from another
40:13 - key to establish a relationship
40:15 - very commonly it's just the name of the
40:17 - other table with underscore id so
40:19 - relationship is when two tables have a
40:21 - reference to one another to join the
40:23 - data together i know this text is really
40:26 - boring but we're just going to cover so
40:27 - much about relational database tables so
40:29 - i didn't feel that we needed a visual
40:30 - here
40:31 - but let's keep moving forward here with
40:33 - relational data
40:38 - so for relational data we have tables
40:40 - and then the relationship between the
40:41 - tables let's talk about those
40:42 - relationships so relational databases
40:44 - establish relationships to other tables
40:46 - via foreign keys referencing another's
40:49 - table's primary key so looking at the
40:50 - example here uh you know this is the
40:53 - primary there's a little icon here that
40:55 - shows you that it's the primary key so
40:56 - this is the primary key
40:58 - and over here in another table it's
41:00 - referencing
41:01 - a foreign key so that's the foreign key
41:03 - that's the primary key okay and there
41:06 - are four
41:07 - types of relationships between
41:09 - relational databases
41:11 - and their tables the first is one to one
41:13 - so imagine a monkey has a banana or here
41:15 - we have a table called country and it
41:18 - has a capital it's one to one then you
41:20 - have one to many so a store has many
41:22 - customers or you could say a book has
41:24 - many pages notice that this denotes the
41:27 - many here then you have many to many so
41:29 - a project that has many tasks and tasks
41:32 - can belong to many projects or here a
41:35 - book can have many authors and an author
41:37 - can have many books so there's many to
41:38 - many and then last is a variant on the
41:41 - many to many so and it's via a join or
41:44 - junction table i just call them join
41:45 - tables so a student has many classes
41:48 - through enrollments and a class has many
41:50 - students
41:50 - uh through enrollments so here it's the
41:53 - same thing a book can have many authors
41:55 - an author can have any books but it's
41:57 - all through a library so you could say a
41:59 - book has many
42:00 - authors through a library and an author
42:02 - has many
42:03 - books through a library okay so there
42:05 - you go
42:06 - [Music]
42:10 - okay so we know that
42:13 - relational databases store tabular data
42:15 - but the thing is that data can also be
42:17 - stored
42:18 - either in a row oriented way or a column
42:21 - oriented way and let's just talk about
42:23 - the differences there and why we would
42:25 - do that so the first case we have row
42:27 - store the data is organized into rows
42:29 - this is great for traditional relational
42:31 - databases
42:32 - which are row stores
42:34 - good for general purpose databases
42:37 - suited for online transaction processing
42:39 - oltp we are going to come back to that
42:42 - term later on great when needing all
42:45 - possible columns in a row
42:47 - which is important during a query not
42:49 - the best at analytics or massive amounts
42:51 - of data all right we're looking at
42:52 - column store data is organized into
42:54 - columns it's faster at aggregating
42:56 - values for analytics so ideas imagine
42:59 - that you want to
43:01 - count how many cities there are for
43:03 - millions of records if it's organized by
43:05 - column like querying based on column or
43:07 - data stored together as columns a lot
43:09 - faster generally these are no sql stores
43:12 - or esco-like databases it's a bit
43:14 - confusing because
43:16 - uh you know like you would think tableau
43:17 - data is just relational databases but
43:20 - when you want to do column store they're
43:21 - basically nosql stores
43:23 - so the term's a bit fuzzy there it's
43:25 - great for vast amounts of data when
43:27 - we're talking about massive amounts
43:28 - we're talking millions and millions of
43:29 - records terabytes worth of data okay
43:32 - suited for online analytical processing
43:35 - oltp
43:36 - great when you only need a few columns
43:38 - so you don't need to get data from all
43:40 - the columns
43:42 - and there you go
43:43 - [Music]
43:47 - let's talk about database indexes which
43:49 - is a data structure that improves the
43:51 - speed of reads from the database table
43:53 - by storing the same or partial redundant
43:55 - data organized in a more efficient
43:58 - logical order and the logical order is
44:00 - commonly determined by one or more
44:01 - columns such as sort keys they're always
44:04 - called that a common data structure for
44:06 - an index is a balanced tree uh
44:09 - and it's short for b tree not to be
44:11 - confused with binary tree which is
44:12 - something else
44:13 - so you might see b tree and be like okay
44:15 - that's how it's doing that
44:17 - uh so here we just have kind of a visual
44:20 - imagine you have a table or a foot like
44:21 - that's for a phone book and you want to
44:23 - quickly find people based on the phone
44:25 - number because maybe you're trying to
44:26 - find them based on the starting number
44:28 - being 344 or something so the idea is
44:30 - you make an index and you say i want
44:32 - this to index by the phone number and so
44:34 - what it's going to do is change the
44:36 - order there so it might just pull the id
44:38 - or the number and reorder it and so now
44:40 - what you'll do is you'll use that index
44:42 - and that index will use as a reference
44:44 - to determine so it's not storing all the
44:45 - data but it will use that as a reference
44:48 - to the original table to quickly return
44:49 - your data
44:51 - and so here's a very easy way to create
44:53 - an index in postgres so we'd say create
44:55 - index and then we give it a unique name
44:56 - and we'd say
44:58 - let's make it index
44:59 - on the um for our addresses but just on
45:02 - the phone number so there you go
45:04 - [Music]
45:08 - let's take a look here at data integrity
45:10 - versus data corruption so data integrity
45:12 - is the maintenance insurance of data
45:13 - accuracy and consistency over its entire
45:16 - life cycle and it's often used as a
45:19 - proxy term for data quality now you
45:21 - might think it's data validation but
45:23 - it's just a prerequisite of data
45:25 - integrity because again data integrity
45:27 - is all about the entire life cycle
45:28 - making sure over all that it's going to
45:31 - stay consistent so validation is just
45:33 - one part of it the goal of data
45:34 - integrity is to ensure data is recorded
45:36 - exactly as intended data integrity is
45:39 - the opposite of data corruption so data
45:41 - corruption is the act or state of data
45:43 - not being in the intended state result
45:46 - in data loss or misinformation and data
45:48 - corruption occurs when unintended
45:49 - changes result when reading writing and
45:52 - so in the case when you're doing reads
45:53 - and writes maybe you have a hardware
45:54 - failure
45:55 - somebody just inputs the wrong data or
45:58 - someone intentionally is being malicious
46:00 - to corrupt your data or there's
46:02 - unforeseen side effects for operator
46:03 - operations via computer code so you
46:06 - wrote code and you didn't know that it
46:07 - was doing something that it wasn't
46:08 - supposed to be doing so how do we ensure
46:10 - data integrity well we have a
46:12 - well-defined and documented data
46:14 - modeling so data modeling if you know
46:16 - exactly how your data is supposed to be
46:17 - and it doesn't match the model there
46:19 - then you'll know logical constraints on
46:21 - your database items so we talked about
46:23 - that when we
46:24 - talked about all the types of relational
46:26 - data so constraints will keep that data
46:28 - integrity in place redundant and
46:29 - versions of your data to compare and
46:31 - restore
46:32 - so you have to be able to um not just
46:34 - validate your data but be able to bring
46:36 - it back to the state that it's supposed
46:37 - to be human analysis of the data so you
46:40 - know that's where data analysis will
46:42 - just check periodically uh hash
46:44 - functions to determine if changes have
46:45 - been tampered with you see this quite
46:47 - often when you're downloading uh open
46:49 - source software or software off like
46:51 - soft soft pedia where you can have an
46:53 - md5 hash to say did the thing i download
46:55 - match the thing that was expected
46:58 - uh principle of least privileges so
46:59 - limiting access to specific actions for
47:01 - specific user roles will mitigate uh
47:04 - problems that are unexpected with your
47:06 - data so all that stuff uh makes up data
47:08 - integrity okay
47:10 - [Music]
47:13 - okay it's time to compare normalized
47:15 - versus denormalized data so normalize is
47:17 - a schema designed to store non-redundant
47:20 - and consistent data whereas denormalize
47:23 - is a schema that combines data so that
47:25 - accessing data or querying it is very
47:27 - very fast so when we see tables and
47:30 - relationships like a relational table
47:32 - where everything is
47:33 - very discreetly
47:35 - organized this is normalized data and
47:38 - then on the right hand side where you
47:39 - could take all those tables on the
47:40 - right-hand side and make them one table
47:43 - this would be extremely efficient so the
47:46 - left-hand side for normalized data
47:47 - integrity is maintained little to no
47:49 - redundant data
47:51 - many tables optimize for storage of data
47:54 - on the right hand side we have data
47:55 - tegrity is not necessarily maintained or
47:57 - there's not good controls in place you
47:59 - have to do extra work to make sure it is
48:01 - in good shape redundant data is common
48:03 - fewer tables excessive data storage is
48:06 - less optimal now when you're using
48:07 - relational databases you can use both
48:09 - normalized and denormalized
48:12 - schemas and when you are using nosql
48:14 - it's a little bit harder but like
48:16 - there's cases where you can kind of
48:17 - model things like tables but generally
48:19 - data is denormalized
48:21 - in nosql so there's a bit more challenge
48:23 - with data integrity but the the upside
48:26 - is you get a lot more performance right
48:28 - so it's just way way faster at scale
48:33 - [Music]
48:35 - a pivot table is a table of statistics
48:37 - that summarizes the data of more
48:38 - extensive tables from a database
48:40 - spreadsheet or business intelligence
48:41 - tool and pivot tables are a technique in
48:44 - data processing they arrange or
48:46 - rearrange so pivot statistics in order
48:48 - to draw attention to useful information
48:50 - and this leads to finding figures and
48:52 - facts quickly making them integral to
48:55 - data analysis so when you're looking at
48:56 - microsoft excel it's very easy to create
48:59 - pivot tables think of a pivot table as
49:01 - an interactive report where you can
49:02 - quickly aggregate or group your data
49:05 - based on various factors so maybe you're
49:07 - grouping it by year month week or day
49:09 - some average min or max
49:11 - and so over here i have an example of a
49:13 - pivot table excel i got this from excel
49:15 - jet which they actually have really good
49:17 - information about
49:19 - pivot tables an example so if you think
49:21 - that you want to learn more about this i
49:22 - would go check out that resource but
49:24 - here you what you can see is that we
49:26 - have a table and notice that it has
49:28 - these little filters at the top right
49:30 - and so the idea is you can drop that
49:31 - down and say sort by date and and other
49:34 - stuff and what that you can do is create
49:37 - here is another pivot table here where
49:38 - we said okay let's sum the sales based
49:41 - on blue and green so this is a pivot
49:43 - table and then created another pivot
49:45 - table okay uh and so um you know it
49:48 - becomes very useful tool in excel all
49:50 - right
49:51 - and just one more thing pivot tables
49:53 - used to be a trademarked word owned by
49:55 - microsoft so a lot of times pivot tables
49:57 - were specifically just in excel or
49:59 - their uh was it their microsoft access
50:01 - database but now pivot tables is a
50:03 - unique term or a general term that
50:05 - everybody uses just for this kind of
50:07 - operations
50:08 - [Music]
50:12 - let's talk about data consistency and
50:14 - this is when data is being kept in two
50:16 - different places and whether the data
50:18 - exactly matches or does not match so
50:21 - when you have to duplicate data
50:23 - in many places and you need to keep them
50:25 - up to date to be exactly matching based
50:27 - on how the data is transmitted and
50:29 - service level the service levels of your
50:31 - cloud service provider they'll use these
50:33 - two terms and we'll hear strongly
50:34 - consistent and eventually consistent so
50:37 - strongly consistent means every time you
50:39 - request data so you query data you can
50:41 - expect consistent data be returned
50:44 - within x time so they might say within
50:46 - 10 milliseconds 100 milliseconds one
50:48 - second so the thing is we will never
50:51 - return to you old data but you will have
50:53 - to wait at least x amount of seconds for
50:55 - the query to return whatever that
50:57 - defined time is we talk about eventual
50:59 - consistency when you're when you request
51:01 - when you request data you may get back
51:04 - inconsistent data within x amount of
51:05 - periods so two seconds
51:07 - we are giving you whatever data is
51:09 - currently in the database you may get
51:11 - new data or old data but if you wait a
51:14 - little bit longer it will generally be
51:15 - up to date why would we have these two
51:17 - methods it just depends on your use case
51:18 - maybe you can tolerate some data to be
51:21 - inconsistent it's more important to get
51:23 - whatever data is available now and
51:24 - sometimes you need an absolute guarantee
51:27 - that the data is one to one okay so
51:29 - those are the two different ones
51:31 - [Music]
51:35 - so synchronous and asynchronous can
51:36 - refer to mechanisms of data
51:38 - transformation uh and data replications
51:41 - let's break these two down so
51:42 - synchronous is continuous streams i'm
51:45 - just going to mark that there continuous
51:47 - stream of data that is synchronized by a
51:49 - timer clock so you get a guarantee of
51:51 - time of when the data will be synced
51:54 - you can only access data once the
51:56 - transfer is complete you get guaranteed
51:58 - consistency of data returned it at the
52:00 - time of access slower access times so
52:03 - here is the data and if you're thinking
52:05 - about strongly consistent that is what
52:07 - this is it's this is going to be when
52:09 - things are strongly consistent then we
52:11 - have asynchronous so continuous stream
52:13 - of data separated by a start and stop
52:15 - bits no guarantee of time can access
52:17 - data anytime but may return older
52:19 - versions or empty placeholder
52:22 - faster access times no guarantee of
52:24 - consistency here it is so you see it's
52:26 - moving in bits right
52:28 - and so the idea is that we can access
52:30 - any time in between here to get maybe up
52:32 - to date data or not up to date data to
52:34 - solidify this let's put in some
52:36 - scenarios so a company has a primary
52:38 - database but they need to have a backup
52:39 - database in case their primary database
52:41 - fails the company cannot lose any data
52:43 - so everything must be in sync the
52:45 - database is not going to be accessed
52:47 - while it is standing by to act as a
52:50 - replacement so the reason why this works
52:52 - is that you know if you have a backup
52:54 - database you have to make sure all your
52:55 - data is one-to-one
52:57 - then on the other side here a company
52:58 - has a primary database but they want a
53:00 - read replica a copy of the database so
53:02 - their data analytics person can create
53:04 - computational intensive reports that do
53:06 - not impact the primary database it does
53:07 - not matter if the data is exactly
53:09 - one-to-one at the time of access because
53:11 - in this scenario it's like any time the
53:13 - database goes down
53:14 - you want it up to date to the second on
53:16 - this side it's like they might run
53:17 - reports once a day whatever so you know
53:20 - there's always new data coming in
53:22 - and b2 uh to burdensome to make sure
53:24 - that the data is always up to the second
53:26 - so there you go
53:31 - hey this is andrew brown from exam pro
53:33 - and we are looking at non-relational
53:34 - data and this is where we store data in
53:37 - a non-tabular form and will be optimized
53:39 - for different kinds of data structures
53:41 - so what kind of data structures well
53:43 - we're looking at key value stores so
53:45 - each value has a key design to scale
53:47 - only simple lookups then you have a
53:50 - document store so primary entity is
53:52 - json-like data structure called a
53:54 - document you have column restore
53:56 - sometimes this falls under relational
53:57 - databases but it is a non-relational
53:59 - data type or
54:01 - database so it has a table like
54:03 - structure but data is stored around
54:05 - columns instead of rows then you have
54:06 - the graph database where data is
54:08 - represented with nodes and structures
54:11 - where relationships really really do
54:12 - matter and so sometimes non-relational
54:14 - databases can be both a key value and
54:16 - document store like azure cosmo db or
54:18 - amazon dynamodb and the reason for that
54:21 - is that documents are actually a subset
54:23 - of key values which we'll talk about
54:25 - later when we get to that point
54:27 - [Music]
54:31 - hey it's andrew brown from exam pro and
54:32 - we're taking a look at data sources so a
54:34 - data source is where the data originates
54:38 - from so an analytics tool may be
54:40 - connected to various data sources to
54:42 - create a visualization report and a data
54:44 - source could be a data lake a data
54:46 - warehouse a data store a database a data
54:49 - requested on demand via an api endpoint
54:52 - from a web app
54:54 - and flat files such as excel or
54:56 - spreadsheet and so the example here is
54:58 - that we have a data source and somehow
55:01 - there has to be a connector between them
55:02 - and it's going to be consumed by either
55:04 - a warehouse an etl engine a data lake or
55:07 - bi tool those are common ones that need
55:09 - data sources
55:10 - so extracting data from data sources so
55:12 - a data tool like a business intelligence
55:15 - software would establish a connection to
55:16 - multiple data sources
55:18 - at the bi would extract data which could
55:21 - uh could be pulled data at the time of
55:23 - report or it could be pulled data on
55:25 - schedule or data could be streamed the
55:28 - mechanism for extracting data will vary
55:30 - per data source i just want you to know
55:31 - that because you know when you're using
55:33 - these services it does really vary on
55:35 - how it pulls the data so i just want you
55:37 - to understand there are a few different
55:38 - ways okay
55:39 - [Music]
55:43 - so a data store is a repository for
55:46 - persistently storing and managing
55:47 - collections of unstructured or
55:49 - semi-structured data so here i have kind
55:51 - of a visual where we have
55:53 - files going into some kind of store and
55:55 - a data store is a very very broad term
55:57 - so it's interchangeably used with
55:59 - databases though databases is
56:01 - technically a subset of a data store but
56:04 - generally a data store indicates working
56:06 - with unstructured or even
56:07 - semi-structured data so if somebody said
56:09 - a data store i'm thinking that it's
56:11 - either unstructured semi-structured okay
56:14 - a data store can be specialized in
56:15 - storing flat files
56:17 - emails maybe a database as we said it
56:21 - was a subset uh or designed to be
56:23 - distributed across many many machines
56:26 - or it could be a directory service okay
56:29 - so that's a data store
56:30 - [Music]
56:34 - so what is a database a database is a
56:36 - data store that stores semi-structured
56:38 - and structured data
56:40 - and but a better term i would say would
56:41 - be a databases more complex data store
56:44 - because it requires using formal design
56:46 - and modeling techniques databases can be
56:48 - generally categorized as either
56:49 - relational databases so this is
56:51 - structured data that strongly represents
56:53 - tabular data so tables roles and columns
56:55 - they're generally either row oriented or
56:58 - columnar oriented
57:00 - and when we talk about non-relational
57:02 - databases we're looking at
57:03 - semi-structured
57:04 - data that may or may not distantly
57:06 - resemble tabular data and i know that i
57:09 - put this one over on the relational side
57:12 - sometimes it ends up here or there just
57:14 - understand that that one kind of floats
57:16 - in between the two really depends on the
57:17 - technology underneath and so here is a
57:20 - pretty common way you get your sql you
57:22 - hit the database you get a table back
57:24 - right so the databases have a rich set
57:26 - of functionality specialized uh language
57:28 - to query so that is our sql here right
57:32 - we have specialized modeling strategies
57:33 - to optimize retrieval for different use
57:36 - cases more fine-tuned control over the
57:38 - transformations of the data into useful
57:39 - data structures or reports the thing
57:41 - here on the end normally a database
57:43 - infers someone is using a relational
57:45 - row-oriented data store so when somebody
57:48 - that's when someone says a database
57:49 - you're usually thinking like mysql sql
57:51 - postgres redb things like that okay
57:54 - [Music]
57:58 - so what is a data warehouse it's a
57:59 - relational data store designed for
58:00 - analytic workloads which is generally
58:02 - column oriented data store and again i'm
58:04 - going to make an emphasis here sometimes
58:06 - it's non-relational sometimes it's
58:07 - relational don't get too worried about
58:09 - that part okay companies will have
58:11 - terabytes and millions of rows of data
58:13 - and they need a fast way to be able to
58:15 - produce analytic reports that's how you
58:17 - know you need a data warehouse okay data
58:20 - warehouses are generally
58:22 - generally perform aggregations
58:24 - so aggregations is grouping of data
58:27 - to find a total or average data
58:29 - warehouses are optimized around columns
58:31 - since they need to quickly aggregate
58:33 - column data and so here is an example
58:36 - where we have our warehouse and
58:39 - the idea is that we're taking data in
58:41 - from like an unstructured source through
58:43 - an etl
58:44 - and then here we have an sql these are
58:46 - two different data sources we use sql to
58:49 - then get our results okay so data
58:51 - warehouses are generally designed to be
58:53 - hot hot meaning that the data will be
58:56 - returned very very fast even though they
58:59 - have vast amounts of data
59:01 - data warehouses are infrequently
59:03 - accessed meaning they aren't intended
59:04 - for real-time reporting but maybe once
59:06 - or twice a day or once a week to
59:08 - generate businesses and user reports now
59:10 - can it
59:11 - report extremely fast of course but it's
59:13 - not like at a at a per millisecond
59:15 - you're running it all day and keeping it
59:17 - all up to date that's more for a stream
59:19 - right a data warehouse needs to be needs
59:21 - to consume data from a relational
59:22 - database on a regular basis or you know
59:25 - through an etl data gets transformed
59:27 - input in there okay generally generally
59:30 - data warehouses are read only so you
59:32 - insert data and then you read it you're
59:33 - not using it for transactional data okay
59:37 - [Music]
59:41 - what is a data mart a data mart is a
59:42 - subset of a data warehouse a data mart
59:45 - will store generally under 100 gigabytes
59:47 - and has a single business focus and so a
59:50 - data mart allows different teams or
59:51 - departments to have control over their
59:53 - own data set for specific use cases so
59:55 - here we have a data warehouse and we are
59:58 - running queries to then put them in
60:00 - their own little data warehouses uh but
60:02 - the idea is that you know there are you
60:04 - know just smaller data sets that are
60:06 - more focused databars are generally
60:07 - designed to be read only because you're
60:08 - going to always have to pull data from
60:10 - the main data warehouse data marks also
60:12 - increase the frequency at which data can
60:14 - be accessed because of just smaller data
60:16 - sets
60:17 - and you don't have to worry about you
60:19 - know a huge cost because the larger the
60:22 - data set you have to query over the more
60:23 - expensive it gets right the cost of
60:25 - query is much much lower
60:27 - and so you might even see people
60:28 - accessing these a lot more frequently
60:31 - than they would a data warehouse so
60:32 - there you go
60:33 - [Music]
60:37 - so what is a data lake it is a
60:39 - centralized storage repository that
60:40 - holds vast amounts of raw data so big
60:42 - data in either semi-structured or
60:44 - unstructured format a data lake lets you
60:47 - store all your data without careful
60:48 - design or having to answer questions on
60:50 - the future use of the data so basically
60:52 - it's hoarding for data scientists here
60:55 - is kind of a visualization where you
60:56 - have a bunch of different data sources
60:58 - dropping into the data lake maybe you
61:00 - want to perform etls and put data back
61:02 - into the data lake and then you know you
61:04 - can extract out for reports ml all sorts
61:06 - of things we'll definitely cover data
61:07 - lakes when we get to the data lake
61:09 - section but uh this is the general
61:11 - overview here a daylight is commonly
61:12 - accessed for data workloads such as
61:14 - visualizations for bis tools real-time
61:16 - analytics machine learning on-premise
61:18 - data data lakes are great for data
61:20 - scientists but it's very hard to use
61:22 - data lakes for bi reporting so it's not
61:25 - that you can't do it it's just that
61:26 - there's additional steps and so there
61:27 - might be a different solution that might
61:29 - be a bit easier such as a data warehouse
61:31 - if data lakes are not well maintained
61:33 - they can become data swamps which is
61:35 - basically like data corruption right so
61:38 - yeah there you go
61:39 - [Music]
61:43 - what is a data lake house well a data
61:45 - lake house combines the best elements of
61:46 - a data lake and data warehouse and this
61:49 - isn't something that azure has an
61:51 - offering for right off the bat right now
61:53 - but you can definitely
61:54 - definitely believe that cloud service
61:56 - providers will have this in the future
61:57 - so i want you to know about this today
61:59 - even if it's not on your exam so data
62:01 - like houses compared to data warehouse
62:04 - can support video audio and text files
62:06 - support data science ml workloads have
62:08 - support for both streaming and etl
62:10 - work with many open source formats data
62:13 - will generally reside in a data lake or
62:14 - blob store so the thing with data lake
62:16 - or data warehouse is usually used
62:18 - proprietary formats so this is a lot
62:20 - more flexible data like houses compared
62:22 - to data lakes can perform bi tasks very
62:25 - well which is something data links
62:26 - cannot do much easier to set up and
62:28 - maintain has management features to
62:30 - avoid a data like becoming a data swamp
62:32 - right because data warehouses are
62:34 - very well
62:35 - data modeled and
62:37 - data lakes aren't
62:38 - so data lakes are kind of in between
62:41 - and uh data lakes and with a data lake
62:43 - house is going to be more performant
62:44 - than a data lake so where would you find
62:47 - a solution right now probably with data
62:49 - delta like which is
62:51 - a data bricks solution so it's
62:54 - apache data like i believe is open
62:56 - source and so if you wanted a managed
62:58 - version of it
62:59 - databricks has an offering for that and
63:01 - so they have this nice little graphic
63:02 - here where they show you that you know
63:04 - you combine the two and you get the best
63:05 - of both worlds so you know you'll see
63:07 - more data likes in the future so there
63:08 - you go
63:09 - [Music]
63:13 - hey it's andrew brown from exam pro we
63:15 - are looking at data structures so what
63:17 - is a data structure this is data that is
63:19 - organized in a specific storage format
63:21 - that enables easy access and
63:23 - modification and a data structure can
63:26 - store various data types so data can be
63:28 - abstractly described to have a degree of
63:30 - structure so we have unstructured a
63:32 - bunch of loose data that has no
63:33 - organization or possibly any relation
63:36 - semi-structured data that can be browse
63:38 - or search with limitations structured
63:40 - data that can be easily browsed or
63:42 - searched so when we look at unstructured
63:44 - data think of a bunch of loose files and
63:46 - this is just a screenshot from one of my
63:47 - folders here's a bunch of stuff or
63:49 - semi-structured data we have xml or
63:52 - structured data where it's like we're
63:53 - using a relational database so yeah
63:56 - there we go let's go drill down into
63:57 - these three types of abstractions
63:59 - [Music]
64:03 - so again what is unstructured data well
64:05 - it's just a bunch of loose data think of
64:07 - it as junk folder on your computer with
64:09 - a bunch of random files not optimized
64:10 - for search analysis or simply no
64:12 - relation between the various data so
64:14 - again there's a bunch of files in my uh
64:16 - one of my folders there and so when
64:18 - we're talking about microsoft azure
64:19 - services that store unstructured data we
64:21 - have sharepoint so shared documents for
64:23 - an organization
64:25 - azure blob storage so unstructured
64:27 - object data store azure files a
64:29 - mountable file system for storing
64:31 - unstructured files azure data lake for
64:33 - big data it's basically blob storage but
64:36 - for vast amounts of data and if i wanted
64:38 - to add a fifth one there you know like
64:39 - azure azure disks but that's more for
64:42 - virtual machines okay
64:44 - [Music]
64:48 - so let's take a look here at
64:49 - semi-structured data which basically has
64:51 - no schema and the data has some form of
64:53 - relationship it's easy to browse data to
64:54 - find related data and you can search
64:57 - data but there are limitations or or
64:59 - when you search you will pay a
65:00 - computative or operational cost a great
65:02 - way of thinking about this is think of a
65:04 - big box of legos and you have these lego
65:06 - pieces so it's not that there's not a
65:08 - schema defined there's a schema defined
65:10 - in the sense of the lego piece so it can
65:13 - connect and make relationships with
65:14 - other things that are compatible but
65:16 - overall the entire data as a whole does
65:19 - not have a schema right you don't have
65:21 - you're not upfronting and doing data
65:23 - modeling so just understand that's why i
65:25 - put the asterisk there so there is a
65:27 - schema for the data structures just not
65:29 - for everything in total totally here
65:31 - that's why it's called semi-structured
65:32 - for concrete semi-structured data
65:34 - structures we've got xml json
65:36 - avro and parquet i don't know if it's
65:38 - pronounced parquet but that's the way i
65:39 - say it and for azure and other services
65:41 - that store semi semi structured data we
65:44 - have azure tables which is a key value
65:45 - store azure cosmodb where its primary
65:48 - one is document of course it stores
65:50 - other types but when we're talking about
65:52 - semi-structure we're talking about a
65:53 - document store mongodb which is an open
65:56 - source document store and then we have
65:57 - apache cassandra which is a
66:00 - y column store database um there's no
66:03 - sql that's just an open source one okay
66:07 - [Music]
66:11 - all right so we're still on
66:12 - semi-structured data structures i just
66:13 - want to give them a little bit more
66:15 - attention because you know you might
66:17 - need to know some of the the guts to
66:18 - these semi-structures so what is
66:21 - semi-structured data semi-structured
66:23 - data is data that contains fields the
66:25 - fields don't have to be the same in
66:26 - every entity
66:28 - you only define the fields that you need
66:29 - on a per entity basis so common
66:32 - semi-structured data structures is
66:34 - javascript object notation json format
66:36 - used in json notation stored data in
66:39 - memory read and write from files apache
66:42 - apache optimize row columner format also
66:45 - known as orc organizes data into columns
66:48 - rather than rows so column or data
66:50 - source structure apache parquet this is
66:52 - another column or data store a parkit
66:54 - file contains rows and groups
66:56 - we have apache avro which is row based
66:59 - format each record contains a header
67:02 - that describes the structure of the data
67:04 - in the record you have also xml we're
67:06 - not going to go through all of these but
67:07 - i do want to drill down on some of these
67:09 - semi-structured data structures so you
67:11 - know how they internally work okay
67:17 - all right let's take a look at json so
67:18 - json stands for javascript object
67:20 - notation and it is a lightweight data
67:23 - interchange format it is easy for humans
67:25 - to read and write it is easy for
67:27 - machines to parse and generate and it is
67:29 - based on a subset of javascript so here
67:31 - is an example of json and json is built
67:34 - on two structures the first is a
67:36 - collection of names
67:38 - name value pairs in other languages this
67:41 - is realized as an object a record a
67:42 - struct a dictionary a hash table key
67:44 - list or associative array so if you've
67:46 - ever heard of those things before that's
67:48 - basically what it looks like the other
67:49 - part is an ordered list of values other
67:51 - languages might call them arrays vectors
67:54 - list or sequence just to point them out
67:56 - there is the collection and there is the
67:58 - ordered list and json is a text format
68:00 - so that it is completely language
68:02 - independent
68:04 - so it is
68:06 - used quite a bit these days
68:12 - all right let's take a look at apache
68:13 - org files which stands for optimize row
68:15 - columner it's a storage format for
68:17 - apache hadoop system so it is similar to
68:20 - rc files and parkit files and is the
68:22 - successor to rc files we're not going to
68:24 - cover rc files here but you know that's
68:26 - where these come from it was developed
68:28 - by facebook to support columnar reads
68:30 - predictive pushdowns and lazy reads is
68:32 - more storage efficient than rc files
68:34 - taking up 75 percent less space orc only
68:37 - supports hadoop hive and pig when we get
68:40 - to the hadoop section you'll understand
68:41 - what those are work performs better with
68:43 - hive than parquet files org files are
68:45 - organized into stripes of data so here
68:48 - is that example there the autonomy of an
68:50 - org file so the file footer stores the
68:52 - auxiliary information the list of
68:54 - stripes in the file the number of rows
68:56 - per stripe each column the data type is
68:59 - column level aggregate information so
69:01 - count min max
69:03 - the stripe footer contains a directory
69:04 - of stream locations
69:06 - and we have the road data which is used
69:09 - for table scans and the index table
69:11 - includes min max values for each column
69:14 - and the row positions for each columns
69:16 - and the default size of a stripe is 250
69:18 - megabytes with large stripe sizes enable
69:21 - large efficient reads for hdfs which is
69:25 - the hadoop file system which we'll talk
69:27 - about when we get to the hadoop section
69:28 - but if you want to
69:30 - just review this here and then take a
69:32 - look at this graphic and it'll make a
69:34 - hundred percent sense here okay
69:36 - [Music]
69:40 - so let's take a look here at parquet
69:42 - files so apache parquet is a column
69:44 - restore file format available to any
69:46 - project in the hadoop ecosystem so hive
69:49 - hbase mapreduce pig spark
69:51 - presto there's a huge list of them and
69:53 - not just here in the hadoop system but
69:56 - other other services azure even aws
69:58 - services work really well with parquet
70:00 - files so you know it's just becoming a
70:03 - very common format for columnar storage
70:06 - formats the parquet is built to support
70:08 - very efficient compression encoding
70:09 - schemes it uses the record shredding
70:11 - assembly algorithm that's why i don't go
70:14 - in detail like work here here about like
70:16 - talking about this data structure
70:17 - because it just gets complicated so i
70:20 - just want you to know that parquet files
70:22 - uh are more generally used in org files
70:24 - or have very particular use cases uh and
70:27 - you're gonna come across parket more
70:29 - when you're doing column or storage file
70:30 - formats okay
70:31 - [Music]
70:35 - let's take a look at avro so apache avro
70:38 - is a row-based format that provides rich
70:40 - data structures compact fast binary data
70:42 - format a container file to store
70:44 - persistent data remote procedure calls
70:47 - rpcs simple integration with dynamic
70:49 - languages avro provides functionality
70:50 - similar to systems such as thrift and
70:52 - protocol buffers here is the data
70:54 - structure so when would you be using
70:56 - avro over parquet well it's just when
70:58 - you uh when you have data like if you
71:00 - want to serialize your json into a more
71:02 - efficient format we're doing general
71:04 - queries right if you're doing analytics
71:07 - right columner based stuff you're doing
71:09 - it for um
71:10 - aggregation and stuff like that
71:12 - if you're just trying to kind of like
71:14 - simulate general general relational
71:16 - database structures for semi-structured
71:17 - or nosql databases you're going to want
71:19 - to use avro
71:21 - [Music]
71:25 - all right let's talk about uh structured
71:27 - data so structured data it has a scheme
71:29 - and data data has a relationship it's
71:31 - easy to browse to find related data it's
71:33 - easy to search data the most common
71:34 - structured data is tabular data
71:36 - representing the rows and columns right
71:38 - so examples here for azure would be its
71:40 - postgres azure
71:42 - data sql database for postgres for msql
71:46 - azure sql for mssql and azure synapse
71:49 - analytics which is the data warehouse
71:50 - service okay
71:56 - hey this is andrew brown from exam pro
71:57 - and we are looking at what is data
71:59 - mining so this is the extraction of
72:00 - patterns and knowledge from large
72:01 - amounts of data not to be confused with
72:03 - the extraction of data itself a lot of
72:05 - people think data mining means go on a
72:06 - website and start scraping that's not
72:08 - what it is uh cross industry standard
72:10 - process for data mining called chris dmm
72:12 - is defined into six phases there's a lot
72:14 - of way to define data mining but i just
72:16 - chose this one because i found that it's
72:18 - the easiest to understand so we got our
72:19 - big wheel here let's break through the
72:21 - six phases so business understanding
72:23 - right so here we are at the start of our
72:26 - journey over here business understanding
72:28 - so what does the business need then we
72:30 - have data understanding which is what do
72:32 - we have to do and what data do we need
72:35 - and you can see that you can work
72:36 - between back and forth before you move
72:37 - on to the next step
72:39 - we have data preparation so how do we
72:40 - organize the data for modeling then we
72:43 - have modeling so what modeling
72:45 - techniques should we apply over here
72:47 - evaluation so what which data model best
72:50 - meets the business objectives and on the
72:52 - end here we have deployment so
72:55 - how do people access the data so there
72:57 - you go
72:57 - [Music]
73:01 - let's take a look at different types of
73:03 - data mining methods and this will
73:04 - definitely not be an exhaustive list but
73:06 - it will give you a good idea what a data
73:08 - miner does so data mining methods or
73:10 - techniques is the way to find valid
73:12 - patterns and relationships in huge data
73:14 - sets so we have classification this is
73:15 - where you classify data into different
73:17 - classes
73:18 - you have clustering a division of
73:20 - information into groups of connected
73:21 - objects you have regression which is
73:24 - ident identify and analyze the
73:26 - relationships between variables because
73:28 - of the presence of other factors we have
73:29 - sequential so evaluating sequential data
73:32 - to discover sequential patterns
73:33 - association rules discover a link or two
73:36 - or more items find a hidden pattern in
73:38 - the data sets
73:39 - these common these common constraints
73:41 - math formulas are used to determine
73:43 - significant and interesting links so we
73:45 - got support so indication of how
73:48 - frequently the item set appears in the
73:49 - data set confidence indication of how
73:51 - often the rule has been found to be true
73:54 - lift indication of importance compared
73:56 - to other items conviction indication of
73:58 - the strength of the rule from the
74:00 - statistical independence for outer
74:02 - detection we have observation of data
74:04 - items in the data set which do not match
74:07 - an expected pattern or expected behavior
74:09 - we have prediction use a combination of
74:11 - data mining techniques such as trends
74:13 - clustering classification to predict
74:14 - future data
74:15 - not super important for you to remember
74:17 - all this but i'm just trying to like get
74:18 - you exposure to these terms and things
74:20 - so that as you see them more it'll make
74:22 - sense okay
74:23 - [Music]
74:27 - hey this is andrew brown from exam pro
74:29 - and we are looking at what is data
74:30 - wrangling so data wrangling is the
74:32 - process of transforming and mapping data
74:34 - from one raw data form into another
74:36 - format with the intent of making it more
74:37 - appropriate and valuable for a variety
74:40 - of downstream purposes such as analytics
74:42 - also known as data munging so there are
74:45 - six core steps behind data wrangling the
74:47 - first is discovery so understand what
74:49 - your data is about and keep in mind
74:52 - domain specific details about your data
74:53 - as you move through the other steps
74:55 - structuring so you need to organize your
74:57 - content into a structure that will be
74:59 - easier to work for our end results
75:02 - cleaning so remove outliers change null
75:04 - values remove duplicates remove special
75:06 - characters standardizing formatting
75:08 - enriching appending or enhancing
75:10 - collected data with relevant context
75:12 - obtained from additional sources
75:14 - validating authenticate the reliability
75:17 - quality and safety of the data and
75:19 - publishing so place your data in a data
75:21 - store so it can be used
75:24 - downstream so there you go
75:26 - [Music]
75:30 - hey this is andrew brown from exam pro
75:32 - and we're looking at what is data
75:33 - modeling but before we can answer that
75:34 - we should ask what is a data model so
75:36 - it's an abstract model that organizes
75:38 - elements of data and standardizes how
75:40 - they relate to one another and to the
75:42 - properties of real world entities a data
75:44 - model could be a relational database
75:45 - that contains many tables so here's
75:47 - actually an example of some data
75:49 - modeling i did which is for the exam pro
75:51 - platform if you ever open up power bi
75:54 - they have like a data modeling tab so it
75:55 - becomes very clear what it is but
75:56 - generally uh you know data models just
75:59 - look like a bunch of tables and
76:00 - relationships but it's going to vary
76:02 - based on what you're using a data model
76:04 - for so a dml could be conceptual so how
76:06 - daters represented the organizational
76:08 - level abstractly without
76:10 - concretely describing how it works
76:12 - within the software so people orders
76:14 - projects relationships logical so how
76:17 - data is presented in software tables
76:19 - columns
76:20 - object oriented classes physical so how
76:23 - data is physically stored so partitions
76:25 - cpus and table spaces so this one would
76:27 - probably be the the middle one here
76:29 - which is logical okay so you know this
76:31 - isn't just exactly how data modelling
76:33 - looks like there's all varieties the way
76:35 - data modeling or a data model can appear
76:37 - so what is data modeling a process used
76:40 - to
76:40 - define and analyze data requirements
76:43 - needed to support the business processes
76:45 - within the scope of the corresponding
76:46 - information systems and organizations so
76:49 - here uh we have our uh data modeling
76:52 - here so you can see that uh it's
76:55 - actually broken up kind of into three
76:56 - sections which maps up really well see
76:58 - where it says physical conceptual things
77:00 - like that it matches up to our three
77:02 - categories here conceptual logical
77:03 - physical so just take in mind that uh
77:06 - you know if you have data modeling you
77:07 - can move from a conceptual to a logical
77:10 - to a physical one all right and so there
77:12 - you go
77:16 - [Music]
77:17 - all right let's take a look at etl
77:18 - versus elt so etl intel is used when you
77:21 - want to move data from one location to
77:23 - another where the data store database
77:25 - have a different data structure so you
77:27 - need to transform the data for the
77:29 - target system a common use case would be
77:32 - mssql to cosmodb so this one is
77:34 - relational this one's nosql they just
77:36 - don't have the same data structures
77:38 - you'd have to do some kind of
77:38 - transformation and so here we have our
77:41 - visuals for etl and elt so let's talk
77:43 - about etl first which stands for extract
77:46 - transform and load so loads the data
77:48 - first into a staging server and then
77:50 - into a target system so even though it's
77:52 - not shown here we actually have an
77:54 - intermediate virtual machine or server
77:56 - that's being loaded temporarily into
77:58 - doing the transformations and then when
78:00 - it's done it's going to output it into
78:02 - its target system uh used for on-premise
78:05 - relational and structured data so it's
78:07 - very common for on-prem like this could
78:09 - be a migration strategy so they could be
78:11 - taking an sql database and just moving
78:13 - it to
78:14 - um a sql database on
78:17 - azure right and so there might be
78:19 - like they're the same type of database
78:20 - but there could be different versions of
78:22 - databases so the the feature sets
78:24 - slightly different so they do some
78:25 - transformations it's good for a small
78:28 - amount of data to be fair etl can be
78:30 - used for larger workloads but you know
78:32 - when we're comparing from elts it's
78:33 - generally smaller doesn't provide data
78:35 - lake support
78:37 - easy to implement
78:38 - mostly supports relational databases
78:40 - okay
78:42 - when we talk about extract load
78:43 - transform loads directly into the target
78:45 - system
78:46 - used for scalable cloud structures and
78:48 - unstructured data sources used for large
78:50 - amounts of data provides data like
78:52 - support requires specialized skills to
78:55 - implement and maintain supports for
78:57 - unstructured data readily available so
78:59 - you're going to see the elt is going to
79:01 - be the more common use case where we're
79:04 - dealing with cloud
79:06 - but it does require a little bit more
79:07 - knowledge where this one is just like if
79:09 - you know sql you're going to be in good
79:10 - shape okay so there you go
79:12 - [Music]
79:16 - hey it's andrew brown from exam pro and
79:18 - we are looking at what data analytics is
79:19 - so this is when you're concerned with
79:21 - examining transforming arranging data so
79:23 - you can extract useful information a
79:26 - person that does data analytics is
79:27 - called a data analyst and they commonly
79:30 - use tools such as sql business
79:32 - intelligence tools and spreadsheets if
79:34 - we look at the data and analytics
79:36 - workflow so you can understand their
79:37 - whole scope of their job they'll do data
79:40 - ingestion so getting data from multiple
79:41 - sources data cleaning and transformation
79:44 - so maybe they're using
79:46 - you know pandas and
79:49 - notebooks and things like that or sql
79:51 - commands
79:53 - dimensional reductions so they have to
79:54 - reduce the amount of data
79:56 - data and analysis
79:58 - which could be like statistics and
80:00 - things like that visualizations you use
80:02 - your bi tools or you are actually coding
80:04 - in dashboards and things like that so
80:07 - yeah that is data analytics
80:09 - [Music]
80:13 - all right let's talk about kpis here so
80:14 - key performance indicators are type of
80:16 - performance measurement that a company
80:18 - will use or their organization will use
80:20 - to determine performance over time so
80:23 - here's an example of a kpi for product
80:25 - revenue and the goal was 3.12 million
80:29 - but the company actually generated out
80:30 - 2.29 million so there's 26 percent under
80:33 - their goal kpi can evaluate the success
80:35 - of an organization or for a specific
80:37 - organization activity and there are two
80:39 - categories of measurement for kpis we
80:41 - have
80:42 - quantitative and these quantitative and
80:44 - qualitative or get easily confused
80:46 - because they're very similar name but
80:47 - quantitative the properties can be
80:49 - measured with a numerical result facts
80:51 - presented with a specific value so
80:54 - monthly revenue numbers of sign ups
80:55 - number of reports or defects so what
80:57 - we're looking up at here is quantitative
81:00 - okay and then we have qualitative so
81:03 - properties that are observed and can
81:05 - generally not be measured with the
81:06 - numerical results numerical or numeric
81:09 - or textual value that represents uh
81:11 - personal feelings tastes and opinions so
81:14 - maybe customer sentiment would be
81:15 - example there so that is kpis
81:17 - [Music]
81:21 - all right so we're taking a look here at
81:23 - data analytics techniques and this is
81:24 - something you 100 need to know for the
81:26 - exam so
81:27 - pay close attention here okay so the top
81:30 - of our list here we have descriptive
81:31 - analytics and the question we are
81:32 - answering here is what has happened uh
81:35 - so here we might have specialized
81:36 - metrics such as kpis or return
81:38 - investment or things that we're more
81:39 - familiar with like generating sales and
81:41 - financial reports
81:42 - at this stage we have a lot of data
81:44 - right it's very accurate comprehensive
81:46 - it's either live data and we can make
81:48 - very effective visualizations to
81:49 - understand our data
81:51 - because we have all that historical
81:52 - information but there's a lot more to it
81:55 - in terms of value and so we'll move down
81:58 - or out
81:59 - move out uh to see the other stuff that
82:02 - we can do here the next is diagnostic
82:03 - analytics so why did it happen it's
82:06 - supplemental to descriptive analytics we
82:08 - can drill down investigate descriptive
82:09 - metrics to determine root cause find and
82:11 - isolate anomalies into its own data set
82:13 - and apply statistical techniques we have
82:15 - predictive analytics so what will happen
82:18 - so we use historical data to predict
82:19 - trends or recurrence uh we use either
82:22 - statistical or machine learning
82:24 - techniques apply this is where a data
82:25 - scientist might get involved we use
82:27 - neural networks decision trees
82:28 - regression classification neural
82:29 - networks just means
82:31 - deep learning machine learning okay
82:33 - uh prescriptive analytics how can we
82:35 - make it happen so goes a step further
82:38 - than predictive and uses ml by ingesting
82:40 - hybrid data to predict future scenarios
82:42 - that are exploitable and then the last
82:44 - one is what if this happens
82:46 - and that's cognitive analysis so using
82:48 - analytics to draw patterns to create
82:50 - what if scenarios and what actions can
82:51 - be taken if the scenarios become a
82:53 - reality so there you go that is data
82:55 - analytic
82:56 - [Music]
83:00 - all right let's take a look here at
83:01 - microsoft onedrive so microsoft onedrive
83:03 - is a storage and storage synchronization
83:05 - service for files which reside in the
83:07 - cloud similar to products like dropbox
83:09 - google drive box and microsoft
83:11 - sharepoint so onedrive is intended for
83:13 - personal storage for a single individual
83:15 - you pay for different sizes of storage
83:17 - so you have five gigabytes which are
83:18 - free 100 gb 1tb and 6tb you do not worry
83:22 - about the underlying hardware the
83:23 - durability resilience fall tolerance
83:25 - availability that's what we call
83:26 - serverless technology here is a nice
83:28 - screenshot of what it looks like to use
83:30 - onedrive both in the browser and on your
83:32 - phone files can be shared easily to
83:34 - other users via a shareable link or a
83:36 - specific email that has onedrive account
83:39 - files are accessed via a web app or
83:41 - shared folders that hold a reference to
83:44 - file stored in the cloud so shared
83:45 - folders mean like you can literally use
83:47 - your file system and you'll have folders
83:49 - that are linked to it files can be
83:51 - synchronized so a copy resides in a
83:52 - local computer hard drive is copied to
83:54 - the cloud a file residing the cloud can
83:56 - be copied to a little computer hard
83:58 - drive
83:58 - copying occurs automatically when files
84:00 - are changed differences and files could
84:01 - result in conflicts and a user must
84:03 - choose which file to keep or how to
84:05 - resolve the conflict might have more
84:06 - options files can be versions so you can
84:08 - recover older versions of files older
84:10 - files may retain for 30 days and be
84:12 - automatically deleted so there you go
84:13 - that is onedrive
84:18 - let's take a look here at microsoft 365
84:21 - sharepoint so 365 sharepoint is a
84:23 - web-based collaborative platform that
84:25 - integrates into the microsoft office
84:26 - intended for document management and
84:28 - shared storage and here is a screenshot
84:30 - of my sharepoint uh and so there's the
84:32 - az-104 so the thing is is that it's
84:35 - basically
84:36 - one drive uh but for companies with a
84:39 - bunch of layers on top of it and it is
84:41 - extremely useful to use it's super super
84:44 - useful so like if you work in a company
84:46 - you should really be using sharepoint
84:47 - for sharing files so just growing down
84:50 - the feature list here so sharepoint
84:51 - sites because there's some concepts here
84:53 - just besides documents they have things
84:55 - like sites so data within sharepoint is
84:57 - organized around sites a site is a
84:59 - collaborative space for teams with the
85:01 - following components so document library
85:03 - pages web parts and more sharepoint
85:05 - doctor document library which will
85:07 - expand there and that's what the
85:08 - screenshot is on the right-hand side so
85:10 - a document library is a file storage and
85:11 - synchronization but designed for teams
85:13 - it is very similar to onedrive but files
85:14 - are owned by the company and not an
85:16 - individual you can apply robust
85:18 - permissions to access files within or
85:20 - outside your organization a site always
85:22 - has a default document library called
85:24 - documents
85:25 - and so there's a lot more going on in
85:27 - sharepoint but this is the most
85:29 - important feature of it and actually
85:31 - when you use onedrive when you install
85:32 - on your computer there's like a
85:33 - synchronization device it's the same
85:36 - thing so sharepoint most likely is using
85:38 - the onedrive technology underneath i
85:40 - don't know 100 but it's most likely the
85:42 - case so there you go
85:43 - [Music]
85:47 - hey this is andrew brown from exam pro
85:49 - and we are looking at data core concepts
85:51 - this one's four sheets long so let's
85:52 - jump into it so the first here is data
85:54 - which is units of information data
85:56 - documents types of abstract groupings of
85:57 - data data sets unstructured logical
85:59 - grouping of data data structures which
86:02 - has some form of structure and there's
86:04 - variance right so we have unstructured a
86:06 - bunch of loose data that has no
86:07 - organization or possible relation we're
86:09 - talking about flat files here various
86:10 - files that can reside in a file system
86:12 - semi-structured so that's data that can
86:14 - be borrowed or searched with limitations
86:15 - so csvs xml json parquet and so if we're
86:19 - talking about xml files the markup looks
86:20 - like html for json it's a text file
86:22 - that's composed of dictionaries and
86:24 - arrays rc files are storage formats
86:26 - designed for map reduce framework not
86:28 - something we covered in the uh lecture
86:30 - content but i just wanted to mention
86:31 - them there work so a columnar data
86:34 - structure 75 more efficient than rc
86:36 - files limited compatibility works very
86:39 - well with hive we have avro so a rose
86:43 - row-wise uh data structure for hadoop
86:46 - systems you have parkette a columnar
86:48 - data structure that has more support for
86:50 - hadoop systems than orc then we were
86:52 - talking about structured data so data
86:53 - that can be easily browsed or searched
86:55 - so tabular data and so tabular data is
86:57 - data that is arranged as tables think of
86:59 - spreadsheets
87:01 - data types how single units of data are
87:03 - intended to be used we're not going to
87:04 - go through the whole list they're not
87:05 - going to ask that on the exam but you
87:06 - should know your data types uh four
87:08 - types of roles that azure cares for you
87:11 - to know we have database administrators
87:12 - so configures and maintains databases
87:14 - data engineer design and implement data
87:16 - tasks really to transfer and storage of
87:18 - big data data analysts analyzes business
87:20 - data to reveal important information
87:22 - then we have our
87:24 - tiers of computing so we have software
87:26 - as a service a product that is run and
87:28 - managed by service provider platform as
87:30 - a service focus on the deployment
87:31 - management of your apps infrastructure
87:33 - as a service basic building blocks of
87:34 - cloud i.t provides access to networking
87:36 - computers data storage and space and
87:38 - remember that uh we're talking about sql
87:41 - it's going to be the sql vms on this
87:43 - layer and then here's going to be the
87:44 - managed sql and
87:48 - azure sql databases okay
87:51 - so we're on to the second page here so
87:53 - let's talk about data stores
87:54 - unstructured or semi-structured data
87:56 - for housing data a broad term that can
87:59 - encompass anything that stores data
88:00 - databases structured data that can be
88:02 - accessed quickly and search generally
88:04 - relative row-based tabular data for oltp
88:07 - data warehouses structured
88:08 - semi-structured data for creating
88:10 - reports and analytics column-based
88:11 - tabular data for olap data mart's a
88:13 - subset of data warehouse for specific
88:15 - business data tasks data lakes combines
88:17 - the best of data warehouses and data
88:19 - lakes notebooks data that is arranged in
88:21 - pages designed for easy consumption
88:23 - batching when you send batches a
88:24 - collection of data to be processed not
88:26 - real time streaming when the data is
88:28 - processed as soon as it arrives so it's
88:29 - real time relational data data that uses
88:32 - struct structure tabular data and has
88:34 - relationships between tables and in
88:36 - terms of relationships for relational
88:38 - relational stuff we have one to one so
88:39 - one to one so think a monkey has a
88:41 - banana one to many a store has many
88:43 - customers many to many a project has
88:45 - many tasks and tasks can belong to many
88:47 - projects
88:48 - a join table a student has many classes
88:50 - through enrollments the enrollments
88:52 - would be the joint table and a class has
88:53 - many students through enrollments then
88:55 - we're talking about row stores so or row
88:58 - wise
88:59 - data organizing rows optimize for oltp
89:02 - then you have column store or columner
89:04 - data organizing columns optimize for
89:06 - olap so analytics now we have indexes a
89:09 - data structure that improves the reads
89:10 - of databases this is also shows up under
89:13 - non-relational databases but i just
89:14 - threw it here just because we have pivot
89:16 - tables it is a table of statistics that
89:18 - summarizes the data of more extensive
89:19 - table from a database spreadsheet or bi
89:22 - tool
89:23 - now talking about non-relational data
89:25 - data that has semi-structured data
89:26 - associated with schema
89:28 - new school databases so we got key value
89:31 - each value has a key designed to scale
89:32 - only simple lookups
89:34 - i like to describe a simple dumb and not
89:36 - a lot of features
89:37 - we have document primary entities xml or
89:39 - json-like data structure called a
89:41 - document columner has a table like
89:44 - structure but the data is stored around
89:46 - columns instead of rows graph data is
89:48 - represented with nodes and structures
89:49 - where relationships matter okay
89:52 - uh we're on to the third page here so
89:54 - data modeling an abstract model that
89:55 - organizes elements of data and
89:57 - standardizes how they relate to one
89:59 - another in the real world entities
90:01 - schema a formal language to describe the
90:03 - structure of data used by databases and
90:05 - data stores during the data modeling
90:07 - phase schema is generally used for when
90:09 - upfront data modeling can be foregone
90:12 - foregone i did not write that right but
90:15 - because the schema is flexible normally
90:17 - used with no skill databases data
90:19 - integrity the maintenance and assurance
90:20 - of data accuracy and consistency over
90:22 - its entire lifecycle data corruption the
90:24 - act of data not being in the intended
90:26 - state will result in data loss
90:28 - or misinformation normalization a schema
90:31 - designed to store non-redundant
90:32 - inconsistent data denormalize a schema
90:35 - that combines data so that access to
90:37 - data is fast elts or etls transform data
90:41 - from one data store to another loads of
90:43 - data in an intermediate stage doesn't
90:45 - work does not work with data lakes e-l-t
90:48 - transformations done at the target data
90:50 - store uh works with data lakes more
90:51 - common in cloud services things of azure
90:54 - app analytics okay or azure synapse
90:56 - analytics where the data is loaded and
90:58 - done uh in the actual data warehouse or
91:01 - etc a query when a user requests data
91:03 - from a data store by using query
91:05 - language to return the data result data
91:07 - source data sources where data
91:08 - originates from
91:10 - so analytics and data warehouse tools
91:12 - may be connected to various data sources
91:14 - bi tools would have data sources as well
91:16 - data consistency when data being kept in
91:18 - two different places and whether the
91:20 - date that the data exactly matches or
91:22 - does not match strongly consistent every
91:24 - time you request data you can expect
91:26 - consistent data to be returned within a
91:28 - time eventually consistent when you
91:30 - request data you may get inconsistent
91:32 - data so like stale data synchronization
91:34 - continuous stream of data that is
91:35 - synchronized by a timer or clock so
91:37 - guarantee of time asynchronous
91:39 - a synchronization continuous stream of
91:41 - data
91:42 - separated by start and stop uh stop bits
91:46 - no guarantee of time and this is
91:47 - synchronization in terms of processing
91:49 - okay data mining the extraction of
91:52 - patterns and knowledge from large
91:53 - amounts of data not the extraction of
91:55 - data itself data wrangling the process
91:56 - of transforming mapping data from one
91:58 - raw data into from
92:00 - form into another format and we're on
92:02 - the last page here so data analytics
92:04 - data analytics is examining transforming
92:06 - arranging data so that you can extract
92:08 - and study useful information key
92:10 - performance indicators probably not
92:12 - talked about the exam but i threw it in
92:13 - here because it's just important to know
92:15 - type of performance measurement that a
92:17 - company organization to determine
92:18 - performance over time then in terms of
92:20 - the types of uh analytics that we can
92:23 - utilize we have descriptive analytics
92:25 - what happened so accurate comprehensive
92:26 - like data
92:28 - effective visualization so dashboards
92:29 - reports kpis roi that's when you have
92:32 - all the information diagnostic analytics
92:34 - why did it happen drill down to
92:36 - investigate root cause sometimes they
92:38 - call that root cause analysis we didn't
92:39 - talk about that in the course but that's
92:41 - what it is focus on a subset of
92:43 - descriptive and now an analytics subset
92:45 - so it's a subset of this one up here
92:48 - okay predictive analytics what will
92:50 - happen so use historical data with
92:52 - statistics and ml probably should
92:54 - highlight that in red there for you to
92:55 - generate trends or predictions
92:57 - predictive analytics what will happen
92:59 - use hybrid data with ml to predict
93:01 - future scenarios that are exploitable
93:04 - cognitive analysts what if this happens
93:06 - so use ml and nlp to determine what if
93:09 - scenarios to create plans if they happen
93:11 - these are all really similar but the
93:13 - thing is is that they just it's it's the
93:15 - lens you put on like the the reason why
93:16 - you're doing it okay
93:18 - then we talk about run drive so storage
93:20 - and uh storage synchronization service
93:21 - for a single user and then we have
93:23 - sharepoint storage and storage
93:24 - synchronization service for an
93:26 - organization there's a little bit more
93:27 - to that but that is it for data core
93:29 - concepts
93:30 - [Music]
93:34 - let's take a look at azure synapse
93:36 - analytics and this is a data warehouse
93:38 - and a unified analyst platform we're
93:40 - going to talk more about the latter
93:42 - because like you know what a data
93:43 - warehouse at this point it's just a
93:44 - column or store
93:46 - and so here is a visual of um the data
93:48 - analytics or data synapses studio so
93:52 - here you can see there's a query going
93:53 - on so we're just querying data but
93:55 - there's a lot we can do uh on the
93:57 - unified analytics platform so we can
93:58 - perform etl and elt processes
94:01 - in a code free a visual environment so
94:03 - you don't have to write any code using
94:05 - just data from more than 95 native
94:06 - connectors
94:08 - deeply integrated with apache spark uses
94:10 - tsql queries on both your data warehouse
94:13 - and spark engines just that's what we're
94:15 - looking at there is the tsql and
94:17 - supports multiple languages so tsql
94:19 - python
94:20 - scala spark sql and net and it's
94:23 - integrated with artificial intelligence
94:25 - so ai and business intelligence tools bi
94:28 - so we could use azure machine learning
94:30 - studio or azure cognito services
94:34 - or microsoft power bi just to get a
94:36 - better visual of the entire flow here on
94:38 - the left hand side you're ingesting data
94:41 - from sources
94:42 - all the data is going to be stored on a
94:44 - data lake
94:46 - storage gen 2 here
94:48 - at the top here we have the azure
94:50 - synapse analytics studios that's where
94:52 - you're going to be doing
94:54 - the interface you're going to be working
94:55 - with and then you're going to be able to
94:57 - output the various services and notice
94:58 - here that we have sql
95:00 - and apache spark which are the different
95:02 - runtime engines this really looks like
95:04 - to me
95:05 - a data lake
95:06 - or lake house which when i was talking
95:08 - about lake house i was like azure
95:09 - doesn't have an offering but now that
95:11 - i'm looking at the screenshot this
95:12 - definitely is a data lake house so yeah
95:16 - i guess azure synapse is a data lake
95:19 - house cool
95:20 - [Music]
95:24 - all right let's talk about synapse sql
95:26 - so snapchat scale is a distributed
95:27 - version of t sql designed for data
95:29 - warehouse workloads it extends tsql to
95:32 - address streaming and machine learning
95:34 - scenarios it uses built-in streaming
95:36 - capabilities to land data from uh
95:38 - load supposed to say load data from
95:41 - cloud data sources into sql tables
95:43 - integrate ai with sql by using ml models
95:46 - to score data using tsql predict
95:48 - function and offers both serverless and
95:50 - dedicated resource models so for the
95:52 - serverless side this is great for
95:54 - unpredictable workloads so unplanned or
95:55 - bursty workloads use
95:58 - use the always available or serverless
96:00 - sql endpoint are our options for
96:02 - predictable workloads we create
96:04 - dedicated sql pools to reserve
96:05 - processing power for data stored in sql
96:08 - tables so here we're talking about
96:10 - dedicated dedicated sql pools and others
96:12 - so let's talk about those very quickly i
96:15 - couldn't even be bothered to make a
96:16 - graphic for this because it's just too
96:18 - much work so i just pulled it right from
96:19 - the docs but dedicated sql pool is a
96:22 - query service over the over the data in
96:24 - your data warehouse the unit of scale is
96:26 - an abstraction of compute power that is
96:28 - known as a data warehouse unit dwd once
96:30 - your dedicated sql pool is created you
96:32 - can import big data with simple poly
96:35 - based tsql queries and then use the
96:37 - power of this distributed query engine
96:39 - to perform
96:41 - high performance analytics then there's
96:43 - the serverless sql pools which looks
96:44 - like this and serverless sql pool is a
96:47 - query service over the data in your data
96:49 - lake scaling done automatically to
96:51 - accommodate query resource requirements
96:53 - as as topology changes over time by
96:55 - adding removing nodes or failure it
96:57 - adapts to changes and makes sure your
97:00 - query has enough resources and finishes
97:02 - successfully they're not going to test
97:03 - you on these data pool things but you
97:05 - know i just figured we'd provide a
97:06 - little more context and some more uh
97:09 - language around here just to help
97:10 - solidify what the service is but there
97:12 - you go
97:13 - [Music]
97:17 - just a couple of things that i just want
97:18 - to give extra emphasis on which is
97:20 - apache spark and data lake with synapses
97:22 - so
97:23 - it's synapse but i just keep on saying
97:25 - synapses just get used to it
97:27 - azure synapse can deeply and seamlessly
97:29 - integrate with apache spark as you can
97:31 - see here in ml models with spark ml
97:33 - algorithms and azure ml integration for
97:35 - apache spark 2.4 with built-in support
97:38 - for linux foundation delta lake there's
97:39 - the apache spark 3. so i'm surprised
97:41 - they're not up to date yet at least when
97:42 - i wrote this simplified resources models
97:45 - that freeze your you from having to
97:47 - worry about managing clusters fast spark
97:49 - startup and aggressive auto scaling
97:51 - built-in supportfor.net for spark allow
97:53 - you to reuse c-sharp expertise in
97:55 - existing.net
97:57 - code with the spark application talking
97:59 - about data lake here azure synapse
98:01 - removes the traditional technology
98:03 - barriers using sql and spark together
98:05 - and you can seamlessly mix and match
98:07 - based on your needs and expertise tables
98:09 - defined on files in the data lake are
98:10 - seamlessly consumed by spark or hive sql
98:13 - and spark can directly explore and
98:15 - analyze parquet csv tsv json file stored
98:18 - in the data lake
98:20 - fast scalable data loading between spark
98:22 - and
98:23 - sql and spark databases so there you go
98:28 - [Music]
98:30 - so a data lake is a centralized data
98:32 - repository for unstructured and
98:33 - semi-structured data and a lake is
98:35 - intended to store vast amounts of data a
98:37 - daylight generally uses object blob or
98:39 - files as its storage medium and so the
98:41 - idea is you will collect data by putting
98:43 - putting various sources in there you'll
98:44 - do transformations so change your blend
98:46 - data into new semi-structures using etl
98:49 - or elt and put it right back in the data
98:51 - lake or we could distribute it by
98:53 - allowing access to data to various
98:55 - programs and apis or publish the data
98:57 - set to a meta catalog so analysts can
98:59 - quickly find useful data now if we want
99:01 - to have an azure data lake uh you know
99:04 - you're gonna have to create a data lake
99:05 - storage but the thing is is that gen one
99:07 - is no longer really intended to be used
99:09 - the only people that should be using it
99:10 - are the people that are still in use but
99:13 - we'll focus on gen 2. so gen 2 is the
99:16 - data lake storage for azure blob storage
99:18 - which has been extended to support big
99:20 - data analytic workloads designed to
99:22 - handle petabytes of data and hundreds of
99:24 - gigabytes of throughput in order to
99:25 - efficiently access the data data lake
99:27 - storage adds a hierarchical hierarchical
99:30 - namespace to the azure blob storage and
99:32 - this is what it looks like on the
99:33 - left-hand side so
99:35 - we have two ways of access through the
99:36 - blob endpoint
99:38 - w-a-s-b-s and the dfs endpoint abfs
99:42 - these are different drivers one is for
99:44 - object one is for file but they'll both
99:46 - get you in there and they're both
99:47 - compatible with hdfs which is hadoop and
99:50 - then the hierarchical namespace you get
99:52 - access control throttling timeout
99:53 - management performance and optimizations
99:56 - okay so there you go
99:57 - [Music]
100:02 - all right let's take a look here at
100:03 - polybase and this is a data
100:04 - virtualization feature of sql servers
100:07 - and specifically we're saying msql right
100:09 - microsoft sql polybase enables your sql
100:12 - server instance to query data with tsql
100:14 - directly from sql server oracle
100:16 - teradata mongodb hadoop clusters cosmodb
100:19 - without separately installing client
100:21 - connection software and so here's a
100:23 - fancy example to show you uh how great
100:26 - this tool is and polybase allows you to
100:28 - join data from an sql server instance
100:30 - with external data prior to polybase to
100:32 - join data to external data sources you
100:33 - can either transfer half your data so
100:35 - that all the data was in one location or
100:38 - query both sources of data then write
100:40 - custom query logic to join and integrate
100:42 - the data at the client level
100:44 - so there you go
100:45 - [Music]
100:49 - let's talk about how elts happen in um
100:52 - synapse analytics so you can perform
100:54 - elts uh in synapse sql and uh within the
100:59 - snaps analytics so the fastest and most
101:01 - scalable way to load data is through
101:03 - poly base external tables and copy
101:05 - statements that's why we talked about
101:06 - polybase so with polybase and copy
101:08 - statement you can access external data
101:10 - stored in azure blob storage data lake
101:12 - store
101:13 - via the ts sql language makes sense
101:16 - because if you can do data like store
101:18 - blob storage is the same thing so here
101:20 - is a graphic where you can see we are
101:22 - ingesting from uh sql into polybase into
101:25 - our data warehouse and then we can talk
101:27 - to our data lake and do a variety of
101:29 - other things so the basic steps for an
101:31 - etl are extract the source data into
101:33 - text files load the data into blob
101:35 - storage or azure data lake store prepare
101:37 - the data for loading load the data into
101:39 - into staging tables with polybase or
101:41 - copy command transform the data and
101:43 - insert the data into the production
101:45 - tables
101:46 - [Music]
101:50 - so azure data lakes analytics is an
101:52 - on-demand analytics job service that
101:53 - simplifies big data instead of deploying
101:55 - configuring and tuning hardware you
101:56 - write queries called usql to transform
101:59 - your data and extract valuable insights
102:02 - so the idea here is that by exporting
102:04 - approximately 2.8 billion rows of tcps
102:07 - ds store sales data 500 gigabytes into a
102:09 - csv it took less than seven seven
102:11 - minutes and importing a full terabyte of
102:13 - source uh took uh under with a connector
102:16 - took under less than six hours so the
102:17 - idea is that it's pretty darn fast and
102:20 - just to show you where it is it's over
102:21 - here in the middle so the idea is that
102:23 - this tool just lets you do run uh
102:26 - queries on your data lake okay
102:29 - let's talk about usql so usql is a
102:30 - structured query language included with
102:32 - data like analytics to perform queries
102:34 - on your data lake you can see there's a
102:36 - bunch of stuff in here like extract and
102:38 - stuff um and so usql can query and
102:40 - combine data from a variety of data
102:41 - sources including azure data lake
102:43 - storage blob storage sqldb data
102:46 - warehouse sql server instances running
102:48 - on azure vm you can install the azure
102:50 - data lake tools for visual studio to
102:52 - perform you sql jobs on your azure data
102:54 - lake i didn't see it in um
102:57 - azure data studio but it might be there
102:59 - too
103:00 - [Music]
103:04 - hey this is andrew brown from exam pro
103:06 - we're taking a look here at the azure
103:07 - synapse and data lake cheat sheet for
103:08 - the dp900 let's jump into it a data lake
103:11 - is a centralized data repository for
103:12 - unstructured and semi-structured data a
103:14 - data lake is intended to store vast
103:16 - amounts of data data likes generally use
103:17 - objects blobs or files as its storage
103:20 - medium for the azure data lake storage
103:23 - generation 2 this is an azure blob
103:25 - storage which has been extended to
103:27 - support big data analytics workloads it
103:29 - does this via its hierarchical namespace
103:31 - and what the oracle namespace gives you
103:33 - is acls throttle management performance
103:35 - optimizers you can access your data like
103:37 - via the wasb protocol blob or abfs which
103:42 - is a file system protocol azure synapse
103:45 - analytics is a data warehouse and
103:46 - unified analytics platform has two
103:48 - underlying transformation engines so we
103:49 - have esqel pools and spark pools synapse
103:52 - sql is tsql but designed to be
103:53 - distributed sql dedicated pools is
103:56 - reserve compute for processing
103:57 - serverless endpoints on-demand no
103:59 - guarantee of performance data stored on
104:02 - azure data lake store generation 2
104:05 - operations are performed within the
104:06 - azure synapse studio
104:09 - polybase enables your sql server
104:11 - instance to query data with tsql used to
104:13 - connect many relational database sources
104:15 - probably use with other services not
104:16 - just with azure snaps but there you go
104:23 - all right let's take a look at azure
104:25 - blob so blob storage is an object store
104:27 - that is optimized for storing massive
104:28 - amounts of unstructured data
104:30 - unstructured data is data that doesn't
104:32 - adhere to a particular data model or
104:33 - definition such as text or binary data
104:35 - as your blobs are composed of the
104:36 - following components so we have storage
104:38 - accounts which is a unique namespace in
104:40 - azure for your data you have containers
104:42 - which is similar to a folder and a file
104:43 - system and then the actual data being
104:45 - stored so azure storage supports three
104:48 - types of blobs we've got block blobs so
104:50 - these store text environment data made
104:52 - up of blocks of data that can be managed
104:53 - individually so we're up to 4.75
104:55 - terabytes
104:57 - we have a pen blobs these optimize for
104:59 - append operations ideal for scenarios
105:00 - such as logging data from virtual
105:02 - machines and we have page blobs these
105:04 - store random access files to up to eight
105:07 - terabytes in size and store virtual hard
105:09 - drives vhd files and serve as disks for
105:12 - azure virtual machines and there you go
105:18 - all right let's take a quick look here
105:19 - at azure files so azure files is a fully
105:22 - managed file share in the cloud and a
105:23 - file share is a centralized server for
105:25 - storage that allows for multiple
105:26 - connections it's like having one big
105:28 - shared drive that everyone
105:30 - you know virtual machines can work on at
105:32 - the same time so here's an example or a
105:34 - diagram of it so to connect to the file
105:36 - share you use a network protocol such as
105:37 - server message block smb or network file
105:40 - system nfs when a connection is
105:42 - established the files shares file system
105:45 - will be accessible in the in the
105:46 - specific directory within your own
105:48 - directory tree this is known as mounting
105:50 - so some use cases here completely
105:52 - replace your supplement your on-premise
105:54 - file servers
105:56 - nas drives a lift and shift of your
105:58 - on-prem storage to the cloud via classic
106:01 - lift or hybrid lift lift and shift means
106:03 - when you move workloads without
106:04 - re-architecting so importing local vms
106:06 - to the cloud a classic lift would be
106:08 - where both the application and its data
106:10 - are moved to azure a hybrid lift is
106:12 - where the application data is moved to
106:14 - azure files and the application
106:15 - continues to run on premise we have
106:17 - simplified the cloud deployment so
106:19 - shared application settings so multiple
106:21 - vms and developer workstations need to
106:23 - access the same config files or
106:25 - diagnostic share we have all vms logged
106:28 - to the file share developers can mount
106:30 - and debug all logs in a centralized
106:32 - place we can dev test and debug so
106:34 - quickly share tools for developers
106:36 - needed for local environments we can do
106:38 - containerization so you can have azure
106:40 - files to persist volumes for stateful uh
106:42 - containers super useful when you're
106:43 - working with containers why use azure
106:46 - files instead of setting up your own
106:48 - file share well shared access so already
106:50 - set up to work with the standard
106:51 - networking protocols it's fully managed
106:53 - so it's kept up to date with security
106:55 - patches designed to scale uh it has
106:57 - scripting tools to automate the
106:58 - management and creation of file uh files
107:01 - shared with azure api and powershell and
107:03 - it has resilience so it's built to be
107:05 - durable and always
107:06 - working so there you go
107:08 - [Music]
107:12 - hey this is andrew brown from exam pro
107:14 - and we're looking at azure account
107:16 - storage cheat sheet and this is a very
107:17 - short section so azure storage accounts
107:19 - an umbrella service for various forms of
107:21 - managed storage you have azure tables
107:23 - blob storage and files there's of course
107:25 - cued and some other things in there but
107:26 - these are the three that we care about
107:28 - azure blob storage object storage is
107:30 - distributed across many machines
107:32 - supports three types so we got blah blah
107:34 - so store text and binary data blocks of
107:36 - data can be managed individually up to
107:38 - 4.7 terabytes append blocks optimize for
107:40 - append operations ideal for logging page
107:43 - blobs store random access files up to 8
107:45 - terabytes in size azure files is a fully
107:48 - managed file share in the cloud to
107:49 - connect to the file share and network
107:51 - protocols used either smb or nfs azure
107:55 - storage explorer a standalone
107:56 - cross-platform app to access various
107:58 - storage formats within the azure storage
108:00 - accounts and there you go
108:06 - let's talk about business intelligence
108:07 - tools so bi is both a data analysis
108:10 - strategy and technology for business
108:11 - information the most popular bi tools
108:14 - are tableau microsoft power bi and
108:16 - amazon quick site we're going to
108:17 - obviously be focusing on power bi
108:19 - because that's what azure would like us
108:20 - to focus on
108:21 - bi helps organizations make data-driven
108:23 - decisions by combining business
108:25 - analytics data mining data visualization
108:27 - data tools infrastructure and best
108:29 - practices and there's the logo of the
108:31 - three so you know what it is and now
108:33 - we'll jump into power bi
108:34 - [Music]
108:38 - hey it's andrew brown from exam pro and
108:40 - we're taking a look at microsoft power
108:42 - bi which is a business intelligence tool
108:44 - for visualization business data and
108:46 - here's a screenshot of the power bi
108:47 - desktop and power bi can get a little
108:50 - bit confusing because they have a lot of
108:51 - things under the power bi name but i'll
108:53 - break them down here so it's nice and
108:55 - clear so the power bi desktop is a way
108:57 - to design and adjust reports the power
109:00 - bi mobile is a
109:01 - view reports on the go on your phone
109:03 - power bi service sometimes called the
109:05 - power bi portal is to access some
109:07 - modified reports in the cloud and power
109:08 - bi embedded is a way to embed power bi
109:11 - components into your applications and
109:14 - usually you need to get data into a
109:16 - power api and so this is one of the most
109:18 - powerful reasons why people like using
109:20 - it it's because it ingests with so many
109:22 - data sources so in here this is a
109:23 - desktop one you go in and you can go
109:26 - under azure and there's all like every
109:27 - azure service you'd ever want if you go
109:29 - the database tab there's a lot of
109:30 - database integrations for postgres mysql
109:33 - everything it's crazy
109:35 - and so power bi can directly integrate
109:36 - with azure services as you saw here i
109:38 - couldn't be bothered to make a graphic
109:40 - here but you know here you can see you
109:42 - can get things from hd insights sql
109:44 - databases
109:45 - account storage machine learning stream
109:47 - analytics event hubs things like that
109:50 - uh so just to compare the two because
109:52 - these are the most important services is
109:53 - the desktop and the service and they're
109:55 - very easy to get mixed up so power bi
109:57 - desktop is a dell is is a downloadable
109:59 - free windows application and installed
110:01 - on a local windows computer if you're on
110:03 - a mac you cannot use it sorry or linux
110:06 - either report uh it it can it has
110:08 - reports or sorry so the role that
110:11 - somebody would be using would be you
110:12 - would be a report designer and you'd use
110:15 - uh the desktop application to publish
110:17 - power bi reports to the power bi service
110:19 - okay and power bi service is a
110:21 - cloud-based service where users view and
110:23 - interact with reports users in power bi
110:26 - service can edit the reports and create
110:27 - visuals based on the existing data model
110:29 - and they can share and collaborate with
110:31 - co-workers so just looking at the
110:33 - overlapping services power bi desktop
110:35 - has many data sources transforming
110:37 - shaping and modeling measures calculated
110:39 - columns python themes rls creation
110:42 - then on the power bi server side you
110:44 - have some data sources that you can
110:46 - ingest
110:47 - dashboards that is the key thing for
110:49 - power bi services that you get
110:50 - dashboards you don't get that on the
110:52 - power bi desktop part apps and
110:54 - workspaces sharing data flow creation
110:56 - paginated reports rls management gateway
110:59 - connections paginate reports is actually
111:02 - with the
111:03 - builder which you have to download so
111:04 - i'm not sure why it's in there both you
111:07 - get reports visualization security
111:08 - filters bookmarks q a and r visuals but
111:12 - just
111:13 - make a note here that you use the power
111:16 - bi desktop to create reports and then
111:19 - they're get they can be used in power bi
111:21 - service to create dashboards okay
111:24 - [Music]
111:28 - let's talk about data visualizations and
111:29 - chart types and specifically power bi
111:31 - ones so power bi has many kinds of
111:33 - visualizations we'll cover the most
111:35 - common ones but you can see over here
111:37 - like look at all these little little
111:38 - squares that represents all different
111:40 - kinds of visualizations you can make and
111:42 - even with them they're highly
111:44 - configurable okay so let's go and look
111:46 - at bar and column charts so see how a
111:48 - set of variables changes across
111:50 - different categories we've all seen bar
111:51 - charts it supports stacked ones and
111:54 - bar charts stacked side by side or
111:56 - horizontal ones you know the you know
111:58 - what bar charts are line charts overall
112:00 - shape of an entire series of values so
112:02 - they're just lines
112:04 - uh we have a matrix so that is where you
112:06 - have a tabular structure that summarizes
112:08 - the data you have key influencers the
112:10 - major contributors to a selected result
112:12 - or value and that one kind of has a very
112:14 - cool looking visualization
112:16 - you have tree map charts of colored
112:18 - rectangles with size representing the
112:20 - relative value of each item
112:22 - we have scatter graphs of represent
112:24 - relationships between two numerical
112:26 - values so you have an x and y it's
112:27 - basically a bunch of dots on a graph you
112:29 - have bubble chart it's the same thing
112:31 - but the the dots now are bubbles and the
112:33 - larger the bubble can represent a third
112:35 - dimension
112:36 - you have dot plot charts and these are a
112:38 - little bit confusing to look at but they
112:40 - are basically bubble charts but you
112:42 - they're organized based on an x-axis so
112:44 - you're basically putting those into
112:45 - categories i'm always confused when i
112:48 - look at that one but that's just one and
112:50 - one more for us here is a field map so
112:52 - you have a geographic map where
112:53 - different areas can be filled
112:55 - so like here you have states of
112:57 - different colors that represent things
112:59 - it could be gradients
113:00 - there's a lot you can do with maps and
113:02 - geographical maps and that again is not
113:05 - all the data visualizations but the most
113:06 - common ones you'll come across
113:12 - all right let's take a look here at
113:13 - power bi embedded and honestly this
113:15 - probably won't show up on the exam it's
113:16 - just that when you look use azure and
113:18 - you type in power bi it shows up in the
113:19 - console and i was like what is this
113:21 - thing and i thought this was kind of
113:23 - interesting and i feel like it's it's
113:24 - relevant so that's why i have it in here
113:26 - so azure power bi embedded is a platform
113:28 - as a service analytics embedding
113:30 - solution that allows you to quickly
113:31 - embed visuals reports dashboards into an
113:33 - application for independent software
113:35 - vendors it enables you to visualize
113:37 - application data rather than building
113:39 - the service yourself for developers you
113:40 - embed reports and dashboards into an
113:42 - application for their customers to use
113:44 - azure power bi if you need a power bi
113:46 - pro user account you need to create an
113:48 - app workspace and you need to choose a
113:50 - capacity so either i guess it'd be like
113:52 - billing work via capacity based or
113:55 - hourly metric modes there you go
113:57 - [Music]
114:01 - let's take a look at power bi
114:02 - interactive reports and these and so
114:04 - basically when you're using power bi
114:06 - desktop and you can generate reports in
114:08 - the in the portal or service but uh the
114:10 - reports
114:12 - are interactive so if you're getting
114:13 - confusing like there's power bi reports
114:15 - and interact reports
114:16 - basically by default everything's
114:18 - interactive with power bi okay
114:20 - so here is an example of one i just
114:23 - downloaded the uh like the example one
114:25 - that uh microsoft provides
114:27 - and as you can see they uh like in the
114:30 - middle of it there's like a little knob
114:31 - so that kind of gives you an indication
114:33 - of interactivity or there's like other
114:34 - buttons here so like if you go here you
114:37 - can actually click between map and
114:38 - tabular i believe you can move this
114:40 - range around just get different
114:42 - information so they're highly
114:43 - interactive
114:44 - uh then they're extremely stylized as
114:46 - you can see you can make them look
114:47 - really really good a report can contain
114:49 - many pages and you can assemble reports
114:52 - as easy as choosing a visualization and
114:54 - dragging it out so you take that just
114:56 - drag it out where you want to go and
114:57 - customize it from there
114:59 - now underneath what you can do is you
115:01 - can see the underlying data so it's just
115:04 - like tabular data and all the tables and
115:06 - fields that populate it and you can and
115:09 - i think with like a pro version you can
115:10 - modify it and so you cannot do these
115:12 - with dashboards so we're using the power
115:14 - bi service you're not going to get
115:15 - access to this
115:17 - when you're looking at data modeling
115:18 - it's again in power bi desktop you can
115:21 - see the relationships between models and
115:23 - modify them and do things with them
115:24 - again you cannot do this with dashboards
115:27 - and that's a key thing you need to
115:28 - understand between the interactive
115:30 - reports and the dashboards
115:35 - let's take a quick look here at power bi
115:37 - service and also dashboards which is
115:39 - very important for this so power bi is a
115:41 - cloud-based service where users view and
115:43 - interact with reports and where they can
115:45 - create dashboards and so here is um a
115:48 - screenshot of me logged into power bi if
115:50 - you wanna know how to get there you go
115:52 - app.powerbi.com
115:54 - and uh if you already have you have to
115:56 - even if you have a microsoft account you
115:58 - have to fill in a form and then it
115:59 - activates the service and you can go and
116:02 - explore some dashboards and reports
116:04 - right off the bat for free so it's very
116:06 - easy to jump into
116:07 - one of the concepts that's very
116:08 - important with power bi service is
116:10 - dashboards and so before we talk about
116:12 - that let's talk about what a tile is a
116:13 - tile is a snapshot of data pinned into
116:15 - your dashboard so here's an example of
116:17 - data a tile can be created from a report
116:18 - a data set a different dashboard qa q
116:21 - and a box excel sql server reporting
116:24 - service ssrs
116:26 - and many many more looking at a
116:28 - dashboard is a single page often called
116:31 - a canvas that tells a story through a
116:34 - visualization so there it is the
116:35 - visualizations you can see on the
116:36 - dashboard are called tiles you can pin
116:39 - tiles to a dashboard from reports okay
116:42 - [Music]
116:47 - it is very very very important that we
116:49 - know the difference between reports and
116:50 - dashboards so this isn't going to be a
116:52 - fun slide but we'll have to go through
116:54 - it and work our way through so let's
116:55 - talk about the difference for
116:56 - capabilities they both have pages but
116:58 - dashboard has a single page and reports
117:00 - has multiple pages for data sources one
117:03 - or more reports
117:05 - you can have one or more reports and one
117:08 - or more data sets per dashboard
117:10 - and for report a single data set per
117:12 - dashboard for filtering you can't filter
117:14 - or slice for filtering out reports many
117:17 - different ways to filter highlight and
117:18 - slice
117:19 - you can set alerts for dashboards you
117:21 - cannot for reports for features you can
117:25 - set one dashboard as your featured
117:27 - dashboard reports there's no such thing
117:29 - as a featured report
117:31 - you can see the underlying data set
117:33 - tables and fields so absolutely not for
117:35 - dashboards absolutely yes for reports
117:36 - for customization new and for reports
117:39 - you got tons of customization so there
117:42 - you go
117:43 - [Music]
117:47 - all right let's talk about paginated
117:48 - reports which are reports designed to
117:50 - fit into page formats so they can be
117:52 - printed or shared the data
117:54 - display of all data are tables which can
117:56 - span multiple pages so rdls is an xml
117:59 - representation of an sql server
118:01 - reporting service so that's an ssrs
118:04 - report definition file a report
118:05 - definition contains data retrieval and
118:07 - layout information for report pattern
118:08 - reports are just a visualization of dot
118:11 - rdl files so power bi report builder is
118:14 - used to design pixel perfect
118:16 - remember that word pixel perfect they
118:18 - really use that a hundred times over
118:20 - passionate reports using power bi report
118:22 - builder it is a tool specifically
118:24 - designed for creation of patching
118:25 - reports so if you want to figure out how
118:27 - to download this power bi thing within
118:30 - your power bi service go to the top
118:32 - right corner go to download i don't know
118:34 - why that's animated but it worked out
118:36 - fine and so once you're in there
118:39 - uh you will download the file and you'll
118:41 - install this really old looking software
118:44 - but i guess the thing is is that this is
118:46 - a huge pain point for companies i guess
118:48 - and they really make a huge emphasis on
118:50 - it so i guess we need to know what it is
118:52 - [Music]
118:57 - hey this is andrew brown from exam pro
118:58 - and we are looking at the power bi cheat
119:00 - sheet for the dp900 let's jump into it
119:02 - so the first is business intelligence or
119:04 - bi which is both a data analysis
119:06 - strategy and technology for business
119:08 - information helps organizations make
119:10 - data driven decisions now we're talking
119:13 - about power bi so power bi desktop a
119:15 - desktop app to design interactive
119:16 - reports from various data sources can be
119:18 - published to the power bi service then
119:20 - you have the power bi service also known
119:22 - as the power bi portal a web app to view
119:24 - reports and create interactive
119:26 - shareable dashboards by pinning various
119:28 - data sets and reports visualizations you
119:31 - have power bi mobile a mobile web app to
119:33 - view reports on the go
119:34 - power bi report builder a windows app
119:37 - that builds pixel perfect printable
119:39 - reports used to build page data reports
119:42 - power bi embedded embed power bi
119:44 - visualizations into web apps interactive
119:46 - reports reports in power bi drag
119:48 - visualizations load data from many data
119:50 - sources both
119:51 - in desktop and and service meaning like
119:54 - you can do you can make reports in both
119:57 - power bi desktop and power bi service
119:59 - okay
120:00 - paginate reports pixel perfect printable
120:02 - report files uh tabular data laid out in
120:05 - page format dashboards build shareable
120:07 - dashboards by pinning various power bi
120:09 - visualizations a single page report
120:10 - basically designed for a screen only for
120:13 - the power bi service dashboard tiles or
120:16 - just tiles is a representation or
120:18 - represent a visualization that has been
120:20 - pinned to a dashboard it could be a
120:22 - bunch of other things but that's what
120:23 - the key thing it is visualization is a
120:25 - visualization uh is a chart or graph
120:27 - that's backed by it says my but you see
120:29 - by a data set and uh whoops we went to
120:32 - the next part but that's it for power bi
120:34 - [Music]
120:38 - all right let's take a look at
120:39 - structured query language which uh
120:41 - stands for sql it's designed to access
120:43 - maintain data for a relational database
120:44 - management system on rdbms
120:47 - we use sql to insert update delete view
120:50 - data from our data databases tables and
120:53 - sql can join many tables and include
120:55 - many functions to transform uh the final
120:57 - output of results on the right hand side
120:58 - that is a real query that i use in my
121:01 - postgres database to um
121:03 - grab exam sets so if you're on the xampp
121:05 - pro platform and you're doing a
121:07 - particular set of an exam this query
121:09 - gets that relative information and you
121:11 - can see that it's polling if you look
121:13 - down below here
121:15 - it's joining in tag information and then
121:17 - it has like sub queries and stuff so
121:18 - it's a very complex query and doing that
121:20 - formatting the sql syntax was
121:22 - standardized as iso 9075 that won't show
121:25 - up on your exam but it's good to know
121:27 - relational databases will mostly adhere
121:29 - to the standard while adding in their uh
121:31 - not one but own database specific
121:34 - features sql's highly transferable skill
121:36 - and we see sql being used in
121:38 - non-relational databases provide a
121:40 - popular and familiar querying tool so
121:42 - it's something you definitely want to
121:43 - know how to do
121:44 - [Music]
121:48 - all right let's compare olap to oltp so
121:52 - online transactional processing versus
121:54 - online analytical processing when we're
121:56 - talking about ltp we're generally using
121:58 - databases so databases is built to store
122:00 - current transactions and enables fast
122:02 - access to specific transactions for
122:03 - ongoing business processing so think of
122:06 - uh you know any kind of sql server and
122:09 - then on the right hand side we have data
122:11 - warehouses a data warehouse is built to
122:12 - store large quantities of historical
122:14 - data and enable fast complex queries
122:16 - across all the data so when we visually
122:18 - look at the oltp we have a bunch of
122:20 - small transactions that are evenly
122:23 - distributed so they look pretty similar
122:24 - in the read and writes and then for data
122:26 - warehouse we have very very few
122:30 - retransactions and uh we have large
122:33 - payloads i think that the arrows are
122:35 - supposed to be pointing this way but
122:37 - that's okay it's not a big deal
122:39 - so when we're talking about databases
122:40 - you have a single data source you have
122:42 - short transactions small and simple
122:44 - queries with an emphasis on rights many
122:45 - transactions late it's latency sensitive
122:48 - and you have small payloads on the olap
122:51 - side we have multiple data sources so
122:53 - you're ingesting data long transactions
122:55 - long and complex queries with an
122:56 - emphasis on reads fewer transactions or
122:59 - very few
123:00 - and throughput sensitive and large
123:02 - payloads the use case over here would be
123:04 - general purpose adding items to your
123:05 - shopping cart would be an example a use
123:07 - case on the analytics side would be
123:08 - generating reports so there you go
123:14 - [Music]
123:15 - all right let's take a look at some open
123:16 - source relational databases that we know
123:18 - we are going to definitely encounter on
123:19 - azure starting with mysql which was
123:21 - created by my school a b i believe
123:23 - that's like a switzerland or a swiss
123:25 - company and they they are required by
123:27 - sun microsystems and then they are
123:29 - required by oracle and mysql was or is
123:32 - an open source project uh so mysql is a
123:35 - pure relational database rdbms it is a
123:38 - simple database which makes it easy to
123:40 - set up using maintain has multiple
123:41 - storage engines so in odb and my ism
123:45 - when it says that there's multiple they
123:46 - just mean there's two because i don't
123:48 - know of any other than those two but
123:50 - it's the most popular relational
123:51 - database the reason why it's been around
123:53 - forever it was one of the
123:54 - earliest mysql databases that was open
123:57 - source which is a very important thing
123:59 - to note
124:00 - um you know and it's just very easy to
124:02 - use
124:03 - mario db is a fork of mysql by the
124:05 - original creators of my
124:07 - mysql ab after oracle was acquired my
124:10 - school
124:11 - required mysql there was a concern that
124:13 - oracle may change the open source
124:15 - licensing or stop future my school from
124:18 - being free to use if you know oracle
124:20 - they'll like to
124:21 - charge charge you for their stuff
124:23 - and oracle has their own database and so
124:25 - you know there was a lot of fear around
124:27 - that and the thing is is that when my
124:29 - sql ab sold their database to sun
124:32 - microsystems it it's because they
124:33 - trusted sun but then they didn't know
124:36 - that sun was going through a lot of
124:37 - financial difficulties and then
124:39 - literally a year later some was acquired
124:40 - by oracle
124:42 - it's just how it goes eh so
124:44 - they would have never sold to oracle
124:46 - originally that's why we have mario db
124:48 - um
124:49 - then we have postgres which evolved from
124:51 - ingress
124:52 - the ingress project at the university of
124:54 - california postgres is an object-related
124:57 - relational database so o-r-d-b-m-s
125:01 - it just has a single storage engine
125:03 - which i guess is the ingress engine
125:05 - right here and so it's the most advanced
125:08 - relational database it can support full
125:09 - text search table inheritance triggers
125:11 - rows data types request slot you know
125:13 - they say the most advanced i mean it's
125:15 - more advanced than mysql
125:17 - postgres is the database i love to use
125:19 - it's such a great service i don't
125:21 - understand how it's object relational i
125:23 - think it's just how they store the data
125:24 - underneath and that's what makes it so
125:26 - flexible but i can tell you that
125:28 - postgres is a lot easier to use like
125:30 - initially mysql the syntax is easier but
125:33 - postgres in terms of data modeling is a
125:35 - lot easier because you could just create
125:37 - columns you don't have to worry about
125:38 - them whereas like mysql you've got to
125:40 - fiddle around with the data types it's
125:41 - very frustrating or there's like serious
125:42 - limitations on rows so yeah that's the
125:45 - two there okay and if you want to deploy
125:47 - these on azure it's really simple uh you
125:49 - just type in the name you go mario or
125:52 - mysql or postgres and then you just do
125:54 - azure database for mariodb and you just
125:56 - launch a server okay
126:02 - let's talk about read replicas for azure
126:04 - databases here so a read replica is a
126:06 - copy of your database that is kept
126:08 - synced with your primary database and
126:10 - this additional database is used to
126:12 - improve read contention if you've never
126:14 - heard the word contention before it
126:16 - means heated disagreement uh so the idea
126:19 - is that if you have a lot of reads and
126:21 - it's and it's
126:23 - hurting the database you can offload
126:24 - those reads to your secondary database
126:27 - that's dedicated specifically for read
126:29 - operations so re-replicas can be applied
126:31 - to azure sql and the managed instances
126:34 - so i guess you can't do it with the
126:35 - azure vms
126:37 - uh like the virtual machines the sql vms
126:40 - that's what they're called you can have
126:42 - multiple rewrite for a database i can't
126:44 - remember what the range is maybe it's
126:45 - between two to six i don't think that
126:46 - matters for the exam and just kind of a
126:48 - visual you have your read and writes
126:50 - that go to your primary and then a lot
126:51 - of your reads go to your read replicas
126:54 - and a very common use case to have a
126:55 - re-replica is so that you can use it as
126:57 - an olap when you are a very small size
127:00 - i'm not going to talk about an exam but
127:02 - i just know from a practical standpoint
127:03 - that's something that i've done multiple
127:05 - times over the years so there you go
127:06 - [Music]
127:10 - all right let's take a look at scitis on
127:12 - azure and honestly i don't know if it's
127:14 - cetus situs sometimes i want to say
127:16 - citrus but
127:19 - i could not find a pronunciation for it
127:21 - so i'm going to call it situs and it is
127:23 - an open source postgres extension that
127:24 - transform postgres into a distributed
127:26 - database and so situs extends postgres
127:29 - to provide better support for database
127:31 - sharding real-time queries multi-tenancy
127:34 - which is super useful
127:35 - time time series workloads and if you go
127:38 - to azure postgres and you see hyperscale
127:41 - option it really is just using siteis so
127:44 - um this is a really really really really
127:46 - good service if you are using postgres
127:48 - it's one of the
127:49 - few reasons i would consider using azure
127:51 - for my database because i use postgres
127:53 - as my primary one this will absolutely
127:55 - not show up on the exam but i think it's
127:57 - very useful to know what the service is
127:59 - [Music]
128:03 - all right let's take a look at the azure
128:05 - sql family when we say sql we're talking
128:07 - about microsoft's version of sql
128:10 - and if you search sql you'll see a bunch
128:13 - of stuff here even more than this and it
128:15 - can get really confusing but you
128:16 - absolutely need to know this for the
128:17 - exam you need to know the difference
128:19 - between these three main services
128:21 - so at the top you have sql server on
128:23 - azure virtual machines or sql you'll
128:26 - just see like vm for desktop vm a lot
128:28 - when you need os level control and
128:30 - access when you need to lift and shift
128:32 - your workloads to the cloud when you
128:34 - have an existing sql license and you
128:36 - want to save money via the azure hybrid
128:38 - benefit this is when you're going to use
128:40 - that okay if you've never heard the term
128:42 - lift and shift the idea is that on your
128:44 - on-premise environment you're running a
128:46 - virtual machine that is your database
128:48 - and you can literally save a virtual
128:50 - image import that into azure and it runs
128:52 - exactly how it did
128:54 - on your on premise so you don't get a
128:56 - lot of the advantages of the cloud but
128:58 - the idea is it's the easiest way to get
128:59 - onto the cloud right
129:01 - the next option is sql managed instance
129:03 - this is when you have an existing
129:04 - database you want to modernize it's the
129:07 - broadest sql server engine compatibility
129:09 - highly available just disaster recovery
129:11 - automated backups ideal for most
129:12 - migrations to cloud so the thing is if
129:14 - you're going to do a lift and shift
129:17 - you can you can
129:19 - kind of go to esco manage so you
129:20 - probably have to do a transformation
129:21 - some kind of like etl job or something
129:23 - to go into here but the idea is that
129:25 - there's a lot of different versions of
129:28 - um
129:29 - of uh mssql depending on how old it is
129:32 - and stuff like that so if you aren't
129:34 - going to be bringing your license or
129:35 - need os level you really want to be
129:36 - using this one because you get all the
129:38 - built-in scalability stuff right
129:40 - the third one is azure sql database this
129:42 - is a fully managed sql database designed
129:44 - to be fault tolerant built in disaster
129:46 - recovery hive it's highly available
129:49 - designed to scale uh and it's the best
129:51 - option um but again you know if you have
129:54 - an older database maybe you could do a
129:55 - transformation to it
129:56 - uh and then underneath it has sql
129:58 - servers i thought this was a fourth
130:00 - option but really when you go to azure
130:02 - sql and launch it you actually have to
130:03 - create a server because you can have
130:05 - multiple sql servers associated to a
130:07 - database
130:08 - and there's things called like what's it
130:09 - called like elastic pool or something
130:11 - like that so
130:12 - that's just the underlying server for
130:13 - the azure sql database it's not a
130:15 - service in itself
130:17 - but it's just a component of that
130:18 - service okay even though you can go in
130:20 - the ui and see a list there makes it
130:22 - really really really confusing but yeah
130:24 - there you go
130:30 - so azure elastic pools is a feature of
130:32 - azure sql and it allows you to uh there
130:35 - are simple cost effective solutions for
130:37 - managing and scaling multiple databases
130:39 - that have varying and unpredictable
130:41 - usage demand so databases in elastic
130:44 - pool are on a single server and share a
130:46 - set number of resources at a set price
130:48 - elastic pools in azure sql enable sas
130:50 - developers to optimize the price
130:52 - performance of a group databases within
130:54 - a prescribed budget while delivering
130:55 - performance elasticity for each database
130:57 - why would somebody want to do this
130:59 - because like it doesn't seem like a good
131:00 - practice to put a bunch of databases on
131:02 - a single server it's more like you'd
131:04 - rather want a database to be distributed
131:06 - across
131:06 - servers so
131:08 - if you're running a sas product software
131:10 - as a service you'll have multi-tenancy
131:12 - meaning that each person has their own
131:14 - database there's different levels of
131:16 - tenancy but if you did give let's say
131:18 - you had a company and um
131:20 - or like you had five large clients and
131:22 - they use the same software and they're
131:23 - all varying sizes
131:25 - but they're not large enough to justify
131:27 - their own server this would be the
131:28 - service for you right where you are
131:30 - constantly spinning up databases per
131:32 - customer
131:33 - um but i don't think i would ever use
131:35 - this in practicality and i am a
131:36 - multi-tenant sas but uh it's nice that
131:38 - they provide that option so there you go
131:40 - [Music]
131:44 - hey this is andrew brown from exam pro
131:46 - and we are on to the relational database
131:48 - cheat sheet uh and so let's jump into it
131:51 - so structure query language sql designed
131:53 - to access and maintain data for a
131:55 - relational database management system
131:57 - online transaction processing keyword
131:59 - there is transaction so frequent and
132:01 - short queries for transactional
132:02 - information so databases any kind of
132:04 - generic workload or web app online
132:06 - analytical processing so complex queries
132:09 - for large databases to produce reports
132:11 - and analytics so think data warehouse
132:14 - on to the open source relational
132:16 - databases we got mysql a pure relational
132:18 - database easy to set up most popular
132:21 - open source relation relational database
132:23 - definitely something i started off with
132:24 - mariodb is a fork of mysql postgres is
132:27 - an object
132:28 - relational database now my favorite
132:31 - relational database to use is more
132:32 - advanced and well liked among developers
132:35 - read replicas is a duplicate of your
132:37 - database in sync with the main to help
132:39 - to reduce reads on your primary database
132:41 - now talking about azure sql it's an
132:43 - umbrella service for uh for different
132:45 - offerings of mssql databases hosting
132:47 - services so we have sql vm so for lift
132:51 - and shift when you want os access and
132:53 - control or you need to bring your own
132:55 - license for azure hybrid benefit
132:57 - manage sql for lift and shift when your
133:00 - broadest when you need the broadest
133:01 - amount of compatibility with sql
133:02 - versions mssql in particular you can use
133:05 - uh
133:06 - manage sql on on-premise by using azure
133:09 - arc it gives you many of the benefits of
133:11 - a fully managed database but it's not as
133:13 - good as the azure sql database which is
133:16 - a fully managed sql database has a few
133:18 - options here you can run it as a single
133:19 - server run it as a database which is a
133:21 - collection of servers run in an elastic
133:23 - pool so databases of different sizes
133:25 - residing on this on one server to save
133:29 - cost
133:30 - uh then we have connection policies so
133:32 - we have three modes we got default so
133:34 - choose proxy or default initially
133:35 - depending if the server is within or
133:37 - outside the azure network we have proxy
133:39 - outside the azure network proxy through
133:41 - a gateway
133:42 - it's important to remember to listen on
133:44 - port 1443 this might show up on your
133:46 - exam so remember this port 1443 when
133:49 - connecting via proxy mode through a
133:51 - gateway outside the azure network and
133:52 - then last is redirect redirected with
133:54 - the azure network and that's the
133:55 - recommended way to do it and i just want
133:57 - to point out for these three here i
133:59 - didn't write it in here but remember
134:01 - that this one here is for
134:02 - infrastructures code this one is for
134:04 - platform as a service and this one's for
134:05 - platform as a service
134:10 - okay all right let's talk about tsql
134:13 - which stands for transact sql and it's a
134:15 - set of programming extensions from
134:17 - sybase and microsoft that added feature
134:19 - several features to the structured query
134:21 - language if you've never heard of cybase
134:24 - i think the original company that
134:25 - actually made the microsoft sql database
134:28 - and then maybe microsoft bought them out
134:30 - or et cetera
134:31 - but there's a long history there so tsql
134:34 - expands on the sql standard to include
134:36 - procedural programming local variables
134:38 - very various support functions for
134:39 - string processing date processing
134:42 - mathematics changes to the delete and
134:43 - update statements for the microsoft sql
134:46 - servers there are five groups of sql
134:48 - commands and honestly there's five
134:50 - groups for regular sql
134:53 - actually normally they'll just say
134:55 - even for this exam they're only going to
134:56 - tell you of about the definition
134:58 - manipulation one but i'm going to tell
135:00 - you all of them because you should know
135:01 - all of them it really helps to know them
135:03 - so the first is data definition language
135:05 - so this is
135:06 - ddl used to define
135:09 - the database schema
135:10 - we have data query language dql used for
135:13 - performing queries on the data
135:16 - data manipulation language dml
135:19 - manipulation of the data in the database
135:21 - data control language dcl rights
135:23 - permissions and controls of the database
135:25 - transaction control language tcl
135:27 - transactions within the database and so
135:29 - now that we've covered what tsql is
135:32 - let's dive in and actually look at all
135:34 - these types of documents
135:39 - let's start off with the data definition
135:41 - language which is
135:42 - sql syntax commands for creating and
135:44 - modifying the database or database
135:45 - objects so tables index views store
135:48 - procedures functions and triggers the
135:50 - first is the create
135:52 - command here and so we can create a
135:53 - database or database object so here you
135:55 - would just say create whatever it is you
135:57 - want to create table database etc so
136:00 - here we're creating a table called users
136:01 - and we're providing those fields okay
136:04 - then we have alter so this alters the
136:06 - structure of an existing database so
136:08 - alter table whatever it is the table and
136:10 - then we can add a column if we want to
136:12 - drop the database that deletes all the
136:14 - objects from the database truncate would
136:16 - just delete the records within the
136:17 - database comments is just a comment
136:20 - and rename is if we want to rename a
136:23 - database object now this is what's
136:24 - interesting here where we have execute
136:26 - sp rename so when we said that tsql
136:29 - extends this is its own little special
136:32 - language because in other
136:34 - other sql
136:36 - variants it definitely does not look
136:37 - like that okay
136:38 - [Music]
136:42 - let's take a look at data manipulation
136:44 - language dml and so this is going to
136:46 - have to do with anything with
136:47 - manipulating data so the first thing is
136:49 - we have an insert command to be able to
136:51 - uh insert data so here you can see we
136:53 - say the values we want and if you're
136:55 - wondering how does it know what values
136:57 - it's going to be the order in which the
136:59 - column appears in the in the actual
137:01 - table that's how it knows that andrew
137:03 - goes under first name and email goes in
137:05 - like the email goes into the email field
137:07 - then for update this is uh when we want
137:09 - to update existing materials so we'll
137:11 - say update users then we'll have to say
137:14 - where so we want to match the id 8
137:16 - update user 8 and then we'll set the
137:18 - values that we want to change then we
137:20 - can delete a user very simple delete
137:21 - from users where id equals 6
137:24 - merge or upsert to insert or update
137:26 - records at the same time i don't see
137:28 - these in other languages so i think it
137:29 - might be a tsql specific thing
137:31 - it's kind of hard to show this because
137:33 - they're very large queries but if you
137:34 - need to do an insert and update at the
137:36 - exact same time you use this then you
137:38 - have call this allows you to call
137:40 - proceed or java sub programs basically
137:43 - functions so let's say you need to
137:44 - calculate a function uh to calculate the
137:46 - distance between toronto and chennai uh
137:49 - you could uh do that you have lock table
137:51 - and this is for concurrency control to
137:53 - ensure two people are not writing to the
137:55 - to the program at the same time okay
137:58 - [Music]
138:02 - all right let's take a look here at the
138:03 - data query language dql and the first
138:05 - thing we have is the select and by the
138:07 - way every if it's query that means it
138:09 - has everything to do with selecting data
138:11 - okay so here what we're going to do is
138:13 - select the these particular fields and
138:15 - do from users if we want to get all
138:17 - fields we can just do an asterisk here
138:19 - um but that's the idea is like i want
138:21 - these fields from this users you could
138:22 - even say like where and other stuff in
138:24 - there we have show so this describes
138:26 - what a table looks like most other
138:27 - languages like bicycle would just say
138:29 - like show and then the table name but
138:31 - here again we have that these kind of
138:33 - weird exec sp columns thing which again
138:36 - is a tsql specific thing
138:38 - and so show would describe what the
138:39 - table looks like so what columns is
138:41 - contained okay we have explained plan so
138:44 - returns the query plan of a microsoft
138:45 - azure synapse analytics sql statement
138:47 - without running the statement
138:49 - this thing is really complicated i could
138:51 - not show you an example there but you
138:53 - know that's what it does help is just
138:55 - like i want to understand more
138:56 - information about database objects so
138:58 - here you're asking more information
138:59 - about the user's table again this is the
139:02 - special tsql stuff
139:08 - all right let's take a look here at data
139:09 - control and this has to do with well
139:11 - control right so we have grant saying
139:14 - i have a i have a table called employees
139:16 - i'm only going to let the um
139:18 - ts goal user or the uh mssql user andrew
139:22 - only be able to select insert update and
139:23 - delete on then you have revoke and
139:25 - that's the opposite let's just say you
139:27 - know we don't want bacon to be able to
139:28 - delete anything on the employees table
139:30 - and that's pretty much it
139:34 - [Music]
139:36 - let's take a look here at transaction
139:37 - control language tcl so tcl commands are
139:40 - used to manage transactions in a
139:42 - database transactions is when you need
139:44 - multiple things to happen
139:46 - and if they don't all happen then you
139:48 - roll back on them okay so this is really
139:50 - important in finance where you have
139:51 - multiple people that have to be part of
139:53 - the purchasing decision
139:54 - and if all purchases don't follow
139:56 - through then you don't want to commit
139:58 - that transaction okay so we have command
140:01 - so set to permanently save any
140:03 - transaction to the database rollback
140:05 - restores the database to the last
140:07 - committed state save point used to
140:09 - temporarily save a transaction so that
140:10 - you can roll back to this uh to the
140:12 - point whenever necessary set
140:13 - transactions specify characteristics for
140:16 - the transaction
140:17 - [Music]
140:21 - all right just a quick review of all the
140:23 - sql documents we or syntax documents we
140:25 - just looked at and so we had ddl which
140:28 - is for defining dml which is for
140:30 - manipulating dql which is for querying
140:32 - dcl which is for controlling and tcl
140:34 - which is for transacting now i
140:36 - highlighted ddl and dml in red because
140:39 - these are what the exam will focus on
140:41 - and sometimes they simplify and they'll
140:42 - take things like that that go in um
140:45 - like select and they'll just put it in
140:46 - manipulation okay so these are the two
140:48 - main ones that you'd have to choose
140:50 - between but i wanted to show you all
140:52 - five of them because this is really what
140:53 - sql is based off of uh and so you know
140:56 - this is just the right way of looking at
140:58 - it this is not an exhaustive list of all
141:00 - the possible commands but it was what i
141:02 - could terry pick out that i recognized
141:03 - that i was familiar with okay
141:05 - [Music]
141:09 - hey this is andrew brown from exam pro
141:11 - and we are on to the tsql cheat sheet so
141:13 - transact sql is a set of programming
141:15 - extensions from sybase and microsoft
141:16 - that adds several features to the
141:18 - structured query language this is used
141:20 - for ms sql databases okay for mss google
141:24 - servers there are five groups of sql
141:25 - commands and so we have data definition
141:28 - language used to define the database
141:30 - schema data query language used for
141:32 - performing queries on data data
141:34 - manipulation language manipulation of
141:36 - data in the database data control
141:38 - language rights permissions and other
141:39 - controls of the database
141:41 - transaction control language tcl
141:44 - transactions within the database so on
141:46 - the exam
141:47 - they're going to ask you either this
141:51 - or this they don't bother with all of
141:53 - them but they're actually r5 okay and so
141:55 - that's all you need to know here
141:56 - [Music]
142:01 - let's talk about connectivity
142:02 - architecture so when a connection from a
142:04 - server to a azure sql database the
142:06 - client will connect to a gateway that
142:08 - listens on port
142:10 - 1443 and i want to remember that port
142:12 - number because it's an important part
142:13 - number that might show up on your exam
142:15 - okay so over here on the right-hand side
142:17 - we have a virtual machine connecting to
142:19 - various different sql servers
142:22 - and so the idea here is that
142:25 - based on the connection policy the
142:26 - gateway will grant traffic and route
142:27 - access to the appropriate database so we
142:29 - actually have three kinds of policies we
142:31 - got proxy so connections are proxy
142:33 - through a gateway increased latency and
142:35 - reduced throughput intended for
142:36 - workloads connecting from outside the
142:37 - azure network redirect which is mostly
142:40 - recommended establishes a direct
142:42 - connection reduced latency improved
142:43 - throughput intended for workloads
142:45 - connecting inside the azure network and
142:47 - default which is just going to default
142:49 - if you launch a vm outside the azure
142:51 - network it's going to use proxy if you
142:52 - launch it within the azure network it's
142:54 - going to
142:55 - use redirect so this thing this port
142:57 - 1443 is only important when you're doing
142:59 - proxy because if you're internal you
143:01 - don't need to go through that port
143:02 - there's no gateway to pass through
143:05 - but yeah that is just the different
143:07 - kinds of connection policies there
143:09 - [Music]
143:13 - all right so let's take a look here at
143:14 - ms sql database authentication so during
143:16 - the setup of your ms sql database you
143:18 - must select an authentication mode you
143:20 - got two options here windows
143:22 - authentication mode which enables both
143:24 - windows authentication and disables sql
143:26 - server authentication and mixed mode
143:28 - which enables both windows
143:29 - authentication and sql server
143:31 - authentication so if you were to remote
143:33 - into your windows machine and under your
143:35 - server properties look under security
143:37 - and server authentication there are
143:39 - those two options interestingly they
143:40 - don't call it mixed mode in the ui but
143:43 - that's what it is it's called mix mode
143:45 - so let's talk about what windows
143:46 - authentication and sql
143:49 - server authentication is so windows
143:51 - authentication which is
143:52 - the recommended way is specific windows
143:55 - users and groups group accounts are
143:57 - trusted to log into the sql server and
144:00 - very and this is the most secure and
144:01 - easy way to modify revoke privileges
144:04 - because you know if they're windows
144:06 - users that means that you can then
144:07 - manage them from azure active directory
144:09 - right
144:10 - then you have sql server authentication
144:12 - so we have a username and password which
144:14 - is set and stored on the primary
144:15 - database you cannot use a
144:18 - kerberos security protocol so that's one
144:20 - disadvantage login password must be
144:22 - passed over the network at the time of
144:24 - connection so that's an additional
144:25 - attack vector but this is an easier way
144:28 - to connect to the database from outsider
144:29 - domain or from a web-based interface so
144:31 - it's just going to be based on the uh
144:33 - the scenario that you're in but if you
144:34 - can just stick with windows
144:36 - authentication
144:37 - [Music]
144:41 - let's take a look here at network
144:42 - connectivity so for your sql database
144:44 - you need to choose either a public or
144:46 - private endpoint a public endpoint is
144:48 - reachable outside the azure network over
144:49 - the internet and you would use firewall
144:51 - rules to protect your database for
144:52 - private endpoints you're uh they're only
144:54 - reachable within the azure network or
144:56 - connecting
144:57 - uh or originating from inside the
144:59 - network so you would use azure private
145:01 - links to keep your traffic network
145:03 - within the azure network so here's just
145:05 - the two options you would see when you
145:07 - provision your database you'll just see
145:09 - here that you choose either public or
145:10 - private and you're setting firewall
145:12 - rules or you're creating private
145:13 - endpoints okay
145:15 - [Music]
145:18 - well let's take a look here at azure
145:20 - defender for sql which is a unified
145:22 - package for advanced sql security
145:24 - capabilities and what it does is
145:26 - vulnerability assessment and advanced
145:28 - threat protection so azure defender is
145:30 - available for azure sql the manage
145:32 - instance and synapse analytics and what
145:34 - it does is it discovers and classifies
145:36 - sensitive data or classify sensitive
145:39 - data surfacing and mitigating potential
145:41 - database vulnerabilities detecting
145:43 - anomalous activities and you can turn it
145:45 - on at any time and you just pay a
145:46 - monthly cost within the azure portal so
145:49 - there you go
145:54 - let's take a look at azure database
145:55 - firewall rules so
145:57 - azure databases are protected by server
145:59 - firewalls a server firewall is an
146:01 - internal firewall that resides on the
146:03 - database server all connections are
146:05 - rejected by default to the database so
146:07 - once your database is provisioned you
146:09 - have an option where you click server
146:11 - firewall and what you're going to do is
146:13 - configure it so you'll say
146:15 - here i allow azure so i give 0000
146:19 - and notice also the connection policy
146:20 - remember we talked about that before so
146:22 - you can set proxy or
146:23 - redirect
146:25 - and so there's that there and then if
146:27 - you wanted to do it via tsql you could
146:29 - as well this is allow only allow the
146:31 - server at
146:32 - zero point four at zero zero zero point
146:34 - four because you can specify your range
146:36 - there is azure firewalls which is uh a
146:39 - totally different service and then
146:40 - there's network security groups which it
146:43 - which is like a logical firewall around
146:45 - your uh vm or your i'm sorry around your
146:49 - nic cards and the subnet but this is the
146:52 - one we were talking about here which is
146:53 - the server firewalls for azure databases
146:55 - okay
146:56 - [Music]
147:00 - all right let's take a look here at
147:01 - always encrypted which is a feature that
147:03 - encrypts columns in the azure sql
147:04 - database or sql server so if you had a
147:07 - column like a credit card number and you
147:08 - wanted to always keep it encrypted you'd
147:10 - use always encrypted and so always
147:12 - encrypted uses two types of keys we have
147:14 - column encryption keys which are used to
147:16 - encrypt the data in an encrypted column
147:19 - and call them master keys a key uh
147:21 - protecting the key that encrypts one or
147:23 - more column encryption keys and you can
147:25 - always uh uh you can al you can apply
147:28 - always encrypted using tsql and so since
147:31 - there is a key that encrypts the key
147:32 - that is called envelope encryption and
147:34 - that is a great way of doing that i
147:35 - imagine that maybe it gets stored in eks
147:37 - or whatever the name of the service that
147:39 - azure calls for their uh encryption keys
147:46 - let's take a look here at role-based
147:48 - access control specifically for
147:49 - databases so role-based asset controls
147:52 - is when you apply roles to users to
147:53 - grant them fine grade
147:56 - actions for specific azure services and
147:58 - there's four in particular that we
148:00 - really do care about here uh to
148:01 - databases and this is and this will
148:03 - probably show up on your exam so you
148:05 - definitely need to know these we have
148:07 - sqldbcontributor so this role allows you
148:09 - to manage sql databases but don't access
148:11 - them can't manage their security related
148:13 - policies or their parent sql servers you
148:15 - have sql managed instance contributor
148:17 - this is so you can manage sql managed
148:19 - instances and required network
148:21 - configuration can't give access to
148:23 - others sql security manager manage the
148:26 - security related policies of sql servers
148:28 - and databases
148:29 - uh but not access to the sql servers and
148:32 - last is sql server contributor manage
148:34 - sql servers and databases but not access
148:38 - not have access to them those sql
148:40 - servers okay so our bacs you definitely
148:43 - want to know these four okay
148:44 - [Music]
148:48 - let's take a look here at transparent
148:50 - data encryption tde which encrypts data
148:52 - at rest for microsoft databases it can
148:54 - be applied to server sql uh or sql
148:57 - servers azure sql databases azure
148:59 - synapse analytics tde does real-time i o
149:02 - encryption and decryption of data logs
149:04 - and files encryption uses a database
149:06 - encryption key called a dek data
149:09 - database boot record stores the key for
149:11 - availability during recovery the d e key
149:14 - the d e k is a symmetric key so it's the
149:17 - same cryptographic key for both of the
149:19 - encryption uh of plain text and
149:20 - decryption of cipher text just to give
149:22 - you a visual there to help you out the
149:24 - idea is you have something that's plain
149:25 - text you use the same key to encrypt it
149:27 - and then the same key to decrypt it
149:30 - and that's just how that works so the
149:32 - steps to apply tdd to database create a
149:34 - database master key create a certificate
149:36 - to support the tde create the database
149:38 - encryption key enable tde on the
149:40 - database so that's just how you do it
149:42 - there within the azure portal and there
149:44 - you go
149:45 - [Music]
149:49 - before we start talking about what
149:50 - dynamic data masking is let's define
149:52 - what is data masking this is when a
149:54 - request for data is transformed to mask
149:56 - the sensitive data so imagine you have a
149:58 - credit card here and you do not want to
150:01 - expose that to particular users maybe
150:03 - there are part of your support team or
150:05 - or even the end user themselves and so
150:07 - the idea is that it's in the database
150:09 - it's stored in its raw format it's
150:11 - untransformed but what it'll do is pass
150:13 - through a masking service and that will
150:16 - have different rules on it which will
150:17 - then apply a particular filter so here
150:20 - we'll only show the last three so
150:21 - dynamic data masking which is a feature
150:23 - of a particular
150:27 - azure sql servers
150:29 - anyway can be applied to azure sql and
150:31 - manage instances and synapse um and so
150:34 - the idea is you would just turn this
150:35 - feature on and then you create a masking
150:38 - policy so you could say exclude a
150:40 - particular users for masking so like
150:42 - let's say you have like root users admin
150:44 - users that need to see all the data you
150:45 - can do that you make masking rules so
150:48 - what fields should be masked and then
150:49 - you have masking functions so how to
150:51 - apply the masking field so here this
150:53 - would be a masking function that would
150:55 - say okay only show the last three
150:57 - letters okay
151:02 - let's take a look at private links so
151:04 - azure private links allows you to
151:05 - establish secure connections between
151:06 - azure resources so traffic remains
151:08 - within the azure network so here's a big
151:10 - graphic of what that looks like and the
151:12 - idea is that you have a private link
151:14 - endpoint uh which is just a network
151:16 - interface that connects you privately
151:17 - and securely to a service powered by
151:19 - azure private link and private endpoints
151:22 - uses a private ip address for your vnet
151:24 - so many azure services by default work
151:26 - with private link and third party
151:28 - providers can be powered by private link
151:30 - as well private link service which
151:32 - allows you to connect your own workloads
151:33 - to private link you may need an azure
151:35 - standard internal load bouncer to
151:37 - associate with the link service but the
151:38 - idea here is that if you have your sql
151:40 - server
151:41 - right
151:42 - that way you can connect it to your
151:44 - on-premise
151:46 - loads or if you have an sql server on
151:48 - your on-premise and you want to do it to
151:50 - a vm that's what you're going to use
151:52 - okay
151:57 - hey this is andrew brown from exam pro
151:59 - we are on to database security for
152:01 - azure the dp900 so let's jump into it so
152:04 - mssql database authentication we have
152:06 - two modes when setting it up when you're
152:09 - remoting into a windows machine so you
152:10 - have windows authentication mode which
152:12 - enables windows authentication and
152:13 - disables sql server authentication and
152:16 - we have mix mode where enables both of
152:18 - these things what are these things well
152:20 - windows authentication is when you
152:22 - authenticate via windows users and sql
152:24 - server authentication is between the
152:25 - user and password you can connect from
152:27 - anywhere windows authentication is the
152:29 - recommended one because it's just more
152:31 - secure
152:32 - for network connectivity we have public
152:34 - endpoints so they're reachable outside
152:35 - the azure network over the internet you
152:37 - use server firewalls for production and
152:40 - you have private endpoints so only
152:42 - reachable with the azure network so use
152:43 - azure private links to keep traffic
152:45 - within the azure network azure defender
152:47 - sql a unified package for advanced sql
152:49 - server security capabilities for
152:51 - vulnerability assessment and advanced
152:53 - threat protection
152:54 - server firewall rules an internal
152:56 - firewall that resides on the database
152:57 - server all connections are rejected by
152:59 - default uh to the database always
153:01 - encrypted a feature that encrypts
153:03 - columns in an azure sql database or sql
153:06 - server
153:06 - role-based access uh
153:09 - controls for databases so these are
153:10 - roles you need to know the sql db
153:12 - contributor manages the sql database but
153:14 - not access them can can't manage their
153:16 - security-related policies or their
153:18 - parent sql servers sql manage instance
153:20 - contributor manage sql instances and
153:22 - require network configuration
153:24 - configuration can't
153:26 - can't give access to others sql security
153:29 - manager manage the security related
153:31 - policies of sql servers databases but
153:33 - not access to sql servers and the last
153:35 - is sql server contributor manage sql
153:38 - servers databases but not access to
153:41 - them to the sql servers okay you have
153:44 - transparent data encryption td encrypts
153:46 - data at rest for microsoft databases in
153:48 - many cases it's already turned on for
153:50 - you dynamic data masking you can choose
153:52 - your database columns that will be
153:54 - masked obscured for specific users azure
153:57 - private links allows you to establish
153:58 - secure connections between azure
153:59 - resources
154:00 - so traffic remains within the azure
154:02 - network i should have put one underneath
154:04 - but this is generally if you want to
154:05 - also connect in a hybrid connection okay
154:08 - so there you go
154:09 - [Music]
154:13 - let's take a look at what a key value
154:15 - store is so a key value store is a data
154:18 - store that is really dumb but it's super
154:21 - super fast okay and so they'll lack
154:23 - features that you would normally see in
154:24 - relational databases like relationships
154:26 - indexes aggregation transactions all
154:28 - sorts of things but you know there is a
154:31 - trade-off for that speed okay
154:33 - and so here is kind of a representation
154:35 - of a key value store which uh you have a
154:37 - key which is a unique you know key to
154:39 - identify the value and i'm representing
154:42 - the value as a bunch of ones and zeros
154:43 - because i want you to understand that
154:44 - there aren't really columns it's just
154:46 - key and value so the idea is that
154:48 - imagine that those ones and zeros
154:50 - actually represent
154:51 - a dictionary and that's usually what
154:53 - they are is it associative array hash
154:54 - dictionary underneath
154:56 - okay and so even though it looks like
154:58 - you know what i mean like if this was a
154:59 - relational database you know you could
155:01 - see these as kind of like columns and so
155:03 - if we kind of did that that's how a key
155:06 - value store can kind of mimic
155:08 - um you know a tabular data right
155:11 - but the thing is is that you know there
155:13 - is no consistency between the the rows
155:16 - hence it is schema-less but that's kind
155:18 - of a way to get tabular data from key
155:20 - values but due to their simple design
155:22 - they can scale well well beyond
155:23 - relational databases so relational
155:25 - databases it becomes very hard to shard
155:27 - them and do a bunch of other stuff with
155:28 - them but key value stores are super easy
155:30 - to scale but you know they come with a
155:33 - lot of extra engineering around them
155:35 - because of these missing features
155:40 - all right let's talk about document
155:42 - stores so document store is a no skill
155:43 - database that stores document as its
155:45 - primary data structure a document could
155:46 - be an xml but it's most commonly json or
155:49 - json like structure and documents
155:52 - stores are sub classes of key value
155:54 - stores so the components of a document
155:56 - store compared to relational database
155:58 - looks like this so the idea is that you
156:01 - have tables is now collections rows are
156:03 - documents columns or fields indexes are
156:05 - still the same name and when you do
156:06 - joins they're called embedding and
156:07 - linking so you know if a key value store
156:10 - can kind of store this why would you do
156:12 - it well there's just a lot more features
156:14 - around the documents itself and so you
156:17 - know how we saw key value store didn't
156:18 - have like it had like nothing like no
156:20 - functionality well document store brings
156:22 - a lot more of the functionality that
156:23 - you're used to in a relational database
156:25 - you know and so it makes things a little
156:27 - bit easier to work with okay
156:29 - [Music]
156:33 - all right let's take a quick look here
156:34 - at mongodb which is an open source
156:36 - document database which stores json-like
156:38 - documents and the primary data structure
156:39 - for mongodb is called a bson so a binary
156:42 - json is a subset of json so its data
156:44 - structure is very similar to json but
156:47 - it's designed to be both efficient and
156:49 - storage in both storage space and scan
156:51 - speed compared to json and bson has more
156:54 - data types than json has date times byte
156:56 - arrays regular expressions md5 binary
156:58 - data javascript code json's just strings
157:01 - integers and arrays it's very very
157:03 - simple but because it has all these new
157:05 - other data types and it's stored in this
157:06 - binary format it's not plain text it's
157:09 - actually binary data
157:10 - that's the one reason why it the storage
157:12 - space and the scan speed is so fast now
157:14 - if you did use javascript to perform an
157:17 - operation like say insert data this is
157:19 - what it would look like so you have kind
157:21 - of an idea that you're inserting items
157:22 - into a collection there okay
157:25 - just to list out some features of
157:26 - mongodb
157:27 - it supports searches against fields
157:29 - range queries regular expressions it
157:31 - supports primary and secondary indexes
157:33 - it's highly available it's it's high
157:35 - availability can be obtained via rep
157:37 - replica sets
157:38 - so replica to offload reads or access
157:40 - standby in case of failover momodube
157:42 - scales horizontally using sharding
157:44 - mongodb can run multiple servers via
157:46 - load balancing mongodb can be used as a
157:48 - file system which is called grid fs with
157:51 - with load balancing and data replication
157:53 - features over multiple machines uh for
157:55 - storing files mongodb provides three
157:56 - ways to perform aggregation uh grouping
157:59 - dat and aggregations just grouping data
158:01 - to return a query so aggregation
158:02 - pipeline map reduce single purpose
158:05 - aggregation
158:06 - mongodb supports fixed collections
158:08 - called capped collections i'm going to
158:10 - become claims to support multi-document
158:13 - asset transactions so mongodb when it
158:16 - first came out didn't do all this stuff
158:18 - and people complained about it i like it
158:20 - being very hard to scale but now it's a
158:21 - lot easier to use so you know mongodb is
158:24 - something that is uh more uh a more
158:27 - popular option nowadays than it was a
158:29 - few years ago so there you go
158:31 - [Music]
158:34 - all right let's take a look here at what
158:36 - a graph database is so graph database is
158:38 - a database composed of data structures
158:40 - that use vertices nodes or dots which
158:42 - form relationships to other vertices via
158:44 - edges arcs and lines so some use cases
158:46 - here fraud detection real-time
158:48 - recommendations engines master data
158:49 - management network and it operations
158:52 - identity and access management and
158:54 - there's a lot they're saying like it's
158:55 - really really good for that i am
158:57 - something i want to look into later
158:58 - traceability and manufacturing contact
159:00 - tracing
159:01 - data lineage for gdpr customer 360
159:05 - degree analysis like for marketing
159:07 - product recommendations social media
159:09 - graphing and feature engineering for ml
159:12 - so let's just kind of break down you
159:13 - know the little components here so what
159:15 - you'd have is a node and a node can
159:17 - contain data properties and then through
159:19 - that it would have a relationship
159:21 - through an edge and that relationship
159:23 - can have a direction and also data
159:25 - properties on it and so it's a lot more
159:28 - um
159:29 - verbose like in ter than a relational
159:32 - database and also just how it can point
159:33 - to stuff so uh super useful uh for
159:36 - particular use cases
159:38 - [Music]
159:42 - let's take a look here at azure tinker
159:44 - pop which is a graph computing framework
159:45 - for both graph databases oltps and graph
159:48 - analytic systems olaps so tinkerpop
159:52 - enables developers to use a vendor
159:53 - agnostic distributed framework to
159:55 - traverse query many different graph
159:58 - systems they'll always say traverse
159:59 - because there's so many it's a tree
160:01 - right
160:02 - so there's a lot of databases that this
160:04 - thing connects to and so here they all
160:06 - are but the ones i want to indicate to
160:08 - you that are important is amazon neptune
160:10 - cosmodb hadoop via spark neo4j which is
160:14 - one of the most popular
160:15 - graphing databases orient db and titan
160:18 - okay so the thing is is that this isn't
160:20 - a
160:21 - graph database it is a basically adapter
160:25 - to other graph databases and ticker pop
160:28 - includes a graph traversal language
160:29 - called gremlin which is the single
160:31 - language that can be used for all these
160:33 - graph systems so let's talk about
160:34 - gremlin gremlin is a graph traversal
160:36 - language for apache tinker pop and so it
160:39 - looks like this and sometimes uh you
160:41 - know like even without tinker pop i
160:42 - think this is with cosmodb that they'll
160:44 - support this language by default so you
160:46 - don't necessarily need to have tinker
160:48 - pop to work with some databases but it's
160:50 - great to have that service if you if or
160:52 - like the framework if you need it so
160:54 - gremlin is is designed to write once and
160:56 - run anywhere w-o-r-a gremlin traversal
160:59 - can be evaluated as a real-time query so
161:01 - lltb or a batch analytics query so over
161:04 - here it's just kind of showing you these
161:05 - are the oltps graph databases over here
161:08 - and then on the right-hand side we have
161:10 - olaps okay
161:11 - and so gremlin hosted language embedding
161:14 - means you can use your favorite
161:15 - programming language when you write
161:17 - gremlin okay so there you go
161:18 - [Music]
161:22 - hey this is andrew brown from exam pro
161:24 - and we are looking at azure tables which
161:26 - is a type of storage for nosql key value
161:28 - data store within the azure storage
161:30 - accounts azure table stores
161:32 - non-relational structured data with a
161:33 - schema-less design and there are two
161:35 - ways to interact with azure tables
161:37 - through the storage table storage api or
161:40 - microsoft azure storage explorer which i
161:42 - find is the easiest way to interact with
161:44 - it so just kind of looking at storage
161:47 - explorer there if you wanted to add an
161:48 - entry you'd have to provide a partition
161:51 - key which is a unique identify fire for
161:54 - the partition with a given table and a
161:56 - row key a unique identifier for an
161:58 - entity within a given partial a
161:59 - partition and so you have all your data
162:02 - types here so we see string boolean
162:03 - binary data type
162:05 - double uh
162:06 - guids
162:08 - 32 and 64. if we wanted a query you'd
162:11 - have to query along the partition and
162:13 - row key so you could also do some
162:15 - additional filtering here so just notice
162:16 - here that um you know you have your
162:18 - partition key you put your value like
162:20 - klingon and wharf and then this is not
162:23 - this is just additional properties you
162:24 - added
162:25 - a lot of time the way these key values
162:27 - work is that
162:28 - this will return the results like all
162:31 - the results and then server side and
162:33 - then client-side these will be filtered
162:35 - client-side i don't know if that's the
162:36 - case with azure table but that's
162:38 - generally how these things work and so
162:40 - there you go
162:40 - [Music]
162:44 - hey it's andrew brown from exam pro and
162:46 - we're looking at cosmodb which is a
162:48 - service for fully managed noaa school
162:49 - databases that are designed to scale
162:52 - and be highly performant so cosmodb
162:54 - supports different kinds of nosql
162:55 - database engines which you interact via
162:57 - an api so we have the coresql which is
163:00 - their document datastore their azure
163:02 - cosmodb api for mongodb their azure
163:05 - table
163:06 - and gremlin okay and this will be using
163:08 - uh probably tinker pop um so all of
163:11 - these nosql engines uh uh specify
163:14 - capacity so you can do provision
163:16 - throughput for pay for guarantee of
163:18 - capacity or serverless pay for what you
163:20 - use so if you are just playing around
163:22 - with the service you can go ahead and
163:23 - choose that serv
163:25 - serverless option and so a lot of times
163:27 - when people talk about cosmodb they're
163:28 - usually talking about coresql so if you
163:30 - say cosmodb it's usually document but
163:33 - understand that there's a bunch of stuff
163:35 - underneath it
163:36 - now if you want to start viewing data
163:38 - and making stuff and playing around with
163:39 - it you'd use the cosmo db explorer which
163:42 - is a web interface that you can find at
163:44 - cosmos.azure.com so after you made your
163:46 - cosm db cluster or container whatever
163:48 - they call it
163:49 - then you could go access your database
163:51 - so here we have the sql api
163:54 - and so that would be the document store
163:56 - and you could just see here that we have
163:58 - we've created a new item here for that
164:00 - data okay
164:02 - and so i just want to show you that if
164:03 - you drop down here you choose container
164:05 - or database so we create a new container
164:07 - um also if you are in um
164:10 - azure it looks like they just have it
164:12 - here under the data explorer tab so it's
164:13 - the same thing it's the cosmo db
164:15 - explorer just in line okay so you don't
164:17 - have to like go to that url you could
164:19 - just click into your um your it's called
164:22 - account cosmodb account and go to data
164:24 - explorer i just wanted to show you here
164:26 - like if you made a graph database that
164:27 - you can do everything through this
164:28 - explorer for all the different types the
164:30 - interface will change a bit so here we'd
164:31 - add a new vertex right and it's just
164:33 - slightly different okay
164:35 - [Music]
164:39 - all right so the thing about azure
164:40 - tables is that you can
164:42 - use it within either cosmodb
164:45 - okay or you can use it within account
164:47 - storage and the thing is is that um
164:50 - it's a really good comparison to look at
164:52 - these two things because this way we can
164:55 - really understand like how powerful
164:57 - cosmodb is all right
164:59 - so what we'll do is compare the two so
165:01 - over here when you have azure tables in
165:03 - account storage
165:04 - it's fast but it has no upper bounds of
165:06 - latency
165:07 - for azure cosmodb it's going to give you
165:09 - single digit millisecond latency for
165:11 - reason writes
165:12 - for throughputs it's variable throughput
165:15 - it's limited to 20 000 operations you
165:17 - get a guaranteed uh backed by an sla and
165:20 - no upper limits when you're using cosmo
165:22 - db for global distribution it's a single
165:24 - region
165:25 - and for cosmic db you have 30 plus
165:28 - regions for indexing you only get the
165:30 - primary index or partition and row no
165:32 - secondary indexes and then for
165:35 - cosmodb you get automatic and complete
165:36 - indexing in all properties no index
165:38 - management for green you get query
165:40 - execution uses index for primary key and
165:43 - scans otherwise and for uh cosmodb you
165:46 - get queries that can take advantage of
165:48 - automatic indexing on properties for
165:49 - fast query times for consistency we got
165:52 - strong with primary region and eventual
165:54 - with secondary regions and with uh
165:56 - cosmodb there's like five you know what
165:59 - i mean there's just uh the consistent
166:01 - levels are a lot more flexible okay
166:03 - for pricing it's consumption based and
166:05 - then for
166:06 - cosmodb you have consumption based or
166:08 - provision capacity for the slas it's
166:10 - 99.99 availability and here it it's
166:13 - backed by an sla but some conditions it
166:16 - does not apply okay so
166:18 - you know hopefully that shows you that
166:20 - cosmodb like is very performant is
166:22 - globally available
166:23 - single digit millisecond and i i really
166:25 - feel like this is to compete with um
166:27 - adabus um
166:29 - dynamodb because it sounds so
166:31 - similar to dynamodb but
166:33 - yeah there you go
166:38 - hey this is andrew brown from exam pro
166:41 - and we are on to the azure tables and
166:42 - cosmos db cheat sheet for the dp900 i
166:44 - want to point out something uh that i'm
166:47 - sure you already know about but in the
166:48 - course i spelt cosmos db without the s
166:51 - like everywhere and i'm not going to go
166:53 - back and fix that
166:55 - but i know i'm going to hear like
166:57 - never the end of it for like the next
166:59 - year okay
167:00 - so let's start at the top here azure
167:02 - tables it's a key value data store can
167:04 - be hosted on either azure storage
167:06 - account storage it is designed for a
167:08 - single region and single table can be
167:10 - hosted on cosmos db
167:12 - and when it's hosted here it's designed
167:14 - for scale across multiple regions
167:15 - cosmodb a fully managed nosql service
167:17 - that supports multiple nosql engines
167:20 - called apis why they didn't call them
167:22 - engines i don't know
167:23 - coresql api this is the default one it's
167:26 - a document database you can use sql to
167:28 - query documents and when people are
167:29 - talking about cosmodb that's what
167:31 - they're talking about the document
167:32 - database the default one okay
167:34 - graph apis a graph database you can use
167:38 - with gremlin to transfer traverse the
167:40 - nodes and edges mongodb api a mongodb
167:42 - database it is a document database
167:45 - tables ai is just as your table's key
167:47 - value but within cosmodb apache
167:49 - tinkerpop an open source framework to
167:52 - have an agnostic way to talk to many
167:54 - graph databases they probably won't ask
167:55 - you about tinker pop on the exam gremlin
167:57 - graph traversal language to traverse
167:59 - nodes and edges you definitely need to
168:01 - know what gremlin is and be used to
168:03 - seeing what it is like identify what it
168:04 - looks like mongodb an open source
168:07 - document database and the way it works
168:09 - is it has its own um data structure its
168:12 - document structure called bson which is
168:14 - binary json a storage and compute
168:16 - optimized version of json introduces new
168:19 - data types cosmo db explorer a web ui to
168:22 - view cosmos databases and there you go
168:24 - [Music]
168:28 - so what is apache hadoop well it's an
168:30 - open source framework for distributed
168:32 - processing of large data sets hadoop
168:34 - allows you to distribute large data sets
168:36 - across many servers and computing
168:38 - queries across many servers so htfs and
168:40 - mapreduce were the first features that
168:42 - were launched with hadoop way back in
168:44 - the day in version one and since then
168:46 - there's a lot more services now but the
168:48 - idea behind distributed processing is
168:50 - the idea is the idea is that your
168:52 - computer servers do not need to be on
168:54 - specialized hardware you can run them on
168:56 - common hardware and that's actually how
168:57 - google back in the day
168:59 - like uh there's a story over google they
169:01 - just kept on like adding like all random
169:02 - machines to build up their search engine
169:04 - didn't matter what it was and that
169:06 - eventually became the hadoop system so
169:08 - apache dupe framework has the following
169:10 - so hadoop common collections of common
169:12 - utilities and libraries that support
169:13 - other hadoop modules hadoop distributed
169:15 - file system a brazilian and redundant
169:17 - file storage distributed on clusters of
169:19 - common hardware hadoop mapreduce writes
169:22 - apps that can process multi-terabyte
169:25 - data in parallel on large clusters of
169:26 - common hardware hbase a distributed
169:28 - scalable big data store yarn manage
169:31 - resources nodes containers and perform
169:33 - scheduling hive used for generating
169:35 - reports using sql
169:37 - pig a high level scripting language to
169:39 - write complex data transformations if
169:41 - they sound like they do the same thing
169:43 - they absolutely do they're just slightly
169:44 - different hadoop can integrate with many
169:46 - other open source projects via the
169:47 - hadoop components and we're going to see
169:49 - a lot of those open source
169:51 - things here in a moment
169:56 - let's take a look here at apache kafka
169:58 - which is an open source streaming
170:00 - platform to create high performance data
170:02 - pipeline streaming analytics data
170:03 - integration and mission critical
170:05 - applications absolutely the number one
170:07 - streaming service though it is open
170:09 - source so you have to find a way to uh
170:11 - to host it kafka was originally
170:13 - developed by linkedin and open source in
170:14 - 2011. kafka was written in scala and
170:17 - java so to use kafka you're going to be
170:18 - writing java code
170:20 - here is just kind of a diagram of how it
170:22 - works so you got producers consumers and
170:24 - topics so in kafka data is stored in
170:27 - partitions on a cluster which can span
170:29 - multiple machines which makes it
170:30 - distributed computing producers publish
170:32 - messages in a key and value format using
170:35 - kafka producer api and consumers can
170:37 - listen for messages and consume using
170:39 - the
170:40 - the cough consumer api messages are
170:42 - organized into topics producers will
170:44 - push messages to topics and consumers
170:46 - will listen on topics so there you go
170:52 - so azure hd insights is a managed
170:54 - service to run popular open source
170:56 - analytics services so here's kind of a
170:58 - graphic of how it works and hdinsight
171:00 - supports the following frameworks apache
171:02 - hadoop which is it which is what it is
171:05 - it's the entire system apache spark
171:07 - kafka storm hive hbase and lap and also
171:11 - you can run our workloads hd insights
171:14 - has broad range of scenarios such as etl
171:17 - data warehousing machine learning
171:18 - internet of things just because you can
171:20 - put in so many things here and so
171:22 - you know hdinsights is just a managed
171:24 - version of hadoop and it just makes it
171:26 - really easy to to do stuff so you can
171:28 - consume stuff run these in clusters so i
171:30 - think these are each called a cluster
171:32 - for whatever you're running and they can
171:33 - go out out to somewhere now once you
171:36 - launch hdinsights
171:38 - you can use the apache ambari which is
171:40 - an open source hadoop management web
171:42 - portal to provision manage and monitor
171:44 - your hadoop clusters
171:46 - and so when you create a cluster you're
171:48 - going to get one by default it's just
171:49 - under here where it says cluster
171:51 - dashboard
171:52 - and the idea here is you'll see all the
171:54 - types like htf mapreduce all the stuff
171:56 - here and it just makes it really easy to
171:58 - interact with hadoop okay
172:00 - [Music]
172:05 - hey this is andrew brown from exam pro
172:07 - and welcome to the hadoop cheat sheet
172:09 - for the dp900 let's jump into it so
172:11 - apache hadoop is an open source
172:12 - framework for distributed processing of
172:14 - large data sets
172:15 - and underneath it has the hadoop
172:17 - distributed file system hdfs a resilient
172:19 - and redundant file storage that's
172:21 - distributed on clusters of common
172:23 - hardware you have mapreduce which writes
172:26 - apps that can uh process multi-terabytes
172:28 - data in parallel on large clusters of
172:30 - common hardware hbase a distributed
172:32 - scalable big data store yarn managed
172:34 - resources nodes containers perform
172:36 - scheduling
172:37 - hive used for generating reports using
172:39 - an esco language pig a high level
172:42 - scripting language to write complex data
172:43 - transformations apache spark can perform
172:46 - is 100 times faster in memory and 10
172:48 - times faster than disk than hadoop
172:50 - supports etl streaming and ml flows you
172:53 - can run that on hadoop i didn't just did
172:55 - not put it under the um
172:56 - i debated whether i should put it in in
172:58 - line or there but that's where i put it
173:00 - apache uh kafka a streaming pipeline
173:03 - analytics service hd insights a managed
173:06 - service to run popular open source
173:07 - analytics services it is fully managed
173:09 - hadoop system so it's just hadoop but
173:11 - managed by
173:14 - azure there so there you go that's the
173:15 - chi chi
173:16 - [Music]
173:20 - apache spark is an open source unified
173:22 - analytics engine for big data and
173:24 - machine learning and spark lets you run
173:25 - workloads much much faster than hadoop
173:27 - though you can run it in the hadoop
173:29 - system okay so 100 times faster in
173:31 - memory 10 times faster than disk
173:33 - and which is why spark is being
173:35 - described as lightning fast so when it
173:36 - says hadoop you know it's talking about
173:39 - hive and pig and the other things that
173:40 - usually come along with hadoop and so
173:43 - apache spark is a collection of
173:45 - libraries that work well together to
173:46 - form an analytics ecosystem so we have
173:49 - the spark core this is the underlying
173:50 - engine and api the api supports the
173:52 - following programming languages r sql
173:54 - python scala and java you have spark sql
173:58 - which introduces a data structure called
173:59 - a data frame not the same thing as a
174:01 - pandas data frame but they're just
174:02 - called the same thing okay which can be
174:04 - used with uh dsls to work with
174:07 - structures and semi-structured data you
174:09 - have spark streaming allows spark to
174:11 - ingest data from many streaming services
174:13 - so htfs flume kafka twitter
174:16 - kinesis
174:17 - you have graph x so distributed graph
174:20 - processing framework you have machine
174:22 - learning the mlib library
174:24 - and this is a distributed machine
174:25 - learning framework with common machine
174:26 - learning statistical algorithms
174:29 - the way you are going to interact with
174:31 - spark is through resilient distributed
174:33 - data set rdd which is a dsl to execute
174:35 - various parallel operations on the epoxy
174:37 - spark cluster so here are some common
174:39 - functions
174:40 - map filter distinct count min max mean
174:44 - paralyzed you get the idea and here's an
174:46 - example of rdd api so just notice here
174:50 - um i think basically all of these are
174:52 - are
174:53 - those functions right so
174:55 - that just makes it really easy to
174:57 - transform and work with data okay
175:02 - [Music]
175:03 - so databricks is a software company
175:05 - specializing in providing fully managed
175:07 - apache spark clusters and the company
175:09 - founders were the creators of the apache
175:10 - spark delta lake and ml flow
175:13 - open source projects and databricks has
175:15 - two main offerings the database platform
175:18 - and so dablex cloud based spark platform
175:21 - with an easy to use web ui
175:23 - where you can launch fully managed spark
175:25 - clusters launch notebooks to write code
175:26 - and interact with spark create
175:28 - workspaces to collaborate with team
175:29 - members and role-based access controls
175:31 - create jobs for etl or data analysis
175:34 - tasks that run immediately or on
175:35 - schedule create ml workflows and is
175:38 - available on all main cloud service
175:40 - providers aws azure and gcp they also
175:43 - have the database community edition
175:44 - which is a free version of the
175:46 - databricks platform for educational use
175:48 - create a free michael cluster that
175:50 - terminates after two hours when idle no
175:52 - workspace jobs or rbac so really just a
175:55 - subset of the one above so azure data
175:59 - bricks is a partnership between
176:00 - microsoft and databricks to offer the
176:02 - database platform within the azure
176:04 - portal running on azure compute services
176:06 - and it offers two environments
176:08 - workspaces and so basically this is just
176:11 - the azure database platform with
176:12 - integrations to azure data related
176:14 - services for building big pipelines so
176:16 - if you need to do batching you can use
176:18 - azure data factory streaming apache
176:20 - kafka event hub iot
176:22 - storage azure blob storage azure data
176:25 - like storage the other side of it is
176:27 - azure databricks sql analytics run sql
176:29 - queries on your data lake create
176:31 - multiple visualization types to explore
176:33 - your query results build and share your
176:36 - dashboards
176:37 - now if you want to launch a databricks
176:39 - workspace it's really easy all you got
176:41 - to do is create a workspace and choose
176:42 - your plan launch a workspace
176:45 - use sso to connect to it and start your
176:47 - database platform so if you go up here
176:49 - and this is in the azure portal with
176:50 - azure databricks we'll just choose which
176:51 - one we want and then we launch the
176:53 - workspace it'll sson and then you're in
176:56 - there okay so um you know basically if
176:59 - you launch it's not going to cost you
177:00 - any money so if you want to play around
177:01 - with it you can do that or if you're
177:03 - really antsy you can just go to the
177:05 - databricks website and try the community
177:07 - edition because there's risk there's no
177:09 - risk of spending any money when using
177:10 - that okay
177:11 - [Music]
177:15 - hey this is andrew brown from exam pro
177:17 - we're taking a look here at the apache
177:19 - spark and data bricks cheat sheet so
177:20 - let's jump into it so apache spark is an
177:22 - open source unified analytics engine for
177:24 - big data and machine learning it's a
177:26 - hundred times faster in memory than
177:27 - hadoop ten times faster than disk than
177:29 - hadoop can perform etl so batch
177:31 - streaming and ml workloads uh for the
177:33 - apache ecosystem it's composed of spark
177:36 - core which is the underlying engine api
177:37 - spark sql you use sql it also has a new
177:40 - data structure called a data frame to
177:42 - work with data spark streaming which is
177:44 - a way to ingest data from many streaming
177:46 - services graph x distributed graph
177:48 - processing framework mlib a distributed
177:51 - machine learning framework there's also
177:52 - rdd it's a domain specific dsl
177:55 - to
177:56 - execute various parallel operations on
177:58 - apache spark clusters then we're talking
178:00 - about data bricks here it's a software
178:01 - company specializing at providing fully
178:04 - managed apache spark clusters we have
178:06 - azure databricks a partnership between
178:08 - microsoft and database to offer the the
178:11 - database platform with within the azure
178:13 - portal running on azure compute services
178:16 - azure databricks offers two environments
178:17 - we have the databricks workspace so this
178:20 - is the databricks platform with
178:21 - integrations to azure data related
178:23 - services for building data pipelines if
178:25 - you went to azure or sorry if you went
178:28 - to databricks website and signed up it's
178:29 - the same portal okay azure database sql
178:33 - analytics run your query on your data
178:35 - lake and i believe that this is
178:37 - basically um azure synapse analytics so
178:40 - that's the engine that's in there okay
178:44 - [Music]
178:46 - take a look here at sql management
178:47 - studio also known as ssms which is an
178:50 - ide for managing any sql infrastructure
178:53 - if you take a good look at the
178:54 - screenshot you can see that allows us to
178:56 - work with databases write sql statements
178:59 - and that's pretty much it so access can
179:01 - access configure manage administer and
179:03 - develop all components of sql server
179:05 - azure sql database azure synapse
179:07 - analytics and it has a few components in
179:09 - it so we have object explorer view and
179:11 - manage all objects in one or more
179:13 - instances of sql server template
179:14 - explorer build and manage files
179:16 - for boilerplate text that you can use to
179:18 - speed the development of queries and
179:20 - scripts and this is deprecated but you
179:22 - might hear about it so i just mentioned
179:24 - it here and this is build projects used
179:26 - to manage administration items such as
179:28 - scripts and queries so not a complicated
179:30 - service but uh yeah there you go
179:32 - [Music]
179:37 - all right let's take a look here at
179:38 - server data tools ssdt which transforms
179:42 - data development by introducing
179:43 - ubiquitous declarative models that span
179:45 - all phases of development phases within
179:47 - visual studio so this is a visual studio
179:49 - tool like azure basically has like a
179:52 - tool for everything like it'll be the
179:53 - same tool but it will be repurposed for
179:55 - their different other tooling products
179:57 - and this one's for visual studio so i
179:59 - don't have a great internal screenshot
180:01 - but you can see that it works with um it
180:03 - has a few different components inside
180:05 - there and analysis reporting
180:06 - integrations
180:08 - so it uses
180:09 - sdt transact so tsql to build debug
180:13 - maintain refactor databases also
180:15 - provides a table designer for creating
180:17 - editing tables in either database
180:18 - projects connected database instances be
180:20 - able to view control data loaded files
180:23 - easy to publish to sql database or sql
180:25 - server has an object explorer offers a
180:28 - view of your database similar to sql ssm
180:31 - mess allows you to
180:33 - lightly duty database administration
180:35 - design work easily create edit rename
180:37 - delete tables stored procedures
180:38 - functions edit table data compare
180:41 - schemas execute queries by using
180:42 - contextual
180:43 - menu rights i don't think this is on the
180:45 - exam but because it's just one of these
180:47 - many services i figured we'd throw it in
180:49 - here
180:50 - just so that if you do see this
180:52 - initialism you know what it's for
180:53 - [Music]
180:57 - so azure data studio is a cross-platform
181:00 - database tool for professionals using
181:01 - on-premise and any data platforms for
181:03 - windows mac os and linux and here is a
181:05 - screenshot and
181:07 - if you recognize it it looks just like
181:09 - visual studio code right if you open up
181:11 - the extensions that's what you would see
181:13 - so query design and manage your database
181:14 - and data warehouses data azure data
181:16 - studio offers a modern experience with
181:18 - intelligence very similar to experience
181:20 - to visual studio code code snippets
181:22 - source control integration integral
181:24 - terminal built-in charting customizable
181:27 - dashboards jupiter notebooks connected
181:28 - to your data set and it has a
181:30 - marketplace of free extension so some i
181:32 - know that seemed very useful was sql
181:33 - database inspector so a great way of
181:36 - looking at your data cousteau extension
181:38 - for azure data studio postgres extension
181:41 - and many many many more so there you go
181:43 - [Music]
181:47 - so azure data factory is a managed
181:49 - service for etl elts and data
181:51 - integration create data driven workflows
181:52 - for orchestrating data movement and
181:54 - transferring data at scale and so if you
181:56 - see the image it makes it pretty clear
181:57 - what you can do with it there's a bit of
181:59 - a pipeline there so create pipelines to
182:00 - schedule data-driven workflows build
182:02 - complex etl processes that transfer data
182:04 - visually with data flows using compute
182:06 - services such as hdinsights hadoop data
182:08 - bricks sql database publish and
182:10 - transform data to data stores such as
182:12 - azure synapse analytics raw data can be
182:15 - organized into meaningful data stores
182:17 - and data lakes let's break down the
182:18 - components here so we just have a better
182:20 - idea how this thing works so pipelines
182:22 - is a logical grouping of activities that
182:23 - perform units of work activities is a
182:25 - processing step in the pipeline data
182:28 - sets or data structures within the data
182:29 - store link services define connections
182:31 - information for data sources to connect
182:33 - to the data factory data flows are logic
182:36 - to determine how data moves through the
182:38 - pipeline transform integration runtimes
182:40 - compute infrastructure used by data
182:42 - factory control flows orchestration of
182:45 - pipeline activities that include
182:47 - chaining activities and a sequence of
182:48 - branching you should know what
182:51 - control flows are and data flows are and
182:53 - there you go
182:57 - [Music]
182:58 - so microsoft sql server integration
183:00 - services ssis is a platform for building
183:03 - enterprise level data integrations data
183:04 - transformation solutions
183:06 - ssis can be used to automate sql
183:08 - database servers it can be
183:10 - used as an integration runtime for azure
183:12 - data factory you can perform the
183:13 - following tasks copy files download
183:15 - files loading data within data
183:16 - warehouses cleansing mining managing sql
183:19 - server objects managing sql server data
183:22 - you can perform etl for a variety of
183:23 - sources xml flat files relational data
183:25 - sources
183:26 - and ssis has built-in tasks and
183:29 - transformation graphical tools for
183:30 - building packages integration service
183:32 - catalogs databases
183:34 - where you store run
183:35 - and manage your packages
183:37 - it comes with a graphical interface to
183:39 - transform so you don't have to write any
183:40 - code and the designer is a graphical
183:42 - tool that you can use to create and
183:43 - maintain integration services packages
183:45 - just to show you what it looks like
183:47 - here it is so it allows you to drag out
183:48 - transformations and design different
183:50 - kinds of flows and control flows and
183:52 - data flows notice it has control flows
183:54 - and data flows similar to azure data
183:56 - factory uh but you can see this is the
183:58 - tool kind of azure data factory is like
184:00 - kind of like the web version of this but
184:02 - yeah there you go
184:03 - [Music]
184:07 - hey this is andrew brown from exam pro
184:09 - and we're taking a look here at etl and
184:11 - sql tools cheat sheet so let's jump into
184:13 - it the azure data factory is a managed
184:15 - service for etl elt and data integration
184:18 - jobs you can create data driven
184:20 - workflows for orchestrating data
184:21 - movement and transforming data at scale
184:23 - build elt pipelines visually without
184:25 - writing any code via web interfaces
184:28 - you have ssis the s the sql server
184:31 - integration services
184:32 - a platform for building enterprise level
184:34 - integration data flow solutions
184:36 - a low code tool for building etl
184:38 - pipelines very similar to data factory
184:40 - but existed 15 years prior mostly
184:42 - focused around sql no surprise there
184:44 - integrates with azure data factory so
184:46 - you can extend it to non-relational
184:48 - database workloads okay we have azure
184:50 - data studio an id similar to a very
184:53 - similar to visual studio code those
184:55 - cross-platform and works with sql and
184:57 - non-relational database data has many
184:59 - many many many extensions you have sql
185:01 - server management studio smss an id for
185:04 - managing any sql infrastructure that
185:06 - only works for windows more mature than
185:08 - data studio very similar in terms of
185:11 - parallel so we have like the modern
185:12 - version that's the web mode over this
185:14 - web that's both
185:16 - s like relational and non-relational and
185:18 - then the older one that's mature but it
185:20 - is a a windows app and does a lot of
185:22 - interesting stuff for sql you have sql
185:24 - server data tools ssdt this is a visual
185:27 - studio extension to work and design
185:29 - visually
185:30 - sql databases within visual studio so
185:32 - there you go we're all done
185:33 - [Music]
185:37 - hey this is andrew brown from exam pro
185:39 - and i wanted to show you how to go ahead
185:41 - and install power bi so what i've done
185:43 - is i'm on my windows machine and i
185:45 - pulled up the microsoft store and all i
185:48 - did was go in the top right corner and
185:49 - type in power bi this is the easiest way
185:52 - to install power bi you can download the
185:55 - application but i find this is just the
185:57 - best way to do it and we'll hit get or
186:00 - free to get started here
186:03 - um
186:04 - and it should now start downloading so
186:05 - we go in the top right corner
186:08 - you'll notice here that it is now
186:09 - downloading okay so we just have to wait
186:11 - for that to finish and i'll see you back
186:13 - here in a moment
186:16 - all right so after waiting a short while
186:18 - here it looks like power bi desktop is
186:20 - finished downloading what we can do is
186:21 - click into power bi and we can go ahead
186:24 - and launch this service
186:26 - and so we'll let that go ahead and
186:28 - launch
186:31 - and the initial time you launch it it
186:33 - does take a little bit of time there so
186:34 - i stopped and restarted the video but
186:36 - here you can see we are now inside of
186:38 - power bi
186:39 - uh and if we wanted to get started there
186:41 - are a few ways we can go ahead and do
186:42 - that they have some nice tutorials
186:44 - here's in the bottom right corner but
186:45 - what i want to do is just get something
186:47 - open so we can start looking at
186:49 - something so if you were to type in
186:50 - sample data sets
186:52 - for power bi microsoft has this nice
186:54 - page where you have a bunch of
186:55 - downloadable
186:56 - reports or things we can work with and
186:58 - so maybe we should give the first one a
187:00 - go so i'm going to go ahead and download
187:01 - this one here
187:03 - and if we go down below so we'll just
187:06 - take a look here
187:13 - so i think we just have to go ahead and
187:14 - click here
187:18 - and
187:19 - uh from here i just want to hit that
187:21 - download button so i'm just looking for
187:23 - it
187:29 - here it is
187:30 - okay great and so i'll go ahead and
187:32 - download that file notice that it's it's
187:34 - the
187:35 - power bi x that is the file that we are
187:37 - looking for
187:38 - and we'll go ahead and open this file
187:44 - so after any short while there the file
187:46 - did open it does seem to take a while
187:48 - for things to open but i'll just hit the
187:49 - x there
187:50 - they have a new modeling mode we'll just
187:52 - say not yet
187:53 - and so down below you can see i have a
187:55 - bunch of tabs here and this allows us to
187:57 - go ahead and explore this data but you
188:00 - can see things are extremely interactive
188:02 - here in
188:04 - power bi desktop so you can do all sorts
188:06 - of things
188:07 - uh here okay
188:12 - [Music]
188:16 - hey this is andrew brown from exam pro
188:18 - and in this follow along we're going to
188:19 - be looking all at azure sql so what i
188:22 - want you to do is go to the type here
188:23 - and type in sql and you'll notice you'll
188:25 - get the options like the databases
188:27 - managed instances and virtual machines
188:28 - those are the three under the azure sql
188:30 - tier but to make our lives a little bit
188:31 - easier we'll type in azure sql i find
188:34 - this really confusing and so i'm hoping
188:36 - that i can show you the easiest way to
188:37 - find it but if you go to azure sql here
188:39 - and add it this will actually now give
188:41 - you the option to choose between the
188:42 - three so you can make an informed
188:43 - decision and we'll work our way from the
188:45 - right to the left so sql virtual
188:47 - machines is great when you want os level
188:49 - access to the virtual machine uh when
188:51 - you're doing a lift and shift that means
188:52 - you're moving your
188:53 - sql server from on-prem to the cloud
188:56 - because you want to
188:58 - take advantage of the cloud and also if
189:01 - you want to bring your own license to
189:03 - take advantage of the azure hybrid
189:05 - benefit so if you drop down here you'll
189:07 - notice that you'll see byol and if we
189:10 - expand here it'll tell you all the
189:11 - details here as to why you'd want to use
189:13 - that i'm not going to spin one up
189:14 - because i'm going to show you the price
189:16 - okay so we go over to the price and we
189:17 - scroll on down we go to um page you go
189:20 - you're going to notice whether it's on
189:21 - or off with high uh hybrid benefit
189:22 - you're paying about a dollar to two per
189:24 - hour and so you know it's you're not
189:27 - going to learn that much by spending one
189:28 - up
189:29 - because we are going to spin up an sql
189:30 - server but at this stage it's not going
189:31 - to really matter it's they're all very
189:33 - similar so you know i just i don't want
189:35 - to spend money if you're a student there
189:37 - let's look at sql managed instances
189:38 - we'll expand that again this is for lift
189:40 - and shift meaning you're moving from
189:42 - on-premise to the cloud but here you
189:44 - don't have access to the os level
189:46 - you're not bringing your own license
189:48 - however the upside is it's a fully
189:50 - managed service meaning that it can
189:51 - scale very well it's going to have great
189:53 - backups and things like that and it
189:55 - comes in two flavors we have single
189:57 - instance in single instance as your arc
189:59 - now i didn't cover this in the course
190:00 - but azure arc allows you to um extend
190:03 - your i think it's the control plane it's
190:05 - either data plane or control plane but
190:06 - the control plane uh two different cloud
190:08 - providers and also to your on premise so
190:10 - the idea here is like you can use sql
190:12 - manage instances and ha like you can
190:14 - have all the benefits of here and launch
190:16 - the instance within your own
190:18 - infrastructure so um you know if you
190:20 - really need to keep that server on-prem
190:22 - then you can do that with azure arc but
190:23 - here's a single instance um you know
190:25 - it's just whatever there and so i'll
190:27 - show you the pricing on that one
190:29 - and so we'll go to manage instances
190:30 - we'll scroll on down here
190:32 - and the lows cost one here for pays you
190:34 - goes a dollar so it's still kind of
190:36 - expensive if you're a student
190:38 - again i don't know if it has a free tier
190:40 - and i i wouldn't imagine it would
190:42 - because that's not really a free tier
190:45 - kind of product but you know i just want
190:46 - you to know that that option's there
190:48 - we'll make our way now to uh sql
190:50 - databases
190:52 - and so
190:53 - we have a few options here we have
190:54 - single database elastic pool and
190:55 - database server
190:57 - okay so single database is you have it's
191:00 - a great fit for modern uh cloud
191:03 - cloud-borne applications that need fully
191:04 - managed database with predictable
191:06 - performance so it has hyperscale i mean
191:07 - that it can scale up to 100 terabytes it
191:10 - has serverless compute it's easy to
191:12 - manage you have elastic pools we covered
191:14 - this in in the lecture content this is
191:15 - where you might be like a multi-tenant
191:18 - sas and you have multiple databases one
191:20 - per customer and you want to save class
191:23 - so you actually have them all running on
191:24 - the same server and you have database
191:26 - server and those so this is just to
191:28 - manage a group of single databases and
191:30 - elastic pools it's a way of grouping
191:32 - stuff together and so today what i'm
191:34 - going to do is launch myself a single
191:36 - database
191:37 - okay
191:39 - and what we'll do is we'll just say
191:41 - azure
191:43 - sql or we'll say dp900
191:45 - dp900 azure sql
191:50 - just so i can see what i'm doing we'll
191:51 - go down below here and i'm just going to
191:53 - call this um
191:55 - my azure
191:57 - uh
191:58 - sql
192:00 - we will drop down here and select a
192:01 - server we'll have to create a new server
192:03 - so this will be my azure sql server
192:08 - and you might have to work to choose a
192:09 - name i'm going to do one two three
192:12 - four five six
192:14 - there we go okay for the login i'm going
192:17 - to do azure user
192:19 - for the password i'm going to do capital
192:20 - t testing one two three capital t
192:23 - testing one two three it's really nice
192:25 - sometimes they make you do four five six
192:27 - and then an
192:29 - exclamation mark but we have the shorter
192:30 - password there we're going to say okay
192:34 - do we want to use elastic pool no i
192:35 - don't think we need that today but let's
192:37 - expand the general purpose here i want
192:39 - to make sure that we have the lowest
192:40 - cost possible so notice
192:42 - to the right here it's 381 dollars if
192:44 - you're provision it you could also do
192:46 - serverless
192:48 - i've actually never used the serverless
192:50 - feature here but this would be really
192:52 - great if you're just trying to learn
192:54 - because it's going to be based on usage
192:56 - here i've definitely used serverless
192:58 - services just not this one in particular
193:00 - for sql but what i'm going to do is
193:01 - stick with provision i'm going to look
193:03 - for a basic plan
193:05 - and what we'll do is scroll this all the
193:06 - way to the left here
193:08 - and you know this is just if we forget
193:10 - to turn off our database so now it's
193:12 - five five dollars a a month and probably
193:15 - you know per hour it's you know
193:17 - fractions of a penny so we'll go here
193:18 - and hit apply and we'll scroll down as
193:21 - we have some options geo-redundant zone
193:23 - redundant locally redundant honestly we
193:25 - only need locally redundant so i'm going
193:27 - to go ahead and select that obviously
193:29 - you know if you're doing a production
193:30 - database you want geo-redundant because
193:32 - you're going to have um uh you know
193:34 - better like a better redundancy in terms
193:36 - of like if a region goes down you'll
193:38 - still have your database okay we're the
193:40 - next step to networking
193:42 - and here we either have public or
193:44 - private endpoint this is really
193:45 - important for uh security purposes right
193:48 - and we do cover this in the lecture
193:49 - content as well and so i think it would
193:51 - be a great idea to have that there let's
193:52 - go read about firewall rules allow
193:54 - services and resources to access server
193:56 - yes to communicate from all resources
193:57 - inside the azure boundary that may or
193:59 - may not be private subscription so allow
194:01 - azure services and resources to access
194:03 - this service yes
194:05 - uh add current ip address yeah because
194:07 - that's me so i'm going to say yes to
194:08 - both of those it's going to set up our
194:09 - firewall rules which is really nice
194:11 - we'll go next to security here we have
194:13 - azure defender for sql if you want that
194:15 - additional protection you turn that on
194:17 - it costs 15 a month and we'll go to the
194:19 - next tab here
194:21 - here we could um i guess let's see for
194:23 - backup so start with a blank database
194:25 - restore from a backup or select a sample
194:27 - i'll select a sample because that's nice
194:29 - to have some data i didn't know they had
194:30 - that there
194:32 - and so we'll go to next tags
194:34 - i didn't really cover tags in the course
194:36 - but they're actually really important
194:37 - especially when we're talking about data
194:38 - because you do want to catalog
194:40 - and and categorize all your stuff so
194:42 - tagging is very simple what we do is we
194:44 - have a name so we could say um
194:47 - workload and i'll just say like uh
194:49 - learning right
194:51 - but allows us to filter and find
194:53 - resources later down the road we'll go
194:55 - ahead and hit review create we can
194:56 - review all of our stuff see that the
194:58 - price is okay with us we'll go ahead and
195:01 - create and again we're not going to get
195:02 - billed five dollars by pressing that
195:03 - button
195:05 - because it's again
195:06 - build per hour
195:08 - um so you know we will be build uh some
195:10 - kind of sense i don't know if azure sql
195:12 - has a free tier probably does
195:15 - uh free tier let's see here
195:17 - free
195:18 - tier
195:20 - i don't see it but i'm not too worried
195:22 - about that um so you know again if
195:24 - you're really worried about any spend
195:25 - just uh watch and don't don't do it but
195:27 - i do recommend if you do have the
195:28 - pennies to go launch the stuff of
195:30 - yourself because you'll learn a lot more
195:31 - that way okay so i'll see you back here
195:33 - when this is done uh provisioning
195:36 - all right so after a short little wait
195:37 - there our server is ready so i'm gonna
195:39 - go ahead and hit click go to resources
195:41 - and what we can do on the left hand side
195:43 - is just do a little exploration so look
195:45 - at the top here where it says set server
195:46 - firewall remember from our
195:49 - our security part where if we had set
195:52 - um some default rules well here they are
195:54 - okay so here is my ip address that is
195:56 - allowing me to connect to it notice we
195:58 - have that connection policy where it's
195:59 - default proxy or redirect
196:02 - you know things like that so we'll go
196:04 - back uh one step here
196:06 - and i just want to show you down below
196:08 - that there are some features so notice
196:10 - that tde transparent data encryption is
196:12 - already turned on uh there's your azure
196:14 - defender for sql
196:16 - we have our dynamic data masking so on
196:18 - the left hand side we'll scroll on down
196:20 - and just take a look at some of the
196:21 - security so we go to tde and you can see
196:22 - it's already turned on for us which is
196:24 - great i'm not going to click on the
196:25 - security center because it's a pain to
196:27 - exit out but let's look at dynamic data
196:29 - masking so here what it's doing is it's
196:31 - suggesting
196:32 - uh fields that should be mass and we go
196:34 - ahead and just click add and add and
196:36 - then we can mask these domains so i just
196:38 - click that there and so now these
196:40 - domains are masked
196:41 - and it shows you a bit of the mass
196:43 - function as to what it would do to to
196:44 - mask it so it's as easy as that
196:47 - um data discovery classification this
196:49 - will be useful it's in preview right now
196:50 - it's not showing any information as of
196:53 - yet um
196:54 - but the great thing about this is that
196:56 - it could
196:57 - discover some information that you might
196:59 - care about that needs to be that's
197:00 - considered sensitive right so you might
197:02 - say this is confidential gdpr address
197:05 - here
197:06 - um information types let's see what we
197:07 - can do here so we'll say customer
197:09 - and
197:10 - we'll say credit card
197:12 - and we'll say highly confidential
197:15 - and i'll say add classification
197:18 - i haven't used the service before
197:19 - because i had never seen it before but
197:20 - um it looks like that's a great way to
197:23 - um you know keep track of that kind of
197:25 - stuff
197:26 - so that's cool
197:28 - uh auditing i don't think i ever do
197:30 - auditing but um
197:31 - you can turn
197:33 - azure sql auditing it will track logs
197:35 - and stuff like that so the thing is is
197:37 - that if someone were to get into your
197:38 - database and try to make changes or to
197:41 - corrupt your data stuff like that you'd
197:42 - want to see who was actually accessing
197:44 - those queries so definitely if you care
197:45 - about security you'd absolutely want to
197:47 - turn that on
197:49 - and then we'll just scroll back to the
197:50 - top here back go to our overview
197:52 - i want to show you that uh through
197:54 - connect we can connect to data studio or
197:56 - visual studio if you click this it's
197:57 - going to tell you you need to install it
198:00 - i don't have azure data yet so i'm going
198:01 - to go ahead and install it on this
198:02 - computer and we'll go down below and i'm
198:05 - on windows machine today so
198:07 - this is for windows mac and linux and so
198:10 - i will go ahead and use the
198:12 - user installer i guess
198:14 - and i will download that it's 100
198:16 - megabytes as that's going we'll make our
198:18 - way back
198:19 - to
198:20 - our main page here and i also want you
198:22 - to notice there's power bi so we will go
198:24 - and download this uh we can go download
198:26 - actually right now so if we were to
198:28 - click on um
198:30 - uh is it just get started yeah it's
198:32 - gonna download this my azure pb ids and
198:36 - we'll be able to open that file in power
198:37 - bi and you know we did power bi earlier
198:40 - in earlier follow along so it'll be easy
198:42 - to open i want you to know if you don't
198:44 - have time to do all the stuff because
198:45 - we're going to have this database up and
198:46 - running for a little a little longer
198:48 - than you expect so if you if you don't
198:50 - feel like you can do this all in one
198:51 - setting you can delete this database and
198:53 - always go ahead and spin it up again
198:55 - when you're ready to continue but on the
198:57 - left hand side they have a query editor
198:59 - here and we're just going to go log in
199:01 - so i said my password was testing with
199:03 - capital t one two three we'll hit okay
199:06 - and now that we're in i can see my
199:07 - tables on the left-hand side here
199:10 - uh we got views and stored procedures
199:11 - but let's say we wanted to execute a
199:13 - query so i'll just expand this i've
199:16 - never seen this database before so i'm
199:17 - just going to go with it but we'll type
199:19 - in select
199:20 - um oops
199:22 - and it's going to be very aggressive
199:23 - about autocompletes you got to be really
199:25 - careful here and i'm just going to say
199:26 - select asterisk which means all fields
199:29 - and we want to select it from sales lt
199:32 - dot customer
199:33 - and notice when it auto completes it has
199:35 - square back brackets around it that is
199:37 - just the thing that tsql does let's say
199:39 - you had a field called first name
199:41 - and you wanted to have it with a space
199:43 - and that's what brackets that you do so
199:45 - they're just filling that in case you
199:46 - have reserved words or um you know
199:48 - spaces but we don't actually have any
199:50 - spaces here so we just remove it like
199:51 - that
199:52 - sales lte is kind of like the container
199:54 - for all these um all these tables it's
199:56 - just a naming convention they have for
199:57 - their table names
199:59 - and so we'll go here select all
200:01 - and hit run and we should get some data
200:03 - back and so let's just perform a very
200:06 - simple join so a join will happen when
200:08 - you actually have
200:10 - a foreign key you can join on so here
200:11 - you can see this is the
200:13 - primary key customer doesn't seem to
200:15 - have a whole lot of information so let's
200:17 - go into products
200:19 - and this should have yeah here you see
200:21 - it says product category that is a
200:23 - foreign key so we can use that so i'm
200:25 - gonna go here and type in sales lt and
200:28 - we'll do product
200:31 - okay and then we'll do where
200:34 - or sorry we'll do a left join so we'll
200:35 - say left join
200:37 - and i want to left join on this product
200:40 - id so it would be left join with
200:43 - sales lt dot product
200:46 - uh category
200:48 - and we would say on
200:51 - and so we would say sales lt
200:54 - dot
200:54 - product and it's going to be
200:57 - uh
200:59 - what was it called it is
201:02 - product category okay so we say product
201:05 - category id
201:07 - equals and we'd say sales lt dot product
201:11 - category
201:13 - category
201:15 - dot id so we're saying we want to join
201:18 - where these match on here
201:20 - assuming joins working like every other
201:22 - language which it should and so i'm
201:24 - going to be very particular and just
201:25 - pull out some particular keys that i
201:26 - want so here i'm going to say
201:28 - i want the sales lt.product oops
201:34 - and i wonder if i can do this as on this
201:36 - like as
201:38 - prod
201:40 - or for p here because i'm going to go
201:41 - crazy if i keep on typing that a so
201:43 - we'll just try that um i think it should
201:45 - work um if we do that because then it'll
201:47 - just be easy to read here
201:49 - it's like a way of aliasing it sometimes
201:50 - you can don't even need to put the word
201:52 - as you just put p like this
201:54 - and for sales product we could do that
201:56 - as well so i took out the on so i'll
201:58 - just say like pc
202:00 - and we'll see if it lets us do that
202:04 - okay and so here we have product
202:08 - id
202:10 - and i want to get the product name
202:14 - looks like it's working
202:16 - and
202:17 - we want the category name i think so
202:19 - we'll drop this down
202:23 - i assume it must have a category it's
202:25 - taking it's time to load here but we'll
202:27 - just say pc.name
202:31 - as product name
202:35 - as name
202:37 - as id
202:38 - and we'll see if that lets us run that
202:43 - it's currently not available if the
202:44 - problem persists
202:45 - contact customer supports we'll hit run
202:46 - again
202:49 - i'm not sure why our server's having
202:51 - problems i didn't i didn't turn it off
202:53 - did i
202:54 - let's go back here and take a look
202:56 - it's online yeah it's in good shape here
202:59 - so we should be able to query it
203:01 - maybe just lost our connection so what
203:03 - i'm going to do is just click back here
203:05 - i'll type in
203:07 - testing123
203:10 - [Music]
203:14 - and i'll go ahead and close that there
203:15 - we'll make our way back
203:17 - go back in the query editor
203:23 - there we go
203:24 - i'm not sure i was giving us trouble
203:26 - i threw an error here um
203:30 - invalid id oh yeah it's not called that
203:32 - it's called uh product id
203:34 - there we go
203:36 - we'll run that
203:39 - invalid column name product id
203:42 - must be naming these wrong here
203:45 - capital d here
203:48 - uh is it the same thing down below here
203:50 - normally you know like when i name these
203:52 - i would have them all lowercase
203:53 - underscore but you know in the azure
203:55 - world they they like to do this quite a
203:56 - bit
203:57 - so we will try this
204:00 - um
204:04 - product id so it doesn't oh right it
204:07 - wouldn't be uh
204:09 - this would be
204:11 - product category sorry so this is
204:13 - product category id
204:17 - and yeah that should match
204:20 - there we go so we just did a join
204:21 - between the two tables so we have the
204:22 - name and the product name
204:24 - which is uh pretty good there our azure
204:26 - data studio looks like it's done
204:28 - downloading so i'm going to go ahead and
204:30 - install that so what i'm all i'm doing
204:31 - here is i'm just
204:33 - double clicking the file to open it up
204:41 - and we'll just give that there a second
204:43 - oh it was here the entire time we'll say
204:45 - i accept next next next
204:48 - uh
204:48 - we'll get a desktop icon sometimes i
204:50 - can't find them later registered data
204:52 - suit is an editor for sporto files new
204:55 - i like other other programs for that and
204:57 - we'll go ahead and install that
205:13 - doesn't take too long
205:17 - and we will launch the data studio
205:21 - and yeah if you've ever used visual
205:22 - studio code it's gonna look very
205:23 - familiar to you see
205:26 - um
205:27 - so don't like the light mode rather have
205:29 - it dark appearance can we change that to
205:31 - dark
205:32 - zen mode oh no oh no what did i do uh i
205:36 - don't know how to get out of that okay
205:37 - great maybe not do zen mode i just
205:38 - wanted to make it a dark mode uh not
205:41 - sure how to change that i guess it's not
205:43 - a big deal but what we'll do is we'll go
205:44 - over here the top and we can go ahead
205:47 - and add a new connection
205:48 - and so that's an mssql server
205:51 - what we'll need to do is i want to keep
205:53 - this open because um it's very hard to
205:55 - find the tables here but what we'll go
205:57 - here is go back and we'll get the server
205:59 - name so we'll copy that and then within
206:02 - visual azure data studio we'll paste in
206:04 - our server we have three options windows
206:06 - authentication sql login azure active
206:08 - directory i'm going to use sql login the
206:10 - password is azure user username is azure
206:12 - user the password is capital t testing
206:14 - 123. what i like will tell to remember
206:17 - is that if you drop it down it'll see if
206:18 - the database is there
206:20 - and there it is so my azure sql that's
206:23 - what we called it i'm going to go ahead
206:25 - and connect
206:27 - and if i expand it i should be able to
206:29 - see
206:30 - tables good and stuff like that
206:35 - very good
206:36 - a lot easier to work with than
206:38 - that preview editor there
206:40 - so what we can do i'm just going to go
206:42 - over here and grab our query
206:45 - copy this
206:47 - and what we'll do up here is we'll make
206:49 - a new query so i think you
206:52 - uh
206:53 - it's over here new query maybe it's over
206:54 - here as well let me
206:58 - see now i guess we just do it from there
207:01 - i'm just trying to figure out what's
207:02 - easiest way to go up there and do a new
207:04 - query i'm going to go ahead and paste
207:05 - that in notice we can change our
207:06 - connection i don't think a database is
207:08 - selected right now so we'll go ahead and
207:10 - connect to this database here
207:13 - we'll go ahead and run that
207:14 - and it's the same data okay uh there's
207:16 - the thing called notebooks which is kind
207:18 - of cool so i can go ahead and make a
207:20 - new notebook i think it's using jupyter
207:21 - notebooks underneath
207:23 - um but what we'll do can we save that
207:25 - query
207:26 - we sure can i'm not going to save that
207:28 - query today because i do not care but
207:30 - yeah it is jupiter notebooks great so
207:32 - what we'll do is go file new notebook
207:35 - and here i can write some text so
207:37 - this will
207:38 - return back
207:41 - both products
207:44 - and product categories
207:47 - okay
207:48 - and then what i can do here
207:50 - is add another piece of code
207:53 - we'll paste that in oops
207:56 - and i can even say like limit so i could
207:58 - say like
207:59 - limit to
208:01 - 20
208:02 - or maybe 10.
208:04 - let me go ahead and run that
208:07 - and we'll just connect on the server
208:09 - here just needs to connect
208:11 - okay
208:13 - and we spelt limit wrong so we'll just
208:15 - spell that again there
208:19 - limit
208:22 - i don't think i'm spelling it wrong but
208:24 - i'll just take it out because i don't
208:25 - feel like goofing around with that today
208:26 - and so here we have our records
208:29 - okay
208:30 - it should just be limit right
208:32 - l-i-m
208:35 - maybe it's not limited tsql give me two
208:36 - seconds tsql limit
208:41 - um
208:47 - oh
208:48 - they don't have a limit function i did
208:50 - not know that okay well that's kind of
208:51 - interesting they have things like where
208:53 - row is equal to like i'm just looking at
208:55 - it over here right so c areas use the
208:57 - top record so select top five
209:02 - you know
209:04 - top ten let me try that
209:07 - there we go so i guess it's just not
209:09 - limit so we have that in there which is
209:11 - kind of nice um and so let's say we
209:13 - wanted to narrow that down to something
209:15 - in particular uh if there's a way we
209:17 - could browse the data
209:20 - oops
209:21 - i mean that by accident but what we
209:23 - could do here is just take a look
209:25 - i wanted to see the um i'll do it over
209:27 - here i want to see the
209:29 - product category id
209:31 - because then maybe we could just do a
209:33 - like a where statement really quickly
209:35 - and so these are all mountain bikes when
209:37 - it's five so what we'll do
209:40 - is we'll make a where statement
209:42 - and we will say when pc product
209:46 - category id is equal to five
209:50 - uh and then we'll hit run
209:53 - okay and then up here we'll just say
209:54 - that you know show us
209:56 - oops
209:57 - uh can i edit this
210:00 - yup
210:01 - show us the top
210:03 - 10 mountain bikes
210:08 - it's not really saying like it's not
210:10 - this is not very useful information
210:11 - because it's not saying uh why it's the
210:13 - top ten it's just listing it there's no
210:15 - ordering here so maybe that would be
210:16 - also a good idea here so let's just take
210:18 - a look at our data structure here i mean
210:21 - we could do it in
210:22 - in here as well if we want to
210:24 - uh explore that way but i think it's a
210:27 - bit easier
210:28 - to do it here so let's look at the
210:29 - product
210:31 - um and see if there's like any
210:32 - definitive information that makes it
210:33 - interesting like list price
210:35 - so maybe that's something we could
210:37 - return here so we'd say um p dot list
210:39 - price
210:41 - as price
210:45 - and what we could do is order that so
210:47 - we'd say order by p dot list price
210:52 - descending
210:59 - there we go
211:00 - when you have to do that tutorial it's
211:01 - it's easier to forget so now it's
211:03 - listing it from descending prices uh
211:05 - that amount isn't very human readable i
211:07 - don't know if it's like 3000 whatever i
211:08 - wonder if we could use a function to do
211:11 - that if we're using ms or sorry like
211:13 - postgres it would just be like floor
211:15 - so maybe i could like floor the result
211:17 - here or round it let's just see if i can
211:19 - do that
211:20 - sometimes it's like round yeah there it
211:22 - is
211:23 - those are like built-in functions to
211:24 - most sql languages here
211:27 - and
211:28 - uh it needs two arguments what's the
211:30 - second oh probably what to round it to
211:32 - so we'll probably put a zero here
211:33 - because it's gonna be like rounded to
211:34 - what
211:36 - maybe it needs to be a one
211:39 - other languages you just put round in
211:41 - so i don't know what that is so what
211:42 - we'll do is look that up so we'll say
211:43 - tsql round
211:47 - okay just doing that off screen here
211:49 - and so it says number expression is an
211:51 - expression of the exact numeric
211:52 - approximation is the precision in which
211:55 - you want it to be
211:56 - if i can get a nice example so
211:59 - uh round's a number
212:02 - i just don't want it with zeros
212:06 - um
212:09 - so that will round it to
212:11 - two decimal points and then we have
212:12 - negative two so we'll go up the top here
212:15 - is the precision to numeric to be
212:17 - rounded length must be an expression of
212:19 - tiny intent when the length is a
212:20 - positive number is rounded to the number
212:22 - of decimal points specified with the
212:23 - length with a number
212:25 - uh it's on the left side of the decimal
212:27 - point oh okay that's kind of interesting
212:29 - so what we want to do is probably round
212:33 - the
212:33 - [Music]
212:34 - left or right side
212:36 - so either give it two there
212:39 - can't convert data type
212:41 - v and char oh its name you know what
212:43 - you're probably watching me be like
212:44 - andrew what the heck are you doing so
212:46 - it's supposed to be up here so we do
212:47 - round
212:50 - okay and i'll just say two
212:52 - and that should round it to two decimal
212:53 - points
212:54 - which is fine but i don't i really don't
212:56 - want anything there so i'll say zero
213:00 - and you know i probably would want that
213:01 - just as an integer there's probably a
213:02 - way to cast it so maybe there's like a
213:04 - cast function or two int
213:07 - so i don't see that so
213:09 - cast uh float
213:12 - to
213:12 - [Music]
213:13 - ant uh tsql
213:17 - yeah it's called cast that's what i
213:18 - thought
213:19 - and so it's as simple as that so what we
213:20 - can do
213:22 - and we probably don't need to round it
213:23 - because the cast will probably uh round
213:25 - it itself
213:27 - so we can say cast and
213:30 - say as int
213:34 - very good that looks really nice let's
213:36 - say we wanted to like concat something
213:38 - onto that
213:39 - uh again i don't know how to do concat
213:41 - and tsql so we'll look it up so we'll
213:42 - say tsql concat
213:44 - strings in postgres it's just a pipe
213:47 - oh it's a plus wow we get a plus i love
213:50 - that you don't usually get a plus so
213:52 - what i'll do here is i'll cast this as
213:54 - an end
213:54 - and then i'll cast this as
213:57 - i don't know what it is in this as
213:58 - varchar bar chart yes
214:01 - can i do that first it's still a number
214:03 - great and now that it is a a string we
214:05 - can do maybe string
214:08 - dollar sign like that we'll say run
214:10 - we'll let me do that yes very good and
214:12 - we'll give it
214:13 - also the usd
214:16 - there he goes that's kind of nice so now
214:17 - we kind of have a human readable human
214:20 - readable thing there okay
214:22 - so what i'll do is i'll just copy that
214:24 - over put it in my notebook
214:26 - and
214:27 - we'll just have that there
214:29 - i'll move that off screen
214:31 - and you know that's pretty much how you
214:32 - would create data with sql now now that
214:35 - we have a mysql database we can actually
214:37 - consume it with uh with a lot of other
214:39 - services like power bi or maybe some
214:42 - other services that we can think of but
214:44 - yeah how about we do that next let's try
214:46 - to load this data into power bi
214:48 - [Music]
214:52 - all right so now let's take a look at
214:54 - using power bi with our ms sql server
214:56 - here so in the bottom left corner you
214:58 - saw that i downloaded that file earlier
214:59 - so i'm going to go ahead and open that
215:00 - up if you don't have it just make your
215:02 - way over to power bi and download it
215:04 - again and all it's going to do is
215:05 - establish a connection to the data
215:07 - source now we didn't really need to
215:08 - download this file to do it it's not
215:10 - that hard to establish our own
215:11 - connection uh but it does save us one
215:14 - little step here okay so
215:16 - it's gonna load here just give it a
215:17 - moment
215:19 - it's gonna pop this up if you don't see
215:20 - this what you can do i'm just gonna go
215:22 - ahead and close this to show you how to
215:23 - make a manual connection so i'm going to
215:24 - go over to server name copy that go back
215:27 - and if we go into now you have this
215:30 - button here but we'll just go get data
215:32 - more so you have a more predictable way
215:34 - of loading data in
215:38 - and we'll go to database here we can go
215:40 - to azure and so we have azure sql
215:43 - database um which i think that's what
215:45 - we're using
215:47 - and so we'll go ahead and hit connect
215:52 - and so we'll put the server name in
215:53 - there
215:54 - and the server name was my azure
215:57 - sql
215:58 - we'll hit ok
215:59 - we'll give it a moment to connect and
216:01 - we'll get back to the same screen so
216:02 - here we can import the data that we want
216:04 - we'll say product product category um
216:07 - customer
216:09 - right customer address address
216:11 - the more we bring in the more it's going
216:13 - to be so you know what i'm just going to
216:14 - narrow it down to just product product
216:16 - category and customer you can transfer
216:18 - the data which is really nice i'm not
216:20 - going to get into that because that's a
216:20 - whole other thing but we'll go ahead and
216:22 - hit load and what this is going to do is
216:25 - it's going to start to import this data
216:29 - and we'll give it a moment
216:35 - so it's going to load the data in and
216:37 - then it's also going to detect
216:38 - relationships and do our data modeling
216:40 - so the data is now in if we go to here
216:42 - we'll be able to see these tables and
216:44 - explore that data in its raw format if
216:46 - we go over to this tab here we can see
216:48 - that it's detected a relationship the
216:50 - more tables the more relationships it
216:51 - will auto detect so we didn't want to
216:53 - make that too hard i want to go back
216:55 - over here to our report and i just want
216:58 - to show you how you can visualize some
216:59 - information so one of the easiest ones
217:00 - to do off the bat is card
217:03 - so i'm looking for card
217:05 - it's this one nope slicer
217:08 - card okay and so i'm just dragging this
217:10 - one out just there we go i just clicked
217:12 - it whatever you have to do to get there
217:14 - and this is great if you have a single
217:15 - field so i'm going to go drop this down
217:17 - and i'm going to find product by list
217:19 - price so i'm going to drag that onto the
217:21 - field here notice it went into there it
217:22 - only takes one input for the fields and
217:24 - there's my price if i want to change the
217:26 - look of it and go down here and there's
217:28 - all sorts of things we can change here
217:30 - like shadow
217:33 - you know other things that you can do
217:34 - for fun but you know there's a little
217:36 - bit of styling there
217:37 - and let's say i wanted to see that as a
217:40 - table
217:41 - so what i could do is just change that
217:43 - to table and now it's a table uh table
217:45 - display but this information is not very
217:47 - useful so what i want to do is drop this
217:49 - down and then
217:51 - drag out the name of the product
217:52 - category remember that these have a
217:54 - relationship it's established here so
217:55 - it's going to know how to slice that
217:57 - information to make it useful and so now
217:59 - we have a breakdown of
218:01 - price and name based on
218:03 - category right
218:05 - and i mean that's category which was a i
218:08 - don't know how to rename that very well
218:10 - yeah for this visual here we'll say
218:13 - uh you know product or so this would be
218:15 - category
218:18 - and for here we'll just say we'll just
218:19 - rename that to make it a bit easier
218:21 - we'll just say price
218:22 - there we go now let's say we want that
218:24 - as a graph so we just change that over
218:26 - to a graph so we can kind of see the
218:28 - difference in prices
218:29 - all sorts of things we just click around
218:31 - here to have some fun
218:34 - okay bar chart is going to be the most
218:35 - useful one i believe so yeah
218:38 - i don't know if there's anything else
218:38 - that's really fun here we have another
218:40 - breakdown kind of see like uh
218:42 - the cost there of all the stuff that
218:43 - they're selling
218:44 - okay i think that has the highest price
218:46 - is the bikes right because so the volume
218:48 - of sales is just the cost of each item
218:50 - right that they're selling uh you know
218:52 - so that's how you'd use power bi desktop
218:54 - with um
218:56 - uh with your sql server so yeah there we
218:58 - go
219:01 - so another thing that we can do since we
219:02 - have power bi and we connected it to our
219:04 - mysql server or our sql server no
219:06 - problem is we probably should try to
219:07 - publish
219:08 - it to
219:09 - the power bi service so that we can see
219:11 - how to make dashboards so what i'm going
219:13 - to do is go up the top here and type in
219:14 - app.powerbi.com
219:16 - if this is the first time you've ever
219:17 - gone here it's going to ask you to make
219:19 - an account even even if you have a
219:20 - microsoft account it's just another way
219:22 - of
219:22 - authorizing that there and so here you
219:24 - can see uh we can explore all sorts of
219:26 - data sets if we click into here these
219:28 - are just dashboards right
219:31 - and we'll go in here we can see all this
219:33 - kind of information which is uh really
219:35 - nice and we can go here and
219:37 - just select some information you can see
219:39 - it's very interactive which is really
219:40 - nice
219:41 - um and you know we can go ahead and
219:43 - create our own uh kind of things here if
219:45 - we were to publish a data center or do
219:47 - whatever but uh you know what i want to
219:50 - do
219:51 - is uh to connect some more data sources
219:53 - download power bi desktop we already
219:55 - have that installed so what i want to do
219:56 - is just publish and get something into
219:58 - here so i'm going to reopen up our file
220:01 - we had there earlier
220:03 - and establish a connection to our ms sql
220:06 - server
220:07 - we'll make a little port and we'll
220:09 - publish it and see how we can access it
220:10 - through the power bi service here okay
220:15 - so we'll give it a moment and all i want
220:17 - is product and product category
220:21 - once it decides to load here
220:23 - so we want product and product category
220:26 - and i will say load
220:32 - we'll give it a moment
220:39 - and now that is loaded what we'll do is
220:42 - again make ourselves a visual so i will
220:44 - drag out a card like we did last time
220:48 - and we'll go down to product here i will
220:50 - try price
220:52 - uh it's better probably better if it's a
220:53 - table it's a little bit more useful i
220:55 - think
220:56 - and we'll go here to product category
220:57 - and drag out the name
220:59 - again we'll rename these we'll just say
221:01 - rename price
221:04 - rename
221:08 - category
221:12 - and what we'll do there's a publish
221:14 - bubble go ahead and hit our publish
221:15 - button so do you want to save changes
221:16 - yes
221:18 - uh and we'll just save this whatever we
221:20 - want to call we'll just say my power bi
221:22 - report
221:24 - okay
221:28 - and
221:29 - uh so we will just have to put our email
221:32 - in here
221:40 - okay oops
221:43 - it's thinking we'll give it a second
221:50 - there we go so it's asking me to log
221:51 - into my account
221:55 - and we'll say my workspace sounds great
221:56 - to me
222:00 - and we'll give it a moment to
222:02 - publish that report
222:04 - great that report has now been published
222:06 - we'll make our way back to power bi
222:08 - and so it should be under our workspaces
222:10 - right
222:12 - all right so what we'll do is make our
222:13 - way over to our workspace on the
222:14 - left-hand side and you'll have to click
222:16 - on here it's not very clear and we have
222:18 - a data center report if we click into a
222:19 - report we have this error about missing
222:21 - credentials so what we'll do is make our
222:23 - way over to our data set
222:24 - and we'll just go to uh settings here
222:27 - and we might need to provide some
222:29 - credentials here again so let's edit the
222:31 - credentials
222:32 - and we'll just say azure user capital t
222:35 - testing one two three
222:37 - uh privacy level settings is private
222:41 - we'll sign in
222:43 - and you know what the pro
222:45 - i wonder if it'll give us access to user
222:46 - updates etc because it's from a a uh
222:50 - you know like remember how we had that
222:51 - firewall rule so it could be the
222:52 - firewall rules that is preventing it
222:55 - so by clicking on my report
222:57 - yeah it's still a problem so what i'll
222:58 - do is go over to my azure server set the
223:00 - firewall or rules here
223:02 - allow azure services to access this
223:04 - resource yes
223:06 - um
223:08 - so it's denying the rules
223:13 - so we'll have to figure that out there
223:16 - okay so it looks like actually let's go
223:18 - in the dataset and updating the
223:19 - credentials did work that message was
223:21 - just a bit of a false flag there i mean
223:22 - like it did need to reestablish
223:24 - connection but uh
223:25 - uh you know i thought i thought maybe i
223:27 - had to go digging around in our firewall
223:29 - settings but it since we have this
223:30 - turned on you know powerbi should be
223:32 - able to access it and it can so we have
223:34 - uh we're looking at a report right now
223:36 - within power bi but let's say we want to
223:38 - make a dashboard well how would you do
223:39 - that well you go ahead and you just go
223:40 - and hit the pin
223:42 - we'll say my dashboard
223:44 - we'll go ahead and pin
223:46 - that and uh i mean if we wanted to
223:49 - create a mobile layout we could do that
223:53 - okay so there it is for our mobile
223:55 - layout and we can go back to our web
223:56 - layout right
223:58 - and from there you know now that we have
224:00 - our dashboard we can go ahead and just
224:02 - you know share that with our team or put
224:04 - it in chat you know whatever it is but
224:06 - this is where the point where you would
224:07 - then hit the power bi pro where you have
224:09 - to upgrade but uh yeah it's as simple as
224:11 - that so yeah there you go
224:14 - [Music]
224:19 - so we saw azure data studio as a means
224:21 - to connector database but there's
224:23 - another tool which is um
224:24 - at sql server management studio i
224:26 - figured we should give that a go so what
224:28 - i'll do is go ahead and download that
224:30 - and it's 635 megabytes so you have to
224:33 - decide whether you want to download that
224:34 - once that's downloaded what we'll do is
224:35 - go ahead and install and give it uh give
224:37 - it a look okay
224:38 - all right so after a very long wait sss
224:41 - ms is done downloading and even in here
224:43 - it says that it downloads azure data
224:44 - studio with it so they really really
224:46 - want to use azure data studio which
224:48 - we've already used but you know it
224:49 - doesn't hurt to open up ssmss and give
224:52 - it a go if you can use it
224:56 - and so i'm just going to go ahead and
224:57 - install that there
224:59 - okay
225:02 - and it sounds great
225:07 - it's going to go
225:12 - there we go
225:14 - couldn't tell i hit install or not
225:17 - all right so waiting a little bit of
225:18 - time here that this finally installs
225:20 - we'll go ahead and close that and now
225:21 - what we'll do is go actually ahead and
225:23 - open that program
225:25 - ms
225:27 - i'm not sure where it installs into so
225:28 - i'll see you back here in a moment
225:30 - all right so i'm back and i found it
225:32 - it's under c program files microsoft sql
225:35 - server management studio 18 common seven
225:37 - id now uh you know your start menu you
225:40 - should be able to type in ssms and find
225:43 - it for whatever reason on my computer
225:44 - nothing ever shows up under the start
225:46 - menu and i can't be bothered to fix it
225:48 - but it's good to know where it actually
225:49 - installs to so now we know where it is i
225:51 - just double clicked it and now it is
225:52 - open and so here we can see
225:55 - a bunch of stuff and this will be a way
225:57 - for us to load this so what we'll need
225:58 - is our server name as per usual
226:01 - so i'm going to go back to azure here
226:03 - and i'm going to grab the server name
226:05 - we'll go back paste that in we'll choose
226:08 - sql server authentication we'll type in
226:10 - azure user we'll type in testing with a
226:13 - capital t one two three
226:15 - connect
226:16 - honestly i like this a lot better
226:18 - as your data studio i don't know like
226:20 - real apps just feel feel like they work
226:22 - a lot better you know and so we can go
226:24 - into our database here look at some
226:26 - tables
226:29 - okay give it a moment to load
226:31 - and we can go into like customer or
226:33 - maybe product
226:35 - okay uh there should be a way to
226:37 - visualize whoops like that so maybe go
226:39 - design mode here
226:46 - give it a second to open up
226:50 - yeah so we can kind of see like what
226:52 - columns there are and stuff in here
226:53 - which is really great but what i want to
226:55 - do is create a new query
226:58 - and i just want to show you that you can
226:59 - query it in here too just query it
227:01 - everywhere right and so uh from our
227:03 - azure data studio that i still have open
227:05 - from before what we can do is grab
227:07 - ourselves uh this code here
227:09 - and paste it on in and go ahead and
227:11 - execute that and we can see we get the
227:12 - results so you know this is a very very
227:14 - very very powerful tool um you know and
227:17 - it's just i could do spend all day
227:18 - trying to show you how to use this uh
227:20 - tool here but um yeah i mean this is all
227:22 - you need to know
227:23 - so yeah there you go
227:26 - so that is ssms
227:28 - and i'll just go ahead and close that
227:30 - and there we go
227:31 - [Music]
227:36 - hey it's andrew brown from exam pro and
227:37 - what we're going to look at here is
227:39 - storage accounts and specifically blob
227:40 - storage tables and maybe azure files
227:44 - and so what i want you to do is go all
227:45 - the way to the top here and by the way
227:47 - i'm just carrying this over from my
227:48 - mysql or sql tutorial so something to do
227:51 - while i'm waiting for this thing to
227:52 - install but what we'll do is type in uh
227:54 - account
227:55 - or storage account so we'll go here
227:58 - and oh yeah just hit cancel there that's
228:01 - for my other tutorial
228:03 - we'll hit add
228:05 - and we'll create a storage account
228:07 - okay and what we'll do is i'll say dp
228:11 - 900
228:13 - storage it will say ok
228:16 - and we'll name this storage account
228:17 - we'll say we'll call it
228:19 - my storage account
228:21 - notice we have the option between oops
228:23 - one two three four five six there we go
228:25 - oh
228:26 - somebody really really wants uh to have
228:29 - the same name as me nine eight seven six
228:30 - five four three two one because those
228:32 - are unique names right um oh it doesn't
228:35 - like that uh can only be contains must
228:37 - be three characters is now too long okay
228:39 - there we go and so they're uniquely
228:41 - identifiable across all accounts that
228:43 - kind of gets annoying between standard
228:44 - and premium if you choose it you get a
228:46 - few different options so with premium
228:48 - you get block blobs file shares page
228:50 - blob so we're going to stick with
228:51 - standard to save some money we change
228:53 - the redundancy we'll stick with local
228:55 - redundant zones because that's the most
228:56 - cost effective go over to advance see
228:58 - what else we want here
229:00 - um
229:02 - here we have the option to turn on uh
229:05 - uh
229:06 - higher or higher cool name space that's
229:08 - if we're doing data like gen storage 2
229:10 - which we're not doing at this point in
229:11 - time so i'll do is go ahead and hit
229:13 - review and create
229:16 - and we'll scroll down and say create
229:18 - okay
229:21 - and we'll just wait for that to create
229:23 - the storage account i'll see you back
229:25 - here in a moment all right so we waited
229:26 - a little bit of time here and our
229:27 - storage account is ready so i'll go in
229:29 - here and what we'll do is go to the
229:31 - left-hand side we can see we have some
229:32 - containers we wanted to make a file
229:34 - share it's really easy just hit the file
229:35 - share button and we'll name it so say my
229:38 - azure file share
229:40 - okay
229:42 - and we can pick how the tiers we want um
229:46 - transaction optimize seems okay to me we
229:48 - have to set a quota
229:50 - uh
229:51 - we'll say i don't know one gigabyte
229:52 - we're not really going to do anything
229:53 - real here so i'm just gonna hit create
229:56 - okay and so now we have our nice file
229:58 - share there if we click into it we can
230:00 - uh upload files so i need to go grab
230:03 - some kind of file off the internet and
230:05 - we'll just type in star trek here
230:07 - anything
230:09 - images as long as it's appropriate and
230:12 - yeah here's a photo so go ahead and save
230:13 - that save image as
230:16 - and we'll go to our
230:18 - downloads here
230:20 - and we'll just say
230:23 - kirk
230:24 - and spock
230:26 - okay
230:28 - and that is now downloaded we'll make
230:29 - our way back here i'll say upload
230:34 - what we can do is go to our downloads
230:36 - again
230:37 - kirk and spock we'll say open
230:39 - upload and there's the file right
230:42 - uh not super complicated so that's that
230:46 - um let's go back over to here into our
230:49 - storage accounts
230:50 - uh back here
230:52 - and i want you to show you uh storage
230:54 - explorer so you can go ahead and
230:56 - download search explorer if you haven't
230:58 - so you go here you download it hit
230:59 - download now i think i already have it
231:01 - installed so all i'll do is just click
231:02 - open
231:03 - and say open takes like two minutes to
231:05 - install
231:07 - yep i do
231:08 - and this is just an easier way like if
231:10 - you don't want to have to open up the
231:12 - portal and you want to be able to easily
231:13 - work with files it's another way you can
231:14 - do stuff so
231:20 - we'll give it a moment to open and so
231:21 - here we have our storage account we can
231:23 - drop it down and see what's under there
231:24 - because we have a bunch of disks that i
231:26 - need to go ahead and delete
231:27 - but if we go here to file share we have
231:30 - our file share here
231:32 - and you can just see it's just an
231:34 - alternative interface we can upload and
231:36 - do stuff there if i wanted to download
231:37 - that i could do that okay
231:39 - but let's go take a look at blob storage
231:42 - okay
231:44 - so go over to containers we'll make
231:46 - ourselves a new container we'll say
231:48 - my blob storage
231:50 - uh yeah it can be private that's fine
231:55 - we'll say create
231:57 - we'll click into our blob storage
231:59 - and we need to upload a file so again
232:01 - we'll upload the same file so we'll go
232:02 - here
232:03 - and we'll upload our kirk and spock file
232:06 - here
232:07 - and we'll upload it
232:08 - and there it is again
232:10 - and if we go over to
232:12 - our storage explorer and we go under
232:14 - blob containers
232:15 - and there
232:17 - you can see we have the data there so
232:18 - that's really really nice
232:20 - um
232:21 - so there's that
232:23 - we should probably take a look at
232:26 - tables
232:28 - so go back to my storage account
232:30 - left hand side we'll make ourselves a
232:31 - new table so we'll say my azure table
232:35 - say okay
232:37 - we click into this actually we can't
232:39 - this is kind of a pain you'd have to use
232:41 - the api stuff but this is where the
232:42 - explorer comes in handy so on the
232:44 - left-hand side we will look for tables
232:48 - and we'll say my azure table
232:52 - and uh we'll say add
232:55 - okay we have to set a partition key and
232:57 - a row key so our partition key would be
233:00 - something like
233:02 - we could say wharf
233:04 - and our category could be lieutenant
233:07 - commander
233:09 - okay insert
233:11 - and uh if we wanted to edit this we
233:14 - could add more property such as like
233:17 - um
233:19 - you know
233:21 - planet
233:23 - kronos
233:24 - whatever you want you got all sorts of
233:26 - types here but it's pretty darn simple
233:28 - as you can see not super complicated
233:30 - there
233:31 - um but yeah maybe we should learn how to
233:34 - use the azure tables via the cli so i'll
233:37 - be back here in a second and show you
233:38 - how to do that okay
233:39 - all right okay so i have the commands
233:41 - that we can do to uh do that for the cli
233:43 - so what i want you to do is click up
233:44 - here in the top right corner that is the
233:47 - azure cloud shell there and i want you
233:49 - to start in bash if it asks you to
233:51 - create a storage account go ahead and do
233:53 - that
233:54 - because it needs a volume so under here
233:56 - we'll actually make one this is the one
233:57 - that is for my developer environment
233:59 - so just hit yes and just wait for this
234:02 - to load sometimes it takes a little bit
234:04 - of time
234:05 - and if it's giving you a lot of trouble
234:07 - i'm going to hit restart
234:11 - doesn't usually give me that much
234:12 - trouble but
234:26 - there we go okay great it's i'm so used
234:28 - to waiting around in azure it's crazy
234:30 - but uh so what we can do is type in a z
234:32 - that is the cli command for az if we hit
234:34 - enter it should spit out some
234:35 - information
234:38 - yeah so it tells us all the things we
234:40 - can do there so what we want to do i'm
234:41 - typing clear here az storage identity
234:45 - insert hyphen t we want to put the name
234:47 - of our table table is called my azure
234:50 - table
234:51 - uh we need the
234:53 - container uh account name we'll need we
234:55 - don't need the container name so the
234:56 - account name is
234:58 - up here so i'm going to try to type it
235:00 - in here i'm going to go ahead and paste
235:01 - that in and now we can go ahead and
235:04 - insert
235:06 - so yeah
235:07 - yeah we'll go here to the hyphen e so e
235:09 - is going to be what we insert so we have
235:10 - partition key
235:12 - and this will be uh we'll say uh
235:16 - beverly
235:17 - and we'll put the row key as a commander
235:22 - and then we could say planet
235:25 - equals earth
235:28 - okay
235:29 - and if we hit enter
235:31 - i think we got it all right there give
235:33 - it a second
235:35 - uh it doesn't know what account name is
235:39 - let me just see if i spelt that wrong
235:42 - maybe it just doesn't like where it is
235:44 - so i'm going to just copy it out of here
235:46 - trying to make a bit clean by putting it
235:48 - there but um
235:50 - i will put it on the end here
235:54 - oops go ahead and paste that in there
236:00 - still doesn't like it um if it doesn't
236:02 - like that we could try giving it the
236:04 - container name
236:07 - it might just pick up the account name
236:08 - which is the account we're in um but i'm
236:10 - going to go here and find my container
236:14 - that's where i had this like working
236:15 - like a second ago and just decides not
236:17 - to work okay
236:22 - okay so that's fine so i already have a
236:24 - working version here on the side so what
236:26 - i'm going to do
236:28 - i'm just going to change some of the
236:29 - values here
236:33 - okay
236:36 - and so i have this here and i know this
236:38 - 100 works right so we have
236:41 - um
236:42 - a z a z sword storage identity identity
236:44 - insert insert
236:46 - um
236:48 - hyphen t for the table so maybe that was
236:50 - my problem is that i just didn't do
236:52 - hyphen t there so i'll just paste that
236:53 - in there i don't think container is used
236:55 - anymore but we'll hit enter
236:58 - so it doesn't like the container name so
237:00 - we'll just erase that out there
237:08 - and so there it has inserted the data so
237:09 - there we go so yeah i'll just show you
237:11 - like azure is a bit painful sometimes
237:13 - even when you have perfect instructions
237:14 - it still doesn't work properly we'll go
237:16 - ahead and hit refresh and there is the
237:18 - record in our table database
237:20 - so there you go
237:22 - uh and i might just leave these accounts
237:24 - open here because they might want to
237:25 - adjust them in another tutorial but of
237:27 - course we will clean them up at some
237:29 - point under the end of the course okay
237:34 - [Music]
237:35 - hey it's andrew brown from exam pro and
237:36 - in this uh follow along what we're going
237:38 - to do is take a look at cosmodb so what
237:41 - i want you to do is go to the top here
237:42 - and type in cosmodb
237:44 - and we'll go here and add
237:47 - a new cosmodb account and notice that we
237:49 - have some options we have sql
237:51 - mongodb cassandra gremlin azure table so
237:54 - what i want to do
237:56 - i'm going to be pretty crazy here i'm
237:58 - going to make a new core sql one
238:01 - and so we'll just say under here i'll
238:02 - say cosmo
238:04 - dp900
238:06 - cosmodb
238:08 - okay
238:09 - and i'm just gonna call this one cosmo
238:13 - db
238:14 - coresql
238:17 - and just to be more you can say dp900
238:19 - here i don't know if this one conflicts
238:21 - with other ones so we have provision
238:22 - throughput and serverless i'm going to
238:23 - choose serverless because um i don't
238:25 - need provision throughput
238:27 - and there is a free tier here but i'm
238:30 - going to go with serverless okay just so
238:32 - we don't have to worry about it we've
238:33 - got a lot of options here like global
238:34 - distribution et cetera like that
238:37 - i don't care about any of that stuff
238:40 - and so what i'm going to do is go ahead
238:41 - and hit review and create
238:45 - okay
238:48 - and we're going to repeat this process
238:50 - again and again so what i'm going to do
238:52 - is go back to cosmodb
238:55 - create a new account
238:56 - and we'll create a mongodb one now
238:59 - and we'll drop this down and we'll
239:00 - choose cosmodb
239:02 - i think i called it dp900 cosmodb so
239:05 - this would be dp900 cosmo
239:09 - db
239:11 - db the name is not available you just
239:13 - have to change until you get it i'm
239:14 - going to make this one serverless as
239:15 - well
239:16 - and we'll hit review and create
239:19 - so i just want to show you the variance
239:20 - of these ones okay
239:22 - so this is a little bit of annoying but
239:24 - what we have to do
239:25 - we'll add another one here
239:27 - this one will be azure table
239:29 - we'll drop it down
239:31 - we'll choose uh dp900 cosmodb
239:34 - we'll say cosmodb
239:37 - uh we'll say dp900 cosmo
239:40 - db
239:41 - um
239:43 - azure table
239:45 - serverless
239:47 - go ahead review and create
239:49 - and we'll create that
239:51 - and we'll go one more time we'll do a
239:53 - graph database okay oops
239:55 - doesn't want us to go back
239:58 - and we'll say gremlin
240:02 - and we'll choose
240:03 - our dp900
240:05 - dp900
240:07 - cosmo
240:09 - db
240:11 - gremlin
240:13 - okay serverless review and create
240:16 - and we'll go back to here and i'm just
240:19 - going to wait until these are all done
240:20 - so i'll see you back when we see all of
240:22 - our i think we made four four of our
240:24 - cosmo dbs okay
240:26 - all right so after waiting a little
240:27 - while here actually my gremlin database
240:29 - didn't create or my account didn't
240:30 - create so i had to make that twice but
240:32 - here they are and you can see this one
240:34 - is still creating but we can go and do
240:36 - some things while we're waiting first
240:37 - let's go check check out the core sql
240:40 - and so on the right hand side you'll see
240:41 - there's this quick start it's always a
240:43 - great way to get started so if we wanted
240:45 - to
240:46 - uh you know in azure we could start
240:48 - inserting data so if i was to go over to
240:51 - node.js and we say create items
240:53 - container
240:55 - this is what i'm the most familiar with
240:56 - now a lot of people go over to the
240:58 - explore data but i'm just curious as to
240:59 - what this stuff looks like so
241:01 - because i've never used the quick starts
241:02 - before so i'm just curious to see the
241:04 - quality of it so let's go ahead and
241:05 - create that items container
241:07 - right now i think it's just waiting to
241:09 - create so if i go over to that data
241:10 - explorer tab
241:12 - uh it's probably just going through that
241:14 - process of creating so it actually
241:15 - created us a to-do list which is nice
241:18 - okay and it set up a partition key
241:21 - and oh looks like it's done so now we
241:23 - can go ahead and download that
241:26 - application says npm install npm start
241:29 - it's kind of cool
241:32 - and i have it over here so i just got to
241:34 - unzip it
241:37 - okay so i'm just unzipping that here
241:41 - choosing winrar that's what i got
241:42 - installed and we'll just drag that on
241:45 - over here
241:47 - and i wonder
241:49 - we'll go up here
241:50 - now you may or may not be able to easily
241:52 - open this depending on how your setup of
241:54 - your computer is so if you do you can't
241:55 - just you can just follow along here i'm
241:57 - opening up visual studio code here you
241:59 - just can't see it's off screen
242:02 - but i just want to close some things in
242:04 - here
242:06 - okay
242:07 - and i'm just going to go open
242:10 - go here file
242:12 - open a folder
242:14 - and this is in my downloads
242:16 - so i'm going to navigate all the way to
242:18 - my downloads here so we'll say um
242:20 - [Music]
242:21 - this is users andrews
242:24 - if you want to see me i'm just writing
242:25 - this over here downloads
242:28 - sql and we'll say okay
242:33 - and what we'll do is open up a new
242:34 - terminal here
242:36 - and the instructions said that we should
242:38 - be able to do
242:40 - npm install npm starts we'll give that a
242:42 - go we'll say npm install
242:52 - i might as well take a look here at the
242:54 - code just curious what's going on here
242:56 - so it's actually using the azure's cosm
242:58 - yeah cosmodb client
243:00 - it has a config file over here which
243:02 - contains uh our configuration
243:04 - information looks like it has some
243:05 - initial items that it's going to insert
243:08 - so there's some initial data which looks
243:09 - kind of cool
243:11 - it sets up a partition key here we have
243:13 - an endpoint so
243:15 - what it does create the database if it
243:16 - does not exist
243:18 - read the database definition create the
243:20 - container if it does not exist
243:22 - read the container scale the container
243:27 - create family item
243:29 - query the container
243:32 - so there's a lot of examples here so
243:34 - here it just gives you some stuff so you
243:36 - know if you know you're coding and you
243:38 - want to go through this it's a great way
243:39 - to get started it looks like
243:41 - finished installing we'll do an npm
243:42 - start i don't know if i'll be able to
243:43 - actually view this
243:45 - because it's running through a
243:47 - machine oh so it just runs it okay i
243:49 - thought i was going to like uh
243:50 - output a um
243:52 - like a localhost 3000 you could view it
243:54 - so it actually just ran that stuff so
243:55 - create the to-do list reading created
243:57 - etc
243:59 - so yeah all of it's there so if you're
244:01 - familiar with that you can take a look
244:02 - but what we'll do now is make our way
244:04 - over to day explorer we can also go up
244:06 - to cosmos
244:08 - uh was it
244:10 - cosmos.azure.com is that what it is
244:13 - there we go this has a little bit more
244:15 - room i'm going to click on sign in
244:17 - it's the same thing as the date explorer
244:18 - just a little bit easier to work with
244:20 - subscription one
244:22 - and we are in sql here we'll drop it
244:24 - down now we actually have some data we
244:25 - can take a look at some good items
244:28 - check out items here
244:31 - and
244:32 - it should show us some right
244:37 - hmm
244:40 - go back here
244:46 - it says
244:48 - first name etc so you can see that it
244:50 - created the to-do list reading the
244:51 - database create the items reading
244:55 - oh completed with an error entity with
244:57 - the specified id does not exist in the
244:58 - system okay so it's expecting something
245:00 - to already exist there and that's why it
245:02 - couldn't run
245:04 - so maybe what we'll do is just go ahead
245:06 - and create what it wants okay
245:09 - so while we go ahead and create a new
245:10 - item so the partition key is called
245:12 - partition key that's a very uncreative
245:15 - name but when you create these records
245:17 - you always have to have the id and the
245:18 - partition key name so i'll go here and
245:21 - we'll type in partition key
245:24 - and i guess it could just be whatever we
245:25 - want
245:27 - um i'm just looking at the record here
245:29 - to see what they wrote
245:31 - so we just scroll up here
245:35 - we are looking for create item
245:39 - just scroll up here see if we can find
245:40 - it
245:46 - this is more like reading an item here
245:47 - i'm not necessarily creating
245:50 - partition key at sign country usa so
245:53 - um
245:55 - i guess we just say usa here
245:58 - say name
246:00 - oops
246:02 - andrew
246:04 - go see if he'll let us create that
246:07 - uh we'll go ahead and just copy this
246:09 - oops
246:11 - place with id so we'll just say one
246:13 - we'll say save
246:15 - okay and it added some additional data
246:16 - underscores ts is for uh for a time
246:19 - stamp okay
246:20 - um and now what we can do is go and look
246:23 - at that data so we'll hit more oh i
246:25 - guess we're exploring it right there
246:26 - okay so let's say we wanted to query
246:27 - that data we could create a few other
246:29 - records because that's not a lot of
246:30 - information right so what i'll do is
246:31 - just copy this we'll create a new item
246:33 - here
246:35 - uh i'm technically not the usa i'm in
246:37 - canada so we'll put in vaco here
246:42 - just copy that and we will save
246:45 - we'll create a new item
246:47 - uh they should have been different ids
246:49 - but whatever
246:51 - we'll say brazil
246:53 - put in roger
246:56 - and we will go ahead and save okay and
246:58 - so now let's say we wanted to query that
246:59 - data uh
247:01 - it should be
247:03 - these icons are very cryptic
247:05 - let's store procedure whoops
247:09 - um
247:12 - yeah i guess we could do it right here
247:16 - no
247:17 - usquery that's what i want okay so
247:20 - uh if we hit run
247:22 - execute query to return us all the data
247:25 - but let's say we wanted to only select a
247:26 - subset of data so we just do where
247:29 - um
247:30 - name
247:31 - equals andrew let's give that a go
247:35 - see if that works
247:36 - oops
247:39 - uh what if we do c dot name
247:44 - there we go so yeah it's as simple as
247:46 - writing queries not like any other kind
247:48 - of sql so
247:49 - that's pretty easy let's take a look at
247:52 - another one here so that was that let's
247:54 - close that off and we'll take a look at
247:55 - mongodb next
247:57 - all right let's take a look at mongodb
247:59 - so we'll go ahead and click into here
248:00 - make our way to quickstart and we have
248:02 - some options we have node.js so if you
248:04 - wrote javascript and use the connection
248:05 - girl that's one way of doing it i think
248:07 - mongodb might be the shell might be a
248:09 - fun way to do it so what i'll do
248:11 - so go over here i don't think i have
248:12 - this installed but it comes part of the
248:14 - server installation for mongodb so i'll
248:16 - go ahead
248:18 - if you would like to download the
248:19 - sell shepardly from server yeah that
248:21 - sounds better to me
248:23 - i don't think i need everything and so
248:25 - select the zip download which includes
248:28 - the
248:30 - okay
248:33 - and i guess we'll have to click on the
248:35 - computer community edition here
248:39 - oh
248:39 - windows
248:41 - we can do the zip or msi yeah i guess
248:44 - we'll just install the full thing it's
248:45 - not a big deal
248:47 - maybe i'll use it for something else
248:50 - but we'll go there it's only 200
248:51 - megabytes so we'll just wait for that to
248:53 - download
248:54 - shouldn't take too long here
248:57 - there we go
248:59 - and i just got my downloads over here
249:01 - we'll open that up
249:07 - sure we'll go ahead and hit next i agree
249:10 - next
249:11 - i guess we can customize it
249:15 - so maybe if we only wanted the client
249:18 - i'm going to just take everything
249:20 - um
249:21 - we'll say next that seems all fine to me
249:23 - we'll hit next install
249:31 - this process is on a mac or linux it's
249:33 - just going to be different process but
249:34 - yeah we'll go ahead and see this here
249:40 - all right so after waiting a very long
249:41 - time uh on that compass step it finally
249:44 - uh finished installing i guess it was
249:45 - doing a visual installer there it looks
249:48 - like it is opened
249:50 - and uh i guess we installed
249:52 - compass and it makes it really easy cool
249:54 - uh
249:55 - sure we'll say yes to everything there
249:57 - there's a little bit too much
249:57 - information
249:59 - but i guess what we can do is establish
250:00 - a connection and i guess i was thinking
250:02 - we just use the shell but this looks
250:04 - better so what we'll do is go ahead and
250:07 - take a look here so we will go grab our
250:09 - connection string
250:11 - and hopefully you'll just take it as
250:13 - that it looks like it's hold on here
250:17 - i think what we really want is this
250:20 - this is the string here connection
250:22 - string
250:23 - uh i guess we can grab it from here it's
250:25 - a bit easier
250:26 - we'll go back to our connection service
250:29 - here we'll hit connect
250:30 - fingers crossed it works first time
250:32 - yeah there we are we're in okay cool so
250:35 - uh i it's a new data like it's
250:37 - completely new so i guess we wouldn't
250:39 - have anything in it like a database or
250:40 - anything like that so we can do at the
250:43 - same time as we're doing it let's make
250:44 - our way over to cosmodb
250:47 - um oh i guess i closed it
250:49 - cosmos azure.com
250:52 - and we'll go down and flip over to our
250:54 - mongodb database
250:57 - and we don't we haven't created anything
250:58 - yet so we have to make a new database so
251:00 - i'll just say
251:01 - my database
251:02 - or my mongodb
251:05 - my collection can you think of a
251:07 - collection as like a table right
251:09 - uh shared key
251:11 - whatever you want uh my shared key
251:14 - again i'm just showing you how to insert
251:15 - stuff
251:17 - uh we don't need this sharded so we'll
251:19 - just say unsharded okay
251:22 - makes it a bit easier on us here
251:24 - i just want to see if that actually
251:25 - reflects over here in our manga db
251:27 - compass
251:29 - because it probably should
251:32 - we'll give it a moment there to create
251:33 - the database
251:35 - if it doesn't what we can do because
251:36 - there's a reload here right so let's hit
251:38 - reload until the data loads
251:43 - and
251:44 - yeah looks like we have it now so that's
251:46 - all good and i guess we have to
251:50 - click on this maybe click on this one
251:51 - again here to reconnect
251:54 - great now we have a database
251:56 - so we can go in here
251:58 - click on my collection
252:00 - we can add data let's hoping to execute
252:02 - a command oops
252:05 - and i'm just trying to see if it allows
252:07 - us to do that here in any way
252:09 - again i'm not that familiar with this
252:10 - tool
252:11 - but yeah i guess we can go create a
252:13 - document here we can make it through
252:14 - here so let's go to documents
252:16 - we'll add a new document
252:18 - and say one
252:20 - and
252:21 - i guess we should probably put our shirt
252:23 - key so my shared key
252:26 - say
252:27 - blue
252:30 - we'll just say name
252:32 - andrew
252:35 - and we will go ahead and save that
252:36 - document
252:38 - we'll go back here
252:40 - uh refresh
252:41 - we can see our document there we can
252:43 - edit it in here as well a lot nicer in
252:45 - this say we can add data we'll say
252:47 - document
252:50 - it's a little bit more cryptic
252:52 - i think i prefer to do it in there i'm
252:54 - again i'm not that familiar with this
252:55 - tool but what i really wanted to show
252:56 - you was the command shell so what we'll
252:59 - do
253:01 - oh let's just do shell up here
253:04 - oh that's nice okay i thought we were
253:06 - going to have to uh you know i thought
253:08 - we're going to have to paste this in so
253:09 - maybe we didn't have to install that all
253:11 - along but hey this looks pretty good to
253:13 - me right
253:14 - so what i have pulled up here is the
253:18 - api i had here a second ago at least
253:22 - say mongodb shell
253:24 - i definitely had it open here for us
253:26 - um because i wanted to show you the api
253:33 - uh so there's like right
253:36 - one
253:40 - okay here we go so i found the mongodb
253:43 - api here and if i just go back it's
253:45 - shell method so if you typed in
253:47 - mongodb.com for slash mail reference
253:48 - methods you can see all the kind of
253:50 - stuff that we can do
253:51 - and so let's just go ahead and see if we
253:53 - can insert something we only need to
253:55 - insert a single document and so here we
253:58 - kind of see the uh the stuff here yeah
254:00 - underscore id is the the main field
254:02 - there and what we'll do
254:05 - is scroll on down and we'll just go
254:07 - ahead and copy this command and see
254:10 - if we can get that to work okay
254:13 - so we'll go back
254:15 - over here give that a paste
254:18 - hit enter
254:21 - and we'll go back to our document and
254:22 - see if it's there
254:24 - there it is
254:25 - we will go back and check in our mongodb
254:27 - cluster here give it a refresh
254:31 - i want to see
254:35 - uh well it shows two records
254:40 - so what i'll do
254:42 - yeah it's not what i want so we'll try
254:44 - this again
254:50 - yeah i just started typing it six i
254:52 - already have an error
254:53 - and we want the my shared keys we'll say
254:56 - my shared key
255:00 - okay
255:02 - double quotations we'll say green
255:05 - and we'll just put in here name
255:09 - baco
255:11 - we'll say insert okay that record is
255:13 - there
255:14 - uh we'll give this a refresh here oops
255:18 - there it is there's the record as well
255:23 - it didn't insert the other one i wanted
255:25 - so we'll go back into our shell here
255:27 - we'll say db.products it's probably
255:29 - because i didn't i you know if you don't
255:31 - have the um
255:33 - partition keys just not going to insert
255:34 - it right i thought it would error out or
255:36 - say something that's what i was looking
255:37 - for so i'm going to say id we'll say 5
255:41 - i'm going to put my shared key
255:44 - we'll say red and we will put name here
255:48 - and we'll say roger
255:49 - we'll give that a go
255:53 - and if you don't type it right it will
255:54 - not work so we'll say insert one
255:58 - there we go we'll go back to our
255:59 - document here we'll give it a refresh
256:01 - here
256:05 - i'm having like no luck doing inserts
256:07 - today
256:09 - but you know you get the general idea
256:11 - right so we're not here to really teach
256:13 - you a mongodb tutorial but i just want
256:14 - to show you that you know you use the
256:16 - shell to insert data there's this tool
256:19 - here and what a document looks like so
256:21 - that pretty much covers the document
256:23 - mongodb so what we'll do
256:25 - let's go back here and let's take a look
256:27 - at azure tables alright so let's take a
256:29 - look at cosmo db azure table so click
256:31 - into it we'll make our way over to the
256:32 - data explorer
256:34 - we will say new table and i'll just say
256:36 - my new table
256:38 - say okay
256:42 - and we'll give it a moment to create
256:44 - this new table here
256:48 - as you can see we didn't have to make it
256:49 - a database or anything it was just a lot
256:51 - more straightforward here
256:55 - but it does seem to take a long time to
256:57 - make a table to be fair it is serverless
256:58 - so it could be uh having to provision
257:00 - something before it do something we'll
257:02 - go here and look at entities and we can
257:04 - go ahead and add ourselves
257:07 - uh a new entity here we go up the top
257:09 - here so we'll just say worf
257:12 - lieutenant commander
257:16 - planet
257:17 - kronos
257:22 - add entity and there it is
257:25 - okay
257:27 - go maybe over to the query builder
257:32 - and if we wanted to filter this out we
257:34 - could say partition key i guess we need
257:36 - another record for this to make sense so
257:38 - we'll say um
257:40 - data
257:41 - uh
257:43 - commander
257:46 - um okay he's not from earth but we'll
257:48 - just put earth
257:49 - we'll add that entity we'll add another
257:51 - one might as well
257:53 - we'll say um crusher
257:57 - commander
257:59 - earth
258:02 - and we can go here we just filter it out
258:04 - so we'll say crusher
258:07 - uh partition key yep that's right so it
258:09 - should be able to run that so run query
258:12 - okay gets that exact record so pretty
258:14 - simple not too complicated and so now
258:17 - we'll move on to gremlin
258:19 - all right so let's take a look at
258:20 - cosmodb gremlins so click into it and
258:22 - what i want you to do is go to quick
258:23 - starts we'll go to guided gremlin tour
258:26 - and we'll create ourselves a sales graph
258:28 - collection so we get a little bit more
258:29 - information that we normally would have
258:34 - give it a moment here
258:44 - and as there we go yep it's ready so we
258:46 - created the sample this downloaded a
258:48 - package that contains a console app as
258:50 - dot net the console app needs to be
258:51 - executed first to upload the sample data
258:54 - and the asp.net web app will allow you
258:55 - to visualize your data okay so i guess
258:57 - we have to go ahead and do that so we'll
258:58 - go ahead and download that
259:02 - and there it is
259:04 - i'm just going to unzip it here
259:07 - and we'll double click and do it
259:10 - and it doesn't look like much but i'll
259:13 - drag it out into my folder here
259:16 - whoops
259:18 - and we will go and click into this
259:22 - and
259:23 - i want to know how to use this so i
259:24 - guess we'll open this up in
259:27 - vs code
259:31 - okay let's give this a read
259:35 - sample application shows to interact
259:36 - with the cosmic db gremlin etc etc
259:39 - visual studio code 2015
259:41 - sample data has two projects uploading
259:43 - the sample data is done via the console
259:45 - application project included in the
259:46 - quick start
259:48 - run the console app
259:55 - all right so it's saying that we need to
259:57 - go to tools
259:59 - and we're going to double click this
260:01 - and luckily i have visual studio
260:04 - installed if you don't have install you
260:05 - don't have to go do this but we just
260:07 - want to see a nice looking gremlin graph
260:09 - in our cosmodb
260:12 - and i will go ahead and sign in
260:22 - and i'll use my example account
260:32 - and i guess it really wants me to choose
260:34 - some stuff but i'm fine let's just go
260:35 - ahead and start it
260:49 - uh the c sharp project
260:51 - is targeting.net which is not installed
260:53 - in this machine to proceed select the
260:54 - option below change the target
260:57 - you can change it sure
260:59 - as long as it runs you know
261:01 - i didn't feel like i was going to be
261:02 - doing.net today but we'll go ahead and
261:04 - open this up
261:05 - all that really matters is that we can
261:07 - run it projects loaded ready to use in
261:09 - the background and so we need to
261:11 - run this
261:14 - so it says over here
261:19 - so verify the settings if you download
261:21 - it etc okay if the console is run upload
261:23 - etc i just want to run it run the
261:25 - application f5 so i think it's time to
261:27 - hit f5 on my keyboard
261:29 - and so i have a mac keyboard so i'm not
261:31 - sure if that's going to work but what
261:32 - i'll do
261:34 - is i'll go to the top here oops
261:39 - i might need to install extra components
261:40 - so we'll go ahead and install whatever
261:42 - components it wants
261:44 - just click that over here in the right
261:46 - hand corner there
261:47 - it's probably because it needs a very
261:48 - particular version of net yeah it needs
261:50 - the.net desktop development here
261:53 - even though i'm not a dotnet developer i
261:55 - still know my way around ids pretty darn
261:57 - well
261:58 - so we'll go ahead and give that an
262:00 - install
262:04 - and it looks like it's at 848 megabytes
262:06 - i'll see you back here in a moment okay
262:11 - all right so after waiting a little
262:12 - while here it looks like it's installed
262:13 - and restarted and i think i can actually
262:16 - run the application now so we'll go at
262:18 - the top here
262:20 - and we will run let's say
262:26 - should be like a project run i guess we
262:27 - just hit the start button
262:29 - that works too right
262:32 - you.net fans that are watching you can
262:34 - make as much fun as you want to be okay
262:37 - uh and so you know we just want this to
262:39 - run so that it loads the data right
262:41 - that's all this thing does so it's going
262:42 - to build the project
262:50 - and it's complaining about virus and
262:52 - threat protection not really worried
262:53 - about that
262:55 - and it said it succeeded verified the
262:57 - database exists your collection etc
262:59 - uploading the graph now excellent
263:02 - there we go
263:06 - and
263:08 - we're just going to wait for it to
263:09 - upload those nodes and we're going to
263:11 - have a really really good example here
263:12 - right
263:19 - i'm not sure how many nodes there are in
263:20 - this
263:21 - could be a lot
263:26 - there we go oh now it's starting up
263:28 - edges great
263:31 - this is 469 nodes i wonder if there's
263:33 - going to be as many edges or less we'll
263:35 - see
263:39 - there we go so now it says graph
263:41 - uploaded you can now show this
263:43 - application
263:44 - okay sounds great so what we'll do and
263:47 - i'll just go ahead and close visual
263:49 - studio because i don't need it open
263:50 - anymore
263:51 - and
263:52 - what we'll do is open up the data
263:54 - explorer
263:56 - actually kind of prefer cosmodb so we'll
263:57 - type in cosmodb again
264:00 - and
264:01 - we will
264:03 - switch over to
264:06 - our gremlin database
264:08 - here and go into persons
264:12 - graph database
264:15 - you should be able to see a
264:16 - visualization without have well i guess
264:17 - we can just hit execute search query
264:24 - hmm
264:25 - maybe insert it in this one we'll go
264:27 - ahead and execute it over here
264:30 - there we go so now we have some records
264:32 - so cool
264:33 - all right we got that working and um so
264:36 - yeah this is like that gremlin language
264:38 - up here um i'm not that familiar with it
264:41 - so you know what i'm going to do i'm
264:42 - going to be right back and uh and learn
264:44 - it in two seconds okay
264:45 - all right so i'm back and i learned a
264:47 - little bit here and what i did was i
264:48 - went to the apache gremlin website for
264:51 - tinker pop or gremlin in particular so g
264:54 - dot v parentheses is when we want to get
264:56 - all records back if we put in a
264:58 - particular id here it should give us a
265:00 - particular result i also figured out as
265:01 - we move around here if you scroll into
265:03 - things like this
265:05 - and you click on there it will expand
265:07 - the next area it should there it goes
265:09 - it's a bit slow right
265:11 - but you know we have an id here so let's
265:12 - give it a go and see if it actually
265:14 - works so we'll do g v
265:16 - and
265:18 - let's see if we put that in there like
265:19 - this we'll give it uh maybe quotations
265:22 - they just have the number one so i'm not
265:24 - sure to tell you there and yeah so it
265:26 - grabbed that particular point there
265:28 - um it looks like we can grab different
265:30 - values so i'm assuming that these are
265:32 - values on it here so maybe we'll just do
265:34 - dot values here
265:37 - values
265:39 - uh well if we give a label here
265:43 - there we go service plan so yeah it's
265:44 - not that hard go back here execute the
265:48 - plan and then there's like commands like
265:50 - out e i assume that's the way of like
265:51 - you would select things in the region so
265:54 - um
265:55 - i don't know if like that is a
265:57 - definitive one but we'll just go ahead
265:59 - and put that in there and see what
266:00 - happens
266:04 - and we got nothing
266:07 - because you know we have relationships
266:09 - between other sources like edges
266:13 - and so maybe it's based on the label
266:15 - here so let's try this
266:21 - execute the plan
266:31 - yeah i'm not sure what it is
266:33 - but my point is is that we populated
266:35 - ourselves a graph you can tell that
266:36 - that's how you use the language if you
266:38 - really want to learn more you can go
266:39 - through the tutorial
266:41 - uh on how to create that information but
266:44 - for our purposes we just wanted to show
266:46 - that we could popular graph and play
266:47 - around with one there a bit okay
266:49 - so that is gremlin we did
266:53 - we did azure so that's all of them so
266:55 - we're all done here again i'm going to
266:57 - keep these around just in case i want to
266:58 - pull them into other services and at the
267:00 - end we will destroy all these resources
267:02 - okay
267:03 - [Music]
267:07 - hey this is andrew brown from exam pro
267:09 - we're going to learn how to use data
267:10 - factories to run a transformation job
267:12 - maybe between our sql and our blob
267:14 - storage because we do still have the
267:16 - setup from previous tutorials so i'll
267:18 - make my way over to factories you just
267:19 - type in data factory at the top here and
267:22 - you can see i have one there it's right
267:24 - now deleting because i'm not happy with
267:25 - it
267:26 - but what we'll do is create ourselves a
267:27 - new one so i'm going to make a new group
267:29 - here dp900 data factory 2
267:34 - okay
267:36 - and here i'll have to name it so we'll
267:37 - say dp900 data factory
267:42 - we'll go with version two it doesn't
267:44 - like it so i'm gonna put a two on the
267:45 - end there
267:46 - and we'll go to next i do not want to
267:49 - use git at all that was the reason i uh
267:51 - i restarted this was because it was
267:53 - complaining about that
267:54 - for networking it's going to have a
267:56 - public endpoint for we have the ability
267:58 - to encrypt our data which we do not care
267:59 - today i'll go ahead and hit review and
268:01 - create
268:03 - we'll go ahead and create arc data
268:04 - factory
268:06 - and we'll give it a moment here
268:11 - it doesn't take too long to set up a
268:12 - data factory so
268:16 - there it is so we'll go ahead and go to
268:17 - resource and the way we're going to
268:19 - actually access it i don't know why they
268:21 - have the button here but this is where
268:22 - it is it's author and monitor it's going
268:24 - to open up takes about a second and here
268:26 - we are so what we'll do is we want to
268:28 - create ourselves a new pipeline we'll
268:29 - make it easier on ourselves go to the
268:30 - left hand side hit the plus create a
268:32 - pipeline
268:34 - and we'll say sql to
268:36 - [Music]
268:37 - um sql to what
268:40 - it's a bit glitchy there sql to blob
268:44 - okay
268:45 - and we'll go ahead and save that um i
268:50 - think it uh
268:51 - does it auto save yeah it auto saves so
268:54 - what we'll do is go to the left hand
268:55 - side here go to link services click the
268:57 - plus we need sql we have an sql database
269:00 - so we'll go here we'll say
269:02 - continue we'll have a connection string
269:05 - we'll select our subscription our server
269:08 - name our database name our type is sql
269:10 - authentication azure user
269:14 - capital t testing one two three
269:17 - okay you can have always encrypted on
269:19 - there i'll go ahead and create that you
269:20 - can hit test connection if you're
269:21 - concerned that it's not working i think
269:23 - we can click into it hit text test
269:25 - connection there it's got a nice little
269:27 - green so that's all good we want to put
269:29 - this in our blob storage so i will add
269:32 - another one here
269:33 - we'll type in blob
269:35 - we will create that hit continue
269:39 - i don't care about the name but what
269:40 - we'll do is use a connection string to
269:42 - select it as well
269:44 - so we'll go down here subscription one
269:46 - it's my storage account
269:49 - and we will test the connection and it's
269:52 - all good we'll hit create
269:55 - cool so we'll now go back to our
269:57 - pipeline or our authoring here we need a
270:00 - data set um
270:02 - can we get that from sql we sure can
270:06 - we'll click on that we'll hit continue
270:09 - we'll select our linked service which is
270:11 - here
270:12 - we'll select the table we want so let's
270:14 - say we want to translate over the
270:15 - products
270:17 - and we will go hit ok
270:21 - all right and so now we have our data
270:23 - set
270:25 - okay and so we'll move on to the next
270:26 - step all right so let's go set up a
270:29 - transformation so what we'll do is drag
270:31 - out copy data and we will choose our
270:33 - source and we'll say our sql table and
270:36 - we're going to want to put that into a
270:37 - csv oops i did not create our data set
270:39 - here so we'll create a new data set go
270:42 - over to blob storage here
270:44 - hit continue choose csv format so you
270:47 - can do excel json all fun things
270:50 - we'll choose our link service which is
270:51 - blob storage uh this has to be the name
270:54 - of our blob storage i already forgot
270:57 - what it's called so we'll go up here
271:00 - make our way over to storage accounts
271:03 - on the left-hand side go into storage
271:04 - accounts we need to find that container
271:06 - name
271:07 - there it is
271:08 - go all the way back
271:10 - put that in there
271:12 - call that transform and we'll say
271:14 - products dot csv
271:16 - first row is header
271:18 - sure but we don't have any import scheme
271:20 - because like you could have
271:22 - one in there and say okay this is what
271:23 - the schema should be like have the
271:24 - headings in there so it knows what it
271:26 - needs to translate over to but uh it's
271:29 - going to be a one-to-one mapping so it's
271:30 - going to be something really simple here
271:33 - uh so this seems all fine to me notice
271:35 - we don't have any schema we could import
271:38 - one but there's nothing to import so go
271:40 - back to our copy data we'll choose
271:42 - delimited file and we'll take a look at
271:44 - mappings we'll import schemas it's going
271:46 - to import the schema from the table here
271:48 - are the column names here so as you can
271:51 - see there are no column names which is
271:52 - totally fine
271:54 - and uh what we can do
271:56 - is go ahead and let's go hit uh debug
272:02 - okay and so it's queued up
272:06 - and now what we'll do is go ahead and
272:07 - validate
272:10 - okay
272:15 - and it says it's been validated no
272:17 - issues as of yet
272:19 - we'll go hit publish
272:22 - um yep go ahead and publish it
272:27 - and so now it has actually ran the
272:29 - pipeline so what i want you to do is go
272:31 - over to your storage accounts
272:33 - give it a refresh
272:34 - go into my blob storage now we have a
272:36 - folder called transform we have a
272:38 - product called uh
272:39 - csv called products we'll go ahead and
272:41 - download that
272:44 - okay
272:47 - we can go ahead and open that up in
272:48 - excel
272:49 - and there you go so we just did a
272:51 - transformation it's not the most
272:53 - beautiful transformation but you can see
272:54 - that it's a very powerful tool we didn't
272:56 - have to write any any code whatsoever
272:58 - which is really nice so there you go
273:05 - hey it's andrew brown from exam pro and
273:07 - in this follow along we're going to take
273:08 - a look at a data bricks and sparks so
273:11 - what i want you to do is go to the top
273:12 - and type in azure data bricks we'll go
273:14 - ahead and hit add we'll create an azure
273:16 - databrick service
273:17 - and what we'll do is hit create dp900
273:21 - azure databricks
273:23 - okay
273:25 - within our workspace here what we'll do
273:27 - is choose
273:28 - dp900 azure databricks very uncreative
273:31 - of me but
273:33 - it works out fine
273:36 - and we have some options standard
273:38 - premium trial i'm going to stick with
273:39 - standard i do not want this to cost much
273:41 - whatsoever
273:43 - go ahead and create that environment
273:45 - there
273:48 - and i'll see you back here in a moment
273:49 - when it's deployed
273:51 - all right so um our databricks
273:53 - environment is ready so if you click
273:54 - launch workspace it does take a little
273:55 - bit time to launch so i'm not pressing
273:57 - it but we'll have our environment here
273:59 - and so uh you know there's stuff you can
274:01 - do here like create clusters and run
274:04 - jobs and notebooks but uh just so you
274:06 - don't spend any money i'm going to show
274:08 - you another way you can run this service
274:09 - which is totally uh totally won't cost
274:11 - you anything so we'll go to databricks
274:14 - and type in community edition
274:16 - if we go down below below here i already
274:18 - have a log if you sign up here
274:21 - it's exact same thing pretty much
274:24 - but there's no chance of you having
274:26 - unexpected spend so i prefer to do it
274:28 - this way even the cluster is shut off
274:30 - over a certain amount of time and so
274:32 - we'll click on the explore quick start
274:33 - tutorial here
274:35 - and i think it's shift
274:37 - enter if you want to the commands it's
274:38 - in the top right corner this little
274:39 - keyboard thing it tells you what you can
274:41 - press so run command and move to next
274:43 - cell that sounds like what we want to do
274:45 - so i'm going to hit
274:47 - shift enter
274:48 - shift so it says in the sidebar create a
274:50 - cluster etc but if we just keep on going
274:53 - here it should automatically create this
274:55 - one so automatically launch one without
274:57 - to clusters without prompting so we'll
274:58 - just hit attach and run okay
275:01 - and that's going to start up a cluster
275:03 - but as it's running we can just kind of
275:05 - take a look here and so you can see that
275:07 - we're running sql commands
275:09 - um and so it looks like we're it has
275:11 - we're loading a csv and then we're
275:14 - treating the csv with sql which is
275:16 - really nice so that's the thing where
275:17 - it's like you have data that's not
275:18 - necessarily in a database but you're
275:20 - able to run sql commands on csv files
275:23 - and things like that and then down below
275:25 - you can see uh it's using python to use
275:27 - spark to load and format the file
275:30 - um and uh and you can even visualize
275:32 - information in here so it automatically
275:34 - let us plot it and change some of the
275:36 - options here so we go here make a
275:37 - histogram
275:38 - quantile
275:40 - map that's useless area
275:43 - bars so it's very very useful tool but
275:46 - wait till that cluster spins up here
275:50 - takes a little bit of time
275:52 - if you want to monitor a cluster we can
275:54 - go on the left-hand side and just go to
275:56 - clusters here
275:58 - okay and you could leave this and and
276:00 - and totally not be worried about it you
276:02 - don't have to worry about it like
276:03 - spinning up and costing you money okay
276:06 - so i'd go here and give it a refresh
276:11 - and i don't see the cluster
276:17 - so i think it's just because i was
276:18 - terminating an old one and now it's it's
276:20 - waiting until it can determine uh start
276:22 - up a new one
276:24 - so
276:26 - it might not spin up but it's not really
276:28 - that oh there we go it's terminating
276:31 - great
276:35 - uh is it going now
276:39 - you can never tell these things
276:43 - is the free edition
276:44 - so but if i just go back here we can
276:47 - even look at the commands here cyborg
276:49 - create cluster quick start and database
276:51 - runtime drop down to l3 create a cluster
276:54 - um so is it going to create cluster now
276:58 - nope we'll cancel there yes
277:02 - and let's just follow it manually so i
277:04 - go to clusters create the cluster
277:07 - here it's saying l3 lts
277:12 - kind of gives you an idea how old their
277:14 - tutorial is
277:16 - lts
277:17 - but that's what it wants we should match
277:19 - it exactly
277:20 - sparking scala i doubt that it's for
277:24 - that one there
277:26 - and so
277:27 - we will just name our cluster quick
277:29 - start
277:31 - oops
277:34 - right after the name of what it asked
277:35 - for
277:36 - oops
277:40 - and we'll go ahead and create
277:48 - and we'll i guess wait till that starts
277:50 - up
277:52 - it could take about a few minutes to
277:53 - start up so what i'll do is i'll see you
277:55 - back here in a moment when it started
277:56 - okay
277:58 - all right so we had to wait there a
277:59 - little bit but uh now it looks like our
278:01 - cluster is running so what i want you to
278:03 - do is go back to that notebook we had
278:05 - and this time it should possibly we'll
278:07 - close the other one because that is our
278:09 - the one we don't want to use but we'll
278:11 - do shift enter shift enter shift enter
278:14 - um it's not would you like to launch a
278:17 - new cluster
278:18 - no i want to use the existing one maybe
278:20 - i can drop down and choose the cluster
278:22 - oh here we go um
278:25 - cancel hold on here
278:27 - quick start confirm
278:30 - there we go okay
278:32 - and we'll see if that executes the
278:33 - command
278:45 - now it's a bit slow but again this is a
278:47 - community edition and also imagine if
278:49 - you're dealing with massive amounts of
278:51 - data so it's not really surprised that
278:53 - it's not like super fast um
278:55 - but like you have to think of it at
278:57 - scale right and so it completed the job
278:59 - so that was pretty good
279:01 - we can go ahead and run this line again
279:02 - for fun
279:07 - okay
279:08 - so you get the idea here right
279:11 - so that's all i really wanted to show
279:12 - you here um just kind of a bit of
279:14 - exposure to data bricks there
279:16 - but you'd have to learn all about apache
279:18 - spark to really understand the stuff
279:20 - behind it
279:21 - um but uh yeah so what we'll do and
279:23 - again you don't have to shut it off it
279:25 - doesn't matter but what we'll do is just
279:26 - terminate and be be good about that
279:30 - also we'll delete this at the end all
279:32 - these follow alongs for the dp900 we'll
279:34 - definitely be sure to delete all of them
279:36 - but just for this one in particular i
279:37 - just want to get rid of it
279:39 - so i don't forget about it um yeah there
279:41 - you go that is
279:46 - databricks hey this is andrew brown from
279:49 - exam pro we're going to be looking at
279:50 - azure snaps analytics today in this
279:52 - follow along so what we'll do is go to
279:54 - the type here and type in synapse
279:57 - make our way over to analytics we'll go
279:58 - ahead and add ourselves a new workspace
280:02 - and i'll create a new one here this is
280:04 - the dp900 so we'll say dp900 um synapse
280:08 - workspace
280:10 - okay
280:12 - and we need to give it a managed
280:14 - resource group uh so
280:17 - i guess same thing dp900
280:21 - synapse
280:22 - workspace
280:24 - i'm just going to keep on naming it the
280:25 - same as long as i can have it for our
280:28 - subscription we'll have to create a new
280:30 - account because we do not have
280:33 - a gen 2 storage account i wonder if i
280:34 - could just make it in my other one nope
280:36 - so we'll have to just because i already
280:37 - have a storage account right on there so
280:39 - we'll say data lake storage
280:42 - okay
280:44 - probably wants it all lower case data
280:45 - lake storage
280:47 - dp900
280:49 - okay
280:51 - and uh we don't have a file system so
280:55 - dp900 data lake file system
281:00 - okay
281:01 - and notice here it says assign myself
281:03 - storage blob data contributor role for
281:05 - the data lake storage too if you
281:07 - remember uh from our
281:09 - lecture content about very important
281:12 - roles this is one of them and it's being
281:14 - auto assigned to us
281:16 - go here to security here it has this but
281:18 - i'm going to just say azure user and
281:20 - capital t testing one two three capital
281:22 - t testing one two three just so i can
281:24 - remember between our other ones enable
281:26 - double encryption that sounds good but
281:28 - not something we're doing today
281:31 - um we'll go to network this seems okay
281:33 - to me next review and create
281:43 - and just notice up here it says azure
281:44 - synapse workspace
281:46 - is 6.40 so it's not something we want to
281:49 - uh have lying around
281:51 - um this is per terabyte right but we're
281:52 - not going to be working with a lot of
281:53 - data here so it shouldn't be a big deal
281:55 - for us
281:56 - we'll go ahead and create that
282:07 - and uh yeah i'll see you back when this
282:09 - is all done okay
282:14 - all right so after waiting a little bit
282:15 - of time there our synapse analytics
282:17 - environment is ready what i want to show
282:19 - you on the left hand side is if you
282:20 - scroll on we'll actually got to go to
282:22 - the workspace first scrolling down you
282:24 - have your analytics pool so you have sql
282:26 - pools where you can run sql operations
282:27 - so you can go there and create those
282:29 - clusters these get pretty darn expensive
282:31 - so um
282:33 - it's not even showing the estimation
282:34 - there but um
282:35 - the thing is is that you know if you
282:37 - needed sql you could do that because
282:38 - remember there's two engines that you
282:40 - can use you can use the sql engine or
282:41 - the apache spark
282:43 - engine right so you create those pools
282:45 - sql pools first you can also create them
282:48 - within um the workspace but we'll go up
282:50 - to overview the workspace is the studio
282:52 - so we're going to open that up
282:54 - and once we get in here
282:57 - we can ingest data and do all sorts of
282:59 - things so it makes it very easy but
283:01 - let's actually just go back
283:04 - you can't exit out of that once you open
283:06 - that up so i'll go and reopen this up
283:08 - because they do have some test data we
283:09 - can use
283:10 - which is uh should not cost us much but
283:12 - if we go all the way over to learn here
283:15 - and we go to sample
283:16 - sample data we have some options here if
283:18 - you choose this one it'll create a pool
283:20 - but what i'm going to do is go to query
283:22 - with data with sql
283:24 - and it will give us some sample data so
283:26 - we click on this
283:28 - and what you can see here and this is
283:30 - just me trying to run it before
283:32 - but
283:33 - what you'll see here is
283:36 - we could use
283:37 - the synapse sql right
283:39 - and we're actually able to import files
283:41 - so here it's importing a parquet file
283:44 - from a um
283:45 - azure opens like it's a storage account
283:48 - by azure and it's just an open data set
283:50 - so here you can just kind of see that
283:52 - there's ways of querying it and what we
283:54 - would do is choose where we want to do
283:56 - it so we go built in we can publish that
283:58 - before we uh do that there and we'll go
284:00 - ahead and hit run and so you know it's
284:02 - going to pull that data
284:04 - it's going to take a little bit time to
284:05 - get it loaded in and before it runs it
284:07 - but it doesn't usually take too long and
284:09 - there is the data so
284:12 - yeah that gives you kind of an idea
284:14 - how azure synapse uh works
284:17 - um but yeah so if it was an sql here you
284:19 - just write your apache spark
284:22 - usual stuff there uh yeah and hopefully
284:24 - that gives you a good idea how that
284:25 - works i don't like to keep the service
284:27 - running it is can get very expensive
284:29 - very quickly so me being paranoid i'm
284:31 - going to make sure i terminate this one
284:33 - definitely early
284:35 - even right now
284:37 - um but yeah that is azure synapse
284:40 - analytics okay
284:42 - [Music]
284:46 - all right so we are at the end of our
284:48 - follow alongs and you could tell that
284:49 - they all kind of tied in with one
284:51 - another
284:52 - so what we'll do is we do some cleanup
284:54 - so from the left-hand side go to
284:56 - resource groups
284:57 - and and it's dp900 you can go ahead and
284:59 - delete so what i'm going to do is just
285:01 - click on each one here
285:02 - it could be from other ones but i'll go
285:04 - ahead and just type in like cosmodb
285:07 - again it's based on what you name them
285:09 - but this is the fastest way i find to
285:10 - clean up all my stuff
285:12 - and i'll go here and just keep on going
285:14 - down the list
285:17 - you know because mostly the things we
285:19 - used they're not going to have ongoing
285:20 - cost the one that will will definitely
285:23 - be like
285:24 - the sql server so that's something that
285:26 - we definitely need to terminate
285:29 - okay some of these might give us some
285:31 - trouble
285:32 - so i'm again i'm just running down the
285:33 - list here deleting as much as i
285:36 - can and then i'm going to see if there's
285:39 - any errors here but
285:40 - i think we already deleted this one but
285:42 - we'll give it another go here
285:50 - and yeah that should be all of them here
285:52 - if you get a failure you might have to
285:54 - investigate it so here it says failed to
285:56 - resource the client
285:58 - uh does not have permission yeah i do
286:01 - however that's tonight against the deny
286:03 - assignment so the idea is just you know
286:05 - wait for this to finish
286:07 - uh it could take a little bit of time
286:09 - and sometimes like it's deleted but it
286:11 - just takes time for this to vanish
286:14 - and another thing you can do is go to
286:16 - your resources and just make sure the
286:17 - resources aren't running anymore so i'm
286:19 - looking here
286:20 - and what's going to cost me money is
286:22 - this sql database that's running so
286:25 - you know i want to make sure that stuff
286:26 - is deleted
286:28 - so say yes
286:30 - you could also do it from here as well
286:32 - you should let the resource groups
286:33 - delete it because sometimes it gets
286:35 - confused
286:36 - but uh you know i just want it all gone
286:39 - i've got an ip here
286:41 - data factory storage account all these
286:44 - things i want them gone
286:47 - just say yes
286:50 - uh you know it doesn't show up here is
286:51 - disks i know i have a bunch of disks
286:54 - let's go over here no there's nothing
286:56 - okay good i thought i might have had
286:57 - like a bunch of disks you know why
286:58 - because i was looking at my explorer and
287:00 - probably just showing the old ones so
287:02 - yeah it takes a while for this to to
287:03 - happen but um you know i'll come back
287:05 - here and show you that i've deleted them
287:07 - all and that's what you should do too
287:08 - okay so i'll see you back here in one
287:10 - more wrap-up video and that's it all
287:12 - right so after waiting uh quite a little
287:14 - while here i just wanted to see if it
287:16 - was all cleaned up so i'm going to give
287:17 - it a nice refresh here looks like my
287:18 - resource groups are cleaned up if i make
287:21 - my way over to my resources here it
287:23 - doesn't look like there's much uh here
287:25 - remaining so
287:27 - and just check our notifications make
287:29 - sure everything is deleted so that's how
287:30 - we make sure that everything is cleaned
287:32 - up in our azure account
287:33 - and that's it for our follow alongs okay