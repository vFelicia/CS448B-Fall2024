00:00 - learn highlevel system design by coding
00:02 - a YouTube clone starting with a basic
00:05 - flow you'll gradually add three key
00:07 - Services upload watch and transcoder
00:10 - this course covers the actual highlevel
00:12 - Design Concepts in practice including
00:16 - chunking transcoding and FFM Peg and
00:19 - adaptive bit rate streaming using hls
00:21 - City teaches this course and she will
00:24 - help you build a sophisticated video
00:26 - platform and Master System design
00:28 - principles files are being generated
00:31 - chunks or parts did you see this
00:33 - happened in parallel adaptive Pate
00:35 - streaming let's see the first 10 seconds
00:37 - of the chunk is present in this file so
00:40 - these are the things that we have coded
00:42 - I coded YouTube in 6 hours and it is not
00:45 - just another YouTube clone we wrote
00:47 - three services upload service watch
00:49 - service and a transcod service we also
00:51 - added a popup kka and we also added DP
00:54 - post sequel now before I get into the
00:57 - details before I do a code walk through
00:59 - and tell you everything let me first set
01:00 - some context and tell you why and when
01:02 - did I write this code and how you can
01:04 - benefit from this video by let me Begin
01:08 - by reminding you that just clone
01:10 - projects or just simple web dev projects
01:12 - don't work anymore they are not
01:13 - impressive anymore because the
01:15 - competition has increased so much you're
01:17 - expected to also talk about system
01:19 - design so this is a project where we
01:21 - will be doing high level system design
01:22 - along with the code and the code that we
01:25 - have written we have even done
01:26 - transcoding by ourselves and this code
01:29 - is not easily available online this is
01:31 - something that you know takes a lot of
01:33 - effort to create this is truly quality
01:35 - content I can vouch for that and I take
01:37 - a lot of Bride in that so this is
01:39 - something that you won't find easily and
01:41 - this is a very different kind of project
01:43 - now why did I create it this project was
01:46 - part of my hhld course Hands-On high
01:47 - level system design course where we took
01:49 - up three projects one was WhatsApp
01:51 - second was YouTube and third was Zera so
01:53 - this is one of the projects that we did
01:55 - in hhld why am I saying I took exactly 6
01:58 - hours because all the Cod that you're
02:00 - going to see every single line has been
02:02 - written during the classes and YouTube
02:04 - Project lasted for 2 weeks which was 6
02:06 - hours of classes and every single thing
02:09 - from creating files creating projects
02:12 - even setting up environment variables
02:14 - setting up o Kafka everything has been
02:16 - done within the class even all the
02:19 - explanation was done within the class so
02:21 - when I say Kafka I also explained what
02:24 - is Kafka you know how can you picture it
02:26 - why do we need it oo what is oo all the
02:30 - setup all the theory detailing
02:32 - everything has happened during these 6
02:33 - hours so if you think about it this code
02:36 - has actually been written in less than 6
02:38 - hours maybe 3 hours you can say half of
02:40 - it because I've explained each line of
02:43 - the code at least twice to my students
02:45 - now the question might be that why am I
02:46 - making all of this available for free
02:48 - see this is a project which is extremely
02:51 - great okay this is awesome my students
02:53 - take a lot of pride in being part of edu
02:55 - courses I know that the kind of guidance
02:57 - the kind of motivation that I provide
02:59 - cannot be provided provided outside I
03:01 - know that we have all pushed each other
03:02 - to write better code we support each
03:04 - other we solve each other's doubts and
03:06 - all of that is there but still if you
03:08 - cannot sign up for hld or if you do not
03:10 - want to do it I still want to tell you
03:12 - that how you can you know start thinking
03:14 - about such a project that if you take up
03:17 - making your own YouTube how to go step
03:20 - by step what are the instructions and
03:21 - all how you can go through it and I will
03:23 - also give you the code it will be
03:25 - available for free so you can access it
03:27 - in very simple terms if you are someone
03:29 - who wants to build a project you will be
03:31 - able to because this video will be
03:33 - enough for step-by-step guidance and it
03:34 - will also give you the code but if you
03:37 - are looking for more structured more
03:39 - detailed guidance then you can still
03:42 - enroll for H the recordings are
03:43 - available and the course is still going
03:45 - on we are going to be working on zeroda
03:47 - so you can still enroll for it and
03:49 - recordings are available you will also
03:50 - get access to the next patches so all of
03:52 - that is there the details are there on
03:54 - the site you can check it out actually
03:56 - if you're a complete beginner you can
03:57 - still sign up because we have covered
03:59 - end to end is starting from you know in
04:02 - the first few classes I even explained
04:04 - what is node what is the difference
04:05 - between uh nextjs and reactjs what is
04:09 - nextjs uh how to install packages every
04:12 - single detail that you can think of you
04:14 - started with AWS what is VPC what is
04:16 - subnet fast forward we have done so much
04:19 - load balancers we have used so many
04:21 - services now so all of that was part of
04:23 - the course but okay that was like a
04:25 - quick recap that if you want to sign up
04:27 - for the course you still can but now
04:29 - let's finally get started and I will
04:31 - tell you how you can approach the entire
04:33 - project step by step let's get started
04:36 - since a lot of my students are beginners
04:38 - and a lot of you might also be beginners
04:40 - so what I've done is instead of starting
04:42 - from hld you also can do this start from
04:45 - the core features what are the core
04:47 - features upload watch and transcode
04:49 - right so just focus on these core
04:51 - features and understanding everything
04:53 - step by step and later we'll Stitch
04:55 - everything together so initially we'll
04:57 - focus on how you can upload everything
05:00 - then later we will you know level up and
05:02 - then we will do chunking and upload and
05:04 - after that we will level up and do uh
05:06 - adaptive bitrate streaming while
05:08 - watching and encoding while uploading so
05:10 - in the starting we'll keep things very
05:12 - simple but most of the code is going to
05:14 - change later so right now if you see the
05:16 - code that I am showing on the screen I
05:18 - am showing Google doc why am i showing
05:20 - Google doc because most of the code that
05:21 - I going to show in the starting that is
05:23 - going to change but if you want to see
05:25 - code also you can see there are four
05:27 - main things that we have written three
05:28 - services so this is the the back end all
05:30 - of this has been written in nodejs and
05:32 - client is written in nextjs so let's see
05:34 - how can we get started the first thing
05:36 - that you can see is how to play videos
05:38 - on client so for that there is a very
05:40 - famous package that you can use on react
05:42 - on next year which is called react
05:44 - player so you can use this and here you
05:47 - can see we are not just playing videos
05:49 - on client but we did three things
05:51 - actually so when you want to play video
05:53 - first thing you can play any YouTube
05:55 - video so I said you know take any
05:58 - YouTube URL and play play that using
06:00 - react play the second thing that we did
06:02 - was you should be able to stream your
06:04 - video audio so streaming something like
06:07 - Zoom so that is why first page that we
06:09 - created I called it room. jsx because
06:12 - that gave like a zoom feel you click on
06:14 - a button and you can start streaming
06:16 - yourself both video and audio you can
06:18 - ask for permission start streaming
06:20 - yourself third thing that we did was
06:22 - that create one S3 bucket so here you
06:25 - should understand what is a bucket I
06:27 - explained to my students what is a
06:28 - bucket how can you create a bucket how
06:30 - to add a video just upload manually and
06:33 - from there you will get one URL you can
06:35 - keep the bucket public for now just to
06:37 - see that you able to play using react
06:39 - player and take this URL and using this
06:41 - S3 URL you should be able to play the
06:43 - video so this was the first step that we
06:45 - did and if you want to see the code let
06:47 - me show you from here so this is the
06:49 - first step this is the package react
06:51 - player that we use and you can see I
06:53 - have added three main things right so
06:55 - this react player here I have given one
06:57 - YouTube url here I have given One S 3
06:59 - URL so I uploaded the day n recording of
07:02 - my hhld class and I showed them that you
07:04 - can play like this adding on the S3 same
07:07 - you can also do right and the third
07:09 - thing you can add a button and as soon
07:11 - as you click on the button it I've named
07:13 - it call user to give you the feel like
07:15 - of Zoom something like that and here
07:17 - what you can do is you can the URL is
07:19 - the user stream and how will you get the
07:21 - user stream just like this you can turn
07:23 - on your video audio it will ask you for
07:25 - permissions and you will be able to play
07:26 - it so the first step is completely focus
07:29 - on the client side so that you get the
07:30 - confidence that you can play the videos
07:32 - on the client so let me just write this
07:34 - down so the first step was on the client
07:37 - side that you are able to play the
07:39 - videos so this is client so we are done
07:42 - with step one after that what we did was
07:44 - that we created one upload service so we
07:47 - created so this client is on next J we
07:50 - created one upload
07:53 - service and this is on
07:57 - nodejs and here what we did did was we
08:00 - created one API slash
08:03 - upload and the main goal of this API was
08:06 - to upload some video or some file to S3
08:10 - so to keep things very simple first
08:12 - thing that you can do is upload just PNG
08:14 - that's what we also read after that you
08:16 - can try uploading a small video like you
08:18 - know four 5 second video so you're going
08:21 - step by step you're leveling step by
08:22 - step right and both this PNG or video
08:26 - what you can do is you can hard code for
08:27 - now that just uh add some file in your
08:31 - back end and try uploading that right
08:33 - now so this is the first step right that
08:35 - create upload service upload media on S3
08:38 - and you can test this upload API using
08:40 - Postman so initially we tested using
08:43 - Postman so Postman is going to do/
08:46 - upload and it is going to call this API
08:48 - and whatever hardcoded file you have it
08:51 - should be able to upload on S3 and from
08:53 - here you will get a URL that you can
08:55 - play on your client also and you can sa
08:57 - then after that what we did was to level
08:59 - up now instead of from postmen so we'll
09:02 - just remove this instead of sending from
09:04 - Postman we will send the upload request
09:07 - from the client itself so on the front
09:09 - end right so you're stitching everything
09:11 - together now over here after sending the
09:14 - upload request now here also right now
09:16 - everything is hardcoded right so the
09:17 - next step that you can do is here you
09:19 - will add one input field for file and
09:23 - you will add this in the request in this
09:25 - upload request you will add the file in
09:27 - the upload request send to this backend
09:30 - service and this backend service will
09:32 - upload to S3 so one entire flow is going
09:34 - to be complete so you're going to select
09:36 - a file send it to upload service and
09:38 - that is going to upload it to S3 so
09:40 - let's look at the code now so this was
09:41 - the first step after that the Second
09:43 - Step was to create upload service so in
09:46 - our upload service you can see I have
09:48 - created one API upload I have created
09:51 - one route and the logic is going to be
09:53 - inside of the controller and this is the
09:55 - controller code why I am showing the
09:57 - code in the doc right now is because the
09:59 - code has changed a lot by the end of the
10:01 - project the link to the doc is again in
10:03 - the description you can check it out but
10:05 - to give you an idea see in our upload
10:06 - service there are controllers there are
10:08 - routes right so inside your route you
10:10 - can create one upload route and here you
10:12 - can create like right now there's upload
10:15 - to DB there is complete so in the
10:17 - starting we had one upload API so you
10:20 - can think of something like this right
10:22 - and the code was in the controller so in
10:24 - the controller in the end we'll be
10:26 - chunking and we'll be uploading that but
10:29 - your official code should be in
10:30 - controller okay let's see our controller
10:33 - code now to be able to connect to AWS we
10:35 - using the package AWS SDK and this is
10:37 - just to get the file so right now as I
10:39 - told you in the second step I am
10:41 - hardcoding the file on the back end so
10:42 - this is my hardcoded file and here first
10:45 - I'm just you know uh connecting to my
10:47 - AWS I'm giving the bucket details the
10:49 - file name key uh the access ID all of
10:52 - this just just configuring AWS and this
10:55 - is my main code so here I am uploading
10:59 - and what I'm doing is that if there is
11:01 - any error I'm just logging it and
11:03 - sending the response of 404 and if it is
11:05 - successful just giving the success
11:07 - response and logging in that is it and
11:09 - in the next step what we are going to do
11:10 - is we are going to an the UI because
11:12 - right now the back end is hardcoded now
11:14 - we are going to add the UI so here on my
11:17 - main page I've added one component
11:19 - upload form and if you go inside this
11:21 - component you are you're going to select
11:22 - a file and then send it right so for
11:24 - that there is one input field where you
11:26 - are taking the input of file and here we
11:28 - are handling the file change what are we
11:30 - doing inside that we are just setting
11:32 - the file and when you want to handle the
11:34 - submit you can handle file upload what
11:36 - we are doing inside this is so our
11:38 - backend server our upload service is
11:39 - running at 80 Port so you can just call
11:42 - SL upload and here we are adding our
11:45 - file in form data we are appending it in
11:47 - for file data you can also inspect and
11:50 - see that file will be going in your
11:51 - network I'll be demoing everything in
11:53 - the end now that in this point what have
11:55 - we done we have selected the file from
11:58 - our front end UI or from our client and
12:00 - send it to the upload request on the
12:02 - back end we need to take this file out
12:04 - from our request and we need to send it
12:06 - to S3 because right now on the back end
12:07 - everything is hardcoded right so that is
12:09 - what we are going to do we are going to
12:10 - extract file from the request and
12:12 - service and upload that to S3 so for
12:14 - that we are using mtter or molter how do
12:16 - you pronounce it so again we are using
12:17 - AWS SDK and in before our controller
12:21 - there's a middle we that we have added
12:22 - because there's a single file that is
12:24 - going to be there so this part is same
12:26 - but instead of the hardcoded file we are
12:28 - getting the file from request request.
12:30 - file and again we are just configuring
12:33 - AWS so the same code has now changed
12:35 - right here now what we are doing we are
12:37 - taking out the file from the request and
12:39 - in the upload same thing is happening
12:41 - that we are going to try uploading this
12:42 - and if it is not successful okay and if
12:44 - it is successful good so now that one
12:46 - flow is complete that you are uploading
12:48 - from front end to your back end to your
12:51 - S3 and you're able to play at S3 file on
12:54 - your front end I think you should feel a
12:56 - bit more confident I saw this confidence
12:58 - in my students so what we did was after
13:00 - that I introduced oo so we'll be doing
13:03 - oo in this project actually I had
13:05 - already done o using JWT in my WhatsApp
13:08 - project and we had discussed JWT a lot
13:10 - in detail I'll be creating another video
13:12 - on that as well so you can refer to that
13:13 - video I'll add the link to in the
13:15 - description in this project I have
13:17 - focused on oo which is uh like sign in
13:20 - using Google and later we also compared
13:22 - a bit you know what is the difference
13:23 - between o and SSO uh you can read about
13:26 - it but here what we will be doing is
13:28 - we'll be focusing on sign in with Google
13:30 - what you can do is use next to so if
13:33 - you're doing this the main thing that
13:35 - you need to understand is that this is
13:37 - happening on next J server now this is
13:39 - the main difference between next and
13:40 - react that in react everything is to
13:43 - happen on the client side but in next
13:44 - there is also something called server
13:46 - side and that is why on in a lot of
13:48 - places you will see that on the top we
13:49 - write use client right so next Au is
13:52 - something that we have done on next is
13:53 - but server side instead of writing a
13:55 - completely different service for it
13:57 - because I wanted you all to understand
13:59 - understand that you know you can also
14:00 - work on nextjs server that is why you
14:03 - can write full stack full projects on
14:05 - nextjs itself the front end as well and
14:07 - the pack and it also so here we are
14:09 - using nextjs server here you will have
14:11 - to sign up on Google Cloud console
14:12 - you'll have to create your project and
14:14 - set everything up so once you do that
14:16 - all the steps are return return on this
14:17 - you can refer to it you can add your
14:19 - Google provider and using next to you
14:22 - can uh do sign up so here you can see
14:24 - what you can do is that we have added
14:26 - two buttons one button is for sign sign
14:29 - in and one button is for sign out and
14:32 - here's a simple signin sign out because
14:34 - we using next O next o is amazing guys
14:36 - you should definitely try using it it
14:38 - makes things so much easier and you can
14:40 - get the data from use session and this
14:42 - data actually later in the project you
14:44 - will see that we from this data we took
14:46 - out like username and the image and we
14:48 - displayed that as well and we made sure
14:50 - that only those who are signed in are
14:52 - able to upload the videos so this part
14:55 - is very interesting just read about next
14:56 - Au and try implementing it and and over
14:59 - here so yes session provider and all of
15:01 - this is done so this is what we did in O
15:04 - and after this we discussed a bit of
15:05 - theory like SSO I hope you understand
15:08 - all of that but now let's get to a very
15:11 - very interesting part which is Kafka now
15:13 - that we are done with one flow so we did
15:15 - from front end to back end to S3 and we
15:17 - also talked about o right I think it is
15:20 - time that we start talking about hld so
15:22 - that is what we did so here the first
15:24 - thing that you need to understand in hld
15:26 - is that uploading is not straightforward
15:27 - there are more things that are involved
15:29 - because when you are uploading the video
15:30 - first thing that you need to do is
15:32 - content filtering you need to make sure
15:34 - there's no hate speech like nudity and
15:36 - all of that secondly you need to take
15:38 - care of copyright issues so you need to
15:39 - do all the checks second the third thing
15:42 - that you need to do is transcoding so
15:44 - while playing the YouTube videos you
15:46 - must have noticed there are different
15:47 - resolutions 1080p 720p 480p so while
15:50 - uploading itself you have to transcod
15:52 - the video in different formats and keep
15:54 - it so because there are multiple things
15:56 - that need to be done while uploading
15:58 - itself we need a pubsub why a pubsub so
16:01 - there will be one service that will be
16:03 - responsible for adding to our pbub in
16:07 - this case we'll be using Kafka in our
16:09 - project and from here different
16:11 - different Services can pick up the same
16:14 - message and use it so here Suppose there
16:18 - is one service for transcoding so it can
16:20 - consume the message and it can transcod
16:22 - the video so this is the next thing that
16:24 - you can do that we also did that we
16:26 - implemented Kafka we understood Kafka in
16:28 - details how how it work and all of this
16:30 - so you can also read about it and just
16:32 - do one basic check just to get started
16:35 - that push or publish one message and
16:38 - that message should be consumed by
16:40 - another service which is transcoding so
16:42 - till now we had only one service on the
16:44 - back end which was uploader service now
16:46 - we will create one more service which is
16:48 - basically a new node project which will
16:50 - be transcoder project and there we'll
16:53 - just do this ke yes it is consuming the
16:55 - message the entire code of transcoding
16:57 - and all that is going to come a lot
16:59 - later for now you just need to be able
17:00 - to see that okay Kafka is working and
17:03 - you're publishing to Kafka you're able
17:04 - to consume from Kafka coming to Kafka so
17:07 - I have added a bit of theory because we
17:09 - had like a bit of theoretical class
17:11 - where I explain what is producer
17:12 - consumer broker and all this actually
17:14 - there's one video that I'm creating on
17:16 - Kafka a crash course sort of thing so I
17:19 - will add the link to that also in the
17:20 - description so you can understand Kafka
17:22 - from there if you're a complete bner so
17:24 - as you can see there will be two
17:25 - Services upload service we had already
17:27 - written and that will act as a producer
17:30 - and there will be one more service that
17:32 - we'll be writing which will be the
17:33 - consumer the transporter service and
17:35 - here there are lot of online free
17:37 - solutions for Kafka one is also Cloud
17:39 - kfka that I used in demos for hld batch
17:43 - for hhld I've used iin for all the demos
17:45 - so you can create Services there are a
17:47 - lot of free services over here this is
17:49 - no way any promotion I just found it
17:51 - good so I'm just using it as you can see
17:53 - I have set up one Kafka and you can see
17:55 - all the configurations you can set it up
17:58 - in your uh project and here you can add
18:01 - topics you can create topics so here you
18:03 - can see there's one topic which is
18:05 - transcode so we'll be producing to
18:06 - transport and we'll be listening from
18:08 - here so in both our services upload
18:10 - service and transcor service we have
18:11 - added one folder called Kafka and this
18:14 - code is common because we're just
18:16 - configuring Kafka here we are creating
18:18 - one class and then we are adding Brokers
18:20 - and then we are setting up SSL password
18:22 - admin all of that and then we have
18:24 - written the code for produce and consume
18:26 - now although the code is common in
18:28 - upload service this is going to be used
18:31 - and in transcoder service so if you see
18:33 - over here in this one only consume will
18:35 - be used but I've added in both just to
18:38 - show you and here if you see where am I
18:40 - calling so in transcoder service in my
18:43 - index.js and here if you want to see
18:45 - where are these called suppose let's see
18:47 - in transcoder service first so if you go
18:49 - to
18:50 - index.js here I have added the config
18:52 - and here I'm consuming I'm consuming
18:54 - what I'm consuming transcode and here
18:55 - I'm just logging ke I've gotten data
18:57 - from Kafka that is it this consume is
18:59 - called in the transcoder service where
19:01 - is the publish being called in our
19:03 - upload service so this should be called
19:06 - right produce since we have to upload in
19:08 - the upload service let's see the code
19:10 - for that also so in the index.js you'll
19:13 - be able to see that I had added one more
19:17 - route itself publish and this is the
19:20 - router that I am using and this is the
19:22 - actual code so this is this will be
19:25 - there in the controller so over here
19:27 - send message to Kafka and and here what
19:29 - are we doing we are just producing the
19:31 - message transcod so I will run it and
19:33 - show in the end itself everything so
19:36 - here I am pushing to kafa from the
19:38 - uploader service and what is consuming
19:40 - the transcoder service is consuming and
19:42 - I've also added all the steps in the doc
19:44 - so how to set up Kafka overview all of
19:47 - this set up like you need a certificate
19:49 - and all of this and you can create a
19:51 - topic on I and after that how to
19:53 - configure produce so the ca. JS is going
19:56 - to be there in both the services and
19:58 - this is the publisher code as we had
20:01 - just seen and this is the consumer code
20:04 - on the transcod service that's it when
20:06 - we did the first FL from client to
20:08 - upload to S3 we either did for a PNG or
20:12 - for a small video now that you have
20:15 - understood o and Kafka the next thing
20:17 - that you should be asking yourself is
20:19 - that what is the difference between a
20:21 - you know simple PNG or a small video and
20:24 - what are the problems that will happen
20:25 - when there's a huge video so if there's
20:27 - a huge video it is going to take a lot
20:29 - of space and sending it over network is
20:31 - not going to be possible in one go right
20:32 - suppose it's a 1 hour video what are you
20:34 - going to do so obviously we need to cut
20:37 - our video into different different
20:38 - chunks or parts so each chunk or part
20:43 - can be like of a few seconds say 4
20:45 - seconds 5 Seconds 10 seconds you can
20:47 - decide accordingly right but you know
20:50 - that you need to divide your video into
20:51 - chunks now the question is that where
20:53 - should the chunking happen so a lot of
20:56 - people get confused with this a lot of
20:57 - people say that we we should be chunking
20:59 - on the upload service site which is
21:01 - basically over here so then my next
21:03 - question is that you know if chunking is
21:04 - going to happen over here then how are
21:07 - you going to send the video from front
21:08 - end to your back end also right because
21:10 - if you're doing the chunking over here
21:12 - first thing that you need to do is send
21:14 - to be able to send the video from font
21:15 - end to back end how are you going to do
21:17 - that so the correct answer is that you
21:18 - should be chunking on the front end
21:20 - itself sending it to the back end and
21:22 - which is going to send it to the A3 now
21:24 - the final thing that we want to do is
21:25 - that once we send these chunks from our
21:27 - front end to back end to S3 finally on
21:31 - S3 what should happen is that all of
21:32 - these chunks should put should be put
21:35 - together should be assembled together to
21:37 - a single video right even though we are
21:40 - sending it in chunks the final thing
21:42 - that we want is a single video correct
21:45 - so this huge video how can we play also
21:47 - that we'll discuss later for now let's
21:49 - focus on this huge video how can we
21:51 - upload it so that is the current Focus
21:53 - so your next major agenda should be that
21:55 - how can you do chunking on the front end
21:58 - and then how how can you up send it from
22:00 - your upload service to S3 such that it
22:02 - is assembled back to a single video so
22:05 - this is the next thing that we should
22:06 - see here I have written different ways
22:08 - to upload data on S3 here you can see
22:10 - front end to back end to S3 without
22:12 - chunking so if you do without chunking
22:14 - it is going to be slow and not efficient
22:16 - if you do from front end to S3 without
22:18 - chunking processing like transcoding and
22:20 - all is not possible right so a lot of
22:21 - questions come that what if we remove
22:23 - the back end why do we need back end
22:25 - then how are you going to do all the
22:26 - processing So This Is The Answer front
22:28 - end to back into S3 with chunking this
22:30 - is what we'll be doing faster processing
22:32 - is possible retry resume abot all of
22:35 - this is possible right then there's also
22:37 - one more thing uploading using pre-sign
22:39 - URLs this we are not going to discuss
22:41 - right now because you need to understand
22:42 - pre-sign URLs in S3 for that if you
22:45 - understand that you would have
22:46 - understood this right so next agenda
22:48 - after CF card the first thing that we
22:50 - did was send video in chunks from client
22:52 - to server to S3 so right now what is
22:55 - going to happen is these chunks itself
22:57 - will be uploaded on on the S3 also the
22:59 - reassembling is not going to happen
23:01 - whatever chunking you're going to do in
23:03 - Parts those chunks will be sent to uh
23:06 - your upload service and the upload
23:07 - service will send as it is the chunks
23:09 - itself to S3 so this is what is going to
23:11 - happen right now so first thing that we
23:13 - need to do is to the chunking on the
23:14 - client side so this is how you can do so
23:17 - first thing that you need to see is that
23:19 - what is going to be a chunk size so here
23:21 - everything is in byes so this is going
23:23 - to convert into MBS and you can mention
23:25 - how big you want your each chunk to be
23:27 - suppose you want want your each chunk to
23:29 - be 100 MB so this is going to be a chunk
23:31 - size you can find the number of total
23:33 - chunks you can log it to be sure that
23:35 - how many chunks are there and all of
23:36 - this and after that we are going to do
23:38 - the chunking now chunking is actually
23:39 - very interesting this is where you need
23:41 - to understand your alos a bit so that
23:43 - you know you used to writing this code
23:45 - uh so your chunk index is start is going
23:47 - to start from zero to less than CH total
23:49 - chunks and you're going to slice your
23:52 - file you're going to slice it from start
23:54 - to start plus chunk size so your start
23:56 - is going to keep weying right so suppose
23:58 - it starts from 0er to 100 next time it
24:00 - is going to be 100 to 200 like 101 to
24:03 - 200 after that it is going to be from
24:05 - 2001 to 300 and so on and so forth and
24:08 - how we were sending the entire file
24:10 - earlier in our uh form data now what
24:12 - we'll be doing is we'll be sending the
24:14 - chunks so here I am uploading what I am
24:17 - sending the file name the chunk what are
24:19 - the total number of chunks and what is
24:21 - the index of this particular chunk so in
24:23 - this fall Loop what am I doing these are
24:26 - the number of requests so this is my
24:28 - Loop and inside the follow Loop I'm
24:31 - going to keep uploading the chunks so
24:33 - earlier I was sending one file now this
24:36 - same request is going to get called how
24:38 - many times the number of times as they
24:40 - chunks right I can also show you in the
24:43 - code so this is my client in upload
24:45 - there is one page. jsx obviously this
24:48 - code is like the final code but this is
24:50 - actually the same thing so if you want
24:52 - to see the for Loop you can understand
24:54 - from here we are slicing our file from
24:56 - start to start plus chunk side and then
24:58 - we are sending it in form data so this
25:01 - call is going to happen how many times
25:03 - how many times this fall Loop is going
25:04 - to run the number of iterations now that
25:07 - we have sliced our file into chunks on
25:09 - the client side our backend service
25:12 - should know our upload service should
25:13 - know that now I'm not getting one single
25:15 - file I'm going to keep getting chunks so
25:17 - that is the next thing that we'll be
25:18 - doing so this is just the div so this is
25:20 - where the front end ends now in the
25:23 - backend side so in our route instead of
25:25 - you remember this was this upload do
25:27 - single file now instead of single file
25:29 - there are going to be Fields there's
25:31 - chunk there's total chunks and there's
25:32 - chunk index and our upload file to S3 in
25:35 - the controller is going to uploaded to
25:36 - S3 so same thing but right now what has
25:39 - happened is that the chunks got uploaded
25:40 - to S3 now the thing is that what we use
25:43 - was s3. upload right if you remember in
25:45 - our upload form how were we uploading
25:47 - s3. upload so if you want to see the
25:49 - controller code let's go back to
25:52 - it so we were using AWS SDK and we were
25:56 - doing s3. upload right so it is going to
25:58 - upload one by one now what is the
26:00 - problem is that all of these chunks are
26:02 - going to get saved separately on S3 now
26:04 - AWS SDK gives us a very cool feature so
26:07 - instead of uploading these chunks one by
26:09 - one to to completely different files
26:11 - what we can do is instead of s3. upload
26:14 - we can use something called multi-art
26:16 - upload which means that we are going to
26:18 - send it in chunks and S3 is going to
26:21 - reassemble it
26:23 - together so whatever we did till now was
26:26 - what I had done in the week one of the
26:28 - project so our project was R into two
26:30 - weeks right it was a two week project in
26:31 - hhld so first week is what we did so far
26:35 - second week this was the agenda first
26:37 - agenda was multi-art upload from backend
26:40 - to S3 now what we have done currently is
26:43 - from frontend to back end everything is
26:45 - getting chunked and then the chunks we
26:47 - are uploading to S3 right now for now
26:50 - for the first part you can forget about
26:52 - the client just take any file on the
26:55 - back end slice it in the back end itself
26:57 - just for easy just to be able to
26:59 - understand slice it on the back end
27:01 - itself but what you want is that on the
27:03 - S3 side it should be put together and
27:06 - you should be able to play the file
27:08 - together so for that we are doing
27:10 - multi-art upload so if you go to the
27:13 - docs actually I've have added the link
27:14 - to the docs and the docs is just amazing
27:17 - I'll quickly summarize it for you but
27:18 - you can go through it yourself okay
27:20 - since you're making the project but the
27:23 - multi-art upload process is divided into
27:25 - three steps one is the upload initiation
27:27 - the second is the Parts upload and the
27:29 - third is the completion now initiation
27:32 - is when you're going to tell that you
27:33 - know I'm going to initiate a multi-art
27:35 - upload and then it is going to create a
27:37 - upload ID for you the S3 and it is going
27:39 - to give you back now after this whenever
27:42 - you're going to upload the parts or
27:44 - complete the upload in that you're
27:46 - supposed to send this upload ID so that
27:48 - S3 knows that okay all the parts all the
27:50 - chunks that I am getting or the
27:52 - completion of the upload request all of
27:54 - this is corresponding to this upload
27:55 - that you initiated so it is just going
27:57 - to generate an upload ID and give you so
27:59 - the upload ID is generated where in the
28:01 - initiation part after that there is one
28:03 - Parts upload here you're going to send
28:05 - all the parts and in the response of it
28:07 - you going to get an ntid tag and in the
28:11 - multi-art upload completion what we are
28:13 - going to do is Q are supposed to send
28:15 - all of this information that how many
28:17 - parts were there and with that whatever
28:19 - entity tag it had returned you that you
28:21 - need to respond back see here it is
28:24 - written that when you complete your
28:26 - multipart upload request you must
28:28 - include upload ID and the list of both
28:30 - part numbers and corresponding e tag so
28:32 - you're supposed to send two things part
28:34 - number and E tag values let me show it
28:36 - to you in terms of diagram also so it
28:37 - will be clearer so in simple terms
28:39 - multiart upload S3 expects three
28:42 - requests one is creation second is
28:45 - upload Parts where you're going to
28:46 - upload all your parts and the third is
28:48 - complete multi-art upload what S3 is
28:50 - going to do is it is going to put all of
28:53 - these parts together it is going to
28:54 - reassemble it together into a single
28:57 - video so let's see in the diagram so
28:59 - that it is further clear so what we were
29:01 - doing till now was s3. upload now we are
29:04 - going to try something called s3.
29:06 - multiart upload right so this is this
29:09 - provision is given by S3 itself so it is
29:11 - there in AWS SDK we are using the
29:13 - package right what S3 expects us that we
29:16 - will send three requests the first
29:18 - request is going to be initiation
29:20 - request that we are going to initiate
29:22 - the upload so in the request we will
29:24 - send that okay this is the file name
29:26 - this is the key and all of this and in
29:28 - the response we'll be getting what we'll
29:30 - be getting one upload ID and in the next
29:33 - both the requests we have to send this
29:35 - upload ID in the request so the next
29:37 - request is upload part so here we'll be
29:40 - sending a lot of parts so Suppose there
29:41 - are 100 parts or Suppose there are 200
29:43 - Parts how many hour thousand Parts
29:45 - you're going to upload that and in the
29:46 - request along with your part details
29:49 - like you know what is the part number
29:50 - you'll be giving your upload ID that
29:53 - this is my part number and this is my
29:56 - part ID and in the resp response
29:58 - corresponding to every part number
30:00 - you're going to get something called e
30:02 - tag which is entity tag so in the end
30:04 - what is going to happen is when you're
30:05 - going to send the request of complete
30:08 - upload what happens is you send an array
30:11 - and in the array you send what part
30:14 - number and E tag for every single part
30:17 - so like this you like Suppose there were
30:19 - thousand Parts you will send the array
30:21 - and you will obviously send the upload
30:22 - ID itself so this was going to be there
30:25 - in your request and finally in the
30:27 - respon resp of this complete upload
30:30 - you're going to get one final e tag
30:32 - which is going to be the tag of this
30:34 - entire upload right so earlier you were
30:36 - getting e tag for every response later
30:39 - you will get one final e tag also just
30:41 - for further Clarity every e tag is going
30:43 - to be different because it is for all
30:44 - the parts right and this e tag will also
30:46 - be different so we are going to be
30:48 - implementing the code for this so we
30:50 - need to write three API on our upload
30:52 - service we need to make the three API
30:54 - calls from upload service to S3 now to
30:56 - make things simple for multiart upload
30:59 - earlier what we had done was that we
31:00 - were doing chunking on the front end and
31:02 - we were sending it to the back end and
31:04 - we were sending the chunks itself to S3
31:07 - right so this was how we started so this
31:09 - was the step one now in step two right
31:11 - now we'll just forget this we'll forget
31:13 - it and what we are going to do is that
31:16 - we will do the chunking on the back end
31:18 - we are going to hardcode our file right
31:21 - now just to Able just to be able to test
31:23 - this multiart upload so just to test
31:25 - multiart upload we are going to hard
31:27 - code the file chunk it over here and
31:29 - send it to S3 what is expected is that
31:32 - S3 would have reassembled this video and
31:36 - created a single video so we want that
31:38 - whatever chunks we are sending from a
31:40 - back end upload service to S3 it would
31:42 - have reassembled it this we should be
31:44 - able to play on the client also after
31:46 - this what we will do is we will do the
31:48 - chunking on this these chunks we are
31:51 - going to upload to S3 using multi-art
31:54 - upload only and then this S3 would have
31:58 - reassembled it to a single video so we
32:01 - want to go steps by steps so for the
32:03 - first step what you can do is you can
32:05 - just hardcode it and send it and after
32:07 - that you can send the do the chunking on
32:09 - the front end which is the correct way
32:11 - and then send it so in order to do this
32:13 - what you can do is you can test using
32:15 - Postman so Step One is multiart upload
32:18 - from back into S3 using a fixed file so
32:21 - fixed file so what do I mean by fixed
32:23 - file we are fixing the file on the back
32:25 - end so we have given the file path and
32:28 - seeing if the file doesn't exist we are
32:29 - not doing anything so we are going to be
32:31 - testing this using Postman right uh so
32:34 - here this we have done earlier also we
32:36 - just configured AWS that's it and now
32:38 - you can see inside this try there are
32:40 - basically three requests so this is the
32:42 - first request where we are creating the
32:44 - multi-art upload so this is returning a
32:46 - promise and from this promise if you see
32:50 - we are getting what we are getting the
32:51 - upload ID so multiart parents. upload ID
32:54 - right so whatever we getting from the
32:56 - response of the first one the initiate
32:57 - ation one we are getting this and from
33:00 - this we're getting the upload ID just
33:01 - try it out yourself the documentation is
33:03 - very much clear you can try it out right
33:05 - and again same chunking we are doing so
33:07 - there's a fall Loop so inside this fall
33:09 - Loop we making another call which is
33:11 - upload part and here if you can see I am
33:14 - saying upload part and in the response
33:16 - in this data there is something coming e
33:19 - tag right so we have created an array so
33:21 - the array is empty so we are storing all
33:24 - the e tags or the uploaded Parts in this
33:26 - array so here you can see I pushing the
33:28 - part number and the E tag why because
33:30 - while completion over here I need to
33:33 - send it so here I can see that I am
33:35 - passing this pass so uploaded eags this
33:38 - array I am sending it in the end and in
33:41 - the response I'll get that okay I've
33:42 - uploaded it so this is how you can test
33:44 - multiart upload from back end to S3 but
33:46 - in reality what you should be doing is
33:48 - you should be doing the chunking on your
33:50 - client side so we'll do that and for
33:52 - this by the way for multiart upload you
33:54 - will change your router also and you can
33:56 - set your cost per permissions on bucket
33:58 - and all of this and after that what we
34:01 - need to do is we need to send the chunks
34:02 - from front end to back end in sequence
34:05 - and then multiart upload to S3 so front
34:08 - end is where it should happen and this
34:10 - code I can show you the code also
34:11 - because this is the final sort of code
34:14 - so this is the client side of code so in
34:15 - upload what we have done is in our
34:17 - handle upload if you see there are three
34:19 - parts right so from front end to back
34:21 - end also now there will be three
34:23 - requests instead of just one request so
34:25 - here when there's a button and when we
34:27 - click on upload load so in our handle
34:29 - upload what is going to happen is three
34:31 - requests are going to happen to backend
34:33 - so the first request is going to happen
34:35 - is that I have called as/ upload SL
34:37 - initialize so if you see on the packet
34:40 - there will be three routes let me show
34:41 - that to you see for initialize this one
34:43 - route so this is where we'll do the
34:46 - create multiart upload after that
34:48 - there's just one slash route which is
34:49 - like upload the chunk and here we are
34:51 - expecting chunk now initialize what is
34:53 - going to happen is we are going to
34:54 - expect the file name the title and all
34:56 - of that that is why something is going
34:58 - to come in the body but it is not a file
35:00 - so multer expects you to give upload.
35:02 - none and here there's going to be a
35:04 - chunk and here there's like the third
35:06 - request is complete right so on our
35:08 - client side what we have done is three
35:10 - requests so the first request is upload
35:12 - initialize and here what are we giving
35:14 - just the file name here whatever
35:16 - selected file is there in our input
35:18 - field that name we are giving and this
35:20 - is the first request that is going to
35:22 - happen and from here what are we going
35:23 - to get from the response we are going to
35:26 - get upload ID and once we have this
35:28 - upload ID we are going to use that in
35:29 - the next two request so this was the
35:31 - first part and after this in the second
35:33 - part is where we are going to do the
35:35 - chunking so here we are doing the
35:36 - chunking the code is same that we have
35:38 - seen earlier so this was the first call
35:40 - and in the second call is when we'll be
35:42 - doing the chunking so this code we had
35:44 - seen earlier also we were seeing earlier
35:46 - also that upload call is going to be
35:48 - done so many times so whatever is the
35:49 - selected file we are slicing it and in
35:51 - the form data we are adding everything
35:53 - here you can see that I am creating an
35:55 - array of upload promises and in in the
35:57 - end I'm just pushing all of them so in
35:59 - short we are just uploading the chunks
36:00 - over here now after that there is one
36:02 - more thing that is left which is
36:04 - completion so in the upload complete we
36:06 - are sending that okay this is the parts
36:08 - so these are the total chunks upload ID
36:11 - and all of this and if you want to see
36:13 - the other site the backend site so there
36:16 - are three requests and let's see the
36:17 - controller as well I created another
36:19 - controller so that you can compare that
36:21 - what was the first thing so earlier we
36:23 - were just doing s3. upload now we are
36:25 - doing multi-art upload so now in
36:27 - multiart upload there are three
36:29 - functions you can obviously move them to
36:30 - three different files but for now I've
36:32 - just kept three in the same file so
36:34 - there is initialize upload in this we
36:36 - are configuring all of this and calling
36:38 - create multiart upload there's another
36:41 - one and by the way for each one we'll be
36:43 - sending the response now right because
36:45 - three different requests from front end
36:46 - to back end and the second one we are
36:48 - uploading the chunk so here we are going
36:50 - to send the upload ID the part number
36:52 - and all of this and in the third one we
36:55 - are going to be uh sending the complete
36:58 - upload request so this is how you can do
37:00 - multiart upload so we are done with the
37:02 - first two steps we did multiart upload
37:04 - from back end to S3 you like earlier the
37:06 - file was fixed after that we send the
37:08 - chunks from front end to backend in
37:10 - sequence and then we did the multiart
37:11 - upload to S3 now what we are going to do
37:14 - is send chunks from front end to back
37:15 - end in parallel so what do I mean by
37:17 - that so if you notice the difference let
37:20 - me just tell you something so in the
37:23 - client code over here in the for Loop if
37:26 - I keep keep adding await okay so await
37:29 - means what it is going to wait for this
37:31 - to happen so while one chunk is getting
37:34 - uploaded this is in the second request
37:36 - by the way when we are uploading the
37:37 - paths right so from our front end to
37:39 - back end what is going to happen is
37:41 - because we have added a weight in our
37:43 - for Loop it is going to wait for one
37:46 - part to upload and it is going to happen
37:47 - one by one versus what I can do is
37:51 - create an array of promises and here
37:53 - instead of adding a weight what I'm
37:55 - doing is I'm just keeping the promise
37:57 - prises over here and after the for Loop
38:00 - is over after this for Loop is when I am
38:03 - saying that await all the promises
38:05 - together so what is going to happen is
38:07 - from front end to back end instead of
38:09 - sending it one by one you can send them
38:11 - in parallel now the question that can be
38:13 - asked is that what if you know you are
38:15 - putting a too much load on back end now
38:18 - for that you can also do load balancing
38:20 - because there is upload ID right so from
38:22 - that itself is enough for you to
38:25 - understand that this is part of which
38:26 - upload so if there are way too many
38:28 - upload paths you can still handle that
38:30 - but this will definitely make things
38:32 - efficient so if you notice over here
38:34 - sending chunks from front end to back
38:35 - end in parallel create upload promises
38:38 - array and over here you can keep pushing
38:40 - them and you can await all of them
38:42 - together now that the first three points
38:43 - are over let's do a quick revision so
38:45 - you did chunking on your client then you
38:47 - sent your chunks to your packet and then
38:49 - you did multiart upload to S3 because
38:52 - the multiart upload expects three apis
38:54 - you created three apis from front end to
38:56 - back end also and from back end to S3
38:58 - also so there were three calls from
39:00 - front end to backend and from back end
39:02 - to S3 right initiation upload parts and
39:04 - then complete upload now that upload
39:06 - flow is clear it's time that we start
39:08 - talking about wat service so here we are
39:11 - creating the watch service and also we
39:13 - talked a bit about you know that S3
39:15 - right now whatever we have been using is
39:17 - public you can make it private and you
39:19 - can access using signed URL so you can
39:22 - play around with that you can get the
39:23 - signed URL so this is just something
39:26 - that I wanted to show you can skip it if
39:28 - you know about sign URLs already the
39:30 - main point is that create one wat
39:32 - service because from watch service also
39:34 - what we going to do now is that we are
39:35 - going to attach a DB and we are going to
39:38 - add all the video details in that DB and
39:40 - from the watch service you're going to
39:41 - watch that let me explain using the
39:43 - diagram so what we have done right now
39:45 - is that on our front end we are doing
39:47 - the chunking we are sending it to our
39:49 - upload service and then we are sending
39:52 - that to S3 using multi-art upload now
39:56 - first thing that we did was created one
39:58 - more service which is what
40:01 - service okay then we created one more
40:06 - database which is postgress SQL and here
40:09 - we are going to be using om Prisma and
40:12 - we are going to do what when we do
40:14 - upload we are not just going to upload
40:16 - the video on S3 but after completing the
40:19 - upload basically in the third request
40:20 - after completing the upload in our postp
40:23 - SQL we are going to be adding some
40:26 - metadata like like for example our title
40:30 - our
40:31 - description our author name and because
40:34 - we have finished the upload we would
40:36 - also have what we would also have S3 URL
40:38 - in this right after completing the
40:40 - upload so we can add the S3 URL as well
40:44 - so what what are we going to do we are
40:46 - going to first do the upload to S3 and
40:49 - after the completion we are going to get
40:50 - the URL after that we are going to call
40:53 - this postgress SQL and we are going to
40:55 - add the metadata title description on
40:57 - the S3 URL so we can send the S3 URL to
41:00 - the post SQL and then what we are going
41:02 - to do is we are going to add a route in
41:04 - what service to list all the videos so
41:06 - right now we going to be using the same
41:08 - DB itself so we are going to do
41:10 - something like get all
41:13 - videos and our client will be able to
41:16 - like from our home so basically YouTube
41:18 - home is going to give all the videos
41:21 - right so there will be like a list of
41:22 - videos on our client so this is the next
41:26 - thing that we are going to do
41:28 - and let me tell you a step further also
41:30 - because I can walk through the code
41:31 - together once you can watch all the
41:33 - videos what you can do is on this screen
41:35 - itself you can add a button for upload
41:39 - right but who can upload only signed in
41:41 - users so we have we have already done
41:43 - off right so we'll just integrate it
41:45 - together and what we'll do is only those
41:47 - who have signed in can upload and those
41:50 - who have not signed in cannot upload so
41:52 - they will be either sign in button like
41:55 - sign in button or they will be be like
41:58 - sign out and upload
42:00 - button and you can also put hello name
42:03 - and image and all of that if the person
42:04 - is signed in so this this is the next
42:07 - agenda as I explained after getting the
42:09 - list of the videos what you can do is
42:10 - add the front end code firstly to list
42:12 - all the videos and to play the videos
42:14 - and all of that and only authorized
42:16 - users should be able to upload so let's
42:18 - see the code for all of this and I
42:19 - promise I'm going to show you the demo
42:21 - it is going to be awesome to create
42:22 - video metadata DB again we are using
42:24 - post again we are using the fre solution
42:27 - from Ian itself so you can set it up uh
42:30 - you can set up your database so here I
42:33 - had set one hhld class during the class
42:35 - and we were using that and interestingly
42:38 - YouTube actually uses vus which is not
42:41 - free otherwise we would have used that
42:43 - so you can read about vtis vtis is
42:45 - actually great for horizontal scaling
42:46 - even though it is a relational database
42:49 - and so I have listed out the differences
42:51 - between V and post SQL and we'll be
42:53 - using Prisma for om which makes things
42:56 - very much easier so instead of writing
42:58 - queries queries you can treat that like
43:01 - your code itself so like how you create
43:04 - table you basically create an object
43:06 - something like that while creating the
43:08 - entry so the documentation of Prisma is
43:10 - just awesome it is very detailed so all
43:12 - the steps that I added over here are
43:14 - taken from the documentation itself so
43:15 - you can go through that in detail and we
43:17 - are creating the metadata and stuff
43:19 - coming to the code within upload service
43:20 - itself if you see I've created another
43:22 - folder DB within that there's db. JS
43:24 - just name simple itself and here we are
43:27 - doing add video details to DB where are
43:29 - we doing all of this so when we do
43:31 - Prisma in it you can see the commands
43:33 - are written in the doc and it is also
43:34 - there in the Prisma documentation also
43:36 - there's one file that is created which
43:38 - is schema. Prisma and here it creates
43:40 - all of this for yourself only you just
43:42 - have to create a model so here we have
43:43 - created the model so ID is an integer
43:45 - and we are Auto incrementing it there's
43:47 - title description author and all of this
43:49 - and the migrations are created by itself
43:51 - so uh and if you see in EN EnV there's
43:54 - database URL that is added and how have
43:56 - I said this up from my Ian right so I
43:59 - have created a model over here and over
44:01 - here what I'm doing is Prisma Dov dat.
44:04 - create so you can see creation becomes
44:06 - so easy it is like code only that you
44:08 - are writing so you'll be adding data
44:10 - title description author URL and so on
44:12 - and soort if you want to see your data
44:15 - in UI version so you can I have
44:16 - installed something called PG admin
44:18 - again something I found for free so go
44:20 - to servers here you can see hld demo and
44:22 - then here you can see uh table video
44:25 - data right so here we had created datab
44:27 - is called hhld class and if you go over
44:29 - here to video data here you will be able
44:31 - to see the columns and all uh let me
44:34 - actually just go to the query tool and
44:37 - let me write a query select all from
44:40 - video data so this is whatever data
44:43 - you're going to see we have created
44:44 - everything during the class itself and
44:47 - whatever data like during the upload
44:49 - watching I have created everything
44:50 - during the class so yep so these are the
44:54 - random names that I added and you can
44:55 - see the URL so when we upload we got the
44:57 - URL and then we uploaded that over here
45:00 - and I will show it to you running in the
45:01 - end but you can see the database like
45:03 - this so this is what we just saw you can
45:05 - add model in Prisma schema and db. JS we
45:09 - just saw this right we can add video
45:11 - details to the DB you can create right
45:13 - and you can test using Postman how you
45:15 - can test using Postman so for this what
45:17 - you can do is so if you come over here
45:20 - in upload controllers here we had seen
45:23 - three functions right initialize upload
45:25 - upload chunk and complete upload so just
45:27 - to test that uploading to DB is working
45:29 - fine I've created another route and here
45:31 - I am testing upload to DB but initially
45:34 - you can test via Postman and in the
45:37 - request you can send title description
45:40 - and author right and uh URL as well so
45:43 - let me show you from Postman also how
45:45 - your request can look like so whenever I
45:47 - create apis my usual practic is that
45:49 - first I'm going to test from Postman
45:50 - make sure that everything is working
45:52 - fine on the back end and then add on the
45:53 - front end so this is what I have done
45:55 - I've just tested over here upload to
45:57 - title description author URL so that is
45:59 - why I'm getting all of this in the
46:01 - request and then I'm adding the video
46:02 - details to DB so you can do the same you
46:04 - can test that everything is working fine
46:06 - on the back and you're able to upload
46:08 - and after this what we can do is we can
46:09 - come back and write the front end code
46:11 - so I've also mentioned the postman body
46:14 - and the router that you can add and over
46:16 - here what we are going to do we are
46:17 - going to send the video details from the
46:19 - front end during completion of upload
46:20 - and add it to the DB so on the front end
46:23 - itself if you remember there were three
46:24 - calls that were happening right so if
46:26 - you go back to the client
46:27 - and source and upload C page. jsx so if
46:31 - you remember there were three things
46:32 - that were happening first was the
46:34 - initiation second was the uploading of
46:35 - the parts and then third was the
46:37 - completion now in the completion if you
46:39 - see these things I have added now so
46:41 - this was not needed like before now we
46:44 - we are going to be adding title
46:45 - description and author and now what I've
46:47 - done is that my in my form I have added
46:49 - more input Fields not just file but
46:51 - there's also an input field for author
46:53 - for description and for title so this is
46:55 - how you can you know keep improving your
46:57 - project step by step you can keep adding
46:59 - the things so here we have added title
47:01 - and then we have also put a check that
47:03 - title and author cannot be empty they
47:05 - are required and then after that in the
47:07 - completion you can add title description
47:09 - and author earlier there was just F name
47:11 - total chunks and upload ID and later we
47:13 - have added all of this and you can just
47:15 - add the input felds I can actually I
47:17 - think we are ready to see the upload
47:19 - flow at least from the front end to the
47:21 - uh back end let me finally show you how
47:23 - it is working so in order to run I have
47:25 - created two terminals one for client so
47:27 - I can run this using npm run Dev so it
47:30 - is running at 3,000 and upload service
47:33 - I'm going to run using node moreon and
47:36 - Dev start so this is running at
47:39 - 8080 yep at 8080 so now if we go into
47:43 - complete upload so finally in the end
47:46 - what is going to happen is that we are
47:47 - also going to be uploading to Kafka
47:49 - right so right now I'm comenting this
47:51 - because we have not discussed this so
47:52 - far we will be adding video details to
47:54 - DB and we'll be uploading right so we
47:56 - should be able to see it in the DB
47:57 - though so if you want to see what all
47:59 - things are there in the DB if I just run
48:01 - this again so you can see there are
48:03 - seven entries in the DB and if you want
48:05 - to see S3 this is the hhld classes and
48:08 - if I refresh this there's nothing I've
48:10 - deleted everything right so we are going
48:12 - to try uploading but before we go ahead
48:14 - and see the working code there's one
48:15 - thing that I want to talk about so here
48:17 - since we are doing multiple things add
48:19 - video details to DB push the details to
48:21 - Kafka and all of this and we are also
48:23 - completing uh the upload so there are
48:25 - three things that are happening right
48:26 - completion adding video details to DB
48:28 - and putting to Kafka which we will do
48:30 - but okay three things are going to
48:31 - happen now you should be asking
48:34 - questions like key what if one of them
48:36 - fails how are you going to handle it now
48:38 - we have not gone into that level of
48:39 - detailing in this project but that is
48:41 - something that you should be talking
48:42 - about for now you can just put
48:44 - everything into error you can catch the
48:45 - error but ideally what you should be
48:47 - doing is if addition to the video DB
48:49 - failed then you should be talking about
48:51 - what is going to happen on the video
48:52 - that is there on the S3 are you going to
48:54 - show a popup to the client and ask the
48:56 - client to add the video details again or
48:58 - what is going to happen how are you
48:59 - going to handle all of this so these
49:01 - questions should be coming because we're
49:02 - talking about H but because right now we
49:04 - are keeping things simple now I'm going
49:06 - to go to Local Host
49:09 - 3000 and this is how the UI looks title
49:11 - description author I've have kept things
49:13 - very very simple right now and if you
49:15 - want to see the network tab let's do
49:16 - that so let's go to the network So
49:18 - currently there's no call there's
49:20 - nothing on the console right uh suppose
49:22 - I want to just add uh YouTube hhld and
49:26 - description is like with suppose kti
49:30 - pwani YouTube
49:33 - channel and suppose author is kti itself
49:37 - and we are going to choose a file for
49:39 - upload and let's say it is the day 15
49:41 - recording of my uh hld classes and I'm
49:45 - going to try uploading this okay and
49:47 - let's see the network is going to be
49:48 - interesting so did you see this happened
49:51 - in parallel so first request that went
49:52 - was initialized and you can see all of
49:54 - this happened for parallel some is
49:56 - happening already and some is taking
49:58 - some time right the upload this is
50:00 - basically upload parts that is happening
50:01 - and why is it happening in parel because
50:03 - promises we had not awaited we had just
50:06 - created one array and we were awaiting
50:07 - it together so that is why the upload
50:09 - from front end to back end is happening
50:12 - in parallel right so there are so many
50:13 - chunks in this file this pretty big file
50:15 - it's a two and a half or 3 hour class
50:17 - right so after two hours we do chitchat
50:19 - and all of this so it might be like 3
50:21 - hours of Zoom recording so if you see
50:23 - the initialized one here we are just
50:24 - sending the file name and we are sending
50:26 - it form data if you remember and in the
50:28 - headers you can see everything so you
50:30 - can see upload Parts is still happening
50:32 - some chunks are uploaded some chunks are
50:34 - still happening and you don't see any
50:36 - call after this right so after all the
50:38 - parts are uploaded there's going to be
50:39 - one more request what request is going
50:41 - to be it is going to be the complete
50:42 - request and if you want to see what is
50:44 - happening on the back end so you can see
50:46 - the back end terminal as well you can
50:48 - see that the data that we receiving back
50:50 - there's an e tag right so you can see
50:53 - there's an uploading chunk that is
50:54 - happening so we started from over here
50:58 - initializing upload was the first thing
51:00 - it got the file name from the request
51:02 - and then it we got what in the response
51:05 - we got the upload ID so in the
51:06 - corresponding request we would have sent
51:08 - this upload ID and you can see a lot of
51:10 - requests are coming in parallel why are
51:12 - they coming in P because we added it in
51:14 - front end now you can see the E tags are
51:16 - different for each chunk and it is
51:18 - happening so it is still happening it
51:20 - takes a lot of time since it's a big
51:22 - video so you can see right and if you
51:25 - want to see what is the chunk number
51:26 - also you'll be able to see if I see the
51:29 - uh request over here a few minutes later
51:32 - so you can see we are at the final few
51:34 - upload chunks so finally I guess three
51:37 - are left let's see and as soon as this
51:39 - happens there should be one more request
51:41 - that goes let's see let's wait for it a
51:44 - little longer than a few minutes later
51:47 - all right we waited for a couple of
51:48 - minutes let's see the console as well so
51:50 - you can see so many upload requests were
51:53 - called right and over here you can also
51:56 - see upload successfully message that
51:58 - came because there was the final request
52:00 - that went which was the complete request
52:03 - and if you want to see over here
52:04 - completing upload is also there right
52:06 - and in here you can see the final e tag
52:09 - that is there and the final URL and this
52:12 - URL should have been uploaded to the
52:14 - database and if you remember there was
52:17 - seven entries and the ID that got
52:18 - generated because we had added Auto
52:21 - increment in our schema right so the ID
52:23 - is 8 so if we go to our data datase and
52:27 - if I run this again so you can see this
52:29 - right so our data got added to the
52:31 - database we were able to upload in
52:33 - chunks and now let's check our S3 as
52:35 - well so if I come back over here and if
52:37 - I refresh there's a D15 recording and I
52:40 - can actually play it we can open and we
52:43 - can hear so okay I don't want to hear
52:46 - myself I'm going to mute and play so
52:48 - this is the recording of our hld class
52:51 - so day 15 class but yes so we did one
52:54 - upload request from our front end we
52:56 - were able to add to the DB we were able
52:58 - to do chunking but wasn't this amazing
53:01 - what we just did was we added the video
53:03 - from the front end from our client we
53:05 - were not able to just chunk it on the
53:07 - front end we were able to send it on the
53:09 - back end the back end tended multiart
53:12 - upload to the S3 and also added the
53:14 - details to our post sequence if you
53:16 - think what you just saw was very cool
53:18 - just trust me and wait for it because
53:19 - things are going to get even more cooler
53:21 - a lot cooler in fact so let me just show
53:24 - you the code you have already seen this
53:26 - I just added log completing upload and
53:28 - in the request body now title
53:29 - description author is coming and after
53:31 - completion of multiart upload we did add
53:34 - video details to DB so you already saw
53:36 - the code for this and you saw that I got
53:38 - the URL if you just want to see what I'm
53:40 - talking about in my complete upload over
53:43 - here I am getting the URL from over here
53:46 - so let me just open word wrap and over
53:49 - here you will be able to see upload
53:50 - result. location because the URL we are
53:53 - getting from the completion of the
53:55 - upload and that URL I am sending to add
53:57 - video details to DB so this is it now
54:00 - that we have understood the upload part
54:02 - very nicely let's come to our watch
54:04 - service and let's work on that again our
54:07 - watch service needs to be connected to
54:08 - our database right because we are going
54:10 - to work on a URL where we are going to
54:12 - see our YouTube home and where we'll be
54:14 - able to list down all our videos and
54:16 - play them so that is what I'm going to
54:18 - do over here in my watch service I'm
54:20 - creating an API get all the videos
54:22 - basically so over here what I'm doing is
54:25 - writing a simple Prisma so there are
54:27 - many ways of writing this we could have
54:28 - used the metadata and wrote this but
54:30 - just wanted to show that you can also do
54:32 - query draw over here and you can try
54:34 - this out and we'll get all the videos
54:36 - over here and you can test using Postman
54:38 - and after that you can add the front end
54:40 - code so on the front end code we are
54:42 - creating another page YouTube home and
54:45 - we are again doing the async request to
54:47 - get all the videos and we are going to
54:49 - do this in the use effect that means
54:51 - that as soon as the app is going to load
54:53 - that is when we are going to get the all
54:55 - the YouTube videos and if you want to
54:57 - see how to do that let me just show you
54:59 - the client code so over here in the page
55:01 - just to show you I added upload form
55:03 - right now now what I'm doing is adding
55:06 - YouTube home so YouTube home is the new
55:08 - page that I've added so if you go over
55:10 - here YouTube home uh set video set
55:13 - loading so initially it is going to say
55:15 - it is loading and if it is loading in
55:17 - the between it is just going to say
55:19 - loading like in the middle of the screen
55:21 - it is going to say like this and once it
55:23 - is done loading it is going to run right
55:25 - so upload service I had stopped let me
55:27 - just run it again and let me run the
55:29 - watch Service as well so we are going to
55:32 - go inside the watch service and we are
55:34 - going to run it again using
55:36 - noemon and this is also running and this
55:40 - is running at 8082 and let's see if our
55:43 - request was going to 82 itself so that
55:45 - everything is working fine so let's go
55:46 - to our YouTube home and let's check the
55:49 - request is going to 82 obviously I
55:51 - should be putting this to environment
55:52 - variables but this is just like quick
55:54 - demo I had done all of the in the end of
55:56 - the WhatsApp project so uh I wanted to
55:59 - hurry in this project and focus on the
56:01 - other things so I've written like this
56:03 - okay and if we refresh this so you can
56:05 - see loading and you can see all the
56:07 - videos have come including the ones that
56:10 - we just added can you see YouTube hldd
56:12 - and summer camp and you can actually
56:14 - play all of this this is pretty cool now
56:17 - this these things that you're seeing is
56:19 - like because you're seeing the final
56:20 - project but yeah so I know the UI is not
56:23 - so good but this is enough and uh this
56:26 - is is all during the classes that I have
56:28 - written so whatever was the best we
56:29 - could do we have done so this is how we
56:32 - added route in the wat service to list
56:34 - all the videos so this was the back end
56:35 - part so here we just did get get all the
56:38 - videos and here was the front end part
56:40 - which we which I just showed you right
56:42 - videos and loading and we just saw this
56:44 - also after this the sign out sign in
56:47 - part over this what I'm talking about
56:49 - right over here you can see an ed
56:51 - because it is HL with the two courses
56:53 - and all of this so how can we add that
56:55 - so if you remember I had told you about
56:58 - next to right and use session sign in
57:00 - sign out so we are I had told you that
57:02 - you will be able to access data anywhere
57:04 - and everywhere and why I'm able to do
57:06 - that is let me show you the code so in
57:08 - the client if you look at the layout. JS
57:10 - I have wrapped all the children within
57:13 - session provider o because of which the
57:15 - session is available to all the children
57:17 - so that is why what I've done is in my
57:19 - YouTube home on the top so if you notice
57:22 - over here on the top I have added one
57:24 - very simple Navar right so created one
57:26 - Navar over here so that is a new
57:29 - component Al together and if you go over
57:31 - here I am able to get data using use
57:34 - session over here and I am going to do
57:37 - what once I able to sign in I will
57:40 - should be able to go to upload right and
57:43 - I have added sign in sign out buttons
57:45 - and here from the data I'm able to get
57:47 - the usern name and the user image so
57:49 - right now I didn't have any image so it
57:51 - returned H by default if I would have
57:54 - had any proper image it would have
57:55 - returned me that right so now let's come
57:58 - over here and let's try this out so if I
58:00 - sign out and if I sign in you will be
58:04 - able to see that I do sign in with
58:07 - Google and here now I get the upload
58:10 - button and if I go to
58:12 - upload I am able to come over here and
58:15 - the rest of the flow you have seen right
58:17 - so this is sort of a more complete flow
58:19 - now there are couple of more things that
58:21 - you can add to your project like for
58:22 - example if I am signed out and if I try
58:25 - going to upload like this itself I
58:27 - should not be able to go but right now
58:28 - you'll be able to go so these small
58:30 - things you can keep adding because I had
58:31 - WR all of this because I had done all of
58:34 - this during the class I have not done
58:35 - this level of checking and changes and
58:37 - even this you could have lazy loading
58:39 - pagination and things like that for now
58:41 - it's a simple sign in sign out able to
58:43 - upload able to get the entire flow so if
58:45 - I go to upload and if I upload from here
58:48 - the rest of the flow you know about it
58:49 - right it's time to finally move to one
58:52 - of the most interesting things which was
58:54 - actually the last class of the YouTube
58:56 - project that we did so all of these
58:58 - things are done and now it is time that
59:00 - we come to one of the most interesting
59:03 - parts of the entire project which is
59:04 - adaptive P streaming so you saw how we
59:07 - started step by step started from just
59:09 - you know seeing one entire flow to
59:11 - understanding Kafka understanding
59:13 - postgress uh put adding one or and doing
59:16 - all of this now it is time to level up
59:18 - one level further and we are going to be
59:21 - talking about adaptive bet streaming so
59:24 - for the h students what I did was we
59:27 - discussed a bit of theory resolution
59:28 - format bet water resolutions uh talked a
59:31 - bit about TCP UDP web RTC rtmp what is
59:35 - hls what is Dash and all of this and
59:38 - then we got to finally the code let me
59:40 - first quickly explain to you what is
59:41 - adaptive betr streaming so you must have
59:43 - seen on YouTube and I'll show it to you
59:45 - also that when you play a video
59:47 - sometimes what happens when the network
59:48 - is good the resolution when you have set
59:50 - the setting to Auto the resolution
59:52 - sometimes keeps changing so you must be
59:54 - watching something at say 7 20p and the
59:56 - network connection gets better it might
59:58 - switch to 1080P or if the network
60:00 - connection becomes worse it might switch
60:02 - to 480p or 320p right so what is
60:05 - happening actually is that each video
60:07 - when we divide it into
60:09 - chunks what we have on a back end is
60:12 - that each video C chunk now that chunk
60:16 - also is saved in different different
60:18 - resolutions on our pend so if this chunk
60:20 - is there this chunk is saved at 320p
60:23 - 480p 720P 1080P and similarly for all
60:27 - the chunks so what happens is suppose
60:30 - over here we detect that you know
60:31 - network connection is actually worse so
60:34 - the next chunk that is going to come
60:36 - that is going to come with a lower
60:38 - resolution and suppose over here we
60:40 - realize that oh the network connection
60:42 - is now better the bandwidth is better so
60:44 - then after this what is going to happen
60:45 - is the resolution is going to improve so
60:47 - this is called adaptive pit rate
60:49 - streaming I have covered this in two
60:52 - other videos as well there was one video
60:53 - where I was explaining adaptive betr
60:55 - streaming to my father and there's one
60:57 - more video where I've covered the hld of
60:59 - YouTube with hararat where we did an
61:01 - entire discussion there also we have
61:03 - discuss this so I will link both the
61:05 - videos in the description you can check
61:06 - them out but I hope you have understood
61:08 - what is adaptive betr streaming so as
61:10 - the network connection changes the next
61:13 - chunks that are going to come from the
61:14 - back end that are going to come from the
61:17 - server are going to uh change
61:19 - accordingly the resolution of those
61:21 - chunks is going to change accordingly so
61:23 - obviously what what we need to do during
61:25 - upload is that we need to transcode each
61:27 - and every chunk into all the solutions
61:30 - right so there are two things that we
61:32 - need to do one is during upload and one
61:34 - is during streaming or watching so
61:36 - during upload what we need to do is that
61:38 - for each of these chunks we need to
61:41 - convert them to all the possible
61:43 - resolutions and all of these possible
61:45 - resolutions we want to upload to S3 and
61:47 - who is going to do this with service is
61:50 - going to do this yes transcoder service
61:52 - so if you remember we had added one
61:54 - Kafka so the upload
61:56 - service was adding some message to Kafka
61:58 - and then Kafka from Kafka transcod
62:01 - service was consuming this message right
62:03 - so while completion of our upload what
62:05 - should happen is that uploader service
62:07 - should put all the details to the Kafka
62:11 - and transporter service is going to pick
62:13 - it up from Kafka and then process and
62:15 - then upload it to S3 so this is one
62:17 - thing that we need to do second thing
62:19 - that we need to do is during watch right
62:21 - because now okay everything is available
62:23 - for us to be able to have B adaptive P
62:26 - streaming but how are we going to do
62:28 - adaptive P streaming so for that there
62:30 - are two solutions one is hls and the
62:33 - other is Dash so this is going to happen
62:35 - where this is going to happen on the
62:37 - client side so let me show you the
62:39 - difference between hls and dash there's
62:40 - a bit of theory that I added on the doc
62:42 - so let me quickly explain that to you if
62:44 - you see the doc I have added a quick
62:45 - comparison of hls and dash see both are
62:48 - adaptive streaming protocols used for
62:50 - delivering multimedia uh the only
62:52 - difference is that hls was developed by
62:53 - Apple so it is actually easier to use it
62:56 - with iOS devices and dash was uh is an
62:58 - open standard developed by Microsoft
63:00 - Netflix Google and all of this so in
63:02 - this project we'll be using hls which is
63:04 - HTTP live streaming but you can also use
63:07 - Dash which is dynamic adaptive streaming
63:09 - over HTTP let me tell you that the
63:10 - concept is essentially the same thing
63:12 - just the file extensions and all are
63:14 - going to be different one very important
63:16 - file that you need to know about when
63:17 - we're talking about adaptive betr
63:19 - streaming is the Manifest file now
63:21 - before explaining all of that to you let
63:23 - me actually show you how things are
63:24 - working let's actually run it can see so
63:26 - for now what we going to do is forget
63:28 - everything else and just see our
63:30 - transcoder service because in our
63:32 - transcod service what we had written so
63:34 - far that we are just consuming the
63:36 - message from kamka it's time that we see
63:38 - the actual code of transcoding right so
63:41 - let's go back to our code over here so
63:43 - coming to the transcod service in index
63:45 - what I have done is just to test right
63:46 - now in transcode I'll be commenting this
63:49 - because this is the final one that we'll
63:50 - be using for now we'll be using convert
63:52 - to hls and what is happening inside this
63:55 - is if I just go to this convert. hls and
63:58 - hls folder so here is where the actual
64:01 - transcoding is happening for now for
64:03 - demo what we are going to do is test
64:05 - this using Postman and what video are we
64:07 - going to uh transcod there's one test
64:10 - video that I have added test. MP4 we are
64:12 - going to be transcoding that and we'll
64:14 - see that the output will actually be
64:16 - generated so there's a for Loop over
64:18 - here that is going to you know generate
64:21 - the uh chunks in all of these
64:22 - resolutions so there's an array so the
64:24 - code is also extended princible that if
64:26 - you want to add more resolutions you can
64:27 - add them if you want to remove the
64:29 - resolutions you can just remove from the
64:31 - array and what we are going to do over
64:33 - here is mention the video bit rate audio
64:35 - bit rate I'm going to go through this
64:37 - but for the output what it needs is an
64:39 - output folder so I'm going to create
64:40 - another folder inside this which is
64:42 - going to be output and inside this is
64:45 - where the files should come right now
64:48 - you can see there's nothing inside
64:49 - output okay and right now we were
64:51 - playing only with two services and one
64:53 - client let's create another one for
64:55 - transcoder service and over here let me
64:59 - just run
65:00 - it and so it is running at 8083 but it
65:05 - is also listening to Kafka so for now
65:08 - just to keep things simple let me just
65:10 - comment this out so that you know uh the
65:13 - Kafka messages stop coming and it just
65:16 - loaded and it is running at 883 so what
65:18 - we're going to do is go to postmen and
65:20 - run this so we got the response
65:23 - immediately but let's see what is
65:24 - happening in the back end so can you see
65:26 - this getting generated can you see
65:28 - something is getting generated yes can
65:30 - you all see this this is awesome right
65:32 - so these files are getting generated by
65:35 - itself can you see
65:38 - it so it is presently getting generated
65:41 - at 1280 by 720 right so what are the
65:45 - three resolutions that we had given
65:46 - let's see that so let's go to the
65:48 - transport code we had given three
65:50 - resolutions right 320 by 180 so these
65:53 - are the first resolutions and there was
65:54 - a master file for this then there was
65:56 - another resolution 854x480 and then
65:59 - there was another Master file for this
66:01 - so all of these are the chunks and then
66:04 - there's another one for 1280 by 720 then
66:06 - there are the chunks for that and
66:07 - there's another Master file and then
66:09 - there's this final Master file now what
66:12 - is this what is happening let's see a
66:13 - bit of theory so what is happening
66:16 - exactly is that m3u8 is the playlist
66:18 - file which is containing the URLs and
66:21 - transport stream files containing the
66:23 - actual media segments so what we did
66:25 - what we segmented and transcoded that is
66:28 - saved in this TS so this TS is nothing
66:30 - but small small chunks but what we need
66:33 - during streaming is one place from where
66:35 - we can read that this Chunk in this
66:37 - format is present where this Chunk in
66:39 - this format is present where so this is
66:41 - what it is telling this particular
66:43 - Master file is telling let see the first
66:45 - 10 seconds of the chunk is present in
66:48 - this file then the next 10 seconds are
66:50 - present in this file then the next 10
66:52 - seconds are present in this file so you
66:53 - can see it is getting generated by zero
66:55 - 0 1 02 03 and so on and so forth so
66:58 - there is one master file for this
67:01 - resolution then there's another Master
67:03 - file for this resolution so again you
67:05 - can see that see this the first 10
67:07 - seconds are present in this chunk this
67:09 - these 10 seconds are present in this
67:10 - chunk now you must be thinking that KY
67:12 - where did we give 10 seconds right so we
67:15 - have given that in our code so if you
67:16 - see over here okay if you see over here
67:19 - we have given the hls time should be 10
67:21 - 10 seconds so if you change this your
67:23 - chunk size the number of chunks and all
67:24 - of that is going to to vary let me just
67:26 - show you once more so if these are all
67:28 - 1280 x 720 K chunks okay and if you come
67:31 - over here this is the Manifest file for
67:33 - that so the first 10 seconds are present
67:35 - in this file the next 10 seconds are
67:37 - present in this file now this is for
67:39 - each resolution now you want one master
67:42 - playlist right so if you see over here
67:45 - uh so one is segment then there is
67:47 - Master playlist variant all of this is
67:50 - there and after that what you need is
67:52 - one master playlist what is Master
67:54 - playlist so these were the smaller
67:56 - playlist that we had created and what is
67:58 - there in the master playlist is C this
68:01 - resolution playlist file is this this
68:03 - resolution playlist file is this this
68:05 - resolution playlist file is this so
68:07 - essentially when adaptive betr streaming
68:09 - is going to happen by hls what is going
68:11 - to happen is that it is first going to
68:13 - come to this file and then it is going
68:14 - to see that oh network connection is
68:16 - good let's see that okay we can use 1280
68:19 - by 720 let's refer to this manifest file
68:22 - and if we are referring to this manifest
68:24 - file it is going to get start getting
68:26 - the chunks from over here then as soon
68:28 - as the you know uh the network
68:30 - connection is going to go worse it is
68:32 - going to say that oh no I need you know
68:33 - the ones with the res resolution so it
68:35 - is going to say that for this bandwidth
68:37 - like you know for this resolution which
68:39 - is the playlist file then it is going to
68:41 - go to this playlist file and then it
68:43 - will get the chunks for that so like
68:45 - this it will keep doing what adaptive
68:47 - bet streaming so M3 u8 is the extension
68:50 - and TS is the extension for hls when it
68:53 - comes to dash the extensions are going
68:55 - to be MPD and m4s but it is essentially
68:58 - the same thing so what we have done so
69:00 - far is that we just did one testing for
69:03 - transcoding we sent the request from
69:05 - postmen and it was able to transcode
69:08 - right now that you have understood that
69:09 - what is exactly happening the Manifest
69:11 - files are getting generated and
69:13 - transcoding is happening let's look at
69:14 - the code at how did we write the code
69:16 - right so we are using two packages over
69:19 - here fmpg is very common for transcoding
69:22 - you can read about it again all the URLs
69:24 - and everything all the links are added
69:26 - over here in the doc so FFM PG is very
69:29 - famous for transcoding and what we have
69:31 - used is its binary so these binaries
69:33 - have come from ffmp static and we are
69:36 - actually encoding using this so this guy
69:39 - this package expects you to have
69:40 - binaries you can either download your
69:42 - own binaries and set everything up or
69:44 - you can use this for getting the
69:45 - binaries so this is what we have done
69:47 - and over here this is my file name and
69:50 - okay uh so you can see these file names
69:53 - that are getting generated right TS and
69:55 - 3 it how did this happen so we are
69:58 - actually generating these file names and
69:59 - how that is happening is that whatever
70:01 - is the file name in that from dot I am
70:03 - replacing it with underscore so test.
70:06 - MP4 became testore MP4 and then
70:10 - underscore resolution so underscore
70:12 - resolution and then M3 U so this is for
70:15 - M3 U and for the segment file what I
70:17 - have done is that after all of this I am
70:20 - adding this 0 0 0 1 02 03 and so on so
70:23 - that is how I am doing this so so after
70:25 - I've generated the names what I'm doing
70:27 - is and in this for Loop so this for Loop
70:29 - is for every resolution so if you
70:31 - remember in this aray there are three
70:32 - things right so from every array what
70:35 - are we getting is we are getting
70:37 - resolution video bit rate and audio bit
70:39 - rate so here you you can see that I'm
70:41 - generating the output file name segment
70:43 - file name and then what I'm doing is I
70:45 - am actually doing the transcoding in FFM
70:48 - MPG this is the output options that we
70:50 - have given we have mentioned that h264
70:52 - CC a for audio video bit rate audio bit
70:56 - rate resolution and this is the time now
70:59 - this is usually used for uh live
71:01 - streaming and all that so I'm not going
71:02 - to go into details right now and this is
71:04 - the output file name that where it is
71:06 - going to be there and then you can have
71:08 - your error end and all of this and your
71:10 - entire so as you go through this what
71:12 - you have done is you have created one
71:14 - array and as you keep generating these
71:17 - TS and these M3 U you keep adding them
71:20 - to the this array that this array push
71:23 - variant playlist so what is the
71:24 - resolution what is the output file name
71:26 - because what is going to happen is that
71:27 - you need to create your master playlist
71:29 - after this so in this master playlist
71:31 - you are going to map this uh variant
71:34 - playlist if you don't understand map and
71:36 - all you can refer to the JavaScript
71:38 - Basics video the link is in the
71:39 - description and uh you can uh get the
71:42 - resolution and everything and this is
71:44 - where you are creating your final Master
71:46 - playlist so if you can see this is how
71:48 - it looks like and this is exactly what
71:50 - I'm doing where did my code go you can
71:53 - see this is exactly what I'm doing over
71:55 - here I am adding all of this right so
71:57 - this is the final code that you have
71:59 - understood uh but now let's come to the
72:01 - doc also so you understood and coding on
72:03 - packet this is how it is happening
72:05 - variant playlist blah blah blah so
72:07 - Master playlist we created and this is
72:09 - done so you have understood that how we
72:11 - did the transcoding on the back end and
72:13 - we tested it using Postman now what we
72:15 - want to do is we want to see that we can
72:18 - play this on our client side or not now
72:21 - how do we test it so how we have done so
72:23 - far is all our t files and 38 everything
72:26 - is in this output folder so for now just
72:28 - for testing just for step one what you
72:30 - can do is you can upload this entire
72:32 - folder in your AWS so let's try doing
72:35 - that I'm going to upload a folder so in
72:37 - hhld YouTube let's go to transcoder
72:39 - service inside this there's an output
72:41 - folder let's upload the folder as it
72:43 - is yes we want to upload everything so
72:46 - you can see the TS files M38 everything
72:49 - is going to come okay we are going to
72:50 - add the folder and upload the entire
72:53 - thing
72:56 - now that we have added all the files to
72:58 - Output let's come back to the bucket so
73:00 - here there is output right so everything
73:02 - is there and what do I want to access
73:04 - the master U so if I come over here and
73:07 - if I copy this URL I should be able to
73:09 - access the permissions are public right
73:11 - now so the bucket is publicly accessible
73:13 - and if you go over here on the client
73:15 - side what I have done is I've created a
73:16 - video player and I've cre and I've given
73:18 - one URL right so right now just to fix
73:21 - like I know everything is hardcoded
73:23 - right now but this is just to test so if
73:25 - I go to my video player and I come over
73:28 - here so I can give one URL over here
73:30 - right so right now let's just give this
73:32 - URL I'm not sure if it is going to
73:34 - generate the same URL okay so this is
73:37 - the URL and okay and what we are going
73:40 - to do is if hls is supported and we are
73:43 - going to attach the video the source and
73:45 - everything and we are going to play the
73:47 - video okay so this is a very simple
73:49 - frontend code uh but in our page right
73:51 - now I'm leaving everything else and just
73:53 - adding video player and and I'm going to
73:55 - go to client and let's try running
74:01 - this so here you can see the video is
74:03 - getting played right and if we go to
74:05 - inspect and if we go to network and load
74:09 - this again so can you see m3u8 and TS
74:13 - files so see where did this come from
74:15 - from S3 and the TS files also see all
74:19 - because we got 1280 by 720 these also
74:22 - came the same now because this is a very
74:24 - small video I can't show this to you but
74:26 - in the class we also did what we did was
74:28 - uh we had a huge video and from there
74:31 - what we did was from no throttling you
74:33 - can change to slow 3G now suppose I do
74:36 - this and you can see this is still
74:39 - refreshing now later what it is going to
74:42 - do instead of getting 1280 by 720 it is
74:45 - going to get something else now let's
74:47 - see if it is going to happen uh let's
74:49 - just change quickly fast
74:51 - 3G and no throttling it's going to
74:54 - happen very fast fast and slow 3G I
74:56 - didn't get time so initially you can see
74:58 - it was 320 by 180 then it switched to
75:00 - 1280 x 720 because it's is a very small
75:03 - video I was not able to show properly
75:05 - but you can try uploading for a huge
75:07 - video I actually showed it in the class
75:08 - you can try it out yourself because it's
75:10 - going to take a lot of time to show uh
75:12 - essentially you get the point that
75:14 - adaptive betr streaming is happening so
75:16 - not only we transcoded on the back end
75:18 - we also saw that how adaptive bitrate
75:20 - streaming is happening on the client how
75:22 - did that happen that happened because we
75:24 - used h H LS so we have used hls.js over
75:27 - here and if it is supported we have
75:28 - attached the media loading loaded source
75:30 - and if it is not supported you can just
75:32 - play the original file the original file
75:34 - is also going to be on the S3 right now
75:36 - you will say that kti for now what we
75:38 - are doing is we getting the S3 link I
75:41 - know right now this is how how we have
75:42 - done that what we are going to do is
75:44 - firstly we going to let the upload
75:46 - service upload the entire thing to S3
75:49 - and in the response of that if you go
75:50 - back to your upload server so this is my
75:52 - service and if I go back to my
75:54 - controllers and multiart upload over
75:56 - here what I'll be doing is that once the
75:58 - upload is complete I'll be pushing it to
76:00 - Kafka from Kafka my transcor service is
76:02 - going to pick it up and what am I
76:05 - pushing to Kafka the location what is
76:07 - location the URL and this is the URL
76:10 - that transcod wanted right the
76:12 - transcoder wanted so it is going to
76:14 - upload it and now after this what you
76:16 - can do is see I've have added all the
76:18 - steps over here so HL is streaming on
76:20 - client right now this is hardcoded after
76:22 - this what you can do is by the way I
76:24 - also added the Dash conversion code for
76:26 - reference if you want to do go ahead
76:27 - with Dash instead of hls now what you
76:29 - can do is pick up the video from S3
76:32 - transcod it and push it back so let's
76:34 - just see this then after whole
76:36 - connection you'll be able to see okay so
76:39 - over in the transcoder service for now
76:42 - so if we go back to the transcoder
76:45 - service by the way if it is getting
76:47 - overwhelming don't worry like if you go
76:49 - step by step you'll be able to get it
76:51 - and if you need guidance you can also
76:53 - sign up for hhld we are here to guide
76:55 - you in any way possible if you can quote
76:57 - this out yourself it is great if you
76:59 - think you know guidance to be able to
77:00 - create projects like this will be useful
77:02 - to you you can check out the H course
77:04 - the link is in the description if you
77:06 - have come so far you definitely like my
77:07 - teaching style you're definitely loving
77:09 - the entire project so if that is the
77:11 - case if you want to see more projects
77:13 - like this and if you want to be part of
77:14 - some amazing courses amazing Community
77:17 - where we help each other out you can
77:18 - check out the hld course also like the
77:21 - community is so helpful whenever someone
77:23 - runs into any kind of bugs any kind of
77:25 - issues let it be related to AWS or any
77:27 - code we are here to help each other out
77:29 - so yep that is there so now let me just
77:32 - walk you through what we're going to do
77:33 - in our transcoder service what we had
77:36 - done was that we were transcoding using
77:38 - this convert to hls now we'll be using
77:40 - S3 to S3 what is the difference in S3 to
77:43 - S3 is that here we are going to pick up
77:46 - the URL from S3 transcod it and put it
77:49 - back so let me quickly walk you through
77:51 - the entire code okay so this is client
77:53 - this is transcod service S3 to S3 okay
77:56 - so how we have written the code is that
77:58 - uploader service is going to upload
78:00 - everything to S3 which will be the
78:01 - original file once it get the URL this
78:04 - URL is given to our uh service our
78:07 - transcod service you can also do like
78:08 - before the upload itself the chunking
78:10 - can start but this is how we have done
78:12 - okay so uh here we have configured S3
78:15 - and done all of this now what is going
78:17 - to happen is it is going to download the
78:19 - S3 file where are we going to get this
78:21 - URL this URL is going to come come from
78:25 - so if you see s32 S3 mp4 file name right
78:28 - so right now I have hardcoded this you
78:30 - can give this anything right now I have
78:32 - hardcoded it what it is going to do is
78:35 - it is going to download the file and how
78:37 - does it know where to download from
78:38 - because we have given the bucket name
78:41 - and we will Al we have also given the
78:43 - file name right so bucket name and key
78:44 - is enough for it to be able to download
78:47 - and once it downloads it is going to
78:49 - create like you will be able to see I
78:51 - will run it and show it to you it will
78:53 - create in its local it it will convert
78:55 - all of it and after that it like you can
78:58 - see this code is same right resolutions
79:00 - and from the read stream we are piping a
79:02 - right stream and you can see this code
79:04 - is same right resolutions the for Loop
79:07 - all of this is same creating the master
79:09 - playlist all of this is exactly the same
79:11 - right this is same what we have just
79:14 - done is that till here it's all good
79:16 - from here after we have generated
79:18 - everything locally we are going to
79:19 - delete it and then we are going to
79:21 - upload everything to our AWS and how are
79:24 - we doing that we are creating one hls
79:26 - folder and we'll be uploading it over
79:28 - there let me show this to you how it is
79:30 - going to run right now the file name
79:32 - that I have added is trial to because
79:33 - this was actually a very small file this
79:35 - was just part of one of the classes so
79:37 - let me just upload the same file for
79:40 - easier purposes okay so let me just go
79:42 - to the hhld videos and trial to I'm
79:46 - uploading so this file it is going to
79:49 - find so ideally what is going to happen
79:52 - is that our uploader Serv will send this
79:54 - to Kafka from Kafka it will pick up the
79:56 - URL location and then it will be able to
79:58 - go ahead right for now we have done like
80:00 - this itself and you can see what all
80:02 - things are there there's output there's
80:03 - trial to all of this we actually don't
80:05 - need output for now but okay I'm going
80:07 - to let it be there and in our transort
80:11 - in our index we have made this change
80:13 - right so this is the transcor service in
80:15 - our index we have made the change that
80:16 - S3 to S3 is going to get caught okay so
80:19 - just notice what is going to happen over
80:21 - here so let's go back to postman let's
80:24 - send this request again and let's come
80:27 - back over here so can you see this being
80:30 - generated in local so the files are
80:33 - being generated and after getting
80:36 - generated you will see that also
80:37 - deletion will happen and you can see
80:39 - that logs are coming right because if
80:41 - you go to s3. S3 this is where we were
80:43 - actually adding the logs and you can see
80:46 - Master got generated because it was a
80:47 - small file it is happening by itself now
80:49 - you can see that files are getting
80:51 - deleted can you see file get getting
80:53 - deleted because because it deleted
80:55 - locally uh downloaded S3 mp4 file and
80:58 - now it is uploading the segments to S3
81:01 - right so it is deleting everything so it
81:03 - generates locally and then it is going
81:05 - to upload to S3 so it took the URL from
81:08 - S3 and then it is uploading to S3 you
81:11 - don't even need the entire URL what do
81:13 - you need just the key and the bucket
81:14 - name so if I load over here hls folder
81:17 - is created and inside this you can find
81:19 - everything right so this is how you can
81:21 - do from S3 to S3 now that we have
81:23 - finally seen everything happening let me
81:26 - quickly show you the final hld diagram
81:28 - we have seen all the parts working
81:31 - properly what you have to do is now code
81:33 - all of this and I hope from the diagram
81:36 - you will be able to understand exactly
81:38 - what happened what all you saw and I
81:40 - know for bers this can be overwhelming
81:42 - but my job is to make things easy for
81:43 - you coming to the hld we drew one client
81:47 - right we made one client which was
81:48 - written in next years after that we
81:50 - created three services one was what
81:53 - service the first one that we had
81:55 - created was the upload
81:57 - service and we also added our
82:00 - transcor now what were the requests that
82:04 - went from where to where did it go there
82:06 - was one home request that got us all the
82:09 - list of the videos right so uh client
82:12 - sent home request which got it all the
82:14 - list of the videos upload service it
82:16 - sent upload and this was basically three
82:20 - API calls right one was initiate so let
82:22 - me write all the three calls for further
82:24 - CL Clarity so that you don't get
82:25 - confused the first one was initiate it
82:29 - was upload initiate but okay so then it
82:31 - was upload and then it was complete
82:33 - upload right so upload SL complete it
82:36 - was so you sent all of these three apis
82:38 - to upload service then what happened was
82:40 - between upload service and transcoder
82:42 - service there was one
82:44 - cfom and we also added databases so
82:48 - there was
82:49 - S3 and post SQL right so let's write
82:53 - post SQL over here and S3 over here so
82:57 - what happened was when we finished up
83:00 - complete while finishing of the complete
83:02 - what all things happened was upload
83:04 - service did three things one was that it
83:07 - added the original file to S3 that was
83:10 - one thing second thing that it did was
83:12 - it added the metadata to post SQL
83:14 - basically title description and author
83:19 - and the URL the S3 URL because it had
83:21 - already uploaded to S3 it has the S3 URL
83:23 - and the third thir thing that it did was
83:25 - that it added to Kafka and transcod the
83:28 - service consumed from the Kafka what did
83:30 - it need from Kafka it needed just the
83:32 - key of the bucket so the key and the
83:36 - bucket it got so this transcor service
83:39 - went to the S3 so it went to S3 and it
83:43 - got the file locally it transcoded
83:46 - everything and then it put it back so
83:48 - what service is getting all the details
83:50 - for slome from where from the post SQL
83:52 - so these are the things that we have
83:54 - coded and this is amazing right we
83:56 - actually coded everything that is there
83:58 - in the hld diagram these are the main
84:00 - features I know there are a lot more
84:01 - features that you can add there's
84:02 - recommendation engine that you can add
84:04 - content filtering and a lot more things
84:06 - major things are like comment uh Channel
84:10 - user table and all of this you can add
84:12 - but I guess the main features of YouTube
84:14 - is pretty cool this is a pretty cool
84:16 - project what do you guys think
84:19 - [Music]
84:25 - I hope you all had a good time if you
84:27 - have watched it here that really means a
84:29 - lot and I hope you all can be part of Ed
84:31 - courses as well the link to edu courses
84:33 - is in the description we have mentioned
84:35 - all the details on the site the various
84:38 - courses that we have the bundles the
84:40 - curriculums the testimonials the FAQs
84:43 - everything is mentioned on the site just
84:45 - check it out and if you still have any
84:46 - questions you can reach out to us at
84:48 - support at the ril courses.com we would
84:50 - love to be part of your uh Learning
84:52 - Journey and please don't forget to
84:54 - subscribe I hope uh you know you like
84:56 - all the hard work that I'm putting at
84:58 - least you can subscribe it is completely
85:00 - free for you just subscribing it will
85:02 - motivate me so much thank you so much
85:04 - and see you next time bye