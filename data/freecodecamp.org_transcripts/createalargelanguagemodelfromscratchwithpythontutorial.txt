00:00 - Learn how to build your own large language model from scratch.
00:04 - This course goes into the data handling, math, and transformers behind large
00:08 - language models.
00:09 - Elia Arleds created this course.
00:12 - He will help you gain a deep understanding of how LLMs work and how they can be
00:16 - used in various applications.
00:19 - So let's get started.
00:21 - Welcome to Intro to Language Modeling.
00:22 - In this course, you're going to learn a lot of crazy stuff.
00:25 - Okay.
00:26 - I'm just going to give you a heads up.
00:27 - It's going to be a lot of crazy stuff we learn here.
00:29 - However, it will not be insanely hard.
00:31 - I don't expect you have any, any experience in calculus or linear algebra.
00:36 - Uh, a lot of courses out there do assume that, but I will not.
00:39 - We're going to build up from square one.
00:41 - We're going to take baby steps when it comes to new, uh, fundamental concepts
00:44 - in math and machine learning, and we're going to take a larger steps once things
00:48 - are fairly clear and they're sort of easy to figure out, uh, that way we don't
00:52 - take forever just taking baby steps through every little concept.
00:55 - This course is inspired by Andre Karpathy's, uh, building a GPT from scratch
01:00 - lecture, so I'll shout out to him and yeah, we don't assume you have any
01:06 - experienced, maybe three months of Python experience, uh, just so the syntax is
01:11 - sort of familiar and you can, you're able to follow along that way, but, uh, no
01:16 - matter how smart you are, how quick you learn the willingness to put in the
01:20 - hours is the most important because this is material that you won't normally
01:24 - come across. Um, so as long as you're able to put in that constant effort, uh,
01:29 - push through these lectures, even if it's hard, take a quick break, grab a snack,
01:33 - whatever you need to do, grab some water, water is very important.
01:37 - And yeah, hopefully you can make it to the end of this.
01:41 - You can do it, uh, since it's free code camp, everything will be local
01:46 - computation, nothing in the realm of paid data sets or cloud computing, uh,
01:52 - uh, we'll be scaling the data to 45 gigabytes for the entire, uh, training
01:57 - data set, so have 90 reserved so we can download the initial 45 and then
02:02 - convert it to an easier to work with 45.
02:05 - So, uh, yeah, if you don't actually have 90 gigabytes reserve, that's totally
02:09 - fine, you can just download a different, uh, data sets and sort of follow the
02:14 - same, uh, data pipeline that I do in this video through the course, you may
02:20 - see me switch between macOS and windows, the code still works, all the same,
02:23 - both operating systems, and I'll be using a tool called SSH.
02:28 - It's a server that I can connect from my Mac book to my windows PC that
02:32 - I'm recording on right now, and that will allow me to execute, run, build,
02:37 - whatever, do anything coding related, uh, command prompt related on my, uh,
02:43 - Mac book, so I'll be able to do everything on there that I can, my windows
02:47 - computer, it'll just look a little bit different for the recording.
02:50 - So, uh, why am I creating this course?
02:54 - Well, like I said before, a lot of beginners, they don't have the fundamental
02:58 - knowledge like calculus, linear algebra to help them get started or accelerate
03:02 - their learning in this space.
03:04 - So I intend to build up from baby steps and then larger steps when things
03:08 - are fairly simple to work with, and I'll use logic analogies and step-by-step
03:14 - examples to help consent conceptualize rather than just throw tons of
03:18 - formula at you.
03:19 - So with that being said, let's go ahead and jump in to the good stuff.
03:25 - So in order to develop this project step-by-step, we're going to use
03:28 - something called Jupiter notebooks.
03:29 - And you can sort of play with these in the Anaconda prompt or at
03:33 - least launch them from here.
03:35 - So Anaconda prompt is just great for anything machine learning related.
03:39 - So make sure to have this installed.
03:41 - I will link a video in the description so that you can sort of set this up
03:46 - and install it step-by-step guide in there.
03:48 - Um, so we can do from this point is sort of just set up our project
03:52 - and initialize everything.
03:54 - So I'm going to do is just, uh, head over into my directory that I want.
04:00 - Just going to be a Python testing.
04:04 - We're going to make a directory free code camp GPT course, and then from
04:11 - this point, uh, we're going to go and make a virtual environment.
04:14 - So virtual environment, it will, and initially in your desktop, you will
04:19 - have, uh, just all of your Python libraries, all your dependencies there,
04:24 - just floating around.
04:26 - And what the virtual environment does is it sort of separates that.
04:28 - So you have this isolated environment over here, and you can just play
04:32 - around with this however you want, and it's completely separate.
04:35 - So that won't really, uh, cross with, uh, all of the global libraries
04:40 - that you have, all the ones that just affect the system when you're not
04:43 - in a virtual environment, if that makes sense.
04:46 - So we're going to go ahead and set that up right now by using Python, uh,
04:50 - dash M, and then we're going to go V and V for virtual V and V and then CUDA.
04:55 - So the reason why we say CUDA here is because, uh, later when we, uh, try
05:00 - to accelerate our learning, uh, or the model's learning, uh, we're going
05:05 - to need to use GPUs, GPUs are going to accelerate this a ton and basically
05:10 - CUDA is just that little feature in the GPU that lets us do that.
05:13 - So we're going to make our environment called CUDA.
05:16 - I go and press enter.
05:17 - It's going to do that for, it's going to take a few seconds.
05:20 - So now that that's done, we can go ahead and do CUDA and we're just going
05:24 - to basically activate this environment so we can start developing in it.
05:27 - We're going to go backslash and we're going to go scripts and then activate.
05:32 - So now you can see it says CUDA base.
05:35 - So we're in CUDA and then secondary base.
05:38 - So it's going to prioritize CUDA.
05:41 - So from this point, we can actually start installing some stuff, uh, some libraries
05:45 - here, so we can go pip three, install, uh, mat plot lib, numpy.
05:52 - Uh, we're going to use P Y L M Z a L Z M a, and then what are some other ones?
06:04 - We're going to do IPY kernel.
06:07 - This is for the actual Jupiter notebooks and, uh, being able to bring the CUDA,
06:12 - uh, virtual environment into those notebooks.
06:14 - So that's why that's important.
06:16 - And then just the actual, uh, Jupiter notebook feature.
06:19 - So go ahead and press enter.
06:21 - Those are going to install.
06:22 - That's going to take a few seconds to do.
06:25 - So it might actually happen is you'll get a build error with, uh, P Y L Z M a,
06:31 - which is a compression algorithm.
06:33 - And don't quote me on this, but I'm pretty sure it's based in C plus plus.
06:37 - So you actually need some build tools for this.
06:40 - And you can get that with, uh, visual studio build tools.
06:45 - So what you're, you might see, you might see a little error and
06:48 - basically go to that website and you're going to get this right here.
06:51 - So just go ahead and download build tools.
06:54 - What's going to download here, you're going to click on that.
06:57 - It's going to, it's going to set up and then you're going to go
07:02 - ahead and click continue.
07:05 - And then at this point, uh, you can go ahead and click modify if you see this
07:09 - here, and then you might get to a little, uh, workloads section here.
07:14 - So once you're at workloads, that's good.
07:16 - What you're going to make sure is that you have, uh, these two
07:19 - checked off right here, just make sure that you have these two.
07:22 - Um, I'm not sure what desktop particularly does.
07:25 - It might help, uh, but it's just kind of good to have, uh, some of these
07:29 - build tools on your PC anyways, even for future projects.
07:32 - So, uh, just get these two for now.
07:35 - That'll be good.
07:35 - And then you can click modify over here if you want it to modify, just like that.
07:40 - And then you should be good to, uh, rerun that command.
07:45 - Command.
07:46 - So from this point, what we can actually do is we're going to install torch and
07:51 - we're actually going to do it by using pip install, uh, three, install torch.
07:58 - We're not going to do it like this.
07:59 - What we're actually going to do is we're going to use a separate command and
08:04 - this is going to install CUDA with our, uh, torch.
08:08 - So it's going to install the CUDA extension, which will
08:10 - allow us to utilize the GPU.
08:12 - So it's just this command right here.
08:14 - And if you want to find like, uh, a good command to use, what you can do is go to
08:19 - the, uh, pie torch docs, uh, just go to, go to get started and then, uh, you'll
08:27 - be able to see this right here.
08:28 - So we have stable, uh, windows, pip, Python, then CUDA 11.7 or 11.8.
08:35 - So I just clicked on this and since we aren't going to be using, uh, torch
08:39 - vision or torch audio, I basically just did hip three, install torch, and then
08:44 - with this index URL for the, uh, CUDA 11.8.
08:49 - So that's pretty much all we're doing there to install CUDA.
08:53 - That's a part of our torch.
08:54 - So we can go ahead and click, uh, enter on this.
08:58 - So great.
08:59 - We've installed a lot of things, uh, libraries, a lot of setup has been
09:03 - done already, uh, what I want to check now is just to make sure that
09:06 - our Python version is what we want.
09:08 - So Python version 3.10.9, that's great.
09:12 - Uh, if you're between 3.9, 3.10, 3.11, uh, that's perfect.
09:17 - So if you're in between those, it should be fine.
09:20 - Uh, at this point we can just jump right into our Jupyter Notebook.
09:24 - So the command for that is just Jupyter Notebook spelled like that, click enter.
09:31 - It's going to send us into here and I've created this little
09:36 - bi-gram.ipynb here, uh, in my VS code.
09:42 - So, uh, pretty much you need to actually type some stuff in it and you need to
09:46 - make sure that it has the IPYNB, uh, extension or else it won't work.
09:52 - So if it's just IPYNB and doesn't have anything in it, uh, I can't really
09:55 - read that file for some reason.
09:57 - And yeah, so just, just make sure you type some stuff in it, open that in VS code.
10:01 - Type like, I don't know, A equals three or STR equals banana.
10:05 - I don't care, uh, at this point, let's go ahead and pop into here.
10:12 - So this is what our Notebook's going to look like and we're going to be
10:15 - working with this quite a bit, uh, throughout this course.
10:19 - So what we're going to need to do next here is make sure that our virtual
10:24 - environment is actually inside of our Notebook and make sure that we can
10:28 - interact with it from this, uh, kernel rather than just through the command
10:33 - prompt.
10:33 - So we're going to go ahead and check here and I have a virtual environment here.
10:37 - Uh, you may not, but all we're going to do is basically go into here.
10:42 - We're going to end this and all we're going to do is we're going to go ahead
10:48 - and do Python, uh, dash M and then IPY, uh, kernel install user.
10:57 - And you'll see why we're doing this in the second user name equals CUDA.
11:04 - This is from the virtual environment we initialized before.
11:08 - So that's the name of the virtual environment.
11:10 - And then the display name, how it's actually going to look in the terminal
11:14 - is going to be, uh, display name.
11:20 - Uh, we'll just call it, um, CUDA GPT.
11:28 - I don't know.
11:29 - That sounds like a cool name and then we'll go and press enter.
11:33 - It's going to make this environment for us.
11:35 - Great installed.
11:37 - Good.
11:38 - So we can go and run our Notebook again and we'll see if this changes.
11:43 - So we can go ahead and pop into our BiGram again, kernel change, kernel,
11:49 - boom, CUDA GPT, let's click that sweet.
11:52 - So now we can actually start, um, doing more and just sort of experimenting
11:57 - with, uh, how the notebooks work and actually how we can build up this
12:02 - BiGram model and sort of learning how, uh, language models work from scratch.
12:07 - So let's go ahead and do that.
12:08 - So before we jump into this actual code, we're going to go ahead and
12:12 - into this actual code here.
12:13 - What I want to do is, uh, delete all of these.
12:19 - Good.
12:20 - So now what I'm going to do is just get a small little dataset, just very small
12:25 - for us to work with that we can sort of try to make a BiGram out of something
12:29 - very small.
12:30 - So what we can do is go to this website called project Gutenberg, and they
12:36 - basically just have a bunch of free books that are, uh, licensed under creative
12:40 - commons, so we can use all of these for free.
12:45 - So let's use, uh, the wizard of Oz at the end, the wizard of Oz.
12:53 - Great.
12:55 - So what we're going to want to do is just click on plain text here.
12:58 - Great.
12:59 - So now I can go, uh, control S to save this.
13:02 - And then we could just go wizard of Oz wizard underscore of underscore Oz.
13:10 - Good.
13:12 - So now what I'm going to do is we should probably drag this into, we should
13:20 - drag this into our folder here.
13:24 - So I'm just going to pop that into there.
13:30 - Good stuff.
13:30 - Did that work?
13:32 - Sweet.
13:33 - So now we have our wizard of Oz text in here.
13:34 - We can open that.
13:36 - Uh, what we can do is start of this book.
13:40 - Okay.
13:41 - So we can go ahead and we go down to when it starts.
13:58 - Sweet.
13:58 - Sweet.
14:00 - So maybe we'll just cut it here.
14:02 - That'd be a good place to start.
14:03 - Just like that.
14:06 - I'm going to put a few spaces.
14:07 - Good.
14:08 - So now we have this book, uh, go to the bottom here just to get rid of some of
14:13 - this other licensing stuff, which is, might get in the way with our predictions
14:19 - in the, in the context of the entire book.
14:23 - So let's just go down to when that starts and of the book.
14:27 - Okay.
14:35 - Okay.
14:36 - So we've gotten all that, that is done for the illustration there.
14:41 - Perfect.
14:42 - So now we have this wizard of Oz text that we can work with.
14:44 - Let's close, close that up.
14:46 - 233 kilobytes.
14:48 - Awesome.
14:48 - Very small size.
14:49 - We can work with this.
14:50 - This is great.
14:51 - So we have this wizard of Oz.txt file and what are we going to do with that?
14:55 - Well, we're going to try to train, uh, a transformer or at least a
14:59 - by-ground language model on this text.
15:01 - So in order to do that, we need to sort of learn how to manage this text
15:05 - file, how to open it, et cetera.
15:07 - So we're going to go ahead and open this and do wizard of Oz like that.
15:16 - And we're going to open in read mode and then we're going to use the
15:19 - encoding UTF eight, just like that.
15:22 - So, uh, this is a file mode that you're going to open in others.
15:26 - Read mode.
15:26 - There's right mode.
15:27 - There's read binary.
15:29 - There's right binary.
15:31 - Uh, and those are really the only ones we're going to be boring out.
15:34 - Uh, we're worrying about for this video.
15:36 - Um, the other ones you can look into in your spare time, if you'd like to,
15:41 - uh, we're just going to be using those four for now.
15:43 - Uh, and then the encoding is just what type of character
15:46 - or coding are we using?
15:48 - Uh, that's pretty much it.
15:49 - We could just open this as F short for file and to go text, uh, equals F dot
15:54 - read, you can read this file stored in a string variable, and then we can, you
15:59 - know, we could print and print some stuff about it so we can go print the length
16:03 - of, uh, print the length of this text.
16:05 - Run that we get the length of the text.
16:08 - Um, we could print the first, uh, 200 characters of the text shirt.
16:17 - So you're the first 200 characters.
16:18 - Great.
16:20 - Um, so now we know how to, you know, just play with characters.
16:24 - Um, at least just see what the characters actually look like.
16:28 - So now we can do a little bit more from this point, which is going to be, uh,
16:32 - encoders and, uh, before we get into that, what I'm going to do is put these into a
16:39 - little vocabulary list that we can work with.
16:41 - So all I'm going to do is I'm going to say, we're going to make a, a charge variable.
16:47 - So the charge is going to be all the charge or all the characters, um, in this
16:52 - text piece, so we're going to make a, uh, sorted set of text here and we're going
17:03 - to just, uh, print out, uh, so look at that.
17:09 - We have a giant array of all of these characters.
17:13 - So now we can, what we can do is we can use something called a tokenizer and a
17:17 - tokenizer consists of an encoder and a decoder, what an encoder does is it's
17:23 - actually going to convert each character or sorry, each element of this array to
17:29 - an integer, so maybe this would be a zero, uh, this would be a one, right?
17:34 - So a new, uh, a new line or an enter would be, uh, a zero, a space would be a one,
17:41 - exclamation mark would be a two, et cetera, right?
17:43 - All the way to the length of them.
17:45 - And then what we could do is we could even, uh, we could even print the
17:48 - length of these characters so we could see how many there actually are.
17:52 - So there's 81 characters in the entire, in the, in the entire wizard of Oz book.
17:57 - So I've written some code here that is going to do that job for us, the job of
18:00 - tokenizers.
18:01 - So what we do is we just use a little generator, some generator, four loops here,
18:06 - uh, generative form, four loops rather, and we make a little mapping from strings
18:11 - to integers and integers to strings, uh, given the vocabulary.
18:15 - So we just enumerate through each of these.
18:17 - Um, we have one assignment first element assigned to a one second assigned to a
18:21 - two, et cetera, right?
18:22 - That's basically all we're doing here.
18:24 - And we have an encoder and a decoder.
18:27 - So let's say we wanted to, uh, convert, uh, the string, hello to integers.
18:33 - So we'd go encode and we could do hello just like that.
18:39 - And then we could, uh, go ahead and print this out.
18:45 - Perfect.
18:45 - Let's go ahead and run that boom.
18:48 - So now we have a conversion from characters to integers.
18:51 - And then if we wanted to maybe convert this back, so decode it, we can store this
18:57 - in a little, maybe decoded, uh, hello equals that.
19:03 - And then we could go, uh, or encoded rather encoded, hello, and then we could
19:12 - go, uh, decoded, uh, hello is equal to, we go decode, decode, decode, decode,
19:19 - is equal to, we go decode and we can use the encoded hello.
19:25 - So we're going to go ahead and encode this into integers.
19:29 - And then we're going to decode the integers back to, uh, a character format.
19:34 - So, uh, let's go ahead and print that out.
19:37 - We're going to go ahead and print the decoded hello.
19:42 - Perfect.
19:43 - So now we get that.
19:44 - So I'm going to fill you in on a little background information about these tokenizers.
19:48 - So right now we're using the character level tokenizer, which takes basically
19:53 - each character and, uh, converts it to an integer equivalent.
19:56 - So we have a very small vocabulary and a very large amount of, uh, tokens to convert.
20:03 - So if we have 40,000 individual characters, that means we have a small
20:07 - vocabulary to work with, but a lot of characters to encode and decode, right?
20:12 - If we have, if we work with maybe a word level tokenizer, that means we
20:18 - have a ton, like every single word in the English language.
20:21 - I mean, if, if you're working with, uh, multiple languages, this could be like,
20:26 - you know, a lot, very large amount of, uh, tokens.
20:31 - So you're going to have like maybe millions or billions or trillions.
20:35 - If you're, if you're doing something weird, but in that case, you're going to have a
20:39 - way smaller, uh, set to work with.
20:43 - So you're going to have a very large vocabulary, but a very small amount to
20:47 - encode and decode.
20:49 - So if you have a subword tokenizer, that means you're going to be somewhere in
20:53 - between a character level and a word level tokenizer, if that makes sense.
20:58 - So in the context of language models, it's really important that we're
21:01 - efficient with our data and just having a giant string might not work the best.
21:06 - And we're going to be using a machine learning framework called
21:09 - PI torch or torch.
21:11 - So I've imported this right here.
21:13 - And pretty much what this is going to do is it's going to handle a lot of the math,
21:17 - a lot of the calculus for us as well.
21:20 - A lot of, a lot of the linear algebra, which involves, uh, a type of data
21:25 - structure called tensors.
21:27 - So tensors are pretty much major sees.
21:29 - If you're not familiar with those, that's fine.
21:31 - We'll go over them more in the, in the course, but pretty much what we're going
21:34 - to do is we're going to just put everything inside of a tensor so that it's
21:38 - easier for PI torch to work with.
21:40 - So I'm going to go ahead and delete these here.
21:42 - And all we can do is just make our data element.
21:47 - We could, this is going to be the entire text data of the entire wizard of Oz.
21:52 - So we could go ahead and make this, uh, data equals, and we're going to go torch.tensor.
22:02 - And then we're going to go, uh, and the code, we're going to put the text inside of that.
22:07 - So we're going to go ahead and encode, uh, this text right here.
22:10 - And we're going to make sure that we have the right data type, which is a torch.long,
22:15 - uh, data, data type equals torch.long.
22:22 - So this basically means we're just going to have this as a, uh, super long sequence of
22:27 - integers and yeah, let's go see what we can do with this, uh, torch.tensor element right here.
22:35 - So I've just written a little print statement where we can just, uh,
22:39 - print where we can just print out the first a hundred characters or a hundred integers of this data.
22:44 - So it's, it's pretty much the same thing in terms of working with arrays.
22:49 - It's just a different, uh, type of data structure, uh, in the contents, in the context of PyTorch,
22:58 - sort of easier to work with in that way.
22:59 - Uh, PyTorch is just primarily revolved around tensors and modifying them, uh, reshaping,
23:05 - changing dimensionality, multiplying, doing dot products, uh, which I mean,
23:10 - that sounds like a lot, but, uh, we're going to go over some of this stuff later in the course,
23:14 - just about how to do all this math.
23:16 - We're going to actually go over examples on, you know, how to, how to multiply this matrix
23:22 - by this matrix, even if they're not the same shape and even dot prodding, dot
23:27 - producting, that kind of stuff.
23:28 - So next one I'm going to talk about is something called, uh, validation and training splits.
23:34 - So why don't we just, you know, use the entire text document and only train on that
23:39 - entire text corpus.
23:41 - Why don't we train on that?
23:42 - Well, the reason we actually split into training and validation sets, I'm going to show you right here.
23:47 - So we have this giant text corpus.
23:50 - It's a super long text file.
23:51 - Think of it as a, you know, an essay, but a lot of pages.
23:55 - So this is our entire corpus and we make our training set, you know, 80% of it.
24:00 - So maybe this much, and then the other validation is this 20% right here.
24:05 - Okay.
24:06 - So if we were to just train on the entire thing after a certain number of iterations, it would
24:12 - just memorize the entire text piece and it would be able to, you know, simply write it, just write
24:17 - it out, it would have it in the entire thing memorized and it wouldn't really get anything
24:20 - useful out of that.
24:21 - You would only know this document, but what the purpose of language modeling is, is to
24:27 - generate text that's like the training data, and this is exactly why we put it into splits.
24:33 - So if we, if we run our training split right here, it's only going to know 80% of that entire
24:39 - corpus and it's only going to generate on that 80% instead of the entire thing.
24:45 - And then we have our other 20%, which only knows 20% of the entire corpus.
24:49 - So the reason why we do this is to make sure that the generations are unique and not an
24:55 - exact copy of the actual document. We're trying to generate text that's like the document.
25:00 - Like for example, in Andrej Karpathy's lecture, he trains on Shakespearean text, an entire
25:07 - piece of Shakespeare.
25:09 - And the point is to generate a Shakespearean like text, but not exactly what it looked
25:15 - like, not that exact, you know, 40,000 lines or like a few thousand lines of that entire
25:21 - corpus, right? We're trying to generate text that's like it. So that's the entire reason, or
25:25 - at least that's most of the reason why we use train and vowel splits.
25:29 - So you might be wondering, you know, like, why is this even called the Bagram language
25:33 - model? I'm actually going to show you how that works right now.
25:36 - So if we go back to our whiteboard here, I've drawn a little sketch.
25:40 - So if we have this piece of content, the word hello, let's just say it, we don't have to
25:45 - encode it as any integers right now.
25:47 - Right now we're just working with characters. Pretty much we have two, right?
25:52 - So by means two.
25:54 - The by prefix means two.
25:56 - So we're going to we're going to have a Bagram.
25:58 - So given maybe there's nothing before an H in this content.
26:03 - So we just assume that's the start of content and then that's going to point to an H.
26:08 - So H is the most likely to come after the start.
26:12 - And then maybe given an H, we're going to have an E, then given an E, we're going to have
26:18 - an L, then given an L, we're going to have another L, and then L leads to O.
26:22 - Right. So maybe there's going to be some probabilities associated with these.
26:28 - So that's pretty much how it's how it's going to predict right now.
26:31 - It's only going to consider the previous character to predict the next.
26:35 - So we have given this one, we predict the next.
26:38 - So there's two why it's called Bagram language model.
26:40 - So I've known my terrible writing here, but we're actually going to go into how we can
26:46 - train the Bagram language model to do what we want, how we can actually implement this
26:51 - into a neural network, an artificial neural network and train it.
26:55 - So we're going to get into something called block size, which is pretty much just taking
27:01 - a random snippet out of this entire text corpus here, just a small snippet.
27:05 - And we're going to make some predictions and we're going to make some targets out of that.
27:11 - So our block size is just a bunch of encoded characters or integers that we have predictions
27:17 - and targets.
27:19 - So let's say we take a small little size of maybe block size of five.
27:23 - OK, so we have this tiny little tensor of five integers and these are our predictions.
27:30 - So given some context right here, we're going to be predicting these and then we have our
27:38 - targets, which would be offset by one.
27:42 - So notice how here we have a five and then here the five is outside and then this 35 is
27:47 - outside here and now it's inside.
27:48 - So all we're doing is just taking that block from the predictions in order to get the
27:53 - targets. We just offset that by one.
27:56 - So we're going to be accessing the same indices.
27:59 - So index zero is going to be five index zero is going to be sixty seven, right?
28:03 - So sixty seven is following five in the background language model.
28:08 - So that's pretty much all we do.
28:10 - We just look at how much of a difference is that target away from or how much far as the
28:18 - prediction away from the target and then we can optimize for reducing that error.
28:24 - So the most basic Python implementation of this in the character level tokenizers or the
28:32 - character level tokens rather would be just simply this right here.
28:38 - So we would we would take we would take a little snippet random.
28:43 - It would be pretty much just from the start or some some whatever, just some snippet all
28:54 - away from the start of the snippet up to block size.
28:58 - So five ignore my terrible writing again.
29:06 - And then this one would just be.
29:13 - It would just be one up to block size or five.
29:18 - Plus one to be up to six, right?
29:24 - And that's that's pretty much all we do.
29:26 - This is exactly what it's going to look like in the code.
29:28 - So I've written some code here that does exactly what we just talked about in Python.
29:33 - So I've defined this block size equal to eight just so you can kind of see what this looks
29:37 - like on a larger scale, a little bit larger and just what we wrote right there in the
29:43 - Jupyter Notebook, this position zero up to block up to block size and then offset by one.
29:51 - So we make it position one up to block size plus one little offset there.
29:56 - We pretty much just wrote down here X as our productions as and Y as our targets and then
30:01 - just a little for loop to show what the prediction and what the targets are.
30:08 - So this is what this looks like in Python.
30:10 - Great. We can do predictions, but this isn't really scalable yet.
30:15 - This is sequential, right?
30:16 - Sequential is another way of describing what the CPU does.
30:20 - CPU can do a lot of complex operations very quickly, but it only happens sequentially.
30:26 - It's this one and this task and this task and this task, right?
30:30 - But with GPUs, you can do a little bit more simpler task, but very, very quickly or in
30:36 - parallel, so we can do a bunch of very small or not computationally complex computation
30:46 - and a bunch of different little processors that aren't as good, but there's tons of them.
30:52 - So pretty much what we can do is we can take each of these little blocks and then we can
30:57 - stack them and push these to the GPU to scale our training a lot.
31:04 - So I'm going to illustrate that for you right now.
31:08 - So let's just say we have a block.
31:09 - Okay. Block looks like this and we have some.
31:16 - We have some integers in between here.
31:18 - Okay. So this is a block.
31:22 - Okay. Now, if we want to make multiple of these, we're just going to stack them.
31:28 - So we're going to make another one.
31:29 - Another one.
31:36 - Another one.
31:37 - So let's say we have four batches.
31:38 - Okay.
31:39 - Or sorry, four blocks.
31:41 - So we have four different blocks that are just stacked on top of each other and we can
31:47 - represent this as a new hyper parameter called batch size.
31:51 - This is going to tell us how many of these sequences can we actually process in parallel.
31:56 - So the block size is the length of each sequence and the batch size is how many of these are
32:02 - we actually doing at the same time.
32:04 - So this is a really good way to scale language models.
32:07 - And without these, you can't really expect any fast training or good performance at all.
32:13 - So we just went over how we can actually get batches or rather how we can use batches to
32:18 - accelerate the training process.
32:20 - And we can, it just takes one line to do this actually.
32:24 - So all we have to do is call this little function here saying, if CUDA dot torch dot CUDA is
32:31 - available, we'll just check if the GPU is available based on your CUDA installation.
32:38 - And if it's available, like it says, if it's available, we'll set the device to CUDA else CPU.
32:44 - So we're going to go and print out the device here.
32:47 - So that's going to run and we get CUDA.
32:50 - So that means we can use the GPU for a lot of our processing here and while we're here, I'm
32:56 - actually going to move up this hyper parameter block size up to the top block size.
33:01 - And then we're going to use batch size, which is how many blocks we're doing in parallel.
33:07 - And we're just going to make this four for now.
33:09 - So these are our two hyper parameters that are very, very important for training.
33:14 - And you'll see that why these become much more important later when we scale up the data and
33:18 - use more complex mechanisms to train and learn the patterns of the language based on the text that
33:26 - we give it.
33:27 - So if it doesn't work right away, if you're, if the new Jupyter notebook doesn't work right away,
33:31 - I'd recommend just hitting control C to cancel this, hit it a few times, might not work the first.
33:39 - It'll shut down and you just go up Jupyter notebook again and then enter.
33:42 - And then after this is done, you should be able to just restart that and it will work, hopefully.
33:53 - There we go.
33:54 - So I go ahead and restart and clear outputs.
33:59 - And we can run that, see, we get boo.
34:02 - So awesome.
34:04 - Now let's try to do some actual cool PyTorch stuff.
34:09 - So we're gonna go ahead and import torch here and then let's go ahead and try this rand int feature.
34:17 - So you go rand int, we'll do equals torch.randint and then let's say we go minus a hundred to a
34:27 - hundred and then in brackets we go six, just like that.
34:32 - So if we want to print this out here, or we can just go rand int.
34:36 - Or we can just go rand int like that, could run this block first, good, and boom.
34:44 - So we get a tensor type and all of these numbers are, we have, we have six of them.
34:51 - So one, two, three, four, five, six, and they're between negative 100 and 100.
34:56 - So we're going to have to keep this in mind right here when we're getting our random batches from
35:03 - this giant text corpus.
35:04 - So let's try out a new one.
35:06 - Let's just try, we can make tensors.
35:10 - We've done this before.
35:10 - So we could do tensor equals torch.tensor and we could go 0.1, 1.2.
35:24 - Here, I'll just copy and paste one right here.
35:29 - So we do this, boom.
35:31 - And we can just do tensor and we'll get exactly this.
35:38 - So boom, we get a three by two matrix.
35:45 - Now we're going to try a different one called zeros.
35:48 - So zeros is just torch.zeros.
35:52 - And then inside of here, we could just do the dimensions or the shape of this.
35:56 - So two by three, and then we can just do zeros.
36:00 - And then go ahead and run that so we get a two by three of zeros and these are all floating
36:07 - point numbers, by the way.
36:10 - Maybe we could try ones.
36:11 - Now, I know ones is pretty fun ones.
36:13 - So we go torch, torch.ones, it's pretty much the same as zeros.
36:19 - We could just do like maybe three by four and then print that ones out.
36:25 - So we have a three by four of ones.
36:28 - Sweet.
36:28 - So what if we do input equals torch.empty and we can make this two by three.
36:45 - So these are interesting.
36:47 - These are pretty much a bunch of very, either very large or very small numbers.
36:55 - I haven't particularly found a use case for this yet.
36:58 - But just another feature that PyTorch has.
37:01 - We have a range, so we go arrange equals torch.arrange, and we could do like five, for
37:10 - example, just do range.
37:13 - So now we have a tensor just sorted zero or rather starting at zero up to four.
37:21 - So five, just like that.
37:24 - Line space equals torch.line, line space, spelling is weird, two, three, ten, and then
37:40 - steps, for example, equals five.
37:44 - This all makes sense in a second here, go run, and we get a line space.
37:50 - So steps equals five.
37:51 - So we have five different ones, boom, boom, boom, boom, boom.
37:55 - And we go all the way from three to ten.
37:57 - So pretty much getting all of the constant increments from three all the way up to ten
38:02 - over five steps.
38:03 - So you're doing, you're basically adding the same amount every time.
38:07 - So three plus one point seven five is four point seven five plus another one one
38:12 - seven five is six point five and then eight point two five and then ten.
38:15 - Right.
38:15 - So just over five steps, we want to find what that constant increment is.
38:19 - So that's a pretty cool one.
38:23 - And then we have we'll do log space, which is interesting.
38:27 - Log space equals torch.logspace.
38:32 - And then we'll go start.
38:39 - Start equals negative ten and equals ten.
38:47 - These are, these are both start and end.
38:50 - So you can either put these here.
38:52 - You can either put the start with them, start equals, or you don't have to.
38:56 - It's honestly up to you.
38:57 - And then we can put our steps again, steps equals give you five.
39:04 - Let's go ahead and run that, or oops, to put logspace there.
39:10 - So we get that.
39:11 - So we start at one of the negative ten.
39:15 - And then we just do this little increments here.
39:17 - So it goes ten, negative five, zero, plus five, ten, for start five steps.
39:20 - So that's pretty cool.
39:22 - Um, what else do we have here?
39:24 - So we have I, torch.I, I just have all these on my second screen here.
39:30 - So a bunch of examples just written out and we're just kind of visualizing what
39:35 - these can do and maybe you might even have your own creative little sparks of thought
39:40 - that you're going to maybe find something else that you can use these for for your
39:45 - own personal projects or whatever you want to do.
39:47 - So we're just kind of experiment experimenting with these, uh, what we can do
39:51 - with the basics of PyTorch and some of the very basic functions.
39:55 - So, uh, first.I, we go, uh, print this I out here.
40:02 - So we get pretty much just a diagonal line and it's, it's in five.
40:10 - So you get a five by five matrix and yeah, pretty much just, uh, reduced row, uh, each
40:19 - long form.
40:20 - I don't know how to pronounce it, but, uh, that's pretty much what it looks like.
40:24 - So pretty cool stuff.
40:27 - Um, let's see what else we have.
40:30 - We have empty like empty, like torch.empty.
40:39 - Like, uh, a, and then we'll just say maybe make a equal to, we'll make it a torch.empty.
40:53 - And then we can go, uh, two by three and then, uh, data type torch.int 64.
41:06 - So 64 bit integers, uh, and then let's see what happens here.
41:11 - Empty boom.
41:18 - So that's pretty cool.
41:20 - What else do we have?
41:21 - Yes, we can do timing as well.
41:23 - So I'm just going to erase all of these.
41:27 - Uh, you can, I mean, you can scroll back in the video, just look and maybe
41:32 - experiment with these a little bit, try a little bit more than just, you know, what
41:34 - I've done with them, maybe modify them a little bit.
41:37 - Um, but yeah, I'm actually going to delete all of these here.
41:41 - So you just do, and then we can go ahead and do the device equals, uh, CUDA.
41:53 - And we're going to go ahead and switch this over to the CUDA GPT environment.
41:57 - Well, CUDA, if, if torch dot CUDA, uh, underscore is, uh, dot CUDA is available.
42:16 - Uh, and then it helps go CPU print out our device here.
42:21 - Let me run this CUDA suite.
42:28 - So we're going to try to do stuff with, uh, the GPU now compared to the CPU and
42:35 - really see how much of a difference, uh, CUDA or the GPU is going to make in
42:39 - comparison to the CPU when we change, uh, the shape and dimensionality, and we're
42:44 - just doing different, um, experiments with a bunch of different tensors.
42:49 - So in order to actually measure the difference between the GPU and the CPU,
42:54 - uh, I just imported a library called time.
42:56 - So this comes with the operating system or sorry, with, with Python.
43:00 - Uh, you don't have to actually install this manually.
43:03 - So, uh, basically what we do is we whenever we called time dot time, uh,
43:08 - and then brat, uh, parentheses, it will just take, uh, the current time snippet
43:13 - right now, so start time will be like right now and then end time, maybe
43:16 - three seconds later will be, you know, right now, plus three seconds.
43:20 - So if we subtract end time, I start time, we'll get a three second difference.
43:24 - And that would be the total all apps time.
43:27 - And then, uh, this little number here, this four will be, uh, just how
43:32 - many decimal places we have.
43:34 - So I can go ahead and run this here.
43:37 - Time is not defined.
43:38 - Let's run that first.
43:42 - It's going to take, you know, almost no time at all.
43:45 - So we can actually increase this if we want to 10 and then run that again.
43:49 - Again, it's, you know, we're, we're making up pretty much a one by one matrix.
43:53 - So just a, it's just a zero.
43:55 - So, uh, we're not really going to get anything significant from that.
44:00 - Um, but anyways, for, for actually testing the difference between the GPU and the CPU,
44:10 - what we're going to worry about is that iterative process, the process of forward
44:14 - pass and, uh, back propagation through the network, that's primarily what we're trying
44:19 - to optimize for actually pushing all these parameters and all these, um, model
44:24 - weights, uh, to the GPU, isn't really going to be the problem.
44:27 - It'll take maybe a few seconds at most, like maybe 30 seconds to do that.
44:32 - And that's not going to be any time at all in the entire training process.
44:36 - So what we want to do is just see, you know, which is better numpy on the CPU
44:41 - or, uh, torch using CUDA on the GPU.
44:46 - So I have some code for that right here.
44:48 - So we're going to initialize a bunch of matrices here.
44:53 - So, or sorry, tensors, and we have, uh, just basically random ones.
44:58 - So we have a 10,000 by 10,000, uh, all around all random floating point numbers.
45:03 - And then we're going to push these to the GPU and we have two of these.
45:06 - And then same thing for numpy.
45:08 - So in order to actually multiply matrices with PyTorch, we need to use this at symbol here.
45:15 - So we multiply these and we get this new, uh, we get this new, uh, random tensor.
45:21 - And then we stop it.
45:23 - And then we do the same thing over here, except we use, uh, numpy.multiply.
45:27 - So if I go ahead and run these, it's going to take a few seconds to initialize these
45:33 - and, or not even a few seconds.
45:35 - And then we have, see, look at that.
45:38 - So for the, uh, GPU, it took a little while to do that.
45:44 - And then for the CPU, it didn't take as long.
45:47 - So this is because there's the shape of these matrices are not really that big.
45:54 - They're just two dimensional, right?
45:57 - So it's, see, this is something that the CPU can do very quickly, because
46:01 - there's not that much to do, but let's say we want to bump it up a notch.
46:05 - So if we go to 100, 100, 100, and then maybe we'll throw in another 100 there.
46:13 - Hopefully that works.
46:14 - And then we can do, uh, we'll just do the same thing.
46:17 - So let's paste this.
46:23 - So now if we try to run this again.
46:25 - You'll see that the GPU actually took less than half the time that the CPU did.
46:34 - And this is because, uh, there's, you know, a lot more going on here.
46:39 - There's a lot more simple, uh, multiplication to do.
46:44 - So the reason why this is so significant is because when we have, you know,
46:48 - millions or billions of parameters in our language model, we can do that.
46:52 - Millions or billions of parameters in our language model.
46:56 - We're not going to be doing, uh, very complex operations between all these
47:01 - tensors, they're going to be very similar to what we saw in here.
47:04 - The, the dimensionality and shape is going to be very similar to
47:09 - what we're seeing right now, you know, maybe three or four dimensions.
47:12 - Uh, and it's going to be very easy for our GPU to do this.
47:16 - They're not complex tasks that we need the CPU to do.
47:19 - They're not very hard at all.
47:20 - So when we, uh, give this task to parallel processing, it's going to be a ton quicker.
47:29 - So you're going to see why this matters later in the course.
47:32 - You're going to see this with, uh, some of the hyper parameters we're going to use,
47:35 - which I'm not going to get into quite yet, but, uh, over the next little bit,
47:41 - you're going to see why the GPU is going to matter a lot for, uh,
47:45 - increasing the efficiency of that iterative process.
47:49 - So this is great.
47:50 - Now, you know, a little bit more about why we use, uh, the GPU instead
47:54 - of the CPU for, uh, training efficiency.
47:58 - So there's actually another term that we can use called a percentage percentage time.
48:03 - I don't know if that's exactly how you're supposed to call it, but, uh, that's what
48:07 - it is and pretty much what it'll do is time, how long it takes to execute a block.
48:12 - So we can see here there's CPU times, uh, zero nanoseconds.
48:17 - The N is for nano, uh, billionth of a second is a nanosecond and then wall time.
48:22 - So CPU time is how long it takes to, uh, execute on the CPU, the time that it's
48:29 - doing operations for, and then the wall time would be, uh, how long it actually
48:35 - takes like in real time, how long do you have to wait?
48:38 - Do you have to wait until it's finished?
48:40 - So the only thing that the CPU CPU time doesn't include is waiting.
48:45 - So in an entire process, there's going to be some operations and
48:49 - there's going to be some waiting wall time is going to have them, uh, both of
48:52 - those and CPU time is just the execution.
48:56 - So let's go ahead and continue with, uh, some of the basic PyTorch functions.
49:02 - So I've written some stuff down here.
49:05 - So we're going to go over, uh, torch.stack torch.multinomial torch.trill, uh,
49:13 - triu, I don't think that's how you pronounce it, but we'll get into that
49:16 - more, uh, transposing, uh, linear, concatenating and the softmax function.
49:24 - So let's first start off here with the torch.multinomial.
49:28 - So this is essentially a probability distribution based on the index that you
49:33 - give it. So we have probabilities here.
49:36 - We say 0.1 and 0.9.
49:38 - These numbers have to add up to one to make a hundred percent, a hundred
49:42 - percent is one, one hole.
49:44 - So I have 10% and 90%.
49:47 - This is an index zero.
49:49 - So there's a 10% chance that we're going to get a zero and a 90% chance that we're
49:54 - going to get a one.
49:55 - So if I go ahead and run these up here,
50:05 - give this a second to do its thing.
50:07 - So we, you can see that, uh, in the end, we have our num sample set to 10.
50:11 - So it's going to give us 10 of these one, two, three, four, five, six, seven, nine,
50:14 - 10, and all of them are ones if we run it again and make it slightly different
50:19 - results. So now we have some zeros in there, but the zeros have a very low
50:24 - probability of happening, as a matter of fact, exactly a 10% probability of
50:28 - happening. So, uh, we're going to use this later in, uh, predicting what
50:34 - board is going to come next.
50:35 - Let's move on to torch, got torch.cat or short for torch.concatenate.
50:41 - So this will essentially concatenate two tensors into one.
50:44 - So I initialize this tensor here, torch.tensor, uh, one, two, three, four, it's
50:49 - one dimensional and we have another tensor here that just contains five.
50:54 - So if we concatenate one, two, three, four, and five, then we get, uh, one, two,
51:00 - three, four, five, you just combine them together.
51:03 - And, uh, this is what'll come out in the end.
51:05 - So you run that one, two, three, four, five.
51:08 - Perfect.
51:09 - So this is going to, we're going to actually use this when we're generating,
51:13 - when we're generating text, given a context, so it's going to start, uh, it's
51:18 - going to start from zero, we're going to use our probability distribution to
51:22 - pick the first one and then, uh, based on the first one, we're going to, uh, you
51:29 - know, we're going to, we're going to predict the next character and then once
51:33 - we have predicted that, we're going to concatenate, uh, the new one with the
51:39 - ones that we've already predicted.
51:41 - So we have this, maybe like a hundred characters over here, and then the
51:44 - next character that we're predicting is over here, we just concatenate these.
51:47 - And by the end, we will have all of the, uh, integers that we've predicted.
51:54 - So next up we have torch.trill.
51:56 - And what this stands for with a trill stands for is, uh, triangle lower.
52:01 - So it's going to be in a sort of a triangle formation like this diagonal.
52:06 - It's going to be going from, uh, top left to bottom right.
52:11 - And so you're going to see a little bit more why later in this course, but this
52:15 - is important because when you're actually trying to predict, uh, integers
52:20 - or, uh, next tokens in the sequence, you have, you only know what's in the
52:26 - current history, we're trying to predict the future.
52:29 - So giving the answers in the future, uh, isn't what we want to do at all.
52:34 - So maybe we've just predicted one and the rest of them we haven't predicted yet.
52:38 - So we set all these to zero and then we predicted another one and these are still zero.
52:43 - So these are talking to each other in history.
52:45 - And as, and as our predictions add up, we're going to have to
52:49 - uh, predictions add up.
52:51 - Uh, we have more and more history, uh, to look back to and less future, right?
52:58 - Um, basically the premise of this is just making sure we can't communicate with the answer.
53:04 - We can't predict while knowing what the answer is.
53:07 - Just like when you write an exam, you can't use the answer sheet.
53:11 - They don't give you the answer sheet.
53:13 - So you have to know based on your, uh, history of knowledge, which answers to
53:18 - predict, and that's all, all that's going on here.
53:22 - And we have, I mean, you could probably guess this triangle upper.
53:26 - So we have all the upper ones.
53:28 - These are, you know, lower on the lower side and then these are on the upper side.
53:32 - So same concept there.
53:34 - And then we have a mask fill.
53:37 - So this one's going to be very important later because in order to actually get to
53:43 - this point, all we do is we just exponentiate every element in here.
53:47 - So if you exponentiate zero, if you exponentiate zero, it'll become one.
53:53 - If you exponentiate negative infinity, it'll become zero.
53:57 - All that's going on here is we're doing, uh, approximately 2.71.
54:03 - And this is a constant that we use in, uh, the, the dot exp function.
54:08 - And then we're putting this to whatever, uh, power is, uh, in that current slot.
54:14 - So we have a zero here.
54:15 - So 2.71 to the zeroth is equal to one, 2.71 to the one is equal to 2.71.
54:27 - And then, uh, 2.71 to the negative, uh, infinity is, of course, zero.
54:40 - So that's pretty much how we get from this to this.
54:43 - And, uh, we're, we're just, we're simply just masking these over.
54:47 - So that's great.
54:49 - And I sort of showcase what, uh, the exp does.
54:53 - We're just using this one right here.
54:55 - We're using this, this output and we're just plugging it into here.
54:58 - So, uh, it'll go from negative infinity to zero and then zero to one.
55:04 - So that's how we get from here to here.
55:07 - Now we have, uh, transposing.
55:10 - So transposing is when we sort of flip or swap the dimensions of a tensor.
55:16 - So in this case, I initialize a torch.zero tensor with dimensions two by three by four.
55:23 - And we can use the transpose function to essentially flip, uh, any dimensions that
55:28 - we want.
55:29 - So what we're doing is we're looking at the zero with, as it sounds weird, does
55:35 - not say first dimension, but we're pretty much swapping the zero with position.
55:39 - With the second, so zero, one, two, we're swapping this one with this one.
55:44 - So the end result, like you would probably guess the shape of this is going
55:48 - to be four, three, two instead of two, three, four.
55:50 - So you kind of just take a look at this and see, you know, which ones are being
55:54 - flipped and, uh, those are the dimensions and that's the output.
55:58 - So hopefully that makes sense.
56:00 - Next up we have torch.stack and this is what we're actually going to go.
56:05 - We're going to, we're going to do more of this.
56:07 - We're actually going to use torch.stack stack very shortly here when we're, uh,
56:13 - getting our batches.
56:15 - So remember before when I was talking about batch size and how we take a bunch
56:20 - of these blocks together and we just stack them, a giant, uh, a giant, uh,
56:24 - length of integers or tokens, and all we're doing is we're just stacking them
56:30 - together in blocks or to make a batch.
56:33 - So that's pretty much what we're going to end up doing and that's what torch.stack
56:37 - does so we can take something that's, um, maybe one dimensional and then we can
56:43 - stack it to make it two dimensional and we can take something that's two
56:46 - dimensional and stack it a bunch of times to make it three dimensional.
56:52 - Or we can say three dimensional.
56:54 - For example, we have a bunch of cubes and we stack those on top of each other.
56:57 - Now it's four dimensional.
56:59 - So hopefully that makes sense.
57:00 - All we're doing is we're just, uh, passing in each tensor that we're going to stack
57:03 - in order, so this is our little output here and that's pretty much all it is.
57:09 - The next function that's going to be really important, uh, for our model, and
57:13 - we're going to be using this the entire time, uh, from start to finish, it's
57:16 - really important it's called the nn.linear function.
57:20 - So it is a pretty much a function of the nn.module and this is really important
57:27 - because you're going to see later on, nn.module is, it contains anything that
57:32 - has, uh, learnable parameters.
57:35 - So when we do a transformation to something, when we apply a weight and a
57:38 - bias, in this case, it'll be false, but, uh, pretty much when we apply a weight or
57:43 - a bias, uh, under nn.module, it will learn those and it'll become better and better.
57:50 - And it'll basically train based on, uh, how accurate those are.
57:54 - And, uh, how close certain parameters bring it to the desired output.
57:59 - So pretty much anything with nn.linear, uh, is going to be very important
58:04 - and it's going to be learnable.
58:06 - So we can see over here, um, this is the torch.nn, uh, little site here on the docs.
58:12 - So we have containers, a bunch of different layers, like activations, layers,
58:16 - uh, pretty much just layers.
58:19 - That's all it is.
58:21 - And so these are, these are important.
58:23 - We're going to, we're basically going to learn from these and you're going to see
58:27 - why we're going to use something called keys and values, uh, keys, values, and
58:32 - queers later on, you know, see why those are important.
58:34 - But, uh, if that doesn't make sense yet, how many, let me illustrate value for
58:38 - you right now.
58:39 - So I drew this out here.
58:41 - So if we look back at our examples, we have a, we make, we initialize a tensor.
58:49 - We initialize a tensor, um, it's 10, 10 and 10.
58:54 - What we're going to do is we're going to do a linear transformation.
58:56 - This linear stands for linear transformation.
58:58 - So pretty much we're just going to apply a weight and a bias through
59:03 - each of these layers here.
59:04 - So we have an input and we have an output.
59:07 - X is our input, Y is our output.
59:09 - And this is of size three and this is of size three.
59:13 - So pretty much we just need to make sure that these are lining up and, uh, for
59:18 - more context, the nn.sequential is sort of built off nn.linear.
59:26 - So if we go ahead and search that up right now, this'll make sense in a second.
59:32 - This is also some good prerequisite knowledge in general for machine learning.
59:37 - So let's see, and then dot sequential, uh, doesn't show it here, but pretty much,
59:43 - um, if you have, let's say two, you have two input neurons and maybe you have one
59:52 - output neuron, okay?
59:53 - You have a bunch of hidden layers in between here.
59:56 - Let's say we have maybe one, two, three, four, and then one, two, three.
60:02 - So pretty much you need to make sure that the inputs, uh, aligns with this hidden
60:09 - layer, this hidden layer aligns with this one and this one aligns with this one.
60:12 - So you're going to have, uh, a transformation of two to four, so two, four,
60:20 - and then this one's going to be, um, four to three, four to three, and then you're
60:26 - going to have a final one, so this is two to four right here, four to three here,
60:30 - and then this final one, it's going to be three to one.
60:33 - So you pretty much just need to make sure that these are lining up.
60:38 - So we can see that we have two, four, and then this four is carried on from this.
60:42 - Uh, output here, and pretty much this will just make sure that our shapes are
60:47 - consistent and of course, if they aren't consistent, if the shapes don't work out,
60:51 - the math simply won't work.
60:52 - So when you can make sure that our shapes are consistent, uh, if that didn't make
60:56 - sense, I know I'm not like super great at explaining, uh, architecture of
60:59 - neural nets, but, uh, if you're really interested, I could use a chat GPT of
61:04 - course, and that's a really good learning resource, a chat GPT going on to get up
61:08 - discussions maybe, or just looking at documentation, uh, and if you're not good
61:13 - at reading documentation, then you could take maybe some, some little keywords
61:19 - from here, like, uh, a sequential container.
61:22 - Well, what is a sequential container?
61:24 - You can ask chat GPT those types of questions and just sort of revert
61:28 - engineer to the documentation and figured things out step by step.
61:31 - It's really hard to know what you're doing if you don't know all of the math
61:36 - and all of the functions that are going on.
61:38 - You don't need to memorize them, but while you're working with them, it is
61:41 - important to understand what they're really doing behind the scenes, especially
61:45 - if you want to make, uh, an efficient and popular working neural net.
61:50 - So, uh, that's that and pretty much what's going to happen here with these
61:55 - linears is linear layers is we're just going to simply transform from one to
62:02 - the other input to output, no hidden layers, and we're just going to be able
62:05 - to learn best parameters for doing that.
62:07 - And you're going to see why that's useful later.
62:09 - Um, now we have the softmax function.
62:13 - So that sounds scary and the softmax function isn't actually what it sounds
62:17 - like at all.
62:18 - Um, let me illustrate that 40 right now.
62:21 - So let's go ahead and change the color here.
62:26 - So let's say we have a array.
62:28 - We have a one, two, three, let's move.
62:32 - We'll make them floating point numbers, 2.0, 3.0, et cetera, right?
62:36 - Floating points, whatever.
62:37 - So pretty much if we put, if we put this into the softmax function, what's going
62:45 - to happen is we're going to exponentiate each of these and we're going to divide
62:50 - them by the sum of all of these exponentiated.
62:53 - So pretty much what's going to happen, let's say we exponentiate one.
62:56 - Okay.
62:57 - So what that's going to do is it's going to do, um, this is what it's going to look
63:02 - like in code.
63:02 - It's going to go one dot e X P. And I think I talked about this up here.
63:08 - Um, this is exponentiating when we have 2.71 to the power of whatever
63:13 - number we're exponentiating.
63:15 - So if we have this one, we're going to exponentiate that and that's going to
63:19 - give us, uh, it's going to give us 2.71 and we have this two here and that's
63:25 - going to give us, uh, whatever, uh, whatever two is exponentiated 2.71 power of two.
63:34 - Okay.
63:35 - So we're going to get 7.34, so we're going to get 7.34.
63:40 - Door by writing, it's terrible, uh, 2.71 to 3 cubed, so 19.9.
63:48 - So pretty much what's going to happen is we, we can rearrange this in a new array.
63:56 - Uh, 7.34 and 19.9.
64:01 - So if we add all these up together, we have all these up together.
64:05 - We're going to get 2.71 plus this one.
64:06 - So let's do this math real quick.
64:08 - I'm just going to walk you through this to help you understand what the softmax
64:11 - function is doing.
64:12 - 7.34 plus 19.9.
64:20 - That's going to give us a total of 29.95.
64:23 - Great.
64:24 - 29.95.
64:29 - So all we do is we just divide each of these, uh, elements by the total.
64:36 - So 2.71 divided by this is going to give us maybe X.
64:40 - Okay. And we do 7.34 divided by this.
64:43 - It's going to give us Y and then we have 19.9 by this.
64:46 - It's going to give us, uh, Z.
64:49 - So pretty much you're going to exponentiate all of these.
64:54 - You're going to add them together to create a total, and then you're going to
64:58 - divide each of those exponentiated elements by the exponentiated total.
65:02 - So after that, this X right here is just, we're just going to wrap these again.
65:08 - And all this softmax function is doing is it's converting this 1, 2, 3 to X, Y,
65:16 - Z. That's all it's doing.
65:18 - Um, and yeah, it's, it's, it's not really, it's not really crazy.
65:22 - Uh, there's, there's a weird formula for it.
65:25 - Uh, softmax, softmax function.
65:30 - So if you go on Wikipedia, uh, you're going to crap yourself because there's a lot of
65:34 - terms in here and a lot of math that's, you know, above the high school level.
65:39 - But, uh, yeah, like this formula here, I believe this is what it is or standard
65:44 - unit, softmax function, there you go.
65:46 - So pretty much this is what it does and there's your easy explanation of what it
65:50 - does, so, uh, you're going to see why this is useful later, but it's just
65:55 - important to know what's going on so that, uh, you won't lag behind later in the
65:59 - course when this background knowledge becomes important.
66:02 - So if we go over a little example of that, of the softmax function in code, uh, it
66:07 - looks like this right here.
66:08 - So we import a torsa and a dot functional as F, F short for functional, and we pretty
66:14 - much just do F dot softmax and then plug in a tensor and, uh, what we want the, uh,
66:20 - dimension to be the output dimension.
66:22 - So if we plug this into here and we print it out, I'm going to go and print it out.
66:27 - It's going to take a second, not a torch, it's not defined, so let's run this from
66:34 - the top here, boom, and let's try that again, boom, there we go.
66:41 - So if you took all those values, let's actually do this again from scratch.
66:46 - So if we do 2.71, 2.71 divided by 29.95, we get 2.71 divided by 29.95.
66:56 - We get 0.09, 0.09, good.
67:01 - Uh, and then if we do 7.34 divided by 29.95, we get 0.245, 0.245.
67:14 - Well, it's kind of close, um, really close actually, and then 66.52.
67:20 - So if we go, uh, what was that last one there?
67:23 - 19.9, 19.9 divided by 29.95, 66.4, so 66.5, it's pretty close.
67:35 - Uh, again, we're rounding, so it's not perfectly, uh, it's not perfectly accurate,
67:41 - but as you can see, they're very close.
67:43 - And for, you know, only having two decimal places, uh, we did pretty good.
67:47 - So that's just sort of illustrating what the softmax function does
67:51 - and what it looks like in code.
67:52 - We have this, uh, sort of shape here.
67:55 - Zero dimensions means, uh, we just take, you know, it's just kind of a straight line.
68:01 - It's just, just like that.
68:03 - Um, so now we're going to go over embeddings and I'm not actually,
68:08 - I don't have any code for this yet.
68:10 - We're going to figure this out step-by-step with chat CPT because I want to
68:13 - show you guys, uh, sort of the skills and what it takes to reverse engineer
68:19 - an idea or function or just understand how something works in general in machine learning.
68:23 - So if we pop in a chat CPT here, uh, we say, what is an n dot embedding?
68:33 - Oh, an n dot, let me type m bedding, n and n embedding, class in the PyTorch library.
68:45 - Okay. Actual language processing max, uh, maps each discrete input
68:49 - to a dense vector representation.
68:52 - Okay.
68:53 - How does this work?
68:53 - Let's see.
68:54 - So we have some vocab, so that's probably our vocabulary size.
68:58 - So I think we, we talked about that earlier, vocabulary size, how many characters,
69:03 - how many unique characters are actually in our dataset?
69:05 - That's the vocabulary size.
69:07 - And then some embedding dimension here, which is a hyper parameter.
69:10 - So let's see, this doesn't quite make sense to me yet.
69:15 - So maybe I want to learn, what does this actually look like?
69:19 - Can you explain this to a, uh, maybe an eighth grader and provide a visualization?
69:32 - Certainly.
69:33 - Okay.
69:33 - Certainly. Okay. Little secret codes that represent the meaning of the words.
69:40 - Okay.
69:41 - That helps.
69:43 - So if we have cat, okay.
69:45 - So cat, cat's a word.
69:47 - So maybe we want to know what it would look like on a character level.
69:52 - What about on a character level instead of the word level?
70:03 - So it's probably going to look very similar.
70:05 - We have this little vector here, storing some information about whatever this is.
70:11 - So a, it means this here.
70:13 - Okay.
70:13 - So a is your 0.2 and this is really useful.
70:18 - So we've pretty much just learned what embedding vectors does.
70:21 - And if you haven't kept up with this, pretty much what they'll do is they'll store some
70:26 - vector of information about this character.
70:30 - And we don't even know what each of these elements mean.
70:34 - We don't know what they mean.
70:35 - This could be maybe positivity or should be the start of a word, or it could be any
70:41 - piece of information, maybe something we can't even comprehend yet.
70:44 - But the point is if we actually give them vectors and, uh, we feed these into a
70:50 - network and, uh, learn because as we saw before, nn.embedding right here is a part
71:00 - of the nn.module.
71:02 - So these are learnable parameters, which is great.
71:05 - So it's actually going to learn the importance of each letter and it's going
71:10 - to be able to produce some amazing results.
71:11 - So in short, uh, the embedding vectors are essentially a vector or a numerical
71:17 - representation of the sentiment of a letter.
71:21 - In our case, it's character level, not subword, not word.
71:25 - It's character level.
71:25 - So it's going to represent some meaning about those.
71:29 - So that's what embedding vectors are.
71:31 - Let's go figure out how they work in code.
71:33 - We have this little, uh, character level embedding vector and it contains a list.
71:39 - There's five elements in here.
71:40 - One, two, three, four, five, and it's by the vocab size.
71:46 - So we have all of our vocabulary by, uh, the length of each embedding vector.
71:51 - So this actually makes sense because our vocab size by the embedding dimension,
71:57 - which is, uh, how much information is actually being stored in each of these
72:01 - characters.
72:02 - So this now is very easy to understand.
72:04 - I'm just going to copy this code from here and I'm going to paste it down here
72:10 - and just get rid of the, uh, torch torch dot n because we already
72:14 - initialized that at both.
72:17 - So if we just run this, actually let's turn that down to maybe a thousand
72:22 - characters, let's try that out.
72:25 - And it's not defined.
72:27 - Oh, we did not initialize it.
72:40 - So let's go back down here and look at that.
72:43 - So, uh, this dot shape is going to essentially show the shape of it this much
72:48 - by, uh, by this much.
72:50 - Uh, so it's four by a hundred and yeah, so we can, we can work with these and we
72:56 - can store stuff about characters in them.
72:59 - And, uh, you're going to see this in the next lecture, how we actually use
73:04 - embedding vectors.
73:05 - So no need to worry if a lot of this doesn't make sense yet.
73:08 - That's fine.
73:09 - Uh, you're going to learn a little bit more about how we use these over the
73:12 - course, you're going to get more confident with using them, uh, even in
73:15 - your own projects.
73:16 - So don't, don't stress about it too much right now.
73:18 - Uh, embeddings are pretty tricky at first to learn.
73:22 - So don't worry about that too much, but there are a few more things I want to
73:26 - go over, uh, just to get us prepared for some of the linear algebra and matrix
73:32 - multiplication in particular that we're going to be doing, uh, in neural networks.
73:36 - So if we have, uh, I remember before we pulled out this, a little sketch of, uh,
73:43 - this is actually called a multilayer perceptron, but people like to call it
73:46 - a neural network because it's easier to say, but that's the architecture of
73:50 - this, a multilayer perceptron.
73:52 - Um, but pretty much what's happening is we have a little input here and
73:56 - we have a white matrix.
73:58 - So white matrix is, looks like this, it's like this and we have some, uh,
74:02 - we have some values in between X one, Y one, and maybe Z, Z one, so a bunch of,
74:12 - uh, weights and maybe biases too, that we add to it.
74:16 - So the tricky part is how do we actually multiply our input by this weight matrix?
74:23 - We're just doing one matrix times another.
74:25 - Well, that's called matrix multiplication and I'm going to show you how to do that
74:29 - right now.
74:29 - So first off, we have to learn something called dot products.
74:34 - So dot products are actually pretty easy and you might've actually done them before.
74:39 - So let's say we go ahead and take, uh, we go ahead and take this array here.
74:44 - We go, um, one, two, three, that's going to be what a is.
74:50 - And then we have, um, four or five, six.
74:54 - So if we want to find the dot product between these two, uh,
74:58 - all we have to do is simply take the index of both of these, uh,
75:02 - the first ones and the second ones, the third ones, multiply them together and
75:06 - then add, so we're going to go ahead and do, uh, one, multiply four, one times
75:13 - four, and then add it to, uh, two times five and then add it to three times six.
75:22 - So, uh, one times four is four, two times five is 10, three times six is 18.
75:28 - So we're going to go ahead and add these up.
75:29 - We get 14 plus 18, I believe is 32.
75:33 - So, uh, the dot product of this is going to be 32 and that's pretty much
75:40 - how simple dot products are.
75:41 - It's just taking each index of both of these arrays, uh, multiplying them
75:46 - together and then, uh, adding all of these.
75:49 - Um, products up as a dot product.
75:52 - So we actually need dot products for matrix multiplication.
75:56 - So let's go ahead and jump into that right now.
75:58 - So I'm just going to create, uh, two matrices that are going to be
76:01 - pretty easy to work with.
76:03 - So let's say we have a, and I have one matrix over here.
76:09 - It's going to be one, two, three, four, five, six, seven, eight, nine,
76:16 - four, five, and six, uh, this is going to be equal to a, and then b is
76:25 - going to be another matrix.
76:27 - So we're going to have, uh, seven, eight, nine, 10, 11, 12, ignore my terrible
76:36 - writing, um, pretty much what we do is to multiply these together.
76:42 - First, we need to make sure that they, they can multiply together.
76:45 - So we need to take a look at the amount of rows and columns that these have.
76:49 - So this one right here is three rows, one, two, three, three rows and two columns.
76:54 - So this is going to be a three by two matrix.
76:58 - And this one has two rows and three columns.
77:01 - So it's a two by three matrix.
77:04 - So all we have to make sure that if we're multiplying, uh, a dot product with
77:11 - B, and this is the PI torch syntax for multiplying matrices, if we're
77:16 - multiplying a by B, then we have to make sure, uh, the following is true.
77:21 - So if we do three by two and then, uh, dot product with two times three, we
77:29 - have to make sure that these two, uh, inner values are the same.
77:34 - So two is equal to two.
77:35 - So we cross these out and then the ones that we have left over are three by three.
77:39 - So the resulting matrix would be a three by three.
77:43 - However, if you had like a three by four times a five by, uh, five by one, that
77:52 - doesn't work because these values aren't the same.
77:55 - So these two matrices couldn't multiply.
77:58 - And, uh, sometimes you actually have to flip these to make them work.
78:02 - So maybe we, we change this value here to a three.
78:06 - We change this value to a three in this order.
78:10 - They do not multiply, but if we switch them around, we have a, uh, we have a three
78:17 - by five with a, uh, three by, or sorry, uh, five by three, sorry, five by three
78:26 - with a three by four.
78:28 - So these two numbers are the same that works and we're resulting matrix is a five by four.
78:32 - So, uh, that's how you make sure that two matrices are compatible.
78:37 - So now to actually multiply these together, what we're going to do, I'm going to make
78:42 - a new line here, so we're going to rewrite these.
78:46 - Uh, now we don't have to rewrite them.
78:48 - Let's just cross that out here.
78:50 - So pretty much what we have to do is we have to take, uh, these two and dot product
78:59 - with these two, and then once we're done that, uh, we do the same with these and these,
79:05 - these and these.
79:07 - So we start with, um, the first, the first row in the A matrix and we iterate through
79:15 - all of the columns in the B matrix.
79:18 - And then after we're done that, we just go to the next row in the A matrix and then
79:21 - center, right?
79:22 - So let's go ahead and do this right now.
79:24 - That probably sounds confusing to start, but let me just illustrate this.
79:26 - Uh, how this sort of works right here.
79:29 - So we have our, uh, one times, uh, our one times seven plus two times 10.
79:38 - So one times seven plus two times 10, and this is equal to 27.
79:48 - So that's the first dot product of, uh, one and two and seven and 10.
79:54 - So what this is actually going to look like in our new matrix, we'll go ahead and write
79:57 - this out here.
79:58 - So this is our new matrix here, this 27, he's going to go right here, let's continue.
80:07 - So next up, we're going to do, uh, one and two and then eight and 11, one, uh, one
80:17 - times eight plus two or sorry, uh, two and 11, so one times eight is eight and then two
80:34 - times 11 is 22.
80:35 - So our result here is 30 and 30 is just going to go right here.
80:41 - So 27 30, and you can see how this is going to work, right?
80:44 - So we are in our first, uh, in our first row of a, we're going to get the first row of
80:52 - this resulting matrix.
80:54 - So let's go ahead and do the rest here.
80:58 - So we have a one and two and then nine and 12 times nine, two times 12 times nine, two
81:12 - times 12 is 24.
81:13 - So if we do, uh, that's like 33, I believe.
81:17 - So 33, and we can go ahead and write that here.
81:23 - So now let's move on to the next wave of three and four, uh, three, three and four dot
81:33 - product was seven and 10.
81:34 - So, uh, three will multiply seven and then we're going to go ahead and add that to four
81:44 - times 10, three times seven, uh, three times seven is 21 and then four times 10 is 40.
81:54 - So we're going to get 47 is our output there so we can go ahead and write 47 right there.
82:01 - Our next one is going to be, uh, three and four dot product with eight and 11.
82:10 - So eight plus, uh, four times 11.
82:20 - Perfect.
82:21 - So we get three times eight is 24 and then, uh, plus 44.
82:27 - So 24 plus 44, that's 68.
82:32 - So we get 68 and we can go ahead and write that here.
82:39 - So next up we have three and four and nine and 12.
82:43 - So three times nine is 27, uh, and then four times 12.
82:57 - So let's just, let's just do that.
82:59 - I'm not doing that in my head.
83:01 - Uh, 27 plus was four times 12.
83:03 - So that's 48, 27 plus 48 gives us 75.
83:07 - Let's go to write our 75 here.
83:14 - Then we can go ahead and slide down to this row since we're done, uh, since we're done
83:18 - that, and then we go five, uh, five and six dot product was seven and 10.
83:23 - So our result from this, uh, five times seven is 35 and then six times 10 is 60.
83:42 - So we're going to get 95.
83:45 - We can go ahead and write our 95 here.
83:47 - And then, uh, uh, five and six dot product with eight and 11.
84:06 - So five times eight is 40 and then six times 11 is 66.
84:10 - So we get a 104 and then the last one.
84:22 - So five and six dot product with nine and 12.
84:25 - So five, uh, five times nine is 45 and then six times 12 is, uh, what six times
84:42 - 12, 72, I think so, uh, six times 12, 72.
84:48 - Yeah.
84:48 - So 45 plus 72, 117, and that is how you do a, uh, three by two matrix and a two by
85:02 - three matrix multiplying them together.
85:05 - So, uh, the result would be, uh, C equals that.
85:12 - So as you can see, it takes a lot of steps that took actually quite a bit of
85:17 - time compared to a lot of the other stuff I've covered in this video so far.
85:21 - So you can see how it's really important to get computers to do this for us and
85:26 - especially to, uh, scale this on a GPU.
85:30 - So I'm going to keep emphasizing that point more and more to have the GPU is
85:34 - very important for scaling your training, but pretty much that's how you do a dot
85:39 - products and matrix multiplication.
85:41 - So I actually realized I messed up a little bit on the math there.
85:44 - So this hundred four, uh, that's actually 106.
85:48 - So I messed up there, uh, if you caught that, uh, good job, but pretty much this
85:55 - is what this looks like in three lines of code.
85:58 - So all of this up here that we just covered, all of this is in three lines.
86:05 - So we initialize an A tensor and a B tensor.
86:08 - Uh, each one of these is a row.
86:10 - Each one of these is a row and it'll pretty much, uh, multiply these together.
86:15 - So this at symbol, this is a shorthand, how you multiply two matrices in
86:21 - PyTorch together, uh, another way to do this is to use the torch dot matrix
86:28 - multiply function or matmul for short, and then you can do A and B.
86:33 - So these will print literally the same thing.
86:38 - Look at that.
86:38 - So I'm not too sure on the differences between them.
86:42 - I use, uh, A at B for short, but, uh, if you really want to know, just, you know,
86:48 - take a look at the documentation or as chat CPT one of the two and, uh, should
86:53 - be able to get an answer from that, but I'm going to move on to something that
86:58 - we want to watch out for, especially when we're doing our, uh, matrix
87:03 - multiplication in our networks.
87:05 - So where's our network here?
87:07 - Where's our network here?
87:09 - Imagine we have, uh, we have some matrix, some matrix A and, uh, every element in
87:17 - this matrix is a floating point number.
87:19 - So, uh, if it's like a one, it would be like one dot zero or something, or
87:24 - just like a one dot, that's what it would look like as a floating point number.
87:28 - But if it were an integer, say B is full of ones with integers, it would just be a one.
87:32 - There wouldn't be any decimals, zero, zero, et cetera, right?
87:35 - It would just be one.
87:37 - So in PI torch, you cannot actually multiply, uh, integers and floating
87:42 - point numbers because they're not the same data type.
87:45 - So I showcased this right here.
87:48 - Uh, we have an int 64.
87:50 - So, uh, type of it is an integer and a float 32, uh, 64 and 32 don't mean anything.
87:56 - All we have to know is an integer and floating point number.
87:59 - So I've initialized a, uh, torch.randint, I covered above and set above here.
88:12 - And, uh, maybe not anyways, this pretty much does a torch.randint is going.
88:24 - Uh, the first parameter here is anything.
88:27 - It's pretty much your range.
88:29 - So I could do like zero to five, or I could just do like one.
88:34 - So it'll do zero up to one and then, uh, your shape of the matrix that it generates.
88:40 - So I said it's a random int, so that means it's going to generate a
88:44 - tensor with the data type integer, uh, 64.
88:49 - So we have a three by two, and then I initialize, uh, another random, uh, key,
88:57 - uh, key detail here.
88:58 - We don't have the, uh, int suffix, so this just generates floating point numbers.
89:05 - And if we actually return the types of each of these, so if I print, um, int 64.d
89:14 - type, and then float 32.d type and save that, I'm just going to comment this out
89:22 - for now, uh, we get a, uh, int 64 and float 32.
89:29 - So if we just try to multiply these together, try to multiply these together.
89:37 - Expected scalar type long, but found float.
89:40 - So long is pretty much when you have a sequence of integers and float is, of
89:45 - course, you have the decimal place, so you can actually multiply this together.
89:49 - So pretty much what you can do is, uh, cast the float method on this.
89:56 - If you just do dot float and then, uh, parentheses and then run this, uh, it'll
90:02 - actually work so you can cast, uh, integers to floats and then I think there's
90:08 - a way you can cast floats to integers, but it has some rounding in there.
90:11 - So probably not the best for, uh, input and weight matrix multiplication, but yeah,
90:18 - pretty much if you're doing any weight or matrix multiplication, it's going to be
90:21 - using floating point numbers because, uh, the weights will get extremely precise.
90:26 - So you want to make sure that they have, uh, sort of room to float around.
90:30 - So that's pretty much how you avoid that error.
90:33 - Uh, let's move on.
90:34 - So congratulations.
90:35 - You've probably made it further than, uh, quite a few people already.
90:38 - So congratulations on that.
90:41 - Uh, that was one of the most comprehensive parts of this entire course,
90:44 - understanding, uh, the math is going on behind the scenes.
90:48 - For some people, it's very hard to grasp if you're not very fluent with math.
90:51 - Um, but yeah, let's continue the Bagram language model and let's pump out some
90:57 - code here.
90:57 - So to recap, we're using CUDA to accelerate the training process.
91:02 - We have two hyper parameters, block size for the length of integers and, uh, batch
91:08 - for how many of those are running in parallel, two hyper parameters.
91:11 - We open our text, uh, we make some, we make a vocabulary out of it.
91:16 - We initialize our encoder and decoder, we get our data, encoding all this text, and
91:22 - then we get our train and bow splits, and then this next function here, get batch.
91:28 - So before I jump into this, go and run this here.
91:33 - So this is pretty much just taking the, uh, the first little, I don't know, we
91:40 - have eight characters, so it's taking, uh, the first eight characters and then index
91:46 - one, all the way to index nine.
91:48 - So it's offsetting by one and we can pretty much use this to show what the, uh,
91:55 - current input is and then what the target would be.
91:58 - So, uh, if we have 80 target is one, 80 and 80 and one target is one, 80 and one
92:03 - and one target is 28, et cetera, right?
92:05 - So this is the premise of the Bagram, Bagram language model.
92:09 - Given this character, we're going to predict the next.
92:11 - It doesn't know anything else in the entire history.
92:14 - It just knows what's before it, or just knows what the current character is.
92:17 - And based on that, we're going to predict the next one.
92:21 - So we have this get, get batch function here.
92:25 - And this part right here is the most important piece of code.
92:28 - This is going to, this is going to work a little bit more later with our train
92:31 - and bow splits, making sure that, you know, I'll try to explain this in a
92:36 - different way with our training, bow splits.
92:37 - So imagine you're, you take a course as you take a math course.
92:41 - Okay.
92:42 - And 90% of all your work is done just learning how the course works,
92:47 - learning all about the math.
92:49 - So that's like 90% of data you get from it.
92:52 - And then maybe another 10%, another 10% at the end is that final exam, which
92:58 - might have some questions you've never seen before.
93:01 - So the point is in that first 90%, you're tested on, uh, based on what you
93:05 - know, and then this other, uh, 10% is what you don't know.
93:09 - And this pretty much means you can't memorize everything and then just
93:13 - start generating based on your memory.
93:15 - You generate something that's alike or something that's close based on what
93:18 - you already know and the patterns you captured, uh, in that 90% of the course.
93:24 - So you can write your final exam successfully.
93:27 - So that's pretty much what's going on here.
93:28 - The training is the course, um, learning everything about it.
93:32 - And then validation is validating the final exam.
93:36 - So pretty much what we're doing here is we initialize IX and that'll take a
93:42 - random, uh, random manager between, uh, pretty much between zero and then
93:50 - length of the length of the entire text minus block size.
93:54 - So if you, uh, if you get the index that's at length of data minus block size,
94:00 - it'll, you'll still get the characters up to the length of data.
94:04 - So that's kind of how that works.
94:05 - And if we print this out here, uh, it'll just give us this right here.
94:09 - So we get some random integers.
94:10 - These are some random, uh, indices in the entire text that we can start generating
94:18 - from, so print this out and then torch.stack.
94:23 - We covered this before pretty much what this does.
94:25 - It's just going to stack them in batches.
94:28 - This is the entire point of batches.
94:29 - So, uh, that's what we do there.
94:34 - We get, uh, X and then Y is just off the same thing, but offset by one like this.
94:40 - So that's what happens there.
94:42 - And let's get into, uh, actually I'm going to add something here.
94:47 - This is going to be very important.
94:49 - We're going to go, uh, X, going to go X and Y is equal to model dot, or we're
94:56 - going to go, uh, X dot to device.
95:02 - So notice how, uh, no, we didn't do it up here.
95:06 - Okay.
95:06 - We'll cover this later, but pretty much you're going to see what
95:11 - this does in a second here.
95:15 - Two device.
95:18 - We return these and you can see that the device changed.
95:23 - So now we're actually on CUDA and this is really good because, uh, this, uh,
95:29 - these, these two pieces of data here, the inputs and the targets are no
95:32 - longer on the CPU, they're no longer going to be processed sequentially,
95:36 - but rather, uh, in our batches in parallel.
95:39 - So that's pretty much how you push any piece of data or parameters to the
95:45 - GPU is just dot two, and then the device, which you initialized appear.
95:49 - So now we can go ahead and actually initialize our neural net.
95:53 - So what I'm going to do is I'm going to go back up here and we're
95:58 - going to import some more stuff.
96:00 - So I'm going to import torch dot NN as NN and you're going to see why a lot
96:06 - of this is important in a second.
96:08 - I'm going to explain this here.
96:09 - I just want to get some code out first and down here we can initialize this.
96:22 - So it's a, it's a class we're going to make it bigram language model sub-class
96:31 - of NN dot module, and the reason why we do NN dot module here is because
96:38 - it's going to take in, it's going to take an NN dot module.
96:41 - I don't know how to explain this like amazingly, but pretty much when we use
96:45 - the NN dot module functions in PyTorch and it's inside of a NN dot module sub-class,
96:53 - they're all learnable parameters.
96:56 - So I'm going to go ahead and look at the documentation here so you can sort of
97:00 - understand this better.
97:01 - If we go to NN, okay, so pretty much all of these convolutional layers, recurrent
97:11 - layers, transformer, linear, like we looked at linear layers before.
97:16 - So we have NN dot linear.
97:18 - So if we use NN dot linear inside of this, that means that the NN dot linear
97:24 - parameters are learnable.
97:26 - So that white matrix will be changed through gradient descent and actually I
97:32 - think I should probably cover gradient descent right now.
97:35 - So in case some of you don't know what it is, it's going to be really hard to
97:39 - understand exactly how we make the network better.
97:43 - So I'm going to go ahead and set up a little graph for that right now.
97:47 - So I'm going to be using a little tool called Desmos.
97:49 - Desmos is actually great.
97:51 - It acts as a graphing calculator.
97:53 - So you can plug in formulas and move things around and just sort of visualize
97:57 - how math functions work.
97:59 - So I've written some functions out here that'll basically calculate derivative
98:04 - of a sine wave.
98:06 - So if I move A around, you'll see that changes.
98:10 - So before I get in to what's really going on here, I need to first tell you what
98:15 - the loss actually is.
98:17 - If you're not familiar, it's the loss.
98:19 - Let's say we have 80 characters in our vocabulary and we have just started our
98:25 - model, no training at all, completely random weights.
98:28 - Theoretically, there's going to be a one in 80 chance that we actually predict
98:32 - the next token successfully.
98:34 - So how we can measure the loss of this is by taking the negative log likelihood.
98:42 - So the likelihood is one out of 80.
98:44 - Take the log of that and then negative.
98:47 - So if we plug this in here, we'll get 4.38.
98:51 - So that's a terrible loss.
98:54 - Obviously, that's one out of 80.
98:55 - So it's like, you know, not even 2% chance.
98:59 - So that's not great.
99:01 - So pretty much the point is to minimize the loss, increase the prediction accuracy
99:06 - or minimize the loss.
99:08 - And that's how we train our network.
99:10 - So how does this actually work?
99:11 - How does this actually work out in code, you ask?
99:13 - So pretty much, let's say we have a loss here, okay?
99:17 - Start off with a loss of 2, just arbitrary loss, whatever.
99:21 - And what we're trying to do is decrease it.
99:24 - So over time, it's going to become smaller and smaller if we move in this direction.
99:30 - So how do we know if we're moving in the right direction?
99:32 - Well, we take the derivative of what the current point is at right now,
99:38 - and then we try moving it in different directions.
99:39 - So if we move it this way, sure, it'll go down.
99:42 - That's great.
99:43 - We can hit the local bottom over there, or we can move to this side.
99:47 - And then we can see that the slope is increasing in a negative direction.
99:51 - So we're going to keep adjusting the parameters in favor of this direction.
99:56 - So that's pretty much what gradient descent is.
99:59 - We're descending with the gradient.
100:03 - So pretty self-explanatory.
100:05 - That's what the loss function does.
100:08 - And gradient descent is an optimizer.
100:11 - So it's an optimizer for the network.
100:13 - Optimizes our parameters, our weight, major C's, et cetera.
100:17 - So these are some common optimizers that are used.
100:20 - And this is just by going to torch.optim, short for optimizer.
100:25 - And these are just a list of a bunch of optimizers that PyTorch provides.
100:30 - So what we're going to be using is something called atom w.
100:34 - And what atom w is, is it pretty much, I'm just going to read off my little script here,
100:42 - because I can't memorize every optimizer that exists.
100:47 - So atom, without atom, just atom, not atom w,
100:52 - atom is a popular optimization algorithm that combines ideas of momentum.
100:58 - And it uses a moving average of both the gradient and its squared value
101:03 - to adapt the learning rate of each parameter.
101:06 - And the learning rate is something that we should also go over.
101:10 - So let's say I figure out I need to move in this direction.
101:15 - I move, I take a step like that.
101:17 - Okay, that's a very big step that I say, okay, we need to keep moving in that direction.
101:22 - So what happens is I go like this, and then I end up there.
101:26 - And it's like, whoa, we're going up now, what happened?
101:29 - So that's because you have a very high learning rate.
101:31 - If you have a lower learning rate, what will happen is you'll start here.
101:35 - Let's take little one pixel steps or very, very small steps, like boom.
101:39 - Okay, that's good.
101:40 - That's better.
101:41 - It's even better.
101:42 - Keep going in this direction.
101:43 - This is great.
101:44 - And then you keep going down.
101:46 - You're like, okay, this is good.
101:48 - We're descending.
101:49 - And it's starting to flatten out.
101:51 - So we know that we're hitting a local bottom here.
101:54 - And then we stop because it starts ascending again.
101:57 - So that means this is our best set of parameters
101:59 - because of what that loss is or what the derivative is of that particular point.
102:09 - So pretty much this is what the learning rate is.
102:13 - So you want to have a small learning rate so that you don't take two large steps
102:17 - so that the parameters don't change dramatically and end up messing you up.
102:21 - So you want to make them small enough so you can still have efficient training.
102:25 - Like you don't want to be moving in like a millionth of one or something.
102:34 - Like that would be ridiculous.
102:35 - You'd have to do so many iterations to even get this far.
102:38 - So maybe you'd make it decently high, but not too high that it'll go like that, right?
102:45 - So that's what the learning rate is, just how fast it learns pretty much.
102:49 - And yeah, so Adam W is a modification of the Adam optimizer and it adds weight decay.
103:02 - So pretty much there's just some features that you add on to gradient descent
103:05 - and then Adam W is the same thing, except it has weight decay.
103:09 - And what this pretty much means is it generalizes the parameters more.
103:13 - So instead of having very high level performance or very low level,
103:17 - it takes a little generalize in between.
103:20 - So the weight significance will actually shrink as it flattens out.
103:27 - So this will pretty much make sure that certain parameters in your network,
103:32 - certain parameters in your weight matrices aren't affecting the output of this model drastically.
103:40 - That could be in a positive or negative direction.
103:42 - You could have insanely high performance from some lucky parameters in your weight matrices.
103:48 - So pretty much the point is to minimize those, to decay those values.
103:54 - That's what weight decay is, to prevent it from having that insane or super low level performance.
104:00 - That's what weight decay is.
104:02 - So that's a little background on gradient descent and optimizers.
104:08 - Let's go ahead and finish typing this out.
104:09 - So next up, we actually, we need to initialize some things.
104:17 - So we have our init, self of course, since it's a class, vocab size.
104:27 - I want to make sure that's correct.
104:29 - Vocabulary size.
104:30 - I might actually shrink this just a vocab size because it sounds or it's way easier to type out.
104:47 - And vocab size, good.
104:49 - So we're going to pump out some more code here.
104:53 - And this is just assuming that you have some sort of a background in Python.
104:58 - If not, it's all good, just understanding the premise of what's going on here.
105:05 - So we're going to make something called an embedding table.
105:09 - And I'm going to explain this to you in a second here, why the embedding table is really important.
105:20 - Notice that we use the nn, we use the nn module in this.
105:25 - So that means this is going to be a learnable parameter, the init dot embedding.
105:30 - So we're going to make this vocab size by vocab size.
105:36 - So let's say you have all 80 characters here and you have all 80 characters here.
105:41 - I'm going to actually show you what this looks like in a second here
105:44 - and why this is really important.
105:46 - But first off, we're going to finish typing out this background language model.
105:51 - So we're going to define our forward pass here.
105:55 - So the reason why we type this forward pass out instead of just using what it offers by default
106:02 - is to, let's say we have a specific use case for a model and we're not just using some tensors
106:09 - and we're not doing a simple task.
106:12 - This is really good practice because we want to actually know what's going on
106:16 - behind the scenes in our model.
106:17 - We want to know exactly what's going on.
106:20 - We want to know what transformations we're doing, how we're storing it
106:25 - and just a lot of the behind the scenes information that's going to help us debug.
106:29 - So I actually asked this, the chat GPT says,
106:32 - why is it important to write a forward pass function in PyTorch from scratch?
106:37 - So like I said, understanding the process, what are all the transformations that are
106:41 - actually going on, all the architecture that's going on in our forward pass,
106:45 - getting an input, running it through a network and getting an output, our flexibility,
106:52 - debugging, like I said, debugging is going to bite you in the ass
106:59 - if you don't sort of follow these best practices because if you're using weird data
107:06 - and the default isn't really used to dealing with it, you're going to get bugs from that.
107:13 - So you want to make sure that when you're actually going through your network,
107:17 - you're handling that data correctly and each transformation it actually lines up.
107:22 - So you can also print out at each step what's going on.
107:25 - So you can see like, oh, this is not quite working out here.
107:29 - Maybe we need to, you know, use a different function.
107:32 - Maybe this isn't the best one for the task, right?
107:34 - So it'll help you out with that, especially.
107:36 - And of course, customization, if you're building custom models, custom layers, right?
107:41 - And optimization, of course.
107:45 - So that's pretty much why we write out the forward pass from scratch.
107:49 - It's also just best practice.
107:51 - So it's never really a good idea to not write this, but let's continue.
107:58 - So self, and then we'll do index and targets.
108:05 - So we're going to jump into a new term here called logits.
108:10 - But before we do that, and I'm kind of all over the place here,
108:13 - before we do logits, I'm going to explain to you this embedding table here.
108:22 - Paste that in.
108:30 - Return logits.
108:32 - You're going to see why we return logits in a second here.
108:35 - So this, and end on embedding here, is pretty much just a lookup table.
108:41 - So what we're going to have, I'm actually going to pull up my notebook here.
108:45 - So we have a giant sort of grid of what the predictions are going to look like.
108:54 - That's going to look, can I drag it in here?
108:56 - No.
108:57 - So go ahead and download this full screen.
109:00 - Boom. This is in my notion here, but pretty much this is what it looks like.
109:06 - And I took this picture from Andre Karpathy's lecture,
109:10 - but what this is, is it has start tokens and end tokens.
109:14 - So start is at the start of the block and end tokens are at the end of the block.
109:19 - And it's pretty much is predicting, it's showing sort of a probability distribution
109:26 - of what character comes next given one, given one character.
109:30 - So if we have, say, I don't know, an A, 6,640 times out of this entire distribution here.
109:46 - So if we just add up all these, if we normalize them,
109:49 - and we get a little probability of this happening.
109:52 - I don't know if we add up all these together.
109:54 - I don't know what that is.
109:55 - Something some crazy number, maybe 20,000 or something, something crazy.
109:59 - Pretty much that percentage is the percentage of the end token coming after the character A.
110:08 - And then same thing here.
110:09 - Like if we do R, that's an RL or an RI.
110:14 - I don't know.
110:14 - I'm blind.
110:15 - That's an RI.
110:16 - But pretty much we normalize these, which means normalizing means you take
110:23 - how significant is that to that entire row.
110:26 - So this one's pretty significant in proportion to the others.
110:30 - So this one's going to be a fairly high probability of coming next.
110:33 - A lot of the times you're going to have an I coming after an R.
110:37 - And that's pretty much what that is.
110:39 - That's the embedding table.
110:40 - So that's why we make it vocab size by vocab size.
110:44 - So that's a little background on what we're doing here.
110:48 - So let's continue with the term logits.
110:52 - So what exactly are the logits?
110:56 - You're probably asking that.
110:58 - So let's actually go back to a little notebook I had over here.
111:05 - So remember our softmax function, right?
111:09 - Our softmax right here.
111:10 - So we exponentiated each of these values and then we normalized them.
111:15 - Normalized.
111:16 - We took its contribution to the sum of everything.
111:20 - That's what normalizing is.
111:21 - So you can think of logits as just a bunch of floating point numbers that are normalized, right?
111:28 - So you have a total here.
111:32 - I'll write this out.
111:33 - So let's say we have.
111:38 - That's a terrible line.
111:39 - Let's draw a new one.
111:43 - Good.
111:43 - Okay, so let's say we have.
111:55 - I say we have two, four and six, and we want to normalize these.
112:04 - So take two out of the totals.
112:06 - What's the total?
112:07 - We have six plus four is 10 plus two is 12.
112:09 - So two divided by 12, we take the percentage of that.
112:15 - Two out of 12 is 0.16 something.
112:20 - Okay, so 0.16, we'll just do 0.167.
112:28 - And then four out of 12 would be double that.
112:31 - So four out of 12 would be 33, 33%.
112:35 - And then six out of 12, that's 50.
112:40 - So 0.5.
112:43 - So that's what these looks like normalized.
112:45 - And this is pretty much what the logits are, except it's,
112:48 - it's more of a probability distribution.
112:50 - So let's say we have, you know, a bunch of, a bunch of bigrams here, like,
112:56 - I don't know, a followed by B and then a followed by C and then a followed by D.
113:03 - We know that from this distribution, a followed by D is most likely to come next.
113:09 - So this is what the logits are.
113:10 - They're pretty much a probability distribution of what we want to predict.
113:15 - So given that, let's hop back into here.
113:19 - We're going to mess around with these a little bit.
113:22 - So we have this embedding table and I already showed you what that looked like.
113:28 - Looked like this right here.
113:29 - This is our embedding table.
113:30 - So let's use something called, we're going to use a function called.view.
113:38 - So this is going to help us sort of reshape what our logits look like.
113:43 - And I'm going to go over an example of what this looks like in a second here.
113:46 - I'm just going to pump out some code.
113:48 - So we have our batch by our time.
113:51 - So the time is, you can think of time as that sequence of integers.
113:56 - That's the time dimension, right?
113:58 - You start from here.
113:59 - Maybe through the generating process, we don't know what's here next.
114:03 - We don't know what's on the, we don't know what the next token is.
114:06 - So that's why we say it's time.
114:07 - Because there's some we don't know yet and there's some that we already do know.
114:10 - That's why we call it the time dimension.
114:12 - And then channels would just be, how many different channels are,
114:16 - what's the vocabulary size?
114:18 - Channels is the vocabulary size.
114:20 - So we can make this the logits.shape.
114:24 - This is what logits going to return here is B by T by C.
114:27 - That's the shape of it.
114:28 - And then our targets do, actually, no, we won't do that yet.
114:37 - We'll do logits equals logits.view.
114:46 - And then we'll, this is very important, B by T.
114:51 - So because we're particularly paying attention to the channels,
114:57 - the vocabulary, the batch in time, they, I mean, they're not as important here.
115:04 - So we can sort of blend these together.
115:06 - And as long as the logits and the targets have the same batch in time,
115:13 - we should be all right.
115:15 - So we're going to do B, B times T by C.
115:22 - And then we can go ahead and initialize our targets.
115:26 - It's going to be targets.view, and it's going to be just a B by T.
115:40 - And then we can make our loss.
115:41 - Remember the loss function, right?
115:43 - So we do the functional of cross entropy,
115:47 - which is just a way of measuring the loss.
115:49 - And we basically take, there's two parameters here.
115:51 - So we have the logits and the targets.
115:58 - So I'm going to go over exactly what's going on here in a second.
116:02 - But first, you might be asking, what does this view mean?
116:04 - What exactly does this do?
116:06 - So I'm going to show you that right now.
116:08 - I've written some code here that initializes a random tensor of shape two by three by five.
116:14 - And so what I do is I pretty much unpack those,
116:17 - I unpack those dimensions by using a dot shape.
116:22 - So shape takes the two by three by five.
116:27 - We get x equals two, y equals three, and z equals five.
116:31 - So then we can do dot view,
116:36 - and that'll pretty much make that tensor again with those dimensions.
116:42 - So then we can just print that out afterwards.
116:45 - We go, we can print out, I don't know, print x, y, z.
116:54 - We have two, three, five, print a dot shape.
117:01 - And actually, I'll print out a dot shape right here first.
117:04 - So you can see that this actually does light up a dot shape.
117:12 - And then down here as well, same exact thing.
117:15 - So that's what view does, basically allows us to unpack with the dot shape.
117:21 - And then we can use view to put them back together into a tensor.
117:25 - So you might be asking, why in this notebook did we have to reshape these?
117:30 - Why did we do that?
117:32 - Well, the answer sort of falls into what the shape needs to be here with cross entropy.
117:39 - What does it expect?
117:40 - What does PyTorch expect the actual shape to be?
117:42 - So I looked at the documentation here, and it pretty much says that we want
117:51 - either one dimension, which is channels, or two, which is n, which I believe n is also the batch.
117:58 - So you have n different blocks or batches.
118:02 - And then you have some other dimensions here.
118:06 - So pretty much what it's expecting is a B by C by T instead of a B by T by C,
118:15 - which is precisely what we get out of here.
118:19 - It's the logits dot shape is B by T by C.
118:22 - And we want it in a B by C by T.
118:25 - So pretty much what we're doing is we're just putting this into,
118:30 - we're just making this one parameter by multiplying those.
118:33 - That's what's going on here.
118:34 - And then that means the second one is going to be C.
118:37 - So you get like a B times T equals n, and then C, just the way that it expects it, right?
118:43 - Just like that.
118:45 - So that's pretty much what we're doing there.
118:48 - And a lot of the times you might get errors from passing it into a functional function in PyTorch.
118:57 - So it's important to pay attention to how PyTorch expects the shapes to be,
119:02 - because you're going to get errors from that.
119:04 - And I mean, it's not very hard to reshape them.
119:07 - You just use the dot view and dot shape and you unpack them, reshape them together.
119:12 - Just it's overall pretty simple for beginner to intermediate level projects.
119:19 - So shouldn't really be a trouble there.
119:21 - But just watch out for that because it will come back and get you if you're not aware at some point.
119:28 - So I'm adding a new function here called generate.
119:30 - And this is pretty much going to generate tokens for us.
119:33 - So we pass an index, which is the current index or the context.
119:39 - And then we have max new tokens, and this is passed in through here.
119:43 - So we have our context.
119:46 - We make it a single zero, just the next line character.
119:52 - And then we generate based on that.
119:54 - And then our max new tokens, second parameter, we just make it 500 second parameter.
119:59 - So cool.
120:01 - What do we do inside of here?
120:02 - We have a little loop that pretty much it generates
120:09 - based on the length of or the range of the max new tokens.
120:15 - So we're going to generate max new tokens, tokens.
120:19 - That makes sense.
120:22 - Pretty much what we do is we call forward pass
120:24 - based on the current state of the model, the model parameters.
120:28 - And I want to be explicit here and say self forward, rather than just self index,
120:33 - it will call self for when we do this, but let's just be explicit and say self forward here.
120:39 - So we get the largest loss from this.
120:43 - We focus on the last time step.
120:44 - That's the only one we care about diagram language model.
120:46 - We only care about the single previous character.
120:49 - Only one doesn't have context before.
120:52 - And then we apply the softmax to get a probability distribution.
120:57 - And we already went over the softmax function before.
121:00 - The reason why we use negative one here is because we're focusing on the last dimension.
121:07 - And in case you aren't familiar with negative indexing, which is what this is here.
121:11 - And same with here is imagine you have a little number line.
121:16 - Okay.
121:16 - So it starts at index zero, one, two, three, four, five, et cetera.
121:21 - So if you go before zero, it's just going to loop to the very end of that array.
121:30 - So when we call negative one, it's going to do the last element, negative two,
121:34 - second last element, negative three, third last element, et cetera.
121:37 - So that's pretty much all this is here.
121:39 - And you can do this for anything in Python.
121:41 - Negative indexing is quite common.
121:44 - So that's what we do here.
121:47 - We've applied softmax to the last dimension, and then we sample from the distribution.
121:54 - So we already went over torch dot monomial.
121:57 - We get one sample.
122:00 - And this is pretty much the next index or the next encoded character
122:05 - that we then use torch dot cat short for concatenate.
122:09 - It concatenates the previous context or the previous tokens
122:14 - with the newly generated one.
122:17 - And then we just combine them together.
122:19 - So they're one thing.
122:21 - And we do this on a B by T plus one.
122:27 - And if that doesn't make sense, let me help you out here.
122:29 - So we have this time dimension.
122:32 - Let's say we have, you know, maybe just one element here.
122:35 - So we have something in the zeroth position, and then whenever we generate a token,
122:39 - we're going to take the information from the zeroth position
122:44 - and then we're going to add one to it.
122:45 - So it becomes a B by T.
122:47 - Since there was only one element, the length of that was one.
122:50 - It is now two.
122:51 - Then we have this two.
122:53 - We make it three.
122:54 - And then we have this three.
122:55 - And we make it four.
122:57 - So that's pretty much what this doing.
122:59 - Let's just keep concatenating more tokens onto it.
123:04 - And then we, you know, after this loop, we just return the index.
123:08 - So this is all the generated tokens for max new tokens.
123:11 - And that's pretty much what that does.
123:15 - Model up to device here.
123:18 - This is just going to push our parameters to the GPU for more efficient training.
123:24 - I'm not sure if this makes a huge difference right now
123:26 - because we're only doing background language modeling.
123:29 - But yeah, it's handy to have this here.
123:33 - And then, I mean, this is pretty self-explanatory here.
123:37 - We generate based on a context.
123:40 - This is the context, which is just a single zero or a next line character.
123:44 - We pass in our max new tokens.
123:47 - And then we pretty much decode this.
123:49 - So that's how that works.
123:52 - Let's move on to the optimizer and the training loop, the actual training process.
123:58 - So I actually skipped something and probably left you a little bit confused.
124:02 - But you might be asking, how the heck did we actually access the second out of out of
124:11 - three dimensions from this logits here?
124:13 - Because the logits only returns two dimensions, right?
124:19 - You have a B by T or you have a B times T by C.
124:24 - So how exactly does this work?
124:26 - Well, when we call this forward pass, all we're passing in is the index here.
124:32 - So that means targets defaults to none.
124:35 - So because targets is none, the loss is none.
124:39 - And this code does not execute.
124:42 - And it just uses this logits here, which is three dimensional.
124:46 - So that's how that works.
124:48 - And honestly, if you're feeding in your inputs and your targets to the model,
124:57 - then you're obviously going to have your targets in there.
125:00 - And that will make sure targets is not none.
125:04 - So then you'll actually be executing this code and you'll have a two dimensional logits
125:09 - rather than a three dimensional logits.
125:11 - So that's just a little clarification there, if that was confusing to anybody.
125:17 - Another quick thing I want to cover before we jump into this training loop is this little
125:22 - torch.long data type.
125:24 - So torch.long is the equivalent of int 64 or integer 64, which occupies 64 bits or eight bytes.
125:35 - So you can have different data types.
125:38 - You can have a float 16, you can have a float 32, float 64, I believe you can have an int 64,
125:45 - int 32.
125:47 - The difference between float and int is float has decimals.
125:50 - It's a floating point number and then integer is just a single integer.
125:55 - It's not really anything more than that.
125:56 - It can just be bigger based on the amount of bits that it occupies.
126:01 - So that's just an overview on torch.long.
126:05 - It's the exact same thing as int 64.
126:08 - So that's that.
126:11 - Now we have this training loop here.
126:14 - So we define our optimizer and I already meant over optimizers previously,
126:23 - atom w, which is atom weight decay.
126:26 - So we have weight decay in here and then all of our model parameters and then our learning rate.
126:34 - So I actually wrote a learning rate up here.
126:36 - So I would add this and then just rerun this part of the code here if you're typing along.
126:40 - So I have this learning rate as well as max iterators,
126:45 - which is how many iterations we're going to have in this training loop.
126:49 - And the learning rate is special because sometimes you're learning why it will be too high
126:56 - and sometimes it'll be too low.
126:58 - So a lot of the times you'll have to experiment with your learning rate and see which one
127:03 - provides the best both performance and quality over time.
127:09 - So with some learning rates, you'll get really quick advancements and then it'll like
127:14 - overshoot that little dip.
127:16 - So you want to make sure that doesn't happen,
127:19 - but you also want to make sure the training process goes quickly.
127:23 - You don't want to be waiting like, you know, an entire month
127:26 - for a Biogram language model to train by having, you know, by having a number like that.
127:33 - So that's a little overview on like,
127:37 - basically we're just putting this, this learning rate in here.
127:41 - That's where it belongs.
127:42 - So now we have this training loop here,
127:45 - which is going to iterate over the max iterations.
127:47 - Let me just give each iteration the term iter.
127:51 - And I don't think we use this yet, but we will later for just reporting on the loss over time.
127:58 - But what we do is we get, we get a batch with the train split specifically.
128:04 - We're just, again, we're just, we're just training.
128:07 - This is the training loop.
128:07 - We don't care about validation.
128:09 - So we're going to call train on this.
128:11 - We're going to get some X inputs and some Y targets.
128:17 - So we go and do a model dot forward here.
128:22 - We've got our logits and our loss.
128:25 - And then we're going to do our optimizer dot zero grad.
128:29 - And I'll explain this in the second here.
128:31 - It's a little bit confusing, but again,
128:36 - we have our, we have our loss dot backward.
128:38 - And this in cases doesn't sound familiar in case you are not familiar with training loops.
128:43 - I know I can go by this a little bit quickly,
128:45 - but this is the standard training loop architecture for basic models.
128:51 - And this is what it'll usually look like.
128:53 - So you'll, you know, you'll, you'll get your data, get your inputs or outputs, whatever.
128:59 - You'll do a forward pass.
129:01 - You'll define some thing about the optimizer here.
129:03 - In our case, it's zero grad.
129:04 - And then you'll have a loss dot backward,
129:08 - which is backward pass and the optimizer dot step,
129:12 - which lets gradient descent work.
129:15 - It's magic.
129:16 - So back to optimizer does zero grad.
129:19 - So by default pie torch will accumulate the gradients over time via adding them.
129:28 - And what we do by,
129:31 - by putting zero grad is we make sure that they do not add over time.
129:34 - So the previous gradients do not affect the current one.
129:38 - And the reason we don't want this is because previous gradients are from previous data
129:43 - and the data is, you know, kind of weird sometimes.
129:47 - Sometimes it's biased and we don't want that determining, you know,
129:52 - how much like what our error is, right?
129:55 - So we only want to decide, we only want to optimize
129:58 - based on the current gradient of our current data.
130:01 - And this little parameter in here, we go set to none.
130:06 - This pretty much means we're going to set,
130:09 - we're going to set the gradients instead of zero.
130:12 - Instead of zero gradient, we're going to set it to none.
130:16 - And the reason why we set it to none is because none occupies a lot less space.
130:22 - It just, yeah, it just occupies a lot less space when you have a zero.
130:25 - That's probably an int 64 or something that's going to take up space.
130:29 - And because, you know, we might have a lot of these accumulating that takes up space over time.
130:34 - So we want to make sure that the set to none is true.
130:39 - At least for this case, sometimes you might not want to.
130:42 - And that's pretty much what that does.
130:47 - It will, if you do have a zero grad on commonly,
130:52 - the only reason you'll need it is for training large recurrent neural nets,
130:57 - which need to understand previous context because they're recurrent.
131:02 - I'm not going to dive into RNNs right now,
131:05 - but those are a big use case for not having zero grad.
131:10 - Gradient accumulation will simply take an average of all the accumulation steps
131:15 - and just averages the gradients together.
131:18 - So you get a more effective, maybe block size, right?
131:22 - You get more context that way, and you can have the same batch size.
131:26 - So just little neat tricks like that.
131:29 - We'll talk about gradient accumulation more later in the course,
131:33 - but pretty much what's going on here.
131:35 - We define an optimizer Adam W.
131:38 - We iterate over max hitters.
131:40 - We get a batch training split.
131:42 - We do a forward pass, zero grad, backward pass,
131:46 - and then we get a step in the right direction.
131:49 - So we're gradient descent works as magic.
131:51 - And then at the end, we could just print out the loss here.
131:53 - So I've run this a few times, and over time,
131:57 - I've gotten the loss of 2.55, which is okay.
132:01 - And if we generate based on that loss, we get, you know,
132:08 - still pretty garbage tokens.
132:11 - But then again, you know, this is a diagram language model.
132:15 - So actually, I might need to retrain this here.
132:18 - It's not trained yet.
132:19 - So I'm actually going to do is run this,
132:22 - run this, run this, boom.
132:27 - And then what I'll do, oh,
132:28 - looks like we're printing a lot of stuff here.
132:31 - So that's coming from our get batch.
132:33 - So I'll just comment that or we can just delete it overall.
132:38 - Cool.
132:40 - And now if we run this again,
132:41 - give it a second.
132:53 - Perfect.
132:56 - So I don't know why it's still doing that.
133:01 - If we run it again, let's see.
133:03 - Where are we printing stuff?
133:15 - Get batch.
133:16 - No.
133:19 - Ah, yes.
133:20 - We have to run this again after changing it.
133:22 - Silly me.
133:27 - And of course, 10,000 steps is a lot.
133:30 - So it takes a little while, it takes a few seconds,
133:33 - which actually quite quick.
133:34 - So after the first one, we get a loss of 3.15.
133:38 - We can generate from that.
133:40 - And we get something that is less garbage.
133:42 - You know, it has some next line characters.
133:44 - It understands a little bit more to, you know,
133:46 - space things out and whatnot.
133:47 - So that's like slightly less garbage than before.
133:52 - But yeah, this, this is pretty good.
133:57 - So I lied.
133:57 - There aren't actually any lectures previously
134:00 - where I talked about optimizers.
134:01 - So I might as well talk about it now.
134:05 - So you have a bunch of common ones.
134:07 - And honestly, you don't really need to know
134:10 - anything more than the common ones
134:11 - because most of them are just built off of these.
134:16 - So you have your main squared error,
134:19 - common loss function using regression problems,
134:22 - where it's like, you know, you have a bunch of data points,
134:26 - find the best fit line, right?
134:27 - That's a common regression problem.
134:29 - Goals to prediction continues output
134:31 - and measures the average square difference
134:33 - between the predicted and actual values,
134:36 - often used to train neural networks for regression tasks.
134:39 - So cool.
134:42 - That's the most basic one.
134:43 - You can look into that more if you'd like,
134:45 - but that's our most basic optimizer.
134:48 - Gradient descent is a step up from that.
134:51 - It's used to minimize the loss function in a model,
134:54 - measures how well the model,
134:56 - the gradient measures how well the model is able to predict
134:59 - the target variable based on the input features.
135:02 - So we have some input X,
135:04 - we have some weights and biases, maybe WX plus B.
135:08 - And all we're trying to do is make sure that the inputs
135:14 - or make sure that we make the inputs become the desired outputs.
135:21 - And based on how far it is away from the desired outputs,
135:26 - we can change the parameters of the model.
135:28 - So we were over gradient descent recently or previously,
135:32 - but that's pretty much what's going on here.
135:36 - And momentum is just a little extension of gradient descent
135:41 - that adds the momentum term.
135:44 - So it helps smooth out the training
135:47 - and allows it to continue moving in the right direction,
135:52 - even if the gradient changes direction or varies magnitude.
135:55 - It's particularly useful for training deep neural nets.
135:58 - So momentum is when you have, you know,
136:01 - you consider some of the other gradients.
136:05 - So you have something that's like maybe passed on from here
136:08 - and then it might include a little bit of the current one.
136:11 - So like 90%, like a good momentum coefficient
136:14 - would be like 90% previous gradients
136:17 - and then 10% of the current one.
136:19 - So it kind of like lags behind
136:21 - and makes it converge sort of smoothly.
136:23 - That makes sense.
136:24 - RMS prop, I've never used this,
136:26 - but it's an algorithm that used the moving average
136:29 - of the squared gradient
136:31 - to adapt the learning rates of each parameter.
136:33 - Helps to avoid oscillations in the parameter updates
136:35 - and can improve convergence in some cases.
136:38 - So you can look more into that if you'd like.
136:41 - Atom, very popular,
136:44 - combines the ideas of momentum and RMS prop.
136:47 - It uses a moving average,
136:49 - both the gradient and its squared value
136:51 - to adapt the learning rate of each parameter.
136:54 - It's often used as the default optimizer
136:57 - for deep learning models.
136:58 - And in our case, when we continue to build this out,
137:02 - it's going to be quite a deep net.
137:04 - And Atom W is just a modification of the Atom optimizer
137:09 - that adds weight decay to the parameter updates.
137:11 - So helps to regularize
137:13 - and it can prove generalization performance.
137:16 - But using this optimizer
137:17 - as it best suits the properties of the model
137:20 - we'll train in this video.
137:22 - So, of course, I'm reading off the script here.
137:25 - There's no really other better way to say
137:27 - how these optimizers work.
137:29 - But yeah, if you want to look more into,
137:32 - you know, concepts like momentum or weight decay
137:35 - or, you know, oscillations
137:37 - and just some statistic stuff, you can.
137:40 - But honestly, the only thing that really matters
137:44 - is just knowing which optimizers are used for certain things.
137:48 - So what is momentum used for?
137:51 - What is Atom W great for?
137:54 - What is MSE good for, right?
137:57 - Just knowing what the differences and similarities are
138:00 - as well as when is the best case to use the optimizer.
138:06 - So yeah, you can find more information about that
138:09 - at torch.optim.
138:11 - So when we develop language models,
138:13 - something really important in language modeling,
138:15 - data science, machine learning at all, is just being able
138:19 - to report a loss or get an idea of how well our model
138:23 - is performing over, you know, the first thousand iterations
138:26 - and then the first two thousand iterations
138:28 - and four thousand iterations, right?
138:30 - So we want to get a general idea of how our model
138:32 - is converging over time.
138:34 - But we don't want to just print every single step of this.
138:36 - That wouldn't make sense.
138:38 - So what we actually could do is print every, you know,
138:42 - 200 iterations, 500.
138:44 - We could print every 10,000 iterations
138:45 - if you're running a crazy big language model
138:48 - if you wanted to.
138:49 - And that's exactly what we're going to implement right here.
138:52 - So actually this doesn't require an insane amount
138:56 - of Python syntax.
138:57 - This is just, I'm actually just going to add it
139:00 - into our for loop here.
139:02 - And what this is going to do is it's going to do
139:04 - what I just said is print every, you know,
139:08 - every certain number of iterations.
139:10 - So we can add a new hyper parameter up here
139:13 - called eval iter's.
139:17 - And I'm going to make this 250 just for,
139:22 - just to make things sort of easy here.
139:24 - And we're going to go ahead and add this in here.
139:28 - So I'm going to go if iter and we're going to do
139:34 - the modular operator.
139:35 - You can look more into this if you want later.
139:38 - And we're going to do eval iter's equals equals zero.
139:42 - So what this is going to do is it's going to check
139:45 - if the current iteration divided by,
139:52 - or sorry, if the remainder of the current iteration
139:57 - divided by our eval iter's parameter,
140:01 - if the remainder of that is zero,
140:03 - then we continue with it.
140:05 - So hopefully that made sense.
140:07 - If you want to, you could just look at,
140:08 - you could just ask GPT.
140:10 - You could just ask GPT four or GPT 3.5, whatever you have,
140:15 - just this modular operator.
140:17 - And you should get a good general understanding
140:18 - of what it does.
140:20 - Cool.
140:21 - So all we can do now is we'll just say,
140:25 - we'll just have a filler statement here.
140:26 - We'll just do print an F string.
140:32 - And then we'll go losses,
140:36 - losses, maybe not.
140:40 - Or actually, I'm going to change this here.
140:42 - We can go step iter.
140:49 - Add a little colon in there.
140:53 - And then I'll go split.
141:01 - Actually, no, we'll just go loss.
141:03 - And then losses like that.
141:07 - And then we'll have some sort of put in here.
141:12 - Something soon.
141:14 - I don't know.
141:16 - And all I've done is I've actually added
141:19 - a little function here behind the scenes.
141:21 - You guys didn't see me do this yet.
141:23 - But pretty much, I'm not going to go through
141:26 - the actual function itself.
141:28 - But what is important is that, you know,
141:31 - this this decorator right here,
141:32 - this probably isn't very common to you.
141:34 - So this is torch dot no grad.
141:37 - And what this is going to do is it's going to make sure
141:39 - that PyTorch doesn't use gradients at all in here.
141:42 - That'll reduce computation.
141:44 - It'll reduce memory usage.
141:45 - It's just overall better for performance.
141:47 - And because we're just reporting a loss,
141:50 - we don't really need to do any optimizing
141:52 - or gradient computation here.
141:54 - We're just getting losses.
141:55 - We're feeding some stuff into the model.
141:57 - We're getting a loss out of it.
141:59 - And we're going from there.
142:01 - So that's pretty much what's happening
142:03 - with this torch no grad.
142:06 - And, you know, for things like, I don't know,
142:11 - if you have other classes or other outside functions,
142:14 - like, I mean, get batch by default isn't using this
142:17 - because it doesn't have the model thing passed into it.
142:20 - But estimate loss does have model pass into it right here.
142:25 - So we just kind of want to make sure
142:27 - that it's not using any gradients.
142:29 - We do reduce computation that way.
142:32 - So anyways, if you want,
142:34 - you can just take a quick read over of this
142:37 - and it should overall make sense.
142:41 - Terms like.item,.mean are pretty common.
142:45 - A lot of the other things here, like model x and y,
142:47 - we get our logits in our loss.
142:49 - This stuff should make sense.
142:51 - Should be pretty straightforward.
142:53 - And only two other things I want to touch on
142:56 - is model.eval and model.train
142:59 - because you probably have not seen these yet.
143:01 - So model.train or model.train
143:06 - essentially puts the model in the training mode.
143:09 - The model learns from the data,
143:11 - meaning the weights and biases.
143:12 - If we have both,
143:13 - sometimes you only have weights.
143:14 - Sometimes you, you know,
143:16 - sometimes you have weights and biases, whatever it is.
143:19 - Those are updated during this phase.
143:21 - And then some layers of the model,
143:23 - like dropout and batch normalization,
143:25 - which you may not be familiar with yet.
143:27 - But operate differently in training mode.
143:30 - For example, dropout is active.
143:32 - And what dropout does
143:34 - is this little hyper parameter that we add up here.
143:37 - It'll look like this dropout and be like 0.2.
143:41 - So pretty much what dropout does
143:43 - is it's going to drop out random neurons in the network
143:47 - so that we don't overfit.
143:49 - And this is actually disabled in validation mode or eval mode.
143:54 - So this will just help our model
143:57 - sort of learn better when it has little like pieces of noise
144:00 - and when things aren't in quite the right place
144:03 - so that you don't have, you know,
144:05 - certain neurons in the network taking priority
144:08 - and just making a lot of the happy decisions.
144:10 - We don't want that.
144:11 - So dropout will just sort of help our model train better
144:14 - by taking 20% of the neurons out 0.2 at random.
144:19 - And that's all dropout does.
144:20 - So I'm just going to delete that for now.
144:22 - And then, yeah, model about train will dropout
144:28 - is active during this phase.
144:31 - During training, randomly turning off
144:34 - random neurons in the network.
144:35 - And this is to prevent overfitting.
144:37 - We went over overfitting earlier, I believe.
144:40 - And as for evaluation mode,
144:43 - evaluation mode is used when the model is being evaluated
144:47 - or tested just like it sounds once being trained
144:49 - what the other mode is being validated or tested.
144:53 - And layers like dropout and batch normalization
144:56 - behave differently this mode.
144:58 - Dropout is turned off in the evaluation, right?
145:00 - Because what we're actually doing is
145:03 - we're using the entire network.
145:04 - We want everything to be working sort of together.
145:08 - And we want to actually see how well does it perform.
145:10 - Training mode is when we're just, you know, sampling,
145:12 - doing weird things to try to challenge the network.
145:15 - So we're training it.
145:15 - And then, evaluating or validations would be when
145:20 - we just get the network in its optimal form
145:23 - and we're trying to see how good of results it produces.
145:26 - So that's what eval is.
145:28 - And the reason we switched into eval here
145:30 - is just because, well, we are testing the model.
145:33 - We want to see, you know, how well it does
145:35 - with any given set of data from a Git batch.
145:39 - And we don't actually need to train here.
145:41 - There's no training.
145:42 - If there was training, this would not be here
145:44 - because we would not be using any gradients.
145:47 - So we would be using gradients if training was on.
145:51 - Anyways, that's estimate loss for you.
145:54 - This function is, you know, just general,
145:57 - generally good to have in data science,
145:59 - your training, validation, splits, whatnot.
146:02 - And yeah, good for reporting.
146:05 - You know how it is.
146:07 - And we can go ahead and add this down here.
146:10 - So there's something soon, we'll go losses is equal to estimate loss.
146:19 - And then we can go ahead and put a,
146:25 - yeah, we don't actually have to put anything in here.
146:28 - Cool.
146:30 - So now let's go ahead and run this.
146:32 - Let me run from the start here.
146:34 - Boom, boom, boom, boom, boom, boom.
146:43 - Perfect.
146:52 - Now we're running for 10,000 iterations.
146:54 - That's interesting.
146:57 - Okay.
146:58 - So, yes.
147:02 - So what I'm going to do actually here
147:03 - is you can see this loss part is weird.
147:06 - So I'm actually going to change this up
147:08 - and I'm just going to switch it to,
147:12 - we're going to go train loss.
147:16 - And we're going to go losses and we're going to do the train split.
147:21 - And then we're going to go over here and just do the validation loss.
147:28 - We can do validation or just bow for short.
147:30 - And I'm going to make it consistent here,
147:35 - though we have a colon there, a colon here,
147:39 - and then you just go losses and do that.
147:44 - Cool.
147:45 - So I'm going to reduce these maxators up here to only 1000.
147:50 - Run that, run this.
147:53 - Oh, somebody did a match.
148:23 - Yeah, so what actually happened here was since we were using these little ticks,
148:40 - what was happening is these were matching up with these.
148:44 - And it was telling us, oh, you can't do that.
148:46 - You can't start here and then end there and have all this weird stuff.
148:49 - Like you can't do that.
148:51 - So pretty much we just need to make sure that these are different.
148:54 - So I'm going to do a double quote instead of single
148:56 - and then double code to finish it off.
148:58 - And as you can see, this worked out here.
149:02 - So I'll just run that again so you guys can see what this looks like.
149:06 - And this is ugly because we have a lot of decimal places.
149:11 - So we can actually do here is we can add in a little format or a little decimal place
149:18 - reducer if you call it just for, you know, so you can read it.
149:23 - So it's not like some weird decimal number.
149:25 - And you're like, Oh, does this eight matter?
149:27 - Probably not just like the first three digits, maybe.
149:30 - So all we can do here is just add in, I believe this is how it goes.
149:38 - I don't think it's the other way.
149:41 - We'll find out some stuff in Python is extremely confusing to me.
149:46 - But there we go.
149:50 - So I got it right, colon and then period.
149:53 - And as you can see, we have those digits reduced.
149:55 - So I can actually put this down to three F.
150:04 - Wonderful.
150:05 - So we have our train loss and our validation loss.
150:10 - Great job.
150:10 - You made it this far.
150:11 - This is absolutely amazing.
150:13 - This is insane.
150:15 - You've gotten this far in the video.
150:16 - We've covered all the basics, everything you need to know about
150:20 - Bagram language models, optimizers, training loops, reporting losses.
150:26 - I can't even name everything we've done because it's so much.
150:29 - So congratulations that you made it this far.
150:32 - You should go take a quick break, give yourself a pat on the back
150:35 - and get ready for the next part here because it's going to be absolutely insane.
150:39 - We're going to dig into literally state of the art language models
150:44 - and how we can build them from scratch, or at least how we can pre-train them.
150:49 - And some of these terms are going to seem a little bit out there,
150:53 - but I can ensure you by the end of this next section here,
150:57 - you're going to have a pretty good understanding about the state of language models right now.
151:03 - So yeah, go take a quick break and I'll see you back in a little bit.
151:08 - So there's something I'd like to clear up and I actually sort of lied to you a little bit.
151:13 - A little while back in this course about what normalizing is.
151:18 - So I recall we were talking about the softmax function and normalizing vectors.
151:25 - So the softmax is definitely a form of normalization, but there are many forms.
151:32 - There are not just a few or like there's not just one or two normalizations.
151:36 - There are actually many of them and I have them on my second monitor here,
151:41 - but I don't want to just dump that library of information on your head
151:45 - because that's not how you learn.
151:46 - So what we're going to do is we're going to plug this into GPT-4.
151:50 - We're going to say, can you list all the forms of normalizing in machine learning?
152:04 - And how are they different from one another?
152:17 - GPT-4 is a great tool.
152:19 - If you don't already use it, I highly suggest you use it or even GPT 3.5,
152:25 - which is the free version.
152:27 - But yeah, it's a great tool for just quickly learning anything.
152:30 - And then you could give you example practice questions with answers
152:36 - so you can learn topics in like literally minutes
152:39 - that would take you several lectures to learn in a university course.
152:44 - But anyways, there's a few here.
152:47 - So min-max normalization, yep.
152:51 - Z-score, decimal scaling, mean normalization, unit vector or layer two,
152:57 - robust scaling, power transformations.
152:59 - Okay, so yeah, and then softmax would be another one.
153:03 - What about softmax?
153:09 - It is in data type normalization, but it's not typically using for normalizing input data.
153:19 - It's commonly used in the output layer.
153:21 - So softmax is a type of normalization, but it's not used for normalizing input data.
153:27 - And honestly, we proved that here by actually producing some probabilities.
153:36 - So this isn't something we used in our forward pass.
153:39 - This is something we use in our generate function
153:41 - to get a bunch of probabilities from our logits.
153:45 - So this is, yeah, interesting.
153:47 - It's good to just figure little things like these out
153:50 - for just to put you on the edge a little bit more
153:54 - for the future when it comes to engineering these kinds of things.
153:58 - All right, great.
153:59 - So the next thing I want to touch on is activation functions.
154:03 - And activation functions are extremely important
154:07 - in offering new ways of changing our inputs that are not linear.
154:12 - So for example, if we were to have a bunch of linear layers, a bunch of, let me erase this.
154:20 - If we were to have a bunch of, you know,
154:22 - NN dot linears in a row,
154:26 - what would actually happen is they would all just, you know,
154:30 - they would all squeeze together and it would essentially apply one transformation
154:35 - that sums up all of them, kind of.
154:37 - They all sort of multiply together and it gives us one transformation
154:42 - that is kind of just a waste of computation,
154:45 - because let's say you have a hundred of these NN dot linear layers
154:49 - and nothing else.
154:52 - You're essentially going from inputs to outputs,
154:55 - but you're doing a hundred times the computation for just one multiplication.
154:59 - That doesn't really make sense.
155:01 - So what can we do to actually make these deep neural networks important?
155:06 - And what can we offer that's more than just linear transformations?
155:10 - Well, that's where activation functions come in.
155:13 - And I'm going to go over these in a quick second here.
155:16 - So let's go navigate over to the PyTorch docs.
155:19 - So the three activation functions I'm going to cover
155:23 - in this little part of the video are the ReLU,
155:26 - the sigmoid and the tanh activation functions.
155:29 - So let's start off with the ReLU or rectified linear unit.
155:35 - So we're going to use functional ReLU.
155:38 - And the reason why we're not just going to use torch dot NN
155:41 - is because we're not doing any forward passes here.
155:43 - I'm just going to add these into our, I'm going to add these.
155:49 - Let me clear this, clear this output.
155:52 - That's fine.
155:54 - I'm actually going to add these into here and there's no forward pass.
155:57 - We're just going to simply run them through a function and get an output
156:00 - just so we can see what it looks like.
156:02 - So I've actually added this up here from torch dot NN import functional as capital F.
156:09 - It's just kind of a common PyTorch practice capital S.
156:11 - And let's go ahead and start off with the ReLU here.
156:17 - So we can go, I don't know, X equals torch dot tensor.
156:26 - And then we'll make it a negative 0.05, for example.
156:32 - And then we'll go D type equals torch dot float 32.
156:37 - And we can go Y equals F dot ReLU of X.
156:45 - And then we'll go ahead and print Y.
156:52 - Oh, has no attribute ReLU.
156:54 - Okay, let's try NN then.
156:56 - Let's try NN and see if that works.
157:01 - Okay, well, that didn't work.
157:03 - And that's fine, because we can simply take a look at this
157:07 - and it'll help us understand.
157:11 - We don't actually need to, we don't need to write this out in code
157:13 - as long as it sort of makes sense.
157:15 - We don't need to write this in the forward pass, really.
157:17 - You're not going to use it anywhere else.
157:19 - So yeah, I'm not going to be too discouraged.
157:23 - That does not work in the functional library.
157:26 - But yeah, so pretty much what this does is if a number is below,
157:31 - if a number is zero or below zero,
157:34 - it will turn that number into zero.
157:36 - And then if it's above zero, it'll stay the same.
157:40 - So this graph sort of helps you visualize that there's a little function here.
157:46 - That might make sense to some people,
157:47 - I don't really care about the functions too much,
157:50 - as long as I can sort of visualize what the function means,
157:52 - what it does, what are some applications that can be used.
157:55 - That usually covers enough for like any function at all.
157:59 - So that's the ReLU function.
158:02 - Pretty cool.
158:03 - It simply offers a non-linearity to our linear networks.
158:07 - So if you have 100 layers deep, and every, I don't know,
158:12 - every second step you put a ReLU,
158:15 - that network's going to learn a lot more things.
158:17 - It's going to learn a lot more linearity, non-linearity,
158:20 - than if you were to just have 100 layers multiplying all into one transformation.
158:26 - So that's what that is.
158:27 - That's the ReLU.
158:29 - Now let's go over the sigmoid.
158:30 - So here we can actually use the functional library.
158:35 - And all sigmoid does is we go 1 over 1 plus exponentiated of negative x.
158:42 - So I'm going to add that here.
158:45 - We could, yeah, why not do that?
158:48 - Negative 0.05 float 32.
158:52 - Sure.
158:52 - We'll go f dot sigmoid.
158:56 - And then we'll just go x, and then we'll print y.
159:00 - Cool.
159:01 - So we get a tensor 0.4875.
159:04 - Interesting.
159:06 - So this little negative 0.05 here is essentially being plugged into this negative x.
159:13 - So 1 over 1 plus 2.71 to the power of negative 0.05.
159:20 - So it's essentially, if we do 2.71, 2.71 to the power of negative negative 0.5,
159:34 - we're just going to get positive.
159:35 - So 1.05, and then 1 plus that, so that's 2.05.
159:44 - We just do 1 over that, 2.05.
159:48 - So we get about 0.487.
159:51 - And what do we get here?
159:53 - 0.487.
159:54 - Cool.
159:56 - So that's interesting.
159:58 - And let's actually look, is there a graph here?
160:00 - Let's look at the sigmoid activation function.
160:05 - Wikipedia.
160:06 - Don't get too scared by this math here.
160:08 - I don't like it either, but I like the graphs.
160:11 - They're cool to look at.
160:13 - So this is pretty much what it's doing here.
160:15 - So yeah, it's just a little curve.
160:19 - Kind of looks like a, yeah, it's kind of just like a wave, but it's cool looking.
160:27 - That's what the sigmoid function does.
160:28 - It's used to just generalize over this line.
160:32 - And yeah, sigmoid function is pretty cool.
160:36 - So now let's move on to the tanh, the tanh function.
160:41 - Google Bing is, or Microsoft Bing is giving me a nice description of that.
160:45 - Cool.
160:46 - Yeah, perfect.
160:47 - E to the negative X.
160:48 - I like that.
160:51 - So tanh is a little bit different.
160:53 - There's a lot more exponentiating going on here.
160:56 - So you have, well, I'll just say expo or exp of X minus exp of negative X
161:03 - divided by exp of X plus exp of negative X.
161:06 - There's a lot of positives and negatives in here.
161:10 - Positive, positive, negative, negative, negative, positive.
161:13 - So that's interesting.
161:16 - Let's go ahead and put this into code here.
161:19 - So I'll go torch shot examples or torch examples.
161:23 - This is our file here and I'll just go tanh.
161:30 - Cool.
161:32 - So negative 0.05.
161:35 - Cool.
161:36 - What if we do a one, what if we do a one here?
161:38 - What will that produce?
161:39 - Oh, 0.76.
161:45 - What if we do a 10?
161:51 - 1.0.
161:52 - Interesting.
161:53 - So this is sort of similar to the sigmoid except it's,
161:58 - you know, let's actually ask you what the difference is.
162:01 - When would you use tanh over sigmoid?
162:13 - Let's see here.
162:16 - Sigmoid function and hyperbolic tangent or tanh function
162:20 - are activations functions used in neural networks.
162:23 - They have a similar s-shaped curve but have different ranges.
162:27 - So sigmoid output values between a 0 and a 1,
162:30 - well tanh is between a negative 1 and a 1.
162:33 - So if you're, you know, if you're rating maybe the,
162:39 - maybe if you're getting a probability distribution,
162:42 - for example, you want it to be between 0 and 1,
162:46 - meaning percentages or decimal places.
162:49 - So like a 0.5 would be 50%, 0.87 would be 87%.
162:54 - And that's what the sigmoid function does.
162:56 - It's quite close to the softmax function actually,
163:00 - except the softmax just, you know,
163:03 - it prioritizes the bigger values and puts the smaller values
163:07 - that are priority.
163:08 - That's all the softmax does.
163:09 - It's kind of a sigmoid on steroids.
163:12 - And the tanh outputs between negative 1 and 1.
163:17 - So yeah, you could maybe even start theory crafting
163:21 - and thinking of some ways you could use even the tanh function
163:24 - and sigmoid in different use cases.
163:26 - So that's kind of a general overview on those.
163:29 - So background language models are finished.
163:31 - All of this we finished here is now done.
163:34 - You're back from your break.
163:35 - If you took one, if you didn't, that's fine too.
163:38 - But pretty much we're going to dig into the transformer architecture now
163:44 - and we're actually going to build it from scratch.
163:47 - So there was recently a paper proposed called the transformer model.
163:53 - And this uses a mechanism called self-attention.
163:57 - Self-attention is used in these multi-head attention little bricks here.
164:02 - And there's a lot that happens.
164:04 - So there's something I want to clarify before we jump right into this architecture
164:09 - and just dump a bunch of information on your poor little brain right now.
164:15 - But a lot of these networks at first can be extremely confusing to beginners.
164:20 - So I want to make it clear.
164:23 - It's perfectly okay if you don't understand this at first.
164:26 - I'm going to try to explain this in the best way possible.
164:29 - Believe me, I've seen tons of videos on people explaining the transformer architecture
164:34 - and all of them have been to some degree a bit confusing to me as well.
164:39 - So I'm going to try to clarify all those little pieces of confusion.
164:46 - Like what does that mean?
164:48 - You didn't cover that piece.
164:49 - I don't know what's going on here.
164:51 - I'm going to cover all those little bits and make sure that nothing is left behind.
164:56 - So you're going to want to sit tight and pay attention for this next part here.
165:02 - So yeah, let's go ahead and dive into just the general transformer architecture
165:07 - and why it's important.
165:09 - So in the transformer network, you have a lot of computation going on.
165:14 - You have some adding and normalizing.
165:17 - You have some multi-head attention.
165:18 - You have some feed forward networks.
165:20 - There's a lot going on here.
165:21 - There's a lot of computation, a lot of multiplying, a lot of matrix multiplication.
165:25 - There's a lot going on.
165:27 - So a question I actually had at first was,
165:29 - well, if you're just multiplying these inputs by a bunch of different things along,
165:34 - you should just end up with some random value at the end
165:38 - that maybe doesn't really mean that much of the initial input.
165:43 - And that's actually correct.
165:45 - For the first few iterations, the model has absolutely no context as to what's going on it.
165:51 - It is clueless.
165:52 - It is going in random directions and it's just trying to find the best way to converge.
165:57 - So this is what machine learning and deep learning is actually all about,
166:01 - is having all these little parameters in the adding and normalizing,
166:06 - the feed forward networks, even multi-head attention.
166:10 - We're trying to optimize the parameters for producing an output
166:14 - that is meaningful.
166:15 - That will actually help us produce almost perfectly like English text.
166:21 - And so this is the entire process of pre-training.
166:23 - You send a bunch of inputs into a transformer
166:27 - and you get some output probabilities that used to generate from.
166:31 - And what attention does is it sets little different scores to each little token in a sentence.
166:42 - For tokens, you have character, subword, and word level tokens.
166:47 - So you're pretty much just mapping bits of attention to each of these,
166:52 - as well as what does its position also mean as well.
166:57 - So you get up two words that are right next to each other.
167:01 - But then if you don't actually positionally encode them,
167:05 - it doesn't really mean much because it's like, oh, these could be like 4,000 characters apart.
167:09 - So that's why you need both to put attention scores on these tokens
167:15 - and to positionally encode them.
167:17 - And that's what's happening here.
167:19 - So what we do is we get to our inputs.
167:24 - We get our inputs.
167:25 - So I mean, we went over this with diagram language models.
167:31 - We feed our X and Y.
167:33 - So X would be our inputs.
167:35 - Y would be our targets or outputs.
167:37 - And what we're going to do is give these little embeddings.
167:44 - So I believe we went over embeddings a little while ago.
167:47 - And pretty much what those mean is it's going to have a little,
167:50 - it's going to have a little row for each token on that table.
167:53 - And that's going to store some vector as to what that token means.
167:59 - So let's say you had the character E, for example,
168:04 - the sentiment or the vector of the character E is probably going to be vastly different
168:11 - than the sentiment of Z, right?
168:14 - Because E is a very common vowel and Z is one of the most uncommon,
168:18 - if not the most uncommon letter in the English language.
168:21 - So these embeddings are learned.
168:25 - We have these both for our inputs and our outputs.
168:28 - We give them positional encodings like I was talking about.
168:31 - And there's ways we can do that.
168:33 - We can actually use learnable parameters to assign these encodings.
168:39 - A lot of these are learnable parameters, by the way.
168:41 - And you'll see that as you delve more and more into transformers.
168:45 - But yeah, so after we've given these inputs, embeddings, and positional encodings,
168:52 - and same thing with the outputs, which are essentially just shifted right,
168:56 - you have I up to block size for inputs,
168:58 - and then I plus one up to block size plus one, right?
169:04 - Or whatever, whatever little thing we employed here in our background language models.
169:09 - Can't remember quite what it was, or even if we did that at all.
169:18 - No, I'm just speaking gibberish right now,
169:22 - but that's fine because it's going to make sense in a little bit here.
169:25 - So what I'm going to actually do is I'm not going to read off of this right here,
169:33 - because this is really confusing.
169:36 - So I'm going to switch over to a little, I guess, a little sketch that I drew out.
169:42 - And this is pretty much the entire transformer with a lot of other things
169:46 - considered that this initial image is not really put into perspective.
169:50 - So let's go ahead and jump into sort of what's going on in here from the ground up.
169:58 - So like I was talking about before,
169:59 - we have some inputs and we have some outputs which are shifted right.
170:03 - And we give each of them some embedding vectors and positional encodings.
170:09 - So from here, let's say we have n layers.
170:12 - This is going to make sense in a second.
170:14 - n layers is set to four.
170:16 - So the amount of layers we have is set to four.
170:19 - So you can see we have an encoder, encoder, like we have four of these,
170:22 - we have four decoders.
170:24 - So four is actually the amount of encoders and decoders we have.
170:29 - We always have the same amount of each.
170:32 - So if we have 10 layers, that means we'd have 10 encoders and 10 decoders.
170:38 - And pretty much what would happen is after this input,
170:43 - embedding and positional embedding, we feed that into the first encoder layer
170:46 - and then the next and then next and then right as soon as we hit the last one,
170:51 - we feed these into each of these decoders here, each of these decoder layers.
170:58 - So only the last encoder will feed into these decoders.
171:03 - And pretty much these decoders will all run.
171:08 - They all learn different things.
171:10 - And then they'll turn what they learned.
171:13 - They'll apply a linear transformation at the end of it.
171:16 - This is not in the decoder function.
171:18 - This is actually after the last decoder.
171:20 - It'll apply a linear transformation to pretty much
171:25 - sort of simplify or give a summary of what it learned.
171:28 - And then we apply a softmax on that new tensor to get some probabilities to sample from,
171:36 - like we talked about in the generate function in our bigram.
171:40 - And then once we get these probabilities,
171:42 - we can then sample from them and generate tokens.
171:47 - And that's kind of like the first little step here.
171:51 - That's what's going on.
171:52 - We have some encoders.
171:54 - We have some decoders.
171:55 - We do a transformation to summarize.
171:57 - We have a softmax to get probabilities.
171:59 - And then we generate based on those probabilities.
172:01 - Cool.
172:04 - Next up, in the encoder, in each of these encoders,
172:08 - this is what it's going to look like.
172:09 - So we have multi-head attention, which I'm going to dub into a second here.
172:15 - So after this multi-head attention, we have a residual connection.
172:19 - So in case you aren't familiar with residual connections,
172:22 - I might have went over this before.
172:24 - But pretty much what they do is it's a little connector.
172:28 - So I don't know.
172:30 - Let's say you get some inputs x.
172:32 - You have some inputs x down here and you put them into some sort of function here,
172:38 - some sort of feed-forward network, whatever it is.
172:40 - A feed-forward network is essentially just a linear, a relu, and then a linear.
172:45 - That's all feed-forward network is right here.
172:47 - Linear, relu, relu, linear.
172:50 - And all you do is you wrap those inputs around
172:57 - so you don't actually put them into that feed-forward network.
173:01 - You actually wrap them around and then you can add them to the output.
173:05 - So you had some x values here, go through the relu, and then you had some wrap around.
173:11 - And then right here, you simply add them together and you normalize them
173:16 - using some layer norm, which we're going to cover in a little bit.
173:19 - And the reason our residual connections are so useful in transformers
173:26 - is because when you have a really deep neural network,
173:29 - a lot of the information is actually forgotten in the first steps.
173:33 - So if you have your first view encoder layers and your first view decoder layers,
173:38 - a lot of the information here is going to be forgotten
173:41 - because it's not being carried through.
173:43 - The first steps of it aren't explicitly being carried through
173:47 - and sort of skipped through the functions.
173:51 - And yeah, you can sort of see how they would just be forgotten.
173:55 - So residual connections are sort of just a cheat for getting around that,
174:00 - getting around that for not having deep neural networks forget things from the beginning
174:04 - and having them all sort of work together to the same degree.
174:07 - So residual connections are great that way.
174:10 - And then at the end there, you would add them together and then normalize.
174:16 - And there's two different ways that you can do this add a norm.
174:20 - There's add a norm and then norm an add.
174:23 - So these are two different separate architectures
174:27 - that you can do in transformers.
174:31 - And both of these are sort of like meta architectures,
174:34 - but pretty much pre-norm is the normalize then add,
174:40 - and then post-norm is add then normalize.
174:43 - So in this attention is all you need paper
174:46 - proposed by a bunch of research scientists was.
174:50 - Initially, you want to add these, you want to add these together and then normalize them.
175:00 - So that is what we call the post-norm architecture.
175:05 - And then pre-norm is just flip them around.
175:08 - So I've actually done some testing with pre-norm and post-norm
175:13 - and the original transformer paper turned out to be quite actually a lot better,
175:21 - at least for training very small language models.
175:24 - If you're training bigger ones, it might be different.
175:26 - But essentially, we're just going to go by the rules that we use in here.
175:31 - So add a norm, we're not going to do norm and add, add a norm in this video specifically,
175:36 - because it works better.
175:38 - And we just don't want to break any of the rules and go outside of it
175:41 - because then that starts to get confusing.
175:43 - And actually, if you watch the Andre Carpathi lecture on building GPTs from scratch,
175:48 - he actually implemented it in the pre-norm way.
175:54 - So normalize then add.
175:56 - So yeah, based on my experience, what I've done on my computer here
176:01 - is the post-norm architecture works quite better.
176:06 - So that's why we're going to use it.
176:08 - We're going to do add then normalize.
176:10 - So then we essentially feed this into a feed-forward network, which we covered earlier.
176:17 - And then, how did it go?
176:21 - We have a, yeah, so our encoder, we do a residual connection from here to here.
176:30 - And then another residual connection from like outside of our feed-forward network.
176:36 - So each time we're doing some other things, like some, you know, some computation blocks in here,
176:41 - we're going to have a res connection.
176:43 - Same with our feed-forward res connection.
176:47 - And then, of course, the output from here, we just, when it exits,
176:51 - it's going to feed into the next encoder block if it's not the last encoder.
176:56 - So this one is going to do all this, it's going to feed into that one,
176:58 - it's going to do the same thing, feed into this one, going to feed into that one.
177:02 - And then the output of this is going to feed into each of these decoders, all the same information.
177:09 - And yeah, so that's a little bit scoped in as to what these encoders look like.
177:15 - So now that you know what the encoder looks like, what the feed-forward looks like,
177:19 - we're going to go into multi-head attention, sort of the premise,
177:23 - sort of the highlight of the transformer architecture and why it's so important.
177:27 - So multi-head attention, we call it multi-head attention because there are a bunch of these
177:34 - different heads learning different semantic info from a unique perspective.
177:39 - So let's say you have 10 different people looking at the same book.
177:45 - If you have 10 different people, let's say they're all reading,
177:50 - let's say they're all reading the same Harry Potter book.
177:53 - These different people, they might have different cognitive abilities,
177:57 - they might have different IQs, they might have been raised in different ways,
178:01 - so they might interpret things differently, they might look at little things in that book and
178:08 - they'll imagine different scenarios, different environments from the book.
178:12 - And essentially why this is so valuable is because we don't just want to have one person,
178:19 - just one perspective on this, we want to have a bunch of different heads in parallel looking at
178:25 - this, looking at this same piece of data because they're all going to capture different things
178:33 - about it. And keep in mind each of these heads, each of these heads in parallel,
178:38 - these different perspectives, they have different learnable parameters.
178:42 - So they're not all the same one looking at this piece of data, they're actually,
178:50 - they all have different learnable parameters. So you have a bunch of these at the same time
178:55 - learning different things and that's why it's so powerful. So this scaled dot product attention
179:03 - runs in parallel, which means we can scale that to the GPU, which is very useful.
179:08 - It's good to touch on that. Anything with the GPU that you can accelerate is just an automatic win
179:13 - because parallelism is great in machine learning. Why not have parallelism, right? If it's just
179:21 - going to be running the CPU, what's the point? That's why we love GPUs. Anyways, yeah, so you're
179:28 - going to have these different, you're going to have these things that are called keys,
179:31 - queries and values. I'll touch on those in a second here because keys, queries and values
179:37 - sort of point to self attention, which is literally the entire point of the transformer.
179:41 - Transformer wouldn't really mean anything without self attention. So I'll touch on those in a second
179:47 - here and we'll actually delve deeper as we hit this sort of block. But yeah, you have these keys,
179:52 - queries and values. They go into scaled dot product attention. So a bunch of these running
179:56 - in parallel and then you can catenate the results from all these different heads running in parallel.
180:02 - You have all these different people, you can catenate all of them, you generalize it and
180:06 - then you apply a transformation to a linear transformation to pretty much summarize that
180:13 - and then do your add a norm, then pay for a network. So that's what's going on in multi
180:20 - head attention. You're just doing a bunch of self attention in parallel, concatenating and then
180:25 - continuing on with this part. So scaled dot product attention, what is that?
180:32 - So let's just start from the ground up here. So you have, we'll just go from left to right.
180:37 - So you have your keys, queries and values. What do your keys do? Well a key is, let's just say you
180:43 - have a token in a sentence, okay? So if you have, let me just roll down here to a good example.
180:51 - So self attention uses keys, queries and values. Self attention helps identify which of these
181:03 - tokens in a sentence, in any given sentence are more important and how much attention you should
181:09 - pay to each of those characters or words, whatever you're using. We'll just use words to make it
181:16 - easier to understand for the purpose of this video. But essentially imagine you have
181:22 - these two sentences here. So you have, let me bring out my little piece of text. So you have, oh,
181:35 - that didn't work. So imagine you have
181:39 - server, can I have the check? And then you have, and you have looks like I crashed the server.
181:53 - So I mean, both of these have the word server in them, but they mean different things. Server
181:59 - meaning like the waiter or the waitress or whoever is billing you at the end of your restaurant
182:05 - visit. And it looks like I crashed the server is like, oh, there's actually a server running in
182:10 - the cloud, not like a person that's billing me, but an actual server that's maybe running a video
182:16 - game. And these are two different things. So what attention can do is it can actually identify
182:23 - which words would get attention here. So it can say server, can I have the check? Can I have? So
182:31 - it's maybe you're looking for something, you're looking for the check and then server is like,
182:37 - oh, well in this, in this particular sequence or in this, in the sentiment of this sentence here,
182:43 - server is specifically tied to this one meaning maybe a human, someone at a restaurant
182:51 - and then crash, crash the server crashes, crash is going to get a very high attention score
182:57 - because you don't normally crashes, crash a server at a restaurant. That doesn't particularly
183:03 - make sense. So when you have different words like this, what self attention will do is it will learn
183:12 - which one of the, which, which words in the sentence are actually more important and which
183:16 - should, which word should it pay more attention to. So that's really all that's going on here.
183:22 - And K the key is essentially going to emit a different, it's going to emit a little tensor
183:32 - for each of these words here saying, you know, what do I contain? And then query is going to say,
183:40 - what am I looking for? So what's going to happen is if these like, let's say server,
183:48 - server, it's going to look for things like, you know, check or crashed. So if it sees crashed,
183:54 - then that means the key and the query are going to multiply and it's going to get a very high
183:59 - attention score. But if you had something like the and the it's like, oh, the kid does literally in
184:07 - like almost any sentence. So that doesn't mean much. We're not going to pay attention to those
184:11 - words. So that's going to get a very low attention score. And all attention is, is you're just
184:17 - dot product dot producting these vectors together. So you get a key and a query, you dot product them,
184:24 - we already went over dot products in this course before. And then all you do here, and this is a
184:30 - little, little bit of a confusing part is you just scale, you scale it by one over the square root
184:37 - of the, of the length of a row in the quiz keys or queries matrix, otherwise known as dk.
184:45 - So let's say we have, you know, our key and our query, these are all going to be the same length,
184:51 - by the way, let's say our keys is, you know, maybe, maybe our keys is going to be like 10 characters
184:58 - long, our, our keys are going to be 10 characters long as well. So it's going to do one over the
185:05 - square root of 10, if that makes sense. And so that's just, that's just essentially a way of
185:13 - preventing these dot products from exploding, right? We want to scale them because as we have,
185:22 - as it in, as the length of it increases, so will the ending dot product, because there's more
185:29 - of these to multiply. So we pretty much just want to scale it by using an inverse square root.
185:36 - And that'll just help us with scaling, make sure nothing explodes in unnecessary ways.
185:43 - And then the next little important part is using tort dot trill, which I imagine we went over
185:49 - in our examples here. Trill. Yeah. So you can see that it's a, it's a diagonal, it's a left
186:01 - triangular matrix of ones. And these aren't going to be ones in our self attention here
186:07 - in our torque dot trail or masking. What this is going to be is the scores at each time step,
186:17 - combination of scores at each time step. So if we've only gone, you know, if we're only looking
186:25 - at the first time step, we should not have access to the rest of things or else that would be cheating.
186:31 - We shouldn't be allowed to look ahead because we haven't actually produced these yet. We need to
186:36 - produce these before we can put them into perspective and, you know, put a weight on them.
186:42 - So we're going to set all these to zero and then we go to the next time step. Okay. So now we've,
186:47 - we've just generated this one, we haven't generated these yet. So we can't look at them.
186:52 - And then as we go more and more as, as the time step increases, we know more and more context
186:57 - about all of these tokens. So that's all that's doing. Mask attention is pretty much just saying,
187:05 - we don't want to look into the future. We want to only guess with what we currently know
187:10 - in our current time step and everything before it. You can't jump into the future. You can only
187:15 - look at what happened in the past and do stuff based on that. Right? Same thing applies to life.
187:20 - You can't really skip to the future and say, Hey, if you do this, you're going to be a billionaire.
187:24 - You're going to be a billionaire. No, that would be cheating. You're not allowed to do that. You
187:27 - can only look at the mistakes you made and say, how can I become a billionaire based on all these
187:32 - other mistakes that I made? How can I become as close to perfect as possible? Which no one can
187:38 - ever be perfect, but that's my little analogy for the day. So that's mass attention. Pretty much
187:45 - just not letting us skip time steps. So that's fun. Let's continue. Two more little things I
187:51 - want to touch on before I jump forward here. So these keys, queries, and values, each of these
187:56 - are learned through a linear transformation. Just an n n dot linear, uh, is applied and that's how
188:02 - we get our keys, queries, and values. So that's, that's just a little touching there. If you're
188:06 - wondering, how do we get those? It's just an n n dot linear transformation. Uh, and then as for
188:13 - our masking, we don't actually apply this all the time. You might've seen right here, we have
188:19 - multi-head attention, multi-head attention, and then masked multi-head attention. So this mass
188:25 - attention isn't used all the time. It's only used actually one out of the three attentions we have
188:31 - per layer. So I'll give you, I'll give you a little bit more information about that as we,
188:38 - you know, progress more and more into the architecture and as we, as we learn more about
188:42 - it. I'm not going to dive into that quite yet though. So let's just continue on with what's
188:48 - going on. So we have a softmax and why softmax important? Well, I actually mentioned earlier
188:56 - softmax is not commonly used as a normalization method, but here we're actually using softmax
189:02 - to normalize. So when you have all of these, uh, when you have all these, you know, attention
189:09 - scores, essentially what the softmax is doing is it's going to exponentiate and normalize all of
189:17 - these. So all of the attention scores that have scored high, like maybe 50 to 90 percent or
189:24 - whatever it is, those are going to take a massive effect in that entire, uh, attention, I guess,
189:31 - tensor if you want to call it that. And that's important. Well, it's, it's, it might not seem
189:38 - important, but it's essentially just giving the model more confidence as to which tokens matter
189:44 - more. So for example, if we just did a normal normalization, we would have words like server
189:53 - and crash and then server and check. And then you would, you would just know, you know, a decent
190:00 - amount about those. Those would be pay attention to a decent amount because they're, because they
190:05 - multiply together quite well. But if you softmax those, then it's like, those are almost the only
190:11 - characters that matter. So it's looking at the context of those two. And then we're sort of
190:17 - filling in, like we're learning about the rest of the sentence based on just the, uh, sentiment of
190:23 - those attention scores because they're so high priority because they multiply together to such
190:28 - a high degree. We want to emphasize them and then basically let the model learn more about which
190:35 - words matter more together. So that's pretty much just what the softmax does. It increases our
190:41 - confidence in attention. And then a matrix multiply will we go back to our V here and this is a value.
190:49 - So essentially what this is, is just a linear transformation and we apply this on our, uh,
190:56 - we apply this on our inputs and it's just going to have some value about, you know,
191:02 - uh, what exactly those tokens are. And after we've gotten all of our attention, our softmax,
191:09 - everything done, it's just going to multiply the original values by everything we've gotten so far,
191:16 - just so that you don't have any information that's really lost or we don't have anything scrambled.
191:21 - Just that we have like a general idea of, okay, these are actually all the tokens we have.
191:25 - And then these are, uh, which ones we found interesting, the attention scores. So yeah,
191:34 - we have an output, just a blend of input vector values and attention placed on each token. And
191:40 - that's pretty much what's happening in scaled dot product attention in parallel. So we have a bunch
191:44 - of these that are just happening at the same time. Um, any of these happening at the same time. And
191:50 - yeah, so that's what attention is. That's what feed forward networks are. That's what residual
191:57 - connections are. Uh, and yeah. And then so after this, after we've, you know, fed these into our,
192:05 - our decoders, get an output, we apply a linear transformation to summarize softmax probabilities
192:13 - and then we generate based on that, based on everything that we learned.
192:16 - And actually what I didn't quite write a lot about was the decoder. So what I'm actually
192:23 - going to talk about next is something I didn't fill in yet, which is why, why the heck do we
192:30 - use mass attention here, but not in these places? So why the heck do we have a multi attention here
192:36 - or that attention here, but mass attention here? So why is this? Well, the purpose of the encoder
192:42 - is to pretty much learn the present, past and future and put that into a vector representation
192:51 - for the decoder. That's what the encoder does. So it's okay if we look into the future and understand
192:56 - tokens that way, because we're technically not cheating. We're just learning the different
193:00 - attention scores. Uh, and yeah, we're just using that to help us predict based on, you know,
193:06 - what the sentence looks like, but not explicitly giving it away, just giving it an idea of,
193:11 - you know, what to look for type of thing. And then we use mass attention here because,
193:17 - well, we don't want to look at it. We just want to, we want to look at the present and the past.
193:23 - And later on says, see, we're not, we're not given anything explicit explicit here. We're
193:29 - not given anything yet. So we want to make some raw guesses. They're good. They're not going to
193:34 - be very good guesses at first. We want to make some raw guesses. And then later on we can feed
193:40 - these, the added and normalized guesses into, uh, into this next multi-head attention, which,
193:47 - which isn't masked. And then we can use this max multi-head attention with the vector representation
193:55 - given by the encoder. And then we can sort of do more useful things with that rather than just
194:00 - being forced to guess, you know, raw attention scores, and then being judged for that, we can
194:05 - sort of introduce more, uh, more and more elements in this decoder block to help us learn more
194:11 - meaningful things. So we start off with taking this, uh, mass multi-head attention and then
194:21 - combining that with our, uh, we then afterwards we do a multi-head attention with the vector
194:30 - representation from the encoder. And then we can make decisions on that. So that's kind of why
194:35 - that, that works this way. Uh, if I, if you don't think I explained it like amazingly well, you can
194:41 - totally just, you know, ask GPT-4 about it or GPT-3.5 and get a pretty decent answer, but that's how
194:49 - that works. And, uh, yeah, another thing I kind of wanted to point out here is these linear
194:56 - transformations that you see, uh, I mean, there's, there's a lot of them in the, uh, what is it,
195:02 - scaled dot product attention. So you have your, uh, linears for your value or key value and key
195:09 - query and values. So these linears as well as the one up here, linears are great for just expanding
195:17 - or shrinking a bunch of important info into something easier to work with. So if you have
195:23 - a bunch of large, if you have a large vector containing a bunch of info learned from this
195:28 - scaled dot product attention, you can, you can sort of just compress that into something more
195:35 - manageable through a linear transformation. And that's essentially what's just happening here for
195:40 - the Softmax as well as in our, uh, scaled dot product attention here for these, uh, linear
195:47 - transformations from our inputs to, uh, quick keys, queries, and values. That's all that's happening.
195:55 - Uh, yeah, if you want to read more about, you know, linear transformations, the importance of them,
196:00 - you can totally go out of your way to do that, but that's just sort of a brief summary as to
196:04 - why they're important, just shrinking or expanding, uh, factors. So that's sort of a brief overview on
196:10 - how transformers work. However, in this, uh, course we will not be building the transformer
196:16 - architecture. We'll be building something called a GPT, which you're probably familiar with. And
196:21 - GPT stands for Generatively Pre-trained Transformer or Generative Pre-trained Transformer, one of the
196:27 - two. And pretty much what this is, it's pretty close to the transformer, uh, this architecture here,
196:34 - except it only adopts, uh, the decoder blocks and it takes away this multi-head attention here.
196:40 - So all we're doing is we're removing the encoder as well as what the encoder plugs into.
196:46 - So all we have left is just some inputs, our maximal multi-head attention, our post-norm
196:56 - architecture. And then right after this, we're not going to a non-mass multi-head attention,
197:02 - but rather to a feed forward network and then a post-norm. So that's all it is. It's just one,
197:07 - two, three, four. That's all it's going to look like. That's all the blocks are going to be.
197:12 - Uh, it is still important to understand the transformer architecture itself because you
197:16 - might need that in the future. And it is sort of a good practice in language modeling to
197:22 - have a grasp on and to understand, you know, why we use mass multi-head attention
197:27 - in the decoder and why we don't use it in the encoder and stuff like that. So anyways,
197:32 - we're going to go ahead and build this. Uh, if, if you need to look back, if something wasn't quite
197:37 - clear, definitely skip back a few seconds or a few minutes through the video and just make sure you
197:43 - clarify everything up to this point. Uh, but yeah, I'm going to go over some more math on the side
197:49 - here and just some other little, uh, little widgets we're going to need for building the
197:55 - decoder GPT architecture. So let's go ahead and do that. Before we actually jump into
198:01 - building the transformer method, building the GPT from scratch, what I want to do is linger on
198:06 - self-attention for a little bit, or rather just the attention mechanism and the matrix
198:11 - multiplication behind it and why it works. So I'm going to use whiteboard to illustrate this.
198:18 - So we're going to go ahead and draw out a, uh, we'll just use maybe a, a four token sequence
198:24 - here of words. My dog has fleas. Okay. So we're going to highlight which words are probably
198:35 - going to end up correlating together or, uh, the attention mechanism is going to multiply them
198:42 - together to a high amount based on what it learns about those tokens. This is what this is. So I'm
198:47 - going to help us illustrate that and what the, uh, GPT is going to see sort of from the inside,
198:53 - what it looks like from the inside. So I'm going to go ahead and draw this out here.
198:58 - So just make a table here. We'll give it
199:11 - four of these and then draw a little line through the middle.
199:17 - My drawing might not be perfect, but it's definitely better than on paper.
199:21 - Okay. So cool. We have this, we have my, oh, let me go here. Dog
199:33 - has fleas and then my, my dog. Oh, let's delete that. My dog has fleas. Cool. So to what degree
200:02 - are these going to interact? Well, my and my, I mean, it doesn't really give away that much.
200:07 - It's only just the start. So maybe it is, it'll interact to a low amount. And then you have
200:13 - my and dog. These might interact to a medium amount because it's like your dog. So we might go,
200:21 - you might go medium like that. And then mine has, well, that doesn't give away too much. So maybe
200:26 - that'll be low. And then my and fleas, it's like, oh, that doesn't really mean much. My fleas,
200:32 - that doesn't really make sense. Maybe we'll have it interact to a low amount. And then
200:39 - these would be the same thing. So my and dogs would be medium and then has and has would be low.
200:48 - And then mine fleas would also be low. And then you have dog and dog. So these might interact
200:53 - to a low amount. They're the same word. So we'll just forget about that. And then we have a dog
201:00 - has. So these might interact to a medium amount. Dog has the dog has something.
201:07 - And then dog and fleas. These might interact to a high amount because they're associating the dog
201:13 - with something else, meaning fleas. We have has and dog. These would interact to the same amount.
201:18 - So medium and then has and has be probably to a low amount. And then we could do low or we could
201:31 - do what was it high for this one as well fleas and dog. So these will interact to a high amount.
201:38 - And then we have has and fleas. So these could interact maybe a medium amount,
201:45 - medium and then fleas and fleas which would be low. So what you get, I'll just highlight this in
201:53 - I'll just highlight this in green here. So you get all the medium and high attention scores.
202:00 - You'd have your medium here, medium here, high, medium, medium, high, medium and medium. So you
202:11 - can see these are sort of symmetrical. And this is what the attention map will look like. Of course,
202:16 - there's going to be some scaling going on here based on the amount of actual attention heads
202:21 - we have running in parallel. But that's besides the point. Really what's going on here is the
202:28 - network is going to learn how to place the right attention scores because attention is simply being
202:35 - used to generate tokens. That's how the GPT works. It's using attention to generate tokens.
202:42 - So we can make those sort of attention scores how they're placed. We can make those learnable
202:50 - through all of the embeddings like everything we have in the entire network can make sure that
202:56 - we place effective attention scores and to make sure that they're measured properly.
203:00 - So obviously I didn't quantify these very well, like not with floating point numbers, but this
203:07 - is sort of the premise of how it works and how we want the model to look at different tokens and
203:12 - how they relate to one another. So that's what the attention mechanism looks like under the hood.
203:19 - So this is what the actual GPT or decoder only transformer architecture looks like.
203:25 - And yeah, so I'm just going to go through this step by step here and then we can
203:29 - hopefully jump into some of the math and code behind how this works.
203:33 - So we have our inputs, embeddings and positional encodings. We have only decoder blocks and then
203:40 - some linear transformation and then pretty much just we do some softmax probability distribution.
203:48 - We sample from those and then we start just generating some output and then we compare
203:53 - those to our inputs and see how off they were optimized from that. In each of these decoder
203:58 - blocks we have our multi add attention, res connections, feed forward network consists of
204:04 - a linear, relu linear in that order and then another res connection. In each of these multi
204:11 - add attentions we have multiple heads running in parallel and each of these heads is going to take
204:18 - a key query and value. These are all learnable linear transformations and we're going to
204:26 - basically dot product the key and query together. Concatenate these results and do a little
204:33 - transformation to sort of summarize it afterwards and then what actually goes on in the dot product
204:38 - tension is just the dot product meaning of the key and query, the scaling to prevent these values
204:44 - from exploding, to prevent the vanishing gradient problem and then we have our masking to make sure
204:51 - that these, to make sure the model isn't looking ahead and cheating and then softmax matrix
204:57 - multiply we output that and then kind of fill in the blank there. So cool, this is a little bit
205:05 - pretty much the transformer architecture, a little bit dumbed down, a little smaller in
205:10 - complexity to actually understand but that's kind of the premise of what's going on here.
205:14 - So still implements a self-attention mechanism. So as you can see now I am currently on my MacBook
205:27 - M2 chip. I'm not going to go into the specs of why it's important but really quick
205:32 - I'm just going to show you how I SSH onto my other PC. So I go SSH just like that and then I type in
205:40 - my ipv4 address and then I just get a simple password from here, password that I've memorized.
205:52 - Cool, so now I'm on my desktop computer and this is the command prompt that I use for it.
205:57 - So awesome, I'm going to go ahead and go into the free code camp, a little directory I have.
206:04 - So cd desktop, cd python testing and then here I'm actually going to activate my
206:11 - CUDA virtual environment, oop not accelerate, we go CUDA, activate, cool and then I'm going to go
206:22 - cd into free code camp gbt course, awesome. So now if I actually do code on here like this to
206:29 - open up my VS code it doesn't do that. So there's another little way I have to do this and you have
206:35 - to go into VS code, go into a little remote explorer here and then you can simply connect.
206:42 - So I'm just going to connect to the current window itself. There's an extension you need for this
206:49 - called open ssh server, I think it's what it's called and simply the same password I used in
206:55 - the command prompt, I can type it correctly. Awesome, so now it's ssh into my computer upstairs
207:10 - and I'm just going to open the little editor in here.
207:15 - Nice, so you can see that it looks just like that, that's wonderful.
207:19 - So now I'm going to open this into Jupyter notebook, actually
207:27 - cd into desktop here, cd python testing, CUDA scripts, activate, cd free code camp gbt course
207:39 - and then code like that and it will open. Perfect, how wonderful is that and I've already done a
207:46 - little bit of this here but we're going to jump into exactly how we can build up this
207:53 - transformer or gbt architecture in the code itself. So I'm going to pop over to my Jupyter notebook
208:02 - in here. Cool, I know this little address, I'm going to paste that into my browser.
208:12 - Awesome, so we have this gbtv1 Jupyter notebook. So what I've actually done
208:23 - is I've done some importations here, so I've imported all of these python importations,
208:32 - all the hyper parameters that we used from before, I've imported the data loader,
208:38 - I've imported the tokenizer, the train and bell splits, the get batch function,
208:44 - estimate loss, just everything that we're going to need and it's all in neatly organized little
208:49 - code blocks. So awesome, now what? Well let's go ahead and continue here with the actual upgrading
209:00 - from the very top level. So I remember I actually showed, and you can skip back to this,
209:07 - I actually showed the architecture of the gbt sort of lined out in a little sketch that I did
209:18 - and all we're going to do is pretty much build up from the high level, the high high level
209:23 - general architecture down to the technical stuff, down to the very root dot product attention that
209:30 - we're going to be doing here. So I'm going to go ahead and start off with this gbt language model
209:37 - which I just renamed, I replaced bigram with gbt here. So that's all we're doing and I'm going to
209:47 - add some little code bits and just walk through step by step what we're doing. So let's do that.
209:55 - So great, next we're going to talk about these positional encodings. So I go back to the paper
210:02 - here, rather this architecture. We initially have our tokenized inputs and then we give them
210:10 - embedding, so token embeddings and then a positional encoding. So this positional encoding,
210:14 - going back to the attention paper, is right here. So all it does is every even token index,
210:23 - we apply this function and then every odd token index we apply this function. You don't really
210:27 - need to know what it's doing other than the fact that these are the different sine and cosine
210:32 - functions that it uses to apply positional encodings to the tokenized inputs. So on our
210:41 - first index or whatever, let's say we have hello world. There's five characters here, h will be
210:47 - index zero, so it'll get an even encoding function and then e will be odd since it's index one. So
210:55 - it'll get this one and then l will get this, the next l will get this and then, or I don't know if
211:01 - I messed up that order, but essentially it just iterates and it goes back and forth between those
211:06 - applying these fixed functions. The thing is with fixed functions is that they don't actually learn
211:13 - about the data at all because they're fixed. So another way we could do this would be using
211:18 - nn.embedding which is what we use for the token embedding. So I'm going to go ahead and implement
211:25 - this here in our GBTV1 script. So I'm going to go ahead and add on this line self.position
211:33 - embedding table nn.embedding block size. So the block size is the length or the sequence length
211:41 - which in our case it's going to be eight. So there's going to be eight tokens and this means
211:48 - we're going to have eight different indices and each one is going to be of size n.embed and this
211:54 - is a new parameter I actually want to add here. So nn.embed will not only be used in positional
212:01 - embedding but it will also be used in our token embedding because when we actually store
212:08 - information about the tokens we want that to be in a very large vector. So not necessarily
212:14 - a probability distribution or what we were using before in the Bagram language model
212:19 - but rather a really large vector or a list you could think about it as a bunch of different
212:26 - attributes that are about a character. So maybe you know a and e would be pretty close because
212:34 - they're both vowels versus like e and z would be very different because z is not a very common
212:40 - letter and e is the most common letter in the alphabet. So we pretty much just want to have
212:46 - vectors to differentiate these tokens to place some semantic meaning on them and anyways that's
212:54 - a little talk about what token embedding table is going to do when we add n.embed and then
212:59 - positional embedding table is just the same thing but instead of each character having its own thing
213:06 - each letter index in the input is going to have its own embedding. So I can go and add this up here
213:13 - the n underscore embed and we can just make this maybe 384. So 384 is quite huge this may be a
213:23 - little too big for your pc but we'll see in a second. So what this is going to do is it's going
213:29 - to have a giant vector it's going to be like I don't know we could say like embedding say like
213:36 - embedding vector and then it would be like be like this and you would have a bunch of different
213:42 - attributes like 0.1 0.2 0.8 1.1 right except instead of four this is 384 elements long
213:57 - and each of these is just going to store a tiny little attribute about that token. So
214:04 - let's say we maybe had like a two-dimensional and we were using a word. So if we had sad
214:10 - versus happy sad might be sad might be 0.1 and then 0.8 or 0.8 whereas happy sad would be sad
214:30 - would be maybe the the positivity of what it's saying and then 0.8 would be uh is it showing
214:36 - some sort of emotion which is a lot right it's 80 emotion and 0.1 of maybe positive sentiment
214:45 - and then if we had 0.9 would be happy because it's it's happy it's very good
214:51 - and then 0.8 is emotional because they're sort of the same on the emotional level.
214:56 - But yeah so this is what our embedding vectors are pretty much describing and
214:59 - and all this hyper parameter is concerned with is how long that vector actually is.
215:06 - So anyways let's continue with the GPT language model class. So the next bit I like to talk about
215:13 - is how many decoder layers we have. So in here let's just say we have four decoder layers all
215:20 - right so we have four of these it's going to go through this one and then this one and then this
215:24 - one and then this one this is all happening sequentially. So we could actually make a little
215:30 - sequential neural network with four decoder layers. So I'm actually going to add this in
215:36 - and then a little bit extra code which I'll explain in a second here. So this uh self
215:42 - dot blocks is how many decoder blocks we have running sequentially or layers blocks and layers
215:49 - can be used interchangeably in this context. But yeah we have an nn dot sequential and this
215:54 - asterisk is pretty much saying we're going to repeat this right here for how many n layer is
216:03 - and then layer is another hyper parameter we're going to add. We go n underscore layer equals four
216:09 - okay so n underscore layer equals four that means it's going to make four of these uh I guess blocks
216:19 - or layers sequentially. It's going to make four of them uh and this little block thing we're going
216:24 - to build on top of this in a second here we're going to make an actual block class and I'm going
216:29 - to explain what that does. Uh but for now this is going to be some temporary code as long as you
216:34 - understand that this is what uh this is how we create our four layers our four decoder layers
216:40 - that's all you need to know for now. I'm going to move more into this block uh later. Uh as for this
216:46 - self dot layer norm final this is a final layer norm all this is going to do is we're just simply
216:54 - going to add this to the end of our network here uh just simply at the end here and all this is
217:04 - going to do is just going to help the model converge better. Layer norms are super useful
217:10 - and yeah so you'll see more how that works. I'll actually remove it later on and we'll actually
217:16 - compare and see how good uh it actually does and you can you can totally go out of your way
217:23 - to experiment with different normalizations and see how well the layer norm helps the model
217:30 - perform or how well the loss sort of converges over time when you put the layer norm in different
217:35 - places. So let's go back here and now we have this uh end here which is the language uh I believe
217:47 - this is language modeling head or something uh again this is this is what andre carpath used
217:52 - I'm assuming that means language modeling head uh but pretty much all we're doing is we're just
217:58 - projecting and we're doing this final uh transformation here this final uh little
218:04 - linear layer here from all of these sequential decoder outputs and we're just going to transform
218:12 - that to something that the softmax can work with so we have our layer norm afterwards to
218:18 - sort of normalize help the model converge after all these after all this computation
218:23 - we're going to feed that into a linear layer to make it I guess softmax
218:29 - workable so the softmax can work with it and yeah so we're just simply projecting it from
218:38 - an embed which is the vector length that we get from our decoder and uh and this vocab size so
218:48 - the vocab size is going to essentially give up a little probability distribution on each token
218:54 - that we have or the vocabulary so anyways I'm going to make this back to normal here and we're
219:02 - going to just apply this to the forward pass so a little thing I wanted to add on to uh this
219:10 - positional embedding or rather just the idea of embeddings versus the fixed definite function of
219:18 - that the sinusoidal functions and the cosine functions that we used here these are both
219:26 - actually used in practice the reason I said we're going to use embeddings is because we just want
219:31 - it to be more oriented around our data however in practice sinusoidal encodings are used in
219:38 - base transformer models whereas learned embeddings what we're using are used in variants like GBT
219:45 - and we are building a GBT so we're probably going to find out a performance from learning
219:50 - able embeddings and this is just uh summing up the experts do it's a little practice that experts do
219:56 - when they're building transformer models versus variants like GBTs so that's just a little
220:01 - background on why we're using learnable embeddings so now let's continue with the forward pass here
220:10 - so I'm going to paste in some marcode and let me just make sure this is formatted properly cool
220:20 - so we have this token embedding which is our token embedding table we take an idx
220:27 - we get our token embedding here then what we do with this positional embedding table so we have
220:32 - this torch dot arrange we make sure this is on the CUDA device uh the GPU device so it's in parallel
220:39 - and all this is going to do is it's going to look at how long is t and let's say t is uh t is our
220:47 - block size so t's going to be eight so all it's going to do is give us eight indices it's going
220:52 - to be like zero one two three four five six seven there's eight of those and we're essentially just
220:58 - going to give each of those uh each of those indices a different uh a different
221:07 - n embedding vector for each of those indices just a little lookup table and uh that's what that is
221:15 - so all we do now is it's actually quite simple this is a very efficient way to do it is you
221:20 - just add these two together so uh torch broadcasting rules which you might want to look into i'll
221:27 - actually search that up right now uh torch uh also we'll search broadcasting semantics pie torch
221:39 - broadcasting i cannot spell broadcasting semantics so uh these are a little bit funky
221:49 - when you look at them the first time but pretty much these are just rules about how you can do
221:55 - arithmetic operations and just operations in general to tensors so tensors are like
222:01 - you think of matrices where it's like a two by two tensors can be the same thing but they could be
222:06 - like a uh a two by two by two or a two by two by two by two by two by two whatever dimension you
222:12 - want to have there and pretty much it's just rules about how you can uh have two of those weirdly
222:22 - shaped tensors and do things to them so uh just some rules here i would advise you familiarize
222:30 - yourself with these even play around with it if you want just for a few minutes and just get an
222:35 - idea for uh which like just try to multiply tensors together and see which ones throw errors
222:41 - and which ones don't so it's a good idea to understand how broadcasting rules work
222:45 - uh obviously this this term is a little fancy and it's like oh that that's like a crazy advanced
222:52 - term uh not really just it's pretty much just some rules about how you're multiplying these
222:59 - really weirdly shaped tensors so yeah uh anyways if we go back to here uh
223:06 - uh we are allowed to broadcast these we're allowed to actually add them together so the
223:12 - positional embedding and the token embedding we get extra from this which is a b by t by c shape
223:19 - so now what we can do with these is we can actually feed it in to the uh gpt or i guess
223:27 - sort of a transformer network if you want to say that so we have these embeddings and positional
223:33 - encodings we add these together and then we feed them into our sequential network so how are we
223:39 - doing this well we go self dot blocks which is up here and we essentially just feed an x which is
223:46 - literally exactly what happens here we have our tokenized inputs we get our embeddings and our
223:51 - positional encodings through learnable embeddings we add them together and then we feed them into
223:56 - the network directly so that's all that's happening here and that's how we're feeding an x which is
224:02 - the output of these then uh after all of the this is like way after we've gotten through all of these
224:09 - Trent uh all these gpt layers or blocks we do this final layer norm and then this linear transformation
224:17 - to get it to a softmax uh to get it to essentially probabilities that we can feed into our softmax
224:24 - function and then other than that this forward pass is exactly the same other than this little
224:30 - block of code here so if this makes sense so far that is absolutely amazing let's continue i'm
224:36 - actually going to add a little bit of uh in practice some some little weight initializations
224:44 - that we should be using in our language model uh in a module subclass so i'm going to go over
224:52 - a little bit of math here but this is just really important for practice and to make sure
224:57 - that your model does not fail in the training process this is very important that's going to
225:02 - be a little little funky on the on the conceptualizing but yeah bring out some pen and paper
225:08 - and do some math with me we've built up some of these initial gpt language model architecture
225:14 - and before we continue building more of it and the other functions some of the math stuff that's
225:19 - going on in the parallelization that's going on in the script i want to show you some of the math
225:24 - that we're going to use to initialize the weights of the model to help it train and converge better
225:30 - so there's this new thing that i want to introduce called standard deviation
225:35 - and this is used in intermediate level mathematics the symbol essentially looks like this population
225:41 - standard deviation so n the size so it's just going to be an array the length of the array
225:48 - and then x i we iterate over each value so x at position zero x at position one
225:54 - x at position two and then this u here is the mean so essentially we're going to
226:04 - iterate over each element we're going to subtract it by the mean we're going to square that and
226:11 - then keep adding all these squared results together and then once we get the sum of that
226:16 - we're going to subtract or we're going to divide this by the number of elements there are and then
226:23 - once we get this result we're going to square root that so this this symbol here might also look a
226:29 - little bit unfamiliar and let me just illustrate this out for you so we go to our whiteboard and
226:36 - this e looks like looks like that let's just say we were to put in x i like that and our array
226:51 - let's just say for instance our array is 0.1 0.2 0.3 so what would the result of this be
227:00 - well if we look at each element iteratively add them together so 0.1 plus 0.2 plus 0.3
227:07 - well we get 0.6 from that so this would essentially be equal to 0.6 that's what that equals we just
227:18 - add each of these up together or we do whatever this is iteratively whatever this element is we
227:24 - iterate over the number of elements we have in some arbitrary array or vector or list or whatever
227:32 - you want to call it and then we just sort of look at what's going on here and we can do some basic
227:38 - arithmetic stuff so let's walk through an exhibit let's walk through a few examples just to
227:43 - illustrate to you what the results look like based on the inputs here so i'm going to go back to my
227:50 - whiteboard we're going to draw a little line here just to separate this and let's go down so
227:58 - i want to calculate the standard deviation do standard deviation of
228:06 - and then we'll just make some random array negative 0.38 negative 0.38
228:14 - 0.52 and then 2.48 cool so we have this array this is three elements so that means n is going
228:26 - to be equal to three let me drag this over here so n is the number of elements so n is going to be
228:32 - equal to three our mean well our mean is just we add all these up together and then we average them
228:42 - so our mean is going to be equal to let's just say 0 negative 0.38 plus 0.52 plus 2.48
228:56 - and then divided by 3 and the answer to this i did the math ahead of time is a problem is
229:02 - it is literally 0.873 repeated but we're just going to put 0.87 for simplicity's sake cool so
229:09 - the mean of this is 0.87 and n is equal to three now we can start doing some of the other math so
229:18 - we have this we have this o has a cool line and we do square root
229:28 - one over n which is equal to three and then we multiply this by
229:35 - sigma that's what this six uh that's what this symbol is that's sigma that's the name for it
229:42 - and then we go x i minus and then our mean of 0.87 apologies for this sloppy writing
230:01 - and then we square that so cool let me drag this out awesome so let's just do this step by step
230:11 - here so the first one is going to be 0.38 so we have 0. or negative 0.38 and we're going to do
230:23 - minus the mean here so minus 0.87 and i'm just going to wrap all this in brackets so that we
230:29 - don't miss anything kind of wrap it in brackets and then just square it and see what we get after
230:35 - so i'm just going to write all these out then we can do the calculations so next up we have
230:39 - 0.52 minus 0.87 we'll square that and then next up we have 2.48 minus 0.87
230:53 - and then we square that as well so awesome what is the result of this the result of
231:01 - negative 0.38 minus 0.87 squared is 1.57 the result of this line is 0.12 these again these
231:15 - are all approximations they're not super spot on we're just doing this to understand what's going
231:21 - on here just to overview the function not for precision then the next one is going to be 2.59
231:28 - and you can double check all these calculations if you'd like i have done these preemptively so
231:35 - that is that and now from here all we have to do is add each of these together so 1.57 plus 0.12
231:45 - plus 2.59 divided by 3 is
231:55 - 1.57 plus 0.12 plus 2.59 all that divided by 3 is going to be equal to 1.42
232:05 - and then keep in mind we also have to square root this so the square root of that
232:10 - is going to be 0 or 1.19 approximately we'll just add this guy ahead of it little approximation
232:21 - thing and so that's what the standard deviation of this array is zero negative 0.38 0.52 2.48
232:31 - standard deviation is approximately 1.19 awesome let's do another example so let's say
232:42 - i want to do the standard deviation of 0.48 0.50 i guess 0.52 cool so there's a little pattern here
233:06 - just goes up by 0.02 each time and you're going to see why this is vastly different than the other
233:13 - example so let's walk through this so first of all we have n
233:21 - n is equal to 3 cool what does our mean our mean well if you do our mean our mean is 0.5
233:30 - if you do 0.48 plus this plus that and divided by three that's going to be 0.5 and if you're
233:38 - good with numbers you'll probably already be able to do this in your head but that's okay if not
233:44 - next up we're going to do this in the formula so what what does these what do these iterations
233:51 - look like so zero point let's just do these in brackets the old way minus 0.0.5 squared
234:04 - the next one is 0.5 minus 0.5 squared which we already know is zero
234:11 - and then this one is 0.52 minus 0.5 squared so the result of 0.48 minus 0.5 squared
234:24 - and we'll just write equals here is going to be approximately 0.02 squared so that'd be 0.004
234:35 - like that so i'll make this not actually overlap 0.004 and then this one we obviously know would be
234:43 - zero because 0.5 minus 0.5 that's zero then you square zero still the same thing and then this one
234:50 - is 0.0004 as well so when we add these two together we're going to get 0.0008 just like that and then
235:05 - if we divide them by three or whatever n is then we end up getting 0.00026 repeating so i'll just
235:15 - write two six six like that and so all we have to do at this point is just find the square root of
235:24 - this and we'll just do square root of 0.0026 approximately and that's going to be equal to
235:36 - about 0.0163 so that is our standard deviation of both of these arrays here so 0.048 and then 5
235:49 - 0.5 and then 0.52 our standard deviation is 0.0163 so very small and then we have
236:00 - negative 0.38 0.52 and 2.48 we get a standard deviation of 1.19 so you can see that these
236:07 - numbers are vastly different one is like one is literally a hundred times greater than the other
236:15 - so the reason for this is because these numbers are super diverse they uh i guess another way
236:24 - you could think of them is that they they stretch out very far from the mean so this essentially
236:31 - means when you're initializing your parameters that if you have some outliers then your network
236:38 - is gonna your network is gonna be funky because it's the learning process just messed up because
236:44 - you have outliers and it's not just learning the right way it's supposed to whereas if you had way
236:50 - too small of a standard deviation from your initial parameters like in here but maybe even
236:56 - smaller so let's say they were all 0.5 right then all of your neurons would effectively be the same
237:04 - and they would all learn the same pattern so then you would have no learning done so one would
237:10 - either be you're learning a super super unstable and you have outliers that are just learning
237:16 - very distinct things and not really not really not really letting other neurons get opportunities
237:23 - to learn or rather other parameters to learn you yeah if you have a lot of diversity you just have
237:30 - outliers and then if you have no diversity at all then essentially nothing is learned
237:37 - and your network is useless so all we want to do is make sure that our standard deviation
237:42 - is balanced and stable so that the training process can learn effective things so each neuron
237:49 - can learn a little bit so you can see here this would probably be an okay standard deviation if
237:54 - these were some parameters because they're a little bit different than each other they're
237:58 - not all like super super close to the same and yeah so essentially what this looks like in
238:07 - code here is the following so you don't actually need to memorize what this does as it's just used
238:15 - in practice by professionals but essentially what this does is it initializes our weights around
238:23 - certain standard deviations so here we set it to 0.02 which is pretty much the same as what we had
238:30 - in here so point point yeah this one's a little bit off in the standard deviation we set here
238:39 - but essentially we're just making sure that our weight our weights are initialized properly
238:46 - and you don't have to memorize this at all it's just used in practice and it's going to help our
238:50 - training converge better so as long as you understand that we we can apply some initializations
238:56 - on our weights that's all that really matters so cool let's move on to the next part of our GBT
239:03 - architecture so awesome we finished this GBT language model class everything's pretty much
239:08 - done here we did our init we did some weight initializations and we did our forward pass
239:13 - so awesome that's all done now let's move on to the next which is the block class so what is block
239:21 - block well if we go back to this diagram each of these decoder blocks is a block so we're pretty
239:28 - much just going to fill in this gap here our GPT language model has these two where we get our
239:33 - tokenized inputs and then we do some transformations and a softmax after and essentially we're just
239:40 - filling in this gap here and then we're going to build out and just sort of branch out until it's
239:44 - completely built so let's go ahead and build these blocks here what does this look like
239:49 - like that's what this does so we have our init we have a forward pass as per usual init
239:58 - and a forward pass as seen in the GPT language model class though all them are going to look
240:02 - like this forward and an init so the init is going to just initialize some things it's going to
240:09 - initialize some some transformations and some things that we're going to do in the forward pass
240:14 - that's all it's doing so what do we do first well we have this new head size parameter introduced
240:21 - so head size is the number of features that each head will be capturing in our multi-head attention
240:28 - so all the heads in parallel how many features are each of them capturing so we do that by
240:34 - dividing n embed by n head so n head is the number of heads we have and n embed is the number of
240:41 - features we have or we're capturing so 384 features divided by four heads so each head is going to be
240:48 - capturing 96 features hence head size so next up we have self.sa which is just short for self
240:58 - attention we do a multi-head attention we pass in our n head and our head size and you'll see how
241:03 - this how these parameters fit in later once we build up this multi-head attention class so cool
241:10 - now we have a feed forward which is as explained just in the diagram here our feed forward is just
241:18 - this which we're actually going to build out next and we have some and we have two layer norms and
241:24 - these are just for the post norm slash pre-norm architecture that we could implement here in this
241:30 - case it's going to be a post norm just because I found that it converges better for this for this
241:37 - course and the data that we're using and just the model parameters and whatnot it just works better
241:42 - so also that is the original architecture that we use in the attention paper so you might have seen
241:50 - that they do an add a norm rather than a norm an add so anyways we've initialized all of these
241:57 - cool so we have head size self attention feed forward and then two layer norms so in our forward
242:04 - pass we do our self attention first let's actually go back to here so we do our our self attention
242:10 - then add a norm then a feed forward and then add a norm again so what does this look like self
242:18 - attention add a norm feed forward add a norm cool so we're doing an add so we're going x plus the
242:27 - the previous answer which is adding them together and then we're just applying a layer norm to this
242:32 - so cool if you want to look up more into what layer norm does and everything and why it's so
242:36 - useful you can totally go out of your way to do that but layer norm is essentially just going to
242:44 - help smoothen out our features here so we have this and honestly there's not much else to that
242:52 - we just return this final value here and that's pretty much the output of our blocks so next up
242:58 - I'm going to add a new little code block here which is going to be our feed forward so let's
243:06 - go ahead and do that so feed forward just going to look exactly like this it's actually quite simple
243:12 - so all we do is we make an nn dot sequential uh torch dot nn we make this sequential network
243:18 - of linear linear relu and then linear so in our linear we have to pay attention to the shapes
243:25 - here so we have n in bed and then n in bed times four and then the relu will just essentially
243:32 - what the relu will do is it looks it looks like this help me let me illustrate this for you guys
243:39 - so essentially you have this graph here and let's just make this a whole plane actually
243:48 - so all of these values that are below zero all these values that are below zero on the x-axis
243:59 - and equal to zero will be changed just to zero like that so you have all these values that look
244:04 - like this and then everything that is above zero just stays the same so you essentially just have
244:09 - this funny looking shape it's like straight and then diagonal that's what the relu function does
244:14 - it looks at a number sees if it's equal to or less than zero if that's true we give that number zero
244:21 - and if it's not then we just leave the number alone so cool very cool non-linearity function
244:28 - you can read papers on that if you'd like but essentially the shape of this just doesn't
244:35 - matter all we're doing is we're just making sure that we're just converting some values if they're
244:39 - equal to or below zero that's all this is doing and then we essentially are multiplying this
244:47 - we're doing this linear transformation times this one so we have to make sure that these inner
244:53 - we have to make sure that these inner dimensions line up so four times n embed and four times n
244:57 - embed those are equal to each other so our output shape should be n embed by n embed cool so now we
245:05 - have our dropout and in case you don't know what dropout is it pretty much just makes a certain
245:11 - percentage of our neurons just drop out and become zero this is used to prevent overfitting and some
245:19 - other little details that i'm sure you could you could figure out through experimenting
245:25 - so all this actually looks like in a parameter form is just dropout
245:31 - dropout equals we'll just say 0.2 for the sake so 0.2 means 20% or 0.2 is going to yeah so 0.2
245:46 - in percentage form is just going to drop out 20% of our neurons turn them to zero to prevent
245:52 - overfitting that's what that's doing so cool we have our feed forward network we drop out after
245:58 - to prevent overfitting and then we just call it forward on this sequential network so cool feed
246:04 - forward pretty self-explanatory let's jump into the next piece we're going to add the multi-head
246:09 - attention class so we've built all these decoder blocks we've built inside of the decoder blocks
246:15 - we built the feed forward and our res connections and now all we have to do left in this block is
246:21 - the multi-head attention so it's going to look exactly like this here we're going to ignore the
246:26 - keys and queries for now and save this for dot product attention so we're gonna yeah essentially
246:32 - just make a bunch of these multiple heads and we're going to concatenate results and do a linear
246:40 - transformation so what does this look like in code well let's go ahead and add this here
246:46 - all that attention cool it's a multiple hedge of attention in parallel i explained this earlier
246:51 - so not going to jump into too much detail in that but we have our knit we have our forward
246:57 - and what are we doing in here so our self dot heads is just a module list and module list is
247:04 - kind of funky i'll dive into it a little bit later but essential we're doing is we're having
247:10 - a bunch of these heads essentially in parallel for each head so num heads let's say our our
247:17 - num heads is set to our num heads is set to maybe four in this block we do multi-head attention we
247:28 - do n heads and then head size so n heads and then head size so num heads essentially what it is
247:35 - so for the number of heads that we have which is four we're going to pretty much make one
247:40 - headed running in parallel so four heads running in parallel is what this does here then we have this
247:47 - projection which is essentially just going to project the head size times the number of times
247:56 - the number of heads to a n embed and you might ask well that's weird because num heads times
248:03 - this is literally equal to an embedding if you go back to the math we did here and the purpose of
248:11 - this is just to be super hackable so that if you actually do want to change these around it won't
248:15 - be throwing you dimensionality errors so that's what we're doing just a little projection from our
248:22 - whatever these values are up to this constant feature length of an embed so then we just follow
248:31 - that with a dropout dropping out 20 percent of the networks neurons now let's go into this forward
248:37 - here so forward torch dot concatenate or torch dot cat we do four h and self dot heads so we're
248:45 - going to concatenate each head together along the last dimension and the last dimension in this case
248:52 - is the B batch by time by we just say feature dimension or channel dimension the channel
249:03 - dimension here is the last one so we're going to concatenate along this feature dimension
249:10 - and let me just help you illustrate what exactly this looks like so when we concatenate along these
249:18 - we have this B by T and then we'll just say our features are going to be H1 like our each of our
249:29 - heads here another H1 H1 H1 these are all just features of head one and then our next would be
249:37 - H2 H2 H2 H2 then if let's just say we have a third head go H3 H3 H3 H3 like that so we have
249:52 - maybe four features per head and there's three heads so essentially all we're doing
249:58 - when we do this concatenate is we're just concatenating these along the last dimension
250:03 - so to convert this like ugly list format of just each head features sequentially in order
250:11 - which is like really hard to process we're just concatenating these so they're easier to process
250:16 - so that's what that does and then we just follow this with a dropout so we do our self
250:20 - brought self dot projection and then just follow that with a dropout so cool if that didn't totally
250:28 - make sense you can totally just plug this code into chatgbt and get a detailed explanation on
250:34 - how it works if something wasn't particularly clear but essentially that's the premise you have
250:39 - your batch by sequence length or time used interchangeably and then you have your features
250:48 - which are all just in this weird list format of each feature just listed after another so cool
250:54 - so that's what multi-head attention looks like let's go ahead and implement dot product attention
251:00 - or scale dot product attention so a little something i'd like to cover before we go into
251:05 - our next scaled dot product attention was just this linear transformation here
251:10 - and you might think well what's the point if we're just transforming
251:13 - an embed to an embed right that it's just kind of weird to have the match like that
251:17 - and essentially what this does is it just adds in another learnable parameter for us
251:25 - so it has a await and a bias if we set bias to false like that then it wouldn't have a bias but
251:33 - it does have a bias so another just w x plus b if you will await times x plus a bias
251:39 - so it just adds more learnable parameters to help our network learn more about this text
251:46 - so cool i'm going to go ahead and add in this last but not least
251:53 - scaled dot product attention or head class so there's going to be a bunch of these heads
251:59 - hence the class head running in parallel and inside of here we're going to do some scaled dot
252:05 - product attention so there's a lot of code in here don't get too overwhelmed by this but i'm
252:10 - going to walk through this step by step so we have our init we have our forward awesome so what do we
252:17 - do in our architecture here so we have a key a query and a value the keys and the queries
252:26 - dot product together they get scaled by one over the square root of length of a row in the keys
252:33 - or queries matrix so we'll just say maybe keys for example the row of keys the length of a row in
252:42 - keys and then we just do our masking to make sure the network does not look ahead and cheat
252:48 - and then we do a softmax and a matrix multiply to essentially add this value weight on top of it
252:57 - so cool we do this key or keep in mind this initialization is not actually doing any
253:05 - calculations but just rather initializing the linear transformations that we will do in the
253:10 - forward pass so this self dot key is just going to transform and embed to head size bias false
253:19 - and then i mean the rest of these are just the same and embed to head size because each head
253:25 - will have 96 features rather than 384 so we kind of already went over that but that's just what
253:31 - that's doing cool that's just a linear transformation that's happening to convert from 384 to 96
253:37 - features then we have this self dot register buffer well what does this do you might ask
253:44 - register buffer is essentially just going to register this no look ahead masking in the model
253:51 - state so instead of having to reinitialize this every single head for every single forward and
253:57 - backward pass we're just going to add this to the model state so it's going to save us a lot of
254:02 - computation that way on our training so our training time is going to be reduced just because
254:06 - we're registering this yeah so it's just going to prevent some of that overhead computation of
254:13 - having to redo this over and over again you could still do training without this it would just take
254:19 - longer so that's what that's doing yeah so now we have this dropout of course and then in our
254:28 - forward pass let's let's break this down step by step here so b by t by c so batch by time by
254:36 - channel is our shape we just unpack those numbers and then we have a key which is we're just calling
254:44 - this linear transformation here on an input x and then a query which is also calling the same
254:51 - transformation but a different learnable transformation on x as well so what we get
254:56 - is this instead of b by t by c we get b by t by head size hence this transformation from
255:02 - through 384 to 96 so that's what that is and that's how these turn out here so now we can
255:09 - actually compute the attention scores so what do we do we'll just say way we'll just say weights
255:15 - is our attention weights or yeah i guess you can say that we have our queries dot product matrix
255:23 - multiply with the keys transposed so what does this what does this actually look like and i
255:32 - want to help i want to help you guys sort of understand what transposing does here so let's
255:38 - go back to here and draw out what this is going to look like so essentially what transposing is
255:46 - going to do is it is just going to make sure let me draw this out first
255:56 - so let's say you had i don't know maybe
255:59 - b a of b c d and you have a b c and d cool let's draw some lines to separate these
256:29 - so essentially what this does is the transposing puts it into this form so if we didn't have
256:42 - transpose then this would be in a different order it wouldn't be a b c d in both from like top to
256:49 - bottom left to right type of thing it would be in a different order which would essentially not allow
256:53 - us to multiply them the same way so we do a by a a times b it's like sort of a direct multiply
257:02 - if you will i don't know if you remember times tables at all from elementary school
257:07 - but that's pretty much what it is we're just setting up in a times table form and we're
257:12 - computing attention scores that way so that's what that is that's what this transposing is
257:18 - doing it is doing and all this does is it just flips the second last dimension with the last
257:25 - dimension so in our case our second last is t and our last is head size so it just swaps these two
257:33 - so we get b by t by head size and then b by head size by t we dot product these together
257:39 - also keeping in mind our scaling here which is taking this we're just taking this scaling of
257:51 - one over the square root of length of a row in the keys
257:55 - if we look at this here now there's a little analogy i'd like to provide for this
258:00 - scaling right here so imagine you're in a room with a group of people and you're trying to
258:05 - understand the overall conversation if everyone is talking at once it might be challenging to
258:11 - keep track of what's being said it would be more manageable if you could focus on one person at a
258:16 - time right so that's similar to how a multi-headed tension in a transformer works so each of these
258:22 - heads divides the original problem of understanding the entire conversation i.e. the entire input
258:29 - sequence into smaller more manageable sub-problems each of these sub-problems is a head so the head
258:37 - size is the number of these sub-problems now consider what happens when each person talks
258:43 - louder or quieter if someone if someone speaks too loudly or the values in the vectors are very large
258:51 - it might drown out the others this could make it difficult to understand the conversation
258:56 - because you're only hearing one voice or most of one voice to prevent this we want to control
259:01 - how loud or how quiet each person is talking so we can hear everyone evenly the dot product of the
259:08 - query and key vectors in the attention mechanism is like how loud each voice is if the vectors are
259:16 - very large or high dimensional or many people are talking the dot product can be very large
259:21 - to control this volume by scaling down the dot product using the square root of the head size
259:30 - this scaling helps ensure that no single voice is too dominant allowing us to hear all the voices
259:35 - evenly this is why we don't scale by the number of heads or the number of time steps they don't
259:41 - directly affect how loud each voice is so in sum multi-headed tension allows us to focus on
259:48 - different parts of the conversation and scaling helps us to hear all parts of the conversation
259:53 - evenly allowing us to understand the overall conversation better so hopefully that helps you
259:59 - understand exactly what this scaling is doing so now let's go into the rest of this here
260:06 - so we have this scaling applied for our head size or yeah our head size dimension
260:11 - we're doing this dot product matrix multiplication here we get our b by t by t and then what is this
260:20 - masked fill doing so let me help you illustrate this here so masked fill is essentially like
260:27 - we'll take just uh we'll take we'll say block size is three here all right so we have initially
260:34 - let's say we have a uh like a 1 a 0.6 and then like a 0.4 okay then our next one is
260:45 - uh yeah we'll just say all of these are the same okay so essentially in our first one
260:54 - we want to mask out everything except everything except for the first time step
260:58 - and then when we advance one so let's just change this here back to zero
261:06 - when we go on to the next time step we want to expose the next piece so 0.6 i believe it was
261:12 - and then zero again and then when we expose the next time step after that we want to expose all
261:17 - of them so just kind of what this means is as we um as the time step advances in this sort of
261:25 - i guess vertical part is every time this step's one we just want to expose one more token or one
261:32 - more of these values sort of in like a staircase format so essentially what this masked fill is
261:39 - doing is it's making this uh t by t so block size by block size and for each of these values we're
261:47 - going to set them to negative infinity so for each value that's zero we're going to make that
261:52 - the float value negative infinity so it's going to look like this negative infinity
262:00 - negative infinity
262:04 - negative infinity just like that so essentially what happens after this
262:08 - is our softmax is going to take these values and it's going to exponentiate normalize them
262:14 - um we already went over the softmax previously but essentially what this is going to do this
262:23 - this last dimension here concatenate or not concatenate rather apply the softmax along the
262:29 - last dimension is it's going to do that in this sort of horizontal here so this last
262:36 - uh this last t it's like blocks it's like block size by block size so it's like we'll say t1
262:45 - and t2 each of these being length of block size we're just going to do it to this last
262:50 - t2 here and this horizontal is t2 so hopefully that makes sense and essentially
262:58 - what this exponentiation is going to do is it's going to turn these values to zero
263:02 - and this one is obviously going to remain a one and then it's going to turn these into zero and
263:11 - it's going to probably sharpen this one here so this one is going to be more significant it's
263:16 - going to grow more than the 0.6 because we're exponentiating and then same here so this one
263:21 - is going to be very uh very sharp compared to 0.6 or 0.4 so that's what the softmax does
263:30 - essentially the point of the softmax function is to make the values stand out more it's to make
263:36 - the model more confident in highlighting attention scores so when you have one value that's like very
263:41 - big but not too big not exploding because of our scaling right I want to keep a minor scaling
263:46 - but when a value is big when a score or attention score is very big we want the model to put a lot
263:52 - of focus on that and to say this this is very important in the entire sentence or the entire
263:57 - thing of tokens and we just want it to learn the most from that so essentially that's what
264:03 - softmax is doing instead of just normal normalizing mechanism it's just doing some
264:09 - exponentiation to that to make the model more confident in its predictions so this will help
264:14 - us score better in the long run if we just highlight what tokens and what attention scores
264:21 - are more important in the sequence and then after this softmax here we just apply a simple dropout
264:29 - on this way variable this new this new calculated way scale dot product attention
264:38 - masked and then softmaxed we apply a dropout on that and then we perform our final weighted
264:45 - aggregation so this v multiplied by the output of the softmax cool so we get this v self dot value
264:57 - of x so we just multiply that a little pointer I wanted to add to this module list which is
265:06 - yeah module list here and then our order to go yes our sequential network here so we have this
265:18 - sequential number of blocks here for n layers and we have our module list so what really is
265:25 - the difference here well module list is not the same as n and dot sequential in terms of the
265:31 - uh asterisk usage that we see uh in the language model class module list doesn't run one layer or
265:40 - head after another but rather each is isolated and gets its own unique perspective sequential
265:47 - processing is where one block depends on another to synchronously complete so that means we're
265:53 - waiting on one to finish before we move on to the next so they're not completing asynchronously or
265:58 - in parallel so the multiple heads in a transformer model operate independently and their computations
266:05 - can be processed in parallel however this parallel parallelism isn't due to the module list that
266:12 - stores the heads instead it's because of how the computation are structured to take advantage of the
266:21 - GPU's capabilities for simultaneous computation and this is also how the deep learning framework
266:29 - PyTorch interviews interfaces with the GPU so this isn't particularly something we have to worry
266:34 - about too much but you could supposedly think that these are sort of running in parallel
266:42 - yeah so if you want to get into hardware then that's that's like your whole realm there but
266:46 - this is PyTorch this is software uh not hardware at all i don't expect you have to have any hardware
266:52 - knowledge about GPUs CPUs anything like that so anyways that's just kind of a background on what's
266:58 - going on there so cool uh so let's actually go over what is going on from the ground up here
267:07 - so we have this uh gpt language model we get our token embeddings positional embeddings we
267:12 - have these sequential blocks initialize our weights for each of these blocks we have a
267:20 - this this class block so we get a head size parameter which is an embedded 384 divided by
267:26 - n heads which is four so we get 96 from that that's the number of features we're capturing
267:32 - self-attention we do a feed forward two layer norms so we go self-attention layer norm feed forward
267:38 - uh layer norm in the post-norm architecture then we do a feed forward just a linear followed by
267:47 - a relu followed by a linear and then dropping that out and then we have our multi-head attention
267:53 - which just sort of structured these attention heads uh running in parallel and then concatenates
268:00 - the results and then for each of these heads we have our keys queries and values we register
268:07 - a model state to prevent overhead computation excessively then we just do our scaled dot dot
268:15 - product attention in this line we do our masked fill to prevent look ahead we do our softmax to
268:20 - make our values sharper and to make some of them stand out and then we do a dropout finally on that
268:28 - and just some weighted aggregation we do our weights or this this final this final weight
268:35 - variable multiplied by our weighted value from this from this initially this linear transformation
268:44 - so cool that's what's happening step by step in this gbt architecture amazing give yourself a
268:52 - good pat on the back go grab some coffee do whatever you need to do even get some sleep
268:57 - and get ready for the next section this is going to be pretty fun so there's actually another
269:02 - hyper parameter i forgot to add which is nlayer and nlayer is essentially we'll say equal to four
269:11 - nlayer is essentially equal to the number of decoder blocks we have so instead of like nblock
269:19 - we just say nlayers doesn't really matter what it what it's called but that's what it means
269:24 - and then number of heads is how many heads we have running theoretically in parallel
269:29 - and then n embed is the number of total dimensions we want to capture from all the heads concatenated
269:35 - together type of thing we already went over that so cool hyper parameters block size sequence length
269:42 - batch size is how many of these do we want at the same time max iter is just training how many
269:48 - iterations we want to do learning rate is what we cover that in actually the desmos calculator that
269:56 - i showed a little while back just showing how how we update the model weights based on the derivative
270:02 - of the loss function and then eval iter which was just reporting the loss and then
270:11 - lastly the dropout which is dropping out 0.2 or 20 percent of the total neurons so awesome
270:20 - that's pretty cool let's go ahead and jump into some data stuff so i'm going to pull out a paper
270:25 - here so let's just make sure everything works here and then we're actually going to download
270:29 - our data so i want to try to run some iterations and just make sure that our actually i made some
270:36 - changes uh pretty much this was this was weird and didn't work so i just changed this around
270:43 - to making our characters empty opening this text file uh opening it storing it in a variable
270:51 - with utf8 format and then just making our vocab this sorted list set of our text and then just
271:01 - making the vocab size the length of that so let's go ahead and actually run this through i did
271:07 - change the block size to 64 batch size 128 some other hyper parameters here so honestly the block
271:15 - size and batch size will depend on your computational resources so just experiment
271:22 - with these i'm just going to try these out first just to show you guys what this looks like
271:33 - okay so it looks like we're getting idx is not defined or could that be
271:37 - okay yep so this is yeah we could just change that it's just saying idx is not defined we're
271:43 - using index here idx there so that should work now and we're getting a local variable t reference
271:50 - before assignment okay so we have some we have t here and then we initialize t there so let's just
271:59 - bring up up to there cool now let's try and run this oh shape is invalid for input size of okay
272:11 - let's see what we got it turns out we don't actually need two token embedding tables a little
272:16 - bit of a selling mistake but we don't need two of those so i'll just delete that and then what i'm
272:23 - going to do is go ahead and run this again let's see a new error local variable t reference for
272:29 - assignment okay so our t is our t is referenced here and well how can we initialize this what we
272:37 - can do so we could take this index here of shape b by t because it goes b by t plus one etc and
272:43 - just keeps growing so we could actually unpack that so we could go b b and t is going to be index
272:52 - dot shape just unpack that so cool
272:57 - so now we're going to run this training loop and it looks like it's working so far
273:03 - so that's amazing super cool step zero train loss 4.4 that's actually a pretty good training loss
273:10 - overall so uh we'll come back after this is done i've set it to train for uh i've set it to train
273:18 - for 3000 iterations printing every 500 iterations so we'll just see the lock the loss six times over
273:24 - this entire training process or we should i don't know why it's going to 100 eval iterators
273:35 - you got it okay estimate loss is
273:41 - okay so we don't actually need eval interval get rid of that
273:44 - we'll just make this sure why not 100 we'll keep that and it's just going to keep going here
273:54 - we'll see our loss over time is hopefully going to get smaller so i'll come back when that's done
274:00 - as for the data we're going to be using the open web text corpus and let's just go down here
274:07 - so this is the this is a paper called survey survey of large language models all right so i'll just
274:14 - go back to open web text wherever that is up it's just fine okay so open web text this is
274:29 - consisted of a bunch of reddit links or just reddit upvotes so if you go and write it and
274:33 - you see a bunch of those posts that are highly upvoted or downvoted they're pretty much those
274:41 - pieces of text are valuable and they contain things that we can train them so pretty much
274:48 - web text is just a corpus of all these upvoted links but it's not publicly available so somebody
274:55 - created an open source version called open web text hence open and it's pretty much just an
275:02 - open version of this so we're going to download that there's a bunch of other corpora here like
275:07 - common crawl which is really really big so like petabyte scale data volume you have a bunch of
275:13 - books you know so this is a good paper to read over it's just called a survey of large language
275:21 - models you can search this up and it'll come up you can just download the pdf for it so this is
275:26 - a really nice paper read over that if you'd like but anyways this is a download link for this open
275:32 - web text corpus so just go to this link i have it in the github repo and you just go to download
275:39 - and it'll bring you to this drive so you can go ahead and right click this and just hit download
275:44 - it'll say 12 gigabytes exceeds maximum files as it can scan so it's like this might have a virus
275:50 - don't worry it doesn't have a virus this is actually created by a researcher so
275:54 - not really bad people are in charge of creating text corpora so go ahead and download anyway
276:00 - okay i have actually already downloaded this so uh yeah i'll come back when our training
276:09 - is actually done here so i'm actually going to stop here iteration 2000 because we're not
276:14 - actually getting that much amazing progress and the reason for this is because our hyper parameters
276:20 - so batch size and block size i mean these are okay but we might want to change up as our learning
276:25 - rate so some combinations of learning rates that are really useful is like three three to the
276:31 - negative three you go three e to the negative four you go one e to the negative three one e
276:40 - one e to the negative four so these are all learning rates that i like to play around with
276:45 - these are just sort of common ones it's up to you if you want to give them or not but
276:48 - uh what i might do actually is just downgrade two three to the negative four and we'll retest it
276:55 - as well i'm going to bump up the uh the number of heads and the number of layers
277:02 - so that we can capture more complex relationships in the text thus having it learn more so i'm
277:08 - going to change each of these to eight i'll go eight and i go actually kernel will go restart
277:22 - now we'll just run this from the top
277:30 - and we'll run that cool so let's see what we actually start off with and what our loss looks
277:38 - like over time
277:50 - cool so we got step one four point five about the same as last time it's like point two off or
277:55 - something so it's pretty close uh let's see the next iteration here
278:26 - that's wonderful so before we were getting like three point one ish or swimming around that range
278:32 - three point one five now we're getting two point two so you can see that as we change
278:37 - hyper parameters we can actually see a significant change in our loss so uh this is amazing this is
278:44 - just to sort of prove how cool hyper parameters are and what they do for you so uh given that
278:50 - let's uh let's start changing around some data stuff so this is this right here is the wizard
278:56 - of oz text just a simple text file it's the it's the size isn't super large so we can actually
279:02 - open it all into ram at once but if we were to use the open web text we cannot actually read
279:10 - you know 45 gigabytes of utf8 text in ram at once just can't do that unless you have like maybe 64
279:17 - or 128 gigabytes of ram this is really just not feasible at all so we're going to do some data
279:26 - pre-processing here some data cleaning and then just a way to simply load data into the
279:32 - gpt so let's go ahead and do that so the model has actually gotten really good at predicting
279:36 - the next token as you can see the train loss here is 1.01 so let's actually find
279:43 - uh what the prediction accuracy of that is so i might just go into gpt4 here
279:50 - and just ask it uh what is the prediction accuracy of loss 1.01
280:04 - the loss value comes with a loss function during the praying process okay so let's let's see
280:16 - cross entropy loss doesn't mean the model is 99 accurate
280:22 - okay so that pretty much means that the model is really accurate but i want to find a value here
280:30 - so if the we'll go to wolfram alpha
280:38 - and just we'll just guess some values here so negative ln of let's say 0.9 okay so probably not
280:48 - that 0.3 0.2 0.4 0.35 yep so the model has about a 35 percent chance of guessing the next token
281:04 - as of right now so that's actually pretty good so one in every three tokens are spot on
281:11 - so that is wonderful this is converging even more we're getting 0.89 so now it's getting like every
281:17 - like 40 are being guessed properly uh our validation is not doing amazing though
281:24 - but we'll we'll linger on that in a little bit here and you'll see some sort of how this changes
281:29 - as we scale our data but uh yeah so i've installed this web text dot tar file
281:37 - tar files are interesting so in order to actually extract these you simply just
281:42 - uh right click on them you go extract to and then it'll just make a new file here so it'll process
281:49 - this you have to make sure you have winra or else this might not work to the fullest extent
281:54 - and yeah so we'll just wait for this to finish up here you should end up with something that looks
282:00 - like this so open web text and inside of here you have a bunch of xz files cool so there's actually
282:07 - 20 000 of these so we're gonna have to do a lot of uh it's gonna definitely there's definitely gonna
282:11 - be some for loops in here for sure so let's just handle this step by step in this data extract file
282:19 - so first off we're gonna need to import some python modules we're gonna use os for interacting
282:25 - with the operating system lzma for handling xz files which are a type of compressed file
282:32 - like seven zip for example and then tqdm for displaying a progress bar so you see a progress
282:39 - bar but left to right in the in the terminal and that is pretty much going to show us how quick we
282:44 - are uh that's executing the script so next up we're going to find a function called xz files
282:52 - in dir it takes a directory as an input returns a list of all of the xz file names in that directory
282:59 - it's going to use os.list dir to get all the file names and os path is file os path is file
283:08 - to check if each one is a file and not a directory or symbolic link if a file name ends with.xz
283:16 - and it's a file it'll be added to the list so we just have a bunch of these files
283:22 - each element is just the title of each file in there so that's pretty much what that does
283:26 - and then next up here we'll set up some variables folder path it's just going to be where our xz
283:33 - files are located so i'm actually going to change this here because that's like an incorrect file
283:38 - path but yes just like that you have to make sure that these uh slashes are actually forward
283:52 - slashes or else you might get bytecode errors so when it actually tries to read the string
283:57 - it it doesn't think that these are separated or that the backward slashes do like weird things
284:03 - and so you could either do like a one forward slash or two backward slashes and that should work
284:09 - so awesome just make sure you have forward slashes and you should be good
284:12 - so folder path is where all these files are located all these xz files are located as you saw
284:16 - uh output file is the pattern for output file names in case we want to have more than one
284:22 - of them so if you want to have 200 output files instead of one then it'll just be like output 0
284:27 - output 1 output 2 etc and then a vocab file is where we want to save our vocabulary keep in mind
284:35 - in this giant corpus you can't push it on to ram it once so what we're going to do is as we're
284:41 - reading these little compressed files 20,000 of them we're going to take all of the new
284:46 - characters from them and just push them into some vocab file containing all of the different
284:50 - characters that we have so that way we can handle this later and just pretty much sort it into some
284:56 - list containing all of our vocabulary split files how many files we want to split this into so
285:03 - pretty much this it ties back to output file and just these these curly braces here how many
285:09 - do we want to have if we want to have more than one then we would this would take effect
285:13 - so cool now we'll use our x files in dir to get a list of file names and store them in this variable
285:26 - we'll count the number of total xd files simply the length of our file names
285:32 - now in here we'll calculate the number of files to process for each output file
285:37 - if the user has requested more than one output file for request more than one output file
285:41 - this is the total number of files divided by the number of output files rounded down
285:50 - so if the user only wants one output file max count is the same as total files and
285:57 - that's how that works so next up we'll just create a set to store a vocabulary when we
286:04 - start appending these new characters into it a set is a collection of unique items in case
286:10 - you did not know entirely what a set was now this is where it gets interesting now we're ready to
286:18 - process our.xz files for each output file we'll process max count files for each file we'll open
286:26 - it read its contents and write the contents of the current output file and then add any unique
286:32 - characters to our vocabulary set after processing max count files remove them from our list of files
286:38 - and then finally
286:49 - finally we'll write all our vocabulary to this file so we pretty much just open
286:56 - yeah we just write all of these characters in the vocab to this vocab file which is here vocab.txt
287:02 - so awesome now um honestly we could we could just go ahead and run this so
287:11 - let's go ahead and go in here i'm going to go to cls to clear that
287:16 - we'll go python data extract.py let's see if this works it's magic
287:24 - how many files would you like to split this into we'll go one
287:27 - one then we get a progress bar 20 000 files and we'll just let that load i'll come back to you in
287:34 - about 30 minutes to check up on this okay so there's another little one of thing we
287:39 - want to consider for and it's actually quite important is our splits so our train and bow
287:45 - splits uh we it would be really inefficient to just get blocks and then creating you know
287:50 - train and bow splits as we go every new batch we get so in turn what we might be better off doing
287:58 - is just creating a train an output train file and an output file file so just two of them instead
288:04 - of one train is 90 of our data val is 10 of our data if that makes sense so pretty much what i
288:10 - did is i took away that little input line for how many files do you want as you can see i got quite
288:16 - a bit of files produced here by not doing that correctly so don't do that and
288:27 - yeah essentially we're just we're pretty much just doing that so we're processing some training
288:33 - files we're separating 90 of the names on the left side and then 10 of the names on the right
288:40 - side we're just separating those into two different arrays file names and then we're
288:45 - just processing each of those arrays based on the file names so i took away that little bit that
288:50 - was asking you know how many how many of those how many files per split do you want so i took that
288:56 - away and this is like effectively the same code just a little bit of tweaks and yeah so i'm going
289:03 - to go ahead and run this data extract cool so we've got an output train and then after this it's
289:13 - going to do the output validation set so i'll come back after this is done so awesome i have just
289:20 - downloaded both or i've both got both these splits output train and val train so just to confirm
289:27 - they're actually the right size got 38.9 and then 4.27 so if we do this divided by nine so about
289:35 - 30 38.9 divided by nine we get 4.32 and it's pretty close to 4.27 so we can confirm that
289:43 - these are pretty much the uh the length that we expect them to be so awesome we have this vocab.txt
289:50 - file wonderful so now we have to focus on is getting this into our batches so when we call
289:58 - our git batch function actually cd out of this open this into jupiter notebook
290:03 - let's copy my desktop
290:13 - paste it over here and perfect so it was open one web text folder with these files awesome
290:24 - and our gptv1
290:25 - um so this git batch function is going to have to change also these are going to have to change as
290:36 - well and this one too these are probably not going to be here um but pretty much let's go ahead and
290:43 - first of all get this vocab.txt in so what i'm going to do i'm just going to go
290:48 - we're going to go open web text slash vocab.txt cool so that's our vocab right there text read
290:59 - vocab size the length of that nice so that's what our vocab is and then uh what we're going to do
291:07 - next is change this git batch function around so first of all i'm going to go ahead and get rid of
291:13 - this here get rid of that data and then i've actually produced some code specifically for
291:20 - this so i'm just going to go back to my i'm going to find this folder okay so i've actually produced
291:32 - some code here i produced this off camera but uh pretty much what this is going to do
291:41 - is it's going to let us call a split okay so we have our git batch function all of this down
291:46 - here is the same as our gptv1 file and then this data is just going to get a random chunk of text
291:54 - so a giant block of text and the way that we get it is actually pretty interesting so the way that
292:01 - we get this text is something called memory mapping so memory mapping is a way to look at
292:06 - disk files or to open them and look at pieces of them without opening the entire thing at once
292:12 - so memory mapping look at i'm not a hardware guy so i can't really talk about that but
292:18 - uh yeah memory mapping is pretty cool it allows us to look at little chunks at a time in very
292:23 - large text files so that's essentially what we're doing here we're passing this split split uh file
292:30 - name is equal to train split this is just an example text file if the split is equal to train
292:38 - then this is our file name else file split and then we're going to open this file name in binary
292:44 - mode this has to be in binary mode it's also a lot more efficient in binary mode and then we're
292:50 - going to open this with a mem map so i don't expect you to memorize all the mem map syntax
292:54 - you can look at the docs if you would like but i'm just going to explain sort of logically what's
292:59 - happening so we're going to open this with the memory map library and we're going to open this
293:06 - as mm so the file size is literally just the length of it so determining the file size
293:14 - and all we're doing from this point is we're just finding a positions we're using the random library
293:19 - and we're finding a position between uh zero and the file size minus block size times batch size
293:28 - so pretty much we have this giant uh this giant text file we could either what we want to do is
293:35 - we want to start from zero and go up to like just before the end because if we actually sample uh
293:42 - that last piece then it's still going to have some wiggle room to uh reach further into the file if
293:48 - we just made it from like the first like the very start of the file to the very end then it would
293:55 - want to do is it would want to look past the end because it would want to look at more tokens from
294:00 - that and then we would just get errors because you can't read more than the file size if that
294:05 - makes sense so that's why i'm just making this little threshold here and uh yeah that's what
294:12 - that does that's the starting position could be a random number between the start and a little bit
294:17 - a little margin from the end here so next up we have this seek function so seek it's going to
294:24 - go to the start position and then block is going to read we're going to go up to the start position
294:32 - it's going to seek up to there that's where it's going to start it's going to go up to it and then
294:36 - the read function is going to find a block of text that is block size times batch size so it's going
294:42 - to find a little snippet of text in there at the starting position and it's going to be of size
294:48 - it's going to have this the same amount of i guess bytes as block size time times batch size
294:54 - then all that minus one just so that it fits into this start position we don't get errors here
294:59 - that's why i put the minus one but yeah so we'll get a pretty we'll get a pretty decent uh
295:07 - text amount i guess you could say it's going to be enough to work with you could you could of
295:11 - course increase this if you wanted to you could do like you know times eight if you wanted you
295:16 - like times eight and then times eight up here but we're not going to do that based on my experience
295:21 - this has performed pretty well so we're going to stick with this method here and then we just
295:27 - decode this bit of text the reason we decode it is it's it's because it's uh we read it in binary
295:33 - form so once we have this block of text we actually have to decode this to uf8 format or utf8 format
295:41 - and then any like bytecode errors we get we're just going to ignore that this is something you
295:46 - learn through practice is when you start dealing with like really weird data or if it has like
295:51 - corruptions in it you'll get errors so all you want to do is all this does is it pretty much says
295:57 - okay we're just going to ignore this bit of text and we're just going to sample everything around
296:01 - it and not include that part and plus since we're doing so many iterations it won't actually
296:06 - interfere that much so we should be all right and then for this replace little function here
296:11 - i was noticing i got errors about this slash r so all this does is just replaces that with an
296:16 - empty string and then finally we have all this uh we have all this decoded data so all we're
296:23 - going to do is just encode this into the tokenized form so it's all in it's all in the tokenized form
296:29 - uh integers or torch dot longs data type and we just that that's what our data is instead of a
296:37 - bunch of characters it's just a bunch of numbers and then we return that into our git batch
296:43 - and this is what our data is so that's pretty cool we can get either train or bow split and
296:50 - that's sort of what it looks like in practice that's how we sample from very large text files
296:56 - at a smaller scale bit by bit so let's go ahead and implement this here i'm gonna go grab this
297:02 - entire thing and pop over to here i'm just going to replace that so get random chunk get batch
297:13 - cool so now we can actually go ahead and perhaps run this actually before we run this there's a
297:19 - little something we need to add in here so i have this train split dot txt and a vow split dot txt
297:27 - so i actually need to change these full score rename will go uh train split dot txt
297:35 - and then vow split dot txt cool and then we could just go open web text forward slash
297:45 - and then same thing for here cool let's go ahead and run this now
297:56 - oh and we're getting errors mem map is not defined okay so that's another thing we need
298:01 - to probably add in then so i'm actually just gonna stop this process from running here
298:07 - we're gonna go pip install mem map oh my map is not defined oh we don't actually need to install
298:19 - this it by default comes with the operating system so what we actually need to do is we just close
298:28 - this gptv1 awesome is everything everything is good nothing is broken sweet so what i actually
298:39 - need to do up here is import this so i need to go import mem map just like that
298:49 - and should be good to start running the script name random is not defined again another importation
298:56 - we have to make import random
299:07 - and we should start seeing some progress going here so once we see the first iteration i'm going
299:11 - to stop it come back at the last iteration and then we'll start adding some little bits and pieces
299:17 - onto our script here to make it better so we're already about 600 iterations in and you can see
299:21 - how the training loss has actually done really well so far it's gone from 10.5 drop all the way to 2.38
299:28 - and we can actually see that we might be able to actually get a val loss that is lower than the
299:36 - train because keep in mind in train mode the dropout takes effect but in val in eval mode
299:44 - uh let me just roll up to this here yes some model about eval what this does is it turns off the
299:52 - dropout so we don't we don't lose any of the neurons and they're all sort of showing the same
299:58 - features and giving all the information that they're supposed to because they're all active
300:01 - but in train mode 20 of them are off so once you actually see uh in ethyl mode it does better
300:09 - that means that the network has started to form a sense of completeness in its learning so it's
300:17 - just adjusting things a little bit once it hits that point and we might see this happen momentarily
300:23 - but this is really good progress so far a loss of 1.8 is amazing so uh yeah in the meantime
300:31 - i'm just going to add some some little tweaks here and there to improve this script so i've
300:36 - actually stopped the iteration process but we got into 700 steps and we can already see
300:41 - that val loss is becoming less than train loss which is showing that the model is actually
300:46 - converging and doing very well so uh this architecture is amazing we've pretty much
300:51 - we've pretty much covered every architectural math pytorch part of this script has to offer
300:58 - uh the only thing i want to add actually a few things i want to add one of them being
301:02 - uh torch.load and torch.save so one thing that's going to be really important when you start to
301:09 - scale up uh your your iterations is you don't just want to run a script that executes you know a
301:15 - training loop with an architecture and that's it you want to have some way to store those
301:21 - learnable parameters so that's what torch.load and torch.save does uh save some file uh right
301:29 - and you can pretty much uh you could put it into like a serialized format when you save it you
301:37 - take your initial architecture in our case it would actually be the gpt language model so you
301:42 - would save this because it contains everything all these other classes as well they're all inside
301:47 - of gpt language model you'd save that architecture and you'd essentially serialize it into some
301:54 - pickled file that would have the file extension.pkl so essentially instead of using torch we're
302:03 - just going to use a library called pickle because they're essentially the same thing uh pickle is
302:09 - pickle's a little bit easier to use or at least a little bit easier to understand
302:13 - there's less there's less to it uh pickle will only work on one gpu so if you have like eight
302:20 - gpu's at the same time you're going to want to learn a little bit more about hardware stuff and
302:25 - some pytorch docs but pretty much if we want to save this after training what we're going to do
302:32 - is we're going to use a little library called pickle and this comes pre-installed with windows
302:39 - um what am i typing windows import pickle okay so what we want to do is implement this after the
302:45 - training loop after all these parameters have been updated and learned to the fullest extent so after
302:51 - this training loop we're simply going to open what we could do with open and we could just go
302:57 - model zero zero one like that and then just that.pkl is the file extension for it
303:05 - and then since we're writing to it we're going to go write binary
303:08 - as f and then in order to actually save this we just go pickle dot dump and then we can use
303:19 - model and then just f like that so if i start recording this it's going to make
303:27 - if i start recording this training process it's going to make my it's going to make my clip leg
303:33 - so i'm going to come back to this after we've done let's just say about 100 iterations we're
303:39 - going to do 100 editors and i'm going to come back and show you guys what the model looks like
303:45 - what i actually did is i changed some of the model hyper parameters because it was taking way too
303:51 - long to perform what we wanted it to so i changed and head to one and layer to one and i half
303:57 - batch size all the way i'm from 64 to 32 so what i'm actually going to add here is just to make
304:03 - sure i just want to i like to print this out at the beginning just print device make sure that
304:07 - the device is CUDA uh let's go back down so it did in fact train the model so we got all this done
304:15 - uh and yeah so i don't know i did 2.54 or whatever that that was just some entire loss okay
304:22 - so model saved awesome what does this actually look like here so this model dot pkl 106 megabytes
304:32 - isn't that wonderful so this is our model file this is what they look like so it's just serialized
304:39 - pretty much the entire architecture all the parameters of the model the state everything
304:43 - that it contains and we just can compress that into a little pkl file take that out decompress
304:49 - it and then just use it again with all those same parameters so awesome and all this really took
304:56 - was uh we just open as this we do a pkl dot dump and then just to make sure that actually save i
305:03 - just like to add a little print statement there cool so next what i'd like to add is a little wait
305:10 - for us to uh instead of just doing all of our training at once and then saving the model being
305:15 - able to train multiple times so i'm going to go up here up to our gpt language model here
305:25 - and let's just see what i'm going to do i'm going to go with open and we're going to go
305:35 - model 0 1 pkl and we're going to go read binary so actually going to read it we're going to
305:41 - we're going to load this into our script here so i'm going to go as f and then i believe it's
305:53 - pkl dot load you just go yeah model equals uh pkl dot load and then we'll just essentially
306:03 - if we dump that right in there go print loading model parameters dot dot dot
306:17 - and then just put f in there and then once it is loaded we'll do print loaded successfully
306:28 - okay cool so i'm actually going to try this out now go do that boom boom and boom okay
306:42 - so loading model parameters loaded successfully and we'll actually see this start to uh work on
306:50 - its own now so is it going to begin or is it not going to begin let's run that okay perfect so now
306:59 - we should take the loss that we had before which was about 2.54 i believe something around those
307:05 - something along those lines you can see that our training process is greatly accelerated
307:13 - so we had 100 now it's just going to do an estimate loss
307:15 - cool and we're almost done
307:26 - 1.96 awesome and the model saved so essentially what we can do with this is we can now
307:34 - save models and then we can load them and then iterate further so if you wanted to
307:38 - you could create a super cool gpt language model script here and you could essentially give it
307:46 - like 10 000 or 20 000 iterations to run overnight you'd be able to save it and then import that into
307:52 - say a chatbot if you want so that's pretty cool and that's just kind of a good good thing good
307:59 - little it's kind of essential for language modeling because what are you what's the point
308:05 - in having a machine learning model if you can't actually use it and deploy it so you need to save
308:10 - for this stuff to work all right now let's move on to a little something in this task manager here
308:17 - which i'd like to go over so this shared gpu memory here and it's dedicated gpu memory so dedicated
308:26 - means how much v ram video ram does your gpu actually have on the card so on the card it's
308:33 - going to be very it's going to be very quick memory because it's it doesn't have to the
308:37 - electrons don't have to travel as quickly that's just that's kind of the logic of it the electrons
308:42 - don't have to travel they don't have to travel as far because um the little ram chip is right there
308:49 - so they're going to dedicate a gpu memory is a lot faster shared gpu memory is essentially if this
308:56 - gets overloaded it'll use some of the ram on your computer instead so this will typically be about
309:03 - half of your computer's ram i have 32 gigabytes of ram on my computer so 16.0 makes sense half 32
309:11 - and yeah so you want to make sure you're only using dedicated gpu memory uh having having your
309:17 - shared gpu memory go up is not usually a good thing a little bit is fine but uh dedicated
309:24 - gpu memory is the fastest and you want everything to stick on there just try to make sure all of
309:29 - your parameters sort of fit around this whatever your max capacity is maybe it's four maybe it's
309:35 - eight maybe it's 48 who knows and a good way to figure out what the highest amount of ram you can
309:44 - use on your gpu without it getting memory errors or using shared memory is to actually play around
309:51 - with these parameters up here so uh block size and batch size actually let me let me switch those
310:01 - around these are not supposed to be in that order but all good make our batch size 64 it's 128 okay
310:16 - okay so batch size and block size are very big contributors to how much memory you're going to
310:22 - use learning rate is not max iterations is not evalators is not but these three will so these
310:29 - are the amount of features that you store the amount of heads you have running in parallel
310:33 - and then also n layers so some of these will not affect you as much because they're more
310:40 - sort of restrained to computation how quickly you can do operations if something is sequential uh
310:47 - so n like n layer won't strain you as much as something like batch and block size
310:52 - but uh those are just good little things to sort of tweak and play around with so i found the optimal
311:00 - sort of set of hyper parameters for my pc and that happens to be eight eight 384 uh learning
311:07 - rate stays the same and then 64 128 for this so that happened to be the optimal uh hyper parameters
311:14 - for my computer it'll probably be different for yours if you don't have eight gigabytes of ram
311:19 - on your gpu so anyways uh that's a little something you have to pay attention to to make
311:25 - sure you don't run out of errors and a technique you can use which i'm not actually going to show
311:30 - you in this course but it's quite useful is something called auto tuning and what auto
311:35 - tuning does is it pretty much runs a bunch of these uh a bunch of models with different sets
311:42 - of hyper parameters so it'll run like uh batch size 64 batch size 32 batch size 16 batch size
311:48 - maybe 256 we'll be like okay which ones are throwing errors and which ones aren't so what
311:53 - it'll do if you have if you properly uh if you properly set up an auto tuning script is
311:58 - is you will be able to find the most optimal set of parameters for your computer most optimal set
312:06 - of hyper parameters that is possible so our tuning is cool you could definitely look more into that
312:13 - there's tons of research on it and yeah so our tuning is cool let's dig into the next part the
312:19 - next little cool trick we use in practice especially by machine learning engineers it's
312:24 - a little something called arguments so you pass an argument into uh not necessarily a function
312:29 - but into the command line so this is what it'll look like this is just a basic example of what
312:34 - arg parsing will look like so just go python uh arg parsing because that's a script's name
312:40 - i go dash uh llms because that's what it says right here this is what the argument is
312:46 - and then we can just pass in a string say hello the provided whatever is hello so cool you can
312:56 - add little arguments to this and i'm maybe going to change this around let's say uh batch size
313:07 - and then we'll just go like that batch batch size please provide a batch size
313:27 - i'm gonna do the same thing again uh and see it says uh following arguments require batch size
313:35 - so that obviously didn't work and if we actually try it the correct way our parsing.py then we go
313:40 - dash batch size we can make it 32 oh that's because it's not a string so what we need to
313:56 - actually do is it's bs somewhere okay so args parse args so we need to change this
314:10 - to bs like that go batch size batch size is 32 okay so even i'm a little bit new to
314:19 - to uh arguments as well but this is something that comes in very handy when you're trying to
314:24 - you know each time you're trying to change some some parameters if you add no new gpu or whatever
314:30 - and you're like oh i want to double my batch size it's like sure you can easily do that so a lot of
314:35 - the times it won't just have one but you'll have like many meaning like maybe a dozen or so of
314:40 - these uh of these little arguments so uh that is what this looks like and we're going to go ahead
314:47 - and implement this into our little script here so uh i'm just going to pop over to gpt one i'm gonna
314:57 - pull this up on my second monitor here and uh in terms of you know these i'm just gonna
315:07 - i'm just gonna start off by making a importation arg uh arg parser or arg parse rather that's what
315:19 - it's called and then we go parser is equal to i'll just i'll just copy and paste this entire thing
315:29 - and why not cool okay so we get a batch size or something and then we'll add in the second part
315:47 - here so args parse the arguments here and we'll just go batch size like that our batch size is
316:08 - equal to whatever that was and let's go args dot args dot batch size so cool i'm gonna run this
316:22 - and not defined oh yes so i got a little not defined thing here and pretty much all i missed
316:29 - was that we return this so essentially this should be equal to this right here so i'm just
316:37 - going to go ahead and copy that and uh boot parse args except we don't have a parse args function
316:50 - so what do we need to do instead well it actually that might just work on its own let's try it out
316:57 - okay so it looks like it's actually expecting some input here in code so that's probably working
317:09 - and if we if we ported this into a script then it would simply ask us for some input so i believe
317:16 - we're doing this correctly let's go ahead and actually switch over and pour all of this into
317:23 - some code so i'm going to make a training file and a chat file so the training file is going to be
317:30 - all of our parameters whatever all of our architecture and then the actual training loop
317:34 - itself we're going to have some arguments in there and then the chat bot is going to be
317:39 - pretty much just a question answer thing that just reproduces text so it'll just be like prompt
317:44 - completion type of thing and yeah so let's go ahead and implement that here so in our
317:51 - uh gpt course here i'm going to go training.py and we're going to go
318:00 - at bot.py just like that so in training let's go ahead and drag everything in here
318:10 - i'm just going to move this over to the second screen and just copy and paste
318:14 - uh everything in in order here so next up we have our characters and then we have our tokenizer
318:26 - and then our get random chunk and get batches
318:34 - sweet our estimate loss function
318:37 - and then this giant piece of code containing most of the architecture we built up
318:53 - just going to add that in there cool we're not getting any warnings
318:58 - and then the training loop
319:00 - and the optimizer awesome then after this uh we would simply have this context but
319:09 - the point of this is that we want to have this in our chat bot script so what i'm going to do
319:15 - is in this training.py i'm going to keep all of these the same i'm going to keep this entire
319:21 - thing the same get rid of this little block of code and we're going to go into the uh chat bot
319:29 - here so loading model parameters good we want to load some in train some more and then dump it
319:35 - chat bot is not going to dump anything it's just going to save so i'm going to take all of our
319:40 - training here and instead of dumping take that away i'll also take away the training loop as well
319:50 - okay i don't believe we have anything else to actually bring in we don't need our get batch
320:01 - we do not need our get random chunks so awesome we're just importing these parameters by default
320:08 - like that awesome so from this point we have imported uh we've imported our model cool so
320:19 - let's go ahead and port in our little uh chat bot here this little end piece which is going to
320:26 - allow us to essentially chat with the model so this is what this is what it looks like
320:30 - a little while loop we have a prompt we just input something uh prompt next line that should
320:36 - be fairly self-explanatory and we have this tensor we're going to encode this prompt into a bunch of
320:41 - integers or torch dot long data types on the gpu where device is cuda and then after after we've
320:50 - actually generated these so model dot generate we're going to unsqueeze these remember it's
320:55 - a torch dot tensor so it's going to be in the matrices form so it's going to look like this
320:59 - it's going to be uh it's going to look like this or whatever like uh that that's essentially what
321:03 - the shape is so all we're doing when we unsqueeze it is we're just taking away this wrapping
321:08 - around it so awesome and then we're just doing max your tokens for example 150 here
321:15 - and then uh to a list format and then we can just print these out as generated characters
321:20 - awesome so it's just going to ask us prompt and then do some compute give us a completion
321:25 - so on so forth so that's what this is doing here and another thing i wanted to point out is
321:31 - actually when we load these parameters in at least on training it's going to initially give us errors
321:39 - if we don't have a model to load it from we're going to get errors from that because the model
321:44 - will just not be anything and we won't be able to import stuff so that's going to give you errors
321:48 - first of all another thing you want to pay attention to is to make sure that when you've
321:52 - actually trained this initial model that it matches all of the architectural stuff and the
321:58 - hyper parameters that you used that when you're you're using to load up again so when you're
322:05 - running your forward pass and whatnot you just want to make sure that this architecture uh sort
322:09 - of lines up with it just so that you don't get any architectural errors those can be really
322:14 - confusing to debug so yeah and the way we can do this is actually just commenting it out here
322:20 - so awesome we're able to save load models and we're able to use a little loop to create a
322:28 - sort of chap up that's not really helpful because we haven't trained it an insane amount on data
322:35 - that actually is useful so another little detail that's very important is to actually make sure
322:41 - that you have nn module in all of these classes and subclasses nn.module basically works as a
322:48 - tracker for all of your parameters it makes make sure that all of your nn extensions run correctly
322:55 - and just overall a cornerstone for PyTorch like you need it so make sure you have nn module in
323:01 - all of these classes i know that block sort of comes out of gpt language model and so on so forth
323:07 - but just all of these classes with nn or any learnable parameters you will need it in it's
323:14 - overall just a good practice to have nn module in all of your classes overall just to sort of avoid
323:20 - those errors so cool i didn't explicitly go over that at the beginning but that's just a heads up
323:26 - you always want to make sure nn module is inside of these so cool now something i'd like to highlight
323:35 - is a little error that we get we try to generate when we have max new tokens above block size so
323:40 - let me show you that right now so you just go python chatbot and then batch size 32 so we could
323:48 - say we could say hello for example okay so it's going to give us some errors here and what exactly
323:59 - does this error mean well when we try to generate 150 new tokens what it's doing is it's taking the
324:08 - previous you know H E L L O exclamation mark six tokens and it's pretty much adding up 150 on top
324:18 - of that so we have 156 tokens that we're now trying to fit inside a block size which in our case is
324:26 - 128 so of course 156 does not fit into 128 and that's why we get some errors here so
324:35 - so all we have to do is make sure that we essentially what we could do is make sure
324:43 - that max new tokens is small enough and then be sort of paying attention when we make prompts
324:50 - or we could actually make a little cropping cropping tool here so what this will do is
324:57 - it'll pretty much crop through the last block size tokens and this is super useful because it
325:05 - it pretty much doesn't make us have to pay attention to max new tokens all the time
325:09 - and it just essentially crops it around that 128 limit so i'm going to go ahead and replace
325:16 - index here with index cond or index condition and we go ahead and run this again
325:27 - so i could say hello
325:28 - hello
325:32 - and we get a successful completion awesome we can keep asking new prompts
325:43 - and awesome so yeah we're not really getting any of these dimensioning dimensionality like
325:49 - architecture fitting type errors if you want to call them if you want to make it super fancy that
325:53 - way but yeah there's not really that much else to do yeah there's a few points i want to go over
326:00 - including fine tuning so i'm going to go over a little illustrative example as to what fine
326:05 - tuning actually looks like in practice so in pre-training which is what this course is based off
326:11 - of in pre-training you have this giant text corpus right you have this giant corpus here
326:19 - with some text in it
326:20 - and essentially what you do is you take out little snippets these are called
326:28 - blocks or batches or chunks you could say you take a little batch of the
326:32 - batches of these you sample random little blocks and you take multiple batches of them
326:37 - and you essentially have this let's just say h-e-l-l-o and maybe the next predict
326:46 - maybe the outputs are the or the targets rather are e-l-l-o exclamation mark so it's just shifted
326:55 - over by one and so given this given this sequence of characters you want to predict this which is
327:03 - just the input shifted by one that's what pre-training is and keep in mind that these
327:08 - are the same size this is one two three four and five same thing here these are both five
327:15 - characters long fine tuning however is not completely the same so i could have hello
327:23 - and then maybe like a question mark and it would respond you know
327:31 - the model might respond how are you maybe that's just a response that it gives us
327:39 - we can obviously see that hello does not have the same amount of characters
327:43 - with the same amount of indices as how are you so this is essentially the difference between
327:50 - fine tuning and pre-training with fine tuning you just have to add a little bit of different
327:54 - things in your generate function to compensate for not having the same amount of indices in your
328:01 - inputs and targets and rather just generate until you receive an end token so what they
328:07 - don't explicitly say here is at the end of this question there's actually a little end token which
328:13 - we usually do it usually looks like this so go like that or like this these are these are end
328:23 - tokens and then you typically have the same first start token so like an s or start like that
328:31 - pretty simple and essentially you would just you just append them and and a start token the
328:40 - start token doesn't matter as much because we essentially just are looking at what this does
328:46 - and then we start generating the start doesn't really matter because we don't really need to
328:51 - know when to start generating it just happens but the end token is important because we don't want
328:56 - to just generate an infinite number of tokens right because these aren't the same size it could
329:01 - theoretically generate a really really long completion so all we want to make sure is that
329:07 - it's not generating an infinite amount of tokens consuming infinite amount of computation
329:12 - and just to prevent that loop so that's why we append this end token to the end here sort of this
329:19 - you have this little end bit and essentially once this end token is sampled you would end
329:27 - the generation simple as that and we don't actually sample from the token itself but rather
329:34 - the actual uh the i guess you could say index or the the miracle value the encoded version of end
329:44 - which is usually just going to be the length of your vocab size
329:49 - plus one so if your vocab size for in our case is like maybe 32 000 your end token would be at
329:56 - index 32 000 and one so that way when you sample when you sample an end token when you sample that
330:06 - 32 000 and one token you actually just end the sequence and of course when you train when you
330:13 - train your model you're always appending this end token to the end so you get your initial inputs
330:18 - and then inside of either your training data or when you actually are processing it and feeding
330:25 - it into that transformer you have some sort of function that's just appending that little uh
330:30 - 32 000 and one token index to it so that's pretty much what fine tuning is that sums up fine tuning
330:38 - and the whole process of creating these giant language models is to of course help people
330:45 - and there's no better way to do that than to literally have all the information that humans
330:51 - have ever known meaning like common crawl open web text or Wikipedia and even research papers
330:57 - pre-training on all that so just doing again same size and then shift over for targets
331:03 - and then after you've iterated on that many many times you switch over to fine tuning
331:08 - where you have these specifically picked out prompt and completion pairs and you just train
331:14 - on those for a really long time until you are satisfied with your result and yeah that's what
331:19 - language modeling is there are a few key pointers i want to leave you with before you head on your
331:24 - way to research and development and machine learning so first things first there's a little
331:30 - something called uh efficiency testing or just finding out how quickly certain operations take
331:37 - so we'll just call this efficiency testing and i'll show you exactly how to do this right here
331:42 - uh efficiency yeah i don't know if i spelled that correctly i don't know why it's doing that okay
331:51 - uh anyways we'll just pop into code here and essentially we'll just do we'll do i'm testing
332:05 - go import time and essentially uh all we're gonna do is just test we're just gonna time how long
332:14 - operations take so uh in here you can go i don't know you can go start time equals time dot time
332:23 - and essentially what this function does is it just takes a look at the current time right now
332:27 - the current like millisecond very precise and we can do some little operation like i don't know
332:36 - for i in range uh we'll just go i don't know ten thousand
332:49 - print i for example for print i times two okay and then we could just end the time here so we'll
332:56 - go end time equals time dot time again calling the current time so we're doing right now versus
333:03 - back then and that little difference is how long it took to execute so all we can do is just do
333:08 - you can say total time you want we can say total time equals end time minus start time
333:15 - amoscope print uh and time or i'm taken
333:27 - let's go total total time like that just execute this
333:33 - python time testing cool time taken 1.32 seconds so you can essentially time every single operation
333:46 - you do with this method and you can see even in your i encourage you to actually try this out
333:51 - i'm not going to but i encourage you to try out uh how long the model actually takes to do certain
333:56 - things like how long does it take to load a model how does it take to save a model how long does it
334:00 - take to estimate the loss right play around with hyper parameters see how long things take
334:06 - and maybe you'll figure out something new who knows but this is a little something we use to
334:10 - pretty much test how long something takes how efficient it is and then to also see if it's
334:15 - worth investigating a new way of approaching something in case it takes a ridiculous amount
334:20 - of time so that's time testing and efficiency testing for you the next little bit i want to
334:26 - cover is the history i'm not going to go over the entire history of ai and llm's but essentially
334:35 - we originated with something called rnn's okay rnn's are called recurrent neural networks
334:41 - and they're really inefficient at least for scaled ai systems so rnn's are a little
334:48 - essentially think of that as little loop keeps learning and learning and this is sequential
334:53 - right it does this and then this and then this and then this it has to wait for each completion
334:57 - it's synchronous you can't have multiple of them at once because they're complex GPUs cannot run
335:03 - complex things they're only designed for just matrix multiplication and very simple math like
335:08 - that so rnn's are essentially a little bit dumber than transformers and they are run on the cpu
335:17 - so rnn's was where we last sort of stopped at and what i encourage you to do is look into
335:22 - more of the language modeling and ai history and research that has led up to this point
335:28 - so you can have an idea as to how researchers have been able to quickly innovate given
335:35 - all these historical innovations so you have like all these things leading up to the transformer
335:40 - well how did they all philosophize philosophize up to that point and yeah it's it's just something
335:48 - good to sort of be confident in is innovating as both a researcher an engineer and a
335:56 - business person so cool rnn's were where we where we sort of finished off and now it's
336:02 - transformers and gpts that's the current state of ai next up i would like to go over something
336:09 - called quantization so quantization is essentially a way to reduce the memory usage
336:16 - by your parameters so there's actually a paper here called q laura efficient fine tuning of
336:22 - quantized llms so all this does in simple form is it pretty much instead of using 32-bit floating
336:31 - point numbers it goes not only to 16-bit of half precision but all the way down to four
336:38 - so what this actually looks like is in binary code or in bytecode
336:42 - uh it will look somewhere here there's some array of numbers that it uses
336:57 - okay i can't find it but pretty much what it is is it is a bunch of it's a bunch of
337:03 - floating point numbers and they're all between negative one and one and there are 16 of them
337:11 - if you have a four-bit number that means it can hold 16 different values zero through 15
337:17 - which is 16 values and all you pretty much do is you have this array of floating point numbers
337:23 - you use the bytecode of that of that four-bit number to look up the index in that array and
337:30 - that is your weight that is the weight they use in your model so this way instead of using 32-bit
337:37 - and just having these super long numbers that are like super precise you can have super precise
337:42 - numbers that are just generally good parameters to have that just perform decently they're just
337:49 - sort of well spread out and experimented on and they just they happen to work and you have 16 of
337:55 - them instead of you know a lot so that's a that's another cool little thing that's going on right
338:01 - now is four-bit quantizations it's a little bit harder to implement i would encourage you to
338:08 - experiment with half precision meaning 16-bit floating point numbers so that means it occupies
338:15 - 16 on and off switches or capacitors on your gpu and yeah so quantization is cool to sort of
338:23 - scale down the memory so that way you can scale up all of your hyper parameters and have a more
338:28 - complex model with these uh yeah just essentially to have bigger models with less space taken up
338:37 - so that is quantization um and this is the paper for it this little link you can search
338:44 - on if you want to get more familiar with this see sort of performance standards and whatnot
338:50 - the next thing i'd like to cover is gradient accumulation so you might have heard of this
338:54 - you might not have heard of this gradient accumulation will ascend what gradient
339:00 - accumulation does is it will accumulate a gradients over say we just set a variable x
339:08 - so every x iterations it'll just accumulate those iterations average them and what this allows you
339:15 - to do is instead of uh updating each iteration you're updating every x iterations so that allows
339:23 - you to fit more parameters and more info or generalization into this one piece so that way
339:29 - when you update your parameters it's able to generalize more over maybe a higher batch size
339:36 - or a higher block size so when you distribute this over many iterations and average them
339:44 - you can fit more into each iteration because it's sort of calculating all of them combined
339:48 - so yeah that's a cool little trick you can use if uh your gpu maybe isn't as big if it doesn't
339:57 - have as much vram on it so gradient accumulation is wonderful and it's used lots in practice
340:04 - the final thing i'd like to leave you guys off with is something called hugging face
340:08 - and you've probably heard a lot a lot about this so far but let me just guide you through and show
340:13 - you how absolutely explosive hugging face is for machine learning so you have a bunch of models
340:20 - data sets spaces docs etc and let's just go to models for example so just to showcase how cool
340:28 - this is you have multimodal ais which could be like uh image and text or video etc right you
340:35 - have multiple different modes so it's not just text or not just video it's many different ones
340:40 - at the same time so you have multimodal models you have computer vision you have natural language
340:47 - processing and we're we're actually doing natural language processing in this course
340:51 - we have audio a tabular and reinforcement learning so this is really cool and you can
340:57 - actually just download these models and host them on your own computer so that is really cool
341:02 - you also have data sets which are even cooler and these are pretty much just really high quality
341:08 - data sets of prompt and answer completions at least for our purpose if you want to use those
341:14 - so you have uh question answering or conversational so if i go to the open orca data set for example
341:23 - that's 9 000 downloads 500 likes it has a bunch of uh ids system prompt so you're an ai assistant
341:32 - whatever and then you have the cool stuff which is you'll be given a definition of a task first
341:37 - and some input of the task etc and then the response it's like oh we just gave it an input
341:42 - and asked it to answer in a format and actually did that correctly so you could pretty much train
341:49 - these on a bunch of prompts that you would be able to feed into gpt4 and try to make your model
341:55 - perform that way and this actually has 4.23 million rows in the training split which is amazing
342:01 - okay so data sets are wonderful and you can find the best ones at least the best fine tuning data
342:08 - sets on open orca really good that's from pre-training i believe i mentioned this earlier
342:14 - in this survey of large language models paper that if we just put it down through the reddit links
342:21 - yes you could use like open web text you could use common crawl you could use books you could
342:31 - use wikipedia these are all pre-training data sources so yeah hopefully that leaves you with
342:38 - a better understanding on how to create gpts transformers and pretty good large language
342:44 - models from scratch with your own data that you scraped or that you downloaded and yeah that's it
342:51 - thanks for watching so you've learned a ton in this course about language modeling how to use
342:56 - data how to create architectures from scratch maybe even how to look at research papers so
343:03 - if you really enjoy this content i would encourage you to maybe subscribe and like on my youtube
343:08 - channel which is in the description i make many videos about ai programming and computer
343:14 - science in general so you could totally feel free to subscribe there if you don't want to subscribe
343:19 - that's fine you could always unsubscribe later if you want to it's completely free but yeah i also
343:26 - have a github repo in the description for all the code that we used not the data because it's way
343:31 - too big but all of the code and the wizard of oz wizard of oz text file so that is all in the
343:38 - github repo in the description thanks for watching