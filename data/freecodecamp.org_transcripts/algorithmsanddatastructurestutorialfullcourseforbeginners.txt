00:00 - this is a full-length course from
00:01 - treehouse we at free code camp are
00:03 - longtime fans of their learning platform
00:06 - they were kind enough to let our
00:07 - non-profit make this course freely
00:09 - available on our youtube channel if you
00:11 - like this course treehouse has a lot
00:13 - more courses like this one the link is
00:15 - in the description
00:16 - along with time codes to the different
00:18 - sections in this course
00:21 - [Music]
00:29 - hi my name is passan i'm an instructor
00:31 - here at treehouse and welcome to
00:33 - introduction to algorithms
00:34 - whether you are a high school or college
00:36 - student a developer in the industry or
00:39 - someone who is learning to code you have
00:41 - undoubtedly run into the term algorithm
00:44 - for many people this word is kind of
00:46 - scary it represents this body of
00:48 - knowledge that seems just out of reach
00:51 - only people with computer science
00:52 - degrees know about algorithms now to
00:55 - others this brings up feelings of
00:56 - imposter syndrome
00:58 - you might already know how to code but
01:00 - you're not a real developer because you
01:03 - don't know anything about algorithms
01:05 - personally it made me frame certain jobs
01:08 - as above my skill level because the
01:10 - interview contained algorithm questions
01:13 - well whatever your reasons are in this
01:16 - course our goal is to dispel all those
01:18 - feelings and get you comfortable with
01:20 - the basics of algorithms
01:22 - like any other subject i like to start
01:24 - my courses with what the course is and
01:27 - is not
01:28 - in this course we're going to cover the
01:30 - very basic set of knowledge that you
01:32 - need as a foundation for learning about
01:34 - algorithms
01:36 - this course is less about specific
01:38 - algorithms and more about the tools you
01:40 - will need to evaluate algorithms
01:43 - understand how they perform
01:44 - compare them to each other and make a
01:46 - statement about the utility of an
01:48 - algorithm in a given context
01:50 - now don't worry none of this will be
01:52 - theoretical and we will learn these
01:54 - concepts by using well-known algorithms
01:57 - in this course we will also be writing
01:59 - code so i do expect you to have some
02:01 - programming experience if you intend to
02:03 - continue with this topic
02:05 - you can definitely stick around even if
02:07 - you don't know how to code but you might
02:09 - want to learn the basics of programming
02:11 - in the meantime
02:13 - in this course we will be using the
02:14 - python programming language python reads
02:17 - a lot like regular english and is the
02:19 - language you will most likely encounter
02:21 - when learning about algorithms these
02:23 - days
02:24 - if you don't know how to code or if you
02:26 - know how to code in a different language
02:28 - check out the notes section of this
02:30 - video for links to other content that
02:32 - might be useful to you
02:34 - as long as you understand the
02:36 - fundamentals of programming you should
02:38 - be able to follow along pretty well
02:41 - if you're a javascript developer or a
02:43 - student who's learning javascript for
02:45 - example chances are good that you'll
02:47 - still be able to understand the code we
02:48 - write later i'll be sure to provide
02:51 - links along the way if you need anything
02:53 - to follow up on
02:55 - let's start with something simple
02:57 - what is an algorithm
02:59 - an algorithm is a set of steps or
03:01 - instructions for completing a task
03:04 - this might sound like an over
03:06 - simplification but really that's
03:08 - precisely what an algorithm is
03:11 - a recipe is an algorithm your morning
03:13 - routine when you wake up is an algorithm
03:16 - and the driving directions you follow to
03:18 - get to a destination is also an
03:19 - algorithm
03:20 - in computer science the term algorithm
03:23 - more specifically means the set of steps
03:25 - a program takes to finish a task
03:28 - if you've written code before any code
03:30 - really generally speaking you have
03:32 - written an algorithm
03:34 - given that much of the code we write can
03:36 - be considered an algorithm what do
03:38 - people mean when they say you should
03:40 - know about algorithms
03:42 - now consider this
03:44 - let's say i'm a teacher in a classroom
03:46 - and i tell everyone i have an assignment
03:47 - for them on their desks they have a
03:49 - picture of a maze and their task is to
03:52 - come up with a way to find the quickest
03:54 - way out of the maze
03:56 - everyone does their thing and comes up
03:58 - with a solution
03:59 - every single one of these solutions is a
04:01 - viable solution and is a valid example
04:04 - of an algorithm the steps one needs to
04:06 - take to get out of the maze but from
04:08 - being in classrooms or any group of any
04:11 - sort you know that some people will have
04:13 - better ideas than others we all have a
04:15 - diverse array of skill sets
04:18 - over time our class picks the best of
04:20 - these solutions and any time we want to
04:22 - solve a maze we go with one of these
04:24 - solutions this is what the field of
04:27 - algorithms is about
04:29 - there are many problems in computer
04:30 - science but some of them are pretty
04:32 - common regardless of what project you're
04:35 - working on
04:36 - different people have come up with
04:37 - different solutions to these common
04:39 - problems and over time the field of
04:42 - computer science has identified several
04:44 - that do the job well for a given task
04:47 - when we talk of algorithms we're
04:49 - referring to two points
04:51 - we're primarily saying there's an
04:53 - established body of knowledge on how to
04:55 - solve particular problems well and it's
04:58 - important to know what the solutions are
05:01 - now why is it important
05:03 - if you're unaware that a solution exists
05:05 - you might try to come up with one
05:07 - yourself and there's a likelihood that
05:09 - your solution won't be as good or
05:11 - efficient whatever that means compared
05:14 - to those that have been thoroughly
05:15 - reviewed
05:16 - but there's a second component to it as
05:18 - well
05:19 - part of understanding algorithms is not
05:22 - just knowing that an algorithm exists
05:24 - but understanding when to apply it
05:27 - understanding when to apply an algorithm
05:29 - requires properly understanding the
05:32 - problem at hand and this arguably is the
05:34 - most important part of learning about
05:36 - algorithms and data structures
05:39 - as you progress through this content you
05:41 - should be able to look at a problem and
05:43 - break it down into distinct steps
05:46 - when you have a set of steps you should
05:47 - then be able to identify which algorithm
05:50 - or data structure is best for the task
05:52 - at hand
05:53 - this concept is called algorithmic
05:55 - thinking and it's something we're going
05:57 - to try and cultivate together as we work
05:59 - through our content
06:01 - lastly learning about algorithms gives
06:03 - you a deeper understanding about
06:05 - complexity and efficiency in programming
06:08 - having a better sense of how your code
06:10 - will perform in different situations is
06:12 - something that you'll always want to
06:14 - develop in hone
06:16 - algorithmic thinking is why algorithms
06:18 - also come up in big tech interviews
06:21 - interviewers don't care as much that you
06:23 - are able to write a specific algorithm
06:25 - in code but more about the fact that you
06:28 - can break a seemingly insurmountable
06:30 - problem into distinct components and
06:32 - identify the right tools to solve each
06:35 - distinct component
06:37 - and that is what we plan on doing in
06:39 - this course though we're going to focus
06:41 - on some of the tools and concepts you'll
06:43 - need to be aware of before we can dive
06:46 - into the topic of algorithms if you're
06:48 - ready let's get started
06:51 - hey again in this video we're going to
06:52 - do something unusual we're going to play
06:54 - a game and by we i mean me and my two
06:57 - friends here brittany and john
06:59 - this game is really simple and you may
07:01 - have played it before it goes something
07:02 - like this i'm going to think of a number
07:04 - between 1 and 10 and they have to guess
07:07 - what the number is easy right
07:09 - when they guess a number i'll tell them
07:11 - if their guess is too high or too low
07:13 - the winner is the one with the fewest
07:15 - tries all right john let's start with
07:17 - you i'm thinking of a number between one
07:19 - and ten what is it
07:21 - between you and me the answer is three
07:23 - uh quick question does the range include
07:25 - one and ten
07:27 - that is a really good question so what
07:29 - john did right there was to establish
07:30 - the bounds of our problem
07:32 - no solution works on every problem and
07:34 - an important part of algorithmic
07:36 - thinking is to clearly define what the
07:38 - problem set is and clarify what values
07:40 - count as inputs
07:42 - yeah 1 and ten are both included is it
07:45 - one too low is it two too low is it
07:47 - three correct
07:49 - okay so that was an easy one it took
07:51 - john three tries to get the answer let's
07:53 - switch over to brittany and play another
07:55 - round using the same number as the
07:57 - answer okay brittany i'm thinking of a
07:59 - number between 1 and 10 inclusive so
08:01 - both 1 and 10 are in the range what
08:02 - number am i thinking of
08:04 - is it 5 too high
08:06 - 2 too low
08:08 - is it 3 correct all right so what we had
08:10 - there was two very different ways of
08:12 - playing the same game
08:14 - somehow with even such a simple game we
08:16 - saw different approaches to figuring out
08:18 - a solution
08:19 - to go back to algorithmic thinking for a
08:21 - second this means that with any given
08:24 - problem there's no one best solution
08:27 - instead what we should try and figure
08:29 - out is what solution works better for
08:31 - the current problem
08:32 - in this first pass at the game they both
08:35 - took the same amount of turns to find
08:36 - the answer so it's not obvious who has
08:39 - the better approach and that's mostly
08:40 - because the game was easy
08:42 - let's try this one more time now this
08:44 - time the answer is 10.
08:46 - all right john you first is it one too
08:48 - low is it two still too low is it three
08:51 - too low is it four too low is it five
08:54 - still too low is it six too low is it
08:56 - seven too low is it eight low is it nine
09:00 - do low is it ten correct you got it okay
09:03 - so now same thing but with britney this
09:05 - time
09:06 - is it five too low
09:08 - eight too low is it nine still too low
09:11 - it's ten
09:13 - all right so here we start to see a
09:14 - difference between their strategies when
09:16 - the answer was three they both took the
09:18 - same number of turns this is important
09:21 - when the number was larger but not that
09:23 - much larger 10 in this case we start to
09:26 - see that britney strategy did better she
09:28 - took four tries while john took 10.
09:31 - we've played two rounds so far and we've
09:33 - seen a different set of results based on
09:34 - the number they were looking for
09:36 - if you look at john's way of doing
09:38 - things then the answer being 10 the
09:40 - round we just played is his worst case
09:42 - scenario he will take the maximum number
09:45 - of turns 10 to guess it
09:48 - when we picked a random number like
09:49 - three it was hard to differentiate which
09:52 - strategy was better because they both
09:54 - performed exactly the same
09:56 - but in john's worst case scenario a
09:58 - clear winner in terms of strategy
10:00 - emerges
10:01 - in terms of algorithmic thinking we're
10:03 - starting to get a sense that the
10:05 - specific value they're searching for may
10:07 - not matter as much as where that value
10:10 - lies in the range that they've been
10:11 - given
10:12 - identifying this helps us understand our
10:15 - problem better
10:16 - let's do this again for a range of
10:18 - numbers from one to one hundred we'll
10:20 - start by picking five as an answer to
10:22 - trick them
10:23 - okay so this time we're going to run
10:25 - through the exercise again this time
10:26 - from one to one hundred and both one and
10:28 - one hundred are included is it one at
10:30 - this point without even having to run
10:32 - through it we can guess how many tries
10:33 - john is going to take since he starts at
10:35 - one and keeps going he's going to take
10:37 - five tries as we're about to see is it
10:39 - five cool correct
10:41 - okay now for brittany's turn
10:43 - is it 50 too high is it 25 still too
10:46 - high
10:47 - is it 13 too high is it seven
10:50 - too high
10:51 - is it four too low
10:53 - is it six too high is it five correct
10:57 - let's evaluate john took five tries
11:00 - brittany on the other hand takes seven
11:01 - tries so john wins this round but again
11:04 - in determining whose strategy is
11:06 - preferred there's no clear winner right
11:08 - now
11:09 - what this tells us is that it's not
11:11 - particularly useful to look at the easy
11:13 - answers where we arrive at the number
11:15 - fairly quickly because it's at the start
11:16 - of the range
11:18 - instead let's try one where we know john
11:20 - is going to do poorly let's look at his
11:22 - worst case scenario where the answer is
11:24 - 100 and see how britney performs in such
11:27 - a scenario
11:28 - okay john let's do this one more time
11:30 - one through 100 again
11:32 - is it one we can fast forward this scene
11:34 - because well we know what happens john
11:36 - takes 100 tries
11:38 - hi brittany you're up
11:40 - is it 50 too low is it 75 too low 88 too
11:44 - low
11:45 - 94 too low is it 97 too low
11:49 - 99 too low
11:51 - 100.
11:53 - okay so that took brittney seven turns
11:55 - again and this time she is the clear
11:57 - winner if you compare their individual
11:59 - performances for the same number set
12:01 - you'll see that britney's approach
12:03 - leaves john's in the dust
12:05 - when the answer was five so right around
12:06 - the start of the range john took five
12:08 - turns but when the answer was 100 right
12:11 - at the end of the range he took 100
12:13 - tries it took him 20 times the amount of
12:15 - tries to get that answer compared to
12:17 - britney
12:19 - on the other hand if you compare
12:20 - britney's efforts when the number was 5
12:22 - she took seven tries but when the number
12:24 - was 100 she took the same amount of
12:26 - tries this is pretty impressive if we
12:29 - pretend that the number of tries is the
12:31 - number of seconds it takes britney and
12:33 - john to run through their attempts this
12:35 - is a good estimate for how fast their
12:38 - solutions are
12:39 - ok we've done this a couple times and
12:41 - brittany and john are getting tired
12:42 - let's take a break in the next video
12:44 - we'll talk about the point of this
12:46 - exercise
12:47 - in the last video we ran through an
12:49 - exercise where i had some of my
12:50 - co-workers guess what number i was
12:52 - thinking so was the point of that
12:54 - exercise you might be thinking hey i
12:56 - thought i was here to learn about
12:57 - algorithms
12:59 - the exercise we just did was an example
13:01 - of a real life situation you will run
13:04 - into when building websites apps and
13:06 - writing code
13:08 - both approaches taken by john and
13:10 - brittany to find the number i was
13:11 - thinking of are examples of searching
13:14 - for a value
13:15 - it might be weird to think that there's
13:16 - more than one way to search but as you
13:19 - saw in the game the speed at which the
13:21 - result was obtained differed between
13:23 - john and brittany think about this
13:25 - problem from the perspective of a
13:26 - company like facebook
13:28 - at the time of this recording facebook
13:30 - has 2.19 billion active users
13:33 - let's say you're traveling in a
13:34 - different country and meet someone you
13:36 - want to add on facebook
13:38 - you go into the search bar and type out
13:39 - this person's name
13:41 - if we simplify how the facebook app
13:44 - works it has to search across these 2.19
13:47 - billion records and find the person you
13:50 - are looking for
13:51 - the speed at which you find this person
13:53 - really matters imagine what kind of
13:55 - experience it would be if when you
13:57 - search for a friend facebook put up a
14:00 - spinning activity indicator and said
14:02 - come back in a couple hours
14:04 - i don't think we'd use facebook as much
14:06 - if that was the case
14:07 - from the company's perspective working
14:09 - on making search as fast as possible
14:12 - using different strategies really
14:14 - matters
14:15 - now i said that the two strategies
14:17 - britney and john used were examples of
14:19 - search
14:20 - more specifically these are search
14:22 - algorithms
14:24 - the strategy john took where he started
14:26 - at the beginning of the range and just
14:28 - counted one number after the other is a
14:31 - type of search called linear search
14:33 - it is also called sequential search
14:36 - which is a better description of how it
14:37 - works or even simple search since it
14:40 - really is quite simple
14:42 - but what makes his approach an algorithm
14:45 - as opposed to just looking for something
14:47 - remember we said that an algorithm is a
14:49 - set of steps or instructions to complete
14:51 - a task
14:53 - linear search is a search algorithm and
14:55 - we can define it like this
14:57 - we start at the beginning of the list or
14:59 - the range of values
15:01 - then we compare the current value to the
15:03 - target
15:04 - if the current value is the target value
15:06 - that we're looking for we're done
15:09 - if it's not we'll move on sequentially
15:11 - to the next value in the list and then
15:13 - repeat step 2.
15:15 - if we reach the end of the list then the
15:17 - target value is not in the list
15:20 - this definition has nothing to do with
15:22 - programming and in fact you can use it
15:24 - in the real world for example
15:26 - i could tell you to walk into a
15:27 - bookstore and find me a particular book
15:30 - and one of the ways you could do it is
15:32 - using the linear search algorithm
15:34 - you could start at the front of the
15:36 - bookstore and read the cover or the
15:38 - spine of every book to check that it
15:40 - matches the book that you're looking for
15:42 - if it doesn't you go to the next book
15:44 - and repeat until you find it or run out
15:46 - of books
15:48 - what makes this an algorithm is the
15:50 - specificity of how it is defined
15:53 - in contrast to just jumping into a
15:55 - problem and solving it as we go along an
15:58 - algorithm follows a certain set of
15:59 - guidelines and we use the same steps to
16:02 - solve the problem each time we face it
16:05 - an important first step to defining the
16:07 - algorithm isn't the algorithm itself but
16:09 - the problem we're trying to solve
16:12 - our first guideline is that an algorithm
16:14 - must have a clear problem statement
16:17 - it's pretty hard to define an
16:18 - instruction set when you don't have a
16:20 - clear idea of what problem you're trying
16:22 - to solve
16:23 - in defining the problem we need to
16:25 - specify how the input is defined and
16:28 - what the output looks like when the
16:30 - algorithm has done its job
16:32 - for linear search the input can be
16:34 - generally described as a series of
16:36 - values and the output is a value
16:38 - matching the one we're looking for
16:41 - right now we're trying to stay away from
16:42 - anything code related so this problem
16:44 - statement definition is pretty generic
16:46 - but once we get to code we can actually
16:48 - tighten this up
16:49 - once we have a problem an algorithm is a
16:51 - set of steps that solves this problem
16:54 - given that the next guideline is that an
16:57 - algorithm definition must contain a
16:59 - specific set of instructions in a
17:01 - particular order
17:03 - we really need to be clear about the
17:05 - order in which these instructions are
17:07 - executed
17:08 - taking our simple definition of linear
17:11 - search if i switched up the order and
17:13 - said move sequentially to the next value
17:16 - before specifying that first comparison
17:18 - step if the first value were the target
17:20 - one our algorithm wouldn't find it
17:22 - because we moved to the second value
17:24 - before comparing
17:26 - now you might think okay that's just an
17:28 - avoidable mistake and kind of common
17:30 - sense
17:30 - the thing is computers don't know any of
17:32 - that and just do exactly as we tell them
17:35 - so specific order is really important
17:38 - the third guideline is that each step in
17:40 - our algorithm definition must not be a
17:42 - complex one and needs to be explicitly
17:44 - clear what i mean by that is that you
17:47 - shouldn't be able to break down any of
17:49 - the steps into further into additional
17:51 - subtasks
17:53 - each step needs to be a distinct one we
17:55 - can't define linear search as search
17:57 - until you find this value because that
17:59 - can be interpreted in many ways and
18:02 - further broken down into many more steps
18:04 - it's not clear
18:06 - next and this one might seem obvious but
18:08 - algorithms should produce a result
18:11 - if it didn't how would we know whether
18:13 - the algorithm works or not
18:16 - to be able to verify that our algorithm
18:18 - works correctly we need a result
18:21 - now when using a search algorithm the
18:23 - end result can actually be nothing which
18:25 - indicates that the value wasn't found
18:27 - but that's perfectly fine
18:29 - there are several ways to represent
18:31 - nothing in code and as long as the
18:33 - algorithm can produce some results we
18:35 - can understand its behavior
18:37 - the last guideline is that the algorithm
18:40 - should actually complete and cannot take
18:41 - an infinite amount of time
18:43 - if we let john loose in the world's
18:45 - largest library and asked him to find a
18:48 - novel we have no way of knowing whether
18:50 - he succeeded or not unless he came back
18:52 - to us with a result
18:54 - okay so quick recap what makes an
18:56 - algorithm an algorithm and not just
18:58 - something you do
19:00 - one it needs to have a clearly defined
19:02 - problem statement input and output
19:05 - when using linear search the input needs
19:07 - to be just a series of values but to
19:10 - actually use brittany's strategy there's
19:12 - one additional precondition so to speak
19:15 - if you think about her strategy it
19:17 - required that the numbers be sorted in
19:19 - ascending order
19:21 - this means that where the input for john
19:22 - is just a series of values to solve the
19:25 - problem the input to brittany's
19:27 - algorithm needs to be a sorted series of
19:29 - values
19:30 - so clearly defined problem statement
19:32 - clearly defined input and clearly
19:34 - defined output
19:36 - second the steps in the algorithm need
19:38 - to be in a very specific order
19:41 - the steps also need to be distinct you
19:44 - should not be able to break it down into
19:46 - further subtasks
19:48 - next the algorithm should produce a
19:50 - result
19:51 - and finally the algorithm should
19:53 - complete in a finite amount of time
19:56 - these guidelines not only help us define
19:58 - what an algorithm is but also helps us
20:01 - verify that the algorithm is correct
20:04 - executing the steps in an algorithm for
20:06 - a given input must result in the same
20:09 - output every time
20:12 - if in the game i played the answer was
20:14 - 50 every time then every single time
20:17 - john must take 50 turns to find out that
20:20 - the answer is 50. if somehow he takes 50
20:23 - turns in one round then 30 the next and
20:26 - we technically don't have a correct
20:27 - algorithm
20:29 - consistent results for the same set of
20:31 - values is how we know that the algorithm
20:33 - is correct
20:35 - i should stress that we're not going to
20:36 - be designing any algorithms on our own
20:39 - and we'll start off and spend most of
20:41 - our time learning the tried and true
20:43 - algorithms that are known to efficiently
20:45 - solve problems
20:46 - the reason for talking about what makes
20:48 - for a good algorithm though is that the
20:50 - same set of guidelines makes for good
20:53 - algorithmic thinking which is one of the
20:55 - most important skills we want to
20:56 - cultivate
20:58 - when we encounter a problem before
21:00 - rushing in and thinking about solutions
21:02 - what we want to do is work through the
21:04 - guidelines
21:05 - first we break down the problem into any
21:07 - possible number of smaller problems
21:09 - where each problem can be clearly
21:11 - defined in terms of an input and an
21:13 - output
21:15 - now that we know how to generally define
21:17 - an algorithm let's talk about what it
21:18 - means to have a good algorithm
21:20 - an important thing to keep in mind is
21:22 - that there's no one single way to
21:25 - measure whether an algorithm is the
21:27 - right solution because it is all about
21:29 - context
21:30 - earlier we touched on two concepts
21:33 - correctness and efficiency
21:35 - let's define correctness more clearly
21:37 - because before we can evaluate an
21:39 - algorithm on efficiency we need to
21:41 - ensure its correctness
21:43 - before we define our algorithms we start
21:45 - by defining our problem
21:48 - in the definition of that problem we
21:50 - have a clearly defined input satisfying
21:52 - any preconditions and a clearly defined
21:55 - output
21:56 - an algorithm is deemed correct if on
21:59 - every run of the algorithm against all
22:01 - possible values in the input data we
22:03 - always get the output we expect
22:06 - part of correctness means that for any
22:09 - possible input the algorithm should
22:11 - always terminate or end
22:14 - if these two are not true then our
22:16 - algorithm isn't correct
22:19 - if you were to pick up an algorithm's
22:20 - textbook and look up correctness you
22:22 - will run into a bunch of mathematical
22:25 - theory this is because traditionally
22:27 - algorithm correctness is proved by
22:29 - mathematical induction which is a form
22:32 - of reasoning used in mathematics to
22:34 - verify that a statement is correct
22:37 - this approach involves writing what is
22:38 - called a specification and a correctness
22:41 - proof
22:42 - we won't be going into that in this
22:44 - course
22:45 - proof through induction is an important
22:47 - part of designing algorithms but we're
22:49 - confident that you can understand
22:51 - algorithms both in terms of how and when
22:53 - to use them without getting into the
22:55 - math so if you pick up a textbook and
22:58 - feel daunted don't worry i do too but we
23:00 - can still figure things out without it
23:02 - all right so once we have a correct
23:04 - algorithm we can start to talk about how
23:07 - efficient an algorithm is
23:09 - remember that this efficiency ultimately
23:12 - matters because they help us solve
23:13 - problems faster and deliver a better end
23:16 - user experience in a variety of fields
23:19 - for example algorithms are used in the
23:22 - sequencing of dna and more efficient
23:24 - sequencing algorithms allow us to
23:27 - research and understand diseases better
23:29 - and faster but let's not get ahead of
23:31 - ourselves we'll start simple by
23:34 - evaluating john's linear search
23:36 - algorithm in terms of its efficiency
23:39 - first what do we mean by efficiency
23:41 - there are two measures of efficiency
23:43 - when it comes to algorithms time and
23:46 - space sounds really cool and very sci-fi
23:48 - huh
23:49 - efficiency measured by time something
23:52 - you'll hear called time complexity is a
23:54 - measure of how long it takes the
23:56 - algorithm to run
23:58 - time complexity can be understood
24:00 - generally outside the context of code
24:02 - and computers because how long it takes
24:04 - to complete a job is a universal measure
24:06 - of efficiency the less time you take the
24:09 - more efficient you are
24:10 - the second measure of efficiency is
24:12 - called space complexity and this is
24:15 - pretty computer specific
24:16 - it deals with the amount of memory taken
24:19 - up on the computer
24:21 - good algorithms need to balance between
24:23 - these two measures to be useful
24:25 - for example you can have a blazingly
24:27 - fast algorithm but it might not matter
24:29 - if the algorithm consumes more memory
24:31 - than you have available
24:33 - both of these concepts time and space
24:35 - complexity are measured using the same
24:38 - metric but it is a very technical
24:40 - sounding metric so let's build up to it
24:42 - slowly and start simple
24:45 - a few videos ago i played a game with
24:47 - brittany and john where they tried to
24:49 - guess the number i was thinking of
24:51 - effectively they were searching for a
24:52 - value
24:53 - so how do we figure out how efficient
24:56 - each algorithm is and which algorithm
24:58 - was more suited to our purposes
25:01 - if we consider the number of tries they
25:03 - took to guess or search for the value as
25:06 - an indicator of the time they take to
25:09 - run through the exercise this is a good
25:11 - indicator of how long the algorithm runs
25:13 - for a given set of values
25:16 - this measurement is called the running
25:17 - time of an algorithm and we'll use it to
25:19 - define time complexity
25:21 - in the game we play it four rounds let's
25:24 - recap those here focusing on john's
25:26 - performance
25:27 - in round one we had 10 values the target
25:30 - was 3 and john took 3 turns
25:33 - in round 2 we had 10 values the target
25:35 - was 10 and john took 10 turns
25:38 - in round 3 we had 100 values the target
25:40 - was
25:41 - john took five tries and finally in
25:43 - round four when the target was 100 given
25:46 - 100 values john took 100 tries
25:50 - on paper it's hard to gauge anything
25:52 - about this performance
25:53 - when it comes to anything with numbers
25:55 - though i like to put it up on a graph
25:56 - and compare visually
25:58 - on the vertical or y-axis let's measure
26:01 - the number of tries it took john to
26:03 - guess the answer or the running time of
26:05 - the algorithm on the horizontal or
26:08 - x-axis what do we put
26:10 - for each turn we have a number of values
26:13 - as well as a target value
26:16 - we could plot the target value on the
26:18 - horizontal axis but that leaves some
26:20 - context and meaning behind it's far more
26:23 - impressive that john took five tries
26:25 - when the range went up to 100 then when
26:28 - he took three tries for a maximum of 10
26:30 - values
26:32 - we could plot the maximum range of
26:34 - values but then we're leaving out the
26:35 - other half of the picture
26:37 - there are data points however that
26:39 - satisfy both requirements
26:41 - if we only plot the values where the
26:44 - target the number john was looking for
26:46 - was the same as the maximum range of
26:49 - values we have a data point that
26:51 - includes both the size of the data set
26:53 - as well as his effort
26:55 - there's an additional benefit to this
26:57 - approach as well
26:58 - there are three ways we can measure how
27:00 - well john does or in general how well
27:02 - any algorithm does
27:04 - first we can check how well john does in
27:06 - the best case or good scenarios from the
27:09 - perspective of his strategy
27:11 - in the range of 100 values the answer
27:13 - being a low number like three at the
27:15 - start of the range is a good scenario he
27:18 - can guess it fairly quickly one is his
27:21 - best case scenario
27:22 - or we could check how well he does on
27:24 - average we could run this game a bunch
27:26 - of times and average out the running
27:28 - time
27:29 - this would give us a much better picture
27:30 - of john's performance over time but our
27:33 - estimates would be too high if the value
27:35 - he was searching for was at the start of
27:37 - the range or far too low if it was at
27:39 - the end of the range
27:41 - let's imagine a scenario where facebook
27:43 - naively implements linear search when
27:44 - finding friends
27:46 - they looked at the latest u.s census saw
27:48 - that 50 of names start with the letters
27:51 - a through j which is the first 40 of the
27:54 - alphabet and thought okay on average
27:57 - linear search serves us well
27:59 - but what about the rest of those whose
28:01 - names start with the letter after j in
28:03 - the alphabet
28:04 - searching for my name would take longer
28:06 - than the average and much longer for
28:08 - someone whose name starts with the
28:09 - letter z
28:10 - so while measuring the run time of an
28:12 - algorithm on average might seem like a
28:14 - good strategy it won't necessarily
28:16 - provide an accurate picture
28:19 - by picking the maximum in the range
28:21 - we're measuring how our algorithm does
28:23 - in the worst case scenario
28:26 - analyzing the worst case scenario is
28:27 - quite useful because it indicates that
28:30 - the algorithm will never perform worse
28:32 - than we expect there's no room for
28:34 - surprises
28:36 - back to our graph we're going to plot
28:38 - the number of tries a proxy for running
28:40 - time of the algorithm against the number
28:42 - of values in the range which will
28:44 - shorten to n
28:46 - n here also represents john's worst case
28:48 - scenario when n is 10 he takes 10 turns
28:52 - when n is 100 he takes 100 turns
28:55 - but these two values alone are
28:57 - insufficient to really get any sort of
28:59 - visual understanding moreover it's not
29:02 - realistic
29:03 - john may take a long time to work
29:05 - through 100 numbers but a computer can
29:07 - do that in no time
29:09 - to evaluate the performance of linear
29:11 - search in the context of a computer we
29:13 - should probably throw some harder and
29:15 - larger ranges of values at it
29:18 - the nice thing is by evaluating a worst
29:20 - case scenario we don't actually have to
29:22 - do that work
29:24 - we know what the result will be for a
29:26 - given value of n using linear search it
29:28 - will take n tries to find the value in
29:31 - the worst case scenario so let's add a
29:33 - few values in here to build out this
29:35 - graph
29:36 - okay so we have a good picture of what
29:38 - this is starting to look like as the
29:40 - values get really large the running time
29:42 - of the algorithm gets large as well
29:45 - we sort of already knew that
29:47 - before we dig into this runtime any
29:48 - deeper let's switch tracks and evaluate
29:50 - brittany's work
29:52 - by having something to compare against
29:54 - it should become easier to build a
29:55 - mental model around time complexity
29:58 - the algorithm john used linear search
30:01 - seemed familiar to us and you could
30:03 - understand it because it's how most of
30:04 - us search for things in real life anyway
30:07 - brittany's approach on the other hand
30:09 - got results quickly but it was a bit
30:11 - harder to understand so let's break it
30:13 - down
30:14 - just like john's approach britney
30:16 - started with a series of values or a
30:17 - list of numbers as her input
30:20 - where john just started at the beginning
30:22 - of the list and searched sequentially
30:24 - brittany's strategy is to always start
30:26 - in the middle of the range
30:28 - from there she asks a comparison
30:30 - question
30:31 - is the number in the middle of the range
30:33 - equal to the answer she's looking for
30:35 - and if it's not is it greater than or
30:37 - less than the answer
30:39 - if it's greater than she can eliminate
30:41 - all the values less than the one she's
30:43 - currently evaluating if it's lesser than
30:46 - the answer she can eliminate all the
30:48 - values greater than the one she's
30:49 - currently evaluating
30:51 - with the range of values that she's left
30:53 - over with she repeats this process until
30:56 - she arrives at the answer
30:58 - let's visualize how she did this by
31:00 - looking at round three
31:02 - in round three the number of values in
31:04 - the range was 100 the answer was 5.
31:07 - the bar here represents the range of
31:08 - values one of the left 100 at the right
31:11 - and this pointer represents the value
31:13 - britney chooses to evaluate
31:15 - so she starts in the middle at 50. she
31:18 - asks is it equal to the answer i say
31:20 - it's too high so this tells her that the
31:23 - value she is evaluating is greater than
31:25 - our target value which means there's no
31:28 - point in searching any of the values to
31:30 - the right of 50 that is values greater
31:33 - than 50 in this range so she can discard
31:35 - those values altogether
31:37 - she only has to consider values from 1
31:40 - to 50 now
31:41 - the beauty of this strategy and the
31:43 - reason why britney was able to find the
31:45 - answer in such few turns is that with
31:47 - every value she evaluates she can
31:50 - discard half of the current range
31:53 - on her second turn she picks the value
31:55 - in the middle of the current range which
31:56 - is 25. she asks the same question i say
32:00 - that the value is too high again and
32:02 - this tells her that she can discard
32:04 - everything greater than 25 and the range
32:06 - of values drops from 1 to 25.
32:10 - again she evaluates the number in the
32:11 - middle roughly so that'd be 13 here i
32:14 - tell her this is still too high she
32:16 - discards the values greater moves to
32:18 - value at 7 which is still too high
32:21 - then she moves to 4 which is now too low
32:24 - she can discard everything less than 4
32:26 - which leaves the numbers 4 through 7.
32:28 - here she picked 6 which was too high
32:31 - which only leaves one value 5.
32:34 - this seems like a lot of work but being
32:36 - able to get rid of half the values with
32:38 - each turn is what makes this algorithm
32:41 - much more efficient
32:42 - now there's one subtlety to using binary
32:45 - search and you might have caught on to
32:46 - this
32:47 - for this search method to work as we've
32:49 - mentioned the values need to be sorted
32:51 - with linear search it doesn't matter if
32:53 - the values are sorted since a linear
32:56 - search algorithm just progresses
32:58 - sequentially checking every element in
33:00 - the list if the target value exists in
33:02 - the list it will be fouled but let's say
33:05 - this range of values 100 was unsorted
33:08 - britney would start at the middle with
33:10 - something like 14 and ask if this value
33:12 - was too low or too high i say it's too
33:15 - high so she discards everything less
33:17 - than 14.
33:18 - now this example starts to fall apart
33:20 - here because well britney knows what
33:22 - numbers are less than 14 and greater
33:24 - than one she doesn't need an actual
33:26 - range of values to solve this a computer
33:29 - however does need that
33:31 - remember search algorithms are run
33:33 - against lists containing all sorts of
33:35 - data it's not always just a range of
33:37 - values containing numbers
33:39 - in a real use case of binary search
33:41 - which we're going to implement in a bit
33:43 - the algorithm wouldn't return the target
33:45 - value because we already know that it's
33:48 - a search algorithm so we're providing
33:50 - something to search for instead what it
33:52 - returns is the position in the list that
33:54 - the target occupies without the list
33:56 - being sorted a binary search algorithm
33:59 - would discard all the values to the left
34:01 - of 14 which over here could include the
34:03 - position where our target value is
34:06 - eventually we'd get a result back saying
34:07 - the target value doesn't exist in the
34:09 - list which is inaccurate
34:11 - earlier when defining linear simple
34:14 - search i said that the input was a list
34:16 - of values and the output was the target
34:18 - value or more specifically the position
34:21 - of the target value in the list
34:23 - so with binary search there's also that
34:25 - precondition the input list must be
34:27 - sorted so let's formally define binary
34:30 - search
34:31 - first the input a sorted list of values
34:34 - the output the position in the list of
34:36 - the target value we're searching for or
34:39 - some sort of values indicate that the
34:41 - target does not exist in the list
34:44 - remember our guidelines for defining an
34:45 - algorithm let me put those up again
34:47 - really quick
34:48 - the steps in the algorithm need to be in
34:50 - a specific order the steps also need to
34:52 - be very distinct
34:54 - the algorithms should produce a result
34:56 - and finally the algorithm should
34:58 - complete in a finite amount of time
35:00 - let's use those to define this algorithm
35:03 - step one we determine the middle
35:05 - position of the sorted list
35:07 - step two we compare the element in the
35:09 - middle position to the target element
35:12 - step three if the elements match we
35:14 - return the middle position and end
35:17 - if they don't match in step 4 we check
35:20 - whether the element in the middle
35:21 - position is smaller than the target
35:23 - element
35:24 - if it is then we go back to step 2 with
35:27 - a new list that goes from the middle
35:29 - position of the current list to the end
35:31 - of the current list
35:32 - in step five if the element in the
35:34 - middle position is greater than the
35:36 - target element then again we go back to
35:38 - step two with a new list that goes from
35:40 - the start of the current list to the
35:42 - middle position of the current list
35:44 - we repeat this process until the target
35:47 - element is found or until a sub list
35:50 - contains only one element
35:52 - if that single element sublist does not
35:55 - match the target element then we end the
35:57 - algorithm indicating that the element
35:59 - does not exist in the list
36:02 - okay so that is the magic behind how
36:04 - britney managed to solve the round much
36:06 - faster
36:07 - in the next video let's talk about the
36:09 - efficiency of binary search
36:12 - [Music]
36:16 - we have a vague understanding that
36:18 - britney's approach is better in most
36:19 - cases but just like with linear search
36:22 - it helps to visualize this
36:23 - much like we did with linear search when
36:26 - determining the efficiency of an
36:27 - algorithm and remember we're still only
36:29 - looking at efficiency in terms of time
36:32 - time complexity as it's called we always
36:35 - want to evaluate how the algorithm
36:37 - performs in the worst case scenario now
36:39 - you might be thinking well that doesn't
36:41 - seem fair because given a series of data
36:44 - if the target value we're searching for
36:46 - is somewhere near the front of the list
36:48 - then linear search may perform just as
36:50 - well if not slightly better than binary
36:52 - search and that is totally true
36:55 - remember a crucial part of learning
36:57 - algorithms is understanding what works
36:59 - better in a given context
37:01 - when measuring efficiency though we
37:03 - always use the worst case scenarios as a
37:06 - benchmark because remember it can never
37:08 - perform worse than the worst case
37:10 - let's plot these values on the graph we
37:12 - started earlier with the number of tris
37:15 - or the runtime of the algorithm on the y
37:17 - axis and the maximum number of values in
37:20 - the series or n on the horizontal axis
37:23 - to represent the worst case scenario we
37:25 - have two data points when n equals 10
37:28 - britney took four tries using binary
37:30 - search and when n equals 100 it took
37:33 - seven tries
37:34 - but even side by side these data points
37:36 - are sort of meaningless
37:38 - remember that while there is quite a
37:40 - difference between the run time of
37:41 - linear search and binary search at an n
37:44 - value of 100 for a computer that
37:46 - shouldn't matter
37:47 - what we should check out is how the
37:49 - algorithm performs at levels of n that
37:52 - might actually slow a computer down
37:54 - as n grows larger and larger how do
37:57 - these algorithms compare to one another
37:59 - let's add that to the graph
38:01 - okay now a picture starts to emerge
38:04 - as n gets really large the performance
38:07 - of these two algorithms differs
38:09 - significantly
38:10 - the difference is kind of staggering
38:12 - actually
38:13 - even with the simple game we saw that
38:14 - binary search was better but now we have
38:17 - a much more complete idea of how much
38:19 - better
38:20 - for example when n is 1000 the runtime
38:24 - of linear search measured by the number
38:26 - of operations or turns is also 1000.
38:29 - for binary search it takes just 10
38:32 - operations
38:33 - now let's look at what happens when we
38:35 - increase n by factor of 10
38:37 - at 10 000 linear search takes 10 000
38:40 - operations while binary search takes 14
38:43 - operations
38:44 - and increased by a factor of 10 in
38:46 - binary search only needs four more
38:48 - operations to find a value
38:51 - if we increase it again by a factor of
38:53 - 10 once more to an n value of 100 000
38:57 - binary search takes only 17 operations
39:00 - it is blazing fast
39:02 - what we've done here is plotted on a
39:04 - graph how the algorithm performs as the
39:07 - input set it is working on increases
39:10 - in other words we've plotted the growth
39:12 - rate of the algorithm also known as the
39:15 - order of growth
39:17 - different algorithms grow at different
39:19 - rates and by evaluating their growth
39:21 - rates we get a much better picture of
39:23 - their performance because we know how
39:26 - the algorithm will hold up as n grows
39:28 - larger
39:29 - this is so important in fact it is the
39:32 - standard way of evaluating an algorithm
39:34 - and brings us to a concept called big o
39:38 - you might have heard this word thrown
39:39 - about and if you found it confusing
39:41 - don't worry we've already built up a
39:43 - definition in the past few videos we
39:46 - just need to bring it all together
39:48 - let's start with a common statement
39:50 - you'll see in studies on algorithms
39:52 - big o is a theoretical definition of the
39:55 - complexity of an algorithm as a function
39:57 - of the size
39:59 - wow what a mouthful this sounds really
40:01 - intimidating but it's really not let's
40:03 - break it down
40:04 - big o is a notation used to describe
40:08 - complexity and what i mean by notation
40:10 - is that it simplifies everything we've
40:12 - talked about down into a single variable
40:16 - an example of complexity written in
40:18 - terms of big o looks like this
40:21 - as you can see it starts with an
40:22 - uppercase letter o that's why we call it
40:25 - big o it's literally a big o
40:28 - the o comes from order of magnitude of
40:31 - complexity so that's where we get the
40:32 - big o from now complexity here refers to
40:35 - the exercise we've been carrying out in
40:37 - measuring efficiency
40:39 - if it takes brittany 4 tries when n is
40:42 - 10
40:42 - how long does the algorithm take when n
40:45 - is 10 million
40:46 - when we use big o for this the variable
40:49 - used which we'll get to distills that
40:51 - information down so that by reading the
40:53 - variable you get a big picture view
40:55 - without having to run through data
40:56 - points and graphs just like we did
40:59 - it's important to remember that
41:01 - complexity is relative
41:03 - when we evaluate the complexity of the
41:05 - binary search algorithm we're doing it
41:07 - relative to other search algorithms not
41:10 - all algorithms
41:12 - bigo is a useful notation for
41:14 - understanding both time and space
41:15 - complexity but only when comparing
41:18 - amongst algorithms that solve the same
41:20 - problem
41:21 - the last bit in that definition of big o
41:23 - is a function of the size and all this
41:26 - means is that big o measures complexity
41:29 - as the input size grows because it's not
41:32 - important to understand how an algorithm
41:33 - performs in a single data set but in all
41:36 - possible data sets
41:38 - you will also see big o referred to as
41:40 - the upper bound of the algorithm and
41:43 - what that means is that big o measures
41:45 - how the algorithm performs in the worst
41:47 - case scenario
41:49 - so that's all big o is
41:51 - nothing special it's just a notation
41:53 - that condenses the data points and
41:55 - graphs that we've built up down to one
41:57 - variable okay so what do these variables
41:59 - look like
42:00 - for john's strategy linear search we say
42:03 - that it has a time complexity of big o
42:06 - and then n so that's again big o with an
42:08 - n inside parentheses
42:10 - for britney strategy binary search we
42:13 - say that it has a time complexity of big
42:15 - o of log n that's big o with something
42:17 - called a log and an n inside parentheses
42:20 - now don't worry if you don't understand
42:22 - that we'll go into that in more detail
42:23 - later on in the course
42:26 - each of these has a special meaning but
42:28 - it helps to work through all of them to
42:29 - get a big picture view so over the next
42:32 - few videos let's examine what are called
42:34 - common complexities or common values of
42:36 - big o that you will run into and should
42:38 - internalize
42:40 - in our discussions of complexity we made
42:42 - one assumption that the algorithm as a
42:44 - whole had a single measure of complexity
42:47 - that isn't true and we'll get at how we
42:49 - arrive at these measures for the entire
42:51 - algorithm at the end of this exercise
42:54 - but each step in the algorithm has its
42:56 - own space and time complexity
42:59 - in linear search for example there are
43:01 - multiple steps and the algorithm goes
43:03 - like this
43:04 - start at the beginning of the list or
43:06 - range of values compare the current
43:08 - value to the target if the current value
43:10 - is the target value that we're looking
43:12 - for we're done
43:13 - if it's not we'll move on sequentially
43:15 - to the next value in the list and repeat
43:17 - step two
43:19 - if we reach the end of the list then the
43:21 - target value is not in the list
43:23 - let's go back to step two for a second
43:25 - comparing the current value to the
43:27 - target
43:28 - does the size of the data set matter for
43:31 - this step
43:32 - when we're at step two we're already at
43:34 - that position in the list and all we're
43:36 - doing is reading the value to make a
43:38 - comparison reading the value is a single
43:41 - operation and if we were to plot it on a
43:44 - graph of runtime per operations against
43:46 - n it looks like this a straight line
43:49 - that takes constant time regardless of
43:52 - the size of n since this takes the same
43:55 - amount of time in any given case we say
43:58 - that the run time is constant time it
44:00 - doesn't change
44:02 - in big o notation we represent this as
44:04 - big o with a 1 inside parentheses now
44:08 - when i first started learning all this i
44:10 - was really confused as to how to read
44:12 - this even if it was in my own head
44:14 - should i say big o of one
44:16 - when you see this written you're going
44:18 - to read this as constant time so reading
44:20 - a value in a list is a constant time
44:23 - operation
44:24 - this is the most ideal case when it
44:26 - comes to run times because input size
44:28 - does not matter and we know that
44:30 - regardless of the size of n the
44:32 - algorithm runtime will remain the same
44:35 - the next step up in complexity so to
44:37 - speak is the situation we encountered
44:39 - with the binary search algorithm
44:42 - traditionally explaining the time
44:44 - complexity of binary search involves
44:46 - math i'm going to try to do it both with
44:48 - and without
44:50 - when we played the game using binary
44:52 - search we notice that with every turn we
44:55 - were able to discard half of the data
44:58 - but there's another pattern that emerges
45:00 - that we didn't explore
45:02 - let's say n equals 10. how long does it
45:04 - take to find an item at the 10th
45:06 - position of the list we can write this
45:08 - out so we go from 10 to 5 to 8 to 9 and
45:12 - then down to 10.
45:14 - here it takes us four tries to cut down
45:16 - the list to just one element and find
45:18 - the value we're looking for
45:20 - let's double the value of n to 20 and
45:23 - see how long it takes for us to find an
45:25 - item at the 20th position so we start at
45:27 - 20 and then we pick 10 from there we go
45:29 - to 15 17 19 and finally 20.
45:33 - so here it takes us five tries
45:36 - okay let's double it again so that n is
45:38 - 40 and we try to find the item in the
45:40 - 40th position
45:42 - so when we start at 40 the first
45:44 - midpoint we're going to pick is 20 from
45:46 - there we go to 30 then 35 37 39 and then
45:50 - 40.
45:51 - notice that every time we double the
45:54 - value of n the number of operations it
45:57 - takes to reduce the list down to a
45:59 - single element only increases by 1.
46:02 - there's a mathematical relationship to
46:04 - this pattern and it's called a logarithm
46:07 - of n
46:08 - you don't really have to know what
46:09 - logarithms truly are but i know that
46:11 - some of you like underlying explainers
46:13 - so i'll give you a quick one
46:15 - if you've taken algebra classes you may
46:18 - have learned about exponents here's a
46:20 - quick refresher
46:22 - 2 times 1 equals 2. now this can be
46:24 - written as 2 raised to the first power
46:27 - because it is our base case two times
46:30 - one is two
46:31 - now two times two is four this can be
46:33 - written as two raised to the second
46:35 - power because we're multiplying two
46:37 - twice first we multiply two times one
46:40 - then the result of that times 2.
46:43 - 2 times 2 times 2 is 8 and we can write
46:46 - this as 2 raised to the 3rd power
46:48 - because we're multiplying 2 3 times
46:51 - in 2 raised to 2 and 2 raised to 3 the 2
46:54 - and 3 there are called exponents and
46:57 - they define how the number grows
47:00 - with 2 raised to 3 we start with the
47:02 - base value and multiply itself 3 times
47:06 - the inverse of an exponent is called a
47:08 - logarithm so if i say log to the base 2
47:12 - of 8 equals 3 i'm basically saying the
47:15 - opposite of an exponent
47:16 - instead of saying how many times do i
47:18 - have to multiply this value i'm asking
47:20 - how many times do i have to divide 8 by
47:23 - two to get the value one
47:25 - this takes three operations
47:27 - what about the result of log to the base
47:29 - two of sixteen that evaluates to four
47:33 - so why does any of this matter
47:35 - notice that this is sort of how binary
47:37 - search works
47:38 - log to the base 2 of 16 is 4.
47:42 - if n was 16 how many triads does it take
47:45 - to get to that last element
47:47 - well we start in the middle at 8 that's
47:49 - too low so we move to 12 then we move to
47:51 - 14 then to 15 and then to 16 which is 5
47:55 - tries or log to the base 2 of 16 plus 1.
48:00 - in general for a given value of n the
48:03 - number of tries it takes to find the
48:05 - worst case scenario
48:06 - is log of n plus one
48:09 - and because this pattern is overall a
48:12 - logarithmic pattern we say that the
48:14 - runtime of such algorithms is
48:16 - logarithmic
48:17 - if we plot these data points on our
48:19 - graph a logarithmic runtime looks like
48:22 - this
48:23 - in big o notation we represent a
48:25 - logarithmic runtime as big o of log n
48:28 - which is written as big o with log n
48:31 - inside parentheses or even sometimes as
48:34 - l n n inside parentheses
48:37 - when you see this read it as logarithmic
48:39 - time
48:40 - as you can see on the graph as n grows
48:43 - really large the number of operations
48:46 - grows very slowly and eventually
48:48 - flattens out
48:50 - since this line is below the line for a
48:52 - linear runtime which we'll look at in a
48:54 - second you might often hear algorithms
48:57 - with logarithmic runtimes being called
48:59 - sublinear
49:00 - logarithmic or sub-linear runtimes are
49:03 - preferred to linear because they're more
49:05 - efficient but in practice linear search
49:08 - has its own set of advantages which
49:09 - we'll take a look at in the next video
49:13 - next up let's look at the situation we
49:15 - encountered with the linear search
49:16 - algorithm
49:17 - we saw that in the worst case scenario
49:20 - whatever the value of n was john took
49:22 - exactly that many tries to find the
49:24 - answer
49:25 - as in linear search when the number of
49:27 - operations to determine the result in
49:29 - the worst case scenario is at most the
49:32 - same as n
49:33 - we say that the algorithm runs in linear
49:36 - time
49:37 - we represent this as big o of n now you
49:40 - can read that as big o of n like i just
49:41 - said or you can say linear time which is
49:44 - more common
49:45 - when we put that up on a graph against
49:47 - constant time and logarithmic time we
49:50 - get a line that looks like this
49:52 - any algorithm that sequentially reads
49:55 - the input will have linear time
49:57 - so remember anytime you know a problem
50:00 - involves reading every item in a list
50:02 - that means a linear run time as you saw
50:05 - from the game we played brittany's
50:07 - strategy using binary search was clearly
50:09 - better and we can see that on the graph
50:12 - so if we had the option why would we use
50:14 - linear search which runs in linear time
50:17 - remember that binary search had a
50:19 - precondition the input set had to be
50:21 - sorted
50:22 - while we won't be looking at sorting
50:24 - algorithms in this course as you learn
50:26 - more about algorithms you'll find that
50:28 - sorting algorithms have varying
50:30 - complexities themselves just like search
50:32 - does so we have to do additional work
50:34 - prior to using binary search
50:37 - for this reason in practice linear
50:40 - search ends up being more performant up
50:42 - to a certain value of n because the
50:44 - combination of sorting first and then
50:47 - searching using binary search adds up
50:50 - the next common complexity you will hear
50:52 - about is when an algorithm runs in
50:54 - quadratic time if the word quadratic
50:56 - sounds familiar to you it's because you
50:59 - might have heard about it in math class
51:01 - quadratic is a word that means an
51:03 - operation raised to the second power or
51:06 - when something is squared
51:08 - let's say you and your friends are
51:10 - playing a tower defense game and to
51:12 - start it off you're going to draw a map
51:14 - of the terrain
51:16 - this map is going to be a grid and you
51:18 - pick a random number to determine how
51:20 - large this grid is let's set n the size
51:23 - of the grid to four
51:25 - next you need to come up with a list of
51:26 - coordinates so you can place towers and
51:28 - enemies and stuff on this map so how
51:31 - would we do this
51:32 - if we start out horizontally we'd have
51:34 - coordinate points that go 1 1 1 2 1 3
51:38 - and 1 4.
51:39 - then you go up one level vertically and
51:41 - we have points 2 1 2 2 2 3 and 2 4.
51:46 - go up one more and you have the points 3
51:48 - 1 3 2 3 3 and 3 4 and on that last row
51:52 - you have the points 4 1 4 2 4 3 and 4 4.
51:57 - notice that we have a pattern here
51:59 - for each row we take the value and then
52:01 - create a point by adding to that every
52:03 - column value
52:05 - the range of values go from 1 to the
52:07 - value of n
52:08 - so we can generally think of it this way
52:11 - for the range of values from 1 to n for
52:13 - each value in that range we create a
52:16 - point by combining that value with the
52:18 - range of values from 1 to n again
52:21 - doing it this way for each value in the
52:23 - range of 1 to n we create an n number of
52:27 - values and we end up with 16 points
52:30 - which is also n times n or n squared
52:34 - this is an algorithm with a quadratic
52:36 - runtime because for any given value of n
52:39 - we carry out n squared number of
52:41 - operations
52:42 - now i picked a relatively easy so to
52:45 - speak example here because in english at
52:47 - least we often denote map sizes by
52:50 - height times width so we would call this
52:52 - a 4 by 4 grid which is just another way
52:55 - of saying 4 squared or n squared
52:58 - in big o notation we would write this as
53:00 - big o of n squared or say that this is
53:03 - an algorithm with a quadratic runtime
53:06 - many search algorithms have a worst case
53:09 - quadratic runtime which you'll learn
53:10 - about soon
53:12 - now in addition to quadratic runtimes
53:14 - you may also run into cubic runtimes as
53:16 - you encounter different algorithms in
53:19 - such an algorithm for a given value of n
53:21 - the algorithm executes n raised to the
53:23 - third power number of operations
53:26 - these aren't as common as quadratic
53:28 - algorithms though so we won't look at
53:30 - any examples but i think it's worth
53:31 - mentioning
53:32 - thrown up on our graph quadratic and
53:35 - cubic runtimes look like this
53:37 - so this is starting to look pretty
53:39 - expensive computationally as they say we
53:42 - can see here that for small changes in n
53:44 - there's a pretty significant change in
53:46 - the number of operations that we need to
53:48 - carry out
53:50 - the next worst case runtime we're going
53:52 - to look at is one that's called
53:53 - quasi-linear and a sort of easier to
53:55 - understand for lack of better word by
53:58 - starting with the big o notation
54:00 - quasi-linear runtimes are written out as
54:03 - big o of n times log n
54:06 - we learned what log n was right a
54:08 - logarithmic runtime whereas n grew the
54:11 - number of operations only increased by a
54:13 - small factor with a quasi-linear runtime
54:16 - what we're saying is that for every
54:18 - value of n we're going to execute a log
54:21 - n number of operations hence the run
54:23 - time of n times log n
54:26 - so you saw earlier with the quadratic
54:28 - runtime that for each value of n we
54:30 - conducted n operations it's sort of the
54:32 - same in that as we go through the range
54:35 - of values in n we're executing login
54:37 - operations
54:38 - in comparison to other runtimes a
54:40 - quasi-linear algorithm has a runtime
54:43 - that lies somewhere between a linear
54:45 - runtime and a quadratic runtime
54:47 - so where would we expect to see this
54:50 - kind of runtime in practical use
54:52 - well sorting algorithms is one place you
54:54 - will definitely see it
54:56 - merge sort for example is a sorting
54:58 - algorithm that has a worst case runtime
55:00 - of big o of n log n
55:02 - let's take a look at a quick example
55:05 - let's say we start off with a list of
55:07 - numbers that looks like this and we need
55:08 - to sort it
55:10 - merge sort starts by splitting this list
55:12 - into two lists down the middle
55:15 - it then takes each sub list and splits
55:17 - that in half down the middle again
55:19 - it keeps doing this until we end up with
55:22 - a list of just a single number
55:24 - when we're down to single numbers we can
55:26 - do one sort operation and merge these
55:29 - sub-lists back in the opposite direction
55:32 - the first part of merge sort cuts those
55:34 - lists into sub-lists with half the
55:36 - numbers
55:38 - this is similar to binary search where
55:40 - each comparison operation cuts down the
55:43 - range to half the values
55:45 - you know the worst case runtime in
55:47 - binary search is log n so these
55:49 - splitting operations have the same
55:51 - runtime big o of log n or logarithmic
55:54 - but splitting into half isn't the only
55:56 - thing we need to do with merge sort we
55:58 - also need to carry out comparison
56:00 - operations so we can sort those values
56:02 - and if you look at each step of this
56:04 - algorithm we carry out an n number of
56:07 - comparison operations and that brings
56:09 - the worst case runtime of this algorithm
56:12 - to n times log n also known as quasi
56:15 - linear don't worry if you didn't
56:17 - understand how merge sort works that
56:19 - wasn't the point of this demonstration
56:21 - we will be covering merge sorts soon in
56:23 - a future course
56:25 - the run times we've looked at so far are
56:27 - all called polynomial runtimes an
56:30 - algorithm is considered to have a
56:31 - polynomial runtime if for a given value
56:34 - of n its worst case runtime is in the
56:37 - form of n raised to the k power where k
56:40 - just means some value so it could be n
56:42 - squared where k equals 2 for a quadratic
56:44 - runtime n cubed for a cubic runtime and
56:47 - so on
56:48 - all of those are in the form of n raised
56:50 - to some power
56:52 - anything that is bounded by this and
56:54 - what i mean by that is if we had a
56:56 - hypothetical line on our graph of n
56:58 - raised to the k power anything that
57:00 - falls under this graph is considered to
57:03 - have a polynomial runtime
57:05 - algorithms with an upper bound or a
57:07 - runtime with a big o value that is
57:09 - polynomial are considered efficient
57:12 - algorithms and are likely to be used in
57:14 - practice
57:15 - now the next class of runtimes that
57:17 - we're going to look at are a runtimes
57:19 - that we don't consider efficient and
57:22 - these are called exponential runtimes
57:25 - with these runtimes as n increases
57:27 - slightly the number of operations
57:29 - increases exponentially and as we'll see
57:32 - in a second these algorithms are far too
57:34 - expensive to be used
57:36 - an exponential runtime is an algorithm
57:39 - with a big o value of some number raised
57:41 - to the nth power
57:43 - imagine that you wanted to break into a
57:45 - locker that had a padlock on it let's
57:47 - assume you forgot your code
57:49 - this lock takes a two digit code and the
57:52 - digit for the code ranges from zero to
57:54 - nine
57:55 - you start by setting the dials to zero
57:57 - and then with the first dial remaining
57:59 - on zero you change the second dial to
58:02 - one and try and open it if it doesn't
58:04 - work you set it to two then try again
58:06 - you would keep doing this and if you
58:08 - still haven't succeeded with the second
58:10 - dial set to 9 then you go back to that
58:12 - first dial set it to 1 and start the
58:15 - second dial over
58:16 - the range of values you'd have to go
58:18 - through is 0 0 to 9 9 which is 100
58:22 - values
58:23 - this can be generalized as 10 to the
58:26 - second power since there are 10 values
58:28 - on each dial raised to two dials
58:31 - searching through each individual value
58:33 - until you stumble on the right one is a
58:35 - strategy called brute force and brute
58:38 - force algorithms have exponential run
58:40 - times
58:41 - here there are two dials so n is 2 and
58:44 - each dial has 10 values so again we can
58:46 - generalize this algorithm as 10 raised
58:48 - to n where n represents the number of
58:51 - dials
58:52 - the reason that this algorithm is so
58:54 - inefficient is because with just one
58:56 - more dial on the lock the number of
58:58 - operations increases significantly
59:01 - with three dials the number of
59:03 - combinations in the worst case scenario
59:05 - where the correct code is the last digit
59:07 - in the range is 10 raised to 3 or 1 000
59:10 - values
59:11 - with an additional wheel it becomes 10
59:13 - raised to 4 or 10 000 values
59:16 - as n increases the number of operations
59:18 - increases exponentially to a point where
59:21 - it's unsolvable in a realistic amount of
59:23 - time
59:24 - now you might think well any computer
59:27 - can crack a four digit numerical lock
59:29 - and that's true because n here is
59:31 - sufficiently small but this is the same
59:34 - principle that we use for passwords
59:36 - in a typical password field implemented
59:39 - well users are allowed to use letters of
59:41 - the english alphabet so up to 26
59:43 - characters numbers from 0 to 9 and a set
59:46 - of special characters of which there can
59:48 - be around 33
59:50 - so typically that means each character
59:52 - in a password can be one out of 69
59:55 - values
59:56 - this means that for a one character
59:58 - password it takes 69 to the nth power so
60:02 - 1 which equals 69 operations in the
60:05 - worst case scenario to figure out the
60:07 - password
60:08 - just increasing n to 2 increases the
60:11 - number of operations needed to guess the
60:13 - password to 69 squared or
60:17 - 4761 operations
60:19 - now usually on a secure website there
60:22 - isn't really a limit but in general
60:24 - passwords are limited to around 20
60:26 - characters in length
60:28 - with each character being a possible 69
60:30 - values and there being 20 characters the
60:33 - number of operations needed to guess the
60:35 - password in the worst case scenario is
60:38 - 69 raised to the 20th power or
60:41 - approximately 6 followed by 36 zeros
60:44 - number of operations
60:46 - an intel cpu with five cores can carry
60:50 - out roughly about 65 000 million
60:52 - instructions per second that's a funny
60:54 - number i know to crack our 20-digit
60:57 - passcode in this very simplistic model
61:00 - it would take this intel cpu
61:02 - to race to 20th power years to brute
61:06 - force the password
61:08 - so while this algorithm would eventually
61:10 - produce a result it is so inefficient
61:12 - that it's pointless
61:14 - this is one of the reasons why people
61:15 - recommend you have longer passwords
61:18 - since brute forcing is exponential in
61:20 - the worst case each character you add
61:23 - increases the number of combinations by
61:24 - an exponent
61:26 - the next class of exponential algorithms
61:29 - is best highlighted by a popular problem
61:31 - known as the traveling salesman
61:34 - the problem statement goes like this
61:36 - given a list of cities and the distance
61:38 - between each pair of cities what is the
61:41 - shortest possible route that visits each
61:43 - city and then returns to the origin city
61:46 - this seems like a simple question but
61:48 - let's start with a simple case three
61:50 - cities a b and c
61:53 - to figure out what the shortest route is
61:55 - we need to come up with all the possible
61:57 - routes
61:58 - with three cities we have six routes in
62:00 - theory at least some of these routes can
62:03 - be discarded because abc is the same as
62:05 - c b a but in the opposite direction
62:08 - but as we do know sometimes going from a
62:11 - to c through b may go through a
62:12 - different route than c to a through b so
62:15 - we'll stick to the six routes and from
62:17 - there we could determine the shortest no
62:19 - big deal
62:20 - now if we increase this to four cities
62:22 - we jump to 24 combinations
62:24 - the mathematical relationship that
62:26 - defines this is called a factorial and
62:29 - is written out as n followed by an
62:31 - exclamation point
62:33 - factorials are basically n times n minus
62:36 - one repeated until you reach the number
62:39 - one so for example the factorial of
62:41 - three is three times two times one which
62:44 - is six which is the number of
62:46 - combinations we came up with for three
62:47 - cities
62:49 - the factorial of four is four times
62:51 - three times two times one or 24 which is
62:54 - the number of combinations we arrived at
62:56 - with four cities
62:58 - in solving the traveling salesman
63:00 - problem the most efficient algorithm
63:03 - will have a factorial runtime or a
63:05 - combinatorial runtime as it's also
63:07 - called
63:09 - at low values of n algorithms with a
63:11 - factorial runtime may be used but with
63:13 - an n value of say 200 it would take
63:16 - longer than humans have been alive to
63:18 - solve the problem
63:19 - for sake of completeness let's plot a
63:22 - combinatorial runtime on our graph so
63:24 - that we can compare
63:26 - an algorithm such as one that solves the
63:28 - traveling salesman problem as a worst
63:30 - case run time of big o of n factorial
63:34 - studying exponential runtimes like this
63:36 - are useful for two reasons
63:38 - first in studying how to make such
63:40 - algorithms efficient we develop
63:42 - strategies that are useful across the
63:44 - board and can potentially be used to
63:46 - make existing algorithms even more
63:48 - efficient
63:50 - second it's important to be aware of
63:51 - problems that take a long time to solve
63:54 - knowing right off the bat that a problem
63:56 - is somewhat unsolvable in a realistic
63:58 - time means you can focus your efforts on
64:01 - other aspects of the problem
64:03 - as beginners though we're going to steer
64:05 - clear of all this and focus our efforts
64:07 - on algorithms with polynomial runtimes
64:09 - since we're much more likely to work
64:11 - with and learn about such algorithms
64:14 - now that we know some of the common
64:15 - complexities in the next video let's
64:18 - talk about how we determine the
64:19 - complexity of an algorithm because there
64:22 - are some nuances
64:24 - over the last few videos we took a look
64:26 - at common complexities that we would
64:27 - encounter in studying algorithms but the
64:30 - question remains how do we determine
64:32 - what the worst case complexity of an
64:34 - algorithm is
64:36 - earlier i mentioned that even though we
64:38 - say that an algorithm has a particular
64:40 - upper bound or worst case runtime each
64:43 - step in a given algorithm can have
64:45 - different run times
64:47 - let's bring up the steps for binary
64:48 - search again
64:50 - assuming the list is sorted the first
64:52 - step is to determine the middle position
64:54 - of the list
64:56 - in general this is going to be a
64:57 - constant time operation
64:59 - many programming languages hold on to
65:02 - information about the size of the list
65:04 - so we don't actually need to walk
65:06 - through the list to determine the size
65:08 - now if we didn't have information about
65:10 - the size of the list we would need to
65:12 - walk through counting each item one by
65:15 - one until we reached the end of the list
65:18 - and this is a linear time operation but
65:21 - realistically this is a big o of 1 or
65:24 - constant time
65:26 - step 2 is to compare the element in the
65:28 - middle position to the target element
65:31 - we can assume that in most modern
65:32 - programming languages this is also a
65:34 - constant time operation because the
65:37 - documentation for the language tells us
65:39 - it is
65:40 - step 3 is our success case and the
65:42 - algorithm ends
65:44 - this is our best case and so far we have
65:47 - only incurred two constant time
65:49 - operations
65:50 - so we would say that the best case run
65:52 - time of binary search is constant time
65:55 - which is actually true
65:56 - but remember that best case is not a
65:58 - useful metric
66:00 - step 4 if we don't match is splitting
66:02 - the list into sub-lists
66:04 - assuming the worst case scenario the
66:07 - algorithm would keep splitting into
66:08 - sub-lists until a single element list is
66:11 - reached with the value that we're
66:13 - searching for
66:14 - the run time for this step is
66:16 - logarithmic since we discard half the
66:18 - values each time
66:20 - so in our algorithm we have a couple
66:22 - steps that are constant time and one
66:25 - step that is logarithmic overall
66:28 - when evaluating the run time for an
66:29 - algorithm we say that the algorithm has
66:32 - as its upper bound the same runtime as
66:35 - the least efficient step in the
66:37 - algorithm
66:38 - think of it this way let's say you're
66:40 - participating in a triathlon which is a
66:43 - race that has a swimming running and a
66:45 - cycling component
66:47 - you could be a phenomenal swimmer and a
66:49 - really good cyclist but you're a pretty
66:51 - terrible runner
66:52 - no matter how fast you are at swimming
66:54 - or cycling your overall race time is
66:57 - going to be impacted the most by your
66:59 - running race time because that's the
67:01 - part that takes you the longest
67:03 - if you take an hour 30 to finish the
67:06 - running component 55 minutes to swim and
67:08 - 38 minutes to bike it won't matter if
67:11 - you can fine tune your swimming
67:13 - technique down to finish in 48 minutes
67:15 - and your cycle time to 35 because you're
67:18 - still bounded at the top by your running
67:20 - time which is close to almost double
67:22 - your bike time
67:24 - similarly with the binary search
67:26 - algorithm it doesn't matter how fast we
67:28 - make the other steps they're already as
67:30 - fast as they can be
67:32 - in the worst case scenario the splitting
67:34 - of the list down to a single element
67:36 - list is what will impact the overall
67:38 - running time of your algorithm
67:40 - this is why we say that the time
67:42 - complexity or run time of the algorithm
67:44 - in the worst case is big o of log n or
67:47 - logarithmic
67:48 - as i alluded to though your algorithm
67:50 - may hit a best case runtime and in
67:53 - between the two best and worst case have
67:55 - an average run time as well
67:57 - this is important to understand because
67:59 - algorithms don't always hit their worst
68:01 - case but this is getting a bit too
68:02 - complex for us for now we can safely
68:05 - ignore average case performances and
68:07 - focus only on the worst case in the
68:10 - future if you decide to stick around
68:11 - we'll circle back and talk about this
68:13 - more
68:15 - now that you know about algorithms
68:16 - complexities and big o let's take a
68:19 - break from all of that and write code in
68:21 - the next video
68:23 - [Music]
68:28 - so far we've spent a lot of time in
68:29 - theory and while these things are all
68:31 - important things to know you get a much
68:33 - better understanding of how algorithms
68:35 - work when you start writing some code as
68:38 - i mentioned earlier we're going to be
68:39 - writing python code in this and all
68:42 - subsequent algorithm courses
68:44 - if you do have programming experience
68:46 - but in another language check the notes
68:48 - section of this video for an
68:50 - implementation in your language
68:52 - if you don't have any experience i'll
68:54 - try my best explain as we go along
68:57 - on the video you're watching right now
68:59 - you should see a launch workspaces
69:01 - button
69:02 - we're going to use a treehouse coding
69:04 - environment call workspaces to write all
69:06 - of our code
69:08 - if you're familiar with using python in
69:10 - a local environment then feel free to
69:12 - keep doing so workspaces is an
69:14 - in-browser coding environment and will
69:16 - take care of all the setup and
69:18 - installation so you can focus on just
69:21 - writing and evaluating code workspaces
69:24 - is quite straightforward to use on the
69:26 - left here we have a file navigator pane
69:29 - which is currently empty since we
69:31 - haven't created a new file
69:33 - on the top we have an editor where we
69:35 - write all our code and then below that
69:37 - we have a terminal or a command line
69:39 - prompt where we can execute the scripts
69:41 - that we write let's add a new file here
69:43 - so at the top in the editor area we're
69:45 - going to go to file new file and we'll
69:48 - name this linear
69:50 - underscore search
69:52 - dot pi
69:54 - in here we're going to define our linear
69:57 - search algorithm as a standalone
69:59 - function
70:00 - we start with the keyword def which
70:03 - defines a function or a block of code
70:06 - and then we give it the name linear
70:08 - underscore search
70:10 - this function will accept two arguments
70:13 - first the list we're searching through
70:15 - and then the target value we're looking
70:18 - for both of these arguments are enclosed
70:20 - in a set of parentheses and there's no
70:22 - space between the name of the function
70:24 - and the arguments
70:26 - after that we have a colon
70:28 - now there might be a bit of confusion
70:30 - here since we already have this target
70:32 - value what are we searching for unlike
70:35 - the game we played at the beginning
70:37 - where john's job was to find the value
70:39 - in a true implementation of linear
70:42 - search we're looking for the position in
70:44 - the list where the value exists
70:46 - if the target is in the list then we
70:48 - return its position
70:50 - and since this is a list that position
70:52 - is going to be denoted by an index value
70:55 - now if the target is not found we're
70:57 - going to return none the choice of what
70:59 - to return in the failure case may be
71:01 - different in other implementations of
71:03 - linear search
71:04 - you can return -1 since that isn't
71:07 - typically an index value
71:09 - you can also raise an exception which is
71:11 - python speak for indicating an error
71:13 - occurred
71:14 - now i think for us the most
71:15 - straightforward value we can return here
71:17 - is none now let's add a comment to
71:19 - clarify this so hit enter to go to the
71:21 - next line
71:22 - and then we're going to add
71:24 - three
71:26 - single quotes
71:27 - and then below that on the next line
71:29 - we'll say returns
71:31 - the position or the index
71:33 - position
71:35 - of the target
71:36 - if found
71:38 - else returns none
71:40 - and then on the next line we'll close
71:42 - off those three quotes
71:43 - this is called a doc string and is a
71:46 - python convention for documenting your
71:48 - code the linear search algorithm is a
71:50 - sequential algorithm that compares each
71:53 - item in the list until the target is
71:55 - found
71:56 - to iterate or loop or walk through our
71:59 - list sequentially we're going to use a
72:02 - for loop
72:03 - now typically when iterating over a list
72:05 - in python we would use a loop like this
72:08 - we'd say for item in list
72:11 - this assigns the value at each index
72:13 - position to that local variable item
72:16 - we don't want this though since we
72:18 - primarily care about the index position
72:21 - instead we're going to use the range
72:23 - function in python to create a range of
72:26 - values that start at 0 and end at the
72:29 - number of items in the list
72:31 - so we'll say 4 i i stands for index here
72:35 - in range
72:36 - starting at 0 and going all the way up
72:38 - to the length of the list
72:42 - we can get the number of items in the
72:44 - list using the len function
72:46 - now going back to our talk on complexity
72:48 - and how individual steps in an algorithm
72:51 - can have its own run times this is a
72:53 - line of code that we would have to be
72:55 - careful about
72:56 - python keeps track of the length of a
72:58 - list so this function call here len list
73:02 - is a constant time operation now if this
73:05 - were a naive implementation let's say we
73:07 - wrote the implementation of the list
73:10 - and we iterate over the list every time
73:12 - we call this length function then we've
73:15 - already incurred a linear cost
73:17 - okay so once we have a range of values
73:19 - that represent index positions in this
73:21 - list we're going to iterate over that
73:23 - using the for loop and assign each index
73:26 - value to this local variable i using
73:29 - this index value we can obtain the item
73:31 - at that position using subscript
73:33 - notation on the list
73:35 - now this is also a constant time
73:37 - operation because the language says so
73:40 - so we'll do if list so once we have this
73:43 - value which we'll get by using subscript
73:45 - notation so we'll say list i
73:47 - once we have this value we'll check if
73:49 - it matches the target so if the value at
73:52 - i
73:53 - equals target
73:55 - well if it does then we'll return that
73:57 - index value because we want the position
74:00 - and once we hit this return statement
74:02 - we're going to terminate our function if
74:04 - the entire for loop is executed and we
74:06 - don't hit this return statement then the
74:08 - target does not exist in the list so at
74:11 - the bottom here we'll say return none
74:14 - even though all the individual
74:16 - operations in our algorithm run in
74:18 - constant time
74:19 - in the worst case scenario this for loop
74:22 - here will have to go through the entire
74:24 - range of values and read every single
74:26 - element in the list
74:28 - therefore giving the algorithm a big o
74:30 - value of n or running in linear time now
74:34 - if you've written code before you've
74:35 - definitely written code like this a
74:37 - number of times and i bet you didn't
74:38 - know but all along you are implementing
74:40 - what is essentially a well-known
74:42 - algorithm
74:43 - so i hope this goes to show you that
74:45 - algorithms are pretty approachable topic
74:48 - like everything else this does get
74:49 - advanced but as long as you take things
74:51 - slow there's no reason for it to be
74:53 - impossible remember that not any block
74:55 - of code counts as an algorithm to be a
74:58 - proper implementation of linear search
75:01 - this block of code must return a value
75:04 - must complete execution in a finite
75:06 - amount of time and must output the same
75:09 - result every time for a given input set
75:11 - so let's verify this with a small test
75:15 - let's write a function called verify
75:18 - that accepts an index value
75:20 - if the value is not none it prints the
75:23 - index position if it is none it informs
75:25 - us that the target was not found in the
75:27 - list so def verify
75:30 - and this is going to take an index value
75:33 - and we'll say if index is not none
75:37 - then we'll print
75:40 - target
75:42 - found at index
75:47 - oops that's a colon here
75:50 - index else
75:54 - that needs to go back
75:56 - there we go
75:57 - else we'll say target
76:01 - not found in list
76:04 - okay using this function let's define a
76:06 - range of numbers now so this will be a
76:09 - list numbers
76:11 - and we'll just go from 1 to
76:14 - let's say 10.
76:20 - now if you've written python code before
76:22 - you know that i can use a list
76:24 - comprehension to make this easier but
76:26 - we'll keep things simple
76:28 - we can now use our linear search
76:29 - function to search for the position of a
76:32 - target value in this list so we can say
76:34 - result
76:35 - equal
76:36 - linear underscore search
76:39 - and we're going to pass in the numbers
76:40 - list that's the one we're searching
76:42 - through and we want to look for the
76:44 - position where the value 12 exists
76:47 - and then we'll verify
76:48 - this result
76:50 - if our algorithm works correctly the
76:53 - verify function should inform us that
76:55 - the target did not exist so make sure
76:57 - you save the file which you can do by
76:58 - going up to file and save or hitting
77:00 - command s
77:02 - and then below in the terminal
77:06 - you're going to type out python
77:08 - linear search or you can hit tab and it
77:11 - should auto complete linear search dot
77:13 - pi
77:14 - as you can see correct the target was
77:16 - not found in the list so the output of
77:18 - our script is what we expect
77:20 - for our second test let's search for the
77:22 - value 6 in the list so you can copy this
77:26 - command c to copy and then paste it
77:28 - again and we'll just change 12 here to 6
77:31 - and then come back down to the terminal
77:33 - hit the up arrow to execute the same
77:35 - command again and hit enter you'll
77:37 - notice that i forgot to hit save so it
77:39 - did not account for that new change
77:41 - we'll try that again
77:42 - and there you'll see that if it works
77:45 - correctly which it did the index should
77:48 - be number five run the program on your
77:50 - end and make sure everything works as
77:52 - expected
77:53 - our algorithm returned a result in each
77:55 - case it executed in a finite time and
77:58 - the results were the ones we expect in
78:00 - the next video let's tackle binary
78:02 - search
78:03 - in the last video we left off with an
78:05 - implementation of linear search
78:08 - let's do the same for binary search so
78:09 - that we get an understanding of how this
78:11 - is represented in code
78:13 - so we'll do this in a new file back to
78:15 - file new file
78:17 - and we'll name this one binary
78:20 - search
78:22 - dot
78:23 - py
78:24 - like before we're going to start with a
78:25 - function named binary search so we'll
78:27 - say def
78:28 - binary underscore search
78:31 - that takes a list and a target
78:34 - if you remember binary search works by
78:36 - breaking the array or list down into
78:39 - smaller sets until we find the value
78:41 - we're looking for
78:43 - we need a way to keep track of the
78:45 - position of the list that we're working
78:47 - with so let's create two variables first
78:50 - and last to point to the beginning and
78:52 - end of the array so first equal
78:56 - zero now if you're new to programming
78:59 - list positions are represented by index
79:01 - values that start at zero instead of one
79:04 - so here we're setting first to zero to
79:07 - point to the first element in the list
79:09 - last is going to point to the last
79:11 - element in the list so we'll say last
79:14 - equal
79:15 - len list
79:17 - minus one now this may be confusing to
79:20 - you so a quick sidebar to explain what's
79:22 - going on
79:23 - let's say we have a list containing 5
79:25 - elements if we called len on that list
79:28 - we should get 5 back because there are 5
79:30 - elements
79:31 - but remember that because the position
79:33 - numbers start at 0 the last value is not
79:36 - at position 5 but at 4. in nearly all
79:39 - programming languages getting the
79:41 - position of the last element in the list
79:43 - is obtained by determining the length of
79:46 - the list and deducting 1 which is what
79:48 - we're doing
79:49 - okay so we know what the first and last
79:52 - positions are when we start the
79:53 - algorithm
79:54 - for our next line of code we're going to
79:56 - create a while loop
79:58 - a while loop takes a condition and keeps
80:00 - executing the code inside the loop until
80:03 - the condition evaluates to false
80:06 - for our condition we're going to say to
80:08 - keep executing this loop until the value
80:11 - of first is less than or equal to the
80:14 - value of last
80:16 - so while first less than or equal to
80:19 - last
80:21 - well why you ask why is this our
80:22 - condition well let's work through this
80:24 - implementation and then a visualization
80:27 - should help
80:28 - inside the while loop we're going to
80:30 - calculate the midpoint of our list since
80:32 - that's the first step of binary search
80:35 - midpoint equal
80:37 - so we'll say first
80:39 - plus last
80:41 - and then we'll use the floor division
80:43 - double slash here
80:44 - divided by two
80:46 - now the two forward slashes here are
80:48 - what python calls a floor division
80:50 - operator what it does is it rounds down
80:52 - to the nearest whole number so if we
80:55 - have an eight element array first is
80:57 - zero last is 7 if we divided 0 plus 7
81:02 - which is 7 by 2 we would get 3.5 now 3.5
81:06 - is not a valid index position so we
81:08 - round that down to 3 using the floor
81:10 - division operator okay so now we have a
81:13 - midpoint the next step of binary search
81:15 - is to evaluate whether the value at this
81:18 - midpoint is the same as the target we're
81:20 - looking for so say if list
81:23 - value at midpoint
81:25 - equals the target
81:28 - well if it is then we'll go ahead and
81:30 - return the midpoint
81:32 - so we'll say return
81:33 - midpoint
81:34 - the return statement terminates our
81:36 - algorithm and over here we're done this
81:39 - is our best case scenario
81:42 - next we'll say else if
81:44 - list at midpoint
81:48 - or value at midpoint is less than the
81:50 - target now here if the value is less the
81:53 - value at midpoint is less than the
81:55 - target then we don't care about any of
81:57 - the values lower than the midpoint so we
82:00 - redefine first
82:02 - to point to the value after the midpoint
82:05 - so we'll say midpoint plus 1.
82:07 - now if the value at the midpoint is
82:10 - greater than the target then we can
82:12 - discard the values after the midpoint
82:14 - and redefine last to point to the value
82:17 - prior to the midpoint so we'll say else
82:22 - last equal midpoint
82:25 - minus 1.
82:26 - let's visualize this we're going to
82:28 - start with a list of nine integers
82:31 - to make this easier to understand let's
82:33 - specify these integers to be of the same
82:35 - value as its index position so we have a
82:38 - range of values from 0 to 8.
82:40 - our target is the worst case scenario
82:42 - we're looking for the position of the
82:44 - value 8. at the start our algorithm sets
82:47 - first to point to the index 0 and last
82:51 - to point to the length of the list minus
82:53 - 1 which is 8.
82:55 - next we hit our while loop the logic of
82:57 - this loop is going to be executed as
82:59 - long as the value of first is not
83:01 - greater than the value of last or as
83:04 - we've defined it we're going to keep
83:06 - executing the contents of the loop as
83:08 - long as first is less than or equal to
83:10 - last
83:12 - on the first pass this is true so we
83:14 - enter the body of the loop
83:16 - the midpoint is first plus last divided
83:18 - by two and rounded down so we get a nice
83:21 - even four the value at this position is
83:24 - four now this is not equal to the target
83:26 - so we move to the first else if
83:29 - four is less than eight so now we
83:32 - redefine first to point to midpoint plus
83:34 - one which is five
83:37 - first is still less than last so we run
83:40 - through the body of the loop again the
83:42 - midpoint is now six
83:44 - six is less than eight so we move first
83:47 - to point to seven
83:49 - seven is still less than or equal to
83:51 - eight so we go for another iteration of
83:53 - the loop
83:54 - the midpoint is seven oddly enough and
83:57 - seven is still less than the target so
83:59 - we move first to point to eight first is
84:02 - equal to last now but our condition says
84:05 - keep the loop going as long as first is
84:08 - less than or equal to last so this is
84:11 - our final time through the loop
84:13 - the midpoint is now 8 which makes the
84:16 - value at the midpoint equal to the
84:18 - target and we finally exit our algorithm
84:21 - and return the position of the target
84:23 - now what if we had executed all this
84:25 - code and never hit a case where midpoint
84:28 - equal the target well that would mean
84:30 - the list did not contain the target
84:32 - value so after the while loop at the
84:35 - bottom
84:36 - will return
84:37 - none
84:38 - we have several operations that make up
84:41 - our binary search algorithm so let's
84:43 - look at the runtime of each step we
84:45 - start by assigning values to first and
84:48 - last
84:49 - the value assigned to last involves a
84:51 - call to the len function to get the size
84:54 - of the list but we already know this is
84:56 - a constant time operation in python so
84:59 - both of these operations run in constant
85:01 - time
85:02 - inside the loop we have another value
85:05 - assignment and this is a simple division
85:07 - operation so again the runtime is
85:09 - constant
85:10 - in the next line of code we're reading a
85:12 - value from the list and comparing the
85:14 - midpoint to the target both of these
85:17 - again are constant time operations the
85:20 - remainder of the code is just a series
85:22 - of comparisons and value assignments and
85:25 - we know that these are all constant time
85:27 - operations as well
85:28 - so if all we have are a series of
85:30 - constant time operations why does this
85:33 - algorithm have in the worst case a
85:35 - logarithmic runtime
85:37 - it's hard to evaluate by just looking at
85:39 - the code but the while loop is what
85:41 - causes the run time to grow
85:44 - even though all we're doing is a
85:45 - comparison operation by redefining first
85:48 - and last
85:50 - over here or rather in the last two
85:52 - steps over here we're asking the
85:54 - algorithm to run as many times as it
85:57 - needs until first is equal or greater
86:00 - than last
86:01 - now each time the loop does this the
86:03 - size of the data set the size of the
86:05 - list grows smaller by a certain factor
86:08 - until it approaches a single element
86:11 - which is what results in the logarithmic
86:13 - runtime
86:14 - okay just like with linear search let's
86:17 - test that our algorithm works so we'll
86:19 - go back to linear search.hi
86:21 - and we're going to copy paste
86:23 - so command c to copy if you're on a mac
86:26 - then go back to binary search and at the
86:28 - bottom
86:30 - oops
86:31 - we're going to paste in that verify
86:33 - function
86:34 - okay we'll also go back and grab this
86:36 - numbers
86:38 - you know what let's go ahead and copy
86:39 - all all of these things so numbers and
86:41 - the two verify cases we'll paste that in
86:45 - as well
86:46 - and the only thing we need to change
86:47 - here is instead of calling linear search
86:49 - this is going to call binary search
86:54 - okay we'll hit command s to save the
86:56 - file and then i'm going to drag up my
86:59 - console and we'll run python binary
87:02 - search dot
87:03 - and hit enter and you'll see like just
87:05 - like before we get the same results back
87:08 - now note that an extremely important
87:10 - distinction needs to be made here
87:12 - the numbers list that we've defined
87:15 - for our test cases
87:17 - right here
87:18 - has to be sorted the basic logic of
87:21 - binary search relies on the fact that if
87:23 - the target is greater than the midpoint
87:26 - then our potential values lie to the
87:28 - left or vice versa since the values are
87:31 - sorted in ascending order if the values
87:34 - are unsorted our implementation of
87:36 - binary search may return none even if
87:39 - the value exists in the list
87:42 - and just like that you've written code
87:44 - to implement two search algorithms how
87:46 - fun was that
87:47 - hopefully this course has shown you that
87:49 - it isn't a topic to be afraid of and
87:51 - that algorithms like any other topic
87:54 - with code can be broken down and
87:55 - understood piece by piece
87:58 - now we have a working implementation of
88:00 - binary search but there's actually more
88:02 - than one way to write it so in the next
88:04 - video let's write a second version
88:07 - i'm going to create a new file
88:09 - as always file new file
88:12 - and we'll name this recursive
88:15 - underscore binary underscore search dot
88:18 - p
88:20 - y
88:21 - okay so we're going to add our new
88:23 - implementation here so that we don't get
88:25 - rid of that first implementation we
88:27 - wrote let's call this new function
88:29 - recursive binary search
88:31 - unlike our previous implementation this
88:33 - version is going to behave slightly
88:35 - differently in that it won't return the
88:38 - index value of the target element if it
88:40 - exists
88:41 - instead it will just return a true value
88:43 - if it exists and a false if it doesn't
88:46 - so recursive
88:49 - underscore binary underscore search
88:52 - and like before this is going to take a
88:54 - list it accepts a list and a target to
88:57 - look for in that list
88:59 - we'll start the body of the function by
89:01 - considering what happens if an empty
89:04 - list is passed in in that case we would
89:06 - return false so i would say if the
89:08 - length of the list which is one way to
89:10 - figure out if it's empty if it's equal
89:12 - to zero
89:14 - then we'll return false
89:16 - now you might be thinking that in the
89:18 - previous version of binary search we
89:20 - didn't care if the list was empty well
89:22 - we actually did but in a roundabout sort
89:25 - of way so in the previous version of
89:27 - binary search our function had a loop
89:30 - and that loop condition was true when
89:33 - first was less than or equal to last so
89:36 - as long as it's less than or equal to
89:38 - last we continue the loop
89:40 - now if we have an empty list then first
89:42 - is greater than last and the loop would
89:45 - never execute and we return none at the
89:47 - bottom
89:48 - so this is the same logic we're
89:50 - implementing here we're just doing it in
89:51 - a slightly different way if the list is
89:54 - not empty we'll implement an else clause
89:57 - now here we'll calculate the midpoint
90:01 - by dividing the length of the list by 2
90:04 - and rounding down
90:05 - again there's no use of first and last
90:08 - here so we'll say length of list
90:11 - and then using the floor division
90:13 - operator we'll divide that by 2.
90:15 - if the value at the midpoint which we'll
90:18 - check by saying if list
90:20 - using
90:21 - subscript notation we'll say midpoint as
90:24 - the index now if this value at the
90:27 - midpoint is the same as the target
90:30 - then we'll go ahead and return true
90:34 - so far this is more or less the same
90:37 - except for the value that we're
90:39 - returning
90:40 - let me actually get rid of all that
90:45 - okay
90:46 - all right so if this isn't the case
90:48 - let's implement an else clause now here
90:50 - we have two situations so first if the
90:53 - value at the midpoint is less than the
90:55 - target so if
90:58 - value at midpoint
91:01 - is less than the target
91:04 - then we're going to do something new
91:06 - we're going to call this function again
91:09 - this recursive binary search function
91:12 - that we're in the process of defining
91:14 - we're going to call that again and we're
91:16 - going to give it the portion of the list
91:18 - that we want to focus on in the previous
91:20 - version of binary search we moved the
91:23 - first value to point to the value after
91:26 - the midpoint
91:27 - now here we're going to create a new
91:29 - list using what is called a slice
91:31 - operation and create a sub list that
91:34 - starts at midpoint plus 1 and goes all
91:37 - the way to the end
91:39 - we're going to specify the same target
91:41 - as a search target and when this
91:43 - function call is done we'll return the
91:45 - value so we'll say return the return is
91:48 - important
91:50 - then we'll call this function again
91:51 - recursive
91:53 - binary search
91:55 - and this function takes a list and here
91:57 - we're going to use that subscript
91:59 - notation to perform a slice operation by
92:02 - using two indexes a start and an end so
92:05 - we'll say our new list that we're
92:06 - passing in needs to start at midpoint
92:08 - plus one
92:10 - and then we'll go all the way to the end
92:12 - and this is a
92:13 - python syntactic sugar so to speak if i
92:16 - don't specify an end index python knows
92:19 - to just go all the way to the end all
92:21 - right so this is our new list that we're
92:22 - working with
92:24 - and we need a target we'll just pass it
92:26 - through if you're confused bear with me
92:28 - just like before we'll visualize this at
92:30 - the end
92:31 - okay we have another else case here
92:34 - and this is a scenario where the value
92:36 - at the midpoint is greater than the
92:38 - target
92:39 - which means we only care about the
92:40 - values in the list from the start going
92:43 - up to the midpoint now in this case as
92:45 - well we're going to call the binary
92:47 - search function again and specify a new
92:49 - list to work with this time the list is
92:52 - going to start at the beginning and then
92:53 - go all the way up to the midpoint so it
92:56 - looks the same we'll say return
92:58 - recursive
93:01 - binary search
93:03 - we're going to pass in a list here so if
93:05 - we just put a colon here
93:07 - without a start index python knows to
93:10 - start at the beginning and we're going
93:11 - to go all the way up to the midpoint
93:14 - the target here is the same
93:16 - and this is our new binary search
93:18 - function so let's see if this works
93:22 - actually
93:23 - yes
93:24 - down here we'll make some space
93:26 - and we'll define a verify function
93:29 - we're not going to copy paste the
93:30 - previous one
93:31 - because we're not returning none or an
93:34 - integer here so we'll verify the result
93:36 - that we pass in and we'll say print
93:39 - target found
93:41 - and this is just going to say true or
93:43 - false whether we found it
93:45 - okay so like before we need a numbers
93:47 - list
93:48 - and we'll do something one two three
93:50 - four all the way up to eight
93:54 - okay and now let's test this out so
93:55 - we'll call
93:57 - our recursive
94:00 - binary search function
94:02 - and we'll pass in the numbers list
94:05 - and the target here is 12.
94:08 - we're going to verify this
94:11 - verify the result make sure it works and
94:13 - then we'll call it again this time
94:15 - making sure that we give it a target
94:16 - that is actually in the list so here
94:18 - we'll say 6
94:19 - and we'll verify this again
94:23 - make sure you hit command s to save
94:26 - and then in the console below we're
94:28 - going to type out
94:30 - python recursive binarysearch.pi
94:34 - run it and you'll see that we've
94:35 - verified that search works
94:38 - while we can't verify the index position
94:39 - of the target value which is a
94:41 - modification to how our algorithm works
94:44 - we can guarantee by running across all
94:46 - valid inputs that search works as
94:48 - intended
94:49 - so why write a different search
94:52 - algorithm here a different binary search
94:54 - algorithm and what's the difference
94:56 - between these two implementations anyway
94:58 - the difference lies in these last four
95:02 - lines of code that you see here
95:05 - we did something unusual here now before
95:08 - we get into this a small word of advice
95:11 - this is a confusing topic and people get
95:13 - confused by it all the time
95:15 - don't worry that doesn't make you any
95:18 - less of a programmer in fact i have
95:20 - trouble with it often and always look it
95:22 - up including when i made this video
95:24 - this version of binary search is a
95:26 - recursive binary search
95:29 - a recursive function is one that calls
95:31 - itself
95:33 - this is hard for people to grasp
95:35 - sometimes because there's few easy
95:37 - analogies that make sense but you can
95:39 - think of it and sort this way so let's
95:41 - say you have this book that contains
95:43 - answers to multiplication problems
95:46 - you're working on a problem and you look
95:48 - up an answer
95:49 - in the book the answer for your problem
95:51 - says add 10 to the answer for problem 52
95:56 - okay so you look up problem 52 and there
95:59 - it says add 12 to the answer for problem
96:02 - 85
96:04 - well then you go and look up the answer
96:05 - to problem 85 and finally instead of
96:08 - redirecting you somewhere else that
96:10 - answer says 10. so you take that 10 and
96:13 - then you go back to problem 52 because
96:15 - remember the answer for problem 52 was
96:18 - to add 12 to the answer for problem 85
96:22 - so you take that 10 and then you now
96:24 - have the answer to problem 85 so you add
96:27 - 10 to 12 to get 22.
96:29 - then you go back to your original
96:31 - problem where it said to add 10 to the
96:33 - answer for problem 52 so you add 10 to
96:36 - 22 and you get 32 to end up with your
96:38 - final answer so that's a weird way of
96:41 - doing it but this is an example of
96:42 - recursion
96:44 - the solution to your first lookup in the
96:46 - book was the value obtained by another
96:49 - lookup in the same book which was
96:51 - followed by yet another lookup in the
96:53 - same book the book told you to check the
96:55 - book until you arrived at some base
96:57 - value
96:59 - our function works in a similar manner
97:01 - so let's visualize this with an example
97:03 - of list
97:04 - like before we have a nine element list
97:07 - here with values zero through eight
97:09 - the target we're searching for is the
97:12 - value eight
97:13 - we'll check if the list is empty by
97:15 - calling len on it this list is not empty
97:18 - so we go to the else clause next we
97:20 - calculate the midpoint 9 divided by 2 is
97:22 - 4.5 rounded down is 4 so our first
97:26 - midpoint value is 4.
97:28 - we'll perform our first check is the
97:30 - value at the midpoint equal to the
97:32 - target
97:33 - not true so we go to our else clause
97:35 - we'll perform another check here is the
97:38 - value at the midpoint less than the
97:39 - target now in our case this is true
97:42 - earlier when we evaluated this condition
97:44 - we simply change the value of first
97:47 - here we're going to call the recursive
97:49 - binary search function again and give it
97:51 - a new list to work with
97:53 - the list starts at midpoint plus 1 so at
97:57 - index position 5 all the way to the end
98:00 - notice that this call to recursive
98:02 - binary search inside of recursive binary
98:05 - search includes a return statement
98:08 - this is important and we'll come back to
98:10 - that in a second
98:12 - so now we're back at the top
98:14 - of a new call to recursive binary search
98:17 - with effectively a new list although
98:20 - technically just a sub list of the first
98:22 - one
98:23 - the list here contains the numbers 6 7
98:26 - and 8.
98:27 - starting with the first check the list
98:29 - is not empty so we move to the else
98:32 - the midpoint in this case length of the
98:35 - list 3 divided by 2 rounded down is 1.
98:39 - is the value of the midpoint equal to
98:40 - the target well the value at that
98:42 - position is 7 so no in the else we
98:46 - perform the first check is the value at
98:49 - the midpoint less than the target indeed
98:51 - it is so we call recursive binary search
98:54 - again and provided a new list
98:56 - this list starts at midpoint plus 1 and
98:59 - goes to the end so in this case that's a
99:01 - single element list
99:03 - since this is a new call to recursive
99:05 - binary search we start back up at the
99:08 - top
99:09 - is the list empty no the midpoint is
99:12 - zero
99:13 - is the value at the midpoint the same as
99:15 - the target it is so now we can return
99:18 - true
99:19 - remember a minute ago i pointed out that
99:22 - when we call recursive binary search
99:24 - from inside the function itself it's
99:26 - preceded by a return statement
99:29 - that plays a pretty important role here
99:31 - so back to our visualization
99:34 - we start at the top and recall binary
99:36 - search with a new list but because
99:38 - that's got a return statement before it
99:40 - what we're saying is hey when you run
99:42 - binary search on this whatever value you
99:45 - get back return it to the function that
99:48 - called you
99:49 - then at the second level we call binary
99:51 - search again along with another return
99:53 - statement like with the first call we're
99:56 - instructing the function to return a
99:58 - value back to the code that called it
100:01 - at this level we find the target so the
100:03 - function returns true back to the caller
100:06 - but since this inner function was also
100:08 - called by a function with instructions
100:10 - to return it keeps returning that true
100:12 - value back up until we reach the very
100:15 - first function that called it going back
100:17 - to our book of answers recursive binary
100:20 - search instructs itself to keep working
100:22 - on the problem until it has a concrete
100:24 - answer
100:25 - once it does it works its way backwards
100:28 - giving the answer to every function that
100:30 - called it until the original caller has
100:33 - an answer
100:34 - now like i said at the beginning this is
100:36 - pretty complicated so you should not be
100:39 - concerned if this doesn't click honestly
100:41 - this is not one thing that you're going
100:42 - to walk away with knowing fully how to
100:44 - understand recursion after your first
100:46 - try i'm really not lying when i say i
100:48 - have a pretty hard time with recursion
100:51 - now before we move on i do want to point
100:52 - out one thing
100:54 - even though the implementation of
100:56 - recursion is harder to understand
100:58 - it is easier in this case to understand
101:01 - how we arrive at the logarithmic run
101:03 - time since we keep calling the function
101:06 - with smaller lists let's take a break
101:08 - here in the next video let's talk a bit
101:11 - more about recursion and why it matters
101:14 - [Music]
101:19 - in the last video we wrote a version of
101:21 - binary search that uses a concept called
101:23 - recursion
101:25 - recursion might be a new concept for you
101:27 - so let's formalize how we use it
101:30 - a recursive function is one that calls
101:33 - itself
101:34 - in our example the recursive binary
101:36 - search function called itself inside the
101:40 - body of the function
101:42 - when writing a recursive function you
101:44 - always need a stopping condition
101:46 - and typically we start the body of the
101:48 - recursive function with this stopping
101:50 - condition it's common to call this
101:53 - stopping condition the base case
101:55 - in our recursive binary search function
101:58 - we had two stopping conditions
102:01 - the first was what the function should
102:03 - return if an empty list is passed in
102:07 - it seems weird to evaluate an empty list
102:09 - because you wouldn't expect to run
102:11 - search on an empty list but if you look
102:14 - at how our function works recursive
102:16 - binary search keeps calling itself and
102:19 - with each call to itself the size of the
102:21 - list is cut in half
102:23 - if we searched for a target that didn't
102:25 - exist in the list then the function
102:28 - would keep halving itself until it got
102:30 - to an empty list
102:32 - consider a three element list with
102:34 - numbers one two three where we're
102:36 - searching for a target of four
102:39 - on the first pass the midpoint is 2 so
102:42 - the function would call itself with the
102:44 - list 3.
102:45 - on the next pass the midpoint is 0 and
102:48 - the target is still greater so the
102:50 - function would call itself this time
102:52 - passing in an empty list because an
102:55 - index of 0 plus 1 in a single element
102:58 - list doesn't exist
102:59 - when we have an empty list this means
103:02 - that after searching through the list
103:04 - the value wasn't found
103:06 - this is why we define an empty list as a
103:08 - stopping condition or a base case that
103:10 - returns false if it's not an empty list
103:13 - then we have an entirely different set
103:15 - of instructions we want to execute
103:17 - first we obtain the midpoint of the list
103:20 - once we have the midpoint we can
103:22 - introduce our next base case or stopping
103:24 - condition
103:25 - if the value at the midpoint is the same
103:28 - as the target then we return true
103:31 - with these two stopping conditions we've
103:33 - covered all possible paths of logic
103:36 - through the search algorithm you can
103:38 - either find the value or you don't
103:41 - once you have the base cases the rest of
103:43 - the implementation of the recursive
103:45 - function is to call the function on
103:47 - smaller sub-lists until we hit one of
103:50 - these base cases going back to our
103:52 - visualization for a second we see that
103:54 - recursive binary search calls itself a
103:57 - first time which then calls itself again
104:00 - for the initial list we started with the
104:03 - function only calls itself a few times
104:05 - before a stopping condition is reached
104:08 - the number of times a recursive function
104:10 - calls itself is called recursive depth
104:13 - now the reason i bring all of this up is
104:16 - because if after you start learning
104:18 - about algorithms you decide you want to
104:20 - go off and do your own research you may
104:22 - start to see a lot of algorithms
104:24 - implemented using recursion
104:27 - the way we implemented binary search the
104:29 - first time is called an iterative
104:32 - solution
104:33 - now when you see the word iterative it
104:35 - generally means the solution was
104:36 - implemented using a loop structure of
104:38 - some kind
104:40 - a recursive solution on the other hand
104:42 - is one that involves a set of stopping
104:44 - conditions and a function that calls
104:46 - itself computer scientists and computer
104:49 - science textbooks particularly from back
104:51 - in the day
104:52 - favor and are written in what are called
104:55 - functional languages
104:57 - in functional languages we try to avoid
104:59 - changing data that is given to a
105:01 - function
105:02 - in our first version of binary search we
105:04 - created first and last variables using
105:07 - the list and then modified first and
105:09 - last as we needed to arrive at a
105:11 - solution
105:12 - functional languages don't like to do
105:14 - this all this modification of variables
105:16 - and prefer a solution using recursion
105:19 - a language like python which is what
105:21 - we're using is the opposite and doesn't
105:24 - like recursion in fact python has a
105:27 - maximum recursion depth after which our
105:30 - function will halt execution python
105:32 - prefers an iterative solution now i
105:35 - mentioned all of this for two reasons
105:38 - if you decide that you want to learn how
105:40 - to implement the algorithm in a language
105:42 - of your choice that's not python then
105:45 - you might see a recursive solution as
105:47 - the best implementation in that
105:49 - particular language
105:51 - i'm an ios developer for example and i
105:53 - work with a language called swift
105:55 - swift is different from python in that
105:58 - it doesn't care about recursion depth
106:00 - and does some neat tricks where it
106:01 - doesn't even matter how many times your
106:03 - function calls itself
106:05 - so if you want to see this in swift code
106:07 - then you need to know how recursion
106:09 - works
106:10 - well and now you have some idea now the
106:12 - second reason i bring it up is actually
106:14 - way more important and to find out on to
106:16 - the next video
106:17 - at the beginning of this series i
106:19 - mentioned that there were two ways of
106:20 - measuring the efficiency of an algorithm
106:23 - the first was time complexity or how the
106:25 - run time of an algorithm grows as n
106:27 - grows larger
106:29 - the second is space complexity
106:31 - we took a pretty long route to build up
106:33 - this example but now we're in a good
106:35 - place to discuss space complexity
106:38 - space complexity is a measure of how
106:40 - much working storage or extra storage is
106:43 - needed as a particular algorithm grows
106:47 - we don't think about it much these days
106:49 - but every single thing we do on a
106:51 - computer takes up space in memory in the
106:54 - early days of computing considering
106:56 - memory usage was of paramount importance
106:58 - because memory was limited and really
107:01 - expensive
107:02 - these days were spoiled our devices are
107:04 - rich with memory this is okay when we
107:07 - write everyday code because most of us
107:10 - aren't dealing with enormously large
107:11 - data sets
107:13 - when we write algorithms however we need
107:15 - to think about this because we want to
107:17 - design our algorithms to perform as
107:19 - efficiently as it can as the size of the
107:22 - data set n grows really large
107:25 - like time complexity space complexity is
107:28 - measured in the worst case scenario
107:30 - using big-o notation
107:32 - since you are familiar with the
107:33 - different kinds of complexities let's
107:35 - dive right into an example
107:38 - in our iterative implementation of
107:40 - binary search the first one we wrote
107:43 - that uses a while loop let's look at
107:45 - what happens to our memory usage as n
107:47 - gets large
107:49 - let's bring up that function
107:52 - let's say we start off with a list of 10
107:54 - elements now inspecting the code we see
107:57 - that our solution relies heavily on
107:58 - these two variables first and last
108:02 - first points to the start of the list
108:03 - and last to the end
108:05 - when we eliminate a set of values we
108:08 - don't actually create a sub list instead
108:10 - we just redefine first
108:12 - and last as you see here
108:15 - to point to a different section of the
108:17 - list
108:18 - since the algorithm only considers the
108:20 - values between first and last when
108:22 - determining the midpoint
108:25 - by redefining first and last as the
108:27 - algorithm proceeds we can find a
108:29 - solution using just the original list
108:32 - this means that for any value of n
108:34 - the space complexity of the iterative
108:37 - version of binary search is constant or
108:40 - that the iterative version of binary
108:42 - search takes constant space
108:45 - remember that we would write this as big
108:47 - o of one
108:48 - this might seem confusing because as n
108:51 - grows we need more storage to account
108:53 - for that larger list size
108:55 - now this is true but that storage is not
108:58 - what space complexity cares about
109:00 - measuring
109:01 - we care about what additional storage is
109:03 - needed as the algorithm runs and tries
109:06 - to find a solution
109:08 - if we assume something simple say that
109:10 - for a given size of a list represented
109:12 - by a value n it takes n amount of space
109:16 - to store it whatever that means
109:18 - then for the iterative version of binary
109:20 - search regardless of how large the list
109:23 - is at the start middle and end of the
109:26 - algorithm process the amount of storage
109:29 - required does not get larger than n
109:32 - and this is why we consider it to run in
109:34 - constant space
109:36 - now this is an entirely different story
109:38 - with the recursive version however in
109:40 - the recursive version of binary search
109:42 - we don't make use of variables to keep
109:44 - track of which portion of the list we're
109:46 - working with
109:47 - instead we create new lists every time
109:50 - with a subset of values or sub-lists
109:53 - with every recursive function call
109:56 - let's assume we have a list of size n
109:58 - and in the worst case scenario the
110:00 - target element is the last in the list
110:03 - calling the recursive implementation of
110:05 - binary search on this list and target
110:08 - would lead to a scenario like this
110:10 - the function would call itself and
110:12 - create a new list that goes from the
110:14 - midpoint to the end of the list
110:17 - since we're discarding half the values
110:19 - the size of the sub list is n by 2.
110:22 - this function will keep calling itself
110:24 - creating a new sub list that's half the
110:26 - size of the current one until it arrives
110:29 - at a single element list and a stopping
110:31 - condition
110:32 - this pattern that you see here where the
110:35 - size of the sublist is reduced by a
110:37 - factor on each execution of the
110:39 - algorithmic logic well we've seen that
110:41 - pattern before do you remember where
110:44 - this is exactly how binary search works
110:46 - it discards half the values every time
110:49 - until it finds a solution now we know
110:51 - that because of this pattern the running
110:53 - time of binary search is logarithmic
110:56 - in fact the space complexity of the
110:58 - recursive version of binary search is
111:00 - the same
111:01 - if we start out with a memory allocation
111:04 - of size n that matches the list
111:06 - on each function call of recursive
111:09 - binary search we need to allocate
111:11 - additional memory of size n by 2 n by 4
111:14 - and so on until we have a sub list that
111:17 - is either empty or contains a single
111:19 - value because of this we say that the
111:22 - recursive version of the binary search
111:24 - algorithm runs in logarithmic time with
111:27 - a big o of log n
111:29 - now there's an important caveat here
111:31 - this totally depends on the language
111:34 - remember how i said that a programming
111:36 - language like swift can do some tricks
111:38 - to where recursion depth doesn't matter
111:40 - the same concept applies here if you
111:43 - care to read more about this concept
111:45 - it's called tail
111:46 - optimization it's called tail
111:48 - optimization because if you think of a
111:51 - function as having a head and a tail
111:54 - if the recursive function call is the
111:56 - last line of code in the function as it
111:59 - is in our case
112:01 - we call this tail recursion since it's
112:04 - the last part of the function that calls
112:06 - itself
112:07 - now the trick that swift does to reduce
112:09 - the amount of space and therefore
112:11 - computing overhead to keep track of this
112:14 - recursive calls is called tail call
112:16 - optimization or tail call elimination
112:20 - it's one of those things that you'll see
112:21 - thrown around a loss in algorithm
112:23 - discussions but may not always be
112:25 - relevant to you
112:27 - now what if any of this is relevant to
112:29 - us well python does not implement tail
112:32 - call optimization so the recursive
112:34 - version of binary search takes
112:36 - logarithmic space
112:38 - if we had to choose between the two
112:40 - implementations given that time
112:42 - complexity or run time of both versions
112:45 - the iterative and the recursive version
112:47 - are the same we should definitely go
112:49 - with the iterative implementation in
112:51 - python since it runs in constant space
112:55 - okay that was a lot but all of this with
112:57 - all of this we've now established two
112:59 - important ways to distinguish between
113:02 - algorithms that handle the same task and
113:04 - determine which one we should use
113:07 - we've arrived at what i think is a good
113:09 - spot to take a long break and let all of
113:11 - these new concepts sink in but before
113:14 - you go off to the next course let's take
113:16 - a few minutes to recap everything we've
113:17 - learned so far
113:19 - while we did implement two algorithms in
113:22 - this course in actual code much of what
113:24 - we learned here was conceptual and will
113:26 - serve as building blocks for everything
113:28 - we're going to learn in the future so
113:30 - let's list all of it out
113:32 - the first thing we learned about and
113:33 - arguably the most important was
113:35 - algorithmic thinking
113:37 - algorithmic thinking is an approach to
113:39 - problem solving that involves breaking a
113:41 - problem down into a clearly defined
113:43 - input and output along with a distinct
113:46 - set of steps that solves the problem by
113:48 - going from input to output
113:51 - algorithmic thinking is not something
113:53 - you develop overnight by taking one
113:55 - course so don't worry if you're thinking
113:57 - i still don't truly know how to apply
113:58 - what i learned here
114:00 - algorithmic thinking sinks in after you
114:02 - go through several examples in a similar
114:04 - fashion to what we did today
114:07 - it also helps to apply these concepts in
114:09 - the context of a real example which is
114:12 - another thing we will strive to do
114:13 - moving forward
114:15 - regardless it is important to keep in
114:16 - mind that the main goal here is not to
114:18 - learn how to implement a specific data
114:21 - structure or algorithm off the top of
114:23 - your head i'll be honest i had to look
114:25 - up a couple code snippets for a few of
114:27 - the algorithms myself in writing this
114:29 - course
114:30 - but in going through this you now know
114:32 - that binary search exists and can apply
114:35 - to a problem where you need a faster
114:37 - search algorithm
114:39 - unlike most courses where you can
114:40 - immediately apply what you have learned
114:42 - to build something cool learning about
114:44 - algorithms and data structures will pay
114:46 - off more in the long run
114:48 - the second thing we learned about is how
114:51 - to define and implement algorithms we've
114:53 - gone over these guidelines several times
114:55 - i won't bore you here again at the end
114:58 - but i will remind you that if you're
115:00 - often confused about how to effectively
115:02 - break down a problem in code to
115:04 - something more manageable following
115:06 - those algorithm guidelines is a good
115:08 - place to start
115:09 - next we learned about big o and
115:12 - measuring the time complexity of
115:13 - algorithms this is a mildly complicated
115:16 - topic but once you've abstracted the
115:18 - math away it isn't as hazy a topic as it
115:20 - seems
115:21 - now don't get me wrong the math is
115:23 - pretty important but only for those
115:25 - designing and analyzing algorithms
115:27 - our goal is more about how to understand
115:30 - and evaluate algorithms
115:32 - we learned about common run times like
115:35 - constant linear logarithmic and
115:37 - quadratic runtimes these are all fairly
115:40 - new concepts but in time you will
115:42 - immediately be able to distinguish the
115:44 - runtime of an algorithm based on the
115:46 - code you write and have an understanding
115:48 - of where it sits on an efficiency scale
115:51 - you will also in due time internalize
115:53 - runtimes of popular algorithms like the
115:56 - fact that binary search runs in
115:58 - logarithmic time and constant space
116:00 - and be able to recommend alternative
116:02 - algorithms for a given problem
116:05 - all in all over time the number of tools
116:07 - in your tool belt will increase
116:10 - next we learned about two important
116:12 - search algorithms and the situations in
116:14 - which we select one over the other
116:16 - we also implemented these algorithms in
116:18 - code so that you got a chance to see
116:20 - them work
116:21 - we did this in python but if you are
116:23 - more familiar with a different language
116:25 - and haven't gotten the chance to check
116:27 - out the code snippets we've provided you
116:29 - should try your hand at implementing it
116:31 - yourself it's a really good exercise to
116:34 - go through
116:35 - finally we learned about an important
116:37 - concept and a way of writing algorithmic
116:39 - code through recursion recursion is a
116:42 - tricky thing and depending on the
116:44 - language you write code with you may run
116:46 - into it more than others
116:48 - it is also good to be aware of because
116:50 - as we saw in our implementation of
116:52 - binary search
116:53 - whether recursion was used or not
116:55 - affected the amount of space we used
116:58 - don't worry if you don't fully
117:00 - understand how to write recursive
117:02 - functions i don't truly know either the
117:04 - good part is you can always look these
117:06 - things up and understand how other
117:08 - people do it
117:09 - anytime you encounter recursion in our
117:12 - courses moving forward you'll get a full
117:14 - explanation of how and why the function
117:16 - is doing what it's doing
117:18 - and that brings us to the end of this
117:20 - course i'll stress again that the goal
117:22 - of this course was to get you prepared
117:24 - for learning about more specific
117:26 - algorithms by introducing you to some of
117:29 - the tools and concepts you will need
117:31 - moving forward
117:32 - so if you're sitting there thinking i
117:34 - still don't know how to write many
117:35 - algorithms or how to use algorithmic
117:37 - thinking that's okay we'll get there
117:40 - just stick with it
117:41 - as always have fun and happy coding
117:46 - [Music]
117:53 - hi my name is passant i'm an instructor
117:55 - at treehouse and welcome to the
117:57 - introduction to data structures course
117:59 - in this course we're going to answer one
118:01 - fundamental question why do we need more
118:04 - data structures than a programming
118:05 - language provides
118:07 - before we answer that question some
118:09 - housekeeping if you will
118:11 - in this course we're going to rely on
118:12 - concepts we learned in the introduction
118:14 - to algorithms course
118:16 - namely big-o notation space and time
118:19 - complexity and recursion
118:21 - if you're unfamiliar with those concepts
118:23 - or just need a refresher check out the
118:25 - prerequisites courses listed
118:27 - in addition this course does assume that
118:29 - you have some programming experience
118:32 - we're going to use data structures that
118:34 - come built into nearly all programming
118:36 - languages as our point of reference
118:38 - while we will explain the basics of how
118:40 - these structures work we won't be going
118:42 - over how to use them in practice
118:45 - if you're looking to learn how to
118:46 - program before digging into this content
118:49 - check the notes section of this video
118:50 - for helpful links
118:52 - if you're good to go then awesome let's
118:54 - start with an overview of this course
118:56 - the first thing we're going to do is to
118:58 - explore a data structure we are somewhat
119:00 - already familiar with arrays
119:02 - if you've written code before there's a
119:04 - high chance you have used an array
119:06 - in this course we're going to spend some
119:08 - time understanding how arrays work what
119:10 - are the common operations on an array
119:13 - and what are the run times associated
119:15 - with those operations
119:17 - once we've done that we're going to
119:18 - build a data type of our own called a
119:20 - linked list
119:22 - in doing so we're going to learn that
119:23 - there's more than one way to store data
119:26 - in fact there's way more than just one
119:28 - way
119:29 - we're also going to explore what
119:30 - motivates us to build specific kinds of
119:32 - structures and look at the pros and cons
119:34 - of these structures
119:36 - we'll do that by exploring four common
119:38 - operations accessing a value searching
119:41 - for a value inserting a value and
119:43 - deleting a value
119:45 - after that we're actually going to
119:46 - circle back to algorithms and implement
119:48 - a new one a sorting algorithm
119:51 - in the introductions to algorithms
119:52 - course we implemented a binary search
119:55 - algorithm a precondition to binary
119:57 - search was that the list needed to be
119:59 - sorted
120:00 - we're going to try our hand at sorting a
120:02 - list and open the door to an entirely
120:04 - new category of algorithms
120:07 - we're going to implement our sorting
120:08 - algorithm on two different data
120:10 - structures and explore how the
120:11 - implementation of one algorithm can
120:14 - differ based on the data structure being
120:16 - used
120:17 - we'll also look at how the choice of
120:18 - data structure potentially influences
120:21 - the run time of the algorithm
120:23 - in learning about sorting we're also
120:24 - going to encounter another general
120:27 - concept of algorithmic thinking called
120:29 - divide and conquer
120:30 - along with recursion dividing conquer
120:32 - will be a fundamental tool that we will
120:34 - use to solve complex problems all in due
120:37 - time in the next video let's talk about
120:39 - arrays
120:41 - a common data structure built into
120:43 - nearly every programming language is the
120:45 - array
120:46 - arrays are a fundamental data structure
120:48 - and can be used to represent a
120:49 - collection of values but it is much more
120:52 - than that arrays are also used as
120:54 - building blocks to create even more
120:56 - custom data types and structures
120:58 - in fact in most programming languages
121:00 - text is represented using the string
121:02 - type and under the hood strings are just
121:05 - a bunch of characters stored in a
121:06 - particular order in an array
121:09 - before we go further and dig into arrays
121:12 - what exactly is a data structure
121:15 - a data structure is a way of storing
121:16 - data when programming it's not just a
121:19 - collection of values and the format
121:21 - they're stored in but the relationship
121:23 - between the values in the collection as
121:25 - well as the operations applied on the
121:27 - data stored in the structure
121:29 - an array is one of very many data
121:32 - structures in general an array is a data
121:35 - structure that stores a collection of
121:37 - values where each value is referenced
121:39 - using an index or a key
121:42 - a common analogy for thinking about
121:44 - arrays is as a set of train cars
121:47 - each car has a number and these cars are
121:49 - ordered sequentially
121:51 - inside each car the array or the train
121:54 - in this analogy stores some data
121:57 - while this is the general representation
121:59 - of an array it can differ slightly from
122:01 - one language to another but for the most
122:03 - part all these fundamentals remain the
122:05 - same in a language like swift or java
122:08 - arrays are homogeneous containers which
122:11 - means they can only contain values of
122:13 - the same type
122:14 - if you use an array to store integers in
122:17 - java it can only store integers
122:20 - in other languages arrays are
122:22 - heterogeneous structures that can store
122:24 - any kind of value in python for example
122:27 - you can mix numbers and text with no
122:29 - issues
122:30 - now regardless of this nuance the
122:32 - fundamental concept of an array is the
122:34 - index
122:35 - this index value is used for every
122:38 - operation on the array from accessing
122:40 - values to inserting updating and
122:42 - deleting
122:44 - in python the language we're going to be
122:46 - using for this course it's a tiny bit
122:48 - confusing
122:49 - the type that we generally refer to as
122:51 - an array in most languages is best
122:54 - represented by the list type in python
122:57 - python does have a type called array as
123:00 - well but it's something different so
123:01 - we're not going to use it
123:03 - while python calls it a list when we use
123:06 - a list in this course we'll be talking
123:08 - about concepts that apply to arrays as
123:10 - well in other languages so definitely
123:12 - don't skip any of this there's one more
123:14 - thing
123:15 - in computer science a list is actually a
123:17 - different data structure than an array
123:20 - and in fact we're going to build a list
123:22 - later on in this course
123:24 - generally though this structure is
123:25 - called a linked list as opposed to just
123:28 - list so hopefully the terminology isn't
123:30 - too confusing
123:32 - to properly understand how arrays work
123:35 - let's take a peek at how arrays are
123:37 - stored under the hood
123:39 - an array is a contiguous data structure
123:42 - this means that the array is stored in
123:44 - blocks of memory that are right beside
123:46 - each other with no gaps
123:48 - the advantage of doing this is that
123:50 - retrieving values is very easy
123:52 - in a non-contiguous data structure we're
123:55 - going to build one soon the structure
123:57 - stores a value as well as a reference to
123:59 - where the next value is
124:01 - to retrieve that next value the language
124:03 - has to follow that reference also called
124:05 - a pointer to the next block of memory
124:08 - this adds some overhead which as you
124:10 - will see increases the runtime of common
124:13 - operations a second ago i mentioned that
124:16 - depending on the language arrays can
124:18 - either be homogeneous containing the
124:20 - same type of value or heterogeneous
124:22 - where any kind of value can be mixed
124:25 - this choice also affects the memory
124:27 - layout of the array
124:28 - for example in a language like c swift
124:31 - or java where arrays are homogeneous
124:34 - when an array is created since the kind
124:36 - of value is known to the language
124:38 - compiler and you can think of the
124:40 - compiler as the brains behind the
124:42 - language
124:43 - it can choose a contiguous block of
124:45 - memory that fits the array size and
124:47 - values created
124:49 - if the values were integers assuming an
124:51 - integer took up space represented by one
124:54 - of these blocks then for a five item
124:56 - array the compiler can allocate five
124:59 - blocks of equally sized memory
125:02 - in python however this is not the case
125:04 - we can put any value in a python list
125:07 - there's no restriction
125:08 - the way this works is a combination of
125:11 - contiguous memory and the pointers or
125:13 - references i mentioned earlier
125:16 - when we create a list in python there is
125:18 - no information about what will go into
125:21 - that array which makes it hard to
125:23 - allocate contiguous memory of the same
125:25 - size
125:26 - there are several advantages to having
125:28 - contiguous memory
125:29 - since the values are stored beside each
125:31 - other accessing the values happens in
125:33 - almost constant time so this is a
125:36 - characteristic we want to preserve
125:38 - the way python gets around this is by
125:40 - allocating contiguous memory and storing
125:43 - init not the value we want to store but
125:46 - a reference or a pointer to the value
125:49 - that's stored somewhere else in memory
125:51 - by doing this it can allocate equally
125:54 - sized contiguous memory since regardless
125:56 - of the value size the size of the
125:58 - pointer to that value is always going to
126:00 - be equal this incurs an additional cost
126:03 - in that when a value is accessed we need
126:06 - to follow the pointer to the block of
126:08 - memory where the value is actually
126:09 - stored but python has ways of dealing
126:12 - with these costs that are outside the
126:13 - scope of this course
126:15 - now that we know how an array stores its
126:17 - values let's look at common operations
126:19 - that we execute on an array
126:21 - regardless of the kind of data structure
126:23 - you work with all data structures are
126:25 - expected to carry out four kinds of
126:27 - operations at minimum
126:29 - we need to be able to access and read
126:31 - values stored in the structure we need
126:33 - to be able to search for an arbitrary
126:35 - value
126:36 - we also need to be able to insert a
126:38 - value at any point into the structure
126:40 - and finally we need to be able to delete
126:42 - structures
126:43 - let's look at how these operations are
126:45 - implemented on the array structure in
126:47 - some detail starting with access
126:50 - elements in an array are identified
126:52 - using a value known as an index and we
126:55 - use this index to access and read the
126:57 - value
126:58 - most programming languages follow a
127:00 - zero-based numbering system when it
127:02 - comes to arrays and all this means is
127:04 - that the first index value is equal to
127:07 - zero not one
127:08 - generally speaking when an array is
127:10 - declared a base amount of contiguous
127:13 - memory is allocated as the array storage
127:16 - computers refer to memory through the
127:18 - use of an address but instead of keeping
127:20 - a reference to all the memory allocated
127:22 - for an array the array only has to store
127:25 - the address of the first location
127:27 - because the memory is contiguous using
127:30 - the base address the array can calculate
127:32 - the address of any value by using the
127:34 - index position of that value as an
127:37 - offset
127:38 - if you want to be more specific think of
127:40 - it this way
127:41 - let's say we want to create an array of
127:42 - integers and then each integer takes up
127:45 - a certain amount of space in memory that
127:47 - we'll call m
127:48 - let's also assume that we know how many
127:50 - elements we're going to create so the
127:52 - size of the array is some number of
127:54 - elements we'll call n
127:56 - the total amount of space that we need
127:58 - to allocate is n times the space per
128:00 - item m
128:01 - if the array keeps track of the location
128:04 - in memory where the first value is held
128:06 - so let's label that m0 then it has all
128:09 - the information it needs to find any
128:11 - other element in the list
128:13 - when accessing a value in an array we
128:15 - use the index
128:16 - to get the first element in the list we
128:18 - use the zeroth index to get the second
128:21 - we use the index value 1 and so on
128:24 - given that the array knows how much
128:25 - storage is needed for each element it
128:28 - can get the address of any element by
128:30 - starting off with the address for the
128:32 - first element and adding to that the
128:34 - index value times the amount of storage
128:37 - per element
128:38 - for example to access the second value
128:41 - we can start with m0 and to that add m
128:44 - times the index value 1 giving us m1 as
128:47 - the location in memory for the second
128:49 - address
128:50 - this is a very simplified model but
128:52 - that's more or less how it works
128:54 - this is only possible because we know
128:56 - that array memory is contiguous with no
128:59 - gaps
129:01 - let's switch over to some code
129:03 - as i mentioned earlier we're going to be
129:05 - using python in this course
129:07 - if you don't know how to code or you're
129:09 - interested in this content but know a
129:11 - language other than python check the
129:13 - notes section of this video for more
129:15 - information while the code will be in
129:17 - python the concepts are universal and
129:20 - more importantly simple enough that you
129:22 - should have no issue following along in
129:24 - your favorite programming language
129:26 - and to get started click on the launch
129:28 - workspaces button on the video page that
129:31 - you're watching right now
129:33 - this should spin up an instance of a
129:35 - treehouse workspace an in-browser coding
129:38 - environment right now your workspace
129:40 - should be empty and that's expected so
129:42 - let's add a new file in here i'm going
129:44 - to go to file new file
129:46 - and we'll call this arrays
129:49 - dot py pi
129:51 - creating a list in python is quite
129:54 - simple so we'll call this new underscore
129:56 - list
129:57 - we use a set of square brackets around a
130:00 - set of values to create a list so one
130:02 - and we comma separate them so space two
130:06 - and space three this allocates a base
130:09 - amount of memory for the array to use or
130:12 - when i say array know that in python i
130:14 - mean a list
130:15 - since this is python the values aren't
130:18 - stored in memory
130:19 - instead the values 1 2 and 3 are stored
130:23 - elsewhere in memory and the array stores
130:25 - references to each of those objects
130:28 - to access a value we use a subscript
130:32 - along with an index value so to get the
130:34 - first value we use the index 0 and if we
130:38 - were to assign this to another variable
130:40 - we would say result
130:42 - equal new list
130:44 - we write out new lists since this is the
130:46 - array that we're accessing the value
130:48 - from and then a subscript notation which
130:50 - is a square bracket
130:51 - and then the index value
130:53 - as we saw since the array has a
130:56 - reference to the base location in memory
130:59 - the position of any element can be
131:01 - determined pretty easily
131:03 - we don't have to iterate over the entire
131:05 - list
131:06 - all we need to do is a simple
131:08 - calculation of an offset from the base
131:11 - memory since we're guaranteed that the
131:13 - memory is contiguous
131:15 - for this reason access is a constant
131:18 - time operation on an array or a python
131:21 - list
131:22 - this is also why an array crashes if you
131:24 - try to access a value using an index
131:27 - that is out of bounds of what the array
131:29 - stores
131:30 - if you've used an array before you've
131:32 - undoubtedly run into an error or a crash
131:35 - where you try to access a value using an
131:37 - index that was larger than the number of
131:39 - elements in the array since the array
131:42 - calculates the memory address on the fly
131:45 - when you access a value with an out of
131:47 - bounds index as it's called the memory
131:49 - address returned is not one that's part
131:52 - of the array structure and therefore
131:54 - cannot be read by the array now in
131:56 - python this is represented by an index
131:58 - error and we can make this happen by
132:00 - using an index we know our array won't
132:03 - contain
132:04 - now i'm writing out my code here inside
132:06 - of a text editor which obviously doesn't
132:08 - run the code so let's drag up this
132:10 - console area here
132:12 - and i'm going to write python
132:15 - to bring up the python interpreter
132:18 - and in here we can do the same thing so
132:19 - i can say new
132:21 - list equal one
132:23 - comma two comma three and now this is an
132:26 - interpreter so it's actually going to
132:27 - evaluate our code
132:29 - all right so now we have a new list if i
132:30 - type out new list it gets printed out
132:33 - into the console
132:34 - okay i can also do new list square
132:37 - bracket 0 and you'll see that i get the
132:39 - value 1 which is the value stored at the
132:41 - zeroth index
132:43 - now to highlight that index error we can
132:45 - do new list
132:47 - and inside the square brackets we can
132:48 - provide an index that we know our array
132:51 - doesn't contain so here i'll say index
132:54 - 10
132:55 - and if i hit enter you'll see it say
132:57 - index error list index out of range
133:00 - and those are the basics of how we
133:02 - create and read values from an array in
133:04 - the next video let's take a look at
133:06 - searching
133:07 - in the last video we learned what
133:09 - happens under the hood when we create an
133:11 - array and read a value using an index
133:13 - in this video we're going to look at how
133:15 - the remaining data structure operations
133:17 - work on arrays
133:19 - if you took the introduction to
133:20 - algorithms course we spent time learning
133:22 - about two search algorithms linear
133:25 - search and binary search
133:27 - while arrays are really fast at
133:28 - accessing values they're pretty bad at
133:30 - searching
133:32 - taking an array as is the best we can do
133:34 - is use linear search for a worst case
133:37 - linear runtime linear search works by
133:40 - accessing and reading each value in the
133:42 - list until the element in concern is
133:44 - found
133:45 - if the element we're looking for is at
133:47 - the end of the list then every single
133:49 - element in the list will have been
133:50 - accessed and compared
133:52 - even though accessing and comparing our
133:54 - constant time operations having to do
133:56 - this for every element results in an
133:58 - overall linear time
134:00 - let's look at how search works in code
134:03 - in python we can search for an item in
134:06 - an array in one of two ways
134:08 - we can use the in operator to check
134:11 - whether a list contains an item so i can
134:13 - say if
134:15 - one in new underscore
134:18 - list
134:19 - then print
134:21 - true
134:23 - the in operator actually calls a
134:25 - contains method that is defined on the
134:28 - list type which runs a linear search
134:31 - operation
134:32 - in addition to this we can also use a
134:34 - for loop to iterate over the list
134:36 - manually and perform a comparison
134:39 - operation
134:40 - so i can say
134:42 - for
134:43 - n in new list
134:48 - if n equals one then print
134:52 - true
134:53 - and then after that break out of the
134:55 - loop
134:56 - this is more or less the implementation
134:58 - of linear search
134:59 - if the array were sorted however we
135:01 - could use binary search but because sort
135:04 - operations incur a cost of their own
135:07 - languages usually stay away from sorting
135:09 - the list and running binary search since
135:11 - for smaller arrays linear search on its
135:13 - own may be faster
135:15 - now again remember that
135:17 - since this is an editor this is just a
135:19 - text file none of these lines of code
135:21 - are evaluated so you can try that out in
135:23 - here so we'll copy that we can come down
135:26 - here and say python and hit enter and
135:29 - then when it starts up we can paste in
135:31 - our list
135:32 - and now we can try what we just did so
135:34 - if one in new list
135:38 - print
135:39 - true
135:41 - and there you go it prints true now
135:43 - because we've already learned about
135:44 - linear and binary search in a previous
135:47 - course there's nothing new going on here
135:50 - what's more interesting to look at in my
135:51 - opinion is inserting and deleting values
135:54 - in an array let's start with inserting
135:57 - in general most array implementations
136:00 - support three types of insert operations
136:03 - the first is a true insert using an
136:06 - index value where we can insert an
136:08 - element anywhere in the list this
136:10 - operation has a linear runtime imagine
136:13 - you wanted to insert an item at the
136:15 - start of the list when we insert into
136:18 - the first position what happens to the
136:20 - item that is currently in that spot
136:22 - well it has to move to the next spot at
136:25 - index value one what happens to the
136:27 - second item at index position one
136:29 - that one moves to the next spot at index
136:32 - position two
136:33 - this keeps happening until all elements
136:36 - have been shifted forward one index
136:38 - position
136:39 - so in the worst case scenario inserting
136:41 - at the zeroth position of an array every
136:44 - single item in the array has to be
136:47 - shifted forward and we know that any
136:49 - operation that involves iterating
136:51 - through every single value means a
136:53 - linear runtime
136:55 - now the second way we can insert an item
136:57 - into an array is by appending appending
137:00 - although technically an insert operation
137:03 - in that it inserts an item into an
137:05 - existing array doesn't incur the same
137:07 - runtime cost because appends simply add
137:10 - the item to the end of the list
137:13 - we can simplify and say that this is
137:15 - constant time this is a constant time
137:18 - operation but it depends on the language
137:20 - implementation of array
137:22 - to highlight why that matters let's
137:24 - consider how lists in python work in
137:28 - python when we create a list the list
137:30 - doesn't know anything about the size of
137:32 - the list and how many elements we're
137:34 - going to store
137:35 - creating a new empty list like so so
137:38 - numbers equal and two empty brackets
137:42 - so this creates a list and allocates a
137:45 - space of size n plus one
137:48 - since n here is zero there are no
137:50 - elements in this array in this list
137:53 - space is allocated for a one element
137:56 - list to start off
137:57 - because the space allocated for the list
137:59 - and the space used by the list are not
138:02 - the same
138:03 - what do you think happens when we ask
138:05 - python for the length of this list so i
138:07 - can say len numbers
138:10 - we correctly get 0 back
138:13 - this means that the list doesn't use the
138:15 - memory allocation as an indicator of its
138:17 - size because as i mentioned it has
138:20 - allocated space for a one element list
138:22 - but it returns zero so it determines it
138:25 - in other ways
138:26 - okay so numbers this list currently has
138:29 - space for one element
138:31 - let's use the append method defined on
138:33 - the type to insert a number at the end
138:36 - of the list so you can say numbers dot
138:39 - append and i'll pass in 2.
138:42 - now the memory allocation and the size
138:44 - of the list are the same since the list
138:47 - contains one element
138:49 - now what if i were to do something like
138:50 - this numbers.append
138:53 - there needs to be a dot
138:56 - and i'll add another value 200.
138:59 - now since the list only has an
139:00 - allocation for one item at this point
139:03 - before it can add the new element to the
139:06 - list it needs to increase the memory
139:08 - allocation and thereby the size of the
139:10 - list it does this by calling a list
139:13 - resize operation list resizing is quite
139:16 - interesting because it shows the
139:18 - ingenuity in solving problems like this
139:22 - python doesn't resize the list to
139:24 - accommodate just the element we want to
139:27 - add
139:28 - instead in this case it would allocate
139:30 - four blocks of memory to increase the
139:32 - size to a total of four contiguous
139:35 - blocks of memory
139:36 - it does this so that it doesn't have to
139:38 - resize the list every single time we add
139:40 - an element but at very specific points
139:44 - the growth pattern of the list type in
139:47 - python is 0 4 8 16 25 35 46 and so on
139:54 - this means that as the list size
139:56 - approaches these specific values
139:59 - resize is called again if you look at
140:02 - when the size of the list is four
140:04 - this means that when appending four more
140:06 - values until the size of eight
140:09 - each of those append operations do not
140:11 - increase the amount of space taken
140:14 - at specific points however when resizing
140:17 - is triggered space required increases as
140:20 - memory allocation increases
140:23 - this might signify that the append
140:25 - method has a non-constant space
140:27 - complexity but it turns out that because
140:30 - some operations don't increase space and
140:32 - others do
140:34 - when you average all of them out append
140:36 - operations take constant space
140:39 - we say that it has an amortized constant
140:42 - space complexity this also happens with
140:45 - insert operations
140:46 - if we had a four element array we would
140:48 - have four elements and a memory
140:50 - allocation of four
140:51 - an insert operation at that point
140:54 - doesn't matter where it happens on the
140:55 - list but at that point it would trigger
140:57 - a resize
140:59 - inserting is still more expensive though
141:01 - because after the resize every element
141:03 - needs to be shifted over one
141:06 - the last insert operation that is
141:08 - supported in most languages is the
141:10 - ability to add one list to another
141:13 - in python this is called an extend and
141:15 - looks like this
141:16 - so i'll say numbers now if you let me
141:19 - actually clear out the console
141:21 - oh actually you will let's exit python
141:25 - we'll clear this out so we're back at
141:26 - the top and we'll start again
141:28 - so i'll say numbers
141:31 - and we'll set it to an empty list and
141:33 - now we can say numbers dot extend
141:37 - and as an argument we're going to pass
141:39 - in a new list entirely so here we'll say
141:42 - 4 comma 5 comma 6
141:45 - and then once i hit enter if i were to
141:48 - print out numbers you'll see that it now
141:50 - contains the values 4 5 and 6.
141:52 - so extend takes another list to add
141:55 - extend effectively makes a series of
141:58 - append calls on each of the elements in
142:00 - the new list until all of them have been
142:03 - appended to the original list this
142:05 - operation has a run time of big o of k
142:09 - where k represents the number of
142:11 - elements in the list that we're adding
142:12 - to our existing list
142:14 - the last type of operation we need to
142:16 - consider are delete operations deletes
142:19 - are similar to inserts in that when a
142:22 - delete operation occurs the list needs
142:24 - to maintain correct index values so
142:27 - where an insert shifts every element to
142:29 - the right a delete operation shifts
142:32 - every element to the left
142:34 - just like an insert as well if we delete
142:36 - the first element in the list every
142:38 - single element in the list needs to be
142:40 - shifted to the left
142:42 - delete operations have an upper bound of
142:45 - big o of n also known as a linear
142:47 - runtime now that we've seen how common
142:50 - operations work on a data structure that
142:52 - we're quite familiar with let's switch
142:54 - tracks and build our own data structure
142:58 - [Music]
143:02 - over the next few videos we're going to
143:04 - build a data structure that you may have
143:06 - worked with before a linked list
143:09 - before we get into what a linked list is
143:11 - let's talk about why we build data
143:13 - structures instead of just using the
143:14 - ones that come built into our languages
143:17 - each data structure solves a particular
143:19 - problem
143:20 - we just went over the basics of the
143:22 - array data structure and looked at the
143:24 - cost of common operations that we carry
143:26 - out on arrays
143:27 - we found that arrays were particularly
143:29 - good at accessing reading values happens
143:32 - in constant time but arrays are pretty
143:34 - bad at inserting and deleting both of
143:36 - which run in linear time
143:38 - linked lists on the other hand are
143:40 - somewhat better at this although there
143:42 - are some caveats and if we're trying to
143:44 - solve a problem that involves far more
143:46 - inserts and deletes than accessing a
143:49 - linked list can be a better tool than an
143:51 - array
143:52 - so what is a linked list
143:54 - a linked list is a linear data structure
143:57 - where each element in the list is
143:59 - contained in a separate object called a
144:02 - node a node models two pieces of
144:04 - information an individual item of the
144:06 - data we want to store and a reference to
144:09 - the next node in the list
144:11 - the first node in the linked list is
144:13 - called the head of the list while the
144:15 - last node is called the tail
144:17 - the head and the tail nodes are special
144:20 - the list only maintains a reference to
144:22 - the head although in some
144:23 - implementations it keeps a reference to
144:25 - the tail as well
144:27 - this aspect of linked lists is very
144:29 - important and as you'll see most of the
144:31 - operations on the list need to be
144:33 - implemented quite differently compared
144:34 - to an array
144:36 - the opposite of the head the tail
144:38 - denotes the end of the list
144:40 - every node other than the tail points to
144:42 - the next node in the list but tail
144:45 - doesn't point to anything this is
144:47 - basically how we know it's the end of
144:49 - the list nodes are what are called
144:51 - self-referential objects the definition
144:54 - of a node includes a link to another
144:57 - node and self-referential here means the
144:59 - definition of node includes the node
145:01 - itself linked lists often come in two
145:04 - forms a singly linked list where each
145:06 - node stores a reference to the next node
145:08 - in the list or a doubly linked list
145:11 - where each node stores a reference to
145:13 - both the node before and after if an
145:16 - array is a train with a bunch of cars in
145:19 - order then a linked list is like a
145:21 - treasure hunt
145:22 - when you start the hunt you have a piece
145:24 - of paper with the location of the first
145:26 - treasure you go to that location and you
145:29 - find an item along with a location to
145:31 - the next item of treasure
145:33 - when you finally find an item that
145:35 - doesn't also include a location you know
145:37 - that the hunt has ended
145:39 - now that we have a high level view of
145:40 - what a linked list is let's jump into
145:42 - code and build one together we'll focus
145:45 - on building a singly linked list for
145:46 - this course there are advantages to
145:49 - having a doubly linked list but we don't
145:51 - want to get ahead of ourselves
145:54 - let's start here by creating a new file
145:59 - we're going to put all our code for our
146:00 - linked list so we'll call this linked
146:03 - underscore list
146:04 - dot pi and first we're going to create a
146:08 - class to represent a node
146:12 - say class
146:13 - node
146:15 - now node is a simple object in that it
146:18 - won't model much so first we'll add a
146:21 - data variable
146:23 - it's an instance variable here called
146:25 - data and we'll assign the value none
146:27 - initially
146:28 - and then we'll add one more we'll call
146:30 - this next node and to this we'll assign
146:33 - none as well so we've created two
146:35 - instance variables data to hold on to
146:38 - the data that we're storing and next
146:41 - node to point to the next node in the
146:43 - list
146:44 - now we need to add a constructor to make
146:46 - this class easy to create so we'll add
146:49 - an init
146:50 - method here that takes self and some
146:53 - data to start off
146:55 - and all we're going to do is assign
146:57 - data to that instance variable we
146:59 - created so that's all we need to model
147:02 - node
147:03 - before we do anything else though let's
147:05 - document this so right after the class
147:07 - definition let's create a docs string so
147:10 - three quotes
147:12 - next line and we'll say an object
147:15 - for storing
147:16 - a single
147:18 - node of a linked list
147:22 - and then on the next line we'll say
147:23 - models two attributes
147:28 - data
147:29 - and
147:30 - the link to the next
147:34 - node in the list
147:36 - and then we'll close this doc string off
147:39 - with three more quotation marks okay
147:41 - using the node class is fairly
147:43 - straightforward so we can create a new
147:46 - instance of node with some data to store
147:48 - now the way we're going to do this is
147:49 - we're going to bring up the console
147:51 - and we're going to type out like we've
147:52 - been typing out before python followed
147:55 - by the name of the script that we wrote
147:57 - which is linked list linked underscore
147:59 - list.pi but before we do that we're
148:01 - going to pass an argument to the python
148:03 - command we're going to say dash or
148:05 - python i and then the name of the script
148:09 - linked underscore list dot pi so what
148:11 - this does is this is going to run the
148:14 - python repl
148:16 - the read evaluate print loop in the
148:18 - console but it's going to load the
148:19 - contents of our file into that so that
148:22 - we can use it
148:23 - so i'll hit enter and we have a new
148:25 - instance going and now we can use the
148:27 - node in here so we can say n1
148:29 - equal node
148:31 - and since we defined that constructor we
148:33 - can pass it some data so we'll say 10
148:35 - here
148:36 - now if we try to inspect this object the
148:39 - representation returned isn't very
148:41 - useful
148:42 - which will make things really hard to
148:44 - debug as our code grows so for example
148:46 - if i type out n1 you'll see that
148:49 - we have a valid instance here but it's
148:50 - not very helpful the way it's printed
148:52 - out
148:53 - so we can customize this by adding a
148:56 - representation of the object using the
148:58 - wrapper function now in the terminal
149:00 - still we'll type out exit
149:02 - like that hit enter to exit the console
149:06 - and then down here
149:08 - let's add in some room
149:12 - okay and here we'll say def
149:14 - double underscore
149:16 - wrapper another set of double
149:17 - underscores
149:18 - and then this function takes the
149:20 - argument self
149:21 - and in here we can provide a string
149:23 - representation of what we want printed
149:26 - to the console when we inspect that
149:29 - object inside of it inside of a console
149:32 - so here we'll say return
149:34 - again
149:35 - this is a string representation so
149:36 - inside quotes we'll say
149:38 - node so this represents a node instance
149:41 - and the data it contains here we'll say
149:44 - percent s
149:46 - which is a python way of substituting
149:48 - something into a string string
149:51 - interpolation and outside of the string
149:53 - we can say percent again
149:55 - and here we're saying we want to replace
149:57 - this percent s with
150:00 - self.data okay
150:02 - let's hit save and before we move on
150:04 - let's verify that this works so i'm
150:06 - going to come in here
150:08 - type clear to get rid of everything
150:11 - and then we'll do what we did again and
150:13 - you can just hit the up arrow a couple
150:15 - times to get that command
150:17 - all right so hit enter and now just so
150:19 - you know every time you run this you
150:21 - start off you know from scratch so n1
150:23 - that we created earlier not there
150:24 - anymore so let's go ahead and create it
150:26 - n1 equal node
150:29 - 10
150:30 - and we can type n1 again and hit enter
150:32 - and you have a much better
150:33 - representation now so we can see that we
150:35 - have a node and it contains the data 10.
150:38 - we can also create another one n2 equal
150:40 - node that contains the data 20 and now
150:43 - we can say n1.next n1.nextnode
150:46 - equal n2 so n1 now points to n2 and if
150:51 - we say n1.nextnode
150:53 - you'll see that it points to that node
150:55 - the node containing 20.
150:58 - nodes are the building blocks for a list
151:00 - and now that we have a node object we
151:02 - can use it to create a singly linked
151:05 - list so again i'm going to exit out of
151:06 - this
151:09 - and then go back to the text editor
151:12 - and here we'll create a new class so
151:14 - class
151:15 - linked
151:16 - list
151:17 - the linked list class is going to define
151:20 - a head and this attribute models the
151:23 - only node that the list is going to have
151:25 - a reference to so here we'll say head
151:28 - and we'll assign none initially and then
151:30 - like we did earlier let's create a
151:32 - constructor
151:33 - so double underscore init double
151:35 - underscore this takes self
151:39 - and then inside like before we'll say
151:41 - self dot head
151:43 - equal none this is the same
151:46 - as doing this so we can actually get rid
151:48 - of that and just use the constructor
151:51 - okay so again this head attribute models
151:54 - the only node that the list will have a
151:56 - reference to since every node points to
151:59 - the next node to find a particular node
152:02 - we can go from one node to the next in a
152:04 - process called list traversal
152:07 - so in the class constructor here we've
152:08 - set the default value of head to none so
152:11 - that new lists created are always empty
152:14 - again you'll notice here that i didn't
152:16 - explicitly declare the head attribute at
152:18 - the top of the class definition
152:20 - and don't worry that's not an oversight
152:22 - the self.head in the initializer means
152:25 - that it's still created okay so that's
152:27 - all there is to modeling a linked list
152:30 - now we can add methods that make it
152:32 - easier to use this data structure
152:34 - first a really simple docstring to
152:36 - provide some information
152:38 - so here we'll to create a docstring
152:40 - three quotation marks
152:41 - and then we'll say singly linked list
152:44 - and then close it off
152:46 - a common operation carried out on data
152:49 - structures is checking whether it
152:51 - contains any data or whether it's empty
152:54 - at the moment to check if a list is
152:56 - empty we would need to query these
152:58 - instance variables head and so on every
153:01 - time
153:02 - ideally we would like to not expose the
153:04 - inner workings of our data structure to
153:06 - code that uses it
153:08 - instead let's make this operation more
153:10 - explicit by defining a method so we'll
153:13 - say def is empty
153:16 - and this method takes self as an
153:18 - argument and here we'll say return
153:20 - self.head double equal none
153:24 - all we're doing here is checking to see
153:26 - if head is none
153:27 - if it is this condition evaluates to
153:29 - true which indicates the list is empty
153:32 - now before we end this video let's add
153:34 - one more convenience method to calculate
153:37 - the size of our list the name
153:39 - convenience method indicates that what
153:41 - this method is doing is not providing
153:43 - any additional functionality that our
153:45 - data structure can't handle right now
153:48 - but instead making existing
153:49 - functionality easier to use
153:52 - we could calculate the size of our
153:53 - linked list by traversing it every time
153:56 - using a loop until we hit a tail node
153:59 - but doing that every time is a hassle
154:01 - okay so we'll call this method
154:04 - size and as always it takes self
154:07 - unlike calling len on a python list not
154:10 - to be confused with a linked list which
154:12 - is a constant time operation our size
154:16 - operation is going to run in linear time
154:18 - the only way we can count how many items
154:20 - we have is to visit each node and call
154:23 - next until we hit the tail node
154:26 - so we'll start by getting a reference to
154:28 - the head we'll say current
154:30 - equal self.head let's also define a
154:33 - local variable named count with an
154:36 - initial value of 0 that will increment
154:39 - every time we visit a node once we hit
154:42 - the tail count will reflect the size of
154:44 - that list
154:46 - next we'll define a while loop that will
154:48 - keep going until there are no more nodes
154:51 - so say while current
154:53 - while current is the same as writing out
154:56 - while current does not equal none but
154:59 - it's more succinct so we'll go with this
155:01 - former
155:02 - if the ladder is more precise for you
155:03 - you can go with that
155:05 - now inside this loop we'll increment the
155:07 - count value so count plus equal one
155:10 - plus equal if you haven't encountered it
155:12 - before is the same as writing count
155:13 - equal count plus one so if count is zero
155:16 - initially so it's zero plus one is one
155:19 - and then we'll assign that back to count
155:21 - okay so count plus equal one
155:24 - next we're going to assign the next node
155:26 - in the list to current so current equal
155:30 - current dot next
155:33 - node
155:34 - this way once we get to the tail and
155:36 - call next node current will equal none
155:39 - and the while loop terminates so the end
155:41 - we can return
155:43 - count
155:44 - as you can see we need to visit every
155:46 - node to determine the size meaning our
155:49 - algorithm runs in linear time so let's
155:51 - document this
155:52 - up in our docs string which we'll add
155:54 - now to size
155:56 - we'll say
155:57 - returns
155:58 - the number of nodes in the list
156:02 - takes
156:04 - linear time
156:06 - let's take a break here we can now
156:08 - create lists check if they're empty and
156:10 - check the size
156:12 - in the next video let's start
156:13 - implementing some common operations
156:16 - at the moment we can create an empty
156:18 - list but nothing else let's define a
156:20 - method to add data to our list
156:23 - technically speaking there are three
156:24 - ways we can add data to a list
156:26 - we can add nodes at the head of the list
156:28 - which means that the most recent node we
156:30 - created will be the head and the first
156:32 - node we created will be the tail
156:34 - or we could flip that around most recent
156:36 - nodes are the tail of the list and the
156:38 - first node to be added is the head i
156:41 - mentioned that one of the advantages of
156:42 - linked lists over arrays is that
156:44 - inserting data into the list is much
156:46 - more efficient than to the array
156:48 - this is only true if we're inserting at
156:50 - the head or the tail
156:52 - technically speaking this isn't an
156:54 - insert and you'll often see this method
156:56 - called add prepend if the data is added
156:59 - to the head or append if it's added to
157:01 - the tail
157:02 - a true insert is where you can insert
157:04 - the data at any point in the list which
157:07 - is our third way of adding data we're
157:09 - going to circle back on that if we
157:11 - wanted to insert at the tail then the
157:13 - list needs a reference to the tail node
157:15 - otherwise we would have to start at the
157:17 - head and walk down the length of the
157:19 - list or traverse it to find the tail
157:22 - since our list only keeps a reference to
157:24 - the head we're going to add new items at
157:26 - the head of the list
157:29 - now before we add our new method i
157:32 - forgot that i didn't show you in the
157:33 - last video how to actually use the code
157:36 - we just added and how to check every
157:38 - time you know when we add new code that
157:39 - it works correctly
157:41 - so like before we're gonna bring up the
157:43 - console and here we're gonna say python
157:45 - dash i
157:47 - linked underscore list dot pi which
157:50 - should load it
157:51 - load the contents of our file
157:53 - and now we'll start here by creating a
157:55 - linked list so l equal linked list
157:59 - and then we'll use a node so n1 equal
158:03 - node
158:04 - with the value 10
158:06 - and now we can assign n1 to the nodes or
158:09 - to the linked lists head attribute so l1
158:12 - dot head equal n1
158:14 - and then
158:15 - we can see if size works correctly so if
158:18 - we call l1 dot size and since this is a
158:21 - method we need a set of parentheses at
158:22 - the end
158:23 - and enter you'll see that we get back
158:25 - one correctly okay so it works
158:29 - now let's add our new method which we're
158:31 - going to call add
158:33 - add is going to accept some data to add
158:36 - to the list inside of a node
158:40 - so we'll say def
158:41 - add
158:42 - and every python method takes self as an
158:45 - argument and then we want to add some
158:47 - data to this node so we're going to say
158:49 - data for the second argument
158:51 - inside the method first we'll create a
158:53 - new node to hold on to the data so new
158:56 - underscore node equal
158:59 - node with the data
159:00 - before we set the new node as the head
159:02 - of the list we need to point the new
159:05 - node's next property at whatever node is
159:08 - currently at head this way when we set
159:10 - the new node as the head of the list we
159:12 - don't lose a reference to the old head
159:15 - so new underscore node dot next node
159:20 - equal self.head
159:22 - now if there was no node at head this
159:25 - correctly sets next node to none
159:29 - now we can set the new node as the head
159:31 - of the node so say self.head equal
159:35 - new underscore node because the insert
159:39 - operation is simply a reassignment of
159:41 - the head and next node properties this
159:44 - is a constant time operation so let's
159:46 - add that in as a docs string
159:50 - first what the method does so it adds a
159:52 - new node
159:55 - containing data
159:57 - at the head of the list
160:02 - this operation takes
160:04 - constant time which is our best case
160:07 - scenario
160:08 - okay let's test this out so i'm going to
160:10 - bring the console back up we'll exit out
160:13 - of
160:14 - our current reply
160:16 - and we'll load
160:18 - the contents of the file again
160:20 - and now we don't need to create a node
160:22 - like we did earlier so we can say l
160:25 - equal linked
160:26 - list
160:28 - l.add one
160:30 - okay let's see if this works we'll call
160:32 - size
160:33 - and if it worked
160:34 - the linked list should now have a size
160:37 - of one there we go you can also do
160:39 - l.add2
160:40 - l.add three
160:44 - and l dot size should now be three there
160:47 - we go now if we i were to type l and
160:49 - just hit print
160:51 - again what we get in the repel is
160:53 - nothing useful
160:54 - so like before we'll implement the
160:57 - wrapper function for our linked list
161:00 - now
161:01 - i'm just going to copy paste this in and
161:03 - we'll walk through it
161:05 - okay so this is what our implementation
161:08 - of wrapper looks like for the linked
161:09 - list object you can grab this code from
161:12 - the notes section of this video
161:15 - okay so at the top you'll see a docs
161:16 - string where it says it returns a string
161:18 - representation of the list and like
161:20 - everything we need to do with a linked
161:22 - list we need to traverse it so this is
161:24 - going to take linear time we start by
161:27 - creating an empty list now i need to
161:29 - distinguish this is a python list not a
161:31 - linked list so we create an empty list
161:34 - called nodes and two nodes we're going
161:36 - to add strings that have a description
161:38 - that provide a description of each node
161:41 - but we're not going to use the
161:42 - description that we implemented in the
161:44 - node class because we're going to
161:46 - customize it a bit here
161:48 - next we start by assigning self.head to
161:51 - current so we sort of have a pointer to
161:53 - the head node as long as current does
161:56 - not equal none which means we're not at
161:58 - the tail we're going to implement some
162:00 - logic
162:00 - so in the first scenario if the node
162:03 - assigned to current is the same as the
162:05 - head
162:06 - then we're going to append this string
162:08 - to our nodes list
162:11 - and the string is simply
162:13 - going to say that hey this is a head
162:14 - node and it contains some data which
162:17 - will extract using current.data
162:19 - next scenario is if the node assigned to
162:23 - current's next node is none meaning
162:26 - we're at the tail node then we'll assign
162:28 - a different kind of string so it's the
162:29 - same as earlier except we're saying tail
162:31 - here and then finally in any other
162:33 - scenario which means we're not at the
162:35 - head or not of the tail we'll simply
162:37 - print the node's value inside and again
162:40 - we'll extract it using current.data with
162:42 - every iteration of the loop we'll move
162:44 - current forward by calling
162:46 - current.nextnode and reassigning it
162:48 - and then at the very end when we're done
162:50 - we'll join all the strings that are
162:52 - inside the nodes list together using the
162:55 - python join method and we'll say that
162:59 - with every join so when you join these
163:01 - two strings together to make one string
163:03 - you need to put this set of characters
163:05 - in between all right so let's see what
163:07 - this looks like
163:08 - so i'm going to come down here exit out
163:10 - of the console again
163:11 - clear it out
163:13 - load the contents of the file again and
163:16 - let's try that so we'll say l equal
163:18 - linked list
163:20 - all right so l dot add one l dot add two
163:25 - l dot add three that seems enough
163:28 - and then now if i type out l and hit
163:29 - enter we get a nice string
163:32 - representation of the list so you can
163:34 - see that we add every new node to the
163:36 - head so we added one first one ends up
163:39 - being the tail because it keeps getting
163:40 - pushed out
163:42 - then two and then finally three so three
163:44 - is at the head
163:46 - so far we've only implemented a single
163:48 - method which functions much like the
163:50 - append method on a python list or an
163:53 - array
163:54 - except it adds it to the start of the
163:57 - linked list it pre-pens it
163:59 - like append this happens in constant
164:02 - time in the next video let's add the
164:04 - ability to search through our list for
164:07 - the search operation we're going to
164:08 - define a method that takes a value to
164:10 - search for and returns either the node
164:13 - containing the value if the value is
164:15 - found or none if it isn't
164:17 - so right after actually you know what
164:19 - we'll make sure wrapper is the last
164:21 - function our last method
164:24 - in our class so we'll add it above it so
164:26 - here we'll say def search
164:28 - self
164:30 - and then key
164:32 - in the last video we implemented the
164:34 - wrapper method to provide a string
164:36 - representation of the list
164:38 - so we're going to use similar logic here
164:40 - to implement the search function we'll
164:43 - start by setting a local variable
164:45 - current to point to the head of the list
164:49 - while the value assigned to current is a
164:51 - valid node that is it isn't none
164:54 - we'll check if the data on that node
164:57 - matches the key that we're searching for
164:59 - so while current
165:02 - we'll say if
165:03 - current.data
165:05 - is the key
165:07 - then we'll return current
165:09 - if it does match we'll go ahead and
165:11 - return it like we've done here but if it
165:14 - doesn't we'll assign the next node in
165:16 - the list to current and check again so
165:19 - say else
165:21 - current equal current dot next node
165:26 - once we hit the tail node and haven't
165:29 - found the key
165:30 - current gets set to none and the while
165:32 - loop exits
165:34 - at this point we know the list doesn't
165:36 - contain the key so we can return
165:39 - none
165:40 - okay that completes the body of our
165:42 - method
165:43 - let's add a docs string to document this
165:46 - so up at the top we'll say search
165:49 - for the first node
165:52 - containing data that matches
165:57 - the key
165:58 - now this is important because if our
166:00 - linked list contains more than one node
166:02 - with the same value it doesn't matter
166:04 - we're going to return the first one with
166:06 - this implementation
166:08 - we'll also say here that it returns the
166:10 - node or none
166:13 - if not found
166:16 - in the worst case scenario we'll need to
166:18 - check every single node in the list
166:20 - before we find the key or fail and as a
166:23 - result this operation runs in linear
166:25 - time so i'll say takes
166:28 - o of n or linear time
166:33 - so far we haven't seen anything that
166:35 - indicates this data structure has any
166:37 - advantage over an array or a python list
166:40 - but we knew that
166:41 - i mentioned the strength of linked lists
166:44 - comes in inserts and deletes at specific
166:47 - positions we'll check that out in the
166:49 - next video but as always before we end
166:52 - this one let's make sure everything
166:54 - works
166:55 - so we'll load the contents of the file
166:57 - again
167:01 - l equal linked
167:02 - list
167:05 - and then we'll say l.add 10
167:08 - l dot add
167:09 - 20 2 doesn't matter l dot add
167:13 - 45 and one more metal dot add
167:16 - 15.
167:17 - now we can say
167:18 - l.search and we need to give it a value
167:20 - so we'll say 45 and this returns a node
167:24 - or none so we'll say n equal
167:27 - and then we'll hit enter if this works
167:30 - n should be a node
167:32 - okay weirdly n
167:35 - does not work here
167:36 - at least it says it's not a node which
167:38 - means i made a mistake in typing out our
167:40 - code
167:41 - and looking at it immediately it's
167:42 - fairly obvious so this return none needs
167:44 - to be outside of the while loop okay so
167:48 - i'm going to hit save now so make sure
167:49 - it's on the same indentation here which
167:52 - means it's outside the while loop
167:54 - and then we'll run through this again
167:58 - okay so l is linked list
168:02 - l.add 10
168:04 - l dot add 2
168:06 - l.add
168:08 - 45 and what was the last one we did i
168:10 - believe it was 15
168:12 - and now we should be able to say
168:14 - l.search remember we're assigning this
168:17 - to a node to a variable so l.search
168:21 - 45
168:24 - and there you go we get that node back
168:26 - and we can hit l
168:27 - and we'll see a representation of our
168:29 - list
168:30 - okay so again in the next video inserts
168:32 - and deletes at specific positions
168:35 - insert operations on linked lists are
168:37 - quite interesting
168:39 - unlike arrays where when you insert an
168:41 - element into the array all elements
168:43 - after the particular index need to be
168:45 - shifted with a linked list we just need
168:48 - to change the references to next on a
168:50 - few nodes and we're good to go
168:52 - since each node points to the next one
168:54 - by swapping out these references we can
168:56 - insert a node at any point in the list
168:59 - in constant time
169:00 - much like binary search though there's a
169:02 - catch
169:03 - to find the node at that position we
169:05 - want to insert we need to traverse the
169:08 - list and get to that point
169:10 - we just implemented our search algorithm
169:12 - for the linked list type and we know
169:14 - that this runs in linear time so while
169:17 - actually inserting is fast finding the
169:19 - position in the list you want to insert
169:21 - it is not
169:23 - this is why i mentioned that there were
169:24 - some caveats to inserting
169:26 - anyway let's see what this looks like in
169:28 - code
169:29 - we'll define a method named insert that
169:32 - takes data to insert along with an index
169:34 - position so we'll do this after search
169:37 - right here
169:39 - say def
169:40 - insert
169:43 - and this takes some data to insert and a
169:46 - position to insert it at
169:50 - you may be thinking wait a minute linked
169:52 - lists don't have index positions right
169:54 - and you're correct but we can mimic that
169:56 - behavior by just counting the number of
169:59 - times we access next node
170:01 - if the index value passed into this
170:03 - argument is 0 that means we want to
170:06 - insert the new node at the head of the
170:08 - list this is effectively the same
170:11 - behavior as calling add which means the
170:13 - logic is the same so we don't need to
170:15 - repeat it we can call the add method we
170:18 - wrote earlier so we'll say if
170:20 - index if index equals 0 or if index is 0
170:25 - then self dot add
170:28 - data
170:29 - if the index is greater than 0 then we
170:32 - need to traverse the list to find the
170:35 - current node at that index
170:37 - so if index is greater than zero
170:40 - now before we do that we need to create
170:43 - a new node containing the data we want
170:45 - to insert so we'll say new equal node
170:48 - with some data
170:50 - i'm going to assign index the argument
170:52 - passed to our function to a local
170:55 - variable named position and the head of
170:57 - the list to a variable named current
171:00 - position
171:01 - equal index
171:04 - current equal self.head
171:07 - every time we call
171:09 - current.nextnode meaning we're moving to
171:11 - the next node in the list we'll decrease
171:14 - the value of position by 1.
171:16 - when position is zero we'll have arrived
171:19 - at the node that's currently at the
171:21 - position we want to insert in
171:23 - in reality though we don't want to
171:24 - decrease it all the way to zero
171:27 - imagine we have a list with five nodes
171:29 - and we want to insert a node at position
171:32 - 3. to insert a node at position 3 we
171:35 - need to modify the nodes at positions 2
171:37 - and 3.
171:39 - node 2's next node attribute is going to
171:41 - point to the new node and the new node's
171:44 - next node attribute will point to node
171:46 - 3.
171:47 - in this way an insert is a constant time
171:49 - operation we don't need to shift every
171:52 - single element we just modify a few next
171:54 - node references
171:56 - in a doubly linked list we can use node
171:59 - 3 to carry out both of these operations
172:02 - node 3 in a doubly linked list would
172:04 - have a reference to node 2 and we can
172:07 - use this reference to modify all the
172:09 - unnecessary links
172:10 - and a singly linked list though which is
172:12 - what we have if we kept decreasing
172:15 - position until we're at 0 we arrive at
172:18 - node 3.
172:19 - we can then set the new node's next node
172:22 - property to point to node 3 but we have
172:24 - no way of getting a reference to node 2
172:27 - which we also need
172:29 - for this reason it's easier to decrease
172:31 - position to just 1 when it equals 1 and
172:34 - stop at node 2. so in here we'll say
172:39 - while
172:40 - position
172:41 - is greater than one
172:43 - now while the position is greater than
172:45 - one we'll keep calling next node and
172:47 - reassigning the current node so current
172:51 - equal node.next
172:53 - node and at the same time we'll
172:55 - decrement position so position
172:58 - equal to position
173:01 - minus one which you can also succinctly
173:04 - write as minus equal
173:06 - one
173:07 - this way when the position equals one
173:10 - the loop exits and current will refer to
173:13 - the node at the position before the
173:15 - insert point so outside the while loop
173:18 - we'll say previous equal current
173:22 - and next equal current dot next
173:25 - node
173:26 - to make things more clear what i've done
173:29 - here is name the node before the new one
173:31 - previous and the node after the new one
173:34 - next
173:35 - all that's left to do now is to insert
173:37 - the new node between previous and next
173:39 - so we'll say previous dot next
173:42 - node
173:44 - equal
173:45 - new
173:46 - and then new dot next node
173:49 - equal next
173:52 - now it seems like there's an issue with
173:53 - variable naming here and i'm most
173:56 - probably conflicting with some globally
173:58 - named next variable so actually go ahead
174:00 - and call this next node
174:02 - and
174:04 - previous node so that we don't mess
174:06 - things up here
174:08 - previous node
174:12 - so the dot next node is obviously the
174:14 - attribute on a node but this is just a
174:16 - local variable let's document this
174:18 - method so up at the top
174:21 - we'll add a docs string and it will say
174:23 - inserts a new node
174:26 - containing
174:27 - data at index
174:30 - position
174:33 - insertion takes
174:35 - constant time
174:39 - but finding the node
174:43 - at the insertion point
174:46 - takes linear
174:50 - time
174:52 - let's add this to the next line
174:54 - there we go
174:55 - and then we'll say therefore it takes an
174:58 - overall
175:00 - linear time
175:03 - this is why even though we can easily
175:05 - insert a new node without having to
175:08 - shift the rest ultimately adding to
175:10 - either the head or the tail if you have
175:12 - a reference is much more efficient
175:15 - we have one more operation to add to our
175:17 - linked list that will make it a robust
175:19 - data structure
175:20 - much like inserts removing a node is
175:22 - actually quite fast and occurs in
175:24 - constant time but to actually get to the
175:26 - node that we want to remove and modify
175:29 - the next connections we need to traverse
175:31 - the entire list in our worst case so in
175:33 - the worst case this takes linear time
175:36 - let's add this operation to our data
175:37 - structure
175:39 - there are two ways we can define the
175:41 - remove method one where we provide a key
175:44 - to remove as an argument and one where
175:46 - we provide an index now in the former
175:49 - the key refers to the data the node
175:52 - stores so in order to remove that node
175:54 - we would first need to search for data
175:56 - that matches the key i'm going to
175:58 - implement that first method which we'll
176:00 - call remove and i'll leave it up to you
176:02 - to get some practice in and implement a
176:05 - remove at index method to complete our
176:07 - data structure so we'll add this after
176:09 - the insert method right here
176:14 - remove
176:15 - is going to accept a key which we'll
176:17 - need to search for before we can remove
176:20 - a node earlier we defined a search
176:22 - method that found a node containing data
176:24 - that matches a key but we can't use that
176:27 - method as is for the implementation of
176:29 - remove when we remove a node much like
176:32 - the insert operation we need to modify
176:35 - the next node references
176:37 - the node before the match needs to point
176:39 - to the node after the match
176:41 - if we use the search method we defined
176:43 - earlier we get the node we want to
176:45 - remove as a return value but because
176:48 - this is a singly linked list we can't
176:51 - obtain a reference to the previous node
176:53 - like i said earlier if this was a doubly
176:55 - linked list we could use the search
176:57 - method since we would have a reference
176:59 - to that previous node
177:01 - we'll start here by setting a local
177:03 - variable named current to point to the
177:06 - head let's also define a variable named
177:09 - previous
177:10 - that will set to none to keep track of
177:13 - the previous node as we traverse the
177:15 - list
177:16 - finally let's declare a variable named
177:18 - found that we'll set to false
177:21 - found is going to serve as a stopping
177:23 - condition for the loop that we'll define
177:26 - we'll use the loop to keep traversing
177:28 - the linked list as long as found is
177:31 - false meaning we haven't found the key
177:33 - that we're looking for once we've found
177:35 - it we'll set found to true and the loop
177:37 - terminates so let's set up our loop so
177:39 - we'll say while current
177:41 - and
177:42 - not
177:43 - found
177:46 - here we're defining a while loop that
177:48 - contains two conditions
177:50 - first we tell the loop to keep iterating
177:53 - as long as current does not equal none
177:57 - when current equals none this means
177:59 - we've gone past the tail node and the
178:01 - key doesn't exist
178:03 - the second condition asks the loop to
178:05 - keep evaluating as long as not found
178:08 - equals true now this might be tricky
178:11 - because it involves a negation here
178:13 - right now found is set to false so not
178:16 - found not false equals true this not
178:20 - operator flips the value
178:22 - when we find the key and we set found to
178:25 - true
178:26 - not true not found we'll equal false
178:29 - then and the loop will stop
178:31 - the end in the while loop means that
178:34 - both conditions current being a valid
178:36 - node and not found equalling true both
178:39 - have to be true
178:40 - if either one of them evaluates to false
178:42 - then the loop will terminate
178:44 - now inside the loop there are three
178:45 - situations that we can run into
178:48 - first the key matches the current node's
178:50 - data and current is still at the head of
178:53 - the list
178:54 - this is a special case because the head
178:56 - doesn't have a previous node and it's
178:58 - the only node being referenced by the
179:00 - list let's handle this case so we'll say
179:03 - if current.data
179:05 - double equals the key and current
179:09 - is self.head which you can write out as
179:12 - current equal self.head or current is
179:14 - self.head
179:15 - now if we hit this case
179:18 - we'll indicate that we found the key by
179:20 - setting found to true
179:23 - and then this means that on the next
179:24 - pass
179:25 - this is going to evaluate to false
179:27 - because not true will be false
179:31 - and then the loop terminates once we do
179:33 - that we want to remove the current node
179:35 - and since it's the head node all we need
179:38 - to do is point head to the second node
179:41 - in the list which we can get by
179:43 - referencing the next node attribute on
179:45 - current self.head equal current.nextnode
179:50 - so when we do this there's nothing
179:52 - pointing to that first node so it's
179:54 - automatically removed the next scenario
179:57 - is when the key matches data in the node
179:59 - and it's a node that's not the head
180:02 - so here we'll say else if current dot
180:05 - data equal key
180:08 - if the current node contains the key
180:10 - we're looking for we need to remove it
180:13 - to remove the current node we need to go
180:14 - to the previous node and modify its next
180:17 - node reference to point to the node
180:19 - after current
180:21 - but first we'll set found
180:23 - to true
180:24 - and then we'll switch out the references
180:26 - so
180:27 - previous.nextnode
180:29 - equal
180:30 - current.nextnode
180:32 - so far we haven't written any code to
180:34 - keep track of the previous node
180:37 - we'll do that in our else case here
180:40 - so if we hit the else case it means that
180:42 - the current node we're evaluating
180:44 - doesn't contain the data that matches
180:46 - the key so in this case we'll make
180:48 - previous point to the current node and
180:50 - then set current to the next node so
180:52 - previous equal current
180:55 - and current equal current.nextnode
180:59 - and that's it for the implementation of
181:01 - remove
181:02 - now we're not doing anything at the
181:04 - moment with the node we're removing but
181:06 - it's common for remove operations to
181:08 - return the value being removed so at the
181:10 - bottom
181:12 - outside the while loop
181:13 - let's return
181:16 - current
181:17 - and with that we have a minimal
181:18 - implementation of a linked list and your
181:21 - first custom data structure how cool is
181:24 - that
181:24 - there's quite a bit we can do here to
181:26 - improve our data structure particularly
181:29 - in making it easy to use but this is a
181:31 - good place to stop
181:33 - before we move on to the next topic
181:35 - let's document our method so the top
181:38 - another docs string
181:39 - and here we'll say removes node
181:42 - containing data that matches the key
181:46 - also it returns the node or none
181:50 - if the key doesn't exist
181:53 - and finally this takes
181:56 - linear time because in the worst case
181:57 - scenario we need to search the entire
181:59 - list
182:01 - if you'd like to get in some additional
182:02 - practice implementing functionality for
182:04 - linked lists two methods you can work on
182:07 - are remove it index and node at index to
182:11 - allow you to easily delete or read
182:14 - values in a list at a given index
182:17 - now that we have a linked list let's
182:19 - talk about where you can use them the
182:21 - honest answer is not a lot of places
182:24 - linked lists are really useful
182:26 - structures to build for learning
182:28 - purposes because they're relatively
182:29 - simple and are a good place to start to
182:32 - introduce the kinds of operations we
182:34 - need to implement for various data
182:36 - structures it is quite rare however that
182:38 - you will need to implement a linked list
182:40 - on your own
182:42 - there are typically much better and by
182:44 - that i mean much more efficient data
182:46 - structures that you can use
182:48 - in addition many languages like java for
182:50 - example provide an implementation of a
182:52 - linked list already
182:54 - now that we have a custom data structure
182:57 - let's do something with it let's combine
182:59 - the knowledge we have and look at how a
183:01 - sorting algorithm can be implemented
183:04 - across two different data structures
183:07 - [Music]
183:11 - now that we've seen two different data
183:12 - structures let's circle back and apply
183:14 - what we know about algorithms to these
183:16 - new concepts
183:17 - one of the first algorithms you learned
183:19 - about was binary search and we learned
183:21 - that with binary search there was one
183:23 - precondition the data collection needs
183:25 - to be sorted
183:26 - over the next few videos let's implement
183:28 - the merge sort algorithm which is one of
183:30 - many sorting algorithms on both arrays
183:33 - or python lists and the singly linked
183:35 - list we just created
183:37 - this way we can learn a new sorting
183:39 - algorithm that has real world use cases
183:41 - and see how a single algorithm can be
183:44 - implemented on different data structures
183:46 - before we get into code let's take a
183:48 - look at how merge sort works
183:50 - conceptually and we'll use an array to
183:52 - work through this
183:54 - we start with an unsorted array of
183:56 - integers and our goal is to end up with
183:58 - an array sorted in ascending order
184:01 - merge sort works like binary sort by
184:04 - splitting up the problem into sub
184:06 - problems but it takes the process one
184:08 - step further
184:09 - on the first pass we're going to split
184:11 - the array into two smaller arrays now in
184:14 - binary search one of these subarrays
184:16 - would be discarded but that's not what
184:18 - happens here
184:19 - on the second pass we're going to split
184:20 - each of those subarrays into further
184:23 - smaller evenly sized arrays and we're
184:25 - going to keep doing this until we're
184:27 - down to single element arrays
184:30 - after that the merge sort algorithm
184:32 - works backwards repeatedly merging the
184:34 - single element arrays and sorting them
184:37 - at the same time
184:39 - since we start at the bottom by merging
184:41 - to single element arrays we only need to
184:44 - make a single comparison to sort the
184:46 - resulting merge array by starting with
184:49 - smaller arrays that are sorted as they
184:51 - grow
184:52 - merge sort has to execute fewer sort
184:54 - operations than if it sorted the entire
184:57 - array at once
184:58 - solving a problem like this by
185:00 - recursively breaking down the problem
185:02 - into subparts until it is easily solved
185:05 - is an algorithmic strategy known as
185:07 - divide and conquer but instead of
185:09 - talking about all of this in the
185:10 - abstract let's dive into the code this
185:13 - way we can analyze the runtime as we
185:15 - implement it
185:17 - for our first implementation of merge
185:19 - sort we're going to use an array or a
185:21 - python list
185:23 - while the implementation won't be
185:24 - different conceptually for a linked list
185:27 - we will have to write more code because
185:29 - of list traversal and how nodes are
185:31 - arranged so once we have these concepts
185:33 - squared away we'll come back to that
185:36 - let's add a new file here
185:39 - we'll call this merge underscore sort
185:43 - dot pi
185:45 - in our file let's create a new function
185:47 - named merge sort that takes a list and
185:50 - remember when i say list unless i
185:52 - specify linked list i mean a python list
185:55 - which is the equivalent of an array so
185:57 - we'll say def
185:59 - merge underscore sort
186:01 - and takes a list
186:03 - in the introduction to algorithms course
186:06 - we started our study of each algorithm
186:08 - by defining the specific steps that
186:10 - comprise the algorithm
186:12 - let's write that out as a docstring
186:14 - in here the steps of the algorithm so
186:16 - that we can refer to it right in our
186:18 - code
186:20 - this algorithm is going to sort the
186:22 - given list in an ascending order so
186:24 - we'll start by putting that in here as a
186:26 - simple definition
186:28 - sorts a list in ascending
186:32 - order
186:33 - there are many variations of merge sort
186:36 - and in the one we're going to implement
186:38 - we'll create and return a new sorted
186:41 - list other implementations will sort the
186:43 - list we pass in and this is less typical
186:46 - in an operation known as sort in place
186:50 - but i think that returning a new list
186:51 - makes it easier to understand the code
186:54 - now these choices do have implications
186:56 - though and we'll talk about them as we
186:58 - write this code
187:00 - for our next bit of the docs string
187:02 - let's write down the output of this
187:03 - algorithm so returns
187:06 - a new
187:07 - sorted list
187:09 - merge sort has three main steps
187:12 - the first is the divide step where we
187:14 - find the midpoint of the list so i'll
187:17 - say divide
187:19 - find the mid point of the list and
187:22 - divide
187:26 - into sub-lists
187:29 - the second step is the conquer step
187:31 - where we sort the sub-list that we
187:33 - created in the divide step so we'll say
187:35 - recursively
187:37 - sort the sub-lists created in previous
187:42 - step
187:43 - and finally the combine the combined
187:45 - step where we merge these recursively
187:48 - sorted sub-lists back into a single list
187:51 - so merge the sorted sub-lists
187:55 - created in previous
187:58 - step
187:59 - when we learned about algorithms we
188:01 - learned that a recursive function has a
188:04 - basic pattern first we start with a base
188:07 - case that includes a stopping condition
188:10 - after that we have some logic that
188:12 - breaks down the problem and recursively
188:14 - calls itself
188:16 - our stopping condition is our end goal a
188:18 - sorted array
188:20 - now to come up with a stopping condition
188:22 - or a base case we need to come up with
188:25 - the simplest condition that satisfies
188:28 - this end result
188:29 - so there are two possible values that
188:31 - fit a single element list or an empty
188:35 - list
188:36 - now in both of these situations we don't
188:38 - have any work to do
188:40 - if we give the merge sort function an
188:42 - empty list or a list with one element
188:44 - it's technically already sorted we call
188:46 - this naively sorting so let's add that
188:49 - as our stopping condition we'll say if
188:51 - len list if the length of the list is
188:54 - less than or equal to one
188:57 - then we can return the list
188:59 - okay so this is a stopping condition
189:02 - and now that we have a stopping
189:04 - condition we can proceed with the list
189:06 - of steps
189:09 - first we need to divide the list into
189:11 - sub lists
189:13 - to make our functions easier to
189:14 - understand we're going to put our logic
189:16 - in a couple different functions instead
189:18 - of one large one so i'll say it left
189:21 - half
189:23 - comma right half
189:25 - equal
189:27 - split
189:28 - list so here we're calling a split
189:31 - function that splits the list we pass in
189:34 - and returns two lists split at the
189:36 - midpoint because we're returning two
189:38 - lists we can capture them in two
189:40 - variables
189:42 - now you should know that this split
189:44 - function is not something that comes
189:45 - built into python this is a global
189:47 - function that we're about to write
189:50 - next is the conquer step where we sort
189:52 - each sub-list and return a new sorted
189:55 - sub-list
189:56 - so we'll say left equal
189:59 - merge sort
190:01 - left half
190:04 - and right equal merge sort
190:09 - right half
190:11 - this is the recursive portion of our
190:13 - function so here we're calling merge
190:15 - sort on this divided sub list so we
190:18 - divide the list into two here and then
190:19 - we call merge sort on it again
190:22 - this further splits that sublist into
190:25 - two in the next pass through of merge
190:27 - sort this is going to be called again
190:29 - and again and again until we reach our
190:32 - stopping condition where we have single
190:35 - element lists or empty lists
190:37 - when we've subdivided until we cannot
190:39 - divide any more then we'll end up with a
190:42 - left and a right half
190:44 - and we can
190:45 - start merging backwards so we'll say
190:48 - return
190:49 - merge
190:51 - left and right
190:53 - that brings us to the combined step
190:55 - once two sub-lists are sorted and
190:57 - combined we can return it
191:00 - now obviously none of these functions
191:02 - merge merge sort well merge sort is
191:04 - written but merge and split haven't been
191:06 - written so all we're going to do here if
191:08 - we run it is raise an error so in the
191:10 - next video let's implement the split
191:12 - operation
191:14 - the first bit of logic we're going to
191:16 - write is the divide step of the
191:17 - algorithm this step is fairly
191:19 - straightforward and only requires a few
191:21 - lines of code but is essential to get
191:23 - the sorting process going
191:25 - all right so as we saw earlier we're
191:27 - going to call the function for the
191:28 - divide step split so we'll say def split
191:32 - and split is going to take as an
191:34 - argument a list to split up
191:37 - let's document how this function works
191:40 - so we'll say
191:42 - divide the unsorted list at midpoint
191:47 - into sub lists and it's always good to
191:50 - say what we're returning as well so
191:52 - we'll say returns to sub-lists
191:56 - left and right
192:00 - all right so the first step is to
192:01 - determine the midpoint of this list of
192:04 - this array
192:05 - we're going to use the floor division
192:07 - operator for this
192:09 - floor division carries out a division
192:11 - operation and if we get a non-integer
192:13 - value like 2.5 back it just gets rounded
192:16 - down to two we'll define the midpoint to
192:19 - be the length of the list divided by two
192:22 - and then rounded down
192:24 - so
192:25 - lan list
192:27 - and using the
192:28 - two forward slashes for the floor
192:30 - division operator we'll put number two
192:33 - after it
192:34 - okay once we have the midpoint we can
192:36 - use the slicing notation in python to
192:40 - extract portions of the list we want to
192:42 - return
192:43 - for instance we can define left
192:47 - as the left sub-list that goes all the
192:49 - way from the start of the list
192:52 - all the way up to the midpoint without
192:54 - including the midpoint
192:56 - now over here we're using the slicing
192:59 - syntax where it's like using the you
193:02 - know subscript notation to access a
193:04 - value from a list but instead we give
193:06 - two index values as a start and stop
193:09 - if we don't include a start value as
193:11 - i've done here python interprets that as
193:14 - starting from the zeroth index or the
193:16 - start of the list now similarly we can
193:19 - define right
193:20 - [Music]
193:22 - to be values on the right of the
193:24 - midpoint so starting at the midpoint and
193:27 - going all the way up to the end of the
193:29 - list
193:30 - so a couple things to note as i said
193:33 - earlier when you don't include the
193:34 - starting index it interprets it as to
193:36 - start at the very beginning of the list
193:39 - the index you give as the stopping
193:40 - condition that value is not included in
193:43 - the slice so over here we're starting at
193:46 - the very beginning of list and we go all
193:48 - the way up to midpoint but not including
193:50 - midpoint and then right starts at
193:53 - midpoint so it includes that value and
193:55 - then goes all the way to the end of the
193:56 - list
193:57 - now once we have these two sub-lists we
194:00 - can return them
194:02 - so we'll return left and right notice
194:05 - that we're returning two values here and
194:07 - then in the merge sort function when we
194:10 - call that split function
194:12 - we're declaring two variables left half
194:14 - and right half to assign so that we can
194:17 - assign these two sub lists to them
194:20 - okay and that's all there is to the
194:22 - split function in the next video let's
194:24 - implement the crucial portion of the
194:26 - merge sort logic
194:28 - once we run the split function
194:30 - recursively over the array we should end
194:32 - up with several single member or empty
194:34 - arrays
194:36 - at this point we need to merge them all
194:37 - back and sort them in the process which
194:39 - is what our merge function is for
194:42 - the merge function is going to take two
194:44 - arrays or lists as arguments and to
194:47 - match the naming conventions we used in
194:49 - the split function we'll call this left
194:51 - and right as well so we'll say def merge
194:55 - takes a left and a right list
194:58 - now like before let's add some
194:59 - documentation to our function so this
195:02 - function merges to lists or arrays
195:07 - sorting them in the process
195:11 - and then it returns a new merged list
195:15 - since our function is going to return a
195:17 - new list let's start by creating one
195:21 - now in the process of merging we need to
195:24 - sort the values in both lists
195:27 - to sort we need to compare values from
195:29 - each array or each list so next let's
195:33 - create two local variables to keep track
195:35 - of index values that we're using for
195:37 - each list
195:39 - so the convention here is i and j so
195:41 - we'll stick to it
195:42 - so i equals 0 j equals 0.
195:45 - as we inspect each value in either list
195:48 - we'll use the variables to keep track of
195:51 - the indexes of those values so we'll use
195:54 - i to keep track of indexes in the left
195:56 - list and j for indexes in the right list
196:00 - when merging we want to keep sorting the
196:02 - values until we've worked through both
196:05 - lists so for our loop let's set up two
196:08 - conditions with an and operator so we'll
196:10 - say while
196:12 - let's just stay up here while i is less
196:15 - than
196:18 - while i is less than the length of the
196:21 - left list
196:22 - and j
196:24 - is less than the length
196:27 - of the right list then we'll keep
196:30 - executing our loop so here we're
196:32 - ensuring that as long as i is less than
196:34 - the length of the left list
196:36 - and the and is important and j is less
196:39 - than the length of the right list we're
196:41 - going to keep executing the code now i
196:44 - and j are both set to zero initially
196:46 - which means that our first comparison
196:48 - operation will be on the first element
196:51 - of each list respectively so we'll say
196:55 - if
196:56 - left i so i zero so this is going to get
196:59 - the first value out of the left list
197:02 - is less than right
197:04 - j
197:05 - and again here
197:07 - j is zero so we're going to get the
197:08 - first value out of the right list now if
197:11 - the value at index i in the left list is
197:14 - less than the value at index j in the
197:16 - right list what do we do well that means
197:19 - the value being compared in left is less
197:22 - than the value in the right and can be
197:24 - placed at position 0 in the new array l
197:28 - that we created earlier so here we'll
197:30 - say l dot append
197:32 - left
197:33 - i
197:35 - since we've read and done something with
197:38 - the value at position i let's increment
197:41 - that value so we move forward to
197:43 - evaluate the next item in the left list
197:49 - i plus one or we can say i plus equal
197:52 - one okay next is an else statement
197:57 - and here we'll say
197:58 - if the value at index i so i don't have
198:01 - to write out the actual logic because
198:04 - it's implied so here we're saying that
198:07 - left the value at left is less than the
198:09 - value at right now in the else clause if
198:11 - the value at so i equal
198:15 - is greater and i haven't written out
198:16 - that condition because it's implied so
198:18 - here we're saying if the value in the
198:20 - left is less than the value in the right
198:22 - so in the else clause it's going to mean
198:24 - that the value in the left is either
198:25 - greater than or equal to the value in
198:28 - the right but when we hit the else
198:30 - clause if the value at index i in the
198:32 - left list is greater
198:34 - then we place the value at index j from
198:38 - the right list at the start of the new
198:40 - one list l
198:42 - and similarly increment j so here we'll
198:45 - say l dot append
198:47 - right
198:48 - j and then j equal j plus one
198:55 - doing this doesn't necessarily mean that
198:57 - in one step we'll have a completely
198:59 - sorted array but remember that because
199:01 - we start with single element arrays and
199:04 - combine with each merge step we will
199:06 - eventually sort all the values more than
199:09 - one time and by the time the entire
199:11 - process is done all the values are
199:13 - correctly sorted
199:15 - now this isn't all we need to do in the
199:17 - merge step however there are two
199:19 - situations we can run into one where the
199:22 - left array is larger than the right and
199:24 - vice versa so this can occur when an
199:27 - array containing an odd number of
199:29 - elements needs to be split so how do you
199:31 - split a three element array or list well
199:34 - the left can have two elements and the
199:36 - right can have one or the other way
199:38 - around
199:38 - in either case our while loop uses an
199:41 - and condition
199:42 - where the variables used to store the
199:44 - indexes need to be less than the length
199:46 - of the lists if the left list is shorter
199:49 - than the right then the first condition
199:51 - returns false and the entire loop
199:53 - returns false because it's an and
199:55 - condition
199:56 - this means that in such an event when
199:58 - the while loop terminates not all the
200:00 - values in the right list will have been
200:02 - moved over to the new combined list
200:05 - so to account for this let's add two
200:07 - more while loops
200:09 - the first while loop is going to account
200:11 - for a situation where the right list is
200:14 - shorter than the left and the previous
200:16 - loop terminated because we reached the
200:19 - end of the right list first
200:21 - so in this case what we're going to do
200:23 - is simply add the remaining elements in
200:25 - the left to the new list
200:27 - we're not going to compare elements
200:28 - because we're going to assume that
200:30 - within a list the elements are already
200:32 - sorted
200:33 - so while i
200:35 - is less than length of left
200:38 - then
200:39 - it's the same logic l dot append left
200:43 - i
200:44 - and i
200:46 - plus equal one
200:48 - so the while loop is going to have the
200:50 - similar condition keep the loop going
200:52 - until it's at the last index
200:54 - inside the body we're incrementing the
200:56 - index with every iteration of the loop
200:58 - our final loop accounts for the opposite
201:00 - scenario where the left was shorter than
201:03 - the right
201:04 - the only difference here is that we're
201:05 - going to use the variable j along with
201:07 - the right list so we'll say while j
201:10 - is less than length of right
201:14 - l dot append
201:16 - right
201:17 - j
201:20 - and j plus equal one okay let's stop
201:23 - here in the next video let's test out
201:26 - merge sort make sure our code is running
201:28 - correctly and everything is written well
201:30 - and then we'll wrap up this stage by
201:31 - documenting our code and evaluating the
201:34 - run time of our algorithm in the last
201:36 - video we completed our implementation
201:39 - for the merge sort algorithm but we
201:41 - didn't test it in any way let's define a
201:43 - new list at the bottom that contains
201:45 - several numbers
201:47 - you can put whatever you want in there
201:48 - but make sure that the numbers are not
201:50 - in order i'll call mine a list
201:55 - and in here
201:57 - we'll say 54
201:58 - 26 or 62 doesn't matter 93 17
202:04 - 77
202:05 - 31
202:07 - just add enough so that you can make out
202:10 - that it's sorted okay next we're going
202:12 - to call the merge sort algorithm
202:17 - and pass in our list let's assign this
202:19 - to some variables so we'll say l equal
202:21 - merge
202:22 - underscore sort
202:24 - a list
202:27 - and then if it works correctly we should
202:29 - be able to print this list and see what
202:32 - it looks like so i'm going to hit save
202:34 - down here in the console we'll tap out
202:35 - python
202:37 - merge sort dot pi
202:39 - and before i hit enter i actually
202:41 - noticed i made an error in the last
202:42 - video but i'll hit enter anyway and you
202:44 - should see the error pop up okay so what
202:47 - i forgot to do which is a pretty crucial
202:49 - part of our algorithm is in the merge
202:52 - function i forgot to return the list
202:54 - containing the sorted numbers after
202:56 - carrying out all this logic
202:58 - so here at the bottom
203:00 - we'll say return
203:02 - l
203:03 - all right we'll save again
203:05 - and now we'll clear this out and try
203:08 - that one more time
203:10 - and there we go
203:11 - you should see a sorted list printed out
203:14 - we can write out a more robust function
203:16 - to test this because with bigger arrays
203:18 - visually evaluating that printed list
203:20 - won't always be feasible so bring this
203:23 - back down
203:25 - let's get rid of this
203:27 - and we'll call our
203:29 - function verify sorted
203:32 - and this will take a list
203:35 - first we're going to check inside the
203:37 - body of the function we'll check the
203:38 - length of the list
203:40 - if the list is a single element list or
203:44 - an empty list we don't need to do any
203:46 - unnecessary work because remember it is
203:48 - naively sorted so we'll say if n
203:51 - equals 0 or
203:54 - if n equals
203:56 - 1
203:57 - then we'll return true we've verified
203:59 - that it's sorted
204:00 - now to conclude our function we're going
204:02 - to write out one line of code that will
204:04 - actually do quite a bit of work
204:06 - so first we'll say return
204:09 - list
204:10 - zero so we'll take the first element out
204:12 - of the list and we'll compare and see if
204:14 - that's less than the second element in
204:17 - the list okay so first we'll check that
204:20 - the first element in the list is less
204:22 - than the second element in the list
204:24 - this returns either true or false so we
204:26 - can return that directly
204:28 - but this isn't sufficient if it were we
204:31 - could trick the verify function by only
204:33 - sorting the first two elements in the
204:34 - list
204:35 - so to this return statement we're going
204:37 - to use an and operator to add on one
204:41 - more condition for this condition we're
204:43 - going to make a recursive function call
204:46 - back to verify
204:48 - sorted
204:50 - and for the argument we're going to pass
204:52 - in the list
204:53 - going from the second element all the
204:56 - way
204:57 - to the end let's visualize how this
204:59 - would work
205:00 - we'll use a five element list as an
205:02 - example so we'll call verify sorted and
205:05 - pass in the entire list
205:06 - this list is not one or zero elements
205:09 - long so we skip that first if statement
205:12 - there's only one line of code left in
205:14 - the function and first we check that the
205:16 - element at index 0 is less than the
205:18 - element at index 1. if this is false the
205:21 - function returns immediately with a
205:23 - false value
205:25 - an and operator requires both conditions
205:28 - to be true for the entire line of code
205:30 - to return true
205:31 - since the first condition evaluates to
205:33 - false we don't need to bother evaluating
205:35 - the second the second condition is a
205:38 - recursive call with a sub-list
205:41 - containing elements from the original
205:42 - list starting at position 1 and going to
205:45 - the end
205:46 - so on the second call again we can skip
205:48 - that first if statement and proceed to
205:50 - check whether the value at element 0 is
205:53 - less than the value at element 1.
205:55 - remember that because this list is a
205:57 - sub-list of the original starting at the
205:59 - element that was the second element in
206:01 - the original list
206:02 - by comparing the elements at position 0
206:05 - and 1 in the sub list we're effectively
206:08 - comparing the elements at position 1 and
206:10 - 2 in the original list with each
206:13 - recursive call as we create new sub
206:16 - lists that start at index position 1
206:19 - we're able to check the entire list
206:21 - without having to specify any checks
206:23 - other than the first two elements
206:26 - since this is a recursive function it
206:28 - means we need a stopping condition and
206:30 - we have it already it's that first if
206:32 - condition
206:34 - as we keep making sub lists once we
206:36 - reach a single element list that element
206:38 - is already sorted by definition so we
206:41 - can return true since this recursive
206:43 - function call is part of an and
206:45 - condition
206:46 - it means that every single recursive
206:49 - call has to return true all the way back
206:51 - to the beginning for our top level
206:53 - function to return true and for the
206:56 - function to say yes this is sorted
206:58 - now we could have easily done this using
207:00 - an iterative solution and a for loop but
207:02 - this way you get another example of
207:04 - recursion to work through and understand
207:07 - so let's use this function
207:08 - at the bottom we'll say print
207:11 - verify sorted and first we'll pass in a
207:15 - list
207:16 - oops we got rid of that didn't we
207:19 - okay let me write it out again so a list
207:23 - equal
207:24 - and i think i have those original
207:25 - numbers here somewhere so we'll say 54
207:29 - 26 93
207:36 - okay and then we assigned to l the
207:39 - result of calling merge
207:41 - sort
207:42 - on a list
207:44 - okay so now here we're going to use the
207:46 - verify sorted function
207:48 - and we'll check first that a list is
207:51 - sorted that should return false and then
207:53 - we'll check the same call on we'll pass
207:56 - an l and this should return true
207:59 - okay so now at the bottom here in the
208:01 - console
208:03 - we'll call python merge sort dot pi and
208:05 - there we go it returned false for a list
208:08 - meaning it's not sorted but l is sorted
208:11 - cool so our merge sort function works in
208:14 - the next video let's talk about the cost
208:16 - of this algorithm
208:18 - if we go back to the top level the merge
208:21 - sort function what is the run time of
208:23 - this function look like and what about
208:25 - space complexity how does memory usage
208:27 - grow as the algorithm runs
208:29 - to answer those questions let's look at
208:31 - the individual steps starting with the
208:33 - split function in the split function all
208:36 - we're doing is finding the midpoint of
208:38 - the list and splitting the list at the
208:40 - midpoint
208:41 - this seems like a constant time
208:43 - operation but remember that the split
208:45 - function isn't called once it's called
208:47 - as many times as we need it to to go
208:50 - from the initial list down to a single
208:52 - element list
208:53 - now this is a pattern we've seen a
208:55 - couple times now and we know that
208:57 - overall this runs in logarithmic time so
209:00 - let's add that as a comment so here i'll
209:03 - say
209:04 - takes
209:05 - overall
209:06 - big o of log
209:08 - n time now there's a caveat here but
209:11 - we'll come back to that
209:13 - so next up is the merge step in the
209:15 - merge step we've broken the original
209:18 - list down into single element lists and
209:21 - now we need to make comparison
209:22 - operations and merge them back in the
209:24 - reverse order
209:26 - for a list of size n we will always need
209:29 - to make an n number of merge operations
209:31 - to get back from single element lists to
209:34 - a merge list
209:35 - this makes our overall runtime big o of
209:38 - n times log n because that's an n number
209:41 - of merge steps multiplied by log n
209:44 - number of splits of the original list
209:47 - so to our merge step here let's add a
209:49 - comment we'll say it runs
209:51 - in overall
209:53 - oops there we go runs an overall linear
209:56 - time
209:57 - right it takes an n number of steps
210:00 - number of merge steps but now that we
210:02 - have these two so linear here and
210:05 - logarithmic here we can multiply these
210:07 - and say that the merge sort function the
210:10 - top level function we can conclude that
210:12 - the runtime of the overall sorting
210:15 - process is big o of n times log n
210:20 - now what about that caveat i mentioned
210:22 - earlier
210:23 - so if we go back to our split function
210:26 - here
210:28 - right here there we go
210:31 - let's take a look at the way we're
210:32 - actually splitting the list so we're
210:34 - using python's list slicing operation
210:37 - here
210:38 - and passing in two indexes where the
210:40 - split occurs
210:42 - now if you go and poke around the python
210:44 - documentation which i've done it says
210:46 - that a slicing operation is not a
210:48 - constant time operation and in fact has
210:51 - a runtime of big o of k where k
210:54 - represents the slice size
210:57 - this means that in reality our
210:59 - implementation of split this
211:01 - implementation of split does not run in
211:03 - logarithmic time but k times logarithmic
211:06 - time because there is a slice operation
211:09 - for each split
211:10 - this means that our implementation is
211:13 - much more expensive so
211:15 - overall
211:16 - that makes our overall top level merge
211:18 - sort function not n times log n but k n
211:22 - times log n which is much more expensive
211:25 - now let's get rid of all that
211:29 - to fix this we would need to remove this
211:32 - slicing operation now we can do that by
211:34 - using a technique we learned in a
211:36 - previous course
211:38 - in the introduction to algorithms course
211:40 - we looked at two versions of binary
211:42 - search in python a recursive and an
211:45 - iterative version
211:46 - in the recursive one we use list slicing
211:49 - with every recursion call but we achieve
211:51 - the same end result using an iterative
211:53 - approach without using list slicing
211:56 - over there we declared two variables to
211:58 - keep track of the starting and ending
212:00 - positions in the list
212:02 - we could rewrite merge sort to do the
212:04 - same but i'll leave that as an exercise
212:07 - for you if you want some hints if you
212:09 - want any direction i've included a link
212:11 - in the notes with an implementation so
212:13 - that is time complexity now just so we
212:16 - know before moving on for python here
212:19 - our
212:20 - overall run time is not what i've listed
212:22 - here but this is what the actual run
212:25 - time of the merge sort algorithm looks
212:26 - like so the merge step runs in linear
212:29 - time and the split step takes
212:31 - logarithmic time for an overall n times
212:34 - log n and that is how merge sort
212:37 - actually works
212:38 - okay so what about space complexity
212:40 - the merge sort algorithm takes linear
212:42 - space and this is weird to think about
212:44 - it first but as always a visualization
212:47 - helps
212:48 - so if we start at the top again with our
212:50 - full list and carry out the split method
212:53 - until we have single element lists each
212:56 - of these new lists take up a certain
212:58 - amount of space
212:59 - so the second level here we have two
213:01 - lists where each take up an n by two
213:04 - amount of space
213:05 - now this makes it seem that the sum of
213:07 - all this space is the additional space
213:09 - needed for merge sort but that's not
213:11 - actually the case in reality there are
213:14 - two factors that make a difference
213:16 - first not every single one of these sub
213:18 - lists are created simultaneously
213:21 - at step two we create two n by two size
213:24 - sub lists
213:25 - when we move to the next step however we
213:27 - don't hold on to the n by two sub lists
213:30 - and then create four n by four size sub
213:33 - lists for the next split
213:35 - instead after the four n by four size
213:38 - sub lists are created the n by two ones
213:40 - are deleted from memory there's no
213:42 - reason to hold on to them any longer now
213:45 - the second point is that our code
213:47 - doesn't execute every path
213:49 - simultaneously
213:50 - think of it this way when we pass our
213:53 - list to the top level merge sort
213:55 - function
213:56 - our implementation calls split
213:59 - which returns a left half and a right
214:01 - half
214:02 - the next line of code then calls merge
214:05 - sort on the left half again
214:08 - this runs the function the merge sort
214:10 - function again with a new list
214:12 - in that second run of the function split
214:14 - is called again we get a second left and
214:17 - right half and then again like before we
214:20 - call merge sort on this left half as
214:22 - well what this means is that the code
214:24 - walks down the left path all the way
214:27 - down until that initial left half is
214:30 - sorted and merged back into one array
214:33 - then it's going to walk all the way down
214:35 - the right path and sort that until we're
214:37 - back to that first split with two n by
214:40 - two sized sublists
214:42 - essentially we don't run all these paths
214:44 - of code at once so the algorithm doesn't
214:47 - need additional space for every sub-list
214:50 - in fact it is the very last step that
214:52 - matters
214:53 - in the last step the two sub-lists are
214:56 - merged back into the new sorted list and
214:59 - returned
215:00 - that sorted list has an equal number of
215:02 - items as the original unsorted list
215:06 - and since this is a new list it means
215:08 - that at most the additional space the
215:11 - algorithm will require at a given time
215:14 - is n
215:15 - yes at different points in the algorithm
215:17 - we require log n amount of space but log
215:20 - n is smaller than n and so we consider
215:23 - the space complexity of merge sort to be
215:25 - linear because that is the overall
215:27 - factor
215:28 - okay that was a lot so let's stop here
215:31 - don't worry if you've got questions
215:32 - about merge sort because we're not done
215:34 - yet
215:35 - over the next few videos let's wrap up
215:37 - this course by implementing merge sort
215:39 - on a linked list
215:41 - [Music]
215:45 - over the last few videos we implemented
215:47 - the merge sort algorithm on the array or
215:49 - list type in python merge sort is
215:52 - probably the most complicated algorithm
215:54 - we've written so far but in doing so we
215:57 - learned about an important concept
215:59 - divide and conquer
216:00 - we also concluded the last video by
216:02 - figuring out the run time of merge sort
216:04 - based on our implementation
216:07 - over the next few videos we're going to
216:08 - implement merge sort again this time on
216:11 - the linked list type
216:12 - in doing so we're going to get a chance
216:14 - to see how the implementation differs
216:17 - based on the data structure while still
216:19 - keeping the fundamentals of the
216:20 - algorithm the same and we'll also see
216:22 - how the run time may be affected by the
216:24 - kinds of operations we need to implement
216:27 - let's create a new file to put our
216:29 - second implementation of merge sort in
216:32 - so file over here new file
216:35 - and it's going to have a rather long
216:37 - name we'll call this linked
216:39 - list
216:40 - merge sort with underscores everywhere
216:43 - dot pi
216:45 - we're going to need the linked list
216:47 - class that we created earlier so we'll
216:49 - start at the top by importing the linked
216:52 - list class from the linkedlist.pi file
216:56 - the way we do that is we'll say from
216:58 - linked
216:59 - list
217:00 - import linked list
217:03 - right so that imports the class
217:05 - uh let's test if this works really quick
217:09 - we'll just do something like l equal
217:11 - linked
217:12 - list
217:14 - l.add
217:15 - ten
217:16 - or one doesn't matter print l
217:19 - okay and if i hit save
217:22 - and then down here we'll say python
217:25 - linked list
217:26 - merge sword dot pi
217:28 - okay it works so this is how we get some
217:30 - of the code how we reuse the code that
217:32 - we've written in other files into this
217:34 - current file
217:36 - and get rid of this now
217:38 - okay like we did with the first
217:40 - implementation of merge sort we're going
217:42 - to split the code up across three
217:44 - functions
217:45 - the main function merge sort
217:48 - a split function and a merge function
217:52 - now if you were to look up a merge sort
217:54 - implementation in python both for a
217:55 - regular list an array or a linked list
217:58 - you would find much more concise
218:00 - versions out there but they're kind of
218:01 - hard to explain so splitting it up into
218:04 - three will sort of help it you know be
218:06 - easier to understand so we'll call this
218:08 - merge sort at the top level and this
218:11 - time it's going to take a linked list
218:15 - let's add a dog string to document the
218:17 - function
218:18 - so say that this function sorts a linked
218:22 - list in ascending order
218:25 - and like before we'll add the steps in
218:27 - here so we'll say you first recursively
218:30 - divide the linked list into sub lists
218:35 - containing
218:37 - a single
218:38 - node
218:40 - then
218:41 - we repeatedly
218:43 - merge these sub-lists
218:46 - to produce sorted sub-lists
218:49 - until one remains
218:52 - and then finally this function returns a
218:54 - sorted
218:56 - linked list
218:58 - the implementation of this top level
219:01 - merge function is nearly identical to
219:03 - the array or list version we wrote
219:05 - earlier so first we'll provide a
219:07 - stopping condition or two if the size of
219:10 - the list is one or it's an empty list
219:12 - we'll return the linked list since it's
219:15 - naively sorted so if linked
219:18 - list dot size remember that function we
219:20 - run equal one
219:23 - then we'll return
219:24 - linked
219:25 - list
219:27 - else if
219:28 - linked list dot head
219:31 - is none meaning it's an empty list then
219:34 - we'll return linked list as well okay
219:38 - next let's split the linked list into a
219:41 - left and right half
219:43 - conceptually this is no different but in
219:45 - practice we need to actually traverse
219:47 - the list we'll implement a helper method
219:50 - to make this easier
219:51 - but we'll say left half
219:53 - comma right half
219:56 - equal split
219:58 - linked list
220:00 - now once we have two sub lists like
220:01 - before we can call merge sort the top
220:04 - level function on each
220:06 - so left equal merge sort
220:09 - left half
220:14 - and right equal merge sort
220:17 - on the right half
220:20 - finally we'll merge these two top-level
220:22 - sub-lists and return it so merge left
220:26 - and right
220:27 - okay nothing new here but in the next
220:29 - video let's implement the split logic
220:32 - the next step in the merge sort
220:34 - algorithm is the divide step or rather
220:36 - an implementation of the split function
220:39 - so down here
220:40 - we'll call this split like before and
220:42 - this is going to take a linked
220:44 - list
220:47 - documenting things is good and we've
220:48 - been doing it so far so let's add a
220:50 - docstring
220:52 - divide the unsorted list at midpoint
220:58 - into sub-lists
221:00 - now of course when i say sub-lists here
221:02 - i mean sub-linked lists but that's a
221:04 - long word to say
221:06 - now here's where things start to deviate
221:08 - from the previous version
221:10 - with the list type we could rely on the
221:12 - fact that finding the midpoint using an
221:14 - index and list slicing to split into two
221:17 - lists would work even if an empty list
221:20 - was passed in
221:21 - since we have no automatic behavior like
221:23 - that we need to account for this when
221:25 - using a linked list so our first
221:27 - condition is if the linked list is none
221:30 - or if it's empty that is if head is
221:33 - equal to none
221:34 - so we'll say if linked list
221:37 - equal none
221:38 - or you can write is there it doesn't
221:39 - matter or linked list dot head is none
221:45 - well linked list can be none for example
221:47 - if we call split on a linked list
221:49 - containing a single node a split on such
221:51 - a list would mean left would contain the
221:54 - single node while right would be none
221:56 - now in either case we're going to assign
221:59 - the entire list to the left half and
222:01 - assign none to the right so we'll say
222:03 - left half
222:05 - equal linked list
222:08 - and then right half
222:11 - equal none
222:12 - you could also assign the single element
222:15 - list or none to left and then create a
222:17 - new empty linked list assigned to the
222:19 - right half but that's unnecessary work
222:22 - so now that we've done this we can
222:24 - return
222:25 - left half
222:27 - and right half
222:29 - so that's our first condition let's add
222:31 - an else clause to account for non-empty
222:34 - linked lists first we'll calculate the
222:37 - size of the list now this is easy
222:39 - because we've done the work already and
222:41 - we can just call the size method that
222:42 - we've defined we'll say size equal
222:46 - linked underscore list dot size
222:49 - using this size we can determine the
222:50 - midpoint so mid equal size and here
222:53 - we'll use that floor division operator
222:55 - to divide it by two once we have the
222:57 - midpoint we need to get the node at that
223:00 - midpoint
223:01 - now make sure you hit command s to save
223:04 - here and we're going to navigate back to
223:06 - linkedlist.hi
223:09 - in here we're going to add a convenience
223:11 - method at the very bottom right before
223:13 - the wrapper function right here
223:16 - and this convenience method is going to
223:18 - return a node at a given index so i'll
223:21 - call this node
223:24 - at
223:25 - index and it's going to take an index
223:27 - value
223:29 - this way instead of having to traverse
223:31 - the list inside of our split function we
223:34 - can simply call node at index and pass
223:36 - it the midpoint index we calculated to
223:38 - give us the node right there so we can
223:40 - perform the split
223:41 - okay so this method accepts as an
223:44 - argument the index we want to get the
223:46 - node for if this index is zero then
223:48 - we'll return the head of the list so if
223:51 - index double equals zero return
223:55 - self.head
223:56 - the rest of the implementation involves
223:58 - traversing the linked list and counting
224:01 - up to the index as we visit each node
224:05 - the rest of the implementation involves
224:07 - traversing the linked list and counting
224:09 - up to the index as we visit each node so
224:13 - i'll add an else clause here
224:14 - and we'll start at the head so we'll say
224:16 - current equal self.head
224:19 - let's also declare a variable called
224:21 - position
224:23 - to indicate where we are in the list
224:26 - we can use a while loop to walk down the
224:28 - list our condition here is as long as
224:32 - the position is less than the index
224:34 - value
224:36 - so i'll say while position
224:39 - is less than index
224:41 - inside the loop we'll assign the next
224:44 - node to current and increment the value
224:46 - of position by one
224:48 - so current equal current dot next node
224:51 - position
224:54 - plus equal one
224:57 - once the position value equals the index
224:59 - value current refers to the node we're
225:01 - looking for and
225:03 - we can return it we'll say return
225:05 - current
225:08 - let's get rid of all this empty space
225:10 - there we go now back in linked list
225:13 - merge sort
225:14 - dot pi
225:16 - we can use this method to get at the
225:18 - node after we've calculated the midpoint
225:20 - to get the node at the midpoint of the
225:23 - list
225:24 - so we'll say mid node
225:26 - equal
225:27 - linked
225:28 - list
225:29 - dot node at index
225:32 - and here i'm going to do something
225:34 - slightly confusing i'm going to do mid
225:36 - minus 1. remember we're subtracting 1
225:39 - here
225:41 - because we used size
225:43 - to calculate the midpoint and like the
225:46 - len function size will always return a
225:48 - value greater than the maximum index
225:51 - value
225:52 - so think of a linked list with two nodes
225:55 - size would return two
225:57 - the midpoint though and the way we're
225:58 - calculating the index we always start at
226:01 - zero which means size is going to be one
226:03 - greater than that so we're going to
226:04 - deduct one from it to get the value we
226:07 - want but we're using the floor division
226:09 - operator so it's going to round that
226:10 - down even more no big deal with the node
226:13 - at the midpoint now that we have this
226:15 - midnote we can actually split the list
226:18 - so first we're going to assign the
226:19 - entire linked list to a variable named
226:21 - left half so left half
226:24 - equal linked list
226:27 - this seems counterintuitive but make
226:29 - sense in a second
226:30 - for the right half we're going to assign
226:33 - a new instance of linked list so right
226:36 - half equal
226:38 - linked list
226:40 - this newly created list is empty but we
226:42 - can fix that by assigning the node that
226:44 - comes after the midpoint so after the
226:47 - midpoint of the original linked list we
226:49 - can assign the node that comes after
226:51 - that midpoint node as the head of this
226:54 - newly created
226:55 - right linked list
226:57 - so here we'll say right half
226:59 - dot head
227:00 - equal mid node dot
227:03 - node
227:04 - once we do that we can assign none to
227:07 - the next node property on mid node to
227:10 - effectively sever that connection and
227:12 - make what was the mid node now the tail
227:15 - node of the left linked list so i'll say
227:18 - mid node
227:20 - dot next node
227:22 - equal none
227:23 - if that's confusing here's a quick
227:25 - visualization of what just happened
227:28 - we start off with a single linked list
227:30 - and find the midpoint the node that
227:32 - comes after the node at midpoint is
227:34 - assigned to the head of a newly created
227:36 - linked list and the connection between
227:39 - the midpoint node and the one after is
227:41 - removed
227:42 - we now have two distinct linked lists
227:45 - split at the midpoint
227:47 - and with that we can return the two sub
227:49 - lists so we'll return left half
227:52 - and right half
227:54 - in the next video let's tackle our merge
227:56 - function in the last video we defined an
227:59 - implementation for the version of the
228:01 - split function that works on linked
228:03 - lists it contained a tiny bit more code
228:05 - than the array or list version that was
228:07 - expected the merge function is no
228:10 - different because like with the split
228:12 - function after we carry out a comparison
228:14 - operation we also need to swap
228:16 - references to corresponding nodes all
228:19 - right let's add our merge function over
228:21 - here at the bottom below
228:24 - the split functions we'll call this
228:26 - merge and it's going to take a left
228:28 - and right
228:30 - now because this can get complicated
228:32 - we're going to document this function
228:34 - extensively and as always we're going to
228:36 - start with a doc string
228:40 - so we'll say that this function merges
228:43 - two linked lists
228:45 - sorting
228:46 - by data in the nodes
228:50 - and it returns a new
228:53 - merged list
228:56 - remember that in the merge step we're
228:58 - going to compare values across two
229:00 - linked lists and then return a new
229:02 - linked list with nodes where the data is
229:05 - sorted
229:06 - so first we need to create that new
229:08 - linked list let's add a comment in here
229:10 - we'll say create
229:12 - a new linked list
229:15 - that contains nodes from
229:18 - let's add a new line merging left and
229:21 - right
229:22 - okay and then create the list so merged
229:24 - equal
229:25 - new linked list
229:28 - to this list we're going to do something
229:30 - unusual we're going to add a fake head
229:33 - this is so that when adding sorted nodes
229:35 - we can reduce the amount of code we have
229:37 - to write by not worrying about whether
229:39 - we're at the head of the list once we're
229:41 - done we can assign the first sorted node
229:44 - as the head and discard the fake head
229:46 - now this might not make sense at first
229:48 - but not having to worry about whether
229:50 - the new linked list already contains a
229:52 - head or not makes the code simpler we'll
229:55 - add another comment
229:56 - and a fake hand that is discarded
230:00 - later
230:01 - we'll say merged dot add zero
230:04 - like we've been doing so far we'll
230:06 - declare a variable named current to
230:09 - point to the head of the list
230:12 - set current to the head of the linked
230:16 - list and then current equal
230:19 - merged dot head
230:21 - next we'll get a reference to the head
230:23 - on each of the linked lists left and
230:26 - right
230:27 - so we'll say obtain
230:29 - head nodes
230:31 - for left and right linked lists
230:35 - and here's call this left head
230:38 - equal
230:40 - left dot head
230:41 - and right hand equal right dot head
230:48 - okay so with that setup out of the way
230:50 - let's start iterating over both lists
230:54 - so another comment
230:56 - iterate over left and right
230:59 - as long
231:00 - or we'll say until
231:02 - the
231:03 - until we reach the tail node
231:06 - of either
231:08 - and we'll do that by saying while
231:11 - left head
231:12 - or right head
231:15 - so this is a pattern that we've been
231:16 - following all along we're going to
231:18 - iterate until we hit the tail nodes of
231:20 - both lists and we'll move this pointer
231:22 - forward every time so that we traverse
231:24 - the list with every iteration if you
231:26 - remember the logic behind this from the
231:28 - earlier version once we hit the tail
231:30 - note of one list if there are nodes left
231:33 - over in the other linked list we don't
231:35 - need to carry out a comparison operation
231:37 - anymore and we can simply add those
231:39 - nodes to the merged list
231:41 - the first scenario we'll consider is if
231:43 - the head of the left linked list is none
231:46 - this means we're already past the tail
231:48 - of left and we can add all the nodes
231:51 - from the right linked list to the final
231:53 - merge list so here i'll say if
231:56 - the head node of left is none
232:00 - we're past the tail
232:03 - add the node
232:06 - from the right from right to merged
232:09 - linked
232:10 - list so here we'll say if
232:13 - left head
232:15 - is none
232:16 - current dot next node
232:19 - remember current points to the head of
232:21 - the merge list that we're going to
232:23 - return so here we're setting its next
232:25 - node reference to the head node on the
232:28 - right link list so we'll say right head
232:33 - then when we do that we'll move the
232:35 - right head forward to the next node so
232:38 - let's say right
232:40 - head
232:41 - equal right hand
232:43 - dot next node
232:46 - this terminates the loop on the next
232:48 - iteration let's look at a visualization
232:50 - to understand why
232:52 - let's say we start off with a linked
232:54 - list containing four nodes so we keep
232:56 - calling split on it until we have lists
232:58 - with just a single head single node
233:01 - linked lists essentially
233:02 - so let's focus on these two down here
233:04 - that we'll call left and right
233:06 - we haven't implemented the logic for
233:08 - this part yet but here we would compare
233:10 - the data values and see which one is
233:12 - less than the other
233:13 - so we'll assume that left's head is
233:15 - lesser than right's head so we'll set
233:17 - this as the next node in the final merge
233:19 - list
233:20 - left is now an empty length list so left
233:23 - dot head equals none on the next pass
233:26 - through the loop left head is none which
233:29 - is the situation we just implemented
233:31 - here we can go ahead and now assign
233:34 - right head as the next note in the merge
233:36 - link list we know that right is also a
233:38 - singly linked list
233:40 - here's the crucial bit when we move the
233:42 - pointer forward by calling next node on
233:45 - the right node there is no node and the
233:48 - right link the right linked list is also
233:51 - empty now which means that both left
233:54 - head and right head are none and either
233:56 - one of these would cause our loop
233:57 - condition to terminate
233:59 - so what we've done here is encoded a
234:02 - stopping condition for the loop so we
234:04 - need to document this because it can get
234:06 - fuzzy so right above that line of code
234:08 - i'll say call next
234:10 - on right
234:12 - to
234:13 - set loop condition
234:16 - to false
234:17 - okay there's another way we can arrive
234:18 - at this stopping condition and that's in
234:20 - the opposite direction if we start with
234:22 - the right head being none so here we'll
234:24 - say i'm going to add another comment
234:28 - if oops not there there
234:31 - if
234:33 - the head node of right
234:36 - is none
234:38 - we're past the tail
234:40 - then we'll say
234:41 - add the tail node from left
234:45 - to merged linked list
234:48 - and then we'll add that condition we'll
234:49 - say else if right head
234:52 - is none
234:53 - now remember we can enter these even if
234:56 - left head is none we can still go into
234:59 - this condition we can still enter this
235:01 - if statement and execute this logic
235:03 - because the while loop the loop
235:05 - condition here is an or statement so
235:08 - even if left head is false if this
235:10 - returns true because there's a value
235:11 - there there's a node there the loop will
235:13 - keep going
235:15 - okay now in this case we want to set the
235:17 - head of the left linked list as the next
235:20 - node on the merge list so this is simply
235:22 - the opposite of what we did over here
235:25 - we'll set current dot next node
235:28 - equal to left head
235:30 - and then we'll move so after doing that
235:33 - we can move the variable pointing to
235:35 - left head forwards which as we saw
235:37 - earlier is past the tail node and then
235:39 - results in the loop terminating so we'll
235:41 - say left hand
235:43 - equal
235:44 - left
235:45 - head dot next node and we'll add that
235:48 - comment here as well so we'll say call
235:50 - next on left to set loop condition
235:55 - to false
235:56 - because here right head is none and now
235:58 - we make left head none these two
236:00 - conditions we looked at where either the
236:02 - left head or right head
236:05 - were at the tail nodes of our respective
236:07 - lists those are conditions that we run
236:10 - into when we've reached the bottom of
236:12 - our split where we have single element
236:14 - linked lists or empty linked lists let's
236:17 - account for our final condition where
236:20 - we're evaluating a node that is neither
236:22 - the head nor the tail of the list
236:25 - and this condition we need to reach into
236:27 - the nodes and actually compare the data
236:29 - values to one another before we can
236:31 - decide which node to add first to the
236:34 - merged list
236:35 - so here this is an else because we've
236:37 - arrived at our third condition third and
236:39 - final
236:40 - and here we'll say not at either tail
236:43 - node
236:45 - obtain
236:47 - no data to perform
236:49 - comparison operations so let's get each
236:52 - of those data values out of the
236:54 - respective nodes so that we can compare
236:56 - it so we'll say left
236:57 - data equal left head dot data
237:01 - and write data
237:03 - equal right head righthead.data
237:06 - okay what do we do next well we compare
237:08 - but first let's add a comment so we'll
237:10 - say if data
237:12 - on left
237:14 - is less than right
237:16 - set current to left node and then
237:21 - move
237:22 - actually we'll add this in a second so
237:24 - here we'll say if left data
237:26 - is less than write data
237:29 - then current dot next node
237:32 - equal left head
237:34 - and then
237:35 - we'll add a comment and we'll say move
237:38 - left head to next node
237:40 - on that list so we'll say left head
237:43 - equal left head
237:45 - dot next node
237:49 - just as our comment says we'll check if
237:51 - the left data is less than the right
237:54 - data if it is since we want a list in
237:57 - ascending order we'll assign the left
237:59 - node to be the next node in the merged
238:01 - list we'll also move the left head
238:03 - forward to traverse down to the next
238:05 - node in that particular list now if left
238:09 - is larger than right then we want to do
238:11 - the opposite so we'll go back to spaces
238:14 - another comment
238:15 - if data on left is greater than right
238:19 - set current
238:21 - to
238:22 - right node
238:24 - okay so else
238:26 - here we assign the right head to be the
238:29 - next node in the merge list so
238:31 - current.nextnode
238:32 - equal right
238:35 - head
238:36 - and then comment
238:38 - move right head to next node
238:43 - so right
238:44 - head equal
238:46 - right head
238:47 - dot next
238:49 - node okay after doing that we move the
238:52 - right head pointer to reference the next
238:54 - node in the right list
238:56 - and finally at the end of each iteration
239:00 - of the while loop so not here but two
239:02 - spaces back right make sure we're
239:04 - indented at the same level as the while
239:07 - so we got to go yep or not the same
239:10 - level as the wild but the same outer
239:11 - scope
239:12 - and then there we're going to say
239:15 - move current to next node
239:19 - so current equal current dot next node
239:23 - okay don't worry if this is confusing as
239:25 - always we'll look at a visualization in
239:28 - just a bit so we'll wrap up this
239:29 - function by discarding that fake head we
239:31 - set earlier setting the correct node as
239:34 - head and returning the linked list so
239:37 - we'll add a comment
239:38 - discard fake head
239:40 - and set
239:41 - first
239:44 - merged node as head so here we'll say
239:47 - head equal
239:49 - merged dot head dot next
239:52 - node and then merged dot head equal head
239:57 - and finally return
239:59 - merged
240:00 - okay we wrote a lot of code here a lot
240:02 - of it was comments but still it's a
240:04 - bunch let's take a quick break in the
240:06 - next video we'll test this out evaluate
240:09 - our results and determine the runtime of
240:11 - our algorithm
240:12 - okay first things first let's test out
240:15 - our code now we'll keep it simple
240:17 - because writing a robust verify function
240:19 - would actually take up this entire video
240:21 - instead i'll leave that up to you to try
240:23 - as homework
240:25 - okay so
240:26 - at the very end
240:29 - let's create a new linked list
240:33 - let's add a few notes to this so l add
240:35 - i'm going to copy paste this
240:37 - so it makes it easier for me
240:39 - not to have to retype a bunch so i'll
240:41 - add 10
240:42 - uh then set 2
240:44 - 44
240:45 - 15
240:47 - and something like 200. okay then we'll
240:50 - go ahead and print l so that we can
240:52 - inspect this list
240:55 - next
240:56 - let's create a declare variable here so
240:59 - we'll call this sorted linked list
241:02 - and to this we're going to assign the
241:04 - result of calling merge sort on l
241:07 - and then we'll print this
241:09 - so sorted
241:10 - linked
241:12 - list
241:13 - okay since we've taken care of all the
241:15 - logic we know that this gets added in as
241:17 - nodes and then
241:18 - let's see what this looks like all right
241:20 - so hit save
241:21 - and then bring up the console we're
241:23 - going to type out python
241:26 - linked
241:27 - list underscore merge sort dot pi and
241:30 - then enter okay so we see that linked
241:34 - list we first created remember that what
241:36 - we add first right that eventually
241:39 - becomes a tail or right yeah so 10 is
241:41 - the tail 200 is the last one so 200 is
241:43 - the head because i'm calling add it
241:45 - simply adds each one to the head of the
241:46 - list so here we have 10 to 44 15 and 200
241:51 - in the order we added and then the
241:53 - sorted linked list sorts it out so it's
241:56 - 2 10 15 44 and 200. look at that a
242:00 - sorted linked list
242:01 - okay so let's visualize this from the
242:03 - top
242:04 - we have a linked list containing five
242:06 - nodes with integers 10 2 4 15 and 200 as
242:12 - data respectively our merge sort
242:14 - function calls split on this list the
242:17 - split function calls size on the list
242:19 - and gets back 5 which makes our midpoint
242:22 - 2.
242:23 - using this midpoint we can split the
242:25 - list using the node at index method
242:28 - remember that when doing this we deduct
242:30 - 1 from the value of mid so we're going
242:32 - to split here using an index value of 1.
242:35 - effectively this is the same since we're
242:37 - starting with an index value of 0 1
242:39 - means we split after node 2. we assign
242:42 - the entire list to left half
242:44 - then create a new list and assign that
242:46 - to right half
242:48 - we can assign node 3 at index value 2 as
242:51 - the head of the right list and remove
242:53 - the references between node two and node
242:56 - three so far so good right
242:58 - okay so now we're back in the merge sort
243:00 - function after having called split and
243:03 - we have two linked lists
243:05 - let's focus on just the left half
243:07 - because if you go back and look at our
243:08 - code we're going to call merge sort on
243:11 - the left linked list again
243:13 - this means the next thing we'll do is
243:15 - run through that split process since
243:17 - this is a linked list containing two
243:19 - nodes this means that split is going to
243:21 - return a new left and right list each
243:23 - with one node again we're back in the
243:25 - merge sort function which means that we
243:28 - call merge sort on this left list again
243:32 - since this is a single node linked list
243:34 - on calling merge sort on it we
243:36 - immediately return before we split since
243:38 - we hit that stopping condition so we go
243:40 - to the next line of code which is
243:42 - calling merge sort on the right list as
243:45 - well but again we'll get back
243:46 - immediately because we hit that stopping
243:48 - condition now that we have a left and
243:50 - right that we get back from calling
243:52 - merge sort we can call merge on them
243:55 - inside the merge function we start by
243:57 - creating a new linked list and attaching
244:00 - a fake head
244:01 - then we evaluate whether either the left
244:03 - or the right head is none
244:05 - since neither condition is true we go to
244:07 - the final step where we evaluate the
244:08 - data in each node
244:10 - in this case the data in the right node
244:13 - is less than the left node so we assign
244:15 - the right node as the next node in the
244:17 - merge link list and move the right head
244:20 - pointer forward
244:22 - in the merge link list we move our
244:23 - current pointer forward to this new node
244:25 - we've added and that completes one
244:27 - iteration of the loop
244:30 - on the next iteration righthead now
244:32 - points to none since that was a single
244:35 - node list and we can assign the rest of
244:37 - the left linked list which is
244:39 - effectively the single node over to the
244:41 - merge link list here we discard the fake
244:44 - head move the next node up to be the
244:47 - correct head and return the newly merged
244:50 - sorted linked list remember that at this
244:52 - point because right head and left head
244:55 - pointed to none are while loop
244:56 - terminated
244:58 - so in this way we recursively split and
245:00 - repeatedly merge sub-lists until we're
245:03 - back with one sorted linked list
245:05 - the merge sort algorithm is a powerful
245:08 - sorting algorithm but ultimately it
245:10 - doesn't really do anything complicated
245:12 - it just breaks the problem down until
245:14 - it's really simple to solve
245:16 - remember the technique here which we've
245:18 - talked about before is called divide and
245:20 - conquer so i like to think of merge sort
245:22 - in this way there's a teacher at the
245:24 - front of the room and she has a bunch of
245:26 - books that she needs to sort into
245:27 - alphabetical order instead of doing all
245:30 - that work herself she splits that pile
245:32 - into two and hands it to two students at
245:34 - the front
245:35 - each of those students split it into two
245:38 - more and hand it to the four students
245:40 - seated behind them as each student does
245:42 - this eventually a bunch of single
245:45 - students has two books to compare and
245:47 - they can sort it very easily and hand it
245:49 - back to the student who gave it to them
245:51 - in front of them who repeats the process
245:53 - backwards so ultimately it's really
245:55 - simple work is just efficiently
245:57 - delegated
245:59 - now back to our implementation here
246:01 - let's talk about runtime so far other
246:03 - than the node swapping we had to do it
246:05 - seems like most of our implementation is
246:08 - the same right in fact it is including
246:10 - the problems that we ran into in the
246:12 - list version as well so in the first
246:15 - implementation of merge sort we thought
246:17 - we had an algorithm that ran in big o of
246:19 - n log n but turns out we didn't why well
246:23 - the python list slicing operation if you
246:26 - remember actually takes up some amount
246:28 - of time amounting to big o of k
246:31 - a true implementation of merge sort runs
246:33 - in quasi-linear or log linear time that
246:36 - is n times log n so we almost got there
246:40 - but we didn't now in our implementation
246:42 - of merge sort on a linked list we
246:44 - introduce the same problem so if we go
246:46 - back up to
246:48 - the merge or rather the split function
246:50 - this is where it happens now swapping
246:52 - node references that's a constant time
246:54 - operation no big deal comparing values
246:57 - also constant time
246:59 - the bottleneck here like list slicing is
247:03 - in splitting a late list at the midpoint
247:06 - if we go back to our implementation you
247:08 - can see here that we use the node at
247:11 - index method which finds the node we
247:13 - want by traversing the list
247:16 - this means that every split operation
247:19 - incurs a big o of k cost where k here is
247:22 - the midpoint of the list effectively n
247:25 - by 2 because we have to walk down the
247:28 - list counting up the index until we get
247:30 - to that node
247:31 - given that overall splits take
247:34 - logarithmic time our split function just
247:37 - like the one we wrote earlier
247:39 - incurs a cost of
247:41 - big o of
247:43 - k log n so here we'll say it takes
247:46 - big o of k times log n
247:49 - now the merge function also like the one
247:51 - we wrote earlier takes linear time so
247:53 - that one is good that one runs in the
247:55 - expected amount of time so here we'll
247:56 - say runs in linear
248:00 - time
248:01 - and that would bring our overall run
248:03 - time so up at the merge sort function we
248:06 - can say this runs in
248:08 - big o of k n times log
248:12 - n
248:13 - it's okay though this is a good start
248:15 - and one day when we talk about constant
248:17 - factors and look at ways we can reduce
248:19 - the cost of these operations using
248:21 - different strategies we can come back
248:24 - and re-evaluate our code to improve our
248:26 - implementation for now as long as you
248:28 - understand how merge sort works
248:30 - conceptually what the run time and space
248:33 - complexities look like and where the
248:35 - bottlenecks are in your code that's
248:37 - plenty of stuff
248:39 - if you're interested in learning more
248:40 - about how we would solve this problem
248:42 - check out the notes in the teachers
248:44 - video in the next video let's wrap this
248:46 - course up
248:48 - and with that let's wrap up this course
248:50 - in the prerequisite to this course
248:52 - introduction to algorithms we learned
248:54 - about basic algorithms along with some
248:56 - concepts like recursion and big o that
248:59 - set the foundation for learning about
249:01 - implementing and evaluating algorithms
249:04 - in this course we learned what a data
249:06 - structure is and how data structures go
249:08 - hand in hand with algorithms
249:10 - we started off by exploring a data
249:12 - structure that many of us use in our
249:14 - day-to-day programming arrays or lists
249:17 - as they are known in python
249:19 - we take a peek under the hood at how
249:21 - arrays are created and stored and
249:23 - examine some of the common operations
249:25 - carried out on arrays
249:27 - these are operations that we write and
249:29 - execute all the time but here we took a
249:31 - step back and evaluated the run times of
249:33 - these operations and how they affect the
249:35 - performance of our code
249:37 - after that we jumped into an entirely
249:39 - new world where we wrote our own data
249:41 - structure a singly linked list
249:44 - admittedly linked lists aren't used much
249:46 - in day-to-day problem solving but it is
249:49 - a good data structure to start off with
249:51 - because it is fairly straightforward to
249:53 - understand and not that much different
249:55 - from an array
249:56 - we carried out the same exercise as we
249:58 - did on arrays in that we looked at
250:00 - common data operations but since this
250:02 - was a type we defined on our own we
250:05 - implemented these operations ourselves
250:07 - and got to examine with a fine-tooth
250:09 - comb how our code and the structure of
250:11 - the type affected the runtime of these
250:13 - operations
250:15 - the next topic we tackled was
250:16 - essentially worlds colliding we
250:18 - implemented a sorting algorithm to sort
250:21 - two different data structures
250:23 - here we got to see how all of the
250:25 - concepts we've learned so far
250:26 - algorithmic thinking time and space
250:29 - complexity and data structures all come
250:31 - together to tackle the problem of
250:33 - sorting data
250:35 - this kind of exercise is one we're going
250:37 - to focus on moving forward as we try to
250:39 - solve more real-world programming
250:41 - problems using different data structures
250:43 - and algorithms
250:45 - if you've stuck with this content so far
250:47 - keep up the great work this can be a
250:49 - complex topic but a really interesting
250:51 - one and if you take your time with it
250:53 - you will get a deeper understanding of
250:55 - programming and problem solving as
250:57 - always check the notes for more
250:59 - resources and happy coding
251:03 - [Music]
251:10 - you may have heard that algorithms and
251:12 - computer science are boring or
251:14 - frustrating they certainly can be hard
251:16 - to figure out especially the way some
251:18 - textbooks explain them but once you
251:20 - understand what's going on algorithms
251:22 - can seem fascinating clever or even
251:24 - magical
251:25 - to help further your understanding of
251:27 - algorithms this course is going to look
251:29 - at two categories sorting algorithms and
251:32 - searching algorithms you could argue
251:34 - that these are the easiest kinds of
251:35 - algorithms to learn but in learning how
251:38 - these algorithms are designed we'll
251:39 - cover useful concepts like recursion and
251:42 - divide and conquer that are used in many
251:44 - other sorts of algorithms and can even
251:46 - be used to create brand new ones
251:48 - by the way all the code samples i'm
251:50 - going to show in the videos will be in
251:51 - python because it's a popular language
251:53 - that's relatively easy to read but you
251:56 - don't need to know python to benefit
251:57 - from this course you can see the
251:59 - teacher's notes for each video for info
252:01 - on implementing these algorithms in your
252:03 - own favorite language
252:05 - our goal with this course is to give you
252:07 - an overview of how sorting and searching
252:08 - algorithms work but many algorithms have
252:11 - details that can be handled in different
252:13 - ways some of these details may distract
252:15 - from the big picture so we've put them
252:17 - in the teachers notes instead you don't
252:19 - need to worry about these when
252:20 - completing the course for the first time
252:22 - but if you're going back and referring
252:23 - to it later be sure to check the
252:25 - teacher's notes for additional info
252:27 - suppose we have a list of names it's a
252:29 - pretty big list a hundred thousand names
252:31 - long this list could be part of an
252:33 - address book or social media app
252:36 - and we need to find the locations of
252:37 - individual names within the list
252:39 - possibly to look up additional data
252:41 - that's connected to the name
252:43 - let's assume there's no existing
252:44 - function in our programming language to
252:46 - do this or that the existing function
252:48 - doesn't suit our purpose in some way
252:50 - for an unsorted list our only option may
252:53 - be to use linear search also known as
252:55 - sequential search
252:57 - linear search is covered in more detail
252:58 - elsewhere on our site check the
253:00 - teacher's notes for a link if you want
253:01 - more details
253:03 - you start at the first element you
253:05 - compare it to the value you're searching
253:06 - for if it's a match you return it if not
253:09 - you go to the next element
253:11 - you compare that to your target if it's
253:13 - a match you return it if not you go to
253:15 - the next element and so on through the
253:17 - whole list
253:19 - the problem with this is that you have
253:20 - to search the entire list every single
253:23 - time we're not doing anything to narrow
253:25 - down the search each time we have to
253:27 - search all of it
253:29 - if you're searching a big list or
253:30 - searching it repeatedly this amount of
253:32 - time can slow your whole lap down to the
253:34 - point that people may not want to use it
253:36 - anymore that's why it's much more common
253:39 - to use a different algorithm for
253:40 - searching lists binary search
253:43 - binary search is also covered in more
253:45 - detail elsewhere on our site check the
253:47 - teacher's notes for a link
253:49 - binary search does narrow the search
253:51 - down for us specifically it lets us get
253:53 - rid of half the remaining items we need
253:55 - to search through each time
253:57 - it does this by requiring that the list
253:59 - of values be sorted
254:02 - it looks at the value in the middle of
254:04 - the list
254:05 - if the value it finds is greater than
254:07 - the target value it ignores all values
254:09 - after the value it's looking at
254:11 - if the value it finds is less than the
254:13 - target value it ignores all values
254:14 - before the value it's looking at
254:17 - then it takes the set of values that
254:18 - remain and looks at the value in the
254:20 - middle of that list again if the value
254:23 - it finds is greater than the target
254:24 - value it ignores all values after the
254:26 - value it's looking at if the value it
254:28 - finds is less than the target value it
254:30 - ignores all values before the value it's
254:32 - looking at
254:33 - but as we mentioned binary search
254:35 - requires the list of values you're
254:36 - searching through to be sorted
254:39 - if the lists weren't sorted you would
254:40 - have no idea which half of the values to
254:42 - ignore because either half could contain
254:44 - the value you're looking for you'd have
254:46 - no choice but to use linear search
254:49 - so before we can use binary search on a
254:51 - list we need to be able to sort that
254:53 - list we'll look at how to do that next
254:56 - our end goal is to sort a list of names
254:58 - but comparing numbers is a little easier
255:01 - than comparing strings so we're going to
255:02 - start by sorting a list of numbers
255:05 - i'll show you how to modify our examples
255:07 - to sort strings at the end of the course
255:10 - to help make clear the importance of
255:12 - choosing a good sorting algorithm we're
255:14 - going to start with a bad one it's
255:16 - called bogosort basically bogosort just
255:19 - randomizes the order of the list
255:21 - repeatedly until it's sorted
255:24 - here's a python code file where we're
255:26 - going to implement bogosort
255:28 - it's not important to understand this
255:29 - code here at the top although we'll have
255:31 - info on it in the teachers notes if you
255:33 - really want it all you need to know is
255:35 - that it takes the name of a file that we
255:37 - pass on the command line loads it and
255:39 - returns a python list which is just like
255:42 - an array in other languages containing
255:44 - all the numbers that it read from the
255:45 - file
255:47 - let me have the program print out the
255:48 - list of numbers it loads so you can see
255:50 - it we'll call the print method and we'll
255:53 - pass it the list of numbers
255:55 - save that let's run it real quick
255:58 - with python bogosort.pi
256:03 - oh whoops and we need to
256:05 - provide it
256:06 - the name of the file here on the command
256:08 - line that we're going to load so it's in
256:10 - the numbers directory a slash separates
256:13 - the directory name from the file name
256:16 - five dot text
256:18 - and there's our list of numbers that was
256:20 - loaded from the file
256:21 - okay let me delete that print statement
256:23 - and then we'll move on
256:25 - bogo sort just randomly rearranges the
256:28 - list of values over and over so the
256:30 - first thing we're going to need is a
256:32 - function to detect when the list is
256:34 - sorted
256:35 - we'll write an is sorted function that
256:37 - takes a list of values as a parameter
256:42 - it'll return true if the list passed in
256:44 - is sorted or false if it isn't
256:47 - we'll loop through the numeric index of
256:49 - each value in the list from 0 to 1 less
256:52 - than the length of the list like many
256:54 - languages python list indexes begin at 0
256:57 - so a list with a length of 5 has indexes
257:00 - going from 0 through 4.
257:02 - if the list is sorted then every value
257:04 - in it will be less than the one that
257:06 - comes after it so we test to see whether
257:09 - the current item is greater than the one
257:11 - that follows it
257:12 - if it is it means the whole list is not
257:15 - sorted so we can return false
257:17 - if we get down here it means the loop
257:19 - completed without finding any unsorted
257:21 - values python uses white space to mark
257:24 - code blocks so unindenting the code like
257:26 - this marks the end of the loop
257:28 - since all the values are sorted we can
257:30 - return true
257:32 - now we need to write the function that
257:34 - will actually do the so-called sorting
257:36 - the bogosort function will also take the
257:38 - list of values it's working with as a
257:40 - parameter
257:41 - we'll call our is sorted function to
257:44 - test whether the list is sorted we'll
257:46 - keep looping until is sorted returns
257:48 - true
257:49 - python has a ready-made function that
257:51 - randomizes the order of elements in the
257:53 - list
257:54 - since the list isn't sorted we'll call
257:56 - that function here
257:57 - and since this is inside the loop it'll
257:59 - be randomized over and over until our is
258:02 - sorted function returns true
258:04 - if the loop exits it means is sorted
258:07 - returned true and the list is sorted so
258:09 - we can now return the sorted list
258:12 - finally we need to call our bogosort
258:14 - function pass it the list we loaded from
258:16 - the file and print the sorted list it
258:18 - returns
258:20 - okay let's save this and try running it
258:22 - we do so with python the name of the
258:25 - script bogosort.pi
258:27 - and the name of the file we're going to
258:29 - run it on
258:30 - numbers directory5.txt
258:35 - it looks like it's sorting our list
258:37 - successfully
258:38 - but how efficient is this let's add some
258:41 - code to track the number of times it
258:42 - attempts to sort the list
258:44 - up here at the top of the bogus sort
258:46 - function we'll add a variable to track
258:48 - the number of attempts it's made we'll
258:50 - name it attempts and we'll set its
258:51 - initial value to zero since we haven't
258:53 - made any attempts yet
258:56 - with each pass through the loop we'll
258:58 - print the current number of attempts
259:01 - and then here at the end of the loop
259:02 - after attempting to shuffle the values
259:04 - we'll add one to the count of attempts
259:10 - let's save this and let's try running it
259:12 - again a couple times
259:15 - in the console i can just press the up
259:17 - arrow to bring up the previous command
259:18 - and re-run it
259:20 - so it looks like this first run to sort
259:22 - this five element list took 363 attempts
259:25 - let's try it again
259:27 - this time it only took 91 attempts
259:30 - we're simply randomizing the list with
259:32 - each attempt so each
259:34 - run of the program takes a random number
259:36 - of attempts
259:38 - now let's try this same program with a
259:40 - larger number of items
259:42 - python bogo sort
259:45 - numbers
259:47 - i have a list of eight items set up here
259:50 - in this other file
259:53 - this time it takes 11 000 attempts
259:58 - only 487 this time
260:02 - and this time thirteen thousand you can
260:04 - see that the trend is increasing
260:06 - steadily
260:07 - the problem with bogosort is that it
260:09 - doesn't make any progress toward a
260:11 - solution with each pass
260:13 - it could generate a list where just one
260:15 - value is out of order but then on the
260:16 - next attempt it could generate a list
260:18 - where all the elements are out of order
260:20 - again
260:21 - stumbling on a solution is literally a
260:23 - matter of luck and for lists with more
260:25 - than a few items it might never happen
260:28 - up next we'll look at selection sort
260:31 - it's a sorting algorithm that's still
260:33 - slow but it's better than bogo's sort
260:36 - previously we showed you bogo sort a
260:38 - terrible sorting algorithm that
260:40 - basically randomizes the order of a list
260:42 - and then checks to see if it happens to
260:44 - be sorted
260:46 - the problem with bogo's sort is that it
260:48 - doesn't get any closer to a solution
260:50 - with each operation and so with lists
260:52 - that have more than a few items it'll
260:53 - probably never finish sorting them
260:56 - now we're going to look at an algorithm
260:58 - named selection sort it's still slow but
261:01 - at least each pass through the list
261:02 - brings it a little closer to completion
261:06 - our implementation of selection sort is
261:08 - going to use two arrays an unsorted
261:10 - array and a sorted one some versions
261:13 - move values around within just one array
261:15 - but we're using two arrays to keep the
261:17 - code simpler the sorted list starts out
261:19 - empty but we'll be moving values from
261:22 - the unsorted list to the sorted list one
261:24 - at a time
261:25 - with each pass we'll look through each
261:27 - of the values in the unsorted array find
261:29 - the smallest one and move that to the
261:31 - end of the sorted array
261:33 - we'll start with the first value in the
261:35 - unsorted array and say that's the
261:36 - minimum or smallest value we've seen so
261:39 - far
261:40 - then we'll look at the next value and
261:41 - see if that's smaller than the current
261:43 - minimum if it is we'll mark that as the
261:45 - new minimum
261:46 - then we'll move to the next value and
261:48 - compare that to the minimum again if
261:50 - it's smaller that becomes the new
261:52 - minimum we continue that way until we
261:54 - reach the end of the list
261:56 - at that point we know whatever value we
261:58 - have marked as the minimum is the
262:00 - smallest value in the whole list
262:02 - now here's the part that makes selection
262:04 - sort better than bogo sort we then move
262:07 - that minimum value from the unsorted
262:09 - list to the end of the sorted list
262:11 - the minimum value isn't part of the
262:13 - unsorted list anymore so we don't have
262:15 - to waste time looking at it anymore all
262:18 - our remaining comparisons will be on the
262:20 - remaining values in the unsorted list
262:23 - then we start the process over at this
262:25 - point our list consists of the numbers 8
262:27 - 5 4 and 7. our first minimum is 8.
262:31 - we start by comparing the minimum to
262:33 - five five is smaller than eight so five
262:36 - becomes the new minimum then we compare
262:38 - five to four and four becomes the new
262:40 - minimum four is not smaller than seven
262:42 - though so four remains the minimum four
262:45 - gets moved to the end of the sorted
262:46 - array becoming its second element
262:49 - the process repeats again eight is the
262:51 - first minimum but five is smaller so
262:53 - that becomes the minimum seven is larger
262:55 - so five stays is the minimum and five is
262:57 - what gets moved over to the sort of
262:59 - array and so on until there are no more
263:01 - items left in the unsorted array and all
263:03 - we have left is the sorted array
263:06 - so that's how selection sort works in
263:09 - general now let's do an actual
263:10 - implementation of it
263:12 - this code here at the top is the same as
263:14 - we saw in the bogo sword example it just
263:17 - loads a python list of numbers from a
263:19 - file
263:20 - let's implement the function that will
263:22 - do our selection sort we're going to
263:24 - pass in our python list containing all
263:26 - the unsorted numbers we'll create an
263:29 - empty list that will hold all our sorted
263:31 - values
263:33 - we'll loop once for each value in the
263:35 - list
263:37 - we call a function named index submin
263:39 - which we're going to write in just a
263:41 - minute that finds the minimum value in
263:43 - the unsorted list and returns its index
263:46 - then we call the pop method on the list
263:48 - and pass it the index of the minimum
263:50 - value pop will remove that item from the
263:52 - list and return it we then add that
263:55 - value to the end of the sorted list
263:57 - going up a level of indentation signals
264:00 - to python that we're ending the loop
264:02 - after the loop finishes we return the
264:04 - sorted list
264:06 - now we need to write the function that
264:08 - picks out the minimum value we pass in
264:10 - the list we're going to search
264:13 - we mark the first value in the list as
264:15 - the minimum it may or may not be the
264:17 - actual minimum but it's the smallest
264:19 - we've seen on this pass through the list
264:21 - it's also the only value we've seen on
264:23 - this pass through the list so far
264:25 - now we loop through the remaining values
264:27 - in the list after the first
264:30 - we test whether the value we're
264:32 - currently looking at is less than the
264:33 - previously recorded
264:36 - minimum if it is then we set the current
264:39 - index as the new index of the minimum
264:41 - value
264:42 - after we've looped through all the
264:44 - values we return the index of the
264:45 - smallest value we found
264:47 - lastly we need to actually run our
264:49 - selection sort method and print the
264:51 - sorted list it returns
264:53 - let's save this and now let's try
264:55 - running it we run the python command and
264:58 - pass it the name of our script
265:00 - selectionsort.pi
265:03 - in the numbers directory i've saved
265:04 - several data files filled with random
265:06 - numbers one on each line five dot text
265:08 - has five lines eight dot text has eight
265:11 - lines and to help us really measure the
265:12 - speed of our algorithms ten thousand dot
265:15 - text has ten thousand lines i've even
265:17 - created a file with a million numbers
265:20 - our script takes the path of a file to
265:22 - load as an argument so i'll give it the
265:23 - path of our file with five numbers
265:25 - numbers slash five dot text
265:29 - the script runs reads the numbers in the
265:31 - file into a list calls our selection
265:33 - sort method with that list and then
265:35 - prints the sorted list
265:37 - let me add a couple print statements
265:39 - within the selection sort function so
265:41 - you can watch the sort happening
265:43 - don't worry about figuring out the
265:44 - python formatting string that i use it's
265:46 - just there to keep the two lists neatly
265:48 - aligned
265:49 - i'll add the first print statement
265:51 - before the loop runs at all
265:57 - i'll have it print out the unsorted list
265:59 - and the sorted list
266:01 - i'll add an identical print statement
266:03 - within the loop so we can watch values
266:05 - moving from the unsorted list to the
266:06 - sorted list
266:09 - let's save this
266:13 - and we'll try running the same command
266:15 - again the output looks like this you can
266:18 - see the unsorted list on the left and
266:20 - the sorted list on the right
266:22 - initially the sorted list is empty on
266:24 - the first pass it selects the lowest
266:26 - number 1 and moves it to the sorted list
266:29 - then it moves the next lowest number
266:31 - over four
266:33 - this repeats until all the numbers have
266:34 - been moved to the sorted list
266:37 - i have another file with eight different
266:38 - numbers in it let's try our program with
266:40 - that
266:41 - python selection sort dot pi numbers
266:45 - 8.text
266:49 - you can see the same process at work
266:51 - here notice that this file had some
266:53 - duplicate values too that's okay though
266:56 - because the index of min function only
266:58 - updates the minimum index if the current
267:00 - value is less than the previous minimum
267:02 - if they're equal it just keeps the first
267:04 - minimum value it found and waits to move
267:06 - the duplicate value over until the next
267:09 - pass through the list
267:11 - so now we know that the selection sort
267:13 - algorithm works but the data sets we've
267:16 - been giving it sort are tiny in the real
267:18 - world algorithms need to work with data
267:21 - sets of tens of thousands or even
267:23 - millions of items and do it fast i have
267:26 - another file with ten thousand random
267:28 - numbers in it
267:33 - let's see if selection sort can handle
267:35 - that
267:36 - if i run this as it is now though it'll
267:38 - print out a lot of debug info as it
267:40 - sorts the list so first i'm going to go
267:42 - into the program and remove the two
267:44 - print statements in the selection sort
267:46 - function
267:50 - now let's run the program again on the
267:53 - dot text file and see how long it takes
267:56 - python selection sort dot pi
267:59 - numbers
268:01 - ten thousand dot text
268:04 - one one thousand two one thousand three
268:06 - one four one thousand five one thousand
268:09 - six one thousand seven one thousand
268:10 - eight one thousand nine one thousand ten
268:12 - one thousand eleven thousand twelve
268:15 - thousand thirteen thousand and it prints
268:17 - out all ten thousand of those numbers
268:19 - neatly sorted it took a little bit
268:21 - though how long well counting the time
268:24 - off vocally isn't very precise and other
268:26 - programs running on the system can skew
268:28 - the amount of time your program takes to
268:30 - complete
268:31 - let me show you a unix command that's
268:33 - available here in workspaces which can
268:35 - help you type time followed by a space
268:38 - and then the command you want to run
268:41 - so this command by itself will print the
268:43 - contents of our 5.txt file cat as in
268:46 - concatenate numbers 5.text
268:50 - and this command will do the same thing
268:51 - but it'll also keep track of how long it
268:54 - takes the cat program to complete and
268:56 - report the result time
268:58 - cat
268:59 - numbers five dot text
269:03 - the real row in the results is the
269:05 - actual amount of time for when the
269:07 - program started running to when it
269:09 - completed we can see it finished in a
269:11 - fraction of a second but as we said
269:13 - other programs running on the system can
269:15 - take cpu resources in which case your
269:18 - program will seem slower than it is so
269:20 - we generally want to ignore the real
269:22 - result
269:23 - the user result is the amount of time
269:25 - the cpu actually spent running the
269:27 - program code so this is the total amount
269:30 - of time the code inside the cat program
269:32 - took to run
269:33 - the sys result is the amount of time the
269:36 - cpu spent running linux kernel calls
269:38 - that your code made the linux kernel is
269:41 - responsible for things like network
269:42 - communications and reading files so
269:45 - loading the 5.txt file is probably
269:47 - included in this result
269:49 - in evaluating code's performance we're
269:51 - generally going to want to add together
269:53 - the user and sys results but cad is a
269:56 - very simple program let's try running
269:58 - the time command on our code and see if
270:01 - we get a more interesting result
270:03 - time python
270:05 - selection sort dot pi
270:07 - numbers
270:08 - ten thousand dot text
270:13 - this takes much longer to complete
270:15 - nearly 12 seconds according to the real
270:17 - time measurement but as we said the real
270:20 - result is often skewed so let's
270:21 - disregard that
270:23 - if we add the user and cis runtimes
270:26 - together we get about 6 seconds
270:30 - the time for the program to complete
270:32 - will vary a little bit each time you run
270:33 - it but if it's doing the same operations
270:36 - it usually won't change more than a
270:37 - fraction of a second if i run our
270:39 - selection sort script on the same file
270:41 - you can see it completes in roughly the
270:43 - same time
270:44 - now let's try it on another file with 1
270:47 - million numbers time python selection
270:50 - sort dot pi numbers
270:52 - 1 million dot text
270:56 - how long does this one take i don't even
270:58 - know while designing this course i tried
271:00 - running this command and my workspace
271:02 - connection timed out before it completed
271:04 - so we'll just say that selection sort
271:06 - takes a very very long time to sort a
271:09 - million numbers
271:10 - if we're going to sort a list that big
271:12 - we're going to need a faster algorithm
271:14 - we'll look into alternative sorting
271:16 - algorithms shortly
271:18 - the next two sorting algorithms we look
271:20 - at will rely on recursion which is the
271:22 - ability of a function to call itself so
271:25 - before we move on we need to show you
271:27 - how recursion works
271:29 - recursive functions can be very tricky
271:31 - to understand imagine a row of dominoes
271:34 - stood on end where one domino falling
271:36 - over causes the next domino to fall over
271:38 - which causes the next domino to fall
271:40 - over causing a chain reaction it's kind
271:43 - of like that
271:44 - let's suppose we need to write a
271:46 - function that adds together all the
271:47 - numbers in an array or in the case of
271:49 - python a list
271:51 - normally we'd probably use a loop for
271:53 - this sort of operation
271:55 - the function takes a list of the numbers
271:57 - we want to add
271:58 - the total starts at zero
272:01 - we loop over every number contained in
272:03 - the list and we add the current number
272:05 - to the total
272:07 - once we're done looping we return the
272:09 - accumulated total
272:11 - if we call this sum function with a list
272:13 - of numbers it'll return the total when
272:16 - we run this program it'll print out that
272:18 - return value 19. let's try it real quick
272:21 - python
272:23 - recursion.pi oh whoops
272:25 - mustn't forget to save my work here
272:28 - and run it and we see the result 19.
272:32 - to demonstrate how recursion works let's
272:35 - revise the sum function to use recursion
272:37 - instead note that recursion is not the
272:39 - most efficient way to add a list of
272:41 - numbers together but this is a good
272:43 - problem to use to demonstrate recursion
272:45 - because it's so simple one thing before
272:48 - i show you the recursive version though
272:50 - this example is going to use the python
272:52 - slice syntax so i need to take a moment
272:54 - to explain that for those not familiar
272:56 - with it
272:57 - a slice is a way to get a series of
272:59 - values from a list
273:01 - let's load up the python repel or read
273:03 - evaluate print loop so i can demonstrate
273:07 - we'll start by creating a list of
273:08 - numbers to work with numbers equals
273:11 - a list
273:13 - with 0 1 2 3 and 4
273:16 - containing those numbers
273:18 - like arrays in most other languages
273:20 - python list indexes start at 0 so
273:23 - numbers
273:24 - 1
273:25 - will actually get the second item from
273:27 - the list
273:28 - with slice notation i can actually get
273:30 - several items back
273:32 - it looks just like accessing an
273:34 - individual index of a list
273:37 - but then i type a colon
273:39 - followed by the list index that i want
273:41 - up to but not including
273:43 - so numbers 1 colon 4 would get us the
273:46 - second up to but not including the fifth
273:49 - items from the list that is it'll get us
273:52 - the second through the fourth items
273:54 - now i know what you're thinking and
273:56 - you're right that up to but not
273:57 - including rule is a little
273:58 - counterintuitive but you can just forget
274:01 - all about it for now because we won't be
274:03 - using a second index with any of our
274:04 - python slice operations in this course
274:08 - here's what we will be using when you
274:10 - leave the second index off of a python
274:12 - slice it gives you the items from the
274:14 - first index up through the end of the
274:16 - list wherever that is so numbers 1 colon
274:19 - with no index following it will give us
274:21 - items from the second index up through
274:23 - the end of the list
274:25 - numbers 2 colon will give us items from
274:27 - the third index up to the end of the
274:29 - list
274:30 - you can also leave the first index off
274:32 - to get everything from the beginning of
274:34 - the list numbers
274:36 - colon 3 will get us everything from the
274:39 - beginning of the list up to but not
274:41 - including the third index
274:43 - it's also worth noting that if you take
274:45 - a list with only one item and you try to
274:47 - get everything from the non-existent
274:49 - second item onwards the result will be
274:51 - an empty list
274:54 - so if i create a list with just one item
274:57 - in it
274:58 - and i try
275:00 - to access from the second element
275:03 - onwards the second element doesn't exist
275:06 - so the result will be an empty list
275:09 - don't worry too much about remembering
275:11 - python slice syntax it's not an
275:13 - essential part of sorting algorithms or
275:15 - recursion i'm only explaining it to help
275:17 - you read the code you're about to see
275:20 - so i'm going to exit the python rebel
275:22 - now that we've covered recursion we can
275:24 - convert our sum function to a recursive
275:27 - function
275:28 - it'll take the list of numbers to add
275:30 - just like before
275:32 - now here's the recursive part we'll have
275:34 - the sum function call itself we use
275:37 - slice notation to pass the entire list
275:39 - of numbers except the first one
275:42 - then we add the first number in the list
275:43 - to the result of the recursive function
275:45 - call and return the result
275:48 - so if we call sum with four numbers
275:50 - first it'll call itself with the
275:52 - remaining three numbers that call to sum
275:54 - will then call itself with the remaining
275:56 - two numbers and so on
275:58 - but if we save this and try to run it
276:01 - pythonrecursion.pi
276:07 - well first we get a syntax error it
276:08 - looks like i accidentally indented
276:10 - something i shouldn't have so let me go
276:12 - fix that real quick
276:16 - there we go that's suggested to python
276:18 - that there was a loop or something there
276:19 - when there wasn't
276:21 - so let's go back to the terminal and try
276:22 - running this again
276:24 - there we go now we're getting the error
276:26 - i was expecting recursion error maximum
276:29 - recursion depth exceeded
276:31 - this happens because some gets into an
276:33 - infinite loop it keeps calling itself
276:35 - over and over the reason is that when we
276:38 - get down to a list of just one element
276:40 - and we take a slice from the
276:41 - non-existent second element to the end
276:43 - the result is an empty list that empty
276:46 - list gets passed to the recursive call
276:48 - to sum which passes an empty list in its
276:50 - recursive call to sum and so on until
276:53 - the python interpreter detects too many
276:55 - recursive calls and shuts the program
276:57 - down what we need is to add a base case
276:59 - to this recursive function a condition
277:02 - where the recursion stops this will keep
277:04 - it from getting into an infinite loop
277:06 - with the sum function the base case is
277:08 - when there are no elements left in the
277:10 - list in that case there is nothing left
277:12 - to add and the recursion can stop
277:15 - a base case is the alternative to a
277:17 - recursive case a condition where
277:19 - recursion should occur for the sum
277:21 - function the recursive case is when
277:23 - there are still elements in the list to
277:25 - add together
277:26 - let's add a base case at the top of the
277:28 - function
277:31 - python treats a list that contains one
277:33 - or more values as a true value and it
277:35 - treats a list containing no values as a
277:38 - false value
277:39 - so we'll add an if statement that says
277:41 - if there are no numbers in the list we
277:43 - should return a sum of zero that way the
277:46 - function will exit immediately without
277:48 - making any further recursive calls to
277:50 - itself
277:51 - we'll leave the code for the recursive
277:52 - case unchanged
277:54 - if there are still numbers in the list
277:56 - the function will call itself with any
277:57 - numbers after the first then add the
277:59 - return value to the first number in the
278:01 - list
278:02 - let's save this and try running it again
278:05 - python recursion dot pi
278:10 - output the sum of the numbers in the
278:11 - list 19 but it's still not really clear
278:14 - how this worked let's add a couple print
278:16 - statements that will show us what it's
278:18 - doing
278:19 - we'll show the recursive call to sum and
278:21 - what it's being called with
278:26 - we'll also add a call to print right
278:28 - before we return showing which of the
278:29 - calls the sum is returning and what it's
278:32 - returning
278:38 - let me save this and resize the console
278:41 - a bit
278:43 - and let's try running it again
278:46 - python recursion.pi
278:49 - since the print calls are inside the sum
278:51 - function the first call to sum 1279
278:54 - isn't shown only the recursive calls are
278:57 - this first call to sum ignores the first
278:59 - item in the list 1 and calls itself
279:02 - recursively it passes the remaining
279:04 - items from the list 2 7 and 9.
279:07 - that call to sum again ignores the first
279:09 - item in the list it receives 2 and again
279:11 - calls itself recursively it passes the
279:13 - remaining items in the list 7 and 9.
279:16 - that call ignores the 7 and calls itself
279:19 - with a 9
279:20 - and the last call shown here ignores the
279:22 - 9
279:23 - and calls itself with an empty list
279:26 - at this point none of our recursive
279:27 - calls to sum have returned yet each of
279:30 - them is waiting on the recursive call it
279:32 - made to sum to complete
279:34 - python and other programming languages
279:36 - use something called a call stack to
279:38 - keep track of this series of function
279:40 - calls each function call is added to the
279:42 - stack along with the place in the code
279:44 - that it needs to return when it
279:46 - completes
279:47 - but now the empty list triggers the base
279:49 - case causing the recursion to end and
279:51 - the sum function to return zero
279:54 - that zero value is returned to its
279:56 - caller the caller adds the zero to the
279:58 - first and only value in its list nine
280:01 - the result is nine
280:03 - that nine value gets returned to the
280:04 - caller which adds it to the first value
280:06 - in the list it received seven
280:09 - the result is sixteen
280:12 - that sixteen value is returned to the
280:14 - caller which adds it to the first value
280:15 - in the list it received two the result
280:18 - is 18.
280:21 - that 18 value is returned to the caller
280:23 - which adds it to the first value in the
280:25 - list it received one the result is 19.
280:30 - that 19 value is returned to the caller
280:32 - which is not the sum function
280:33 - recursively calling itself but our main
280:35 - program this is our final result which
280:38 - gets printed
280:40 - it's the same result we got from the
280:41 - loop-based version of our program the
280:44 - end
280:45 - we don't want the print statements in
280:46 - our final version of the program so let
280:48 - me just delete those real quick
280:51 - and there you have it a very simple
280:52 - recursive function well the function is
280:55 - simple but as you can see the flow of
280:57 - control is very complex don't worry if
280:59 - you didn't understand every detail here
281:01 - because we won't be using this
281:03 - particular example again
281:05 - there are two fundamental mechanisms you
281:07 - need to remember a recursive function
281:09 - needs a recursive case that causes it to
281:11 - call itself
281:12 - and it also needs to eventually reach a
281:15 - base case that causes the recursion to
281:17 - stop
281:18 - you've seen bogo sort which doesn't make
281:20 - any progress towards sorting a list with
281:22 - each pass either it's entirely sorted or
281:25 - it isn't
281:26 - you've seen selection sort which moves
281:29 - one value over to a sorted list with
281:31 - each pass so that it has fewer items to
281:33 - compare each time
281:35 - now let's look at an algorithm that
281:36 - speeds up the process further by further
281:38 - reducing the number of comparisons it
281:40 - makes it's called quick sort
281:43 - here's some python code where we'll
281:45 - implement quick sort again you can
281:47 - ignore these lines at the top we're just
281:49 - using them to load a file full of
281:51 - numbers into a list
281:52 - the quick sort algorithm relies on
281:54 - recursion to implement it we'll write a
281:57 - recursive function we'll accept the list
281:59 - of numbers to sort as a parameter
282:02 - quicksort is recursive because it keeps
282:04 - calling itself with smaller and smaller
282:06 - subsets of the list you're trying to
282:07 - sort we're going to need a base case
282:10 - where the recursion stops so it doesn't
282:12 - enter an infinite loop
282:14 - lists that are empty don't need to be
282:16 - sorted and lists with just one element
282:18 - don't need to be sorted either in both
282:20 - cases there's nothing to flip around so
282:23 - we'll make that our base case if there
282:25 - are zero or one elements in the list
282:27 - passed to the quick sort function we'll
282:29 - return the unaltered list to the caller
282:32 - lastly we need to call our quick sort
282:34 - function with our list of numbers and
282:36 - print the list it returns
282:42 - that takes care of our base case now we
282:44 - need a recursive case
282:46 - we're going to rely on a technique
282:48 - that's common in algorithm design called
282:50 - divide and conquer basically we're going
282:52 - to take our problem and split it into
282:54 - smaller and smaller problems until
282:56 - they're easy to solve
282:58 - in this case that means taking our list
283:00 - and splitting it into smaller lists
283:02 - viewers a suggestion the process i'm
283:05 - about to describe is complex there's
283:07 - just no way around it if you're having
283:09 - trouble following along remember the
283:11 - video playback controls feel free to
283:13 - slow the play back down rewind or pause
283:15 - the video as needed after you watch this
283:18 - the first time you may also find it
283:20 - helpful to rewind and make your own
283:21 - diagram of the process as we go
283:24 - okay ready here goes
283:27 - suppose we load the numbers from our
283:29 - 8.txt file into a list how do we divide
283:32 - it
283:33 - it would probably be smart to have our
283:35 - quicksort function divide the list in a
283:37 - way that brings it closer to being
283:39 - sorted let's pick an item from the list
283:42 - we'll just pick the first item for now
283:44 - four
283:45 - we'll call this value we've picked the
283:46 - pivot like the center of a seesaw on a
283:48 - playground
283:50 - we'll break the list into two sublists
283:52 - the first sub-list will contain all the
283:54 - items in the original list that are
283:55 - smaller than the pivot the second
283:57 - sub-list will contain all the items in
283:59 - the original list that are greater than
284:00 - the pivot
284:02 - the sub list of values less than and
284:04 - greater than the pivot aren't sorted
284:06 - but what if they were you could just
284:09 - join the sub lists and the pivot all
284:10 - together into one list and the whole
284:12 - thing would be sorted
284:14 - so how do we sort the sublist we call
284:16 - our quick sort function recursively on
284:18 - them this may seem like magic but it's
284:21 - not it's the divide and conquer
284:23 - algorithm design technique at work
284:25 - if our quick sort function works on the
284:27 - big list then it will work on the
284:29 - smaller list too
284:31 - for our first sub list we take the first
284:33 - item it's the pivot again
284:35 - that's three
284:37 - we break the sub list into two sub lists
284:39 - one with everything less than the pivot
284:41 - and one with everything greater than the
284:42 - pivot
284:43 - notice that there's a value equal to the
284:45 - pivot that gets put into the less than
284:47 - sub-list our finished quicksort function
284:50 - is actually going to put everything
284:51 - that's less than or equal to the pivot
284:53 - in the first sub-list
284:55 - but i don't want to say less than or
284:57 - equal to over and over so i'm just
284:58 - referring to it as the less than pivot
285:01 - sub-list
285:02 - also notice that there are no values
285:04 - greater than the pivot that's okay when
285:06 - we join the sub-lists back together that
285:08 - just means nothing will be in the return
285:10 - list after the pivot
285:12 - we still have one sub list that's more
285:14 - than one element long so we call our
285:16 - quick sort function on that too you and
285:18 - i can see that it's already sorted but
285:20 - the computer doesn't know that so it'll
285:22 - call it anyway just in case
285:24 - it picks the first element 2 as a pivot
285:27 - there are no elements less than the
285:28 - pivot and only one element greater than
285:30 - the pivot
285:32 - that's it for the recursive case we've
285:34 - finally hit the base case for our quick
285:35 - sort function it'll be called on both
285:38 - the empty list of elements less than the
285:39 - pivot and the one item list of elements
285:41 - greater than the pivot but both of these
285:44 - lists will be returned as they are
285:46 - because there's nothing to sort
285:48 - so now at the level of the call stack
285:50 - above this the return sorted lists are
285:52 - used in place of the unsorted sub-list
285:54 - that's less than the pivot and the
285:55 - unsorted sub-list that's greater than
285:57 - the pivot
285:58 - these are joined together into one
286:00 - sorted list remember that any empty
286:02 - lists get discarded
286:04 - then at the level of the call stack
286:06 - above that the return sorted lists are
286:08 - used in place of the unsorted sub-lists
286:10 - there again they were already sorted but
286:12 - the quick sort method was called on them
286:14 - anyway just in case
286:16 - the sub-lists are joined together into
286:18 - one sorted list at the level of the call
286:21 - stack above that the return sorted list
286:23 - is used in place of the unsorted
286:25 - sub-list that's less than the pivot so
286:27 - now everything that's less than or equal
286:28 - to the pivot is sorted
286:30 - now we call quick sort on the unsorted
286:32 - sub-list that's greater than the pivot
286:34 - and the process repeats for that
286:36 - sub-list
286:37 - we pick the first element six is the
286:39 - pivot we split the sub-list into
286:41 - sub-lists of elements that are less than
286:43 - and greater than this pivot and we
286:45 - recursively call the quicksort function
286:47 - until those sub-lists are sorted
286:49 - eventually a sorted sub-list is returned
286:52 - to our first quick sort function call
286:54 - we combine the sub-list that's less than
286:56 - or equal to the pivot the pivot itself
286:59 - and the sub-list that's greater than the
287:00 - pivot into a single list and because we
287:03 - recursively sorted the sub lists the
287:05 - whole list is sorted
287:07 - so that's how the quick sort function is
287:09 - going to work in the next video we'll
287:11 - show you the actual code
287:13 - quicksort works by picking a pivot value
287:15 - then splitting the full list into two
287:17 - sub-lists the first sub-list has all the
287:20 - values less than or equal to the pivot
287:22 - and the second sub-list has all the
287:23 - values greater than the pivot the quick
287:25 - sort function recursively calls itself
287:28 - to sort these sub-lists and then to sort
287:30 - the sub-lists of those sub-lists until
287:32 - the full list is sorted
287:34 - now it's time to actually implement this
287:36 - in code
287:38 - we already have the base case written
287:40 - any list passed in that consists of 0 or
287:42 - 1 values will be returned as is because
287:45 - there's nothing to sort
287:46 - now we need to create a list that will
287:48 - hold all the values less than the pivot
287:50 - that list will be empty at first we do
287:53 - the same for values greater than the
287:55 - pivot
287:56 - next we need to choose the pivot value
287:58 - for now we just grab the first item from
288:00 - the list
288:01 - then we loop through all the items in
288:03 - the list following the pivot
288:06 - we check to see whether the current
288:07 - value is less than or equal to the pivot
288:10 - if it is we copy it to the sub-list of
288:13 - values less than the pivot
288:16 - otherwise the current value must be
288:18 - greater than the pivot
288:20 - so we copy it to the other list
288:26 - this last line is where the recursive
288:28 - magic happens we call quick sort
288:31 - recursively on the sub-list that's less
288:33 - than the pivot we do the same for the
288:35 - sub-list that's greater than the pivot
288:37 - those two calls will return sorted lists
288:40 - so we combine the sort of values less
288:42 - than the pivot the pivot itself and the
288:44 - sort of values greater than the pivot
288:46 - that gives us a complete sorted list
288:48 - which we return
288:50 - this took a lot of prep work are you
288:52 - ready let's try running it python
288:57 - quick sort
288:58 - dot pi
288:59 - numbers 8.text
289:02 - it outputs our sorted list
289:04 - i don't know about you but this whole
289:06 - thing still seems a little too magical
289:08 - to me let's add a couple print
289:09 - statements to the program so we can see
289:11 - what it's doing
289:13 - first we'll add a print statement right
289:14 - before the first call to the quick sort
289:16 - function so we can see the unsorted list
289:19 - we'll also add a print right within the
289:21 - quick sort function right before the
289:23 - recursive calls again this string
289:25 - formatting code is just to keep the info
289:27 - aligned in columns
289:38 - let's try running this again
289:40 - and now you can see our new debug output
289:43 - each time quicksort goes to call itself
289:45 - recursively it prints out the pivot as
289:47 - well as the sub list of items less than
289:49 - or equal to the pivot if any and the sub
289:52 - list of items greater than the pivot if
289:54 - any you can see that first it sorts the
289:56 - sub list of items less than the pivot at
289:58 - the top level
290:00 - it goes through a couple levels of
290:02 - recursion to do that
290:04 - there are actually additional levels of
290:06 - recursion but they're from calls to
290:08 - quick sort with a list of 0 or 1
290:10 - elements and those calls return before
290:12 - the print statement is reached
290:14 - then it starts sorting the second sub
290:16 - list from the top level with items
290:18 - greater than the original pivot
290:20 - you can see a couple levels of recursion
290:22 - for that sort as well
290:24 - finally when both sublists are
290:26 - recursively sorted the original call to
290:28 - the quicksort function returns and we
290:30 - get the sorted list back
290:32 - so we know that it works the next
290:34 - question is how well does it work let's
290:36 - go back to our file of ten thousand
290:38 - numbers and see if it can sort those
290:41 - first though i'm going to remove our two
290:42 - debug calls to print so it doesn't
290:44 - produce unreadable output
290:47 - a quick note if you try running this on
290:49 - a file with a lot of repeated values
290:51 - it's possible you'll get a runtime error
290:53 - maximum recursion depth exceeded
290:56 - if you do see the teacher's notes for a
290:58 - possible solution
291:01 - now let's try running our quick sort
291:02 - program against the ten thousand dot
291:04 - text file python
291:06 - quick sort dot pi
291:08 - numbers 10 000 dot text
291:12 - there we go and it seems pretty fast but
291:15 - how fast exactly let's run it with the
291:17 - time command to see how long it takes
291:19 - time python
291:22 - quick sort dot pi
291:24 - numbers 10 000.text
291:28 - remember we need to ignore the real
291:30 - result and add the user and sys results
291:33 - it took less than a second of cpu time
291:36 - to sort 10 000 numbers with quicksort
291:39 - remember that selection sort took about
291:41 - 13 seconds
291:42 - that's a pretty substantial improvement
291:45 - and with a million numbers selection
291:47 - sort took so long that it never even
291:49 - finished successfully let's see if
291:51 - quicksort performs any better
291:54 - time python quick sort dot pi
291:58 - numbers
292:00 - 1 million dot text
292:08 - not only did quicksort sort a million
292:10 - numbers successfully it only took about
292:12 - 11 seconds of cpu time
292:15 - quicksort is clearly much much faster
292:17 - than selection sort how much faster
292:20 - that's something we'll discuss in a
292:21 - later video
292:23 - what we've shown you here is just one
292:24 - way to implement quicksort
292:26 - although the basic algorithm is always
292:28 - the same the details can vary like how
292:31 - you pick the pivot see the teacher's
292:33 - notes for more details
292:35 - let's review another sorting algorithm
292:38 - merge sort so that we can compare it
292:39 - with quick sort merge sort is already
292:42 - covered elsewhere on the site so we
292:44 - won't go into as much detail about it
292:46 - but we'll have more info in the
292:47 - teacher's notes if you want it
292:49 - both quicksort and merge sword are
292:51 - recursive the difference between them is
292:54 - in the sorting mechanism itself whereas
292:56 - quicksort sorts a list into two
292:58 - sub-lists that are less than or greater
293:00 - than a pivot value
293:02 - merge sort simply splits the list in
293:04 - half recursively and then sorts the
293:06 - halves as it merges them back together
293:08 - that's why it's called merge sort
293:11 - you may recognize this code at the top
293:13 - by now it just loads a file full of
293:15 - numbers into a list
293:17 - let's define a recursive merge sort
293:19 - function as usual it'll take the list or
293:22 - sub-list that we want it to sort
293:24 - our base case is the same as with
293:26 - quicksort if the list has zero or one
293:28 - values there's nothing to sort so we
293:30 - return it as is
293:32 - if we didn't return it means we're in
293:34 - the recursive case so first we need to
293:36 - split the list in half we need to know
293:39 - the index we should split on so we get
293:41 - the length of the list and divide it by
293:43 - two so for example if there are eight
293:45 - items in the list we'll want an index of
293:47 - four
293:48 - but what if there were an odd number of
293:50 - items in the list like seven we can't
293:52 - have an index of 3.5 so we'll need to
293:54 - round down in that case since we're
293:56 - working in python currently we can take
293:59 - advantage of a special python operator
294:01 - that divides and rounds the result down
294:03 - the floor division operator it consists
294:06 - of a double slash
294:09 - now we'll use the python slice syntax to
294:11 - get the left half of the list
294:13 - we'll pass that list to a recursive call
294:15 - to the merge sort function
294:18 - we'll also use slice syntax to get the
294:20 - right half of the list and pass that to
294:22 - merge sort as well
294:26 - now we need to merge the two halves
294:27 - together and sort them as we do it we'll
294:30 - create a list to hold the sorted values
294:33 - and now we get to the complicated part
294:35 - merging the two halves together and
294:37 - sorted them as we do it
294:39 - we'll be moving from left to right
294:40 - through the left half of the list
294:42 - copying values over to the sorted values
294:44 - list as we go this left index variable
294:47 - will help us keep track of our position
294:50 - at the same time we'll also be moving
294:52 - from left to right through the right
294:53 - half of the list and copying values over
294:56 - so we need a separate write index
294:57 - variable to track that position as well
295:00 - we'll keep looping until we've processed
295:02 - all of the values in both halves of the
295:04 - list
295:13 - we're looking to copy over the lowest
295:15 - values first so first we test whether
295:17 - the current value on the left side is
295:20 - less than the value on the right side
295:23 - if the left side value is less that's
295:26 - what we'll copy over to the sorted list
295:32 - and then we'll move to the next value in
295:34 - the left half of the list
295:36 - otherwise the current value from the
295:38 - right half must have been lower
295:40 - so we'll copy that value to the sorted
295:42 - list instead
295:49 - and then we'll move to the next value in
295:50 - the right half of the list
295:53 - that ends the loop at this point one of
295:55 - the two unsorted halves still has a
295:57 - value remaining and the other is empty
295:59 - we won't waste time checking which is
296:01 - which we'll just copy the remainder of
296:03 - both lists over to the sorted list the
296:05 - one with the value left will add that
296:07 - value and the empty one will add nothing
296:10 - all the numbers from both halves should
296:11 - now be copied to the sorted list so we
296:13 - can return it
296:15 - finally we need to kick the whole
296:16 - process off we'll call the merge sort
296:18 - function with the list of numbers we
296:20 - loaded and print the result
296:30 - let's save this
296:36 - and we'll try it out on our file with
296:38 - eight numbers
296:39 - python merge sort dot pi
296:42 - numbers
296:43 - eight dot text
296:45 - and it prints out the sorted list
296:47 - but again this seems pretty magical
296:50 - let's add some print statements to get
296:51 - some insight into what it's doing
296:55 - first we'll print the unsorted list so
296:57 - we can refer to it we'll add a print
296:59 - statement right before we call the merge
297:01 - sort function for the first time
297:06 - then we'll add another print statement
297:07 - within the merge sort function right
297:09 - after the recursive calls this will show
297:12 - us the sorted left half and right half
297:13 - that it's returning again don't worry
297:15 - about the fancy python formatting string
297:18 - it just keeps the values neatly aligned
297:20 - let me resize my console
297:23 - clear the screen
297:24 - and then we'll try running this again
297:28 - what we're seeing are the values being
297:30 - returned from the recursive merge sort
297:32 - function calls not the original calls to
297:34 - merge sort so what you see here is after
297:37 - we reach the base case with a list
297:38 - that's only one item in length and the
297:40 - recursive calls start returning
297:43 - the original list gets split into two
297:45 - unsorted halves four six three and two
297:48 - and nine seven three and five
297:51 - the first half gets split in half again
297:53 - four and six and three and two
297:57 - and each of those halves is halved again
297:59 - into single element lists
298:01 - there's nothing to sort in the single
298:03 - element list so they're returned from
298:05 - the merge sort function as is
298:07 - those single element lists get merged
298:09 - into two sub lists and sorted as they do
298:11 - so the four and six sub-list looks the
298:14 - same after sorting as it did before
298:16 - sorting but the three and the two get
298:18 - sorted as they're combined into a
298:19 - sub-list the new order is two three
298:23 - the order is shifted again when those
298:24 - two sub-lists get combined back into a
298:27 - single list two three four six
298:30 - then we recursively sort the right half
298:32 - of the original list
298:34 - nine seven three five
298:36 - it gets split in half again nine seven
298:39 - and three five
298:41 - and each of those halves get broken into
298:43 - single element lists
298:45 - there's nothing to sort there so the
298:46 - single element lists are returned as is
298:50 - the first two are sorted as they're
298:51 - merged seven nine and so are the second
298:54 - three five
298:56 - and then those two sub lists get sorted
298:58 - as they're combined into another sub
299:00 - list three five seven nine
299:04 - and finally everything is sorted as it's
299:06 - merged back into the full sorted list
299:09 - two three three four five six seven nine
299:13 - that's how merge sort works on a list of
299:15 - eight numbers let's see if it works on a
299:16 - bigger list
299:19 - first i'll remove the two print
299:21 - statements so we don't get an
299:22 - overwhelming amount of debug output
299:28 - then i'll run it on a list of ten
299:30 - thousand items python merge sort dot pi
299:33 - numbers ten thousand dot
299:36 - text
299:38 - not only did it work it was pretty fast
299:41 - but which is faster merge sort or quick
299:43 - sort we'll look at that next
299:46 - i've removed the call to print that
299:47 - displays the sorted list at the end of
299:49 - our selection sort quick sort and merge
299:51 - sort scripts
299:53 - that way it'll still run the sort but
299:55 - the output won't get in the way of our
299:56 - comparing runtimes
299:59 - let's try running each of these scripts
300:01 - and see how long it takes
300:04 - time python
300:07 - selection sort we'll do that one first
300:09 - numbers
300:10 - 10 000 dot text
300:16 - we combine the user and sys results and
300:19 - that gives us about six seconds
300:21 - now let's try quick sort time python
300:24 - quick sort
300:26 - dot pi numbers
300:28 - ten thousand dot text
300:32 - much faster less than a second and
300:34 - finally time python
300:37 - merge sort dot pi numbers ten thousand
300:41 - dot text
300:44 - a little longer but far less than a
300:46 - second so even on a list with just 10
300:49 - 000 numbers selection sort takes many
300:52 - times as long as quicksort and merge
300:54 - sort
300:55 - and remember i ran the selection sort
300:57 - script on a file with a million numbers
300:59 - and it took so long that my workspace
301:01 - timed out before it completed
301:04 - it looks like selection sort is out of
301:06 - the running as a viable sorting
301:07 - algorithm it may be easy to understand
301:10 - and implement but it's just too slow to
301:12 - handle the huge data sets that are out
301:14 - in the real world
301:17 - now let's try quicksort and merge sort
301:18 - on our file with a million numbers and
301:20 - see how they compare there time python
301:25 - quicksort dot pi
301:27 - numbers
301:29 - million
301:30 - dot text
301:35 - looks like it took about 11 seconds of
301:37 - cpu time
301:38 - now let's try merge sort time python
301:42 - merge sort dot pi
301:44 - numbers
301:45 - 1 million
301:47 - dot text
301:51 - that took about 15 seconds of cpu time
301:54 - it looks like quicksort is marginally
301:56 - faster than merge sort on this sample
301:58 - data
301:59 - we had to learn a lot of details for
302:01 - each algorithm we've covered in this
302:03 - course developers who need to implement
302:05 - their own algorithms often need to
302:07 - choose an algorithm for each and every
302:08 - problem they need to solve and they
302:11 - often need to discuss their decisions
302:12 - with other developers can you imagine
302:15 - needing to describe all the algorithms
302:17 - in this same level of detail all the
302:19 - time you'd spend all your time in
302:21 - meetings rather than programming
302:23 - that's why big o notation was created as
302:26 - a way of quickly describing how an
302:28 - algorithm performs as the data set it's
302:30 - working on increases in size
302:32 - big o notation lets you quickly compare
302:34 - several algorithms to choose the best
302:36 - one for your problem
302:38 - the algorithms we've discussed in this
302:40 - course are very well known some job
302:42 - interviewers are going to expect you to
302:44 - know their big o run times so let's look
302:46 - at them
302:47 - remember that the n in big o notation
302:50 - refers to the number of elements you're
302:51 - operating on with selection sort you
302:54 - need to check each item in the list to
302:56 - see if it's the lowest so you can move
302:57 - it over to the sorted list so that's in
303:00 - operations
303:02 - suppose you're doing selection sort on a
303:04 - list of five items and in this case
303:06 - would be five so that's five operations
303:09 - before you can move an item to the
303:10 - sorted list
303:11 - but with selection sort you have to loop
303:14 - over the entire list for each item you
303:15 - want to move there are five items in the
303:18 - list and you have to do five comparisons
303:20 - to move each one so it's more like 5
303:22 - times 5 operations or if we replace 5
303:25 - with n it's n times n or n squared
303:29 - but wait you might say half of that 5 by
303:31 - 5 grid of operations is missing because
303:34 - we're testing one fewer item in the
303:35 - unsorted list with each pass so isn't it
303:38 - more like one half times n times n
303:41 - and this is true we're not doing a full
303:43 - n squared operations
303:46 - but remember in big o notation as the
303:48 - value of n gets really big constants
303:50 - like one half become insignificant and
303:53 - so we discard them
303:55 - the big o runtime of selection sword is
303:57 - widely recognized as being o n squared
304:01 - quicksort requires one operation for
304:03 - each element of the list it's sorting
304:06 - it needs to select a pivot first and
304:07 - then it needs to sort elements into
304:09 - lists that are less than or greater than
304:11 - the pivot
304:12 - so that's n operations to put that
304:14 - another way if you have a list of eight
304:16 - items then n is eight so it will take
304:18 - eight operations to split the list
304:20 - around the pivot
304:22 - but of course the list isn't sorted
304:24 - after splitting it around the pivot just
304:25 - once you have to repeat those eight
304:27 - operations several times in the best
304:30 - case you'll pick a pivot that's right in
304:31 - the middle of the list so that you're
304:33 - dividing the list exactly in half
304:35 - then you keep dividing the list in half
304:37 - until you have a list with a length of
304:39 - one
304:40 - the number of times you need to divide n
304:42 - in half until you reach one is expressed
304:44 - as log n
304:47 - so you need to repeat n sorting
304:48 - operations log n times that leaves us
304:51 - with the best case run time for quick
304:53 - sort of o n log n
304:56 - but that's the best case what about the
304:59 - worst case well if you pick the wrong
305:01 - pivot you won't be dividing the list
305:03 - exactly in half if you pick a really bad
305:05 - pivot the next recursive call to
305:07 - quicksort will only reduce the list
305:08 - length by one
305:10 - since our quicksort function simply
305:12 - picks the first item to use as a pivot
305:14 - we can make it pick the worst possible
305:16 - pivot repeatedly simply by giving it a
305:18 - list that's sorted in reverse order
305:20 - if we pick the worst possible pivot
305:22 - every time we'll have to split the list
305:24 - once for every item it contains and then
305:27 - do end sorting operations on it
305:30 - you already know another sorting
305:31 - algorithm that only manages to reduce
305:33 - the list by one element with each pass
305:35 - selection sort
305:37 - selection sort has a runtime of o n
305:39 - squared and in the worst case that's the
305:41 - run time for quicksort as well
305:43 - so which do we consider when trying to
305:45 - decide whether to use quicksort the best
305:47 - case or the worst case
305:49 - well as long as your implementation
305:51 - doesn't just pick the first item as a
305:53 - pivot which we did so we could
305:55 - demonstrate this issue
305:56 - it turns out that on average quicksort
305:58 - performs closer to the best case
306:01 - many quicksort implementations
306:03 - accomplish this simply by picking a
306:04 - pivot at random on each recursive loop
306:07 - here we are sorting our reverse sorted
306:09 - data again but this time we pick pivots
306:12 - at random which reduces the number of
306:13 - recursive operations needed
306:16 - sure random pivots sometimes give you
306:18 - the best case and sometimes you'll
306:19 - randomly get the worst case but it all
306:21 - averages out over multiple calls to the
306:23 - quick sort function
306:25 - now with merge sort there's no pivot to
306:27 - pick your list of n items always gets
306:30 - divided in half log n times
306:32 - that means merged sort always has a big
306:35 - o runtime of o and log in
306:38 - contrast that with quicksort which only
306:40 - has a runtime of o and log n in the best
306:42 - case in the worst case quick sorts
306:44 - runtime is o n squared
306:46 - and yet out in the real world quicksort
306:49 - is more commonly used than merge sort
306:51 - now why is that if quicksort's big o
306:53 - runtime can sometimes be worse than
306:55 - merge sorts
306:56 - this is one of those situations where
306:58 - big o notation doesn't tell you the
307:00 - whole story all big o can tell you is
307:02 - the number of times an operation is
307:04 - performed it doesn't describe how long
307:06 - that operation takes
307:08 - and the operation mergesor performs
307:10 - repeatedly takes longer than the
307:12 - operation quicksort performs repeatedly
307:15 - big-o is a useful tool for quickly
307:17 - describing how the runtime of an
307:18 - algorithm increases is the data set it's
307:20 - operating on gets really really big
307:23 - but you can't always choose between two
307:24 - algorithms based just on their big o
307:26 - runtimes sometimes there's additional
307:29 - info you need to know about an algorithm
307:31 - to make a good decision
307:33 - now that we can sort a list of items
307:34 - we're well on our way to being able to
307:36 - search a list efficiently as well we'll
307:38 - look at how to do that in the next stage
307:42 - [Music]
307:46 - now that we've covered sorting
307:48 - algorithms the groundwork has been laid
307:50 - to talk about searching algorithms
307:52 - if you need to search through an
307:53 - unsorted list of items binary search
307:56 - isn't an option because you have no idea
307:58 - which half of the list contains the item
308:00 - you're looking for your only real option
308:03 - is to start at the beginning and compare
308:05 - each item in the list to your target
308:06 - value one at a time until you find the
308:09 - value you're looking for
308:10 - this algorithm is called linear search
308:13 - or sequential search because the search
308:15 - proceeds in a straight line or sequence
308:18 - even though linear search is inefficient
308:20 - searching for just one name will happen
308:22 - so fast that we won't be able to tell
308:24 - anything useful about the algorithm's
308:26 - runtime so let's suppose we had a
308:28 - hundred different names and that we
308:29 - needed to know where they appear in a
308:30 - list of unsorted names
308:32 - here's some code that demonstrates
308:35 - as usual this code at the top isn't
308:37 - relevant to the search algorithm it's
308:39 - just like the code that loaded a list of
308:41 - numbers from a file in the previous
308:42 - stage but this code calls a different
308:44 - function load strings that loads a list
308:47 - of strings in
308:48 - if you want the load strings python code
308:50 - we'll have it for you in the teacher's
308:52 - notes
308:53 - here's a separate hard-coded list
308:55 - containing the 100 names we're going to
308:57 - search for we'll loop through each name
308:59 - in this list and pass it to our search
309:01 - function to get the index within the
309:03 - full list where it appears
309:05 - now let's implement the search function
309:07 - compared to the sorting algorithms this
309:09 - is going to be short the index of item
309:12 - function takes the python list you want
309:14 - to search through and a single target
309:15 - value you want to search for
309:18 - now we need to loop over each item in
309:20 - the list the range function gives us a
309:22 - range of numbers from its first argument
309:24 - up to but not including its second
309:27 - argument so if our list had a length of
309:29 - 5 this would loop over the indexes 0
309:31 - through 4.
309:33 - we test whether the list item at the
309:34 - current index matches our target
309:37 - if it does then we return the index of
309:40 - the current item this will exit the
309:42 - index of item function without looping
309:44 - over the remaining items in the list
309:46 - if we reach the end of the loop without
309:48 - finding the target value that means it
309:50 - wasn't in the list so instead of
309:52 - returning an index we return the special
309:54 - python value none which indicates the
309:56 - absence of a value
309:58 - other languages have similar values like
310:01 - nil or null but if yours doesn't you
310:03 - might have to return a value that would
310:04 - otherwise be impossible like an index of
310:06 - negative 1.
310:08 - now let's call our new search function
310:10 - we start by looping over the list of 100
310:12 - values we're looking for we're using the
310:14 - values themselves this time not their
310:16 - indexes within the list so there's no
310:18 - need to mess with python's range
310:20 - function here's the actual call to the
310:23 - index of item function we pass it the
310:25 - full list of names that we loaded from
310:27 - the file plus the name we want to search
310:29 - for within that list then we store the
310:31 - index it returns in a variable
310:33 - and lastly we print the index we get
310:35 - back from the index of item function
310:38 - let's save this and go to our console
310:40 - and see if it works
310:42 - python linear search dot pi
310:45 - names
310:48 - unsorted dot text
310:51 - and it'll print out the list of indexes
310:53 - for each name
310:55 - i actually set it up so that the last
310:56 - two items in the list of names we're
310:58 - going to search for corresponded to the
311:00 - first and last name within the file
311:03 - so if we open up our unsorted.txt file
311:06 - we'll see mary rosenberger is the first
311:08 - name and alonso viviano is the last name
311:13 - and those are the last two values in our
311:15 - list of names we're searching for
311:17 - so it returned an index of zero for that
311:19 - second to last name and you can see that
311:21 - name here on line one of the file
311:24 - the line numbering starts at one and the
311:26 - python list indexes start at zero so
311:28 - that makes sense
311:30 - and for the last name it returned an
311:31 - index of 109873
311:36 - and you can see that name here on line
311:38 - 109 874 so we can see that it's
311:41 - returning the correct indexes
311:43 - but right now we're just searching for a
311:44 - hundred different names in a list of one
311:46 - hundred thousand names in the real world
311:49 - we're going to be looking for many more
311:50 - names than that within much bigger lists
311:52 - than that can we do this any faster yes
311:56 - but we'll need to use the binary search
311:58 - algorithm and for that to work we need
312:00 - to sort our list of strings we'll do
312:02 - that in the next video
312:04 - before we can use the binary search
312:06 - algorithm on our list of names we need
312:08 - to sort it let's do that now we need to
312:10 - load our unsorted list of names from a
312:12 - file sorted and write the sorted names
312:14 - back out to a new file
312:16 - again this code at the top just loads a
312:18 - file full of strings into a list
312:21 - we'll use our quick sort method to sort
312:23 - the list of names its code is completely
312:25 - unchanged from when you saw it in the
312:27 - previous stage
312:28 - we just call our quick sort function on
312:30 - the list of names loaded from the file
312:32 - and save the list to a variable
312:36 - then we loop through each name in the
312:37 - sorted list
312:40 - and we print that name
312:43 - that's all there is to it let's save
312:45 - this script and try running it
312:47 - python
312:49 - quicksort strings stop pi
312:52 - and we'll pass it the
312:54 - names unsorted.text file
312:56 - let me resize the console window here a
312:59 - little bit
313:03 - that prints the sorted list of names out
313:05 - to the terminal but we need it in a file
313:08 - so we'll do what's called a redirect of
313:10 - the program's output we'll run the same
313:12 - command as before but at the end we'll
313:14 - put a greater than sign followed by the
313:16 - path to a file that we want the program
313:18 - output written to names
313:21 - sorted dot text
313:26 - redirecting works not only on linux
313:28 - based systems like workspaces but also
313:30 - on macs and even on windows machines you
313:33 - just need to be careful because if you
313:34 - redirect to an existing file its
313:36 - contents will be overwritten without
313:38 - asking you
313:40 - let me refresh the list of files in the
313:42 - sidebar
313:44 - and you'll see that we now have a new
313:46 - sorted dot text file in the names
313:47 - directory
313:49 - it's the same number of lines as the
313:51 - unsorted dot text file but all the names
313:53 - are sorted now
313:55 - now we can load this file of sorted
313:56 - names into a list and we'll be able to
313:58 - use that list with the binary search
314:00 - algorithm we'll see how to do that next
314:03 - now that we have our list of names
314:04 - sorted we can use the binary search
314:07 - algorithm on it let's see if we can use
314:09 - it to speed up our search for the
314:10 - indexes of 100 names
314:13 - binary search keeps narrowing down the
314:14 - list until it has the value it's looking
314:16 - for it's faster than linear search
314:18 - because it discards half the potential
314:20 - matches each time
314:22 - our code here at the top of our binary
314:24 - search script is unchanged from the
314:26 - previous scripts we just call the load
314:27 - strings function to load our 100 000
314:30 - sorted names from a file
314:32 - here we've hard coded the list of 100
314:34 - names we're going to search for again
314:36 - it's identical to the list from the
314:37 - linear search script except that i've
314:39 - again changed the last two names to
314:41 - correspond to the names on the first and
314:42 - last lines of the file we'll be loading
314:45 - now let's write the function that will
314:46 - implement our binary search algorithm
314:49 - like the linear search function before
314:51 - it'll take two arguments the first is
314:53 - the list we're going to search through
314:55 - and the second is the target value we'll
314:57 - be searching for again the binary search
315:00 - function will return the index it found
315:01 - the value at or the special value none
315:04 - if it wasn't found
315:05 - binary search is faster than a linear
315:07 - search because it discards half the
315:09 - values it has to search through each
315:11 - time to do this it needs to keep track
315:13 - of a range that it still needs to search
315:15 - through
315:16 - to start that range is going to include
315:18 - the full list
315:19 - the first variable will track the lowest
315:22 - index in the range we're searching to
315:24 - start it's going to be 0 the first index
315:26 - in the full list
315:28 - likewise the last variable will track
315:30 - the highest index in the range we're
315:32 - searching to start we'll set it to the
315:34 - highest index in the full list
315:36 - if the first and last variables are
315:38 - equal then it means the size of the
315:40 - search range has shrunk to zero and
315:42 - there is no match until that happens
315:44 - though we'll keep looping to continue
315:46 - the search we want to divide the list of
315:48 - potential matches in half each time to
315:51 - do that we need to check the value
315:52 - that's in the middle of the range we're
315:54 - searching in
315:55 - we add the indexes in the first and last
315:57 - variables then divide by two to get
316:00 - their average we might get a fractional
316:02 - number which can't be used as a list
316:04 - index so we also round down using
316:06 - python's double slash floor division
316:09 - operator
316:10 - all this will give us the index of the
316:12 - list element that's the midpoint of the
316:14 - range we're searching we store that in
316:16 - the midpoint variable
316:18 - whoops looks like my indentation got
316:20 - mixed up there let me fix that real
316:22 - quick there we go now we test whether
316:24 - the list element at the midpoint matches
316:27 - the target value
316:32 - if it does we return the midpoint index
316:34 - without looping any further our search
316:36 - is complete
316:37 - otherwise if the midpoint element's
316:39 - value is less than the target value
316:45 - then we know that our target value can't
316:46 - be at the midpoint or any index prior to
316:49 - that so we move the new start of our
316:51 - search range to just after the old
316:53 - midpoint
316:54 - otherwise the midpoint element's value
316:56 - must have been greater than the target
316:58 - value
316:59 - we know that our target value can't be
317:00 - at the midpoint or any index after that
317:03 - so we move the new end of our search
317:05 - range to just before the old midpoint
317:08 - by unindenting here we mark the end of
317:10 - the loop if the loop completes it means
317:12 - the search range shrank to nothing
317:14 - without our finding a match and that
317:16 - means there's no matching value in the
317:17 - list so we return the special python
317:20 - value none to indicate this
317:23 - lastly just as we did in our linear
317:25 - search script we need to search for each
317:27 - of the 100 names we loop over each name
317:30 - in our hard-coded list
317:32 - and we call the binary search function
317:34 - with the sorted list of names we're
317:35 - going to load from the file and the
317:37 - current name we're searching for
317:39 - we store the returned list index in the
317:41 - index variable
317:43 - and finally we print that variable
317:47 - let's save this and go to our console
317:49 - and try running it
317:51 - python
317:52 - binarysearch.pi
317:54 - and it's important to give it the name
317:55 - of the sorted file if it loads the
317:57 - unsorted file the binary search won't
317:59 - work so names
318:01 - sorted dot text
318:05 - again it prints out the list of indexes
318:07 - for each name
318:08 - i once again set it up so the last two
318:10 - items in the list of names we're going
318:12 - to search for corresponded to the first
318:14 - and last name in the file
318:16 - so it returned an index of zero for the
318:18 - second to last name
318:21 - and you can see that name
318:27 - here's the second to last name aaron
318:29 - augustine
318:32 - you can see that name here on line one
318:34 - of the file
318:35 - and for the last name it returned an
318:37 - index of one zero nine eight seven three
318:40 - and you can see that name here on line
318:42 - one zero nine eight seven four
318:49 - let's check the third to last name for
318:50 - good measure it looks like an index of
318:54 - 97022 was printed for that name stephen
318:57 - daras
319:00 - let's search for steve and daras within
319:03 - the file
319:05 - and here it is on line 97023
319:09 - remember that line numbers start on one
319:11 - instead of zero so this actually matches
319:13 - up with the printed list index of 97022
319:17 - it looks like our binary search script
319:19 - is working correctly
319:21 - let's try our linear search and binary
319:23 - search scripts out with the time command
319:25 - and see how they compare i've commented
319:27 - out the lines that print the indexes of
319:29 - matches in the two scripts
319:32 - that way they'll still call their
319:33 - respective search functions what the 100
319:36 - names we're searching for but they won't
319:37 - actually print the indexes out so we
319:39 - won't have a bunch of output obscuring
319:41 - the results of the time command
319:44 - first let's try the linear search script
319:47 - time python
319:49 - linear search dot pi
319:51 - names
319:53 - and we can just use the unsorted list of
319:54 - names for linear search
319:59 - remember we want to ignore the real
320:01 - result and add the user and sys results
320:03 - together
320:04 - it looks like it took about .9 seconds
320:07 - for linear search to find the 100 names
320:09 - in the list of one hundred thousand
320:11 - now let's try timing the binary search
320:13 - script time
320:15 - python
320:16 - binarysearch.pi
320:18 - names and for this one we need to use
320:21 - the sorted list of names
320:25 - looks like that took around a quarter
320:27 - second so less than half as long
320:29 - bear in mind that part of this time is
320:31 - spent loading the file of names into a
320:32 - list the difference between linear
320:34 - search and binary search will be even
320:36 - more pronounced as you search through
320:38 - bigger lists or search for more items
320:41 - let's wrap up the course by looking at
320:43 - the big o runtimes for linear search and
320:45 - binary search these are going to be much
320:47 - simpler to calculate than the sorting
320:49 - algorithms were
320:50 - for linear search you need to do one
320:52 - comparison to the target value for each
320:55 - item in the list again theoretically we
320:57 - could find the target value before
320:59 - searching the whole list but big o
321:01 - notation is only concerned with the
321:02 - worst case where we have to search the
321:04 - entire list so for a list of eight items
321:07 - that means eight operations
321:10 - the big o runtime for linear search is o
321:13 - n where n is the number of items we're
321:15 - searching through
321:16 - this is also known as linear time
321:18 - because when the number of items and
321:20 - number of operations are compared on a
321:22 - graph the result is a straight line
321:25 - linear search looks pretty good until
321:27 - you compare it to binary search for
321:29 - binary search the number of items you
321:31 - have to search through and therefore the
321:32 - number of operations is cut in half with
321:35 - each comparison
321:36 - remember the number of times you can
321:38 - divide n by two until you reach one is
321:40 - expressed as log n so the run time of
321:43 - binary search in big o notation is o log
321:46 - n
321:47 - even for very large values of n that is
321:50 - very large lists you have to search
321:51 - through the number of operations needed
321:53 - to search is very small binary search is
321:56 - a very fast efficient algorithm
321:59 - that's our tour of sorting and searching
322:01 - algorithms be sure to check the
322:03 - teacher's notes for opportunities to
322:05 - learn more thanks for watching