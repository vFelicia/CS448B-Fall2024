00:00 - welcome to the beginner's course on how
00:02 - to build 12 data apps in python with
00:05 - streamlit my name is chenin
00:07 - ahmad and i'll be your instructor for
00:09 - today some of you might know me as the
00:12 - data professor from my youtube channel
00:14 - data professor where i teach data
00:16 - science machine learning and also
00:19 - provide a bioinformatics project
00:21 - walkthrough aside from being a youtuber
00:24 - i'm also an associate professor of
00:26 - bioinformatics where i teach and do
00:28 - research at the interface of machine
00:31 - learning and computational drug
00:33 - discovery in this course we will be
00:36 - building 12 interactive data-driven web
00:39 - applications in python using the
00:42 - streamlit library so streamlit will
00:44 - allow you to make use of data and also
00:47 - all of the python libraries such as
00:50 - numpy scipy maps.lib seaborn right
00:54 - inside the python environment and you'll
00:56 - also be able to create an interactive
00:59 - web application that will be able to
01:01 - pre-process data sets visualize the data
01:04 - and also make prediction from machine
01:06 - learning in the form of a web
01:07 - application so a basic working knowledge
01:10 - of python is assumed but don't worry
01:12 - i'll be holding your hand and provide
01:15 - you with a step-by-step walkthrough
01:17 - where i will be trying my best to
01:19 - simplify the concept and topics covered
01:22 - before proceeding further let's take a
01:24 - look at the 12 data web applications
01:27 - that we will be building today i've also
01:30 - rearranged the topic according to the
01:33 - topics covered and so in apps number one
01:35 - and eight we will be building a very
01:37 - simple stock price application app
01:40 - number eight we will be building a
01:42 - simple bioinformatics dna count
01:45 - application afterwards we will be
01:47 - building four eda applications using the
01:51 - basketball data football data s p 500
01:54 - stock price and also the cryptocurrency
01:57 - price data we'll also be developing two
02:00 - classification models and embedding that
02:03 - into the web application for the iris
02:05 - data set and also the penguins data set
02:08 - we'll also be building two regression
02:10 - models on the boston housing data and
02:13 - also the bioinformatics solubility data
02:16 - and finally we'll also be showing you
02:18 - how you can deploy your application to
02:20 - the heroku platform and also to the
02:23 - trimlet sharing platform and so for more
02:25 - data science machine learning and
02:28 - bioinformatic projects please make sure
02:31 - to subscribe to my youtube channel the
02:33 - data professor and also follow me on
02:36 - medium where i regularly publish blog
02:39 - posts on data science and also machine
02:41 - learning so links to all of these are
02:43 - provided in the description of this
02:46 - video also grab yourself a cup of coffee
02:49 - and without further ado let's get
02:51 - started
02:58 - have you ever wanted to build a data
03:01 - driven web application for your data
03:03 - science projects but perhaps you might
03:05 - be intimidated by the difficulty of
03:08 - coding in django or in flask if you
03:11 - answered one or all of the above then
03:13 - you want to watch this video to the end
03:15 - because i'm going to show you how you
03:16 - could build a data-driven web
03:18 - application in just a few lines of code
03:21 - and so without further ado let's get
03:23 - started so the name of the python
03:25 - library that allows you to build a
03:27 - simple data-driven web application is
03:30 - called streamlit actually this python
03:33 - library was brought to my attention by
03:35 - one of the subscribers of this youtube
03:37 - channel so please give a big hand to
03:39 - iqbal for recommending this excellent
03:43 - python library that will allow you to
03:45 - develop a simple data-driven web
03:47 - application for your data science
03:49 - project and so the first thing that you
03:52 - want to do is head over to the streamlet
03:54 - website by typing in streamlit.io
03:58 - and so i'm going to provide you the link
04:00 - in the description of this video
04:02 - so this is the website of streamlit and
04:05 - as you will see it says that it is the
04:07 - fastest way to build a data application
04:14 - so here you can see that you could build
04:16 - a opencv web application from within
04:20 - streamlit and you could add a lot of
04:22 - interactive elements as well
04:25 - so in order to get started you want to
04:27 - install streamlid and so you could do
04:29 - that by typing in pip install streamlit
04:32 - and after the installation process is
04:34 - finished you could type in streamlit
04:36 - hello in order to check that it has
04:38 - successfully been installed
04:42 - and as you can see here a simple web
04:44 - application could be built in just a few
04:46 - lines of code
04:48 - and you will see in this second example
04:50 - that you could also add widgets to the
04:53 - web application as well and so this
04:55 - slider widget will allow you to select
04:58 - numbers just by sliding the slider bar
05:02 - and in this third example here you could
05:04 - deploy your web application easily using
05:07 - git
05:08 - and there you have it a minimal
05:09 - framework for building a powerful web
05:12 - application while just requiring you
05:15 - just a few lines of code
05:18 - and so here are some of the gallery of
05:20 - web application built using streamlit so
05:23 - let's have a look at the gallery
05:26 - okay so this awesome web application
05:28 - using tensorflow was built in streamlit
05:32 - and there are other awesome examples of
05:36 - streamlit applications that were built
05:39 - by the user community and so here are
05:42 - just a selection of these so if you have
05:44 - built a web application using streamlit
05:47 - you could also share it via twitter and
05:50 - the streamlate website will be
05:51 - showcasing your web application in this
05:55 - gallery page
05:56 - so you can see here that a wide variety
05:59 - of web applications have been built
06:01 - using streamlit
06:04 - okay so now that we have a brief
06:06 - introduction about streamlib let's have
06:08 - a look at how we can build one for
06:10 - ourself
06:11 - okay so
06:12 - the first thing that you want to do is
06:14 - fire up your terminal so if you're using
06:16 - a microsoft windows you want to type in
06:19 - the search bar cmd
06:22 - and then you will see a terminal prompt
06:24 - coming up and in this terminal prompt
06:26 - you want to type in pip install and then
06:29 - stream lit
06:30 - and then hit enter and since i have
06:32 - already installed streamlit so i'm going
06:34 - to proceed with showing you how you can
06:36 - build the application so i installed
06:39 - streamlit inside the contact environment
06:42 - and so i want to activate my
06:44 - environment by typing in conda activate
06:47 - dp
06:51 - all right so i've created a python file
06:54 - called
06:54 - myapp.py and the contents of the file is
06:58 - shown here so you can see that it is
07:01 - approximately 20 lines of code so if you
07:04 - deduct the empty spaces then it should
07:07 - be less than 20 lines of code and so
07:10 - aside from installing streamlit in this
07:12 - example you also want to install why
07:15 - finance so you could type in
07:17 - pip install
07:19 - y finance okay and
07:22 - after you have done so then you want to
07:24 - type the following lines of code in but
07:26 - for your convenience i'm going to share
07:28 - you the link to this file on the data
07:31 - professor github so you want to check in
07:33 - the description of this video and
07:35 - download this file okay so the first
07:37 - three lines of code are just simply
07:39 - importing the y finance as y f import
07:43 - streamlit sst import pandas as pd and
07:47 - then this block of code we're going to
07:49 - write the header of the web application
07:52 - so as you will see here that this is in
07:54 - markdown language and with the hashtag
07:57 - here it is indicating that this line is
08:00 - a heading type one so it's going to be a
08:02 - big text and then it's going to be an
08:04 - ordinary text saying shown are the stock
08:07 - closing price and volume of google and
08:09 - then here in this blocks of code i've
08:12 - taken from the towards data science
08:14 - article so you want to check that
08:16 - article out and give this article a clap
08:19 - and so i extracted some lines from this
08:22 - article and so this line of code will be
08:26 - the ticker symbol of google and so it is
08:28 - g-o-o-g-l and then in this line of code
08:32 - we're gonna take in the ticker symbol of
08:34 - google and so we're gonna retrieve
08:36 - historical data of google stock price
08:39 - with a period set at one day and the
08:42 - starting date is may 31st 2010 with a
08:46 - ending date of may 31st 2020 and then
08:49 - we're gonna save this into the ticker
08:51 - data frame and then the contents of this
08:53 - data frame will comprise of the
08:55 - following columns open high low close
08:59 - volume dividends and stock splits okay
09:03 - so in this web application we're going
09:05 - to show you two line chart and we're
09:07 - going to show the closing price and also
09:10 - the volume okay and so this is a very
09:13 - simple web application and then you
09:15 - could customize this to your own liking
09:17 - so i'm going to show you that we could
09:19 - also edit the contents of the file and
09:22 - the web application will be serving the
09:25 - updated version in real time
09:28 - alright so let's type in cd desktop
09:32 - because this file is on the desktop and
09:35 - then i'm gonna type in streamlit
09:38 - run and then the name of the application
09:40 - which is my app dot py enter
09:44 - and that's it
09:45 - okay so it's gonna spawn up a web server
09:48 - and this is what you're gonna see a
09:50 - simple stock price application okay so
09:52 - let me show you side by side the code
09:56 - and the application
10:00 - all right here so here the simple stock
10:03 - price app here is the heading and since
10:06 - it is one hashtag it means that it is
10:08 - having the heading one style h1 in html
10:12 - language but if we have two then it will
10:14 - be a bit smaller so let's save it and
10:16 - then it detects that the source file has
10:18 - changed and then we should select always
10:22 - run and then we should select always
10:24 - rerun and so it's going to update to be
10:26 - a bit smaller as you will see here and
10:28 - if i add additional hashtag and save it
10:31 - it will be even smaller okay so i'm
10:32 - going to change it back to one hashtag
10:34 - and so it's heading one let me try
10:37 - okay so let's maybe modify this a bit in
10:41 - markdown manner so closing price i'm
10:44 - gonna make it bold
10:46 - volume
10:49 - i'm gonna make it bold and an italic see
10:52 - so you could customize the style in
10:56 - markdown style
10:57 - so
11:00 - you want to refer to the markdown cheat
11:02 - sheets and i think this is a good one to
11:04 - refer to
11:06 - so the markdown cheat sheet by adam
11:09 - pritchard and it has everything that you
11:11 - would ever wanted to know about markdown
11:14 - all right so
11:16 - you could add cool stuff in here you
11:18 - could add
11:20 - lists
11:21 - unordered lists
11:22 - you could add links
11:24 - you could add images
11:26 - you could add tables
11:29 - okay
11:30 - like for example let me copy this
11:35 - and then i'm gonna just put it here
11:39 - all right so it's a table
11:43 - okay very neat right okay but let's
11:46 - delete it for simplicity
11:48 - so the ticker symbol is google so
11:52 - we can even customize the ticker symbol
11:54 - to be other values as well so let's say
11:57 - aapl for apple
11:59 - save it and then this is the price for
12:02 - apple so i could update this like that
12:05 - okay as you can see it will update
12:07 - automatically
12:09 - to the website okay so this is the date
12:12 - range that are shown in this line chart
12:15 - and this is the actual line chart so if
12:18 - i wanted to delete one of them and then
12:20 - save it then only one will be shown
12:24 - okay
12:25 - and let's say that i wanna write
12:28 - something in
12:31 - okay so i'm gonna write here the
12:34 - heading one closing price
12:36 - maybe make it heading two
12:40 - all right and i'm gonna do the same for
12:43 - the volume
12:48 - and see we'll see this is the customized
12:51 - version
12:55 - okay so if you zoom into mail you could
12:57 - even do that as well this is a
12:59 - interactive chart
13:03 - and if you want to zoom to the original
13:06 - version you would just double click on
13:07 - it
13:09 - right same thing here
13:11 - just double click on it
13:13 - and it'll go to the original state
13:17 - and so there you have it a data driven
13:19 - web application in just a few lines of
13:21 - code
13:28 - do you want to build a bioinformatics
13:30 - web application if you answered yes then
13:32 - you want to watch this video to the end
13:34 - because in this video i'm going to show
13:36 - you how you could build a very simple
13:38 - bioinformatics web application in python
13:41 - and without further ado we're starting
13:43 - right now so the bioinformatics web
13:45 - application that we are going to build
13:47 - today is called the dna nucleotide count
13:50 - web application okay so let's fire up
13:53 - the terminal
13:56 - and let me activate my conda for python
14:00 - click on the activate and the
14:02 - environment is called the dp dp standing
14:05 - for data professor
14:07 - and so let me go to the folder where i
14:09 - have my streamlet web application files
14:13 - cd desktop cd streamlet
14:16 - cd dna
14:19 - okay and so
14:20 - we have a total of three files here and
14:23 - so the aromatase.fasta
14:26 - is a example data file but actually
14:29 - we're not using it to build the web
14:31 - application so essentially we're going
14:33 - to have only the python dna app dot py
14:37 - and also the dna logo.jpg and the dna
14:40 - logo will be displayed in the web
14:43 - application right here on line number
14:44 - 14. so let me fire up the web app so i'm
14:48 - going to have to
14:49 - type in streamlit
14:51 - and then run and then the name of the
14:53 - app which is dna dash app dot py
14:59 - okay and so for those of you who don't
15:01 - have streamlit installed you could
15:03 - install it via pip so you could simply
15:05 - do a pip install streamlit okay and so
15:08 - the web application is right here so
15:10 - this image here is the logo and the logo
15:13 - looks like this let me show it here
15:18 - the logo is right here
15:20 - so for this logo i've drawn it using the
15:22 - good notes application in ipad
15:25 - alright so let's have a look at the web
15:27 - application
15:31 - and let me show it side by side
15:35 - [Music]
15:37 - okay
15:40 - let me also increase the font size here
15:45 - all right there you go so it's bigger
15:46 - for you guys
15:48 - and right here too
15:51 - okay there you go so it's a lot bigger
15:53 - now
15:55 - okay so let's take a look at the code
15:57 - here so the first couple of lines here
15:59 - we're going to be importing the
16:01 - necessary libraries for this web
16:03 - application so on line number five here
16:05 - we're going to make use of the data
16:07 - frame from the pandas library and the
16:09 - basis of this web application will be
16:12 - using the streamlid library so we're
16:14 - going to import streamlit as well and
16:17 - for the graph here we're going to make
16:19 - use of the alt air or altair library and
16:22 - then for displaying the logo we're going
16:24 - to have to import from pil import image
16:28 - okay so the block of code here from line
16:31 - number 10 to 24 we're essentially going
16:34 - to show the dna logo so we're going to
16:37 - create a variable called image which
16:39 - contains the name of the logo and then
16:42 - we're going to display the image here
16:43 - and then we're going to display it by
16:45 - allowing the image to expand to the
16:48 - column width here so it will expand to
16:50 - the column width and then we're going to
16:51 - print out the header here dna nucleotide
16:54 - count web application right here shown
16:56 - in bold and then we're going to have a
16:58 - short explanation this app counts the
17:00 - nucleotide composition of the query dna
17:04 - and then the three asterisks here will
17:05 - be showing the hr line so the hr line is
17:09 - essentially the horizontal line here
17:12 - okay and so the next block of code here
17:14 - will be showing the text box here the
17:18 - dna text box here enter dna sequence so
17:22 - st.header will be showing the header
17:25 - here enter dna sequence which is right
17:27 - here and then the text box here will be
17:29 - displayed using the st.text area okay
17:33 - and then we're going to have the height
17:35 - to be 250 so if you want it to be a bit
17:38 - smaller then you could adjust the number
17:40 - here
17:41 - and so you can see that when the height
17:43 - is 150 the height is smaller and if it's
17:45 - 350 it'll be bigger so let's have it at
17:48 - 250.
17:50 - okay and sequence input here this is the
17:54 - sample dna sequence shown here so if i
17:57 - modify the name here the name will be
17:58 - modified here and and the slash n here
18:01 - is the new line so if i don't have it
18:03 - here then there won't be a new line
18:08 - okay
18:13 - like for example you could
18:16 - you know import your dna sequence
18:19 - [Music]
18:22 - and then command enter and then you're
18:24 - going to get a different output
18:29 - okay
18:31 - and so upon reading in the sequence here
18:35 - st.text area the input here in the text
18:38 - box will be provided or assigned to the
18:42 - sequence variable and then for the
18:44 - sequence variable we're going to split
18:46 - the lines
18:48 - so each of the line here will be split
18:51 - okay by splitting it means that it will
18:54 - create a list of each of the line so the
18:57 - first member of the list will be dna
18:59 - query and then the second line will be
19:01 - the second member of the list third line
19:04 - will be the third member and the fourth
19:06 - line will be the fourth member of the
19:07 - list
19:10 - and to provide this even clearer let me
19:14 - show you
19:17 - not there but here
19:22 - all right right here so you're gonna see
19:24 - here that when we use sequence and then
19:27 - we use dot split lines we see that each
19:30 - of the line here line number one line
19:32 - number two line number three line number
19:34 - four will be shown here as four members
19:37 - here of the list one two three four okay
19:42 - and the lines will be indicated by the
19:44 - backslash n which is the syntax for
19:47 - newline which is also right here so this
19:50 - is the equivalent of pressing the enter
19:52 - key on the keyboard as so
19:54 - like that okay that's a new line
19:57 - all right so let me take that out
20:01 - and so we're gonna skip the first line
20:04 - here sequence one colon which means that
20:07 - we're going to read lines number two
20:09 - onwards
20:12 - because the first line will be the name
20:14 - of the sequence right here
20:16 - which we don't want because we want to
20:18 - compute the dna composition using from
20:22 - lines number one onward i mean index
20:24 - number one onwards which corresponds to
20:26 - the second member of the list the index
20:30 - of one onwards so that means index
20:32 - number one two and three until the end
20:35 - right because the n is line number three
20:37 - index number three okay so line number
20:39 - forty will essentially
20:41 - skip the sequence name because the index
20:44 - of the sequence name is zero so it's
20:46 - gonna skip this and we're going to slice
20:49 - and select lines number one i mean index
20:51 - number one onwards so here we're gonna
20:54 - slice or select in the bracket here
20:56 - index number one onwards meaning that
20:59 - index number one here onwards until the
21:01 - end
21:02 - and it has a total of three additional
21:04 - lines index one in x2 and x3 and then it
21:07 - will be assigned to the same name
21:09 - sequence okay so let's show it here
21:11 - again
21:16 - you can see the name is now gone and now
21:19 - we have only the sequence and we have a
21:21 - total of three members in the list now
21:24 - and before there were a total of four
21:27 - members including the sequence name as
21:29 - the first position but because we have
21:32 - already selected the second member
21:34 - onwards right here second member onwards
21:36 - second third fourth
21:38 - it means that word this it means that
21:40 - we're discarding the sequence name and
21:43 - so here we have only the sequence and so
21:45 - it's very helpful to
21:47 - have a look at the contents of the
21:49 - variable line by line so that you can
21:51 - see how the input data is being modified
21:55 - accordingly for each line of code here
22:04 - okay and notice that
22:06 - we have a total of three
22:09 - members in the list and so for the next
22:12 - line of code here right here we're going
22:14 - to join
22:15 - the three lines here together in order
22:18 - to form a long stretch of dna sequence
22:21 - let me show you what i mean
22:39 - all right here so you can see that using
22:42 - the dot join sequence and then because
22:45 - here we're saying that we don't want any
22:47 - space in between so we're gonna get one
22:49 - line of sequence here
22:52 - okay so the three lines
22:54 - shown here will be combined into one and
22:57 - notice here that we have two quotation
22:59 - marks if i add a space here
23:03 - there will be a space added
23:05 - right here space edit here space added
23:07 - here
23:08 - okay
23:10 - or we can even make it a new line for
23:12 - example to show you how the data is
23:14 - being modified
23:17 - okay and becomes a new line
23:19 - or if we say we want to add a dot
23:24 - it will add a dot right here see it will
23:26 - add a dot here
23:30 - and it will add a dot here
23:34 - so we don't want to add anything here
23:36 - we're going to just have the two
23:37 - quotation marks close together
23:40 - and it will be one long stretch of line
23:42 - here
23:45 - okay and now we have
23:47 - the dna sequence
23:48 - that is pre-processed and ready for
23:51 - computation
23:54 - so let me delete this again
24:05 - all right let's take a look further
24:09 - so here line number 46 is the comment
24:11 - for prince the input dna sequence so
24:15 - the dna sequence that we see here will
24:18 - be printed here in the input header dna
24:21 - query so this is the input that we have
24:24 - already preprocessed by joining all of
24:26 - the lines together so the three lines
24:28 - will be joined so as you can see
24:31 - initially we read in the dna sequence
24:33 - here and then we deleted the
24:36 - first line by discarding it where we
24:39 - only take into consideration the
24:41 - nucleotide information here and then we
24:44 - join it together into a single line as
24:46 - shown here
24:48 - and now
24:49 - let's proceed to line number 50 dna
24:51 - nucleotide count heather is output dna
24:54 - nucleotide count right here
24:56 - so under this heading we're going to
24:58 - show you the four different ways that
25:01 - you could display the output so before
25:04 - displaying the output we're going to do
25:06 - some computation
25:08 - so in the first method here print
25:11 - dictionary all right so we're going to
25:13 - use the subheader so the subheader will
25:15 - be slightly smaller than the header the
25:17 - font size will be slightly smaller
25:20 - all right and here lines number 55 to 62
25:24 - we're going to create a custom function
25:26 - for counting the dna nucleotide from the
25:29 - sequence so here def
25:32 - and then the name of the function and
25:34 - then the input argument seq and then
25:37 - we're going to create a d variable which
25:39 - is a dictionary and then it's going to
25:42 - contain four members here atgc and for a
25:45 - we're going to count the number of a in
25:48 - the dna sequence and for t we're going
25:50 - to count the number of t using the dot
25:53 - count function same thing for g same
25:55 - thing for c and then at the end it will
25:57 - return the d variable and so the d
25:59 - variable will be a dictionary containing
26:02 - the name of the nucleotide atg or c
26:05 - along with the count of each of the
26:07 - nucleotide so essentially it will be a
26:10 - dictionary like for example a 59 t 43 g
26:15 - 52 c 56 so 59 43 52 56 means that there
26:22 - are a total of 59 adenine here 59 of a
26:26 - here 43 of t i mean 52 of g guanine and
26:31 - 56 of c the cytosine okay so we're
26:34 - finished here now we're up to here
26:38 - okay and we're displaying the x which is
26:41 - essentially
26:42 - applying the dna custom function and the
26:45 - input sequence and then we assign it to
26:47 - x here and i believe that we didn't use
26:49 - the label or values here let's save it
26:53 - all right
26:54 - and so
26:55 - okay and so let's move on to the next
26:58 - block of code here number two print the
27:00 - text
27:01 - print the text okay so let's say that we
27:04 - want to print in human readable form
27:07 - because previously it looks like a
27:09 - dictionary right so not so friendly for
27:12 - the end user so let's say that we want
27:14 - to print out the text here we want to
27:16 - say there are 59 adenine there are 43
27:20 - thymine there are 52 adenine oh no i
27:22 - mean guanine it's typo let me do it
27:29 - so i guess i copy and paste it
27:33 - okay so the abbreviation all right so
27:35 - the second part is finished now let's
27:37 - move on to the third part display the
27:38 - data frame all right so here we're
27:40 - displaying it as a data frame and so
27:42 - essentially there are two columns the
27:44 - nucleotide and the count and so this
27:47 - block of code here we're going to create
27:49 - a data frame from the dictionary
27:51 - function and then we're going to rename
27:53 - the column because at default it will
27:55 - become zero here let me show you
27:56 - [Music]
28:00 - it will become zero so we're gonna
28:02 - relabel zero to become count
28:05 - okay
28:08 - okay and so as you can see this block of
28:10 - code here will give you this data frame
28:13 - and this data frame will be used for
28:15 - creating the following bar charts in
28:17 - number four display the bar chart and so
28:20 - for the fourth one here we're going to
28:22 - be making the bar chart and we're going
28:24 - to make use of the altair library so
28:27 - line number 87 we'll print the sub
28:29 - header lines number 88 to 91 it will
28:33 - create the actual plot and then in lines
28:36 - number 92 through 94 we're going to
28:38 - adjust the width of the bars because by
28:41 - default the bars will be pretty thin let
28:44 - me show you
28:54 - right here yeah so it's pretty thin
28:57 - so we're gonna adjust it to be 80
29:00 - which should look a bit better
29:04 - all right so it looks a bit better and
29:06 - notice here that the chart are assigned
29:09 - to the p variable and then in order to
29:12 - show the plot or the chart we're going
29:13 - to put it inside as the input argument
29:16 - to the st dot write function so that it
29:18 - becomes a streamlit object
29:21 - okay and so there you have it a very
29:23 - simple bioinformatics web application
29:26 - feel free to modify this to be another
29:29 - web application in bioinformatics or for
29:33 - any industry as well because the code is
29:36 - quite applicable and you could use it as
29:38 - a template for building your own
29:40 - personal data science project
29:48 - okay so this video is the fifth part of
29:51 - the streamlit tutorial series where i go
29:54 - into detail step by step on how you can
29:57 - build a data science web application and
29:59 - so let's have a quick recap in the first
30:01 - part i have shown you the first web app
30:03 - that you could build using streamlit in
30:05 - python and with that application you're
30:07 - obtaining data dynamically from the y
30:10 - finance library where it will retrieve
30:13 - stock data directly from the yahoo
30:15 - finance and then we're going to make a
30:17 - simple line chart in the second part we
30:19 - have built a simple iris predictor where
30:22 - we employed machine learning algorithm
30:24 - and in the third part we used the
30:26 - machine learning algorithm to classify
30:28 - penguins into one of three species and
30:31 - so the concept is similar to the iris
30:33 - predictor but with a notable difference
30:35 - in that the input parameters have three
30:38 - additional ordinal or qualitative
30:41 - variables so that required a little bit
30:43 - more data pre-processing in order to
30:45 - encode the ordinal features into binary
30:48 - form and in the fourth part i've shown
30:50 - you how you could deploy your data
30:52 - science web application onto the cloud
30:54 - so that other people and your friends
30:56 - can have access to your data science web
30:59 - application and we did that by deploying
31:02 - the application onto hiroku and so today
31:05 - it is the fifth part of the streamlet
31:07 - tutorial series and we're going to
31:09 - combine two prior videos in the
31:12 - development of a web application today
31:14 - so essentially we're going to
31:15 - dynamically retrieve data from the
31:17 - internet by doing web scraping of the
31:21 - basketballreference.com website and so
31:23 - after we have web scraped the data we're
31:25 - going to do some data filtering and so
31:28 - all of this will be done right inside
31:30 - streamlit and so finally we're going to
31:32 - perform a simple exploratory data
31:34 - analysis by creating a simple heat map
31:37 - and so all of this in less than 70 lines
31:40 - of code and so without further ado we're
31:42 - starting right now okay so the code that
31:44 - we're going to use today is called
31:46 - basketball underscore app dot py and
31:49 - upon opening the code we're going to see
31:52 - this
31:53 - so all of this code is less than 70
31:55 - lines of code and so this is also
31:57 - including empty spaces as well
32:00 - so if we are to delete all of the empty
32:02 - spaces i would think it is approximately
32:05 - 60 lines of code and so a lot to be done
32:08 - here and it will occupy just a few lines
32:11 - of code here so the data science web
32:13 - application that we are building today
32:15 - is called the nba player stats explorer
32:18 - and so before we take a deep dive into
32:21 - the code let's try to run this code and
32:23 - let's have a look at the web application
32:25 - so let's close the file for a moment and
32:28 - let's open up a command prompt so if you
32:31 - are on a windows type in cmd if you are
32:34 - on a mac or a ubuntu you want to open up
32:36 - your terminal
32:38 - and so this is only going to work on my
32:40 - computer because i'm going to type in
32:43 - conda activate dp dp being the name of
32:46 - the conduct environment that is
32:48 - installed on my computer so if you have
32:50 - a conda environment installed on your
32:52 - computer you can activate that
32:54 - particular conduct environment so you
32:56 - could type in conda activate and then
32:57 - the name of your environment for example
32:59 - if the name of your environment is my
33:01 - env then you would type in conda
33:03 - activate my env but because on my
33:06 - computer it's called dp i'm going to use
33:08 - conda activate dp and so the lengthy
33:10 - explanation is due to the prior videos i
33:13 - have been noticing that there have been
33:15 - some misunderstanding that you will have
33:18 - to type in conda activate dp which is
33:20 - not the case because if your computer is
33:22 - using just normal python you could just
33:25 - proceed with following the tutorial here
33:28 - however if you're using conda then you
33:29 - will want to activate your own
33:31 - environment okay so let's continue
33:34 - so now i have activated my conda
33:36 - environment i'm going to
33:38 - change directory to the desktop
33:41 - and then
33:42 - to the streamlet folder
33:45 - which is where
33:46 - all of my data are residing in
33:50 - so for your case you want to change the
33:52 - directory to where all of your files are
33:55 - located particularly for the streamlid
33:57 - tutorial so the links to this code will
34:00 - be provided in the description down
34:01 - below so you want to check that out
34:05 - okay here it is so the name of the app
34:07 - is basketball underscore app dot py so a
34:10 - point in note
34:12 - for the first time that we're running
34:13 - this basketball application
34:16 - you will see some error message and the
34:19 - error message will tell us that some
34:21 - libraries are missing and so that we
34:23 - will have to install the prerequisite
34:25 - library in python so let's have a look
34:27 - at this together so that we could
34:29 - overcome this error together
34:33 - okay so you want to type in streamlit
34:35 - run basketball underscore app dot py and
34:38 - so we're gonna see this pop-up browser
34:41 - and so here it says that xml not found
34:44 - please install it so let's close this
34:49 - and then we are going to install lxml
34:59 - activate
35:00 - tpp pip install
35:04 - lxm
35:06 - lxml
35:10 - okay so it's installed
35:13 - let's head over back
35:18 - so notice that i could type fast here
35:20 - because i type in the first few
35:22 - characters of the folder name and then i
35:25 - hit on the tab button and so the tab
35:27 - button will auto complete the folder
35:30 - name for example i type in streamlit run
35:33 - ba
35:34 - or just b and then tab it'll auto
35:36 - complete the name for me
35:39 - let's run the application again
35:44 - okay so it seems to work now and so this
35:48 - is our nba player stats explorer web
35:51 - application that we are going to build
35:53 - today so let's have a look at the
35:55 - general characteristic of this web app
35:57 - so you're going to see that on the
35:58 - sidebar on the left we're going to have
36:00 - three input parameters so the first one
36:03 - is the year of the data that you want to
36:05 - have a look at and then the second
36:07 - parameter will be the team and so notice
36:10 - here that you could select multiple
36:12 - teams and by default it will select all
36:15 - of the teams for you and then you could
36:17 - take on the teams that you don't want
36:19 - and then the results will be updated on
36:21 - the fly so notice that whenever i click
36:23 - on the x mark here you will see that the
36:25 - number of rows will reduce
36:29 - right from 683 to 665.
36:34 - and so the third parameter of the input
36:36 - is the position of the players so we're
36:38 - gonna have the five traditional
36:40 - positions here center power forward
36:42 - small forward point guard and shooting
36:44 - guard and so to the right here which is
36:46 - the main panel so here we have the name
36:49 - of the web app and then some description
36:52 - and then we're gonna have some header
36:53 - here followed by the description of the
36:55 - data dimension and then we're gonna
36:57 - display the data frame of our data set
37:00 - so this data set will be downloaded
37:02 - spontaneously from
37:03 - basketballreference.com
37:05 - and so we're gonna use pandas to do the
37:07 - web scraping and then it will do some
37:10 - simple data filtering so i'm going to
37:12 - show you in the code in just a moment
37:14 - and so once we have done some simple
37:16 - data filtering we're going to display
37:18 - the data accordingly and the data frame
37:21 - that you see here we will be able to
37:23 - export the data as a csv file and
37:26 - finally we will also have a look at the
37:29 - intercorrelation heat map of the input
37:32 - parameters and so this is the
37:34 - exploratory data analysis that i have
37:36 - mentioned so we're going to do only one
37:38 - plot here so please feel free if you
37:40 - want to explore by including other data
37:43 - visualization plots in your web app so
37:46 - in this web application i made it so
37:48 - that the heat map will be hidden unless
37:50 - you click on the button and upon
37:52 - clicking on the button you will see the
37:53 - heat map coming up okay
37:56 - so
37:58 - pretty cool right so let's now have a
38:00 - look in the code shall we
38:03 - let's open up the atom editor
38:06 - file open file and then basketball app
38:11 - all right so let's have a look at the
38:12 - code side by side
38:17 - okay here so in the first six lines of
38:19 - the code we're going to import the
38:21 - necessary libraries so we're going to
38:23 - import the streamlight library because
38:25 - we're using streamlit to build the web
38:27 - app we're going to import pandas because
38:30 - we use pandas to handle the data frame
38:32 - and also to perform the web scraping we
38:34 - use the base64 library in order to
38:37 - handle the data download for the csv
38:40 - file because it's going to encode the
38:42 - ascii to byte conversion and so finally
38:45 - we're using matplotlib seaborn and numpy
38:48 - in order to create the heat map plot
38:50 - okay so let's have a look further on
38:52 - line number eight st.title is the name
38:54 - of the web app and the name of the web
38:56 - app is nba player stats explorer which
38:59 - is right here on the right and then some
39:01 - description of the web app is provided
39:04 - here so you will notice that we're using
39:06 - markdown language so this description
39:09 - here will correspond to this line
39:12 - the two bullet point will be
39:13 - corresponding to the two asterisks here
39:16 - and the both text will be corresponding
39:18 - to the two asterisks here and so we're
39:20 - also including the data source link
39:22 - which is
39:23 - basketballreference.com which is where
39:25 - we are downloading the data and so the
39:27 - following code here are for the sidebar
39:29 - so this is the header name of the
39:31 - sidebar so we're using the
39:34 - st.sidebar.header and then in the input
39:36 - argument we're using user input features
39:39 - which is right here user input features
39:42 - and then in the following line on line
39:44 - number 17 selected year is the name of
39:47 - the variable and then we're going to use
39:49 - this line of code here
39:52 - to display the years that we want to see
39:54 - the data for
39:56 - so we're making it as a drop down menu
39:58 - so we're using the select box function
40:01 - and so as the import argument the first
40:03 - argument that you see here is name year
40:05 - and so year is right here so if we
40:07 - change this a bit save it and then we
40:10 - say always rerun and then you're going
40:12 - to see that the question mark appears as
40:14 - well so if we take it out save it have a
40:16 - look back then the question mark
40:18 - disappears okay and then the following
40:20 - input argument here is we're going to
40:22 - create a range of numbers so we're going
40:25 - to have 1950 until 2019 which is right
40:29 - here and then we are going to reverse
40:31 - the list of the numbers because it's
40:33 - going to start from 1950 in 1951 1952
40:37 - and then at the bottom it will be 2019
40:40 - and so i use the reversed and then
40:42 - convert it into a list in order for it
40:45 - to display 2019 at the top right
40:48 - otherwise it will display 1950 at the
40:50 - top and then it will be the default
40:52 - value
40:54 - so notice that whenever you change the
40:56 - year here the corresponding data frame
40:58 - will also change
41:01 - so all of these data are downloaded on
41:03 - the fly on demand upon clicking on the
41:07 - input parameters so nothing is stored
41:09 - locally on the server side
41:15 - okay and then in this block of code here
41:18 - it will be performing the web scraping
41:21 - and also some data pre-processing and
41:23 - this is taken from a previous tutorial
41:26 - video where i will also provide the link
41:28 - in the description of this video as well
41:30 - where we perform a simple web scraping
41:33 - of the
41:34 - basketballreference.com website and then
41:36 - the only thing that we're using for the
41:38 - web scripting is only one line which is
41:41 - pd.read.html so it's going to read the
41:43 - html file particularly the data is in
41:46 - the form of a table therefore pandas can
41:48 - easily read the data from the table
41:50 - which is inside the html file and then
41:53 - we're going to use it by dropping some
41:56 - of the redundant header which is present
41:58 - throughout the table data and then after
42:01 - removing those we will perform some
42:04 - simple deletion of some index column
42:08 - here called rk because it will be
42:10 - redundant with the index provided
42:11 - normally by pandas and then finally we
42:14 - will display the pre-processed data and
42:17 - then on line number 29 we're going to
42:19 - make use of this custom function load
42:21 - data and then the selected year is the
42:23 - input argument which is the variable
42:26 - here selected year so the custom
42:28 - function load data will retrieve the nba
42:30 - player stash data by using the input
42:33 - argument which is provided in the
42:35 - selected year variable and the selected
42:37 - year variable is accepting input from
42:40 - the drop down menu here so whenever you
42:42 - say 2019 the value of 2019 will go into
42:46 - the load data 2019 which is the input
42:49 - argument so we're going to retrieve the
42:51 - data for 2019 and etc for other years
42:54 - that are selected
42:56 - okay so lines number 31323 we're going
42:59 - to allow the user to select the teams
43:01 - that we are going to display in the data
43:04 - frame
43:05 - and so line number 32 is the variable
43:07 - called sorted unique team and so here
43:10 - we're going to use the player stats data
43:13 - frame particularly the team column and
43:16 - then we're only going to display the
43:18 - unique values okay and then after that
43:21 - we're going to sort the team
43:23 - alphabetically and then it will be
43:25 - displayed here right starting from a
43:27 - until w here
43:29 - and then on lines number 35 36 37 we're
43:33 - also going to allow the user to select
43:35 - the positions
43:37 - and so here we have specified the five
43:40 - traditional positions okay so let's have
43:42 - a look at the import argument of the
43:45 - multi-select function so this
43:47 - multi-select function will allow us to
43:49 - display all of the possible values
43:52 - inside the team variable and the
43:54 - position variable
43:56 - and the second and third input argument
43:58 - here will be telling the multi-select
44:01 - what are the possible values that we are
44:04 - going to be using so the second input
44:06 - argument here will be using all of the
44:09 - team names and all of the position names
44:12 - as provided on lines number 36 and 32
44:16 - and then the third input argument for
44:18 - both of them we're going to also use as
44:21 - default value which we will be
44:22 - displaying here to be the same thing so
44:25 - essentially we're keeping the second and
44:27 - third argument to be the same so this
44:29 - means that we're going to display all of
44:32 - the possible values in
44:34 - here so let me show you what if i do a
44:38 - simple slicing of only the first index
44:40 - number so i will be showing you only one
44:43 - position which is the first position
44:45 - right here center so if i do the same
44:47 - thing here
44:52 - i'm going to show only the first team
44:54 - which is atlanta right so here you're
44:56 - gonna see that the options are showing
44:58 - only one value so if i change this to
45:01 - two
45:03 - or
45:04 - three
45:05 - just some arbitrary number and so you're
45:07 - gonna see that these arbitrary numbers
45:09 - that i have provided will show you the
45:12 - input values as the default
45:15 - so if i just say everything we're gonna
45:18 - use everything
45:21 - so therefore
45:22 - the second and third will be the same
45:26 - okay
45:28 - so let's save it okay so here we have
45:31 - all of the possible values
45:33 - so let's say if we change the year to
45:35 - 2018 you're going to notice that it's
45:37 - going to be loading the data for the
45:38 - first time and so it will say running
45:41 - load data function however if we switch
45:44 - back to 2019 where the data has already
45:47 - been loaded and so
45:48 - it will use the cache data and so if i
45:51 - switch back to 2018 the data has already
45:53 - been loaded just a few moments ago
45:56 - and so the data will be instantaneously
45:58 - displayed however if i select 2017 where
46:02 - the data has never been loaded before it
46:05 - will load for the first time and you're
46:06 - gonna see running load data function
46:10 - and then it will catch the data okay
46:13 - right here
46:14 - in the
46:16 - line 20 we're using the add sign
46:19 - st.catch before the load data function
46:22 - and therefore whenever we
46:25 - select a new year it will run for the
46:27 - first time and then it will make a catch
46:29 - of that data so that the second time we
46:31 - make use of it it will be performing
46:34 - much quicker
46:36 - and all of the underlying data will be
46:38 - updated as well
46:47 - filtering data okay here let's have a
46:50 - look on line number 40.
46:52 - so line number 40 will be filtering data
46:55 - based on the input selection in the
46:58 - sidebar menu so if we select
47:01 - dallas out denver out chicago out
47:04 - cleveland out then these resulting input
47:08 - will be dictating which rows to be shown
47:12 - and so you're going to notice that the
47:13 - data dimension will be updated to be 512
47:15 - rows now and so if we take away some of
47:18 - it then the number of rows will be
47:20 - impacted here and so all of this is
47:22 - possible because of this filtering data
47:24 - part which is line number 40. so
47:26 - whenever we make a selection on the
47:29 - sidebar it will be in the selected team
47:31 - variable right here
47:33 - and also in the selected position
47:36 - variable right here which is
47:38 - corresponding to team here and position
47:41 - here
47:42 - so whenever we update the selection
47:46 - the data frame will be updated because
47:48 - the selected team variable and the
47:50 - selected position variable will be
47:52 - updated and so we're also going to only
47:55 - filter the data and show only the
47:57 - remaining input selection okay so all of
48:00 - this is possible by this line so i might
48:02 - make another video about selecting data
48:05 - in pandas so you could do this by using
48:07 - the name of the data frame and then you
48:09 - use a bracket and then inside the
48:11 - bracket you put in your condition okay
48:13 - so this is very powerful and i believe
48:16 - that for those of you working with data
48:18 - wrangling or data cleaning then this
48:21 - filtering capability here will be
48:23 - immensely useful for your project
48:25 - particularly when you are preparing your
48:28 - own data set all right lines number 42
48:30 - 43 and 44 will be displaying the
48:34 - header here
48:36 - and also the description of the data
48:38 - dimension
48:39 - and it will be showing the data frame
48:42 - itself right here so it will be showing
48:43 - this df selected team data frame the one
48:46 - that we have done filtering on
48:49 - okay and then lines number 48 through 52
48:52 - it is a custom function called file
48:54 - download so the resulting data frame
48:56 - that we have here we're going to put it
48:58 - inside the file download as the input
49:01 - argument and therefore it will create
49:03 - this link for us download csv file so
49:06 - notice at the bottom that you're going
49:08 - to see a long generated code here and so
49:11 - this is made possible by the base 64
49:14 - library and so it's going to perform
49:16 - some encoding and decoding okay and so
49:18 - all of this will be inside this function
49:20 - and so this function is brought to you
49:23 - by the wonderful discussion in this
49:25 - streamlit how to okay and so let's have
49:28 - a look at the heat map so lines number
49:30 - 56 until 68
49:33 - is the heat map right here where we
49:35 - click on the button and then it shows
49:36 - the heat map
49:38 - it is possible by this if statement if
49:41 - st button and then we have the name of
49:44 - the button inside as argument and then
49:46 - we have all of these lines of code as
49:48 - the underlying code so st dot header is
49:51 - the header here
49:52 - so it is right here the header and then
49:54 - we're going to have df selected team
49:57 - saving it into a csv file and then we're
50:00 - gonna read it back in and so the reason
50:02 - is why are we doing that because i have
50:04 - been trying to create the heat map using
50:07 - the df selected team and it's not
50:09 - working and upon exporting it out as a
50:12 - file and then reading it back in it read
50:14 - perfectly so i think it has something to
50:16 - do with the data type that is inside the
50:18 - pandas data frame but upon reading it
50:20 - back in there were no issues okay and so
50:23 - then we performed some intercorrelation
50:25 - matrix calculation and then we created
50:28 - the heat map and using this line of code
50:31 - here we created this
50:33 - heat map where half of it are not shown
50:35 - because they are masked
50:43 - do you like football do you like data
50:46 - science if you answered yes to both then
50:48 - this video is for you and today we're
50:51 - going to talk about how you could merge
50:53 - football and data science together and
50:56 - without further ado we're starting right
50:58 - now so in this video we're going to
50:59 - cover how you could build your very own
51:02 - simple web application for exploring the
51:05 - nfl player stats data okay so let's get
51:08 - started so the first thing you want to
51:10 - do is fire up your google chrome or your
51:13 - internet browser and then you want to
51:15 - click on the seasons tab go to 2019 nfl
51:19 - click on the team stats and standings
51:23 - and then click on the player stats and
51:26 - then for the standard you want to click
51:28 - on rushing and so please note that the
51:30 - data that we're going to be using today
51:32 - will be based on the rushing data and if
51:34 - you would like to use other then please
51:36 - feel free to play around with it and so
51:38 - you want to click on the rushing
51:41 - okay and so we're going to be taken to
51:42 - this page here and so you're going to
51:44 - see the player stats right here and so
51:46 - we're going to be scraping the data from
51:48 - this website and so let us copy this url
51:53 - all right and so now we're going to
51:55 - fire up the terminal and as always i'll
51:58 - be activating my conda environment
52:01 - so on your own computer if you have
52:03 - conduct installed and you have a contact
52:06 - environment then you could activate your
52:07 - own content environment but if you don't
52:09 - have any then you don't have to do
52:10 - anything so i like to use conda because
52:12 - it allows me to contain all of the
52:15 - libraries and packages and dependency in
52:18 - a self-contained manner so that it
52:19 - doesn't ruin the other project that i'm
52:21 - working on on the same computer
52:25 - so i'm going to desktop here
52:30 - streamlight folder going to the football
52:34 - all right and so that's the
52:39 - file that we have football app dot py
52:43 - okay and so we're gonna take a look at
52:45 - the code in adam
52:47 - and let me also fire up the web app as
52:50 - well
52:57 - okay and so this is the web application
53:00 - that you're seeing here and on the left
53:01 - you're going to see a collapsible side
53:04 - panel
53:05 - and so the side panel here will have a
53:06 - total of three input sections so the
53:09 - first one will be taking the user input
53:11 - of which year you want the data to be
53:13 - from so here we're using a default of
53:15 - the year 2019 and if we click down on it
53:18 - you will notice that it'll start from
53:21 - 1990 and so we could change that year
53:23 - and i'll be showing you that in just a
53:25 - moment and the teams will be detected
53:28 - directly from the data frame here
53:30 - and then the position is taken from the
53:33 - pos column here okay
53:37 - and so notice that this web app that i'm
53:39 - going to be demonstrating today we have
53:41 - not yet done any data cleaning and so
53:44 - we're using only the completed data so
53:47 - you're going to see that there are a
53:48 - total of 117
53:50 - rows here
53:52 - so i'll be leaving it to you as a hobby
53:55 - project for you to clean the data and
53:57 - let's see how bigger the data set will
53:59 - become so let me show you
54:02 - what does the raw data looks like
54:05 - okay so that's the raw data
54:07 - okay
54:10 - and as you can see there are a total of
54:11 - about 344 rolls here
54:15 - versus 117 which is the complete data
54:19 - okay so it's clean here the one that we
54:22 - have
54:23 - so let me hide this again
54:28 - okay and let's take a look further at
54:30 - the functionality of the web app that we
54:32 - have here so if you click on the
54:33 - intercorrelation heat map you're going
54:35 - to be seeing the intercorrelation of the
54:38 - variables here
54:41 - okay so let's take a line by line
54:42 - explanation of the code here so the
54:45 - first six lines of code will be
54:47 - importing the necessary libraries that
54:49 - we are going to be using today and so
54:51 - the first one is the streamlit because
54:53 - it allows us to build this essentially
54:55 - this web app and then we're going to
54:57 - import pandas as pd because of the data
54:59 - frame that we're using here we're going
55:01 - to be using base64 because of
55:04 - here the functionality to download the
55:07 - data as a csv file so it will be
55:10 - essentially encoding it and decoding it
55:13 - and so here we're going to be making use
55:15 - of matplotlib dot pi plot as plt and
55:18 - also the import seaborn as sns so we're
55:21 - going to use both of them together to
55:23 - make the histogram plot that we see
55:25 - right here and so here as you can see
55:26 - we're using numpy in the creation of
55:29 - this histogram plot
55:31 - all right and so now let's move on to
55:33 - line number eight line number eight here
55:35 - is the title of the web app and the
55:37 - title is nfl football stats rushing
55:41 - explorer lines number 10 through 14 is
55:44 - right here the explanation of the web
55:47 - app this app performed simple web
55:50 - scraping of nfl football player stats
55:52 - data focusing on rushing and so the
55:55 - python libraries that we're using is
55:57 - included right here and actually is not
55:59 - yet complete we also have numpy
56:02 - we also have matplotlib
56:05 - and
56:06 - seaborn
56:08 - and so the data source is coming from
56:10 - theprofootballreference.com
56:13 - and so notice that a couple of minutes
56:15 - ago i've shown you how
56:18 - i've copied this url link
56:20 - okay this link at the top here
56:23 - all right and then i've pasted right
56:25 - here as a reference so in the load data
56:28 - function notice that for the url
56:31 - right here this is the url that we are
56:34 - going to web script the data and notice
56:36 - that here we're setting the date in the
56:38 - range of 1990 to 2020 and so notice that
56:42 - in the url you have the first component
56:45 - here which is essentially right here the
56:47 - first segment and then the second
56:49 - segment will be the year which we are
56:51 - going to do programmatically by
56:53 - replacing it with a string of the year
56:55 - and then we're going to add forward
56:57 - slash
56:58 - rushing.htm
57:00 - right here with the forward slash and so
57:02 - essentially this will give us this url
57:05 - right here if 2019 is selected here but
57:08 - if we select 2018
57:11 - then this becomes 2018
57:14 - and the data will then be updated
57:16 - and here we set the header to be one
57:20 - in order to retrieve the data set here
57:23 - so this is the function for doing the
57:26 - web scripting using the pandas library
57:29 - and so as you can see that the web
57:30 - scripting is done in only one line of
57:32 - code here and then the other lines we're
57:35 - going to essentially be pre-processing
57:38 - the data set meaning that we're dropping
57:40 - some redundant headers or we're also
57:43 - dropping some repetitive columns and
57:46 - values etc
57:49 - and then finally we're going to be
57:50 - assigning the load data to the player
57:53 - stats variable
57:55 - and then we're going to be sorting the
57:57 - team
57:58 - and you're going to see the sorted team
58:00 - right here in the user input features
58:02 - here okay which is line number 33 and
58:05 - 34. so after we sorted the team and
58:08 - we're showing the unique values because
58:11 - here you're going to see that in the raw
58:12 - data the team values will be repetitive
58:16 - and so here we're sorting according to
58:18 - the unique values and so we're going to
58:20 - see only
58:21 - unique values of the team names here and
58:23 - in sorted order
58:25 - all right and then lines number 36
58:27 - through 38 is going to be right here the
58:30 - position so here we have said it to be
58:33 - rb qb wr fb and te okay and we're also
58:38 - using it as a multi-select meaning that
58:40 - you could select multiple values at the
58:42 - same time or you could just delete it
58:44 - and then select the ones that you like
58:48 - okay
58:51 - okay you could add one by one
58:54 - the values that you like
58:56 - okay
58:59 - and then alliance number 40 and 41 here
59:02 - we're going to be filtering the data
59:04 - based on the input of the sidebar of the
59:08 - team selection and the position
59:10 - selection so line number 41 will be
59:13 - essentially filtering the data frame
59:15 - that we're seeing right here
59:17 - based on our input selection the team
59:20 - and the position
59:22 - lines number 43 through 45 we're going
59:26 - to be displaying the header called
59:29 - display player stats of selected team
59:32 - right here
59:33 - and also the
59:34 - data dimension in a normal text
59:38 - underneath the header and then the
59:39 - actual data frame will be displayed
59:41 - according to line number 45 right here
59:44 - okay and then lines 47 through 55 it
59:48 - will allow us to download this data
59:51 - frame into a csv file
59:54 - so let's download it
59:57 - and as a recall we're using the base64
60:00 - library in order to perform the
60:03 - encoding decoding of the data and so
60:05 - you're going to see that the data is
60:07 - downloaded into this file
60:11 - and then the remaining part of the code
60:13 - will essentially allow us to make the
60:14 - heat map shown right here if you click
60:17 - on the button enter correlation heat map
60:19 - you click on it and then you will be
60:21 - seeing this heat map here of the
60:23 - intercorrelation between the variables
60:26 - and so this block of code will allow you
60:28 - to make the heat map
60:30 - and so as you can see here all of this
60:32 - is just under 70 lines of code and it
60:35 - allows you to build a very simple data
60:38 - driven web application for retrieving or
60:41 - web scraping the nfl football player
60:44 - stats data
60:52 - in this video i'm going to show you how
60:54 - you could build a data driven web
60:56 - application in python for web scraping s
60:59 - p 500 stock prices and without further
61:03 - ado we're starting right now okay so the
61:05 - first thing that you want to do is fire
61:07 - up your terminal
61:10 - so today i'm going to use the microsoft
61:12 - windows
61:13 - and then i'm going to as always activate
61:15 - my conda environment
61:21 - and the files are on the desktop
61:23 - and so i'm going to provide you the
61:25 - links to the files described in this
61:27 - tutorial so you want to check out the
61:29 - video description
61:33 - sp 500
61:36 - okay so you're going to see that the
61:38 - only file that we're going to use is the
61:40 - sp 500-app.py
61:43 - so let's have a look at that
62:01 - and before doing that let's also open up
62:03 - the web app
62:06 - streamlet run
62:08 - sp 500
62:10 - app.py
62:15 - all right so
62:16 - here we are okay so you're going to see
62:18 - that in the web app we're going to have
62:20 - a sidebar on the left
62:23 - and here we're going to have two input
62:25 - features the first one will allow us to
62:28 - select the sectors and the second one
62:30 - will allow us to select the number of
62:32 - companies
62:35 - and the data for the names of the
62:37 - company in the s p 500 was taken from
62:41 - wikipedia and so i'm going to show you
62:43 - where we got that the links is right
62:45 - here
63:01 - okay so the table that we're going to
63:03 - web scrape is this table here
63:06 - so we're going to see that it has the
63:08 - symbol
63:09 - for the company
63:10 - the name of the company
63:12 - and other information like the sector
63:15 - and the sub industry sector
63:19 - the headquarter location
63:21 - and also the founding dates of the
63:23 - company
63:24 - and notice that there will be
63:26 - two tables
63:28 - and the second table will be here but
63:30 - for the purpose of this tutorial we're
63:32 - going to use only this table here
63:35 - so before we take a dive into the web
63:38 - application let's have a look at how did
63:41 - i created the essential functions for
63:44 - making this web app
63:47 - so let me first go to google collab
63:57 - so let me expand this a bit
64:01 - okay so
64:02 - here is the details of how we're going
64:05 - to
64:05 - make the web application so firstly
64:08 - we're going to create a function a
64:10 - custom function that will allow us to
64:12 - web script the data from wikipedia so
64:15 - here we're defining a function called
64:17 - load data and then
64:19 - the first thing is we're going to put in
64:21 - the url of the s p 500 which is
64:25 - essentially right here and then we know
64:26 - that we're going to take this table so
64:28 - the prerequisite of web scraping using
64:31 - pandas read html function is that the
64:35 - data must be in a table so if data is in
64:38 - a paragraph like this it wouldn't work
64:41 - so you would need something like
64:43 - beautiful soup and selenium in order to
64:45 - do that
64:47 - and because the data is organized in a
64:49 - very simple table manner here we're
64:51 - going to just specify that we want the
64:54 - first table so we're going to specify
64:56 - this to be zero okay and then the data
64:58 - will go into the df which we will return
65:02 - here and upon running this cell
65:05 - let's do it
65:15 - so you're going to see that the web
65:17 - script data is provided in the df
65:20 - variable here and it looks very nice
65:24 - let's take a look at the sectors here in
65:26 - the fourth column sector
65:30 - so we're gonna select the sector column
65:33 - from the df data frame and then we're
65:35 - going to take out the unique count
65:38 - how many unique sectors are there and so
65:41 - we're going to see that there are a
65:43 - total of 11 sectors comprising of
65:46 - industrials healthcare information
65:49 - technology communication services
65:51 - consumer discretionary utilities
65:54 - financials materials real estate
65:57 - consumer staples energy
66:01 - okay and then so let's say that we're
66:03 - going to aggregate the data by grouping
66:06 - them according to the specific sector
66:08 - name which is comprised of 11 different
66:11 - sectors
66:12 - and then we're going to show only the
66:14 - first company for each of the sector of
66:17 - the 11 sector
66:18 - so we're going to see that for each of
66:20 - the 11 sector in the first index column
66:23 - here we're going to see the first
66:24 - example of the company name so this
66:27 - represents the first company for each of
66:30 - the 11 sectors
66:33 - now let's do a descriptive statistics of
66:36 - this sector variable
66:39 - so we're going to see that there are a
66:40 - total of 26 companies in the
66:42 - communication services 60 company in the
66:45 - consumer
66:46 - discretionary 32 company and consumer
66:49 - staples
66:52 - let's take a look at only the health
66:53 - care so let's take a look at only the
66:55 - health care sector so we're going to use
66:58 - the get group function
67:00 - and then here we're going to see 63
67:03 - companies
67:05 - so all of the companies here are
67:07 - belonging to the healthcare sector
67:11 - okay so this first part are data derived
67:15 - directly from the wikipedia website and
67:18 - now let's do a second part where we take
67:21 - the names of the s p 500 and then we're
67:24 - going to retrieve the stock price data
67:26 - from the y finance library in python
67:30 - so the first thing is to install the y
67:32 - finance library okay and so now we have
67:35 - installed it and now we're going to
67:36 - import it as yf
67:39 - okay and so here these are the list of
67:42 - the company symbols from the df data
67:44 - frame that we have got above where we
67:47 - web script from wikipedia
67:49 - so here all of the company names are
67:51 - here
67:52 - and now we're going to retrieve the
67:54 - stock data so using the example code
67:57 - from this url directly from the y
68:00 - finance pipe project web page there is a
68:03 - function to download the stock price
68:05 - data and here we're going to use the
68:08 - period of ytd which is the year to date
68:11 - so this will be from the beginning of
68:13 - the year right until right now which is
68:15 - october 6 2020 and then the ticker will
68:18 - be all of the data in the s p 500 so
68:22 - there's going to be more than 500
68:23 - companies here
68:25 - and then the data will be grouped by the
68:27 - ticker which is the symbol of the
68:28 - company
68:30 - and then everything else we're just
68:31 - going to leave it as default
68:34 - all right so let's run it
68:36 - so this should take a moment because
68:38 - it's going to download all of the stock
68:39 - price data from the y finance and it
68:43 - will be placed into the data data frame
68:50 - and note that the interval is one day
68:53 - which is also the default and you could
68:55 - feel free to change the interval to like
68:57 - one minute two minute but then this will
68:59 - generate a lot of data or you can make
69:01 - it in monthly intervals as well or even
69:04 - weekly intervals
69:06 - alright and so it apparently is finished
69:08 - let's have a look further so it's saying
69:11 - that two companies could not fetch the
69:13 - data from bf and brk
69:17 - and 505 have been completed out of the
69:19 - 507
69:22 - okay and so this is the stock price data
69:25 - and you can see that the price
69:27 - information are grouped according to the
69:29 - ticker symbol here
69:31 - and then you could easily retrieve
69:33 - specific companies by specifying it in
69:36 - the bracket
69:37 - okay
69:40 - and then you get the data that you need
69:42 - 192 rows because
69:45 - it is starting since january until right
69:48 - now this is october 192 days
69:51 - okay and so what we're gonna do here is
69:54 - we're going to move the date to be one
69:56 - of the column
69:58 - as you can see that now the date is part
70:00 - of the column here and we want to make
70:03 - use of the closing price so our new data
70:06 - frame will contain two columns the date
70:09 - and also the close price so you could
70:11 - feel free to play around with the price
70:13 - data if you want to use the open or the
70:16 - high or the low price but for this
70:18 - tutorial we're only going to stick to
70:20 - the closing price
70:22 - all right and now we're going to do the
70:23 - fun part we're going to make a plot of
70:25 - it
70:26 - and so this is the plot of the closing
70:28 - price and in the x-axis is the date and
70:32 - the closing price will be in the y-axis
70:34 - so you're going to see that the price is
70:36 - dipped around april and then it kind of
70:38 - went up and if you compare from
70:40 - beginning of the year until right now
70:42 - the price is about 18
70:45 - higher okay
70:46 - so let's do the same thing but then
70:48 - we're going to make a custom function
70:50 - and the reason for making a custom
70:51 - function is that it's going to make our
70:53 - life
70:54 - and the reason for making this custom
70:55 - function is that it's going to make our
70:57 - life much easier because we're only
70:59 - going to specify the name of the symbol
71:01 - that's the input argument like right
71:03 - here
71:04 - and then it's going to make the plot for
71:05 - you like here that we don't have to copy
71:08 - the entire code here then modify it to
71:11 - suit the name of the ticker symbol
71:14 - so we're going to make a general helper
71:16 - function
71:19 - let's do it here
71:21 - so the custom function that we're making
71:23 - is called the price plot
71:26 - and then we're gonna use price plot and
71:28 - then the input argument will be the
71:30 - ticker symbol
71:32 - we could change the company if we like
71:37 - all right
71:38 - and then you're going to see that the
71:39 - price is
71:40 - is updated now
71:43 - and then the beautiful part about this
71:45 - is that we can even run it as a for loop
71:49 - so the price plot will be inside the for
71:51 - loop and then we're gonna iterate
71:53 - through all of the company names but
71:56 - then here we're gonna select only the
71:57 - first 10 companies
72:06 - okay i have to run
72:27 - oh okay
72:29 - we updated the name
72:31 - here
72:32 - df okay we're using the same name then
72:34 - we have to
72:41 - take the name here
72:47 - we have to rerun
72:50 - the load data function
72:53 - okay and now it will be reassigned to
72:56 - the df
72:57 - because previously we have overwritten
72:59 - the df variable name
73:03 - and it is lacking the symbol column
73:07 - right here
73:13 - right here we have overwritten the same
73:15 - name
73:16 - so i think we should call this something
73:18 - else
73:21 - let's call it df2
73:41 - and this will have to be df2
73:49 - right
73:50 - let's do it again
73:52 - this is df2
73:55 - and then
73:58 - okay it works again
74:07 - it should work now because we have
74:09 - already
74:10 - run the symbol
74:12 - here
74:20 - df symbol
74:23 - all right it's working
74:25 - and we want the first 10 companies
74:28 - all right 1
74:30 - 2
74:31 - 3
74:32 - 4
74:33 - 5
74:34 - 6 7 8
74:36 - 9
74:37 - 10. all right there we have it
74:41 - and so the proof of concept is now
74:42 - working and we have tried the proof of
74:44 - concept directly on the google code lab
74:47 - so the great part of making this proof
74:49 - of concept on the google code lab is
74:51 - that we could be quite flexible in
74:53 - working on different computers and using
74:55 - the same google code lab and once we're
74:57 - finished with that then we could start
74:59 - the production phase where we will
75:01 - deploy it to the web application in
75:03 - streamlit and so now we're ready to
75:05 - deploy it onto streamlit and let's do
75:08 - that
75:12 - and there you go we have already
75:13 - deployed it to streamlid a couple of
75:15 - minutes ago all right and so let's take
75:17 - a look at this web application so in the
75:20 - user input here we're going to have the
75:22 - name of the sectors where we could
75:24 - essentially select the sector here and
75:26 - then we also are able to select the
75:28 - number of companies that we want to show
75:31 - in the show plot area so if we have two
75:34 - companies then the show plot area will
75:36 - also have two buffs
75:40 - up to a maximum of five and the maximum
75:43 - of five could be changed to another
75:45 - value that you would like
75:48 - okay and so i'm using five just an
75:50 - arbitrary number and here we have five
75:52 - plots
75:54 - and the great thing about this is that
75:55 - you could also modify this web
75:57 - application so that it will be tailored
75:59 - to your own country's stock price so let
76:02 - me know in the comments how you're
76:04 - making use of this or how you are
76:06 - modifying this web application
76:12 - and
76:13 - if you would like to share
76:15 - your creation let me know and i could
76:17 - share it on one of the tutorial videos
76:20 - where i could show how to recreate the
76:22 - web application that you have made and
76:24 - so drop me an email and the email will
76:26 - be provided in the description of the
76:29 - video okay and so let's take a line by
76:31 - line look at the explanation of the code
76:34 - so here the first seven lines will be
76:36 - importing the necessary libraries so
76:39 - first one is the streamlit the second
76:41 - one is pandas and we're gonna use
76:43 - streamlit because this web application
76:46 - is built on top of the streamlit library
76:49 - and we're going to use the pandas
76:51 - because of the data frame by which we
76:53 - are showing the data and then the base
76:55 - 64 will allow us to encode the data so
76:58 - that we are able to provide it as a csv
77:01 - file
77:02 - and here we're going to use matplotlib
77:04 - for making the plots and apparently we
77:06 - didn't use seaborn so let me delete that
77:09 - and also from here
77:12 - all right
77:13 - and we also have y finance
77:18 - we're going to add it here
77:23 - all right and so that left us with a
77:25 - total of
77:26 - six library
77:28 - and then the next one is numpy and why
77:31 - finance
77:33 - let's see did we use numpy
77:36 - no we did not use numpy sorry
77:39 - because i was using a previous web
77:42 - application as the template for this one
77:44 - so even less now we're having only
77:49 - a total of five libraries so why finance
77:52 - the fifth one is going to allow us to
77:55 - retrieve the stock price for the s p 500
77:58 - and so you're gonna see here that we are
78:00 - making use of only five python libraries
78:03 - all right and so the st.title will be
78:06 - the function that we're gonna use to
78:08 - make the title of the web application
78:10 - here s p 500 app shown as a both text
78:13 - here and then we're going to make use of
78:15 - the markdown function in order to
78:18 - display the details of the web
78:20 - application and also the libraries that
78:22 - we're using in python and also the data
78:24 - source which is taken from wikipedia
78:27 - and then we have the st.sidebar dot
78:30 - header
78:32 - it means that we're going to create
78:34 - a header called user input features and
78:36 - then we're going to put it inside the
78:38 - sidebar to the left here on line number
78:40 - 19 we're going to say that we want to
78:43 - catch the data if it has already been
78:45 - run for the first time so that the
78:46 - second time or subsequent time it
78:49 - wouldn't need to redownload the data
78:51 - again and again and so it only needs to
78:53 - do that only once for the first time so
78:55 - lines number 20 to 24 is a custom
78:58 - function that we have taken directly
79:00 - from the google code lab where we have
79:02 - experimented with the creation of this
79:05 - web scraper of the s p 500 data and so
79:08 - we just paste it right here and then
79:09 - we're going to assign the web script
79:11 - data into the df data frame and then
79:14 - we're going to group by the sector name
79:16 - which is right here we're going to group
79:17 - by the it's not shown here
79:19 - okay right here we're going to group by
79:21 - the sector names on line number 27
79:24 - and then lines number 30 and 31 we're
79:26 - going to display the sector name as an
79:30 - option where you could select you want
79:31 - to select only particular one
79:38 - here we are only selecting communication
79:40 - services and so we're going to see that
79:42 - there are 26 companies here
79:45 - or we could also add one by one the
79:48 - companies that we like
79:51 - i mean the sectors of the company that
79:52 - we like
79:55 - all right and then line number 34 is
79:57 - only going to filter out from the entire
80:00 - data frame
80:01 - the sectors that you have selected in
80:04 - the
80:05 - user input feature panel here
80:08 - so it's going to use the is in function
80:11 - in order to see which are coming from
80:14 - the selected sectors right here three
80:17 - selected sector which is right here
80:19 - selected sector
80:21 - coming from the sidebar and multi-select
80:24 - function
80:25 - multi-select means here we have multi
80:28 - selection
80:29 - and now the data will be in the df
80:32 - selected sector and then we're going to
80:34 - write it out here in the st
80:36 - data frame and the input argument is df
80:39 - selected sector which is coming from
80:41 - here and it's filtering all data
80:44 - belonging to the three selected sectors
80:46 - here or it could be four
80:50 - right here okay
80:53 - and in lines number 37 it's going to
80:55 - write out the dimension right here
80:57 - number of rows and number of columns and
81:00 - the st.header will be the function to
81:02 - display the header here
81:06 - all right and lines number 42 until 46
81:10 - is going to be the custom function to
81:12 - allow us to decode encode the data and
81:15 - make the csv file of this data available
81:18 - for download right here
81:25 - and then you get access to the csv data
81:28 - all right
81:30 - and then the second part is the finance
81:32 - data that i have shown you just a moment
81:34 - ago on the google code lab and so here
81:36 - we're going to make use of the constant
81:39 - function that we have already done just
81:41 - a moment ago and then for this example
81:43 - we're just going to limit it to about 10
81:45 - companies otherwise it might have taken
81:47 - longer to create the web application
81:50 - okay and so let's have a look further
81:52 - and then the custom function to make the
81:54 - plot was also taken from the google
81:56 - collab
81:57 - and so lines number 52 until 61 will
82:00 - download the data of the stock price
82:02 - directly from y finance
82:05 - lines number 63 until 73 is the custom
82:09 - function to create the plot that we're
82:11 - going to see here in the show plot
82:15 - okay in line number 75 it is the slider
82:17 - for you to select the number of
82:19 - companies so if you want to make it into
82:21 - 10 then you could do so as well multiply
82:24 - that to be 10 and then the number here
82:26 - will then be 10 and now you could have
82:28 - 10 plus
82:31 - okay you could have 10 plots now
82:35 - and make sure to play around with this
82:37 - number as well
82:39 - make sure that this number is greater
82:41 - than
82:43 - this number
82:45 - okay because it will be the total number
82:47 - of stock price data that you're going to
82:49 - have
82:53 - let me change it back to five
83:06 - and let's say that you don't want to
83:07 - hide the
83:08 - plots here
83:10 - you just want to show it
83:13 - you could hide that function the if
83:16 - let's see
83:20 - it's indented
83:24 - okay let's see again
83:28 - all right and now you have the plots
83:30 - immediately
83:33 - if this is 10 then
83:36 - you're going to get the 10 plots right
83:39 - here
83:42 - okay
84:08 - do you like cryptocurrency
84:10 - do you like data science
84:12 - if you answered yes to both questions
84:15 - then this video is for you because today
84:17 - i'm going to show you how you could
84:19 - build a cryptocurrency price web
84:22 - application
84:23 - and without further ado we're starting
84:25 - right now okay so the first thing that
84:27 - you want to do is fire up your terminal
84:34 - and then for me i have to activate my
84:36 - conda environment
84:40 - and you could do the same for your own
84:42 - environment as well
84:46 - and i'm going to the desktop because
84:48 - that's where i keep my files
84:51 - for the streamlet tutorials
84:54 - and
84:55 - i'm having the data in the crypto price
84:59 - folder
85:02 - all right and so
85:04 - in here you'll notice that we'll have
85:06 - two files the crypto app itself
85:09 - and the logo
85:13 - so let's open up the
85:15 - web application
85:16 - so i have to run streamlit
85:19 - run
85:20 - crypto
85:22 - price app dot py
85:29 - all right and here you go this is the
85:30 - web application that we are going to be
85:33 - building today
85:35 - and
85:36 - this is the cryptocurrency logo that i
85:39 - have drawn
85:40 - and it is the logo.jpg file
85:44 - so to the left here is the
85:47 - input parameters
85:49 - in the side panel
85:51 - so here you could select the currency
85:53 - for the price
85:55 - at default it will be using the usd
85:58 - and you have a selection of three
86:00 - choices usd
86:03 - btc
86:04 - or eth
86:08 - and then here we'll be listing the 100
86:11 - top cryptocurrency
86:16 - okay
86:18 - and then here
86:20 - it will ask for the option to select how
86:24 - many top cryptocurrency should be
86:26 - displayed
86:28 - in the data frame table here and also in
86:31 - the chart here
86:34 - another option that you get to select is
86:36 - the percent change for the time frame
86:40 - the percent change meaning the price
86:42 - change
86:43 - that has occurred within the last seven
86:46 - days
86:48 - and you have a selection of three
86:50 - choices
86:51 - within seven days within 24 hours or
86:54 - within one hour
86:58 - and then the last option here that you
87:00 - could select is
87:01 - do you want to sort the values in here
87:05 - in the chart
87:07 - so you see here that the green color
87:09 - will represent the
87:11 - price change that is changed for the
87:14 - positive gain
87:16 - while some of the cryptocurrency will
87:18 - have a negative
87:20 - pricing here
87:22 - meaning that when it's compared between
87:25 - the first day and the seventh day
87:28 - when the seven days selected
87:29 - if the price change is negative it means
87:32 - that the price has reduced
87:34 - however if the price has increased
87:37 - then there is a gain okay so it's
87:39 - essentially the
87:40 - gain
87:41 - of the price in green or the loss in red
87:47 - okay and so this is the cryptocurrency
87:49 - web app that we are going to be building
87:51 - today
87:52 - and you can notice that here the
87:55 - interface and the layout of the web
87:57 - application is full screen now
88:00 - because before
88:03 - the web application will be a bit
88:06 - centered at the middle
88:08 - and so we're going to be using the
88:11 - estate of the whole entire width of the
88:14 - monitor here
88:16 - and then the other exciting
88:18 - update from srimlet is that
88:21 - aside from being able to use the entire
88:23 - width of the screen
88:25 - we are also able to
88:27 - divide our contents into multiple
88:30 - columns
88:32 - so here we have a total of three which
88:34 - is the side panel
88:36 - so we're going to consider the side
88:37 - panel as the first column
88:40 - but also it should be noted that it
88:41 - could also be
88:43 - collapsed as well and then you're also
88:45 - going to have the entire
88:47 - screen
88:49 - for the content of the web application
88:52 - however for this web application we are
88:55 - going to consider the sidebar as of
88:57 - column
88:58 - one
88:59 - and then we're also going to consider
89:01 - this part here the data frame as column
89:04 - two
89:05 - and then we're going to consider the bar
89:07 - chart here as column three
89:10 - okay
89:11 - so before we would have only
89:15 - a sidebar
89:17 - and
89:17 - the main content so essentially we would
89:20 - be having like two columns
89:23 - however in the recent update that
89:25 - streamlit has made
89:26 - we are able to have multiple columns so
89:29 - it could be more than three we could
89:31 - make it four or we could make it five
89:34 - and so the multiple column will allow us
89:36 - to
89:37 - partition the content
89:39 - in a very
89:40 - aesthetic way
89:42 - meaning that we could group contents
89:44 - that are related in a particular column
89:47 - and so that would be more visually
89:49 - appealing
89:50 - okay so let's proceed with interpreting
89:54 - the meaning of the code line by line
89:58 - so let me open up the
90:01 - atom
90:08 - okay so i'm working on an automl web
90:10 - application right now so that's going to
90:13 - be for the future video
90:15 - right here crypto price
90:18 - okay
90:20 - so let me
90:23 - make it half the screen
90:34 - okay
90:38 - so you'll be noticing here that the
90:40 - first 11 lines will be the libraries
90:43 - that we are going to be using today
90:46 - and so you will be noticing that we have
90:48 - more than 10 lines in this tutorial in
90:50 - other tutorials we'll have like about
90:52 - six or five lines meaning like five
90:54 - libraries to use
90:57 - but in this tutorial we'll be using a
90:59 - lot of libraries here so the first one
91:02 - is the streamlit which is the basis for
91:05 - this web application
91:07 - so it's the web framework that we are
91:08 - going to be using
91:10 - and then we're going to be using the
91:12 - logo so we're going to use the from pil
91:15 - library
91:16 - and then we're going to import the image
91:18 - function
91:19 - and then we're going to make use of the
91:20 - pandas in order to show the data frame
91:24 - and then import base 64 because we need
91:27 - to encode decode the data in order to
91:29 - allow the user to download the
91:32 - cryptocurrency data here as a csv file
91:38 - because we're making the plots here
91:40 - we're needing the matplotlib
91:42 - all right so actually it's not 11 but
91:45 - only nine libraries okay
91:48 - so probably taken it from the previous
91:52 - project
91:54 - so i've just deleted the numpy and the
91:55 - seaborn so here we're using only the
91:57 - matplotlib for the bar chart
92:00 - and numpy was not used in this project
92:02 - and here we're going to make use of the
92:04 - beautiful soup
92:06 - in order to web scrape the data from the
92:09 - coin market cap website
92:12 - and the basis for the web scraping was
92:15 - taken from
92:16 - the article from brian feng
92:20 - and the article is entitled
92:22 - web scraping crypto prices with python
92:27 - and it is on the medium platform so you
92:29 - could click on this link in order to
92:31 - read the full article by brian
92:34 - so i have adapted his code in the
92:36 - tutorial
92:38 - in order to web scrape the
92:39 - cryptocurrency price here and make it
92:41 - into a web application for our tutorial
92:44 - and so the process of web scraping will
92:47 - require the request library the json and
92:51 - also the time here as well
92:54 - all right so
92:56 - we're finished with importing the
92:57 - libraries and then
92:59 - it should be noted that
93:02 - in order to make use of the
93:04 - new feature particularly the
93:07 - the page width that i have mentioned
93:09 - and also the extra column that is a new
93:12 - feature you need to upgrade your
93:14 - streamlit if you already have it
93:15 - installed on your computer
93:17 - however if you haven't yet installed it
93:19 - then you could install a fresh version
93:22 - but in order to upgrade it you need to
93:24 - type in pip install
93:26 - dash dash upgrade and then streamlit
93:30 - okay
93:32 - and because it is a new feature
93:34 - streamlit has probably used the term
93:37 - beta in front of the option here
93:39 - set page config
93:42 - and then the layout will be equal to
93:44 - wide so this will allow us to expand the
93:46 - content to the full width of the page
93:51 - so let's try commenting it out and see
93:53 - what happens
93:58 - all right and here you go you see that
94:00 - when we comment out the page width
94:03 - it will be centered
94:05 - so you see that there will be white
94:07 - space to the left
94:08 - and to the right
94:12 - and if we use the new feature
94:20 - it will be expanding to the entire paste
94:23 - width here
94:24 - so we have some extra real estate
94:28 - for the plots and for the data frame
94:32 - and considering that we have over 100
94:34 - cryptocurrency here
94:35 - it's always nice to have some extra room
94:38 - to show the plot and the data frame
94:42 - so we're going to make use of this new
94:43 - feature here
94:46 - all right and lines number 21 until 29
94:51 - is going to be the
94:53 - logo here
94:55 - which is line number 21 and 23
94:58 - where we import the logo.jpg file and
95:02 - we're using a width of 500 so the width
95:05 - of the image will be 500 pixel and then
95:08 - the title here is crypto price app right
95:10 - here crypto price app in the st.title
95:15 - and then the description
95:17 - shown here
95:18 - is in the st.markdown function
95:22 - right here
95:25 - and then another new feature here is the
95:27 - about section
95:30 - which is hidden inside the plus symbol
95:32 - here if we click on it it will expand
95:35 - and so this is part of the expander
95:38 - function
95:40 - so i'm calling it the expander bar
95:42 - variable
95:44 - equals to st.beta expander
95:47 - and so as always this is a new feature
95:49 - so they include it by using the beta in
95:52 - front beta so when we're using the
95:55 - expander bar
95:56 - we put it here
95:58 - and then
95:59 - followed by the markdown function
96:02 - and then we
96:03 - include the
96:05 - following
96:07 - bullet points here in the markdown
96:10 - syntax
96:14 - all right and then lines number 43 and
96:17 - 44
96:18 - we are essentially setting the page
96:20 - layout of the web
96:22 - app so we're gonna create a new variable
96:25 - called column one equal to st dot
96:28 - sidebar
96:29 - which is the sidebar here
96:32 - and then we're gonna create two
96:33 - variables at the same time column two
96:36 - and column 3
96:38 - column 2 is right here the data frame
96:40 - and column 3 is right here the bar plot
96:44 - so notice that we have the
96:47 - values of 2 1 inside here
96:50 - it means that we want to have the data
96:52 - frame column
96:53 - to be two times greater than
96:56 - the
96:57 - bar plot column
96:59 - meaning that the width of the
97:02 - second column here will be two times
97:04 - greater
97:05 - than the third column
97:07 - okay so you can see here that the width
97:09 - of this column
97:10 - is twice bigger
97:12 - than the third column
97:14 - which is right here
97:17 - if it's one to one it means that both
97:19 - columns will have equal size
97:22 - okay they're equal
97:24 - and if it's one and then three
97:26 - it means that
97:28 - you want to guess
97:32 - you are correct and so the bar plug will
97:34 - have three times
97:36 - wider
97:37 - width than the first column
97:42 - okay so we're leaving it at
97:44 - two to one
97:51 - all right
97:52 - and let's hop on to the next section
97:55 - here
97:56 - so i'm commenting this part to be side
97:58 - panel
97:59 - or the sidebar plus the main panel
98:02 - so column one dot header is the input
98:05 - option so this is
98:07 - column one meaning the sidebar
98:10 - and so input options is this one the
98:12 - header here
98:15 - and then currency price unit column one
98:18 - dot select box select currency for price
98:21 - it's right here
98:23 - so this select box will allow the user
98:25 - to select which cryptocurrency
98:29 - price unit to use would it be usd would
98:31 - it be bcc or would it be ath
98:36 - and upon selecting
98:38 - the options
98:40 - i mean the data frame will be updating
98:43 - the price unit
98:44 - from usd right here the price unit from
98:47 - usd to
98:48 - bcc or to eth
98:52 - and so the
98:53 - currency price unit will be used later
98:56 - on in the code okay i'm going to show
98:58 - you later on where it is used
99:01 - so this is the
99:03 - ui or the user interface part
99:07 - all right and so here comes the fun part
99:09 - lines
99:11 - will catch the following data
99:14 - meaning that the second time around
99:16 - where you
99:17 - reload the web page it will not re-web
99:19 - scrape the data so it's going to perform
99:21 - the web scraping only once the first
99:24 - time and it's going to keep a catch of
99:26 - that for the subsequent use
99:29 - so that would be beneficial for you when
99:31 - you're
99:32 - trying to improve this web application
99:34 - incrementally without even have to
99:37 - re-perform the web scraping over and
99:39 - over again so that will save a lot of
99:40 - resources
99:43 - so as i mentioned
99:44 - earlier on in this video the web
99:47 - scraping of the coin market cap data was
99:49 - performed according to the article by
99:52 - brian feng and you could read the full
99:55 - detail in his article on medium
99:58 - and so i have adapted that into this
100:00 - customized function
100:02 - load data
100:07 - so some of the data that we're going to
100:08 - web script from the coin market cap so
100:10 - let's have a look at the coin market tab
100:12 - here
100:16 - oops okay i have to change the
100:19 - coin
100:20 - market
100:22 - cap okay let me copy the link
100:26 - and i'll put it here
100:35 - okay
100:37 - so this is the website of the coin
100:39 - market cap so it's displaying the price
100:42 - of the various cryptocurrency the first
100:45 - top hundred
100:46 - and so this is essentially what we are
100:48 - going to web scrape today
100:50 - the top hundred price here
100:55 - so we're gonna get the data of the
100:58 - the name
100:59 - of the cryptocurrency the symbol
101:03 - bitcoin the name of it right and then
101:05 - the
101:06 - symbol of it is btc
101:08 - the price
101:10 - the 24 hour price change
101:13 - the seven day price change and also the
101:15 - one hour price change
101:17 - the market cap value and also the volume
101:21 - so we're gonna web script that today
101:23 - here
101:25 - and all of the data will be placed
101:27 - inside the df
101:29 - variable
101:32 - okay lines number 99
101:35 - and
101:36 - and 100
101:38 - let's have a look here
101:44 - line number 99 and 100 will be
101:46 - multi-selection of the web app okay it's
101:49 - right here
101:51 - so line number 99 and 100 will give us
101:53 - this multiple selection box here
101:57 - so the top 100 will be shown here and so
102:00 - you could select which cryptocurrency
102:01 - you want to be shown here
102:03 - let's say we delete everything and then
102:05 - we're selecting btc
102:08 - eth right what else
102:12 - bnb
102:14 - right and so
102:16 - we selected only three so far and three
102:18 - are shown here
102:20 - okay
102:22 - so let's use the default one of 100
102:28 - all right let's continue
102:31 - so upon our selection of the coins here
102:35 - 102 will
102:37 - display only the selected coins so if we
102:40 - selected five only five will be shown in
102:43 - this data frame
102:45 - right here
102:46 - and then it will also be shown in the
102:48 - bar plot as well
102:50 - so line 102 will do the data filtering
102:56 - line number 104 let's see number of
102:58 - coins to display
102:59 - it's right here
103:01 - display the number of coins here
103:03 - so it's going to be a slider from 1
103:06 - through 100
103:09 - okay one through a hundred
103:15 - all right so if i have it to 55
103:20 - it's going to be showing the first
103:23 - coins
103:28 - see
103:30 - index 54 which is the 55th line
103:34 - and then the bar plot will be also 55
103:39 - coins as well or cryptocurrency
103:44 - all right and so let's have a look at
103:46 - line number 106.
103:48 - so same thing it's going to do the data
103:50 - filtering of the selected coins that you
103:52 - selected
103:54 - line number 108 percent change time
103:56 - frame okay as
103:58 - i mentioned earlier on you could select
104:00 - the time frame that you want to be used
104:04 - so by default it's using the seven days
104:06 - time frame here for the percent change
104:10 - and you could change that to like 24
104:12 - hours if you like
104:14 - or even one hour
104:18 - so the price change that happens within
104:20 - the last hour
104:23 - okay and then you're gonna see which
104:25 - cryptocurrency had a
104:27 - gain a positive gain or a significant
104:30 - loss
104:31 - five percent loss
104:33 - and this one is about the first one is
104:35 - about three percent gain within the last
104:37 - hour
104:39 - okay
104:43 - line number 114
104:47 - sidebar sorting values okay so right
104:49 - here so you're going to see that the
104:51 - values are sorted from highest to
104:54 - lowest gain
104:57 - and also loss as well
105:00 - so if the sort value is yes it's going
105:02 - to sort the value if it's no then it's
105:05 - not sorting the value
105:07 - so you see here that the values are not
105:09 - sorted
105:11 - and the values are in the same order
105:16 - so the
105:17 - first listed coin ranked number one is
105:20 - vtc
105:21 - so it's going to be shown at the bottom
105:23 - here the second rank eth will be shown
105:25 - here
105:26 - usdt will be shown as the third one from
105:29 - the bottom
105:30 - xrp will be shown as the fourth one
105:34 - so it's going to be in the same order as
105:36 - this data frame here but a bit inverted
105:39 - okay
105:40 - the first round will be at the bottom
105:42 - second round will be the second from the
105:43 - bottom
105:44 - etc
105:48 - all right
105:50 - one one seven
105:52 - line number one one seven price data
105:54 - okay so this is the data frame right
105:55 - here
105:58 - data dimension so it's right here it's
106:00 - going to print out
106:02 - that that there are 100 rows and eight
106:04 - columns here eight columns
106:09 - column two data frame the if coins so
106:12 - this is
106:13 - df coins is the data frame here
106:17 - line number one two two
106:19 - one two eight it's going to be
106:22 - allowing us to download the data here as
106:25 - a csv file
106:31 - and then lines number
106:35 - 133
106:36 - preparing the data for the bar plot so
106:38 - here are here
106:40 - in column three we're going to make the
106:42 - bar plot
106:45 - so the following lines of code or the
106:47 - blocks of code here will be preparing
106:49 - the data for making the bar plot
106:54 - so what it will essentially do let's
106:56 - have a look
107:00 - so here we're going to select some of
107:03 - the columns
107:05 - so the coin symbol
107:07 - and then we're gonna select the price
107:08 - change the percent change of one hour 24
107:12 - hour
107:12 - and seven days and we're gonna create a
107:15 - new data frame called df change
107:18 - all right and then we're gonna set the
107:20 - index to coin symbol
107:22 - so coin symbol will be moved to the
107:24 - index of that data frame
107:26 - and then we're going to create some
107:28 - additional
107:30 - new columns here we're going to call it
107:32 - positive
107:33 - in front
107:34 - of the one hour
107:36 - positive in front of the 24 hour and
107:39 - positive of these seven days
107:42 - and we're gonna do a conditional here
107:46 - so if the
107:48 - percent change that happens in the last
107:50 - hour if it is positive
107:53 - meaning it has a value greater than zero
107:56 - we're going to assign it to the
107:59 - positive column
108:01 - for the given time frame of one hour and
108:04 - so for the time frame of 24 hour if the
108:06 - value is greater than zero
108:09 - it will be assigned a value of one
108:12 - for the positive
108:14 - percent change 24 hour and same thing
108:16 - for the seven days period
108:18 - if it has a value of greater than zero
108:21 - it will be assigned a value of one
108:24 - in the new column called positive
108:25 - percent change
108:28 - and if it has a value of one in the
108:31 - positive percent change here it means
108:34 - that it will have a green color
108:37 - however if it has a value of
108:40 - less than zero it will have a red color
108:44 - okay
108:45 - so that will be shown in the following
108:49 - lines 144 until 168.
108:58 - okay so here you're going to see that
109:01 - for the creation of the bar plot we're
109:03 - going to make use of the conditional
109:06 - so we're going to have three blocks here
109:08 - if else if
109:10 - and
109:11 - else
109:13 - so the first
109:14 - will check whether it is
109:16 - seven days
109:18 - and the second block will check whether
109:19 - it is 24 hours
109:21 - and the third one will be
109:23 - one hour
109:25 - so right here
109:26 - did we select seven days
109:28 - or is it 24 hour or is it one hour
109:34 - and the code is right here let me find
109:36 - it for you
109:39 - time period
109:44 - right here percent time frame
109:47 - select box okay and the option is 7 days
109:50 - 24 hour and 1 hour
109:53 - okay percent time frame which is right
109:56 - here
109:57 - percent time frame
109:59 - okay so it's doing a conditional check
110:01 - if it's seven days it's going to create
110:03 - the seven days plot
110:05 - if 24 hour is selected it will create 24
110:08 - hour version so
110:11 - here you see that it says seven days
110:12 - here and this is the plot for the seven
110:14 - days
110:16 - and if i choose the 24 hour
110:19 - then it will go to this second block
110:20 - here
110:21 - and it will create the 24 hour as you
110:24 - see here bar plot
110:26 - and if i selected the one hour
110:30 - and it will go to the third block here
110:33 - and here you notice that it creates the
110:35 - one hour bark plot
110:38 - okay
110:40 - and there you have it a cryptocurrency
110:43 - price web application
110:51 - okay so a couple of days ago i released
110:53 - a video on how you can develop a simple
110:55 - web application in python using the
110:58 - streamlit library and so in this video
111:00 - we're going to incorporate an additional
111:02 - feature which is machine learning
111:04 - capability into the web application and
111:06 - so without further ado let's get started
111:08 - so the web application that we're going
111:10 - to build today is called the simple iris
111:13 - flower prediction app and so the app
111:15 - will do as it says it will predict the
111:17 - iris flower type given the four input
111:19 - parameters okay so let's have a look
111:22 - what does the web application looks like
111:24 - so the first thing that you want to do
111:25 - is head over to your command prompt so
111:28 - if you're on a windows you want to click
111:30 - on the search icon type in cmd
111:32 - if you have a mac or a linux you want to
111:35 - open up your terminal and because i have
111:37 - everything in my environment i will
111:39 - activate it by condactivate dp
111:44 - then move to the desktop and then i'm
111:46 - going to run the web server by typing in
111:48 - streamlit
111:49 - run
111:50 - and then the name of the file enter
111:54 - and so the file to this web application
111:57 - will be included in the video
111:59 - description so you want to check that
112:00 - out in order to follow along with this
112:02 - tutorial so notice here that it's going
112:04 - to spawn up a website for you on your
112:06 - local computer and so the url here is
112:09 - localhost colon and then followed by the
112:11 - port number which is 8501 on my computer
112:14 - okay so this is the web application and
112:17 - the first line here is the header of the
112:20 - web app
112:21 - let's have a look what does this looks
112:23 - like
112:24 - let me expand it a bit okay so as you
112:26 - can see here the web application is
112:28 - comprised of two components the first
112:31 - component is the main panel here shown
112:33 - to the right and the second component is
112:36 - shown to the left which is the sidebar
112:38 - panel and so the sidebar panel here as
112:41 - it is called here user input parameters
112:43 - it will accept the input parameters
112:45 - comprising of the four features that we
112:48 - will be using to make the prediction and
112:51 - under the hood the prediction will be
112:52 - made using the random forest classifier
112:55 - and to the right here upon selecting
112:57 - your user input values for the four
113:00 - parameters here and so you will see that
113:02 - the default values are provided in the
113:05 - slider bar here so we can see that
113:08 - simple link has a default of 5.4 and if
113:11 - we modify the number here to the right
113:14 - in the table we're gonna see that number
113:16 - also modifies and updates as well same
113:18 - thing if i modify the other variables
113:20 - you will see that the numbers inside the
113:22 - table here will be updated and
113:24 - correspondingly the prediction
113:26 - probability and the prediction will also
113:29 - be updated as well and so the model will
113:31 - be applied to make the prediction based
113:33 - on your input parameters so if you
113:36 - change the input parameters the model
113:37 - will be used to make the prediction once
113:40 - again
113:41 - as shown here if i modify it then the
113:43 - predictions will be created okay pretty
113:46 - neat and so let's have a look at the
113:47 - corresponding code which makes all of
113:49 - this possible so let's head over back to
113:52 - the code and so let me give you a
113:53 - line-by-line breakdown of the
113:55 - functionality of the code here okay so i
113:57 - noticed that we didn't use numpy so
113:59 - we're gonna delete that
114:01 - okay so now it's well under 50 lines of
114:04 - code
114:05 - 49 now
114:07 - all right now it's 49
114:09 - so in the first four line we're going to
114:11 - import the library that we're going to
114:12 - use in this app and lines number six
114:14 - through 10 will be a simple markdown
114:17 - text telling us the name of the flower
114:19 - so let me open it side by side with the
114:22 - web application
114:24 - so the header here is right here
114:28 - okay and the text here this app predicts
114:30 - the ice flower type is right here
114:33 - and so i can make it bold
114:37 - save it
114:38 - and it doesn't change so i have to say
114:40 - always rerun
114:42 - so it will be updated automatically all
114:44 - right so you see that it's bold now
114:47 - and so st dot sidebar dot header on line
114:51 - number 12 will be the name of the header
114:54 - of the sidebar panel right here user
114:56 - input parameters and so notice that if i
114:58 - take out the sidebar
115:01 - it will move to the right to the main
115:03 - panel okay so having it in the sidebar
115:06 - panel will be possible if we use the dot
115:09 - sidebar
115:10 - okay so let's save it and it'll move
115:12 - back to the sidebar
115:14 - okay
115:15 - and lines number 14 through 24 will be a
115:19 - custom function used to accept all of
115:22 - the four input parameters from the
115:24 - sidebar and it will create a pandas data
115:27 - frame and the input parameters will be
115:30 - obtained from this sidebar as shown
115:32 - right here to the left hand side and the
115:35 - text here will represent the name shown
115:38 - here so actually i could modify the name
115:41 - as such
115:43 - i can make it capital and then add a
115:45 - space and save it
115:47 - and notice that the name also updates so
115:49 - why don't i do that for the other one as
115:51 - well
115:55 - otherwise it might be confusing whether
115:58 - it is a variable name or not so this
116:00 - could be any text that you want
116:02 - all right and so the name is updated
116:04 - here
116:05 - and so the first value here represents
116:07 - the minimum value and it is 4.3 and the
116:10 - second number here represents the
116:11 - maximum value which is 7.9 and the third
116:14 - value represents the current selected
116:16 - value or the default value which is 5.4
116:19 - so here 4.3 7.9 and 5.4
116:24 - and so if you want to change the default
116:26 - value to something else like 5.8
116:29 - and then it would be changed to 5.8 okay
116:32 - let me change it back to 5.4
116:35 - all right so
116:37 - here we're going to use the custom
116:39 - function that we built above user input
116:41 - features and then we're going to assign
116:43 - it into the df variable and this will be
116:45 - on line number 26
116:49 - okay and lines number 28 and 29 will be
116:52 - right here user input parameters
116:55 - so you can see that in just two lines of
116:57 - code we could have the section header
116:59 - name and the corresponding table below
117:01 - so it's just a simple print out of the
117:03 - data frame
117:05 - and lines number 31 will be essentially
117:07 - just loading in the irs data set line
117:10 - number 32 will assign the iris dot data
117:13 - into the x variable and the iris dot
117:15 - data are essentially the four features
117:18 - comprising of the simple length simple
117:20 - width pedal length pedal width and this
117:21 - will be used as the x variable and it
117:24 - will later be used as the input argument
117:26 - and the y variable here will be using
117:28 - the iris.target and so they are the
117:30 - class index number of 0 1 2. and line
117:33 - number 35 we will be creating a
117:36 - classifier variable comprising of the
117:38 - random forest classifier and line number
117:40 - 36 we're going to apply the classifier
117:43 - to build a training model using as input
117:46 - argument the x and the y data matrices
117:50 - okay and in line number 38 we'll make
117:52 - the prediction 39 will give you the
117:54 - prediction probability lines number 41
117:57 - and 42 will be just a simple print out
118:00 - of the class label and their
118:01 - corresponding index number
118:04 - line number 44 45 will give you the
118:07 - prediction
118:08 - and so the prediction here is the class
118:11 - label of being either setosa versus
118:14 - color or virginica
118:16 - line number 48 and 49 will give you the
118:18 - prediction probability and so the
118:21 - prediction probability will tell you
118:23 - what is the probability of being in one
118:25 - of the three classes so here it is
118:27 - predicted to be iris setosa so what's
118:29 - the probability of it being a setosa so
118:31 - the probability is a hundred percent so
118:34 - if we change the input parameters a bit
118:37 - and we're gonna see that now it is still
118:39 - predicted to be setosa but the
118:41 - probability of being setosa is slightly
118:43 - reduced to ninety percent and there is a
118:45 - seven percent chance of it being a iris
118:48 - diversity color and a three percent
118:50 - chance of it being a iris virginica okay
118:52 - and there you have it a simple iris
118:54 - flower prediction app
119:02 - so in the previous video i have shown
119:04 - you how you can use an alternative to
119:06 - the iris data set called the palmer
119:08 - penguins data set and so in this video
119:11 - i'm going to show you how you could
119:12 - develop a web application for the palmer
119:15 - penguins dataset and so without further
119:17 - ado we're starting right now so as
119:19 - briefly mentioned we're going to use
119:21 - this palmer penguins dataset that is
119:23 - provided by this r library called palmer
119:26 - penguins
119:27 - so for your convenience i'm going to
119:28 - provide that data set on the github of
119:31 - the data professor and so make note that
119:33 - this data set that we're going to use
119:35 - today was derived from this github
119:37 - library and so i'm going to show you
119:39 - where you could have access to the data
119:41 - that i have already exported out from
119:43 - the palmer penguins library package
119:46 - so it's right here in the data professor
119:49 - data repository
119:51 - find penguins cleaned so i have already
119:55 - cleaned the data set so you could also
119:57 - make use of this okay so we're gonna use
120:00 - this in this tutorial okay so let's head
120:02 - to the
120:03 - working directory
120:05 - so i'm gonna provide you the links to
120:07 - all of these code files that we're gonna
120:10 - use in this tutorial okay so before we
120:12 - begin let's have a quick recap and so in
120:14 - the first part of this tutorial series
120:16 - on streamlid i have shown you how you
120:19 - could use data directly from the
120:20 - wi-finance library and how we could
120:22 - display a simple line chart in part two
120:25 - i have shown you how you could build a
120:26 - simple classification prediction web app
120:30 - for the iris data set and in this part
120:32 - three we're going to use the palmer
120:33 - penguins data set in order to make a
120:35 - classification web application so let's
120:38 - have a look at this data set
120:41 - okay so it has already been cleaned and
120:44 - here there are a total of
120:47 - so here there are a total of seven
120:48 - columns so we have species island bill
120:51 - length build up flipper link body mass
120:54 - and sex and let's scroll down
120:57 - and so there are a total of three three
121:00 - four rolls so not including the first
121:03 - row which is the heading there are a
121:04 - total of three three three and so we
121:06 - could see that we have already deleted
121:08 - some of the missing values
121:11 - and so it should be noted that the
121:12 - missing values were deleted totally so
121:14 - that is the simplest approach that i'm
121:16 - using and you could feel free to do some
121:19 - imputation of the data set in order to
121:21 - retain more of the data so the missing
121:24 - value that were deleted could be less
121:26 - than 10. and so please feel free to
121:28 - provide the link to your github page
121:30 - where you have applied some unique
121:32 - imputation approach and perhaps i could
121:34 - also include it in the github of the
121:35 - data professor as well so all of you
121:37 - guys all of us can have access to your
121:41 - imputed and cleaned data set and so in
121:43 - the meantime we're going to use this
121:44 - data set that i have already cleaned and
121:46 - it is called penguins underscore
121:48 - cleaned.csv and so in part two where we
121:51 - built a simple iris classification web
121:54 - application you might notice that the
121:55 - code is building the prediction model
121:59 - every time that we load the file in and
122:02 - every time that we make adjustment to
122:04 - the input features it's going to rebuild
122:06 - the model over and over and so as some
122:08 - of you have pointed out this particular
122:10 - flaw of the code i totally agree with
122:12 - you and so the previous version was
122:14 - built like that for the simplicity of
122:16 - the tutorial and so in this tutorial
122:18 - we're going to use another approach
122:20 - where we could beforehand build a
122:22 - prediction model pick all the object
122:24 - which is to save it into a file and then
122:27 - within the streamlit code we're going to
122:28 - read in the saved file and so the
122:31 - advantage of that is that there is no
122:33 - need to rebuild the model every time
122:34 - that the input parameters are changed
122:36 - and so let's have a look at that
122:39 - okay so the code that we're using is
122:41 - called penguinsmodelbuilding.py
122:44 - and let's take a look at the code
122:47 - let's edit with adam
122:50 - all right so here so we're going to use
122:51 - pandas spd and then we're going to read
122:54 - in the csv of the
122:56 - penguinsclean.csv here which is also
122:58 - provided in the same directory
123:00 - as you can see here
123:02 - and then we're going to take this
123:04 - penguin's data frame take the data and
123:06 - put it into the df variable and then
123:08 - we're going to define the target and the
123:10 - encode variable according to the
123:12 - excellent kernel kaggle provided in this
123:15 - link from pratik and so kudos to pratik
123:18 - for the code that we're using as the
123:20 - basis of this tutorial and so here we're
123:22 - going to use ordinal feature encoding in
123:24 - order to encode the qualitative features
123:28 - such as species island and sex and so
123:31 - the objective of this tutorial we're
123:33 - going to predict the species of the
123:36 - penguin and so if you would like to
123:37 - predict the sex of the penguin you could
123:39 - replace species with sex and then you
123:41 - could put species in here
123:44 - okay so actually this was the exact
123:47 - parameters used by protech in his kaggle
123:50 - kernel but in this tutorial i have
123:51 - modified it a bit by using the species
123:54 - as the target where we're going to
123:56 - predict the species of the penguin and
123:58 - the sex and island will be used as the
124:00 - input parameters okay so this block of
124:03 - code here will be encoding the sex and
124:06 - island columns and in this block of code
124:08 - here it's going to encode the target
124:10 - species and in this line of code here
124:12 - we're going to apply this custom
124:14 - function in order to perform the
124:16 - encoding and so in this two lines of
124:18 - code we're going to separate the data
124:20 - set into x and y data matrices in order
124:23 - to use it for model building and
124:24 - scikit-learn
124:26 - right because here x will be the input
124:28 - features and y will be the species so in
124:31 - x we have six features and in y we have
124:34 - one feature and so here we're going to
124:36 - build a random forest model and from
124:39 - sklearn.ensemble we're going to import
124:41 - random forest classifier and we're going
124:44 - to assign the random force classifier to
124:46 - the clf variable and then we're going to
124:48 - use the fit function in order to build
124:50 - the model using x and y as the import
124:52 - argument and then finally here we're
124:54 - going to save the model using the pico
124:56 - library and we're going to use the
124:58 - pickle.dump function and as input
125:00 - argument we're going to use the clf
125:03 - which is the model that we have already
125:04 - built and then we're going to open or
125:07 - we're going to create a file called
125:08 - penguins underscore clf.pkl
125:14 - okay so let's close the file and run
125:17 - this code so we could do this right
125:19 - inside the command line
125:22 - so i'm opening up the command prompt
125:24 - heading over to desktop
125:27 - going into the streamlet folder
125:29 - going into the penguins folder
125:32 - and then i'm going to activate my
125:34 - environment
125:35 - and then we're going to run the code
125:38 - in model building
125:42 - python
125:46 - so i have to make sure to type in
125:48 - penguins dash and then the tab function
125:51 - okay there you have it all right so as
125:53 - you can see the file popped up here and
125:56 - the pickled file has been created
125:58 - successfully
125:59 - all right so we're going to copy this to
126:02 - the previous folder
126:04 - in here
126:06 - okay so let's have a look at the
126:09 - penguins app here
126:12 - okay so the first five lines we're going
126:14 - to import the necessary libraries so
126:17 - here we're going to import srimlet sst
126:20 - import pandas spd import numpy snp
126:24 - import pickle and from sklearn.ensemble
126:27 - import random forest classifier okay and
126:30 - in this block of code here is the title
126:33 - in markdown format and the corresponding
126:35 - description of this web application so
126:37 - why don't we have a look side by side
126:44 - let's resize the window a bit
126:50 - cd desktop cd streamlet
126:53 - conda activate
126:56 - dp
126:58 - penguins
127:02 - okay streamlit
127:04 - run
127:06 - penguins
127:07 - dash app
127:09 - in order to run the application
127:13 - so
127:14 - it's popping up the window here
127:16 - okay so this is the finished application
127:19 - that we're gonna build today and as you
127:21 - can see if we change the input
127:23 - parameters the prediction here will be
127:25 - changed automatically
127:28 - so we could see
127:30 - right
127:31 - and we're also going to get the
127:33 - corresponding prediction probability as
127:35 - well
127:37 - and so it should be noted here that this
127:40 - web application was much more difficult
127:42 - than using the iris data set partly
127:44 - because of the issue with the two
127:46 - qualitative features that we're using so
127:49 - the thing is with iris data set if we're
127:51 - using that it's going to contain only
127:53 - the quantitative features so there won't
127:55 - be any ordinal features like sex or
127:57 - island and so under the hood we have to
127:59 - encode the ordinal features and for
128:02 - example for the island feature here
128:04 - we're going to create three additional
128:06 - columns called island bisco
128:09 - island dream island torgersen and for
128:12 - each of this three feature we're going
128:14 - to have binary value one or zero it has
128:17 - the island being bisco if the island
128:19 - bisco is having a value of one and so if
128:22 - the island basically has a value of one
128:24 - for a particular penguin then it will
128:26 - also have corresponding value of zero
128:28 - for dream and corresponding value of
128:30 - zero for torgerson and so the same thing
128:32 - for sex we're going to create two
128:34 - additional features sex male sex female
128:37 - and for input feature here if the sex is
128:40 - male then the sex male feature will have
128:42 - a value of one and the sex female will
128:44 - have a value of zero so therefore we
128:46 - will create five additional features on
128:48 - top of the four features that we have
128:50 - that are quantitative so that will bring
128:52 - us to a total of nine features so that
128:55 - was a bit complicated but we have
128:56 - already solved the issue for you guys
128:58 - and the code is inside here so let's
129:00 - have a look
129:01 - so here we're using st.sidebar
129:04 - dot header user input feature and so
129:06 - that is the name of this sidebar heading
129:09 - and then we have st.sidebar markdown and
129:12 - so we're going to provide example of the
129:15 - csv file and the link to the csv file
129:19 - that is an example is called penguins
129:21 - example and so you can see that we're
129:23 - providing the link to this csv file in
129:26 - markdown format
129:29 - and so you could click on it and it will
129:30 - bring you to this data set so in order
129:33 - to download this you would have to right
129:34 - click and then save link as in order to
129:37 - do that so the reason for having the
129:39 - example csv input file here is because i
129:42 - have been receiving some comments
129:43 - whether i could include a feature where
129:46 - the user could upload their input file
129:48 - and so in this tutorial i'm gonna show
129:50 - you that okay so here we're gonna
129:52 - download the csv file
129:54 - and notice that it's providing
129:56 - the extension.txt and we have to make it
129:59 - csv
130:00 - save it
130:02 - all right so this is the input features
130:04 - that we're going to use
130:07 - and we're going to upload the file
130:10 - and there you go the uploaded file are
130:12 - used as the input features here and a
130:14 - prediction is being made here okay so it
130:16 - predicts this input as a delhi and with
130:19 - corresponding prediction probability
130:21 - okay so as you can see there are two
130:23 - possibilities for the input features so
130:26 - the first option is to import the file
130:28 - as csv format and the second possibility
130:31 - is to directly input the parameters by
130:34 - the slider bar
130:38 - and so under the hood the code will be
130:40 - using one or the other as the input for
130:42 - the predictions to be made and so for
130:44 - doing that we're going to use the if
130:46 - else conditional
130:48 - so let's have a look further
130:50 - so in line number 22 here it is using
130:53 - the st.sidebar
130:55 - file uploader and so it will give us
130:58 - this functionality to allow us to upload
131:00 - files and so the uploaded file will go
131:03 - into the variable called uploaded file
131:05 - and so we're going to make use of
131:07 - conditionals here and so there will be
131:09 - two scenarios we're gonna use if there
131:11 - is an uploaded file and the uploaded
131:14 - file is not empty not none meaning that
131:17 - if the uploaded file is not empty then
131:20 - we should create a input df variable and
131:23 - then we're going to read in the uploaded
131:25 - file or in else then we're going to run
131:28 - this
131:29 - block of code here and so the block of
131:31 - code here will essentially define a
131:32 - function that will accept the input
131:35 - parameters from the slide bar here okay
131:38 - so as you can see the conditional have
131:40 - two possibilities if there is an
131:42 - uploaded file create a data frame and
131:44 - read in the uploaded file else read in
131:47 - the input parameters directly from the
131:49 - slider bar
131:51 - okay and so notice that the contents of
131:53 - the input will be saved into the same
131:56 - input df variable so that will be easy
131:59 - for the following blocks of code
132:02 - so previously i've shown you now how you
132:05 - can import the library write the header
132:07 - of the web app shown here and along with
132:09 - the description and then
132:12 - the header of the sidebar along with the
132:14 - link to the example csv file here and
132:17 - then the upload functionality shown here
132:20 - and then in this block of conditional if
132:23 - else
132:24 - regarding the input features if there is
132:26 - a file to be uploaded or if not then
132:29 - we're going to use the slider bar as the
132:30 - input okay so now the fun part comes in
132:34 - reading in the data from the penguins
132:37 - underscore clean dot csv and then we're
132:40 - gonna drop the species column because
132:43 - we're going to predict that column and
132:45 - therefore we're gonna drop it here and
132:46 - now we're going to combine the input
132:49 - underscore df to the entire data set of
132:51 - penguins and so the encoding code that
132:54 - we're using it is expecting that there
132:56 - are multiple values inside the
132:58 - particular column that it wants to
133:00 - encode so for example in the island
133:03 - variable it is expecting that there are
133:05 - three possibilities right the three
133:07 - different island or in the sex column it
133:10 - is expecting that there are two male
133:11 - female okay and so the thing is the
133:14 - input feature that we're using here will
133:15 - be from one penguin sample and so let's
133:18 - say that one penguin sample could have
133:20 - island as biscuo and therefore it will
133:22 - only know of one possibility and so this
133:25 - block of code will not work so therefore
133:27 - we have to integrate this input features
133:29 - on top of the existing penguin dataset
133:32 - so before we might have 333 rows then
133:35 - it's three three three plus one so that
133:38 - makes it three three four and then we're
133:40 - going to perform the encoding and then
133:42 - there will be two possibility for sex
133:44 - three possibility for island
133:46 - okay
133:48 - all right so now we're going to display
133:50 - the user input in the user input
133:52 - features so right here user input
133:54 - feature is this block of code and we're
133:57 - gonna use conditional again and so the
133:59 - first possibility is if there is an
134:02 - uploaded file write out the content
134:04 - otherwise write out the content of the
134:07 - slider bar and then also put a text that
134:10 - we are awaiting the csv file to be
134:13 - uploaded so in this scenario it's
134:15 - telling us that there is no input file
134:18 - uploaded and if we upload the input file
134:20 - then this line of code will be
134:21 - disappearing okay so let's proceed
134:23 - further and so this is the
134:25 - classification model part so in the
134:27 - previous part two of the streamlight
134:29 - tutorial on iris dataset we've built the
134:31 - random forest model right inside this
134:33 - streamlit application but in this
134:35 - tutorial we're just reading in the saved
134:37 - file which i have shown you at the
134:39 - beginning of this video so the pickled
134:41 - object is called penguin clf dot pkl so
134:44 - we're going to read that in and we're
134:46 - going to assign it the load clf variable
134:49 - and then here on line 71 we're going to
134:51 - create a prediction variable and then
134:53 - we're going to assign the value of the
134:54 - predicted value and so we're using load
134:57 - underscore clf dot predict function and
135:00 - then input argument is df and df is
135:03 - corresponding to the input features
135:05 - either from the uploaded file or from
135:07 - the sidebar okay
135:09 - and so that is df and then on line 72
135:12 - we're also going to use df as the input
135:14 - argument to the predict prabha function
135:17 - and that will provide you the prediction
135:19 - probability okay and so in the block of
135:22 - code of line 75 through 77 it's going to
135:25 - write out the predicted value of the
135:27 - penguin and so here it is predicted to
135:29 - be gen 2 and then on line 79 and 80 it's
135:32 - going to predict the probability okay
135:34 - and so the probability values are shown
135:36 - in this data frame here and so here we
135:38 - can see that it is predicted to be gen 2
135:41 - and there is a prediction probability of
135:43 - 81
135:44 - so this can be taken as the relative
135:46 - confidence that we have in this
135:48 - prediction so it's kind of like we're 81
135:50 - confident that this prediction is
135:52 - correct however for other prediction the
135:54 - probability could be decreased to 67. so
135:57 - as you can see when we reduce the body
135:59 - mass the probability decreased but if we
136:02 - increase the body mass the probability
136:05 - increased
136:12 - three two one okay so welcome back to
136:15 - the data professor youtube channel my
136:18 - name is
136:20 - and i'm an associate professor of
136:22 - bioinformatics and in this video i'm
136:24 - going to talk about how you can develop
136:26 - a web application powered by machine
136:28 - learning for the boston housing data set
136:31 - so without further ado we're starting
136:34 - right now okay so today we're going to
136:35 - create a simple web application that is
136:38 - powered by machine learning and we're
136:40 - going to use the boston housing data set
136:42 - as our example so in your data science
136:45 - learning journey you probably have came
136:47 - across the boston housing data set so
136:51 - let's take a quick look
136:52 - so the boston housing data set is
136:54 - comprised of 506
136:57 - rolls 14 columns
137:00 - so the 14 columns are provided here
137:03 - so these are the descriptions of the 14
137:06 - columns and the target response that
137:08 - we're going to predict is called the
137:11 - median value of owner occupied homes in
137:15 - one thousand dollar units so if you
137:17 - would like some more details
137:19 - read these
137:21 - resources and
137:22 - so let's dive into the code
137:26 - okay so let me open up my adam
137:29 - text editor
137:31 - all right so let's have a look at the
137:33 - code
137:35 - so
137:36 - the code is
137:37 - 86 lines
137:40 - so i provided you some bonuses here so
137:42 - aside from making the prediction we're
137:44 - also going to provide you the
137:47 - understanding behind the prediction okay
137:50 - so these will be in about 10 lines of
137:53 - code and it's using the shaft library so
137:57 - actually the entire code for the web
137:59 - application for predicting the house
138:02 - price
138:03 - is
138:04 - 71 lines of code so actually we could
138:06 - make it fewer lines of code if we just
138:10 - deleted the extra spacing that i have
138:12 - made here so like for example if i just
138:14 - do it like this then i would save a lot
138:17 - of spaces
138:18 - but i'm going to use excessive lines in
138:22 - order to make the code look as simple as
138:24 - possible okay so let's get started
138:28 - so the first six lines of code as we see
138:30 - here are for importing the various
138:33 - libraries so the first line is we're
138:35 - gonna import streamlit as st line number
138:38 - two we're going to import pandas as pd
138:41 - so line number three we're going to
138:43 - import the shaft library and the shape
138:46 - library will provide you the
138:47 - understanding behind the prediction
138:49 - number four we're going to import
138:51 - matplotlib.pyplot
138:53 - as plt line number five we're going to
138:56 - import the dataset from sklearn in line
138:59 - number six we're going to import the
139:01 - random forest regressor from
139:03 - sklearn.ensemble
139:06 - okay so
139:07 - let me run this web application so that
139:10 - you could see it side by side with the
139:13 - code
139:16 - so let me click on the search and then
139:18 - type in cmd
139:20 - i'm going to go into my
139:23 - environment
139:24 - for running data science
139:27 - so i'm going to activate my environment
139:30 - called dp
139:31 - so for those of you you don't have dp as
139:34 - your environment so you can create your
139:36 - own environment and then when you have
139:38 - already created that you will activate
139:40 - it using conda activate dp okay so i
139:44 - might probably make a future video about
139:45 - how you can create environments in conda
139:48 - so maybe a brief teaser so the reason
139:51 - for using environment is that we don't
139:53 - want to mess up our computer so for
139:55 - example if we installed libraries that
139:58 - might influence the version number of
140:00 - other dependency library so that might
140:03 - ruin your other data science project so
140:05 - for example if you're using pandas 1.0
140:08 - in one project and then in the other
140:10 - project you require panda 0.93 and so
140:14 - there will be incompatibility issues
140:16 - right so it's working for one version
140:18 - but then when you install another
140:20 - library which is depending on 0.93 then
140:23 - it's not going to work when you have
140:25 - pandas 1.0 so in order to counter that
140:28 - it would be best to run a different data
140:30 - science project in a different
140:32 - environment okay so different meaning
140:34 - that not every single project that
140:36 - you're running you have to create a new
140:38 - environment but you're going to create a
140:41 - new environment if you're using
140:42 - different libraries okay so like for
140:45 - example here in this tutorial i'm using
140:47 - predominantly sk learn and streamline so
140:50 - whenever i do tutorials about sklearn
140:52 - and streamlit i would use this
140:54 - particular environment
140:56 - okay so let's continue so i have already
140:59 - activated the environment and
141:01 - i'm going to move into the desktop here
141:04 - and then i'm going to run the code
141:06 - streamlit run and then the code name is
141:09 - boston house
141:12 - all right so i just type in the first
141:14 - field character hit on the tab and then
141:16 - it's going to auto fail for you
141:18 - hit on enter
141:20 - and then we're going to load up the
141:22 - local version
141:26 - all right so this is the web application
141:29 - that we're developing
141:31 - okay so i'm not sure why feature
141:33 - important is not shown here
141:38 - try loading it again
141:43 - it's running
141:44 - all right feature importance is loaded
141:47 - so this is the web app
141:49 - let's have it side by side all right
141:52 - here so lines number eight through 13
141:55 - right here is going to be corresponding
141:57 - to the title of the webpage which is
141:59 - right here so line number
142:01 - nine we're using the hashtag which is a
142:04 - equivalence to the h1 heading in html
142:09 - and it is called boston house price
142:11 - prediction app
142:13 - and then in the normal text we're going
142:15 - to type in this app predicts the boston
142:18 - house price and then in markdown
142:20 - language we're using two asterisk which
142:22 - is equivalent to bolding the text okay
142:27 - if i make it only one
142:29 - asterisk it will be
142:32 - in italic
142:37 - see it'll be in italy
142:39 - okay so i'm just undoing it that
142:43 - all right and then in lines number 16
142:47 - we're going to import the boston house
142:50 - data set and then we're going to split
142:52 - that into x and y data frames so the x
142:56 - data frame will contain only the
142:58 - independent variables or the x variables
143:01 - so sometimes you call these as the input
143:04 - features and then in the y variable here
143:06 - we're going to have the target and the
143:08 - target here is the median value of the
143:10 - house price
143:13 - all right and on line number 22
143:16 - until
143:19 - line number 52
143:21 - it is the side panel
143:23 - right here
143:25 - so line number 22 it is the header of
143:27 - the side panel specify input parameters
143:31 - so lines number 24 until
143:33 - 52
143:34 - we're defining a custom function for
143:37 - accepting user input features
143:40 - so here we see that we have the 13
143:43 - features and then the 13 features will
143:45 - accept input here so we're going to make
143:46 - use of the st.sidebar dot slider and
143:50 - then as the input argument we're going
143:52 - to have the name of the feature crim and
143:55 - then the next is the minimum value
143:57 - the maximum value
143:59 - and the mean value so why do we need to
144:01 - add these values here minimum maximum
144:04 - and the mean so the minimum value here
144:07 - is the value that you're going to see
144:08 - here at the lower limit of the slider
144:11 - and then the max value will be at the
144:13 - upper limit of the slider and the mean
144:15 - value will be the default value that
144:18 - we're going to put into the input
144:20 - parameters
144:21 - okay
144:22 - so the mean value of krim is 3.61 right
144:24 - but once we change this then the
144:26 - prediction will be modified so we're
144:29 - going to use the same logic for the
144:31 - remaining
144:32 - input features and then finally we're
144:34 - going to put all of the features here
144:36 - into a data frame and the data frame
144:38 - will be returned
144:40 - here and then we're going to make use of
144:43 - the custom function and then we're going
144:45 - to call it df we're going to assign the
144:48 - default input features here
144:50 - into the df data frame
144:56 - and then in lines number 59
144:58 - it's going to be corresponding to here
145:00 - specified input parameters
145:03 - line number 60 will be printing out the
145:05 - df
145:07 - line number 61 will be printing out this
145:10 - long horizontal line as a divider
145:14 - and line number 64 until 67 we're going
145:17 - to build a random forest
145:19 - model
145:21 - we're going to train the model on line
145:22 - number 65. and then we're going to make
145:25 - the prediction on line number 67.
145:28 - so a point you note here is that a model
145:30 - will be built every time the input
145:32 - parameters are modified so in order to
145:35 - improve the code
145:37 - let me make it your homework
145:39 - try to create the model outside of
145:42 - streamlit web app file that you're
145:44 - seeing here
145:46 - make a pickle file
145:48 - save it as dot pkl
145:51 - and then you want to load that pico file
145:53 - into this web app so that you don't have
145:56 - to rebuild the model every time
145:58 - okay so you just built the model once
146:01 - you read it in to the web app the pkl
146:04 - file and then you will apply the pkl
146:06 - file which is the model to make the
146:08 - prediction
146:10 - using the input parameters that you
146:12 - specify in the sidebar
146:15 - okay
146:16 - so think of that as your homework and
146:18 - let me know in the description how that
146:20 - went
146:22 - all right
146:23 - so line number 69 you're going to see
146:26 - the header here prediction of the median
146:28 - value and then you're going to make the
146:30 - st right to print out the prediction
146:33 - value which is
146:35 - 20.084 and then finally in the remaining
146:38 - 10 lines of code we're going to print
146:41 - out the plots provided by the shop
146:43 - library so line number 75 and 76 it's
146:47 - going to extract the shaft values line
146:50 - number 78
146:52 - is going to print out the header here
146:53 - feature importance line number 79 is
146:56 - going to print the header of the plot i
146:59 - mean the title of the plot line number
147:01 - 80 is going to make a feature importance
147:04 - plot and then finally we're going to see
147:06 - the typical feature importance plot that
147:08 - we normally see when we're using random
147:11 - forest so here you're not going to see
147:13 - how it is contributing to the overall
147:16 - prediction for example you're going to
147:17 - see that l-stat is important rm is
147:20 - important but then you're not going to
147:21 - see important in what aspect are they
147:23 - contributing positively or negatively to
147:26 - the predicted values so in the shop plot
147:29 - here you're going to see that l-stat is
147:32 - contributing relatively equal on the
147:34 - negative aspect and the positive aspect
147:37 - but for the feature importance plot
147:39 - using the shaft value you're going to
147:41 - see the relative distribution on whether
147:43 - it contributed to the negative or the
147:45 - positive side of the feature importance
147:48 - okay so something handy and useful to
147:50 - see
147:57 - in this video i'm going to show you how
147:59 - you can build a bioinformatics web
148:01 - application and without further ado
148:03 - we're starting right now
148:05 - so in prior episodes i have shown you
148:08 - how you could use the streamlet library
148:10 - in python to build simple web
148:12 - application ranging from a simple
148:15 - financial web application where you
148:17 - could check the stock price a simple web
148:20 - application where you could predict the
148:22 - boston housing price a penguin species
148:25 - prediction web application and so in
148:27 - today's episode we're going to talk
148:29 - about how you could build a simple
148:32 - bioinformatics web application and it's
148:35 - going to be based on the prior tutorial
148:38 - videos that are mentioned in this
148:40 - channel so the bioinformatics web
148:42 - application that we're going to be
148:44 - building today will be an extension of a
148:47 - tutorial series where i have shown you
148:50 - how you could build a molecular
148:52 - solubility prediction model using
148:54 - machine learning where particularly we
148:56 - are applying machine learning and python
148:59 - to the field of computational drug
149:01 - discovery and if you think of it in the
149:03 - grand scheme of things it is part of the
149:05 - bioinformatics research area and so this
149:08 - video will focus more on the aspect of
149:11 - actually building the web application
149:13 - and if you're interested in how to build
149:15 - the prediction model on the molecular
149:17 - solubility that we will be using today
149:20 - let me refer you to the prior tutorial
149:22 - videos on this channel and the links
149:25 - will be provided in the video
149:27 - description and also the pen comments of
149:29 - this video okay and so let's get started
149:31 - shall we
149:32 - so we're going to go to the streamlet
149:35 - directory and then it's going to be
149:38 - located in the solubility folder and so
149:41 - all of these are the actual files that
149:45 - we're going to be using today and so
149:47 - actually i think i have the prediction
149:49 - model on
149:51 - google collab
149:54 - let me download that
150:00 - okay it's right here and so this will be
150:02 - a concise version of the contents of a
150:05 - prior video where i have shown you how
150:08 - you could build machine learning models
150:10 - of the molecular solubility data set and
150:12 - so as i mentioned already i will provide
150:14 - the links to this video and so let's
150:18 - connect to this and let's have a look
150:23 - so let me
150:25 - clear all of the outputs
150:28 - and then it's already connected and so
150:30 - we're going to import pandas as pd
150:34 - we're going to be
150:36 - downloading the
150:38 - calculated descriptors directly from the
150:41 - github of the data professor
150:43 - and so this has already been computed as
150:46 - mentioned in the prior video
150:49 - and so these are the computed
150:50 - descriptors moloch p mole weight number
150:54 - of rotatable bonds aromatic proportion
150:57 - so these four are the computed
150:59 - descriptor from the prior video that i
151:01 - have mentioned and the log s is the y
151:04 - variable that we will be computing or
151:07 - predicting so these four variables are
151:09 - the x variables okay and so we're going
151:12 - to
151:13 - separate this data frame into two sets
151:16 - of variables one is the x variable so
151:19 - we'll be dropping the log s column and
151:22 - then we're going to have the last column
151:25 - as the index number is indicating here
151:28 - minus one which is log s okay so the x
151:30 - variables will be containing all of the
151:33 - columns except for log s and the y
151:35 - variable will be containing only the log
151:38 - s okay
151:39 - and so we're going to be building the
151:41 - linear regression model here and so
151:43 - we're going to import the linear model
151:46 - from sklearn and we're going to also
151:48 - compute the performance metric by
151:51 - importing mean squared error and the r2
151:54 - score function from the sklearn.metric
151:58 - okay let's so let's run that and then
151:59 - we're going to run the linear regression
152:02 - and then we're going to assign this to
152:03 - model and then we're going to perform
152:05 - the model.fit using the x variables in
152:08 - order to predict the y and so we will be
152:12 - building a training model here
152:14 - x not defined okay so i haven't yet run
152:16 - this so let me do that first
152:19 - all right so let's do the model building
152:22 - okay so the model has already been built
152:25 - and then we're going to be performing
152:26 - the prediction all right prediction has
152:29 - been made and it is assigned to the y
152:31 - pred variable and the prediction is made
152:35 - using the model.predict and then using x
152:38 - as the input argument and then we're
152:40 - going to print the model performance
152:42 - here and these four values are the
152:45 - regression coefficient values for each
152:48 - of the four input variables of the x
152:52 - compressing of molar p molecular weights
152:54 - number of rotatable bonds aromatic
152:56 - proportion
152:58 - so the regression coefficient will
152:59 - represent the magnitude that each of
153:02 - these four variables are contributing to
153:04 - the prediction of the y
153:07 - so the greater the magnitude the greater
153:09 - the influence of that particular
153:11 - variable and so the y-intercept is right
153:13 - here the mean squared error 1.01 the
153:16 - r-squared value 6.77
153:19 - and then let's print out the model
153:21 - equation okay so this is the equation
153:24 - and then we're going to visualize the
153:26 - scatter plot of the actual versus the
153:29 - predicted
153:32 - okay so let's run the scatter plot here
153:37 - all right so this is the experimental
153:39 - versus the predicted log s value so as
153:43 - we have seen here from the r squared
153:45 - we're getting pretty good correlation
153:48 - between the experimental and the
153:50 - predicted lock s values
153:54 - and finally we're going to
153:56 - pickle the object here
153:59 - so essentially we're going to save the
154:00 - model into a file called
154:02 - solubilitymodel.pkl
154:05 - and so we're going to import this into
154:07 - the streamlit web application and so we
154:10 - have already run it and then we could
154:12 - simply click on this corner here and
154:15 - then click on the download file okay and
154:17 - it's essentially the same thing that we
154:19 - have right here but it should be noted
154:21 - that the version of scikit-learn on
154:24 - google collab and on the local version
154:27 - that i have installed will be different
154:30 - and it will be giving some warning
154:32 - values
154:33 - so
154:34 - it might be worthwhile to copy all of
154:36 - the code here into a file and then run
154:39 - it locally so that it's going to be
154:40 - using the same version of the
154:42 - scikit-learn okay so let's think of that
154:45 - as a homework for you guys
154:47 - and
154:48 - let's get started in building our web
154:50 - application now
154:52 - so let's head over to the streamlight
154:54 - folder solubility and let's have a look
154:56 - at the contents here so the delani
154:59 - solubility here so these are the input
155:01 - descriptors comprising of the x and y
155:04 - variables that we have been using just a
155:06 - moment ago to build the prediction model
155:08 - so i'm going to provide you these files
155:11 - in the github that is dedicated to this
155:13 - tutorial and also the jupiter notebook
155:16 - that we have seen just a moment ago for
155:18 - building the prediction model of this
155:20 - solubility data set and so that jupyter
155:23 - notebook will be using this input file
155:26 - here but also in the code it is
155:29 - downloading directly from the github of
155:31 - the data professor so actually we don't
155:34 - actually need this as well so i can just
155:36 - delete that
155:38 - and then i'm just gonna provide you the
155:40 - juvenile notebook and
155:43 - okay so a total of three files will be
155:46 - used for this web application so the
155:49 - first one is the logo
155:52 - so this is the logo that we will be
155:54 - using for the web application so i have
155:57 - drawn this in the ipad using the good
155:59 - notes application so here it is called
156:02 - molecular solubility prediction app so
156:05 - given input molecule the machine
156:07 - learning model will be predicting the
156:09 - lock s value
156:13 - okay so let's fire up the atom
156:17 - editor
156:18 - let's have a look at the contents of
156:20 - solubility app dot py
156:28 - okay so let's see so there's a total of
156:30 - about 110
156:32 - lines of code and so notice that i have
156:35 - also included several lines of code that
156:38 - are essentially the comments here and so
156:40 - these are just for ease of reading
156:44 - having a look at what each block of
156:46 - codes are doing all right so if deleting
156:49 - the comments it will be probably just
156:53 - under 100 lines of code
156:55 - so let's take a look at the code here
156:58 - okay so the first block of code that we
157:00 - will be using are essentially
157:03 - importing libraries that are needed here
157:05 - and so we'll be importing the numpy
157:08 - and so numpy will be used for the
157:10 - descriptor calculation we'll be using
157:12 - pandas because we need to read in the
157:15 - data set and actually we will be using
157:18 - it to prepare the data frames as well in
157:21 - the
157:22 - generate code here
157:23 - the function that we use for computing
157:26 - the molecular descriptors
157:28 - so full detail as i have mentioned will
157:30 - be provided in a prior video that i have
157:33 - shown you in a step-by-step manner and
157:36 - the hero of this tutorial is the
157:38 - streamlet library so this is the basis
157:41 - of this web application and we'll be
157:43 - importing pickle in order to allow us to
157:46 - save the machine learning model and then
157:49 - importing it in to this web application
157:52 - and then we're going to be using the
157:54 - image function from pil in order to
157:57 - display the logo that i have shown you
158:00 - just a moment ago and here we'll be
158:02 - using the chem and descriptors function
158:05 - of the rd kits it would be for computing
158:07 - the molecular descriptors and the
158:09 - molecular descriptors are essentially
158:12 - allowing us to describe molecules in
158:16 - terms of their physical chemical
158:18 - properties and so for ease of usage we
158:20 - have already created the custom function
158:23 - for calculating the molecular
158:25 - descriptors and they are provided here
158:27 - from lines here 12
158:30 - which are the comments until lines 57
158:33 - and so we will be having two custom
158:35 - function aromatic proportion and the
158:38 - generate function
158:40 - and let's take a look at the web
158:42 - application so let me run the web
158:43 - application right now
158:46 - conda activate dp you don't need to do
158:49 - this if you have already installed all
158:52 - of the libraries up for your python
158:54 - directly in the command line but if you
158:56 - use conda then you want to activate the
158:58 - environment that is dedicated for
159:01 - running your data science projects so i
159:05 - highly recommend installing conda and
159:07 - then creating a specialized environment
159:10 - for your data science projects so that
159:13 - will help us to maintain the library
159:16 - dependencies otherwise if you have
159:19 - several projects on your computer and
159:21 - then when you upgrade one library it
159:23 - might downgrade other libraries or other
159:26 - dependencies as a result and then it
159:29 - might make some of your prior data
159:31 - science project not workable
159:34 - so i highly recommend to use conda and
159:38 - specialized environment for managing
159:40 - your data science project okay so we
159:43 - have already activated dp here and we're
159:46 - going to go to the directory
159:50 - streamlit solubility
159:54 - okay so we have three files here and
159:57 - we're going to run it streamlit run
160:00 - solubility
160:02 - app dot py
160:06 - and now
160:07 - this is the web application
160:09 - okay so the left hand side here is the
160:12 - input parameters which is the smiles
160:15 - input will represent the chemical
160:18 - information of the input molecule so
160:21 - each line represents a different
160:23 - molecule so we have three lines here as
160:26 - an example and you could replace this
160:28 - with your own data and then we will be
160:30 - able to predict the value of the
160:33 - solubility
160:34 - as a function of the input smiles
160:37 - notation here so let me repeat this
160:39 - again this portion
160:41 - the smiles notation here each line
160:44 - represents a single molecule so you're
160:47 - seeing here three lines so it represents
160:49 - three molecules and each smile citation
160:53 - will tell us what is the atomic
160:56 - composition of the molecule and so here
160:58 - we see that the first line of code here
161:01 - cccc it has five carbon atoms and the
161:04 - second one has three carbon atoms and
161:06 - the third one is a carbon atom and a
161:08 - nitrogen atom okay and we can even
161:12 - search for smiles of a molecule of our
161:15 - interest and let's search for a molecule
161:18 - of our interest let's search for aspirin
161:23 - and let's click on it
161:27 - and let's find smiles i'm going to
161:30 - search for it command f and then
161:32 - search for smiles and we have it here
161:36 - in 2.1.4
161:38 - canonical smiles so we're going to copy
161:40 - that
161:41 - so this is the smiles notation and so
161:44 - i'm going to copy and then i'm going to
161:45 - paste it here
161:48 - and then after we paste it here we have
161:50 - to press command and enter in order to
161:53 - apply this
161:54 - and then note that the predicted value
161:57 - here will be updated so let's try it
161:59 - command enter because i'm on a mac so i
162:01 - have to do command enter and so on
162:04 - windows is probably ctrl enter and so
162:07 - here we have the input smiles notation
162:10 - which is right here the same thing on
162:11 - the left and then
162:13 - below here we have the computed
162:15 - molecular descriptors which are the four
162:18 - physical chemical properties here
162:20 - comprising of the moloch p molecular
162:22 - weight number of rotatable bonds
162:25 - aromatic proportion so these four
162:27 - variables are computed using the rd kids
162:30 - library let me go back here rd kit right
162:33 - here rd kit rd kit okay so using the
162:36 - chem and descriptors function and using
162:40 - the custom function that we have written
162:42 - to compute the aromatic proportion which
162:44 - is right here aromatic proportion and
162:47 - also the
162:48 - mole log p which is the first column
162:51 - here
162:52 - molecular weight which is the second
162:54 - column here
162:55 - number of rotatable bonds which is right
162:57 - here
162:58 - and
162:59 - the aromatic proportion
163:01 - is a custom function because rd kit does
163:04 - not compute that property and so we will
163:07 - have to create our own function in order
163:10 - to compute the aromatic proportion and
163:13 - so why did we use these four variables
163:16 - here how did we know that we have to
163:18 - compute these four descriptors it's
163:20 - because the original work by john dilani
163:23 - he used these four variables for
163:26 - building his prediction model so if you
163:28 - would like for more information please
163:31 - have a look at this original research
163:32 - paper and so here finally we have the
163:35 - predicted log s value and we have it
163:37 - predicted to be minus 2.0931
163:41 - so this is the relative solubility value
163:44 - okay
163:45 - so there we have it a simple web
163:48 - application for predicting the molecular
163:51 - solubility values so let's have a look
163:54 - further into the code
163:57 - so image this line of code here 63 line
164:00 - 63 is this image
164:03 - it is for displaying the image by
164:05 - essentially assigning it opening it and
164:07 - then assigning it to the image variable
164:09 - and then using the st.image in order to
164:12 - actually display the image and then use
164:15 - column width to be true in order to
164:17 - expand image to fit the column width and
164:19 - then here we're going to write out
164:21 - molecular solubility prediction app
164:23 - right here we can even modify this to be
164:25 - like web app
164:27 - save it and then
164:30 - rerun it and then you're gonna see that
164:32 - the name is updated to be web app
164:36 - and then these are the descriptors so
164:38 - this is in markdown format so markdown
164:42 - is allowing us to format text to have
164:46 - links italic or bold face as well for
164:50 - example if we want to make the text bold
164:52 - we're going to be using double asterisk
164:54 - before and after okay for highlighting
164:57 - solubility lock s we're going to add the
164:59 - double asterisk before and after if we
165:02 - want this to be bold and also in italic
165:04 - we need three of them
165:07 - click on always rerun
165:09 - so you see that it is in isolate okay
165:12 - and also bold
165:14 - and what if we have only one italic what
165:16 - happens here
165:18 - so a single asterisk will make it in a
165:21 - italic form and if we have two it
165:24 - becomes bold okay
165:28 - and if you want to add links to it then
165:30 - we have the bracket here to
165:33 - define the boundaries of the text that
165:36 - we want to be
165:38 - linked and then in parenthesis we're
165:40 - going to add the link the url here
165:44 - and so
165:45 - you can click on this and it will take
165:47 - us to the original paper
165:50 - okay and this is the original paper
165:54 - it was published in 2004.
165:59 - okay so
166:00 - this block of code here will be
166:03 - reading in the input features which is
166:05 - the smiles notation
166:07 - so sc.sidebar header will be the header
166:10 - here the heading user input features and
166:13 - then read smiles input so this is the
166:16 - example input that we will be using and
166:19 - notice here that we use 5c
166:23 - let's rerun this
166:25 - so five c is right here and then notice
166:27 - that when we have a backslash n it
166:29 - becomes a new line and then we have
166:31 - three c which is right here and then we
166:33 - have a backslash n which is a new line
166:36 - and then it becomes cn okay which is the
166:39 - third line here okay ccc here
166:42 - ctc here
166:43 - and then
166:44 - cn
166:45 - and cn
166:48 - okay
166:50 - and this is the
166:52 - input text of the smiles notation okay
166:56 - so the text box that we see here
166:58 - comes from this text area function and
167:02 - the input argument include the smiles
167:04 - input which is the name of this right
167:06 - here and then the smiles underscore
167:08 - input will be the example smiles
167:12 - notation that we have used here so if we
167:14 - change this
167:17 - we change the first one from c to n and
167:19 - then this will be changed
167:22 - right it becomes n now
167:26 - okay
167:27 - and then we're going to add a dummy
167:29 - first item here in order to allow us to
167:32 - simply
167:33 - read in multiple lines of smiles
167:37 - notation and then later we're going to
167:39 - be skipping that dummy first item and so
167:42 - we're going to do the same for multiple
167:44 - times on line number 91 96 and also when
167:48 - we make the prediction
167:49 - so this will allow us to simply create
167:53 - a data frame of the
167:55 - smiles notation and also for generating
167:59 - the data frame of the computed molecular
168:01 - descriptors for a single input parameter
168:04 - so imagine if we have only a single
168:06 - molecule
168:12 - okay this works okay but if we don't
168:15 - have the dummy items let me show you
168:22 - which is at
168:23 - three places
168:24 - lines number 87
168:27 - 96
168:28 - 110. so let's save it so without the
168:31 - dummy let's see what happens here
168:35 - okay and when we try to make a
168:37 - prediction
168:38 - we get an error
168:42 - right here let's you okay so this is
168:45 - having
168:50 - okay
168:52 - it's skipping here okay i have to hide
168:54 - this one too 91 as well
168:56 - so actually it is 87 91 96 and also 110.
169:03 - okay let's rerun it
169:06 - so this will work right
169:10 - oh okay so
169:12 - i think we're skipping
169:14 - oh yeah we cannot
169:17 - okay we have to display x here otherwise
169:19 - nothing will be shown
169:21 - and then we have to display smiles here
169:24 - otherwise nothing will be shown see
169:26 - nothing is shown here and then we need
169:28 - to show the prediction so we need to
169:31 - type in prediction okay so instead of
169:34 - slicing from the second value onwards
169:37 - we're going to have it printing all of
169:39 - it okay now it should work
169:42 - all right it works now so notice that it
169:44 - works if it has multiple lines
169:47 - for the smiles notation here but what if
169:49 - it has only one
169:52 - okay only one
169:54 - command enter
169:56 - and now we have an error here okay
170:00 - so for ease of handling a single
170:04 - input parameters
170:06 - we're going to use the dummy item
170:08 - as i have mentioned
170:16 - right here we're going to be reading the
170:18 - second value onwards okay
170:21 - now it works right we have a single
170:23 - value here and we make a prediction it
170:26 - works okay
170:29 - all right let's continue
170:31 - so i have already mentioned
170:33 - that we have the input smiles notation
170:36 - here and the computed molecular
170:38 - descriptors is provided here in this
170:41 - block of code so very simple here so all
170:43 - of the hard work is done here in the
170:45 - custom function and so we're going to be
170:47 - using only the generate because generate
170:49 - will be using the aromatic proportion
170:52 - right here aromatic proportion function
170:54 - and then we're going to be using the
170:56 - generate function right here line number
170:58 - 95 and then the input argument will be
171:00 - the smiles and the smiles here will be
171:02 - what it will be the input smiles that we
171:05 - have okay so these will be the input
171:07 - right so it will be split according to
171:09 - the backslash n so each line of code
171:12 - here each line of the smiles notation
171:15 - will represent a unique molecule okay
171:17 - and then each molecule will be computed
171:19 - for its molecular descriptors and then
171:22 - it will be shown here in the x variable
171:25 - so if we recall y equal to f of x
171:29 - therefore x will be the input parameters
171:32 - for predicting the y value which is the
171:35 - log s okay
171:38 - and so the model has already been
171:39 - pre-built in the google colab
171:42 - just a moment ago and we are loading in
171:45 - the model here using the pickle.load
171:48 - function and the solubilitymodel.pkl
171:52 - and then we're going to make use of this
171:54 - loaded model here
171:56 - loadmodel.predict and then using the
171:58 - input x which we have already computed
172:01 - here right here right so there's three
172:02 - major component so the input smiles here
172:07 - is here these are the example right
172:10 - but if we change this to something else
172:13 - like aspirin right then this whole thing
172:17 - theoretically will be here
172:20 - we'll be here okay we'll be like that
172:23 - okay
172:24 - and then it will enter here
172:28 - and then it will be
172:30 - subjected to
172:31 - descriptor calculation
172:34 - using the generate function
172:36 - and then
172:38 - finally we have the x variables
172:41 - and then the x variable here will be
172:44 - used as the input argument when we want
172:46 - to make a prediction okay and so finally
172:49 - here we're going to be using the
172:51 - computed molecular descriptors that are
172:53 - contained within the x variables and
172:56 - then we're going to use the loaded model
172:58 - that we have already predicted which we
173:00 - have already built on the google code
173:02 - lab and then we save it out using the
173:04 - pickle object and then we're loading it
173:06 - back in into this streamlit web
173:08 - application and then we're going to be
173:10 - applying this built model here load
173:12 - model dot predict and then the input
173:15 - argument will be x which are the
173:17 - computed molecular descriptors and
173:19 - finally here fc.header is corresponding
173:23 - to right here predicted lock as values
173:26 - and then the actual prediction will be
173:28 - shown right here prediction and it's
173:30 - going to be showing right here okay so
173:32 - we can have multiple lines
173:34 - let's say that we have multiple input
173:37 - files so here let's say that we have
173:39 - multiple molecules let's change this to
173:42 - something else so we are going to get a
173:45 - different molecule
173:47 - so we're going to see that the
173:48 - prediction value also changes right if
173:50 - we want to change a carbon to the
173:52 - nitrogen what will happen here right so
173:54 - the log s value changes here and so
173:57 - you're going to see that it influences
174:00 - the molecular weight and also the molar
174:02 - p but not the number of rotatable bonds
174:05 - and the aromatic proportion
174:07 - so changing a carbon to a nitrogen is
174:10 - essentially changing one atom and so it
174:12 - will be influencing two descriptors here
174:16 - all right so congratulations you have
174:18 - now built a very simple bioinformatics
174:20 - web application particularly for drug
174:23 - discovery
174:31 - have you been using streamlit library in
174:33 - creating some web applications perhaps
174:35 - you have already successfully deployed
174:37 - the web application locally but do you
174:40 - want to deploy it onto the internet so
174:42 - that other people could have access to
174:44 - your awesome web application
174:46 - if you answered yes then you want to
174:48 - watch this video to the end because i'm
174:50 - going to show you how you could deploy
174:52 - the streamlet web application onto the
174:54 - internet using hiroku so without further
174:57 - ado we're starting right now okay so the
175:00 - first thing that you want to do is head
175:01 - over to the github of the data professor
175:04 - and you want to go to the repository
175:06 - called penguins heroku so all of the
175:09 - files that we will be using to deploy
175:11 - onto hiroku will be provided in this
175:14 - repository so the web application that
175:16 - we are going to deploy is the penguins
175:19 - classification web app that we have
175:21 - previously created in the third part of
175:24 - this streamlet tutorial okay so let's
175:26 - have a look at this repository so we're
175:28 - gonna see that it comprises of this
175:31 - initiating readme so this will provide
175:33 - us with the details that we see on the
175:35 - github
175:37 - and other than that we have copied the
175:40 - following four files from the github of
175:42 - the penguins web application
175:45 - penguinsapp.py penguinsclean.csv
175:50 - penguinclf.pkl penguinsexample.csv
175:54 - so these four files were copied directly
175:57 - from the repository that i'm going to
175:59 - show you right now
176:01 - so we're going to code we're going to
176:03 - streamlit and it was from part three
176:07 - so we copied everything except for the
176:09 - modelbuilding.py
176:11 - because the
176:12 - modelbuilding.py will produce the pkl
176:15 - and that's what we need which is the
176:17 - saved model that we have created
176:20 - let's head back
176:23 - all right and aside from the four files
176:26 - which is directly related to the
176:27 - streamlade web application we're also
176:29 - going to create three additional files
176:32 - called the proc file
176:34 - requirements.txt
176:36 - setup.sh so let's have a look at the
176:38 - proc file so the proc file will
176:40 - essentially run the setup.sh file and
176:43 - run the streamlet web application
176:47 - the requirements.txt will tell hiroku to
176:50 - install the following python library and
176:52 - the corresponding version and so i'm
176:54 - going to show you in just a moment how i
176:56 - came up with this precise version number
177:01 - and we also created this setup.sh so
177:04 - this will handle issues regarding the
177:06 - server side in terms of the port number
177:09 - and it will add it to the configuration
177:13 - okay so let's have a look at the
177:14 - requirements.txt so what did i do
177:17 - let us open up the command prompt so i
177:20 - typed in cmd in the search and i'm going
177:24 - to activate my
177:25 - conda environment so if you have a
177:28 - contact environment you want to activate
177:30 - that using the name of your conda
177:32 - environment and so the contact
177:33 - environment on my computer is called dp
177:36 - so i'm going to activate that
177:41 - so let me type conda list
177:45 - and we're going to see the exact version
177:47 - number that we have installed on our
177:49 - computer so let's see what do we have we
177:52 - have streamlit
177:54 - so we have
177:57 - right here streamlit version oh okay so
178:00 - i've been using different computer so
178:02 - actually on this computer that i'm using
178:04 - to record this youtube video i'm using a
178:07 - lenovo desktop which is running on a
178:09 - windows and the number that you see here
178:11 - is from my macbook pro so they're using
178:14 - slightly different number here and so
178:16 - pandas on this windows notebook is
178:20 - 1.0.1 whereas i have the older version
178:23 - on my macbook pro and numpy
178:26 - 1.18.1 numpy 1.18.1 so i could learn
178:30 - 0.22.1
178:32 - let's have a look
178:34 - scikit-learn 0.2.1
178:37 - okay so the important thing is if you
178:39 - have already tested that your web
178:41 - application works on your current
178:43 - computer and you want to copy the exact
178:45 - version number into your
178:47 - requirements.txt so at the time of
178:49 - testing i used my macbook pro and so
178:53 - these were the version numbers from the
178:54 - macbook pro and so this worked perfectly
178:57 - so nothing wrong with this so what you
178:59 - could do in this requirements.txt file
179:01 - is to include all of the library that
179:04 - you are using in your code so why did i
179:06 - include these for library let's have a
179:09 - look at the code it's because in the
179:11 - penguinsapp.py we've been using the
179:14 - streamlit library the pandas library
179:16 - numpy and because pickle is built in we
179:19 - don't need to include that and also the
179:21 - scikit-learn so there's a total of four
179:24 - streamlid one pandas numpy and
179:27 - scikit-learn okay
179:29 - and then i included the exact version
179:31 - number and make a note that you need to
179:34 - have two equal sign here okay and so we
179:37 - have a total of seven files and
179:40 - including the readme that we have eight
179:42 - files and as a recall the four files
179:45 - here beginning with the penguins are
179:47 - related to the web application that we
179:49 - have created in srimlet and the three
179:51 - files comprising of proc file
179:54 - requirements.txt setup.sh will tell
179:57 - hiroku what to do what library to
179:59 - install which version to use and what
180:02 - precise command to run in order to run
180:04 - the streamlet web application okay so
180:07 - let's head over to heroku now
180:13 - so if you haven't yet signed up for
180:14 - hiroku you want to sign up and if you
180:16 - have already signed up you want to log
180:18 - in
180:26 - okay so after logging in we will see
180:29 - this dashboard and so these are the web
180:31 - application that i have already created
180:34 - in hiroku and so if we want to create a
180:36 - new app what we need to do is click on
180:38 - new
180:39 - and then click on create new app
180:42 - and then you want to give the name of
180:44 - your app here so let me try
180:47 - penguins
180:50 - okay that doesn't work
180:52 - app
180:57 - how about penguins st st for streamlit
181:00 - or even
181:01 - just penguins streamlit okay so that
181:04 - works and then once you're satisfied
181:06 - with the name of your app and make sure
181:08 - that it is available otherwise it will
181:10 - give you a
181:12 - error like this in red
181:14 - it's not available and then you just
181:17 - find a name that's available
181:21 - okay penguin streamlet and you could
181:23 - also select the region of the server
181:26 - that is going to host your web
181:28 - application so the options here are
181:30 - united states and europe so we're going
181:32 - to select the united states and then you
181:35 - want to hit on the create app
181:38 - wait a few moments
181:39 - all right so in the deployment method
181:42 - you want to click on github
181:48 - okay so i have already connected my
181:51 - github account with hiroku so if you
181:53 - haven't yet done that then you want to
181:55 - also do that as well and after you have
181:57 - already connected your github account
181:59 - with heroku then you could type in the
182:01 - name of your repository so what was the
182:04 - name of the repository it is called
182:07 - penguins heroku
182:09 - so let's copy that
182:12 - penguins heroku
182:14 - and then click on search
182:17 - click on connect
182:22 - and here you could also enable the
182:24 - option for automatic deploy meaning that
182:27 - whenever there is a change in your web
182:29 - application files on the github it will
182:32 - automatically deploy your web
182:34 - application onto hiroku so i'm just
182:36 - going to leave that out and we're just
182:38 - going to proceed with the manual deploy
182:41 - so don't do anything here so the default
182:44 - is master and then you want to click on
182:46 - the deploy branch
182:51 - okay scroll down and then you want to
182:53 - have a look at this feed here so it's
182:55 - going to provide you with the output in
182:58 - real time so what it is currently doing
183:00 - is installing the necessary python
183:02 - library
183:06 - so the good thing about heroku is that
183:08 - you don't have to worry about the server
183:11 - you don't have to maintain the server so
183:13 - the only thing that you need to care
183:14 - about is the application itself so the
183:17 - thing is you just upload your
183:18 - application onto github and then you
183:20 - connect that with heroku and then you
183:22 - could just simply deploy that web
183:24 - application so here it is also
183:26 - installing dependencies as well
183:31 - so this might take a couple of minutes
183:34 - so in the meantime you might want to
183:36 - grab a cup of coffee and sit back relax
183:39 - and enjoy
183:42 - okay so it is finished so it's going to
183:45 - be deployed at penguins dash
183:49 - streamlit.heroku app.com wait a couple
183:51 - of seconds and it's going to create the
183:54 - link
183:55 - at the bottom yeah right here so
183:57 - whenever it is finished you will see the
184:00 - link at the bottom it will say your app
184:02 - was successfully deployed and then you
184:04 - could simply click on the view link here
184:08 - and this will bring you the web
184:10 - all right so it's apparently loading and
184:12 - when that web application is loading for
184:14 - the first time it might take you some
184:16 - time okay so now the web application is
184:19 - loaded and so let's play around with the
184:21 - input parameters and as we can see the
184:24 - prediction label changes along with the
184:27 - prediction probability okay so
184:29 - congratulations you have now
184:30 - successfully deployed your streamlid web
184:33 - application onto heroku
184:41 - do you want to deploy your web
184:43 - application that you have just created
184:45 - in python using the streamlet library if
184:47 - you answered yes then this video is for
184:50 - you because today i'm going to show you
184:53 - how you could easily deploy your web
184:55 - application onto streamland and without
184:57 - further ado we're starting right now
185:00 - okay so the first thing that you want to
185:01 - do is head over to the github of the
185:03 - data professor and then you want to
185:05 - click on the repositories
185:08 - and then click on streamlet 10
185:12 - and so a streamline10 will essentially
185:14 - be the 10th episode of the streamlet
185:16 - series on this youtube channel and so
185:19 - here we're using the sp 500 dash app dot
185:23 - py so
185:25 - i'll provide you the links of the 10th
185:28 - episode in the description of this video
185:31 - and so we're gonna use this to deploy
185:34 - the app in this tutorial so let's have a
185:37 - look at that
185:38 - okay so this is the app contents
185:42 - so in a nutshell it will essentially web
185:46 - scrape the data directly from wikipedia
185:48 - of the s p 500
185:51 - and then it's going to download the y
185:54 - finance data set directly from the yahoo
185:57 - finance via the y finance library and
186:01 - then it will create a data frame of the
186:04 - data and then it will also allow us to
186:07 - download the data as a csv file and
186:11 - finally we're also able to make some
186:15 - beautiful plots here
186:17 - all right and so we're gonna proceed
186:19 - with deploying the app and so before
186:21 - continuing further it should be noted
186:24 - that we need also the
186:26 - requirements.tf
186:28 - file which will list all of the
186:30 - dependency meaning the libraries that
186:33 - we're using in our web app and also the
186:36 - corresponding version number so how do i
186:38 - get this let me show you
186:42 - so you can get this by going to your
186:44 - terminal and i have already logged into
186:47 - my conda environment so i will directly
186:50 - type pip
186:52 - freeze
186:53 - and then
186:54 - requirements
186:57 - dot txt
187:00 - and then let's have a look at the
187:01 - contents of the file
187:04 - and then notice that
187:06 - i have all of the libraries that are
187:08 - installed in my conda environment along
187:10 - with the corresponding version number so
187:12 - i will selectively choose the libraries
187:16 - that are used for the web app and then
187:19 - i'm going to copy the corresponding
187:21 - lines like for example we're making use
187:23 - of the y finance library so we're going
187:25 - to copy this line we also made use of
187:28 - the matplotlib library and so we're
187:30 - going to find matplotlib right here and
187:32 - then we're going to copy that and then
187:34 - do the same thing for all of the other
187:35 - libraries that you're using
187:40 - okay so essentially you have the app
187:43 - file itself you have the
187:45 - requirements.txt and then you also have
187:48 - the readme.md file so this is normally
187:51 - created automatically if you ticked on
187:53 - it and it will allow you to show this
187:56 - readme here so i'm going to show you how
187:58 - you could include a button that will
188:01 - allow you to click on it and then it
188:03 - will launch the web application okay so
188:06 - let me open up the
188:08 - share
188:08 - streamlet website
188:13 - share streamlet
188:14 - and so it should be noted that for this
188:17 - one i've signed in using my github so
188:20 - let me try signing out
188:22 - and so for the first time when you sign
188:24 - in it will ask for your authorization
188:28 - that you're allowing streamlit to have
188:30 - access to your github
188:32 - account and so i have already done that
188:35 - so it's going to log in
188:37 - seamlessly
188:39 - okay and so this is the dashboard part
188:42 - of
188:43 - the share streamlet website upon logging
188:46 - in
188:47 - and so i'm going to deploy the first web
188:48 - application here so i'm going to click
188:51 - on the new app button
188:54 - all right and then i'm going to click on
188:55 - the repository
189:00 - and then i'm going to click on the data
189:02 - professor streamlet 10
189:08 - and then the branch will automatically
189:10 - detect to be main
189:11 - because if i go back to the github here
189:14 - notice that this says mean
189:17 - okay
189:19 - or it could be master depending on your
189:21 - own github profile
189:26 - all right
189:28 - and then let's have a look at what is
189:29 - the name of the app and it's called sp
189:32 - 500 app dot py
189:35 - let's copy that
189:39 - paste it here
189:40 - and then that's all we need to do in
189:42 - order to deploy the app and then you
189:44 - click on the deploy button
189:47 - and then wait for a while
189:51 - and so if you see this icon
189:53 - coming up it means that your app is
189:56 - probably going to be successfully
189:58 - deployed given that you also have the
190:00 - requirements.txt
190:02 - file that i have mentioned previously
190:06 - so in just a moment while it is
190:08 - installing all of the necessary
190:10 - libraries for you
190:14 - all right so you see all of the log here
190:17 - so most of it has met the requirement
190:21 - here
190:22 - and it has already installed the y
190:24 - finance library
190:28 - and it says here that it has processed
190:30 - the dependency and then you saw the
190:32 - balloons it means that it had already
190:34 - successfully deployed
190:37 - so let me click on this bottom right
190:40 - button
190:41 - in order to minimize it
190:43 - or i could bring it up again by clicking
190:45 - the manage app
190:48 - and then hide it
190:49 - again
190:51 - alright and so congratulations now you
190:54 - have successfully deployed the web
190:57 - application onto the streamlit website
191:00 - in the
191:01 - share.streamlit.io so let's have a look
191:03 - at the plots here
191:08 - we see one plot here
191:10 - if i move to two
191:12 - then i'll be seeing two plots here
191:17 - all right so there you have it this is
191:19 - the deployed web application
191:21 - which is quite easy
191:23 - and it should be noted that currently
191:25 - right now streamlit is hosting a
191:28 - beta trial so it means that you need to
191:31 - be in the 1000 selected users in order
191:35 - to try out this new feature so head over
191:37 - to the srimlet share website and click
191:40 - on request for invitation so that the
191:44 - streamlet team will invite you to try
191:46 - out this awesome feature and so i'll
191:49 - provide you the links to that as well