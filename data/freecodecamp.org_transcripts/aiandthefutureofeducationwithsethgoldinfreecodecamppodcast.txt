00:02 - [Music]
00:21 - welcome back to the freeco camp podcast
00:24 - I'm Quincy Larson teacher and founder of
00:27 - freeco camp.org and each week we're
00:29 - bringing you Insight from developers
00:32 - entrepreneurs and ambitious people who
00:34 - are getting into Tech this week we're
00:36 - joined by Seth golden he's an open
00:40 - source project maintainer developer a
00:43 - prolific Freo Camp contributor and he is
00:47 - a computer science student at Yale
00:49 - welcome
00:50 - Seth thank you for having me Quincy yeah
00:54 - it's great to have you here man uh
00:55 - you're somebody whom I've enjoyed
00:57 - talking with for years at this point and
01:00 - you have a very special relationship
01:02 - with a gentleman whom uh has who has
01:06 - also contributed a lot to freeo Camp uh
01:09 - Sam cromby and I wanted to start by just
01:12 - going back in time a few years to how
01:15 - you met
01:17 - Sam yeah so Sam and I actually met we're
01:22 - both originally from South Florida and
01:26 - we met through a mutual friend uh Joe
01:29 - who also one of my really good
01:31 - friends and they were working together
01:34 - on a project and Joe and I had worked
01:36 - together on a project and somehow Sam
01:39 - and I got connected and we all have this
01:44 - interest in
01:46 - edtech and that's education technology
01:49 - and
01:50 - so it just like immediately like hit it
01:53 - off um and our first big project
01:55 - together Sam and I was we recognized
01:59 - that there's this amazing wealth of
02:02 - resources that exist on the internet
02:04 - which is the fact that like most college
02:06 - CS curriculums are available online for
02:09 - free publicly yeah and at the time we
02:12 - saw that these were courses that were
02:14 - just being passed around on like Reddit
02:17 - through like Google Sheets like people
02:18 - would be sharing like Google Sheets of
02:20 - like these are all the Stanford CS
02:22 - courses or these are all the MIT CS
02:26 - courses and we were like okay we could
02:29 - probably build better interface than
02:31 - Google Sheets via Reddit
02:33 - post um for like just sort of
02:36 - agglomerating and collecting all of
02:37 - those resources to make them more
02:39 - accessible to people make it easier to
02:41 - find so that was our first project that
02:43 - we worked on together and college
02:46 - compendium dorg again a giant directory
02:50 - of uh online courses that you can take
02:54 - in most cases for free uh through
02:57 - various universities including you know
02:59 - many any prestigious universities like
03:01 - Yale um so uh tell me a little bit about
03:05 - how you two met like what were the
03:08 - circumstances were were you working on
03:10 - similar projects or did you just bump
03:11 - into each other at a
03:13 - party it wasn't a party I was working on
03:17 - an academic information system for the
03:19 - school district of pompage County at the
03:20 - time and Sam and Joe were working on a
03:23 - hallpass system during Co so like a
03:26 - digital Hall Pass solution and there was
03:29 - just a lot of overlap in our interests
03:31 - we got connected and started talking um
03:35 - and what actually ended up happening was
03:38 - the that project uh compendium which
03:42 - isn't like super mantained but there's
03:44 - still a lot of amazing resources that
03:46 - are on that website for free still today
03:49 - um was we learned a lot about like a lot
03:52 - of the challenges that exist with
03:55 - auditing courses
03:56 - online and so that's how we made our
03:59 - first fre code Camp course was we
04:00 - recognized that there are a lot of like
04:03 - sort of tips and tricks that you can use
04:05 - to help yourself get through a lot of
04:07 - these courses that we had discovered
04:10 - through our process of auditing CS
04:12 - courses like Berkeley 61b for example is
04:14 - a really famous yeah example um and you
04:18 - know over the years Sam and I looked at
04:20 - ml courses NLP courses data structure
04:23 - learning natural language processing
04:25 - yeah I'm sorry I'm just going to like
04:27 - there are lots of acronyms used in
04:29 - computer science CS true just if I
04:32 - interject occasionally it's just to make
04:33 - sure that everybody in the audience
04:35 - knows what we're talking about here of
04:37 - course so we thought that that could be
04:41 - a really like useful addition to the
04:43 - free code Camp community and it seems
04:45 - like it was it was oh yeah absolutely
04:46 - like hundreds of thousands of people
04:49 - have watched the auditing course and we
04:53 - weren't sure at the time are people
04:54 - really that interested in formally
04:57 - auditing a university course when they
04:59 - could just just go watch Like You Know
05:01 - video recordings of courses uh or uh do
05:05 - like the you know the online course
05:07 - version of it through like you know
05:09 - corsera or edex or something like that
05:12 - but it turns out there's a lot of
05:13 - interest
05:14 - among developers in the freeo camp
05:16 - community so yeah thanks for making that
05:19 - course of course it's something that
05:21 - we're really proud of and I think a lot
05:23 - of
05:24 - the things that we did after that were a
05:28 - reflection of what that experience
05:29 - taught us which is it's really
05:31 - challenging to finish a course online
05:33 - like learning online is really hard and
05:36 - so the next thing that we worked on
05:38 - samon was this tool that we were trying
05:41 - to build to help
05:43 - people um get through courses like sort
05:46 - of a digital ta which was like a very
05:48 - very early version of chat GPT using
05:51 - gpt3 to create basically a chatbot um in
05:55 - like April of 2022 I think it was um um
06:00 - and so a fun story actually is I ended
06:02 - up skipping my graduation for high
06:04 - school to fly to Dartmouth for like a
06:06 - pitch competition that was happening
06:09 - there um yeah Dartmouth being the school
06:11 - that Sam just graduated from Recently
06:13 - but this was back your high school
06:16 - graduation yeah so that we could um like
06:20 - pitch this new idea and get started on
06:22 - working on it and that eventually turned
06:25 - into a tool that Joe ended up joining us
06:28 - for and the three of us worked together
06:29 - on it for about a year um eventually it
06:32 - being called Friday Friday like like his
06:35 - girl Friday like the idea of a chatot
06:37 - that can help guide you through uh a
06:40 - whole lot of um you know struggles that
06:43 - you might encounter trying to get
06:44 - through these courses right yeah and it
06:47 - was an incredible experience to work on
06:49 - that with the two of them and hopefully
06:52 - it helped a lot of students we had a lot
06:53 - of students using it um over the span of
06:56 - the last year so I think like in general
06:59 - when I think about sort of courses and
07:03 - and auditing courses like it has never
07:06 - been easier to do that than it is today
07:09 - with tools like Chachi PT and all of
07:11 - these advances recently in large
07:13 - language models like even taking courses
07:15 - in person um it's sort of like a a
07:19 - massive step forward that it's difficult
07:22 - to imagine what life was like before
07:24 - these tools existed like yeah I find
07:26 - myself constantly asking questions now
07:30 - that I know I'm going to get like a
07:31 - quick response to that makes sense and
07:32 - is thorough that like a Google search
07:34 - wouldn't have provided
07:36 - um you know as as succinct C or as as
07:40 - explanatory um before and that's
07:42 - actually probably a good segue into the
07:44 - second course that we worked on yes and
07:47 - uh just just to be clear we're going to
07:48 - talk a lot about AI that's going to be a
07:50 - huge thing we talk about later but I I
07:52 - want to hear about like one of the
07:53 - earlier versions of AI uh Google search
07:57 - itself I mean the p rank algorithm and
08:00 - all the additional software they've
08:02 - layered on top of it is this Tool uh
08:06 - that I mean where does the AI really
08:09 - begin I mean it it used to just be if
08:11 - then logic and now it's it's neural
08:14 - networks and things like that but
08:15 - somewhere in between was Google search
08:18 - and Google search still is incredibly
08:20 - useful I use it I don't know 50 times a
08:22 - day probably that's not an exaggeration
08:24 - but I also use llms uh I use uh GPT 4 a
08:28 - lot um especially for language learning
08:31 - as I've talked a little bit about in
08:32 - previous episodes of the free C Camp
08:34 - podcast but maybe you could talk a
08:36 - little bit about your course on Google
08:38 - searching this technology that's been
08:39 - with us for like 20 plus years that a
08:41 - lot of people take for granted like
08:43 - don't you just type what you want no all
08:46 - right you don't just type what you want
08:48 - there's a whole lot of different um kind
08:50 - of tactics you can use to get what you
08:53 - want out of Google
08:56 - right yeah I mean that's something that
08:59 - over the the years
09:01 - like we had picked up uh a lot of sort
09:05 - of tips and tricks about using Google
09:08 - search to like more quickly more
09:10 - accurately find what you're looking for
09:12 - like sure you can just sort of type into
09:15 - the search bar or anything and Google's
09:17 - really good at finding what it is that
09:18 - you're looking for and not just Google
09:19 - search endom in general like that's what
09:21 - they're meant to do um but there are a
09:25 - lot of like operators that you can add
09:28 - to your query to make it more accurately
09:34 - find what you're looking for especially
09:35 - when you're programming and you're
09:37 - looking to like debug this like one
09:39 - error that you can't figure out and
09:41 - you're trying to find that like one
09:42 - stack Overflow post or GitHub Forum
09:45 - comment that like solves your problem uh
09:49 - something as simple as just like putting
09:50 - quotations around like the specific
09:53 - thing that you're looking for so that an
09:54 - exact matth searches for it I use that
09:56 - like every day that's a
09:59 - um like worth it to watch the first five
10:02 - minutes of that course if you didn't
10:03 - know that was a thing that existed and
10:05 - there's a lot of operators like that
10:07 - that I think I find really valuable I
10:10 - think another like that course was sort
10:11 - of a combination of a bunch of things
10:13 - like it was like anything that could
10:14 - possibly be useful to you about Google
10:17 - was how Google used to work how Google
10:20 - works today to our best understanding a
10:22 - lot of these operators that you can use
10:25 - and then also just like strategies for
10:27 - when you're searching
10:29 - like this I think becomes Al it's it
10:33 - remains relevant even in a world of
10:35 - large language models in that like it's
10:38 - still all about how you prompt the model
10:41 - like the the question that you ask is
10:43 - just as important
10:45 - as what it is that you're trying to find
10:48 - like you sort of have to imagine like
10:50 - what it is that you're trying to end up
10:53 - with and then craft a prompt or a query
10:57 - that you think will best result in that
10:59 - result versus that just sort of like
11:02 - putting in like a question um and mlms
11:06 - are pretty good at answering those
11:07 - things too large langu it's also yeah
11:11 - yeah very much about Sor sorry I just
11:12 - wanted to jump in just in case anybody
11:14 - doesn't know that acronym yet you're
11:15 - going to hear that a lot over the next
11:16 - few years large language models llms uh
11:19 - just just to kind of like recapitulate
11:22 - what you just
11:24 - said communication is ultimately what
11:27 - this is and programming is communicating
11:30 - what is in your head to a computer right
11:32 - Google searching effectively is
11:34 - communicating what you want to a
11:37 - computer uh prompt engineering if you
11:40 - want to call it that like essentially
11:42 - The Art of Getting an llm to tell you
11:45 - what or give you the information that
11:47 - you need right or do the task that you
11:49 - needed to do it it's all about
11:51 - communicating and the way that you
11:53 - communicate with a human uh a native
11:56 - English speaking human that grew up with
11:58 - similar cultural context to you and
12:00 - everything like that like you grew up in
12:02 - South Florida I grew up in Oklahoma like
12:04 - the way you and I would communicate is
12:06 - very different from how you would
12:08 - communicate with Google or you would
12:10 - communicate with gp4 right uh so so the
12:14 - tactics that you use to effectively
12:16 - communicate and and get the machine to
12:19 - do your bidding essentially that's
12:21 - that's what your Google course is about
12:23 - right yeah and I think like the thing is
12:26 - the end goal I I think at least is for
12:28 - for the way that we interact with
12:30 - computers to be exactly the same as the
12:32 - way we interact with other people that
12:35 - that is possible and like eventually
12:38 - something
12:39 - that I mean just makes the most
12:42 - intuitive sense like that's what
12:43 - programming is I would agree to point
12:46 - however like it with a human you do have
12:48 - to kind of be polite you don't have to
12:49 - be polite to a computer a computer does
12:51 - not care about politeness that's true uh
12:53 - if you look at like and I always you
12:55 - know I'm a huge Star Trek fan especially
12:57 - the Next Generation which is my my
12:59 - personal favorite uh but I I'm I I'm
13:02 - working through like dpace 9 and I
13:04 - enjoyed Voyager and stuff too but
13:07 - um if you look at how they talk to a
13:09 - computer it's
13:11 - incredibly uh imperative but it's also
13:13 - very declarative in the sense that like
13:15 - they're telling the computer exactly
13:16 - what to do but the computer is also able
13:19 - to kind of make inductive leaps and
13:21 - figure out oh they probably meant this
13:23 - or this but the way they talk to the
13:25 - computer is not like hey how are you
13:26 - doing computer could you please do this
13:29 - they're just like computer XYZ you know
13:31 - and the computer goes and does it so in
13:33 - a way like communicating with a computer
13:36 - that doesn't have feelings doesn't care
13:38 - about like doesn't have a physical form
13:40 - doesn't have all these different
13:41 - considerations emotions that the humans
13:43 - have it can be much more efficient and
13:46 - direct and you can give it like you
13:49 - could just give it a big line of like
13:52 - code essentially instead like where it
13:53 - would take forever to like articulate
13:55 - exactly what you want code can be much
13:57 - more succinct in some instance
13:59 - for explaining what you want to be done
14:01 - so while I agree that you know there's
14:04 - this book called snow crash that I read
14:06 - when I was a kid and he has like a
14:08 - librarian and he has these conversations
14:09 - with the librarian and the librarian
14:11 - teaches him everything he needs to know
14:12 - about whatever topic he's is is relevant
14:15 - kind of just in time learning and stuff
14:17 - like I can definitely understand like a
14:18 - computer embodied in kind of like a
14:20 - hologram or something that that has like
14:22 - a role like oh I want Mark Twain to be
14:24 - my my professor or something like that
14:26 - right at the same time there's a of
14:28 - value in just treating the computer like
14:30 - a computer and just saying you're Google
14:33 - you do this go do this now you know this
14:35 - is how you get things done you know and
14:37 - and understanding why the computer does
14:39 - what it does how it does like you were
14:41 - saying like understanding how Google has
14:43 - changed over time the more understanding
14:45 - you have of the tool the more you can
14:48 - kind of empathize with the tool and what
14:50 - it's trying to do and and kind you have
14:52 - that kind of theory of mind for the
14:55 - computer you're interacting with
14:57 - right yeah
14:59 - I mean I think what you're talking about
15:00 - in terms of proactivity is really
15:03 - interesting to me um we can talk more
15:06 - about proactive Computing and also
15:09 - contextual Computing which is something
15:10 - I'm super interested in right now later
15:12 - in this I've never even heard the term
15:14 - proactive Computing can you tell me what
15:15 - that is real quick
15:17 - just so proactive in contextual
15:20 - Computing is this idea that very soon
15:23 - we're going to be able to use
15:25 - information that we have about people um
15:28 - like context on for example the
15:30 - transcript of this podcast or like
15:33 - screenshots from your computer and
15:37 - also like be able to just start doing
15:40 - things for you I don't like this is
15:42 - starting to get sort of sci-fi but if
15:43 - you've seen like even like some of the
15:45 - Marvel movies or her with walking
15:48 - Phoenix and Scarlet excellent movie yeah
15:52 - these are ideas of having AI assistants
15:55 - that are capable of doing things for you
15:57 - without you asking asking them to do
15:59 - that which doesn't really exist yet you
16:01 - still have to like hold down and say Hey
16:03 - Siri which I just triggered Siri but
16:06 - like you have to ask for things still
16:08 - you have to ask chat gbt to do XYZ for
16:11 - you in the next couple years I think
16:13 - what we'll start to see is these
16:17 - assistants will like send you a
16:18 - notification and be like based on the
16:20 - conversation you just had like XYZ or
16:24 - like hey Quincy I just finished um you
16:28 - know listening to your podcast recording
16:30 - I edited you know these things and I'm
16:32 - ready to upload it when you are and then
16:34 - you just click like yes and then it does
16:35 - it for you and like sort of automating
16:39 - tasks that you don't necessarily need to
16:41 - do yourself is something that we can
16:42 - start to see over time so proactive
16:44 - contextual Computing like really cool
16:47 - yeah what you were talking about before
16:49 - though um about
16:51 - programming I mean I feel like that's
16:54 - what programming is is it's a sort of
16:57 - more human version of machine code like
17:00 - it's a way for like levels of
17:02 - abstraction that we're able to use like
17:04 - something like python for example is
17:06 - like a lot closer to natural language
17:09 - than something like assembly or C
17:13 - um so like I find that really
17:17 - interesting too is is the ways in which
17:19 - we're communicating with computers
17:21 - already starting to have more of that
17:23 - natural language element to it and it'll
17:26 - be interesting to see like what future
17:27 - programming angues look like um given
17:30 - that we now have this ability to sort of
17:32 - compile cross
17:33 - languages um with like large language
17:37 - models and so on or or at least that
17:39 - that will get better and better over
17:40 - time something I want to ask you
17:42 - actually is what you sort of think the
17:44 - future of this world looks like the
17:46 - future of programming the future of
17:48 - online education given that we have all
17:52 - of these new opportunities and resources
17:54 - that didn't exist like a year or two ago
17:56 - in the same way and that are constantly
17:58 - getting faster and more accurate and
18:00 - less expensive yeah I mean I I think the
18:03 - vision of The Librarian from Snow Crash
18:05 - is is
18:07 - pretty apt uh you know it's it's like a
18:10 - 30-year-old novel
18:12 - maybe um but it's I mean it given the
18:17 - Computing resources given the amount of
18:20 - instructional designers and teachers and
18:23 - subject matter experts and stuff who
18:25 - would compile their knowledge the movie
18:27 - Her which you mentioned
18:29 - the operating system that uh is voiced
18:33 - by uh gosh what's her name uh yeah
18:37 - scarjo um that is the product of like
18:41 - millions of programmers they say it like
18:43 - millions of programs have contributed
18:45 - code that ultimately went into this and
18:46 - if you think about it like you know a
18:49 - similarly complicated application like
18:52 - Google search for example may have at
18:54 - this point hundreds of thousands of
18:55 - contributors that have worked at Google
18:57 - over the years
18:58 - and worked on that right so we can see
19:01 - how all this software would get layered
19:03 - on top and and also if you consider all
19:05 - the content being absorbed legally or
19:07 - illegally we're we're not going to talk
19:09 - about that debate uh but you know the
19:12 - common crawl and every Wikipedia post
19:15 - every Reddit post every free C Camp
19:16 - Forum post of which there are hundreds
19:18 - of thousands you know being uh absorbed
19:21 - by GPT and uh you know thus in the great
19:25 - plagiarism engine that is them uh
19:29 - basically it could it could kind of
19:31 - reproduce this and and again I don't
19:33 - want to act like I don't want people to
19:35 - get the the vibe that like oh I think
19:38 - you know uh one way or another cuz my
19:40 - thoughts on it are very nuanced and
19:42 - complicated and they're still forming uh
19:45 - but as a human writer who's written
19:47 - hundreds of Articles over the years I'm
19:50 - probably subconsciously cribbing all
19:52 - kinds of stuff from The New York Times
19:55 - or the Wall Street Journal or the
19:57 - Atlantic or the New Yorker or like all
19:59 - these other Publications that I've read
20:00 - over the years like different ways of
20:02 - saying things different ways of thinking
20:04 - and at what point was that an original
20:07 - thought you know who knows uh that
20:11 - that's almost like a philosophical
20:12 - discussion though it's not a programming
20:14 - and computer science question but what
20:15 - you asked about original yeah yeah is is
20:18 - anything original can we have different
20:19 - breakfast flavor cereals or have we
20:21 - tried them all or they all just
20:22 - variations of the same you know uh few
20:24 - tastes or something I don't know but
20:27 - what I will say is
20:28 - I think the idea of a tutor a one toone
20:31 - student ratio uh teacher ratio is pretty
20:33 - amazing and I think in some cases a t
20:36 - two or three student to one teacher kind
20:38 - of like if you go to a fancy school like
20:40 - yo you probably have like seminar
20:41 - courses where it's a very small number
20:43 - of Learners and one professor and you
20:48 - get that benefit of just being in a
20:50 - small group and having a lot of
20:51 - intelligent people asking questions that
20:53 - maybe you yourself wouldn't have thought
20:54 - to ask uh around you that is very good
20:58 - good too so I do think that we're going
20:59 - to continue to have classes where
21:01 - there're you know more than one student
21:03 - but I think a lot of education is going
21:05 - to be
21:06 - with a software system and let me be
21:09 - clear free Cod Camp is a software system
21:11 - free Cod Camp's Core Curriculum is
21:14 - essentially you could almost say like an
21:16 - AI in the sense that it's running your
21:18 - code it's evaluating it it's returning
21:20 - test information all the things it says
21:23 - are based on okay this condition didn't
21:25 - meet we didn't meet this condition and
21:28 - some instructional designer like you
21:30 - know Jessica or Naomi or Chris or one of
21:33 - the many people on our instructional
21:35 - design team has come up with like
21:36 - messages that help guide you almost like
21:38 - Socratic method did you mean to leave
21:40 - this you know uh this curly brace out or
21:44 - did you mean to uh not close your Loop
21:47 - or something like that right like like
21:49 - that's the the dream is asking you
21:52 - questions and guiding you toward the
21:54 - answer that you discover for yourself
21:55 - because that method of discovery that
21:58 - that moment burns into your memory like
22:01 - what you did wrong and that's one of the
22:03 - things about programming that's so
22:05 - amazing is you can sit there with a AI
22:10 - that's been built by thousands of
22:12 - contributors to free Cod camp and you
22:14 - can have an interaction and you can
22:15 - learn programming and the intelligence
22:18 - of all of those people is kind of like
22:19 - represented in the software that you're
22:22 - using that's guiding you to uh an
22:25 - understanding of how to write a sequel
22:28 - query or how how to write you know a
22:31 - python for Loop or something like that
22:32 - right so yeah yeah I I think that uh
22:36 - that's that's the future I it's an
22:38 - extremely broad question but but I think
22:40 - that's where things are heading is just
22:41 - having a extremely intelligent systems
22:44 - that people interact with in addition to
22:46 - you know I I don't think teachers are
22:48 - going to go away uh I I do think that
22:50 - classes are going to get smaller uh and
22:53 - I think they should get smaller because
22:54 - there's you know the only reason that
22:56 - you go to a public school and they 40
22:58 - kids in there with one teacher is it's
23:00 - an economic necessity for all practical
23:03 - pedagogical reasons there should be like
23:05 - three students in there or one student
23:07 - you know I I just uh I and I think we're
23:09 - heading toward an area where there's
23:11 - such an abundance of resources that
23:14 - anyone who wants to learn anything will
23:17 - be able to free probably pull up
23:21 - something that's dedicated to teaching
23:22 - them that specific
23:24 - skill a lot of that may be video which
23:26 - is what you know your media
23:28 - uh has been recently uh with with the
23:30 - video courses the three video courses
23:32 - that you published on free C Camp and I
23:35 - still think that that's a great mode
23:37 - even though it's not interactive people
23:39 - can code along at home and they can spin
23:40 - up the server and they can you know
23:42 - clone your repo and they can do that and
23:44 - and that is some degree of interactive
23:45 - but I really think the gold standard
23:47 - where things are heading fully
23:49 - interactive fully like you know
23:52 - everything considered by a human teacher
23:55 - who has subject matter expertise and I I
23:58 - do think a lot of people are going to
24:00 - kind of paper over a lot of that human
24:01 - expertise by just like oh the AI can do
24:03 - it but the the AI is going to be
24:04 - mediocre frankly it's only going to be
24:07 - derivative it's not going to have the
24:09 - kind of flashes of insight that a human
24:11 - teacher has in my humble opinion and
24:13 - maybe eventually it will but we have no
24:15 - idea how soon that will come to be and
24:17 - we may be stuck in this Paradigm and
24:19 - things people may think things are going
24:21 - like this but they may actually be doing
24:23 - step changes you know we just
24:24 - experienced a step change so sorry this
24:26 - is not the let's interview Quincy Lon
24:29 - podcast I did not mean to just talk for
24:31 - a really long time but I I think a great
24:32 - deal about these things and I know you
24:34 - too what do you
24:36 - think yeah I mean well first of all like
24:40 - that that's why I came on the podcast is
24:42 - I wanted to hear your thoughts about all
24:43 - of this
24:45 - um I think that you're absolutely right
24:48 - that things are going to become more
24:51 - personalized um more generative um and
24:55 - then like what you were saying about
24:57 - like Interac gamified like I I think
24:59 - things are going to become more gamified
25:02 - in the same way that like like right now
25:04 - I use I use du lingo like every day um
25:08 - and you can debate how like effective
25:10 - that is at teaching you um like a
25:13 - language I use it as a supplement
25:14 - because I'm also taking a language
25:15 - course so it's just like it makes me
25:17 - feel good about myself to like do the
25:20 - Dual lingo every day but it's really
25:22 - effective at getting me to do it every
25:24 - day I'm at like a you know more than a
25:25 - year streak at this point um um and I
25:29 - think that that kind of sort of like
25:32 - dopamine pushing um like structure to a
25:36 - course could be like very effective in
25:39 - getting people to get through those um
25:43 - in a way that's also like more fun like
25:45 - for example like I took a bunch of
25:46 - courses online that were were like for
25:49 - credit for school and those courses were
25:52 - structured in a way where they were you
25:54 - would watch like 3 hours worth of videos
25:57 - and do like 20 Pages worth of readings
26:00 - or something and then you would answer a
26:02 - 10 question quiz and then you would
26:04 - repeat and to me that just feels like
26:07 - maybe not the most a missed opportunity
26:09 - man is it like what if the questions
26:11 - were sort of interspersed throughout all
26:13 - of that materials that you were being
26:15 - checked on how you understood it as you
26:18 - were going through it versus like right
26:21 - all at the end like I think structurally
26:24 - our education system hasn't changed a
26:26 - lot in a really long time
26:28 - and this is an opportunity to sort of
26:29 - revisit like first principles what is
26:32 - the purpose of Education how is like
26:35 - what is the most effective way to learn
26:37 - um and that could be different for
26:39 - different people I mean I personally
26:41 - find it a lot more entertaining and
26:43 - enjoyable to work on projects like when
26:46 - I'm when I'm trying to learn a CS course
26:47 - I prefer CS courses that you like you
26:49 - were discussing where I'm spinning up an
26:51 - environment and like doing something in
26:53 - real time while I'm watching the video
26:55 - versus like just like watching lecture
26:57 - for 10 hours and then doing something
27:00 - but someone else might prefer the latter
27:02 - so it's like what's really nice too is
27:05 - that there's a level of personalization
27:07 - that exists within auditing in the sense
27:10 - that you can just pick what's better for
27:12 - you and then do that you sort of get to
27:15 - pick and choose and build your own
27:16 - curriculum and something that Sam and I
27:18 - spent a lot of time thinking about was
27:21 - this idea of like Pathways that you
27:23 - could create your own that it didn't
27:25 - really auditing didn't stop after you
27:28 - finished one course then it became about
27:31 - okay now what course do I pick and then
27:32 - what course do I audit
27:35 - and what's really cool is that those
27:37 - courses can be from completely different
27:39 - places you don't have to lied to one
27:42 - school you're exactly I mean there are
27:44 - 5,000 universities just in the US right
27:47 - I mean frankly a lot of those are
27:49 - nothing to write home about but even if
27:51 - you look at like really good
27:53 - universities uh there's still hundreds
27:55 - of them and they have different
27:56 - professors who have different expertise
27:58 - in different subjects and there's a good
28:00 - chance that uh there are maybe like just
28:04 - a small handful of universities to teach
28:07 - a given topic that you're really
28:08 - interested in so by being able to audit
28:10 - you can draw from all of them and you
28:12 - can have the ultimate level of
28:14 - customization right absolutely yeah yeah
28:19 - so you think that uh building tooling
28:21 - around these Pathways which I've been
28:24 - very interested in the first project I
28:26 - built once I you know learned coding was
28:29 - basically uh after I'd worked as a
28:31 - software engineer for a while and I
28:32 - wanted to build my own first big project
28:35 - was basically that it would it would
28:37 - strap together a whole lot of different
28:39 - courses and books um and depending on
28:42 - what you wanted to learn and what you
28:43 - knew and it would like you'd sign in
28:46 - with LinkedIn it would read all your uh
28:48 - LinkedIn information including back then
28:50 - they had like these skills and you could
28:51 - have people like rank you for different
28:53 - or rate you for different skills oh I
28:54 - think Quincy knows sequel sure why not
28:57 - so you get like 99 people that like all
29:00 - think that Quincy knows squel you do you
29:02 - remember that on LinkedIn when they used
29:03 - to have the skills and it'd be like um
29:07 - but anyway it would pull in your
29:08 - LinkedIn I know I'm dating myself um and
29:11 - then it would just use that as the basis
29:13 - like oh okay so you want to go from what
29:15 - you know now to working in uh you know
29:18 - business intelligence right or or using
29:22 - like powerbi and different tools like
29:24 - that to
29:25 - like crunch charge and and figure out
29:28 - where to prioritize resources and stuff
29:30 - like that right like so okay well then
29:32 - you should read this book you should
29:34 - take this course and by the way there's
29:36 - like a critical path of dependencies
29:39 - essentially like you need to know this
29:41 - field of mathematics in order to do this
29:43 - type of programming you need to know
29:44 - this type of programming in order to you
29:46 - know use this type of software or
29:49 - something like that so there'd be like a
29:50 - dependency chain and it would work
29:51 - backward and it would create kind of
29:53 - like a a list of courses you should take
29:55 - but to some extent like
29:59 - how do you know those are the right
30:00 - courses you could build like a big
30:01 - engine and you could have like a you
30:04 - know a whole bunch of people rating the
30:05 - challenges yeah yeah um I think like if
30:09 - I were to do this in a a more structured
30:12 - way like not my University there's a lot
30:15 - of people who have already like made
30:17 - like public GitHub repositories doing
30:19 - exactly this where they're like okay I
30:21 - did like this course and then this
30:23 - course and then this course and this is
30:25 - how I felt about it and I probably check
30:27 - out a couple of those before like
30:29 - embarking on something is like sort of
30:33 - timec consuming and like commit
30:36 - commitment level I think is
30:38 - also for a long-term path a little bit
30:42 - higher than just like picking one course
30:44 - yeah but it's also like to some extent
30:46 - there's no commitment right like I think
30:50 - there's a lot of things I've bookmarked
30:51 - in my Chrome tabs where it's like I want
30:53 - to read this article one day or like I I
30:56 - don't know if you have a pocket for
30:59 - so many articles to pocket that I've
31:01 - never read um so I think that that's
31:04 - also a challenge is like how do you get
31:06 - from that crucial point of that looks
31:09 - interesting at some point I want to read
31:11 - this article or watch this video and my
31:12 - my YouTube watch later thing is like 400
31:15 - videos that are like a lot of them are
31:17 - free code Camp courses I have to go and
31:18 - clear them out periodically because it
31:20 - just becomes meaningless like I just
31:21 - have this big heap of undifferentiated
31:24 - things that at one point I thought were
31:25 - a good idea to consume you know and I
31:29 - think like that's a good resource to
31:32 - have available and so the question then
31:34 - becomes like how do you sort of flick
31:37 - the switch go from zero to one of like
31:39 - okay I added this to something I want to
31:41 - do how do I actually start doing it and
31:44 - that
31:45 - is often like really just a matter of
31:48 - like personal drive to do something like
31:51 - how excited you are about it like if
31:54 - you're feeling like something is a chore
31:56 - it's not something that you like you're
31:57 - doing it
31:59 - for like because you think you should
32:01 - instead of because you want to it's a
32:03 - lot harder to do it and so at that point
32:06 - it's maybe a good idea to like reflect
32:08 - on on just like why am I doing this um
32:10 - which is certainly something I've done
32:12 - in the past as well yeah well maybe I
32:15 - mean you're somebody who has uh probably
32:17 - a pretty good information diet I would
32:19 - imagine you're you know pursuing your uh
32:23 - computer science degree at Yale uh maybe
32:26 - we could talk a little bit about going
32:28 - to Yale and what inspired you to go
32:30 - there because you know I know that
32:32 - you've got you probably got into lots of
32:34 - universities including a lot of
32:35 - engineering universities like Caltech um
32:39 - what inspired you specifically to go to
32:41 - kind of a stodgy old Northeastern
32:43 - liberal arts college instead of going to
32:46 - college
32:47 - yeah yeah I mean so I I chose to go to
32:51 - Yale over schools that are perhaps like
32:53 - better known for engineering or computer
32:55 - science programs on the West Coast Coast
32:58 - because
33:00 - I like also like really love a lot of
33:03 - the liberal arts classes like there's a
33:05 - there's a lot of like smaller things
33:07 - that added up to it um but I would say
33:10 - the big thing is I personally believe
33:13 - that a computer science curriculum is
33:15 - pretty standardized across all colleges
33:17 - yeah like it's the same data structures
33:20 - it's the same algorithms you're not
33:22 - learning like
33:23 - necessarily like a a novel you know
33:27 - binary tree or something if you choose
33:29 - to go to an engineering school to the
33:31 - same extent and a lot of the curriculum
33:33 - here is also like adapted from other
33:35 - schools and they like do acknowledge
33:37 - that as well often so it's a good enough
33:39 - CS program but it also has everything
33:42 - else that you get from attending a
33:44 - liberal art school and you know
33:46 - philosophy and emology and and different
33:49 - topics like that right computer science
33:52 - rankings if you go to like CS rankings.
33:55 - org. I don't remember which website it
33:56 - is but but there's like there's a lot of
33:57 - websites that like keep track of like CS
33:59 - rankings in the United States those are
34:01 - based on two things which are also kind
34:04 - of the same thing which is the amount of
34:06 - research that gets output by that
34:08 - University which has nothing to do with
34:10 - teaching field yes which is absolutely
34:13 - no like at least in my experience like
34:17 - research being a really good researcher
34:19 - does not mean you're a good
34:20 - professor um and so like a school like
34:23 - CMU for example is very high up on there
34:26 - because they just have
34:27 - number of insane number of Cs faculty it
34:31 - is objectively just an incred school to
34:33 - go to for computer science it's super
34:35 - rigorous it's a great program um but
34:39 - like I don't I think as an
34:40 - undergrad like for me at least having
34:44 - like the highest CS ranking wasn't the
34:47 - most important thing to me like I also
34:48 - spent a lot of time thinking about like
34:51 - the experience of of being a student
34:53 - there um and I'm not talking
34:57 - specifically about any one school here
34:59 - versus any one other school um and I
35:01 - think that's a personal decision that a
35:03 - lot of people have to make for
35:04 - themselves for me at least like I always
35:07 - thought I was going to go to an
35:08 - engineering school back in in high
35:11 - school and it was literally like I went
35:13 - to the pre-orientation days for for Yale
35:16 - and I met some awesome people
35:20 - and I realized like that that was just
35:22 - where I wanted to to spend the next four
35:24 - years um and so it wasn't n
35:27 - a it wasn't really about computer
35:30 - science I guess was my answer which I I
35:33 - I don't know if that's necessarily like
35:34 - a satisfying answer yeah but I mean that
35:37 - makes sense if if you look at like
35:39 - computer science curricula as like it
35:40 - meets a certain threshold of quality and
35:42 - then it doesn't have to be the absolute
35:44 - best I think people get preoccupied with
35:47 - optimizing for one dimension when really
35:49 - you want your little character Pentagon
35:51 - to like you know be more
35:53 - three-dimensional I guess uh I mean like
35:55 - I think it matters a lot more for like
35:57 - Master's programs grad school like
36:00 - certainly then I would spend a lot of
36:01 - time considering um okay like what is
36:04 - this and but then even then it's not
36:06 - really about the school it's about the
36:08 - specific Pi that you're doing research
36:11 - with um principal investigator like the
36:14 - main principal investigator okay I
36:16 - didn't even know that
36:17 - acronym okay it's it's like the the main
36:20 - person in a lab who it like that could
36:23 - be at any school and often it's schools
36:25 - that you like wouldn't necessar like a
36:27 - school like y or Harvard might not be
36:29 - the best school for a particular area of
36:31 - research um and so then it really is a
36:36 - lot more about like doing your research
36:38 - on what kind of um I guess research you
36:42 - want to be doing and then rankings I
36:46 - guess within computer science are more
36:48 - likely to correlate to like the school
36:50 - that you're going to do research on but
36:52 - that's not really something I spent a
36:54 - lot of time considering for undergrad if
36:56 - that makes sense um yeah so just to kind
36:59 - of recap what you just said like first
37:00 - of
37:01 - all CS programs are more similar than
37:04 - they are different generally if you're
37:06 - going to like a State University or if
37:08 - you're going to a good private
37:10 - university like Yale um and you can
37:14 - consider other things like the campus
37:16 - experience I mean uh you're getting to
37:18 - live on a you know 3004 year 100 year
37:21 - old campus I don't know how old Y is
37:23 - it's pretty old um it's more than three
37:26 - like yeah it was founded in I think 1701
37:28 - so older than the US yeah yeah so so
37:32 - you're getting to like and they probably
37:34 - literally got ivy on some of the
37:35 - buildings it's part of the ivy league um
37:39 - so I can imagine that experience and
37:41 - also you're from the south like I am
37:44 - you're from uh Florida and getting to
37:45 - live up there in is is it in New England
37:50 - I'm I'm it's in yeah yeah I'm lying a
37:52 - lot of my ignorance about the Northeast
37:54 - I've never lived up there but
37:57 - um it's about like an hour and 50 minute
37:59 - train from New York city so yeah it's in
38:01 - Connecticut um I grew up in South
38:05 - Florida never lived in the northeast or
38:07 - anywhere there was snow or it got cold
38:09 - so this is a total shock to me as well
38:13 - did you have one of those experiences
38:15 - like oh my first time seeing snow do you
38:16 - remember that I did have a magical
38:19 - experience the first time it snowed here
38:21 - yeah it was um it doesn't snow that
38:23 - often unfortunately um with like other
38:27 - people are like yay and I'm like I want
38:29 - more snow um but that's just the like
38:35 - inner child in me I guess
38:37 - I I think that what I would say is I
38:40 - have never been like I've never been a
38:43 - student at a at a State
38:45 - University um so I can't compare like
38:48 - I've never been a student at other
38:49 - schools so I can't tell you that ex
38:51 - school I can't tell you they're exactly
38:53 - the same I can't tell you that they're
38:55 - similar I can just say based on like my
38:57 - perception of things I'm very happy with
39:00 - how all of this stuff sort of correlates
39:02 - I do know that I have friends at schools
39:04 - that are like like for example like at
39:05 - Stanford um where they do have like
39:09 - courses that yell doesn't offer in some
39:12 - areas like there there's definitely The
39:15 - Core Curriculum is very often very
39:17 - similar but then there's like elective
39:19 - classes that are really awesome if you
39:20 - go to a school that's a lot more focused
39:23 - on computer science for example um but
39:26 - you know there's pros and cons with
39:28 - everything so that would sort of be my
39:30 - breakdown of of computer science at at
39:33 - different universities and and I'll just
39:34 - say like a lot of people watching this
39:36 - are or listening to this uh are probably
39:39 - thinking like why are they talking about
39:41 - un like I I'm like in my 30s I'm in my
39:44 - 40s I don't care about universities move
39:46 - on to something interesting quy but I
39:48 - take university courses all the time uh
39:51 - like I I audited courses I'm a 43y old
39:55 - dude right like why am I doing this
39:57 - because I like learning and I think that
40:00 - it's like a huge Competitive Edge that
40:01 - so many other people are you know
40:04 - sedentary and like oh I'm just going to
40:07 - tune out and like watch Netflix or
40:09 - something and I'm sitting here learning
40:11 - as much as I can right uh anyone can
40:14 - benefit from the courses that yell is
40:17 - publishing online right and and like
40:19 - certainly I've I've taken a lot of
40:20 - Stanford courses uh computer science
40:23 - courses over the years because I just
40:25 - studied at a state school
40:26 - and I don't have a computer science
40:28 - degree I actually have a degree in
40:29 - English right because I was an English
40:31 - teacher so um just because we're talking
40:34 - about universities and and you may be
40:36 - past University age don't don't
40:37 - completely tune out of this section
40:39 - because there's a lot to be learned and
40:41 - a lot of the benefit of having all these
40:44 - great teachers in one place teaching
40:47 - courses and being able to publish them
40:49 - on the Internet like we can reap the
40:50 - benefits of that too even if we're in
40:52 - our 30s and 40s and and if you're
40:55 - somebody who's High School age and
40:57 - you're considering going to get a
40:59 - University degree
41:02 - um we've got the guy who created College
41:05 - compendium right here maybe you could
41:07 - give some advice to people who are like
41:09 - just some quick advice to people who are
41:11 - younger like you know maybe 16ish uh
41:14 - thinking about what they're going to do
41:16 - in the future whether they should go to
41:17 - university do you think people should
41:19 - still go to
41:20 - university I mean I'm biased because I I
41:23 - am currently a university um I think
41:26 - like there's a lot of factors that go
41:28 - into that decision um and like some of
41:33 - them are personal some of them are
41:35 - family some of them are Financial some
41:37 - of them are like what your goals are um
41:40 - to do long-term in life I think that
41:42 - going to University opens a lot of doors
41:44 - like having a University degree does
41:47 - help you get a job after the fact
41:49 - especially in computer science um but
41:52 - you don't
41:53 - necessarily like need to do that there's
41:56 - no like one path in life um yeah and you
41:59 - don't have you certainly don't have to
42:01 - go to any one school there's like this
42:03 - idea I think in in high school that like
42:08 - there's um like this this track Like
42:12 - they definitely push you in high school
42:13 - to apply to college as like sort of the
42:15 - next PATH um and like for a lot of
42:20 - people that is the right choice um and
42:22 - for some people it's not and that's like
42:24 - a personal choice you kind of have to
42:25 - make
42:27 - um but certainly what I would say is
42:29 - like it's worth it to learn computer
42:31 - science regardless of if you're doing
42:34 - either of those things like I think that
42:37 - a lot of the computer science that I
42:39 - learned I learned before coming to you
42:42 - um and it wasn't like in a high school
42:45 - classroom necessarily either it was
42:46 - through platforms like free code camp
42:48 - like solo learn um like learn being a
42:52 - mobile app that people use sometimes uh
42:55 - to supplement free Cod Camp I've talked
42:57 - with the founder uh yeah yeah there's a
43:00 - there's I mean there's literally a
43:02 - million amazing free resources to learn
43:05 - like literally anything you want to
43:06 - learn not just in computer science but
43:08 - like in any field um like available
43:11 - already on the web and so I think it it
43:15 - really just boils down to like what are
43:17 - you interested in learning about and
43:18 - then actually pursuing that like
43:20 - something that I I've learned recently
43:23 - is there's actually a Wikipedia app um
43:26 - mhm and this Wikipedia app will just
43:28 - show
43:29 - you information about all kinds of
43:32 - historical events that happened on like
43:34 - today like each day and it's just sort
43:38 - of like a simple random way too that you
43:40 - can just discover a lot more knowledge
43:42 - than you like previously had um and so
43:46 - like I think if you're a curious person
43:48 - there's a lot of resources like that
43:50 - that exist that are free and even if
43:51 - you're not that curious it's it's a
43:53 - supplement to your curiosity because I
43:54 - wouldn't necessarily think to learn
43:56 - about for example the Tower of knowledge
43:59 - I was in uh Pittsburgh and there's this
44:01 - University that built this they wanted
44:02 - to have like an educational skyscraper
44:05 - and it was so intriguing to me like
44:06 - what's the Tower of knowledge like I
44:08 - never would have thought that but I just
44:09 - happened to uh be reading like the
44:11 - Wikipedia article of for Pittsburgh and
44:14 - it just mentioned that and I was like oh
44:16 - what's that and I I like learned all
44:18 - about this fascinating structure uh the
44:20 - tallest educational building in the
44:22 - world basically built maybe like 30 or
44:24 - 40 years ago maybe 50 years ago uh but
44:26 - it's like so intriguing and you know
44:29 - this this information will just come to
44:31 - you if you keep your antennas out I'm
44:32 - hoping that at least some of the things
44:34 - that you're hearing just here on this
44:36 - podcast you know inspire you to go and
44:38 - learn more about topics one topic though
44:41 - that we do want to talk about absolutely
44:44 - is ethics and everybody's eyes are
44:47 - probably glazing over oh my goodness AI
44:50 - ethics and machine learning right like
44:53 - most computer science programs and we
44:55 - did like a a survey of the top you know
44:57 - 50 or so computer science program most
44:59 - computer science programs this was part
45:01 - of us essentially assembling the Freo
45:03 - Camp University degree program and
45:05 - figuring out what to teach to whom and
45:08 - uh Sam Sam cromby was also involved in
45:11 - this effort and huge thanks to him we're
45:12 - going to have him on the podcast in the
45:14 - near future as well
45:17 - but essentially ethics is completely
45:20 - under taught in my humble opinion it it
45:23 - it's it's almost kind of like a
45:24 - punchline a joke like oh business
45:25 - business ethics as as though there are
45:27 - any ethics and something is Cutthroat as
45:29 - business right or AI ethics as long as
45:31 - though computer scientists really care
45:33 - about that but actually a lot of people
45:35 - do care about uh AI ethics and there are
45:39 - a ton of ethical considerations and it's
45:42 - not just um thinking in terms of like
45:46 - what does it mean to be a good AI right
45:49 - A lot of it is there's real world risk
45:53 - because these these models are being
45:54 - deployed and and you just created this
45:57 - entire 2-hour course about it and uh
46:01 - unfortunately it was like one of your
46:03 - least popular course it's just because I
46:05 - think there's not a lot of interest in
46:06 - ethics but maybe you could use this
46:09 - moment to kind of like give people an
46:12 - idea of why ethics is important for
46:15 - programmers and Computer Sciences
46:17 - scientists especially when they're
46:18 - working with AI
46:21 - models I think I would frame the course
46:24 - as sort of
46:26 - everything you need to know to have like
46:28 - a conversation about AI like maybe like
46:32 - half the course is about ethics a lot of
46:34 - the course is also about just like what
46:37 - an neural network is and like how
46:41 - to talk about a lot of these
46:43 - philosophical questions that are really
46:45 - interesting about artificial
46:46 - intelligence and which like make for
46:48 - interesting conversations like the black
46:49 - and white room experiment or um the
46:52 - Chinese room experiment is another
46:53 - famous One um
46:56 - the ethical questions are also there and
46:59 - also important and I think the reason
47:03 - they're important is because of the just
47:05 - sheer impact that these tools that we're
47:08 - building have um like we
47:12 - have what's really amazing about
47:14 - computer science like in my in my
47:16 - opinion the thing that I think is the
47:17 - the coolest about learning to code is
47:20 - the fact that you can like make a
47:21 - website or make an app or make some
47:23 - application something with code
47:26 - that it impacts literally every person
47:28 - in the world who has internet or like
47:31 - who has access to that yeah you can push
47:33 - free C Camp and anybody can open any
47:36 - device that has a browser and they can
47:38 - go there and they can consume that
47:40 - instantly and that's just insane like
47:42 - that is amazing and something that like
47:47 - is by far to me the coolest thing about
47:49 - like any of of programming or the
47:52 - Internet is just the sheer level of
47:53 - impact you're able to have like if I
47:56 - taught the same I did teach this class
47:58 - this AI ethics and machine learning
47:59 - course that I taught um for free free
48:02 - code Camp I taught it to a small group
48:04 - of like 20 high school students um
48:06 - before that and I spent a lot of time
48:09 - teaching them this course and I'm sure
48:12 - like it was really cool to have that
48:13 - experience of teaching this like in
48:15 - person um but maximum 20 people can ever
48:20 - experience that versus like
48:22 - theoretically anyone who wants to you
48:24 - the viewer can watch this video course
48:26 - if you're interested and that's really
48:29 - special to me um but what it also means
48:32 - is that whatever you say in that course
48:36 - whatever um you put in that website or
48:39 - application or how it's designed even if
48:42 - you're like not really thinking about it
48:43 - or it's accidental or it's incidental
48:47 - um can have a really big impact cuz
48:50 - there's like a a butterfly effect um for
48:53 - really small things that that are like
48:57 - really really dangerous at scale that's
48:59 - why like programming and and design of
49:01 - good programming it's really important
49:04 - to think about for example like when
49:05 - you're taking a class on data structures
49:08 - that things run in like you know o of
49:11 - one or o of nend time like you want your
49:13 - time complexity to be low not because it
49:15 - really matters when things are small but
49:18 - because it really matters when things
49:20 - are big and you can apply that same idea
49:24 - or principle to
49:26 - AI applications and deploying them in
49:29 - real world environments like if you're
49:31 - building a self-driving car yes we
49:35 - should definitely talk about this
49:36 - because Cruz has been in the news and
49:38 - apologies to anybody who works at cruise
49:40 - but like that company was a total
49:43 - disaster based on what I've read about
49:45 - it like I've never
49:47 - seen uh well that that's hyperbole I
49:50 - I've seen very few companies that are so
49:52 - flippant with safety uh I mean it's
49:57 - amazing but uh just to give you some
49:59 - context uh I heard uh that in this
50:03 - report that was conducted and one of the
50:05 - reasons Cruz is no longer in operation
50:07 - it may not ever be in operation again uh
50:10 - I did not know that yeah really they
50:11 - took all the cars off the road after
50:13 - billions and billions of dollars in
50:14 - investment in San Francisco right yeah
50:16 - yeah so what happened was um uh a woman
50:20 - was like bumped by a car hit by a car
50:23 - whatever she flew over into the path of
50:25 - the crew Cruise car and the cruise car
50:28 - basically dragged her for like 20 or 30
50:30 - ft while it was trying to pull over cuz
50:32 - that was what his programming was is
50:33 - like oh there's been an accident I need
50:35 - to pull over so he's dragging this woman
50:37 - who's in critical condition now I think
50:39 - she did live but you could imagine how
50:42 - like some decision like oh if there's an
50:44 - accident just pull over there's no
50:45 - consideration for like what if
50:47 - somebody's like right in front of the
50:48 - car or like you know how how can you be
50:50 - sure I'm I think that they probably had
50:53 - sensors in place to know whe whether
50:55 - somebody was being dragged like huh
50:57 - that's weird we're like driving really
50:58 - slow or the is the parking break engaged
51:00 - or something I don't know but
51:03 - um it came out that like their AI system
51:07 - had difficulty identifying small
51:08 - children and yet they were in production
51:12 - like the cars were on the road so you
51:14 - can imagine like if they hadn't been
51:15 - pulled lots of kids might have gotten
51:17 - hit by these cars I mean like they
51:19 - weren't just on the Road Quincy I rode
51:21 - in one like the last time I was in San
51:23 - Francisco I've been in self-driving
51:26 - Cruise like literally like God in one
51:29 - I'm sitting in the back there's like a
51:31 - panel in between you and the wheel and
51:33 - it's just going on its own yeah that was
51:36 - a pretty crazy experience it was
51:37 - actually it was really funny cuz we were
51:39 - trying to go from one like location to a
51:43 - location that was maybe like five blocks
51:45 - away and it did the most circuitous
51:47 - route it possibly could have like I
51:49 - think it was like a 40-minute ride for
51:51 - like something that could have been like
51:52 - a 10-minute ride um but I think it's
51:54 - maybe sometimes you look at Google Maps
51:56 - and you're like why is it taking me all
51:57 - the way over there well it's just the
51:59 - software is not perfect right a human
52:02 - Taxi Driver would be like a lot of yeah
52:05 - SF has a lot of really like difficult
52:08 - roads that I think they just try to
52:10 - avoid in general so that is my
52:12 - assumption for why it don't ever drive
52:14 - the
52:15 - street yeah they really wind you one um
52:19 - so all that to say like I I
52:23 - definitely think I like is a very
52:26 - obvious example of how like an
52:27 - autonomous vehicle um which granted is a
52:31 - really difficult thing to program um
52:34 - like I like yes like it you know
52:37 - obviously that's a really bad situation
52:40 - um that they they should have been
52:42 - pulled for for but it's also
52:47 - like it's it's a very difficult thing to
52:50 - build a self-driving car absolutely 100%
52:53 - that is perfect right out of the gate G
52:56 - and I guess like the the the thing of AI
52:58 - safety AI ethics um it is important
53:01 - because it doesn't matter as much when
53:04 - they're really really small numbers of
53:07 - those cars on the road just in like one
53:09 - city right yeah but when you scale it
53:11 - like like time complexity scale it up
53:14 - you have millions of people taking
53:16 - rides it starts to get a lot more
53:18 - dangerous and even though like in theory
53:20 - self-driving cars could already be safer
53:22 - than human drivers there's all of these
53:24 - like complex questions about
53:26 - accountability like who's responsible
53:28 - when things go wrong like if it's a
53:30 - human driver we know who's in the wrong
53:33 - always like it's one of the drivers for
53:35 - the most part um but when there's a
53:37 - self-driving car involved it adds all of
53:39 - this extra complexity that we haven't
53:40 - really figured out yet to my knowledge
53:43 - like how that's going to work um and so
53:48 - there there's like a
53:50 - lot still left I guess yeah there's a
53:53 - lot of real world encroaching on this
53:55 - beautiful engineering problem isn't
53:57 - there yeah I mean because the real world
54:01 - is complicated and like in a perfect
54:03 - world autonomous vehicles could probably
54:05 - be ready to like drive like today yeah
54:08 - the the difficulty is human drivers and
54:12 - human pedestrians are like really
54:14 - difficult to predict like some guy might
54:16 - just fall asleep and swerve into the the
54:18 - car or like some pedestrian might decide
54:21 - they're going to make a a run for it
54:23 - across the street even though the cars
54:25 - are going and it's a red light for them
54:28 - because that's what humans do and it's
54:30 - really difficult for cars to predict all
54:33 - of those possible outcomes and then also
54:35 - make the choice that like saves the most
54:37 - number of people or like or or protects
54:41 - the do you protect the person in the car
54:43 - at all costs do you protect the most
54:45 - number of people there's all kinds of
54:47 - interesting questions as well within
54:49 - that like that can be thought about I
54:54 - think interesting ways and I actually
54:56 - don't think the trolley problem is that
54:58 - interesting of a question here yeah I I
55:00 - don't either but I do some of these
55:01 - things uh you you have to choose whether
55:04 - you want to just leave the switch alone
55:05 - in which case this train is going to go
55:07 - and kill five people or if you want to
55:08 - hit the switch in which case this train
55:10 - is going to dout and it's going to go
55:12 - kill one person but when you do that
55:14 - when you hit that switch technically
55:16 - you're sentencing that person to death
55:18 - right whereas in action you can kind of
55:20 - like oh it wouldn't me but so it's kind
55:22 - of like a philosophical question but one
55:24 - thing that I loved about the web comic
55:26 - is how people imagine it and it's them
55:28 - standing at the switch trying to decide
55:30 - what to do and how it actually is going
55:33 - to be when when like things are deployed
55:36 - at scale and you've got all these
55:37 - systems making these decisions is you're
55:39 - one of the people on the track and
55:41 - somebody else is making the decision or
55:42 - some AI engineer has already kind of
55:44 - made the decision in the code as to what
55:46 - is supposed to happen in this situation
55:48 - and you're the one who's going to die
55:49 - for their decision that's one of the
55:51 - reasons that ethics is so important is
55:53 - because we want to make sure software
55:55 - Engineers who are designing these
55:56 - systems like self-driving cars care
55:58 - about it and think about think things
56:00 - through and have a framework for you
56:03 - know dealing with the messy reality the
56:06 - complexity of reality yeah and I mean
56:09 - like these things are also like not
56:11 - everything is life and death but since
56:13 - we're on the topic of life and death
56:15 - like one of the to elaborate on my my
56:18 - major one of the and why I came to Yo
56:21 - actually yo is one of the the best
56:23 - schools in the world for spefic
56:24 - specifically politics and like
56:26 - geopolitics International politics um
56:28 - and my my four major is computer science
56:30 - and Global Affairs so I did some work I
56:34 - do do some work in the intersection of
56:36 - those things um and like over the last
56:39 - year I've done some work in like AI
56:41 - policy area and I had the opportunity to
56:45 - learn a lot about a lot of the things
56:48 - that people consider like existential
56:50 - risk like what are like some of the
56:52 - really dangerous things that exist with
56:54 - with an AI that you actually should be
56:56 - scared about as someone who lives in a
56:59 - free
57:00 - democracy um whether that's you know
57:03 - potential micr targeting that could
57:05 - influence elections in the next like 6
57:08 - to 12 months or even something like a a
57:12 - bioweapon that is like a another Co that
57:16 - could be easily created by some of these
57:18 - models yeah AI is speeding up drug
57:21 - Discovery but it's also speeding up
57:22 - discovery of other things that you don't
57:24 - necessarily want to discover ex exactly
57:27 - like there's like sort of a like it's
57:29 - such a powerful tool and like all
57:33 - tools it's you know like a hammer you
57:35 - can use it for good or you can use it
57:37 - for evil and we have to like do our best
57:41 - to make sure that these tools are
57:42 - deployed in ways that are like ethically
57:45 - responsible and like don't harm the
57:49 - human race like as much as possible um
57:52 - and not everything is like about like
57:55 - the singularity like this idea like oh
57:57 - no get all the headlines I mean a lot of
57:59 - people but you and I I don't think we've
58:01 - ever talked about the singularity in our
58:02 - many conversations I think we have I
58:04 - don't think we've ever talked about
58:05 - Skynet taking over the world and killing
58:07 - everybody uh not everything is movies
58:09 - like I think the the the real world ways
58:12 - in which AI is applied are much are very
58:15 - are also really important and less
58:16 - discussed um for example in hiring and
58:21 - in um recruiting like I is examining
58:25 - resums it's like a first pass now in a
58:27 - lot of places um and there's a lot of
58:30 - studies for example there was a study
58:32 - that that Amazon had where like they're
58:34 - they were using AI in their recruiting
58:36 - process and it was in some way
58:39 - discriminating against different groups
58:40 - of people without like people really
58:42 - even realizing it because the problem
58:45 - with AI systems in general and this is
58:48 - like a problem that you can really boil
58:50 - down almost all of ai's problems into um
58:54 - from an eth eal question is that they're
58:56 - black boxes which is to say we really
58:58 - don't understand in a lot of cases why
59:01 - they make the decisions they do yeah um
59:05 - and we also don't therefore have a way
59:11 - to like
59:13 - just we we can't argue with an AI when
59:17 - it makes a decision if we don't know why
59:18 - you can't even ask it why it made the
59:20 - decision it did and it might just
59:23 - lie understand decision he made it might
59:27 - just come up with some kind of like
59:29 - postao like story to like oh oh I gave
59:33 - you this recommendation because what can
59:35 - I say here that'll sound plausible you
59:38 - know well it's like when you use Chachi
59:40 - BT right to do math it's actually pretty
59:43 - good at doing math now or writing code
59:45 - but that's because it's like
59:48 - also like calling a specific function to
59:52 - like run MTH problems and code that part
59:55 - of it versus just doing it using what it
59:57 - was originally doing when you first
59:59 - launched chat gbt which was just like
60:02 - the same sort of text generation it was
60:04 - always doing um and that's because there
60:06 - was like a lack of understanding of what
60:08 - the math actually represented like you
60:10 - can think of these AI models
60:11 - specifically within the large language
60:12 - model context as like really really
60:16 - really really good
60:18 - autocomplete and it's really good like
60:22 - almost like it's magical almost um until
60:25 - it isn't and the more you use it the
60:27 - more you realize that there's sometimes
60:30 - like a lack of understanding that's
60:32 - really critical um and that problem is
60:37 - getting resolved in multiple different
60:39 - ways like it's way more accurate now
60:41 - than it was when it originally launched
60:43 - yeah um but it's also something where
60:46 - you can liken it there's a really great
60:48 - article that was written by the author
60:51 - of this book um claraa and the sun I
60:53 - think or maybe it the son maybe it was
60:56 - the over story I'm not 100% sure which
60:58 - which author wrote it um I had give me
61:00 - the link to that and I'll add that to
61:01 - the the video description SL podcast
61:04 - show notes yeah it was about how chap PT
61:07 - is kind of a blurry jpeg of the internet
61:10 - yes I read that I read that that's a
61:11 - really you read that you read this
61:12 - article yeah great article um because
61:14 - it's also
61:15 - supered lossy yeah it's a loss it's not
61:19 - a lossless um program and so what you're
61:23 - seeing when it Auto completes is kind of
61:25 - like it trying to like recreate the
61:28 - space between um and sometimes it does
61:30 - that well and sometimes it doesn't and
61:33 - that is also what's happening in a lot
61:36 - of these cases with AI models making
61:38 - making decisions where we are trying to
61:41 - like understand how they're coming to
61:43 - those conclusions and and we don't
61:44 - always understand that um and so like
61:47 - when it comes to AI policy AI ethics AI
61:50 - safety it covers so many different
61:52 - things right from Deep fakes to
61:53 - disinformation to um you know this like
61:57 - existential risk of bioweapon to or just
62:00 - more mundane stuff like really
62:02 - empowering scammers like AI can just you
62:06 - can scam at scale now in a way that you
62:09 - could never do if you had like a call
62:11 - center of scammers uh Opera it's really
62:14 - good at writing spam it's really good at
62:16 - helping people like cheat on their
62:19 - plagiarize yeah um because it's hard to
62:22 - detect and they ways of circumventing
62:25 - the AI detection algorithms that exist
62:28 - and AI is very bad at detecting AI any
62:31 - AI if you're a school if you're like a
62:34 - you know a Chief Information officer at
62:36 - like a university and somebody's trying
62:37 - to sell you AI detection AI plagiarism
62:40 - detection software run the other way
62:42 - that stuff does not work everybody I
62:44 - talk to says it's basically a crapshoot
62:46 - and it's just a cash grab like
62:49 - because I wouldn't describe it as a cash
62:52 - grab but I would describe it as a cat
62:54 - game where like each of them are like
62:57 - constantly getting better at fighting
62:59 - the other thing in the same way that
63:01 - like a vaccine or like a medicine um
63:05 - over time the pathogens learn to resist
63:08 - the medicine and you have to come up
63:09 - with a new I that may be the case but in
63:12 - the case of gbt 4 they people ran a
63:15 - bunch of uh like a bunch of academics
63:18 - ran a bunch of tests on gp4 and said
63:20 - like hey was this content generated by
63:22 - gp4 and even the most powerful llm ever
63:25 - created could not reliably detect AI
63:30 - generated text so it makes depends on
63:33 - how deep you want to get into that but
63:35 - there's like a couple things they're
63:36 - using to actually detect that um yeah
63:40 - let's go deeper into that because I'm
63:41 - very interested in that as a teacher uh
63:43 - and somebody who talks with teachers all
63:45 - the time and tries to you don't want to
63:47 - have false positives where you're
63:49 - blaming your students for plagiarizing
63:51 - when they didn't it's much better you
63:54 - know I was it Ben Franklin that said I'd
63:56 - rather like 99 men go free than innoc
64:00 - guilty men go free than one innocent man
64:03 - you know face execution or something
64:04 - like that right like you want to air on
64:06 - the side of caution and you don't want
64:08 - to like because it's incredibly
64:11 - demoralizing and disruptive to people
64:13 - when they're taking like some sort of
64:14 - exam and like you've been flagged as a
64:16 - cheater by this AI or um people
64:19 - submitted an essay and their teachers
64:21 - like yeah it looks like it was
64:22 - plagiarized but you didn't plagiarize
64:24 - can you imagine how mading that would be
64:25 - you worked your butt off to like create
64:28 - this you know uh term paper and some
64:31 - algorithm written by some you know
64:33 - edtech company education technology
64:35 - company is like oh we we feel 95%
64:39 - confidence interval that you cheated and
64:41 - like now your ecademic record is Tainted
64:44 - just because somebody did a bad job of
64:45 - coding software uh yeah I mean uh maybe
64:49 - you can talk about the state of
64:51 - plagiarism
64:53 - detection yeah I mean so I think you're
64:56 - right like all of this is really boils
64:58 - down to um type one and type two error
65:02 - which is if you're not familiar with
65:04 - Statistics false positives and false
65:06 - negatives right um and actually that's
65:09 - not just the issue at hand for
65:11 - plagiarism detection that's the issue at
65:13 - hand for all AI stuff like all of the
65:16 - these questions of like for example like
65:20 - do you want more of um
65:26 - like what are you trying to select for
65:27 - what is worse like would you rather have
65:29 - someone not get hired because the AI is
65:31 - screwed up or would you rather have the
65:33 - AI like select for this thing over some
65:36 - other situation um there there's like a
65:40 - lot of complexity that exists when
65:43 - you're trying to deal with systems that
65:45 - are really really really um black boxy
65:49 - like yeah um yeah I mean there's no this
65:52 - is the fundamental problem in AI of
65:54 - interpretability like how did the AI
65:57 - come to this conclusion and because it's
65:59 - an inscrutable neural
66:02 - network we don't well people are working
66:04 - on this this is something that's getting
66:06 - a lot better already like this is
66:08 - because it is such a big important thing
66:11 - that a lot of research is currently
66:13 - going into interpretability and it is
66:15 - something that does exist for some
66:16 - models so it's not correct to say that
66:18 - like no AI is understandable
66:21 - AI systems that we can understand yeah
66:24 - that was hyperbole so so real quick we
66:26 - have a course called No blackbox AI uh
66:30 - taught by Radu Dr rodu computer science
66:33 - Professor uh from uh I think Finland uh
66:37 - don't don't blame me ru if I can't
66:39 - remember exactly where you're from but
66:41 - uh he uh basically teaches you how to
66:44 - build an AI where you can understand
66:46 - what's going on but my understanding of
66:49 - the state-ofthe-art is inscrutable
66:52 - systems blackbox systems and
66:54 - you know um AI that we can actually
66:58 - understand how it's making decisions
67:00 - interpretable interpretable uh AI
67:03 - systems are so far behind and most
67:06 - people are just going to go with the
67:07 - most powerful option they're going to
67:09 - they're going to go with uh you know gp4
67:12 - instead of some other system where they
67:14 - can necessarily tease out exactly what's
67:16 - done and in some cases you will want a
67:19 - system where you can do that but there's
67:21 - always going to be the temptation to
67:22 - grab the much more powerful tool that's
67:24 - harder to understand and just for
67:26 - context the YouTube recommendation
67:28 - engine that you know recommends free
67:30 - Camp videos to people we don't know how
67:33 - it works and we've talked with Engineers
67:34 - from YouTube and they don't necessarily
67:35 - know how it works either they're like
67:37 - why does it's really good
67:40 - though yeah it works great but I mean it
67:44 - always that's exactly what I wanted to
67:46 - watch yeah um so I mean if you I worked
67:49 - on to to elaborate on what I meant
67:51 - before um I took an NLP class yeah
67:54 - natural Lang processing um and for my
67:57 - like final project in that class I tried
67:59 - to do some some research and work on
68:03 - making AI like chat GPT generated text
68:07 - undetectable by a lot of the biggest AI
68:11 - detection software like um I think like
68:14 - originality AI is one of them um turnit
68:17 - in.com is another gbd predates AI stuff
68:21 - they they they've long been in that
68:22 - industry of like cheating which again
68:25 - I'm a skeptic of if you haven't if you
68:27 - can't guess I'm very critical of that
68:30 - industry and I think it redu it produces
68:32 - a lot of false positives uh people who
68:34 - are well- meaning who did not cheat who
68:36 - suffer consequences because they were
68:39 - incorrectly labeled as a cheater well my
68:43 - my goal here was to see if it was
68:45 - possible to do that and also then like
68:47 - how that sort of worked um so basically
68:49 - there's two things that AI detection
68:51 - software typically looks at yes um the
68:54 - two like areas one of them is called
68:56 - perplexity like a perplexity score yeah
68:59 - maybe what that is it's like how well
69:03 - language models can like predict um the
69:06 - word sequences that an AI model will
69:09 - generate because like they're they are
69:12 - derivative and therefore like like when
69:15 - we talk about like uniqueness we kind of
69:17 - glossed over this earlier about you were
69:19 - saying like you know is there anything
69:21 - truly unique well
69:24 - it's hard to say but it is true that
69:26 - with AI models like you're able to end
69:29 - up with the same output if you give it
69:31 - the same exact input and and other like
69:35 - very specific details so might be like
69:38 - slightly different word but the general
69:39 - thrust is similar unless you crank up
69:42 - the
69:43 - temperature yes but like I guess what
69:45 - I'm saying is an AI model like I I sort
69:48 - of said before chat GPT is just really
69:51 - really really really good auto complete
69:53 - m M
69:54 - theoretically what what it's doing
69:56 - literally all of the time is just trying
69:58 - to predict the best next word based on
70:00 - the previous input sequence of words yes
70:03 - and so in theory you could
70:07 - do sort of a reverse of that and figure
70:10 - out like okay what was the most probable
70:12 - sequence of words and then if that
70:14 - matches what the student inputed then
70:16 - there's a high chance that they used AI
70:18 - for that that's one thing the other um
70:22 - like thing that they measure is called
70:24 - burstiness burstiness like burstiness to
70:27 - burst to explode kind of burstiness kind
70:30 - of it basically means like like
70:32 - repetition of repetition of phrases but
70:35 - also like when when a human writes we
70:39 - don't write sentences of equal length or
70:41 - like of of us like we usually write some
70:44 - short sentences followed by some longer
70:46 - runon sentences followed by short like
70:47 - humans don't write perfect sentences
70:49 - okay um perfect paragraphs a do burst a
70:52 - burst of Fire from a you know automatic
70:55 - weapon sorry to to use that analogy but
70:57 - that's what I think of when I think of a
70:58 - burst in terms of it's kind of like uh
71:01 - it's like a similarity score yeah it's
71:03 - like a similarity score between like
71:05 - sections or between like different
71:06 - pieces and so those two things combined
71:09 - or how like something like gbd Z Would
71:11 - detect to my understanding to be clear
71:13 - like I didn't work on their tools so I
71:15 - not 100% sure but I I sort of look at
71:20 - that as two like statistics that people
71:23 - use to me
71:24 - um How likely it is that the text is AI
71:27 - generated and the challenge here is as
71:29 - you said before AI is just getting
71:31 - better and better and gbd4 writes text
71:34 - that's way more realistic than any of
71:36 - the previous versions of and and also
71:39 - like we have open source models now like
71:41 - llama like mistol um that are getting
71:44 - better and better that will like
71:47 - eventually I think we're going to see
71:49 - sort of a like model parody or almost
71:52 - like a like there will be a baseline
71:56 - level of quality ac across these models
71:59 - very high computer science programs that
72:01 - us universities yeah one of the
72:03 - challenges humans are humans are not
72:06 - that good at English also like a lot of
72:08 - us like I write sentences with bad
72:10 - grammar all the time like and I'm a
72:12 - college student um if you're like trying
72:15 - to detect plagiarism in like an
72:16 - elementary or middle or High School
72:18 - setting like the average middle schooler
72:21 - probably isn't writing like great
72:24 - essays and so it's going to be really
72:26 - hard to tell um in a lot of those cases
72:29 - like is that actually something they
72:31 - wrote because even if they make like a
72:33 - couple tweaks it it it almost asks the
72:36 - question like do we need to change the
72:37 - way in which WR like writing this tool
72:40 - that has been around forever is that
72:42 - still the best system for like like what
72:45 - is the purpose of writing yeah it's to
72:48 - sort of help you like check that you
72:51 - understand what you're talking about and
72:53 - so the CH the problem is that like if
72:55 - you use AI to write your essay for you
72:58 - that doesn't actually prove that you
72:59 - understand the thing like the process of
73:02 - writing is a process of revision and
73:04 - editing
73:06 - and it has the same outcome which is you
73:08 - turn in this paper but it do I mean the
73:10 - actual deliverable is a piece of paper
73:12 - with words on it even though the process
73:14 - that I go through as somebody who's not
73:17 - cheating is read a bunch of Articles
73:19 - synthesize that into an outline synes SP
73:22 - that outline into anart
73:24 - whereas you know a person who's cheating
73:25 - would be like hey I need to write a term
73:27 - paper on XYZ write me a 500 word you
73:29 - know 10,000 say on this if you want to
73:33 - hear a fun story about AI plagiarism and
73:36 - then we can move on from this top yeah I
73:37 - mean I could talk about this all day but
73:39 - I don't know about the audience if they
73:40 - really care that much about it so I one
73:43 - of the things I I learned when I was
73:44 - working on on some of this last year
73:47 - um was have you heard of Unicode
73:51 - lookalikes Unicode lookalikes yes yes so
73:54 - Unicode characters uh there are
73:56 - thousands of them every Chinese
73:58 - character every uh Arabic letter every
74:01 - English character and then like emojis
74:03 - stuff like that and sometimes you'll
74:05 - find characters that look very similar
74:07 - that uh like is that period actually a
74:10 - period or what language period is that
74:12 - because like Japanese periods look
74:13 - different than English periods and stuff
74:15 - like that so characters that look very
74:17 - similar to the human eye but actually
74:19 - have different Unicode values is that
74:22 - correct that's exactly it and what's
74:25 - really interesting what's really
74:27 - interesting is this has been fixed at
74:30 - this point because they realized that
74:31 - this was a huge vulnerability these
74:33 - these plagarism detection softwares but
74:35 - something you used to be able to do was
74:37 - you could just write something in Chachi
74:39 - BT and then contrl f for like every
74:41 - instance of the letter e or the letter A
74:44 - or something and then replace it with a
74:45 - Unicode lookalike of the letter e or the
74:48 - letter a and it would like completely
74:51 - make it like the burstiness perplexity
74:53 - scores would just explode because in in
74:56 - terms of um like how unlikely it is that
75:01 - it was AI generated because AI doesn't
75:03 - generate text with Unicode lookalikes
75:06 - that's like such a unique character cuz
75:09 - they're not trained on text with that
75:11 - stuff and like AI is not actually
75:15 - looking at like the letter it's looking
75:16 - at like the machine representation of
75:19 - the letter right so um that in itself I
75:23 - guess like yeah and AI would never swap
75:26 - out the you know e for like some you
75:29 - know IPA like International Phonetic
75:31 - alphabit equivalent e or something like
75:32 - that that looks the same a human
75:35 - probably wouldn't do that either and so
75:37 - they figured that out and that was like
75:41 - that was like an exploit uh that was an
75:43 - vulnerability it was exploited that was
75:46 - probably closed hopefully by the AI
75:49 - detection industrial complex uh and uh
75:54 - as a result people are having to move on
75:55 - to different attacks just like any sort
75:58 - of security vulnerability would be
76:00 - patched like there you know Apple
76:02 - discovered that do XYZ to crack a phone
76:05 - and suddenly you know they've got a fix
76:07 - for it in the next release Auto
76:09 - downloaded and we move on and people
76:11 - keep trying to discover these you got
76:13 - all these security researchers so it's a
76:15 - similar kind of arms race to what we
76:17 - have yeah with any other security
76:20 - consideration I mean so like still in
76:22 - the vein of like talking about AI use
76:27 - cases within you know an education
76:29 - system but not necessarily in the same
76:32 - context of plagiarism I suppose it could
76:35 - be be viewed as that is that like AI
76:38 - also writes really good code really
76:40 - really good code um in a lot of cases
76:44 - it's really good at debugging your code
76:45 - it's really good at figuring out like
76:48 - what exactly is going wrong in a memory
76:50 - leak it's really good at understanding
76:52 - error messages
76:54 - um and there's all kinds of new tools
76:58 - that exist now because of that like
77:00 - GitHub co-pilot and cursor if you've
77:02 - used cursor I've heard good things about
77:04 - it yeah there's a lot of even vs code
77:08 - has Integrations now for that so if
77:10 - you're like a developer chances are
77:11 - you've seen this stuff or you've used
77:13 - this stuff and you also can use AI to
77:16 - write a lot of your code like there's a
77:18 - lot of code that exists that's
77:20 - boilerplate there's a lot of code that
77:21 - exists that
77:23 - is like writing a react function or
77:26 - something like a a hook like this is
77:29 - something that you do a lot it's
77:31 - replicable there's a lot of training
77:32 - data for it um and so it like begs these
77:36 - questions of how should this be used in
77:39 - in production settings and how can it
77:41 - like improve our workflows um and I'm
77:44 - actually very optimistic and positive
77:46 - about this because I think it's going to
77:47 - do I'm not worried about AI necessarily
77:50 - replacing a programmer um
77:54 - anytime soon because of complexity
77:58 - across like entire projects like AI is
78:02 - very good at writing one file two files
78:04 - three files um it's not yet capable at
78:09 - least I haven't seen it be capable of
78:11 - like writing an entire full stack
78:13 - project from scratch like maybe
78:15 - extremely simple proof of concept type
78:17 - applications yes like like maybe
78:18 - something simple a simple game that
78:20 - people have made a million tutorials for
78:23 - maybe a website it's not going to be
78:25 - able to build some crazy like full stack
78:27 - application um by itself in one go
78:30 - without someone who actually understands
78:32 - coding can connect the dots and so the
78:34 - way that I see this sort of playing out
78:36 - at least for me in my use of this has
78:38 - been use it to write like a function or
78:41 - two or help me understand what's going
78:42 - wrong within the code so it you can be
78:45 - like a 10x developer in terms of the
78:47 - speed of your your code output but you
78:51 - still have to have like this sort of
78:52 - higher level understanding of how you're
78:54 - connecting all of these pieces so that
78:57 - you build like software that compiles
78:59 - and runs when you're talking about these
79:01 - really large projects with like 50 plus
79:05 - files because like speaking honestly
79:08 - like that's most big projects if you're
79:10 - working at a big organization or a big
79:11 - company or a
79:13 - big personal project or startup like
79:16 - it's likely your code base isn't that
79:18 - simple and especially if a couple
79:21 - different developers have touched it in
79:22 - anytime has passed codebases get very
79:25 - large uh I mean build the context window
79:29 - of llm is pretty small even then like
79:32 - the more you give it the more confused
79:33 - it'll get by stuff it could be many
79:35 - years and many uh kind of like
79:38 - engineering breakthroughs before you
79:40 - could feed a large Legacy codebase into
79:43 - um into well people are working on this
79:46 - yeah I mean it's an active area study
79:48 - but we'll see it's very much a Never Say
79:50 - Never situation um I think like I've
79:53 - seen already some people have built
79:55 - really cool tools that you're able to
79:56 - like give a GitHub repository to and it
79:59 - like reads through the whole GitHub
80:00 - repository and figures out opening
80:02 - issues right and it's able to like
80:04 - explain your code base open issues like
80:07 - these are like really cool use cases um
80:11 - but there's a difference between being
80:12 - able to like understand a code base and
80:14 - being able to like write the whole code
80:16 - base from scratch and I don't I don't
80:19 - know I mean like I think we'll see
80:20 - improvements over time and that's when
80:22 - the really exciting things about this
80:24 - field and this this area not to spend so
80:27 - much time talking about it yeah well
80:28 - whenever new tools come out developers
80:30 - should learn them so I mean this is
80:34 - fairly meta but for this week um maybe
80:38 - not as much meta is Apple but what do
80:40 - you think of the New Vision Pro and like
80:43 - what spatial Computing means for
80:45 - Education like when I see this I I
80:48 - Envision like a couple years down the
80:50 - line or sooner being able to interact
80:52 - with like physical manifestations of
80:55 - like a binary tree or a hash table or
80:58 - like computer chip like
81:01 - abrac things that are like ideas in a
81:04 - textbook and bring them into the realm
81:07 - of like something you can you can see
81:09 - and potentially even touch um I think
81:12 - it's really exciting for a lot of kind
81:15 - of mechanical engineering things for
81:17 - sure maybe for some semiconductor
81:19 - engineering uh but it may also be a
81:21 - helpful tool for understanding you know
81:23 - abstract software Concepts like data
81:25 - structures algorithms like seeing how
81:28 - the data Cascades through a particular
81:31 - application like you know we see all
81:34 - these different Renditions of
81:37 - programming in the future uh I think the
81:39 - show Westworld had one that was pretty
81:41 - cool with like the tablets and of course
81:44 - yes uh the computer interfaces from
81:46 - Minority Report where you have to stand
81:48 - there and move your hands around all day
81:49 - and like you have to be incredibly fit
81:51 - just to use a computer and the world of
81:53 - uh Minority Report but I mean I've
81:56 - always been like very excited about
81:58 - these just from a wow perspective you
82:00 - know like watching you know Iron Man and
82:03 - seeing all the different interface you
82:04 - strap something onto your head and it's
82:06 - essentially like augmented reality and
82:08 - so you can put whatever you want in
82:10 - physical space Microsoft has done this
82:12 - for quite a while with the hollow lens
82:14 - I'm sure there are some good industrial
82:16 - applications uh I will be amazed if it
82:21 - really makes that big of a difference
82:22 - over just looking at a 2d screen like I
82:25 - can represent a lot of stuff really well
82:27 - in 2D on a flat rectangle thing that I'm
82:31 - looking at and it's true good yeah
82:35 - absolutely and you know as Sergey Bren
82:37 - observed if you hold the the screen like
82:40 - a screen that's like this far from your
82:42 - face might as well be an iMac screen you
82:44 - know in terms of like how you're looking
82:46 - at it well that's Pro kind of is right
82:49 - like I think for me like like obviously
82:52 - like
82:53 - VR has existed for like a decade now and
82:56 - you could already do these things where
82:57 - you like move around with objects and
83:00 - virtual reality like using the meta
83:02 - Quest or Oculus um but what's
83:06 - interesting about the app this new Apple
83:08 - device to me is that it's like the best
83:10 - example of um like not really AR cuz
83:15 - you're still looking at
83:16 - screens reality reality it feels it
83:20 - feels like you're still able to see your
83:23 - surroundings so you're not like in this
83:24 - enclosed view you like can kind of see
83:27 - things like in your real life I don't
83:29 - know if this is necessarily
83:31 - like it is still like very early very
83:35 - expensive but um maybe there is a world
83:39 - where that's something that people use
83:42 - for more than just entertainment um yeah
83:45 - I don't know I I think a lot of it comes
83:47 - down to like cost like yeah if you're a
83:49 - wealthy person and you want to have a a
83:54 - system on a chip like laid out in front
83:56 - of you so you can like look at all the
83:57 - different you know components of you
84:00 - know a processor
84:03 - um yeah that's great but until there's
84:06 - like a huge install base of people who
84:08 - can afford to do that who's going to
84:10 - write the software for it right that was
84:11 - like the Google Glass kind of chicken in
84:13 - the egg problem nobody was really that
84:15 - interested in Google Glass and Google
84:17 - was working very hard to try to get
84:18 - people to develop for Google Glass but I
84:20 - knew a lot of people in San Francisco
84:23 - Silicon Valley during the time that
84:24 - Google Glass was like Rising before they
84:28 - kind of abandoned the project uh and I
84:31 - don't think anybody really built apps
84:33 - for it because the install base was so
84:34 - small so you have to get like a big
84:35 - enough install base for people to start
84:38 - really building meaningful apps but like
84:40 - assuming that they can solve that
84:42 - business case problem and that there are
84:44 - really cool you know experiences that
84:46 - you can put in there I have yet to see
84:49 - like a killer app for VR like my
84:52 - understanding is is there something
84:52 - about the way the brain works and the
84:54 - ears work and stuff like you can't
84:55 - really move around in VR without feeling
84:58 - a little sick so almost all the you know
85:01 - VR type games have been like experiences
85:03 - where you have a fixed point of view or
85:05 - you're like sitting somewhere and you're
85:07 - able to look around or you're on like a
85:09 - you know a roller coaster or something
85:10 - like that because that's not going to
85:11 - make sick well you're also fundamentally
85:13 - like wearing a ski mask right like it's
85:15 - it's difficult to move around with that
85:17 - weight on your head without you know
85:18 - recognizing that you have that weight on
85:20 - your head um
85:22 - I do think there's a couple crucial
85:24 - differences with the Vision Pro because
85:26 - it's Apple that solves some of those
85:28 - problems um one of which is it's Apple's
85:32 - ecosystem already so all of your photos
85:35 - all of these applications that are on
85:37 - the app store by default are like also
85:39 - on the Vision Pro as 2D screens and
85:43 - that's not exactly building for the
85:44 - Vision Pro but it it solves a lot of the
85:46 - utility issues that I think other like
85:49 - devices that are starting from scratch
85:51 - have um
85:52 - the other one the backward
85:54 - compatibility by the way video game
85:57 - industry backward compatibility has also
85:59 - always been like a huge deal PlayStation
86:01 - 3 can play PlayStation 2
86:03 - games yeah well I actually think that's
86:05 - one of the challenges that Apple faces
86:07 - is Apple's never really been a gaming
86:09 - company and or a company that like
86:11 - people go to for games and that's one of
86:13 - the main applications of V up until now
86:16 - like Mata has a huge advantage over them
86:18 - on that specifically but you can also
86:21 - just like mirror Dem Mo your PS4 or Xbox
86:24 - to it if you really wanted to if that's
86:25 - something you want to do yeah and to me
86:29 - at least the big difference here is this
86:32 - being Apple I find it unlikely they're
86:34 - going to have and if I think I think
86:36 - they'll put a lot of money into it I
86:37 - think years of R&D are ahead like the
86:41 - iPhone wasn't a perfect product as like
86:44 - the first iteration of it it didn't even
86:46 - have an app store as the first
86:47 - generation of it but like by the time
86:49 - the iPhone 5 or the iPhone 6 rolled
86:51 - around it was like a mass Market product
86:53 - that like basically is the same iPhone
86:55 - that exists today with like some minor
86:57 - changes yeah um and that's like one of
86:59 - the best products ever made and so to me
87:01 - what I'm really excited about is not
87:03 - this really expensive Tech demo that
87:05 - exists today yeah but where's where's
87:07 - heading where is this going to be in
87:09 - five years that's I'm I'm really excited
87:11 - to try that and like I I think that the
87:15 - people who are excited about the Vision
87:17 - Pro today are like really excited about
87:20 - that future too um and you're sort of
87:23 - like thinking about spatial
87:26 - Computing not to go back to contextual
87:28 - Computing but like I think to me one of
87:31 - the coolest things I can imagine is like
87:33 - I I'm wearing context right now right
87:35 - but I I I have glasses I almost always
87:38 - wear glasses when I'm like in classes
87:40 - and stuff um what if my glasses just
87:43 - also like had a screen to the side that
87:46 - was able to show me information about
87:49 - what I'm looking at or who I'm talking
87:50 - to or what I said the last time I talked
87:52 - to this person or when their birthday is
87:56 - or is keeping notes during class or is
87:59 - up relevant information just putting a
88:01 - clock up there would be pretty awesome
88:03 - right like never needing to know what
88:04 - time or which direction am I facing am I
88:06 - facing a clock the weather Direction
88:08 - like there's so much information that
88:09 - like having a HUD um or an HUD like
88:12 - around
88:13 - yourplay yeah yeah um could be really
88:17 - valuable and like so I have a I I know
88:20 - someone at Yale um who's working on
88:23 - these like glasses that are glasses that
88:25 - basically make captions it's like
88:28 - subtitles for your life um and he's
88:31 - working with the Stanford kid to do this
88:33 - um and to me that's like a very early
88:35 - application of what like the long-term
88:37 - potential of this is yeah um which is to
88:40 - like be able
88:42 - to like have all of this additional
88:45 - information I mean well you mention
88:47 - subtitles for your life like literally
88:49 - subtitling I don't want like some
88:52 - simultaneous translation app like
88:54 - telling me what somebody's saying and in
88:56 - my ear I I'd love to just read subtitles
88:58 - when I'm talking to somebody in a
89:00 - language I don't understand or something
89:01 - you know what I'm saying like think of
89:02 - what a game that would be so helpful for
89:04 - me yeah yeah so that's all very
89:08 - interesting um where this is possible to
89:10 - go I'm glad that our yeah I mean I think
89:12 - there's tons of like obvious utility if
89:14 - you can get it to work in the form
89:16 - factor of like a pair of glasses even
89:20 - pack you get the
89:22 - all that stuff I think they'll get there
89:24 - little weird mask yeah I don't want to
89:27 - wear a weird ski mask and I also I want
89:29 - to actually be able to see things and
89:30 - not rely on the software like I'm never
89:33 - going to feel comfortable like driving
89:35 - or something if that could like short
89:37 - out and like suddenly I'm completely
89:39 - blinded or something like that cuz the
89:41 - past revision is actually like they
89:42 - render what the cameras are seeing it's
89:45 - still a screen yeah it's not a past
89:48 - through screen and and to me I'm a lot
89:51 - more interested in AR than I am in
89:54 - augmented reality virtual real I think
89:56 - for the Practical reasons of AR is like
89:58 - informational VR is like experiential
90:01 - like I I do play games uh like I'm I'm a
90:05 - big fan of like Doom Eternal and like
90:06 - the the really deep gameplay of that
90:08 - that came out a few years ago and when
90:11 - I'm playing the game it does not matter
90:13 - what like how big the screen is or
90:15 - anything like I'm I'm in trance there
90:16 - was this great uh documentary about the
90:19 - history of like industrial design that I
90:21 - watched a long time ago I can't remember
90:23 - the name of it it's I think it's by the
90:24 - same guy that did the helvetica um
90:27 - documentary if you haven't seen that
90:28 - that's a really good one but basically
90:31 - they had this like you know German
90:32 - engineer who's talking about like I
90:34 - built the perfect laptop look you can do
90:36 - this got all these different designed
90:38 - factors and stuff and he he gets this
90:40 - made and then he he gets a prototype and
90:42 - he sits down and he's using it and he
90:44 - realized very quickly like once you
90:46 - start actually using a computer all that
90:48 - other stuff just melts away and you're
90:49 - just focused on the task at hand you
90:52 - don't need like you know 4K you know
90:56 - giant screen like you don't need like a
91:00 - lot of the usability interface like you
91:03 - go back and you play an old arcade game
91:05 - from 40 years ago and it's just a joy
91:07 - stick and buttons and you can kind of
91:10 - mesh with that game just as well as you
91:12 - can with you know a modern game where
91:14 - you're using like a keyboard and the
91:16 - mouse or something like that right like
91:18 - it's really amazing how the human brain
91:21 - works how we're so able to tune in and
91:24 - tune out all the noise and really just
91:26 - focus on things so to some extent I do
91:30 - almost feel like using like VR headset
91:33 - is an over Overkill really like do I
91:35 - really need that is it really that much
91:37 - more immersive than what's already going
91:39 - on inside my brain yeah so I've always
91:42 - kind of been skep I I don't have one so
91:45 - yeah I can't really speak to that um
91:48 - yeah and in terms of like yeah there's
91:52 - an argument that reading a book your
91:54 - mind is doing all this stuff and it can
91:56 - be almost as a Mir of experience reading
91:58 - a book as it could be having like some
92:00 - VR you know educational experience now
92:03 - some people might argue that
92:06 - like they're completely different and
92:08 - the use case is completely different but
92:11 - uh it it really just depends on how into
92:14 - reading you get and how powerful your
92:16 - mind is at bringing up these
92:18 - representations and stuff like that um
92:21 - yeah yeah I mean I don't mind I don't
92:24 - want to sound like a lite or anything
92:25 - but like books are great and books are
92:28 - going to exist as long as Humanity
92:30 - exists maybe not paper books I don't
92:32 - think there's really like a strong
92:33 - argument other than like Nostalgia for
92:35 - having a physical paper book you can
92:36 - have like a nice backl screen sorry I I
92:39 - know like half the people in the
92:40 - audience are probably like ah what the
92:41 - heck quizy I love K are great yeah love
92:44 - Kindles yeah yeah but but like the
92:46 - notion of text as an interface for your
92:48 - human brain it just carries so much
92:50 - information you know people say like uh
92:52 - a picture's worth a th words well
92:56 - depends on the words depends on the
92:57 - picture you know like a few words like
93:00 - uh Ernest Hemingway wrote like flash
93:02 - fiction he had like this six-word story
93:06 - which I'm going to tell you right
93:08 - now for
93:10 - sale baby shoes never
93:14 - worn just those six words like conveys
93:17 - like an entire kind of like mood and I
93:20 - mean like it'd be hard to capture that
93:22 - in a picture in the exact same way is
93:24 - that you know the six-word story
93:28 - captures so like six words six words are
93:31 - worth a thousand pictures I mean it just
93:34 - dep if you if you think of this video I
93:36 - mean this video is
93:38 - probably tens of thousands of pictures
93:40 - right it's 30 frames per second our
93:42 - words matter a lot more than like the
93:44 - picture of me in this room with exactly
93:47 - like if you just showed the picture
93:48 - outside of context you'd be like okay
93:49 - it's quzy it's some other guy uh who
93:51 - yeah the audio matters a lot more right
93:54 - um yeah like there's so much more to
93:56 - like education and I feel like the
93:59 - education industry if you want to call
94:01 - it that like because it is
94:03 - industrialized much to my
94:05 - chrin is around chasing these things
94:08 - like oh if we could get everybody a
94:10 - hollow lens or you know we we could
94:12 - completely change the world got
94:14 - everybody a Smartboard or we got every
94:16 - kid like their own laptop or something
94:18 - like that and that's not necessarily
94:21 - like there's a lot of technological
94:24 - solutionism in education and I'm much
94:27 - more of a back to- Basics kind of person
94:29 - like I I obviously I see the value in
94:30 - video see the value in multimedia and
94:33 - stuff like that not just reading
94:34 - textbooks but I also see the value in
94:36 - going up to a whiteboard with somebody
94:38 - and sketching out you know things and
94:40 - and learning from them in that way too
94:43 - uh so the old ways are not necessarily
94:46 - you know abandoned you can just expand
94:48 - your repertoire but I would absolutely
94:50 - say that like in my my opinion most
94:53 - education should probably be textual
94:55 - because it's just an extremely efficient
94:57 - way of communicating things and you know
95:00 - like I I kind of look at I'm an old man
95:03 - yelling at a cloud right uh like Tik Tok
95:06 - and like watching a 60c video that's
95:08 - trying to compress all this information
95:09 - into you get into a a loop of
95:11 - information grazing where you you're
95:13 - getting like really superficial topical
95:15 - kind of information but you're not
95:17 - really going very deep at all so that
95:20 - that is one thing where like I value the
95:22 - fact that people put out these lengthy
95:24 - podcasts like I listen to tons of
95:27 - podcasts that are several hours long uh
95:29 - I I watch tons of video essays on
95:31 - YouTube that are like three or four
95:33 - hours long right and I learn so much
95:36 - more from those than I probably would
95:38 - have learned from watching you know
95:40 - however many Tik toks that would be like
95:41 - 200 Tik toks about the same topic right
95:44 - uh just because there's going to be so
95:45 - much repetition yeah I mean people
95:48 - covering the same thing over and
95:50 - over um there's a really great article
95:53 - sort of a or like essay along the lines
95:56 - of what you were talking about with text
95:57 - um it's called text is the universal API
96:01 - um text is the universal API by Charles
96:04 - Dickens yeah um but very much like sort
96:08 - of speaking to the same thing that you
96:10 - were talking about this idea that like
96:13 - it always has been and all like will be
96:15 - the main way in which we like language
96:18 - is so important in how we understand our
96:20 - world um and convey information it's
96:23 - just so efficient it's so effective um
96:26 - yeah and I mean one of the reasons AI is
96:28 - so good at generating images is because
96:30 - it uses text to like reason about how to
96:33 - generate images like if you look at a
96:36 - lot of the new tools like dolly or uh
96:39 - what's the one that uh there there are
96:41 - several ones uh what's the one that you
96:44 - use Discord to talk
96:46 - with um for some reason it's escaping my
96:49 - memory the really popular one that
96:52 - people use to create all
96:53 - theour mid Journey mid Journey yes it's
96:56 - mid journey I think yeah yeah um I mean
96:59 - for me like the most mind-blowing thing
97:03 - the the first mind-blowing thing that I
97:05 - saw with like all of this AI was um NLP
97:11 - techniques of just like creating vector
97:14 - vector space um of words you would just
97:17 - embed like 10,000 of the most common
97:20 - English words and then you could do
97:22 - arithmetic on those words to do like
97:26 - King minus man plus woman equals Queen
97:31 - yeah and that like you could represent
97:33 - words and like their relationships
97:36 - Through Math that was just incredible to
97:38 - me and that you can do that with all
97:41 - kinds of different words and that you
97:43 - can do that then with um Finance the
97:46 - transducers and and you can um
97:49 - eventually like get to something gpt2
97:52 - which was really the first time I saw
97:54 - something like a generative pre-train
97:55 - Transformer model um and it was just
97:59 - like absolutely mindblowing like very
98:03 - very early version of these AI image
98:06 - generators that could do like a 32x32
98:09 - image um and immediately you're like
98:11 - look this is going to be revolutionary
98:15 - um and that is something I was lucky to
98:19 - be exposed to early through doing
98:21 - research but it's also just a
98:25 - um I don't know like I I think it's
98:27 - going to continue to improve over the
98:29 - the coming years yeah it remains to be
98:32 - seen whether we're going to get giant
98:33 - step changes like we get with you know
98:36 - GPT morla like is it is it going to be
98:39 - morla um or faster who knows um it's too
98:43 - early to say right yeah so I I just uh
98:46 - and you know we we've talked a lot about
98:48 - AI ethics and stuff like that one of the
98:50 - reasons I don't really care to think
98:52 - that much about you know whether we're
98:54 - going to build Skynet is because I'm not
98:56 - convinced that the growth rate is like
98:59 - that right now I don't think we need to
99:01 - worry that much about it I'm not saying
99:02 - you shouldn't worry about that but I
99:05 - don't think that that's the most
99:06 - immediate concern the more immediate
99:09 - concerns are misinformation scams and
99:11 - stuff like that I mean so I agree with
99:14 - that I think the argument that people
99:15 - will make is that it's not
99:18 - a like linear thing or even an
99:22 - exponential but that like there's just
99:24 - some kind of breakthrough that hasn't
99:26 - happened yet that's sort of like we have
99:28 - no idea when that breakthrough is going
99:30 - to happen it could be several
99:31 - consecutive breakthroughs that need to
99:32 - happen but they could happen quickly or
99:34 - they could and who knows like I like if
99:37 - you talk to the best AI researchers in
99:38 - the world like the timeline is all over
99:41 - the place between them so that's for me
99:44 - like why like I'm not an expert on this
99:46 - this is not something that I should be
99:49 - like my opinion doesn't matter very much
99:50 - um
99:51 - and therefore like I I just try to like
99:53 - stay out of those conversations for now
99:56 - one thing that you do have probably a
99:58 - very nuanced opinion on
100:01 - is physical activity as a
100:04 - programmer I I I know that uh at Yale
100:08 - you're a captain on the running team and
100:11 - I can only imagine how good you have to
100:13 - be at running and how into running you
100:15 - have to be to like pull that off but uh
100:18 - maybe you could talk a little bit about
100:20 - running and the role that it plays in
100:21 - your
100:22 - life yeah so importantly I'm a captain
100:25 - on the club running team not
100:27 - the okay Verity running team um they are
100:32 - incredibly talented Runners on like the
100:35 - official team um we do it for
100:38 - fun um but it's a I ran Varsity in high
100:41 - school and then um continued at Yale
100:44 - it's a great group of people to to run
100:46 - with um and I think like what's really
100:51 - interesting to me within the context of
100:53 - our conversation which is focused
100:54 - obviously on audience of mostly
100:56 - programmers or
100:58 - developers um is that there seems to me
101:00 - to be like this huge overlap between
101:03 - people in this ecosystem and running
101:06 - just so many software Engineers I know
101:09 - also like that's their like chosen sport
101:12 - or like mechanism for exercise along
101:15 - with maybe like some lifting or
101:17 - something but um and I have to be very
101:19 - candid that I used to run a lot when I
101:22 - was first learning to code and recently
101:23 - I switched to kind of like what I call
101:24 - Mall walking just because it's like
101:26 - lower impact and it's a little easier
101:28 - for me to like digest a podcast when I'm
101:31 - not hearing thp Thump Thump Thump you
101:33 - know uh it's just a little bit more
101:35 - mellow and you still get a lot of
101:37 - cardiovascular benefit just from walking
101:39 - quickly uh but um but yes I also was a
101:44 - programmer who ran a lot and so it's not
101:47 - lost on me that I know a lot of other
101:49 - programmers who run a lot too and have
101:50 - are very passionate about that type of
101:53 - activity well I guess the thing that I'm
101:55 - interested in asking you about or or
101:57 - talking with you about is what you think
101:59 - it is about running that is just like
102:02 - something that meshes well with yeah
102:06 - people who like to program to to do
102:08 - software engineering um because I have
102:10 - some thoughts on it I think yeah I mean
102:13 - I'll throw out some naive guesses uh it
102:16 - is a way to decompress and it's
102:19 - something that you kind of do
102:20 - automatically like once you start
102:21 - running you don't really have to think
102:22 - about it too much you just keep
102:24 - going and it gives you a lot of time and
102:27 - space to like maybe think about
102:29 - engineering problems you're facing or
102:31 - think about what you want to do next
102:33 - with a given
102:34 - project I mean that's exactly what I do
102:37 - too
102:39 - um I think
102:42 - like there's like definitely the element
102:45 - of it's just something you can do that's
102:47 - like good exercise while also letting
102:48 - your mind sort of like roam free um um
102:51 - and you can either think about what
102:53 - you're working on or you can sort of let
102:55 - yourself have some time to decompress
102:57 - distress um but it's also about I think
103:01 - to me this idea of like like I think a
103:02 - runner's higher like a there's almost
103:05 - like a flow State you enter into when
103:07 - you're running longdistance running
103:09 - because I I do specifically longdistance
103:11 - running
103:13 - um that you just sort of like get into
103:16 - it and it's
103:18 - a very similar feeling I think to the
103:22 - feeling you have when you sit down to
103:23 - like do a like pretty like intense
103:26 - programming session um of like really
103:29 - being sort of zoned into exactly what it
103:31 - is that you're doing right then um to me
103:36 - those are actually very similar things I
103:38 - I was curious if you felt um the same
103:42 - way definitely
103:44 - like I there's this old saying that like
103:47 - there's this old joke that you know
103:48 - what's the purpose of a program
103:51 - body is is is to keep their brain off
103:54 - the ground right like that's how so many
103:56 - programmers and developers and I guess
103:58 - you could say knowledge workers by
104:01 - extension have treated their body let's
104:03 - go eat some fast food we got to we got
104:05 - to get some sleep real quick everything
104:07 - is kind of
104:09 - like it's you're you're almost like
104:12 - paying lip service to the fact that you
104:14 - have a corporeal form that has to be
104:17 - maintained and uh I think that's to
104:20 - people's detriment if they neglect their
104:23 - their health um and running is something
104:25 - that really you know boosts your
104:28 - cardiovascular your respiratory Health
104:31 - um probably has all kinds of other
104:32 - positive knock on effects because
104:35 - historically like you know ancient
104:37 - humans probably ran a lot a lot more
104:40 - than they do to today you know whe
104:42 - whether that's running toward prey or
104:45 - running from predators um and there's
104:48 - there's something that does feel really
104:49 - good you get get all these endorphins
104:51 - when you run so it it maybe it just
104:54 - boosts your energy level it boosts your
104:56 - ability to to think I mean like I said I
104:58 - I feel like I get a lot of these just
104:59 - from walking and again I'm a 43y old man
105:02 - who uh has a lot of like weight just
105:06 - from like working out for years and
105:09 - years like it is kind of hard on your
105:11 - knees if you go running and it's hard on
105:12 - your lower leg too uh
105:15 - so people who are listening to this I
105:18 - know and people who have mobility issues
105:20 - and stuff like I wouldn't say like oh
105:22 - just throw to the as side and go run but
105:24 - I would say any physical activity you
105:26 - can engage in is probably going to be a
105:28 - virtuous circle in terms of improving
105:30 - your productivity improving your ability
105:32 - to think improving your your ability to
105:34 - sleep improving your energy level like
105:36 - all these things uh your body is a
105:39 - machine and you want to make sure that
105:41 - that machine gets regularly used and
105:45 - doesn't just becomes this thing that you
105:47 - kind of like spite there were so many
105:49 - years of my life when I was a kid where
105:50 - I was just like man I wish I was just
105:52 - like the lawnmower man I wish I could
105:54 - just be everywhere and do everything and
105:56 - like not have to think about like oh you
105:59 - know uh what am I going to eat for
106:00 - breakfast or oh man it's it's late I
106:02 - need to get some sleep you know like I
106:04 - didn't want to have those uh limitations
106:06 - I wanted to live in a realm of pure
106:09 - abstraction and have that kind of power
106:12 - of being a disembodied you know entity
106:16 - right probably tale is old as time I
106:18 - wouldn't be surprised if they wrote
106:19 - about this in Greek myth ol or something
106:21 - somebody giving up their body to become
106:22 - this omnipresent spirit that can do
106:25 - whatever right um but the reality is you
106:30 - have a heart you have lungs you have a
106:32 - brain you have all these other things in
106:34 - your body and you need to to keep those
106:36 - sharp if you want to be able to perform
106:38 - and you know I'm optimizing for
106:39 - longevity I want to live to be 100 years
106:41 - old I don't think there's any reason uh
106:44 - unless you have like genetic disorders
106:46 - which I don't seem to have uh I've I've
106:50 - been very blessed in that regard I went
106:52 - and I got a colonoscopy ahead of
106:54 - schedule because I had blood in my stool
106:57 - and I was concerned about it and um
107:00 - thankfully no cancer uh but you know you
107:04 - want to be on top of those things an
107:06 - ounce of prevention is worth a pound of
107:08 - cure and a big part of preventing things
107:10 - is exercise right that's like the the
107:13 - health exercise or like the uh the the
107:16 - diet exercise sleep triangle right the
107:19 - thing that everybody should be thinking
107:20 - about as they age so anyway this I'll
107:23 - get off my soap box but that is my
107:27 - immediate response to why I think so
107:28 - many programmers are Runners because
107:30 - they're also probably pretty smart like
107:33 - well adjusted people who who do want to
107:35 - optimize for like a long healthy
107:38 - life yeah I mean and it totally is a
107:42 - great way to do that um I think on the
107:46 - subject of of viewing bodies as machine
107:51 - um and this might be a great like last
107:53 - question to talk about on our podcast
107:55 - today um we've talked about a lot of
107:58 - like different sci-fi and and
107:59 - interesting media related to programming
108:01 - or technology or AI today from Westworld
108:04 - to her to I think you mentioned Minority
108:07 - Report and Y A couple others but I guess
108:12 - like you have any recommendations for me
108:14 - like what are your favorite like books
108:17 - movies um TV shows within the genre
108:20 - medium like for any of the
108:23 - listeners maybe go check out I I have
108:26 - also read and watched all of the things
108:27 - you just mentioned and they're all
108:29 - amazing yeah I mean I would just say
108:30 - like if you haven't seen her you should
108:32 - watch that that's definitely the best
108:33 - kind of Singularity movie I've ever seen
108:36 - um I think it's the most realistic
108:38 - frankly I think the machines are going
108:40 - to just be these kind of benevolent like
108:42 - they're just going to see humans as like
108:45 - oh whatever you know like go do your
108:46 - little human things we're going to be
108:48 - doing our super intelligence thing over
108:49 - here
108:50 - you know there could be some dark
108:51 - feature like I used to watch a lot of
108:53 - like cyberp Punk and dystopian stuff and
108:55 - read all the William Gibson novels and
108:58 - all that stuff and I I'm not that
109:00 - pessimistic anymore I'm much more
109:02 - optimistic
109:03 - so I think her is is pretty optimistic
109:06 - as a movie in general like if we're
109:08 - going to have a singularity hopefully
109:10 - that's the singularity we have where the
109:12 - AIS are too interested in like
109:13 - recreating Mark Twain and talking to him
109:16 - than uh than like figuring out a way to
109:18 - like you know turn all the humans into
109:21 - paperclips so to speak the the famous
109:23 - Nick Bostrom paperclip problem of if you
109:25 - give it an AI something to optimize I I
109:28 - mean rather than recommending science
109:29 - fiction movies and stuff books like I'll
109:32 - be completely C I don't have a lot of
109:33 - time to to consume fiction these days
109:36 - almost everything I watch is like a
109:37 - documentary but um again any good ones
109:41 - of those her her would be my
109:43 - recommendation again probably
109:44 - everybody's already seen it rewatch The
109:46 - Matrix uh Trilogy a lot of people like
109:49 - bash the the second and the third one as
109:51 - not that interesting or like bad CG or
109:53 - something um computer generated imagery
109:58 - CGI but I genuinely think that's a
110:01 - really well-made and I haven't seen the
110:03 - fourth one but I I genuinely think I
110:06 - haven't either the early the late 1990s
110:09 - early 2000s Matrix movies if you rewatch
110:12 - it there much deeper than you may have
110:14 - realized when you were a kid watching
110:16 - them and what it says about artificial
110:19 - intelligence what it says about The
110:20 - Human Condition if you really go down
110:22 - that that rabbit hole of like
110:24 - understanding The Matrix there there's a
110:26 - lot there like I I watched it uh maybe
110:30 - like five or six years ago I watched all
110:31 - three of them and I was just like wow so
110:33 - like like I never got any of this as a
110:36 - kid I always thought the machines were
110:38 - bad right they're trying to kill
110:39 - everybody but then the more I watched
110:41 - the more I realized oh this is actually
110:42 - like you know it's a dystopian movie
110:45 - because humans make it dystopian in a
110:48 - way so I would recommend rewatching
110:51 - those and maybe reading some of the the
110:52 - fan theories and stuff like that I'm not
110:54 - sure if the the orasi sisters have ever
110:56 - publicly talked about like exactly why
111:00 - like what the story is supposed to mean
111:02 - or anything but there's a lot there and
111:03 - it's of course certainly the first one
111:06 - is an amazing movie it's one of my
111:07 - favorite movies of all time so would you
111:09 - take the blue pill or the red pill
111:11 - Quincy uh the red pill is the one where
111:13 - you go deeper in absolutely I would take
111:16 - that yeah because I mean I would want to
111:18 - know I I wouldn't be content like that
111:20 - guy that wants to get put back in The
111:21 - Matrix I would never want to do that
111:23 - like once you know something you can't
111:26 - turn back you know you got to be true to
111:28 - yourself you can't delude yourself um
111:30 - and I think most strong you
111:33 - know people would would not succumb to
111:36 - that kind of uh temptation of wanting to
111:38 - go back to being ignorant I don't think
111:40 - there's any uh turning back the clocks
111:43 - what about you you probably have a lot
111:45 - more recommend what are what are some uh
111:47 - some science fiction uh that you would
111:49 - recommend if people want to have a good
111:50 - feel for like AI good cases bad cases oh
111:54 - the expanse the creators is pretty good
111:58 - they they've said in the future AI like
112:00 - in the expans universe AI doesn't even
112:02 - really matter that much like it's doing
112:04 - all kinds of things passively like what
112:05 - you were saying earlier about like
112:07 - contextual or what was the term you used
112:10 - where AI just figures out what needs to
112:11 - be done proactive Compu proactive yes so
112:14 - there's tons of proactive AI it's like
112:16 - figuring out like all the different
112:17 - little minor calibrations that the ship
112:20 - needs to make while the pilot is sitting
112:21 - there actually doing like the most I
112:24 - guess important highle decisions and
112:26 - then those decisions are cascading down
112:28 - through all throughout the ship systems
112:30 - so AI is there but in the expans
112:33 - universe AI is not that Earth changing
112:36 - of a thing or I guess solar system
112:38 - changing of a thing it's just a thing
112:40 - that is a tool that they use and there
112:43 - wasn't some like big AI war or anything
112:45 - like that in the expanse history uh that
112:48 - was just you know how they do it and to
112:50 - some extent Star Trek similarly AI is
112:52 - just another tool it's one of many
112:55 - fascinating Technologies alongside
112:56 - teleportation Tri quarters you know all
112:59 - these different tools
113:01 - right yeah um the expans is a great one
113:04 - although there's not a whole lot of like
113:06 - AI as part of that story
113:09 - um I think if I think about great movies
113:12 - that are in this this area or genre um
113:14 - absolutely love both Blade Runner movies
113:17 - um those are both really good this don't
113:20 - sleep on the sequel It's good the sequel
113:22 - the sequel's super underrated um with
113:25 - Ryan goling absolutely like I think the
113:28 - maybe equally good depiction of um like
113:32 - a personal AI as like the one in her um
113:35 - exmachina is also one that people sleep
113:37 - on sometimes um good movie I saw
113:41 - that yeah um I
113:46 - also like I I think there's a lot of
113:48 - really good
113:52 - TV and and books in this genre um
113:54 - Westworld is very sadly cancelled I'm
113:57 - really like I only saw the first two
113:59 - seasons the first season was much better
114:01 - than the second season did you watch the
114:03 - first season was incredible yeah I
114:05 - watched the whole show there's four
114:06 - seasons actually oh okay um I think that
114:10 - they sort of went back to basics with
114:13 - the fourth season um and then they kind
114:15 - of left I think they knew it was going
114:16 - to get canceled so they kind of left it
114:18 - on like an ending that's sort of
114:19 - unsatisfying but still like relatively
114:22 - ties things up kind of um but that has
114:26 - so many interesting questions about
114:29 - Consciousness and exploitation and um
114:32 - like what it means to be human versus
114:35 - like uh an artificial intelligence or a
114:38 - machine like definitely really
114:40 - interesting questions that aren't fully
114:42 - explored but like are at least asked
114:44 - kind of in the same way that like L does
114:46 - um Dune is actually really interesting
114:49 - even though though there's no AI in it
114:51 - because their history of Dune is that
114:53 - there was this like massive AI war and
114:56 - then they just like got rid of all the
114:58 - AI afterwards that's very battles Star
114:59 - Galactica so that's another show like I
115:01 - watched like the I guess it was the 90s
115:03 - battles Star Galactica early 2000s and
115:06 - that is all about Ai and the dangers of
115:08 - AI you can't Network computers because
115:10 - the moment you network them you run the
115:12 - risk of them becoming super intelligent
115:14 - and plotting against humans and they're
115:16 - they're basically fighting this entire
115:18 - race of artificial
115:20 - beings that they've constructed and
115:22 - they've been at this this ly War and the
115:25 - machines are trying to annihilate the
115:27 - humans really cool yeah I mean I
115:30 - mentioned claraa and the sun before um
115:33 - but that book is also about like sort of
115:36 - a a dystopian World in which people are
115:39 - like sort of
115:41 - genetically um designer babies um but
115:45 - also they have like AI tutors sort of
115:48 - that live with them
115:50 - it's like a very
115:53 - um like sort of Slice of Life Book that
115:56 - has like some deeper subtext um yeah I
115:59 - mean I think there's like even something
116:01 - as simple as like Mary Shell's
116:02 - Frankenstein like these are books that
116:04 - don't really have necessarily anything
116:07 - to do with AI like in the surface level
116:10 - um but it it fundamentally is about
116:13 - like mankind trying to like sort of play
116:17 - God yeah and what does that that's is on
116:20 - some level what we're doing here and
116:21 - what does that mean for us to be trying
116:23 - to create like intelligence beyond our
116:25 - own um there's a lot of interesting
116:27 - questions there yeah and what does
116:29 - intelligence beyond our own mean because
116:32 - humans when you group them together can
116:33 - make pretty big important decision I
116:35 - mean you could argue that like the
116:38 - economy is a whole bunch of IND you know
116:41 - individual self you know um I guess
116:44 - self-interested actors that has the net
116:46 - effect of us having you know this
116:48 - abundance that we have today for for
116:49 - example right um so like emergent
116:53 - behavior from different species like
116:55 - leaf cutter ants uh we we were uh we
116:58 - were hanging out with some leaf cutter
117:00 - ants the other day and uh my kids were
117:03 - terrified of them but we we spent some
117:05 - time watch some documentaries about how
117:06 - they work and stuff it's fascinating and
117:08 - it's just emergent behavior from lots of
117:10 - entities together and uh yeah
117:13 - so will we have a super intelligence
117:15 - that not only exceeds individual human
117:17 - beings but exceeds the
117:20 - compute of the entire human kind of
117:22 - wetware network that is I gu we'll see
117:25 - right yeah we'll might be in our
117:27 - lifetimes yeah I mean imagine the answer
117:30 - is yes we will eventually have that it's
117:32 - just a what's the timeline are we
117:34 - talking you know Blade Runner timeline
117:36 - or are we talking uh you know the expans
117:39 - timeline or Star Trek timeline right
117:41 - 2049 could be yeah we'll see awesome
117:44 - well it's been such a blast talking with
117:46 - you I mean we talked about so many
117:47 - different things uh I am going to leave
117:51 - videos uh your courses that you've
117:53 - created uh your AI course Ai and safety
117:56 - and also safe. aai which is a charity
118:00 - that's I think largely funded by uh AI
118:03 - companies including open AI they have an
118:06 - AI course that we published on free C
118:07 - Camp as well uh that I think uh is much
118:11 - more detail oriented than yours but
118:13 - yours is like the broad sweeping like
118:15 - here's a framework for thinking about AI
118:17 - safety and here are the you know just
118:20 - some of the many considerations and why
118:22 - AI safety is important so if you want to
118:23 - watch the safe AI course I recommend
118:25 - watching Seth's course first um and I
118:28 - will link to both in the show notes as
118:30 - well and then um yeah I mean thanks for
118:33 - everything you're doing for the free C
118:35 - cam community Through teaching these
118:37 - courses uh thanks for you know
118:40 - developing these tools and uh continuing
118:42 - to push forward the state-ofthe-art
118:43 - we're very excited to see what you all
118:47 - come off with next thanks for having me
118:50 - Quincy it was a blast until next week
118:53 - happy coding