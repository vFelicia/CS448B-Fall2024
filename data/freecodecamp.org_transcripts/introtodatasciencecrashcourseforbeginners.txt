00:02 - hey everyone and welcome to my mini
00:04 - course on the essentials of data science
00:06 - this mini course provides a super basic
00:08 - looking to data science what it is and
00:11 - the three main components that make up
00:13 - data science data science is a very
00:15 - mainstream word like it's thrown around
00:17 - a lot but its actual definition is quite
00:19 - vague this mini course is designed to
00:22 - help those of you who are curious about
00:24 - data science develop a better and more
00:26 - specific understanding of the topic
00:28 - there are definitely more advanced
00:30 - techniques within data science such as
00:32 - machine learning but even these can be
00:34 - traced back to the three essential
00:36 - components that we'll cover before we
00:38 - get straight into it
00:39 - I thought I'd quickly introduce myself
00:40 - my name is Max and I work as a data
00:43 - scientist after getting my degree in
00:45 - physics I find myself more and more
00:48 - drawn into the world of data science so
00:50 - instead of diving into the realm of
00:51 - physics research I taught myself all the
00:54 - tools and techniques a data scientist
00:56 - needs and shortly after landed my dream
00:59 - data science job I've since also started
01:02 - teaching data science to others and have
01:04 - been fortunate enough to teach what is
01:05 - currently over 9,000 students the skills
01:08 - of gathered and learned over the past
01:09 - five years of my data science journey so
01:12 - let's jump right into it so what is data
01:16 - science well data science is you can
01:19 - kind of summarize it in different ways
01:20 - but the main parts of it are
01:22 - transforming data into information and
01:25 - this is a really big step because a lot
01:28 - of people talk about you know data and
01:29 - big data and all these things but data
01:31 - by itself isn't really that useful until
01:35 - you can turn it into information and so
01:38 - if you just have a bunch of numbers
01:39 - appearing somewhere and it's just you
01:41 - know so much of it no one can make sense
01:44 - of that and that's where you need a data
01:45 - scientist to be able to transform all of
01:48 - these all of this vagueness and kind of
01:51 - this noise to that's going on and you
01:53 - need to be able to extract information
01:54 - from it
01:55 - and that's what a data scientist does
01:57 - now what you do with this - with this
02:00 - information or how you get this
02:02 - information
02:02 - it's through analyzing your data so a
02:04 - big part of it would be you know
02:06 - cleaning things up doing some some
02:09 - processes on it and then you analyze
02:11 - once you've clean things up
02:13 - and that is one of the ways that you can
02:15 - then get information out of your data
02:18 - through this analysis and you can kind
02:20 - of continue on and you see trends and
02:22 - patterns and all types of correlations
02:24 - hopefully and all of these things again
02:27 - build up into this turning data into
02:29 - information component and then
02:33 - ultimately you also need to
02:34 - contextualize everything that you have
02:36 - so your computer can't do that for you
02:38 - you can Peter can kind of crunch the
02:40 - numbers and stuff but it's your
02:41 - responsibility also to make sense what's
02:43 - in front of you and even if you see
02:45 - something you just don't blindly trust
02:47 - it but you need to understand you know
02:49 - where am I at where am i coming from
02:52 - where is this data coming from you need
02:53 - to be able to contextualize these things
02:55 - and then of course be able to apply as
02:57 - well as understand them and so once you
03:00 - have this data you know it's great but
03:02 - turning it into an information into
03:04 - great information that you can use and
03:06 - directly apply that's where the real
03:09 - power lies and that's also kind of the
03:12 - role of a data scientist so that's what
03:14 - the data that's what data science pretty
03:17 - much is and so what is the data
03:19 - scientists do well we kind of already
03:21 - talked about this just a little bit but
03:23 - let's go over it again any more concrete
03:25 - examples um so a data scientist would
03:28 - for example get and process this raw
03:31 - data and then convert it into something
03:33 - a little bit cleaner so you can imagine
03:36 - kind of just like a data stream coming
03:37 - in and it's you have this measuring
03:39 - device and constantly is just measuring
03:41 - all sorts of data and because like
03:44 - nothing is really constant so everything
03:45 - will be fluctuating up and down and so a
03:47 - data scientist would be to kind of take
03:49 - all of this data it'd be to kind of
03:51 - clean it up a little bit you know maybe
03:53 - reduce this fluctuation that you know
03:55 - isn't supposed to be there that's just
03:56 - kind of background stuff going on and
03:59 - then put it into a format so that you
04:01 - can easily plot it against some things
04:03 - and then we already get to the next
04:05 - point that you know once this data is
04:06 - cleaner you can maybe do start doing
04:08 - some calculations on them figuring out
04:11 - the core statistical components you know
04:13 - like what is the average values of these
04:15 - what what am I really dealing with you
04:18 - know getting a first look at first
04:19 - understanding of what it actually is
04:21 - that you're tackling and then once you
04:23 - have this kind of understanding then you
04:25 - can start to do some visualize they
04:26 - which helped you as a data scientist
04:29 - maybe see some trends or patterns
04:30 - already but visualization was also
04:32 - really key because they let you show it
04:35 - to other people and there are great
04:37 - means of communication so they help both
04:39 - you as a data scientist as well as
04:41 - helping others when you try to convey
04:42 - this information to them all right and
04:45 - then finally you have to suggest some
04:47 - applications of the information so it's
04:49 - not really enough to just be able to
04:50 - look at it and say like yeah I see it
04:53 - goes up and down and that's that's good
04:55 - but what does that mean how does this
04:57 - transfer into something useful and
04:59 - that's also one of the key roles of a
05:02 - data scientist transferring information
05:04 - into knowledge and so you've got this
05:06 - data into information step but you also
05:09 - need to transfer this information into
05:11 - knowledge and those are two really
05:13 - powerful things that are worth a lot a
05:15 - lot and that's pretty much what a data
05:18 - scientist focuses on and then you can go
05:20 - further you know and take this data and
05:22 - do machine learning with it or something
05:24 - if you really understand what's going on
05:25 - or if you have some hypotheses of you
05:28 - know what could happen so you can take
05:29 - things a lot further but ultimately this
05:31 - kind of turning data into information
05:33 - and then into knowledge that's kind of
05:36 - your role all right so let's go into the
05:39 - essential techniques or the essential
05:41 - components of data science so the first
05:44 - essential component and we kind of
05:45 - touched on this already is statistics
05:48 - and basically we're gonna cover this
05:51 - later on but let's just give a kind of
05:53 - quick wrap down so in statistics need to
05:56 - understand different data types that you
05:58 - can encounter and so there are data can
06:01 - come in different ways and we'll go
06:03 - again into more detail with this later
06:05 - but it's not just you know you get a
06:07 - bunch of numbers data can come in very
06:09 - many different ways depending on the
06:11 - field that you're in and so you need to
06:12 - be prepared and you need to kind of be
06:14 - aware that data may not always just be a
06:16 - direct number for you then of course you
06:19 - need to understand some key statistical
06:20 - terms like you know the different types
06:22 - of means and also understanding
06:24 - fluctuations in data and the reason that
06:26 - this is important is because these key
06:29 - statistical terms give you an overview
06:31 - of how this data is behaving and
06:33 - depending on how the data is behaving
06:35 - you may want to approach it differently
06:36 - so if you know that your data is
06:39 - very clean there's a very little
06:40 - fluctuation then if you visualize things
06:43 - you can probably trust what's going on
06:45 - or if you want to maybe fit some curves
06:47 - to it or something but if you see
06:49 - there's a lot of fluctuation in your
06:50 - data visualizing it is going to be much
06:52 - more difficult because you just see
06:54 - jumps everywhere and you're not really
06:55 - sure which of this is actually true and
06:58 - which of this is caused by you know like
07:00 - some interference somewhere or someone
07:02 - is messed with my system and so all of
07:05 - these things will kind of be hinted to
07:07 - you through statistical terms so it's
07:10 - probably good that you know you're kind
07:12 - of comfortable with these things and
07:13 - that you can be able to get some meaning
07:16 - and meaning out of them all right and
07:19 - then finally it be in statistics to be
07:22 - able to you know split up and group or
07:24 - segment data points so that when you
07:27 - have this big data set you want to be
07:28 - able to you know maybe split it up into
07:30 - smaller things compare different regions
07:32 - look more into more detail into some
07:35 - things and maybe you know isolate two
07:37 - components because you know hey these
07:39 - things are probably going to be
07:40 - important the rest I don't really care
07:42 - about that much so being able to kind of
07:44 - pinpoint an isolate and meddle with the
07:46 - data a little bit so these are the kind
07:48 - of statistical components that we're
07:49 - gonna look into all right so the next
07:52 - big thing and we've already talked about
07:54 - this too is data visualization and we'll
07:57 - see why data visualization is a really
07:59 - key skill for data scientists and then
08:02 - we're also be gonna be covering
08:04 - different types of grass that you can
08:05 - use and how you can compare different
08:07 - number of variables so for example you
08:09 - can have one variable grass where you
08:11 - only look at one thing and you only want
08:14 - to look at this and you want to see how
08:15 - these how this changes you have your
08:17 - typical two variable grass which you
08:19 - probably know where you have this X and
08:21 - a y-axis and then you can kind of see
08:23 - how two variables relate to each other
08:25 - or you can have three variable or even
08:27 - higher variable graphs and where you
08:29 - plot maybe three different things or
08:31 - even more if you want as long as it
08:33 - makes sense next to each other so that
08:36 - you can compare multiple things at the
08:37 - same time all right and now we come to
08:42 - the other big thing that you're probably
08:43 - going to need as a data scientist which
08:45 - is going to be the ability to program
08:47 - now not every data scientist can do this
08:51 - but this is
08:52 - really really essential in my opinion to
08:56 - your role as a data scientist because
08:58 - knowing how to program is going to make
08:59 - your life so much easier if you know how
09:02 - to program you can kind of take your
09:04 - ideas and your thoughts and you can put
09:06 - them into actions in the computer and
09:08 - you can just automate everything you can
09:10 - customize things you can explore you can
09:12 - prototype you can test and you're not
09:14 - reliant on some you know application you
09:17 - don't have to master some application
09:19 - and if it doesn't work or if one feature
09:20 - isn't there you have to contact customer
09:22 - support and maybe it's not even possible
09:25 - and then you have to wait for an update
09:26 - or maybe something is bugged
09:28 - with programming there's just you're so
09:30 - much more reliant on yourself and you
09:32 - can really just do whatever it is you
09:34 - want to do and you're not reliant on
09:36 - other people or on the tools that other
09:38 - people have built for you but rather you
09:40 - can just pretty much go and you know
09:42 - just do what you want to do without
09:44 - there being major roadblocks and then
09:47 - we'll also look at some essential
09:49 - packages in Python so in programming you
09:52 - never want to reinvent the wheel you
09:54 - always want to start off where the last
09:55 - person left off and so the ability to
09:58 - program and be able to write simple
10:00 - programs you would need to teach
10:02 - yourself but you wouldn't need to write
10:03 - highly complex mathematical packages or
10:06 - data analysis packages those are already
10:08 - out there all you need to do is be able
10:10 - to download them and implement them in
10:13 - your code and they're gonna work you
10:15 - know they've been tested a lot there's a
10:17 - huge community's working on them on
10:20 - improving them and everything all of
10:22 - this is for the community and so the
10:24 - whole community kind of works together
10:26 - to improve it no one's really directly
10:28 - trying to make a lot of money off of it
10:29 - so they're not going to charge you all
10:31 - of these service fees and everything
10:33 - everyone's just trying to improve their
10:34 - package because if it improves everyone
10:37 - also benefits from it and so we'll look
10:39 - at some of the libraries or we'll talk
10:42 - about some libraries that you can use
10:44 - especially in Python and to help you
10:46 - along your way with data analysis and to
10:49 - become a successful data scientist in
10:51 - this chapter we're going to talk about
10:53 - statistical data types now we're going
10:56 - to look at the three different types of
10:58 - data which are summarized as numerical
11:01 - categorical and ordinal types of data
11:04 - now these are the types of data that we
11:07 - talked about before how you can't just
11:09 - expect your data to be cut be kind of
11:11 - numerical and so we'll see the miracle
11:14 - data but we'll also see the two other
11:16 - types of data that you may be you know
11:19 - encountering in your career as a data
11:21 - scientist all right
11:23 - so let's talk about numerical data first
11:25 - though numerical data is also known as
11:28 - quantitative data and it's pretty much
11:31 - things that you can kind of measure it's
11:33 - it's great numerical stuff that you can
11:36 - do math with you can compare it you know
11:39 - saying this Plus this makes sense he is
11:42 - greater than B these are you know all
11:45 - examples of numerical data numerical but
11:48 - data can we split up into two different
11:50 - segments one of them is going to be
11:52 - discrete and so discrete means the
11:54 - values only take on distinct numbers and
11:57 - an example of this would be you know IQ
12:01 - or something like that a measurement of
12:02 - IQ or if you do a coin toss the number
12:05 - of times that you toss heads so you can
12:07 - you know you can have 15 heads you can
12:10 - have 12 heads out of you know 20 coin
12:12 - tosses you can have 500 heads out of a
12:14 - thousand coin tosses or 500 out of 600
12:17 - or all of these things but all of these
12:18 - are distinct numbers and now they don't
12:22 - have to be whole specifically but they
12:25 - do have to be distinct so that's that's
12:28 - the kind of very important part that you
12:30 - know there's a kind of step size that
12:33 - you're dealing with and of course you
12:35 - can still say hey you know flipping
12:37 - eight heads out of twenty is better than
12:38 - filling seven heads out of twenty so if
12:41 - you want to flip heads lettuce or
12:43 - flipping eight out of 20 is worse than
12:45 - flipping 7 out of 20 if you're going for
12:47 - as many tails as you can so all of these
12:49 - kind of comparisons that make sense so
12:52 - that's the discrete part of numerical
12:54 - data then we have the continuous part
12:56 - and now the continuous part is really
12:59 - that values can just take on any number
13:01 - and they're not unlimited by decimal
13:02 - place so a value that can you know can
13:05 - be like one point one and then the next
13:08 - value would be one point two that's not
13:09 - continuous that's still discrete because
13:12 - you have this step size of zero point
13:14 - one continuous means literally ever
13:18 - number from start to finish can be taken
13:21 - on and this doesn't mean that every
13:23 - possible number in the universe from
13:25 - negative infinity to plus infinity and
13:27 - all imaginary numbers and everything
13:29 - that comes with it that doesn't that
13:31 - that's not required for continuous it
13:33 - could really be that just every number
13:35 - between zero and one can be taken on so
13:38 - for example let's say you have a bottle
13:41 - of water and this bottle of water can
13:43 - hold one liter now if you fill your
13:47 - bottle up and it starts off empty and
13:49 - you fill it all the way up to the top
13:51 - the amount of water that you've had
13:53 - needed to take on every single number
13:56 - between zero and one because you can't
14:00 - just fill up water you know in kind of
14:01 - small increments of say hey I'm gonna
14:03 - put in 0.2 liters every single time
14:06 - because the water doesn't just you know
14:08 - teleport from A to B but when you're
14:10 - pouring in water it's more like we see
14:13 - in the stream here and the water level
14:15 - rises and Rises and Rises and so the
14:18 - amount of water that we have in our cup
14:20 - needs to take on every value between
14:22 - zero and one so that's an example of
14:25 - continuous data for but you see that you
14:29 - know we can be limited to zero and to be
14:31 - between zero and one we don't have to
14:33 - you know start at zero and go all the
14:35 - way up to infinity or something but it's
14:37 - just that the range that we're looking
14:39 - at every single number can can be
14:44 - applied or every single number can you
14:46 - know happen another good example would
14:50 - be the speed of a car if you start and
14:52 - you you know you're standing still and
14:55 - you're studying you're standing at a
14:56 - stoplight and then you want to
14:58 - accelerate in the speed limit us say you
15:00 - know 50 miles an hour or something to
15:02 - get to 50 miles an hour from your
15:05 - starting position your car has to take
15:07 - on every single speed in between and of
15:11 - course you won't see that you know on
15:13 - your spot on the speedometer it would
15:14 - say something like zero miles an hour
15:16 - one mile an hour you know maybe you can
15:18 - go to like it's going 0.1 0.2 0.3 or
15:21 - something like that so it may look
15:23 - discrete to you but that's not how your
15:25 - car is going your car doesn't say it
15:27 - like oh I'm gonna go in these step sizes
15:29 - of speed it's gonna accelerate
15:31 - gonna take on every value starting from
15:33 - zero going up to 50 miles an hour and
15:36 - you're gonna when you're in this
15:38 - transition you're gonna take on every
15:40 - single one of those speed values so
15:43 - that's how continuous data looks like
15:45 - and it's important to understand the
15:48 - difference between this discrete and
15:49 - continuous just because you may want to
15:53 - approach it differently now of course if
15:55 - we're dealing with computers our
15:56 - computers can't deal with infinite
15:59 - numbers in the decimal places we have to
16:01 - cut it off somewhere and so usually
16:04 - continuous data is gonna be rounded off
16:06 - at some point but it's still important
16:08 - for you to know that you're dealing with
16:09 - continuous data here rather than
16:11 - discrete so that you know hey there can
16:13 - still be other stuff in between here and
16:15 - or all of these things rather than you
16:17 - know having specific step sizes and all
16:19 - you see is just kind of a bunch of lines
16:21 - at every step size but you can expect
16:24 - that when you have continuous data that
16:26 - everything is just kind of filled filled
16:29 - up that everything can and may even well
16:32 - be in between certain places so that's
16:35 - that's kind of the important thing to
16:36 - note between discrete and continuous
16:39 - alright so the next type of data that
16:42 - we'll have is categorical now
16:44 - categorical data doesn't really have a
16:47 - mathematical meaning and you may also
16:49 - know it to be qualitative data um and
16:52 - categorical data it describes
16:54 - characteristics so a good example of
16:56 - this would be for example gender so here
17:00 - there is no real mathematical meaning to
17:02 - gender of course you know if you have
17:04 - good data you can say male is zero and
17:07 - female is one but you can't really
17:10 - compare the two numbers even though you
17:12 - assign numbers to them and you may just
17:14 - do this so that you can split it up
17:15 - later on there your computer can
17:17 - understand but it doesn't really make
17:19 - any sense to compare you can't say you
17:22 - know is male equal what you can say male
17:25 - is not equal to female but you can't
17:27 - really say is one greater than the other
17:29 - or is one approximately equal to the
17:32 - other those things don't really make
17:33 - sense because they're not well-defined
17:35 - what does that mean and you can't really
17:38 - add them up either you can't say male
17:40 - plus female but that doesn't it doesn't
17:43 - give you a third
17:45 - category or something so categories that
17:48 - you can't really apply math to them but
17:49 - there are nice ways to kind of split up
17:51 - or group your data and they provide
17:53 - these nice qualitative pieces of
17:55 - information that are still important
17:56 - it's just you can't really go that well
18:00 - about you know like plotting them on a
18:01 - line or something like that
18:03 - so those are important things to note
18:05 - with categorical data and then another
18:07 - example would for example be yeah
18:08 - ethnicity or you could also have
18:10 - nationality all of these things are
18:13 - examples of categorical types of data
18:17 - yeah I'm so like we said you can assign
18:19 - numbers to them but that's really just
18:21 - for your code so that it's easy to kind
18:23 - of split them up but you still can't
18:26 - really compare them how are you gonna
18:27 - compare nationalities there is really no
18:30 - definition for you know comparing one
18:33 - type of category to another alright and
18:37 - so the third type of data that you can
18:39 - encounter is something called ordinal
18:41 - data
18:41 - now ordinal data is a mixture of
18:44 - numerical and categorical data and a
18:47 - good example of this would be both tell
18:49 - ratings so you have you know star
18:52 - ratings 0 0 1 2 3 4 or 5 stars or maybe
18:56 - even 6 stars or you know whatever it is
18:58 - whatever the hotels go up to these days
19:00 - um but it's still not as straightforward
19:02 - to compare so I'm sure you've seen two
19:05 - different types of three-star hotels one
19:07 - of them you know had the bare minimums
19:09 - the beds were okay but it wasn't really
19:11 - anything special and then you had this
19:14 - three-star hotels that you could have
19:15 - sworn we're at least four-star and so
19:18 - star ratings do make sense we can say
19:21 - you know a four-star hotel is probably
19:24 - better than the three tier hotel because
19:26 - there have been standards there are
19:28 - standards for these things they have
19:29 - been checked you know if you go to a
19:30 - four-star hotel you know what to kind of
19:32 - expect but still it's not completely
19:37 - defined so like you know coming back to
19:39 - this three star example it's very hard
19:42 - if you just say hey we're going to a
19:43 - three-star Hotel it's very hard to know
19:45 - exactly what to expect because there are
19:48 - different parts of three-star hotels
19:50 - there are three-star hotels that have
19:52 - developed onto like have a swimming pool
19:54 - maybe or something like that and then
19:56 - there are those three-star hotels
19:58 - that are really more like hostels or
20:00 - something that I've just made it past
20:01 - the to start place and so there it's
20:03 - much harder to kind of define or to know
20:06 - what to expect now if you take averages
20:10 - of these star systems though then you do
20:12 - get a much better idea of what's going
20:14 - on so if you have you know consumer
20:16 - reviews or something like that and you
20:18 - say Oh from you know 500 reviews our
20:21 - hotel has an average rating of like 3.8
20:24 - then you know that the three star hotel
20:27 - that you're looking at is pretty much a
20:29 - four star hotel it feels like a four
20:32 - star hotel even though it may not have
20:34 - all of those qualifying characteristics
20:37 - that's the kind of feel you get from it
20:39 - whereas from another three star hotel
20:41 - you may have a rating of like 2.9 or
20:44 - something and there you know oh you know
20:46 - this hotel is more towards the lower end
20:48 - of the three star some people may not
20:49 - even consider it to be three stars and
20:51 - of course you know this rating may be a
20:53 - little bit biased because they went to a
20:55 - different three hard star hotel first
20:57 - and then they went to this one and they
20:59 - were expecting something completely else
21:00 - from the three star hotel so they said
21:02 - this can't be three stars this is two
21:03 - stars but it's because of the way that
21:06 - the ranking system is defined underneath
21:08 - and everything and so when we have these
21:10 - averages of these ordinal numbers then
21:12 - they kind of start to make a little bit
21:14 - more sense alright so let's go over a
21:18 - small exercise and see if we can
21:20 - identify what type of data we're dealing
21:22 - with so the first thing we'll look at is
21:25 - gonna be the survey response to
21:27 - happiness now you have people filling
21:29 - out a survey and then this and then one
21:31 - of the questions is you know how would
21:33 - you rate your happiness and it's gonna
21:34 - be bad neutral good or excellent what
21:38 - type of data with this be well this
21:42 - would be an ordinal type of data because
21:44 - it's still in a form of categories and
21:47 - you're asking for the subjective opinion
21:49 - but it does make sense so you can still
21:51 - compare them you can say excellent is
21:53 - greater than good good is greater than
21:55 - neutral neutral is greater than bad but
21:57 - what exactly does it mean to be good and
22:00 - excellent you know where do different
22:02 - people draw the line for this there's
22:04 - it's still a little bit of vagueness
22:05 - involved but generally it does make
22:07 - sense and you can compare it and if you
22:09 - have a lot of surveys
22:11 - and you averaged them the values you're
22:13 - gonna get are probably going to be very
22:15 - well representative or at least pretty
22:16 - good representative all right so if we
22:20 - look at the next thing which is going to
22:22 - be the height of a child what type of
22:25 - data is that now we can say it's
22:29 - probably numerical and well it actually
22:32 - most definitely is numerical so the
22:35 - height of a child is a numerical value
22:37 - but let's go a little bit deeper and say
22:39 - is the height of a child discrete or is
22:41 - the height of a child continuous well
22:45 - even though when you measure height you
22:47 - get something like you know five foot
22:49 - five foot three or 160 centimeters or
22:53 - something like that it's not a discrete
22:57 - value because to get that height you
22:59 - have to have reached every single height
23:02 - before and so even though at the moment
23:05 - you may be measuring it you're kind of
23:07 - rounding it off to how much your
23:09 - measuring tape can measure so like your
23:12 - measuring tape is kind of limiting the
23:14 - height but if you had a super super
23:16 - precise measuring instrument you could
23:18 - measure not just you know five foot
23:20 - three or something like that you could
23:22 - really go into detail with the inches
23:24 - and the decimal places and there and
23:26 - everything kind of going on so the
23:29 - height of a child would be a numerical
23:30 - data type but it would be continuous all
23:33 - right now let's take about talk about
23:34 - the weight of an adult do you expect the
23:37 - weight of an adult to be either discrete
23:38 - or continuous so we can probably agree
23:41 - that it's numerical because it's a
23:43 - weight value it's it's pretty much
23:45 - defined to be a number and what do you
23:48 - expect it to be discrete or continuous
23:49 - well the right answer here is gonna be
23:52 - continuous again because to reach a
23:55 - certain weight they would have had to
23:57 - have reached every single weight in
23:59 - between before so again the weight is
24:02 - something that we can consider to be
24:04 - continuous all right and so finally
24:07 - let's look at the number of coins in
24:09 - your wallet again we can already by the
24:12 - name it says a number of coins so we can
24:14 - probably agree that this is a numerical
24:17 - type of data but the number of coins in
24:19 - your wallet would that be discrete or
24:22 - continuous well the answer would be
24:24 - discrete
24:24 - because it doesn't really matter what's
24:27 - your anoint your coins are they could be
24:28 - 50 cent pieces that could be 25 cent
24:30 - pieces ten or five or ones or anything
24:33 - you know like a two or something like
24:34 - that but they're not going to be but the
24:37 - number of coins that you're gonna have
24:38 - we're gonna sum up to a whole number so
24:40 - you can have one coin you can have two
24:41 - you can have three all of these things
24:44 - but you can't have infinite fractions of
24:47 - a coin you can't have say you know the
24:50 - square root of two number of coins that
24:52 - doesn't really make sense so you have a
24:55 - defined step size you have one coin and
24:57 - then if you have a second coin then you
24:59 - have to you get a third coin meaning of
25:00 - three you're going in step sizes of one
25:02 - so for the number of coins in your
25:04 - wallet we'd be having discrete numerical
25:08 - data in this tutorial we're going to
25:10 - talk about the different types of
25:12 - averages now we're going to see the
25:14 - three different types of averages which
25:16 - is the mean the median and the mode
25:19 - alright let's get started so we'll start
25:22 - off with the mean now the mean is the
25:24 - typical average that you know and really
25:28 - what the mean is is you just sum all of
25:30 - your values up and then you divide them
25:32 - by the total number of values that you
25:33 - have now the great pros of the mean is
25:36 - that it's very easy to understand it
25:39 - makes sense we just have everything we
25:40 - have and just kind of add it all up and
25:43 - then divided by what we have and that
25:45 - should give us a good representation of
25:48 - what is the average and it also takes
25:50 - into account all of the data so since
25:52 - we're adding everything up and then but
25:54 - dividing by how much data we have we're
25:56 - taking into consideration every single
25:58 - data point now there are some problems
26:01 - with this so one of the problems is that
26:03 - the mean may not always be the best
26:06 - description and we'll see why when we
26:09 - look at examples for when we should use
26:11 - the median and the mode and the mean is
26:14 - also very heavily affected by outliers
26:15 - so since we're taking everything into
26:19 - consideration if we have big outliers
26:22 - that's really gonna change how our mean
26:25 - looks like so if we just have normal
26:27 - values you know between like one and
26:30 - five and all of a sudden we have like
26:31 - 10,000 in there that's really gonna
26:33 - affect our mean so mean is heavily
26:36 - influenced by outliers and the bigger
26:38 - the outlier
26:38 - more the mean is influenced by it all
26:41 - right so let's see some examples of the
26:43 - mean we'll go through a worked example
26:46 - first and we can see our data set here
26:48 - which is just a bunch of numbers and
26:51 - what we're gonna do to calculate the
26:53 - mean is we're just gonna take every
26:55 - single one of these numbers and we're
26:56 - gonna add them up and we can see the
26:59 - total result that we get here and then
27:01 - the next thing we're gonna do is we're
27:02 - gonna take this total result we're gonna
27:03 - count the amount of data points that we
27:06 - have and we're gonna divide one by the
27:08 - other which then gives us our mean as we
27:11 - can see here so that's an example
27:13 - calculation of the mean but let's see
27:15 - some example applications of the means
27:17 - so when would we use it well good
27:19 - application would say if you look at the
27:22 - time it takes you to walk to the
27:24 - supermarket so sometimes you walk a
27:26 - little bit faster and maybe it takes you
27:28 - 20 minutes to get there sometimes you
27:30 - walk a little bit slower it takes you 25
27:32 - but on average it takes you somewhere
27:35 - like 22 or maybe 22 and a half minutes
27:38 - or something like that so if you say I'm
27:40 - gonna go to the supermarket you're like
27:42 - it's gonna take me this much time to get
27:45 - there another good example of the mean
27:48 - would be exam score for a class so to
27:51 - get a good understanding of how people
27:54 - do in an exam or in a class you can look
27:58 - at the mean exam score last year and
28:00 - since our exam scores are kind of in a
28:03 - smaller range a mean is gonna be good to
28:06 - use it because you can get anything
28:07 - between 0 and 100 but realistically
28:10 - speaking no one's probably going to get
28:12 - a zero so your range is even smaller and
28:14 - so you're less affected by outliers and
28:16 - you kind of know how hard a class is
28:19 - gonna be just by being you know able to
28:21 - compare their means so if you look at
28:22 - one class and it's mean is higher than
28:26 - the other but they have a large number
28:28 - of students or something then you can
28:30 - probably say hey it's easier to get a
28:32 - good grade here or something like that
28:33 - or maybe you know some of these it's
28:36 - more simpler overviews without diving
28:38 - too deep into it alright another good
28:41 - example of the mean would be to say how
28:43 - much chocolate do you require when you
28:45 - get this kind of sweet craving and
28:47 - you're not gonna say like oh you know I
28:50 - require one chocolate bar two chocolate
28:52 - bars
28:52 - but like you're gonna say Oh on average
28:55 - you know I require you know maybe
28:57 - three-quarters of a chocolate bar and
28:59 - sometimes I may want a little bit more
29:01 - because I feel like it and when I start
29:03 - eating chocolate I crave it even more
29:05 - sometimes you know I have it at first
29:08 - and like the tasters doesn't sit right
29:09 - with me right now and so I have a little
29:11 - bit less but these are kind of the
29:14 - amount of things so like if you have
29:15 - this craving you know either you say oh
29:17 - I'm gonna try to be strong or you like
29:18 - hmm well I know this feeling and I know
29:20 - if I eat about you know three-quarters
29:22 - of a bar of chocolate or something I'm
29:24 - gonna feel good my craving is gonna be
29:26 - satisfied so you kind of know what to
29:29 - expect so these are some of the examples
29:31 - for how we would deal with a mean well
29:35 - when we would use mean all right so
29:37 - let's look at the next thing which is
29:38 - going to be the median now the median
29:41 - represents the middle value in your debt
29:44 - data says now if you have an even number
29:47 - of data points you don't really have a
29:48 - middle value and so in that case the
29:50 - meeting is gonna be the mean of the two
29:53 - values so it's going to be the two
29:55 - meeting values added together and then
29:57 - divided by two so the pros of using a
30:01 - median value is that the median can
30:03 - sometimes be more accurate than the mean
30:06 - and we'll see some examples of this
30:08 - the median also evenly splits your data
30:10 - so you're not really you know affected
30:13 - by the mean in the sense that if you
30:14 - have an outlier in the mean and it drags
30:17 - everything to the right it could be that
30:19 - your outlier drags things so far to the
30:22 - right that all of your data is to the
30:24 - left of the mean and only the outliers
30:25 - to the right so that would be an extreme
30:27 - case but that can happen whereas the
30:29 - median you know it's always located
30:31 - directly in the center of your data and
30:34 - the median also doesn't care about
30:35 - outliers so if you have huge outliers at
30:37 - the beginning and at the end it doesn't
30:40 - really care because outliers by
30:41 - definition aren't very common because
30:44 - they're outliers and so if you have them
30:46 - at the beginning or house them at the
30:47 - end they're gonna be very few in number
30:49 - which makes them outliers and therefore
30:52 - the median doesn't really care about
30:54 - outliers that much a con though is that
30:57 - the median doesn't really give you much
30:59 - information on the rest of the data
31:00 - sure you know you know what's at the
31:03 - center but you don't know how does
31:04 - everything
31:05 - we behave you only know where is the
31:08 - center of our data so let's see some
31:10 - examples well do a working example first
31:13 - where we see our data set here and we
31:15 - can count how many values we have is we
31:17 - go from left to right then we can say
31:18 - we've got 1 2 3 4 5 6 7 8 9 10 11 12 and
31:26 - 13 data points so we've got an odd
31:28 - number and so our median value our
31:30 - center value is going to be the seventh
31:33 - data point because it's 6 from the
31:36 - beginning and it's also 6 from the end
31:38 - so it's equally spaced both from the
31:40 - beginning and from the end and so that's
31:43 - why we see our median value here is 26
31:45 - it's located directly in the center now
31:49 - what is the median useful for well the
31:52 - median is often used if you look at you
31:54 - know household incomes for a country
31:56 - because if you were to use the moon then
31:58 - these billionaires they would just
32:00 - completely you know they would give you
32:03 - a false description of what really an
32:06 - average household income is because
32:09 - normally if you have you know like an
32:12 - average value and you can say oh the
32:13 - average household income from this
32:15 - family would be say $40,000 or something
32:18 - like that or that would be the median
32:20 - value but if you were to use the mean
32:22 - instead then all of the billionaires and
32:24 - all the millionaires in the country they
32:25 - would change that household income and
32:28 - then you would say oh you know the
32:29 - average household income per family
32:31 - would look like 60k and that's a bad
32:33 - representation because that doesn't
32:36 - actually give you a realistic look at
32:39 - what the average household family has
32:42 - and the average household family really
32:44 - does it's you know centered at like 40k
32:47 - and sure there are people below and
32:48 - there few people be high but that's
32:50 - what's in the middle whereas if you were
32:52 - to use the mean instead for your average
32:54 - you would kind of get this inflated
32:57 - household income which wouldn't be
32:58 - representative to the rest of your and
33:01 - the rest of the country another good
33:04 - example of the median would be the
33:06 - distance that people cover to get to
33:08 - work so if you look at this in terms of
33:10 - you know kilometers then you can say
33:12 - like oh you know some people they walk
33:14 - to work and it's like you know one
33:16 - kilometer at most so something like that
33:19 - and then you can expect people to travel
33:21 - most people travel around three
33:23 - kilometers to work and sure there are
33:25 - some you know that travel much further
33:26 - because they want to live outside of the
33:28 - city and there are some that travel very
33:31 - very short distances because they have a
33:32 - house right next to the office where
33:35 - their house is the office or something
33:36 - like that depending on where you're
33:37 - working but then you can look at you
33:40 - know like we're in the middle how do
33:43 - people travel to work what time or what
33:45 - distance do they need to cover and so
33:48 - that would be another good use of the
33:50 - median and a meeting another good
33:52 - meeting value is what do you usually
33:54 - spend when you buy a new item of
33:56 - clothing and so sure you know sometimes
33:58 - may go to that expensive clothing store
34:00 - and you could get a jacket that costs I
34:03 - don't know north of a couple hundred
34:04 - euros or dollars whatever system you
34:06 - want to use and sometimes you can go to
34:08 - a secondhand store and get it for very
34:10 - cheap but usually if you go into stores
34:13 - a jacket I don't know maybe cost you
34:15 - like a hundred dollars or something like
34:17 - that and so you know if you go out you
34:19 - can expect to pay about $100 no not
34:24 - really you know taking that much
34:26 - accountant - what story going in - so
34:28 - most of the stores that you're gonna
34:29 - visit are gonna have that price for the
34:32 - jacket so that would be another good use
34:34 - for the median all right let's look at
34:36 - the third type of average that we can do
34:38 - which is the mode now the mode looks at
34:41 - the most common value in your data and
34:44 - it's not really defined if there are
34:46 - several most common values but if
34:48 - there's only one most occurring value
34:50 - then that's what your mode would be and
34:53 - so we'll see an example of this in a
34:54 - second to the pros of using the mode is
34:57 - that it's not only applicable to
34:59 - numerical data so if you look at
35:01 - categories for example then you can say
35:03 - hey we've got five people from the US
35:05 - you know and two from Canada and one
35:08 - from France and you know that the mode
35:11 - is gonna be the US because there are
35:13 - five people from the US so mode is the
35:17 - great average that's not only applicable
35:18 - to numerical data in this sense but you
35:22 - can technically also apply it to
35:24 - categories or to ordinal numbers if you
35:27 - wanted so that you can say the most
35:28 - common country that we have were the the
35:31 - average kind of country that we would
35:32 - expect
35:33 - tear is the US and sure there are other
35:35 - countries but the average or the most
35:37 - common one is gonna be the u.s. in this
35:39 - case so yeah and then of course and the
35:43 - other Pro is that we allow to see what's
35:45 - most common what pops up the most so
35:48 - that's a great use of the mode if there
35:51 - are cases when you know recurring values
35:54 - happen a lot which is the case for
35:56 - discrete numbers for example so in
35:58 - discrete numbers values recur often and
36:01 - so it's good to use the mode icon of the
36:05 - mode it's gonna be that it doesn't
36:07 - really again give you good understanding
36:09 - the rest of the data similar to what we
36:11 - had for the median but also it's not
36:13 - really applicable if you just have a
36:15 - bunch of different types of data then
36:17 - there isn't really gonna be a mode if
36:19 - there's not enough of each data it's not
36:22 - really good to use the mode you don't
36:23 - want to you know have thousands of data
36:25 - points and the most reoccurring value it
36:28 - reoccurs like three times that's not
36:31 - good you want to use the mode for
36:33 - situations where data re occurs often so
36:36 - like we saw the country example but
36:38 - let's actually see a worked example but
36:41 - also some other examples for the mode so
36:43 - the worked example here would be again
36:45 - we take our data set and we can count
36:48 - how many times different numbers appear
36:50 - and so if we go through the numbers
36:52 - we'll see that twenty six occurs the
36:55 - most and so that's gonna be our mode
36:57 - here so we've got 22 and 25 that both
37:00 - occurred twice but 26 occurs three times
37:03 - and so 26 is gonna be our Millah it's
37:06 - gonna be our most occurring value now
37:08 - the mode is gonna be useful for things
37:11 - like the peak of a histogram so if you
37:14 - draw this histogram and if you don't
37:15 - know what a histogram is don't worry
37:16 - we'll cover that in a later lecture to
37:19 - let me go into data visualization but
37:21 - the peak of a histogram that's gonna
37:23 - show you the mode of the data the most
37:25 - occurring data a good another use of the
37:29 - mode will be if you look at employee
37:31 - income at a company because at a company
37:33 - you know you can again have the boss
37:34 - which takes off the mean and you can
37:37 - have you know higher level employees to
37:40 - which we kind of shift the median but if
37:42 - one third of your employees earn minimum
37:45 - wage
37:45 - that's gonna be the best average or say
37:49 - 40% of your employees earn minimum wage
37:51 - you're probably not your employees
37:52 - because that wouldn't be a very good
37:54 - system to have but a 40% of the
37:56 - employees at the company that you're
37:57 - looking at earning a minimum wage that's
38:00 - not a really good thing to have and if
38:03 - you look at the mode you'll easily see
38:05 - that the average in this case would be
38:07 - to earn minimum wage because that's what
38:09 - most people earn and sure you know the
38:12 - boss he or the CEO or something you know
38:15 - he may shift the mean up heavily and
38:17 - then the fact that you have higher ups
38:19 - if you look at the median value you may
38:21 - even well be too far you know too far to
38:26 - the right that you really don't consider
38:27 - these employees that all are in the same
38:29 - amount but you really want to get that
38:32 - description which is what you get here
38:34 - from the mode and then also the out kind
38:36 - of an of an election is where you use
38:37 - the mode for and sure sometimes you may
38:40 - only have two values sometimes you may
38:41 - have three but if you have different
38:44 - candidates and say you have five
38:46 - different candidates then the person
38:48 - with the most votes is gonna win the
38:50 - election because they have the most and
38:52 - so they are again you'll use the mode in
38:54 - this lecture we're gonna look at a
38:57 - spread of data and we're going to start
38:59 - off with looking at the terms range and
39:01 - domain then we're gonna move on to
39:03 - understanding what variance and standard
39:04 - deviation means and then finally we'll
39:07 - look at covariance as well as
39:08 - correlation all right so let's start off
39:11 - with a range and domain now let's start
39:15 - off with the range though so the range
39:16 - is basically the difference between the
39:18 - maximum and the minimum value in our
39:21 - data set so that's that's kind of simple
39:23 - to think about so let's just kind of go
39:26 - through this with a work example let's
39:28 - set up a company in the town and this is
39:30 - the only company in the town and the
39:32 - owner of the company earns a salary of
39:34 - 200k a year and then the employees you
39:38 - know they all have different salaries
39:39 - but the lowest employees or maybe the
39:41 - part-time workers they earn something
39:43 - like 50k a year so we've got data kind
39:47 - of ranging from 15k up to 200 K and so
39:51 - our range is the difference between the
39:53 - maximum and the minimum value in our DNA
39:54 - so we take 200 K and we subtract 15 K
39:58 - from it
39:59 - and we've got a range of 185 K in salary
40:03 - so that's how big our salary can change
40:07 - so it can if we start at 15 K it can go
40:10 - all the way up to 200 K so that's a
40:12 - hundred and eighty five K range of
40:14 - salary that people in this company can
40:16 - have all right and the domain is going
40:19 - to be the values that our data points
40:21 - can take on or the region that our data
40:24 - points lie in so if we look at this
40:27 - example again our domain is gonna start
40:30 - at 15 K and go up to 200 K so what the
40:34 - domain defines is it defines kind of
40:37 - starting and ending points or it defines
40:40 - a section in our data and so in this
40:44 - case the domain would define you know we
40:46 - would start at 15k and it would end at
40:48 - 200 K and what the domain tells us is
40:50 - that everything or all salaries within
40:54 - you know between 15k and 200 K that they
40:57 - are possible but within this domain or
41:00 - within this company it's not possible to
41:02 - have salaries outside of the glist
41:04 - domain so if our domain again is 15k to
41:06 - 200 K then we can't have a salary of 14
41:10 - K because that's outside of our domain
41:12 - and we also can't have a salary of 205 K
41:15 - because again that's outside of our
41:17 - domain so pretty much all salaries
41:20 - within 15 to 200 K are possible anything
41:24 - outside of the domain is not possible
41:26 - because that's no longer in our domain
41:28 - all right so let's move on and look at
41:31 - the variance and standard deviation and
41:34 - we'll talk about the variance first and
41:37 - what the variance tells us it pretty
41:39 - much tells us how much our data differs
41:41 - from the mean value and it looks at each
41:43 - mean value and it looks at how different
41:46 - each value is from the mean value and
41:49 - then it gives us the variance it does
41:51 - some calculation and we don't really
41:53 - need to know the formula it's more
41:54 - important right now just to understand
41:56 - the concept of variance and so what it
41:59 - variance really tells us is it tells us
42:00 - how much our data can fluctuate so if we
42:03 - have a high variance that means a lot of
42:06 - our values differ greatly from the mean
42:08 - value and that will make our very
42:10 - it's bigger if we have a low variance
42:12 - that means a lot of our values are very
42:15 - close to the mean value and so that will
42:17 - make our variance lower and now if we
42:20 - turn to the standard deviation the
42:22 - standard deviation is literally just the
42:24 - square root of the variance so if you
42:27 - understand one then you also understand
42:29 - the other and now we can combine this if
42:32 - we know the range of our data to kind of
42:35 - get a better feel for our data and so
42:37 - let's use an example where we have two
42:40 - different countries just countries a and
42:42 - B and they have the same mean height for
42:45 - women which in this case will say is 165
42:48 - centimeters or 5 feet 4 and we'll say
42:52 - that the range of heights for them could
42:55 - be identical so let's say they can range
42:57 - you know the range let's say it could be
42:59 - like 30 centimeters or something you can
43:01 - go anywhere from say 150 all the way up
43:05 - to 80 or we can even increase that and
43:07 - say like anywhere from as low as 140 up
43:09 - to like two meters or something like
43:11 - that but let's just keep the range for
43:13 - these the same and they both have the
43:15 - mean height now if country a has a
43:18 - standard deviation of five centimeters
43:20 - which is approximately two inches and
43:23 - country B has a standard deviation of 10
43:25 - centimeters which is approximately four
43:28 - interests then what you can expect
43:30 - knowing these values is that if you go
43:32 - into country a the people that you're
43:34 - gonna see are gonna be much more similar
43:38 - in height so our standard deviation is
43:40 - lower that means our values differ lower
43:43 - from the mean and so that means a lot of
43:45 - the women that you're gonna see are
43:47 - going to be very close to 165
43:50 - centimeters or 5 feet 4 plus minus 2
43:53 - inches so it's very what you can expect
43:55 - when you go to this company that when
43:56 - you go to this country is that everyone
43:59 - is gonna be or every a lot of the women
44:01 - are gonna be about that height whereas
44:03 - if you go to country B they have a much
44:06 - larger standard deviation and so you
44:09 - can't really expect everyone to be about
44:10 - 5'4 because it fluctuates a lot more and
44:14 - so if you go to that country you can
44:16 - expect to see a lot more women of
44:18 - different heights both taller and
44:20 - shorter than
44:22 - for all right and so that's how we can
44:25 - kind of use the variants in the standard
44:27 - deviation or the standard deviation to
44:30 - give us a little bit more perspective on
44:33 - our data and kind of allow us to infer
44:35 - some stuff about our data all right so
44:38 - let's talk about covariance and
44:40 - correlation and so covariance will or
44:44 - already has the name very incident but
44:47 - covariance is measured between two
44:49 - different variables and it pretty much
44:53 - measures if you have two variables so
44:56 - let's say we've got you know me drinking
44:59 - coffee in the morning and my general
45:02 - tiredness so if I use these two values
45:06 - and you know get data point so this is
45:08 - how much coffee I drank in the morning
45:10 - and this is how tired I feel this
45:12 - morning or something like that and so
45:13 - what the covariance does is it looks at
45:15 - how much one of these values differs or
45:19 - changes when I change the other one so
45:22 - what does that mean for example well if
45:24 - I drink more coffee what the covariance
45:26 - would look at is how much does my
45:28 - tiredness change so that's what you do
45:31 - with covariance you see you say I change
45:34 - one how much does that affect the other
45:36 - thing that I look at and our correlation
45:39 - is very similar to covariance so we kind
45:42 - of normalize the covariance by dividing
45:44 - by the standard deviation of each
45:46 - variable so what that means is we get
45:48 - the covariance for my drinking coffee
45:49 - versus feeling tired and then we would
45:52 - just divide by the standard deviation of
45:55 - me drinking coffee and a standard
45:56 - deviation of me feeling tired and so
45:59 - really what we're doing with the
46:01 - correlation is we're just kind of
46:02 - bringing it down to relative terms that
46:06 - would fit our data better so that's kind
46:09 - of the abstract idea the important thing
46:11 - to just keep in mind is that we're
46:12 - looking at one and we're seeing how much
46:14 - that changes and we're seeing how much
46:17 - that change affects the other one all
46:21 - right so there are different types of
46:22 - correlation values that we can have and
46:24 - they can range anywhere between negative
46:26 - 1 and 1 or so their domain is between
46:28 - negative 1 and 1 and a correlation of 1
46:31 - means a perfect positive correlation so
46:35 - that
46:35 - when one variable goes up the other goes
46:37 - up so for my coffee example that would
46:40 - be if I have coffee in the morning then
46:43 - I also feel more happy so the more
46:46 - coffee I have the more happy I feel and
46:49 - of course there's going to be a limit
46:50 - but let's say I only drink up to two
46:52 - cups of coffee or something like that
46:53 - and I can drink anything in between and
46:56 - the more I have the more happy I am
46:58 - about it so that would be a positive
47:00 - correlation the more I have of coffee
47:03 - the more I have of happiness and so they
47:06 - would kind of go up together
47:07 - and then when we get closer to zero the
47:12 - zero point is gonna mean no correlation
47:14 - to us so anything between zero and one
47:16 - is going to be a kind of slightly
47:18 - positive correlation it's not going to
47:19 - be a super strong and we'll actually see
47:21 - some examples on the next slide but yeah
47:25 - so anything between zero and one is
47:26 - going to be a kind of slight positive
47:27 - correlation not super strong and the
47:30 - closer you get to zero the more it means
47:32 - no correlation so an example for the
47:35 - zero case would be that it doesn't
47:37 - matter how much coffee I drink in the
47:38 - morning it's not gonna affect the
47:40 - whether they're unrelated one does not
47:42 - affect the other so I could drink you
47:45 - know one cup of coffee during a sunny
47:47 - day and one cup of coffee during the
47:49 - rainy day and it's not gonna change the
47:51 - weather it's not gonna affect the
47:52 - weather so they're pretty much
47:54 - uncorrelated and then we can also go
47:57 - down into the negative range and so the
48:00 - closer we get to negative one or if we
48:02 - reach exactly negative one that
48:03 - correlation of negative one means a
48:05 - perfectly negative correlation and so
48:08 - here we can take our example of coffee
48:10 - versus tiredness and so the more coffee
48:13 - I have the less tired I'm gonna be so
48:16 - coffee goes up and tiredness goes down
48:20 - so that's how we can kind of understand
48:22 - this correlation and it comes from the
48:25 - covariance so it was important to
48:27 - understand the covariance we usually use
48:29 - the correlation because the correlation
48:31 - because we divided by the standard
48:33 - deviation of each is much better fit to
48:36 - our data now there is one thing that's
48:38 - very important to remember and that's
48:40 - that correlation does not imply
48:42 - causation so just because two things are
48:45 - correlated that does not mean that
48:48 - one causes the other so a good example
48:50 - of this would be if I live in a climate
48:53 - where it's usually cloudy in the morning
48:55 - and I know it to be sunny in the
48:57 - afternoon but every morning when it's
48:59 - cloudy I drink coffee and then it
49:02 - becomes sunny in the afternoon that's
49:04 - not even though they may be correlated
49:06 - me drinking coffee and it becoming sunny
49:08 - it me drinking coffee does not cause it
49:11 - to be sunny that's just you know by
49:13 - chance it's just because it happens
49:15 - every day and by chance there's this
49:18 - kind of correlation that appears but
49:20 - that does not mean that me drinking
49:21 - coffee you know
49:23 - results in the weather getting better a
49:26 - causation would be me drinking coffee
49:29 - and me feeling less tired or me drinking
49:32 - coffee and me feeling happy about it
49:35 - because I like the taste those would be
49:37 - causations so that's an important thing
49:39 - to keep in mind just because things are
49:41 - correlated does not mean that one causes
49:43 - the other all right so let's see these
49:46 - things on a graph and so here we have
49:48 - the examples again that we've talked
49:49 - about but we can kind of see how the
49:51 - data would look like for different types
49:53 - of correlations and so we can see a
49:56 - perfectly the perfect correlation of one
49:59 - so one goes up the other goes up we can
50:02 - see on the left side and we pretty much
50:04 - get this really nice straight line so
50:06 - one value goes up the other value goes
50:09 - up with it and then the closer we reach
50:11 - zero the less related or the less
50:15 - correlation there is between them and
50:17 - then the more kind of variance we have
50:19 - in data so we'll notice for the case of
50:23 - perfect correlation which is the one or
50:25 - the case of perfect anti-correlation
50:27 - which is the minus one which again we
50:29 - had the example of more coffee less
50:31 - tired and in those cases you know we
50:33 - have a very nice thin line and our data
50:37 - doesn't jump around a lot but the closer
50:39 - we get to zero the less we can see you
50:42 - know one causing the other and the more
50:44 - we can see our data kind of spread out
50:46 - and so that's what correlation would
50:50 - look like in terms of graphics in this
50:53 - tutorial we're gonna go through
50:54 - quantiles and percentiles all right so
50:58 - let's get started
50:58 - so what our quantiles
51:01 - quantiles allow us to split our data
51:04 - into certain regions that if we're
51:06 - dealing with probability they all have
51:08 - the same probability of occurring or if
51:10 - we're just dealing with you know sizes
51:12 - of data we want to split our data into
51:14 - equal regions so that's what we can do
51:17 - with quantiles is just splitting
51:19 - everything up so that every time we
51:21 - split it you know we have equal amounts
51:24 - of data all right and so an example of a
51:28 - quantile would be something known as a
51:31 - quartile and so that's when we split our
51:34 - data into four equal regions hence the
51:37 - name quartile so a quantile is the
51:39 - general name for doing this
51:41 - splitting procedure and then if we say
51:43 - quartile that means we're doing
51:45 - quantiles but for four equal regions and
51:48 - so this is something that you'd probably
51:50 - often see unlike university admissions
51:51 - pages or something like that and they
51:53 - say the top 25 percent of our applicants
51:56 - have at least a test score of like 90
51:59 - percent or something you know and then
52:00 - they would say the bottom 25 percent for
52:03 - applicants or our admission or admitted
52:06 - students or something like that have a
52:07 - test score that is I don't know 70
52:10 - percent or 75 percent or something like
52:11 - that and then the median test score is
52:15 - 85 percent so that's how you would go
52:18 - about quartiles is that you would have
52:20 - you know the lower 25% the middle 25 to
52:25 - 50 then you've got the 50 to 75 and then
52:28 - you've got the top 25 percent so the 75%
52:31 - to 100 and so you've got these four
52:33 - equal regions which also include your
52:35 - minimum value at the very bottom your
52:37 - maximum at the very top and in the
52:39 - middle you've got your median values so
52:42 - that's the value directly in the middle
52:44 - it's because you're splitting it up into
52:46 - four equal regions and so the value that
52:49 - separates the second quantile which
52:52 - would be the 25 to 50 from the third
52:55 - quartile which would be from 50 to 75
52:58 - that value there would be the median
53:00 - value all right and so if we go into
53:03 - percentiles so percentiles that may have
53:07 - been a name that you you've probably
53:08 - heard before percentiles again an
53:10 - example of a quantile but instead of
53:14 - saying you know like a quartile we do it
53:16 - for for a percentile
53:18 - I mean splitting it into 100 equal
53:20 - segments
53:21 - hence the percentiles of the perks name
53:25 - at the beginning though that's that's
53:27 - kind of where or the percent you may
53:29 - have noticed percent means out of 100 or
53:32 - so that's if you are familiar with
53:34 - percent and that's also the same kind of
53:37 - reasoning where this comes from and so
53:38 - we've got percentiles which means
53:41 - splitting into you 100 equal segments
53:44 - and so on an example of this is often
53:48 - used in test scores so if you've ever
53:52 - taken something like the SATs or
53:54 - something like that then you get a test
53:56 - score but you also get a percentile and
53:59 - the reason I've done that is it's to
54:01 - judge not you versus the test but you
54:04 - versus everyone else and so if it's a
54:06 - difficult test than something like
54:09 - getting a test score of 60% but you're
54:13 - in the 95th percentile means your score
54:16 - is actually a lot better and so what you
54:18 - can say with percentiles for example is
54:20 - that every percentile that you're in
54:22 - means you're better than you know that's
54:25 - many other people so for example if you
54:28 - reach the 99th percentile that means
54:30 - you're better than 99% of the people
54:32 - that took the test the 95th percentile
54:34 - would be 90 you're better than 95% of
54:37 - the people that take the test or
54:38 - something like that and so that's why
54:40 - percentiles are often used for tests and
54:43 - they're often used for normalization
54:44 - because they allow you to take into
54:47 - consideration you know these factors of
54:48 - like is it a difficult test is an easier
54:51 - test maybe more people are scoring
54:52 - higher so they don't really judge you
54:55 - directly versus the test but they
54:58 - normalize you against everyone else that
55:01 - took the test so you take the test you
55:03 - get a score and then um the percentile
55:07 - checks where that score lies relative to
55:11 - everyone else and so these percentiles
55:13 - they allow you to give a good
55:15 - normalization and they allow you to do
55:16 - great comparisons because they allow you
55:19 - to kind of eliminate some of these
55:21 - factors of test difficulty and of course
55:24 - you know there can always be luck
55:25 - involved and stuff and that may not get
55:27 - filtered out on and
55:28 - visual basis but if you do this for a
55:30 - lot of students and that's also why it's
55:32 - done in these kind of big standardized
55:33 - tests is that you get a percentile along
55:36 - with your score so that you understand
55:39 - if you know maybe if your score is lower
55:41 - but the test was really hard you can
55:44 - still see you know I I did really well
55:45 - because people found this test really
55:47 - hard and it was even harder for them
55:50 - than it was for me in this tutorial
55:52 - we're going to talk about the importance
55:54 - of data visualization all right so what
55:58 - we're going to talk about is first we're
56:00 - gonna look at the role that the computer
56:02 - plays kind of for us and what role the
56:05 - computer is actually made for then we're
56:07 - gonna look at what role the human should
56:10 - play in terms of data science then we're
56:12 - going to look at presenting data and
56:15 - finally we'll talk about interpreting
56:17 - data alright so let's get started and
56:20 - talk about the role that the computer
56:22 - plays no computer is much much faster at
56:26 - calculating than human because that's
56:27 - what it's made for it's made for
56:30 - crunching numbers it's made for doing
56:32 - fast calculations you know if you think
56:35 - about how faster computers are there in
56:36 - the gigahertz range so Giga means
56:39 - billions so they just do billions of
56:42 - things every second and so they're
56:44 - really good for doing repetitive things
56:46 - because they can do them so fast and
56:49 - then we can give them these logical
56:51 - tasks in terms of programming and we
56:54 - give them a structure and they just do
56:57 - it and they can do it over and over and
56:58 - over again they're not gonna mess up I
57:00 - can just repeat the same thing they
57:02 - won't get tired of it and they're really
57:05 - good and they're really fast at doing
57:07 - these things so that's the role that the
57:09 - computer should play for you it should
57:11 - be kind of a means to get these hard
57:14 - number crunching and all of these things
57:16 - done so there's there's really no need
57:18 - for you to kind of work out all this
57:20 - complicated math because your computer
57:21 - can do it much better and much faster
57:24 - than you and it's also less prone to
57:27 - error if you code it correctly so that's
57:30 - kind of the only part where you come in
57:31 - and it's only gonna mess up if you mess
57:33 - up but generally our computer does
57:35 - exactly what we tell it to do and it's
57:37 - really good and it's
57:38 - really fast at it now what role should a
57:41 - human play in terms of data science well
57:44 - humans have naturally developed to
57:47 - identify patterns and we've done this
57:49 - first for survival so that if we're
57:52 - walking around somewhere and we see a I
57:55 - don't know a big predator or hiding that
57:58 - we can identify that pattern of the
58:01 - predator and we can kind of pick it out
58:03 - even though it's trying to camouflage
58:04 - itself so humans by nature have become
58:08 - very very good at identifying patterns
58:10 - and you can also see this if you look at
58:13 - the clouds and you see things where you
58:14 - see animal shapes and the clouds or
58:16 - other things so those patterns aren't
58:18 - actually there but humans have become so
58:20 - good at identifying patterns we can see
58:23 - things in many many places and so that's
58:26 - what humans are really really good at
58:28 - we're able to look at things and we're
58:30 - able to pick out patterns now another
58:33 - thing that's really good for humans is
58:35 - we are very creative and through their
58:38 - creativity we can also use memory and
58:41 - bring it outside knowledge and we can
58:43 - also use a general understanding so
58:45 - these are all things that computers
58:47 - can't do so computers are kind of a
58:50 - means of getting stuff to us but once
58:52 - it's actually there it's our job to use
58:55 - our pattern recognition abilities and of
58:58 - course you can train machine learning
59:00 - algorithms for specific patterns later
59:01 - on or specific cases and make them
59:04 - really good at that but generally if you
59:05 - don't know exactly what's gonna come
59:07 - then our or your first step as a data
59:10 - scientist would be to try to identify
59:12 - these patterns use your creativity use
59:15 - your memory you know bring in all of
59:16 - these different things use all of these
59:18 - different things that make you human and
59:19 - use all of that on the data all of these
59:22 - things that a computer just doesn't have
59:24 - any access to okay
59:27 - so usually you know you're considering
59:29 - all this the best way to do all this
59:32 - would be in terms of data visualization
59:33 - so you can't just show spreadsheets with
59:38 - a bunch of numbers that won't really
59:39 - help you because looking at numbers it's
59:41 - really hard to pick out patterns the
59:44 - best way to do it would just be to plot
59:46 - values and then if we have these visuals
59:48 - in front of us then you know we can
59:50 - really identify
59:51 - we can see things go up and down and you
59:54 - know we can see them fluctuating and we
59:55 - can see them make very thin lines we can
59:58 - just look at a graph and we can just see
60:00 - things and of course you know we need a
60:02 - little bit of practice to understand
60:04 - what that graph is trying to tell us
60:06 - but once we understand the graph and in
60:09 - general then you know we can look at new
60:11 - grass and we can just see things so we
60:13 - can start to see patterns and they may
60:16 - not always be true but that doesn't mean
60:18 - you know and we can't pick them out and
60:20 - then that's later on you would also do
60:22 - some testing trying to see if those
60:23 - patterns are true if they make sense but
60:25 - generally data visualization is very
60:27 - good for this because it allows you to
60:29 - invoke all of your human characteristics
60:33 - the things that are really good that you
60:35 - know make us human the things that we
60:38 - talked about and the last slide all the
60:40 - things like the computer can't do and
60:43 - sometimes you know you if you deal just
60:46 - with just these numbers it's data
60:49 - visualization is for you and one sense
60:51 - so that you can see these things and try
60:53 - to pick them out I use them later on but
60:55 - also if you're trying to show these
60:57 - things to other people so maybe you have
61:00 - to do a presentation in a kind of
61:01 - summary then you want to make sure that
61:03 - your data visualizations are good
61:05 - because the people that are going to be
61:07 - looking at it are much much less trained
61:09 - looking at data and analyzing data than
61:12 - you are and so if you try to convey them
61:14 - a message and just show them a big
61:15 - spreadsheet with numbers and just point
61:17 - out like here look look look these
61:19 - numbers you know they pop up and they're
61:21 - gonna be like what are you talking about
61:22 - so that's why it's really important to
61:25 - have really good data visualization
61:26 - skills one of them is to enable you to
61:29 - do your job but the other part of it is
61:31 - to show it to other people and to kind
61:34 - of help you convey information to them
61:37 - you know and of course we talked about
61:40 - statistical values and statistical
61:41 - values are very important and they can
61:44 - give us a kind of good idea about the
61:47 - data and what's going on inside of the
61:48 - data but visualizing data is just taking
61:51 - it to the next level and statistical
61:52 - values aren't enough there they can give
61:55 - us you know they can help us they can
61:57 - support us they can give us ideas but if
61:59 - we really want to understand what's
62:01 - going on sometimes we just have to take
62:02 - a look at what's going on
62:05 - and of course they are it's also
62:07 - important to make sure you choose the
62:08 - right visualizations and everything
62:10 - because other times you know may just
62:12 - look extremely weird but just this skill
62:15 - of being able to present data both for
62:17 - yourself as well as for other people is
62:19 - very very important for a data
62:22 - scientists and then we go over to
62:25 - interpreting data and we've kind of
62:27 - touched on this in the last section
62:28 - already but really with data
62:29 - visualization it just allows you to see
62:32 - this data and it allows you to apply
62:35 - some reasoning to the system and so you
62:36 - can if you look at data either you see
62:39 - something which is great you know that
62:41 - means you can try to test something see
62:44 - if it's actually there or you don't see
62:46 - anything and that also tells you
62:47 - something that you aren't really able to
62:50 - pick out a pattern so that there isn't
62:51 - there isn't anything obvious that's
62:53 - going on there may be something
62:54 - underlying that's more complicated but
62:57 - obvious to the user you know it's not
62:59 - there and so all of these things allow
63:02 - you to you know kind of easily or much
63:05 - more easily analyze your data and kind
63:07 - of prepare what are you gonna do after
63:08 - that so this data visualization it
63:11 - really gives you a deep deep
63:13 - understanding of what's going on with
63:15 - your data and then when we interpret
63:17 - this data and we look at these
63:19 - visualizations you know maybe you see
63:21 - dips and you know maybe you see some
63:24 - hills somewhere we can try to understand
63:27 - all of this by bringing in our outside
63:29 - knowledge so again what the human is
63:30 - really good at we can you know bring in
63:33 - the context of things you know maybe
63:35 - people are going out to lunch here and
63:37 - so that's why activity decreases or
63:39 - maybe everyone is coming to work in the
63:41 - morning and so that's why activity
63:42 - increases compared to you know 6 a.m. so
63:45 - all of these things we can bring in all
63:48 - of this context we can bring in all of
63:49 - this understanding to try to interpret
63:52 - the data try to better understand what's
63:55 - going on and then of course we're gonna
63:58 - see hopefully some trends or patterns of
64:02 - course like I said these may not always
64:04 - be there so we're actually so good at
64:06 - pattern recognition that we can see
64:09 - sometimes patterns that aren't really
64:10 - there and so a good example again of
64:12 - this would be just looking at the clouds
64:14 - in the sky and you can see animal
64:16 - patterns maybe but that's
64:18 - really not there that's just our minds
64:20 - you know identifying all of these
64:21 - patterns and so yeah that's that's
64:25 - pretty much why data visualization is so
64:27 - important to a data scientist it's
64:30 - because this whole you human aspect is
64:32 - it's just key in data science it's key
64:36 - and data analytics to be able to
64:38 - understand what's in front of you to be
64:40 - able to understand these this outside
64:42 - knowledge to be able to contextualize
64:43 - this creativity that's really key to a
64:47 - good data scientist and a computer can
64:50 - help you with all of this the computer
64:51 - can help you you know do the number
64:53 - crunching a computer can help you set up
64:55 - the visualizations and it can plot
64:57 - whatever you want for it but ultimately
64:58 - it's up to you to choose the right
65:00 - visualizations to do to look at the data
65:02 - to be able to communicate the
65:05 - visualization as well all of those
65:06 - things are up to you and so that's why
65:09 - the human is so so important in data
65:11 - science in this tutorial we're going to
65:14 - look at one variable graph so we're
65:16 - actually going to see some of the types
65:18 - of graphs that we can do you know that
65:20 - we talked about in our last tutorial
65:22 - where we just looked at the importance
65:24 - of data visualization so now we're gonna
65:25 - go into data visualization and look at
65:28 - the types of grass that you may want to
65:31 - use or that you may want to choose from
65:33 - all right and so the graphs that we're
65:36 - gonna look out in terms of one variable
65:37 - graphs are gonna be histograms bar plots
65:40 - and pie charts so let's get started with
65:43 - histograms now we can see an example of
65:47 - a histogram on the right but what's
65:49 - really cool about histograms is that it
65:51 - shows us the distribution of the data
65:53 - and it shows us the distribution across
65:55 - all the values in our data and so it
65:58 - shows us what happens the least and it
65:59 - also shows us what happens the most and
66:03 - histograms it they let us see where our
66:05 - data is concentrated and they also let
66:08 - us see how its distributed and so the
66:10 - through this it kind of shows a general
66:11 - behavior and so really what histogram is
66:15 - is it looks at each value and it just
66:17 - looks at how often the value has
66:19 - occurred and so what we see here for
66:21 - example is that around 0 you know we
66:24 - have the most occurrence of whatever
66:25 - value we're looking at and as we move to
66:28 - the left and as we move to the right
66:29 - these values start to
66:31 - drop off so they start to become less
66:33 - frequent and so that's what histogram
66:36 - shows us they the istagram shows us a
66:38 - kind of frequency how often these things
66:40 - occur and so there are different types
66:43 - of histograms that you can encounter or
66:45 - I mean generally a histogram is just
66:48 - this plotting of frequency versus your
66:51 - value and so there are different ways
66:53 - that this histogram can look like one of
66:55 - them is the one that we've just seen
66:57 - which is a normal distribution or it's
66:59 - called Gaussian like histogram because
67:01 - it follows this Gaussian distribution or
67:03 - this normal distribution that you know
67:05 - but we can also have like an
67:07 - exponentially decaying value so we start
67:11 - off very high and the further we get
67:13 - away from that initial value the quicker
67:16 - it's then it gonna decrease and you can
67:17 - actually compare that to the Gaussian
67:19 - like or to the normal distribution so
67:21 - the normal distribution looks more of
67:23 - like a bell it kind of goes up and then
67:25 - curves down slowly whereas the
67:27 - exponential it cuts off very fast and
67:29 - then kind of slows down later on so they
67:34 - do have different behaviors and then of
67:36 - course you know we can also get not just
67:39 - one peak like we see in this first case
67:42 - and the Gaussian like distribution but
67:44 - we can also get things like two peaks or
67:47 - we can even get three peaks or more we
67:48 - can have very large extended peaks and
67:51 - so our histograms there are means of
67:55 - showing us how this data is distributed
67:56 - there are means of showing us you know
67:58 - what things occur most frequently where
68:01 - is our data concentrated but that don't
68:04 - that doesn't mean that they're gonna
68:05 - have to have a specific value and so
68:08 - they're or your specific shape so there
68:10 - are many different shapes that our
68:11 - histograms can take on and depending on
68:13 - what shape that you get that also tells
68:15 - us something very different about our
68:17 - data all right so the next one variable
68:22 - plot that we'll look at is going to be
68:23 - bar plots and so what bar plots do is
68:27 - they may look a little bit similar to
68:29 - histograms at first but they are very
68:31 - different in some sense because bar
68:33 - plots allow us to compare across
68:35 - different groups and so that's what we
68:37 - see on the x-axis down there is we look
68:41 - at different groups and so we use the
68:43 - same
68:43 - em but we can compare that variable over
68:47 - different groups and so if we look at
68:49 - that in example so what we see on the
68:51 - right here is we look at different
68:53 - countries and what we show is we show
68:55 - the average income tax and so we see
68:58 - that country B for example has the
69:00 - highest average income tax whereas
69:02 - country D has the lowest income tax so
69:07 - through this you know we're still only
69:08 - looking at the income tax variable but
69:11 - we are able to compare us over different
69:13 - groups over different categories if you
69:15 - will so other examples would be if you
69:18 - look at control groups and test groups
69:20 - or if you're doing some like medical
69:22 - study or maybe some psychology study or
69:25 - something like that you always want to
69:26 - have your control group and then you can
69:28 - have different types of test groups and
69:30 - then you can plot each of these groups
69:32 - as a bar plot and you can look at the
69:34 - same variable but you can look how that
69:36 - changes over the different groups
69:38 - another example would be something like
69:41 - comparing male versus female heights so
69:43 - you've got one group that's male the
69:45 - other group that's female and you can
69:46 - just plot their average height and then
69:49 - the tax the income tax of different
69:51 - countries which is what we seen on the
69:53 - right over here all right and so the
69:56 - last one variable graph that we're gonna
69:57 - look at is gonna be pie charts and what
70:00 - pie charts are allowed to do is they
70:02 - allow us to section up our data on the
70:04 - and then we can kind of split it into
70:06 - percentiles and because of this we can
70:08 - see what our data is made up of so the
70:12 - whole pie corresponds to a hundred
70:14 - percent and then we kind of cut it down
70:16 - at different slices and through that
70:19 - slicing and then hopefully also color
70:21 - coding like we've done here and maybe
70:22 - even labeling or most definitely
70:24 - labeling so that you know what slice
70:26 - corresponds to what value we're able to
70:29 - see what categories you know or what
70:32 - what categories our data is made up of
70:34 - and so we can see what is most prominent
70:37 - but we can also see what is at least
70:39 - prominent in all of these things and
70:41 - then again here we can see also
70:43 - distributions not as well as in the
70:45 - histogram but we can still see
70:47 - distributions in terms of dominance in
70:49 - terms of how many groups there are is
70:51 - the data spread evenly is it you know
70:54 - heavily concentrated in one part of the
70:58 - pie all of these things allow you know
71:01 - is that's what we're able to do with pie
71:02 - charts we get this nice kind of group
71:04 - overview of one variable so examples of
71:09 - this would be you can look at NASA D
71:12 - distribution in a university and so you
71:14 - can have a pie chart and just each slice
71:16 - of pie which is to represent a different
71:17 - ethnicity and depending on you know how
71:21 - much of our percentage they make up the
71:22 - total University profile that's how big
71:25 - the slice of pie would be and so you can
71:27 - see dominance of some ethnicities as
71:30 - well as you know minorities but you can
71:33 - also see just by how many slices that
71:36 - are you can see how many different
71:37 - ethnicity groups there are and another
71:39 - example would be you can split up star
71:42 - reviews for a product so rather than you
71:44 - know looking at the average mm star
71:47 - review you can also just use a pie chart
71:49 - and you can see how many of my reviews
71:51 - are 5 stars how many of them were 4
71:52 - stars 3 2 and 1
71:55 - and so there you can again also get this
71:57 - nice different overview of how the
72:00 - review system would work now we're going
72:03 - to talk about two variable graphs so the
72:07 - graphs that we're gonna look at are
72:08 - gonna be scatter plots line graphs 2d
72:12 - histograms or two-dimensional instagrams
72:13 - and box and whisker plots alright so
72:18 - let's start off with scatter plots now
72:20 - for a scatter plot what we're doing is
72:23 - we're really scattering all of our data
72:25 - points onto a graph and so pretty much
72:29 - every data point that we have we kind of
72:30 - put a little dot onto it on the graph
72:33 - and scatter plots are great because they
72:36 - allow us to see spread of data between
72:38 - two variables so we're always plotting
72:40 - one variable on the x-axis and another
72:42 - variable on the y-axis and it just
72:45 - pretty much allows us to see how the
72:47 - data is distributed for these two
72:49 - variables and then through that we can
72:51 - also see more dense areas we can also
72:53 - see some sparse areas and we can also
72:56 - look at correlations so maybe you
72:59 - remember in the lecture we talked about
73:01 - correlations we were able to see through
73:04 - scatter plots where those correlations
73:05 - where or
73:07 - weren't any correlation so all of these
73:09 - things that's what scatter plots are
73:11 - really really nice for in scatter plots
73:14 - of course we can also use them to have
73:16 - like we see here little clusters so not
73:19 - everything needs to be connected by a
73:21 - line or a curve maybe something is more
73:23 - like a circle and so that's what scatter
73:26 - plots can show us too they can kind of
73:27 - show us these groupings and we see one
73:30 - cluster here but maybe you know you have
73:32 - bigger plots and then there would be
73:33 - smaller you know like ten little
73:36 - different groupings for different things
73:38 - so it's got our plots are really great
73:39 - for that because they just show us where
73:42 - the data points are located for these
73:44 - two variables and then we can you
73:46 - ourself see you know like how how do
73:49 - these look like do does one variable
73:50 - affect the other is there may be certain
73:52 - groupings that we can see where our
73:54 - dense areas where it's sparse where are
73:57 - things concentrated you know is
73:59 - everything spread up all over the place
74:01 - is it very very narrow and only in
74:03 - specific region scatter plots allow us
74:05 - to see all of these things very easily
74:07 - and so some examples where we could use
74:10 - scatter plots would be if we see if we
74:13 - look at the graph on the right we can
74:15 - look at something like a car price
74:16 - versus the number of cars sold so each
74:20 - of these data points pretty much
74:22 - represents a car that's been sold and
74:24 - then the x-axis tells us the price that
74:27 - the car has been sold at and the y-axis
74:29 - tells us the number of cars that that
74:34 - have been sold at this price and so what
74:36 - we see here for example very easily is
74:38 - that the more the car is priced the less
74:40 - it gets sold and then maybe you can
74:42 - think of that in terms of well the more
74:44 - its price maybe people don't want to buy
74:46 - such an expensive car maybe they found a
74:48 - cheaper version of it so maybe it's just
74:51 - a branding thing which is why it's more
74:53 - expensive
74:54 - maybe there's something just as good
74:55 - quality that's cheaper maybe people just
74:58 - don't have enough money so that's
75:00 - probably a big factor to that people
75:01 - just don't have enough money to buy
75:02 - these expensive cars and so that's why
75:05 - they drop off and so it may look a
75:08 - little bit different in terms of profits
75:09 - but the higher the car is priced the
75:12 - less we see it being sold so that's one
75:15 - example of a scatterplot then something
75:19 - else that we can look at
75:20 - is maybe the income versus years of
75:23 - education so only we would look at on
75:26 - the x-axis how many years someone has
75:29 - been educated and then we would look at
75:32 - that current income and that would just
75:33 - be a point on the on the graph and we
75:36 - can do that for many many different
75:37 - people and then we can see how different
75:40 - education for different people how that
75:41 - affects their current income so that's
75:43 - another thing where we can do a scatter
75:46 - plot for we can also go back to one of
75:49 - the earlier examples that we used very
75:51 - early on where we talked about people
75:53 - traveling to work and we can just plot
75:55 - the distance traveled versus the time it
75:58 - takes and travel to work and then we can
76:00 - see you know maybe some people travel
76:03 - faster it could be that some people
76:04 - travel the same distance but one takes
76:06 - longer than the other because one goes
76:08 - by car the other one goes by bike the
76:11 - other one takes public transport all of
76:13 - these things so all of that we can see
76:15 - in these scatter plots and just kind of
76:17 - take into account these different
76:18 - situations and see how that all looks
76:20 - for the more for the general population
76:23 - of our data or just generally for our
76:25 - data so scatter plots are really really
76:28 - great as a kind of first go-to just also
76:31 - identifying trends identifying regions
76:34 - and just giving a good overview of your
76:36 - data now the next thing that we'll look
76:39 - at is going to be line plots and line
76:41 - plots in some sense are kind of similar
76:44 - to scatter plots so we have the same
76:46 - bases of the X and the y axis but the
76:49 - points are connected and now it's very
76:51 - important to know when to choose line
76:53 - plots and want to choose scatter plots
76:55 - so line plots can carry a lot of
76:57 - advantages with them because this
76:59 - connectedness it makes it very easy for
77:01 - us to see trends because we can see
77:03 - where are these lines go not just trying
77:05 - to connect the points in our head you
77:07 - know like kind of connect the dots but
77:09 - that's exactly what I'm a line plot does
77:11 - is it connects the dots for us and so we
77:13 - can see these lines it's great if we
77:15 - want to see an evolution of something so
77:18 - maybe we want to see an evolution over
77:19 - time maybe you want to see an evolution
77:21 - over space and evolution with people
77:24 - something like that just if our data
77:25 - points are connected it's great to use a
77:28 - line plot so if we know that whatever
77:31 - happened before is connected to what
77:33 - happens and
77:34 - it's great to use line plots because
77:36 - line plots show us how things evolve
77:39 - because they're all connected as a line
77:40 - but if we're to do scatter plots and we
77:44 - just kind of plot points randomly and
77:46 - just because if we go back to or our
77:48 - kind of car sold car price example just
77:51 - because someone bought an expensive car
77:53 - or if we look at the expensive car and
77:54 - it's been bought say like five times and
77:57 - we look at a cheaper car it's and bought
77:58 - a hundred times there isn't really a
78:01 - logical connection to make between the
78:03 - two and so if we were to use line plots
78:05 - where we should use scatter plots really
78:07 - what we'd see is just a bunch of lines
78:08 - all over the place and so that's why
78:11 - it's important to kind of know when to
78:14 - use lime stalks and want to use scatter
78:15 - plots because it can be very very
78:17 - helpful if you use a scatter plot
78:19 - instead of a line plot it's gonna be a
78:21 - bit more confusing because you have to
78:22 - try to connect the dots yourself in your
78:24 - head but if you use a line plot instead
78:26 - of a scatter plot it's gonna look really
78:28 - weird because there's just lines all
78:30 - over the place and you can't really see
78:31 - anything so an example where we could
78:35 - use line plots is you have the typical
78:37 - distance versus time so you can look at
78:41 - you know how far someone or what time it
78:44 - is and then you know how far someone has
78:46 - traveled just a general curve of
78:47 - distance versus time that's very very
78:49 - common and you can look at the profit of
78:52 - a company versus the number of employees
78:54 - so the more employees they imply
78:56 - employee how does that change their
78:58 - profits so of course they have to pay
79:00 - the employees more but maybe the
79:02 - employees can also do more work and
79:04 - hopefully you know that kind of cancels
79:06 - out what you pay them and then increase
79:09 - this company profits and then what we
79:11 - can see on the right here is we can look
79:13 - at your creativity and how that changes
79:15 - with stress so you can see that the more
79:18 - stressed out you are the less creative
79:20 - you are and here it's also good to use a
79:22 - line plot because you kind of gradually
79:25 - advanced and stress and so each point
79:28 - and stress is kind of related and the
79:30 - higher you go up and stress the lower
79:32 - you go down in creativity and so there's
79:34 - this kind of relation where we can see
79:36 - this evolution so the more you get
79:38 - stressed out the less creative you
79:40 - become and so line plots are really nice
79:42 - here because there's not this chaotic
79:44 - movement everywhere but it's very nice
79:46 - and it's very easy
79:47 - to see this line it's very easy to
79:49 - follow okay so the next graph that we
79:53 - can talk about is 2-dimensional
79:55 - histograms now we've seen
79:57 - one-dimensional histograms in the last
79:59 - tutorial where we looked at the spread
80:01 - of data and we looked at the peaks and
80:04 - how you know things were distributed to
80:06 - the right and to the left but we can
80:07 - also do a 2-dimensional histogram and so
80:10 - what a 2-dimensional Instagram is it's a
80:12 - one-dimensional histogram but it's a
80:15 - pretty much a histogram for every single
80:18 - point of the other variable that we're
80:20 - looking at so really what these things
80:24 - allow us to see is they allow us to see
80:25 - how the different distributions of the
80:28 - two variable is relative to to another
80:30 - so we can see here for example in the
80:33 - red region that for those specific
80:35 - values them they happen a lot so that
80:38 - combination of values happens a lot and
80:41 - so we're able to kind of pinpoint these
80:43 - frequency occurrences again and we're
80:46 - also able to look at drop-offs but we're
80:49 - able to pinpoint that to two specific
80:51 - values now rather than just 1 which is
80:54 - what we did to the 2d histogram and
80:56 - these things are much harder to see in
80:58 - scatter plots because in scatter plots
81:00 - if we have a value occurring a hundred
81:02 - times it would just be the same dot and
81:05 - the dot wouldn't get bigger now of
81:06 - course you can make the dot bigger
81:07 - yourself if you wanted to or you could
81:10 - change the color or something like that
81:11 - but really if you do a scatter plot and
81:13 - the same thing happens a hundred times
81:15 - it's just gonna look like one dot
81:17 - whereas for two-dimensional histograms
81:20 - we can see that it's not just you know
81:24 - it's not just happening once but we can
81:26 - actually see the frequency of those
81:28 - variables or those those two variables
81:30 - together so an example of a
81:33 - two-dimensional histogram would be if we
81:35 - look at ticket prices versus tickets
81:37 - sold and so if you look at the lower
81:40 - left corner and we can kind of see this
81:41 - red peak so that's cheaper ticket prices
81:44 - but the tickets are also sold often so
81:47 - we know that tickets at that price are
81:49 - sold quite often and these could be you
81:53 - know like new rising brand bands these
81:55 - could be like you know you kind of
81:57 - standard bands that maybe you want to
81:58 - take someone on a day-to but you don't
82:00 - want to spend
82:01 - much money on a ticket but still a
82:02 - concert is a nice idea and so that's a
82:05 - good ticket price that sells a lot of
82:08 - tickets because it gives you the
82:09 - pleasure of the event without making it
82:12 - too expensive and then if you move more
82:14 - towards higher ticket prices and then if
82:17 - you move more towards more tickets sold
82:20 - then you can see that for high tickets
82:23 - high ticket prices which would be you
82:25 - know like these big bands then we can
82:27 - again see how many tickets we've sold so
82:30 - we can see that for you know a higher
82:33 - price and if we go up and ticket sold so
82:36 - if you want to see lots of tickets sold
82:37 - for a high price then the red Peaks are
82:40 - gonna give us all of these more famous
82:43 - artists so that's you know one kind of
82:45 - application but of course there are many
82:48 - many better ones it's just these things
82:51 - you know if you're in the moment and you
82:53 - you can kind of then you would realize
82:54 - oh this is when a two-dimensional
82:56 - histogram would be a great thing for me
82:58 - to use so a lot of these graphs they're
83:01 - great to know and once you're in the
83:03 - moment then it's much easier for you to
83:05 - pick out which graph would be best
83:07 - representative finally the last graph
83:10 - that we're gonna look at is gonna be a
83:11 - box-and-whisker plot and I want box and
83:15 - whisker plots allow us to do is they
83:16 - allow us to see the spread within our
83:19 - data so it's not just like a bar plot
83:21 - which just shows us one value but we can
83:23 - actually see the statistical spread so
83:26 - we can see median values which is what
83:28 - we see here we can see quartiles the
83:31 - little dots on the outside actually show
83:33 - us outliers and so what box and whisker
83:36 - plots allow us to do is they allow us to
83:37 - see the statistical information but they
83:40 - allow us to see it visually and that
83:42 - makes comparing across different groups
83:44 - which is what we're doing here much
83:46 - easier and so a good example of that
83:49 - would be if we look at ticket prices for
83:51 - football games for different teams so we
83:54 - have different teams and different teams
83:55 - of course use different stadiums and
83:57 - there they have different popularities
83:59 - and some teams may be much more
84:01 - expensive or the ticket prices may be
84:03 - much more expensive than other ones and
84:06 - so we can compare these ticket prices
84:08 - using box and whisker plots and then we
84:10 - can see you know what is the higher end
84:12 - of these
84:14 - so those are gonna be the more luxurious
84:15 - seats and then we go to the bottom and
84:17 - those are going to be the less luxurious
84:19 - seats probably the ones where you stand
84:21 - and then you have middle values
84:23 - depending on you know the standard seats
84:25 - and where you are in the stadium if
84:26 - you're close to the field if you're
84:28 - further away from the field but you're
84:30 - still sitting all of these things we can
84:32 - kind of see here and that's what gives
84:34 - us a spread we can compare that across
84:36 - different teams and we can see the
84:38 - spread across difference teams we can
84:40 - also see which teams are more expensive
84:41 - you know where do the prices vary the
84:45 - most for specific teams and maybe some
84:47 - teams have a super launch and then they
84:49 - have your you know standing places that
84:53 - are just much cheaper and so you would
84:55 - see a lot larger spread or maybe some
84:57 - teams just have you know only seats and
84:59 - see you'd see a much lower spread and so
85:01 - all of these things were able to compare
85:03 - using box-and-whisker plots over
85:05 - different groups in this tutorial we're
85:08 - going to talk about 3 and higher
85:10 - variable graphs so the graphs that we're
85:13 - gonna look at are is gonna be heat maps
85:15 - and then we'll also look at multi
85:17 - variable bar plots as well as how we can
85:19 - add more variables to some of the lower
85:21 - dimensional graphs that we've talked
85:22 - about earlier all right so let's start
85:26 - with heat maps now what heat maps allow
85:28 - us to do is they allow us to plot two
85:30 - variables against each other and the X
85:32 - and the y and they laws to show an
85:34 - intensity or a size or something like
85:36 - that in the Z direction or towards us so
85:40 - an example of this which is kind of what
85:43 - I've tried to illustrate on the right is
85:44 - a customer moving through a storm and so
85:48 - we can track the path of the customer in
85:50 - the X and y direction of the store so
85:52 - you can kind of get this bird's eye view
85:54 - and see where they move to and the
85:56 - darker spots actually tell us the
85:58 - positions where they spend more time at
86:00 - so we can see that they spend a little
86:03 - bit of time you know at the beginning
86:04 - they moved in and then they stopped
86:06 - lunch was what we see with dark spot
86:07 - being maybe they found like the candy
86:09 - aisle or something there was a specific
86:10 - piece of candy that they wanted and then
86:12 - they moved on and then they started to
86:15 - go towards the corner around the corner
86:17 - a little bit and maybe they reached the
86:18 - fruits in the vegetable section there
86:20 - and picked out several things and then
86:22 - they started to head towards the
86:24 - checkout counter which happens at the
86:26 - very end and they were moving at a more
86:28 - constant pays sometimes they stop to
86:30 - look a little bit but they just kind of
86:31 - continued moving on and so the three
86:34 - variables that we've shown here as we've
86:36 - shown their exposition in the store
86:37 - we've shown they're by position in the
86:39 - store and to their color we've also
86:41 - shown the time that they spend at each
86:43 - position so that's what we can use heat
86:46 - maps for and then another example the
86:49 - heat map would for example be if you
86:51 - take a flashlight and you move it over
86:53 - the screen and really what you're
86:55 - showing is the amount of time that
86:58 - you've shown the flashlight onto a
86:59 - specific region so that's kind of
87:01 - another example the heat map but usually
87:03 - heat map as the name implies it allows
87:06 - you to track positions and so it's very
87:08 - often used for things like tracking
87:10 - customers through stores or just
87:12 - tracking general people location where
87:14 - they like to spend their time and the
87:17 - intensity that you see in terms of the
87:19 - color is usually the amount of time that
87:21 - they spent there all right so we can
87:24 - also do multi variable bar plus a multi
87:27 - varied bar plot and so it is very
87:30 - similar to a single bar plot where we
87:32 - just plotted one value over different
87:35 - groups but rather than just plotting one
87:37 - we kind of cramped them together and we
87:39 - plot several and so an example of this
87:42 - would be that we plot you know goal
87:44 - scores
87:45 - I'm goal scored 14 the shots taken on
87:47 - goal as well as the shots on target and
87:50 - so we can see maybe there are teams that
87:52 - shoot less on goal without score less
87:55 - but that's because they also shoot less
87:57 - and therefore they also shoot less on
87:59 - target or maybe there are some teams
88:01 - that do score a lot and that's because
88:03 - they shoot a bunch but they just don't
88:05 - hit the target that often or maybe there
88:07 - are really good teams that score a lot
88:09 - and they also shoot a lot on target and
88:12 - so all of these things were able to then
88:14 - compare over different groups and so
88:17 - that's what we can use multi variable
88:19 - bar plots for if there are several
88:21 - variables that would give us a better
88:24 - understanding of the system than just
88:26 - looking at the variables in one at a
88:28 - time but it also be really cool if we
88:30 - could compare all of them then we could
88:31 - use multi variable bar plots for that
88:33 - and just plot them on the same bar plot
88:36 - and then we can see how they change you
88:38 - know within a group we can also see how
88:40 - they change over
88:41 - different groups okay and something that
88:45 - we can do is we can also just add extra
88:47 - dimensions to lower dimensional graphs
88:49 - that we've had and so we're kind of
88:50 - limited to three dimensions because
88:52 - that's the amount of space dimensions
88:54 - that we live in but if we take a scatter
88:56 - plot for example where we started off
88:58 - with just the X and the y axis and
89:00 - points located what we can do is we can
89:03 - actually add a third axis so we can take
89:05 - the X and the y and then we can add a Z
89:08 - and that gives us an extra depth
89:10 - dimension which is exactly what we see
89:11 - here so rather than just plotting unlike
89:13 - a two dimensional field unlike a plane
89:16 - we even actually plot it in a volume and
89:19 - so we can see this kind of scattered
89:21 - ball that we've done kind of kind of all
89:23 - that we've done here which is kind of
89:24 - located at the center of our plot and so
89:29 - this can be really cool because it
89:30 - allows us to see depth to the problem
89:33 - with this is that we have snapshots
89:36 - every time and so really we're looking
89:38 - at two-dimensional snapshots and so to
89:40 - get the best understanding of this we
89:43 - need to rotate our scatter plots or our
89:46 - plots as we do them so that we can also
89:47 - add in our depth perception because
89:50 - right now for looking at it it may look
89:52 - three-dimensional but really it's just a
89:54 - two-dimensional snapshot and to get the
89:57 - best understanding if our scatter plot
90:01 - is located more towards us and more
90:03 - towards the left or something like that
90:04 - or maybe it's just really high and close
90:07 - to us or maybe it's really low and far
90:09 - away to understand all of these things
90:11 - we need to be able to rotate our scatter
90:13 - plot so that we can see it from
90:15 - different angles which then gives us
90:17 - this depth perception and we can do the
90:19 - same thing with 3d line graphs so here
90:22 - we see an example of maybe the position
90:24 - of a skier as they're skiing down a hill
90:27 - and then we can kind of trace that
90:29 - through time and we see that they're
90:31 - kind of they're going down the hill in
90:33 - this nice exact motion as you should and
90:36 - we can just track their position over
90:38 - time so here we've added this extra
90:40 - dimension to the 3d line graph rather
90:42 - than just taking maybe a time and a
90:45 - position at a time or something like
90:47 - that we've added a second positions or
90:50 - actually even a third position so we've
90:51 - got the X to the one there's that
90:53 - position and then we just trace it
90:55 - and over time and so that gives us this
90:57 - whole line here and so that's how we can
91:00 - take these lower dimensional plots that
91:02 - we've looked at before and we can just
91:05 - add extra dimensions to them if we want
91:07 - as long as it's still easy to see as
91:09 - long as it makes sense what we're
91:10 - looking at yeah we're really just able
91:13 - to maybe just slap on another direction
91:16 - there and you know compare another
91:18 - variable in this tutorial we're going to
91:20 - touch on the third major section that is
91:23 - really great for data scientists or that
91:25 - should be an essential of data
91:26 - scientists which is the ability to
91:28 - program okay and so why do we program
91:33 - well there are different reasons why we
91:35 - want to be able to program the first one
91:37 - is going to be the ease of automation
91:38 - the second one will be the ability to
91:41 - customize and finally it's because there
91:43 - are many great external libraries for us
91:45 - to use that it would just make our job
91:47 - so much easier alright but so let's get
91:50 - started let's talk about the ease of
91:51 - automation for us what do I mean with
91:53 - that well being able to program it
91:56 - really allows you to prototype really
91:58 - fast allows us to automate things and it
92:01 - also gives us the extra benefit if if we
92:03 - have something in our mind we can just
92:05 - take that and kind of put it into the
92:08 - computer by programming it and so we're
92:11 - able to automate everything very fast
92:13 - and we don't have to do these repetitive
92:14 - tasks
92:15 - you know maybe copy pasting stuff into
92:18 - or from Excel or all these things and if
92:21 - we just want to repeat something or we
92:23 - want to quickly change something up and
92:25 - just change a small thing we don't have
92:26 - to do a lot of stuff we can just change
92:29 - that in our code and then click play and
92:31 - let the computer take care of all of
92:33 - that for us rather than us having to do
92:35 - everything manually so it's very easy
92:37 - for us to automate things um and also
92:40 - for doing reports it's very easy to
92:42 - automatically create these reports you
92:45 - know all you have to do is set up your
92:47 - program to deal with the data that
92:49 - you're going to give it and then I can
92:51 - automatically create reports every week
92:53 - and the reports can be different because
92:55 - you give a different data and it should
92:57 - still look the same but the data the
92:59 - values can be different and so that will
93:01 - just automatically create all these
93:03 - reports for you and you don't have to do
93:05 - that all yourself the program does it
93:06 - for you
93:08 - but you've built the program and you're
93:10 - giving it this different data so you're
93:11 - still doing all of the analysis it's
93:13 - just you get to skip the part of copy
93:15 - pasting and like looking across and
93:17 - taking over the values and doing all the
93:19 - formatting of just doing the same report
93:22 - over and over and over again all of that
93:24 - is taken care of for you and all you
93:26 - have to do is just put in the right data
93:28 - you know write out everything that you
93:29 - want to do and then click play and let
93:32 - the computer handle all that for you
93:33 - because remember that's what the
93:35 - computer is doing and good at doing
93:37 - doing these repetitive tasks okay we
93:40 - also want to be able to program because
93:42 - it really allows us to customize it's
93:44 - very easy once we go into data analysis
93:48 - and when we see things that we get these
93:50 - ideas that we want to expand or
93:52 - different directions that we want to
93:53 - progress our analysis into and being
93:55 - able to program it really just allows us
93:58 - to take all that and put it as a code
93:59 - and just choose that direction and we
94:02 - can very easily dive much deeper into
94:05 - our analysis and discover things fast
94:06 - because it's up to us to where we want
94:09 - to go and so this ability to customize
94:12 - with programming it's it's very very
94:13 - important because we're not reliant on
94:16 - anything else we're not reliant on you
94:19 - know some software and maybe it breaks
94:21 - down or maybe we don't know how to
94:22 - perfectly use it and we have to read the
94:24 - manual and read a like a Help section
94:26 - and know we know how to program and we
94:29 - just type down exactly what we want to
94:30 - do exactly where we want to take it
94:32 - exactly what we want to see and we can
94:35 - customize very very fast with that we
94:37 - can also prototype very very fast with
94:38 - that and maybe if a visualization is not
94:41 - working to turn a scatterplot into a
94:43 - line plot it's very easy you just change
94:45 - one word so all of these things are very
94:47 - very easy to do with programming because
94:49 - we have all that power at our fingertips
94:51 - and we can just you know change
94:53 - everything that we're looking at
94:55 - everything that's being calculated maybe
94:56 - we want to calculate an extra thing and
94:58 - take out something else because it's
94:59 - irrelevant all of these things were able
95:02 - to customize and all of that we can do
95:04 - because we're able to program so really
95:07 - what we're doing is we're making the
95:09 - data ours so we're taking full control
95:11 - of the data we're taking full control of
95:13 - where we want to go with our analysis
95:16 - what we want to see and what we want to
95:18 - show all right
95:21 - so let's talk about first libraries but
95:24 - also give you two great Python libraries
95:26 - that you should you know maybe feel
95:28 - comfortable with or that you should
95:29 - maybe consider using for data analysis
95:31 - so first of all what are libraries will
95:34 - libraries are pieces of code have been
95:36 - pre-written by others that you can just
95:38 - take in and use and so a very good
95:42 - example of this is something known as a
95:44 - math library and so that has all the
95:46 - square root functions taking to the
95:47 - power you know taking the exponential
95:50 - the sine the cosine all of these things
95:52 - that you know and you want to use but
95:55 - you don't want to program yourself so
95:56 - like it pretty much avoids that middle
95:59 - step of you having to program the
96:01 - equation to calculate a sine because all
96:03 - of these things those are things that we
96:04 - don't want to do we don't want to get
96:06 - distracted from our target we want to be
96:08 - able to do exactly what we want to do
96:09 - without having the program completely
96:12 - other stuff and so that's what libraries
96:14 - are great for they're developed for by
96:15 - the community for everyone to use you
96:18 - know everyone is helping each other and
96:20 - these libraries they just bring a lot of
96:23 - power with it and so one of these
96:25 - libraries is called pandas and pandas is
96:28 - pretty much like Excel but it allows us
96:31 - to do or we can do programming with it
96:33 - which just makes it so much better
96:34 - because we can do things so fast with it
96:37 - we can do all this customization we can
96:38 - do all this automation whereas you know
96:40 - like Excel if you give it too much stuff
96:42 - too much to run it would just start to
96:44 - crash because it has to handle all of
96:45 - this other things all these other visual
96:47 - things you know the UI and there's a lot
96:49 - more it's a lot it's not a structure as
96:52 - well whereas in programming the program
96:54 - you know your computer just goes through
96:56 - everything step-by-step it doesn't have
96:57 - to take care of all of these
96:58 - visualization things it just does the
97:01 - calculations down below but we can still
97:04 - do all sorts of data management with
97:06 - them so we can shift our data around we
97:09 - can drop columns we can split things up
97:11 - you know we can split things up our row
97:13 - we can pick out certain rows we can even
97:15 - do statistical calculations on our data
97:17 - so we can say you know hey calculate the
97:19 - mean for this we don't even have to you
97:22 - know make our own formula for
97:23 - calculating the mean or for calculating
97:24 - the standard deviation or for
97:26 - calculating correlation between
97:28 - different columns all of that can be
97:31 - done with pandas with just a you know
97:33 - a couple of key words and so it's really
97:35 - easy to do data analysis with it because
97:38 - all of the functions that are there and
97:40 - we know exactly what we want to do but
97:42 - we don't have to write the code for all
97:44 - of it so if you wanted to look at
97:45 - correlations we just say hey pandas do
97:48 - correlations rather than having to you
97:50 - know code all the correlations for
97:53 - ourselves and doing you know coding that
97:55 - whole algorithm and that makes it really
97:56 - easy and really fast to get results and
97:59 - to get to where you're heading because
98:01 - you don't have to go into any of these
98:03 - mineral places you can pretty much just
98:04 - skip the middleman of having to you know
98:07 - write all of those algorithm to yourself
98:09 - and you can just use them so that you
98:11 - have your start you have your idea you
98:14 - know exactly what you want to do and you
98:15 - can do exactly that to get to your goal
98:18 - the other library that is very cool
98:22 - would be matplotlib which is what I use
98:24 - a lot for data visualization it allows
98:26 - me to create graphs allows me to
98:28 - visualize my data allows a bunch of
98:31 - customization so I can really just move
98:33 - everything around in it I can move my
98:35 - spines I can turn things on and off you
98:37 - know all of these things are very easy
98:40 - to do with MATLAB there's a lot of great
98:44 - customization that I'm able to do with
98:46 - it so these are like kind of two basic
98:48 - Python libraries that you should
98:50 - probably maybe get to know em or you can
98:53 - look at some of my other courses and one
98:55 - of them pandas would deal with the data
98:57 - analysis part and matplotlib would help
98:59 - you deal with the data visualization
99:01 - part of it so that's it that's a super
99:06 - basic breakdown of the three main
99:07 - components of the otherwise vague term
99:09 - data science if any of this has piqued
99:12 - your interest then you may have a data
99:14 - science future ahead of you and I
99:15 - encourage you to continue to pursue your
99:17 - interest if you want to learn more from
99:19 - me I've got a blog on my website coding
99:21 - with max comm that dies more into
99:23 - different topics related to data science
99:25 - you can also get access to some of the
99:27 - resources such as cheat sheets and
99:29 - workbooks that I've compiled for you
99:30 - there if you're serious about learning
99:32 - data science you can also check out my
99:34 - courses on data science which are
99:36 - designed to teach you all you need to
99:37 - know about data science even if you have
99:39 - no prior experience of course if you
99:42 - have any questions that aren't answered
99:44 - at my website you can always feel free
99:45 - to reach out
99:46 - me personally