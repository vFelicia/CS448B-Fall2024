00:00 - welcome to this course about development
00:01 - with large language models or llms
00:04 - throughout this course you will complete
00:06 - Hands-On projects that will help you
00:08 - learn how to harness the immense
00:10 - potential of LMS for your own projects
00:13 - you'll build projects with LNS that will
00:15 - enable you to create Dynamic interfaces
00:17 - interact with vast amounts of Text data
00:20 - and even Empower LMS with the capability
00:23 - to browse the internet for research
00:25 - papers you'll learn about the intricate
00:27 - workings of LMS from their historical
00:29 - Origins to the algorithms that power
00:32 - models like GPT option is a passionate
00:35 - educator in the field and he teaches
00:37 - this course
00:38 - hi welcome to this course on llm
00:41 - engineering and development brought to
00:43 - you by loop.ai my name is akshat and I'm
00:46 - excited to be teaching you guys this
00:47 - course
00:49 - so who should watch this anyone who's
00:51 - interested to learn Hands-On llam usage
00:53 - in theory through explanations and
00:55 - multiple guide projects paired with a
00:58 - fairly basic Python Programming
01:00 - knowledge should be pretty comfortable
01:01 - following along
01:04 - so here are all the projects that we're
01:06 - going to be working towards we're going
01:08 - to be able to create a clone of the chat
01:09 - gbd user interface along with the large
01:12 - language model that will help us
01:14 - interact with it with custom personas
01:16 - we're going to be able to have
01:18 - conversation so with our documents like
01:20 - text files and PDF files uh that chair
01:23 - gbd may have not been trained on we're
01:26 - going to be able to use Asians which are
01:28 - self-prompting large language models and
01:30 - enable these large language models to
01:33 - browse the web and research and literary
01:36 - research papers using the archive API
01:39 - we're going to be able to enable these
01:40 - large language models to use more than
01:42 - five tools in the real world along with
01:45 - equipping them with their own custom
01:46 - tools
01:48 - so here's all the course content and
01:51 - let's get started with the basic
01:53 - introduction to other guns
01:55 - so an llm basically happens when you
01:59 - combine a massive neural network with
02:02 - huge amounts of data and train it on
02:05 - this huge amounts of data once it's been
02:07 - trained you then align it to human
02:09 - values in an attempt to create a
02:11 - reasoning engine so examples of these
02:13 - LMS are bird llama more famously gbt 3.5
02:17 - and gpt4
02:18 - so the concept of alums has been around
02:21 - since 1996 but why have they only
02:24 - recently gained traction
02:25 - the reason why is because this is the
02:28 - first time in history where llms have
02:30 - been actually able to outperform human
02:32 - reasoning in certain contexts and the
02:35 - reason for this is due to a huge import
02:37 - improvements in performance and scale
02:39 - now more on scale uh as you can see
02:43 - modern llms like gbd 3.5 have a huge
02:47 - amount of money and skill that goes into
02:50 - developing them GP 3.5 has over 175
02:54 - billion parameters and you can think of
02:57 - parameters as neurons in your brain
02:59 - along with this it has huge amounts of
03:01 - training data
03:03 - GPT 3.5 is not where this cycle of LM
03:06 - development stops because open air has
03:09 - another model called gpt4 that is
03:12 - essentially an upgraded version of GPT
03:14 - 2.5 that performs all the data GPT 3.5
03:18 - can but just seems to meet the goal a
03:22 - lot better
03:23 - the reason for this is due to its huge
03:25 - amount of parameters 1.76 trillion and
03:29 - with this it has the capacity to undergo
03:32 - training with even greater amounts of
03:34 - training data
03:36 - a huge model like this that's trained on
03:39 - a huge amount of training data is pretty
03:42 - dangerous to humanity so the demand for
03:45 - aligning this model to human values and
03:49 - feedback is also very important and
03:52 - plays a very important role
03:54 - apart from GB 3.5 and gpd4 by openai
03:57 - there are several competitors in this
03:59 - landscape like Microsoft Facebook Google
04:02 - who are all actively publishing
04:04 - resubscribers and breakthroughs pretty
04:07 - much every week at the time when this is
04:09 - recorded
04:11 - so now let's look through some of the
04:13 - algorithmic breakthroughs that got us to
04:15 - this point I'm going to go through these
04:17 - by explaining to you the typical
04:19 - architecture that you would follow uh if
04:22 - you want to train your own custom large
04:23 - language model so let's start with
04:26 - choosing the architecture and tokens
04:29 - so
04:30 - a large language model you can think of
04:33 - is basically a mathematical function
04:35 - that has just learned to predict the
04:37 - right words given the given some input
04:40 - context
04:41 - and if you want to use this as a
04:43 - mathematical function you'd obviously
04:45 - have to deal in numbers so we use
04:47 - something called tokenization that
04:49 - converts a string of text into a vector
04:52 - of numbers
04:53 - and so the tokenizer splits these words
04:56 - into individual elements and then
04:59 - assigns it a unique number
05:03 - so once you assign the unique number uh
05:06 - you also need to tell the llm when to
05:08 - actually stop generating or otherwise
05:10 - it'll be stuck in an input too and
05:12 - that's why you have this stop token here
05:15 - so now let's look at the brain which is
05:18 - actually what learns these relationships
05:21 - between words and predicts the right
05:22 - word
05:23 - so you don't need to really know all the
05:27 - complex math that goes in that gets
05:29 - involved in
05:31 - making this neural network for this
05:33 - course but I'm just going to give you a
05:35 - quick intuition as to how this works so
05:38 - let's say one of this input sequence
05:40 - like this word here gets inputted you
05:43 - can think of all these layers as random
05:46 - numbers initially and what you can think
05:49 - of them doing is just multiplying them
05:52 - multiplying this input sequence uh yeah
05:55 - these many times and then you would get
05:57 - an output
05:58 - so obviously if all these layers are set
06:01 - to random numbers you're gonna have a
06:04 - pretty bad and random guess so what we
06:06 - do after we call that the output is we
06:09 - look at what word we actually want the
06:12 - model to output and then we compute a
06:14 - difference between our output value that
06:16 - we predicted and the the actual value
06:19 - that we need once we have that the
06:22 - algorithm takes node and it adjusts all
06:25 - of these parameters here uh to step in
06:28 - that direction
06:29 - so once that's done
06:32 - we've pretty much finished one training
06:34 - step and and adding to this overall
06:39 - scale concept this whole training
06:41 - process occurs hundreds of billions of
06:43 - times so you can imagine that even after
06:45 - the first 10 million iterations it's
06:47 - going to get pretty good at guessing
06:49 - what the next word is supposed to be
06:53 - so now more on the training problem I
06:55 - was just talking about
06:56 - the way these large language models are
07:00 - trained is through a next word token
07:02 - prediction task which basically gives
07:04 - the model the question as well as a
07:07 - blank that it's supposed to fill in so
07:09 - here obviously the answer is books so
07:11 - it's given all this context here
07:12 - surrounding context and it's asked to
07:14 - predict the word
07:16 - so these these are just four questions
07:19 - here but the model gets inputted
07:21 - billions of questions from data uh from
07:25 - code to college textbooks to articles to
07:28 - lyrics to podcasts and pretty much any
07:30 - data that you can scrape off the
07:32 - internet
07:33 - so good so now we've effectively trained
07:37 - this model that can predict the next
07:39 - token in our sequence but this is still
07:42 - very limiting because it's just one
07:43 - token we want we want our slm to ideally
07:47 - express ideas and thoughts and actually
07:49 - reason so the way we do that is by
07:52 - actually predicting this next token here
07:54 - and then inputting the predicted token
07:56 - back into the model so that it can
07:58 - predict again and you just keep you know
08:01 - collecting these predictions and you'll
08:03 - get a string of text and
08:06 - um going back to the stop token this is
08:08 - where it's important because once the
08:10 - model is finished with the thought it
08:11 - needs to be informed that
08:14 - um yeah you should just stop the thought
08:18 - so great so now we have this huge llm
08:21 - that's been trained on a bunch of
08:23 - different data and we can use it as a
08:26 - reasoning engine but maybe the llm
08:29 - doesn't have the knowledge that it needs
08:31 - in order to work in your specific use
08:33 - case and this is where fine tuning comes
08:36 - in fine tuning
08:38 - um find you can use fine tuning to
08:41 - further train a model to your own
08:43 - personal context and this is good
08:45 - because you don't need that much data
08:47 - anymore you just need a small label
08:49 - Corpus of your example data and examples
08:53 - of something that you can use fine
08:54 - tuning for is generating custom mid
08:57 - Journey or image generation prompts for
08:59 - another model as well as letting it
09:01 - learn information Beyond its knowledge
09:03 - cut off so you know say
09:06 - um you wanted to teach it about the
09:07 - latest cancer research
09:10 - so a quick note here uh the second
09:13 - example where you're actually teaching
09:15 - it information it's not the most
09:17 - efficient uh when you find you we're
09:19 - going to be using a much better method
09:20 - called Vector databases in this course
09:22 - uh more on that later but generally
09:26 - you'd fine tune if you want to change
09:27 - its Behavior like this example here
09:32 - so now that we have this model with its
09:36 - optional fine tuning you can further
09:39 - take this to production and make it safe
09:42 - so that it doesn't
09:43 - um spear harmful content through RL HF
09:47 - which is reinforcement learning from
09:49 - Human feedback the reason we do this is
09:51 - because the the training data that we
09:54 - scrape comes from a bunch of different
09:55 - sources like articles podcasts textbooks
09:59 - lyrics Etc and this data is bound to
10:03 - have a lot of bias in it and alignment
10:05 - with human values basically fine tunes
10:08 - it to remove the bias so here's
10:12 - um here's what they do what open AI does
10:15 - to fix this issue
10:18 - so basically it's uh it uses a human
10:21 - labeler to just get safe output and
10:25 - reinforce the model to Output these safe
10:26 - tokens
10:30 - okay so now we pretty much went through
10:32 - the entire pipeline which is everything
10:34 - from training to fine tuning to aligning
10:38 - to human values and now we're gonna
10:40 - actually look into how to get the output
10:42 - from our models so the the question that
10:46 - you asked the process of asking a large
10:48 - language model a question is called
10:50 - prompting
10:51 - and the model generates something called
10:54 - a completion which is just a string of
10:56 - text that's likely to complete your
10:59 - previous text
11:01 - inference parameters are another
11:03 - technique that you can use to enhance
11:05 - the creativeness of the model and we'll
11:08 - be trying this out in the chat gbt
11:09 - playground but these are the four
11:11 - parameters you can change so these two
11:15 - basically make your outputs more random
11:17 - by changing a lot of things like the
11:21 - window for the probable words to how
11:24 - much these low probability words are
11:26 - weighted
11:28 - add frequency parity and presence parity
11:30 - are the other two inference parameters
11:32 - and they basically make sure that your
11:35 - model doesn't output the same answer in
11:38 - the same style for every single for
11:42 - every single identical question that you
11:44 - ask it so now let's go ahead and try
11:46 - this out in the chat GPD player
11:49 - so quick ignore from this future uh if
11:52 - you already know the basics of chat GPT
11:54 - and calling the API You Should Skip the
11:57 - next two sections and come to the third
11:59 - section where we'll be learning how to
12:01 - clone the entire chat GPT user interface
12:05 - so now let's explore the playground
12:07 - environment
12:09 - so in your browser just type in chat GPT
12:11 - breakdown and you should be prompted
12:15 - with this user interface here
12:17 - another way you could do this is just
12:19 - through the open AI website itself
12:21 - where you just log in
12:28 - and done
12:31 - okay
12:34 - and just hit API here
12:38 - and just hit play dot Okay so
12:41 - the reason why we're choosing to use the
12:43 - playground over the usual charge gbt
12:46 - user interface is because the playground
12:48 - will give us a more
12:50 - customizability as well as a better feel
12:53 - for the actual API that we're going to
12:55 - be using throughout this course so let's
12:57 - get started from um left to right in
12:59 - this column here you can add in your
13:01 - system message uh like and say you are a
13:04 - programmer
13:06 - something like that and basically the
13:08 - system message assigns the llm's
13:12 - personality throughout the entire
13:13 - conversation so here you can maybe say
13:16 - something like our high
13:19 - could end out the first 10 and date
13:22 - sheet so
13:25 - it pull it okay
13:28 - defense
13:31 - and it should give you some Outlet
13:41 - okay
13:43 - so as you can see it does give us the
13:45 - output and one thing to notice here is
13:48 - that there's always an alternation
13:50 - between the user and the assistant and
13:52 - so in the API as well when we script
13:54 - some kind of conversation we're going to
13:56 - be using this so just remember that it's
13:58 - only valid it
14:00 - it's user followed by assistant
14:02 - so now let's move on to this main column
14:05 - here that is going to let us customize a
14:08 - bunch of different parameters so uh it's
14:11 - generally recommended to keep the mode
14:13 - in chat but
14:14 - you can explore the models here and
14:17 - we have options between the versions of
14:20 - the GPD 3.5 line and the gpt4 line and
14:24 - basically the difference between GT4 and
14:27 - gdt 3.5 is uh gpt4 is slower but it's a
14:32 - lot better at logical reasoning and
14:34 - creativity related tasks because it's
14:36 - just trained on so much data and gbt 3.5
14:40 - is obviously the opposite is faster but
14:42 - it's it's a little worse at uh you know
14:45 - these tasks and so there's a speedo
14:47 - there's a trade-off between intelligence
14:49 - and speech here but in general I would
14:52 - recommend using GPT code
14:54 - so we can just give it another message
14:56 - maybe I could write a poem
15:00 - and yeah so as you can see uh the
15:03 - previous
15:04 - suffers so much faster than how it's
15:06 - outputting now
15:08 - and this process of like stringing
15:10 - together these texts live is called
15:12 - streaming where you're not uh you're not
15:15 - just presenting uh the whole text after
15:18 - it's all done processing you're just
15:20 - doing it like it is putting it together
15:22 - so now once we have this we can actually
15:25 - change the other parameters here these
15:27 - in press parameters so temperature is a
15:31 - measure of
15:33 - creativity as I've mentioned before so
15:36 - when you put temperature at zero it's
15:39 - very deterministic so it's useful when
15:41 - you're trying to just try to understand
15:43 - some kind of data and when temperature
15:47 - equals one it's a lot more creative so
15:50 - you can
15:51 - you know use it for a bunch of different
15:53 - uh
15:54 - poem writing or just just like arguments
15:58 - in general
15:59 - maximum length is pretty obvious and so
16:02 - just we can uh just test out the
16:04 - deterministic part of this by comparing
16:06 - to outputs so we can write direct code
16:09 - for a lot
16:10 - thing
16:12 - sorting and
16:15 - dirt from scratch
16:18 - so I'll just copy this and uh
16:21 - submit this
16:24 - and while that's executing uh I'll just
16:26 - go over like topi is another example
16:29 - well that allows us to control the
16:32 - modern's creativity and it's only advice
16:34 - to you is one of these parameters uh at
16:37 - a time uh either temperature or top B
16:40 - so moving on we have frequency penalty
16:42 - and presence polity and if you want the
16:45 - model to Output a different answer for
16:48 - every same question or just cut down the
16:51 - number of repeated words in an answer
16:52 - need to use this frequency parity so
16:55 - anyway let's just so we did this when
16:58 - temperature equals zero so now let's
16:59 - compare this output to run temperature
17:01 - equals uh something higher like well
17:13 - so while that's happening uh as I said
17:15 - before frequency penalty uh basically
17:18 - penalizes based on how often a word
17:20 - occurs by repeated verticals and
17:22 - presence penalty penalizes the model
17:24 - based on uh just whether or not the word
17:26 - exists
17:28 - so let's just compare these two outputs
17:31 - with different temperatures so here the
17:34 - algorithm is using bubble sort
17:37 - but here the algorithm is using the home
17:40 - selection sort so as you can see the
17:44 - although the answers will be correct the
17:47 - way it approaches these answers is going
17:49 - to be widely different based on the
17:51 - temperature or the top B
17:53 - so another quick thing that we can do
17:57 - here that makes our life a lot easier is
17:59 - when we want to use this in the code
18:01 - which we'll get to in the next session
18:03 - you can just hit this view code button
18:05 - it'll give us everything that we
18:07 - actually need to get started so all we
18:10 - have to do is just copy this code
18:13 - and I'll see you in the next video where
18:16 - we're actually gonna do
18:17 - work through this API in our YouTube
18:20 - channel
18:22 - you should find this notebook Linked In
18:24 - the description below and this is
18:27 - something this is called a collab
18:28 - notebook which is just an online
18:30 - environment that helps us run our python
18:32 - code so to get started all you have to
18:34 - do is just press shift and enter and it
18:37 - you can just go through all this collab
18:40 - notebook
18:43 - so we'll just wait for all the necessary
18:44 - python packages to install and these
18:48 - packages basically let us call the open
18:51 - AI API and actually access charge apt so
18:55 - let's just wait for that to finish
18:57 - and let's go through
19:00 - what this open ebi game means
19:04 - middle black or not
19:07 - okay
19:08 - so now let's move on to this API key and
19:12 - so what is an API key
19:15 - since
19:17 - open AI builds you on using the API
19:21 - you'll need a password so that no one
19:24 - can get access to this but you and
19:27 - that's what the API key is it's just a
19:29 - password to your API so obviously I'm
19:32 - going to be revealing this password to
19:34 - you guys but I'm going to deactivate it
19:37 - as soon as this tutorial ends so I'm
19:39 - going to show you guys how to do all of
19:42 - that
19:43 - so you don't have to worry even if
19:45 - someone gets access to this key you can
19:47 - just disable it so what you would do is
19:49 - you would go out to your uh you know
19:51 - this account and you would hit view API
19:54 - keys and once you're in this dashboard
19:56 - you can just create a new secret key you
19:59 - can call it anything so for this one I'm
20:01 - probably just gonna follow course Radio
20:02 - 2 for one and create the secret
20:07 - I'm going to copy this now and
20:10 - once it's copied I can just
20:15 - yeah I can just paste it in here so this
20:18 - will be a password that's unique to you
20:20 - and say you've accidentally revealed
20:23 - this what you can do is you can actually
20:25 - just hit the trash icon here called
20:27 - edible key and it basically disables the
20:30 - gear so you can't access it through this
20:32 - key anymore
20:34 - I'm not gonna disable this but I've made
20:36 - a couple of ones here that I'm going to
20:38 - disable
20:39 - and yeah I'm going to be disabling this
20:42 - API key as soon as this video ends so
20:44 - let's just
20:46 - shift and enter and that runs this code
20:49 - so your environment variable is now
20:51 - opening Aid
20:52 - and this is just an example of how we
20:55 - call the epr
20:56 - I'm not gonna use this as the example
21:00 - I'm gonna go back here and go to the
21:02 - playground and use this uh example that
21:04 - open air has provided
21:07 - so in this case let's just hit new chord
21:12 - and then quality
21:16 - yeah let's put there
21:18 - okay so we can remove this open ABI Cube
21:21 - if there's already defined that up here
21:22 - and now
21:24 - this response body in this content here
21:27 - which is this part here we can just say
21:30 - what is your name
21:33 - and I'll show you guys the other games
21:37 - after we're done with this
21:40 - so as you can see it does run but it
21:42 - doesn't just return us with the response
21:45 - so what I can do is I can just uh the
21:47 - whole value is
21:50 - um in shorting this response variable so
21:52 - I can just print out response
21:54 - and as you can see it gives us a bunch
21:57 - of the state up here and the only thing
21:59 - we're actually interested in is this
22:01 - part here right it says digital
22:03 - assistant so notice how I decided it all
22:06 - equals you through here but here it's
22:07 - all equal to assistant the content is I
22:09 - am in the AI language model developed by
22:11 - open AI I don't have a personal name
22:14 - whatever
22:16 - so obviously this this
22:18 - is the only thing that we want the model
22:20 - the output and everything here is just
22:22 - uh something that we shouldn't be you
22:26 - know using so what we can do is we can
22:28 - just
22:29 - extract just to reply to this
22:33 - and so this goes through the entire Json
22:37 - data and it just gets us the content
22:40 - here so it's the same thing here let's
22:43 - run up here
22:45 - so another thing you can do is again
22:47 - just um everything that you do in this
22:49 - playground you can do here so you can
22:51 - put on system message here and the way
22:54 - you do that is just by defining this
22:56 - other dictionary
22:57 - and
22:59 - you put the row a system and the content
23:01 - does whatever the system as it is so
23:04 - here you can see your helpful assistant
23:07 - um you know
23:08 - obsess like potatoes
23:14 - and shift enter again
23:19 - it should take a little while but we
23:21 - should be clicking on the output pretty
23:23 - soon
23:25 - yeah
23:27 - so here
23:30 - as you can see it does assign that
23:33 - personality to it because you know it's
23:35 - just potatoes but it also just completes
23:38 - the task as well another thing that you
23:40 - can notice in this example is I have the
23:43 - power to change everything that I've
23:45 - changed in this pretty loud so I have
23:46 - the bar to change the model the messages
23:49 - the temperature and if you come up here
23:52 - you can actually see that everything
23:54 - that I changed up there you can sort of
23:56 - just assign a general variable and
23:58 - change it I'm not going to be doing that
23:59 - because I don't think it's very
24:01 - necessary for this tutorial but we can
24:04 - look to some more prompting now
24:07 - so one thing to note is that GPD 3.5
24:10 - doesn't really pay attention to the
24:12 - system message as much so generally
24:15 - whenever you want to assign a system
24:17 - message you should be deported
24:21 - so let's go to this example here which
24:24 - is few short prompted
24:26 - so if you guys remember what I said
24:28 - about fine tuning in the previous in the
24:31 - slides explanation this is sort of
24:34 - analogous to that when you're sort of
24:36 - giving the model example answers you're
24:39 - scripting this entire conversation here
24:41 - so you're saying that the system method
24:43 - is this the user has inputted this and
24:46 - the assistant has responded like this so
24:48 - this didn't actually happen but you're
24:50 - saying that this is like the ideal uh
24:52 - response that the assistant should be
24:54 - giving you and once you give it lineup
24:58 - examples it sort of learns uh to you
25:01 - know output the answer based on
25:03 - how the user desires it so here let's
25:06 - just change the model to gpd4
25:11 - and we can run this code there
25:18 - yeah so as you can see it does give us
25:21 - this output we can try this
25:24 - well we can track this is sort of a
25:26 - simple example but we can try this maybe
25:31 - Maybe
25:34 - if so I'll just uh
25:37 - what
25:45 - liquid
26:02 - so as you can see like we can compare
26:05 - the output that it's given previously
26:08 - with the few short prompting and now and
26:11 - I would say like the output before was a
26:13 - lot more concise and in-farm than
26:16 - whatever this is
26:18 - so again another example of things short
26:21 - prompting where we can actually just
26:24 - assign the system message as something
26:25 - that follows pattern so that it it can
26:28 - transistor
26:30 - Mom it enhances whatever it learns so I
26:33 - highly recommend you guys to just go
26:35 - into this notebook it is just your API
26:38 - Key by connecting with your car and just
26:42 - simulating a bunch of conversations and
26:44 - tweaking a lot of these parameters here
26:47 - and so I'll get a feel for it because
26:48 - there are no ideal set of parameters for
26:52 - every
26:53 - um for every problem
26:55 - much like there is no ideal dump for
26:58 - every problem it's a process that comes
27:01 - from just uh you know keep or driving
27:04 - error and just trying out a bunch of
27:06 - different solutions
27:08 - so one thing I didn't mention so far is
27:10 - that like this API is actually built so
27:15 - and the way open AI charges you for this
27:19 - for using this API is through the number
27:21 - of tokens so you can think of a token as
27:25 - I've explained before as three-fourths
27:27 - of a word approximately and so for every
27:29 - three-fourths of a word every thousandth
27:31 - reports of a word you would be charged
27:34 - differently based on what model you're
27:36 - using so this took this Library here
27:41 - um sort of helps us understand how much
27:43 - we're using it but we are a much better
27:46 - way to do this is just going through uh
27:49 - is going through your manage account and
27:51 - just looking through uh how much you're
27:54 - being built because openm provides you
27:56 - with all the data and accessory we're
27:59 - here is just a way to do this
28:00 - programmatically
28:02 - um you can just this is just something
28:03 - where you just copy paste the code in
28:05 - and it helps you with that there's
28:07 - really no bonded understanding what this
28:09 - means
28:12 - okay so as you can see it's counted 129
28:15 - prompt tokens so you can just do the lab
28:17 - and see how much that charges so in
28:19 - general the high-end models like gpd4
28:21 - are slightly more expensive than gbt 3.5
28:24 - but I would say the price is
28:27 - um pretty minor it's like
28:30 - 0.03 dollars or something or a thousand
28:33 - tokens per one of the models
28:36 - so that was it for using the API uh
28:39 - hopefully I've given you guys a much
28:41 - better
28:42 - intuition on how this works
28:45 - so uh one last thing that we can do is
28:47 - actually the finding our prompt through
28:49 - error Labs so
28:51 - as you can see all of this was just
28:53 - asking it a question and what we
28:55 - continue to sort of help with asking
28:58 - better questions is asking the model
29:00 - itself to make a better question so what
29:02 - we can do is like
29:04 - uh can you trace this question
29:09 - to be more
29:13 - so here
29:14 - consists and to the point
29:18 - and then you would just ask it like
29:21 - what is python
29:25 - or
29:26 - that's pretty concise already maybe we
29:28 - could do like
29:29 - um
29:30 - if I had three apples and my brothers
29:38 - father Eric or
29:42 - how many do
29:45 - in total
29:48 - and so we can just you know run that
29:52 - thank you
29:55 - yeah so it makes it a lot about concise
29:57 - and you can use this as your prompt
29:59 - instead of your other prompt and the
30:02 - reason why this is helpful because is
30:03 - because you can cut down the token size
30:05 - and so you know open AI charges you less
30:08 - we're using more concise plugs so you
30:11 - can just say reduce this
30:13 - and hit submit
30:16 - yeah so this is just a match token so uh
30:19 - over time if you keep calling some
30:21 - prompt you can just optimize it through
30:24 - charge GPD so
30:26 - so that was it for prompting now let's
30:29 - go into actually applying this into some
30:31 - projects my first project will be
30:33 - cloning the entire chart GPT UI and
30:36 - assigning it a custom personality so
30:38 - that we can interact with uh you know
30:40 - custom characters let's unveil it users
30:45 - we're gonna be using the chains that
30:47 - package in order to make our chat GPT
30:50 - column
30:51 - so chain lit is basically this framework
30:53 - that allows us to build user interfaces
30:56 - really easy if you have had experience
30:58 - with a package like streamlit you can
31:00 - imagine chain that is like streamlit but
31:03 - for a large language model applications
31:06 - so comes with a bunch of different
31:08 - features and unique Integrations and
31:11 - ideally uh this is what our end goal is
31:14 - going to be as shown by this video
31:17 - so just skip through
31:21 - yeah so
31:23 - as you can see it's a pretty pretty
31:26 - feature-itch user interface
31:29 - and now let's get started building this
31:33 - okay so now let's actually get started
31:36 - with uh cloning this user interface
31:40 - the first thing that we're going to want
31:41 - to do is import our change that package
31:44 - and the way we install it before we
31:47 - import is we go to the terminal and
31:49 - ignore everything that's happening here
31:50 - all we need is uh just a python
31:54 - environment and you will do this command
31:56 - so bip install chain lid and we're doing
31:59 - this in the terminal tab so once you do
32:02 - that
32:03 - it should install all your packages
32:06 - and if you haven't already you'd want to
32:09 - do pip install openai as well
32:11 - and so this basically installs all the
32:14 - packages from
32:16 - external source that you can use to you
32:19 - know work with chain bit and all these
32:21 - features
32:23 - okay so now let's start our first goal
32:26 - will be the created user interface that
32:30 - just outputs everything that the user
32:32 - inputs
32:33 - let's start by doing defining a tag so
32:38 - so on message you would Define a
32:41 - function
32:42 - uh main this would be as asynchronous
32:46 - don't worry about async because that
32:48 - just means that it's going to wait for
32:50 - the user to send the message
32:53 - so it start executed immediately so
32:56 - basically this function main takes in uh
32:59 - the parameter of message which is
33:02 - supposed to be this is going to be a
33:04 - mapped to a string so this message is
33:06 - going to be whatever the user inputs and
33:08 - all we're going to do is we're going to
33:09 - say await chain that scl CL dot uh
33:14 - message and then we're going to send
33:16 - this
33:18 - and right now what it's going to do is
33:21 - it's just gonna return an empty object
33:24 - so what we have to do is we have to
33:26 - actually uh specify what we want in the
33:28 - contract so in the content here we can
33:31 - uh yeah so in the content we can just
33:34 - say message
33:37 - addressing message because that's what
33:39 - the user inputs
33:40 - and I think yeah so this is about it
33:44 - for our basic example what we can now do
33:47 - to run this and actually see our user
33:50 - interface is in your terminal again uh
33:53 - you have to learn to get really familiar
33:56 - with your terminal because it can be
33:57 - really useful for the upcoming projects
34:00 - so go to terminal and just run chain
34:04 - later on and look at your bicon file's
34:08 - name minus main.py here so I'm just
34:11 - going to put main.py and then I'm just
34:13 - gonna use the W flag so you should
34:16 - really remember that
34:19 - um you know you should remember that
34:22 - you're not supposed to run a chain with
34:24 - program with your run but run button so
34:28 - I'll make this more clear uh after we
34:30 - run this command
34:35 - okay so as you can see it's running on
34:39 - localhost Port it calls it
34:42 - is just just just so that we have
34:45 - complete Clarity we will be using the
34:47 - screen button to drop something for code
34:49 - but this executes
34:51 - it in a server and chain that helps us
34:54 - you know deal with the server and all of
34:57 - the back end so all we have to do is
34:58 - just run that commit and now if we just
35:00 - put it in high uh you know
35:05 - whatever uh whatever the user inputs the
35:09 - chatbot will out
35:12 - so for some of you running this for the
35:15 - first time it might not actually look
35:17 - like this you might have a bunch of
35:18 - random text that's appearing there like
35:20 - welcome to chain lit and the reason why
35:23 - that's happening is because of the chain
35:24 - the dot MD file so here's just a sneak
35:27 - peek up where we're going to be covering
35:29 - in the future but for now go to this
35:31 - channel.md file as you can see mine is
35:34 - empty but yours might be just full of a
35:37 - bunch of text and all you have to do is
35:39 - that none of the text here is imported
35:42 - so you can just you know Ctrl a and
35:44 - delete all of it or you can add
35:47 - something like welcome to this interface
35:55 - Begin by
35:57 - sending uh wait that's it or something
36:00 - like that so once you do this hit
36:03 - command s which saves it as you can see
36:06 - because of our W flag that we put here
36:08 - uh the server actually watches for
36:11 - changes so one as soon as we hit the
36:14 - Ctrl save it says file modified change
36:17 - the dot MD preloading app and it should
36:20 - be reloaded yeah okay so it as you can
36:23 - see it says welcome to this interface
36:24 - Begin by sending a map subtract there's
36:26 - a CFI and it goes away but as you can
36:30 - see it just outputs everything that we
36:32 - have a couple of more options that we
36:34 - have in this user interface is
36:38 - hiding you know chain of part and
36:40 - expanding these messages but more
36:43 - broadly you know you can double between
36:45 - dark more in light mode I'm gonna stick
36:47 - to dark mode for this tutorial
36:51 - okay so now we pretty much have you know
36:54 - the basis
36:56 - and the user interface all we have to do
36:59 - now is pass the message into
37:05 - into the API
37:08 - and we should and then you know just dot
37:12 - send
37:14 - the answer
37:16 - right
37:20 - so let's let's do that now and this is
37:23 - going to be pretty easy because we've
37:25 - already done this uh bunch of times in
37:27 - the collab notebook what we're gonna do
37:30 - is just you know wait for
37:32 - okay so here all we're gonna do is maybe
37:36 - just make something called response here
37:38 - and in this response object we're just
37:40 - going to put uh the chat completion and
37:44 - dot create
37:47 - and in this we're gonna put our model
37:49 - our uh we're just gonna leave that empty
37:52 - for now
37:53 - and our messages
37:56 - again
37:57 - MD for now
38:03 - and
38:04 - maybe your temperature so why not come
38:07 - for a shirt
38:09 - equals something
38:12 - so you you need to remember to put
38:14 - commas after all of them except for the
38:17 - last one
38:18 - and commas between the messages as well
38:23 - so the model can be anything you want
38:25 - another gbd4
38:27 - and the messages
38:30 - is an array of dictionaries so in the
38:33 - dictionaries you'll need to pass two
38:36 - parameters you'll need a role and you'll
38:39 - need the contact
38:41 - and I'll explain what that is in just a
38:44 - second
38:44 - so again roll
38:48 - or whatever that is and then the corn
38:50 - bat
38:53 - a temperature you can decide to build
38:55 - you know whatever you want between zero
38:57 - and one or two
39:00 - and once we're done with this we gain a
39:04 - tourism singer
39:08 - yeah it should be fun Okay so
39:11 - so since so in our role
39:14 - uh key value pair we can just put this
39:17 - as the assist step
39:20 - and here we're just gonna put it as the
39:22 - user
39:27 - so as you can see here basically what
39:30 - happens is that this is the assistant
39:31 - message as you've seen in the chat GPT
39:34 - playground and this is
39:36 - saying that uh you you're like asking
39:40 - the aggression basically so in the
39:43 - content for the system message you can
39:45 - put you are uh helpful
39:48 - assistant
39:52 - and in the in the user message we're
39:55 - actually going to pass in our message
39:57 - variable that's passed on by the user
40:00 - and once we have this we instead of you
40:04 - know content equals whatever the user
40:06 - messages we're just gonna return the
40:08 - response from childcpd
40:12 - Okay so
40:14 - now let's run this again same command in
40:17 - terminal
40:25 - okay so it's saying uh no idea keep
40:28 - provided
40:29 - let me provide will make the idea
40:32 - and
40:34 - so maybe
40:36 - uh we can just we'll store it
40:40 - started
40:51 - and then we can just put this in so and
40:54 - skip it
40:59 - save that
41:11 - hey then just say
41:14 - just something I'm going to draw it here
41:17 - and I'm not quite sure we're going to
41:21 - drop
41:21 - so let's try to debug this maybe
41:27 - uh
41:30 - okay so maybe what we can try doing is
41:33 - just try this response into a string
41:37 - and hit save
41:46 - let's let's see if this works
41:50 - okay perfect
41:52 - so yeah um basically the mistake I made
41:55 - was that this was just returning just an
41:58 - object a Json object and the content on
42:02 - the uh support strings
42:06 - so as you can see in this object again
42:09 - uh let me just you know redo that okay
42:13 - so here
42:14 - to my message hi it says content is
42:17 - hello how can I assist you today and
42:20 - obviously we don't want any of this
42:22 - other stuff around it and as I've shown
42:25 - you guys in the collab tutorial all
42:27 - we're gonna do is just uh remove these
42:30 - brackets and then we're gonna index our
42:33 - way through to get our message so
42:37 - we're gonna go to choices
42:40 - and then inside choices we're gonna pick
42:43 - the zero the element
42:45 - and then the zero element we're gonna go
42:47 - to the message
42:49 - and then send message where it went to
42:51 - go to content
42:55 - okay
42:57 - so
42:58 - all of this is going to be wrap in an
43:02 - app string so I'm just gonna
43:04 - put this
43:06 - like such and then you just remove this
43:09 - and then you put this here
43:15 - Okay cool so once we did once we do this
43:19 - let's just put a cover here where say
43:22 - I'm not 60 and then
43:24 - Mom they're saved about
43:27 - so now let's try running it again
43:31 - oh yeah okay so
43:33 - it works you can uh uh you know
43:38 - tell me
43:40 - a short one like
43:42 - story
43:48 - yep so as you can see we've successfully
43:51 - cloned the chat GPD user interface and
43:55 - we can also uh call our API through this
43:59 - and modify any of our parameters here to
44:03 - suit the content
44:05 - now what we're going to do is something
44:06 - interesting where we can sort of talk to
44:11 - the model
44:12 - um that is assigned a specific
44:15 - personality so what what I mean by that
44:17 - exactly is we're going to be able to
44:19 - change the user message to something
44:22 - that changes its personality so here you
44:25 - can say like you are an assistant
44:28 - that is obsessed with
44:32 - power rabies Legolas
44:35 - all right so once you do that you can
44:38 - just hit Ctrl save it should modify a
44:40 - bunch of times and
44:43 - you know you can just start by sending a
44:45 - message so Phi
44:46 - and once we wait for a while it should
44:49 - return us the output
44:53 - so yeah it it follows the system message
44:57 - and it really pays attention to it
45:00 - because as you can see here you know
45:03 - it's obsessed with legless
45:05 - so this is how you can assign uh
45:08 - personality to it and this is how uh if
45:10 - you've ever gone to GitHub training
45:12 - repositories there's a bunch of these uh
45:15 - you know different
45:17 - different uh repositories that all they
45:19 - do is just you know change the
45:22 - system message to make it behave a
45:24 - certain way and that's exactly how
45:27 - you're able to get this Behavior without
45:30 - fine dma
45:33 - so next let's address some limitations
45:35 - of this approach so we have successfully
45:38 - built uh working chat gbd clone but
45:42 - there are a couple of limitations to
45:44 - this regarding the user experience
45:47 - so there's no streaming involved and
45:50 - streaming is basically this live uh
45:52 - token prediction of outputs that happens
45:54 - instead of just processing the output
45:57 - all at once and passing it directly
45:59 - there's no generating the messages
46:03 - um Mass uh you know button or message
46:06 - that basically helps the user actually
46:09 - identify it the backend process is
46:11 - running
46:12 - because otherwise you'll just have to
46:14 - wait without any confirmation on whether
46:18 - or not anything is actually happening on
46:20 - the back end
46:21 - and additionally after that there is no
46:23 - back-end context
46:25 - and by that I mean the user doesn't know
46:29 - what kind of llm or what is actually
46:31 - running in the background this third
46:33 - feature is pretty optional but it's uh
46:38 - very useful when it comes to debugging
46:40 - when we go into more complex large
46:42 - language model systems
46:44 - so let's see how we ideally wanted to
46:48 - look like this is from the official
46:50 - chain that website as you can see there
46:52 - is streaming here you know these tokens
46:54 - are being spit out live there is this
46:56 - background step that I talked about and
47:00 - it reaches to rewind the video again uh
47:02 - you can see that that stop task button
47:04 - there allows us to actually see whether
47:06 - or not this llm is actually generating
47:09 - and we are able to stop the flow
47:13 - so how do we get here
47:15 - the answer to that is through Nang chain
47:18 - so line chain is the most popular python
47:21 - library that helps us deal with error
47:24 - ramps it has some pretty Advanced
47:26 - functionality uh you know excluding
47:29 - whatever we just talked about here and
47:32 - we're going to be using this extensively
47:34 - in later parts of our course when we're
47:37 - going to web browsing agents and using
47:40 - our tools with agents so anyway let's
47:44 - get into our land check implementation
47:46 - now
47:49 - great so now let's look into how to
47:53 - actually integrate Lang chain so that we
47:56 - get access to all of those user-friendly
47:58 - features this is what we're going to be
48:00 - working with now uh I'm just gonna you
48:03 - know stop that app that we just ran for
48:06 - running and we're gonna pip install
48:09 - and this is very important U Lang chin
48:12 - and what this EU flag does is it
48:15 - basically installs the very latest
48:18 - version of blank chip and the reason why
48:20 - that's important is because line chain
48:22 - is a library that gets updated pretty
48:24 - much you know every week or so so
48:27 - anything that works today might be
48:30 - depreciated or discontinued next week
48:33 - and in order to you know keep all your
48:35 - dependencies in check we're going to be
48:37 - having to you know update our land chain
48:41 - so we I just defined a random string
48:44 - here so
48:46 - this template just gonna allow the llm
48:49 - to think step by step as it does here
48:52 - and the first feature I'm going to show
48:55 - you from Land chain
48:56 - is prompt template so if you guys have
48:59 - ever worked with the python format
49:01 - function this is pretty much the same
49:03 - thing
49:04 - what I mean by this is uh this is just
49:06 - vanilla python so I'm just gonna do uh
49:09 - template dot format and then inside this
49:11 - question equals uh
49:14 - you know whatever you want like what is
49:17 - one two three four whatever so let's
49:20 - just I'll happen on this
49:28 - okay my bad um you're supposed to
49:31 - print this up okay so let's just print
49:34 - this up
49:37 - yep so
49:39 - what it does is it takes this template
49:41 - variable and then it formats it to one
49:44 - two three four but
49:46 - how's that actually happening well these
49:48 - curly braces indicate that this is an
49:51 - object that needs to be formatted so
49:54 - everything inside this curly braces is
49:56 - going to be replaced by uh what is one
49:58 - two three four and that's exactly what
49:59 - prompt template does but it makes it a
50:01 - lot easier and a lot more llm friendly
50:05 - so we're gonna
50:07 - get right ahead with the line chain and
50:11 - change that implementation we're going
50:13 - to just Define two tags here we're gonna
50:15 - do CL dot on message as well
50:19 - and once we do this we are going to
50:25 - yeah so we are just going to
50:29 - create a main chat so we're gonna just
50:32 - Define our main function here and inside
50:35 - this we're gonna do our prompt equals
50:37 - something our llm chain equals something
50:40 - and then CL dot user session dot set
50:47 - all right
50:49 - llm shin
50:54 - tool as an M chain so
50:57 - and chain
50:59 - you can set that to
51:01 - llm
51:04 - shin
51:08 - okay so now that we have that let's
51:11 - actually uh go into
51:14 - what we're doing here so what on chat
51:17 - star does is as soon as this object of
51:20 - you know the chain lit UI is deployed
51:22 - here are the variables that we need to
51:24 - initiate
51:25 - so I'll just um
51:29 - yeah I'll just create the prompt here so
51:32 - prompt template is the object and set
51:35 - this we're gonna put our template as
51:37 - template
51:39 - and it takes some random term I take
51:41 - some template and with this it takes an
51:45 - input variable so input variables
51:48 - because you have something else so in
51:51 - this case our template would equals
51:55 - the template variable and our input
51:57 - variable is the stuff inside the girly
52:00 - bracket so that's this question
52:02 - okay so that's it for our prompt and now
52:05 - let's initialize what our llm shape so
52:08 - what is an llm chain here and llm chain
52:12 - for now you can think of it as something
52:14 - that connects the prompt with our large
52:18 - language model
52:19 - so in this case let's just uh
52:22 - foreign
52:32 - a bunch of uh parameters so we're going
52:34 - to do prompt equals something or else
52:37 - the lamb equals something
52:39 - and our streaming equals something and
52:43 - our employables equals something so I'm
52:47 - I'm gonna go through all what all of
52:49 - these mean
52:50 - and the reason why it's showing that is
52:52 - because
52:53 - it needs to be commas so our prompt is
52:56 - basically just our prompt uh which is
52:58 - the variable I Define here
53:01 - uh and our slm which is going to be the
53:05 - regular open AI model but there's a
53:07 - different way to define it now instead
53:10 - of you know going through the hassle of
53:11 - doing a bunch of different uh you know
53:14 - API calls all you need to do is just
53:16 - open Ai and when we pass in our
53:19 - temperature temperature here
53:22 - and my bad streaming is supposed to be
53:25 - uh parameter inside the alarm itself so
53:28 - uh streaming you can just set to true
53:31 - and this would you know stream yard and
53:34 - temperature will set the one for that
53:36 - after this verbose is basically uh our
53:42 - hot process this will make more sense
53:45 - when we cover the agents tutorial but
53:47 - you can think of verbose for now as just
53:49 - this extra additional text that goes
53:53 - into and you know helps the llm with
53:55 - resume so the thought process that leads
53:59 - up to the answer and then we're going to
54:01 - take this llm chain and then we're gonna
54:03 - store it in a user session variable
54:05 - called llm chain so that we can access
54:07 - it on the on message call
54:10 - so in our on message call where you know
54:13 - this we've done this one already let's
54:15 - define a mean another main function and
54:17 - there are message B string whatever and
54:20 - inside this we're gonna retrieve the
54:23 - chain from our user session so llm chain
54:26 - equals CL dot user session dot get so
54:30 - this time we we did get instead of set
54:33 - and in here we're just going to build
54:35 - llm Shake
54:37 - which is the variable that we passed in
54:40 - here
54:41 - okay so that basically just gets us this
54:46 - variable across tags and after we've
54:48 - done this
54:50 - it's just pretty simple now uh all we do
54:52 - is just
54:54 - uh call it a desert result variable so
54:57 - instead of calling our model itself
55:00 - we'll now be calling our llm chain so
55:02 - what we do is await llm underscore chain
55:06 - dot asynchronous call and then inside
55:09 - this we're gonna do our message and then
55:12 - call back so don't worry about what this
55:15 - callbacks thing is it just helps with uh
55:18 - streaming
55:19 - uh it helps its streaming because you
55:22 - know it just calls back and establishes
55:24 - a socket an action from what I
55:27 - understand with it and then afterward we
55:30 - do this all we do is just
55:33 - return or output as we've always done so
55:37 - CL Dot message
55:39 - and then we're just gonna sell you that
55:41 - and then here we're going to do result
55:44 - address
55:46 - um
55:47 - text
55:49 - so as you can see the
55:51 - although this may not save a lot of you
55:55 - know lines of code and might be slightly
55:58 - more we have a much more organized
56:01 - framework to think about things now
56:03 - because once we adopt this land chain
56:06 - framework uh you can you know sort of do
56:10 - stuff that's a lot more complicated than
56:12 - what we're doing within similar lines of
56:15 - code so an example of this is what we're
56:17 - going to be covering next uh you
56:19 - obviously you're not expected to
56:21 - understand if this right now but all of
56:23 - this is very close to uh how many ever
56:27 - we're doing here and this is infinity
56:29 - more complex than what we're attempting
56:32 - to do here
56:33 - so once we're done with that let's
56:36 - um go back and fetch our API key
56:46 - okay
56:47 - so once we're done with just putting our
56:50 - API Keys uh we can actually
56:53 - test this out and look at all of our new
56:56 - features
56:58 - so let's change the drawing
57:02 - and then change that run and
57:07 - I file name here's 19. integration
57:12 - right yeah Dot py and then we're just
57:15 - gonna put a watchdog there then
57:17 - so just wait for that to run
57:23 - okay great so now if we put high as you
57:27 - can see we get everything that
57:30 - was in the media that I showed you so hi
57:33 - what is your question about you know a
57:36 - thought process here is there's a
57:38 - question about
57:39 - this thought process stuff is going to
57:41 - be more relevant once I look into other
57:43 - Concepts but this is basically the same
57:45 - thing you can just say uh what is your
57:48 - name
57:52 - how are you doing
57:56 - so and it's actually streaming here and
57:59 - then passes us the final output
58:03 - bad yeah so everything that you did
58:05 - previously with your API you can sort of
58:08 - uh test out working with our system
58:11 - message here uh you can desktop working
58:14 - with different user messages different
58:16 - parameters like dog B
58:19 - Etc
58:19 - and now let's get into something that's
58:24 - slightly more interesting
58:25 - we're gonna be able to
58:28 - use this line chain framework to ask
58:31 - chat GPD questions on our own PDF and
58:35 - text documents that could be you know
58:38 - any size like 2000 pages
58:40 - so more on that now
58:43 - so now let's talk a little bit about
58:44 - battery databases and embeddings
58:48 - here are some examples that you might
58:50 - have heard of we're going to be using
58:52 - chroma DB and some of the others for
58:54 - this course
58:57 - so what are vector databases what two
59:00 - databases are basically this database or
59:03 - storage for specifically embedding
59:06 - information
59:07 - Vector databases allow us to query and
59:11 - utilize the this embedding information
59:13 - as fast and efficiently as possible
59:16 - so what's an embedding well and
59:19 - embedding is just this multi-dimensional
59:21 - space where all the similar objects are
59:24 - grouped together so in this case
59:27 - um in the Simmons all the symbols that
59:28 - represent two are sort of grouped
59:30 - together all the symbols that represent
59:32 - four are grouped together and all the
59:34 - symbols that represent nine are grouped
59:36 - together
59:37 - so why is this important well when you
59:40 - have the ability to group some similar
59:43 - objects based on a bunch of different
59:45 - parameters together you can build
59:47 - recommendation systems you can build
59:48 - search engines
59:50 - if this is also used in L Adams
59:52 - themselves in generative Ai and for this
59:56 - specific case we're going to be using it
59:59 - for context window expansion
60:01 - so if you remember from one of the
60:03 - previous slides in the introduction I
60:05 - said that fine tuning is not a very
60:08 - recommended way of enhancing a model's
60:11 - knowledge well this is where Vector
60:13 - databases come in because while fine
60:15 - tuning has its disadvantages because of
60:18 - catastrophic for gaming Vector databases
60:20 - just simply retrieve the relevant
60:22 - context information for the language
60:24 - model so it can use it what do I mean by
60:27 - this well let's get into this let's get
60:31 - a little more technical into this so say
60:33 - I have this 2000 page PDF which I want
60:36 - to use and I have a couple of questions
60:39 - about it and I want to be able to ask
60:41 - sharp CPT or any other Ram uh this
60:44 - question
60:46 - in general where if you'd want to do
60:49 - this I guess the naive way would be to
60:52 - just copy paste every single letter in
60:55 - that
60:55 - um in that book and just paste it into
60:57 - the chat GPT terminal window but
61:00 - obviously this won't work because you'd
61:03 - be hit with a context limit because the
61:06 - there's a max number of tokens that can
61:08 - be inputted into chat GPT
61:10 - and the way to get around this context
61:13 - help it work with new information is
61:15 - where Vector database is coming so
61:17 - basically I'm just going to put this PDF
61:20 - into a text splitter which will split it
61:23 - into
61:24 - text chunks of equal length so you can
61:27 - imagine like just five words sentences
61:29 - or a thousand word sentences and once
61:32 - these chunks are made they put into
61:34 - something called an embeddings generator
61:36 - and these embeddings are then stored in
61:39 - our Vector database
61:40 - so now what's cool about this is we can
61:42 - actually ask the question and the vector
61:45 - database performs an action called
61:48 - cosine similarity and what cosine
61:50 - similarity is is it finds the nearest 10
61:54 - or however how many are you want uh 10
61:57 - relevant sentences Within These
62:00 - embeddings so the 10 closes sentences
62:02 - and then it just outputs them so that's
62:06 - what the embeddings and the vector
62:08 - database do and this Vector database
62:11 - returns those 10 relevant sentences
62:14 - and these 10 relevant sentences go into
62:17 - a question answering model along with
62:19 - our question so that it can be answered
62:21 - and the reason why this is important is
62:23 - because we went from a 2000 page book to
62:27 - 10 relevant sentences based on any query
62:31 - and to me 10 sentences is obviously a
62:34 - lot more manageable and useful in a
62:36 - model than you know a 2000 page book
62:41 - so why use Vector databases I have said
62:44 - before like cosine similarity is the
62:47 - only function that Vector B databases
62:49 - actually perform along with storage so
62:52 - why can't you just put all these
62:53 - embeddings into something like an array
62:55 - and do a cosine similarity function with
62:57 - them it's certainly possible but the
63:01 - reason why Vector databases are just so
63:03 - popular and being used so much is
63:05 - because they have clever algorithms that
63:07 - help us retrieve all these relevant text
63:10 - information at super fast speeds along
63:13 - with a lot more efficient memory
63:15 - efficient usage
63:17 - additionally it's also very convenient
63:20 - because we just we can just retrieve
63:22 - those 10 relevant sentences through one
63:24 - simple function call
63:28 - so this is going to be the architecture
63:30 - for project 2 as I've already mentioned
63:32 - we're going to be able to we're going to
63:34 - build this entire Pipeline and one very
63:37 - important distinction I want to make
63:38 - here is the difference between our
63:40 - embedding generator and Q a model if you
63:43 - remember from the number classification
63:45 - example I told you that the task was
63:47 - digit classification and it's able to
63:49 - group those uh you know symbol similar
63:51 - symbols together
63:53 - this is very similar but here you're
63:56 - doing that next
63:57 - um you know that next token word
63:59 - prediction problem
64:01 - and this is there's an important
64:03 - difference here because the embeddings
64:04 - generator neural network
64:06 - and the Q a model are very different
64:10 - the embeddings generated neural network
64:11 - for this case we're going to be using
64:13 - something called Adda 2 because it's a
64:16 - lot more efficient and it's sort of um
64:19 - you know the standard for generating
64:21 - embeddings whereas the Q a model is
64:23 - going to be something like gpt3 and gpd4
64:27 - um in in theory it is definitely
64:29 - possible to generate embeddings through
64:32 - gpt3 and gpt4 but add a uh tends to
64:36 - perform better and is a lot cheaper
64:39 - so now let's get into the code
64:43 - so now let's get some more hands-on
64:45 - experience with
64:46 - how Vector databases work basically
64:49 - what we're going to be using for this
64:51 - tutorial is chroma DB which is an open
64:54 - source Vector database that we can run
64:56 - locally on our machine as pretty
64:58 - scalable for production
65:01 - so the way we get started is we just do
65:03 - pip install chroma DB
65:07 - and it should be installing everything
65:08 - but
65:10 - if you ever face any problems with
65:13 - installation and I say this because I
65:16 - have personally faced a problem of
65:18 - editing out here when I try to install
65:20 - this package for the first time the way
65:22 - you fix that is running this command
65:24 - that I've commented out here so if you
65:27 - do that the you know the errors will
65:29 - restart the clocks out
65:31 - once we do that we can set our chroma
65:33 - client equal to you know just our Chrome
65:36 - idb so that we can start querying our
65:38 - our question so here we can do
65:41 - collection
65:42 - and then inside this you do chroma
65:44 - client dot create a collection and you
65:51 - can think of a collection as basically
65:52 - this place where we store our embedding
65:55 - so this is the actual uh Vector database
65:58 - so so my correction will be our Vector
66:01 - database
66:03 - this is supposed to be chroma
66:06 - okay and now since we say that it's
66:09 - supposed to be our Vector database
66:11 - we can add information to this Vector
66:15 - database in the form of three variables
66:18 - so there is a documents variable there
66:22 - is a metadata variable
66:25 - and then there's an ID variable
66:30 - so our collection object contains all
66:32 - these three and let's go through them
66:34 - one by one and understand them step by
66:37 - step so document takes a list a metadata
66:40 - sticks on this ID stakes and arrest
66:43 - so document
66:46 - is the actual list of our documents if
66:48 - you remember from
66:50 - the slide explanation that I gave you
66:53 - all of our you know tokenized and split
66:57 - text go into this document so with this
67:00 - example I can just say my name is akshat
67:03 - maybe and
67:06 - another thing I could add is my name is
67:09 - not akshat
67:12 - and then
67:14 - we can uh we can Okay so let's just do
67:18 - that
67:19 - and uh this is supposed to be documents
67:22 - by the way my bad
67:24 - and
67:26 - after we've done this obviously this you
67:28 - can you can put as many documents as one
67:30 - and here they're gonna put our metadata
67:34 - so
67:35 - in this example it's not really going to
67:38 - be important and I'll explain to you why
67:41 - metadata is going to be very important
67:43 - when we uh then we're able to
67:48 - when we're able to build our question
67:50 - answering system
67:52 - so for now let's just put all of our
67:56 - resources so it has to be one source per
67:58 - document
68:02 - my source
68:04 - and you know what maybe let's just
68:06 - change this to name is true and here
68:10 - let's set it to name his fault so this
68:13 - will not be this will not affect any
68:15 - Vector database retrieval or competition
68:17 - this just acts along with the steps
68:20 - and they're each you know ID
68:24 - um each each document has to have a
68:25 - unique ID so id1 ID do
68:29 - and once we're done with that we can uh
68:33 - sort of look into what metadatas is so
68:36 - in so let's just uh imagine that we
68:39 - finished our uh you know PDF retrieval
68:43 - problem which is that we have uh
68:46 - successfully been able to use strategy
68:48 - PD to answer questions on our PDF even
68:51 - if you know this model achieves
68:54 - state-of-the-art performance it's still
68:56 - prone to a maybe a little bit of
68:58 - hallucination
68:59 - and there's obviously a risk of that
69:02 - happening and sources help combat that
69:04 - because even if the model hallucinates
69:07 - it will be forced to uh sort of output
69:11 - The Source from which it got its
69:13 - information so even if it misinterpreted
69:15 - the information the user can go to that
69:17 - source and find out exactly what uh the
69:20 - source is and sort of interpret it
69:23 - themselves so that there is no
69:25 - misinformation being spread
69:27 - so that's it for collection and now we
69:31 - can we can pass in a results object here
69:35 - and inside this results variable we can
69:38 - just query The Collection so
69:40 - collection.query
69:42 - and square your object ticks and do
69:44 - things it takes in query uh texts
69:49 - they're just going to put a singular
69:50 - text in for now but you can pass
69:52 - multiple questions in here and then end
69:54 - results
69:56 - so end result is the number of results
69:59 - obviously and query text this is your
70:01 - question so here this will be what is my
70:03 - name
70:04 - right so once we do that we can then uh
70:09 - Delete all this and then just say print
70:11 - results
70:16 - okay so now let's run the code and see
70:21 - what this has to say okay so it returns
70:24 - us with this so what it's happening is
70:26 - it takes this collection variable it
70:28 - queries it so what that means is it
70:31 - takes the what is my name uh string and
70:34 - it performs a cosine similarity function
70:37 - between what is my name and my name is
70:40 - akshat
70:41 - and it returns to me the distance
70:44 - the metadata
70:46 - the embeddings there's no embeddings for
70:49 - now and in the documents so the
70:53 - embeddings are not being stored at the
70:55 - moment but we will be using Adda in the
70:58 - next example to do this but for now
71:00 - imagine that you know this is something
71:03 - that was just learned so what what the
71:06 - other thing we can do is you know make
71:07 - this a number of results equals two and
71:10 - we can compare which one is more similar
71:13 - or closer to answering the question so
71:16 - as you can see here my name is akshat
71:18 - has a distance of 0.83 but my name is
71:22 - not akshat has a system distance of 0.93
71:25 - so since this distance is closer you
71:30 - know the lower the distance the closer
71:33 - it is to answering the question uh this
71:36 - is going to be favored and in a top case
71:39 - situation we're gonna be passing this
71:41 - sentence here into our language model
71:44 - over this one
71:46 - so that was it for you know just the
71:50 - fundamentals of vector databases now
71:53 - let's apply this and whatever we've
71:55 - learned so far in this course is
71:57 - something more exciting we're going to
71:59 - be able to build a PDF and text bot that
72:04 - can answer any of our questions on a
72:06 - given PDF or text file of any length
72:12 - so now we have everything we need to get
72:15 - started with building our first document
72:18 - question answering system which is
72:20 - Project number two
72:22 - so for the sake of Simplicity
72:25 - um we're just going to be using uh PDF
72:27 - and text files as input to our project
72:30 - but certainly everything that you learn
72:32 - here is going to be easily generalizable
72:36 - to you know any other text file that
72:39 - contains text so
72:41 - before we get started let's import the
72:44 - necessary packages so there's this
72:46 - package here and if you just skip to
72:49 - this one tutorial insert without
72:52 - watching the previous one you'd want to
72:54 - install chroma DB
72:57 - uh pip install chroma
73:02 - comma DP
73:04 - yeah
73:09 - yeah sorry so control chroma DB
73:13 - and once you do that you're going to
73:14 - want to run this command here this is
73:17 - very important
73:18 - but only run it if you're facing any
73:21 - errors with you know um
73:23 - if you're facing any errors with coma DB
73:26 - so anyway let's get started uh we just
73:30 - imported our packages here and let's
73:31 - look into what's happening so just a
73:33 - quick recap on our architecture
73:37 - we have here that uh you know this PDF
73:41 - gets passed into a text better embedding
73:43 - generator gets stored in the embeddings
73:45 - query uh you know Vector database finds
73:48 - the closest
73:49 - um set of 10 sentences to this query
73:52 - using cosine similarity then those 10
73:54 - sentences along with the query are
73:56 - wrapped into the Q a model and then q a
74:00 - model like gbt for lgbt3 it just it then
74:03 - passes the answer out as our output
74:06 - so let's look at what's happening there
74:08 - so our text splitter as I've said before
74:10 - is
74:12 - you know this
74:14 - this element here
74:16 - and we get to choose what the chunk size
74:19 - is which is the size of the sentence
74:22 - chunk so here I just put a thousand
74:24 - characters but you can uh you know you
74:27 - can experiment with different types and
74:29 - then we Define our embedding Sayer here
74:31 - so open AI embeddings as a default uh
74:34 - uses the adder
74:35 - adder to model and that generally
74:39 - provides the best performance along with
74:42 - you know the cheapest cost outbreak so
74:45 - after that this is just a welcome
74:47 - message is this user interface and here
74:50 - we've defined two functions so uh here
74:54 - we've defined a process file function as
74:56 - well as a doc search function so what
74:58 - I'm going to do is I'm just going to
74:59 - zoom out momentarily to just you know
75:02 - give us a bird's eye view of what's
75:04 - happening okay so
75:06 - the process file is basically uh this
75:11 - you know this feature that La uh this
75:14 - feature that chain that offers where you
75:16 - can just input a bug so you can assume
75:19 - that most of this is this boilerplate
75:20 - till here
75:22 - but after this uh Point here is where we
75:25 - actually call the text figure so
75:26 - everything this function is doing is
75:28 - just taking the you know taking the
75:32 - embeddings here uh doing the text
75:34 - picture and applying it on our PDF file
75:37 - and then once we have all the chunks you
75:40 - can then label them as sources so more
75:43 - on this later let's move on to our doc
75:45 - search so doc search is obviously where
75:48 - we you know retrieve our data from our
75:51 - embeddings so once we've sort of split
75:53 - these documents into the smaller chunks
75:55 - we can then process our file here and
75:58 - you know set it in our user session so
76:01 - don't worry about this too much this
76:03 - just make sure that I our docs are
76:06 - available to both the mod around the
76:08 - client
76:09 - and the doc search uh you should be
76:11 - pretty familiar with this if you uh gone
76:14 - over the previous segment so all we're
76:17 - doing is we're just uh using the
76:19 - literary we're defining like retrievers
76:21 - here and the chroma DB takes all these
76:24 - embeddings uh along with you know the
76:26 - model of embeddings and it just Returns
76:29 - the relevant query so that's what this
76:32 - this does
76:34 - okay so now
76:36 - is the actual you know chain that user
76:39 - interface interactions part and I've
76:41 - divided this whole uh action space here
76:45 - into two functions so let's just zoom in
76:47 - and
76:48 - see what each one uh is about so
76:53 - one second
76:54 - so
76:56 - here we're defining an on chat start tag
77:00 - and basically what this does is you just
77:04 - um pass in a bunch of different you know
77:07 - bottle bits text zero you can just put
77:09 - whatever you want so uh welcome
77:13 - do this space you can eat
77:17 - use this to check with your idiots okay
77:22 - and so once we send this message we can
77:25 - move on to yes all the more important
77:27 - lines here so here as you can see it
77:29 - says while files equals none you'll want
77:32 - to ask for a file and here's where you
77:35 - can change uh what kind of file you're
77:37 - actually inputting so after this
77:39 - tutorial I would recommend you guys to
77:41 - do some research and then experiment
77:43 - with using CSV files so here we're just
77:46 - accepting text and media with the max
77:49 - size of this much but you can obviously
77:51 - ask for more and you know just just send
77:54 - that out as a message
77:58 - so basically once this is done we can uh
78:03 - once the user has inputted a pile we
78:06 - then say processing and then the name of
78:09 - the file and then we basically just call
78:11 - these two functions here and return all
78:15 - that output still here so you're just
78:18 - doing
78:18 - um
78:19 - my bad okay so here so you're just doing
78:22 - you know you're just setting our we're
78:24 - getting a retrieval chain ready and
78:26 - setting it here so more on the retrieval
78:29 - chain now this is a new kind of chain uh
78:32 - that we've learned language so the
78:35 - before we just did a basic
78:37 - you know other than chain but here we're
78:39 - doing the retrieval q a with sources
78:41 - chain so as I've mentioned before chains
78:44 - basically unite The Prompt and you know
78:46 - the model together but now we can add a
78:49 - sort of other layer of abstraction to
78:53 - this definition where
78:55 - chains basically combine prompts llms
78:58 - and other functions to this
79:01 - together so in this case it combines a
79:05 - retriever which is
79:08 - which are uh you know these embeddings
79:10 - here so this is a retriever which is our
79:13 - Vector database
79:14 - our model is the chat GPD you know this
79:18 - model and our retriever is you know this
79:22 - doc search as a retriever function
79:26 - so once we're done with that we can then
79:29 - you know just pass in pass this out and
79:32 - set this chain save this chain into our
79:36 - user session so that our backend can
79:38 - actually access this
79:41 - so now this is what the back end looks
79:44 - like
79:45 - let's let's go through this step by step
79:47 - because we've pretty much done all the
79:50 - work already you know we've established
79:52 - everything here and here is the place
79:54 - where we're actually going to call all
79:56 - our functions so here as you can see
79:58 - this is an on message function so
80:00 - whenever a message happens uh you would
80:03 - get fetch this chain this is retrievable
80:06 - chain and then you would
80:08 - you would basically just
80:10 - uh you know just strip your API and make
80:14 - the answer presentable and then just you
80:17 - know give it to the user but all this we
80:20 - could we could just stop it here you
80:22 - know we could just stop it here but a
80:25 - problem with large language models
80:27 - currently is that sometimes very rarely
80:30 - they do hallucinate and make up
80:32 - information when the context sent this
80:34 - sliding around
80:36 - and the way we can get around this is to
80:39 - provide citations to the user about
80:42 - what's actually happening so even if the
80:44 - large language model collusionics if the
80:46 - user is interested in doing further
80:48 - research which you know he should be for
80:51 - the sake of this example he can just
80:53 - click on the sources button and then he
80:55 - can read what's actually you know the
80:58 - the actual retriever output so uh that
81:03 - that's what this section fear does
81:06 - whereas sources you know we just
81:08 - classify our services here and it
81:10 - outputs our solution so all of this will
81:12 - make more sense once I uh
81:15 - done
81:21 - oh bye I don't think I don't remember
81:24 - defining any open the eyed key here so
81:27 - might ask Nate to do that
81:35 - yep open API key let's just go ahead and
81:40 - get an open API Okay so
81:42 - openai.com
81:44 - login
81:46 - and once you're in the login page you
81:48 - can just hit API and then
81:51 - view API keys so I've created one
81:54 - already but I'm just going to create a
81:56 - new one which I will definitely
81:59 - disable as soon as this video ends so
82:02 - create secret View and they'll just use
82:04 - this
82:06 - fail
82:08 - on
82:11 - or did I
82:13 - I mean
82:23 - okay this should work let's let's try
82:26 - doing this again
82:39 - okay I think I know what's happening
82:41 - here so import OS
82:53 - so generally when you're using packages
82:55 - I think it expects you to you know give
82:57 - it in this one rather than what I Define
83:00 - so just so that it's easier for it back
83:03 - just to identify what the keys so let's
83:07 - for the last time hopefully let's try
83:09 - this again
83:11 - yeah okay so it works we'll
83:14 - and this opens up here so welcome to the
83:16 - space you can use it to chat with your
83:19 - PDFs great so
83:21 - this is exactly what
83:24 - we've intended it wanted to do so now
83:27 - what we can do is we can just drop in a
83:30 - PDF file
83:32 - so what I did here was I just passed in
83:35 - you know this Sapient huge history of
83:38 - humankinds just a short extra profit a
83:41 - PDF but since uh this process is running
83:44 - locally you're pretty much unbounded by
83:47 - how much you data that you can input
83:50 - into this model so you'll see that it's
83:52 - processing here and
83:54 - what processing basically means is it's
83:56 - putting these text things into chunks is
83:59 - generating embeddings and passing the
84:02 - embeddings into the vector database so
84:04 - that we can you know perform all these
84:06 - steps here
84:08 - so let's just wait for that to happen
84:11 - one node is that at the time of
84:15 - recording this
84:16 - um I don't think chain that supports the
84:18 - drag and drop feature into files
84:20 - although it does claim to do that so
84:23 - your best bet is to just hit the browse
84:26 - button and then choose the file in the
84:28 - you know window that displays so anyway
84:30 - uh sapiens it's processed so we can
84:34 - you know ask it some questions so a
84:36 - question from this book would probably
84:38 - be like what is the in the Indian Theory
84:44 - and so as you can see here
84:46 - it shows me the retrieval q a chain
84:50 - with sources chain
84:52 - and you know it should be shorter it
84:55 - sort of shows me its hot process here
84:57 - which is exactly what this is so let's
85:00 - just go deeper into this Shin so the
85:03 - retrieval with sources come by is a
85:08 - combination of the stuff documents chain
85:10 - and the llms chain so that they can
85:13 - return our sources so let's just look
85:16 - into the actual output here and you know
85:19 - it so it answers the question obviously
85:22 - but what's more important here and
85:23 - what's more interesting is are the sword
85:26 - lists so I can say you know Source 42
85:30 - it's not that great at formatting
85:32 - because you know the PDF is just a
85:35 - non-structured text and also a
85:38 - tokenization and chunking could sort of
85:41 - affect the alignment but this is pretty
85:43 - easily you know fixed you can just
85:44 - remove Extra Spaces
85:47 - so
85:48 - again I recommend you to you know play
85:50 - with this and sort of test out what you
85:53 - can do to improve this further
85:55 - or I would you know initially
85:58 - like you guys work on is maybe removing
86:00 - these spaces between
86:03 - uh between these text spaces here and
86:06 - just keep asking it questions and see
86:09 - how it responds with uncertainty as well
86:11 - so if I ask it something like Port is
86:15 - CPM till it is uh safety I'm
86:19 - the spawn
86:21 - in a bowl with citations
86:25 - so I have no idea what this answer is
86:27 - kind of output
86:29 - but yeah
86:37 - so yeah
86:40 - it you know Jordan answers the question
86:42 - but as you can see it doesn't really
86:44 - respond in a pawn which is kind of a
86:46 - litigation and you can sort of
86:49 - experiment with how well you know you
86:51 - can make these and how flexible you can
86:54 - make these uh a quick fix to this would
86:58 - be you using an agent that first you
87:02 - know retrieves these sources and this
87:05 - answer and then you would pass this back
87:07 - into the llm so that it can make it a
87:10 - bomb so you could you know experiment
87:12 - with that although we are going to be
87:14 - covering agent structure
87:17 - so that's all for our q a with documents
87:21 - so now we're getting into some of the
87:23 - more advanced things that we can do
87:26 - and we're going to be looking
87:27 - specifically into the web browsing and
87:30 - agent capabilities of llms
87:33 - so why would you want an uh an agent to
87:37 - sort of browse the web well the reason
87:40 - is because there is a knowledge cut off
87:42 - uh in with the training data and you
87:46 - can't train a model on an infinite
87:48 - amount of data because that would be
87:49 - very expensive and time consuming
87:53 - instead you you'd want to train a
87:57 - reasoning engine rather than a database
87:59 - and you'd want to train something that
88:02 - can just have access to all these
88:04 - resources and synthesize it themselves
88:06 - rather than having some internal
88:08 - knowledge base that uh that you know
88:11 - can't be changed
88:13 - additionally some of the information
88:15 - that you might train an alarm on might
88:18 - be biased and might be susceptible to
88:20 - you know changes in the future which it
88:24 - hasn't accounted for yet and browsing
88:27 - the web helps us reduce this because it
88:29 - always has access to the latest
88:30 - information
88:33 - so now we're gonna go into agents which
88:37 - allows uh these LMS to perform Chain of
88:40 - Thought reasoning well here's an example
88:43 - of what I mean this is from the auto GPT
88:46 - GitHub repository
88:49 - welcome to this brief demonstration of
88:51 - Auto gpnc
88:53 - today we'll be showcasing the ai's
88:55 - learning ability by asking it to
88:57 - research information about itself
88:59 - let's begin the program's first step is
89:01 - to use Google to find relevant websites
89:03 - to what it's researching
89:10 - foreign
89:12 - the GitHub repository for auto gbt it's
89:16 - opened up for analysis
89:29 - after scanning the website's contents it
89:31 - is summarized to them and will now place
89:33 - them in the auto gpt.txt file we have
89:36 - opened
89:39 - as we can see auto GPC can teach itself
89:42 - about different topics using the
89:44 - internet allowing it to have a better
89:46 - understanding of the current world than
89:47 - chat should be deep
89:54 - so
89:55 - order GPD is what we call an agent and
89:58 - the way it works is this really cool way
90:01 - where it prompts itself so the user just
90:06 - gives it a prompt and it can formulate a
90:10 - plan and then input that plan back into
90:12 - itself and then output an action
90:15 - input that action back into itself and
90:18 - it can arrange all these chains of
90:21 - thoughts into a pretty sophisticated
90:22 - workflows that you can use to answer
90:25 - questions and now read more recently you
90:29 - can actually perform you know actions in
90:31 - the real world
90:32 - so the first problem that we're gonna
90:35 - try to solve in this basic course is
90:37 - what is our lecture we're gonna be we're
90:40 - gonna want to ask the agent this
90:42 - question and it should be able to use
90:44 - its tools to find the answer on the
90:47 - internet reason why it doesn't know the
90:49 - answer to what is rlhf is because
90:51 - reinforcement learning from Human
90:53 - feedback or our lecture is a New Concept
90:56 - that has only been discovered or
90:59 - invented in 2022 to 2023 so chart GPT
91:03 - having its knowledge cut off in 2021
91:06 - would not be aware of this whatsoever so
91:09 - we're trying to empower it to sort of
91:11 - research and learn about these things so
91:13 - that can answer us
91:16 - so how are we gonna do this we're gonna
91:18 - use Lang chains archive API
91:22 - or integration to do this so archive is
91:25 - an is a platform that allows us to have
91:28 - access to over 2 million scholarly
91:30 - articles in various builds from physics
91:34 - to economics to computer science and it
91:37 - pretty much details all the latest
91:38 - research and information that's
91:41 - available to us
91:43 - here is what the website looks like just
91:45 - in case it might be a little familiar
91:47 - and now that's
91:49 - go to the solution so we need a model
91:52 - that can know its resources and utilize
91:55 - its resources and furthermore it needs
91:58 - to be able to plan how to use these
92:00 - resources sequentially step by step
92:02 - prompt by prompt and
92:05 - when I say prompt by prompt I mean it
92:07 - can it should be able to prompt itself
92:09 - to finish the task without the need of
92:11 - explicit programming as well as not
92:13 - being stuck stuck in loops
92:15 - so thankfully all of this is going to be
92:19 - covered or sort of has been taken care
92:22 - of by line chain already but uh as it
92:25 - has conveniently implemented for us a
92:28 - research paper called the react
92:29 - framework that we can just use
92:32 - um to make any other laminated
92:34 - so here's what um
92:37 - something like an agent uh here's one
92:41 - algorithm that you could use
92:43 - or you can just pause this video and
92:45 - look at this in more detail but now
92:48 - let's just start with a simple example
92:49 - and dive into the code
92:52 - so now we're ready to actually Implement
92:55 - project number three which is going to
92:57 - be GPT researcher
93:00 - so gbt researcher basically answers our
93:03 - questions on pretty much the latest
93:05 - fields of Science and can provide us
93:07 - with citations if we ask it to
93:10 - so
93:11 - here's what we're going to be using if
93:14 - you haven't uh ever heard of archive
93:17 - before it's basically this website where
93:20 - there's like a bunch of scholarly
93:22 - articles that everything from you know
93:24 - physics to math to economics
93:28 - and it has pretty much all the latest
93:30 - research
93:32 - so let's get started here's the archive
93:35 - Plugin or integration in chat GPD and
93:38 - light chain and I've made a notebook
93:43 - I I made a notebook in your GitHub
93:47 - repository so that you can use it so
93:50 - backwards skipping ahead here let's just
93:53 - yeah here
93:56 - okay
93:57 - so here is internet browsing let's zoom
94:02 - in a little bit so that we can see the
94:04 - code better
94:07 - okay
94:10 - so we're just gonna import our basic
94:12 - functions here and we're gonna
94:15 - just Define our
94:18 - open AI API key so let's try to find it
94:22 - if I put it somewhere here
94:24 - and we're gonna put this back into our
94:27 - online
94:32 - okay so once we've defined the API key
94:35 - we can just look through some of the
94:37 - basic things this should look super
94:39 - familiar to you you know we've done this
94:41 - a bunch of times already uh
94:43 - this llm definition
94:46 - and after that we have this special
94:49 - parameter here called tools so basically
94:54 - the tools you know parameter allows us
94:57 - to store tools that the llm and the
95:01 - agent can access based on its needs so
95:04 - one thing that you can do maybe after
95:06 - learning the other tools in this course
95:08 - is you know just separate them with
95:10 - commas and pass in some of the other
95:12 - tools like wrap it
95:14 - that's not exactly how you define it but
95:16 - you can uh you can sort of integrate
95:18 - that into this toolbox so once we Define
95:21 - the Dual uh we Define a new type of
95:24 - chain which is our agent chain
95:27 - so as I've said before a chain basically
95:30 - unites our prompts with our large
95:32 - language models and our algorithms so in
95:35 - this case the algorithm that we're using
95:37 - is zero short react description
95:41 - so this is the algorithm that allows the
95:44 - llm think and you know prompt itself to
95:48 - reach the right conclusions so we're
95:50 - gonna set the max iterations to 5 so
95:53 - that it doesn't you know get stuck in a
95:55 - loop and it doesn't
95:58 - um
95:59 - it we you know it won't be built too
96:02 - much for uh your API usage
96:05 - so we're gonna be just passing our tools
96:07 - here with our llm and then Max
96:10 - iterations we'll post we'll get to that
96:13 - and handling this you should set this to
96:15 - true as well so after that all we do is
96:18 - just agent chain dot run with whatever
96:20 - question we have and let's observe uh
96:23 - the output so
96:25 - let's let's share it on this
96:35 - so while that's running let's just look
96:36 - at like sort of the uh to the many steps
96:40 - that it takes so
96:42 - this algorithm is called zero Shard
96:44 - react description so all we're doing is
96:47 - just asking it first word uh is you know
96:51 - are the Chip And if it doesn't already
96:53 - know that then it would identify an
96:56 - action to perform and so the action to
96:59 - perform here is archive which is our API
97:02 - that searches the research paper domain
97:05 - and then you would input the word r a
97:09 - ledger into our archive API so that it
97:12 - returns a bunch of you know research
97:14 - papers that are search results obviously
97:16 - this is not really presentable I I
97:19 - shouldn't have to you know look through
97:21 - all of this research to get my answer
97:23 - so what it does is you know it observes
97:27 - this output under observation and then
97:31 - it has a thought here so our leadership
97:34 - stands for reinforcement learning so it
97:36 - takes in all of this research that it's
97:38 - just found and then you know it
97:41 - it just finds uh itself an answer to
97:44 - this
97:45 - so that's what
97:47 - that that's how we're able to solve this
97:49 - knowledge Gap problem that we have
97:54 - an additional thing that you can do here
97:55 - is yeah you can just ask it anything you
97:57 - want so what is a black hole for example
98:02 - and it will give you the answer so let's
98:06 - just observe what the algorithm is doing
98:07 - first so it's it identifies that it
98:10 - searches it should search for a
98:13 - scientific definition for that it
98:15 - identifies the correct action this will
98:17 - make more sense when you give it more
98:18 - actions so that it actually has to
98:21 - decide between what actions to choose so
98:23 - it inputs the word black hole into the
98:25 - action
98:26 - and you know all this bead text here is
98:29 - whatever is written by archive so it
98:32 - looks at all this return text and then
98:34 - it thinks meaning that it it you know it
98:39 - um it takes in all this input and then
98:40 - it just gives us an output based on what
98:42 - it thinks so our final answer is a black
98:45 - hole is the reason region of space-time
98:46 - exhibiting gravitational acceleration so
98:49 - strong that nothing no particles or even
98:53 - you know the electromagnetic radiation
98:55 - can escape from them
98:58 - an interesting thing you can do here is
99:00 - you can query sort of the other types of
99:04 - data that come along with this research
99:08 - paper so you can see like with titles
99:11 - you know what were the authors in this
99:13 - paper and was it when was it published
99:18 - so that's all for the archive tool let's
99:21 - move on to the other tool that I'm gonna
99:24 - show
99:26 - but before that let's look into
99:29 - uh chain lit integration to just show
99:31 - you how easy it is to incorporate agents
99:34 - into you know a presentable UI so this
99:38 - is a chain that integration all this
99:41 - just boilerplate you know and all this
99:43 - code here we've already we ran that in
99:45 - the previous python notebook so you can
99:47 - just copy paste it here the other thing
99:48 - you need to remember is to store this in
99:51 - an agent variable and then let's see if
99:54 - this in the agent variable and the
99:56 - answer should be agent.org everything
99:58 - else is just boiler but it could uh and
100:01 - yeah this allows you to spin the final
100:03 - answer as well that's pretty cool
100:05 - so let's now run our
100:09 - agent
100:11 - with
100:12 - uh with chain with so again go to your
100:15 - terminal and then just type in chained
100:18 - run Internet
100:21 - and browsing
100:28 - okay
100:33 - I apologize for the overly long file
100:38 - names but just so then item number which
100:41 - one contains what I should open up this
100:45 - and okay so yeah as usual same error uh
100:49 - let's just
100:52 - copy paste our open API key back into
100:55 - this book here
101:00 - and this time let's make it a lot more
101:02 - convenient
101:04 - by adding that widget slide so
101:07 - it should open it up again and now it
101:10 - should be working yeah
101:12 - so again whatever we uh just created
101:15 - through the asian.run we can do the same
101:17 - thing so what is iron HF
101:22 - and you know I'm not familiar so it's
101:25 - just using archive and then it says
101:28 - title whatever the research that it
101:31 - found and then
101:33 - you know I now know the final answer so
101:35 - this is the final answer
101:38 - again what is uh black hole
101:47 - yeah so you know it answers the question
101:50 - for me and we can look at all the hot
101:52 - process so one final note before we move
101:56 - on to another tool is this parameter
101:58 - here which is where both and basically
102:00 - when you set it to false all it does is
102:02 - it removes all this thought process here
102:05 - and it just gives you the answer
102:07 - and it's useful in a production setting
102:09 - but I think it's kind of fun to just
102:11 - look at vertex thinking and how it's
102:13 - able to come to those conclusions so
102:15 - that you know you can verify whether or
102:18 - not choosing be like
102:19 - so another tool that we're gonna use is
102:23 - human as a Duo this is really cool
102:25 - because
102:26 - say your agent serve goes on the wrong
102:30 - track
102:31 - what you would want to do is you want to
102:34 - validate it and sort of tell it what
102:37 - it's doing wrong so that it can fix that
102:40 - thought process and then move in the
102:41 - right direction instead of having to
102:43 - just run once and then you changing the
102:46 - parameters you can just talk to it and
102:48 - sort of make it work
102:49 - and this is what the human has a tool uh
102:53 - this is what the human has a dual
102:55 - package solves so this is just a simple
102:58 - you know scenario here uh you can look
103:01 - through the code but obviously the only
103:03 - difference here is that in the tools
103:05 - array I've just put humans and other
103:07 - than that too and I've just defined this
103:10 - method I'm here so that I can do the
103:11 - math so let's just run the human as a
103:14 - tool.pi
103:18 - and I'll tell you exactly learning again
103:20 - I hope a night it'll open the attitude
103:23 - I'll remember to add it to the other
103:26 - ones
103:28 - so
103:30 - open aiti key and let's try running this
103:33 - again
103:38 - Okay cool so it's working
103:41 - uh so it says I to figure out what the
103:44 - math problem is and solve it and the
103:46 - reason why is because I wrote the
103:48 - question is what is my math problem and
103:50 - basically what all I can do is I can
103:52 - just say
103:53 - if I have three apples and we will
104:00 - how many more B filter
104:04 - how Danny
104:06 - this is a very basic example but
104:07 - obviously uh it observes this and then
104:11 - it uses the llm map calculator too and
104:14 - it shows me that the answer is five now
104:16 - the answer is two
104:17 - so the reason why this is cool is
104:19 - because you can use this tool as I've
104:22 - mentioned before in between its thought
104:24 - processes so if it uh thinks of a wrong
104:27 - solution you can instantly give it
104:29 - feedback and that's where you can sort
104:31 - of just add this human to to your Cobra
104:34 - cluster tools here
104:37 - another tool that we can check out is
104:41 - our adapter tool
104:45 - so this is the python droplet tool and
104:48 - I'm I think you're sort of noticing a
104:50 - trend here where
104:52 - you don't really need a lot of code in
104:54 - order to deploy all these very complex
104:57 - uh agents because blackshin has got this
105:01 - covered for us already but if you want
105:04 - more customization with your tools we're
105:07 - gonna jump into that a little
105:10 - in a little bit so let's just start
105:13 - running this now and I'll explain what
105:15 - this does
105:20 - you can think of it as an alternative to
105:24 - collab notebook
105:26 - so basically what it does is it writes
105:29 - python code and the limitation with chat
105:32 - GPT is that it can't run its own port
105:34 - right because it's not a computer so it
105:37 - takes that code it puts it into Rebel
105:40 - and it retrieves the output of that code
105:43 - back to us so that you know it codes for
105:46 - us
105:47 - so here my question was what is the 10th
105:50 - Fibonacci number and
105:53 - here's you know a list of Fibonacci
105:55 - numbers
105:56 - um you know it identifies to the action
105:58 - Spike on Rebel and the actual input is
106:01 - the alphabenacci so this is the code
106:03 - that goes into their apple and the
106:05 - observation
106:07 - is as such so the thought is that you
106:10 - know it just prints out Fibonacci
106:13 - Okay so
106:15 - that was the python replied to obviously
106:18 - all you have to do is just combine this
106:21 - with uh every tool that I've mentioned
106:24 - before in order to make it work so
106:27 - another cool tool that we can explore is
106:30 - the YouTube search tool
106:32 - so YouTube search tool it obviously you
106:35 - know it does what it says it does it's
106:37 - just YouTube and in order to make this
106:40 - work you actually need to install the
106:42 - YouTube search package so let's just
106:45 - install that
106:46 - and the cool thing about YouTube search
106:48 - too is that you can uh sorry give it
106:52 - this framework here so this is another
106:54 - way that you define a tool where we
106:56 - Define an array of tool objects a tool
106:59 - object is basically this uh you know
107:03 - object that takes in three parameters so
107:06 - in this case it takes a name takes a
107:09 - function and it takes a description the
107:11 - name can be anything I don't think it
107:12 - matters that much the function is what
107:16 - um what the activity that the language
107:19 - model actually does when it's called
107:22 - and the description is super important
107:24 - because it tells it when to actually use
107:26 - it so in this case when you're asking
107:29 - for YouTube video links
107:31 - you'll want to call this Duo so that's
107:33 - why our description is you need to give
107:35 - it links to YouTube videos and we wanted
107:38 - to put https and all this stuff in front
107:42 - of every link to complete them because
107:43 - it only gives us you know the URL
107:45 - address like whatever it would be it
107:48 - doesn't give us the YouTube like so it's
107:50 - not more convenient to click on the
107:51 - links when you have that so again
107:54 - initialize the agent with all the tools
107:57 - of this stuff and I'm just gonna ask it
108:00 - a simple question here which is what's a
108:03 - Joe Rogan video on an interesting topic
108:06 - so let's let's just try to run this have
108:08 - I put my opening I can no then second
108:19 - okay so we have our opening API key and
108:22 - let's just share it to another list now
108:30 - so as I mentioned before you know this
108:32 - it just Returns the root value in our
108:35 - observation so it just returns to slash
108:37 - watch and what I asked it to do here is
108:41 - to put https youtube.com in front of
108:43 - every link so that it's actually
108:44 - clickable so our final answer is this so
108:47 - let's just click on it and see what it
108:49 - gives us
108:52 - experience I guess I'm talking about it
108:55 - I must
108:57 - so yeah it works it returns to us the
109:00 - YouTube link and I've shown you how to
109:02 - do this already but all you have to do
109:04 - to integrate this into chain lit is just
109:07 - copy paste all of this and replace this
109:11 - this stuff here with uh you know all
109:14 - this archive stuff that's all you have
109:16 - to do it's super simple
109:18 - let's go into more of the integration so
109:21 - I can show you what you can do to
109:25 - what you can do to research and explore
109:28 - more tools there is the
109:31 - there's the zapier land natural language
109:34 - actions API tool that I recommend you
109:36 - guys to explore on your own
109:38 - I'm not going to be doing this one
109:40 - because it's not the safest to show my
109:42 - API keys
109:44 - for so zapier is basically the service
109:47 - that gives you over 5 000 different
109:50 - Integrations I'll be covering the
109:53 - national language actions API in a
109:55 - future video but you know you can sort
109:58 - of combine this with maybe a research uh
110:01 - you know the archive research retrieval
110:04 - by making a bot that every you know
110:06 - single week it just Returns the latest
110:09 - research on a specific topic and it can
110:12 - like tweet it out put it as part of your
110:14 - email newsletter zapier has all those
110:17 - Integrations handle
110:19 - the last tool we're gonna end on is
110:21 - going to be the shelter
110:25 - and once we're done with the shell tool
110:27 - I'll just show you guys how to make your
110:29 - own custom tools
110:31 - so let's go to
110:33 - the shop tool code is in the CLI GPD
110:37 - dot Pi file and this is sort of
110:41 - interesting because you're doing a
110:43 - little bit of configuration here so
110:45 - you're taking the shell tools
110:47 - description
110:48 - you're adding its arguments and you're
110:49 - pasting these arguments with these so
110:52 - that it's easier for the model to
110:54 - execute the
110:56 - local commands so what the shell tool
110:59 - does is basically what we're doing in
111:01 - this terminal except chat gbd can
111:03 - actually run commands in our terminal
111:05 - you know create text files and all of
111:08 - that so
111:09 - here's a basic example obviously you
111:12 - know we'll just initializing the agent
111:13 - or adding all of them it should be a lot
111:16 - more familiar when I just
111:18 - do that out there so once we do that uh
111:23 - if we can just run the agent and my
111:25 - question to the agent is create a text
111:28 - file called empty and insert it add code
111:30 - that trains a basic CNN for for ebooks
111:33 - so
111:34 - as you can see there's no there is an
111:36 - empty.txt file from uh previous
111:40 - test run of this so I'll just delete
111:43 - that and
111:45 - let's run the code again so see that GPD
111:49 - on that
111:55 - um
111:57 - so let's let's do that
112:01 - and maybe buttercups here
112:09 - so okay this one would because it's
112:11 - before 8 or so I'll just put this okay
112:14 - so let's let's run this again
112:18 - okay so let's enter in the execute to
112:21 - chin and this is interesting because uh
112:24 - we can
112:26 - actually look at what exact commands
112:28 - that it's passing through so that this
112:32 - is sort of a step-by-step step
112:34 - sequential reasoning problem where first
112:36 - it it uses one two which is the terminal
112:40 - 2 and and this is the commander passes
112:44 - in so touch D dot dxt
112:46 - and then once the text file has been
112:48 - created it
112:50 - uh puts all of this text into empty.txt
112:54 - and then you know it's thought step
112:57 - and then it just opens until here is all
113:00 - the code we're going for running that so
113:02 - let's just check yeah this is actually
113:04 - worked so empty.dxt is here and yeah so
113:09 - it does have all the code that's
113:10 - required
113:13 - I'm just gonna end uh this tool section
113:16 - by showing you guys how easy it is to
113:19 - yeah again put all of these agent tools
113:24 - inside shame with
113:25 - so everything I'm doing here is just you
113:28 - know all this stuff is boilerplate and
113:30 - all I'm doing is just copy pasting what
113:33 - I'd put in CLI gbd all this code here
113:36 - and I'm just putting that in here and
113:40 - then I put the agent dot running here
113:42 - and store it in this variable so that's
113:45 - that's let's just uh for once and Define
113:49 - a API key
113:50 - and then run it in chain so
113:54 - chain that turn on
113:56 - on so I've named this file called the
114:00 - nation output
114:03 - dot pi
114:05 - and go for the Adobe effect
114:13 - so but yeah we can basically do
114:16 - everything that we did uh previously in
114:19 - the previous python notebook but just
114:21 - Enchanted so we can say what is are not
114:24 - bought as well uh to know installed
114:31 - I will
114:34 - install numpy
114:36 - package
114:38 - in my environment
114:43 - so as you can see it's okay through and
114:45 - see what it can do so it knows that the
114:48 - action is installed numpy so it just
114:51 - does that and I wanted to show this to
114:53 - you guys in specific
114:55 - because this creates a really cool use
114:59 - case where you can
115:01 - um you can sort of automate the process
115:03 - of learning how to record and uh
115:06 - speeding up this process by using this
115:09 - so say a beginner doesn't really know
115:11 - how to you know install packages or
115:13 - configure their environment you can just
115:15 - use this terminal tool and launch it as
115:18 - an application that helps them build
115:20 - that I think um
115:23 - when it comes out GitHub co-pilot will
115:25 - be uh or should be using this tool to
115:29 - make sure that it's very easy to manage
115:31 - dependencies and just make the llm
115:34 - handle all the point first word so that
115:36 - you can focus on actually coding
115:39 - so that's it for the CLI tool now let's
115:43 - look into
115:44 - word is pretty exciting which is our
115:47 - custom tools
115:52 - so I'm just going to give you guys a
115:54 - super basic example of a custom tool but
115:56 - know that you know it's pretty much
115:58 - Limitless what you can do with this so
116:00 - let's just uh stop running this code
116:03 - and let's look into custom tools
116:08 - so one important thing here is
116:12 - you want to enable Lang chain tracing to
116:14 - true so that you can actually Trace
116:16 - through your function so that it knows
116:18 - uh what to execute
116:20 - so in this case again uh same Imports
116:24 - same llm initialization
116:27 - and this is where you get customization
116:29 - so now in your tools array you can
116:31 - actually Define your own custom tool
116:35 - so in this case in the Dual object you
116:38 - can pass in a name a function and a
116:41 - description
116:42 - our name can be anything you want and
116:45 - our function is the parsing multiplier
116:47 - so this is just a simple function that
116:49 - multiplies two things together
116:52 - not supposed to multiply yeah so it
116:55 - multiplies two things together so and a
116:58 - and b so when you pass in the input
117:00 - three times four
117:02 - a general calculator would not be able
117:05 - to answer this because a it's a natural
117:07 - language and B it requires like a little
117:10 - bit of intelligence because uh you know
117:12 - this is a number and this is a series of
117:15 - strings so
117:17 - this is our multiplier function and then
117:19 - parsing multiplier basically takes in
117:21 - these two
117:23 - takes in a string uh that the llm inputs
117:27 - into the parsing multiplier and just
117:30 - splits it based on the comma and just
117:31 - extracts A and B so that it can multiply
117:33 - so an example of this is basically the
117:37 - llm uh I'll just explain what that or
117:41 - what I just said means so the llm would
117:43 - look at these functions and then it
117:46 - would out it would you know act as an
117:50 - input in the function it would probably
117:52 - do like three comma four as its input to
117:56 - the function so this C comma 4 as a
117:59 - string
118:00 - so this is a string three comma four
118:03 - gets passed into our parsing multiplier
118:06 - where it's split into a and b so 3 goes
118:11 - to a and 4 goes to p and then you would
118:13 - return
118:14 - the multiplier of A and B so since so
118:17 - you would just convert them into
118:19 - integers so you just return three times
118:21 - four
118:23 - so this is just one Custom Tool that you
118:26 - can do obviously you can do a lot more
118:28 - complex things you can call apis on the
118:31 - back end you know you can hook it to
118:34 - email you can do a bunch of stuff with
118:36 - this so let's just run this llm chain
118:39 - now
118:40 - and see what it shows us custom tools
118:50 - fail to the lower default bucket so it
118:52 - works
118:54 - I'm not quite sure why uh this would air
118:57 - out but it it does show us uh the actual
119:00 - uh agent chain so here it says I need to
119:03 - multiply two numbers together because
119:05 - that's what it's doing and the action is
119:08 - multiplier and and input is three comma
119:12 - four so the observation would be 12
119:15 - because your action input 3 comma four
119:18 - gets passed into our parsing multiplier
119:20 - which is our function here parsing
119:21 - multiplier splits it into a and b
119:23 - Returns the multiplied output and so the
119:27 - observation is 12 because the output of
119:29 - our function is 12 and then it just says
119:32 - you know I know the final answer
119:34 - final answer is 3 times 4 x 12. so you
119:38 - can change this function to maybe
119:41 - dividing
119:42 - obviously it's not the most creative
119:45 - change
119:46 - but you know when you can Define your
119:49 - own functions and allow this allow them
119:51 - to do things the possibilities are
119:53 - pretty much Limitless
119:55 - uh interesting thing I just saw here is
119:57 - that I did Define this as division but
120:00 - you know it's smarter than that because
120:02 - I think it focuses more on the goal than
120:05 - the actual means to get to the goal so
120:08 - it
120:10 - um you know even though the observation
120:11 - is 0.75 it prioritizes its own like
120:15 - common sense and knowledge maybe through
120:18 - our Ledger techniques uh to make sure
120:20 - that the silencer is not dry so it's 12.
120:24 - so I'm just gonna leave you guys with
120:26 - this this was the end of custom tools
120:29 - and the last segment of this course
120:31 - I'll just leave you guys with maybe one
120:33 - thing to think about which is
120:36 - the how the agents actually work right
120:39 - now
120:40 - is to zero shot react description so you
120:43 - can think of this as basically a pretty
120:46 - elaborate prompt combined with the
120:48 - recursive algorithm so what I would
120:51 - leave you to is instead of instructing
120:53 - these values uh do you think they'd be a
120:57 - way to sort of fine tune the llm based
121:00 - on the output of multiple agents because
121:04 - uh fine tuning is meant for Behavior
121:06 - changes so could you just reinforce that
121:09 - logical thinking Behavior inside an llm
121:12 - itself so I won't have depend on outside
121:15 - react Frameworks in order to work
121:19 - if you've made it this far then
121:21 - congratulations because you just
121:23 - finished the course here's a list of
121:25 - everything that we covered so far
121:32 - this course has been brought to you by
121:33 - loop.ai which is software that allows
121:37 - you to create no code AI flows
121:39 - in 10 minutes you'll be able to perform
121:42 - all the actions that I've covered in
121:45 - this course so far add a lot more with
121:47 - no code
121:49 - as well as in within 10 minutes or less
121:52 - you can deploy and embed a custom AI
121:54 - flow on any website and deploy an embed
121:58 - a custom chat bot on any website from
122:01 - Twitter to Instagram to your own
122:04 - personal website to Shopify
122:06 - I appreciate you guys taking the time to
122:08 - learn from this course if you're excited
122:10 - about the product head on to loop.ai and
122:13 - sign up for Early Access subscribe to
122:16 - our YouTube channel for highlights of
122:18 - the course uploads per section
122:21 - announcements and updates on the product
122:24 - the YouTube channel is also going to be
122:26 - the place where we're going to post any
122:28 - of our future courses
122:30 - follow us on Twitter to be the first to
122:32 - be notified about all of our future
122:34 - events and courses
122:36 - and if you have any doubts queries
122:38 - concerns regarding any part of this
122:40 - course join the Discord for assistance
122:42 - where me and my team are going to be
122:44 - working every day to resolve these
122:46 - doubts and conflicts
122:48 - once again I appreciate you guys taking
122:50 - the time to take this course thank you