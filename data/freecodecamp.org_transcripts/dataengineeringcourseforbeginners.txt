00:00 - welcome to this data engineering crash
00:01 - course created by Justin Chow this
00:04 - course covers the essentials of data
00:06 - engineering you'll learn about databases
00:08 - Docker and analytical engineering you'll
00:11 - explore Advanced topics like data
00:13 - pipeline building with airflow and
00:15 - you'll engage in batch processing with
00:17 - spark and streaming data with Kafka the
00:20 - course culminates in a comprehensive
00:23 - project putting your skills to the test
00:25 - in creating a full end to-end pipeline
00:27 - hey everyone welcome to the course my
00:29 - name is Justin Chow I'm a developer
00:32 - Advocate at airbit I'm going to be going
00:33 - through kind of essentially what you
00:35 - need to know in order to become a data
00:37 - engineer what goes into Data engineering
00:39 - and then this is going to go into a
00:42 - fully fledged demo where you can follow
00:44 - me in creating a data pipeline from
00:46 - scratch and then evolving that into a
00:48 - fully fledged data pipeline using
00:50 - open-source tools such as air bites so
00:52 - hopefully you enjoyed this and this
00:54 - gathers some interest if anything along
00:56 - with data engineering or software
00:57 - engineering in general is of interest to
00:59 - you then this should be good good to
01:00 - know but without further Ado let's get
01:02 - into the course so we're looking over
01:04 - the introduction to data engineering but
01:06 - you may be thinking why data Engineering
01:09 - in the first place well there's a couple
01:10 - reasons why you may want to consider
01:12 - getting into Data engineering first
01:13 - there's a high failure rate of Big Data
01:15 - projects and the percentage there based
01:17 - off of the data that we've collected is
01:19 - 85 to 87% of those projects failing and
01:23 - obviously as I've said that's pretty
01:25 - pretty high those failures are due to
01:26 - unreliable data infrastructures and
01:28 - quality data in the past there's been an
01:31 - expectations for data scientists to
01:33 - build out that data infrastructure
01:35 - that's necessary for those projects with
01:38 - the infrastructure that was put in place
01:40 - there's always incorrect data modeling
01:42 - and redundant work that they have to do
01:44 - because of the incorrect data modeling
01:45 - and the Redundant work that they have to
01:46 - do there's a high rate of data scientist
01:48 - turnover so there's a high need of data
01:51 - Engineers to process that data and
01:53 - extract value instead of the data
01:55 - scientists with all of that being said
01:57 - there is a growing importance of the
01:59 - data engineering role as we speak if
02:02 - we're talking compensation right now in
02:04 - the US the average median salary for
02:06 - data Engineers is 90 to 150k a year and
02:10 - obviously I say in the US it still
02:12 - highly depends on the location and
02:14 - whether you're in the states or not so
02:16 - that number will change now let's look
02:18 - at the business potential and impact
02:20 - that data Engineers can have in any
02:21 - company you will have a crucial role in
02:23 - the success of making data driven
02:25 - decisions in the AI and ml space for
02:28 - applications espec especially with the
02:30 - rise of chat GPT right now and the AI
02:32 - space in general data Engineers are
02:34 - going to have that crucial role in
02:36 - determining what happens with those
02:38 - applications and the data in general now
02:39 - let's look at the business potential
02:40 - that you have as data engineers and the
02:42 - impact you can make in any company that
02:44 - you work for the first one being is that
02:46 - you're going to have a crucial role in
02:47 - the success of making data driven
02:49 - decisions in the AI and ml space for
02:51 - applications and so with the rise of
02:53 - chat GPT and the AI space you're going
02:56 - to be a crucial factor in that aspect
02:59 - and for any company that wants to and
03:01 - should be getting into AI in general the
03:03 - next thing you're going to be doing is
03:04 - ensuring that there's efficient
03:05 - management and processing of the actual
03:08 - data as it moves Downstream while it
03:10 - moves Downstream you're also ensuring
03:12 - that the data quality the security and
03:14 - availability of it to those teams
03:16 - Downstream such as the data scientists
03:19 - the data analysts or any other team that
03:21 - has to work with data in your company
03:24 - that it's quality it's secure it's
03:26 - formatted the way that they need it and
03:29 - it's never not flowing Downstream this
03:31 - will also lead to better insights and
03:33 - outcomes for your company and overall
03:35 - just drive Innovation and
03:36 - competitiveness so that your company can
03:39 - Thrive and that your team can Thrive so
03:41 - now we're moving on into the first
03:43 - module which is the introduction to
03:45 - Docker and what we use Docker for in
03:47 - data engineering but first we have to
03:49 - have a better understanding of what it
03:51 - is if you haven't used it before Docker
03:52 - is an open source platform that
03:54 - simplifies the process of building
03:56 - shipping and running any applications
03:58 - inside containers so imagine IM you are
04:00 - working on a project and you want your
04:02 - co-workers to try and use it to see if
04:05 - there's any issues but they can't run it
04:07 - for some reason and you kind of run into
04:10 - that good old saying of hey it runs okay
04:12 - on my machine well what Docker is able
04:14 - to do is you're able to put your
04:17 - environment into a container and that
04:19 - container will include every dependency
04:23 - and kind of Silo everything into again
04:25 - this container which you can then pass
04:27 - on to your co-workers or even run up in
04:30 - the cloud to have your application
04:32 - running as it was when you were
04:33 - developing it so it essentially just
04:36 - helps organize and make sure everything
04:38 - works as developed so if we're going
04:41 - over containers and their benefits
04:43 - containers are lightweight portable and
04:45 - self-sufficient environments that
04:46 - package an application along with all
04:48 - its dependencies libraries and
04:49 - configurations as I mentioned before
04:52 - again just going over how much Docker
04:54 - has really really helped engineers in
04:57 - General on packaging their their
04:59 - applications to make sure that they
05:01 - don't run into any issues down the line
05:02 - when testing and developing and to make
05:05 - sure that those environments are
05:06 - reproducible in case anything goes wrong
05:08 - and making sure that also when they're
05:10 - pushing up into the cloud or into
05:11 - production that those applications are
05:13 - working as intended and when they were
05:15 - testing it's going to work the same way
05:17 - in prod so now just imagine that all
05:18 - your apps code and all your apps
05:20 - dependencies are landing into one of
05:22 - these containers and you're shipping one
05:24 - of those off out into you know your
05:26 - co-worker place or on AWS or gcp
05:29 - everything that was inside that
05:30 - container will be seen on the other end
05:32 - as well now as I mentioned before in
05:33 - software development we're looking at
05:35 - Docker in the form of testing deployment
05:38 - for its ease of use speed and
05:40 - flexibility it assists with the
05:41 - deployment scaling and management of
05:43 - those applications using what we call
05:45 - containerization which is just a
05:47 - lightweight form of virtualization so if
05:49 - you look on the right hand side's
05:50 - illustration here you can see that we
05:52 - have a couple apps that are in
05:54 - containers using Docker uh as well as
05:57 - you know sitting on top of your oper in
05:59 - system and the infrastructure and it's
06:01 - all running all within the docker
06:04 - ecosystem now there are three main
06:05 - concepts with Docker that we need to
06:07 - understand the first one is going to be
06:08 - Docker files the second one is going to
06:10 - be Docker images and the third one is
06:12 - going to be Docker containers now let's
06:14 - go over what those three are right now
06:16 - the first step is the docker file the
06:18 - docker file is a text file that consists
06:20 - of all the instructions needed hence
06:22 - blueprints that we want Docker to use
06:25 - all in order to create the docker image
06:27 - so the docker image is what is created
06:29 - from the docker file hence the blueprint
06:32 - so the blueprint then creates our Docker
06:33 - image which is the lightweight
06:35 - Standalone executable package that
06:37 - includes everything we need to run your
06:39 - piece of software that includes code the
06:41 - runtime libraries environment variables
06:44 - and config files the one thing to note
06:47 - is that images are readon and immutable
06:49 - so nothing can change within the docker
06:51 - image itself if you wanted to create
06:53 - another Docker image or modify anything
06:55 - that exists you want to modify your
06:57 - Docker file and the instructions that
06:59 - are inside that text file to create a
07:01 - new Docker image so that's why we stick
07:03 - the image into a Docker container which
07:06 - is the runtime instance of our Docker
07:08 - image or in other words the running
07:11 - version of the image is the docker
07:13 - container this Docker container is
07:14 - completely isolated from the host and
07:16 - other containers that you may have
07:18 - running it has its own file system and
07:20 - can be started stopped and deleted
07:23 - independently so for example we may have
07:25 - one Docker image that contains our
07:27 - application but we have three different
07:29 - Docker containers running that same
07:32 - image so we have three different
07:34 - environments three different instances
07:36 - of that image but I can stop one and the
07:38 - other two are still going to run I can
07:40 - delete one and nothing changes in those
07:42 - other containers because they are
07:44 - isolated environments and you can run
07:47 - tests you can do anything you want in
07:48 - those containers but nothing will change
07:51 - unless it is done inside the container
07:53 - so first things first to get started
07:55 - with Docker is obviously we're going to
07:57 - want to install Docker so head over to
08:00 - doer. and here is the landing page of
08:03 - Docker now what we want to do is simply
08:04 - just download Docker for whatever
08:06 - operating system you have I am currently
08:09 - running on a mac Apple chip so I would
08:11 - click this and download Docker desktop
08:13 - which also comes with Docker compose
08:15 - which we will also need for later along
08:18 - in this course but just know that Docker
08:21 - compose and Docker desktop are two
08:23 - prerequisites to running with Docker
08:25 - right now now if you're on Windows
08:27 - obviously follow the instructions for
08:28 - Windows if you're on Linux follow the
08:30 - instructions for Linux but go ahead and
08:32 - download Docker for your machine and if
08:34 - you have any questions uh you can also
08:36 - go over to Docker and they have a really
08:39 - good explanation in their docs on how to
08:42 - install Docker desktop for your machine
08:44 - and get going with that so make sure you
08:46 - do this first now that we have Docker
08:48 - installed I think the docker docks have
08:51 - a great getting started guide on how you
08:53 - can actually containerize an application
08:56 - and so what we're going to do is
08:57 - actually just go through this really
08:58 - quick and it can give us a great idea of
09:01 - how to containerize an application
09:02 - through a Docker file how we build the
09:05 - image getting the image to run inside of
09:06 - a container and then a couple other
09:08 - things like networking and things like
09:10 - that so let's go ahead and go through
09:11 - the getting started guide and then we'll
09:12 - move on into the SQL module right after
09:15 - so if you were still confused on what a
09:17 - container is or what an image is it
09:19 - gives you a good rundown here at the
09:21 - beginning of part one but let's go into
09:23 - part two on how we can containerize an
09:24 - application first things first is they
09:26 - actually have a getting started app
09:27 - which is just a to-do app for you to
09:29 - containerize but what we're going to do
09:31 - first is simply just clone this and and
09:33 - use it for the rest of this guide I'm
09:35 - going to open up my terminal here and
09:36 - make a directory and I'm going to call
09:39 - this Docker tutorial now I'm going to CD
09:42 - into Docker
09:44 - tutorial and after that what we're going
09:46 - to do is paste in the command that was
09:47 - in there which simply just clones the
09:49 - repo and obviously if it wasn't clear
09:51 - enough and make sure you have a text
09:53 - editor installed I'm using vs code and I
09:56 - think that should be it obviously Docker
09:57 - installed now that it's Clon let's go
10:00 - ahead and CD into the getting started
10:02 - guide and what we can see here is if we
10:05 - open up vs code there's a couple things
10:06 - here so we have a spec file we only
10:08 - really need to look at the source file
10:10 - but this is just a simple to-do app so
10:12 - let's go ahead and go back to the
10:14 - getting started guide and it has the
10:17 - instructions for you on Mac and windows
10:19 - so depending on your operating system
10:21 - make sure you follow the right ones but
10:22 - we've already cded into it what we're
10:24 - going to do is inside of the route
10:27 - directory of the getting started app
10:28 - we're going going to create a Docker
10:31 - file and as mentioned before this is
10:32 - going to be the instructions on how we
10:34 - build the image that containerize the
10:36 - application that the docker file is
10:38 - sitting in so now that I've created the
10:39 - docker file if type in LS you can see
10:41 - that the docker file is there so what
10:44 - we're going to do is they've provided us
10:46 - the instructions or the contents that
10:49 - we're going to use for this Docker file
10:51 - now you can see that we have a couple
10:53 - things here that we haven't seen before
10:54 - but essentially what from is is we're
10:56 - going to use the node 18 package that
10:59 - we're pulling from the docker Hub to run
11:01 - our environment in or our run time the
11:03 - working directory is going to be in the
11:05 - SL app or the root directory of the
11:08 - application this simply copies
11:10 - everything and then we're going to run
11:12 - this command which is yarn install
11:14 - production other commands that are going
11:15 - to be is going to be node and we're
11:18 - going to look at in the source index.js
11:20 - file and we're also exposing Port 3000
11:23 - so you can see that these instructions
11:25 - are pretty straightforward read like
11:26 - English the one thing to take in mind is
11:28 - that this does does have to be in order
11:30 - this cannot be misplaced so keep that in
11:32 - mind when you're writing your own Docker
11:34 - files but what we're going to do is
11:35 - we're simply just going to copy this
11:37 - switch back over to our vs code head
11:40 - into Docker file and then just paste
11:42 - this in and save now that that's saved
11:44 - they give us a command so we're going to
11:46 - run Docker build hyphen T which simply
11:48 - just tags our image and then we're going
11:50 - to name it getting started so that will
11:51 - be the name of our image and then the
11:53 - dot we need at the end to tell Docker
11:55 - where to look for everything to
11:57 - containerize which will simply just be
11:59 - in the current directory so I'm going to
12:00 - go ahead and copy this command as well
12:02 - paste this in also make sure that you
12:04 - have the docker Damon running if you
12:06 - install doer desktop just simply open
12:07 - the application let everything launch
12:09 - and then you should be good so I'm going
12:10 - to go ahead and hit enter this is going
12:12 - to do a couple things awesome so now if
12:15 - we go back into Docker desktop and we go
12:18 - into images you can see at the very top
12:20 - getting started is created we have an
12:22 - image the tag is going to be the latest
12:24 - tag hasn't been used yet tells us our
12:26 - size but now we have an image with
12:28 - everything as you can see from the
12:31 - docker file so you can see the commands
12:33 - that we ran we have node version 18
12:35 - which we did with the from command and
12:37 - then we're also exposing Port 3000 so
12:39 - you can see that the docker files
12:41 - blueprint is in play here next thing
12:43 - we're going to do is going to start this
12:44 - container so we can say Docker run which
12:47 - is going to run the image in a container
12:49 - we're going to flag it for DP which the
12:52 - D flag is short for detach which runs
12:55 - the container in the background the P
12:56 - flag is short for publish which means
12:58 - that we're creating the this port
12:59 - mapping so that we can actually access
13:01 - this container locally and with the
13:03 - mapping we're pointing it to a local
13:05 - host here so 12.0.0 point1 and we're
13:08 - opening it on Port 3000 now keep in mind
13:11 - this can change say you have an
13:13 - application already running on Port
13:14 - 3,000 but you wanted to run this on Port
13:17 - 6000 you can definitely change this but
13:19 - this number after the colon needs to
13:21 - stay at 3,000 because in our Docker file
13:24 - we exposed Port 3000 so to be able to
13:27 - access this we need to still keep that
13:29 - number to 3,000 and then after this this
13:31 - is the name of our container so
13:33 - obviously we're going to name it getting
13:34 - started so let me copy and paste this go
13:37 - back in our terminal and as you can see
13:38 - it spits out a container ID uh I'm going
13:41 - to go ah and copy this for later but if
13:43 - we go back into Docker desktop now just
13:45 - for a simpler gooey to see if we go to
13:47 - the containers tab you can see that a
13:48 - container was created with the image
13:50 - getting started and it is currently
13:52 - running on Port 3000 so now if we're
13:55 - going to go back here and we want to go
13:57 - to Local Host Port 3000 you can see that
13:59 - the app is now running locally and we're
14:02 - accessing this container so that is good
14:04 - now we can move on well first of all
14:07 - what we may want to do is I can show you
14:09 - guys that you can actually write stuff
14:11 - to this so as I mentioned before this is
14:13 - a to-do app so I can say get cat food
14:15 - take out the trash very simple react app
14:18 - going on right now that is inside of a
14:20 - container so as I mentioned previously
14:22 - before we started this demo is that
14:24 - Docker contains a runtime it contains
14:27 - all the environment variables and it
14:29 - contains every single dependency that
14:31 - react or this application in general
14:33 - needs in order to run and so we're
14:35 - siloed inside of a container with a
14:38 - runtime with this environment and we're
14:40 - able to actually use the application
14:42 - within the container there's a couple
14:44 - other commands that tells us so in the
14:45 - command line we can run Docker PS and it
14:47 - will name any other containers that are
14:49 - currently running along with their
14:50 - container ID so as I mentioned this
14:52 - container ID popped up but if I type in
14:54 - Docker PS it's going to be the only one
14:56 - running but if there were any other
14:57 - containers running we can pull their
14:59 - container ID along with their names and
15:01 - other information the other thing to
15:03 - note too is that container IDs can be
15:05 - accessed you don't need the full
15:07 - container ID you only need the first
15:09 - couple digits here and you can still
15:11 - access that container so let's move on
15:12 - to part three here now we were going to
15:14 - want to make some changes to this code
15:16 - and this is going to show us how
15:17 - updating the application itself changes
15:19 - things so they want us to change a
15:22 - paragraph inside of the SRC static JS
15:26 - app.js file so let's head back into into
15:29 - vs code so we're in SRC we're in static
15:32 - we're going to go to
15:33 - index.js JS app.js sorry and then go to
15:36 - 56 and then we're simply going to change
15:38 - this text to be what they told us we're
15:40 - going to go ahead and save that but now
15:42 - that we've updated the application that
15:43 - means we actually have to build another
15:45 - version of this image because the image
15:47 - we built cannot be Rewritten it is
15:50 - completely readon and so if we're going
15:52 - to make any changes we have to create
15:54 - another image and so that's exactly what
15:56 - we're going to do we're going to run the
15:57 - same Docker build command in the
16:00 - terminal and that's going to build
16:01 - another image and so now what we're
16:03 - going to do is we're going to run
16:05 - container command but we're going to get
16:06 - an error here because we already have a
16:09 - container with that name that's binded
16:10 - to Port 3000 now if we want to remove
16:13 - the old container that this will bring
16:15 - us back into Docker PS so we're going to
16:16 - run Docker PS and so we can see that
16:18 - this container is still running so what
16:20 - we're going to do is we're going to say
16:22 - Docker RM paste the container ID and
16:25 - that should give me another error saying
16:27 - you cannot remove a running container so
16:28 - we going to say Docker RM and put in the
16:31 - F flag that will stop the container as
16:33 - well as remove the container so I'm
16:36 - going to do that and that should now get
16:38 - rid of it and now we can see that this
16:40 - has stopped and we can copy and paste
16:42 - this again and now we've created another
16:45 - container and it has started running
16:47 - with the updated image now if we go back
16:50 - into the to-do app and refresh this you
16:53 - can see that it's completely fresh and
16:55 - the new text has been added so that's
16:58 - what happen happens when you go and
16:59 - update an application we're going to
17:00 - skip part four but if you wanted to push
17:03 - this image up into the docker hub for
17:05 - other people to access you can
17:07 - absolutely do that we're not doing that
17:09 - right now so at your own you can go
17:11 - through part four but we're going to go
17:12 - to part five and this is actually a big
17:15 - one because sometimes we're messing with
17:17 - data and as we saw in the to-do app we
17:20 - have data and we want to persist it but
17:22 - if we were to stop the container and
17:24 - open it the data that we had previously
17:26 - is going to be gone so the way we can
17:28 - actually persist data is by simply
17:31 - adding a couple things so we're going to
17:32 - go through the steps one more time but
17:36 - we're going to run this command here
17:37 - we're simply going to start a container
17:39 - that has a buntu running and it's going
17:41 - to contain a file called data. text that
17:43 - will have a random number between 1 and
17:45 - 10,000 so let's go open up my terminal
17:48 - and we're going to run Docker through
17:50 - that and as you can see it created
17:52 - another container has an image of auntu
17:54 - I already had it installed locally but
17:56 - if you do not have the auntu image it
17:59 - will immediately detect that and
18:01 - download that from the docker Hub once
18:02 - we have that going we're going to copy
18:04 - and paste this and this will simply
18:06 - check the file that is inside the
18:08 - container which is data. text and it
18:11 - should spit out the name of the or
18:13 - should not spit out the name but it
18:14 - should spit out the number and what we
18:16 - need to do is we need to remove the
18:19 - current container ID or the placeholder
18:22 - there with the actual container ID and
18:24 - there we go we have 848 that's spit out
18:26 - to us so we know that there is data
18:28 - inside they want us to start another
18:30 - container that has a buntu so we're
18:32 - going to go ahead and do that as well
18:34 - and it'll list in every single folder
18:37 - that is currently in the root of that
18:39 - container and then what we're going to
18:41 - do also is we're going to remove that
18:42 - first container so again we're going to
18:44 - go Docker
18:46 - RMF and then the first container should
18:50 - be this one right here so we went ahead
18:52 - and remove that so the way to process
18:54 - data in that container is going to be by
18:55 - creating a volume and it's pretty simple
18:58 - to do so the first thing we're going to
18:59 - do is run this Command right here that
19:01 - they give us essentially we're going to
19:03 - be creating a volume called Todo hyen DB
19:07 - so let me go ahead and open my terminal
19:08 - here and what we're going to see in the
19:11 - volumes tab now is a to do hyphen DB
19:15 - what we're going to have to do is remove
19:17 - the container that we created because it
19:19 - is running without a persistent volume
19:22 - because we created the volume after
19:23 - container creation so we're going to
19:25 - have to create a fresh one so again let
19:27 - me grab grab the container ID of the
19:31 - getting started container and same thing
19:33 - Docker RM hyphen f and with the
19:36 - container ID and that should remove it
19:38 - for other purposes let me also just
19:40 - remove this as well cool so we have a
19:43 - clean slate now we're simply going to
19:45 - copy this command but there's a couple
19:47 - things here so along with the run and
19:49 - run command the dnp flags as well as
19:53 - mounting this or not mounting this but
19:55 - binding it to the port 3000 we're also
19:57 - mounting the volume here which is going
19:59 - to be the flag here that we see or the
20:01 - option and we're specifying the volume
20:03 - that we want to mount so the type is
20:04 - going to be volume the source of the
20:06 - volume is going to be the to-do hyphen
20:08 - DB volume we just created and the target
20:11 - is going to be where the actual data
20:12 - sits inside of the container so that is
20:15 - the path that we're going to do and then
20:17 - getting started we're actually just
20:18 - specifying the image that we what we're
20:20 - using or the container so let me go
20:22 - ahead and copy this command paste it in
20:25 - and now you can see that we have another
20:26 - container with getting started and it
20:28 - will be utilizing the 2db volume that
20:31 - we've created so now what we can do
20:34 - since it's running we're going to go
20:35 - back here to the to-do app and as you
20:37 - can see I have some data that was in
20:39 - there and I didn't add anything you saw
20:41 - that it was empty and there's two tasks
20:43 - in here that I added previously about I
20:45 - mean yesterday and they persisted so now
20:47 - we're able to tap into the volume and
20:49 - the data and anything that we do from
20:51 - here on out let's just say I I don't
20:54 - know go to the gym we want to add that
20:57 - if we stop this container and then start
20:59 - it again and refresh this we can see
21:01 - that the data is persisting now so
21:03 - volume creation is going to be the way
21:05 - to persist databases especially if
21:07 - you're testing stuff this is a good way
21:09 - to do that within Docker and it's easy
21:11 - to play around with servers and actual
21:13 - databases now that we have data that
21:15 - persists instead of having to reload and
21:17 - add more data over and over and over
21:19 - again so there's a couple other things
21:20 - that we can see but if you were to run
21:22 - Docker volume inspect on the volume
21:24 - itself you can see a couple things of
21:26 - metadata here but specifically we get to
21:28 - see the mount point the name the scope
21:30 - of it and it's created time so those are
21:32 - a cool couple things if you want to
21:34 - persist databases within Docker next
21:37 - thing we're going to do is we're going
21:38 - to skip over to part seven I don't
21:39 - really want to touch into the used bind
21:40 - mounts right now but multicontainer apps
21:43 - so as I mentioned we may have a couple
21:45 - things that we're running and so
21:46 - multiple containers are going to be
21:48 - needed in this case we have our to-do
21:50 - app but we also want to create a MySQL
21:52 - container that runs in tandem with the
21:55 - to-do app so that we can isolate our
21:57 - database and our front end app here in
21:59 - order for these two containers to talk
22:01 - we're going to need to create a network
22:02 - and that's exactly what we're going to
22:04 - do right now so we're going to run this
22:06 - command Docker Network create and the
22:08 - Network's name is going to be to-do app
22:10 - so let me go ahead and run that right
22:13 - now it's going to give me an error
22:15 - because I already created this but go
22:17 - ahead and run that and it will create a
22:18 - network there for you so now what we're
22:20 - doing is we're going to create a another
22:22 - container and then attach it to the
22:24 - network that we just created and so you
22:26 - can see that we have a couple Flags here
22:28 - first ones are going to be the network
22:30 - so we're tying the network of this
22:31 - container to the to-do app Network we
22:33 - created the Alias of this is going to be
22:35 - my SQL so when we see the container
22:37 - through the network we're going to know
22:38 - that this container here is the MySQL
22:40 - container some of the variables that we
22:42 - have here are going to be the volume
22:44 - that we want to create is going to be
22:46 - the to-do my SQL hyphen data volume so
22:49 - we're creating a volume there to persist
22:51 - the actual database and then we have two
22:52 - environment Flags specific to my SQL
22:55 - again if you're using something like
22:57 - maybe say postgress there's going to be
22:59 - some different environment variables you
23:00 - need to attach to this container two of
23:02 - the first ones being in my SQL are going
23:05 - to be the root password as well as the
23:06 - database name so we know that our
23:08 - password Here is secret and then the
23:09 - name of our database is going to be
23:11 - to-dos and we're going to be pulling the
23:12 - MySQL version 8 Docker image so what
23:15 - we're going to do is copy this over and
23:18 - we're going to go into our terminal and
23:19 - create that and now we can see we have a
23:22 - MySQL image again I've downloaded
23:23 - locally already and the container is now
23:25 - running to confirm that we have the
23:27 - database running we're going to run this
23:28 - command here so we're exing hyphen it
23:31 - and the container ID will go right after
23:33 - that then it will spit out the my SQL
23:35 - command enter in the user which is root
23:38 - and then ask us for the password so let
23:40 - me go ahead and copy this really quick
23:42 - in here and copy the container ID that
23:45 - it gave us and now replace the
23:47 - placeholder enter It'll ask us for the
23:49 - password and as we saw before it was
23:51 - secret so let me enter that in and now
23:53 - we are inside of our mySQL database the
23:56 - next thing it asks us to do is Type in
23:58 - show databases to confirm that our
24:01 - databases was created and we see the
24:03 - to-dos database is here so our commands
24:07 - ran successfully now we can say exit and
24:09 - now we have a to-dos database we're
24:11 - going to skip this part because it's not
24:13 - really necessary but if you want to
24:15 - follow it it simply just spins up a DNS
24:18 - tool that will allow you to see the IP
24:20 - address of the container and show you
24:23 - that each container has its own IP
24:25 - address so this networking step really
24:27 - is crucial for these two containers to
24:29 - talk to each other so what I'm going to
24:30 - do here is I'm actually going to first
24:32 - remove the container of the getting
24:34 - started app that we created first
24:36 - because it's not tied to this network
24:38 - yet so let me go ahead and remove it
24:40 - first and then we're going to create
24:41 - another one using this command here and
24:43 - as you can see we're doing the same run
24:45 - command here but with a couple other
24:47 - flags so there's just you know we're
24:49 - tying it to the network here there's a
24:50 - couple other environments like hooking
24:52 - it up to the mySQL database so that
24:53 - we're actually able to push data to the
24:55 - database same thing with the node
24:57 - version and then initializing the
24:59 - application with yarn install and and
25:01 - run Dev so let me go ahead and copy and
25:04 - paste this over and there we go we
25:06 - should have another container built with
25:08 - the note 18 Alpine image instead now if
25:10 - we want to connect to the mySQL database
25:12 - let's go ahead and do that here so we're
25:15 - going to copy this command again copy
25:18 - and paste this and then we need to get
25:19 - into the mySQL database so let me copy
25:21 - and paste
25:26 - that
25:28 - enter the password here and it is secret
25:31 - so now that we're in so we're going to
25:33 - do the select function here and we're
25:36 - going to select all from to doore items
25:41 - and we can see that the three items that
25:43 - we created inside of the application are
25:46 - now here inside of the database so we're
25:47 - confirming that the data that we created
25:49 - in the volume is now inside of our my
25:52 - SQL database and that right there is the
25:54 - power of networking earlier in this
25:56 - module I also mentioned dock compose and
25:58 - it makes this whole process that we just
26:01 - did with this application 10 times
26:03 - easier again as I mentioned before also
26:05 - is that Docker compose is a tool that
26:06 - helps you to find and share multiple
26:08 - container applications and so we're
26:11 - going to be doing that right now this
26:12 - right here was the old command that we
26:13 - had to run inside of our terminal to get
26:15 - everything running for the MySQL and to
26:18 - do app but we can simply do that all of
26:21 - this in one Docker file and the docker
26:23 - file now changes to a Docker composed.
26:25 - yo file or you can shorten it to os.
26:28 - yaml so we're going to head back into
26:29 - our terminal here and inside of the
26:31 - route where we initially put our Docker
26:33 - file we're going to create a new file
26:36 - and call it compose do yaml and what
26:39 - Docker has provided us with were a
26:40 - couple things so we're going to go ahead
26:41 - and copy and paste this now make sure it
26:43 - added to space here but it will air out
26:45 - if it has an empty space so cover that
26:47 - space again yaml is very strict with
26:50 - spacing and indentation so keep that in
26:52 - mind but the first thing here we're
26:53 - going to do is specify Services of our
26:56 - compos file the first one is going to be
26:58 - our app which is going to contain the
27:00 - image of the node version 18 so we're
27:03 - going to run node in this container next
27:05 - thing we're going to do is add the
27:06 - command so the command we want to run is
27:08 - going to be the yarn install and and run
27:10 - the dev environment there so we're going
27:12 - to add that here next thing is we're
27:14 - going to open up the ports and it's
27:16 - going to be the same port the 127 on
27:18 - Port 3000 so now we're moving the
27:21 - working directory to the volume mapping
27:23 - uh and by doing that we're going to add
27:25 - the working directory and then the
27:27 - volumes
27:28 - so let me go ahead and add that as well
27:30 - and then finally we're going to add the
27:31 - environments so these environment
27:33 - variables are all going to be the my SQL
27:36 - environment variables so that we can
27:38 - access the database through the
27:40 - application itself when we add a need
27:42 - todos so again the host the user
27:44 - password database name these are all
27:46 - there and are going to specified for
27:48 - this app container now we're going to do
27:51 - the same but for my SQL so we saw the
27:54 - app container but now if we back up a
27:56 - couple spaces is we're going to add the
27:59 - MySQL container so you can see that
28:00 - these two are on the same line one being
28:02 - app with all of its information here and
28:04 - then MySQL will contain the same but now
28:06 - we're pulling the MySQL version 8 image
28:09 - the volumes that we want to use for this
28:11 - container is going to be the to-do my
28:13 - SQL data so let me go ahead and add that
28:16 - and then the overall volumes here are
28:18 - going to be outside of that and then
28:21 - finally what we need to do is we're
28:23 - going to add the environment variables
28:25 - for the specific container so we can
28:27 - access it now everything in your
28:30 - composed. file should look like this
28:32 - again the top level is identifying what
28:34 - services that we want this file to
28:36 - create the first one being our app
28:38 - container using the node 18 image this
28:41 - is the command we want to run this is
28:43 - the port that we want to expose and use
28:45 - to access this application our working
28:47 - directory is going to be this root
28:49 - directory that contains everything here
28:50 - the volumes we're mapping is going to
28:52 - use the app this is the environment
28:54 - variables we use to connect to our MySQL
28:56 - container that also contains its own
28:59 - information and so now it's much easier
29:01 - with Docker compose to run this
29:03 - application now that we have our
29:04 - composed. yl file I can open up my
29:06 - terminal here let me exit my SQL clear
29:09 - this now all I have to do is run Docker
29:12 - compose up and I can add The Hyphen D
29:15 - flag to make sure that this runs in the
29:17 - background and what that's did just now
29:19 - was literally spit out two containers
29:22 - one it looks like it failed because I
29:24 - have a container running already that
29:26 - has ,000 so let me go ahead and just
29:30 - delete this and delete this from the
29:33 - docker desktop app so we can have a
29:34 - clean slate and let me run the same
29:36 - thing and there we go now it's spit out
29:38 - the getting started app and it contains
29:39 - our app and my SQL container both of
29:41 - them running and we have opened on Port
29:43 - 3000 and if I head over into our Local
29:47 - Host Port 3000 I'm refreshing and you
29:49 - can see it's all fresh so we are note
29:51 - we're using a different volume here but
29:53 - the application is now running
29:55 - successfully it is attached to our my
29:57 - SQL database and we can continue and now
29:59 - add data here that will persist because
30:02 - we have tied a volume here to both of
30:05 - these containers now if we wanted to
30:07 - tear it down all we would have to do is
30:08 - Docker compose down and that will shut
30:11 - down and stop our containers from
30:13 - running and that right there is a simple
30:15 - rundown of how you can create images in
30:18 - a container through Docker using Docker
30:20 - files or a composed. yaml file and then
30:23 - accessing those containers to do certain
30:25 - things we ran a application in the form
30:28 - of a to-do app and we also created a
30:30 - mySQL database pretty quickly here in
30:33 - their own environments and that way we
30:34 - have coworkers that are working on the
30:36 - same thing they can run the same image
30:37 - that we're on using the same environment
30:39 - variables using the same dependencies
30:42 - and essentially be able to recreate any
30:44 - problems or just have the same
30:46 - environment that we're running on every
30:48 - single time and that is the power of
30:50 - containerization through Docker we're
30:52 - going to be using a lot of these skills
30:53 - in the coming lessons so hopefully that
30:55 - helps but now let's move move on to the
30:57 - SQL module where we're going to be using
31:00 - SQL but we're also going to start our
31:02 - own little database inside Docker as
31:03 - well so let's get to it so now that we
31:06 - have Docker set up and we know the
31:07 - basics we're going to move it on to SQL
31:09 - or SQL however else you want to call it
31:11 - but SQL stands for structured query
31:13 - language it's a standard language used
31:15 - for database creation and manipulation
31:17 - databases like MySQL or postgress use
31:20 - sqil for you to query any of the data
31:23 - within their own tables so let's go
31:25 - ahead and start our playground we're
31:26 - going to set up a database using
31:28 - postgress within a Docker container and
31:30 - then we can create our own tables with
31:32 - fake data to play around with and we'll
31:34 - use them later on in the course as well
31:36 - all right so first things first is I
31:37 - have a terminal open and I've created a
31:40 - data engineering database directory
31:43 - because we're using Docker we do not
31:44 - have to install postgress locally what
31:46 - we can do is run the command Docker pull
31:49 - postgress and what that's going to do is
31:51 - look up the image of postgress on the
31:54 - docker Hub pull the latest version of
31:57 - that down and now we can actually create
32:00 - a container that's running the postest
32:02 - image so now we got to start our
32:03 - postrest container now that we have the
32:05 - image so I'm going to say Docker run the
32:07 - name of this container is going to be
32:09 - data engineering postgress uh the
32:13 - environment variables that we that I
32:14 - want are going to be related to
32:16 - postgress which is going to be
32:19 - postgress
32:20 - password and what that's going to be is
32:22 - just going to be
32:24 - secret and then I'm going to do the D
32:27 - flag and we're going to say we're going
32:29 - to use the postgress image go ahead and
32:32 - run that now I have my container now
32:35 - what I want to do is I want to create a
32:37 - new database inside of postgress so I'm
32:40 - going to say Docker exec hyphen U for
32:44 - the user which is just postgress as
32:47 - default the name of the container which
32:49 - is just data engineering a
32:53 - postgress going say create DB
32:58 - and the name of the database is going to
33:01 - be
33:04 - postgress
33:06 - DB now that we've run that we've created
33:08 - a sample database inside now let's go
33:10 - ahead and connect to it so I'm going to
33:12 - say Docker
33:14 - exec
33:16 - slit data
33:18 - engineering
33:20 - postgress
33:23 - psql hyphen U for the user on postgress
33:27 - and the name of the database which is
33:29 - going to be postgress
33:31 - T now we are inside as you can see of
33:34 - the postgress database if I do
33:37 - /dt no current tables as of right now so
33:39 - that's what exactly what we're going to
33:40 - do we're going to create our first table
33:42 - I have these commands already written
33:44 - out so I don't waste as much time so the
33:47 - first thing I'm going to do just for an
33:50 - example is create a table called users
33:52 - so inside this command you see a couple
33:54 - things so we have an idid of thep type
33:57 - serial which is also our primary key
33:59 - here as an identifier we have a first
34:01 - name which is at most 50 characters we
34:04 - have a last name which is the same thing
34:06 - we have an email that is also characters
34:09 - but we want it to at least be 100 and
34:12 - then we have a date of birth which is
34:13 - just in the type of date so we're going
34:15 - to hit enter it's going to show us that
34:17 - we have a table so now if I do /dt we
34:20 - can see that the users table is now
34:22 - created and populated inside of this
34:24 - database I'm going to go ahead and copy
34:26 - in some fake data as well and don't
34:28 - worry this will all be in the
34:30 - description below so that way you can
34:32 - copy and paste this in as you follow
34:34 - along so I'm going to and copy and paste
34:35 - this and this is simply just user so it
34:38 - contains a first name a last name an
34:40 - email and date of birth so let me go
34:43 - ahead and enter and you can see it
34:44 - entered in 14 records so now this brings
34:46 - us into the First Command we're going to
34:48 - learn so the First Command we're going
34:49 - to want to learn is the select command
34:51 - and it's pretty self-explanatory as you
34:53 - saw already we entered in 14 rows of
34:55 - data and and if I wanted to select
34:58 - anything from that table I'm going to
35:00 - need to use the select command so I'm
35:04 - going to say
35:05 - select and I'm going to add this
35:08 - asterisk what the asterisk typically
35:10 - means is a wild card pretty much meaning
35:12 - you want all of the columns within a
35:14 - table so you can read this query as
35:16 - select all now I'm going to add from and
35:19 - then the last part of this is going to
35:21 - be the table name so I'm going to say
35:23 - users and do not forget this is very
35:25 - important that every query needs to end
35:27 - in a semicolon otherwise the query won't
35:30 - go through so make sure you always add a
35:32 - semicolon at the end of every query so
35:34 - once I hit enter you're going to see
35:35 - that I get back a formatted view of the
35:38 - table that I just queried we saw before
35:40 - that we entered in 14 rows of data and I
35:42 - see that I get exactly 14 rows back all
35:45 - having the ID first name last name email
35:47 - and date of birth formatted in a much
35:50 - more pleasing way than what we see up
35:52 - here the select query is going to be a
35:54 - frequently used command by everybody who
35:56 - who uses SQL so make sure you know it
35:59 - and understand it but again very
36:01 - straightforward another command that
36:03 - gets used in conjunction with select is
36:04 - also going to be distinct we use
36:06 - distinct in conjunction with select if
36:08 - we want a specified value within a very
36:11 - particular column so let's say we want
36:13 - every single email within the user's
36:15 - table what we'll say is we're going to
36:17 - say select
36:21 - distinct email
36:24 - from and then we're going to say users
36:27 - table so again we're reading this back
36:29 - if you haven't noticed already you
36:31 - typically all the SQL commands are going
36:33 - to be in all caps keep in mind you do
36:35 - not need this but it is much easier and
36:38 - it is probably best practice to
36:41 - capitalize every command that is SQL and
36:43 - lowercase everything that is either a
36:45 - table or some type of value that you
36:47 - need but if we're reading this back we
36:48 - want to select distinct emails from the
36:50 - users table so again it reads just like
36:52 - English we add the semicolon if we hit
36:55 - enter it's going to give us just the
36:57 - emails in every single distinct email
37:00 - now keep in mind this is not in order
37:02 - but we do get 14 rows of data back we
37:04 - can see that the query was successful so
37:06 - that's how you can select data from the
37:07 - whole table to get a better view or
37:09 - select distinct values within a
37:11 - particular column but let's move on to
37:13 - other commands so now that we have our
37:15 - users table that has data that exists in
37:17 - there what if we wanted to update some
37:19 - information say an email if a user
37:22 - changed their email or a date of birth
37:24 - if they entered it wrong this is where
37:26 - the update man comes in handy so let's
37:27 - say for example if we go back up to our
37:29 - table that we wanted to change J's email
37:33 - so what we're going to do is we're going
37:35 - to say update so the query starts with
37:37 - update now after update we're going to
37:39 - enter the table name so we're going to
37:40 - say users and then we're going to say
37:42 - set so now we want to address what
37:44 - column and value we want to change in
37:47 - the users table so we're going to say
37:49 - email and then that is going to equal
37:51 - whatever our value is going to be inside
37:53 - of single quotes so for example we can
37:55 - just say you know
37:57 - newcore mail@gmail.com
38:00 - the problem with this query is if we
38:03 - were to hit enter on this it would
38:05 - change the emails for every single row
38:08 - inside of the user table which isn't
38:10 - exactly what we want and if you were
38:12 - doing this in a production database it
38:14 - would probably go Haywire so how can we
38:17 - modify this query for it to actually
38:19 - only update one person's email inside
38:22 - the table what we can do is introduce
38:24 - the wear command so after we set the
38:27 - email to the new value we're going to
38:28 - say where and then we're going to say
38:31 - first uncore
38:33 - name so now we're saying where the first
38:36 - name so we're selecting a particular
38:39 - first name is equals to and the first
38:41 - name in this case is going to be John
38:44 - and we're going to hit a semicolon so
38:45 - again we're going to backtrack here
38:47 - we're going to update the user table
38:48 - we're going to set the email so we're
38:51 - selecting the column email to newor
38:54 - mail@gmail.com
38:56 - where our the first name is equals to JN
38:59 - so that means that we're going to look
39:01 - for JN inside of the first name column
39:04 - once we find John we're going to go to
39:05 - his email and change the email over to
39:08 - the new value so if I hit enter we can
39:12 - see that we have one updated change now
39:14 - if I say select go back to the cap
39:16 - select all from users table we're going
39:20 - to see that John's actually changed so
39:22 - he gets moved down he still has an ID of
39:24 - one but his email is now new _
39:26 - mail@gmail.com so again if we're using
39:29 - the update command it's very helpful if
39:31 - anything changes in terms of data and we
39:34 - need to update something that's where
39:36 - we' use the update command now let's say
39:38 - you wanted to enter in new data into a
39:41 - table well that's where the insert
39:43 - command comes in if you're starting to
39:44 - see the pattern here SQL commands are
39:46 - very self-explanatory and the fact that
39:48 - they read English and kind of like
39:49 - python so if you wanted to insert new
39:51 - values you would reach for the insert
39:53 - command so we're going to create a new
39:54 - table called the films table add some
39:56 - films films and then show the insert
39:58 - command but go more in depth into it all
40:00 - right so we are back in our database and
40:02 - I have the command here already I'm
40:04 - going to create the table of films again
40:06 - so create table is also a command that
40:07 - we didn't get really into but again
40:09 - self-explanatory we're just creating a
40:10 - table here called films within the
40:12 - parentheses we're going to see the film
40:14 - ID with a type we have a title a release
40:17 - date a price which is new that we're
40:19 - seeing a decimal type we have a rating
40:21 - and then a user rating the one thing to
40:23 - note within the user rating is that
40:24 - we're checking to see if the number
40:25 - value that goes into user rating is
40:28 - greater than or equals to 1 and that the
40:31 - user rating is less than or equals to 5
40:34 - so obviously we want to go between 1 and
40:36 - five here so if I hit enter we're going
40:38 - to see that if I do /dt which is like LS
40:41 - in a normal bash terminal is kind of
40:44 - like listing every table inside of a
40:46 - database we see that the films table is
40:48 - now created so let me go ahead and copy
40:51 - my insert command here and we see that
40:53 - we have insert into so again self
40:56 - explanatory we want to insert values
40:58 - into the films table within the first
41:00 - set of parentheses we see the names of
41:02 - the columns that we want to add and
41:04 - again those are fitting and matching
41:07 - exactly what we have on this side right
41:09 - here when we created the table after the
41:12 - first set of parentheses we have a
41:14 - values command and after that everything
41:17 - inside of the parentheses are values
41:19 - that have to match the order in which
41:23 - you placed every single column so the
41:27 - first one we have is a title which is
41:29 - these are all not fake data but I have
41:32 - them generated so we have Inception the
41:34 - release date which is right here in the
41:36 - date format we have its price in a
41:38 - decibel format we have the rating of the
41:41 - movie as well as the user rating of the
41:43 - movie and so you can see it fits within
41:46 - this set of parenthesis it fits the
41:48 - order of this set of parenthesis so if
41:50 - we hit enter right now you can see that
41:52 - we've entered in 20 rows of data if I
41:54 - say select
41:57 - all from
41:59 - films we can see that's a little out of
42:03 - place here because my terminal is a
42:05 - little big but we can scroll down and we
42:08 - can see that all the data was entered in
42:10 - go ahead and skip this line let me make
42:11 - it smaller so that we can see more of it
42:14 - but if I hit that again we can see the
42:17 - table is formatted properly the same way
42:21 - that the users's table was but that
42:23 - right there is how you can insert data
42:25 - into a table it doesn't have to be that
42:28 - big every single time if we just wanted
42:30 - to insert one more row it would just be
42:33 - the exact query back here except just
42:36 - one more line another really useful
42:37 - command that you should know is going to
42:39 - be the limit command imagine you have a
42:41 - database that has thousands upon
42:43 - thousands of Records but you only need a
42:45 - certain amount so you want to limit the
42:47 - query to that amount that's where limit
42:49 - comes in so as we saw in the new films
42:51 - table we saw that we have 20 rows but
42:53 - what if we only want less than 20 so
42:55 - let's let's say five that's where we
42:57 - would use limit so let's do select
43:00 - all from we're going to go into the
43:03 - films table but after films typically we
43:06 - would have that semicolon to end the
43:07 - query but now we're going to add limit
43:10 - and then the value of what we want to
43:12 - limit the query to so because we have 20
43:14 - we want less than 20 here let's just say
43:17 - I only want five records to come back if
43:19 - I hit enter you're going to see that I
43:21 - only get the first five records in the
43:23 - table to come back so that is a very
43:26 - easy to understand command here and very
43:29 - useful to limit the amount of records
43:32 - that come back cuz you could imagine as
43:33 - I said before if you're trying to query
43:35 - a database or a table that has thousands
43:37 - of records that query could take a very
43:39 - long time so if you want that to speed
43:41 - up a little bit and you only need a
43:42 - certain amount to see what the data
43:44 - looks like if it's formatted properly
43:46 - the limit command is going to be your
43:48 - best friend the next thing we're going
43:49 - to cover is going to include some math
43:51 - here but we're going to go over
43:53 - aggregate functions in SQL aggregate
43:55 - functions in SQL allow us to do
43:57 - computations on top of a single column
43:59 - through every single row of a table it's
44:02 - incredibly useful in data analytics
44:04 - where it allows us to calculate
44:06 - important metrics and gain insights from
44:08 - our data so let's get into the first one
44:10 - the first one is going to be count now
44:13 - the count function allows us to count
44:15 - the number of rows depending on if it
44:17 - matches the condition that we give it so
44:19 - now that we have our films table let's
44:21 - say we wanted to count the amount of
44:22 - films if we don't know how many are
44:24 - inside the table let's start the query
44:27 - here so let's say select right after the
44:30 - select statement we would say count in
44:33 - the parentheses we would count however
44:35 - many we want but if I add an asterisk
44:37 - here that's going to say count every
44:40 - single record inside of this table if it
44:43 - matches the condition I'm about to give
44:44 - it so we're going to say count from
44:47 - film's table so again this is selecting
44:51 - the number of Records within this
44:54 - condition here and what we give it is an
44:55 - asterisk for all from the films and what
44:57 - this should give me back is the number
44:59 - 20 because we have 20 rows inside this
45:01 - column so just remember within this
45:03 - parenthesis here is going to be the
45:04 - condition depending on what that
45:06 - condition is it will spit back to you
45:08 - always a number depending on if it's
45:10 - true within that condition and count is
45:12 - really useful for that the next
45:14 - aggregate function we're going to look
45:15 - at is going to be the sum function now
45:17 - we would use sum to calculate the sum of
45:20 - all the numbers in a numeric column so
45:22 - if we take a look here we have a prices
45:25 - column so now if if we wanted to let's
45:26 - say calculate the sum of every single
45:29 - price in this column we would say select
45:32 - sum inside the parenthesis we would say
45:36 - price because we want to use the price
45:37 - column from the film's table so now if
45:41 - we look back at this we're selecting the
45:42 - sum function is going to look into the
45:45 - price column and add up every single
45:48 - number value within it and it will spit
45:50 - back to us the total sum of every single
45:54 - movie that we have inside this table so
45:56 - if you were to have gone to every single
45:58 - one of these movies at this price you
46:00 - would have spent
46:02 - $228.8 now let's say we want to get the
46:04 - average of something inside of our table
46:06 - say for example we want the average user
46:08 - rating of movies in this table well
46:12 - that's where the average statement or
46:14 - the average function comes in it's going
46:16 - to look the exact same as the sum
46:19 - function so we're going to say select
46:20 - but instead of su we're going to say
46:22 - average or APG and inside the
46:24 - parenthesis we're going to enter in the
46:26 - column that we want to take a look at
46:27 - which is going to be user uncore rating
46:30 - close the parentheses there and then
46:31 - we're going to say from the films table
46:35 - and that then gives us an average of
46:37 - every single user rating that the movies
46:40 - have which is going to be 4.6 and lastly
46:43 - one of the more useful ones is going to
46:45 - be Max and min max and Min are used to
46:47 - find the highest and lowest numeric
46:49 - value within a column so let's look at
46:51 - our price column again if we wanted to
46:53 - get the lowest and the highest values in
46:54 - the prices column our query would look
46:56 - something like select and then we would
46:58 - say Max inside the max column our
47:00 - condition would be the column we want so
47:03 - we want price we're going to add a comma
47:05 - here and we're going to say Min again
47:07 - parentheses and looking at the prices
47:10 - column from the films table now if I hit
47:14 - enter here we're going to see the max
47:16 - value here was $14.99 and the minimum
47:19 - was $7.99 so the the cheapest movie out
47:23 - here that was in the table was $7.99
47:26 - and the max was $4.99 so very helpful to
47:29 - sift through a big table if you're just
47:31 - trying to find the highest value and the
47:32 - lowest value another useful statement in
47:34 - SQL is going to be group bu we use group
47:37 - bu to get identical data into groups
47:41 - it's also typically used with aggregate
47:43 - functions like the ones we just did like
47:45 - count sum min max average so that we can
47:48 - group the result sets into one or more
47:50 - columns let's take a look at how this
47:51 - would look if we were to use group bu
47:53 - with the average aggregate fun function
47:56 - let's take a look at the films table
47:58 - again and let's say we want to group the
48:00 - average user rating based off of the
48:03 - movies actual rating how would we do
48:05 - that we're going to select the rating
48:06 - column here and then we're going to add
48:08 - a comma and add our aggregate function
48:10 - since we're stringing together two
48:11 - select and inside the condition of the
48:14 - average function we're going to put in
48:17 - the user uncore rating
48:22 - column from the films Table after after
48:25 - that we're going to say Group
48:27 - by and what we wanted to group that by
48:30 - is by the rating so we're going to get
48:32 - the average of every user rating inside
48:36 - of the film's table and then group the
48:38 - results by each individual rating so we
48:41 - have PG-13 we have R and we have G npg
48:44 - so we should see four rows inside of the
48:48 - results after I had enter you can see
48:50 - the average rating for R-rated movies
48:52 - was
48:53 - 4.61 the average rating for PG-13 is 4.5
48:57 - PG was 4.8 and G was 4.4 so now you can
49:00 - see how this could be useful to group
49:02 - together identical forms of data and
49:05 - compare them against each other this is
49:07 - especially important in data analysis
49:09 - again it allows us to organize the data
49:11 - into a different way and then run
49:13 - computation on top of that especially
49:14 - with them being in groups it's also
49:16 - important to note the order that we did
49:18 - this query if we were to have switched
49:21 - the rating and the average functions
49:23 - here we would have got a different
49:25 - result so the order of the query does
49:27 - matter depending on how you want to
49:30 - format your data so keep that in mind
49:31 - one cool feature you can do in SQL is
49:34 - join two different tables that have
49:37 - different data sets and get a result
49:39 - that combines both of the data you need
49:42 - into a formatted table there are two
49:44 - different commands that we can use for
49:45 - this and we're going to go over those
49:47 - right now the first one being injoin and
49:49 - the left one being left join they do
49:51 - very similar things but there are key
49:54 - differences that that make them unique
49:57 - so let's go over what those are right
49:58 - now but before we do that we're going to
49:59 - need to create another table that we can
50:02 - use data against alongside our existing
50:05 - films table so let's first create the
50:07 - film category table so I'm going to go
50:10 - ahead and copy and paste this so we have
50:11 - a category ID a film ID and the category
50:14 - name these categories match every single
50:16 - movie that we currently have inside of
50:18 - the films table now I'm going to go
50:20 - ahead and insert our values here so let
50:23 - me go ahead and do that that hit enter
50:27 - we have 39 records here so now if I do
50:29 - select all from the Filmore category
50:35 - table now we can see that we have our
50:37 - category IDs and we have our film IDs as
50:40 - well as their category names go ahead
50:43 - and Skip all these lines here so now we
50:45 - have two tables that we can use to join
50:48 - with each other if we want a table where
50:51 - we have all the movies as well as the
50:54 - film category ories we're going to learn
50:56 - about two things right now we're going
50:57 - to learn about how injin works and we're
50:59 - going to learn about how aliases work in
51:03 - your SQL queries so let's go ahead and
51:05 - get to that so in order to understand
51:07 - how joins work we need to First create
51:11 - some tables that go against the data
51:13 - that we have currently in all those
51:14 - tables and insert some new ones so that
51:17 - we can join them together so the first
51:19 - table we're going to create is going to
51:21 - be the actor's table so this is going to
51:23 - have an actor ID and the actor name I'm
51:25 - going to have all the actors from every
51:28 - single film that we have currently and
51:30 - then we're also going to create the film
51:32 - actors table and this will make sense in
51:34 - a second so this one is going to have
51:37 - the film ID an actor ID as well as a
51:40 - primary key and now into the actors
51:43 - table are going to go every single name
51:47 - of each actor that are associated with
51:49 - the specific films let me go ahead and
51:51 - insert those and now we also have the
51:53 - film actors which are going to to
51:55 - include their IDs as well as the film
51:58 - IDs so now you can see that we have two
52:00 - separate tables that we just created if
52:02 - I do select all from the actors table we
52:08 - can see that we have actor ID as well as
52:11 - the actor name all in this table and
52:14 - then if we say select all from Filmore
52:18 - actors table you can see that we have a
52:20 - film ID and an actor ID so this goes
52:23 - with multiple different tables that we
52:24 - have relationship with and now we can
52:26 - focus on how to join all this data
52:29 - together and make sense of inner join
52:31 - and left join okay so like I said we're
52:32 - going to focus on the inner join and
52:35 - we're also going to add some aliases and
52:37 - what those are going to look like let me
52:39 - copy and paste this in here so we can go
52:41 - over what this looks like first we have
52:43 - our select statement and now we're going
52:45 - to go into what we're selecting so we
52:48 - have f. film ID and this will make sense
52:50 - in a second we have f. tile we have a.
52:53 - actor name so we're selecting the film
52:55 - ID we're selecting the title name as
52:58 - well as the actor name that's what the
53:00 - results are going to look like now we're
53:02 - going to select from the films table and
53:05 - after the films table is a space and an
53:07 - F so the Alias that we want to give our
53:09 - films table is f so now that anytime we
53:12 - want to access the films table all we
53:15 - have to do is type f as we can see here
53:18 - after we select the films table and add
53:20 - the Alias we're going to do an add our
53:22 - inner join statement this is where we're
53:25 - going to add different tables and join
53:27 - them together so after you say in join
53:30 - we're going to take on the film actors
53:32 - table and we're going to give it an
53:34 - alias of fa so short for film actors and
53:37 - we're going to join these two tables on
53:40 - which is why we have on so we're going
53:41 - to select the film's table and then the
53:44 - film ID from the film's table if it
53:46 - equals the same film ID from the film
53:48 - actor's table that's where we're going
53:50 - to join those rows and columns on we're
53:52 - going to add another inner join we can
53:54 - string on as many joins as we want so in
53:56 - this case we're going to do two because
53:57 - we're bringing on the actor's table so
54:00 - we're going to join the actor's table
54:02 - and we're going to give it an alas of a
54:04 - on the fact that any actor IDs from the
54:07 - film actor table is also equals to the
54:11 - actor IDs from the actor's table and
54:14 - lastly we see order by that's a
54:16 - statement we haven't gone over yet but
54:17 - we're going to order them by the film
54:20 - IDs from the film table so we're going
54:22 - to order them by those film IDs but
54:24 - we're going to go from the film table so
54:27 - I know that's a lot to take in but let's
54:28 - go over it one more time the final table
54:31 - that we are going to see we're going to
54:33 - have three different columns we're going
54:35 - to see the film ID from the film's table
54:38 - we're going to see the title from the
54:39 - film's table and then the actor name
54:42 - from the actor's table and now the
54:44 - reason we're not seeing the film actor's
54:46 - table in the select column is because
54:47 - one we're using the film actor's table
54:49 - as a bridge to connect the actor names
54:53 - to their correct films that's what we
54:55 - did here on this join is we took our
54:58 - film actors and if we look up at the
54:59 - film actors table we can see that we
55:01 - only have a film ID and an actor ID each
55:04 - of them are just numbers but they
55:06 - correspond to an actor as well as a
55:08 - movie so we took the data that we have
55:10 - from the film actor table and we joined
55:12 - it on top of the film table now if there
55:16 - was a film ID that matches any of the
55:19 - movies inside of the movies or I'm sorry
55:22 - inside of the films table then we're
55:24 - going to match those together take that
55:26 - data and use it for the results section
55:29 - here now that bridge allows us to say
55:32 - okay well now we want to join the
55:34 - actor's table on top of the film actor's
55:36 - table now now we can associate actors
55:39 - with the correct movies now that we have
55:40 - the film IDs and the actors that go with
55:42 - them so we're taking on the actor's
55:45 - table in the form of or in the Alias of
55:46 - a on top of film actors and now if we
55:49 - have any actor IDs that match the film
55:52 - IDs in the film ID or in the film actors
55:57 - table we're going to take that data and
55:59 - then use that here and then we'll
56:01 - obviously again order them by the film
56:03 - ID in the films table a lot to take in
56:05 - again but if I hit enter we're going to
56:07 - see that we have a very nice formatted
56:10 - table here like I said we're ordering by
56:13 - the f. film ID which we use up here as
56:15 - well so everything is ordered
56:17 - chronologically or you know in numerical
56:19 - order 1 2 3 4 and then that film ID will
56:22 - match Inception and then Le Naruto
56:25 - DiCaprio checks out was in Inception Tim
56:28 - Robbins was in Shashank Redemption
56:30 - Marlon Brando was in Godfather so you
56:33 - can see on and on that this does make
56:34 - sense and now if we were to go and do
56:38 - select all from the films
56:44 - table we can see that Inception had a
56:46 - film ID of one so that checks out right
56:49 - there um and then if we go to
56:53 - select all from the actor's
56:58 - table we can see that Leonardo DiCaprio
57:00 - had an actor IDF one so all of that data
57:03 - that we just took in actor idea of one
57:05 - and film idea of one matches exactly
57:08 - what it was between all the three
57:10 - different tables that we had a
57:11 - relationship with all to give us one
57:14 - very nice table that we can now use for
57:17 - any form of data analysis or what any
57:21 - sort of computation that any of your
57:23 - teams need and that's really a awesome
57:27 - way to join multiple different tables
57:29 - this was an example of inner join left
57:31 - joins do something similar but there is
57:33 - a catch between the two that make them
57:35 - unique as I mentioned before so let's
57:37 - take a look at what left joins look like
57:39 - so in order for joins to make sense we
57:41 - need to add another entry into the films
57:44 - table so let's go ahead and do that I
57:46 - have here a single entry that I will
57:49 - copy and paste right now so I am simply
57:51 - going back into the fils table and
57:53 - entering the mystical Adventures movie
57:55 - so let me go ahead and hit enter and
57:58 - then if we do select all from films we
58:02 - can see that the mythical Advent or
58:04 - mystical Adventures is now added at the
58:06 - very bottom so now we have 21 so now let
58:08 - me copy the left join query that we want
58:12 - to do here and now it's going to look
58:14 - very similar to the inner join where
58:16 - we're taking the film ID from the films
58:17 - table the title from the film's table
58:20 - and the actor name from the actor's
58:22 - table again we're adding the aliases
58:24 - here here with the letter f for the
58:26 - film's table we're left joining the film
58:28 - actors table again using that as the
58:31 - bridge uh it's essentially going to be
58:33 - the same query but if we were to hit
58:35 - enter on this right now you can see that
58:37 - we still do have the table filled out
58:40 - properly and formatted properly but we
58:42 - have mystical Adventures but with no
58:44 - actor so the thing with left joins is
58:47 - even if there is a null value or it's
58:50 - missing a value the result table will
58:52 - still show even if there isn't an exact
58:55 - match now if we were to go back up to
58:58 - the query where we do the inner
59:01 - join after we've now added the new movie
59:04 - we can see that it doesn't show up so in
59:06 - jooin if there is a null value and the
59:09 - condition doesn't the condition isn't
59:10 - fully met we will not see that in the
59:12 - result table so for the result table you
59:15 - see that we only have 20 entries as
59:16 - opposed to the left join where we have
59:18 - the 21 but we just have a missing actor
59:20 - so to now show that this is actually
59:23 - true we're going to insert a new actor
59:26 - as well as the new actor IDs to populate
59:29 - the movie so the movie we added was a
59:32 - fictional movie so let's go ahead and
59:34 - add the actor's name so we're going to
59:36 - add Jane do and John Smith and we're
59:38 - also going to add the film actors as
59:41 - well so 21 and 22 so if we were to run
59:44 - our left join command again we're going
59:47 - to see that mystical Adventures actually
59:49 - does populate now but we're going to see
59:51 - that the mystical Adventures actually
59:53 - does populate now with the actor name so
59:57 - this kind of just proves that no matter
59:59 - what left joins will always show the
60:02 - result table even if the conditions
60:04 - aren't meant and there is a null value
60:06 - there's two more commands we're going to
60:07 - go over the first one is going to be
60:09 - Union and the second one is going to be
60:11 - Union all these do get a little mixed up
60:13 - with the inner joint and left joint
60:14 - since they kind of function the same way
60:16 - in terms of how they work but they are
60:19 - completely different and let's go over
60:20 - what makes them unique here so the next
60:22 - two commands are going to be Union and
60:23 - Union all these are going to be the last
60:25 - commands that we go over so we actually
60:26 - use the union operator to string
60:29 - together two select statements but with
60:31 - the Union statement it actually removes
60:34 - any duplicate rows with the same values
60:36 - but with Union all you're going to get
60:38 - duplicate rows in the result set table
60:40 - so it kind of functions the same way as
60:42 - in joint and left joint in terms of
60:44 - their differences but the queries are
60:46 - going to be very different so let's go
60:48 - ahead and take a look at what that looks
60:49 - like all right so back in the terminal
60:50 - we're going to copy this Union command
60:52 - and we're going to go over every single
60:55 - line so with this command we're going to
60:56 - select title as name from film so we're
61:00 - going to use the name Alias we never use
61:03 - the as operator it is completely
61:06 - optional when giving aliases to use as
61:08 - so you can omit this when doing it but
61:10 - it makes it a little bit more clear if
61:12 - we say Select Title as name so instead
61:14 - of the column being named title it's
61:17 - going to be called name when we get the
61:19 - result table from the films table and
61:22 - then we're going to add the union
61:24 - operator here and then string on another
61:26 - select statement so we're going to
61:28 - select the actor name as name from the
61:31 - actor's table and we're going to order
61:33 - them by name so if I hit enter here
61:36 - you're going to see that doesn't really
61:37 - make too much sense but we can see how
61:39 - it works kind of we have one column
61:41 - since we named them both name down here
61:44 - and we're putting down this is an actor
61:46 - this is an actor this is the name of the
61:49 - movie so we put together a bunch of
61:51 - names and we order them alphabetically
61:53 - by name now if if we were to do that
61:56 - again but instead we were to change the
62:01 - actor name to actor and hit enter and it
62:05 - would be the same thing for Union all
62:07 - except remember there are going to be
62:08 - duplicate rows or duplicate values
62:11 - within here so if we were to scroll down
62:13 - you can see that we get Tom Hanks twice
62:15 - whereas in the table above we only got
62:18 - Tom Hanks once so again this is a prime
62:21 - example of Union and Union all so we're
62:23 - putting together all the data that we're
62:27 - grabbing using two select statements
62:29 - into one result table another cool
62:31 - little thing you can do in SQL that's
62:33 - actually going to be very useful later
62:35 - down the line especially as a data
62:36 - engineer is going to be something called
62:38 - subqueries we've seen them a little bit
62:40 - here by using the parentheses but let me
62:42 - go ahead and show you what one looks
62:44 - like and then we'll go over it so let me
62:46 - copy and paste it over so as you can see
62:49 - here we have first a select statement
62:51 - where we're selecting the title now
62:53 - inside of that we have a par we have a
62:55 - comma and then inside a set of
62:57 - parentheses we're using another select
62:59 - statement but we're selecting the actor
63:01 - name and again using aliases and a join
63:04 - so from actors Alias of a we're joining
63:07 - the film actors again using it as a
63:08 - bridge and then the final result from
63:11 - this select statement is going to be the
63:13 - actor name and then we're pulling this
63:15 - from films using the Alias F so this is
63:18 - just a string select statement but a
63:21 - subquery so before this finishes we're
63:23 - going to to run and find the title from
63:26 - the films table but then we're also
63:28 - going to look here SQL will go ahead and
63:31 - execute this query and then add it into
63:33 - the results table so right now what this
63:35 - is doing is it's retrieving the films
63:37 - title along with the name of one of its
63:39 - actors so if we go ahead andit enter you
63:41 - can see that it's kind of the same thing
63:44 - what we did earlier in the course where
63:45 - we joined two different tables but we
63:48 - used a subquery instead subqueries can
63:51 - be used in multiple different ways we
63:53 - saw this being used used in the select
63:55 - Clause here but we can actually use this
63:57 - also with the in operator so if we
63:59 - wanted to retrieve films that have
64:01 - either you know let's say Leonardo
64:03 - DiCaprio or Tom Hanks this is what the
64:06 - query would actually look like so let me
64:08 - go ahead and show you so we're selecting
64:09 - the title from the films table where the
64:11 - film ID in and then this is where our
64:14 - subquery would start this is where we're
64:16 - going to say hey we're looking for any
64:18 - films and we want the name of them if
64:21 - the condition is met here where the
64:23 - actor's name name is either Leonardo
64:25 - DiCaprio or Tom Hanks so we're looking
64:27 - for both of these movies so we're going
64:29 - to say Obviously go into the film
64:30 - actor's table and do another join here
64:33 - so we're adding that condition by
64:35 - joining in other tables and that's where
64:37 - subquery can really become useful here
64:39 - so if I enter you're going to see that
64:41 - we're only going to get the titles back
64:43 - and Inception Force gum and Toy Story
64:44 - are going to be the three movies that
64:47 - contain leard DiCaprio or Tom Hanks and
64:49 - obviously most of us have seen that so
64:51 - that checks out but that is a quick
64:53 - little intro Pro and awesome little
64:55 - thing we can do in SQL
64:59 - subqueries all right so now that we've
65:00 - gone through SQL we can finally get into
65:03 - building our first data pipeline this is
65:05 - going to evolve as the course progresses
65:08 - but we're going to start off by building
65:10 - this data pipelines from scratch using a
65:13 - python script and moving data that way
65:16 - we're also going to be using Docker to
65:17 - host both of our source and destination
65:19 - databases using postgress so you'll get
65:21 - to see how this all coincides and works
65:24 - together with Docker one thing to note
65:25 - before we keep moving on is there is a
65:27 - repo for this whole project you can see
65:30 - it here with a read me on how to get
65:32 - started and how everything works if you
65:34 - go up here to the branches you can see
65:36 - the evolution of this project so first
65:39 - we're going to be starting with just the
65:42 - main elt script and then we're going to
65:44 - be adding on a Cron job to show
65:46 - orchestration from there we're going to
65:48 - be adding DBT to the mix which we'll get
65:51 - into later and then also airflow and
65:53 - then eventually air bite so there's
65:55 - going to be this progression of the
65:57 - course to show kind of how a main elt
66:00 - script uh that's created manually would
66:02 - look and then as we add in more open
66:05 - source tooling you'll kind of see the
66:07 - power of how everything works together
66:09 - and why you know we are using these open
66:12 - source tools in the first place so just
66:14 - for your reference make sure to look in
66:16 - the description and you can find this
66:18 - link here to my GitHub repo with
66:20 - everything you need to see just in case
66:22 - you get stuck on something but for the
66:23 - rest of the project I'm going to be
66:25 - creating a new directory but it will
66:27 - directly reference this project so just
66:29 - keep an eye on this when you're working
66:31 - through it all right so I have my
66:31 - terminal open so let's go ahead and
66:33 - create our first directory here I'm
66:35 - going to say make and then for here I'm
66:37 - going to say elt and then we're going to
66:40 - CD into elt um and then from there we're
66:43 - going to say touch
66:45 - _
66:47 - script.py let's also create a Docker
66:49 - file so I'm going to use Docker compos
66:51 - in this so I'm going to say Docker
66:53 - compos
66:54 - and let's go ahead and open that up okay
66:57 - so now you can see we have two files
66:58 - that we just created we have the elt
67:00 - script and then we also have the docker
67:02 - compos file all empty but we're going to
67:05 - fill this out now the first thing we're
67:06 - going to do is actually fill out this
67:07 - Docker compose file in order to have the
67:11 - two databases that we need one being our
67:13 - source where our data is coming from and
67:16 - the destination where we want to send
67:17 - data to so that way we can use the elt
67:19 - script to move data between those two
67:21 - now we're going to use postgress for
67:22 - this example because we canuse use it as
67:24 - an open source database and so let's go
67:25 - ahead and fill this out so the first
67:27 - thing we need is going to be the version
67:28 - and uh version three is going to be what
67:30 - I'm going to be using for the doc compos
67:31 - file now we list our services so in here
67:34 - this is where we name our first service
67:36 - and in this case I'm going to name it
67:38 - Source postgress that's going to be our
67:39 - first service now remember with Amo
67:41 - files it is very crucial for you to have
67:44 - proper spacing within the file itself if
67:47 - not the yo file is not going to read
67:49 - properly and it'll actually error out if
67:51 - you don't follow the correct tabbing and
67:53 - indentation so keep that in mind um so
67:55 - the first thing we're going to do is
67:56 - list the image and for the image here
67:58 - I'm just going to say we want to pull
68:00 - from dockerhub the postgress image and
68:03 - the latest version of it so that way
68:04 - we're always up to date the ports so
68:07 - this is going to be very important for
68:08 - us in terms of accessing the actual
68:11 - service itself so with the source and
68:13 - the destination databases we need to
68:15 - expose the ports so that we can actually
68:17 - access them on the local host and so for
68:20 - this I'm going to say we're going to
68:21 - open this up on 5433 and by default we
68:25 - need to do 5432 I believe so let's go
68:28 - ahead and do that now for the networks
68:30 - we're going to create a network for all
68:32 - of our containers so you can look at all
68:35 - these Services as a single container in
68:38 - each single container contains an image
68:40 - that the runtime is going to use and so
68:42 - we actually need to create a network for
68:44 - all of these different containers to
68:46 - talk to now I'm going to name it _
68:48 - Network and we will see that at the end
68:51 - of this file we're going to have a well
68:54 - I can actually just do this right now so
68:55 - we're going to have a networks section
68:57 - and in there we'll list the _ network we
69:01 - also have to specify this is going to be
69:03 - a bridge driver um you don't have to
69:04 - worry about that what that means but
69:06 - just know that within the network all of
69:07 - the containers are going to recognize
69:09 - that this is the network we want it to
69:11 - talk uh remember that is outside of the
69:13 - services tab this is going to be its own
69:15 - section here with its own properties so
69:17 - make sure that you're doing that there
69:19 - now within the source postest service
69:22 - we're going to do some environment
69:23 - variables here that are going to be very
69:25 - important for the first one we're going
69:26 - to do the
69:28 - postgress database so this is going to
69:31 - the name our database and for the name
69:33 - we're going to say sourcecore DB keep it
69:35 - very simple uh the next one we're going
69:36 - to do is going to be the user so postore
69:39 - user and this one let me uh actually
69:42 - should be this way not an equal sign it
69:45 - should be a colon my bad this is going
69:47 - to be user postgress as default and then
69:50 - obviously the next one is going to be
69:52 - password this you can change to however
69:54 - you want it to be I'm going to say
69:56 - secret for this one and now for the
69:58 - volumes this is going to make way more
70:00 - sense and I I should actually create
70:02 - this um this folder now but we're going
70:05 - to create an elt folder we're actually
70:07 - going to drag this elt script into their
70:10 - folder just so we can have some
70:11 - separation between everything the docker
70:13 - composed file will remain in the root
70:15 - but um for the next one we're going to
70:18 - say volumes and if we remember what we
70:20 - were talking about in the docker section
70:22 - the volumes is is how we're able to
70:24 - persist data so each container can
70:27 - create a volume and then whatever we're
70:29 - doing inside of that Docker container
70:31 - will contain the data that we're
70:33 - utilizing within that volume so that
70:36 - data will always persist as long as we
70:37 - do not delete it from within Docker what
70:39 - I'm going to do here is I'm going to
70:41 - specify the path of so Source this will
70:44 - make a lot more sense in a second so
70:45 - Source IND justy uh init so we're
70:47 - actually going to have some data that
70:49 - we're going to initialize this postgress
70:52 - database with and and the init SQL
70:55 - folder or file sorry that we're going to
70:57 - create inside of the source DB init
71:00 - folder is what we're going to utilize
71:02 - and so after we've done that we're going
71:04 - to map this uh so if you see the colon
71:06 - after this this uh directory we're
71:09 - actually mapping this to the directory
71:11 - within the docker container so you could
71:14 - see this as this is our local directory
71:17 - and we're going to map this over into
71:18 - the docker directory and where this is
71:20 - going to go is going to go into the
71:22 - docker entry point init db. d/ init SQL
71:28 - okay so this should be exactly what it
71:29 - looks like for you and this is going to
71:32 - be the same it pretty much looks
71:34 - identical so we're actually going to do
71:36 - this and I'm going to do shift option
71:37 - down to duplicate this we're going to
71:40 - add a space here so this is going to be
71:42 - our very first service here which is
71:44 - going to be the source database and now
71:47 - what we want to create here is going to
71:48 - be the destination database uh and we're
71:50 - going to change the port here to 5434 we
71:54 - were going to keep this on the network
71:55 - on the elt network now this also needs
71:57 - to change so this is going to be
71:59 - destination ZB we'll keep user and
72:02 - password the same and we actually don't
72:04 - want any volume section here why because
72:07 - we don't want to save the data every
72:08 - single time for testing purposes I want
72:11 - to make sure that the El script runs
72:13 - every single time successfully and so if
72:15 - the data persists I can't confirm or
72:17 - deny that the elt script is actually
72:19 - working so that every single time we
72:21 - kill this Docker container I want the
72:23 - data to vanish so that's what we're
72:25 - doing here for the next service and the
72:26 - last one we're going to do for this
72:27 - section is going to be the elt script
72:30 - now why do we need to create one here
72:32 - well we need to create one in order for
72:34 - Docker to see that we're actually
72:36 - utilizing a script here to send data
72:39 - from these source and destination
72:40 - databases so we actually have to use
72:42 - Docker for this and use it for the
72:44 - runtime to actually run the script
72:47 - itself instead of us having to manually
72:49 - do it so what we're going to do here is
72:51 - utilize the manual docker files at the
72:54 - same time as using Docker composed
72:57 - there's different reasons for for doing
72:59 - this I think it's just easier to run
73:01 - certain commands utilizing it this way
73:03 - and we'll actually see what what this is
73:04 - doing so it's going to look a little
73:06 - different so we're going to say build
73:07 - under the build property we're going to
73:09 - add some context uh and the context is
73:11 - essentially what file we're we're
73:12 - building off of and so we're going to
73:14 - say ltor script inside of the root
73:17 - folder so it's going to be here and it's
73:19 - going to be root in reference to where
73:21 - the docker file is and we're just going
73:23 - to say Docker file but I'm going to
73:24 - capitalize the D here um cuz that's what
73:27 - I'm going to do when we create it now
73:28 - the command we're going to run I think
73:30 - that's changed it the command we're
73:33 - going to run we're going to put this in
73:35 - early brackets we're going to say python
73:37 - since this is a python file and then
73:39 - eltor
73:41 - script. same thing with the networks so
73:44 - we're going to say networks and then
73:46 - elore Network there now this is going to
73:48 - be one thing that we learned in Docker
73:50 - so the depends on property here is that
73:53 - this container depends on other
73:55 - containers and so if it has any
73:57 - dependencies this container won't
73:59 - initialize until the other containers it
74:01 - depends on are finished building and so
74:04 - if I say this depends on both the source
74:06 - as well as the
74:08 - destination services this script will
74:11 - not run until these two are finished
74:14 - initializing in which it does take time
74:15 - which would make sense right because we
74:17 - need to initialize the destination
74:19 - database because there is no way for us
74:22 - to move data if there is no data in here
74:24 - yet and this isn't even ready we don't
74:26 - want the script to run or else we're
74:28 - going to run into a lot of errors here
74:30 - so this is exactly why I wanted to use a
74:33 - Docker container for the elt script so
74:34 - that way we can control the way that it
74:36 - functions so now that we have this going
74:38 - on we're going to create a new file here
74:40 - we're going to say Docker file just like
74:42 - we said and now within the docker file
74:43 - this is where we're going to specify a
74:45 - couple things so obviously the structure
74:47 - is going to look a little different here
74:48 - but from is going to be the actual image
74:51 - we want to use so we're going to say 3.8
74:54 - H slim is going to be the first one that
74:58 - we're going to use here now what we want
74:59 - to do is we want to install postest with
75:02 - some command line tools so we're going
75:03 - to say run um and apt get since it's
75:07 - running off of Linux apt get update and
75:09 - and appt get install y
75:13 - postgress UL so that's going to install
75:16 - the postgress client there we're also
75:18 - going to copy so we're going to copy the
75:20 - contents of the actual elt script so
75:22 - that the docker container has access to
75:24 - it and so we're going to say elt or
75:27 - script.py this so make sure you have
75:30 - that period at the end and then the
75:31 - command we're going to run is again
75:34 - python
75:35 - eltor so that is going to be the
75:38 - structure here now the next thing we're
75:40 - going to do is actually create our
75:42 - source destination and initialize it
75:45 - with some data so let's go ahead and do
75:46 - that so if you go over to the repo of
75:49 - the custom elt project you're going to
75:51 - see this folder called Source DB
75:53 - underscore and knit now what this is
75:55 - primarily holding is a SQL file that
75:58 - contains all fake data so we're going to
76:00 - create a couple tables and then insert
76:02 - values into them so that's exactly what
76:04 - we're going to go do so let me go back
76:06 - and switch I'm going to create a new
76:08 - folder and we're going to name it Source
76:10 - dbor init as we just saw inside of there
76:12 - I'm going to do init SQL and if you
76:14 - remember it's exactly what we did here
76:17 - so we're pointing that exact file over
76:20 - into this directory of the container so
76:23 - that we have access to it we're creating
76:24 - a mapping of this over into the docker
76:28 - container so that way we can actually
76:30 - run the commands to initialize the
76:32 - source database from there now inside of
76:34 - this what we're going to do is just go
76:36 - in here and copy I believe I can just
76:38 - copy everything here and paste it so
76:40 - that's exactly what you're going to do
76:42 - just make sure you have access to this
76:43 - copy everything that you see inside the
76:45 - folder so we can save some time going to
76:46 - create a couple tables and insert some
76:49 - fake data for us like I mentioned we're
76:50 - going to save that and now we can
76:52 - actually go up about running this and
76:54 - and creating everything so the way to do
76:56 - that is going to be inside of our elt
76:58 - script so let's fill out elt script now
77:00 - all right so we are in our elt script
77:02 - here and there's a couple things that
77:04 - we're going to import so we're going to
77:05 - import the subprocess first so this is
77:08 - how we're allowing to control inputs and
77:10 - outputs and things like that we're also
77:12 - going to import time um this is going to
77:14 - be make sense a little bit later but the
77:16 - first thing we want to do is obviously
77:18 - double check and I know that the docker
77:19 - composed file checks to make sure that
77:22 - the elt script will not run unless the
77:25 - source and the destination database are
77:27 - working but we can also run a fallback
77:29 - just in case so we're going to create
77:30 - this function here called weight 4ore
77:34 - postgress and uh some of the variables
77:36 - we need are going to be the host we're
77:38 - going to need the maxor retries the
77:41 - default value here is going to be five
77:43 - delay in seconds we're going to leave at
77:45 - five so after that what goes inside this
77:48 - function uh we're going to say
77:50 - retries is zero while retri is less than
77:53 - Max retries we are going to say try
77:58 - results equals subprocess run and then
78:02 - in here we're going to say PG uncore is
78:06 - ready this say hyphen H going to post
78:11 - here check equals true now this is all
78:14 - going to be correlated to postest
78:16 - directly capture output true and then
78:20 - the text is true a couple checks here
78:24 - right uh I think I forgot something here
78:26 - move this over and then we're also going
78:28 - to put an if condition here if accepting
78:30 - connections so obviously we're pinging
78:32 - the actual database itself to make sure
78:35 - that it's ready to go or if it is
78:36 - accepting connections we're going to say
78:38 - print successfully uh connected to
78:42 - postgress and then we're also going to
78:44 - return true here and accept subprocess
78:48 - do called process error as e
78:53 - we're going to say print and F error
78:58 - connecting to post stress colon here I'm
79:01 - going to use that F here or sorry e for
79:04 - the error oh this to be inside
79:07 - retries if it errors we're going to add
79:09 - one to the retry plus equals 1 also
79:12 - going and add another print statement
79:13 - here and we're going to say f retrying
79:17 - in play seconds
79:19 - attempt fiz this is all just for log in
79:23 - here and then Mac
79:25 - tries here see where this isue here
79:29 - outside of the print we're going to say
79:30 - time do sleep delay seconds and then
79:35 - outside of the accept print Max R tries
79:39 - reached exiting in case all else fails
79:43 - and then we're going to return false
79:44 - Okay cool so that is going to be our
79:46 - check function here so I know that was a
79:48 - lot but you tries spelled that wrong
79:52 - both times
79:53 - all good okay that should fix it cool
79:56 - now we don't have any weird Squigly now
79:58 - we're going to get into actually
80:00 - building out the initialization of the
80:02 - source database and then moving on to
80:04 - the destination well we can also add
80:05 - this so we're going to say if not wait
80:07 - uh underscore 4ore postgress post equals
80:11 - sourcecore postgress then we're going to
80:14 - say exit with the eror with code one
80:16 - print um starting LT script well now we
80:21 - can actually do this so we're going to
80:23 - say sourcecore config open this object
80:25 - here and so the first thing we need to
80:28 - specify is going to be the DB name and
80:30 - we're going to be using dump files here
80:31 - if you're familiar with postgress we can
80:33 - use dump files to initialize the actual
80:35 - database but also use those and dump
80:38 - files into the destination so that's
80:40 - exactly how we're going to be using this
80:41 - batch script here for our elt and so
80:44 - we're going to say uh Source DB is going
80:47 - to be the name of our DB name it's
80:49 - actually not a uh sign it's going to be
80:51 - that and then then let me add our comma
80:54 - here same thing here so we're going to
80:56 - say user is going to be what we
80:58 - specified so these are all information
81:00 - that we had in the docker composed file
81:03 - this is going to be the password um and
81:05 - then the password was secret now for
81:07 - this next one we're going to say host
81:10 - now the host should be the name of the
81:12 - service uh that we put inside of the
81:14 - docker compost file so we're going to
81:16 - say sourcecore post that is going to be
81:17 - the source config now we're going to do
81:19 - the exact same thing for the destination
81:21 - unor config DB name is
81:25 - destination DB username is if I can type
81:29 - properly post address password is going
81:32 - to secret as well and then the host is
81:35 - going to
81:37 - destination this destination yep no
81:40 - typos there perfect now we need to
81:43 - create the dump command for initializing
81:47 - The Source data so underneath here we're
81:48 - going to say dump underscore command
81:51 - we're going to say PG dump which is the
81:53 - command here that we're going to use um
81:56 - the H flag hyph H we'll use the
81:58 - sourcecore config and we're going to
82:00 - point to the host since that's our host
82:03 - um let me make sure that this has a
82:06 - comma for the user so we're going to say
82:08 - hyphen U we're going to say Source
82:10 - config and we're going to point it to
82:12 - user so you can see kind of what we're
82:13 - mapping each individual flag so the host
82:17 - user password all that stuff too so
82:20 - we're going to also do uh hyphen d for
82:22 - the database name and we're going to
82:24 - point it to sourcecore config D name
82:26 - hyphen f is going to be the file that
82:29 - we're going to use and where we're
82:30 - pointing that is going to be dataor dump
82:34 - needs to be single quotes
82:35 - dataor dump. SE if we add this flag
82:40 - hyphen W it will not prompt us for the
82:42 - password which we don't have to do every
82:43 - single time which is nice so the other
82:45 - thing too is we're going to set the
82:47 - environment variable to avoid the
82:49 - password dump so we're going to say
82:50 - subprocessor EnV equals di uh PG
82:55 - password equals Source config password
83:00 - so now we we've created a password RPG
83:03 - password environment variable that will
83:05 - not ask us for the password every single
83:06 - time now we're going to execute this
83:08 - dump command so we're going to say
83:09 - subprocess do run dump command which is
83:12 - the one we just created point it to the
83:14 - environment variable that we just
83:15 - created so subprocessor EnV and then
83:18 - check equals true so that's going to
83:20 - dump everything into the source datab
83:22 - this now we actually need to get
83:24 - everything from the source database over
83:26 - to the destination so we're going to
83:28 - create a load command and it's going to
83:32 - look pretty much identical to this so
83:35 - what I'm going to do is copy and paste
83:38 - everything here but we're going to make
83:39 - some slight modifications so instead of
83:41 - PG dump we're going to say psql so we're
83:44 - actually going to get into this SQL
83:45 - command line tool we're going to point
83:47 - it to the host but instead of source it
83:49 - needs to be destination the user is also
83:51 - the same so we need to point it to the
83:56 - destination database name is going to be
83:59 - the
84:00 - [Music]
84:02 - same now we need to or well we don't
84:05 - need this since we're not pointing it to
84:06 - a file well we do what we're actually
84:09 - going to do is concatenate this so we're
84:11 - going to put two commands here we're
84:13 - going to do hyphen a comma F we're going
84:17 - to delete the W uh and then we're still
84:19 - pointing it to the data. dump. SQL file
84:22 - now now we're going to do the same
84:24 - environment variable with
84:27 - subprocessor EnV equals uh we're
84:30 - actually copy and paste this over again
84:33 - cuz I mean it's literally the exact same
84:35 - thing except we're going to be using the
84:38 - destination
84:39 - here destination config now we need to
84:43 - execute the load command um so what
84:45 - we're going to do is we're going to say
84:47 - subprocess do run load command EnV is
84:51 - going to equal sub processore EnV and
84:54 - then we're going to do the check equals
84:56 - true again and we're going to print just
84:58 - for our own purposes ending elt script
85:03 - cool so now this is going to be our full
85:07 - on elt script now how do we run this
85:11 - well what we're going to do is use
85:14 - Docker to completely run everything here
85:16 - so let's go ahead and take a look at
85:17 - what that looks like because we've made
85:19 - everything and hooked everything up with
85:22 - do ER compose it's going to be as simple
85:24 - as this all we have to do inside of our
85:26 - terminal obviously make sure that we're
85:28 - in the directory that we created I'm
85:30 - going to run Docker compose up uh I
85:33 - forgot to do this so I forgot to change
85:36 - the host to hostress here um let me make
85:38 - sure that's that okay so I had a slight
85:40 - error but all we have to do is we're
85:41 - going to do Docker compose up what say
85:44 - Docker compose services. Source oh does
85:47 - it not need this uh I might have screwed
85:49 - up here again which is fine we all make
85:51 - mistakes so it doesn't actually need
85:54 - this section here uh let's see if that
85:57 - will fix it we're going to leave this in
85:59 - here all right we're running into a
86:01 - couple things here so _ script so let's
86:04 - do let's just debug this live cuz why
86:06 - not/ scripts so this should just be elt
86:10 - I'm pretty sure there you go okay so
86:12 - just a couple errors here but hey
86:14 - nothing that's going to shake us up but
86:15 - so now that we run Docker compose up
86:17 - we're actually going to go through and
86:19 - build out the images so we're pulling
86:21 - down from the docker post or from Docker
86:23 - Hub to uh build out these images so now
86:26 - what we can see is we might be running
86:27 - into something here but as you can see
86:29 - in the logs and let me pull this over so
86:31 - you can actually see it so through the
86:33 - logs what we've done is we have we've
86:37 - initialized the destination database and
86:40 - the source database which is good and
86:42 - then we've also created the database
86:44 - we've inserted a couple tables and then
86:46 - the values for them but it looks like we
86:48 - ran into an issue with the elt scripts
86:51 - uh starting elt script so it looks like
86:53 - that was good trace back on line
86:56 - 54
86:58 - [Music]
87:01 - uh okay so let's look at our PG script
87:04 - so you can kind of see like all the
87:06 - little nuances here with creating a bat
87:08 - script and like this is on purpose uh I
87:11 - I wanted to make sure that we ran into
87:13 - some issues here I mean obviously I
87:14 - would like to not run into issues but
87:17 - comes with coding comes with building
87:18 - out these things manually and we'll see
87:21 - that it gets a little bit easier to deal
87:23 - with later down the line um so on line
87:25 - 54 is when it said that we had a bug
87:27 - here so subprocess run dump command I
87:30 - think this should be a hph U or capital
87:33 - u here go ahead and try this one more
87:35 - time so what we're going to do is Docker
87:37 - compos
87:39 - down type oh my God okay so the source
87:42 - is ready to connect still running into
87:44 - an issue here um on the subprocess still
87:47 - on line
87:48 - 54 so we've started the elt script
87:51 - return nonzero exit status let's try
87:53 - without the check here no that's not it
87:55 - DB name sourcecore DB post you got
88:00 - secret Source postgress
88:03 - destination or postgress okay so I've
88:07 - deleted the image on Docker let's try
88:09 - running this one more time let's build
88:11 - the image again and see if it fixes it e
88:15 - error oh I see the issue okay so I
88:17 - screwed that up I think that should be
88:19 - good now so let's Docker compos down
88:23 - again Docker compose up just a casual
88:26 - typo error here I believe it's still not
88:28 - going to work here cuz I think it's
88:30 - still readed the old one so what I'm
88:32 - going to do is kill this Docker compose
88:35 - up or sorry Docker compose down rid of
88:38 - these now let's go into Docker and you
88:39 - can see kind of like what the
88:41 - troubleshooting so we built this image
88:43 - and I don't think it's changed it so I'm
88:45 - going to delete that um we're going to
88:47 - go back into the terminal here say dock
88:52 - compos up so we're going to rebuild the
88:54 - image and it
88:56 - should there we go okay so now you can
88:58 - see after some troubleshooting and then
89:00 - some small typos here and there uh we
89:02 - have
89:03 - successfully gotten our logs back and
89:05 - then at the end we had our ending elt
89:08 - script so the way to check our work now
89:11 - is if we open a new terminal let me make
89:13 - this a little bit bigger go back into
89:15 - our other directory so CDT I'm going to
89:18 - say Docker exec and this is from what we
89:20 - learned in the other tutorial on Docker
89:22 - so Docker exact and we're going to use
89:23 - the interactive flag here uh we're going
89:27 - to grab the name of our _ script 1 so I
89:33 - believe the full name of it should be
89:36 - this so we're going to grab I'm sorry
89:38 - we're not grabbing this we want to grab
89:39 - the destination name so we're going to
89:41 - grab _ postgress one so that's going to
89:44 - be the name then we're going to say psql
89:48 - we want to run the command line the
89:49 - postgress command line hyphen U with the
89:51 - user user postgress now if I hit enter
89:53 - we're going to be in here the one thing
89:55 - we do need to do is we're going to say/
89:57 - C for connect to the
90:00 - destination DB so now we can see that we
90:03 - are in the destination database now if I
90:05 - do slash this is backs slash by the way
90:08 - or forward forward slash I think uh is/
90:11 - DT you can see that all of the tables
90:14 - are now inside of the destination
90:16 - database now if I do select all from
90:19 - actors I need to add the semicolon you
90:21 - can see that all the data that we
90:23 - inserted into the source database is now
90:25 - into the destination database this is
90:27 - now fully working it is not automated at
90:30 - all but you can see that our bat script
90:33 - is now successfully working just by
90:34 - simply taking the dump commands that are
90:36 - built into postgress and then moving
90:38 - those in to the destination database now
90:42 - this is postrest to postrest so this is
90:43 - very very simple but you can see that
90:45 - even this is a very basic script but
90:47 - there's a lot of moving Parts when it
90:49 - comes to building out a script manually
90:52 - and so when things change or if you're
90:53 - trying to connect postgress over into
90:55 - let's say another database type where
90:57 - you potentially have to match schema and
90:59 - things like that in this case we didn't
91:01 - have to worry about that because the
91:02 - same database same protocol and
91:04 - everything but if we're working with
91:05 - something else it can get very very
91:07 - tricky so that's why we're going to be
91:09 - implementing a lot more open source
91:11 - tooling but for this next part we're
91:13 - actually going to add a Cron job to this
91:15 - to automate the script a little bit
91:17 - because as of right now we are simply
91:21 - just
91:22 - manually triggering this so if we needed
91:24 - to do this every day every 3 days we
91:27 - could actually add that in here so let's
91:29 - go ahead and do that now okay so
91:31 - actually I lied the next evolution is
91:33 - not going to be the KRON job next it's
91:35 - actually going to be DBT DBT is going to
91:38 - be an open source tool that we can
91:40 - utilize here in order to write custom
91:43 - Transformations or custom models on top
91:45 - of the data that we've just sent into
91:47 - the destination database so you can take
91:50 - for example that we had a use case where
91:53 - this destination database is primarily
91:55 - going to be used by data analysts or
91:58 - some sort of data scientist and they
92:00 - need their data formatted or structured
92:03 - in a certain way where we're combining
92:04 - certain tables together they can't write
92:06 - SQL or something like that and they
92:08 - actually just need it a certain way or
92:10 - something along those lines we can use
92:12 - DBT to have those custom models so that
92:15 - by the time the data is inside the
92:17 - destination we can run DBT on top of
92:19 - that data and then write custom models
92:22 - here creating custom tables and things
92:23 - like that and it allows us to have
92:25 - certain capabilities it also allows us
92:27 - to even write python to run SQL scripts
92:29 - and things like that so let's go ahead
92:31 - and see what that looks like and
92:32 - implement it now there are a couple
92:34 - prerequisites you need in order to run
92:36 - DBT successfully on your machine or even
92:38 - using Docker so let's go ahead and take
92:40 - a look at what that looks like and
92:41 - actually get installed all right so I'm
92:43 - here on the DPT docs which we will also
92:45 - list down in the description just so you
92:47 - can get to it pretty easily now there is
92:49 - some things you need to do locally uh
92:52 - not everything you can do here Docker so
92:54 - the first thing you need to do is
92:55 - obviously I'm on a Mac so most of the
92:57 - instructions here are going to be for
92:58 - Mac but you do need to have this
93:00 - installed locally and the way to do that
93:02 - is if you probably have Pip if you have
93:04 - python installed and so you're going to
93:06 - follow all these instructions uh I think
93:07 - you could pretty much skip these ones
93:09 - right here uh you're going to want to
93:12 - follow this one for sure I think
93:14 - actually you can probably skip that one
93:15 - too the main one here is installing the
93:17 - actual adapter you're not just
93:18 - installing the adapter but you're also
93:20 - installing DB T core which is the
93:23 - opsource version of DBT now I've already
93:25 - done this but if you run DBT install DBT
93:28 - hyphen postgress we're installing the
93:30 - adapter of postgress that utilizes DBT
93:33 - as well as DBT core you're going to go
93:35 - ahead and run that locally I've already
93:37 - done that uh as you could see here so if
93:40 - I say uh DBT hyen hyen version you can
93:43 - see that it's going to list pretty much
93:45 - I have 1.62 installed I'm not going to
93:46 - update for the sake of this tutorial but
93:48 - there's 1.66 and I have the post cres
93:51 - version installed as well so make sure
93:53 - you go through this installation really
93:55 - quickly and that way we can actually go
93:57 - through and create our models so make
94:00 - sure you do that that is just the only
94:01 - prerequisite for this section okay so
94:04 - now that we have DBT installed let me go
94:06 - ahead and show you how we can initialize
94:09 - a project here so it's as simple as DBT
94:12 - innit we're going to run DBT innit
94:14 - inside of the root directory excuse me
94:16 - uh now it's going to ask us for the name
94:18 - of our project we're going to say uh
94:20 - custom I don't know custom _ postgress
94:23 - so it's going to say custom _ postgress
94:24 - and you can see it's creating these
94:26 - folders here for us now we have the
94:29 - database we would like to use it's
94:30 - asking us right here for postgress and
94:33 - obviously it's the only adapter that we
94:34 - installed so we're going to hit number
94:35 - one and therefore it's created us our
94:38 - it's created two folders actually if we
94:40 - take a look on the right left hand side
94:41 - it's created logs for us so we now have
94:44 - logs for anything regarding DBT as well
94:47 - as our project for DBT so it has a
94:49 - couple folders here analys macros models
94:52 - seats snapshots and tests primarily what
94:55 - we're going to be focusing on is the
94:56 - models and macros for now and then the
95:00 - the rest of it is is kind of like you
95:01 - know you can learn as you go but for the
95:04 - sake of this tutorial we're going to
95:05 - focusing on these folders here so there
95:07 - are other couple things that we need to
95:09 - do in order to set up the project itself
95:11 - before we actually write our models the
95:13 - one thing we actually have to do is
95:14 - going to be this so I've just copi and
95:16 - pasted this one command but you're going
95:17 - to do Nano and inside of the complete
95:19 - route of your local machine when you
95:21 - install all DBT using pip it actually
95:23 - creates this DBT directory and inside
95:25 - that folder is a profiles. yl file so
95:28 - what we're going to do is go in there
95:30 - and you can see that I've already set
95:31 - this up but inside of the dev profile it
95:34 - does say post Crestor Transformations
95:37 - I'm going to go ahead and change this
95:38 - because this is now different so what
95:40 - I've done now is I'm going to do change
95:43 - the name of this because it's no longer
95:45 - post Transformations that's what I
95:46 - normally did uh this is going to be
95:48 - custom postgress you can see it's done
95:51 - that down here it it is actually done
95:53 - that down here actually you know what
95:54 - I'm probably just going to do that
95:55 - instead I'm going to post
96:01 - address so now that we're inside of this
96:03 - folder you can see that I've already
96:05 - done this prior but if we were to go
96:07 - down here into the custom post crust
96:10 - profile which is created for me because
96:12 - of this new project you can see here
96:13 - there there's two different outputs
96:15 - there's prod and Dev we're going to be
96:17 - primarily focusing on dev and we do need
96:19 - to fill out the actual information here
96:21 - so inside of host I'm actually going to
96:23 - do host. doer. internal which is needed
96:26 - in order for DVT to actually recognize
96:29 - the docker containers that we were
96:31 - trying to access here which is going to
96:32 - be our destination database so make sure
96:35 - that this says host. doer. internal and
96:38 - it'll read the actual Network or IP
96:40 - address of your Docker container for the
96:42 - destination database for the port here
96:44 - this is going to be 5434 I'm pretty sure
96:47 - for the destination for the user we're
96:49 - going to change this over to postest for
96:51 - the
96:52 - password we're going to change this over
96:55 - to
96:56 - secret DB name will be destination _ DB
97:02 - and then I forget what I actually put
97:03 - for the schema here uh schema should be
97:06 - public and then I forgot to fill out the
97:08 - threads here so let's change this to one
97:11 - all right so and the target here is
97:13 - going to be Dev so obviously make sure
97:15 - that says it by default it should just
97:17 - Target the dev profile but now that we
97:19 - have our information filled out here it
97:21 - should have the correct information to
97:23 - connect to our destination database I
97:26 - believe if I do write out maybe not
97:28 - write out but if we say x save modified
97:31 - we're going to hit yes tab to complete
97:33 - and hit enter okay so this should have
97:35 - saved it now the other thing to do here
97:37 - is inside of the DBT pro. file the one
97:41 - thing to note is that obviously the
97:43 - profile we just wrote is the custom _
97:46 - postgress you don't have to change
97:47 - anything here except for this bottom SE
97:49 - inside of the bottom section is going to
97:51 - be how we're forming our models now
97:53 - inside of the models folder you can see
97:55 - that there's an example folder and then
97:58 - a couple of models here what we're
97:59 - actually going to do is we're going to
98:01 - change this there's a couple options
98:03 - inside the material section but because
98:05 - we're using postest tables we actually
98:07 - want to change this to table if you keep
98:09 - it as view we're actually not going to
98:11 - see any of the tables that we created
98:14 - come through or anything like that or
98:16 - DBT is not going to be able to recognize
98:17 - them so we have to change it to a table
98:19 - now that we've done that we can actually
98:21 - go in and write our models so let's go
98:23 - ahead and do that all right so we are
98:24 - inside of the models folder here and
98:27 - inside of this example folder are some
98:29 - custom examples that they have created
98:31 - for us uh we don't really need to see
98:33 - this because we're going to be writing
98:34 - our own so I'm actually going to go
98:35 - ahead and delete both of these we're
98:38 - going to keep the schema. yaml for now
98:40 - well we're going to reference that later
98:42 - but let's create our models here so you
98:44 - can think of models as just SQL files
98:47 - we're just querying the data but we're
98:50 - morphing the data around to the needs
98:52 - that we need so we just do it once here
98:55 - and then once the destination database
98:57 - has its data everything that we've
98:59 - queried here all the modifications and
99:02 - the new tables that we've created will
99:04 - end up inside of the destination
99:06 - database and you'll see what I mean by
99:07 - all that once we have it all hooked up
99:09 - the first thing we need to do is
99:10 - actually Source the data itself because
99:13 - it can't just simply reference tables
99:16 - inside of a destination database without
99:17 - having the actual reference itself so
99:19 - what we're going to do is we're going to
99:20 - create our first one so we're going to
99:22 - say Filmore actors which is a table that
99:25 - we're actually writing over into the
99:26 - destination database and this is simply
99:28 - just creating a reference for us to
99:30 - create the custom models so this isn't
99:32 - necessarily a custom model here um so
99:34 - we're going to write some some SQL here
99:36 - select all from and then we're going to
99:38 - do double curly brackets here and inside
99:40 - of that we're going to say the source of
99:41 - our data is going to come from
99:44 - destination database and the name it for
99:47 - this that we're going to reference it
99:49 - from is going to be Filmore actors we're
99:51 - going to need to wrap both of these in
99:52 - single quotes very cool now what we've
99:55 - done is we've sourced the destination
99:56 - database and we've taken the film
99:58 - actor's table and now we have a film
100:00 - actors reference let me go ahead and
100:01 - copy and paste this we're going to do
100:03 - that again for the actors table so
100:05 - actors. SQL we're going to paste this in
100:07 - here and we're simply going to change
100:09 - this over to actors and we're going to
100:11 - do the same for films films. SQL so we
100:14 - have our references now we can actually
100:16 - create our custom models but again
100:18 - before we do that we have to have some
100:20 - prerequisite work so for the schemas
100:22 - these are our custom models here our DBT
100:25 - models and we do have to specify what
100:27 - the schema of these are going to be so
100:31 - that if we do need to test anything
100:32 - which you should be testing you can run
100:35 - a model or you can run DBT against all
100:38 - of these SQL files and if it doesn't
100:39 - match what we've specified inside the
100:41 - schema we're going to get thrown back
100:42 - errrors so you should definitely always
100:44 - do this what I'm going to do is I'm
100:46 - going to head over into our file here
100:49 - and I'm going to look into to the models
100:52 - and example folder and we're going to
100:54 - have a couple things so schema. AML is
100:56 - what we want we see that there's a films
100:57 - table here and every bit of schema here
101:00 - for that one the actors film actors film
101:03 - ratings I think I need the film ratings
101:04 - one two as a yeah that's fine so what
101:07 - we're going to actually just do here is
101:09 - copy and paste this whole thing I
101:11 - implore you to do that too we're going
101:12 - to paste that in there and now we have
101:14 - the schema for all the tables that we
101:17 - just added now the other thing too that
101:19 - we need to do is we're going to go ahead
101:21 - and copy over the sources. EML so now
101:24 - we're specifying the raw data from the
101:26 - destination database so I'm going to
101:28 - copy this as well and you can see film
101:30 - actor and film actors literally the
101:32 - exact tables that we just copied so
101:35 - we're going to go in here and create a
101:37 - new file called sources. EML and paste
101:41 - everything that we just saw so now we
101:42 - actually have sources that we can point
101:44 - to regarding these files or tables
101:47 - essentially that we're querying from so
101:49 - you can see if we think about the this
101:51 - from the top down data is theoretically
101:53 - already written inside of the
101:54 - destination database DBT is going to run
101:58 - after we've move data over so the data
102:00 - is already there and from there what
102:02 - we're doing is taking in the data so
102:05 - that way we can modify it and create a
102:07 - custom model with that data so all we're
102:10 - doing here is referencing data that we
102:12 - already have now let's go in and create
102:14 - the custom models to formulate and
102:18 - modify the data we have into a table
102:20 - that we want okay so the first model
102:23 - we're going to create inside of the
102:24 - example folder I'm going to name Filmore
102:27 - ratings. sequel so what we're going to
102:30 - be doing is appending the films as well
102:33 - as the actors together to create one
102:36 - table but it's going to be primarily
102:39 - focused on the film ratings so let's get
102:42 - to it all right so what we're going to
102:44 - do inside of this file is we're actually
102:46 - going to create two different tables and
102:48 - then use a join to create one if that
102:51 - makes sense so the first one we're going
102:53 - to call is with filmscore with ratings
102:56 - as and then this is where we're going to
102:58 - run our squl to create this table so
103:00 - we're going to say select
103:02 - Filmore ID
103:05 - title release underscore date make sure
103:08 - you're adding commas price rating these
103:11 - are all from the table that we already
103:13 - have this is going to be user rating
103:16 - this is where we add something very
103:17 - specific so this is going to be a case
103:19 - so what the case is kind of like you can
103:22 - think of an if statement per se so we're
103:24 - going to say when user rating is greater
103:28 - than or equals to uh let's say
103:32 - 4.5 then we want the rating to say
103:35 - excellent uh we're going to add another
103:37 - one where user rating is going to be
103:39 - greater than or equals to 4.0 so kind of
103:43 - like in the in between there and we're
103:44 - going to say that we want the rating to
103:46 - read good and then the last one we're
103:47 - going to do is going to the user rating
103:49 - is going to be greater than or equ = to
103:51 - 3.0 then we're going to say that this is
103:54 - going to be average else we're going to
103:57 - label it as poor and we're going to end
104:01 - as rating uncore
104:03 - category so that ends this case
104:06 - statement here and this is where the
104:08 - reference comes in so we're going to say
104:10 - from and we're going to open the double
104:12 - curly brackets again and if we say ref
104:14 - parentheses films we're going to be
104:17 - pulling from this here because we
104:18 - labeled it as films so now we can
104:20 - actually reference it here within this
104:23 - little query here one thing I forgot to
104:26 - mention for this specifically is you saw
104:28 - that we say with and as or with films
104:31 - with ratings this is actually what's
104:33 - referred to as a CTE or Comon table
104:36 - expression so you can see this as a
104:39 - wrapper of this query kind of like a
104:42 - subquery per se but it makes it easy to
104:45 - write queries that are maintainable and
104:48 - they're also scoped just to this CTE so
104:52 - it will not like go outside of this so
104:54 - this allows us to write subqueries and
104:56 - then ultimately reference them later
104:59 - inside of this file so it is actually
105:01 - really really nice part of DBT that lets
105:04 - us write CTE uh to kind of formulate
105:06 - these custom models so that's just
105:09 - something I forgot to mention when we
105:10 - were first writing this but remember
105:12 - this is a CTE or Common Table expression
105:14 - all right so now that we have our first
105:15 - CTE so let's write the second one I
105:18 - added a comma after the CTE so that way
105:20 - we can string on another one this one is
105:21 - going to be films with underscore with
105:25 - actors as and then that's going to open
105:27 - our CTE here our second one so we're
105:30 - going to say select and we're going to
105:31 - do the Alias here um f.
105:35 - Filmore ID f. tile and then we're going
105:38 - to do a string aggregator or string
105:40 - aggregation with a.
105:42 - actor undor name we're going to pend it
105:45 - with a comma there and we're going to do
105:47 - as actors so we're aggregating actor
105:50 - names for each film here and then we're
105:53 - also going to hit the reference so from
105:55 - double curly brackets reference films
105:58 - and then obviously we're going to enable
105:59 - the Alias here with f my auto completes
106:02 - that but there we go so then we're also
106:03 - going to left join Filmore actors table
106:06 - and we're going to give that fa for film
106:08 - actors on f. Filmore ID if it equals the
106:12 - film actors. film ID uh we're going to
106:16 - do another left join here referencing
106:18 - the this left join we needed this so
106:20 - let's go ahead and do that so forgot the
106:23 - curly brackets or I'm sorry the uh
106:25 - parenthesis and single quotes there um
106:28 - let's add the actors for this reference
106:31 - obviously this one is going to be a on
106:33 - fa. actor _ ID if it equals the a. actor
106:39 - ID and then we're going to group by the
106:42 - f. Filmore ID and the f. tile okay so
106:47 - that's going to be the second CT so we
106:50 - have two now technically two queries
106:53 - films with ratings and the films with
106:54 - actors so we've taken the film ID and
106:57 - the title and we've joined the two
106:59 - tables that we had here we had the film
107:01 - actors and the actors table so these are
107:04 - obviously you know we worked with these
107:05 - before but the film actors and the
107:07 - actors is the bridge to the film's table
107:10 - so now we can actually join them to
107:11 - create one so now we're going to have
107:13 - the main query here we're going to
107:14 - select fwf do asterisk here and we're
107:18 - going to say fwa film with actors.
107:21 - actors from the filmscore with now you
107:25 - can see uncore ratings you can see how
107:28 - we're referencing the actual CTE now and
107:30 - we're going to label this as fwf for the
107:32 - Alias then we're going to say left join
107:35 - films with actors which we're going to
107:38 - label fwa on
107:40 - fwf do Filmore ID if it equals the film
107:45 - with actors do
107:48 - Filmore now we're creating essentially
107:50 - what could be two tables two tables
107:53 - using the references we've created and
107:55 - now we're actually joining them together
107:57 - using this custom model now if we were
108:01 - to want to run this we now need to set
108:04 - up the dock container for DBT so let's
108:06 - go ahead and do that and then run this
108:08 - custom model just to see what it would
108:10 - look like okay so we are back inside of
108:13 - our Docker compost file let's go ahead
108:15 - and add the DBT service so underneath
108:17 - the elt script I'm going to say DBT
108:19 - which is going to be the name of our
108:21 - service the image I want to use is going
108:23 - to be GHC r.i / DBT Labs DBT postgress
108:31 - and then the version is
108:33 - 1.47 uh this as of right now at the time
108:36 - of this recording is most likely out of
108:37 - date but it's just what I use so I'm
108:40 - going to go ahead and run with it the
108:41 - command I'm going to run here uh I'm
108:43 - actually going to run a couple now I'm
108:45 - going to open this square brackets and
108:47 - the first one I'm going to do is run so
108:49 - this is technically DB DT run DBT run is
108:51 - the command you would use in order to
108:53 - run your models on top of the
108:55 - destination so I'm going to string a
108:57 - couple here so we're going to say run
108:59 - and then we're actually going to do
109:00 - we're going to point to the profiles
109:02 - directory so if I do double hyphen
109:05 - profiles hypher uh not period I need a
109:09 - comma we're going to say that is going
109:11 - to be in the uh root here and then if
109:15 - we're going to point to the project
109:18 - directory then we're going
109:21 - 2.2 SL DBT which is the name essentially
109:24 - going to be the name of our project
109:25 - directory here the one thing I also want
109:27 - to do uh well we can leave this out for
109:29 - now actually um so after that we're
109:33 - going to do networks and this will be
109:37 - altore Network so we can keep it on the
109:39 - same network here and now we're going to
109:40 - point some volumes here so at the
109:42 - volumes we have the actual project
109:44 - itself so you notice I said / DBT so
109:47 - here we're actually going to create
109:49 - another mapping to point Point 2/ DBT
109:51 - because if we remember correctly on
109:53 - Docker containers every directory here
109:55 - is pretty much going to be pointing to
109:57 - the directory inside of the actual
109:59 - Docker container and not your local
110:00 - machine so you have to map the docker
110:02 - container to look at where the actual
110:05 - project is going to be on your local
110:06 - machine so we're going to create that
110:08 - mapping and this is going to be/ custom
110:11 - postgress and then uh from there we're
110:14 - going to do the colon to create the
110:16 - mapping to The Container path and then
110:19 - the container path is simply just going
110:20 - to be/ DBT which we've pointed right
110:22 - here now we need to create another
110:24 - volume here uh let me add a space here
110:26 - because that requires a space uh we're
110:28 - going to point to the um the DBT profile
110:32 - that we did so we're going to need the
110:34 - squiggly Tilda we need squiggly slbt and
110:38 - we're going to point this over into the
110:40 - root which is where we've pointed if I
110:43 - could spell normally uh this is where
110:45 - the profiles directory was which is
110:46 - going to be in/ root so now we're
110:48 - mapping our local TBT folder with the
110:51 - profiles. emo file to the /root of the
110:55 - docker container hopefully that makes a
110:57 - bunch of sense cuz in my mind it kind of
110:59 - does so we're also going to say depends
111:00 - on the elore scripts we want DBT only to
111:03 - run because remember when this container
111:06 - starts we're running this command and if
111:08 - we run DBT run on top of the destination
111:11 - database without it having any data this
111:14 - will not work because has nothing to
111:15 - reference and it has nothing to write
111:17 - data on so we need this to make sure
111:19 - that it depends on the elt script and
111:21 - the elt script depends on the two
111:23 - databases so this is working down the
111:25 - line we've essentially created a uh work
111:27 - line here and then for the environments
111:29 - we need two here so we need DBT profile
111:32 - this one is going to be it's going to be
111:34 - default and then we have another one
111:37 - this one doesn't need a hyphen actually
111:38 - cuz we made that mistake last time and
111:41 - then we need DBT uncore Target and this
111:44 - one is going to be Dev so this is going
111:46 - to be everything we need for the DBT
111:49 - container now all we need to do go ahead
111:51 - and clear this I don't think compos down
111:56 - Docker compose down everything uh I
111:58 - think the volumes are still up but
111:59 - that's totally fine so now we're going
112:01 - to do Docker compose up did I type that
112:03 - correctly H
112:06 - C.I
112:08 - DBT lab
112:11 - DBT post press seven oh uh 1. 7 okay
112:18 - that's another one another typo there we
112:20 - go so we're going to go ahead and let
112:22 - that run here so elt script exited Code
112:24 - Zero so that's a good sign now the DBT
112:27 - one should pop up invalid not a project
112:29 - missing DBT project. file okay so
112:33 - invalid project directory project
112:35 - directory here custom post press maybe
112:40 - we do not need this let's go ahead and
112:43 - clear this let's do Docker down SLB and
112:48 - then so maybe
112:50 - custom oh I'm dumb uh it should have
112:53 - been an underscore you can see all the
112:56 - typos and everything really does make a
112:58 - difference here so let's run that again
113:00 - and that should map to the correct
113:02 - project now hopefully there we go so
113:05 - it's ran successfully I think unable to
113:07 - do partial parsing encounted an error
113:09 - error sources. runtime error
113:13 - sources. list of actors film actors R
113:16 - while parsing a block mapping uni code
113:19 - okay so there was an error in our
113:20 - sources file here somewhere that might
113:23 - have been it um 13 and 14 film under
113:27 - score actors okay let's do this one more
113:30 - time I think my cat may have done
113:33 - something with that so you can see there
113:36 - a lot of back and forth here scripts ran
113:38 - successfully okay it's taking a little
113:41 - bit longer which is totally fine here
113:43 - model fims was which was not found
113:45 - there's a lot of typas going on film
113:47 - ratings so if we go into Filmore rating
113:50 - reference films fims okay here's the
113:53 - typo let's talker compose down again so
113:57 - also uh if you're curious if I'm Docker
114:00 - composing down hyphen V I'm actually
114:02 - killing the volumes every single time
114:04 - just to make sure that everything is
114:06 - working even if we're starting straight
114:07 - from the beginning without any data
114:09 - persisting so I'm that's the only reason
114:11 - why I'm doing that um so now we Docker
114:13 - compos up again this should work unless
114:15 - I've made even more typos okay so we're
114:18 - doing that again there we go okay so now
114:20 - you can see that we've successfully run
114:23 - our models so we've created a couple
114:25 - models here the first one was the actors
114:27 - the first one was the film actors the F
114:29 - and the second or the third one was the
114:30 - films so these are all our references so
114:32 - we've successfully taken that in but now
114:34 - we've actually done the fourth one which
114:36 - is the model for the Filmore ratings so
114:40 - you can see that that's actually worked
114:41 - now now if we were to go and Docker
114:44 - exact hyphen it and then I think it was
114:47 - elt
114:48 - destination or postgress hyph 1 and then
114:52 - we do
114:53 - psql hyphen U postgress so then we go
114:56 - here we do c
114:58 - destination DB we connect there and then
115:02 - SLT so now you can see Filmore ratings
115:05 - which is the new table that we've
115:07 - written in DBT is actually written now
115:09 - if we select all from Filmore ratings
115:14 - and then with the semicolon you can see
115:17 - that if I move this over it's a a little
115:20 - big but you can see that our case has
115:22 - gone in here uh it's kind of cut off
115:24 - just because of the way that my terminal
115:26 - looks you can see everything is in there
115:27 - so we have our actors we have our film
115:30 - ID we have the release date the price
115:33 - but the most important part is the
115:35 - rating that we've created here so the
115:37 - case statement worked successfully so
115:39 - anything above a 4.5 was excellent
115:41 - anything below that between 4.0 and 4.4
115:45 - was good and you had no average one so
115:47 - you can see that we've actually created
115:48 - this custom model successfully uh along
115:51 - with the actor and the actor name so you
115:53 - had Inception with Leonardo DiCaprio it
115:55 - just looks a little weird because of the
115:57 - way that um if I were to do this here so
116:00 - let's do the same one so now you can see
116:02 - it it it looks a well it still looks a
116:03 - little weird but anyways you can see
116:05 - that it works so that is creating a
116:08 - simple model in DBT now let's look at
116:10 - some other Advanced options you can do
116:13 - inside of DBT to make your queries a
116:15 - little bit more Dynamic is the best way
116:18 - to put it okay so now that we've
116:20 - actually created the models I told you
116:21 - that we're going to get into some
116:22 - advanced stuff with DBT and that's why I
116:26 - mentioned the macros folder so you can
116:28 - think of macros as sort of like a
116:31 - reusable components of DBT where you may
116:35 - have some sort of queries that you want
116:38 - to reuse and is universal through pretty
116:41 - much all your models so what if you
116:42 - could just create a macro where you
116:44 - didn't have to rewrite the same thing
116:45 - every single time well you can
116:47 - absolutely do that with macros let's
116:48 - take the Filmore ratings file for
116:51 - example let's say we wanted to generate
116:53 - this every single time uh now that's
116:55 - probably not going to happen but let's
116:57 - say we wanted to do it and so let's just
117:00 - go ahead and actually take the whole
117:03 - file and cut it now inside of the macros
117:06 - folder what we're going to create is
117:08 - we're going to say Filmore ratings macro
117:13 - do SQL file everything's going to be a
117:15 - SQL file here now what we're going to do
117:16 - is we're going to start the file with
117:18 - curly bracket both sides are going to
117:20 - have a parenthesis so double parenthesis
117:23 - and inside that we're going to say macro
117:25 - generate uncore Filmore ratings uh and
117:29 - then we're going to parenthesis after
117:31 - now the end of the macro is also going
117:34 - to have curly brackets is also going to
117:35 - have double parentheses but inside the
117:38 - parentheses I'm sorry inside of the uh
117:40 - percent signs you're going to say n
117:42 - macro in one word now in between that is
117:44 - where your macro is going to go so I'm
117:46 - going to go ahead and paste what we just
117:48 - took from film ratings into the macro
117:50 - now Filmore ratings has nothing but if
117:53 - we were to go inside of film ratings now
117:56 - all we have to say is two early brackets
117:59 - and we're going to call the generate
118:00 - Filmore ratings macro so you can see
118:03 - that this is the model that we're
118:05 - actually writing and the macros never
118:07 - actually gets written but if I were to
118:09 - go back into our terminal here and let's
118:11 - close this and let's say Docker compos
118:13 - down I and V so let's kill the volumes
118:16 - here we're going to say Docker compose
118:17 - up and it's going to run through
118:18 - everything all over again so let's go
118:20 - ahead and let that go through really
118:22 - quickly DBT is going to start running
118:25 - and then there you can see we still have
118:27 - four models being written if we go back
118:30 - now over to the destination database uh
118:34 - SLC destination _ DB and then /dt the
118:39 - Filmore ratings table is still getting
118:41 - written and if I do select all from
118:46 - Filmore ratings you can see that we're
118:48 - still writing exactly what we wrote in
118:51 - Filmore ratings but we've only been
118:54 - using the macro so you can see that
118:56 - literally anything inside of the macro
118:58 - file in between this section here and
119:01 - here is going to be written exactly as
119:04 - shown so if I were to say let's I don't
119:06 - know let's create a a macro for this
119:09 - case which probably is more plausible
119:11 - than anything so if I wanted to say
119:14 - generate
119:16 - ratings macro SQL and we we wanted to do
119:19 - the same thing where we go uh let's just
119:22 - percent sign this generates ratings and
119:27 - then we're going to end macro right and
119:31 - then we paste this inside all we have to
119:34 - do now is then just say generate ratings
119:38 - call that and then we should be able to
119:40 - do the same thing uh if we do this
119:42 - Docker compose down B compose up let
119:46 - that run again yes it takes a little bit
119:48 - of time to run run it every single time
119:50 - but you know this is just what it comes
119:52 - with it be much harder is undefined
119:54 - under undefined generate underscore
119:57 - ratings it's not generate underscore
119:59 - ratings uh it's generate uncore ratings
120:02 - wondering if you can't call CT here so
120:04 - now you can see kind of how macros could
120:07 - work is it it allows you to have
120:09 - reusable queries throughout your whole
120:12 - DBT project essentially to just plug and
120:14 - play in and not have to rewrite the same
120:17 - thing every single time so if you have
120:19 - reusable queries that you can use go
120:21 - ahead and use macros because it's very
120:24 - very efficient in terms of not having to
120:27 - you know it it implores the dry method
120:30 - of software engineering where you know
120:31 - don't repeat yourself you don't have to
120:33 - repeat yourself every single time if you
120:35 - have the same queries over and over and
120:36 - over again so utilize macros when you
120:38 - can now let's move over into gingas
120:40 - which is another feature of DBT okay so
120:43 - this next section that I just mentioned
120:44 - with gingas gingas are essentially what
120:47 - allow you to create conditionals or add
120:49 - control structures to your SQL queries
120:52 - so for example that we're going to use
120:54 - is let's say we wanted to select a
120:57 - specific title but using a Ginga so we
120:59 - didn't have to actually put it within
121:01 - our actual SQL query that's what it
121:02 - allows us to do it could allow you to
121:04 - select you know do we want to use the
121:06 - prod Target instead of the dev Target
121:09 - inside of our DBT project then we could
121:11 - do that as well and we can change the
121:13 - way that queries are done again adding a
121:15 - control structure so let's go ahead and
121:17 - inside of our example folder we're going
121:19 - to create a file called specific mov SQL
121:23 - and inside of that is where we're going
121:24 - to start with our Ginger here so again
121:26 - starts with a curly brackets and then
121:28 - two percent signs and inside of that
121:30 - we're going to say sets so when you see
121:31 - set this is essentially what is starting
121:33 - our control structure here and we're
121:34 - going to set Filmore title to equal for
121:38 - this example we're going to say Dunkirk
121:39 - cuz I know that's going to be a movie
121:41 - inside of the demo or the um test
121:43 - database here so Dunkirk is going to be
121:44 - a movie that we're going to look for
121:46 - right now inside of that we're going to
121:47 - say select so this is the actual query
121:49 - itself select all from and we're going
121:51 - to reference the uh films table here
121:55 - where let's practice here where title
121:57 - equals and then we can say in the single
122:00 - quotes
122:01 - Filmore title so that is obviously the
122:04 - variable that we've set up here right
122:07 - that's going to be our Ginger here we're
122:08 - setting a control structure where we
122:10 - only want to select the title where
122:13 - dynamically the value we've set here is
122:15 - Dunkirk and obviously this can be
122:17 - anything we want it to be but so let's
122:18 - go ahead and save that and then run our
122:21 - Docker compose up here and let
122:23 - everything go when DBT runs we should
122:27 - have a fifth model that runs which is
122:29 - going to be our specific movie and so
122:31 - let's go ahead and let that run here
122:33 - there's going to be the fifth it's all
122:34 - pass very good let's run our destination
122:38 - database and SLC into it so
122:41 - destination
122:43 - db/dt and you can see specific movie is
122:46 - now a table inside of this database and
122:49 - so if I say select all from
122:52 - specific movie you can see Dunkirk as
122:55 - well as all of its information including
122:57 - the film ID the release date price
122:59 - rating and user rating is going to be in
123:01 - there because we've used the Ginga in
123:03 - there now it allows you to to pretty
123:05 - much do anything you want you know if we
123:08 - wanted to go into the macro here and
123:10 - create conditionals where we have an
123:12 - excellent condition a good condition or
123:14 - an average condition and we set those to
123:17 - be this number we can absolutely do that
123:19 - and allows us to do that outside of the
123:22 - query so that way we can know that up
123:24 - here we can set it to a different movie
123:27 - you know if we wanted to do Inception we
123:29 - we're only changing it Inception pretty
123:32 - sure that's how you spell it if it's not
123:33 - that's going to be very awkward but so
123:35 - let's do this Docker compose down V
123:38 - Docker compose up let's run this again
123:41 - and so when we run this we should see
123:43 - that the specific movie has then changed
123:46 - over to Inception instead of dunker
123:48 - there therefore showing that the ginger
123:50 - works and it it allows for a little bit
123:53 - more readability right because you can
123:55 - see that our film title we've set it to
123:56 - a variable we're not you know manually
123:58 - searching for a a movie here and then
124:02 - we're going to do select all from a
124:05 - specific movie and now you can see
124:07 - Inception is now there so again a very
124:11 - simple example of what a ginger is and
124:14 - kind of how it works but go ahead and
124:16 - play with it I implore you to kind of
124:18 - you know inter integrate gingas with
124:20 - macros to really really you know use the
124:22 - full power of macros and gingas together
124:25 - and I think it could really 10x your SQL
124:28 - queries and your custom models from
124:30 - within DBT and really just Excel that
124:33 - process of the whole data engineering SL
124:35 - data analytics process alog together
124:38 - okay so as I've mentioned before we're
124:40 - going to be adding a Cron job to this
124:43 - not ideal I guess you could say in the
124:45 - sense of that like it's not necessarily
124:48 - the best implementation I guess you
124:50 - could say but there are a lot of other
124:51 - people out there that use Crown jobs I'm
124:54 - going to show you the way you can
124:55 - Implement a crown job because again you
124:57 - don't want to manually be triggering
124:59 - these data syncs or this pipeline to be
125:01 - running on your own you kind of want
125:03 - some automation here so the first thing
125:04 - we're going to do is add the Cron job
125:06 - I'll show you how you can do that in
125:08 - this type of implementation obviously
125:10 - there's different ways to implement it
125:11 - but this will eventually evolve into US
125:13 - adding a tool called airf flow airflow
125:15 - is going to be an orchestration tool
125:16 - that easily allow us to kind of have
125:18 - more control than a Cron job over each
125:21 - individual task inside of our project to
125:23 - allow us to schedule it properly I think
125:25 - the best thing that you can have is more
125:26 - control outside of just scheduling when
125:28 - it runs and so let me go ahead and show
125:30 - you how the Cron job works first and
125:32 - then we'll move into airflow and other
125:34 - orchestration tools like that okay so
125:36 - for the Cron job the main file we got to
125:38 - go into is going to be the docker file
125:41 - now because we're primarily M working on
125:44 - the El script and we want to schedule
125:47 - that this is where we're going to run
125:48 - everything so just going to be some
125:50 - slight modifications here so the one
125:52 - thing we're going to do is we're going
125:53 - to add KRON to the end of the list of
125:56 - installs that we're going to do so we're
125:58 - going to run for postgress client as
126:00 - well as KRON underneath here now what
126:02 - we're going to do is we're going to
126:04 - create a new file called
126:06 - start.sh now this is going to be a bash
126:08 - script that we run so this sits in the
126:09 - root directory and we'll modify this
126:11 - later but above this copy script or
126:14 - above this copy command that we have
126:16 - we're going to add another one and this
126:18 - one is going to be start.sh uh and we're
126:20 - going to copy this over to the app
126:22 - directory of Docker so that Docker now
126:25 - has the start.sh file within its own
126:29 - container here the working directory is
126:31 - going to be the app so we're going to
126:33 - say work is going to be slapp here in
126:36 - between work directory and command this
126:38 - is where we're going to add the crown
126:39 - job now for this example we're going to
126:41 - run the script let's say every day at
126:44 - 3:00 a.m. so we're going to do a run
126:46 - command here and then we're going to
126:47 - Echo and then in quotes we're going to
126:49 - say zero space three and then three
126:52 - asterisks and I'm sure there's like you
126:54 - know sources online that can show you
126:56 - kind of like how to properly write a
126:59 - crown job for a specific time this is
127:00 - just the one that I have now we're going
127:02 - to say python SLA
127:05 - slore script pi and then outside of that
127:08 - we're going to do the divider here and
127:10 - then we're going to say cron tab now
127:12 - we're going to save this and then if we
127:14 - go back into
127:15 - start.sh this is where we're going to
127:17 - write the uh to how to actually start
127:21 - the Kon Damon so at the very top we're
127:23 - going to say cron and and that pretty
127:25 - much is what's going to run the KRON and
127:28 - have it run in the background and then
127:30 - python slapp
127:33 - _ script.py what this is going to do is
127:37 - we have the bash script and it's in our
127:40 - working directory we're going to run
127:42 - this command here when the docker
127:44 - container starts and now this bash
127:46 - script is going to run the pr in in the
127:49 - background and then run this every
127:50 - single time it's 3:00 a.m. and so we'll
127:52 - run the bat scripts at 3:00 a.m. every
127:54 - single time due to this Crown so again a
127:56 - very simple implementation of how the
127:58 - KRON is supposed to work obviously it's
128:00 - hard to demonstrate this because it's
128:01 - 6:50 p.m. right now so I can't really
128:03 - just wait here until 3: a.m. but now at
128:06 - 3:00 a.m. every single time this will
128:09 - run so yeah that's essentially how this
128:11 - Cron job is going to work now let's move
128:13 - into how airflow is going to play in the
128:16 - mix and how we can now orc rate this
128:18 - using a real open source tool that's out
128:21 - there in the wild all right so in this
128:22 - section we're going to be adding airf
128:24 - flow to our project and as I mentioned
128:26 - before airflow is an orchestration tool
128:29 - open source that allows you to
128:31 - essentially have a top- down view of
128:33 - every single task and say Hey I want my
128:35 - elt script to run first and then once
128:37 - this finishes and is
128:39 - successful and that's exactly what we're
128:41 - going to do here we're going to
128:42 - orchestrate all of our tools and all of
128:45 - the working pieces that we have right
128:46 - now to make sure that they work
128:48 - sequentially here and you know there's a
128:50 - lot of different use cases with airflow
128:52 - or other open source tools like Daxter
128:54 - or prefect but we're simply going to be
128:56 - using airflow as our orchestration tool
128:57 - to make sure that our pipeline is
128:59 - working the way we want it to and has
129:01 - some form of automation here all right
129:03 - so first thing we're going to do is
129:04 - going to create a new folder here we're
129:07 - going to name this airflow and inside of
129:09 - airflow we're going to have another
129:11 - folder called dags and a dag is simply a
129:14 - directed ayylic graph I think is how you
129:16 - pronounce the a part but so the dag is
129:19 - what we write in order to orchestrate
129:22 - all of our tasks together and also put
129:24 - in some of our Logic for each task so
129:27 - that's what we're going to be writing
129:28 - here in DBT we were writing uh models
129:31 - custom models in airflow we write dags
129:33 - or in any orchestration tool we're going
129:35 - to be writing a dag so that is another
129:37 - term for you to put into your brain as
129:40 - knowledge to know as a data engineer the
129:41 - other thing we're also going to create
129:43 - here is a file not inside of the dags
129:47 - folder so this is going to go into the
129:49 - root folder of airflow but this is going
129:51 - to be airflow. CFG I think we just need
129:54 - this for airflow specifically we don't
129:55 - need to put anything in it but if you
129:57 - needed to do anything with your
129:58 - configuration of airf flow this is going
130:00 - to be the file that you're going to put
130:01 - it in now that we have this structure
130:04 - put together let's go ahead and go into
130:06 - our Docker compos file and then add the
130:08 - airflow service this is going to be a
130:10 - little extensive because it is a big
130:12 - service so we're going to take our time
130:14 - with this one and really go through it
130:15 - all right so uh underneath the D B
130:18 - section we're going to go here and add a
130:20 - couple spaces because we're actually
130:22 - going to be adding quite a bit of new
130:24 - Services here that have to work with
130:26 - airf flow it would be a lot but I've
130:27 - narrowed it down to be very minimal here
130:29 - so we don't have to worry about that too
130:31 - much but the first thing we're actually
130:32 - going to add is going to be another
130:34 - postgress service and why are we adding
130:36 - another postgress service here well it's
130:38 - because airflow needs a database to
130:41 - store its metadata and its logs so this
130:44 - is the database that's going to go into
130:45 - it's going to be the same look here
130:48 - where we're going to say postgress and
130:49 - the latest image here right um networks
130:52 - we're going to tie into the
130:55 - elcore network and then for the
130:58 - environment let's do this so environment
131:00 - we're going to do postgress user is
131:03 - going to equal airflow here we have
131:07 - postgress password which is going to
131:09 - equal also airflow and then for the
131:12 - postgress DB this is going to be airflow
131:15 - so very simple there now fix this
131:18 - now that's the metadata service so you
131:21 - can name that whatever you want but I'm
131:22 - for Simplicity sake I'm going to keep it
131:24 - as postgress the next thing we need to
131:26 - do is going to be an init airflow
131:28 - service now for the init airflow service
131:31 - what this is primarily doing here is
131:33 - initializing the user and the database
131:37 - that we just created all together for
131:39 - airflow so we had we're going to add two
131:40 - more services after this but this is a
131:43 - very important service for us because
131:46 - without this we're not going to have the
131:47 - admin user and be able to create new
131:50 - connections and things like that so this
131:52 - is very important here now the image
131:54 - that we're going to add is going to be
131:56 - Apache so the Apache project open source
131:58 - projects airflow and then latest this is
132:00 - going to be the same for pretty much
132:02 - every single uh I think it's going to be
132:05 - the same for every single one but I
132:06 - could be wrong U anyway so image this is
132:08 - going to depend on postgress cuz we need
132:10 - that database to be initialize
132:13 - essentially before we initialize airflow
132:15 - because without it we're going to have
132:15 - some errors here networks again we're
132:17 - going to say elore Network for the
132:19 - environments we're going to say this is
132:21 - going to be a long one so airflow
132:23 - underscore uncore so twocor database _
132:28 - SQL underscore alemy _ con for
132:32 - connection is going to equal and this is
132:34 - going to be pointing to our postgress
132:36 - database here that we just created so
132:38 - postgress
132:40 - plusy cop G2 colon SL SL airflow for
132:45 - username and password Here at postgress
132:49 - SL airflow okay so that's that's a long
132:51 - one but that's going to be the
132:54 - environment there now last one we need
132:57 - is a command the command we're going to
132:59 - say uh less than or greater than sign
133:03 - and in we're going to say bash iph C in
133:06 - quotes airflow EB and nit to initialize
133:09 - the database for airflow and and string
133:13 - on another one here push it over here
133:15 - and say airflow users create so we're
133:19 - this is where we create the admin
133:20 - account username airflow password is
133:24 - going to be password let's just say
133:26 - first first name is going to be John
133:31 - last name is going to be do we want the
133:34 - rooll here to be admin and then the
133:36 - email we need to set let's say admin at
133:39 - ample cool all right so that's a pretty
133:41 - long one there already let's go down
133:43 - here into the next service that we need
133:46 - to create so that's our initialization
133:47 - of air flow now we're going to get into
133:49 - the actual like UI of airflow which is
133:51 - going to be the web server service let
133:53 - me add some spacing here the image we're
133:56 - actually going to create a Docker file
133:58 - for this so we need to do that first now
134:00 - in the root of the directory here we're
134:03 - going to say Docker file and then in
134:05 - here we're going to say from Apache SL
134:08 - airflow latest so the same thing that we
134:10 - did uh with the initialization of airf
134:13 - flow now we're going to say run tip
134:15 - install Apache air flow providers Docker
134:20 - now this is going to come in handy later
134:22 - on but we do need this in order to run
134:24 - this command here airflow has a bunch of
134:26 - different providers for you to access
134:27 - different Integrations Docker being one
134:29 - of them and there's going to be another
134:30 - couple ones that we're going to use
134:32 - later down the line but Docker is the
134:34 - one we need so now that we have this
134:36 - we're going to go back into our Docker
134:38 - compose file and then for the build
134:40 - section we're going to say build context
134:42 - just add the dot here the docker file
134:44 - we're going to point to Docker file uh
134:46 - just capitalize this part the US user
134:48 - here is going to be root because we want
134:50 - the root here and then it depends on the
134:52 - postgress service so we do not want
134:54 - airall to initialize until the metadata
134:57 - store essentially is is initialized
134:59 - networks we're going to point over to
135:01 - the elt network and this is going to be
135:02 - a special one here so extra hosts extra
135:05 - hosts is going to be an interesting one
135:07 - because we are running everything on
135:09 - Docker we actually need this for special
135:11 - reasons that'll come up later but so in
135:14 - double quotes we're going to say host.
135:16 - doer. internal and then we're actually
135:18 - going to point that to host Gateway okay
135:21 - so that points to inside Docker the host
135:24 - Gateway so that's going to come in handy
135:26 - later so now that we have that we're
135:27 - going to point to some volumes here now
135:29 - the volumes are going to be very
135:31 - important so we have our dags that we
135:34 - need to find So within that we're going
135:36 - to say/ airflow SL dags so now we have
135:40 - our dags folder now we need to map it so
135:42 - we're going to map it to the slop slair
135:45 - flow/ dags so now we have have the same
135:49 - directory of dags inside the opt folder
135:51 - and then it'll go to airflow dags so now
135:53 - we have that inside of our Docker
135:55 - container next one next one is going to
135:56 - be the elt script that we have because
135:58 - we need access to that
136:00 - so/ is the folder that we have and then
136:02 - we're going to say we're map that over
136:05 - again to SL opt SL airflow SL elt so now
136:09 - we have our own elt folder with the
136:12 - script and all of its contents which is
136:14 - just the bash script there now we need
136:15 - the Transformations because we're also
136:17 - going to be orchestrating not only the
136:20 - script but we're also going to be
136:21 - orchestrating DBT and we need access to
136:24 - those files so we're going to say SL
136:26 - custom hostress that is going to map
136:29 - over to SL opt SL DBT now we have the
136:32 - DBT folder in there we're going to need
136:34 - the profiles for DBT so we're going to
136:35 - say Tilda sl. DBT and then we're going
136:38 - to point that over to SL root
136:41 - sl. okay I hopefully this is all making
136:43 - sense here this one is going to be
136:46 - opening up essentially giving giv airf
136:48 - flow access to the essentially the
136:49 - docker Network so we this is going to be
136:52 - very important here so we're going to
136:52 - say /var run/ doer. sock we're giving
136:56 - access to the socket there SL we MPP get
136:58 - to SL SL run/ Docker okay cool now for
137:02 - the environment variables and this is
137:04 - going to get really long here so we're
137:05 - going to get environment variables here
137:07 - the first one is going to be load
137:09 - underscore x = n these are all just
137:12 - going to be needed for airflow exeggutor
137:15 - equals local and we're we're going to
137:17 - say airflow and we're mapping this back
137:20 - to the database here so underscore
137:22 - uncore database Alchemy con so I've uh
137:25 - just copy and paste this over actually
137:27 - cool so I've got that now one thing
137:29 - airflow does need is it's going to need
137:32 - a furnet key underscore fernet uncore
137:35 - key so that's only one underscore there
137:37 - and then we're going to say equals it
137:38 - essentially encrypts the username and
137:40 - password it's for security reason we're
137:42 - just going to do that because you know
137:44 - for the sake of this video so I've
137:46 - actually if we go to orchest air flow I
137:48 - have this command which I will also
137:50 - leave for you in the description but
137:52 - this is essentially going to generate
137:53 - our fernet key for us and what we're
137:56 - going to do is copy and paste that
137:57 - command into our terminal and it should
138:01 - spit out to us a key which we will then
138:04 - use and paste here so now that is our
138:06 - fit key that we're going to just leave
138:09 - in and will be used for encryption sake
138:12 - right and for security there's that uh
138:15 - now we're going to need airflow
138:17 - underscore uncore web server uncore
138:21 - uncore default user underscore username
138:27 - and that's just going to be airflow I'm
138:28 - actually going to copy this and then
138:30 - just change the last bit here to say
138:34 - oops say password and this was going to
138:38 - be password cool next environment
138:41 - variable here we're going to go airflow
138:44 - uncore 1core www uncore user underscore
138:50 - username equals airflow same thing copy
138:53 - and paste this password the last thing
138:56 - we need is going to be
138:59 - airflow web server
139:02 - underscore secretor key that's just
139:05 - going to be secret cool all right last
139:07 - bits here we're going to say ports so we
139:09 - need to expose the ports here uh this is
139:11 - going to sit on P 8080 so we're going to
139:14 - say 8080 here so that's how we're going
139:15 - to access the web server and the command
139:18 - we're going to run this web cool now the
139:21 - one thing that's going to save us some
139:23 - time here is we're going to copy and
139:25 - duplicate the web server portion because
139:28 - it's going to be the exact same but now
139:30 - instead of web server this is going to
139:32 - be the scheduler so it literally is
139:35 - going to be the exact same so I don't
139:38 - think we need to change anything no so
139:40 - the scheduler is a needed worker for
139:42 - airflow and so we absolutely need both
139:45 - of these one the web server to give us a
139:48 - UI to access and then we're also going
139:50 - to need the scheduler to schedule all of
139:53 - our jobs here and they're going to point
139:55 - to the same exact thing okay so that is
139:57 - going to be the setup for airflow now
139:59 - let's go into writing our first dag in
140:02 - order to actually orchestrate all of our
140:05 - tasks here okay so we're going to go
140:08 - into our dags folder here and we're
140:10 - going to create a new folder and in that
140:12 - folder is going to be
140:14 - eltor dag dopy so we're going to be
140:16 - writing some python now the first things
140:18 - we're going to import are going to be
140:20 - from date time import date time and time
140:24 - Delta so we're going to be using some
140:26 - time values here and the next one is
140:29 - going to be obviously air flow so
140:31 - airflow uh from air flow we're going to
140:33 - import dag this is going to be a default
140:35 - one that we're going to need because uh
140:38 - obviously we need to type our dag from
140:41 - Docker do types we're going to import
140:44 - Mount this is going to come in handy for
140:45 - us later because we're orchestrating
140:48 - Docker services or Docker containers you
140:50 - need some specific Docker types and then
140:52 - you we also going to need a specific
140:54 - Docker operator that comes from airflow
140:56 - in order to actually orchestrate them uh
140:58 - the next thing we're going to need is
140:59 - going to be uh from airflow. operators.
141:03 - python uncore operator we're going to
141:06 - import the python
141:09 - operator so this is going to be very
141:11 - very crucial because our elt script is a
141:15 - python script we need the python
141:16 - operator in order to actually add that
141:18 - in there okay so we're actually going to
141:21 - copy and paste this down here as well
141:23 - because in the same operator package
141:25 - we're going to need the bash operator
141:27 - we're going to change this to bash
141:29 - operator so that way we can actually
141:32 - interact with the bash here and and do a
141:34 - couple commands there last one we're
141:36 - going to do it's going to be from
141:38 - airflow actually should be copy copying
141:41 - this again so from there we're going to
141:43 - say Docker underscore or I think it's
141:46 - just docker here yeah just Docker and
141:49 - then the docker operator cool so these
141:52 - are all the packages oh actually we're
141:53 - going to need one more sub process silly
141:56 - me okay so those are going to be all the
141:58 - packages that we need here now let's go
142:00 - into some default arguments here so
142:02 - we're going to see default uncore args
142:04 - um this is just couple things so the
142:06 - owner of this whole project here is
142:08 - going to be airflow the next one is
142:10 - going to be depends on past so do we
142:13 - want this depends underscore onore past
142:17 - we're going to say false on this so it
142:19 - doesn't email on failure these are just
142:21 - some things that are built into airflow
142:24 - and we are setting some default values
142:26 - here default arguments we're going to
142:27 - say false and then
142:30 - email onore
142:33 - retry I think these should be curly
142:35 - brackets not square brackets I screwed
142:37 - up on that one cool all right now we're
142:41 - going to write a function here Define
142:43 - runor _ scripts so this is going to be a
142:47 - function we use in order to actually run
142:49 - the script now we're going to need the
142:50 - script path obviously script scriptor
142:53 - path we're going to set this variable
142:55 - here and this is going to be pointed to
142:58 - where we've set it inside of the docker
143:00 - container so if you remember correctly
143:03 - this path right here so we have opt airf
143:06 - flow/ elt so we need to point to that
143:10 - and not where it's pointed to locally
143:12 - for us so we're going to say/ opt slair
143:15 - flow sl/
143:17 - _
143:19 - script. Pi so now we're pointed directly
143:23 - to the path of the elt script from
143:25 - within the docker container so what we
143:27 - can do now is we're going to say
143:30 - result equals subprocess do run and this
143:33 - is going to be you know we we've had
143:34 - this command inside of actual batch
143:37 - script itself so we're going to say
143:38 - python we're going to run Python and
143:40 - then we're going to use this point it to
143:42 - the script path now after this we're
143:44 - going to add a comma and we're going to
143:46 - say capture output equals true so we
143:49 - want to capture the output there and the
143:51 - text equals true okay now after that
143:54 - we're going to add a control structure
143:56 - here so if
143:57 - results. return code does not equal
144:01 - doesn't equal zero that means we failed
144:03 - something so we need to raise an
144:04 - exception and in that we're going to say
144:06 - script failed with error and then we're
144:10 - going okay and then else we print the
144:14 - result output cool now now let's
144:17 - actually put this into a dag so we're
144:19 - going to say dag equals and then point
144:22 - it to the dag object that we just
144:24 - imported add a parenthesis and this is
144:26 - where we add in all the information so
144:28 - the first one is going to be the name so
144:29 - we're going to name it elore andore DBT
144:33 - so that's going to be the name of our
144:34 - dag now uh the default arguments this is
144:37 - where it comes into play here so we're
144:39 - going to say default arguments equals
144:40 - the default arguments that we've
144:41 - specified the description of this dag is
144:45 - going to be an LT workflow with DBT
144:50 - rhyming accidentally the start date
144:52 - start underscore date is going to be
144:55 - date time so that's where we're going to
144:56 - use date time here and we're going to
144:58 - say
144:58 - 2023 uh it the time of this recording is
145:01 - currently October 28th so that's going
145:04 - to be our start time there catch up is
145:06 - going to be another one catch up equals
145:09 - pulse we're we're not going to catch up
145:11 - all right oh I didn't close it I'm done
145:13 - so now we're going to go into actually
145:15 - writing out the tasks so you can name
145:16 - name a task whatever you want the first
145:19 - thing I'm going to do or the easiest way
145:21 - to do it for me is going to say T1 for
145:23 - task one that's just the way I like to
145:25 - do it and we're going to use the python
145:26 - operator because the first task we want
145:28 - is the elt script and because it is a
145:30 - python script we need the python
145:32 - operator now let's do the same thing
145:33 - where we initiated the dag here where
145:35 - we're going to add parentheses the task
145:37 - ID is going to be task ID uh we're going
145:41 - to tag this over and you know label this
145:44 - as runor elt script so that we can
145:47 - identify this properly we're going to
145:49 - say python cable equals the Run elt
145:52 - script function that we just created and
145:54 - then the dag is going to equal dag okay
145:57 - so very very simple we've we've given it
145:59 - an identifier we've added the python
146:02 - function here in order for us to
146:04 - actually run the elt script and then
146:06 - we've attached it to the dag that we've
146:08 - created all right the next one is going
146:10 - to be task two which is going to be DBT
146:13 - and obviously for DBT we've added the
146:16 - docker operator so let's go ahead and
146:18 - add all the necessary information for
146:21 - the docker operator the same thing is
146:23 - going to be here so task uncore ID right
146:26 - uh we're going to name this the same one
146:28 - is going to be
146:29 - DBT run very simple there after task ID
146:33 - we're going to need to attach an image
146:35 - to it and so um I we do have a DBT
146:39 - container but we're actually going to
146:41 - remove it and replace it with this so
146:43 - we're going to do the same information
146:45 - that we had in the the docker or the DBT
146:48 - container so we're going to say GHC r.i
146:52 - DBT lab DBT hen postgress for the
146:56 - postgress adapter and then point it
146:58 - directly to
146:59 - 1.4.7 all right and the command we're
147:01 - going to run is going to be the exact
147:03 - same so you can see kind of like the the
147:06 - similarities here uh so we're going to
147:08 - say that we're going to point it to the
147:12 - profiles directory and the profiles
147:14 - directory is going to be SL root the
147:17 - project directory is going to be/ DT and
147:21 - then that should be it the next one for
147:24 - this one is auto remove so Auto remove
147:27 - is do we want to automatically remove
147:30 - the container once it's finished and
147:32 - we're going to say true because we don't
147:34 - want it open all the time so Docker URL
147:38 - is going to be the next one and this
147:39 - we're going to be pointing directly to
147:41 - the the socket that we open so Unix sl/
147:44 - bar/ run/ Doer
147:47 - sock right it can identify where the
147:49 - docker container is now the network mode
147:53 - we're going to label as bridge and if
147:56 - you remember the docker or the network
147:58 - driver for the docker Network that we
148:00 - had was a bridge so that is going to be
148:02 - our mode here and then mounts so mounts
148:04 - is going to be where we imported the
148:06 - type of mounts this is where we're
148:08 - actually doing the same thing that we
148:10 - see here in the DBT service where we've
148:14 - mounted these two volumes here and we
148:16 - want to mount them because we're
148:18 - actually doing exactly what we're doing
148:20 - here but instead we're doing it inside
148:23 - of airflow we're creating this Docker
148:26 - container from scratch using the same
148:28 - information but that way airflow has
148:31 - sight of okay well this is exactly what
148:34 - I'm orchestrating let me go ahead and
148:36 - create this and then run it so the first
148:38 - thing we're going to do is say Mount and
148:39 - then inside of that is going to be
148:41 - Source the source of this is going to be
148:42 - the local path of my actual custom
148:47 - postgress so I'm going to say users SL
148:50 - my name is Jus Chow slev development and
148:53 - this is going to be separate and a very
148:55 - different uh depending on what you're
148:57 - doing here so the inside of development
149:00 - is just the elt folder and then we're
149:02 - going to go into
149:04 - custom press is where my source is now
149:08 - after this we're going to say Target
149:12 - equals slash in single quotes SL DBT
149:16 - well that's
149:17 - essentially mapping the source to the/
149:19 - DVT folder and we're going to say type
149:22 - equals bind okay now what we're going to
149:25 - do is we're going to copy this and we're
149:27 - going to add a comma here now this is
149:29 - going to be shorter here so I can get
149:32 - rid of pretty much everything here and
149:34 - then we're going to say DBT because this
149:36 - is our profile and then instead of
149:39 - Target being SL DBT we want this to be
149:42 - root and the type is going to remain
149:43 - bind the last thing we need after the
149:46 - mount is going to tag it to the dag so
149:48 - dag dag and there we go that's pretty
149:53 - much writing both of our tasks the last
149:54 - thing we need to do here is create the
149:57 - order in which we want it to run and so
149:59 - at the very bottom here we're going to
150:01 - say T1 2 greater than signs and then T2
150:04 - so this means that T1 takes priority
150:07 - over T2 and that's the order that we
150:09 - want it to run in okay so let's go ahead
150:11 - and make sure to save this it looks like
150:13 - we have that's fine we don't have to
150:14 - worry about this so that is pretty much
150:17 - writing our dag here let's go over it
150:19 - one more time we've created some default
150:21 - arguments here that airflow needs inside
150:23 - of the dag we've created our python
150:26 - function in order to correctly identify
150:29 - where the script is and then we're going
150:30 - to go ahead and run it with some error
150:32 - checks here we've created the actual dag
150:35 - itself and named it with the default
150:37 - arguments description and then the start
150:39 - time we've created the first task here
150:41 - using the python operator giving it an
150:43 - identifier pointed the function we
150:45 - created to it so that's what we know to
150:47 - run and then we've attached it to the
150:50 - dag we created and then this is the
150:52 - second operator or the second task here
150:54 - using the docker operator and
150:56 - essentially creating DBT in a new
150:59 - container and then running and mounting
151:01 - everything we need to correctly have you
151:04 - know the DBT project run on top of the
151:07 - destination once the first task has run
151:09 - and then we've created the order of
151:12 - operations here which we wanted to run
151:15 - so hopefully this all makes sense and
151:16 - this next one we're going to go ahead
151:17 - and run it and show you what the web
151:19 - server looks like and then make sure
151:21 - that the dag actually works so let's get
151:23 - into that okay so we have our dag now
151:25 - the one thing to note is that we are
151:27 - actually running this script here and
151:29 - then creating a new Docker container
151:30 - here for TBT so we can actually go back
151:32 - into our Docker compos file and we're
151:35 - going to comment out the DBT as well as
151:38 - the elt script containers because we do
151:40 - not need to create them anymore because
151:42 - we have airf flow doing it we don't need
151:44 - to have them anymore so let's go ahead
151:45 - and comment those out and what we're
151:47 - actually going to do is we're going to
151:49 - do Docker compose up and run all these
151:52 - containers so I forgot to do one thing
151:54 - so remove the port section and then the
151:56 - command should be scheduler not web
151:58 - server that's my bad so let's do Docker
152:01 - compose up again ports must
152:06 - exist there we go okay just some weird
152:09 - issue there so this is airflow
152:11 - initializing everything everything seems
152:13 - to be going into plan here and so now we
152:16 - can do exit with Code Zero we should see
152:19 - the scheduler and the web server pop up
152:21 - in just a second here oh you know what
152:24 - okay so now that we have everything
152:25 - going let's go ahead and create all of
152:28 - our Docker compos files so but the first
152:31 - thing we have to do is actually run the
152:33 - init airflow container first because
152:35 - it's very important so we're going to
152:36 - say Docker compose up init hyphen
152:38 - airflow and we're going to actually say
152:40 - iph d in order to do that first so what
152:42 - that's going to do is going to start the
152:44 - postgress and it's going to start the
152:46 - ini initialize airflow containers first
152:48 - and if we go to Docker desktop here you
152:49 - can see that they they're already
152:51 - running we actually need those to be
152:53 - running first before we enable anything
152:56 - else because without those the web
152:57 - server and the scheduler will not work
152:59 - so now what we can do is we're going to
153:01 - say Docker compose up and that's going
153:02 - to create the rest of the docker
153:04 - containers that we have and so let's go
153:06 - ahead and wait for that now we can see
153:08 - that the web server is now open so what
153:10 - I'm going to do here is going to do open
153:12 - a new window and then we're going to go
153:13 - to Local Host and remember we said 80 80
153:17 - so if we go to Local Host 8080 we are
153:18 - now in the airflow UI so airflow
153:23 - password was the username and password
153:25 - that we enabled here we're going to say
153:27 - no here no but you can see we have an
153:30 - issue opt airflow dags
153:33 - _
153:35 - D.P a broken dag so that's fine I think
153:38 - what we probably forgot to do was
153:42 - install the python operators so let's go
153:46 - ahead and debug this really quickly a
153:49 - nothing like a little bit of problem
153:51 - right so let's just make sure op airflow
153:54 - do oper python oper oh so this should be
153:58 - an operator not operators now let's go
154:02 - ahead and go back and kill all of these
154:06 - again so Docker compose
154:09 - down B we're going to say the same thing
154:12 - so we're going to say Docker Air B let
154:16 - that run for a second and then kner goes
154:19 - up okay let's go back to our web server
154:21 - here should load up in just a second go
154:25 - airflow
154:27 - password in I may have tried a little
154:30 - too soon there we go uh we're still
154:32 - running into an error here docker's the
154:36 - operator I think I probably did this
154:37 - let's look at the docker file here tip
154:41 - install Apache High airflow High
154:47 - dock okay so I found the issue it was
154:50 - from the actual package here so I wasn't
154:52 - doing it properly on the docker file
154:54 - here Apache airflow providers Docker so
154:56 - this should actually be airflow.
154:58 - providers. doer uh and then operators.
155:02 - doer so that should be the import name
155:06 - uh that's why we were getting the issue
155:07 - here uh so we're going to say Docker
155:09 - composed down B do the same thing here
155:12 - end then airflow wait a couple seconds
155:15 - then docker compos up in a second now
155:19 - hopefully this should fix the issue
155:21 - because I had another just like simple
155:24 - typo so if we go back now crash airflow
155:28 - password still the same issue oh I had
155:30 - it twice it's another typo okay so this
155:33 - shouldn't be operators this should be
155:35 - providers A okay
155:39 - compose down I B doer initialize it
155:43 - again doer compose up again just some
155:46 - simple simple typos that are screwing it
155:51 - all up there we go now now that I
155:54 - haven't typoed anything you can see that
155:56 - we have the elt and DBT dag here we have
155:59 - the owner being airflow we have all the
156:01 - runs um so you have failed running
156:04 - success cued you have a schedule your
156:06 - last run next run which we pointed out
156:08 - would be today but we're going to go
156:10 - ahead and manually run this so what
156:12 - we're going to do is actually going to
156:13 - trigger the dag and what we're going to
156:14 - see here is we're going to see it run so
156:16 - we see one is one is uh still cued so we
156:19 - have one successful and I believe we
156:21 - should be waiting for one more to go
156:23 - believe we're still waiting for one more
156:24 - to go yeah okay so we see one has failed
156:26 - now let's go ahead and check why um so
156:28 - the dbt1 failed DBT was not running at
156:31 - the state so we can check the logs here
156:33 - now remember that we created that DBT or
156:35 - sorry that postgress service for the
156:37 - airflow and so that is because of this
156:41 - so we needed the actual postgress
156:43 - database for this reason so we can
156:44 - actually get some logs here if you
156:46 - didn't have it it wouldn't we wouldn't
156:47 - get any logs here so it's actually
156:49 - erroring out because custom post. myc
156:52 - DBT model depends on a node name my
156:54 - first DBT model which was not found
156:57 - which is interesting because this is not
156:59 - what we had in mind here very
157:02 - interesting so it looks like the script
157:04 - worked but the DBT section did not so
157:07 - we're going to have to take a look at
157:09 - why and it has to be something in here
157:11 - right so the DVT run profiles directory
157:14 - profiles directory is going be /root
157:17 - project directory is going to be/ DVD if
157:19 - we check the docker desktop we go into
157:22 - web server go into files here um and
157:26 - then I believe so root should contain
157:29 - our DBT folder with the profiles. yl
157:32 - folder and we can actually see that
157:34 - custom poost press has exactly the
157:38 - information we're looking for so that's
157:40 - roots that is fine the other one that we
157:43 - should have airflow yeah EGS no air
157:47 - where did we put the actual put in/ DBT
157:51 - so let's go back this should be in a DBT
157:54 - but it is not SL op /d and I believe our
157:57 - models should be in here okay so I think
158:00 - we're pointing it to the wrong area here
158:02 - so SL opt SL DBT let's go ahead and see
158:06 - if that changed everything cuz we didn't
158:08 - create the Ste section there so let's go
158:12 - ahead and restart this so we're going to
158:14 - say
158:16 - airflow wait a
158:18 - second okay let's go back to the UI
158:22 - password okay this is still going let's
158:24 - go ahead and Trigger it okay I think
158:27 - we're still waiting on one one's running
158:29 - right now which is a good thing let's
158:31 - hope it doesn't fail fails now let's go
158:33 - ahead and Che the logs here here invalid
158:35 - project directory not a project project
158:38 - AML okay so that's a better sign which
158:41 - means that we just need to change this
158:44 - now SL
158:46 - op SL now I believe that when we run
158:49 - this it should work let's go ahead and
158:51 - kill this same
158:54 - thing close up should be good reset go
158:59 - Ahad and run it so looks like the elt1
159:02 - ran successfully and then we're just
159:04 - going to wait for the second one to pass
159:06 - hopefully it does and it's still failing
159:09 - why is it still pointing to the other
159:10 - models I don't know how this happened
159:12 - but inside of the example folder there
159:14 - was a my second DBT model sequel file
159:17 - that was throwing the error so I just
159:19 - deleted it so now let's try and see if I
159:24 - don't know how that got in there I I
159:25 - don't even remember if I deleted it or
159:27 - not but obviously while we were testing
159:29 - it manually it wasn't throwing an error
159:31 - but now airflow decides to throw the air
159:33 - let's go ahead and see if that's going
159:35 - to fix the issue I'm pretty sure it will
159:38 - very just like didn't even think about
159:40 - it kind of thing but again like I'm
159:42 - leaving all these mistakes in there not
159:43 - because it's like obviously doesn't look
159:45 - that great but I do believe that these
159:47 - errors are just a part of Engineering in
159:50 - general and so I think it's good to put
159:51 - some light on it and just you know not
159:53 - every course or not every lesson is
159:55 - going to be fully fledged I think it's
159:56 - better to just leave that stuff in there
159:58 - because you know we all go through
159:59 - mistakes stuff like this happens where
160:00 - it just you know there was one file that
160:02 - was throwing everything off or something
160:05 - like that now let's go ahead and see if
160:06 - I can actually get in here so I can
160:08 - refresh and I'm in so let me go ahead
160:10 - and hit start on the dag and let's see
160:14 - what we get here so it looks like our
160:15 - batch script worked now we're just
160:17 - waiting for airflow let's hope for a
160:19 - number two on the green I think we're
160:21 - still running cool that was it that was
160:23 - literally it so now you can see we have
160:25 - two on the success part so we have our
160:27 - DBT and our elt script that ran
160:29 - successfully so what we can do now is we
160:32 - can go into our destination post Crest
160:35 - and do the
160:36 - destination
160:38 - DB and we're going to say /dt and you
160:41 - can see all the models and everything
160:43 - that we ran are now inside of the
160:45 - destination
160:46 - and to prove that I can say select all
160:48 - from uh let's say specific movie again
160:51 - and you can see that the Inception
160:53 - selection is still there I can say
160:55 - select all from users and everything
160:59 - should be in there so that is
161:01 - successfully adding airflow with some
161:04 - minor hiccups but obviously you can see
161:05 - here that our python operator and our
161:07 - Docker operator are in play here and
161:10 - working it does take a little bit of
161:12 - work to make sure that everything is
161:14 - moving because they yes there is a lot
161:16 - of moving Parts here but in this next
161:18 - section with the elt script we're making
161:19 - it a lot easier because we're removing
161:22 - the need for the elt script we're
161:24 - actually adding in another open source
161:26 - tool that I happen to work for called
161:28 - air bites to be able to add more
161:30 - Integrations and more data sources more
161:33 - destinations with ease so let's look at
161:36 - how that would work with orchestration
161:38 - and everything like that all put
161:40 - together now all right so for this next
161:42 - and final section like I mentioned we're
161:44 - going to be implementing an open-source
161:47 - data integration tool called airite
161:49 - which I just so happen to work for
161:50 - airite we are pretty much a data
161:52 - Movement platform like I just said so
161:54 - you have a bunch of sources like we have
161:55 - our source database and we have our
161:57 - destination database both in postgress
162:00 - now imagine you have a couple other
162:02 - sources that aren't just postgress but
162:04 - you also have places like Salesforce or
162:06 - LinkedIn Facebook marketing places like
162:09 - that where you need data to come out of
162:12 - and then move them into you know your
162:14 - destination postest data base or you may
162:16 - have some cloud data warehouses things
162:18 - like that airite is the platform to
162:20 - easily sync all of them together without
162:23 - having to manage any of these manual
162:25 - scripts like the one we just made use a
162:27 - dump file to go from postgress to
162:28 - postgress uh it would be much more
162:30 - complicated if we were doing that from
162:32 - you know a different source to a
162:33 - different destination and so I'm going
162:34 - to show you how easy it is to get air
162:36 - bite into your project and it's open
162:38 - source like I said and utilizes Docker
162:40 - so it's pretty easy to hook up to the
162:41 - rest of our application and then get
162:43 - airf flow hooked up as well so let's go
162:44 - ahead and do that so I am on the air
162:46 - bite repo right now which you can just
162:48 - go on GitHub and all we're going to do
162:50 - pretty simply is just copy this uh what
162:53 - we're going to do is we're going to go
162:54 - back into our terminal here we're going
162:57 - to stop all of the docker containers and
162:59 - then what we're going to do is get clone
163:02 - and then we're going to clone air bite
163:04 - into our repo here so that's going to
163:06 - take a little bit the big open source
163:08 - package and then if you want it to read
163:09 - the read me on how to get started and
163:10 - things like that all of it is down here
163:13 - but you can see that literally like you
163:14 - can use this project for free and that's
163:16 - the beauty of Open Source right so now
163:19 - if we hit LS you can see we've created
163:21 - an airite directory here and if we open
163:23 - our project here on vs code you can see
163:26 - that airite is now successfully inside
163:28 - of the file directory along with
163:29 - everything else so now let's hook up
163:31 - everything together to make sure that it
163:33 - works in harmony so before we run
163:35 - anything with air bite we actually got
163:36 - to set up a couple things so we do not
163:38 - need to touch this Docker compos file
163:39 - anymore that is completely all set and
163:42 - ready to go if we go into this Docker
163:44 - file you can see that obviously we've
163:46 - installed the docker provider we're
163:48 - actually going to install the air bite
163:50 - provider as well as a couple other
163:51 - things uh yes like I mentioned that
163:53 - there are a couple other Integrations
163:55 - with airf flow and air bite is one of
163:57 - them with open source it's a little
163:58 - different but we do use the HTTP
164:01 - provider as well as the air bite
164:03 - provider so we're going to change this
164:04 - command up a little bit everything's
164:06 - going to stay the same but when we run
164:07 - pip install the provider's docker we're
164:09 - going to add a little uh Slash here
164:13 - we're going to say and and tip
164:16 - install Apache
164:19 - airflow providers and then we're going
164:22 - to say HTTP we're going add another
164:24 - backs slash here and we're going to add
164:25 - another and and and then this is going
164:28 - to be pip install Apache airflow
164:31 - providers air bite so now we're
164:34 - installing three different packages that
164:37 - we need uh I mean yes we do we do need
164:39 - the docker one still but so yes we we're
164:40 - going to be using airflow for Docker
164:43 - HTTP and air bite works anonymously
164:45 - together HTTP provider is a dependency
164:48 - for the air bite provider so now we're
164:50 - going to save that and I'm actually
164:51 - going to change up this
164:54 - start.sh file so it's going to make it a
164:56 - little bit easier to run the whole
164:58 - project Al together we don't have to
165:00 - keep waiting for the init air flow to
165:02 - start and then we can kind of structure
165:05 - the way that all these Zer containers
165:07 - are running with this bash script so let
165:09 - me go ahead and do that right now okay
165:11 - so we're going to actually delete all
165:12 - this stuff that's in here right now the
165:14 - first thing that we want to do is we
165:16 - want to do Docker compose up and then we
165:18 - want to do init airflow now that's going
165:20 - to obviously do the initialization of
165:23 - air flow after that we're going to wait
165:25 - 5 seconds so we're going to sleep the
165:27 - command for 5 seconds and then next
165:29 - we're going to say Docker compose up I
165:31 - and D to run it in the background
165:32 - because if we don't run in the
165:33 - background we can't really um continue
165:36 - to see air bite come through so after
165:37 - that we're going to sleep for another 5
165:39 - seconds after that we're going to change
165:41 - directories into the air bite directory
165:43 - because if we see it here you can well
165:45 - you don't you won't see it now but there
165:47 - is a Docker compos file that needs to be
165:49 - downloaded here and that's exactly what
165:51 - we're going to do so we're going to CD
165:52 - into air bite and we're going to do a
165:53 - little check here because the way that
165:56 - we run it is there's a run platform uh
165:59 - shell script that we need to run with
166:01 - air bite and that is the initialization
166:04 - where it downloads all the correct files
166:06 - that we need to in order to spin up all
166:07 - the docker containers so we're going to
166:09 - say if and we're going to look for the
166:10 - file here so we're going to say uh
166:13 - hyphen f and then we're looking for
166:14 - docker
166:15 - compose yl that's the file we're looking
166:18 - for if that exists then we're going to
166:22 - uh run Docker compos up hyphen D if it
166:26 - doesn't exist we're going to run slash
166:29 - run hyphen ab
166:33 - platform.sh and then we're going to
166:34 - finish okay so very simple script here
166:37 - uh we're also going to create another
166:39 - one that's going to say stop. sh so this
166:42 - one is going to say Docker compose down
166:44 - I hyphen V and then we're going to uh I
166:48 - think we're going to sleep for 5 seconds
166:49 - we're going to CD into air bite and and
166:53 - then do Docker compose down so that is
166:56 - going to be the start and stop for this
166:59 - to make this easier to run every single
167:02 - container so now what we're going to do
167:04 - we're going to go into our dag and then
167:06 - change up everything because we no
167:09 - longer need this elt script obviously
167:11 - going to still need the docker operator
167:13 - but the python operator we don't need it
167:15 - anymore since we're going to be using
167:16 - air bite to do it so let's go ahead and
167:19 - hook that up right now all right so like
167:22 - I said we we actually didn't end up
167:23 - using the bash operator and we don't
167:25 - need the python operator anymore so we
167:26 - can remove this um let's go ahead and
167:28 - remove the time Delta as well because we
167:31 - do not need that right now so what we're
167:32 - going to do is also remove this function
167:36 - here and then this is what we're going
167:38 - to start doing our actually I'm going to
167:40 - leave this for now so right here above
167:41 - the default ARS we're going to create a
167:43 - Conor ID a variable and this is going to
167:46 - be a string so this will make sense so a
167:49 - connection ID is what this stands for
167:51 - within airite we have a connection ID
167:53 - and a workspace ID and those are just
167:55 - identifiers for your specific instance
167:57 - of airite let's say but more
167:59 - specifically if we're looking at the
168:01 - source and the destination that we
168:02 - currently have the both of the postgress
168:05 - databases connecting those together
168:06 - obviously create a connection and for
168:08 - airite when we connect the to we have a
168:10 - connector and that connector has that
168:12 - connection ID so that's exactly what
168:13 - we're going to use variable for is to
168:16 - grab that and then input it here into
168:18 - the dag to make sure that we're
168:19 - orchestrating the correct connector
168:21 - right now and so here what we're going
168:22 - to do is we're actually going to go back
168:24 - up here we're going to say from airflow
168:26 - uh the first thing we're actually going
168:27 - to do is from utils and dates so we're
168:30 - going to use the dates util from airflow
168:33 - we're going to import days uncore ago
168:35 - under that we're going to say from
168:37 - airflow uh. providers do air. operators.
168:42 - air we're going to import the airite
168:44 - trigger sync operator so this is going
168:48 - to be the module that we're using to
168:50 - trigger syncs on the connection that we
168:52 - create with from within airbag now we're
168:55 - going to replace that replace T1 with
168:58 - this operator here let's go ahead and
168:59 - replace it and now the first thing we're
169:01 - going to do is change the task ID so
169:03 - this is uh oh whoops I replaced the
169:05 - wrong one uh this is the python operator
169:08 - we're trying to replace so the first
169:09 - thing we're going to replace is the task
169:11 - ID so this one is going to be the Airy
169:14 - postgress _ postgress task ID and uh we
169:18 - no longer have a python callable here
169:20 - but the first thing we're going to do is
169:22 - aircore Conor ID which is an argument
169:25 - that we need which obviously we've
169:27 - provided which is going to be airite so
169:29 - this will make more sense once we're
169:31 - inside of webflow but we need to make a
169:32 - connection inside airf flow and we're
169:34 - going to name that air bite so we're
169:36 - going to add that the connection ID is
169:38 - what we provided so this will point to
169:40 - the con ID variable that we had right
169:43 - make sure that my com here underneath
169:45 - that is asynchronous so if we wanted to
169:47 - asynchronously do jobs asynchronous I
169:51 - think is how I spelled it wait did I
169:52 - spell that right asynchronous false so
169:55 - this is not going to be asynchronous we
169:56 - would have to add another operator but
169:58 - we're not doing any jobs asynchronously
169:59 - we're only triggering one job so we're
170:02 - going to leave it at that we're going to
170:03 - leave the timeout to 3600 we're going to
170:05 - say waitor seconds uh we're going to
170:08 - wait 3 seconds and then the dag
170:10 - obviously we're we're putting back into
170:11 - dck so you can see pretty simple in
170:13 - terms of what it's accept here the main
170:15 - thing here to note is going to be these
170:18 - two properties right here so the airb
170:20 - con ID which we're going to find within
170:22 - airflow and then the connection ID is
170:24 - going to be what's important to us here
170:26 - so we're actually going to save that and
170:27 - nothing else changes so we've we've
170:29 - successfully put an airbyte within the
170:31 - dag um the one thing we need to do here
170:33 - is we do need to add our connection ID
170:36 - which we have not gotten yet there is a
170:37 - little bit of a trick here that we're
170:39 - going to have to do um it's kind of
170:41 - inefficient but for the sake of this
170:42 - project we're going to have to do it so
170:44 - let me show you what that is we're going
170:45 - to have to make another bash script
170:47 - potentially but it should all be okay
170:49 - this is what happens when you kind of
170:50 - have to work with multiple different
170:53 - Docker containers since air bite is
170:55 - working in in its own so let's go ahead
170:57 - and do that right now I'm actually going
170:58 - to create another bash script here um
171:01 - this one's going to be el. sh and this
171:03 - one is going to be exactly what we did
171:04 - before where it's just Docker compos up
171:06 - and knit hyphen airflow and then we're
171:08 - going to sleep for five and then we're
171:10 - going to Docker compose up Hy so the
171:14 - reason for this is that airite has its
171:17 - own set of Docker containers that are
171:19 - running to make sure that everything is
171:21 - working and then we have our own El
171:23 - containers that we've already been
171:24 - working with and we can close those
171:26 - separately now in order for this to work
171:29 - we need to grab the connection ID that
171:31 - we make from within air bite and in
171:33 - order to create the connection we need
171:35 - to obviously hook up the source and the
171:38 - destination together which we're going
171:39 - to do next but then when we have the
171:40 - connection ID we can't necessarily save
171:43 - the connection ID in into the dag and
171:46 - then you know load it up we're going to
171:48 - have to kill the container so that the
171:49 - new connector ID propagates but airite
171:52 - will still be active so we're just going
171:54 - to go ahead and kind of go through that
171:56 - process but then at the end we'll see
171:58 - that airflow is able to read everything
172:00 - that we've done and then we can
172:01 - orchestrate both airbit that's replacing
172:03 - our elt script and then using DBT to
172:06 - write on top of the destination
172:08 - databases that we have currently so
172:10 - let's go ahead and run air bite right
172:11 - now and then we'll get going into uh
172:14 - hook up both of the source and
172:16 - destinations to create our first
172:17 - connector all right so now we're in the
172:18 - UI and then now all we got to do is put
172:21 - in our information here you can choose
172:24 - this if you want but I'm going to say
172:26 - get started and then now we're in the
172:27 - airb UI and let me walk you through
172:28 - really quickly it's it's very simple so
172:31 - we have connections so if you have all
172:32 - your sources and destinations and
172:34 - they're connected together this would be
172:36 - your connections if we go to sources you
172:37 - can see we have a bunch of different
172:38 - sources both certified as well as
172:41 - community and the same thing on the
172:42 - destination side so the one thing we're
172:44 - going to do is we're actually going to
172:46 - go for postgress here you can see it's
172:47 - pretty simple and it's just simply
172:49 - logging in to your instances here or our
172:52 - databases so I'm actually going to name
172:54 - this source postgress and then for the
172:56 - host here we're going to say host. doer.
172:58 - internal and for the port I believe Port
173:01 - 5433 is is the name or is the port
173:04 - number and then for the database name is
173:06 - going to be Source DB now for the
173:09 - username was just postgress and for the
173:11 - password was just secret okay now no SSL
173:15 - modes no SSH tunnels needed for this
173:17 - sake I'm going to be using detect
173:18 - changes with the X-Men system column and
173:21 - then you know if you have any jdbc URL
173:23 - parameters here you can absolutely add
173:25 - that let me go ahead and hit setup
173:26 - Source here and then it should just
173:29 - essentially make a call to the postest
173:31 - database there through the API and We're
173:34 - Off to the Races it should just connect
173:36 - properly if everything is correct and if
173:39 - I look on Docker we should have the
173:42 - containers open so that is correct
173:44 - correct cool all the test connections
173:46 - have passed so then it spits us over to
173:48 - create a connection and we're actually
173:50 - going to add a destination now which is
173:52 - going to be also postgress now it's
173:54 - going to be the same thing we're going
173:56 - to name this destination
173:59 - postgress uh host. doer. internal the
174:03 - port here is going to be 5434 DB name is
174:06 - going to be destination DB default
174:09 - schema uh we're going to name the user
174:11 - postgress and then it's kind of hidden
174:14 - right now but in the optional Fields
174:15 - we're going to say secret as the
174:17 - password because we do need that and
174:19 - then we're going to go ahead and hit
174:20 - setup destination and cool all test
174:23 - connections have passed and what you can
174:25 - see up here uh is that we have you know
174:29 - the connection name we have source
174:30 - postgress to destination postgress
174:32 - replication frequency there is an
174:34 - orchestrator from within airite but we
174:37 - want to switch this over to manual
174:39 - because we are orchestrating this with
174:41 - airf flow if we were wanted to just have
174:43 - this work on its own we can absolutely
174:45 - do that if we wanted to add a Cron job
174:47 - you can absolutely do that as well from
174:49 - within here but for our sake we're going
174:51 - to use manual there's a name space
174:53 - there's the stream prefix to identify
174:55 - streams a little easier and here is all
174:57 - the tables coming in from our source
174:59 - database now what I'm going to do here
175:02 - is I'm going to check all of these cuz I
175:03 - want all of these streams or all of
175:05 - these tables to come with and now from
175:07 - here we can choose a sync mode so do we
175:09 - want to fully refresh every single time
175:12 - do we want to incremental which we
175:14 - recommend uh only the changes because it
175:18 - does support CDC do we only want to sync
175:20 - the changes from the database over to
175:23 - the destination depending on the
175:24 - transactions uh in our case we're going
175:26 - to do full refresh overwrite for all of
175:28 - them uh so we can do some uh testing
175:31 - here but uh yes incremental sync is
175:33 - probably the way to go here we do offer
175:36 - normalized tabular data in this so we do
175:38 - some normalization on top of the data
175:40 - and that is totally fine so I'm going to
175:42 - go ahead and set up this connection here
175:43 - and there there we go uh once this is
175:45 - all set up we can see here at the top we
175:48 - have a couple IDs that are very
175:50 - important to us so the first one is
175:52 - going to be the workspaces ID uh the
175:55 - next one is going to be the connections
175:56 - ID we want this connections ID so what
175:59 - we're going to do is copy this head over
176:03 - into our dag and paste this into the
176:06 - connection I now we're going to save
176:08 - this but remember what I said when we uh
176:10 - you know when we launch airflow and and
176:13 - all of these containers right here
176:14 - they're in separate instances right and
176:17 - so the change I just made has not
176:19 - propagated and so we're not going to be
176:21 - able to stop or to start the dag right
176:25 - now so what we're going to do is we're
176:27 - actually going to kill the elt section
176:30 - first and then leave airite open restart
176:33 - this we should be able to then start the
176:36 - sink using airflow so what I'm going to
176:39 - do now is Docker compose up or uh we
176:43 - have that e T script so what I'm going
176:46 - to do is I have to add uh chod plus
176:50 - x/t Dosh and now I can say elt
176:54 - Dosh and that should run the init first
176:57 - and then that should run the rest of the
176:59 - docker containers we need on the elt
177:01 - side um so the init is done and we can
177:04 - see it down here and then it should
177:05 - follow up with all of that so there we
177:07 - go everything should be initialized now
177:10 - what we have to do is obviously
177:12 - everybody is still running now let's go
177:14 - in and do Local Host port 8080 so that's
177:17 - going to let us sign into our instance
177:18 - here uh we're having an issue with the
177:20 - dag uh I think it's the provider here
177:23 - yep okay so I had this issue with this
177:26 - before airflow. providers. a. operators.
177:31 - airb um let me double check to make sure
177:33 - that is the correct installation of it
177:37 - so where we're probably going to need to
177:39 - do is for some reason this doesn't fully
177:43 - work work here user airflow yeah because
177:46 - we let me double check this um oh I
177:49 - forgot to do that okay so the one thing
177:51 - we do actually need to do is we need to
177:54 - remove this user tag here because we
177:57 - want the user to switch every single
177:59 - time so uh we it's a weird thing where
178:02 - you have to be the airflow user before
178:05 - to install the actual correct operator
178:07 - and then switch back to the route in
178:09 - order to run the dag so that's what
178:10 - we're going to do we're going to do
178:12 - Docker compose down hyphen B
178:14 - um and then run our El script again
178:16 - after we remove that so let's go ahead
178:18 - and run it should be good so then it
178:19 - will use it'll go down and read the user
178:22 - airfow first run this command to install
178:24 - all the correct operators and then we'll
178:26 - have back to the user route once they're
178:29 - installed that way we can actually run
178:30 - the dag okay so that should be good let
178:32 - me go ahead and wait for it to run here
178:35 - there we go uh still having an error
178:37 - here all right so I got it working and
178:39 - this is just a note uh there is some
178:41 - weird issues kind of with Docker but so
178:43 - if you you ever run into issues like
178:45 - that you want to go into the images
178:46 - section so I had to look for the elt web
178:49 - server and scheduler because we built
178:52 - those images already and it doesn't
178:54 - propagate with these new commands here
178:56 - and so when I reran or sorry when I
178:59 - deleted the images and reran it it
179:01 - actually went through and installed the
179:03 - correct operator so now if we go back to
179:05 - the web UI you can see that the dag is
179:08 - now in there so we did everything
179:09 - correctly it was just a matter of
179:10 - deleting the images on Docker first and
179:12 - so now what we're going to do
179:14 - is before we even run it we need to go
179:15 - up into connections uh and then hit this
179:18 - plus sign right here and now what we can
179:20 - see is we have a connection type of air
179:21 - bite but that's not exactly what we're
179:23 - going to be using so if you remember
179:24 - this connection ID if we go back to the
179:26 - code of the dag you can see we had a
179:28 - connection ID of airb so what we're
179:30 - going to do is we're going to say air
179:32 - bites we're going to go down into HTTP
179:35 - and then for the host is going to be do
179:38 - uh host. doer. internal we don't need to
179:41 - provide login password but for the UI
179:43 - we're actually going to point it to 801
179:45 - because that's going to be where um we
179:47 - can access the actual UI and everything
179:50 - so we're going to hit save and so now we
179:51 - have this air bite section if we go back
179:53 - to dags let's go ahead and Trigger this
179:55 - and so we triggered it if we go back to
179:57 - the airb connections we should be able
180:00 - to refresh and hopefully see that a
180:02 - stream is going to start here eventually
180:05 - okay so it failed instantly okay so it
180:07 - looks like it errored because let's
180:09 - actually go back to the uh the
180:11 - connections here we actually may need to
180:13 - do a login password so this is host.
180:15 - doer. internal 801 let's add airite and
180:19 - password because this is a default um
180:21 - thing that's needed or login that's
180:23 - needed for us on our side uh for
180:26 - security reasons and obviously that can
180:27 - change let me go ahead and rerun this
180:29 - again and that should give us
180:31 - permissions to run the sync there we go
180:34 - so now you can see that we have a sync
180:36 - running so we've able to trigger the
180:37 - actual sync Now using airflow and that
180:41 - should just work so if we go back to
180:42 - statuses here we should be able to watch
180:45 - it go and then once that's loaded into
180:47 - the destination then the DBT container
180:50 - will spin up and then we will have all
180:52 - of our models written onto the
180:54 - destination side but data was moved from
180:56 - airbit so let's go ahead and wait for
180:58 - that to load here but if we go back to
181:01 - the dags we can see that it's all
181:02 - working now we were able to successfully
181:05 - connect to airbit and it's not going to
181:07 - succeed until this is completely done
181:09 - there we go so sink has succeeded you
181:11 - can see that we have 113 records
181:13 - extracted if we go back to airflow we
181:16 - should be able to see that it will
181:18 - succeed and then it's going to run DBT
181:20 - uh there we go so there's the first
181:22 - success and then now we're waiting for
181:23 - DBT to run and then once DBT has run
181:26 - we're going to go ahead and check so DBT
181:27 - just successfully ran go back into our
181:29 - terminal and then go back into our
181:32 - destination database so see
181:35 - destination DB let's go ahead and do a/
181:40 - DT and then you can see so now we have
181:43 - some extra stuff so you can see airb
181:44 - actually wrote some raw data into the
181:46 - database but it has come with all of the
181:48 - other tables here notice how specific
181:51 - movie and film ratings isn't here so you
181:55 - can see that DBT actually ran we only
181:57 - have the tables coming from the source
181:59 - database and now if we do select all
182:02 - from specific
182:04 - movie that's going to show the actual
182:07 - Inception you know all that stuff along
182:08 - with some airite metadata so you could
182:10 - see that has actually successfully
182:12 - worked we have air B being orchestrated
182:14 - by airf flow and DBT now being
182:17 - orchestrated by airflow all within this
182:19 - simple project and that right there is
182:23 - the full project completed hopefully
182:25 - that give you a pretty high level
182:27 - understanding and example of how you
182:29 - could successfully create a data
182:31 - pipeline from scratch but then
182:33 - ultimately lead that into a
182:35 - fully-fledged open-source data pipeline
182:37 - where you're sending data from a
182:39 - postgress database using a tool like
182:41 - airite to connect it to a destination
182:43 - datab base where you need the data to go
182:45 - to and then from there you're
182:46 - orchestrating it with airflow writing
182:48 - custom models on top of it with DBT to
182:50 - then use for other sort of data analysis
182:52 - or things like that ultimately the
182:54 - destination could be anything where it
182:55 - ends up in a CRM or some sort of tool
182:58 - like metabase or looker for other open-
183:00 - source options there but essentially
183:02 - this is kind of like my version of a
183:03 - storytelling where you can see how a
183:06 - data pipeline can evolve how complicated
183:08 - it can be at the very beginning but then
183:10 - how simple it can be at the very end
183:12 - when you start using some of the tool in
183:13 - the modern data stack where you know you
183:15 - have tools like air bite and airflow and
183:17 - many other tools where you can bring
183:19 - into the stack and just make it a lot
183:20 - easier as a data engineer now the
183:22 - opportunities here are endless and this
183:24 - is can hopefully open some doors to you
183:26 - to test some things and also play around
183:28 - with other things to become a data
183:30 - engineer eventually but please let us
183:32 - know how we did please let me know if
183:33 - you have any questions or if you learned
183:35 - anything new here please let me know
183:37 - down in the comments or anything like
183:39 - that but I hope you all enjoyed this
183:40 - course and thank you so much for
183:42 - watching