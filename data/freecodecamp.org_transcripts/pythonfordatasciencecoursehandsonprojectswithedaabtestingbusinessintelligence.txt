00:00 - this comprehensive python data science
00:02 - course covers the essentials through
00:04 - Theory demos and real world
00:07 - applications with two detailed projects
00:10 - this course is designed to provide
00:12 - practical experience that will prepare
00:14 - you for real world data science you'll
00:17 - gain Hands-On knowledge in data
00:19 - analytics AB testing and business
00:22 - intelligence if you aspiring data
00:24 - analysts data scientists or you are
00:27 - aspiring to get into the field of
00:29 - machine learning or AI then mastering
00:31 - the basics of data and analytics is your
00:34 - starting point in this comprehensive 6
00:37 - Plus hour course we are going to start
00:39 - with the python implementation of data
00:42 - analytics we are going to look into the
00:44 - data analytics Basics when it comes to
00:46 - putting that in Python Programming after
00:50 - this we are going to get into the AB
00:52 - testing Theory which is fundamental for
00:55 - any data analyst or data scientist who
00:57 - wants to drive the experimentation
01:00 - changes in the product like ux design to
01:02 - the algorithms using the data this
01:05 - course will consist of three parts the
01:07 - first part will be dedicated to the
01:09 - python data analytics course in this
01:12 - python data analytics course we are
01:14 - going to cover the basics of Performing
01:17 - and data analytics including data
01:19 - visualization and data processing in
01:21 - Python after this we're going to get
01:23 - into the second part of the course which
01:26 - consists of the theory of data analytics
01:28 - and datadriven exper experimentation
01:31 - this is fundamental for any data
01:33 - analytics and data science professional
01:35 - here you are going to learn the AB
01:36 - testing Theory starting from the
01:39 - hypothesis the business problem up to
01:40 - the point of conducting a data analysis
01:43 - on that collected data to make a data
01:46 - driven decision for different sorts of
01:49 - Online problems then we are going to get
01:51 - into the third part of the course in
01:53 - this third part of the course we are
01:55 - going to conduct the two endtoend
01:57 - portfolio projects the first one will be
02:00 - related to the AB testing where we will
02:02 - conduct an endtoend AB testing online
02:06 - analytics related project which you can
02:08 - also put on your resume and in this 1
02:11 - half hour We're are going to cover from
02:13 - the basics of a testing results and as
02:15 - is in Python to the actual
02:17 - implementation of it and conducting your
02:20 - data analytics in Python Programming and
02:23 - finally the third part of the course
02:25 - will consist of two separate end to end
02:29 - full data analytics projects the first
02:32 - one will be about online testing where
02:34 - we will use the data analytics as well
02:36 - as Python Programming to derive the uh
02:39 - landing page ux design decision on the
02:42 - landing page of
02:43 - lunch. and then the second portfol
02:46 - project will be another 1 half hour end
02:48 - to end data analytics project where we
02:50 - are going to look into the data
02:52 - analytics for Superstore project so
02:56 - those two projects in total of 3 hours
02:59 - will be agre great way to implement the
03:02 - theory into practice in an actual
03:04 - business real life
03:06 - setting dat scientist an AI professional
03:09 - and I've been in this field for more
03:11 - than 5 years I'm co-founder of lunar
03:14 - Tech where we are making data science
03:16 - and AI accessible to everyone
03:18 - individuals businesses and institutions
03:22 - so here is what we are going to cover as
03:24 - part of this full data analytics course
03:26 - in the first part of the course we are
03:28 - going to cover the data analytics in
03:30 - Python Programming so uh it is expected
03:33 - for you to know some basics in Python
03:36 - but not more we're are going to learn
03:38 - how to load data in Python using pendis
03:41 - how to do data wrangling and data
03:43 - preprocessing using librar such as nony
03:46 - and
03:47 - dendis then we are going to look into
03:50 - the data PR processing techniques how to
03:52 - do sorting filtering as well as data
03:55 - aggregation how to join data using
03:58 - different joints including inner join
04:00 - left join left anti join right join how
04:03 - to do uh different uh statistics related
04:07 - task including calculation of data um
04:10 - descriptive statistics for our data
04:12 - using python then we are going to do
04:15 - data sampling in Python we are going to
04:17 - learn different data sampling techniques
04:19 - and we are also going to look into Data
04:22 - visualization in Python which is really
04:24 - important as a data analytics
04:26 - professional when it comes to bringing
04:29 - the the theory of data analytics into
04:32 - practice so after this once we are done
04:35 - with the uh practical uh programming
04:38 - section for the data analytics in Python
04:41 - we're going to get into the second part
04:43 - of this course which is about AB testing
04:46 - here we are going to look into a quick
04:48 - high level theory behind AB testing and
04:51 - then we are going to dive deep into it
04:53 - we're going to learn this ID or B
04:55 - testing online experimentation and how
04:57 - data analytics is relevant for AB
05:00 - testing and we are here we are going to
05:02 - learn this entire cycle of AB testing
05:04 - from the design up to the data analytics
05:07 - or the final results be prepared to
05:10 - learn here uh the concepts like primary
05:12 - metric the design of the test how you
05:15 - can design a proper AB test including
05:18 - choosing the right parameters for your
05:20 - test calculation of the minimum sample
05:22 - size so as prerequisite for this part of
05:25 - the course it requires for you to know
05:27 - some fundamentals in statistics so
05:30 - understanding uh this Basics behind
05:32 - probability probability Theory this uh
05:35 - concept behind normal distribution how
05:39 - uh you can use a sample to dve Insight
05:41 - about your entire
05:43 - population and once we are done with
05:45 - this theory behind AB testing and we
05:48 - have also learned how you you can
05:50 - conduct the data analytics and final
05:52 - data analysis for your ab test we are
05:55 - ready to go into the third and final
05:58 - part for our data analytics full course
06:02 - in this third part of the course we are
06:03 - going to conduct two end to endend case
06:06 - studies in the first one we are going to
06:09 - conduct a datadriven decision making for
06:12 - lunex landing page where we are going to
06:14 - use data analytics data visualization as
06:17 - well as AV testing to understand whether
06:19 - we need to replace our current
06:22 - button so here expect to use uh python
06:26 - in this one half hour course we are
06:28 - going to conduct data wrangling data
06:31 - preprocessing also data visualization
06:34 - and then we are going to uh analyze our
06:37 - results and make a decision by using the
06:40 - theory that we learn as part of the uh
06:42 - second part of the course as well as the
06:45 - uh programming that we learned as part
06:46 - of the first part of this course then uh
06:50 - once we are done with this first end to
06:52 - endend portf project we are then ready
06:54 - to go onto the second project as part of
06:57 - this final part of the course which will
06:59 - be about a pure data analytics case
07:02 - study in this case study we are going to
07:05 - conduct the data analytics for our super
07:07 - store so here we are going to uh start
07:10 - with the overview of this analysis and
07:12 - then we are going to analyze Superstore
07:15 - customers then we are going to see what
07:19 - uh techniques we can use and how we can
07:22 - conduct a Superstore customer
07:24 - segmentation analysis in Python then we
07:27 - are going to analyze the revenue
07:30 - of the superstore by customer segment
07:33 - after this we are going to explore the
07:35 - customer loyalty at the superstore and
07:38 - then we are going to finish off with the
07:40 - insights that we derived based on this
07:42 - analysis for our customers from the
07:45 - sales and uh just in general so we are
07:47 - going to analyze the sales of this
07:49 - customers by segment and then we are
07:51 - going to conclude at the end of this
07:53 - course expect to learn all the
07:55 - essentials for your data analytics
07:57 - Journey so without further do let's get
08:06 - started hi there and welcome back in
08:08 - this demo we are going to talk about how
08:10 - to load data and view this data in order
08:13 - to obtain more information about a
08:15 - certain data that is provided to us
08:17 - we're going to learn how to load CSV
08:19 - files how to load txt files how to load
08:22 - Excel files as well as Json files and
08:24 - also how to load SQL database for this
08:27 - one we are going to use pred L the
08:30 - pendis library a library that we spoke
08:32 - about in the previous demo but we are
08:34 - also going to use some other
08:36 - libraries all right so without further
08:38 - Ado let's actually go ahead and learn
08:40 - how to load CSV files in Python so for
08:44 - that the first thing that I'm going to
08:45 - do is to import the pendas library
08:48 - import pendas as PD and then the next
08:52 - thing that I'm going to do is to pick
08:54 - the name of the CSV file so you might
08:56 - notice that in the left hand side in our
08:58 - uh py charm you can see that we have a
09:01 - file called percent bachelor's degrees
09:03 - woman usa. CSV this is CSV file
09:07 - containing the following data so you can
09:09 - see we have information about the year
09:11 - agriculture architecture art and
09:13 - performance Etc so you might have
09:15 - already guessed that we are dealing with
09:17 - data that describes the percentage of
09:19 - the uh females that have uh completed a
09:22 - bachelor degree in the corresponding
09:24 - fields and the corresponding year of it
09:26 - well let's go ahead and load the data in
09:28 - the python for that what I'm going to do
09:31 - is to use this uh pandas library and as
09:34 - an Acron name we usually always write a
09:36 - name of uh PD for the pendas so let's go
09:40 - ahead and uh name our data frame which
09:43 - we will call let's say uh dataor CSV and
09:48 - this will be equal to and here we need
09:50 - to take the name of the library we are
09:51 - going to use which is PD which stands
09:54 - for pandas Dot and then read uncore as
09:58 - you can see we are getting in many
10:00 - options so we have reor CSV then we have
10:03 - reor Excel we have reor HTML Json par
10:08 - pickle SAS so here you can see all sorts
10:10 - of data formats that you can file
10:13 - formats that you can import and we are
10:15 - going to learn a few of them and those
10:17 - are the most popular uh file formats
10:19 - that you can expect uh whenever you are
10:21 - entering the data science field so um as
10:25 - we have a CSV file we are going to use
10:28 - the read _ CSV option option and within
10:31 - the parentheses we always need to
10:34 - specify the name of the file we are
10:35 - dealing
10:41 - with so uh you always need to put the
10:44 - name of the file as it's a string So
10:47 - within the quotation
10:49 - mark in here so let's go ahead and
10:53 - actually print our data frame to see
10:57 - what is actually going on
11:01 - here we go so you can see we are getting
11:03 - our data nicely so we see that the heads
11:06 - is recognized so we see the column names
11:09 - we see also here the uh indices
11:11 - corresponding to our observations and
11:13 - this is really a great way to look into
11:16 - your data for the first
11:19 - time in the same way by using exactly
11:22 - the same function so read undor CSV we
11:24 - can also load a txt file so txt and CSV
11:28 - files are pretty similar to each other
11:30 - so in case of CSC files which stands for
11:32 - comma separated values uh we uh do not
11:36 - uh usually specify that the separator is
11:38 - comma so uh as you can see in here this
11:41 - is a CSV file and the values
11:43 - corresponding to each of the columns are
11:45 - separated by comma and uh if we're
11:47 - dealing with a txt file in the txt file
11:50 - we don't really know what the separator
11:52 - can be sometimes it's the comma
11:53 - sometimes it's a space sometimes it can
11:55 - be entirely different character so it's
11:58 - really up to to the data that is
12:00 - provided to you but one simple way to
12:01 - load a txt file by using exactly the
12:04 - same function so the read CSC is the
12:06 - following so here we have two different
12:10 - txt files in here we have the student
12:13 - grades. txt and the student schools. txt
12:16 - go ahead and use them so uh we have data
12:20 - uncore txt and it's equal to pd. read
12:24 - uncore txt and then here within the
12:27 - parentheses we have
12:29 - um let's say student and then
12:33 - schools. txt so this is the path but uh
12:37 - before moving on towards the other
12:39 - argument let's actually go ahead and
12:40 - click on this to see how it looks like
12:43 - so you can see we are dealing with a txt
12:44 - file where we do have the header so we
12:46 - have the name school ID and Country
12:48 - which all represent the name of the
12:50 - corresponding columns as you can see
12:51 - here we have the name here we have the
12:53 - school IDs and here we have the
12:55 - countries and um another thing that we
12:57 - can notice is that uh we are dealing
12:59 - with a separator in the form of commas
13:03 - so therefore what we need to do is to
13:05 - provide in here by the way instead of
13:07 - txt we would just use CS3 for Simplicity
13:11 - and here we will mention that the heer
13:14 - is equal to zero so the first row
13:17 - corresponds to the header which means
13:19 - that that row should not be counted as a
13:21 - data and then the next argument that we
13:23 - will use just for knowing how to use it
13:27 - is the separator so here we will meure
13:29 - that the separator being used to
13:30 - separate each column's value in the row
13:33 - is the comma but um if you were to be
13:36 - supplied with the data in a more
13:39 - difficult format where you um had a
13:41 - different separator so let's say the
13:43 - separator is present symbol then here
13:46 - you need to specify that your separator
13:48 - is this uh is this symbol so whatever
13:50 - the um symbol is used the character is
13:53 - used to separate your data that's
13:55 - exactly what you need to put in here
13:56 - such that pythron can understand when it
13:59 - needs to cut and needs to take that
14:01 - value and say that this value
14:03 - corresponds to that specific column and
14:05 - then the same holds also for the header
14:07 - if your header is not present then you
14:09 - need to specify that in your argument
14:13 - header all right so let's go ahead and
14:15 - load this data and see what is
14:16 - underneath so print dataor
14:23 - pxt here we go so as you can see we
14:25 - nicely get all seven rows so the first
14:28 - name is Tina the last name is Anna and
14:30 - then the Country Canada and last one
14:33 - Armenia so let's go ahead and check it
14:36 - in here so uh I always recommend to
14:39 - check the uh first and the last rows of
14:42 - the database to make sure that you
14:43 - correctly have loaded your data and you
14:46 - are not missing any information uh from
14:48 - your database the next thing we are
14:50 - going to learn and you have name of your
14:53 - first sheet equal to sheet one and then
14:55 - the second one corresponding to another
14:57 - name and you have multiple of those um
15:00 - pages in your Excel file is how to load
15:03 - Excel files so let's say you have an
15:05 - Excel file and you want to load only the
15:07 - first page well for that what you can do
15:10 - in here I'm not going to um look into a
15:12 - specific Excel file uh feel free to uh
15:16 - search for an Excel file or maybe one
15:18 - that you can create yourself and then
15:20 - create your own Pages within your Excel
15:23 - file and try to load that uh in Python
15:26 - but for now let's assume that we do have
15:28 - that Excel F in our py charm environment
15:31 - and we are going to load that so uh
15:34 - dataor Excel will be the name of the
15:37 - data frame that we will store our data
15:39 - and then the function we can use is PD
15:41 - Dot and then read and then here we
15:45 - already get a recommendation from
15:46 - pycharm read uncore Excel and then here
15:49 - we
15:50 - have file unor
15:54 - XL s x which is a common uh extension of
15:59 - Excel file file. ex LS6 is the name of
16:01 - your Excel file and here I'm making an
16:03 - assumption that your Excel file is
16:06 - within this python for data side or your
16:08 - own uh folder the one that you are
16:09 - currently using in py charm so here then
16:13 - the next thing we need to specify is the
16:16 - exact spreadsheet we are looking into
16:18 - because um otherwise you will uh get an
16:21 - error in a pie charm and python will not
16:23 - recognize where exactly it needs to look
16:25 - for the data therefore we need to use
16:27 - this argument called she feore name and
16:31 - here you need to specify the name of
16:32 - your exact spreadsheet you are looking
16:34 - for it can be that it is the default um
16:38 - acronym usually used in Excel but in
16:41 - case you have renamed it or someone has
16:43 - renamed it then you need to specify that
16:45 - specific uh name so uh it can be for
16:48 - instance
16:50 - um uh
16:52 - first spread sheet let's say if that's
16:56 - the name of your first spreadsheet or it
16:58 - can be uh hint one which is usually the
17:02 - common convention used in Excel whenever
17:04 - you are not changing the name of your
17:06 - spreadsheet and this is how you can read
17:09 - an Excel file uh I won't be running this
17:11 - code because we do not have the file.
17:14 - Excel S6 in our folder but this
17:17 - something that you can experiment
17:19 - yourself and another common the file
17:22 - format that you can expect the Json
17:24 - format here once again we are following
17:26 - the same ideas as in case of Excel files
17:28 - so feel free to go ahead and look for
17:30 - Json file uh online download it and try
17:34 - to load that into your pie charm
17:36 - environment but this is the way that you
17:38 - can load Json type of data so data
17:42 - uncore Json is equal to pd. traore Json
17:47 - and then here you can specify the um uh
17:52 - let's say Jon uh file. Json so this will
17:58 - be name of your file we can also make
18:00 - this more convenient so let's make it
18:03 - file name here also file uncore name and
18:08 - this is the only thing that you need to
18:10 - specify so this will be the name of your
18:12 - file and here once again I'm assuming
18:14 - that your file name. Json is actually in
18:17 - this folder that you are currently
18:19 - running otherwise you need to specify
18:21 - the exact path of the file that you have
18:24 - so once you write this then you should
18:27 - be able to successfully load your decent
18:29 - type of data in your pie charm and then
18:32 - finally we will look into uh way to load
18:36 - uh SQL databases so SQL databases are
18:39 - common um database format uh whenever
18:42 - you are working with big data this is
18:45 - very common in the field of data
18:47 - analytics but I think it's still worth
18:49 - to know at least the commands and the
18:52 - library you can use in Python in order
18:54 - to load uh this type of data so let's
18:58 - actually go had and import the
18:59 - corresponding Library uh we can use to
19:01 - loow SQL database and the library is
19:04 - called SQL uh te Tre so uh for that we
19:08 - will do import and then SQL and then I
19:13 - and then three then uh what we need to
19:16 - do first is to make a connection with
19:18 - this SQL database and that's exactly
19:20 - what we can do by using this Library so
19:24 - connection DB is equal
19:27 - to SQL
19:30 - it3 do connect and here we need to
19:34 - specify the name of the database we are
19:36 - dealing with so
19:38 - database uncore name do the extension is
19:42 - DB for database and in this way you will
19:45 - make a connection with the corresponding
19:47 - database the way SQL works is that in
19:49 - SQL we are creating databases and within
19:51 - each database we can have multiple
19:53 - tables and each table has its own name
19:56 - and then within each table when we are
19:58 - loading that table we can there run cues
20:00 - I won't go too much into details about
20:02 - what is SQL how you can use database how
20:05 - you can create tables and how you can
20:06 - run cues because that's outside of the
20:09 - scope of this uh course but uh I would
20:11 - highly suggest you to um at least learn
20:13 - the basics of the SQL it's not necessary
20:16 - to enter the field therefore it's also
20:18 - not included as part of this course but
20:20 - it's good to know uh at least what squel
20:22 - is and also how uh you can use it and
20:26 - what what is the a functionality of SQL
20:29 - uh in the entire world of data science
20:31 - so uh I will include some resources
20:34 - about SQL and the usage of it uh in the
20:37 - resources section but just know that in
20:39 - order to be a technical data scientist
20:41 - at least to enter the field of data
20:43 - science you do not need to know SQL it's
20:46 - something that you I would highly
20:47 - suggest you to learn as you grow your
20:49 - career but it's not a must know so once
20:51 - we have made the connection with our
20:53 - database called database name. DB then
20:56 - we can specify the exact cure
20:59 - that we want to run and in this um
21:01 - specific scenario what we mean by the Cy
21:04 - is that we will use the commands
21:07 - commonly used in SQL in order to select
21:09 - all the rows within a specific table so
21:12 - in our database we can have multiple
21:13 - tables and here I will assume that we
21:15 - have a specific table from which we want
21:17 - to import um let's say the First Column
21:21 - only for that what we need to do is to
21:23 - do a cury let's call it a cury uh let's
21:27 - say it's our first cury cury _ 1 is
21:29 - equal to and then here we have quotation
21:32 - mark and then select this is a common
21:35 - way of um specifying that we want to
21:37 - select specific variables from our table
21:40 - and here we can specify the name of the
21:43 - uh column that we want to import let's
21:46 - say callcore
21:47 - one and then we need to specify from and
21:53 - here we will specify the uh table
21:55 - underscore name and in this way the one
21:59 - will then go ahead and select the first
22:01 - column from the table with the table
22:03 - underscore name so uh this is a one way
22:06 - of uh running a cury and selecting just
22:08 - one variable we could also write a cury
22:11 - we will call CY 2 and this will select
22:14 - all variables from uh table uncore name
22:17 - select and then here what we need to do
22:19 - is use star and in SQL whenever we are
22:22 - saying select star it will go ahead and
22:24 - select all the columns included in that
22:26 - data something that we uh usually
22:28 - usually prefer instead of uh just
22:30 - selecting one variable so you will
22:32 - select certain variables only if you are
22:34 - specifically looking for those features
22:36 - but otherwise I would highly suggest you
22:37 - to include all the Cs all
22:53 - right by the name of the data print we
22:55 - want to load this data in and then we
22:59 - read underscore as you can see we
23:02 - already see
23:05 - something so here you can see that we
23:07 - have three different functions we can
23:08 - use we can use reor SQL we can use reor
23:11 - SQL uncore cery or we can use a read
23:14 - underscore SQL underscore table they do
23:17 - defer in the way they import the data so
23:19 - for one for instance you can specify
23:21 - also schema and another one you can
23:22 - specify the index uh of the com you want
23:25 - to import the most generic one is the
23:27 - read SQL similar to the read CSC so we
23:30 - are going to use that one and the next
23:33 - thing we need to specify is the uh cury
23:36 - and the connection is specify cycore 2
23:40 - and then next thing I need to do is to
23:42 - specify the
23:43 - connection so once you run this code
23:46 - what this will do is to make a
23:48 - connection with your SQL database it
23:50 - will then uh specify the Cur and it will
23:53 - go ahead and select all the features and
23:55 - import all the features from a table
23:57 - called table underscore name all the
24:00 - variables and it will then uh be um
24:02 - stored in the pandas data
24:05 - frame this is all for this demo and I
24:07 - will see you in the next
24:10 - one in this demo we are going to
24:13 - continue the process of looking into the
24:15 - data as in the previous demo we learn
24:17 - how to load different sorts of data and
24:19 - in this one we're going to look into it
24:21 - we're going to learn how to explore the
24:23 - data and how to preprocess it we are
24:26 - going to discuss the uh inspection of
24:28 - the data getting information about it
24:30 - getting to know what the shape of the
24:32 - data is how to identify missing values
24:35 - how to drop the missing values how to
24:36 - fill in missing values how to get the
24:39 - type of the data you are dealing with
24:41 - how to access different rows in your
24:43 - data frame by using the infamous iog and
24:46 - loog and what is the difference between
24:48 - the two so from the previous demo we
24:50 - still have the CSV file and uh we saw
24:53 - that uh we got this data structure in
24:56 - the CSV file describing the percentage
24:59 - of the bachelor degrees uh consisting of
25:01 - women in the USA so this how the data
25:04 - look like we had the year agriculture
25:07 - architecture art and performance Etc and
25:11 - then uh in total we got 18 columns in
25:14 - this data frame and 42 rows so the rows
25:17 - are those observations so you can see in
25:19 - here and the columns are all the
25:22 - features included in data frame and we
25:25 - got only one feature describing the year
25:28 - and the rest of them are all the names
25:30 - of different sorts of Bachelor studies
25:33 - so as you can see we have AR culture
25:35 - architecture we have social sciences and
25:37 - history public administration Etc the
25:41 - first thing we are going to look into
25:43 - how to use the Heather functionality in
25:45 - Python in order to get or a snapshot of
25:48 - the data
25:50 - so what we can do here is to uh write
25:53 - down the name of the data frame so data.
25:56 - CSV and what we need to do to do Dot and
25:59 - then head for the header and then here
26:03 - uh inside in it uh we can leave it empty
26:05 - so when we do that what we will get is
26:09 - the following output so let's actually
26:12 - go ahead and remove that one from the
26:14 - printing temporarily as you can see it
26:16 - will print for you the first five rows
26:18 - with all the columns and if we
26:21 - specifically are looking for each number
26:23 - of rows to be presented as part of the
26:25 - snapshot then we can specify that as
26:27 - part of the head function so in here for
26:30 - instance we can say show to us the first
26:34 - 100 observations in this data frame and
26:37 - in that case it will print for you the
26:39 - first 100 observations but as we have
26:42 - only 42 rows it means that it would need
26:44 - to print for you all the rows if we
26:46 - change this to 20 let's say then in that
26:49 - case we will get the top 20 observations
26:52 - present in the data and this is how it
26:55 - looks like all right so this is about
26:59 - header function just a good way to uh
27:02 - have a first look at what kind of
27:03 - variables you have what are the first
27:05 - few columns what are the last two
27:07 - columns what is the number of
27:08 - observations you got by looking in here
27:11 - what is the number of columns you got
27:13 - and uh also what are the different sorts
27:16 - of variables you got and data types in
27:19 - your data frame just by a visual
27:22 - inspection you can see that we have for
27:23 - instance the year column which consists
27:25 - of the integers we have the agriculture
27:28 - and all these are variables that are of
27:30 - a floating uh number type which means
27:33 - that we got a number and then we got Dot
27:36 - and then what comes up to the decimal
27:40 - points and if this heer function will
27:43 - print for you the top X observations we
27:46 - can do exactly the same only from the
27:48 - bottom observations and for that we can
27:50 - use a function called tail so uh we can
27:53 - print for instance the last 20 columns
27:57 - by using this data _ CSV do tail and
28:00 - then within the parenthesis the amount
28:02 - of rows that we want to see from the
28:04 - bottom up so let's go ahead and print
28:09 - that and as you can see if the Heather
28:12 - function will show case the first 20
28:15 - rows the tail function will showcase the
28:19 - last 20 rows so you can see 22 23 up to
28:23 - 41 so this is a great way to see uh how
28:27 - the uh the first few rows look like and
28:31 - how the last few rows look like the next
28:34 - thing what we can do is to use this info
28:36 - function in order to obtain more
28:38 - information about our columns so they
28:40 - data type specifically so this is the
28:43 - output of the info function and this is
28:46 - the number of columns you have in this
28:48 - case the year is the First Column the
28:50 - agriculture is a second column and then
28:52 - the social sciences and histories in
28:54 - last column and then we have the count
28:56 - of the non-n values as you can see all
28:59 - the columns have 42 non-n values which
29:02 - means that we do not have any missing
29:04 - observations then we have the data type
29:06 - corresponding to that specific feature
29:09 - and we already saw from the snapshot
29:11 - that uh the year was the only variable
29:13 - of integer type and everything else was
29:16 - floting uh data type and this is exactly
29:19 - the confirmation for data
29:22 - observation the next thing what we can
29:24 - do is to identify the missing values and
29:28 - drop the missing values so uh from this
29:31 - we can al already see that we do not
29:33 - have any missing values but let's
29:35 - actually go ahead and learn how we can
29:36 - do that so let's say we want to uh drop
29:40 - all the missing observations for that
29:42 - what we can do is to do print is to
29:45 - actually um take the name of the data
29:48 - frame CSV and then what we can do is to
29:51 - do drop and then Na and then parentheses
29:56 - so in this way you will be dropping all
29:58 - the na so all the cases where your
30:01 - observation has an NA for that specific
30:04 - column so as we do not have any missing
30:07 - values in our data frame this will not
30:09 - do much to our case but I think it's
30:12 - really important to know how to drop
30:14 - missing values in case you want to
30:15 - quickly remove them from your
30:21 - database let's say you do not want to
30:23 - drop your na so the missing values in
30:25 - the form of Na but you want to F them
30:27 - with a certain volue what you can do is
30:30 - to use this
30:34 - uh field na function and within the
30:38 - parenthesis you just need to specify
30:40 - what is the value that you want to use
30:42 - to fill the na so in here you can for
30:45 - instance decide to put null instead of
30:48 - Na and this will simply go ahead and
30:51 - fill all the values where it's written
30:53 - na a it will replace it with null
30:56 - values so let's say you have another
30:59 - issue with your data so you have rows
31:01 - that are exact copies of each other one
31:04 - function you can use is what we call
31:06 - drop duplicates so for that you simply
31:09 - need to take the
31:12 - um data frames name and you need to drop
31:16 - and then underscore
31:18 - duplicates and this will quickly remove
31:21 - all the duplicates from your data frame
31:31 - so let's actually go ahead and change
31:33 - the CSC file and see whether we can uh
31:36 - nicely remove the
31:38 - duplicates so let me copy paste this few
31:41 - times and as you can see it's the uh
31:44 - exact copy of the third row and now we
31:46 - have it in the fourth and fifth and
31:48 - sixth rows so let's go ahead and check
31:51 - whether this dropcore duplicate really
31:54 - removes uh those
31:55 - duplicates so let's print the actual CSV
31:59 - file
32:01 - before removing the duplicates and after
32:03 - removing the
32:05 - duplicates in
32:07 - here let's
32:10 - see so in here you can see that this is
32:15 - the uh data frame with in total of 45
32:19 - rows because we just added three
32:21 - additional rows so previously we had 42
32:24 - we added three so we end up with 45 rows
32:26 - and the number of cols is the same and
32:28 - then we apply the drop duplicates
32:30 - function and as you can see after using
32:33 - that function now we once again end up
32:35 - with a 42 rows and if we go ahead and
32:38 - look into the specific column we
32:40 - duplicate you can see that we got only
32:43 - one row corresponding to this year and
32:46 - this is how we know for sure that
32:48 - dropcore duplicates really works and it
32:50 - will remove the duplicates from your
32:51 - data the last thing we will look into in
32:53 - this demo is how to access certain rows
32:55 - in a data frame depending on their index
32:58 - type so uh sometime and actually most of
33:01 - the time we will get a data frame that
33:03 - has an integer as an index and that's
33:06 - also what we got in our data frame as
33:08 - you can see the index is 0 1 2 3 4 so
33:11 - it's in the integer format but there
33:13 - also occasions when you will get your uh
33:16 - data frame with an index that is of
33:18 - string type so you will see that instead
33:20 - of the uh index 01 2 3 for instance you
33:24 - will have ABC or A1 A2 A3 uh
33:28 - Etc so depending on this uh nature of
33:32 - the uh index that you are dealing with
33:34 - you can then use either the ilog or the
33:37 - log functionalities in Python in order
33:40 - to access different rows in dependent
33:43 - data frame and this is by the way a
33:45 - common question you can expect during
33:47 - your programming related data science
33:49 - interviews what is the difference
33:51 - between the iog and loog and how you can
33:53 - use them so let's start with the iog as
33:56 - our data frame or already contains an
33:58 - integer based indexes and let's say we
34:01 - want to access the data in the index uh
34:05 - 10 so uh this is the row that we want to
34:09 - access therefore what we need to do is
34:12 - to take the name of the data frame so
34:15 - dataor CSV and then we need to do Dot
34:20 - and then
34:21 - iog and then in here we need to specify
34:24 - the row that we want to access which is
34:27 - the 10
34:28 - and let's go ahead and print this to see
34:31 - whether we are getting the correct data
34:34 - and uh let's verify that by looking at
34:37 - the gear so as you can see the gear is
34:39 - in incremental order um and we do not
34:42 - have duplicates in the year so uh
34:44 - therefore if we see that the year is
34:46 - equal to 1980 then we have selected the
34:49 - right data and we have accessed the
34:51 - right
34:56 - Ro so this the output and as you can see
35:00 - the year is equal to 1980 and this is
35:03 - all the uh information that is stored in
35:06 - the 10th row and in this way you can
35:09 - access any row that you want in your
35:12 - data frame so it can be for instance the
35:13 - first row or the last row but it can
35:16 - also be a row somewhere in the middle
35:18 - that you want for for some
35:22 - reason and if you want to access a
35:24 - specific column instead of specific row
35:26 - what you can do is to use again the log
35:30 - function so
35:33 - data.
35:35 - log and
35:38 - here instead of providing just one
35:40 - argument you can then provide two
35:42 - arguments so we always have the rows in
35:44 - the beginning and then uh we need to
35:46 - specify the columns and in case of um
35:49 - the in here what we did was to access
35:51 - the specific row therefore we specified
35:53 - only the X but if we want to access a
35:56 - specific column we also want to specify
35:59 - the rows that we want to include then we
36:00 - need to specify both the index of the
36:02 - rows and the index of the column so
36:04 - let's say I want to access the column A2
36:07 - in here I will then specify A2 and as I
36:10 - have specified here a column this means
36:12 - that I want to take all the rows so the
36:14 - indexes corresponding to all these
36:16 - values so let's see what the output of
36:19 - this one
36:20 - is and as you can see here we are
36:23 - getting four five and six so the value
36:26 - corresponding to the index X Y and Zed
36:28 - and this is actually the column A2 so in
36:32 - this way you can specify not only the
36:34 - rows that you want to access but also
36:36 - the column so let's say you only want to
36:39 - access a specific value in that column
36:41 - let's say you want to access the second
36:43 - row and the second column in that case
36:46 - what you need to do I'm sure you already
36:48 - have guessed that is to specify the
36:51 - index of the row and the index of the
36:54 - column that you want to access so the
36:56 - index of the row is Y and then the
36:58 - column that we want to access is A2 so
37:00 - this is the number that I'm chasing
37:02 - let's see where this will provide the
37:04 - value and as you can see it provides
37:06 - five and let's also look into the case
37:09 - when we are dealing with a string based
37:11 - index so uh let's create for that a
37:14 - small data frame and it is the
37:17 - following let's look into
37:22 - it and as you can see this is the uh
37:25 - small data frame that has indexes X Y
37:28 - and Z and let's say we want to access
37:32 - the um data that is stored under the
37:35 - index X so very first row well what we
37:38 - need to do for that is to uh once again
37:42 - take the name of the data frame and
37:44 - instead of using iog this time we need
37:46 - to use the log and then the index name
37:50 - which is similar to what we saw before
37:52 - so uh the index name is X that's what we
37:54 - want to access and let's actually see
37:57 - what is the output of that
38:05 - print here we go so as you can see the
38:08 - first value is one the second value is
38:10 - four and the last value is seven so by
38:14 - using the log functionality we can
38:16 - access the uh row in a data frame where
38:19 - the index is of string type but if you
38:22 - go ahead and you use the ey log in here
38:24 - you will see that you will get an error
38:30 - and the reason for that is because iog
38:31 - doesn't allow you to search with uh case
38:34 - when your index is of string type and
38:36 - for those cases you always need to use
38:38 - the uh log function instead of
38:41 - iog hi there and welcome back to another
38:44 - demo we are going to talk about three
38:45 - very important tasks that you can
38:47 - perform as part of your data analysis
38:49 - and data manipulation toolkit so we are
38:52 - going to learn how to do filtering
38:53 - sorting and grouping in Python data
38:56 - analysis and manipulation involve
38:58 - working with large amounts of data and
39:00 - you can definitely expect this as part
39:02 - of your data science project and then
39:04 - what you need to do is to extract
39:05 - meaningful insights in this context
39:08 - filtering grouping and sorting are
39:10 - really important techniques that allow
39:12 - us to organize extract and analyze data
39:15 - efficiently python has a very powerful
39:18 - tool Library called pendas that we also
39:20 - saw as part of the libraries discussion
39:22 - demo which can be used in order to
39:24 - perform grouping filtering and sorting
39:26 - in a very simple way so when it comes to
39:29 - grouping grouping data helps us to
39:30 - analyze and summarize information and
39:33 - data based on specific criteria here we
39:35 - need to have at least one variable to do
39:37 - the grouping based on but then you can
39:39 - also add extra other variables such that
39:41 - you can aggregate your data not only on
39:44 - one variable but on multiple variable so
39:46 - let's say you want to uh group your data
39:48 - based on the gender or based on the
39:50 - region and then you want to perform some
39:52 - uh descriptive statistics calculation so
39:55 - you can calculate the mean for instance
39:56 - the standard deviation the variance the
39:59 - median the mode the minimum the maximum
40:01 - you get the idea so you can then
40:03 - categorize your data into certain groups
40:06 - and this way you can group your data and
40:08 - then you can obtain some meaningful
40:09 - information and analyze your data this
40:12 - is usually very important part of your
40:14 - data preparation process another thing
40:16 - we are going to talk about is how to
40:17 - filter the data so whenever you are
40:19 - filtering your data it helps you to
40:21 - extract subset of data based on specific
40:24 - condition so let's say you uh know a
40:26 - specific spefic uh year that you are
40:28 - interested in or you are interested in
40:31 - specific region you are interested in
40:33 - specific characteristics then you can
40:35 - use filtering to filter your data to
40:37 - select only a subset of observations
40:39 - from your data and to perform all the
40:41 - analysis calculation and training of
40:43 - your model based on this specific subset
40:45 - or it can also be that you want to
40:47 - identify the outlash in your data or the
40:49 - noise in your data and you want to
40:52 - identify the 99 percentile or the first
40:55 - percentile so you want to identify the
40:56 - largest or the smallest observation in
40:58 - your data and remove them from your data
41:00 - such that that you won't be dealing with
41:02 - a problem of overfitting as an example
41:05 - another thing that we are going to learn
41:06 - as part of this demo is sorting so
41:09 - sorting data helps you organize
41:10 - information in specific order it can be
41:13 - in an ascending order or in a descending
41:15 - order it can help you to visualize your
41:18 - data it can help you to identify certain
41:20 - patterns find EX streams or outliers in
41:22 - your data and sometimes it's also used
41:24 - as part of the time serice analysis or
41:27 - for instance to look into the sales
41:29 - performance to identify the worst
41:30 - performing uh shops or the best
41:32 - performing shops and it's essential part
41:35 - of the reranking and recommender systems
41:38 - so whenever you are dealing with sege
41:39 - engines recommender systems anything
41:41 - that relates to the order and importance
41:44 - then sorting data comes really handy
41:46 - because you want to show the uh best
41:49 - information the most important
41:50 - information to your customers and the
41:52 - way that you can do that is by sorting
41:54 - your data so when it comes to the s
41:57 - versus descending the ascending relates
41:59 - to the case when the smallest values are
42:01 - at the top and then the values would
42:03 - then increase and then at the bottom you
42:05 - have the largest observations and
42:06 - whenever it comes to the descending uh
42:08 - order of the Sorting then we have the
42:10 - largest values at the top and then the
42:12 - values would decrease and then the
42:14 - smallest values will be at the bottom so
42:16 - here I created a very simple data frame
42:19 - using Panda's library and here we have
42:22 - four different columns we have the name
42:23 - the age the salary and the department of
42:25 - an employee and as you can see we are
42:28 - dealing with eight observations the name
42:30 - and the department are of string type
42:33 - and the age and salary are of integer
42:35 - type and what we want to do here is to
42:37 - sort our data under the data frame name
42:40 - data with respect to the salary such
42:43 - that uh in the beginning we have the uh
42:45 - at the top we have uh employees with the
42:48 - smallest salary and at the bottom we
42:50 - have employees with the largest salary
42:52 - so what we expect is that uh this person
42:55 - so this corresponds to saana at the age
42:58 - of 19 from the Department of operations
43:01 - should be at the top because uh this
43:03 - person earns the Leist and then we have
43:06 - um the uh largest earning so the highest
43:08 - earning person which is named Bob and
43:11 - Bob with the age of 20 should be at the
43:13 - very bottom because we want first to
43:16 - sort our data based on salary in an
43:18 - ascending order so for this what we can
43:21 - use is the default um penders function
43:24 - called sword _ volume
43:28 - so data.
43:31 - sortore
43:33 - values so as you can see we are already
43:36 - getting a recommendation for this
43:37 - function uh from Python and uh here you
43:40 - need to specify based on which variable
43:43 - you are sorting and then you need to
43:45 - specify whether it's an ascending or a
43:48 - descending so um the reason why we need
43:50 - to specify the varibles name based on
43:52 - which we are sorting is because here we
43:54 - could have also sorted the data based on
43:56 - the age but uh instead what we want to
43:58 - do is to sort the data based on salary
44:00 - such that the highest earners will be at
44:02 - the bottom and the lowest earners will
44:04 - be at the top therefore we need to
44:06 - specify that buy is equal to salary and
44:09 - then when it comes to the uh other
44:11 - parameter or the argument in this
44:13 - function which is ascending this is a
44:14 - brilliant type of argument so um when
44:18 - ascending is equal to True which is the
44:19 - default value then uh it means that we
44:22 - are sorting our data data frame based on
44:25 - an ascending order but if we want our
44:28 - data to be sorted in a descending order
44:30 - then what we need to do is to change
44:32 - this value so then the ascending
44:34 - argument should be equal to false so
44:36 - let's actually go ahead and see that in
44:37 - the implementation so here we need to
44:41 - specify by is equal two and then the
44:44 - name of the variable based on which we
44:46 - are sorting which is the salary then the
44:48 - next thing what we need to do is to
44:49 - mention s sending uh sending which is
44:52 - the second argument it's a brilant
44:54 - valued argument so it can only take
44:55 - values true and false and and the
44:57 - default value is true which means that
44:59 - we are um sorting our data in an
45:02 - ascending order I could have also
45:04 - skipped this argument you can see in a
45:06 - bit that we are getting exactly the same
45:08 - uh result when we are mentioning this
45:11 - argument SN equal to true and without
45:13 - mentioning this argument simply because
45:15 - the default value is equal to true so
45:18 - let's actually go ahead and remove this
45:20 - for one case for the other s you can see
45:23 - that both will result in the same output
45:27 - here we go so let's also add a line in
45:31 - between to make sure that we are getting
45:36 - everything nicely
45:41 - printed here we go so as you can see
45:44 - here here we are getting us expected
45:46 - Sana at the top and then Bob at the
45:48 - bottom and in this way uh you can verify
45:51 - that your uh sorting occurred
45:53 - successfully and we have done it in an
45:55 - ascending order because the salary is
45:58 - increasing and the lowest salary is at
46:00 - top and then the highest salary is at
46:02 - the bottom so uh the other thing that
46:05 - you can see here is that we are getting
46:07 - exactly the same result when we are
46:09 - mentioning the S standing equal to true
46:12 - so this is simply because um there are
46:14 - certain arguments and the arguments have
46:16 - certain default values in Python and uh
46:19 - whenever you want to have the uh default
46:22 - value as your arguments value you can
46:24 - also skip that argument and you don't
46:26 - need to mention it specifically
46:28 - therefore this results in the same
46:30 - output as this line but if we do want to
46:33 - change the order so we want to have
46:35 - another value corresponding to that uh
46:38 - argument which is not a default value
46:40 - then we need to change that and we need
46:42 - to specifically mention what is the
46:44 - value that we want which means that if
46:46 - we want to order this data so we want to
46:48 - sort this data based on cellary but in
46:50 - this sending order we need to use this
46:53 - argument and instead the true we need to
46:55 - change this to false because if the S
46:58 - sending is equal to false it means that
46:59 - the descending is equal to True hope
47:02 - this makes sense so let's go ahead and
47:04 - print
47:06 - this here we go so as you can see now we
47:09 - have exact opposite of what we had
47:11 - before so we have now the highest earner
47:13 - Bob with the age of 20 with a salary of
47:16 - 220k from the tech department at the top
47:20 - and then the salary would decrease
47:21 - accordingly and then at the very bottom
47:23 - we have saana with an age of 19 with a
47:26 - salary of 10 10K from the operations
47:28 - Department because she's earning the
47:30 - list so this is about sorting this is
47:33 - all what you need to know do you keep in
47:35 - mind about this descending and ascending
47:37 - and how you can use the parameters in
47:39 - this function in order to assort your
47:41 - data accordingly another thing we are
47:44 - going to learn today is how to group
47:46 - your data so let's say we want to group
47:48 - our data based on a department so we
47:51 - want to know per Department what is the
47:53 - number of employees or it can be that we
47:56 - want to obtain per Department the
47:58 - average salary let's go ahead and learn
48:00 - how to do that in Python so let's say uh
48:04 - we first count the number of people in a
48:07 - department for that what we need to do
48:09 - is to take the name of the department uh
48:12 - of the data frame which is data and then
48:14 - Dot and what we need to do here is to
48:16 - use the uh function called group Y and
48:20 - this a pandas function that we can use
48:22 - to group our data and within Group by we
48:25 - need to specify the variable based on
48:27 - which we are doing the grouping and as
48:29 - we want to obtain the number of
48:31 - employees per Department it means we are
48:34 - aggregating the databased on department
48:37 - so here I will mention then the uh
48:39 - Department which is the uh variable we
48:43 - are using for uh grouping our data and
48:46 - then next thing we need to do is to do
48:48 - Dot and then the operation that we are
48:50 - performing in this case we are counting
48:52 - the number of employees this means that
48:55 - I can either use the salary column or I
48:57 - can use the age or the name in order to
48:59 - obtain the number of observations per
49:03 - department so let's go ahead and do uh
49:08 - count by the way we can also uh even
49:10 - skip this part and we can specify that
49:13 - we uh want to just count the number of
49:15 - times the uh Department name appears
49:18 - because this also will go and calculate
49:20 - the number of employees per department
49:23 - so let's go ahead and see what I mean
49:25 - here
49:27 - print
49:30 - then let's also add the closing
49:33 - parentheses for the
49:35 - print here we go so as you can see we
49:39 - are getting as an index for this uh new
49:41 - data frame the name of the department so
49:44 - the variable that we use to do the
49:45 - grouping by and then here we have
49:48 - Healthcare operations and Tech what you
49:50 - are getting here is basically the number
49:52 - of times uh each of this department
49:55 - appear here for the variable name age
49:57 - and salary so as you can see this is the
50:00 - column name the first one and then you
50:02 - have the age and then the salary if we
50:04 - were to go ahead and actually specify
50:07 - the exact column that we want to do the
50:09 - grouping and then aggregation so let's
50:11 - say uh the
50:17 - name then we can expect to do the
50:19 - aggregation only based on the variable
50:22 - name so as you can see here we are
50:24 - getting this new data frame and it says
50:26 - that the Department Healthcare got only
50:28 - one person working and then the
50:30 - operations got two and then Tech got
50:33 - five if you go ahead and count here you
50:35 - can verify that information actually so
50:37 - you have here Tech on two three uh four
50:40 - and five so five times then the
50:42 - healthcare once and then operations
50:44 - twice exactly what we got here so in
50:47 - this way you can do the uh you can count
50:49 - the number of times uh each of the uh
50:52 - observations appear uh based on the
50:54 - variable that you have chosen and you
50:56 - can can decide either to select a
50:58 - specific column to do the aggregation or
51:00 - you can also leave that part and you
51:02 - just uh implement the function and it
51:04 - will be uh it will do the corresponding
51:06 - operation for all the columns so for the
51:10 - name for the age and for the
51:12 - salary so let's actually go ahead and
51:15 - calculate the average salary pair
51:18 - department for that what I need to do is
51:21 - to change the name of the variable based
51:22 - on which I wanted to the agregation and
51:25 - then instead of using the function count
51:27 - I will use the function mean and this
51:30 - function so the mean is the same as the
51:32 - average should calculate the mean or the
51:35 - average salary pair department and as
51:37 - you can see here we are getting that the
51:39 - health care department has an average
51:41 - salary of 170k operations has 20K as the
51:45 - average salary and the tag has the 113k
51:49 - as the average salary so one way you can
51:51 - verify that this information is really
51:53 - correct is for instance by looking at
51:55 - the healthcare and this is an easy way
51:57 - to do that because Healthcare got only
51:58 - one observation and the average of the
52:00 - one observation is equal to that average
52:02 - which means that if we look at the
52:04 - health care salary so the only uh person
52:07 - who is from the healthcare department is
52:10 - the person with name uh Ellis and then
52:12 - the corresponding salary is 170k exactly
52:15 - the same number as we got
52:17 - here we could also go and uh do the same
52:21 - calculation only instead of calculating
52:23 - the average we could calculate per
52:25 - Department the the uh minimum salary so
52:28 - let's go ahead and do that we see we are
52:31 - getting an error because we forgot a
52:37 - parenthesis in here so now you get not
52:40 - the average but the minimum salary per
52:42 - Department you could also do the uh
52:45 - maximum per
52:47 - Department you could also do the um
52:52 - minimum age or the average age by
52:54 - Department to see uh the age group
52:57 - uh accordingly so for that what you had
53:00 - to do is to change the salary to a
53:03 - variable called age because we want to
53:05 - do the aggregation based on age and here
53:07 - we can then change minimum and in this
53:09 - way we can calculate the U average age
53:13 - per department so as you can see from
53:15 - the Department of tech the um people are
53:19 - on average 41 years old from the
53:21 - operations department they are on
53:22 - average 22 years old and in the
53:24 - healthcare department they are uh around
53:26 - 65 years old and this how you can do
53:29 - grouping in Python and then the final
53:31 - part in this demo is to look into the
53:33 - filtering so how we can use the uh
53:36 - filtering in Python in order to uh
53:39 - select data observations based on
53:41 - specific criteria so uh let's say uh we
53:44 - want to keep only the uh name and the
53:47 - information of people in this data frame
53:50 - that got a salary uh higher than certain
53:52 - threshold so let's say uh people whose
53:55 - salary is is larger than
53:58 - 100K so for that what we need to do is
54:01 - like always mentioned the name of the
54:02 - data frame which should be filtered and
54:05 - then here what we need to do is to add a
54:07 - square parenthesis and within the square
54:09 - parenthesis we need to specify the
54:11 - constraint so uh the uh first part so
54:15 - this part where we are specifying the
54:16 - name of the data frame and then the
54:18 - square parenthesis this mentions that
54:20 - look into the data frame and select
54:22 - specific observations so keep only those
54:25 - observations in this data frame the
54:27 - second thing you need to mention within
54:29 - the square parenthesis is the actual
54:31 - condition that this observation should
54:32 - satisfy in order to be uh kept and not
54:35 - to be filtered out for that what we need
54:38 - to pursue is to specify the condition
54:41 - and the column based on which the
54:42 - condition uh should be conducted so data
54:46 - and the square parenthesis and then the
54:49 - name of the variable based on which we
54:51 - are doing the filtering is the salary so
54:54 - therefore we are saying look into the
54:56 - specific column so data salary and then
55:00 - we are saying that the salary should be
55:02 - larger than
55:04 - 100,000 so in this way what we are
55:07 - telling to python to do is to look into
55:09 - the data Frame data and then look into
55:12 - it salary column select all the
55:14 - observations for which the salary is
55:16 - larger than
55:17 - 100,000 and then only provide the data
55:20 - corresponding to those observations so
55:22 - let's go ahead and check it whether this
55:24 - doing everything correctly here we go so
55:27 - as you can see we are removing all the
55:30 - observations from our data frame that do
55:32 - not have a salary higher than 100,000
55:35 - and only keeping the observation to
55:37 - salary is larger than the threshold so
55:40 - as you can see we no longer have for
55:42 - instance Anna in our data frame we no
55:44 - longer have uh the Karan in our data
55:47 - frame or um the fish observation which
55:51 - corresponds to Kevin and then s is also
55:53 - not included in our data but only the
55:55 - people whose salary is larger than 110k
55:59 - are included in this data frame so let's
56:02 - say we want to add an extra condition so
56:05 - we not only want to have people whose
56:07 - salary is larger than 100,000 but we
56:10 - also want to filter our people whose
56:13 - salary is too high for instance whose
56:15 - salary is larger than 200k and in this
56:18 - case what we expect is uh from this data
56:20 - frame Bob should be removed so as you
56:23 - can see now with this current filtering
56:25 - Bob is still included but then if we uh
56:28 - also uh remove the people whose saler is
56:31 - larger than 200k then Bob should be
56:33 - removed so let's go ahead and learn how
56:35 - to do multiple filtering and here what
56:37 - we will use is the uh end operation
56:41 - because we want the two of the
56:43 - conditions mentioned here to be
56:45 - satisfied so for that we need to
56:47 - separate the conditions using this
56:51 - parenthesis so then I will add here the
56:53 - second condition which will be based
56:55 - once again
56:57 - uh on the variable called
57:00 - salary and then we are saying that the
57:03 - salary should be smaller than
57:06 - 200,000 here we
57:08 - go so this symbol stands for end and as
57:12 - you can see now we no longer uh get the
57:15 - information of the Bob because Bob got a
57:17 - salary above 200k so in this way you can
57:21 - specify not just one but multiple
57:23 - conditions that your observation should
57:25 - satisfy in order should be filtered in
57:27 - this new data
57:28 - frame and uh the final thing that we
57:31 - will look into is to how filter your
57:34 - data not based on the larger or smaller
57:37 - but based on specific values so uh let's
57:40 - say you are interested in a data of
57:43 - people whose age is equal to uh 65 and
57:48 - uh 20 so in this case uh you cannot uh
57:52 - you can no longer say that the age
57:54 - should be larger than certain volume
57:56 - smaller than certain value or even if
57:58 - you do that it will be much more
57:59 - complicated then to just to just help to
58:02 - python to select all the observations
58:05 - for which the AG is equal to to this
58:07 - specific values and in those cases uh
58:10 - the uh eing functionality comes very
58:12 - handy so for that what we need to do is
58:15 - to Simply once again take the name of
58:18 - the data frame and then here we'll be uh
58:22 - mentioning again the name of the
58:24 - variable that we need to look into to
58:26 - which is
58:28 - age and then here we need to specify
58:31 - that is in and what this uh function
58:34 - basically does is that it looks into
58:36 - specific values for this age and only
58:38 - keeps the observation that satisfied to
58:41 - uh those values so uh in here I
58:43 - mentioned that I want information only
58:45 - for people with an age of 65 and 20 for
58:48 - that I need to put uh those two values
58:51 - within an array because they are not
58:53 - just one but two values
59:06 - here we go so as you can see now we are
59:08 - getting only the data corresponding to
59:10 - people whose age is equal to 65 or 20 so
59:14 - this is how you can use the easing
59:16 - functionality to filter for specific
59:18 - values in your data and this becomes
59:20 - even more handy when you are dealing
59:22 - with string type of variables so you can
59:24 - no longer say that the corresponding
59:26 - uh column the observations volue should
59:28 - be larger than or smaller something
59:30 - because you dealing with shrinks and in
59:32 - those situations the easy can be really
59:35 - handy so it's really worth to know how
59:37 - to do The Field Ching based on specific
59:39 - values and this is all for this demo
59:41 - where we learned about grouping
59:42 - filtering and sorting in
59:44 - Python stay tuned and I will see you in
59:47 - next
59:48 - demo hi there and welcome to another
59:50 - demo in this demo we are going to talk
59:53 - about descriptive statistics we are we
59:55 - are going to to learn about calculating
59:57 - the mean calculating the standard
59:59 - deviation the variance the mode the
60:01 - Medan different percent has Quant tiles
60:04 - for arrays as well as we are going to
60:06 - learn how to get the descriptive
60:08 - statistics table for pendis data frame
60:11 - so descriptive statistics play very
60:13 - important role in analyzing and
60:16 - summarizing your data here are a few
60:19 - reasons why I believe knowing how to
60:21 - calculate descriptive statistics is
60:22 - important and also how you can use it so
60:26 - first of all descripted statistics help
60:27 - us to summarize our data so descripted
60:31 - statistics provides a coincide summary
60:33 - of the main features of your data set it
60:36 - can help you to understand the data by
60:38 - providing measures such as the mean
60:40 - dispersion so uh the variance and the
60:44 - shape of the distribution summarizing
60:46 - your data will help you to gain more
60:48 - insight about your data in an efficient
60:51 - way making it much easier to interpret
60:53 - as well as to communicate with other
60:55 - people during the presentations whenever
60:57 - you want to explain something about your
60:59 - data and also it's the essential part of
61:02 - every data science and data analytics
61:05 - projects the first thing you need to do
61:07 - is to obtain the descriptive statistics
61:09 - about your data and present it to your
61:11 - stakeholders in order to tell a story
61:13 - about your
61:14 - data another thing you can do with
61:16 - descriptive statistics is to perform
61:18 - data exploration so descriptive
61:20 - statistics is a great way a good
61:23 - starting point for exploring and
61:25 - understanding your your data it provides
61:27 - an overview of a data distribution it
61:30 - can help you to identify certain
61:31 - patterns it can help you to identify
61:33 - outliers by looking at the mean as well
61:36 - as the minimum and the maximum of your
61:38 - different variables it can also help you
61:41 - to identify potential data issues so the
61:44 - outliers noise in your data as well as
61:46 - missing values it can also help you to
61:49 - understand the type of variables you are
61:51 - dealing with because if you get an
61:53 - output from your descriptive statistics
61:55 - you'll get understanding whether you are
61:57 - dealing with a categorical string
61:58 - variables or numeric variable or a
62:00 - floating data point uh or an integer
62:03 - data point
62:04 - Etc it will help you to further
62:07 - investigate your data and to have a good
62:10 - understanding what should be uh your
62:12 - steps in order to clean your data and to
62:14 - prepare your data in the best possible
62:16 - way for your machine learning
62:19 - model another thing you can do by using
62:22 - descriptive statistics is to get an
62:24 - understanding whether the some
62:26 - that you have sampled from the main
62:28 - population it's a good representation of
62:30 - your population so as part of the
62:32 - fundamental statistic section in this
62:34 - course we learned about the difference
62:36 - between sample and population and how we
62:38 - use sample and we randomly sample a
62:40 - small part of your entire population in
62:43 - order to make conclusions about your
62:45 - population but then the criteria is that
62:47 - your sample should be a true and an
62:50 - unbiased representation of your
62:52 - population by using descriptive
62:54 - statistics you can then compare for
62:56 - instance the mean and the variance of
62:59 - your sample to the actual population in
63:01 - order to get an understanding whether
63:03 - you are dealing with a good sample or
63:05 - whether you need to go back and then
63:07 - sample again in order to get a good
63:11 - sample another thing you can do is to
63:13 - visualize your data so by using
63:15 - descripted statistics you can visualize
63:17 - your data in order to represent it to
63:19 - different stakeholders you can use
63:21 - histograms you can use box PLS you can
63:24 - use bar charts or pie charts in order to
63:26 - represent your data in a clear and
63:28 - understanding
63:31 - way so without further Ado let's learn
63:34 - how to calculate different statistics
63:36 - for our array so here I've created an
63:39 - array which consists of the following
63:40 - numbers as you can see 100 205 20 45 -
63:46 - 100 and
63:47 - 46 so first thing we are going to do is
63:50 - to calculate the mean the mean is the
63:53 - average of a set of numbers and in this
63:55 - case it will be the mean of all these
63:57 - numbers and it is calculated by summing
63:59 - up all these different numbers that we
64:01 - have here in the set and dividing it
64:03 - through the number of uh observations we
64:06 - got here so as you can see the length of
64:09 - this of this array is equal to seven
64:11 - which means that we need to sum up all
64:12 - these values and divide it to seven and
64:14 - this will be our mean or the average we
64:18 - can do that by using the nonp library so
64:23 - NP do mean and this the function that
64:26 - will calculate the average and then here
64:28 - within the parenthesis we need to
64:30 - specify what is that the variable or the
64:33 - array that we want to calculate the
64:35 - average
65:03 - let's calculate and let's see what that
65:05 - value is so as you can see the mean of
65:07 - this array is equal to
65:10 - 9043 thing we are going to learn is how
65:13 - to calculate the median so sometimes
65:15 - when we are dealing with sampling
65:17 - distributions the sampling distributions
65:19 - might be skewed so uh it can be left
65:22 - skewed or right skewed for those cases
65:25 - calcul ating the mean might not be the
65:27 - smartest thing to do and therefore it
65:30 - would be better to calculate the median
65:32 - because the median is then the better
65:35 - representation of the overall uh data
65:38 - instead of the mean so uh it's usually
65:40 - handy to calculate both of them the mean
65:42 - and the median and whenever the mean is
65:44 - different from the median it means that
65:46 - you are dealing with a CU distribution
65:49 - so the median is the middle value in a
65:51 - set of numbers when they are arranged in
65:53 - an ascending or descending order
65:56 - if there is an even number of values
65:58 - then the median is the average of the
65:59 - two medial elements and uh the median is
66:03 - also the second quantile from the
66:05 - statistical terminology so it's the 15th
66:10 - percentile let's go ahead and learn how
66:12 - to calculate the median for this array
66:14 - so
66:16 - median underscore is equal to an NP dot
66:21 - as you can see we are already getting
66:22 - the recommendation of the function
66:24 - called median of very
66:26 - straightforward and let's go ahead and
66:29 - print
66:31 - this median o
66:39 - array here we
66:42 - go so as we can see the median of this
66:45 - area is equal to 20 one thing that we
66:47 - can also see is that the mean is very
66:49 - close to the median indicating that we
66:51 - most likely are not dealing with this CU
66:53 - distribution
66:56 - next thing we are going to learn is how
66:57 - to calculate the mode the mode is a
66:59 - value or the values in the set of
67:02 - provided numbers that occurs the most so
67:05 - in our case in our data array we can see
67:09 - that there is a single value that
67:10 - appears the most which is two times and
67:13 - that's the value 20 and this is the mode
67:16 - so in this to calculate the mode of our
67:18 - array we are going to use a different
67:20 - Library so we are not going to use the
67:22 - nonp and instead we are going to use the
67:24 - scipi so the reason for this is because
67:27 - uh n does not contain this corresponding
67:30 - uh functionality to obtain the mode and
67:33 - the reason for that is is because a mode
67:36 - is not a popular measure measure of
67:38 - central ascendency usually we only
67:40 - calculate the mode the median or the
67:43 - mean uh of the data and we are good to
67:46 - go but then uh if you want to calculate
67:49 - the mode it it's still U useful to know
67:52 - how to use this corresponding library to
67:54 - do that so let's go ahead and for that
67:57 - import from
67:59 - scipi the library called
68:03 - stats and then from here we will use the
68:07 - starts. mode function to calculate the
68:11 - mode of our array
68:41 - so as you can see the output is slightly
68:44 - different but the uh idea is the same so
68:47 - as you can see that the mode of this
68:50 - array is this value so 20 and it appears
68:53 - two times so we are getting two uh
68:55 - outputs we are getting the actual value
68:59 - so the value that appears the most and
69:00 - also how many times it appears so count
69:03 - and it's equal to two because we got two
69:05 - of those 20s in our
69:08 - data the next thing we are going to
69:10 - learn is how to calculate the variance
69:12 - and the standard deviation so variance
69:15 - measures the spread or the dispersion of
69:18 - the data it quantifies how far your
69:21 - number is from the mean and a higher
69:24 - variant indicator greater variability
69:26 - your data and lower variant indicate the
69:28 - smaller variability in your data and
69:31 - standard deviation is highly related to
69:33 - the variance the two are basically uh
69:36 - explaining the same thing only standard
69:38 - deviation is the uh square roof of the
69:42 - variance and it is at the level of your
69:45 - uh numbers which makes it uh more
69:48 - prepared whenever it comes to
69:49 - interpreting the results it's the square
69:52 - root of the variance as I mentioned and
69:54 - it provides a standard
69:55 - a way to explain and understand the
69:58 - average distance between each data point
70:01 - and the mean therefore it's uh almost
70:04 - always prefer to uh use standard
70:07 - deviation when you are explaining your
70:09 - data and how much variability is there
70:11 - in your data so let's go ahead and use
70:14 - the nonp library once again in order to
70:17 - calculate the variance and the standard
70:19 - deviation of this
70:24 - data so
70:26 - variance underscore is equal to np. VAR
70:33 - and then
70:34 - data and the standard deviation of the
70:37 - SD uncore is equal to NP do
70:43 - ACD
70:46 - data let's go ahead and print it the
70:54 - variance for
71:30 - here we go as you can see here we are
71:32 - getting the variance of our data and the
71:35 - standard deviation of our
71:38 - array all right so this about
71:40 - calculating various statistics given the
71:42 - array another thing that would be worth
71:45 - to know is how to get the descriptive
71:47 - statistics whenever you are dealing with
71:49 - the data frame so let's go ahead and
71:52 - bring our data that we used previously
71:58 - so you might recall that we saw
71:59 - previously this uh data set uh
72:02 - describing the percentage of woman that
72:04 - completed certain Bachelor uh
72:07 - studies let's also import the pend this
72:11 - data
72:15 - frame as we need that to load this data
72:18 - frame as well as to uh compute the
72:20 - descriptive statistics
72:26 - so this how the data looks like and uh
72:29 - most of the time whenever we are dealing
72:31 - with the pendis data frame we want you
72:33 - get the uh nice descriptive statistics
72:36 - table that will describe our data and we
72:39 - can simply do that by using this uh nice
72:42 - functionality in pandas so we need to
72:45 - specify the name of the data frame and
72:47 - then this CBE and this will go ahead and
72:52 - print for us the descriptive statistics
72:55 - of this data
73:03 - Fame let's go ahead and print
73:16 - that here we go so this how the
73:19 - descriptive statistics table looks like
73:22 - when it comes to the pendis data print
73:24 - so we are get getting the count so the
73:26 - number of observations that we got per
73:29 - variable now we are getting the mean
73:32 - which we just s uh for an array now we
73:35 - are getting the mean perir column in our
73:38 - data frame then we have DD which stands
73:41 - for the standard deviation then we got
73:44 - the mean which is a minimum value per uh
73:47 - column in our data frame then we have
73:49 - the 25 percentile which is the uh lowest
73:54 - 205
73:55 - uh percentile in your column which is
73:58 - basically the first quanti from the
74:00 - statistical uh point of view and then we
74:03 - got the uh 15 percentile which is the
74:06 - median what we also just saw when we
74:08 - were calculating the median of uh array
74:12 - 15 percentile is also the second
74:14 - quantile from uh statistical terminology
74:17 - then we got the 75th percentile which is
74:20 - the third quantile and then we got the
74:23 - maximum which is the maximum
74:25 - corresponding to that column so as you
74:27 - can see this is a great way to summarize
74:29 - your data so you can look for instance
74:31 - the year and you can see that the
74:33 - minimum of the year is uh
74:36 - 1,970 the maximum is 2011 which means
74:40 - that you can say that you have a data
74:42 - spanning from uh 1970 till
74:46 - 2011 then you have the mean uh in case
74:49 - of here it's not really meaningful but
74:51 - when you look at uh other columns for
74:53 - instance when we look at the
74:55 - architecture which describes the P
74:57 - percentage of women that completed the
74:59 - study of architecture across different
75:02 - years you can see that uh across the
75:05 - years so spanning from 1970 till
75:08 - 2011 there were uh on average
75:12 - 34% woman who completed this study so in
75:16 - this way you can then uh tell a story
75:18 - about your data and you can also
75:20 - Identify some problems in your data so
75:23 - this is all for this demo where we learn
75:25 - how to calculate different statistics
75:27 - for an array as well as how to get the
75:29 - descriptive statistics table whenever we
75:31 - are dealing with a pandas data frame so
75:34 - this is all for this demo and I will see
75:36 - you in the next one if you're looking
75:38 - for machine learning deep learning data
75:40 - science or AI resources then check out
75:43 - the free resources section in
75:45 - lunch. or our YouTube channel where you
75:48 - can find more content and you can dive
75:50 - into machine learning and in
75:53 - AI hi there and welcome to another demo
75:56 - in this demo we are going to learn how
75:58 - we can combine so we can merge different
76:00 - tables that we have in our database
76:03 - because most often we get our data not
76:05 - in one file but in multiple files and
76:08 - sometimes we need to do some
76:09 - pre-processing some filtering and then
76:11 - at the end we need to join multiple
76:13 - tables together in order to end up with
76:15 - a single table such that we can use that
76:17 - in our analysis our data visualization
76:20 - any our machine learning training
76:22 - process and we will train our model only
76:24 - on a single data set for that you need
76:27 - to know what are all the different
76:28 - joints out there what are the possible
76:31 - combinations what are the possibilities
76:33 - and how you can do each of them in
76:35 - Python so uh in here in this picture you
76:39 - can see the most popular joints out
76:41 - there and we have here a left joint we
76:44 - have an inner joint we have the right
76:46 - joint we have left anti joint and right
76:48 - an The Joint so let's so uh in here we
76:52 - will be looking at two tables we have
76:55 - have a table X which will contain
76:57 - certain features and then certain
76:59 - observations and then we will have a
77:01 - table Y which will contain a different
77:04 - table with different observations and
77:06 - different features and our goal is to
77:08 - merge the two tables in different ways
77:11 - and the way we can do that it really
77:13 - depends on uh what kind of joint we want
77:16 - to do so let's go each of these joints
77:19 - one by one we will go into definition
77:22 - and here we will use the idea and we
77:24 - will assume that there is one key so
77:26 - there is a key identifier present both
77:28 - in table X and table y that we can use
77:32 - in order to find out whether a certain
77:34 - observation exist in X and in y or not
77:38 - because whenever we are trying to merge
77:40 - two tables we need at least one key
77:43 - identifier to use that to do the merge
77:46 - uh on that so let's say we have a table
77:50 - containing the sales of a a shop and
77:53 - then we have a table containing the
77:56 - customers of the shop at least we need
77:58 - to know the identifier of a shop in
78:01 - order to say that this customer in this
78:03 - shop has both this item and then uh this
78:07 - shop had the corresponding sales in this
78:10 - way the shop identifier will be the key
78:12 - identifier to be used in order to merge
78:14 - this uh sales data with the customer
78:17 - data based on the shop so using this
78:21 - idea we will then look into this
78:23 - different joints so first we will look
78:25 - into the inner joint and by definition
78:27 - an inner joint returns only the matching
78:30 - rows so what you see here the
78:32 - intersection between two tables based on
78:34 - a common column which will be the key
78:36 - identifier so um if we have certain
78:39 - observations that are in table X but
78:42 - then they are not in table Y which means
78:44 - that we are talking about all these
78:46 - observations not highlighted and uh if
78:49 - we have certain observations that are in
78:51 - table y but not in table X so those are
78:53 - all this uh on this part of the Y that
78:56 - is not highlighted then all these
78:58 - observations will not be included in the
79:00 - final joint table and we will only end
79:03 - up with observations that are in this
79:05 - intersection so they appear both in X
79:08 - and in y so in terms of the example that
79:11 - I just mentioned it means that we will
79:13 - be only keeping the data for the shops
79:16 - for which we have both the sales
79:17 - information and the customer purchase
79:20 - information the result will include only
79:23 - the rows where the key ke values will be
79:25 - present in both table X and in table y
79:29 - let's now look into the left joint so in
79:31 - here you can see the left joint and the
79:34 - uh definition of the left joint is that
79:36 - a left joint returns all the rows from
79:38 - the left table and the matching rows
79:41 - from the right table in this case table
79:44 - y based on the common column so the key
79:46 - identifier so if we have certain
79:48 - observations that we do not have
79:51 - information about in the table X so
79:55 - those are all the observation in here so
79:57 - in table y that we uh do not have in
80:00 - table X then those observations will not
80:03 - be included in the final output and
80:05 - inste all the observations in X
80:08 - independent where they are in table y or
80:11 - not they will be included so basically
80:14 - we are selecting all this part of the uh
80:17 - two joints so this will be our output so
80:22 - the result will include all rows from
80:24 - the left table so table X and the
80:27 - matching rows from the right table of
80:29 - table Y and if there is no match it
80:32 - includes uh n values for the coms of the
80:34 - right table so you will see some uh no
80:37 - values appearing in your end result
80:40 - because there will be cases for which uh
80:43 - you can see that observation contains
80:45 - information in here from table X but not
80:48 - in from table
80:49 - y then another interesting joint uh to
80:52 - look into is the right joint right joint
80:55 - is basically the exact opposite of the
80:57 - left joint so right joint returns all
80:59 - the rows from the right table so table Y
81:03 - and the matching rows from the left
81:04 - table so table X based on the common
81:07 - column so those are all the matching
81:09 - rows and those are all the rows that are
81:11 - only in table Y and this will be the
81:14 - output of our table and then we have the
81:17 - left anti joint and right anti joint
81:19 - left anti joint is very um uh it's kind
81:22 - of uh close to the left joint but uh it
81:26 - is basically the um derivation from the
81:30 - left joint and inner joint so um unlike
81:33 - in the left joint where we were
81:35 - including both the observations that
81:37 - were matching in observations that were
81:39 - only present in X in case of left anti
81:42 - join we are only including so the output
81:45 - will be only the observations which are
81:49 - not matching and they are only in X so
81:53 - this highlighted part so by definition
81:56 - left antig returns all the rows from the
81:58 - left table so table X that do not have a
82:01 - match in the right table based on a
82:03 - common column so it will be only this
82:06 - part and in case of right anti join it
82:09 - is the exact opposite of the left an The
82:12 - Joint so by definition a right on the
82:15 - joint returns all the rows from the
82:16 - right table so table y that do not have
82:19 - a match in the left table X based on a
82:22 - common column so let's say we are
82:25 - looking at this example of a shop for
82:28 - which we have the sales information and
82:29 - for which we have the customer purchase
82:31 - information if we um only want to have
82:35 - the um customer information for the
82:37 - shops that do not appear in the sales
82:40 - data so uh the sales data is the table X
82:43 - right and the customer purchase data uh
82:46 - par shop is the table Y and we want to
82:49 - only have information for the shops and
82:53 - their customer purchase
82:54 - information for which we do not have the
82:57 - corresponding sales data so in those
83:01 - cases you can then use the right on the
83:03 - join it might not be reasonable for this
83:06 - specific example but sometimes whenever
83:09 - we are look sules in our database
83:12 - Sometimes using the left on the joint
83:14 - and right on the joint might be handy
83:16 - therefore it's worth to know how to do
83:18 - that in
83:21 - Python so here we have two different uh
83:24 - small data frames that I created in
83:27 - Python uh data one and data two and uh
83:30 - in the First Data frame we have uh in
83:32 - total of seven observations so you can
83:35 - see a b c d e f and in the second one we
83:38 - have the CDE e fgh h and the
83:41 - corresponding indices are from 8 to 13
83:44 - whereas for the data one the indices are
83:46 - from 1 to 7 so note that we do not have
83:49 - any uh intersecting indices because we
83:53 - are going to merge this data and we want
83:55 - to avoid case when we have a different
83:58 - value corresponding to the same index so
84:02 - without further Ado let's first learn
84:04 - how to do an inner joint between these
84:06 - two data frames but before that let's
84:08 - actually look into them so let's print
84:11 - the data one and the data two
84:26 - and as you can see here we are getting
84:28 - our two data frames so you can see that
84:30 - this is the data one and this is the
84:32 - data
84:34 - to so uh what we are going to do is to
84:37 - learn how to do an inner join so how to
84:40 - do a merge where our way of joining is
84:43 - the inner join so for that let's first
84:47 - call uh the a corresponding data frame
84:50 - that we want to create so merge and then
84:52 - inner join
84:54 - and this will be equal and here we are
84:57 - going to use the pandas merge function
85:00 - so
85:01 - merge and then whenever we are using
85:04 - this we first need to specify the data
85:07 - frames that we want to merge so in our
85:09 - case it's data one and then data two and
85:13 - then what we need to do is to use the
85:16 - argument on to specify the uh key so the
85:20 - identify that we are going to use in
85:22 - order to merge the two data frames in
85:25 - this case it is the key because um it
85:28 - does make sense to join the data frames
85:32 - on a variable which uh we do have
85:36 - something in common in the uh other
85:38 - table and in this case as you can see we
85:41 - have certain um rows so for instance the
85:44 - letter c letter d letter E and F appear
85:48 - in both data one and data two and as we
85:52 - have only two variables only one of that
85:54 - makes sense to be used as an
85:57 - identifier so uh in here we will use the
86:01 - the key as our um variable based on
86:05 - which we are going to do the join and
86:07 - then we need to specify the exact uh way
86:11 - we want to do our join so the inner join
86:15 - uh has the corresponding uh
86:17 - parameter uh of inner so whenever you
86:21 - are writing inner as a volume for an
86:23 - argument how then python understands
86:25 - that you want to do an inner join
86:27 - between a data frame one and data frame
86:31 - two so let's go ahead and actually print
86:34 - this so merge and then inner join and um
86:39 - before um printing the output let's
86:42 - actually um understand uh what is the uh
86:45 - expected output so uh there are common
86:49 - uh keys that appear both in data one and
86:52 - data two and um those are the uh row uh
86:57 - corresponding to the key c key D and
87:00 - then uh e and then F and the g so we
87:04 - expect that those keys that appear both
87:07 - in data one and data two and their
87:09 - correspondent values will then be um in
87:12 - the uh inner join so let's go ahead and
87:15 - print
87:16 - it here we go so as you can see as
87:20 - expected we are getting the uh value one
87:23 - and value two corresponding to the key
87:25 - CDE e FG in this output because those
87:29 - are the ones that appear both in table
87:31 - uh data one and in table data
87:34 - too another thing that we are going to
87:36 - learn is how to do left join so as we
87:38 - learned left join uh will provide all
87:41 - the values that appear in the uh first
87:45 - table in the left table as well as all
87:48 - the matching values that appear both in
87:50 - the left table and in the right table so
87:53 - what we need to do for that is basically
87:57 - to have the same so left joint so let's
88:03 - rename it to avoid
88:07 - confusion and then here the only thing
88:09 - that we are going to do is to change
88:11 - this to
88:17 - left let's go ahead and print
88:21 - this and this is our left join so as you
88:25 - can see we are getting all the values so
88:28 - all the keys that we got in a data one
88:32 - and we are also getting all the keys
88:34 - that appear in data two so as you can
88:38 - see in data two we got the key CDE e FG
88:43 - so those
88:44 - values which also appear in here you can
88:47 - see that the value two uh corresponding
88:50 - to 8 9 10 11 and 12 also here and here
88:55 - but then the other values for which we
88:58 - do not have the uh matching uh
89:01 - observations uh in the um table uh data
89:05 - one they do not appear in here so here
89:07 - you can see that we have non
89:08 - corresponding to the uh key a and the B
89:12 - Because those keys do not appear in the
89:15 - uh second table in the left
89:19 - table so this is the output of the left
89:22 - joint as you can see um the uh variables
89:26 - that only appear in the right table they
89:29 - end up getting some nonv values but all
89:32 - the uh fields and observations that were
89:34 - included in the first table so data one
89:37 - they do appear uh in full in
89:41 - here now let's go ahead and do our right
89:44 - joint so um as we just learned right
89:47 - joint is basically the opposite of the
89:49 - left joint which means that we can
89:52 - expect to get all the rows from the data
89:55 - two so the right table but then uh some
89:58 - of the rows that were in data one but
90:01 - they were not in the data two they will
90:03 - not appear so we will get only the
90:05 - matching ones and the ones that are
90:07 - present in the data to so let's also
90:10 - change the parameter in the argument how
90:12 - by right and we should see some nonv
90:15 - values in the value one because value
90:18 - one is a variable appearing in the data
90:20 - one
90:26 - so as expected here we are getting all
90:29 - the matching keys that are both present
90:31 - in the table one data one and data two
90:35 - as well as some nonv values
90:37 - corresponding to the value one because
90:39 - this row does not appear in the data one
90:42 - but it is present in data two and then
90:45 - all the rows that are present in data
90:48 - two are in
90:52 - here and finally let's go and learn how
90:55 - to do left on the join and the right on
90:57 - the join I will leave it to you but the
90:59 - idea should be the same as it's uh in
91:02 - the in its concept they are very similar
91:05 - so in case of left on The Joint we need
91:07 - to uh go the extra mile and do some
91:10 - extra steps in order to get our left on
91:12 - The Joint so the first thing we are
91:15 - going to do is to do a left joint and
91:18 - then from the left joint we want to
91:20 - remove the intersection part so um if
91:23 - you bring back uh the um diagram that we
91:26 - just show uh you might recall that the
91:29 - in case of left anti join we are uh
91:32 - doing something very similar as the left
91:34 - join only instead of uh also uh choosing
91:38 - the matching observations that appear
91:40 - both in the left and right tables in
91:43 - case of left anti joint we are only
91:44 - selecting observation that appear in the
91:47 - left table but not the matching
91:49 - observation so we are removing the
91:51 - intersection part therefore what we are
91:54 - first going to do is to do the uh left
91:57 - joint so uh let's call it merge and then
92:02 - uh left on T it's equal to and then pd.
92:09 - merge so the same function and here we
92:11 - are going to specify the uh left table
92:14 - and then the right table and then once
92:17 - again the uh on argument here we are
92:20 - specifying the variable based on which
92:22 - we are doing the Jo
92:24 - and then how we are going to join which
92:26 - is equal to left as we want to do left
92:30 - on this so first we want to do the um
92:33 - the left join and uh we also this time
92:36 - want to uh save the indicators uh that
92:41 - uh come as a result from the joints so
92:45 - um by default the uh indicator is
92:49 - actually set to false and we are going
92:52 - to change that and we are going to set
92:53 - it to true and what this indicator does
92:57 - is that it shows whether the observation
93:00 - belongs to uh the left table only or it
93:04 - belongs uh to the matching part so um it
93:08 - shows whether the observation was in the
93:11 - table data one only or uh whether the
93:15 - observation belongs both to the data one
93:18 - and data two so it is in the
93:19 - intersection part and then using this
93:22 - indicator and using this classification
93:24 - that will come from this indicator we
93:26 - can then identify all the observations
93:28 - that were part of the
93:31 - intersection and we can remove them and
93:33 - we will end up with all the observations
93:35 - that belonged only to the left table so
93:37 - the data one exactly what is the point
93:40 - behind left on The Joint so let's go
93:43 - ahead and print the output of this table
93:46 - to uh show you what this uh indicator
93:48 - does
93:54 - so as you can see now we are getting
93:56 - pair observation beside of the left
93:58 - joint on the two tables we are also
94:00 - getting this underscore merge column
94:02 - which says whether the observation
94:04 - belongs only to the left part or it
94:07 - belongs to the both parts so it's in the
94:09 - matching in in the inter intersection
94:12 - zone so as you can see we got two
94:14 - observations so observation with the key
94:17 - A and A B that belongs only to the table
94:20 - data one and those are the observations
94:23 - that we want to keep and we want to
94:25 - remove all these observations from the
94:27 - intersection zone so from the uh Keys c
94:31 - d e f and
94:33 - g all right so next thing we are going
94:36 - to do is to Define our left R uh join
94:41 - data so let's do merge and then
94:45 - underscore left
94:48 - unjin sure let's make this to left
94:51 - because it makes quite more sense
94:53 - and then we will use this data
94:58 - frame and here we are going to apply the
95:01 - filtering that we learned previously so
95:04 - we are going to say look into the
95:08 - variable on the score merge that just
95:11 - came from the
95:13 - indicator and uh look into all the cases
95:17 - where this variable is equal
95:20 - to left uncore only
95:25 - and this will then keep only the
95:27 - observations that appear in data
95:31 - one so let's go ahead and print
95:35 - this and you will see that we will end
95:38 - up only keeping the observations for
95:40 - which the uh for which the underscore
95:42 - merge will be equal to left underscore
95:47 - only here we
95:50 - go and this our left on The Joint of
95:53 - course we we don't want to keep this
95:54 - underscore merge anymore cuz we have
95:56 - already used it and uh there is no
95:59 - purpose of keeping it so what I will do
96:01 - is I will drop it so let's do it
96:05 - actually in a new
96:08 - line I will put the name of the data
96:11 - frame I will do Dot and then drop and
96:15 - then here I will do uncore merge because
96:19 - this the variable that I want to drop
96:21 - and I need to specify the AIS so as it's
96:25 - a column I want to remove a column I
96:27 - need to specify that the AIS should be
96:28 - equal to one in Python the AIS equal to
96:31 - zero means rows and in X is equal to one
96:35 - it means column underscore merge is a
96:37 - column I'm mentioning that X is equal to
96:39 - one and let's actually go ahead and
96:42 - overwrite this data frame as I want to
96:45 - keep just one copy of the data frame and
96:49 - this is the output here we go so this is
96:53 - our left on The Joint as you can see it
96:55 - is bit more complicated than the left
96:57 - joint or the right joint but I think
96:59 - it's worth to know how to do it because
97:01 - sometimes it can be very useful to
97:03 - implement this in practice so I will
97:05 - leave the right under the joint to you
97:08 - and this actually completes our demo for
97:10 - today where we learn how to do left join
97:12 - inner join right join and also left anti
97:15 - join this is all and I will see you in
97:18 - the next
97:20 - demo hi there and welcome back to
97:22 - another demo in in this demo we are
97:24 - going to learn how to perform data
97:25 - visualization with math plug Li in
97:27 - Python data visualization is a very
97:30 - important technique for gaining insights
97:32 - from your data and to effectively
97:34 - communicate your findings to your
97:36 - audience whether it's presenting to your
97:38 - stakeholders or whether it's putting in
97:40 - your case study or your paper it's
97:43 - really important to know how to PL those
97:45 - visuals by using python because it's a
97:48 - simple way to go from your data analysis
97:51 - to your data visualization we sometimes
97:53 - we call exploratory data analysis or Eda
97:56 - and Eda sometimes can be the uh
97:59 - essential part of the case study to
98:01 - Showcase your data to find some
98:03 - correlations it can be also a stepping
98:05 - stone towards the next step in your case
98:08 - study whether it's closo analysis or
98:10 - modeling it can help you to identify
98:13 - features that explain your dependent
98:14 - variable it can help you to identify
98:17 - unimportant features or it can help you
98:20 - to identify a noise in your data so
98:23 - therefore it's really important for you
98:24 - to know how to make those
98:27 - visuals so the first type of
98:29 - visualization we are going to learn is
98:31 - the line plots line plots are great way
98:33 - to visualize Trends or patterns in the
98:35 - data they are great way to visualize
98:37 - time series so whatever you are dealing
98:40 - with the graph where the x-axis in the
98:42 - form of a time and then the Y AIS are
98:45 - the values that evolve over time this
98:48 - can be for instance the stock prices or
98:50 - the stock returns or the Roa of a
98:52 - company you get the idea so for that
98:55 - what I have here is a set of X values
98:58 - and Y values that I created in the form
99:00 - of an array and what we're going to do
99:02 - now is to plot this
99:04 - arrays so therefore I've imported here
99:07 - the meth PL pip plot Library so pip plot
99:10 - is a directory in the med plot library
99:13 - and um as a uh way uh of shortening the
99:17 - name of this library is calling it PLT
99:20 - so this similar to the idea of using uh
99:23 - p for the pandas and the NP for the noai
99:27 - so let's go ahead and use a library so
99:29 - it is PLT Dot and then we have a plot so
99:33 - p l o t and then here we need to specify
99:37 - first the X values and then we need to
99:39 - specify the Y Valu so the X underscore
99:44 - values and then
99:48 - y underscore values so the idea is that
99:52 - for each specific X we need to have the
99:55 - corresponding y's if your X array is
99:58 - different from the Y aray which means
100:00 - that for certain xes you don't have the
100:02 - corresponding y values or the other way
100:04 - around so for certain y you don't have
100:06 - the X values then you will get an error
100:08 - so those two areas should be the same
100:11 - and for each X you need to have the
100:12 - corresponding y values so let's go ahead
100:17 - and run this and you will quickly see
100:19 - that you're not getting any output and
100:21 - the reason for that is because in python
100:23 - whenever you are using the pi uh plot uh
100:26 - Library you need to uh specify a PLT
100:30 - show such that uh your uh plot will
100:33 - actually be
100:36 - visualized here we go so this is the
100:38 - plot that we are getting as you can see
100:40 - those are the X values starting from uh
100:43 - one and then ending with 10 and then the
100:45 - Y value starting with one and then
100:47 - ending with 20 so uh as you can see this
100:52 - PL is very based
100:53 - we don't have any extra information
100:55 - explaining it but we want our plots to
100:58 - be self-explanatory we don't want to add
101:00 - too much information and we want our
101:02 - audience to look into the graph and the
101:05 - visuals and to understand what it's
101:07 - about for that you need to uh make use
101:09 - of extra functionalities in P plot to
101:13 - add more information to your visuals for
101:16 - instance it could have been really handy
101:17 - to know what this x-axis represent or
101:20 - what what this Y axis represent or to
101:22 - have a title on the top of the visual
101:25 - saying what this graph is about all
101:27 - those can be done by using this uh
101:30 - PLT and in here we can say PLT Dot and
101:34 - then ex
101:36 - label and in this way we can add a text
101:39 - to our xaxis saying for instance what is
101:42 - the variable that the xaxis represent so
101:45 - X
101:47 - AIS Place
101:50 - holder and then the same we can do with
101:52 - the white
101:56 - AIS here we need to change y here we
102:00 - need to change y so let's say you're
102:02 - visualizing the time series of stock
102:04 - prices and your xaxis represents the
102:07 - time and then the YX represent the stock
102:10 - prices well in this case you can then uh
102:13 - put in uh xais placeholder that um it is
102:17 - the date and then the Y AIS placeholder
102:21 - can be the stock price and then the name
102:24 - of the stock that you are looking into
102:26 - and then finally we can also add a title
102:29 - pl.
102:30 - title and here then you can put the um
102:35 - uh title placeholder for now I will put
102:39 - it as some text but what you can do here
102:42 - is you can replace it with the uh title
102:45 - let's say the stock prices or stock X
102:48 - from a time period uh X to Y so in this
102:52 - way you can then add more information to
102:54 - your graph let's see how this looks like
102:56 - in the actual visualization here we go
102:59 - so here you have the title here you have
103:01 - the text under the x-axis and here you
103:04 - have the text uh beside of the y axis so
103:09 - uh another thing you can do is to uh
103:11 - work with your plot and to uh make it uh
103:15 - nicer and you can do that by for
103:17 - instance changing the way the line is
103:19 - represented so you can go from a line to
103:22 - dots uh or dashes it can be that you
103:25 - want to change the color of your plot
103:27 - this one is really popular and I think
103:29 - it's really uh Worth to know so for
103:32 - instance let's say your presentation is
103:34 - in the green uh color so it's in the
103:37 - green palette and you want your a visual
103:39 - to match the color of your palette what
103:42 - you can do for that is to use this
103:44 - argument color and here you can for
103:47 - specify that it should be of green color
103:53 - here we go so as you can see this graph
103:56 - then changes the color the plot is in
103:58 - the green and this looks uh much more
104:01 - appealing compared to what we had
104:04 - before all right so this is about line
104:07 - plus the next thing we are going to
104:08 - learn is how to plot Scatter Plots
104:11 - scatter plot can be really useful when
104:13 - we are trying to visualize a
104:14 - relationship between two variables so
104:17 - let's say we have um uh two features in
104:20 - our data and we want to understand and
104:23 - whether there is a certain relationship
104:25 - between the two whether there is a
104:27 - correlation because we want to know
104:29 - whether um we have a strong perfect uh
104:33 - correlation between the two which is uh
104:36 - something that we need to check as part
104:38 - of the linear regression model we don't
104:41 - want to have two features being um multi
104:44 - colinear and a perfect
104:47 - multicollinear and um we also want to
104:50 - check sometimes the relationship between
104:52 - the
104:53 - uh feature and between the dependent
104:56 - variable because we want them to be
104:59 - highly correlated in all those cases we
105:02 - can use and the scatter BLS as a way to
105:04 - identify this correlation and to see
105:07 - whether there is a pattern or there is
105:08 - no pattern and Scatter Plots uh can be
105:12 - uh in those exact cases super handy so
105:16 - let's say we have exactly the same data
105:19 - and instead of the line plot we want to
105:22 - have a the Scatter
105:24 - Plots so for that what we need to do is
105:27 - to do PLT do and then scatter so instead
105:31 - of plot we are using the function
105:32 - together and then once again we are
105:34 - specifying the X values and the Y
105:38 - values so let me go ahead and repeat the
105:41 - rest
105:57 - here we go so as you can see now we are
105:59 - getting the uh sket PLO and uh this is
106:03 - basically uh the same plot that we saw
106:05 - before only instead of uh lines now we
106:08 - are getting dots so this is a skatter
106:11 - plot coming from our first case study
106:14 - where we were looking into the uh what
106:16 - factors make a playist successful and
106:19 - this is a vivid example of a skatter clo
106:21 - that helps us to understand understand
106:23 - whether there is a relationship between
106:25 - the number of albums in the playlist
106:28 - versus the average recre active usage
106:30 - and also you can see that there is a
106:32 - positive relationship so in
106:34 - here so in this way using a scatter PL
106:37 - you can identify whether there is a
106:40 - relationship between a pair of variables
106:43 - whether it's your two independent
106:44 - variables whether it's your one
106:46 - independent variable and your dependent
106:50 - variable the next type of visualization
106:52 - we are going going to learn is the bar
106:54 - chart so how to plot bar charts in
106:56 - Python and bar charts can be really
106:58 - useful for comparing different
107:00 - categorical uh values so if you're
107:02 - dealing with a categorical data and you
107:04 - have a certain variable that has
107:06 - categories and you have the
107:07 - corresponding values then you can
107:09 - visualize it nicely by using the bar
107:11 - charts and uh for this I have created
107:14 - here the sample data a very basic one
107:17 - where we have the categories of in the
107:19 - terms of our names of animals so we have
107:22 - cat we have dog we have horse and we
107:23 - have Mouse and then uh we have here
107:26 - categorical uh values so uh this
107:29 - represents for instance the weight of
107:32 - animal so this is the weight of a cat
107:35 - weight of a dog weight of a horse and
107:37 - then weight of a mouse and we want to
107:39 - visualize this let's go ahead and use
107:42 - once again the library and the function
107:44 - PLT so PLT and then bar which stands for
107:49 - bar chart and here we need to First
107:51 - specify the categories and then we need
107:53 - to specify the corresponding values so
107:56 - cat and then values which stand for
107:58 - categorical values and once again we uh
108:02 - need to add the label we need to add the
108:05 - uh title and we need to add PLT show so
108:10 - here we can replace this for instance by
108:14 - um
108:17 - animals and then here we can add for
108:20 - instance
108:22 - the uh weight of an
108:26 - animal and then as a title here we can
108:31 - say um weight
108:36 - on
108:38 - weight hair
108:40 - animal and finally we want to do pl.
108:44 - show in order to show it and I also
108:46 - would like to add a color to this
108:49 - visualization so let's say I want the
108:51 - color to be
108:54 - forest green by the way if you're
108:56 - wondering what are all the possible um
108:59 - colors that can be used for this
109:01 - argument color then uh what you can
109:04 - simply do is to use the chat GPT and um
109:07 - try to search for uh different colors
109:10 - available in PIP plot and met flip and
109:13 - you will get the name of all the
109:15 - possible colors that you can use and
109:17 - then you can use a nice color palette
109:19 - that matches your presentation or your
109:22 - case
109:23 - study so let's go ahead and run
109:27 - this so as you can see we are getting
109:30 - this uh bar chart this nice
109:32 - visualization and we can see that the
109:34 - cat has the uh corresponding weight and
109:37 - then we see the dog and the horse and
109:39 - the mouse we can see for instance that
109:41 - the mouse has the smallest weight and
109:43 - the horse has the largest weight and in
109:45 - this way you can nicely visualize a
109:48 - categorical data
109:53 - so let's now learn how to plot
109:55 - histograms so histograms are useful for
109:57 - visualizing the distribution of a
109:59 - numerical data it can be that you want
110:02 - to visualize your population
110:03 - distribution or you want to visualize
110:05 - your sample data and you want to compare
110:08 - for instance your sampling distribution
110:10 - to the population distribution to know
110:12 - how your sample is representative of
110:15 - your population whether it's an unbiased
110:18 - and a true representation of the
110:19 - population which means that your
110:21 - sampling distribution should be close to
110:23 - your population distribution so for all
110:26 - those kind of test you can definitely
110:27 - use the histograms and by the way this
110:30 - is a very uh common question as during
110:33 - data science interviews when you are
110:35 - asked to uh randomly sample from a
110:38 - normal distribution or from a con
110:40 - uniform distribution and deplo this
110:42 - distribution in Python using histograms
110:45 - well that's exactly what we are going to
110:47 - learn today so you might recall from a
110:49 - demo where we learned how to randomly
110:51 - generate data and to um create a
110:55 - simulated version of a data that we use
110:57 - the Sile library to randomly sample from
111:00 - a normal distribution and here we are
111:02 - sampling from standard normal
111:04 - distribution with a mean of zero and
111:06 - standard deviation of one and here we
111:08 - are sampling 100 observations so uh now
111:12 - we are going to plug this distribution
111:14 - by using histograms and here once again
111:16 - I'm using PLT Dot and then I'm
111:19 - specifying his and then here I need to
111:22 - specify the values that I want to
111:25 - plot and
111:27 - then once again I need to specify the X
111:38 - label so as you can see here we are
111:41 - getting our sample data and here we have
111:44 - the frequency so how often we are
111:46 - getting the corresponding volum and we
111:48 - can see that the distribution of the Su
111:51 - pole is symmetric around zero as
111:53 - expected because normal distribution is
111:56 - symmetric and it's well shaped and it's
111:58 - always symmetric around its mean and we
112:01 - sample data from the standard normal
112:03 - distribution with the mean of zero and
112:05 - as you can see the standard deviation so
112:08 - how spread out the observations are from
112:11 - the mean are very close to one so um one
112:15 - thing that you will also notice is that
112:17 - as we increase this amount so we go um
112:20 - we increase the sample size so we'll
112:22 - make it for instance 2,000 then this
112:25 - distribution should look more and more
112:27 - this hisam should look more and more
112:29 - like the actual normal
112:31 - distribution here we go let's actually
112:33 - go ahead and do something bit more
112:35 - advanced to show you how this looks like
112:38 - when comparing the histogram with a
112:42 - plot and what I want to do here is to
112:45 - visualize the uh sampling distribution
112:47 - so when uh we are generating our own
112:49 - sample randomly sampled from normal
112:51 - distribution and we are comparing it to
112:54 - the actual population distribution by
112:56 - using this Norm function that comes from
112:59 - the library called CI and this one
113:02 - should be So-Cal population distribution
113:04 - and what we had before should be the
113:05 - sampling distribution let's go ahead and
113:07 - put
113:21 - this here we go so here in this part as
113:26 - you can see now we are no longer uh
113:29 - plotting the frequencies but we are
113:31 - plotting the actual probability
113:33 - corresponding to the sampling
113:35 - distribution and we are also in the same
113:37 - graph visualizing the population
113:40 - distribution so the actual normal
113:42 - distribution with the same parameters
113:44 - and the reason why I wanted uh to show
113:47 - this because uh in this way we can
113:49 - compare your uh population distribution
113:52 - to the sampling distribution and one
113:54 - thing that you will see as you change
113:56 - the uh number of observation that you
113:58 - sample is that the higher will be the
114:00 - number of observations so the sample
114:02 - size more will this uh histogram so the
114:05 - um green bars will look like to the
114:08 - actual population distribution and this
114:11 - is also the entire idea behind what we
114:13 - call Central limit theorem so if you're
114:16 - wondering what this normal distribution
114:17 - is what is sampling distribution
114:19 - population distribution what is Central
114:21 - limit the me then head towards a
114:23 - fundamental statistic section of this
114:25 - course to learn everything about this
114:29 - topics so uh one thing that you will
114:32 - notice uh in this specific graph is that
114:35 - we saw uh that there were also Legend
114:37 - added to the visualization so uh those
114:40 - are really helpful when you are plotting
114:42 - not just one but two set of data sets in
114:46 - the same plot and you want to explain
114:47 - what is the difference between them in
114:49 - our case we had this uh something dist
114:52 - tion and we had a population
114:54 - distribution and we had to specify that
114:56 - the uh bars correspond to the sampling
114:58 - distribution and the plot so the line
115:00 - corresponds to the uh population
115:02 - distribution well for that we use what
115:05 - we call Legend and as you can see the
115:07 - way that I'm doing it is by using pl.
115:10 - Legend and what I'm doing here is simply
115:12 - specifying the X values using the nonp
115:15 - range function which arranges values
115:18 - between the minimum and the maximum and
115:20 - then the corresponding incremental value
115:23 - and then we have here the X values which
115:26 - uses the norm function coming from the s
115:29 - pi to um to uh generate the
115:33 - corresponding probability distribution
115:35 - uh values so the probabilities and then
115:38 - here I'm specifying the counts number of
115:40 - beans that I want to be visualized and
115:43 - also what should be ignored so uh here
115:45 - I'm using the histogram and then I'm
115:48 - specifying the first the histogram that
115:51 - I want to plot which is similar to what
115:53 - we had before and uh the only difference
115:56 - is that I'm specifying that we are
115:57 - dealing with a density which means that
115:59 - it's going to plot the probabilities
116:02 - instead of frequencies then I'm
116:04 - specifying the color and then I'm
116:05 - specifying the label so the way that the
116:08 - legend works is that I need to specify
116:10 - per plot what is the name of the plot by
116:13 - using this label argument and then once
116:17 - you add all these labels and then you
116:19 - add here pl. Legend it will then pair
116:22 - plot so per typee of visualization it
116:25 - will then in the right corner or
116:26 - somewhere in the left corner it will
116:29 - then specify the name of the
116:34 - plot and if you want to see similar
116:36 - visualizations how you can for instance
116:39 - randomly draw observations from B
116:41 - distribution from binomial distribution
116:43 - exponential distribution geometric
116:45 - distribution normal distribution poison
116:48 - distribution or student T distribution
116:50 - or uniform distribution and how you can
116:53 - visualize them using the histograms then
116:55 - head towards this GitHub repository that
116:58 - I will post as part of the resources
117:00 - where you can see all those
117:01 - visualizations and their corresponding
117:03 - python codes and this is as part of this
117:06 - mathematics statistics or data science
117:08 - GitHub repository that I
117:10 - created and this actually concludes this
117:13 - demo where we'll learn how to create
117:14 - different visualizations in Python using
117:17 - met PL flip we learned how to do line
117:20 - plots how to do Scatter Plots how to do
117:22 - bar charts and histograms and also how
117:24 - to combine for instance the plots and
117:26 - the histograms in a single visualization
117:30 - this video was sponsored by lunarch at
117:32 - lunarch we are all about making you
117:34 - ready for your dream job in Tech making
117:38 - data science and AI accessible to
117:41 - everyone with is data science artificial
117:44 - intelligence or engineering at lunar
117:46 - Tech Academy we have courses and boot
117:48 - camps to help you become a job ready
117:51 - professional we are here to help also
117:53 - businesses and schools and universities
117:56 - with a topn training modernization with
117:59 - data science and AI corporate training
118:02 - including the latest topics like
118:03 - generative AI with lunar Tech learning
118:07 - is easy fun and super practical we care
118:11 - about providing an end to-end learning
118:13 - experience that is both practical and
118:15 - grounded in fundamental knowledge our
118:18 - community is all about supporting each
118:20 - other making sure you you get where you
118:22 - want to go ready to start your Tech
118:25 - Journey lunner Tech is where you begin
118:28 - for students or aspiring data science
118:30 - and AI professionals visit Lun Tech
118:33 - Academy section to explore our courses
118:35 - and boot camps and just in general our
118:38 - programs businesses in Need for employee
118:41 - training upscaling or data science and
118:44 - AI Solutions should head to the
118:46 - technology section on the learner. page
118:50 - Enterprises looking for corporate
118:51 - training in curriculum modernization and
118:54 - customize AI tools to enhance education
118:57 - please visit the lunch Enterprises
119:00 - section at lunch. for a free
119:03 - consultation and customize estimate join
119:06 - lunch and start building your future one
119:09 - data point at a
119:12 - time AB testing is an important topic
119:15 - for data scientists to know because it's
119:17 - a powerful method for evaluating changes
119:20 - or improvements to the products or
119:22 - Services it allows us to make datadriven
119:25 - decisions by comparing the performance
119:26 - of two different versions of a product
119:29 - or a service usually referred as
119:31 - treatment or control for example a
119:34 - testing allows data scientists to
119:36 - measure the effectiveness of changes to
119:38 - a product or a service which is
119:40 - important as it enables data scientists
119:42 - to make data driven decisions rather
119:44 - they're relying on Intuition or
119:46 - assumptions secondly AB testing helps
119:49 - data Sciences to identify the most
119:51 - effective effective changes to a product
119:53 - or a service which is really important
119:56 - because it allows us to optimize the
119:58 - performance of a product or a service
120:00 - which can then lead to increased
120:02 - customer satisfaction and
120:05 - sales AB testing helps us also to
120:08 - validate certain hypothesis about what
120:10 - changes will improve a product or
120:12 - service this is important because it
120:14 - helps us to build a deeper understanding
120:17 - of the customers and the factors that
120:19 - influence customers Behavior finally AB
120:22 - testing is a common practice in many
120:24 - Industries such as e-commerce digital
120:26 - marketing website optimization and many
120:29 - others so data scientists who have
120:31 - knowledge and experience in a testing
120:34 - will be more valuable to this
120:37 - companies no matter in which industry
120:39 - you want to enter as a data scientist
120:41 - and what kind of job you will be
120:42 - interviewed for and even if you believe
120:45 - more technical data scien is your cup of
120:46 - tea be prepared to know at least higher
120:49 - level understanding and the details
120:50 - behind this method definitely help you
120:52 - to know about this topic when you are
120:54 - speaking with product owners
120:56 - stakeholders product scientists and
120:58 - other people involved in the
121:01 - business let's briefly discuss a perfect
121:03 - audience for the section of the course
121:05 - and prerequisites there are no
121:07 - prerequisites of this section in terms
121:09 - of AB testing Concepts that you should
121:11 - know already but knowing the basics and
121:13 - statistics which you can find in the
121:15 - fundamentals to statistics section is
121:18 - highly recommended this section will be
121:20 - great if you have no prior AB testing
121:22 - knowledge and you want to identify and
121:24 - learn the essential AB testing Concepts
121:26 - from scratch so this will help you to
121:29 - prepare for your job interviews it will
121:31 - also be a good refresher for anyone who
121:34 - does have AB testing knowledge but who
121:36 - wants to refresh their memory or want to
121:38 - fill in the gaps in their knowledge in
121:40 - this lecture we will start off the topic
121:42 - about AB testing where we will formally
121:45 - Define what AB testing is and we will
121:48 - look at the high level overview of AB
121:50 - testing process step by step
121:54 - by definition AB testing or split
121:57 - testing is originated from the cial
122:00 - randomized control trials and is one of
122:02 - the most popular ways for businesses to
122:04 - test new ux features new versions of a
122:07 - product or an algorithm to decide
122:09 - whether your business should launch that
122:10 - new ux feature or should productionize
122:13 - that new recommender system create that
122:15 - new product that new button or that new
122:19 - algorithm the idea behind a testing is
122:22 - that you should show the variated or the
122:24 - new version of the product to sample of
122:26 - customers often referred as experimental
122:28 - group and the existing version of the
122:30 - product to not sample of customers
122:32 - referred as control group then the
122:34 - difference in the product performance in
122:36 - experimental versus control group is
122:38 - tracked to identify the effect of these
122:41 - new versions of the product on the
122:42 - performance of the product so the goal
122:45 - is then to track the metric during the
122:47 - test period and find out whe there is a
122:50 - difference in the performance of the
122:51 - product and what type of difference is
122:53 - it the motivation behind this test is to
122:56 - test new product variants that will
122:58 - improve the performance of the existing
123:00 - product and will make this product more
123:02 - successful and optimal showing a
123:04 - positive treatment effect what makes
123:07 - this testing grade is that businesses
123:09 - are getting direct feedback from their
123:10 - actual users by presenting them the
123:13 - existing versus the variated product
123:15 - version and in this way they can quickly
123:17 - Test new ideas in case of ab Test shows
123:21 - that the variated version is not
123:22 - effective at least businesses can learn
123:24 - from this and can decide whether they
123:26 - need to improve it or need to look for
123:28 - other ideas let us go through the steps
123:31 - included in the AB testing process which
123:33 - will give you a higher level overview
123:35 - into the
123:37 - process the first step in conducting AB
123:39 - testing is stating the hypothesis of the
123:41 - ab test this is a process that includes
123:44 - coming up with business and statistical
123:46 - hypothesis that you would like to test
123:48 - with this test including how you
123:50 - measured the success which R called
123:52 - primary
123:54 - metric next step in AB testing is to
123:57 - perform what we call power analysis and
123:59 - design the entire test which includes
124:02 - making assumptions about the most
124:03 - important parameters of the test and
124:05 - calculate the minimum sample size
124:07 - required to claim statistical
124:10 - significance the third step in AB
124:12 - testing is to run the actual AB test
124:14 - which in practical sense for the data
124:16 - scientist means making sure that the
124:18 - test runs smoothly and correctly
124:20 - collaborate with engineers and product
124:22 - managers to ensure that all the
124:24 - requirements are satisfied this also
124:26 - includes collecting the data of control
124:28 - and experimental groups which will be
124:30 - used in The Next
124:32 - Step next step in AB testing is choosing
124:35 - the right statistical test whether it is
124:38 - z test T Test Ki Square test Etc to test
124:41 - the hypothesis from the step one by
124:44 - using the data collected from the
124:46 - previous step and to determine whether
124:48 - there is a statistically significant
124:50 - difference between the control versus
124:52 - experimental
124:54 - group The Fifth and the final step in AB
124:57 - testing is continuing to analyze the
124:59 - results and find out whether besides
125:01 - statistical significance there is also
125:03 - practical significance in this step we
125:05 - use the second step's power analysis so
125:08 - the assumptions that we made about model
125:10 - parameters and the suiz and the four
125:13 - steps results to determine whether there
125:15 - is a practical significance beside of
125:17 - the statistical significance this
125:19 - summarizes the AB testing process at a
125:21 - higher level in next couple of lectures
125:24 - we'll go through the steps one at a time
125:27 - so buckle up and let's learn about AB
125:30 - testing in this lecture lecture number
125:32 - two we will discuss the first step in a
125:35 - testing process so let's bring our
125:38 - diagram back as you can recall from the
125:41 - previous lecture when we were discussing
125:43 - the entire process of AB testing at a
125:45 - high level we saw that in the first step
125:47 - in conducting AB testing is stating the
125:49 - hypothesis of ab test this this process
125:52 - includes coming up with a business and
125:53 - statistical hypothesis that you would
125:55 - like to test with this test including
125:57 - how you measured the success which we
125:59 - call a primary metric so what is the
126:02 - metric that we can use to say that that
126:05 - the product that we are testing performs
126:08 - well first we need to State the business
126:10 - hypothesis for our AB test from a
126:12 - business perspective so formally
126:15 - business hypothesis describes what the
126:17 - two products are that being compared and
126:19 - what is the desired impact or the
126:20 - difference for the businesses so how to
126:23 - fix a potential issue in the product
126:25 - where a solution of these two problems
126:27 - will influence what we call a key
126:29 - performance indicator or the kpi of the
126:32 - interest business hypothesis is usually
126:35 - set as a result of brainstorming and
126:37 - collaboration of relevant people on the
126:39 - product team and data science team the
126:41 - idea behind this hypothesis is to decide
126:44 - how to fix a potential issue in the
126:46 - product where a solution of these
126:48 - problems will improve the target kpi one
126:51 - example business hypothesis is that
126:53 - changing the color of learn more button
126:55 - for instance to Green will increase the
126:58 - engagement of the web
127:03 - page next we need to select what we call
127:06 - primary metric for our av testing there
127:09 - should be only one primary metric in
127:11 - your AV test choosing this metric is one
127:13 - of the most important parts of baby test
127:16 - since this metric will be used to
127:18 - measure the performance of the product
127:20 - or feature for the experimental and
127:22 - control groups and then will be used to
127:24 - identify whether there is a difference
127:26 - or what we call statistically
127:28 - significant difference between these two
127:29 - groups by definition primary metric is a
127:32 - way to measure the performance of the
127:34 - product being tested in the ab test for
127:36 - the experimental and control groups it
127:39 - will be used to identify whether there
127:40 - is a statistically significant
127:42 - difference between these two groups the
127:44 - choice of the success metric depends on
127:46 - the underlying hypothesis that is being
127:48 - tested with this AB test this is if not
127:50 - the most one of the most important parts
127:52 - of the ab test because it determines how
127:55 - the test will be designed and also how
127:57 - well the proposed ideas perform choosing
128:00 - poor metrics might disqualify a large
128:02 - amount of work or might result in wrong
128:04 - conclusions for instance the revenue is
128:07 - not always the end goal therefore in a
128:09 - testing we need to tie up the primary
128:12 - metric to the direct and the higher
128:14 - level goals of the
128:18 - product the expectation is that if the
128:20 - product makes more money then this
128:22 - suggests the content is great but in
128:24 - achieving that goal instead of improving
128:26 - the overall content of the material and
128:28 - writing one can just optimize the
128:31 - conversion funless one way to test the
128:33 - accuracy of the metric you have chosen
128:35 - as your primary metric for your ab test
128:37 - could be to go back to the exact problem
128:39 - you want to solve you can ask yourself
128:41 - the following question what I tend to
128:43 - call the metric validity
128:46 - question so if this chosen metric were
128:48 - to increase significantly while
128:51 - everything else stays constant would we
128:53 - achieve our goal and would we address
128:54 - our business problem is it higher
128:57 - revenue is it higher customer engagement
128:59 - or is it high views that we are chasing
129:01 - in the business so the choice of the
129:04 - metric will then answer this question
129:07 - though you need to have a single primary
129:08 - metric for your ab test you still need
129:10 - to keep an eye on the remaining metrics
129:13 - to make sure that all the metrics are
129:14 - showing a change and not only the target
129:17 - one having multiple metrics in your ab
129:19 - test will lead to false positives since
129:22 - you will identify many significant
129:24 - differences well there is no effect
129:26 - which is something you want to avoid so
129:28 - it's always a good idea to pick just a
129:30 - single primary metric but to keep an eye
129:32 - and monitor all the remaining
129:38 - metrics so if the answer to the metric
129:40 - validity question is higher Revenue
129:43 - which means that you are saying that the
129:45 - higher revenue is what you are chasing
129:47 - and better performance means higher
129:49 - revenue for your product then you can
129:51 - can use as your primary metric what we
129:53 - call a conversion rate conversion rate
129:55 - is a metric that is used to measure the
129:57 - effectiveness of a website a product or
129:59 - a marketing campaign it is typically
130:01 - used to determine the percentage of
130:03 - visitors or customers who take a desired
130:06 - action such as making a purchase filling
130:08 - out a form or signing up for a service
130:11 - the formula for conversion rate is
130:13 - conversion rate is equal to number of
130:16 - conversions divided to number of total
130:17 - visitors multiplied by 100% for example
130:21 - example if a website has thousand
130:23 - visitors and 50 of them make a purchase
130:26 - the conversion rate would be equal to 50
130:28 - divide 2,000 multiply by 100% which
130:32 - gives us 5% this means that our
130:34 - conversion rate in this case is equal to
130:37 - 5% conversion rate is an important
130:39 - metric because it allows us and
130:41 - businesses to measure the effectiveness
130:43 - of their website a product or a
130:46 - marketing campaign it can help
130:48 - businesses to identify areas for
130:49 - improvement such as increasing the
130:51 - number of conversions or improving the
130:53 - user experience conversion rate can be
130:55 - used for different purposes for example
130:58 - if a company wants to measure the
130:59 - effectiveness of an online store the
131:02 - conversion rate would be the percentage
131:03 - of visitors who make a purchase and on
131:06 - the other hand if a company wants to
131:08 - measure the effectiveness of landing
131:09 - page the conversion rate would be the
131:11 - percentage of visitors who fill out a
131:13 - form or sign up for a service so if the
131:16 - answer to the metric validity question
131:18 - is higher engagement then you can use
131:20 - the click rate or CTR as your primary
131:23 - metric this is by the way a common
131:26 - metric used in a testing whenever we are
131:28 - dealing with eCommerce product search
131:30 - engine recommender system click through
131:33 - rate or the CTR is a metric that
131:35 - measures the effectiveness of a digital
131:37 - marketing campaign or the user
131:39 - engagement or some feature on your web
131:41 - page or your website and it's typically
131:43 - used to determine the percentage of
131:45 - users who click on a specific link or
131:48 - button or call to action CTA out of of
131:51 - the total number of users who view it
131:53 - the formula for the click through rate
131:55 - can be represented as follows so the CTR
131:58 - is equal to number of clicks divided to
132:00 - number of Impressions multiply by 100%
132:03 - not to be confused with click through
132:05 - probability because there is a
132:06 - difference between the click through
132:07 - rate and click through probability for
132:10 - example if an online advertisement
132:12 - receives thousand of Impressions which
132:14 - means that we are showing it to the
132:15 - customers for a thousand times and there
132:17 - were 25 clicks which means 25 out of all
132:20 - the Impressions resulted in clicks this
132:23 - means that the clickr rate for this
132:24 - specific example would be equal to 25
132:27 - divide 2,000 multiply by 100% which
132:30 - gives us 2.5% this means that for this
132:33 - particular example our clickr rate is
132:35 - equal to
132:36 - 2.5% cure rate is an important metric
132:40 - because it allows businesses to measure
132:41 - the effectiveness of their digital
132:43 - marketing campaigns and the user
132:45 - engagement with their website or web
132:47 - pages High click through rate indicates
132:49 - that a campaign or the web page or
132:51 - feature is relevant and appealing to the
132:54 - target audience because they are
132:55 - clicking on it while low clickr rate
132:58 - indicates that a campaign or the web
132:59 - page needs an improvement click show
133:02 - rate can be used to measure the
133:03 - performance of different digital
133:05 - marketing channels such as paid search
133:08 - display advertising email marketing and
133:10 - social media it can also be used to
133:12 - measure the performance of different ad
133:14 - formats such as text advertisements
133:17 - Banner advertisement video
133:19 - advertisements Etc
133:22 - next and the final task in this first
133:24 - step in the process of AP testing is to
133:26 - State the statistical hypothesis based
133:28 - on the business hypothesis and the
133:30 - chosen primary
133:32 - metric next and in the final task in
133:34 - this first step of the AB testing
133:36 - process we need to State the statistical
133:38 - hypothesis based on the business
133:40 - hypothesis we stated and the chosen
133:42 - primary metric in the section of
133:44 - fundamentals to statistics of this
133:46 - course in lecture number seven we went
133:49 - into details about statistical
133:50 - hypothesis testing included what n
133:52 - hypothesis is and what alternative
133:54 - hypothesis is so do have a look to get
133:57 - all the insight about this topic AB
134:00 - testing should always be based on a
134:02 - hypothesis that needs to be tested this
134:05 - hypothesis is usually set as a result of
134:07 - brainstorming and collaboration of
134:09 - relevant people on the product team and
134:12 - data science team the idea behind this
134:14 - hypothesis is to decide how to fix a
134:16 - potential issue in a product where a
134:19 - solution of these problems will
134:20 - influence in the key performance
134:22 - indicators or the kpi of interest it's
134:25 - also highly important to make
134:27 - prioritization out of range of product
134:29 - problems and ideas to test while you
134:32 - want to P that fixing this problem would
134:34 - result in the biggest impact for the
134:37 - product we can put the hypothesis that
134:40 - is subject to rejection so that we want
134:42 - to reject in the ideal World under the
134:45 - null hypothesis what we Define by AG
134:47 - zero well we can put the hypothesis
134:49 - subject to acceptance so the desired
134:51 - hypothesis that we would like to have as
134:54 - a result of AB testing under the
134:56 - alternative hypothesis defined by
134:58 - H1 for example if the kpi of the product
135:01 - is to increase the customer engagement
135:03 - by changing the color of the read more
135:05 - button from blue to green then under the
135:08 - N hypothesis we can state that click
135:10 - through rate of learn more button with
135:12 - blue color is equal to the clickr rate
135:14 - of green button under the alternative we
135:17 - can then state that the click through
135:18 - rate of the learn more button with green
135:20 - color is larger than the click through
135:22 - of the blue
135:26 - button so ideally want to reject this n
135:29 - hypothesis and we want to accept the
135:31 - alternative hypothesis which will mean
135:33 - that we can improve the clickr rate so
135:35 - the engagement of our product by simply
135:38 - changing the color of the button from
135:40 - blue to green once we have set up the
135:43 - business hypothesis selected the primary
135:45 - metrix and stated the statistical
135:47 - hypothesis we are ready to proceed to
135:49 - the next stage in the AB testing
135:53 - process in this lecture we will discuss
135:56 - the next Second Step In AB testing
135:58 - process which is designing the ab test
136:00 - including the power analysis and
136:02 - calculating the minimum sample sizes for
136:05 - the control and experimental groups stay
136:07 - tuned as this is a very important part
136:09 - of AB testing process commonly appearing
136:12 - during the data science
136:14 - interviews some argue that AB testing is
136:16 - an art and others say that it's a
136:18 - business adjusted common statistical
136:20 - test test but the borderline is that to
136:23 - properly Design This experiment you need
136:25 - to be disciplined and intentional while
136:27 - keeping in mind that it's not really
136:29 - about testing but it's about learning
136:31 - following AR steps you need to take to
136:33 - have a solid design for your ab test so
136:37 - let's bring the diagram back so in this
136:39 - step we need to perform the power
136:41 - analysis for our AB test and calculate
136:43 - the minimum sample size in order to
136:45 - design our AB
136:47 - test AB test design includes three steps
136:50 - the first step is power analysis which
136:52 - includes making assumptions about model
136:54 - parameters including the power of the
136:56 - test the significance level Etc the
137:00 - second step is to use these parameters
137:02 - from Power analysis to calculate the
137:04 - minimum sample size for the control and
137:06 - experimental groups and then the final
137:09 - third step is to decide on the test
137:10 - duration depending on several factors so
137:13 - let's discuss each of these topics one
137:15 - by
137:17 - one power analysis for AB testing
137:20 - includes this three specific steps the
137:22 - first one is determining the power of
137:24 - the test this is our first parameter the
137:27 - power of the statistical test is the
137:29 - probability of correctly rejecting the N
137:31 - hypothesis power is the probability of
137:33 - making a correct decision so to reject
137:35 - the N hypothesis when the N hypothesis
137:38 - is false if you're wondering what is the
137:41 - power of the test what is this different
137:43 - concepts that we just talk about what is
137:45 - this null hypothesis and what does it
137:46 - mean to reject the null hypothesis then
137:48 - head towards the fundamental St IC
137:51 - section of this course as we discuss
137:53 - this topic in detail as part of that
137:57 - section the power is often defined by 1
138:00 - minus beta which is equal to the
138:02 - probability of not making a type two
138:05 - error where type two error is the
138:07 - probability of not rejecting the null
138:09 - hypothesis while the null is actually
138:11 - false it's common practice to pick 80%
138:14 - as the power of the ab test which means
138:16 - that we allow 20% of type to error and
138:19 - this means that we are fine with not
138:21 - detecting so failing to reject no
138:23 - hypothesis 20% of the time which means
138:26 - that we are fine with not detecting a
138:28 - true treatment effect while there is an
138:30 - effect which means that we are failing
138:32 - to reject the N however the choice over
138:35 - value of this parameter depends on the
138:37 - nature of the test and the business
138:40 - constraints secondly we need to
138:42 - determine a significance level for our
138:44 - AB test the significance level which is
138:46 - also the probability of type one error
138:49 - is the likelihood of rejecting the no
138:51 - hence detecting a treatment effect while
138:53 - the no is actually true and there is no
138:55 - statistically significant impact this
138:57 - value often defined by a Greek letter
138:59 - Alpha is a probability of making a false
139:02 - Discovery often referred to as a false
139:03 - positive rate generally we use the
139:06 - significance level of 5% which indicates
139:08 - that we have 5% risk of concluding that
139:10 - there exists a statistically significant
139:12 - difference between the experimental and
139:14 - control variant performances when there
139:17 - is no actual difference so we are fine
139:19 - by having five out of 100 cases
139:21 - detecting a treatment effect while there
139:23 - is no effect it also means that you have
139:25 - a significant result difference between
139:27 - the control and the experimental groups
139:29 - within 95% confidence like in the case
139:33 - of the power of the test the choice of
139:34 - the alpha is dependent on the nature of
139:36 - the test and the business constraints
139:38 - that you have for instance if running
139:40 - this AB test is related to high
139:42 - engineering course then the business
139:44 - might decide to pick a high offer such
139:47 - that it would be easier to detect a
139:48 - treatment effect on the other hand the
139:51 - implementation costs of the proposed
139:52 - version in production are high you can
139:55 - then pick a lower significance level
139:58 - since this proposed feature should
139:59 - really have a big impact to justify the
140:01 - high implementation cost so it should be
140:04 - harder to reject no
140:07 - hypothesis finally as the last typ of
140:09 - power analysis we need to determine a
140:11 - minimum detectable effect for the
140:14 - test last parameter as part of the power
140:16 - analysis we need to make assumptions
140:18 - about is what we call minimum detect
140:21 - effect or Delta from the business point
140:23 - of view so what is the substantive to
140:25 - the statistical significance that the
140:27 - business wants to see as a minimum
140:29 - impact of the new version to find this
140:31 - variant investment
140:33 - worthy the answer to this question is
140:36 - what is the amount of change we aim to
140:38 - observe in a new versions metric
140:40 - compared to the existing one to make
140:42 - recommendations to the business that
140:44 - this feature should be launched in the
140:45 - production that it's investment worthy
140:48 - an estimate of this parameter is what is
140:50 - is known as a minimum detectable effect
140:52 - often defined by a Greek letter Delta
140:55 - which is also related to the Practical
140:56 - significance of the test so this MD or
140:59 - the minimum detectable effect is a proxy
141:01 - that relates to the smallest effect that
141:03 - would matter in practice for the
141:05 - business and it's usually set by
141:07 - stakeholders as this parameter is highly
141:09 - dependent on the business there is no
141:11 - common level of it instead so this
141:14 - minimum detectable effect is basically
141:16 - the translation from statistical
141:18 - significance to the Practical
141:19 - significance and here we want to see and
141:21 - we want to answer the question what is
141:23 - this percentage increase in the
141:25 - performance of the product that we want
141:26 - to experiment with that will tell to the
141:29 - business that this is good enough to
141:31 - invest in this new feature or in this
141:33 - new product and this can be for instance
141:35 - 1% for one product it can be 5% for
141:38 - another one and it really depends on the
141:40 - business and what is the underlying
141:44 - kpi a popular reference to the
141:47 - parameters involved in the power
141:48 - analysis for AB testing is like this so
141:51 - 1 minus beta for the power of the test
141:54 - Alpha for the significance level Delta
141:56 - for the minimum detectable effect to
141:59 - make sure that our results are
142:00 - repeatable robust and can be generalized
142:02 - to the entire population we need to
142:04 - avoid P hacking to ensure real
142:07 - statistical significance and to avoid
142:09 - biased results so we want to make sure
142:11 - that we collect enough amount of
142:13 - observations and we run the test for a
142:15 - minimum predetermined amount of time
142:17 - therefore before running the test we
142:19 - need to determine the sample size of the
142:21 - control in experimental groups as well
142:23 - as later on in this lecture we will see
142:25 - also how long we need to run the test so
142:28 - this is another important part of AB
142:30 - testing which needs to be done using the
142:32 - defined power of the test which was the
142:35 - one minus beta the significance level
142:37 - and a minimum detectable effect so all
142:39 - the parameters that we decided upon when
142:42 - conducting the power
142:44 - analysis calculation of the sample size
142:46 - depends on the underlying primary metric
142:48 - as well that you have chosen for
142:50 - tracking the progress of the control and
142:52 - experimental versions of the product so
142:55 - we need to distinguish here two
142:57 - cases so when discussing the primary
142:59 - metric we saw that there are different
143:01 - ways that we can measure the performance
143:03 - of different type of products if we are
143:05 - interested in engagement then we are
143:07 - looking at a metric such as click
143:09 - through rate which is in the form of
143:11 - averages so the case one will be where
143:13 - the primary metric of AB testing is in
143:15 - the form of a binary variable it can be
143:18 - for instance conversion or no conversion
143:21 - click or no click and in case two where
143:24 - the primary metric of the test is in the
143:25 - form of proportions or averages which
143:28 - means mean order amount or mean click
143:31 - through
143:32 - rate for today we will be covering only
143:35 - one of these cases but you can find more
143:37 - details on the second case in my blog
143:39 - which I have posted also as part of the
143:41 - resources section this blog post
143:44 - contains all the details that you need
143:45 - to know about AB testing including the
143:47 - statistical test and their corresponding
143:49 - hypothesis
143:51 - the descriptions of different primary
143:52 - metrics that go beyond what we have
143:54 - covered as part of this section as well
143:56 - as many more details that you need to
143:58 - know about a
144:00 - testing so let's look at a case two
144:03 - where the primary metric of the test is
144:04 - in the form of proportions or averages
144:07 - so let's say we want to test whether the
144:08 - average click to rate of control is
144:10 - equal to the average click to rate of
144:12 - experimental group and under H we have
144:15 - that the MU control is equal to M
144:17 - experimental and under H1 we have that
144:19 - the m control control is not to Mu
144:21 - experimental so here the MU control and
144:24 - mu experimental are simply the average
144:26 - of the primary metric for control group
144:28 - and for the experimental group
144:30 - respectively so this the formal
144:32 - hypothesis we want to test with our AB
144:34 - test and we can assume that this new
144:36 - control is for instance the clickr rate
144:38 - of the control group and the MU
144:40 - experimental is the clickr rate of the
144:42 - experimental
144:44 - group so this is the formal statistical
144:47 - hypothesis we want to test with our AB
144:49 - test if you have having done so I would
144:51 - highly suggest you to head towards the
144:53 - fundamental statistic section of this
144:55 - course where in lecture number seven and
144:57 - eight of the statistical part of this
144:59 - course I go in detail about statistical
145:01 - hypothesis testing the means averages
145:04 - significance level Etc this also holds
145:07 - for the theorem that the some precise
145:09 - calculation is based upon called Central
145:11 - limit theorem so check out the last
145:13 - lecture about infuential statistics
145:16 - where I covered the central limit
145:18 - theorem which we will also use in this
145:20 - section
145:21 - and finally also check the lecture
145:23 - number five in that section where we
145:25 - cover the normal distribution another
145:27 - thing that we will use as part of this
145:30 - section so the central limit theorem
145:32 - states that given a sufficiently large
145:34 - sample size from an arbitrary
145:35 - distribution the sample mean will be
145:37 - approximately normally distributed
145:39 - regardless of the shape of the original
145:41 - population distribution this means that
145:43 - the distribution of the sample means
145:45 - will be approximately normal if we take
145:47 - a large enough sample even if the
145:49 - distribution of the original sample is
145:51 - not normal so when we are dealing with a
145:54 - primary performance tracking metric that
145:56 - is in the form of average such as this
145:57 - one that we are covering today which is
145:59 - a clickr rate we intend to compare the
146:01 - means of the control and experimental
146:03 - groups then we can use the central limit
146:05 - theorem as state that the mean sampling
146:07 - distribution of both controlling
146:09 - experimental groups follow normal
146:11 - distribution consequently the sampling
146:14 - distribution of the difference of the
146:15 - means of these two groups also will be
146:17 - normally distributed
146:22 - so this can be expressed like this where
146:23 - we see that the mean of the control
146:25 - group and mean of the experimental group
146:27 - follows normal distribution with mean mu
146:29 - control and mu experimental respectively
146:32 - and then with the variance of Sigma
146:33 - control squared and sigma experimental
146:35 - squared respectively though derivation
146:38 - of this Pro is out of the scope of this
146:40 - course we can state that the difference
146:42 - between the means of the true group so
146:44 - xar control minus xar experimental also
146:47 - follows normal distribution with a mean
146:49 - new control minus mu experimental and
146:51 - with a variance of Sigma control squ /
146:54 - to n Control Plus Sigma experimental squ
146:56 - / to n experimental so the sample size
146:59 - of the experimental group and the sample
147:01 - size of the control group hence the
147:03 - sample size needed to compare the means
147:05 - of the two normally distributed samples
147:07 - using a two-sided test which pre-specify
147:10 - significance of alpha power level and
147:13 - minimum detectable F can be calculated
147:15 - as
147:17 - follows so here you can see the
147:19 - mathematical represent ation of the
147:21 - minimum sample size so the N which
147:23 - stands for the minimum sample size is
147:25 - equal to and in denominator we have
147:27 - Sigma Control squ Plus Sigma
147:28 - experimental squ multip by Z 1 - Alpha /
147:32 - to 2 + z 1 - beta s / to the Delta squ
147:36 - and here the Alpha and the beta and the
147:38 - Delta we have made assumptions about as
147:40 - part of the power analysis and the sigma
147:43 - control squar and a sigma experimental
147:45 - squared are the uh estimates of the
147:47 - variance that we can come up with using
147:49 - the So-Cal AA testing I would say you do
147:53 - not necessarily need to know this
147:54 - derivation as there are many online
147:56 - calculators that will ask you for the
147:57 - alpha the beta and the Delta values as
148:00 - well as the sample estimates for the
148:01 - sigma squ control and experimental and
148:04 - then these calculators will
148:05 - automatically calculate the minimum S
148:07 - size for you if you're wondering what
148:09 - this AA testing is and how we can come
148:11 - up with the sigma control squared and
148:13 - sigma experimental squared as well as
148:15 - all the other values then make sure to
148:17 - to check out the blog that I posted
148:19 - before and that I mentioned before as I
148:21 - explain in detail all these values as
148:23 - well as check out the resource section
148:25 - where I've included many resources
148:26 - regarding this but for now just keep in
148:29 - mind that the Z1 minus Alpha / to 2 and
148:32 - Z1 minus beta are just two constants and
148:35 - come from the normal distributed and
148:36 - standard normal distributed tables I
148:38 - would say you do not necessarily need to
148:40 - know this derivation as there are many
148:42 - online calculators that will ask you for
148:44 - this Alpha Beta And Delta values as well
148:47 - as the SLE estimates for the sigma squar
148:49 - controlling Sigma experimental control
148:51 - and then we'll calculate automatically
148:53 - the sample size for you for the control
148:55 - and experimental group
148:59 - effectively one example of such
149:01 - calculator is this AB test online
149:03 - calculator but if you Google it you will
149:05 - find many others that will ask you for
149:07 - the minimum detectable effect for the
149:09 - statistical significance or the
149:10 - statistical power and then it will
149:12 - automatically calculate for you the
149:14 - minimum sample size that you should have
149:16 - in order to have a statistical
149:18 - significance and in order to have a
149:19 - valid AB test one thing to keep in mind
149:22 - is that you will notice that the
149:23 - statistical significance level is set to
149:25 - 95% in here which is not what we have
149:28 - seen when we were discussing the alpha
149:30 - significance level so sometimes these
149:32 - online calculators will confuse or will
149:35 - interchangeably use the significance
149:37 - level versus the confidence level which
149:39 - are the opposite so the significance
149:41 - level is usually at the level of 5% or
149:43 - 1% confidence level is around 95% so
149:46 - which is basically 100% minus the alpha
149:49 - there for whenever you see this 95% know
149:52 - that this means that your Alpha should
149:53 - be 5% so it's really important to
149:56 - understand how to use this calculator
149:58 - not to end up with the wrong minimum
149:59 - sample size conduct an entire AB test
150:02 - and then at the end realize that you
150:03 - have used the wrong uh significance
150:08 - level the final step is to calculate the
150:11 - test duration this question needs to be
150:13 - answered before you run your experiment
150:15 - and not during the experiment sometimes
150:18 - people stop the test when they detect C
150:20 - significance which is what we call P
150:22 - hacking and that's absolutely not what
150:24 - you want to do to to determine the
150:26 - Baseline of a duration time a common
150:28 - approach is to use this formula as you
150:30 - can see duration is equal to n ided to
150:33 - the number of visitors per day where n
150:35 - is your minimum sample size that we just
150:37 - calculated in the previous step and the
150:39 - number of visitors per day is the
150:41 - average number of visitors that you
150:43 - expect to see as part of your
150:46 - experiment for instance if this formula
150:48 - results in 14 days or 14 this suggest
150:51 - that running the test for two weeks is a
150:53 - good idea however it's highly important
150:56 - to take many business specific aspect
150:58 - into account when choosing the time to
151:00 - run the test and for how long you need
151:02 - to run it and simply using this formula
151:04 - is not enough for example if you want to
151:08 - run an experiment at the end of the
151:09 - month December with Christmas breaks
151:11 - when higher than expected or lower than
151:13 - expected number of people are usually
151:15 - checking your web page then this
151:17 - external and uncertain event had an
151:19 - impact on on the page to search for some
151:21 - businesses this means
151:25 - a for example if you want to run an
151:27 - experiment at the end of the month of
151:29 - December with Christmas breaks when
151:31 - higher than expected or in some cases
151:33 - lower than expected number of people are
151:36 - usually checking the web page so
151:37 - depending on the nature of your business
151:39 - or the product then this external and
151:41 - uncertain event can have an impact on
151:44 - the page usage for some businesses which
151:47 - means that for some businesses a high
151:49 - increasing the P usage can be the result
151:51 - and for some a huge decrease in
151:53 - usability in this case running AB test
151:56 - without taking into account this
151:57 - external Factor would result in
151:59 - inaccurate results since the activity
152:02 - period would not be true representation
152:04 - of a common page usage and we no longer
152:06 - have this Randomness which is a crucial
152:08 - part of AB
152:10 - testing beside this When selecting a
152:13 - specific test duration there are few
152:14 - other things to be aware of firstly two
152:17 - small test duration might result in what
152:19 - we call novelty effects users tend to
152:22 - react quickly and positively to all
152:24 - types of changes independent of their
152:26 - nature so it's referred as a novelty
152:28 - effect and it vares of in time and it is
152:31 - considered illusionary so it would be
152:33 - wrong to describe this effect to the
152:35 - experimental version itself and to
152:37 - expect that it will continue to persist
152:39 - after the noble T effect wears off hence
152:42 - when picking a test duration we need to
152:44 - make sure that we do not run the test
152:46 - for too short amount of time period
152:48 - otherwise we can have a noble TF effect
152:50 - novelty effect can be a major threat to
152:52 - the external validity of an AV test so
152:54 - it's important to avoid it as much as
152:58 - possible secondly if the test duration
153:01 - is too large then we can have what we
153:02 - call maturation effects when planning an
153:05 - AB test it's usually useful to consider
153:07 - a longer test duration for allowing
153:09 - users to get used to a new feature or
153:12 - product in this way one will be able to
153:14 - observe the real treatment effect by
153:16 - giving more time to returning users to
153:19 - cool down from an initial positive
153:21 - reaction or a spike of Interest due to a
153:23 - change that was introduced as part of a
153:25 - treatment this should help to avoid
153:28 - novelty effect and is better predictive
153:30 - value for the test outcome however the
153:33 - longer the test period the larger is a
153:35 - likelihood of external effect impacting
153:38 - the reaction of the users and possibly
153:40 - contaminating the test results if you
153:43 - like this content make sure to check all
153:45 - the other videos available on this
153:47 - channel and don't forget to subscribe
153:49 - like and comment to help the algorithm
153:51 - to make this content more accessible to
153:54 - everyone across the world and if you
153:56 - want to get free resources make sure to
153:59 - check the free resources section at
154:02 - lunch. and if you want to become a job
154:05 - ready data scientist and you are looking
154:07 - for this accessible boot camp that will
154:10 - help you to make your job ready data
154:12 - scientist consider enrolling to the data
154:15 - science boot camp the ultimate data
154:17 - science boot camp at lunch.
154:20 - you will learn all the theory the
154:22 - fundamentals to become a jbre data
154:24 - scientist you will also implement the
154:27 - learn theory into a real world multiple
154:31 - data science projects beside this after
154:33 - learning the theory and practicing it
154:35 - with the real world case studies you
154:38 - will also prepare for your data science
154:40 - interviews and if you want to stay up to
154:42 - date with the recent developments in
154:44 - Tech what are the headlines that you
154:46 - have missed in the last week what are
154:48 - the open positions currently in the
154:51 - market across the globe and what are the
154:53 - tech startups that are making waves in
154:55 - the tech and sure to subscribe to the
154:58 - data science na newsletter from
155:05 - [Music]
155:08 - lunarch this is what we call maturation
155:10 - effect and therefore running the AP test
155:13 - for too short amount of time or too long
155:15 - amount of time is not recommended as
155:18 - it's a very topic we can talk for hours
155:21 - about this part of the ab test and also
155:24 - a topic that is asked a lot during the
155:26 - data science and product scientist
155:28 - interviews therefore I highly suggest
155:30 - you to check out this book about AB
155:32 - testing which is a Hands-On tutorial
155:34 - about everything you need to know about
155:36 - AB testing as well check out the
155:38 - interview preparation guide in this
155:39 - section that contains 30 most popular AB
155:42 - testing related questions you can expect
155:44 - during your data science interviews
156:00 - looking to elevate your data science or
156:03 - data analytics portfolio then you are in
156:05 - the right place with this AB testing and
156:08 - Trend case study you can showcase your
156:10 - AB testing and coding skills in one
156:13 - place I'm tasan data scientist and AI
156:16 - professional and I'm the co-founder of
156:18 - lunar Tech where we are making data
156:20 - science and AI accessible to everyone
156:24 - individuals businesses and
156:27 - institutions in this case study we are
156:29 - going to complete an endtoend case study
156:32 - with AB testing where we are going to
156:35 - test in a data driven way whether it's
156:37 - worth to change one of our features in
156:39 - our ux design in the lunar text landing
156:42 - page this a real life data science case
156:45 - study that you can conduct and you can
156:48 - put it on your resume in order to
156:50 - Showcase your experience in datadriven
156:53 - decision making where you will showcase
156:55 - your statistical skills experimentation
156:58 - skills with AB testing and your coding
157:00 - skills in Python using Library such as T
157:03 - models but also the pendas npy also met
157:08 - plot lip and Seaburn we are going to
157:10 - start with the business objective of
157:13 - this case study then we are going to
157:15 - translate the business objective into a
157:18 - data science problem
157:20 - then we are going to start with the
157:22 - actual coding we are going to load
157:23 - libraries we are going to look into Data
157:26 - visualize the data The Click data we are
157:28 - going to look into the motivation behind
157:30 - choosing that specific primary metric
157:32 - which is a click through rate then we
157:34 - are going to talk about the statistical
157:37 - hypothesis for our AB testing I will
157:40 - also teach you step by step all the
157:42 - calculations starting from the
157:44 - calculation of the pulled estimate from
157:46 - the clickr rate and then a computation
157:49 - of the the uh ped variance the standard
157:51 - error but also the motivation behind
157:54 - choosing the searches to discal test
157:57 - that I will be using such as the two
157:59 - sample Z test and then how you can
158:02 - calculate the test statistics how you
158:04 - can calculate the P value of the test
158:06 - statistics and they use that with the
158:09 - statistical significance to test the uh
158:12 - statistical significance of your ab test
158:15 - after this we will also then compute the
158:17 - confidence interval comment on the Gen
158:19 - ability of the ab test and then at the
158:22 - end we will also test for the Practical
158:25 - significance of the ab
158:27 - test then we will conclude and we will
158:30 - wrap up and we will make a decision
158:32 - based on our datadriven approach using
158:35 - the ab test to check whether it's worth
158:38 - it to change a feature in our ux design
158:40 - in the lunar text landing page so
158:43 - without further Ado let's get started so
158:46 - let's now start our case study in here
158:49 - here I have in the left hand side this
158:52 - uh version of our landing page so which
158:55 - is our control version so to say the
158:57 - existing version where you can see that
158:59 - here we have start free trial and here
159:02 - we got us our button secure free trial
159:06 - in the right hand side we got this new
159:08 - experimental version that we would like
159:09 - to have which is the Andro Now button so
159:12 - as we saw in the introduction what we
159:14 - are trying to understand is that whether
159:17 - our customers click more on the new
159:20 - version the experimental version versus
159:22 - the existing version the control version
159:25 - so um as of the day of uh loading this
159:29 - and uh conducting this case study our
159:31 - lending page uh has a secure free trial
159:34 - but what we want you to test with our
159:37 - data is whether the uh endroll now is
159:40 - more engaging such that we can go from
159:42 - the secure free trial version to the
159:44 - Andro now version and uh here um for
159:49 - this specific case not only but also in
159:52 - general as we know from a testing is
159:55 - that whenever we got an existing
159:58 - algorithm or existing feature existing
160:01 - button then we are referring this group
160:04 - that we will um where we will expose
160:07 - this existing version of the product we
160:09 - are referring this as a control group so
160:12 - all the users to whom we will show the
160:15 - existing version of our landing page we
160:17 - will refer them as the uh control group
160:20 - participants and then we have the the
160:23 - right hand side our experimental version
160:25 - and our experimental users so the users
160:28 - our existing customers that are selected
160:31 - to be taken part um in our experimental
160:34 - group and in our experment they will be
160:36 - then uh exposed to this new version of
160:40 - our landing page which contains this
160:41 - endroll now button so our end goal in
160:45 - terms of the business as we saw in the
160:48 - introduction is to understand whether we
160:51 - should release the new button which will
160:54 - end up being higher engaging which means
160:58 - that we will have higher CTR or higher
161:01 - uh more uh clicks that will come from
161:03 - our user site which uh automatically
161:06 - means better business because we want to
161:09 - have highly engaging users if they are
161:12 - clicking on this button it means that it
161:15 - interests them more compared to the
161:17 - control version and uh if something on
161:20 - our landing page in this case our call
161:22 - to action is more interesting and highly
161:24 - engaging it means that we are doing
161:27 - something right and our users might uh
161:30 - either make use of our free products or
161:33 - uh purchase our products or um just stay
161:36 - engaged with us to keep Runner Tech in
161:39 - mind and whenever there is someone who
161:42 - uh is interested in data science or AI
161:45 - um Solutions or products then they can
161:48 - at least refer their friends if they are
161:50 - just clicking to understand and to learn
161:52 - more about our products that's also
161:54 - possibility so from a business
161:57 - perspective we therefore are using here
162:01 - as our primary
162:03 - metric uh our click through rate the CTR
162:07 - of this specific button which in our
162:09 - control version is the secure free trial
162:11 - and in our experimental version is the
162:13 - enroll now and what we want to
162:16 - understand is that whether this new
162:18 - button will end end up having higher CTR
162:21 - or not because higher CTR from the
162:24 - technical perspective will translate to
162:26 - higher engagement from the business
162:29 - perspective so here we are making this
162:31 - translation from business versus
162:33 - technical um when it comes to AB testing
162:36 - we can have different sorts of primary
162:38 - metrics we can have a clickr rate as a
162:40 - primary metric we can have a conversion
162:43 - rate as a primary metric or any other
162:46 - primary metric what we want you have as
162:50 - our metric that will work as the single
162:54 - measure that we will compare our control
162:56 - and experimental group to understand
162:57 - which version performs better is first
163:00 - to understand what this definition of
163:02 - better is and how that translates back
163:04 - to the business because if the
163:07 - engagement is what we are referring as
163:10 - Better Business for some reason and I
163:13 - will explain you in a bit why we think
163:15 - the engagement in this case is what we
163:17 - what matter for us at ler Tech
163:19 - then it means that click through rate
163:21 - can be used as a primary metric this is
163:23 - just a universal metric that has been
163:26 - used across um different web
163:28 - applications search engines recommender
163:30 - systems and many other digital products
163:32 - to understand whether the engagement of
163:35 - that specific algorithm feature web
163:37 - design whether that is better or not and
163:41 - in this case in the specific case study
163:43 - we are also going to use the CTR because
163:45 - we are interested in the engagement so
163:48 - at learner te we really care about the
163:51 - engagement um with our users and we want
163:55 - our users to make use of our products
163:58 - but uh ultimately to engage with us
164:01 - because if they engage with us it means
164:03 - that our products are being seen our uh
164:06 - landing page is being visited and the
164:08 - user is actually interested to click on
164:10 - that button and then the action point
164:13 - and then to start either free trial or
164:16 - to enroll to see what is going on
164:18 - because all these are signs of Interest
164:21 - coming from the user side and in the
164:24 - control version as our click to action
164:28 - is to secure a free trial which directly
164:31 - uh lends the user to our free trial to
164:34 - our ultimate data science boot camp but
164:36 - given that we are expanding which means
164:39 - that we are now offering more courses we
164:41 - are offering Freer products and also we
164:43 - have uh Enterprise clients uh we have
164:46 - businesses as clients who want data
164:47 - science and AI Solutions and who want
164:49 - corporate training therefore we want to
164:52 - go from this Niche uh version of a
164:54 - landing page so secure free trial to
164:57 - enroll now because we already have a lot
164:59 - of Engagement in terms of the free trial
165:01 - we want to make it more General so
165:03 - that's the business perspective and on
165:06 - the other hand we also want to change
165:09 - beside of changing this um main um call
165:13 - Action we want to make it generalized
165:17 - and at the same time we want to see
165:19 - whether this generalized version will
165:20 - end up leading us um a higher engagement
165:24 - not only in terms of the other products
165:26 - but also for the tree trial free trial
165:28 - itself because we always are looking for
165:32 - educating people and providing these
165:34 - free trials as that they can make use of
165:36 - our Flagship product which is the the
165:38 - ultimate data science boot camp so now
165:40 - when we understand why we care about the
165:42 - engagement here at ltech and we
165:45 - understand why we want to check whether
165:47 - this new button in our ux design will
165:49 - end up increasing the engagement or not
165:53 - we can now make this translation back to
165:55 - the data science terms because we know
165:58 - now from the business perspective All We
166:00 - Care is to understand whether this
166:02 - experimental version of the product is
166:04 - performing better or not but then this
166:07 - means that we need to conduct an AB test
166:10 - and we need to understand whether the
166:12 - ideas that we got and the speculation
166:14 - that the enroll now more General Button
166:17 - as call you action will be better than
166:19 - the secure free trial version whether
166:22 - this is actually true or not from the
166:24 - customers perspective because if we want
166:27 - to call us a data driven company we
166:30 - cannot just base our conclusions and our
166:32 - decisions for our products or for just
166:35 - in general for our product road map
166:37 - based on Intuition or logic we want this
166:40 - to be data driven which means that the
166:42 - customers are at the first place we are
166:44 - customer driven and our customers need
166:47 - to tell us whether the new um button is
166:50 - better or not and here we have conducted
166:54 - conducted an AB test and um here I won't
166:57 - be using the real data I will be using
167:00 - the uh proxy data or simulated data that
167:03 - I generated
167:05 - myself and um this one contains the
167:08 - similar structure and this uh the same
167:11 - um idea of the data that we got when we
167:14 - were conducting our I test and
167:16 - collecting this data and what is our
167:19 - business uh hypothesis in our business
167:21 - hypothesis we can say that we have at
167:25 - least 10% increase in our click through
167:27 - rate so 10% higher engagement when we
167:30 - have our enroll Now versus the secure
167:32 - free trial version of the product so
167:36 - this is our business hypothesis which
167:37 - means that our enroll now CTR so click
167:41 - through rate of the enroll Now button
167:43 - will result in at least 10% higher
167:46 - CTR than the secure free trial so there
167:49 - exist uh 10% at least 10% difference in
167:52 - terms of the engagement when we compare
167:54 - this new version of the product versus
167:56 - the old version of this new uh
167:59 - button and when we translate this back
168:02 - to statistical hypothesis we can say
168:04 - that under the new hypothesis we are
168:06 - saying that there is no statistically
168:08 - significant difference between the um
168:11 - control p and then P experimental which
168:14 - means the um um probability click
168:17 - through probability clickr rate for
168:20 - control group versus experimental group
168:22 - so under AG n the null hypothesis we are
168:25 - stating what we ideally want to reject
168:27 - we are saying there is no difference
168:30 - between the experimental and control
168:31 - group CTR and under the alternative
168:34 - hypothesis so the H1 we are saying no uh
168:38 - we do have a difference which means that
168:40 - the uh control
168:42 - groups a CTR is different from the
168:45 - control experimental group CTR and
168:49 - one key part here is to mention that
168:52 - they are not just different but they are
168:54 - statistically significantly different so
168:58 - uh when it comes to starting the case
169:00 - study first things first is to load the
169:02 - libraries in this case study we are
169:04 - going to use a numpy we are going to use
169:07 - a pendis as usual for any sort of data
169:10 - analytics data sience um case studies
169:12 - you always need those two usually pendis
169:15 - will be needed for our data wrangling to
169:18 - load the data process the data visualize
169:21 - it nonp will be used to uh work with
169:24 - different arrays and part of the data
169:27 - then we are going to use a ci. stats uh
169:30 - model and from that we will import the
169:32 - norm function later on um we will see
169:35 - that we are using this in order to
169:37 - visualize this um uh rejection region
169:41 - that we get from for our test to
169:43 - understand whether we need to reject our
169:44 - n hypothesis or not then in this case
169:48 - that we also want to visualize our
169:50 - results and visualize our data for which
169:52 - we are going to need our visualization
169:55 - libraries from python which are the curn
169:57 - and the Met plot L let's look into our
169:59 - data so what we have in our data we have
170:02 - four different columns and of course
170:04 - this is a filter data that contains the
170:07 - information that we need but in general
170:10 - you can have a larger database you can
170:12 - have more sorts of um um matric matrices
170:17 - and uh different other Matrix but for
170:20 - conducting your ab test the pure AB test
170:23 - you actually need only the following
170:25 - information so you need your user ID to
170:29 - understand uh what are the user you are
170:31 - dealing with so it's the user one user
170:34 - two user 10 it can be that you have
170:36 - other way of referring to your users and
170:39 - uh those can be for instance this long
170:41 - strings that we use to refer to our user
170:44 - but given that our case is a simple one
170:48 - our case study we have just a user ID
170:51 - and this user ID is just a integers that
170:55 - go from one till uh until the end of our
170:58 - uh data and here we got in total 20,000
171:03 - users therefore this number user ID goes
171:06 - to um
171:07 - 20,000 and those 20,000 um are all part
171:12 - of the user group which means that they
171:15 - are all users and they contain both the
171:18 - experimental and control users then we
171:21 - have our uh click variable and this
171:25 - click variable is a binary variable
171:28 - which can be uh either one or zero where
171:31 - one refers that the user has clicked on
171:35 - the button and zero means the user
171:38 - didn't click on the button this is our
171:41 - primary metric for our AB test then we
171:44 - have the group reference which is this
171:47 - um string varable able and this string
171:49 - variable helps us to understand whether
171:51 - the user comes from the experimental
171:53 - group or from the control group so this
171:55 - can contain only two different values
171:58 - two strings and it is X referring to the
172:01 - experimental and control referring to
172:04 - the uh control group if you can see here
172:07 - we got just the three letters X
172:09 - referring to the experimental group and
172:11 - then if we go in here because we have
172:13 - first the experimental and then the
172:15 - control ones you can see that here we
172:18 - got the uh control group then we have
172:20 - also some time stamp which is uh not
172:23 - something relevant so we'll be skipping
172:25 - that for now um given that this uh data
172:29 - that we have here it's not the actual
172:31 - data our data but it's a synthetic one
172:35 - but similar in terms of its structures
172:37 - in terms of the uh nature of variables
172:40 - and you can Implement exactly the same
172:43 - steps when you have your data and you
172:45 - are getting it from your ab test and
172:47 - then you are conducting your a has uh
172:49 - case study so in here what we are going
172:53 - to make use of the most is our click
172:55 - variable and the group variable because
172:57 - we want to find out per group what are
173:00 - the users that have clicked on the uh
173:03 - button and to be more specific we are
173:06 - looking for these averages so we are not
173:09 - so much interested that that specific
173:11 - user from that specific group has
173:13 - clicked on the product or not that's
173:15 - something that we can explore later but
173:17 - for now we are interested on the more
173:20 - high level so what is this uh
173:22 - percentages what is the click
173:23 - probability or click true rate perir
173:26 - group and here we got groups of
173:28 - experimental and control as it should be
173:31 - in any source of ab test so once we have
173:35 - conducted our AB test then I will also
173:37 - provide you more insights on what you
173:39 - can do with your data especially with
173:41 - this user ID to learn more about uh the
173:45 - idea behind these different decisions or
173:48 - whether your ab test is different per
173:51 - group but the idea is that this AB test
173:54 - that we are conducting by following all
173:57 - the steps and by ensuring that the uh
174:00 - pitfalls are avoided that we are making
174:03 - a decision that um represents the entire
174:07 - population so we are using a sample that
174:09 - is large enough for us to make a
174:12 - decision for our product and for our
174:14 - business that will be generalized and
174:17 - will be a representation
174:19 - and representative when we apply this
174:21 - decision on our
174:23 - population so let me close this part
174:27 - because we no longer need this and let's
174:29 - go ahead and load this data so here I'm
174:32 - using the pendis library and the common
174:35 - uh approvation of PD and I'm saying pd.
174:39 - read CSV and then I'm here referring to
174:43 - the name of the data that contains my
174:46 - click data and here you can see that dat
174:49 - the DAT data is here so abore testore
174:53 - click dat. CSV and I will be providing
174:57 - you this data because you won't have
174:59 - this in your own Google clap you will
175:01 - have the link to this Google clap and
175:04 - I'll provide you also the data such that
175:06 - you can put that data you can download
175:08 - it first from my source and then load it
175:11 - in here by using this
175:14 - specific button in here and by doing
175:17 - that you can then go to that specific
175:19 - folder where you downloaded the data and
175:22 - then you will have also this uh
175:24 - corresponding CSP file in your folders
175:27 - so once you have that then you will uh
175:30 - smoothly run this code and uh here I'm
175:34 - loading that data and putting under the
175:36 - name of DF abore test so basically the
175:39 - data frame containing my ab test click
175:42 - data what I want you to do is to
175:45 - Showcase you how the data looks like so
175:47 - here you will see the header given that
175:49 - here I haven't provided any argument it
175:51 - just looks at the top five elements so
175:55 - the top five rows and here I got only
175:59 - the first five users from the
176:00 - experimental group I see that some of
176:02 - them have clicked some of them didn't
176:04 - click and the corresponding user ID and
176:06 - the time stamp uh that they um done the
176:10 - click
176:12 - action then um when we look at
176:17 - the describe function you can see here
176:21 - that this gives us more general idea uh
176:23 - of uh what the data contains not so much
176:26 - what the top five rows just look like
176:29 - which is great in terms of to understand
176:31 - what kind of data you are dealing with
176:32 - with what kind of variable you have now
176:35 - you can see more the uh total uh picture
176:38 - so high level picture what kind of um
176:41 - data what amount of data you got so the
176:43 - descriptive statistics so here we can
176:45 - see that in total we got 20,000 of users
176:49 - included in this data so 20,000
176:52 - observations 20K rows and then we have
176:55 - the mean for the user ID of course it
176:58 - it's not relevant the mean is
177:00 - 10,000 and um this is an interesting
177:03 - number so we see that the average click
177:07 - uh when we look at both user and control
177:10 - the experimental and control groups it
177:12 - is 40% so
177:15 - 0.40 uh 52 so 4052
177:18 - present however this is not what we are
177:22 - too much interested so this is not to be
177:25 - confused with the click through rate
177:28 - perir group what we are interested is
177:30 - the click- through rate or the mean
177:32 - click through um when it comes to the
177:36 - experimental group and the control group
177:40 - so then we have our standard deviation
177:43 - we see a high standard deviation which
177:46 - is understandable given that we have
177:48 - this uh large variation in our data we
177:51 - got a control group and experimental
177:53 - group and this variation shows that we
177:55 - have a huge difference in them these
177:58 - different
177:59 - values uh when it comes to the click
178:01 - event and then we have the mean and the
178:03 - maxim which doesn't give us too much
178:05 - information because the click event so
178:07 - the click variable is a binary variable
178:09 - it contains the zeros and ones so
178:11 - naturally the minimum will be the value
178:14 - zero because the click can take value
178:16 - zero and one and the large lest one is
178:18 - of course one which means the maximum
178:20 - would be one and then for the rest the
178:22 - 25% so the first quantile the second
178:24 - quantile the 15% which s the median or
178:27 - the third quantile the 75th percentile
178:30 - is not that much relevant so when it
178:32 - comes to the descriptive statistics for
178:33 - this kind of data especially if it's
178:35 - filtered it's not super relevant but if
178:38 - you would have a larger data more
178:40 - matrices beside of Click which is your
178:42 - primary Matrix but you all have also
178:44 - measured some other Matrix which is
178:46 - recommendable then you would see more um
178:50 - values which would be interesting to
178:52 - look at so not only to look at the click
178:54 - rate but also to look at for instance
178:56 - the mean or maybe the median of
178:58 - conversion rate or the uh mean uh amount
179:03 - of time the average amount of time the
179:04 - user has spent on your landing page or
179:07 - how much time did that user end up
179:09 - spending before making that decision of
179:10 - a click those can be all very
179:12 - interesting Matrix to look into from the
179:14 - product uh data science perspective to
179:17 - understand the decision process and the
179:19 - channel and The Funnel of these clicks
179:22 - but for now for our case study what we
179:25 - are purely interested is in our primary
179:28 - metric which is the click event so what
179:32 - we can also see in here is that we got
179:35 - um uh in our group um when it comes to
179:39 - the control group we got uh
179:43 - 1,989 users out of all uh control users
179:47 - that and end up clicking versus the
179:49 - experimental group where we have
179:53 - 6,6 users who did click so do not
179:57 - confuse this with the total amount of
179:59 - users per group this amount is the um
180:04 - grouping of the uh data so using the
180:08 - group by and then group so we are
180:10 - grouping that data per group and we want
180:13 - to see per group what is the sum of this
180:16 - variable sum of the clicks and give that
180:18 - the click is a binary variable we know
180:20 - from basics of python that we are
180:22 - basically accounting the number of Click
180:25 - events because if you got a binary
180:27 - variable containing zeros and ones if
180:29 - you do the sum of the clicks adding the
180:32 - zeros do not doesn't have any impact
180:35 - which means that um you end up just
180:37 - summing up all the ones to each other
180:40 - and then you end up getting the number
180:43 - of or the total amount of uh cases when
180:47 - this click variable is equal to one so
180:49 - in this casee when there is a click
180:50 - event therefore we can see that per
180:53 - experimental group um we got
180:56 - 6,16 uh users out of all the
180:59 - experimental users that end up clicking
181:01 - and then out of control group this
181:03 - amount is much lower so we end up having
181:06 - uh only
181:07 - 989 users clicking so let's now go ahead
181:10 - and visualize this data I want to
181:12 - showcase in a bar chart using this
181:15 - clicks what is the total number of
181:17 - clicks so I I want to show the
181:18 - distribution of the clicks when it comes
181:21 - to um The Click event pair group and
181:25 - here I want to uh see next to each other
181:30 - the experimental group and control group
181:34 - and as you can see here here we are
181:36 - getting our bar charts and the yellow
181:41 - correspond student no which means that
181:43 - there was no click versus the uh black
181:47 - corresp resps to the yes which means
181:49 - there was a click so whenever you see
181:51 - this amount it means that that amount
181:54 - corresponds to no click no engagement
181:57 - from the user side and this is per group
181:59 - so this is what we are referring as a
182:01 - click distribution in our uh data in our
182:05 - experimental uh and control groups and
182:08 - the way that I generated this bar chart
182:11 - is by first creating this um uh list
182:14 - that will contain the colors that I want
182:16 - to assign to each of my groups and I'm
182:18 - saying zero corresponds to the yellow
182:20 - and one corresponds to Black which means
182:22 - that if my variable contains an amount
182:24 - of zero in this case my click is equal
182:27 - to zero it means that I don't have a
182:30 - click so it's a no and this I want to
182:33 - visualize by yellow otherwise I have a
182:35 - black which means that um the um one
182:39 - corresponds to the case when we have um
182:43 - click and in this case we will get a
182:45 - black as you can see here the uh yes
182:48 - which means a click is um visualized by
182:51 - this black color and then what I'm doing
182:54 - is that I'm initializing this uh figure
182:57 - size by saying that I I want to have a
182:59 - figure size of 10 and six you you can
183:01 - also skip it but I I think it's always
183:04 - great to put the size of a figure to
183:06 - ensure that you are getting the size
183:08 - like you want it to be such later on you
183:09 - can also download or take a
183:11 - screenshot then we have this uh here I'm
183:14 - using as you can see a combination of
183:16 - the Met plot leap. pip plot Library as
183:19 - well as the uh caburn because curn has
183:22 - much nicer colors and here I'm saying uh
183:25 - we are going to uh make use of the curn
183:29 - to um create um count plot because we
183:34 - are going to count and we are going to
183:36 - showcase the counts per group uh what is
183:39 - the number or the count of the clicks
183:42 - versus no clicks for a group called
183:46 - experimental and what is the number of
183:49 - um or the percentage of clicks versus no
183:51 - clicks when it comes to the group
183:54 - control and then here I'm specifying
183:56 - that the Hue should be on the click
183:59 - which means that we are looking at the
184:00 - click variable and we are going to use
184:03 - the data dfab test which means that we
184:05 - are going to look in this data from here
184:07 - we are going to select this specific
184:09 - variable called click and we are going
184:11 - to use this in order to group our data
184:14 - based on this group so you can see that
184:16 - we are doing the grouping on the
184:18 - variable called group so the argument is
184:20 - called x x is equal to group we're
184:22 - grouping our dfab has data on this group
184:25 - and we are going to do the count in our
184:28 - count plot based on this variable click
184:30 - basically what I'm saying here is that
184:32 - go and group our data dfap test based on
184:37 - Group which means that we will group
184:39 - based on experimental versus control and
184:41 - then I'm saying go and count the click
184:44 - events count pair group so pair
184:48 - experimental per control group what is
184:50 - the number of times when we have a no so
184:53 - we have a zero and what is the number of
184:55 - times when we have a yes or we have a
184:57 - one as a value for click variable and
185:00 - then as a pellet I'm using my custom
185:02 - pellet that I just created which should
185:04 - be in the form of list as you can see in
185:06 - here if I would have here also my third
185:09 - group or fourth group then I of course
185:11 - need to extend this color palette
185:13 - because I need to have the same amount
185:15 - of colors as the number of groups pair
185:18 - might Target variable in this case The
185:20 - Click has only two possible values 0o
185:22 - and one which means that I'm only only
185:25 - specifying the two colors in my list so
185:28 - then we have the title of our plot
185:31 - always nice to add by and then we have
185:33 - our labels which means that I want to
185:36 - emphasize uh as my X label so here I
185:39 - want to have my group you can see here
185:42 - is my group because I will either have
185:43 - group experiment or control that's my
185:46 - variable on my
185:48 - xaxis and on my y AIS of course I have
185:51 - the the count so I'm counting the number
185:54 - of times I got uh the uh no click versus
185:58 - click event so here note that the um y
186:02 - AIS is in terms of this count so here
186:05 - you can see it's uh 8,000 here sa 7,000
186:10 - or 6,000 5,000 which means that we are
186:12 - talking about the numbers and the counts
186:15 - rather than percentages and this is
186:18 - important because um another thing that
186:21 - I'm also doing is that I'm going the
186:24 - extra mile and I'm also adding beside of
186:27 - this counts on the top of each bar I I
186:30 - want to visualize and clarify what are
186:32 - the corresponding percentages it's
186:35 - always great to enhance your data
186:39 - visualization with some percentages
186:41 - percentages is easier for the uh person
186:44 - who follows your presentation to
186:46 - understand for inance if you got an
186:48 - experimental group and the the users is
186:51 - here 6,000 and um 4,000 they might not
186:55 - quickly understand that you got for
186:57 - instance in total 10,000 of users and
187:00 - then 6,000 has then uh clicked and then
187:03 - 4,000 didn't click so um then the idea
187:09 - is that by adding this percentages we
187:11 - can then see that
187:15 - 61.2% has clicked in this experimental
187:18 - group and
187:20 - 38.8% has not clicked of course this a
187:23 - simulated data I specifically pick the
187:26 - extreme in such way that we can clearly
187:28 - see this difference in the clickr rates
187:31 - but um in the reality you can have a
187:33 - clickthrough rate of 10% up to 14% which
187:36 - is usually a good number if you have a
187:37 - click through rate of 40% is great but
187:40 - it really depend on underline user basee
187:42 - what kind of product you got how large
187:44 - is your user base because if you have
187:46 - very large user base then 10% can be a
187:49 - good clickthrough rate versus if you
187:52 - have a very small user base maybe uh 61%
187:55 - is considered uh good or
187:58 - average so uh in here we have just a
188:02 - simulated data of course and I've have
188:05 - added these percentages uh by using the
188:09 - following code so I won't go too much
188:12 - into detailing here um feel free to
188:15 - check and see uh and if something that
188:17 - doesn't make sense go back to our python
188:19 - for data science course that contains
188:21 - lot of information on the basics in
188:23 - Python but here just quickly what I'm
188:26 - doing is that I am uh calculating the
188:29 - percentages and I'm annotating the bars
188:31 - so I want to know what are this
188:33 - percentages which means that per group I
188:36 - want to take the total amount of clicks
188:38 - I want to understand what is the number
188:40 - of Click event when the click variable
188:41 - is equal to one so and what are the
188:44 - number of cases when there was no click
188:45 - from the user side which is what are the
188:47 - number of cases when the click variable
188:49 - is equal to zero and then I'm counting
188:51 - those amounts and then using the total
188:53 - amount to calculate the percentage for
188:56 - instance in this specific case I'm
188:58 - filtering the data for experimental
188:59 - group I'm looking at the total number of
189:02 - users for this group which is 10K and
189:04 - then I'm counting the number of times
189:06 - when out of this 10,000 users the amount
189:08 - of users that end up clicking on that
189:11 - button which is the click is equal to
189:14 - one case and then I'm taking that number
189:17 - dividing it to the total number of users
189:19 - for this experimental group multiplying
189:21 - by 100 in order to get that in
189:23 - percentages and this is the calculation
189:26 - that you can see in here one thing that
189:29 - is important here is that here I'm using
189:32 - this um uh
189:34 - percentage um so for the current bar I'm
189:37 - saying U as a way to identify whether we
189:40 - are dealing with experimental control
189:42 - group is by getting by looking into this
189:46 - uh p and and uh this p in here is the
189:50 - basically the patches so in this case
189:52 - I'm basically saying if I'm dealing with
189:55 - the experimental group then go ahead and
189:57 - calculate what is this uh total amount
190:00 - of observations and then take what is
190:02 - the uh number of clicks and then divide
190:05 - the two numbers uh C multiply this with
190:08 - 100 and this will then give us the
190:11 - percentage and then I'm doing this for
190:13 - each of those groups so I'm doing it for
190:15 - this group I'm doing for this group
190:17 - group and for this one and for this one
190:18 - so I got two groups but then within each
190:20 - group I got clicks and no clicks and I'm
190:23 - calculating this four different
190:25 - percentages and then I'm adding these
190:27 - percentages on the top of those bars so
190:30 - I not only want to have numbers
190:32 - represented in my visualizations but I
190:34 - also want to add this corresponding
190:36 - percentages at the top just for
190:38 - visualization purposes I wanted to put
190:41 - this out there because this can help
190:43 - your uh data visualization toolkit and
190:46 - it also will um make your audience from
190:50 - your presentations be more thankful to
190:52 - you when you are telling the story of
190:54 - your
190:55 - data so uh this is about the data that
190:58 - we have we see that uh 38.8% of our
191:02 - experimental group users have not
191:04 - clicked on the button versus the
191:07 - 61.2% have clicked on the button based
191:09 - on the simulated data and then uh in the
191:13 - control group we have a quite the
191:15 - opposite situation we got the majority
191:17 - of the users
191:19 - 80.1% not clicking on the button versus
191:23 - the remaining 19.9% have actually
191:25 - clicked on that button so we got a huge
191:29 - difference a dissonance when it comes to
191:31 - the experimental group and uh control
191:33 - group this kind of gives us an
191:35 - indication hey something is going on
191:38 - here we kind of uh have already um high
191:42 - level intuition what the remaining
191:45 - anasis will look like
191:47 - um which is that there most likely will
191:50 - be a difference in their ctrs when it
191:53 - comes to the uh the um uh control versus
191:58 - experimental group and the uh
192:00 - corresponding buttons but uh hey let's
192:04 - continue that's the entire goal behind a
192:06 - testing is to ensure that our intuition
192:09 - our conclusions are all based on the
192:11 - data rather than on our intuition so
192:16 - what are the parameters that I'm using
192:17 - here for conducting our AB test when I
192:21 - was designing this AB test uh the first
192:23 - step was to of course do all these
192:25 - different translations that we learn as
192:27 - part of our AB test course um conducting
192:30 - it properly which means coming up with
192:32 - this three different parameters when
192:35 - doing our power analysis and usually
192:38 - this should be done when you are
192:40 - collaborating also with your colleagues
192:43 - and uh with your product managers or
192:45 - your product people domain EXP experts
192:47 - because they have um a lot of
192:50 - information on what it means to have um
192:54 - thresholds that you need to pass in
192:56 - order to say that for instance this new
192:59 - version of your feature is different and
193:02 - is uh considerably uh different from the
193:05 - existing one and here um in order to for
193:10 - us to understand this uh and make these
193:12 - conclusions we need to come up with the
193:14 - three different parameters that can help
193:16 - us to properly conduct an AB test as we
193:19 - learned when we were looking into
193:21 - designing a proper AB test so first we
193:24 - have our significance level the
193:26 - significance level or the alpha the
193:28 - Greek letter that we are using to refer
193:31 - to the significance level which is also
193:33 - the probability of the type one error
193:36 - and that amount we have chosen following
193:39 - the industry standard which is 5% given
193:42 - that we didn't have any uh previous
193:44 - information or specific reason to choose
193:47 - a different significance level so lower
193:49 - or higher we decided to go with the
193:51 - industry standard which is the 5% this
193:54 - means that we want to have um we want to
193:58 - compare our P value of our uh
194:01 - statistical test to this 5% and then say
194:05 - whether we have a statistically
194:07 - significant difference between the
194:08 - control and experimental group based on
194:10 - this 5% significance level and let's
194:13 - refresh our memory on this Alpha this
194:15 - Alpha uh or significance level is also
194:18 - the probability of type one error so
194:20 - this is the amount of error that we are
194:22 - comfortable making when we um reject the
194:26 - N hypothesis well the null hypothesis is
194:30 - actually um true which means that we are
194:33 - detecting a difference between the
194:36 - experimental and control version while
194:38 - there is no difference and we are making
194:40 - that mistake and here we are saying that
194:42 - we are fine and we are comfortable with
194:45 - making this mistake at maximum of 5% but
194:49 - higher than that it's not allowed we are
194:51 - not comfortable making uh error um
194:54 - higher than 5% then the next variable uh
194:58 - in this case the B uh or beta the
195:00 - probability of type two error which is
195:02 - the opposite of the type one error which
195:03 - is a false negative rate or the amount
195:06 - of time the um um proportion of time
195:11 - when we end up failing to reject the
195:14 - null hypothesis while null hypothesis is
195:17 - false and it should have been rejected
195:19 - then the one minus beta is actually
195:22 - power of the test so what is the amount
195:24 - of time we are correctly rejecting our
195:26 - null hypothesis and correctly stating
195:29 - that there is indeed a statistically
195:30 - significant difference between our
195:32 - experimental group and our control group
195:36 - so we have chosen for this the uh
195:38 - industry standard as well which is the
195:40 - 80% but given that for your results
195:43 - analysis in this case for conducting
195:45 - this case study that part of the power
195:47 - analysis is not relevant we use that
195:49 - when calculating our minimum sample size
195:52 - but we don't need that when conducting
195:53 - our results analyses therefore I'm not
195:56 - initializing that as part of this code
195:59 - so here I'm only providing to my program
196:03 - the values for my significance level
196:05 - which is 0.05 or this is the same as
196:09 - 5% and then the Del which is the third
196:12 - parameter and this Delta is our minimum
196:15 - detectable effect effect so this a Greek
196:18 - letter Delta which is the minimum
196:20 - detectable effect helps us to understand
196:23 - whether beside of having this
196:26 - statistically significant difference
196:28 - whether this difference is large enough
196:30 - for us to say that we are comfortable
196:33 - making that business decision to launch
196:35 - this new button so it can be that when
196:39 - we are conducting an AB T has we are
196:42 - finding out that the experimental group
196:45 - has indeed higher engag ENT than the uh
196:48 - control group and we are uh getting a
196:51 - small P or at least smaller than the
196:54 - alpha and we are seeing that P is more
196:56 - than the alpha level which means that we
196:58 - can reject the null hypothesis and we
197:00 - can say that the uh CTR or the clickr
197:03 - rate of the experimental group is
197:05 - statistically significantly different
197:07 - from the control group at 5%
197:08 - significance level but we know from the
197:11 - theory of ab test that only that is not
197:14 - enough only statistical significance is
197:16 - not enough for the business to make that
197:18 - important decision to launch an
197:20 - algorithm or to launch a feature in this
197:22 - case to change our lending page the
197:25 - button from the uh start free trial to
197:28 - the endroll now which means that we want
197:31 - to have enough users and we want to have
197:35 - enough difference large difference in
197:37 - our click through rates or enough users
197:39 - saying that we are more happy with this
197:41 - uh new version of the landing page for
197:43 - us to go and change our feature
197:47 - and what is this definition of enough
197:49 - what is the difference in the clickr
197:52 - rate that we need to detect after we
197:55 - have detected the statistical
197:56 - significance in order for us to say that
197:59 - we also have a practical significance so
198:02 - practically we are also comfortable
198:05 - making that business decision and then
198:07 - launching this new feature and changing
198:09 - our landing page button and that is
198:12 - exactly what we have under our Delta
198:14 - this minimum detectable effect in this
198:17 - case we have chosen for Delta of
198:20 - 10% so you can see here 0.5 this is 10%
198:25 - this means that our Delta or the MD the
198:28 - minum detectable effect is 10% this
198:31 - means that we are saying not only we
198:33 - should have a statistically significant
198:35 - difference between the experimental
198:37 - group and control group but also we need
198:39 - to have this difference to be at least
198:42 - 10% which means that we need to have
198:48 - detected that the experimental version
198:51 - of the landing page results in at least
198:53 - 10% higher click rate compared to the
198:57 - control version for us to go ahead and
199:01 - to launch this new version and deploy
199:04 - this new uh ux uh feature so this is
199:08 - really important because many people go
199:12 - and check for statistical significance
199:13 - so they do their Alpha and then check uh
199:16 - whether the P values were the alpha and
199:18 - then say hey we have a statistically
199:20 - significant difference and then they are
199:22 - done with that but that's not
199:25 - correct after you have conducted your uh
199:28 - statistical significant analysis and you
199:30 - have detected that your uh experimental
199:33 - version has a statistically significant
199:36 - different um CTR than the control
199:40 - version at your Alpha significance level
199:43 - the next thing you need to do is to
199:45 - ensure that you also have a practical
199:48 - significance beside of the statistical
199:50 - significance and this practical
199:52 - significance you can detect and you can
199:55 - check when you use your mde or your
199:57 - Delta and you compare it to your
200:00 - confidence interval that you have
200:02 - calculated something that we have also
200:04 - learned as part of the theory of
200:05 - conducting a proper AB test but once we
200:09 - come to that point so after we check for
200:11 - our statistical significance I will also
200:13 - explain how exactly uh we will need to
200:15 - do this check and at the same time we
200:18 - will also be refreshing our theory on
200:20 - the Practical significance so let's now
200:23 - go ahead and calculate the total number
200:25 - of clicks per group by summing up these
200:28 - clicks and I also want to calculate and
200:32 - group by this amount just to Showcase
200:34 - how you can do that on your own so here
200:37 - what I'm doing is that I'm taking my ab
200:39 - test data I'm grouping by by group group
200:43 - is the uh variable that contains the
200:46 - reference whether we are dealing with
200:47 - experimental group or control group and
200:50 - as you know from our uh python series
200:53 - and demos python for data science course
200:56 - that uh whenever we want to group that
200:58 - data a pendis data frame first we need
201:00 - to say pendis data frame name do group
201:03 - by Within parenthesis the variable that
201:05 - we are using to do the grouping which is
201:07 - in this case group and then within
201:09 - Square braces I want to emphasize and
201:12 - put the name of a variable that I want
201:14 - to um apply operation on so I want to
201:18 - group my data on the group variable and
201:21 - I want to count the number of times I
201:24 - have a click in my control group and in
201:27 - my experimental group this will be my X
201:30 - control and X experimental variables so
201:33 - X control will then compute contain
201:36 - information about number of Clicks in my
201:38 - control group and then X experimental
201:41 - will contain the number of Clicks in my
201:42 - experimental group and given that um I
201:46 - want to refer to the name of that uh
201:51 - Group after I did my grouping so I am
201:56 - getting this kind of this shape of data
201:59 - frame of course I then need to uh use my
202:03 - do log function in order to properly
202:06 - call that amount so to understand what
202:08 - is this amount corresponding to this
202:10 - index and what is this amount
202:11 - corresponding to this index and given
202:13 - that my index is in strings I'm then
202:15 - using here my do log function something
202:18 - that we also learned as part of our
202:19 - python for data science course so here
202:23 - is basically the printing just writing
202:26 - nicely what are the results which means
202:28 - that we are counting that the let me
202:31 - count again that the uh number of uh
202:36 - clicks for my control group is
202:42 - 1,989 so you can see that it is
202:47 - want to double check and see what we got
202:49 - yes so we got the same number so we are
202:51 - dealing with the same data set just to
202:53 - make sure and here the number of clicks
202:55 - for experimental group is equal to 6K
202:58 - and 116 so
203:00 - 6,116
203:02 - clicks so then we are calculating the uh
203:05 - pulled estimates for the clicks per
203:08 - group let me quickly fix typo so
203:12 - calculating the uh pulled estimate for
203:15 - the clicks pair Group which means the P
203:18 - estimate for the experimental group and
203:21 - for the uh control group so let me
203:23 - quickly add here how I can calculate the
203:26 - uh total cases when we got uh
203:31 - experimental group users so what is the
203:32 - number of users in the experimental
203:34 - group and what is the number of users in
203:36 - the uh control group so here what I want
203:39 - to do is that I want to say that the
203:42 - group The DF test group should be equal
203:45 - to
203:47 - experimental and this of
203:51 - course should be my filter and I want to
203:54 - count
203:56 - this and let me quickly copy this I saw
203:59 - that is already under the control so
204:02 - here I'm changing to the control and
204:05 - this will need to give me the number of
204:09 - users in each of these groups too so
204:12 - number of users in control and number of
204:16 - us users in Click and here I will simply
204:19 - check
204:20 - this so I will print then the number of
204:23 - users per group and at the same time I
204:25 - will also click the number of clicks per
204:28 - group there we go so now when we have
204:33 - done this what we are ready to do is to
204:36 - go ahead and calculate the P estimat for
204:39 - clicks per group which means pair
204:42 - control group and pair experimental
204:43 - group for that what we need to do is to
204:47 - take the number of clicks of the control
204:49 - group divide to the number of all users
204:52 - for control group as you can see in here
204:54 - x control / to n control and we are
204:57 - referring to this variable as P control
205:00 - head because we know that the estimate
205:02 - of this click probability um is always
205:05 - with a hat it's just the way that we
205:07 - reference it in um statistics and in a
205:11 - testing so this is the estimate
205:14 - something that we are estimating the
205:16 - therefore we are saying head and then we
205:18 - have the same for experimental group
205:20 - which means that the estimate of the
205:22 - experimental groups uh click probability
205:25 - is equal to X exp and then divided to n
205:27 - x then um in order to calculate the uh
205:31 - pulled estimate or uh pull click
205:35 - probability which means the value that
205:37 - will describe of the uh control group
205:40 - and experimental group we need to follow
205:42 - this formula which means that we are
205:45 - taking the control we are adding to that
205:48 - X control the X experimental this is our
205:51 - nominator of our uh value and then we
205:55 - are dividing this to the uh sum of the
206:00 - sizes of each of those groups which is n
206:02 - control and N
206:04 - experimental so this is the common
206:07 - formula of the pulled estimate uh when
206:10 - it comes to this type of experimentation
206:13 - when you are dealing with um primary uh
206:15 - met trick that is in the form of zeros
206:18 - and ones and if you want to refresh your
206:21 - memory on this type of formulas then
206:23 - make sure to also check our AB testing
206:25 - course because in there we go in detail
206:29 - in this uh specific lesson of the uh AB
206:32 - test result results analis we are
206:35 - looking into this uh all these formulas
206:37 - on how we can calculate the pulled
206:40 - estimate of this uh click
206:43 - probability so click probability but
206:46 - then we are calling
206:48 - it
206:51 - P click
206:54 - probability and then what we
206:58 - got is this
207:00 - volum so that amount is then 0 and
207:05 - 40 this number should look familiar
207:08 - because this is then the mean that we
207:10 - saw when we were looking at the um uh
207:14 - descriptive statistics table if you can
207:17 - recall this
207:19 - table let me see this
207:22 - number so now basically we are then
207:25 - calculating this manually because we
207:27 - need a variable that will hold this uh
207:30 - volume so it is simply summing up all
207:34 - the
207:35 - clicks for control group and
207:37 - experimental group to get the total
207:38 - number of clicks and we are dividing it
207:41 - to the total number of users so n
207:43 - Control Plus n experiment so now when we
207:46 - have this we are ready to also calculate
207:48 - what we are referring as a pulled
207:49 - variance also something that we have
207:52 - learned as part of the theory for AB
207:54 - testing so the pulled variance is equal
207:56 - to the pulled estimate of the clicks so
207:59 - P had something that we just
208:01 - calculated multiplied by one minus P
208:05 - head so the uh click event the estimate
208:08 - of the click probability multiplied by
208:11 - the estimate of no click and we know
208:14 - already this idea of Bol distrib bution
208:16 - that the variable that uh describes this
208:19 - process of clicks and no clicks follows
208:21 - kind of this idea of bero distribution
208:23 - when we have a click and no click so we
208:26 - have probability of click and then we
208:28 - have probability of no click which is
208:30 - the one minus that click probability so
208:33 - that's the idea or the part of the
208:35 - formula that we are following as kind of
208:37 - an intuition and then this multiplied by
208:41 - 1 / to n control + 1 / to n experimental
208:46 - so here I'm purely following the formula
208:48 - for the PED variance if you want more
208:50 - details and explanations and sure to
208:52 - check the corresponding Theory lecture
208:55 - because we are going into details of
208:57 - each of those formulas and understanding
209:00 - why we calculate this um P variance and
209:03 - P estimates uh in this specific way and
209:06 - using these specific formulas so here by
209:10 - just follow following the uh formula I'm
209:12 - getting that the uh pull uh variance is
209:16 - this
209:17 - amount so this is in nutshell how I
209:20 - calculated my uh pull click probability
209:24 - and a pulled variance of that click
209:26 - event and we are going to need that in
209:29 - the next very important step which is
209:32 - calculating the standard error and
209:35 - calculating the test statistics because
209:39 - in this case what we are doing is that
209:41 - we are dealing with a case when the
209:45 - primary metric
209:46 - is in the form of zeros and one so we
209:49 - let's Now quickly talk about the uh
209:51 - choice of a statistical test be uh
209:55 - before conducting the actual calculation
209:57 - of standard error in the test statistics
209:59 - so here I went for the two samples at
210:02 - test and let me explain you why and what
210:04 - is the motivation because as we learned
210:06 - as part of the theory um whenever we
210:09 - have a primary metric that is in the
210:12 - form of an averages like we have now
210:15 - because we are using the P control head
210:17 - and P experimental head head so we have
210:20 - a primary metric that is the uh click
210:23 - through rate which is the average clicks
210:26 - per group so we have calculated the
210:29 - average click per experimental group and
210:31 - per control group then the primary
210:34 - metric the form of it already dictates
210:38 - given that it's in averages that we need
210:40 - to look at uh either parametric test
210:43 - corresponding to this averages or not
210:46 - parametric test corresponding to the um
210:48 - averages in this case I went for the
210:50 - parametric case because uh it has Better
210:53 - Properties if I have this information
210:56 - about the distribution of my data and
210:59 - why do I have this information and then
211:02 - this also dictates the uh choice of my
211:05 - um statistical test well I have a size
211:09 - of my sample which is over 100 and
211:12 - actually over 30 that's the threshold
211:14 - that we tend to use in statistics and in
211:17 - a testing in order to say whether we
211:19 - have a large size or large data or not
211:23 - if our sample is not large so it
211:25 - contains less than 30 users per group
211:28 - which happens as well then we say that
211:31 - we need to go for um statistical
211:35 - tests uh that will be specific for this
211:38 - kind of cases because we can no longer
211:41 - make use of the uh statistical theorems
211:43 - like the central limit theorem we helps
211:46 - us to um uh to take the uh to the
211:49 - inference so to make use of the
211:50 - inferential statistics and make
211:52 - conclusions regarding the distribution
211:55 - of our population just having the sample
211:57 - and what do I mean by that so if my
211:59 - sample is larger than 30 like in this
212:02 - specific case I got 10,000 users per
212:05 - group so it is definitely larger than 30
212:08 - uh
212:09 - users then in that case I can say that
212:13 - by making use of the central limit serem
212:16 - I can say that my sampling distribution
212:19 - is normally distributed and this is
212:22 - simply making use of the central limit
212:25 - theorem something that we have also
212:27 - learned when we were looking into this
212:29 - concept of inferential statistics as
212:32 - part of the fundamental statistics
212:33 - course uh course um in lunar Tech so
212:37 - this is a powerful theorem that we use
212:40 - in AB testing in order to make our life
212:43 - easier because when we have a sample
212:46 - that is larger than
212:49 - 30 for each of these groups then we can
212:52 - say that even if we don't know the
212:54 - actual distribution or the name of the
212:57 - distribution that our uh sample follows
213:00 - when it comes to the click um event so
213:03 - the random variable that describes this
213:06 - number of clicks or the average click
213:08 - through rate what is that um
213:11 - distribution exactly but given that we
213:14 - have that this uh size is large enough
213:16 - it's large than 30 users we can say that
213:19 - by making use of the central limit
213:21 - theorem we can say that the uh the uh
213:26 - sample distribution follows a normal
213:29 - distribution if given that SLE size is
213:32 - large enough and this helps us to say
213:36 - that well in that case it doesn't matter
213:38 - whether we make use of the two sample Z
213:40 - test or two sample T Test we can make
213:44 - use of either of this test in order to
213:46 - conduct our analysis and we had this
213:49 - specific template to make this Choice
213:52 - easier uh in our AB test course at the
213:57 - ler Tech where we were making all this
213:59 - decisions and saying if the SLE size is
214:02 - this we need to do this if the sample
214:04 - size is this we need to do this and in
214:05 - this specific case following that exact
214:08 - structured and organized approach I
214:10 - ended up seeing that my sample size is
214:13 - large so it's larger than 30 so I can
214:16 - then make use of the central limit
214:18 - theorem I then know what is the random U
214:21 - what my random variable describing this
214:23 - quick through rate um follows the kind
214:26 - of distribution in this case a normal
214:28 - distribution and then this means that
214:30 - whether I use a t test or Z test doesn't
214:33 - really matter I'm going to end up with
214:35 - the same conclusions therefore I will
214:37 - just go with the two sample Z test
214:39 - simply because um it is just easier for
214:43 - me to do for example
214:45 - you can also go with the two sample T
214:47 - Test and you can even change this case
214:50 - study and tweak it and then make it your
214:53 - own and put it on your resume in that
214:54 - way by making it more unique and that
214:57 - will be totally fine because you will
214:58 - see that you are going to end up with
215:00 - exactly the same conclusions as we do in
215:02 - this specific case study because if you
215:05 - have a large enough sample it won't
215:07 - matter whether you have a true sample Z
215:09 - test as your parametric test or the two
215:12 - sample T Test and um if you want to know
215:15 - why why this matters and all the
215:17 - different detail statistical insights
215:19 - make sure to check the actual uh course
215:23 - dedicated to AB testing because there
215:25 - will we cover this all and you will then
215:28 - become a master in the field of AB
215:30 - testing now we know this uh decisions
215:33 - and the motivation behind choosing the
215:36 - uh two samples that test let's now go
215:38 - ahead and do the actual calculations so
215:41 - here we have a standard error which we
215:43 - calculate by taking the pulled variance
215:46 - and taking the square root of it and
215:48 - this is again using the idea of this
215:50 - formulas that we learned as part of the
215:52 - ab test so we are using this P variance
215:55 - taking the square root of this which
215:57 - gives us the standard error and the
216:00 - standard error as you can see here is
216:02 - then equal to
216:05 - 0.69
216:07 - 29499 this
216:09 - amount there we calculate our test
216:12 - statistic for our two sample at test so
216:15 - the test statistic is equal to P control
216:18 - hat minus P experimental hats divided to
216:20 - standard error so here uh you can now
216:22 - see the motivation behind not only
216:25 - Computing the P pulled head but really
216:28 - also the p uh control head and P
216:31 - experimental head and then I take the P
216:34 - control head and subtract the P
216:35 - experimental head and I divide it to the
216:38 - standard error to compute my test
216:41 - statistics once I did this as you can
216:44 - see this is this amount so test
216:46 - statistics for our two sample that test
216:48 - is this amount minus
216:51 - 5956 rounded then um we can also compute
216:56 - the critical value of our Z test which
217:00 - is uh by using this Norm function that
217:03 - we uh loaded in here from the C high and
217:07 - this will help us to understand what is
217:09 - this value from our normal distribution
217:12 - table the standard normal distribution
217:14 - table uh where by making use of this
217:17 - table we
217:19 - identify what is this critical value
217:22 - that we need to have to uh create our
217:26 - rejection regions and to say whether we
217:28 - can uh reject our n hypothesis or not so
217:32 - to conduct our test we need to have a
217:35 - critical
217:36 - value for uh to which we will compare
217:39 - our test statistics and this critical
217:42 - value will be based simply on the
217:43 - standard normal distribution so this is
217:45 - this norm.
217:47 - ppf and then uh probability um uh
217:51 - function basically uh the the
217:54 - probability function that comes from the
217:56 - normal distribution standard normal
217:58 - distribution and as you can see this
218:00 - corresponds specifically to this percent
218:02 - Point function which is the inverse of
218:04 - the cumulative distribution function so
218:08 - this based on the alpha / 2 so 1 - Alpha
218:12 - / 2 is the argument that we need to put
218:15 - for our percent Point uh probability
218:18 - function and why divide it to two
218:21 - because we have a two sample test so
218:25 - because we have a two-sided two sample
218:27 - test sorry so if you want to understand
218:30 - this difference between uh two sample um
218:34 - test two-sided Test please check out the
218:37 - uh fundamental to statistics course at
218:39 - ler Tech because we covered this uh
218:42 - Topic in detail and it's a very involved
218:44 - topic it contain contains many
218:46 - complex from statistical point of view
218:49 - so I won't be spending in this case
218:51 - study too much time on that here I'm
218:53 - assuming that you know this formula
218:55 - already but if you don't and if you
218:57 - quickly need to do your case study na
218:59 - testing feel free just just to copy this
219:02 - line which basically is a value that we
219:05 - need based on the corresponding chosen
219:08 - statistical significance level that we
219:11 - need to compute to compare our test
219:13 - statistics so our test statistics is
219:16 - this value and the value that we need to
219:18 - compare it to is the Z critical volue so
219:22 - we can see that this critical value is
219:25 - then equal to
219:27 - 1.96 this is actually a very common
219:30 - value that we know even without looking
219:33 - at a standard normal table when you make
219:35 - use of this test enough often then you
219:38 - know that the uh critical value
219:41 - corresponding to a two-sided test when
219:43 - it comes to normal table is is equal to
219:46 - uh 1.96 this is just the value that we
219:49 - know and in here by even without
219:53 - calculating the next step which is a P
219:55 - value we can even say already what is
219:57 - the decision we need to make in terms of
219:59 - statistical significance because we know
220:02 - that one way we can test our hypothesis
220:05 - statistical hypothesis is by Computing
220:07 - the test statistics and checking where
220:09 - the test statistics the absolute value
220:11 - of it is larger than the critical value
220:14 - and we see that the test statistics is
220:16 - equal to minus
220:18 - 5956 the absolute value of that is
220:22 - 5956 and that value is much larger than
220:25 - our critical value which is equal to
220:28 - 1.96 this already gives us an idea that
220:31 - we can reject our null hypothesis at 5%
220:34 - statistical significance level but I
220:37 - want to uh go onto the next step
220:40 - actually because that's um more
220:43 - structured more organized way to do
220:45 - and conducting experimentations as in
220:48 - the industry we tend to make use of the
220:51 - P values instead of making use of this
220:54 - econometrical approach and statistical
220:56 - approach of um testing the statistical
220:59 - test so once we have calculated our test
221:02 - statistics the next thing we need to do
221:04 - is to calculate our P value and then use
221:09 - that P value compare to the significance
221:12 - level Alpha and then make a decision
221:15 - with we need to reject our n hypothesis
221:17 - and say that we have a statistical
221:19 - significance or we cannot reject our n
221:21 - hypothesis and then we need to say that
221:24 - we don't have a statistical significance
221:26 - so we don't have enough evidence to
221:28 - reject the hypothesis so the idea here
221:33 - is that we need to make use of our uh
221:36 - normal function and specifically the
221:38 - norm. SF so making use of exactly the
221:41 - same Library the norm from CI that's dos
221:45 - and then this time we're using the
221:47 - survival function which is the one minus
221:49 - the cumulative distribution function of
221:51 - normal distribution this comes again
221:53 - from statistics and then using the
221:57 - absolute value of our test statistics
221:59 - multiplying it by two given that we have
222:01 - a two-sided test I'm calculating my P
222:04 - value this is simply by making use of
222:07 - the same formula that we saw when we
222:09 - were uh studying theab test from a
222:12 - technical point of view because we
222:14 - learned that the P value is than the
222:16 - probability that Z will be smaller than
222:19 - equal the minus test statistics or that
222:22 - the test statistic is small than equal
222:24 - to Z so uh we basically want to
222:27 - calculate what is this probability the P
222:30 - value which is equal to the probability
222:32 - that our test statistics will be smaller
222:36 - than the critical value or
222:38 - our negative of the test statistics will
222:41 - be larger than equal of the critical
222:43 - value and we want to know this
222:45 - probability because what this
222:47 - probability represents is that what is
222:50 - the chance that we will get a large test
222:54 - statistics well this is due to a random
222:56 - chance and not because we have a uh
223:00 - actual statistical difference between
223:02 - the clickr rate of the experimental
223:04 - group versus control group so this is
223:06 - the idea behind P value so what is this
223:09 - chance that we are uh mistaking this
223:13 - random mistake
223:15 - this random observation that we got a
223:18 - large test statistic and saying that
223:20 - there is a statistical significance well
223:23 - there is no such thing and we are purely
223:26 - getting this large test statistics um
223:30 - because of the random chance if the
223:32 - probability of getting a large test
223:34 - statistics by random chance is small so
223:37 - if this P value is small then we can say
223:39 - that we have a statistical significance
223:41 - that's the idea behind it and this P
223:44 - value when we calculate uh we are
223:47 - storing it in this variable called pcore
223:50 - volume and then the next thing what I'm
223:52 - doing is that I'm writing this function
223:54 - quote is statistically significant which
223:56 - takes a argument as P value in Alpha so
223:59 - I just need the P value that I just
224:01 - calculated for my test Set uh test uh
224:04 - statistics and then I want the
224:06 - statistical significance level that I
224:08 - want to use for my test and then this is
224:12 - the value that comes from my power
224:14 - analysis as I mentioned before that's
224:16 - the 5% this P value I'm calculating for
224:19 - my test statistics so in here and then
224:22 - I'm taking the two and I want to compare
224:25 - them so I want to assess whether I have
224:28 - a statistical significance by comparing
224:30 - my P value to my statistical
224:32 - significance level Alpha and what is
224:35 - this comparison well we know uh from the
224:38 - theory that um if we have a low P value
224:42 - and specifically in the P that we are
224:44 - getting the P value is small than equal
224:47 - the 5% or 0.05 which is the significance
224:50 - level then this indicates that we have a
224:53 - strong statistical uh evidence that uh
224:56 - the N hypothesis is false and we need to
224:59 - reject it so we have a strong evidence
225:01 - against the null
225:03 - hypothesis and otherwise if the P value
225:07 - is larger than
225:09 - 0.05 so it's larger than 5% that we have
225:13 - chosen as the maximum transure hold of
225:15 - that mistake so the significance level
225:18 - is uh uh no longer the largest element
225:21 - but the P value is larger than your
225:23 - significance level then this indicates
225:25 - that you don't have enough
225:27 - evidence against the null hypothesis so
225:30 - your evidence is weak this means that
225:33 - you fail to reject the N
225:35 - hypothesis so this is what I'm doing in
225:38 - here with this code so I'm saying print
225:41 - the P value first and we are rounding it
225:44 - up with this round function I'm rounding
225:47 - it to the three decimal and then I want
225:50 - to check and determine whether I have a
225:53 - statistically significant or not and the
225:55 - way that I'm doing that is I'm saying if
225:57 - my P value is more than my alpha or
226:00 - actually lets add smaller than equal
226:02 - than Alpha then we can print that there
226:04 - is a statistical significance which
226:06 - indicates that the observed differences
226:08 - between the experimental and control
226:11 - groups are un unlikely to occur due to
226:14 - random chance
226:16 - which means that this is not random
226:18 - chance and uh we have a strong evidence
226:21 - that there is a statistical significance
226:23 - and this suggests that this new feature
226:27 - that we got this new version of our
226:30 - landing page with this um uh call to
226:33 - action um as the andr Now is better and
226:38 - results in higher statistically
226:40 - significantly higher clickr rate than
226:42 - the existing version of the control uh
226:45 - group so there is a real effect then
226:49 - otherwise if this is not the case which
226:51 - means that my P value is larger than my
226:53 - Alpha then I'm saying print that there
226:55 - is no statistical significance and that
226:58 - the observed difference that we see in
227:00 - the clickr rate is not because uh of the
227:04 - real difference in the performance but
227:06 - TR truly this is just a random chance so
227:09 - here we can see that once we run our we
227:12 - call the function in here which is play
227:15 - the function name and the argument so P
227:17 - value in alpha alpha comes from the
227:19 - initialized value that we had from our
227:21 - power analysis so
227:25 - from here we initialize this value
227:29 - 0.05 and then here we got the P value
227:32 - that we just calculated then what we are
227:35 - getting in here is that our P value is
227:38 - actually so small that it's um rounded
227:41 - to zero so what this means is that that
227:45 - there is evidence that suggests that at
227:47 - 5% statistical level significance level
227:51 - that the uh clickr rate of the
227:53 - experimental group is different from the
227:55 - clickr rate of the control group note
227:58 - that I'm not saying higher or lower
228:00 - because our stal test was two-sided so
228:04 - under n hypothesis we had that the uh P
228:08 - control so in here as you can see our P
228:11 - control was equal to P experimental and
228:13 - under the alternative we had that a p
228:15 - control is not equal to P experimental
228:18 - this means that we um have now rejected
228:22 - the null hypothesis we have found
228:24 - evidence that suggests that the null
228:28 - hypothesis can be rejected since our P
228:30 - value is zero and it's smaller than the
228:33 - statistical significance level
228:35 - 5% and this means that we can reject the
228:38 - H and we can say that uh there is enough
228:42 - evidence to say that P control is not
228:44 - equ experiment and given that that we
228:49 - saw from the visualizations from our
228:52 - calculations that the um clickr rate for
228:55 - our experimental group is much
228:59 - higher than the click rate of the uh
229:02 - control group we can also say that we
229:04 - have found evidence that at 5%
229:08 - significance level we have found out
229:11 - that there is a statistically
229:13 - significant difference between the
229:15 - experimental and control groups clickr
229:17 - rate and that the experimental groups
229:19 - clickr rate is actually higher so
229:22 - statistically significantly higher than
229:24 - the control versions clickr rate so this
229:28 - is really important because this suggest
229:31 - that this difference in their clickr
229:33 - rate is not due to random chance loan
229:35 - but truly that there is evidence
229:37 - statistical evidence that can support
229:40 - this hypothesis that there is a true
229:42 - difference between the performance of
229:45 - the experimental version of the product
229:47 - so in this case in our case the landing
229:50 - page that has enroll Now button versus
229:52 - the control version of the product which
229:55 - had the uh uh startree trial version of
229:59 - the landing page the existing version so
230:01 - beside of calculating this P value it's
230:04 - always a great practice to also
230:05 - visualize your results and this is great
230:08 - for your audience who are technically
230:10 - sound and who know uh these different
230:13 - concepts and you want to visualize uh
230:16 - the results that you got not only by
230:18 - showing some number that is the P value
230:20 - and say hey I have a statistical
230:22 - significance but you also want to
230:24 - showcase the actual picture of what you
230:27 - got what is your test statistic what is
230:29 - the significance level that you use to
230:31 - kind of tell a story around your numbers
230:34 - and that's the uh art behind the data
230:37 - science I would say so let's go ahead
230:39 - and do some art so what I'm doing here
230:42 - is that I am making use of my standard
230:45 - normal distribution or the gausian
230:47 - distribution the way that we are
230:48 - referring to the standard normal
230:49 - distribution in statistics I'm saying
230:52 - that my mean or the MU is equal to zero
230:54 - my Sigma is equal to one which is my
230:56 - standard deviation and I'm saying that
230:58 - my uh I want to now plot my uh standard
231:02 - normal distribution by getting my uh X
231:05 - values which are the uh number of uh X
231:09 - elements that I want to have my xais and
231:12 - then taking the PDF or the probability
231:15 - distribution function for the normal
231:17 - distribution by using the S Library I'm
231:20 - then providing my X values for which I
231:24 - want to get
231:26 - my uh
231:27 - corresponding uh values of Y so
231:30 - basically here are all the values
231:32 - between let's say minus something minus
231:35 - three and then so between minus 3 and
231:38 - three and I want to find all the Y's
231:41 - corresponding to this which basically
231:42 - plus the probability distribution
231:44 - function of the cian distribution or the
231:46 - standard normal distribution and then I
231:48 - want to add to this graph also the uh
231:52 - corresponding rejection region and as
231:55 - you can see it is here so then what I'm
231:58 - adding here by using this part of the
232:01 - plot is that I want to fill in the
232:03 - rejection regions so I'm saying for all
232:07 - the values in this figure whenever the
232:11 - uh value is lower than that TR thrh hold
232:15 - in this case the threshold is z critical
232:18 - 1.96 so whenever my threshold is smaller
232:22 - than minus this uh
232:24 - 1.96 and larger than this
232:28 - 1.96 then we are in the rejection region
232:31 - we are saying then if my test statistics
232:35 - is falling in the rejection region in
232:36 - this case you can see that we are in the
232:38 - far left so the test statistic is minus
232:42 - 5944
232:44 - and it's much lower than this threshold
232:48 - as you can see in here this is this left
232:50 - Blue Line in here then in this case it
232:53 - falls in this rejection region so
232:55 - actually this entire thing is the
232:57 - rejection region it starts from here and
233:00 - it goes all the way to here anything
233:03 - anything in this region means that we
233:05 - need to we have a test statistic falling
233:09 - in the rejection region which means that
233:11 - we can reject to no hypothesis if we
233:14 - were to get a test statistic that is
233:16 - very large and very positive it means we
233:19 - would be in this part of the figure and
233:21 - again in the rejection region anything
233:24 - above this line is then uh going under
233:26 - this category of rejection region and
233:29 - also anything in here so for anything in
233:33 - here we are in the rejection region
233:36 - being in the rejection region it means
233:38 - that we can reject the N hypothesis and
233:41 - we can say that we have a statistically
233:43 - significant result
233:45 - results so now when we have our
233:47 - statistical significance it's always a
233:49 - great idea to go on to the next step and
233:52 - it's actually mandatory to do this
233:55 - because not only a statistical
233:57 - significance is important but also the
233:59 - Practical significance as I mentioned in
234:01 - the beginning of this case study so for
234:03 - that what we are going to do is first we
234:05 - are going to calculate the confidence
234:07 - interval of the test and this confidence
234:09 - interval will help us to first of all
234:12 - make um comments regarding the quality
234:15 - of our test and its
234:18 - generalizability uh at our entire
234:20 - population and the accuracy of our
234:23 - results and then we will use this
234:25 - confidence interval to make a comments
234:28 - and to test for the Practical
234:30 - significance in our AB test so let's go
234:34 - ahead and calculate the confidence
234:36 - interval so as we learned as part of our
234:39 - lectures the confidence interval can be
234:41 - calculated by first taking the uh p
234:44 - experimental head and P control head and
234:46 - the standard error and the Z critical so
234:49 - here we need the two different estimates
234:51 - of the experimental groups click through
234:53 - rate and the control groups click
234:54 - through rate we also need the standard
234:57 - error of our two sample Z test as well
234:59 - as the critical value and then we need
235:02 - to First calculate the lower bound of
235:04 - our confidence interval and then we need
235:06 - to calculate the upper bound of our
235:08 - confidence interval and in this case uh
235:11 - given that the um statistical signific
235:14 - ific level we are using is
235:16 - Alpha uh the uh Z critical is based on
235:20 - that therefore we are also saying that
235:22 - we are calculating the 95% confidence
235:25 - interval so in here the way we will
235:29 - calculate the lower bound is by taking
235:31 - the P experimental head subtracting from
235:33 - that the P control head and then once we
235:35 - have done that we then subtract from
235:37 - that the standard error multiply by Z
235:40 - critical volume and we are just rounding
235:43 - this up up to the three decimal behind
235:45 - the zero then we are doing the same
235:48 - thing only with a plus sign in here for
235:50 - the upper bound calculation of the
235:52 - confidence interval so this is just pure
235:55 - following the formula of the confidence
235:57 - interval that I will set you
235:59 - here and let's go ahead and print this
236:03 - value which is this interval so what we
236:06 - are seeing here is that we have a
236:08 - confidence interval that is from
236:11 - 0 399 so 0.4
236:15 - to 0. uh
236:17 - 43 so quite narrow confidence interval I
236:20 - would say which is actually a good sign
236:23 - because this confidence inter that
236:25 - provides this range of values within
236:27 - which the true difference between this
236:29 - control and experimental groups
236:31 - proportions or the clickr rate is likely
236:33 - to lie within a certain level of
236:36 - confidence in this case 95% confidence
236:39 - this is very narrow and if it's a neuro
236:42 - confidence interval it means that the uh
236:46 - accuracy of our results is higher and it
236:49 - means that the results we are getting
236:50 - based on our smaller sample it will most
236:53 - likely generalize well when we apply
236:56 - these changes and deploy these changes
236:58 - and we put this new product in front of
237:00 - the entire population of users because
237:03 - now we are doing all this experiment for
237:05 - a small group for the sample and this
237:08 - confidence info that is narrow it's not
237:11 - wide it's narrow it means that the
237:13 - results that we are getting is are
237:15 - accurate more or less accurate and this
237:18 - means that the results that we are
237:20 - getting based on a sample are most
237:23 - likely a true representation of the
237:25 - entire population that we got this is
237:28 - the idea behind the width of the
237:30 - confidence interval the narrower it is
237:33 - the higher uh is the quality of your
237:35 - results which means that the uh more
237:38 - generalizable are your results so let's
237:41 - now go on to the final stage of our case
237:44 - study which is to test the Practical
237:47 - significance of our results so now when
237:49 - we know that the statistical
237:51 - significance is there the experimental
237:54 - version of our feature is statistically
237:57 - significantly different from the control
237:58 - version in terms of the clickr rate and
238:01 - we have seen that the competence
238:03 - interval is narrow which means that our
238:05 - results are accurate quite uh with quite
238:08 - high accuracy then we can now comment on
238:12 - the Practical significance of our
238:14 - results this means we want to see
238:16 - whether the significant difference that
238:19 - we obtained whether this difference is
238:21 - actually large enough from the business
238:23 - perspective to say that it's worth to
238:26 - put our engineering resources and our
238:28 - money and our uh uh product into uh to
238:34 - put through through this change and to
238:37 - uh say that it's wor from the business
238:39 - perspective to change this button and to
238:43 - put this into
238:44 - um the production and in front of our
238:46 - users and of course here we are not only
238:49 - talking about the engineering resources
238:51 - that it will take from us to change this
238:54 - and the deployment and the monitoring
238:56 - but also in terms of the quality of the
238:59 - product we are providing to our users
239:01 - because whenever we are making a change
239:03 - to our product it is a risk because we
239:05 - are changing what our user is used to
239:08 - see and this can always be scary uh when
239:12 - it comes uh to the business because we
239:15 - don't want to uh make our customers
239:17 - scared so therefore we need to also
239:20 - check for this practical significance so
239:23 - for that what I'm doing is that I'm
239:24 - creating this python function that will
239:26 - take two arguments so two values that is
239:29 - the minimum detectable effect and then
239:32 - the 95% confidence interval that I just
239:34 - calculated those will be the two
239:36 - arguments for my function and I'm
239:38 - calling this function is practically
239:40 - significant and this function will go
239:44 - and check whether the uh practical
239:47 - significance is there or not and it will
239:49 - then return true or false and then it
239:52 - will also print whether we have a
239:53 - practical significance or not and we
239:56 - learned from the theory and we know from
239:58 - this AB testing concept that whenever
240:01 - the uh mde or the Delta that we got the
240:04 - minimum detectable effect is larger than
240:07 - the lower bound of our confidence
240:09 - interval it means that the lowest
240:12 - possible value that we can get based on
240:15 - the results that we obtain in our
240:17 - sample that that amount is smaller than
240:20 - the minimum detectable effect that we
240:24 - assumed before even conducting our AB
240:27 - test this suggests that we have a
240:29 - practical significance and the
240:31 - difference the minimum difference that
240:33 - we will obtain is large enough for us to
240:36 - have a motivation to make this change in
240:39 - our product for that what I'm doing is
240:42 - that first I'm taking my 95% confidence
240:45 - interval and I'm taking the first
240:47 - element because we know that a
240:49 - confidence interval is actually range so
240:50 - two P of two numbers the lower bound and
240:53 - upper bound I need the lower bound
240:55 - because all I care for this practical
240:57 - significance is to compare the lower
240:59 - bound of the 95% confidence interval to
241:02 - this minimum detectable effect which is
241:05 - my Delta so therefore I'm taking this
241:08 - lower bound of confidence interval
241:10 - putting that into a
241:12 - variable and then I'm using this
241:14 - variable this lower uncore bound uncore
241:17 - CI confidence interval and I'm comparing
241:20 - this to my data I'm saying if my lower
241:23 - Bond of the confidence
241:26 - interval actually I'm noticing that here
241:28 - I got a mistake it should be the other
241:30 - way around we need to say that if our
241:33 - Delta is larger or
241:37 - equal the uh lower bound of the
241:40 - confidence interval which is the same as
241:42 - if our
241:44 - lower bound of the confidence interval
241:48 - is smaller than equal our Delta so if
241:51 - our we can also write this the other way
241:53 - around so if our
241:55 - Delta
241:57 - is larger than equal than our lower
242:01 - underscore bound underscore CI then we
242:05 - can say that we have a practical
242:07 - significance so with the MDA of in this
242:11 - case so I want to use my initial Delta
242:14 - that therefore I won't be initializing
242:16 - this so you might recall here a Delta of
242:19 - 10% I want to still make use of that
242:22 - Delta so therefore I will just go ahead
242:25 - and then in here what I want to do is to
242:28 - call this function by using that
242:31 - specific Delta so I want to have a 10%
242:34 - as my MD and whenever this Delta will be
242:37 - larger than the lower B of my confidence
242:39 - interval that I just obtained I will
242:42 - then say that we have a a practical
242:46 - significance and with an MDA of 10% the
242:50 - difference between control an
242:52 - experimental group is also practically
242:54 - significant so you can see that the
242:57 - lower bound is 0.04 something that we
242:59 - obtain here and that amount is then
243:02 - compared to this Delta and here you can
243:06 - see that we have concluded that we also
243:11 - have a practical significance so so
243:14 - amazing we have come to the end of this
243:16 - case study and in this involved case
243:18 - study we have conducted an entire um AB
243:21 - test result and Asis so this case study
243:23 - Navy test and to end going from the
243:26 - point of loading the data and then
243:29 - understanding this business concept or
243:31 - business objective of ab test where we
243:33 - were testing whether the um enroll Now
243:36 - button which is the new version the
243:38 - experimental version should replace the
243:40 - existing button which is the secure
243:43 - vraal
243:44 - and based on this case study what we
243:46 - found out is that we have a statistical
243:49 - significance at 5% significance level
243:51 - suggesting that we can reject the N
243:53 - hypothesis and we can say that indeed
243:55 - there exists a statistical significant
243:58 - difference between the click through
243:59 - rate in the experimental group versus
244:02 - control group uh and specifically that
244:05 - the enroll n experimental button results
244:07 - a statistically significantly higher
244:10 - click through rate than the uh secure
244:12 - free trial button
244:14 - and beside this we also checked the um
244:17 - accuracy of our results by looking at a
244:19 - confidence interval and we saw that the
244:21 - confidence interval was quite narrow
244:23 - suggesting that the results we obtained
244:25 - were quite uh accurate and this means
244:28 - that the results that we got for the
244:30 - sample will generalize to our population
244:32 - of users and finally we have also
244:35 - checked the Practical significance of
244:37 - our results by using the 95% confidence
244:40 - interval and comparing the lower bound
244:42 - of that interval with our minimum
244:45 - detectable effect Delta and we saw that
244:48 - we will have at least
244:49 - 10% uh significant difference between
244:52 - the control groups CTR and the control
244:55 - uh the experimental group CTR and the
244:57 - experimental group CTR will be at least
244:59 - 10% higher than the uh control groups
245:04 - and this suggests that uh from the
245:06 - business perspective we also have a
245:09 - motivation uh beside of this statistical
245:11 - significance we also have practical
245:13 - significance suggesting that we also
245:15 - have enough motivation and reason from
245:18 - the business perspective to put this new
245:20 - button into production so we can
245:23 - conclude that uh based on this
245:25 - datadriven approach and conducting an AB
245:28 - testing we uh can see a clear motivation
245:32 - of deploying this new button and draw
245:35 - now and replace the existing one secure
245:38 - free trial version and we will then
245:41 - expect to see more users clicking on
245:44 - this and engaging with our product and
245:46 - for now this will be all for this case
245:48 - study if you want to learn more about AB
245:51 - testing make sure to check our AB
245:54 - testing course as well as the ultimate
245:56 - data science boot camp don't forget to
245:58 - try our free trial this time using our
246:00 - enroll Now button and if you want to see
246:04 - more case studies like this make sure to
246:06 - check our a case studies we have many
246:09 - case studies also included as part of
246:11 - our ultimate dat science boot camp where
246:13 - we go in detail of these different steps
246:16 - and we conduct different sorts of case
246:17 - studies to put our data science theory
246:20 - into practice including from the field
246:22 - of NLP machine learning recommended
246:24 - systems Advanced analytics and also AB
246:27 - testing and soon also from AI so for now
246:32 - thank you for staying with me and
246:33 - conducting this case study happy
246:35 - learning this video was sponsored by
246:38 - lunarch at lunarch we are all about
246:41 - making you ready for your dream job in
246:43 - Tech making data science and AI
246:46 - accessible to everyone with these data
246:49 - science artificial intelligence or
246:51 - engineering at lunar Tech Academy we
246:54 - have courses and boot camps to help you
246:56 - become a job ready professional we are
246:59 - here to help also businesses and schools
247:01 - and universities with a top notot
247:03 - training modernization with data science
247:06 - and AI corporate training including the
247:09 - latest topics like generative AI with
247:12 - lunar Tech learning is easy fun and
247:15 - super practical we care about providing
247:18 - an endtoend learning experience that is
247:21 - both practical and grounded in
247:23 - fundamental knowledge our community is
247:26 - all about supporting each other making
247:28 - sure you get where you want to grow
247:30 - ready to start your Tech Journey lunner
247:33 - Tech is where you begin for students or
247:36 - aspiring data science and AI
247:38 - professionals visit Lun Tech Academy
247:40 - section to explore our courses and boot
247:43 - camps and just in general our programs
247:46 - businesses in Need for employee training
247:48 - upscaling or data science and AI
247:51 - Solutions should head to the technology
247:53 - section on the lunch. page Enterprises
247:57 - looking for corporate training
247:59 - curriculum modernization and customize
248:02 - AI tools to enhance education please
248:04 - visit the lunar Tech Enterprises section
248:07 - at lunch. for a free consultation and
248:10 - customize estimate join lunch and start
248:14 - building your future one data point at a
248:21 - time hi I'm vah and in this project we
248:25 - will learn how to understand your
248:27 - customers better track sales patterns
248:29 - and show those results if you like
248:32 - working with data or own a store this
248:34 - video will show you how to use
248:36 - information to make better choices and
248:39 - get better results you will speit your
248:41 - customers into smaller groups based on
248:43 - how they shop this helps you send the
248:46 - right messages to the right people and
248:48 - give them offers they will like loyal
248:51 - customers are the best you will use data
248:53 - to find your biggest supporters and
248:55 - those who are ready to spend more then
248:57 - you can reward your your best customers
249:00 - with programs that fit their shopping
249:02 - habits this makes them happy and stops
249:05 - them from going to other stores you will
249:08 - use data to guess what people will buy
249:10 - and when they will buy it you will find
249:12 - sales patter hiden among different items
249:15 - and figure out what cool new products
249:18 - people will want this lets you always
249:21 - have the right stuff at the right time
249:23 - you won't have too many items everything
249:25 - will sell and customers will be
249:27 - surprised by how well you know what they
249:29 - need we'll look at how sales change
249:33 - throughout the year will this helps you
249:35 - plan for busy times cash slowdowns early
249:38 - and know exactly when to have big
249:41 - sales we will use the location data and
249:44 - what people say about you to find places
249:47 - where sales are going well and where you
249:49 - could grow you'll even show it all on
249:51 - the map this helps you spend your
249:53 - advertising money wisely find great
249:56 - spots for new stores and even choose the
249:58 - perfect things to sell in each place so
250:01 - let's get
250:03 - started all right let's now go over the
250:05 - data I will be using so we are using the
250:08 - superstore cells DS set and it has 9,800
250:15 - rows and the columns order ID order date
250:19 - ship date shipping mode standard class
250:22 - second class or other classes and the
250:25 - customer ID with them customer name the
250:28 - segment
250:29 - meaning um who bought the product was it
250:33 - a consumer a corporate or a home office
250:36 - and the clients mainly come from United
250:40 - States and it's also specified from
250:43 - which city of the United States they
250:45 - come from so we shall import this to our
250:48 - Google clab and start working on it okay
250:51 - so let's now import the
250:53 - necessary python libraries we'll import
250:57 - pandas as PD we also import numai SMP
251:02 - import metf lip High import Seaborn as
251:07 - SMS so let's also import the there and
251:10 - we will be using copy p
251:14 - [Music]
251:26 - perfect so this is how it looked like on
251:28 - kagle and this is also how it looks like
251:30 - when we have imported so let's now look
251:33 - up the frames and larger info so
251:38 - everything seems to be consistent but
251:40 - the postal code it seems that 11 postos
251:45 - are
251:46 - missing okay so what we can do is to
251:50 - fill in those no values
252:06 - [Music]
252:15 - [Music]
252:43 - [Music]
252:53 - okay so as you can see we have replaced
252:55 - the no uh poster codes customers that
252:59 - didn't have any postal code and we have
253:02 - put a zero inside it all right so let's
253:05 - now move on to checking for duplicates
253:18 - if the have that duplicated that sum
253:22 - trigger down zero Dash print so let's
253:26 - now see if they're actually duplicates
253:29 - and if there are duplicates we will
253:31 - print D duplicates exist and if there
253:34 - are not we'll print no duplicate
253:41 - found all right
253:43 - so as you can see there exists no
253:47 - duplicate so let's move on to customer
253:51 - segmentation let's first create a
253:54 - variable named types of customers and
253:58 - let's extract out of our D Frame called
254:04 - [Music]
254:11 - segment as you can see from our data
254:13 - frame we have a married segment within
254:15 - our data this segment includes a list of
254:18 - the types of customers in our data frame
254:21 - we have both consumer and corporate
254:23 - customers so let's get started with
254:25 - customer segmentation the main problem
254:27 - is that many large businesses struggle
254:29 - to understand the contribution and
254:32 - importance of their various customer
254:34 - segments they often lack precise
254:36 - information about their main buyers
254:38 - relying on intuition rather than data
254:41 - this leads to misallocation of resour
254:43 - resources resulting in Revenue loss and
254:45 - decreased customer satisfaction for
254:48 - example if your store primarily caters
254:50 - to Consumers it's crucial to tailor your
254:53 - marketing and customer satisfaction
254:54 - efforts to resonate with their needs and
254:57 - preferences by focusing your resources
254:59 - on understanding and Catering to your
255:01 - consumer base you can avoid
255:03 - misallocating resources to large
255:05 - corporates this ensures you're providing
255:07 - a satisfying customer experience for
255:10 - your primary demographic ultimately
255:12 - leading to increased customer loyalty
255:15 - and revenue growth and we can also we
255:19 - can create a part chart pie chart or
255:22 - bargar from it to clearly illustrate the
255:25 - revenue contribution of each customer
255:27 - segment and this will allow us to tailor
255:30 - more of our marketing resources our
255:34 - customer satisfaction resources towards
255:37 - once you've completed customer
255:38 - segmentation The Next Step depends on
255:41 - your strategic goals here are a few ways
255:43 - to proceed focus on your most valuable
255:46 - segment if your existing customer
255:48 - segmentation reveals a particularly
255:51 - profitable segment such as consumers
255:54 - tailor your marketing product offerings
255:57 - and customer service to deepen your
255:59 - engagement with that group Target new
256:02 - segments if you want to attract more
256:04 - corporates or home offices you'll need
256:07 - to understand their unique needs and
256:09 - pain points start by researching these
256:11 - segments what are are their challenges
256:14 - what solutions would appeal to them
256:16 - develop tailored messaging and consider
256:18 - offering specialized products or
256:20 - services to attract these new customer
256:22 - types all right so let's get started
256:28 - [Music]
256:41 - [Music]
256:44 - so this will extract the types of
256:48 - customers from our data frame perfect so
256:50 - it's consumer corporate and home office
256:53 - those are all the um variables that are
256:55 - in our data
256:57 - frame all right so let's count the
257:00 - unique values in our segment and we will
257:02 - do this by number of customers
257:32 - so what this meaning does is it counts
257:34 - unique values in our segment and resets
257:37 - the index to turn them into a column and
257:39 - then we can correct the renaming
257:43 - of columns so we want to give our
257:47 - segment the name of like total customer
257:50 - or type of customer I will go with the
257:53 - type of customer so we will say number
257:56 - of customers is equal to number of
258:00 - customers that rename and want to call
258:03 - the colon which is the name segment we
258:08 - want to rename it to to type of customer
258:13 - now if you want to print that print
258:17 - number of customers there are 5, 101
258:22 - consumers and for corporate there are
258:24 - like
258:25 - 2,953 corporate buyers
258:29 - and7
258:31 - 1,746 home offices and if you want to
258:34 - create a pie chart out of this we can
258:36 - plot it by saying PL p number of
258:41 - customers and want to B base a pie chart
258:44 - on the count and want to label number of
258:50 - customers toal custom
258:53 - [Music]
259:06 - [Music]
259:10 - R Perfect all right so from as you can
259:14 - see we had the renew um type of customer
259:17 - the total customer so you can see that
259:20 - from this uh pie chart our main consumer
259:24 - segment has 52% 30% of our orders come
259:28 - from corporates and 18% from home
259:36 - offices you can see who we have to
259:38 - exactly focus on which are consumers
259:40 - while consumers hold the majority
259:43 - focusing solely on them Overlook
259:45 - significant potential within the
259:47 - corporate and home office segments let's
259:50 - explore how to balance resource
259:52 - allocation for all three segments to
259:55 - maximize growth to gain even deeper
259:58 - insights we should integrate our
260:00 - customer data with sales figures this
260:03 - analysis will help us identify which
260:06 - segments generate the most Revenue per
260:09 - customer average order value and over
260:12 - all profitability customer lifetime
260:15 - value additionally we can segment
260:18 - customers by purchase frequency and
260:20 - basket size to understand their buying
260:23 - Behavior within each segment here are
260:26 - some additional questions to consider
260:28 - for a more comprehensive analysis
260:31 - customer acquisition cost CAC how much
260:34 - does it cost to acquire a customer in
260:37 - each segment customer satisfaction how
260:40 - satisfied are customers in each segment
260:44 - churn rate what is the rate at which
260:47 - customers leave in each segment by
260:49 - analyzing these factors alongside
260:52 - revenue and customer lifetime value we
260:54 - can create a customer segmentation model
260:58 - that prioritizes segments based on their
261:01 - overall value and growth potential we
261:04 - can also PL a bar graph for the total
261:09 - sales for each uh customer type and
261:12 - group the data by the segment column and
261:14 - calculate the total CS for each segment
261:17 - and you want to do this by so right now
261:19 - you don't see the exact sales numbers
261:22 - the bar chart you can see the exact
261:25 - sales numbers for each customer type so
261:28 - let's PL it
261:33 - [Music]
261:59 - [Music]
262:05 - [Music]
262:12 - [Music]
262:28 - [Music]
262:44 - [Music]
262:51 - [Music]
263:08 - [Music]
263:10 - so there are around one
263:13 - .2 million from our
263:16 - consumers and we have around 600 or
263:19 - 700,000 from our corporates now we can
263:22 - also plot uh barware from
263:26 - this which means plot out bar sales pair
263:31 - segment customer type type of customer
263:34 - sales pair segment called a sale this
263:38 - bar chart effectively illustrates the
263:40 - distribution of sales across our
263:42 - customer segments consumers account for
263:45 - the largest portion of sales 1.2 million
263:49 - followed by corporates 1.0 million and
263:52 - Home Offices 0.8 million while the chart
263:56 - is clear a deeper analysis can help us
263:59 - optimize our marketing efforts customer
264:03 - lifetime value
264:05 - CLTV calculate the CLTV of each segment
264:09 - to identify which segments generate the
264:12 - the most Revenue over time this will
264:14 - help prioritize customer segments for
264:17 - marketing efforts for example if you
264:20 - find that the home office segment has a
264:23 - higher CLTV than the Consumer segment
264:27 - you may want to invest more resources in
264:29 - marketing campaigns targeting home
264:31 - office customers market research conduct
264:35 - market research to understand the
264:37 - specific needs and preferences of each
264:41 - customer segment this will inform the
264:44 - development of targeted marketing
264:47 - campaigns for instance you might
264:49 - discover that consumers in your data are
264:52 - price sensitive while corporate
264:54 - customers are more interested in bulk
264:57 - discounts and reliable service you can
265:00 - use this knowledge to tailor your
265:02 - marketing messages to each segment
265:05 - average order value analyze average
265:08 - order value by segment to identify
265:12 - opportunities to increase Revenue per
265:14 - customer let's say your analysis reveals
265:18 - that corporate customers have a higher
265:21 - average order value than consumers you
265:24 - could develop marketing campaigns that
265:27 - encourage consumers to purchase bundles
265:30 - or higher pric products to increase
265:32 - their average order value customer
265:35 - acquisition cost CAC how much does it
265:38 - cost to acquire a customer in each
265:41 - segment knowing CAC can help determine
265:45 - the return on investment Roi for
265:47 - marketing efforts here's an example
265:50 - let's say it cost $100 to acquire a new
265:53 - corporate customer but only $20 to
265:56 - acquire a new consumer customer if the
265:59 - CLTV customer lifetime value of a
266:02 - corporate customer is significantly
266:05 - higher than the CLTV of a consumer
266:08 - customer then spending $100 to acquire a
266:11 - corporate customer may still be
266:13 - profitable however if the CLTV of the
266:17 - corporate customer is only slightly
266:19 - higher than the CLTV of the consumer
266:23 - customer you may want to focus your
266:25 - marketing efforts on acquiring more
266:28 - consumers because the cost of
266:30 - acquisition is much lower customer
266:34 - satisfaction how satisfied our customers
266:37 - in each segment understanding
266:40 - satisfaction levels can help identify
266:43 - areas for improvement and reduce churn
266:47 - here's an example you can conduct
266:49 - surveys or collect customer feedback to
266:53 - understand satisfaction levels if you
266:55 - find that corporate customers are less
266:57 - satisfied than consumer customers you
267:00 - may want to investigate the reasons for
267:03 - their dissatisfaction and make changes
267:06 - to improve their experience this could
267:09 - involve improving your customer service
267:11 - of offering more competitive pricing for
267:14 - corporate customers or developing
267:16 - products or services that better meet
267:19 - the needs of corporate
267:22 - customers we can also create a pie chart
267:25 - for our sales which we can do by pled Pi
267:29 - Pi sales perir segment total sales and
267:35 - we name it labels is equal
267:38 - to sales perir segment
267:43 - type of customer type of customer 51% of
267:46 - our sales come from our consumers 30%
267:49 - from our corporates and 19% from home
267:53 - offices all right so let's now move on
267:56 - to the customer loyalty as a business
267:59 - you want to make sure that your most
268:01 - loyal customers stay happy this will
268:03 - make sure that those customers keep on
268:05 - coming back keep on bringing new people
268:08 - and also placing new
268:09 - orders so you will decrease the cost on
268:13 - acquisition of new customers because
268:16 - there will be already existing customers
268:19 - and you'll also be able to make sure
268:23 - that your Revenue either stays at the
268:27 - same level or increases by keeping your
268:29 - most loyal customers happy and you want
268:33 - to do that as a business now we can do
268:35 - this by either the following ways we can
268:40 - Brank the most loyal customers by the
268:42 - amount of orders they have placed or the
268:45 - total uh they have spent you have
268:48 - analyzed your data pinpointing your 30
268:50 - most lawyer customers this represents a
268:54 - significant opportunity to strengthen
268:56 - these relationships and maximize their
268:59 - lifetime value here's a powerflow
269:01 - approach design a targeted email
269:05 - specifically for those high value
269:07 - segments proactively offer personalized
269:10 - support with inquiries such as how can
269:12 - we assist you today this demonstrates
269:15 - your commitment to their success for
269:17 - actively addressing potential issues and
269:20 - fostering a deep sense of
269:22 - loyalty loyalty programs consider a
269:25 - tiered loyalty program that offers
269:28 - exclusive rewards tailor to your most
269:31 - valuable customers this include earlier
269:34 - access to new products personized
269:36 - discounts or even Point based reward
269:38 - systems personalized experiences
269:40 - leverage your data in insights to go
269:42 - beyond email consider personalized
269:45 - website recommendations targeted
269:47 - promotions based on past purchase
269:50 - history or even handwritten thank you
269:53 - notes for high value customers customer
269:56 - feedback loops make sure your top
269:58 - customers feel hurt Implement surveys or
270:01 - invite them to participate in exclusive
270:03 - focus groups this demonstrates you value
270:06 - their input and are actively using
270:08 - feedback to improve the customer
270:11 - experience Community Building depending
270:14 - on your business model fostering a
270:17 - community among your most loyal
270:19 - customers can create a sense of
270:21 - belonging this could involve access to
270:24 - online forums exclusive events or
270:27 - opportunities to network with
270:29 - like-minded
270:30 - individuals now this strategy extends
270:33 - Beyond customer satisfaction
270:36 - prioritizing the experience of your top
270:38 - customers directly correlates with
270:41 - increase retention positive referrals
270:44 - and ultimately improved Revenue now um
270:48 - let's dive deeper and see who are our
270:51 - most loyer customers all right so let's
270:56 - now get started with that let's create
270:58 - the variable with the name all so let's
271:01 - first display the first three rows of
271:05 - our day frame so as you can see there is
271:08 - a row called sales or the column call
271:10 - called sales and and each customer has a
271:15 - specific ID with a specific name so if
271:18 - you can count of the number of times
271:23 - this shows up then you also have the
271:27 - number of total orders which then you
271:31 - can which you can use later however you
271:33 - want to so let's start with doing that
271:53 - [Music]
271:59 - [Music]
272:16 - [Music]
272:25 - [Music]
272:40 - now let's rename the columns we want the
272:44 - column order ID which is where is the
272:47 - order ID it's right here when order ID
272:51 - to be named uh total
272:57 - [Music]
273:03 - orders now we want to rename the columns
273:06 - that are equal to order ID this column
273:10 - this must be renamed to Total orders
273:14 - place is equal to True okay so now let's
273:17 - identify the repeat customers customers
273:20 - with order frequency greater than one so
273:23 - repeat customers are equal to customers
273:26 - order frequency custom order frequency
273:29 - total orders and like I said want to
273:32 - make sure it's equal it's greater than
273:34 - one it's equal or great than one perfect
273:37 - now we can we want to organize this in a
273:40 - way that is just the sting
273:42 - we can do that by saying repeat
273:43 - customers sort it or repeat customers
273:47 - that sort values
274:01 - [Music]
274:17 - [Music]
274:24 - [Music]
274:34 - perfect now let's print this out print
274:36 - repeat customers sorted do head 12 want
274:42 - to display our top 12 customers reset
274:52 - [Music]
274:54 - index so the customer with name William
274:57 - Brown who is a consumer has placed to
274:59 - Total 35 orders so this is the list of
275:03 - your top how many customers and as a
275:07 - business you or as a Superstore you can
275:09 - identify exactly the number number of
275:12 - the total orders a person or a business
275:16 - has to place in order to be considered a
275:19 - lower customer and then according to
275:22 - that you can tailor your services to it
275:25 - now the data clearly reveals that a
275:27 - small group of customers Place orders
275:29 - with considerably higher frequency 30
275:32 - plus we have William Brown with 35
275:34 - orders and other home office customers
275:37 - with 34 and many consumers and one
275:40 - corporate with 32 so this shows clearly
275:44 - that we have a loyal group of customers
275:46 - there's also significant potential for
275:49 - our home office segment several of our
275:51 - most loyal customers belong to the home
275:54 - office segment now this implies that the
275:56 - home office segment has strong potential
275:58 - for customer loyalty and deserves
276:01 - targeted marketing efforts it also shows
276:04 - that we just don't have like one
276:06 - dominant group of loyal customers we
276:09 - have home offices consumers and corpor
276:11 - while there are many consumers it
276:13 - doesn't mean that we have to focus on
276:17 - one segment it means that we still have
276:19 - to devise a plan that caters to our
276:22 - multiple segments so some
276:25 - recommendations now we can prioritize
276:27 - loyal customers now segment customers by
276:30 - other frequency and uh we can develop
276:32 - exclusive offers Rewards or Early Access
276:35 - programs ta to our most umer customers
276:41 - so for example we can provide them
276:43 - exclusive discounts tier reward programs
276:46 - and earlier access and we can also
276:48 - Target uh more home offices because we
276:50 - see that home offices um keep on coming
276:53 - back and we are able to satisfy few of
276:58 - the home offices that mean that means we
277:01 - can we have catered to their needs and
277:04 - provided a good enough service for them
277:06 - to keep on coming back that means our
277:08 - product is great for home offices that
277:10 - means we can Target more home offices
277:13 - using content marketing social media ads
277:16 - or other type of marketing strategies
277:18 - and we can also analyze our the behavior
277:22 - or the way we provide the service to our
277:25 - um to these
277:27 - customers and because it worked out
277:29 - pretty well and if we provide this kind
277:32 - of service to our newcoming customers
277:35 - then we increase the chance that they
277:36 - also become a l customer so those are
277:39 - like several conclusions we can make
277:42 - now we can also identify loyal customers
277:45 - by sales so this has uh identifi them by
277:48 - total number of orders they place but we
277:51 - can also use amount of sales so the
277:55 - total amount to identify them because a
277:59 - person can come and place 35 orders but
278:01 - if they Place 35 $1 order then obviously
278:06 - that's just 35 bucks now this doesn't
278:09 - say anything about the sales amount so I
278:12 - um ideally you want to organize it by
278:16 - the sales amount to be able to um
278:19 - identify the actual top spending and
278:22 - loyal
278:23 - customers or that said you when there's
278:28 - significant customer so let's say
278:30 - someone has spent like 25,000 that can
278:34 - be done also in one order so that
278:36 - doesn't mean that it's a repeated
278:39 - customer it's it's just a top Spender
278:41 - now um let's start with identifying our
278:45 - top spending customers so let's first
278:48 - create a variable customer sales go to
278:52 - data frame that's grw by customer ID
278:56 - want their customer ID want to also see
278:59 - the name and also what type of customer
279:02 - they are this segment and want to do it
279:05 - by sales and we want to sum that sum
279:09 - those up and we don't want to resend the
279:12 - index and now let's identify our top
279:16 - Spenders by having them ranked
279:20 - descendingly meaning our top spans will
279:22 - be ranked all the way up customer
279:24 - customer sales the sword values by equal
279:32 - [Music]
279:48 - [Music]
280:04 - [Music]
280:07 - so Sean Miller has spent the most who is
280:10 - from home office using total amount of
280:13 - 25,000 USD William Brown has placed the
280:16 - most number of orders which are 35 but
280:20 - William Brown is nowhere to be found
280:22 - here the same as Sean Miller he has
280:24 - spent he's the he's a customer who spent
280:27 - the most in our Superstore but is's also
280:30 - nowhere to be found here meaning that
280:33 - the repeated customers doesn't really
280:35 - Define their spending habits so if you
280:39 - it depending on the way your you run
280:41 - your super store now obviously I would
280:44 - want to I would want our customer to
280:48 - come back but I would dedicate my
280:52 - resources to the customers who spend the
280:56 - most because those are the customers who
280:58 - BR bring the most business to my to me
281:01 - meaning those are the customers I have
281:02 - to keep uh happy so the number the total
281:06 - number of orders is great but it doesn't
281:08 - really speak that much about their
281:10 - spending habits and about their value to
281:13 - your store all right let's now go over
281:15 - to the next chapter which is shipping
281:18 - now as a superst you also want to know
281:21 - what shipping methods customers
281:24 - prefer and which are the most cost
281:26 - effective and
281:27 - reliable and overall knowing this
281:31 - impacts your customer satisfaction and
281:34 - also meaning it also has a great impact
281:37 - on your Revenue so that so for example
281:42 - Amazon has multiple shipping methods but
281:45 - it has the most popular shipping method
281:47 - which keeps the most amount of customers
281:50 - happy and it also makes uh Amazon the
281:53 - most amount of money so as a superstar
281:57 - you want to know which one of your
281:59 - shipping methods is the most
282:01 - reliable so we create the variable the
282:05 - type of
282:07 - customers so our shipping model let's
282:09 - create a variable we use the data of the
282:13 - data frame ship mode we want to count
282:16 - those values and of course we want to
282:18 - reset the index
282:25 - [Music]
282:41 - [Music]
282:57 - [Music]
283:25 - so our standard class is the most
283:26 - popular by it's almost like four times
283:29 - more popular a first class is the leased
283:32 - and the same day my first class is one
283:34 - the least so let's create a pie chart of
283:38 - this plot out pie shipping model all
283:41 - right so this is our class standard
283:44 - class or these are like the shipping
283:45 - methods the most popular one is standard
283:48 - class which is 60% of the orders use St
283:53 - class shipping and rest is like 40% so
283:57 - as a super store or as any store you
284:00 - invest in your shipping so you end up
284:04 - buying some kind of deals with uh
284:07 - delivery um companies like the sh you PS
284:12 - and others and sometimes you end up
284:15 - recommending the wrong option to your
284:17 - customer so let's say second class is
284:20 - fast but it ends up costing the con
284:23 - customer way too much the customer ends
284:25 - up not buying your product and this
284:28 - decreases the as you can see this F your
284:31 - store but for if you know that standard
284:34 - class is the most popular option then
284:36 - you can have like a button saying this
284:38 - is our most popular option which is time
284:40 - class and most of the time people choose
284:42 - the most popular option so this will
284:45 - help you s this will help the superstore
284:48 - save the cost of investment into these
284:51 - others or dedicate the amount of
284:56 - resources that each class brings and it
285:00 - also allows the superstore to
285:02 - recommend its most popular option which
285:05 - is stand a class so the problem that
285:07 - many Superstores have because many
285:10 - stores have um sores in many locations
285:14 - in many um States but they don't know
285:19 - how much how well each is performing on
285:23 - a dashboard for example you could have
285:25 - that but they have no idea how well each
285:28 - of the stores in each state are
285:30 - performing and leaving them with
285:32 - clueless where where there is an
285:34 - underperformance or for example where
285:37 - they can where there is a high potential
285:39 - area
285:41 - in which they can open a new store in so
285:44 - let's move on to this chapter which is
285:47 - geographical analysis so many stores
285:50 - have hard time in identifying high
285:53 - potential areas or also identifying
285:56 - stores that are underperforming so
285:59 - things like Walmart Target they have
286:02 - like many branches and they they will
286:05 - want to know how well each branch is
286:08 - doing and the perfect way to do this is
286:12 - by counting up the number of sales for
286:15 - each City the number of sales for each
286:17 - state and then this will allow you to
286:19 - see which of the states or which of the
286:22 - cities is performing the best and which
286:24 - of them is performing the least and
286:27 - dedicate your resources accordingly so
286:29 - let's say if one city the story is
286:32 - simply just losing money for years or
286:36 - for more then you will want to adjust
286:39 - your strategy according to that so maybe
286:41 - you will want to close the store or
286:44 - adjust it in a way so it starts bringing
286:46 - in more profit or Revenue well so let's
286:51 - get started with that
287:23 - [Music]
287:59 - [Music]
288:05 - [Music]
288:07 - all right so as you can see the most
288:10 - popular state is California and the
288:12 - least popular is New Jersey so maybe you
288:15 - can go over this and let's say in few of
288:19 - the States where there's still high
288:22 - potential
288:24 - for a profitable sore you can identify
288:28 - that's say Washington and calculate all
288:31 - right so maybe from from this you can
288:35 - see that
288:36 - Washington is performing forth or New
288:40 - Jersey is performing like the least of
288:43 - our top 20 from this You can conclude
288:46 - that you might have to work on New
288:48 - Jersey more to increase the order count
288:51 - this also allows you to increase the
288:53 - revenue or you can see that the
288:55 - California is your most popular option
288:58 - so you might want to keep California
289:00 - happy and you can also do it per City so
289:03 - City DF City value counts reset next
289:10 - print
289:11 - City that had the most top 50 top 15 so
289:15 - the most popular city is New York with
289:17 - the order count of
289:20 - 891 and uh then Los Angeles and Jackson
289:25 - is the least popular out of our top 15
289:28 - and you can also increase this to the 25
289:31 - so not only can you can focus on the
289:33 - states but for each state you can also
289:36 - focus on the the city that's
289:38 - underperforming or overperform forming
289:41 - so this allows you to also dedicate your
289:45 - resources to the to the city that you
289:48 - want maybe to increase your revenue or
289:51 - increase your potential or maybe there
289:53 - is like a city for example L Beach where
289:56 - there's high potential but you're not
289:59 - using any of your
290:02 - resources now we can
290:05 - also uh organize it sales per state
290:08 - let's say state sales so previously we
290:10 - did by order account and we can also do
290:12 - it
290:14 - for state sales we want to sum it up and
290:18 - then reset the index you want to rank it
290:20 - a call
290:23 - fall perfect so as you can see our still
290:29 - our most popular state is California and
290:32 - then New York and then it changes it and
290:34 - then doesn't change yet taxes so this is
290:36 - according to the sales amount the
290:38 - popularity of the state according into
290:40 - the sales amount and let's also sort it
290:45 - for her City
290:52 - [Music]
290:58 - [Music]
291:05 - [Music]
291:21 - [Music]
291:37 - [Music]
291:44 - [Music]
292:02 - so most popular city is New York La
292:05 - Seattle San Francisco this is exactly
292:07 - the same as our previous anoun houses
292:11 - around the city nothing really has
292:13 - changed all right so as a store you want
292:16 - to be able to track down your most
292:19 - popular category of products or your
292:22 - best selling products or sales
292:25 - performance across categories and
292:27 - subcategories and find the sweet spots
292:29 - where strong categories also have top
292:31 - selling subcategories and also spot
292:35 - weaker subcategories within otherwise
292:37 - strong categories that might need
292:39 - Improvement or product popularity
292:41 - fluctuations see save popularity
292:43 - seasonal trading up and down and helps
292:47 - and this helps to forecast the future
292:48 - demand or um you can group it by
292:53 - location for each location there might
292:55 - be a different popular product you you
292:58 - want to put it in a certain place to
293:01 - maximize your store's Revenue so let's
293:04 - get started with finding our top
293:07 - performing products or their categ ories
293:11 - so let's first extract our products the
293:14 - categories of our products from our D
293:17 - Frame the unique print products so right
293:21 - now in our data frame we have only three
293:23 - sorts of products as you can see the
293:26 - category and each one has a subcategory
293:29 - will cases chairs but we have mainly
293:32 - three uh categories which are fiture
293:34 - office supplies and technology so let's
293:37 - go now over to the types of subcategory
293:40 - a product subcategory the uh a print
293:46 - product
293:48 - subcategory Cas is CH they fa and bunch
293:52 - of it now let's group the data by a
293:56 - product category and how many
293:57 - subcategory it has so we want to say for
294:01 - example office supplies may have like 20
294:04 - subcategories or Furniture may have uh
294:07 - five subcategories so let's see how many
294:11 - subcategories each one has
294:14 - [Music]
294:31 - [Music]
294:37 - [Music]
295:00 - [Music]
295:06 - [Music]
295:22 - [Music]
295:32 - [Music]
295:55 - so so there are nine office nine for his
295:59 - office supplies four for furniture four
296:01 - for technology so office supplies is
296:03 - much more sophistic category now we can
296:06 - also see our top performing
296:10 - subcategory so let's say
296:13 - subcategory then you want to count the
296:15 - sales go bu category
296:20 - [Music]
296:39 - [Music]
296:45 - [Music]
296:52 - [Music]
297:08 - [Music]
297:24 - [Music]
297:31 - [Music]
297:46 - so our most popular subcategory is tag
297:50 - specifically phones it has the most
297:52 - amount of sales Furniture chairs office
297:56 - supply storage so from this you can see
297:59 - our most popular subcategories and what
298:02 - subcategories you want to recommend them
298:05 - on a front page
298:07 - or um in the store now let's see which
298:10 - one of our main categories performs or
298:14 - has the most amount of sales product
298:16 - category goby so as expected uh Tech is
298:21 - the most popular one and then furniture
298:23 - and then office supplies so maybe you
298:25 - will have inside the inside your store
298:28 - you will have a much larger department
298:32 - or not much larger maybe a little bit
298:34 - larger or maybe it's in the first row
298:38 - right in front of the customers to be
298:40 - able
298:41 - to or present your most popular option
298:44 - immediately to your customers now this
298:46 - will allow you to of course increase
298:49 - your revenue and sales if you want to
298:53 - create a pie chart for this you can say
298:55 - prodct pie top product category I can
298:59 - organize by sales labels top
299:04 - product category category well it seems
299:08 - that pack is per a little bit bad than
299:12 - most or like these two but it's not that
299:15 - much different that much it's not really
299:17 - that different all right so let's now
299:19 - see which one of our subcategories is
299:23 - the most popular one now remember that
299:25 - we saw which one of our subcategories
299:28 - had the most amount of cells now let's
299:30 - create a barograph out of it we can do
299:34 - this buy sending false and S by SES
299:38 - sending is true let's create the bar
299:41 - graph out bar sub category count sales
299:48 - is category top product subcategory the
299:53 - sales on this SP it and some find now
299:58 - this shows perfectly that
300:02 - um our most popular option is phone and
300:05 - chairs and so since this are generate
300:09 - the most amount of that means that
300:10 - customers are more willing to pay money
300:12 - for this so you can end up spending more
300:15 - of your marketing resources on phones
300:17 - and chairs because it will it's there is
300:20 - already shown that because of the model
300:25 - resources you have provided for phones
300:27 - and chairs it already Works meaning if
300:29 - you increase amount of resources you
300:32 - spend for phones and chairs then your
300:34 - sales will also increase accordingly but
300:36 - you can also uh conclude that art en
300:40 - envelopes and labels aren't that popular
300:43 - so maybe right now you can give a
300:46 - discount and get rid of those and buy
300:49 - less of those for the future so you can
300:52 - buy end up buying more of the popular
300:54 - options for example phones chairs or you
300:56 - can also investigate why they are not
300:59 - popular maybe those are like the most
301:02 - the worst envelopes you could have
301:03 - bought or maybe it's not the right it's
301:07 - not the right art you have bought maybe
301:10 - those kind of art people don't like but
301:11 - if you were to choose complete other
301:14 - form of Arts maybe they will customers
301:16 - will end up buying them so this shows
301:19 - exactly how now this data stores can use
301:23 - to optimize their sales or optimize how
301:27 - their resources are allocated so you
301:30 - would end up making more um more money
301:34 - or more sales now businesses love making
301:36 - sales they love seeing Revenue increase
301:40 - and profits increase that's all lovely
301:44 - but there you should be able to track
301:46 - down yourselves so that you can see in
301:50 - what kind of situation you are and you
301:51 - can adjust to that situation and what's
301:54 - what is a better way than having a pie
301:58 - chart or a bar graph or just a normal
302:01 - graph to see how much growth or how much
302:04 - decline you're experiencing for example
302:06 - if you are a business and there's a the
302:09 - clining Revenue then o year over year or
302:13 - month of month over month then you can
302:15 - see that there is a problem and then you
302:18 - can also allocate your resources towards
302:22 - fixing that problem whether that is
302:24 - investing more in customer preferences
302:27 - or investing more money into marketing
302:31 - or into resources that make your
302:34 - customers more satisfied or adapting new
302:38 - technologies those are all um things
302:41 - that you can do whenever you see a
302:42 - declining Revenue But first you must be
302:44 - able to see it coming and also
302:46 - businesses um have a problem with st
302:49 - growth so they may grow one month and
302:52 - then the next month there's like no
302:54 - growth or maybe there's a decline so you
302:56 - want to see that as a business to be
302:58 - able to St stabilize the growth so use
303:01 - uh continuously grow as a business or
303:05 - missed seasonal opportunities if a
303:07 - business isn't aware of how those
303:10 - changes throughout the the year they
303:12 - could miss out on maximizing profits
303:14 - during Pig Seasons maybe on some Seasons
303:17 - there's a certain product that's uh what
303:21 - that's high in demand but you are you
303:24 - don't have enough stuff enough stock to
303:27 - cover it so you end up not being able to
303:30 - meet the demand and losing out on um
303:33 - revenue and profits and there's so those
303:36 - were regarding our yearly sales friends
303:39 - and there's also a problem with qual
303:41 - today monthly sales so for example cash
303:44 - flow issues now many businesses
303:46 - experience cash flow issues so that may
303:49 - maybe one day they look at their bank
303:51 - and see that they are out of money and
303:54 - they cannot invest more in their
303:56 - business or there's also inventory
303:59 - imbalance or ineffective marketing so
304:02 - for example whenever you have a cash
304:04 - flow issue drastic dips in sales during
304:07 - specific quarters or months can lead to
304:09 - cash crunches making it hard to pay
304:11 - suppliers employees or ongoing expenses
304:15 - or when whenever you have in inventory
304:18 - imbalance some uh periods you're
304:21 - overstocked and and those items you have
304:24 - to give away and some periods you under
304:26 - stock and you are not able to meet the
304:28 - demand or maybe your marketing is
304:30 - ineffective so if you spend significant
304:34 - amount of time in marketing and you
304:37 - don't reach your desired outcome that
304:40 - means there's a major issue with your
304:42 - marketing campaign and then you can see
304:45 - that from the sales you're making so for
304:47 - example if you're spending this amount
304:49 - of money with marketing or increased
304:52 - amount marketing and there's no
304:55 - significant increase of sales that means
304:57 - you're doing something wrong with your
304:58 - marketing campaign or lagging response
305:00 - to emerging Trends so monthly sales data
305:03 - can highlight new trends or drops in
305:05 - demand more quickly than just here
305:07 - overviews so you can react much faster
305:11 - to uh emerging trends for example a
305:14 - certain product was released uh
305:17 - 20124 and all of a sudden it is high in
305:20 - demand in uh many countries then you
305:24 - want to be able to adjust to that demand
305:27 - and get get the supplies for the product
305:30 - but you are not able to do that if you
305:32 - track yellowy sales or don't do any
305:35 - tracking at all so those are all the all
305:38 - the problems that exist if you are not
305:41 - able to track down your sales be it
305:43 - monthly quarterly or yearly and so we
305:46 - are intending to solve that problem by
305:48 - having my graphing it and concluding
305:51 - from the results we get from our graphs
305:53 - all right so let's get started now let's
305:56 - convert the order dates column to data
305:59 - frame format and the order dates is
306:03 - equal to pd. to daytime DF or the dates
306:09 - first is equal to true now let's group
306:12 - the data by years and calculate the
306:14 - total sales amount for each year we can
306:16 - do it by yearly sales scre the variable
306:18 - first and then Group by thef or the date
306:23 - the year sales that's some early sales
306:28 - is equal to early sales let reset the
306:30 - index now I want to give the appropriate
306:32 - calls the appropriate name cuz right now
306:34 - in the data frame it's not the order
306:36 - date should be named year and the uh
306:40 - sales should be named total sales now
306:44 - let's print this out so this is the
306:46 - amount of sales for each year in total
306:50 - and we can also let's plug the bar graph
306:52 - out of this plug a bar into the sales
306:56 - year in the sales total
307:05 - sales all right so from this uh barg
307:09 - graph there can be few uh conclusions
307:12 - made for example there's a steady growth
307:14 - from 2016 to 2018 that might explain for
307:17 - example new product launches they are
307:21 - effective or economic factors or
307:23 - marketing efforts those are all the
307:25 - explanations that a person can make but
307:27 - you can make this conclusions only when
307:29 - you have a larger data available to you
307:32 - and in this dat frame we don't have the
307:36 - marketing cost or any other cost
307:38 - involved regarding this so that's our
307:43 - conclusions are PE limited but what we
307:45 - can see is that um this bar graph
307:48 - combined with um any other bar graph for
307:52 - example marketing
307:54 - cost you can make a pretty good amount
307:57 - of conclusions from that as a business
307:59 - now how about um total we can also PL
308:03 - this using just a normal graph which
308:05 - means I will just copy it or no I won't
308:10 - this plot it you sales here CL the sales
308:14 - so this shows a little bit different now
308:16 - I prefer this sort of graph for instead
308:19 - of a bar graph for tracking the yearly
308:23 - sales because it shows much more clear
308:24 - at the amount of increase with the mod
308:27 - decrease now we can also uh focus on the
308:30 - quar sales like I said to be able to uh
308:34 - react to emerging Trends or to emerging
308:36 - or to um react fast fast to or to be
308:40 - able to react fast to any kind of change
308:44 - now let's again convert the order date
308:46 - to date format or their dates
308:57 - [Music]
309:04 - [Music]
309:27 - [Music]
309:32 - [Music]
309:49 - [Music]
309:58 - [Music]
310:30 - [Music]
311:06 - [Music]
311:12 - [Music]
311:19 - [Music]
311:34 - [Music]
311:51 - [Music]
311:57 - [Music]
312:20 - [Music]
312:26 - [Music]
312:42 - [Music]
312:49 - [Music]
313:23 - [Music]
313:51 - from our Cy sales we can see that
313:54 - there's a steady increase of quar
313:56 - quarter Cs and all of a
313:58 - sudden July it blows up to new heights
314:03 - so from this graph we can see exactly
314:06 - that uh Q3 and Q four did very well and
314:11 - q1 and Q2 didn't so something might have
314:14 - changed for example seasonal Trend
314:17 - or you have increased your marketing or
314:20 - you have introduced a new product or you
314:22 - have targeted a specific customer
314:24 - segment and as a business is really
314:26 - important to know that so you can also
314:29 - expect that if you follow a similar line
314:32 - of actions then you might have higher
314:35 - demand for your products on cute Q3 and
314:39 - Q4 so you might want to Overstock it or
314:42 - maybe you can analyze this uh further
314:46 - and replicate the successful strategies
314:49 - for future quarters and you this will
314:51 - cause for this will make sure that a
314:54 - business can stly grow and increase its
314:56 - Revenue which is good and also for um
314:59 - qon you can see that there's um it
315:03 - starts out with pretty slow I mean
315:05 - businesses may want to start out the
315:07 - year more quickly so they can
315:10 - investigate it also for example is this
315:13 - a seasonal uh for the industry maybe on
315:16 - certain Seasons a certain product is not
315:18 - high in demand or maybe the a competitor
315:22 - did some kind of marketing or um some
315:25 - kind of or use some kind of a strategy
315:29 - to drive more customers to them or maybe
315:32 - we have changed um marketing effort in
315:35 - our Q3 and our marketing efforts were
315:38 - not productive for q1 and Q2 so maybe
315:42 - that's it all right so maybe you want to
315:45 - investigate this uh much more deeply so
315:48 - not quarterly but monthly so let's do it
315:52 - now um let's start off the same way cor
315:54 - the order date column to the data time
315:57 - format DF order dates periods to daytime
316:17 - [Music]
316:52 - [Music]
316:58 - [Music]
317:05 - [Music]
317:21 - [Music]
317:37 - [Music]
317:44 - [Music]
318:12 - [Music]
318:29 - [Music]
318:38 - [Music]
318:51 - all right so from this graph you can see
318:53 - that it's growing month over month
318:55 - beside first month of uh 2019 and also
319:00 - the third month the the third month of
319:04 - 2018 so this is generally an upward
319:07 - Trend which should which might suggest
319:09 - like a healthy sales going on and it
319:12 - looks like August and December might be
319:15 - your seasonal so you might want to
319:17 - Overstock the products there and okay
319:21 - you can also see that there are seasonal
319:23 - dips in third month of 2018 and also
319:28 - 2019 and November of 2018 so you might
319:32 - want to consider it uh like seasonal
319:34 - promotions to stimulate offseason sales
319:39 - or diverse diversify your product
319:41 - service offerings to reduce the Reliance
319:44 - on seasonal demand maybe you want to
319:46 - start deploying new marketing strategies
319:48 - here or try to Target new customer
319:51 - segments by introducing new products so
319:54 - that you might offset the seasonal
319:56 - Trends and overall it seems pretty
319:58 - consistent so it looks like that there's
320:02 - a healthy sales friend so you might want
320:04 - to invest more in your proven strategies
320:07 - which might be for example certain
320:09 - marketing tactic promotion or product
320:12 - offering for a certain month and like I
320:14 - said for the dips the store might um try
320:18 - to deploy new marketing strategies or
320:21 - introduce new products to Target new uh
320:24 - customer segments which might which
320:26 - might offset uh the dip and that said um
320:29 - and that said it's uh more it's
320:32 - important to consider the longer time
320:34 - frame a year is certainly a year
320:36 - certainly has great amount of data but
320:40 - it does not really accurately um reveal
320:44 - seasonal patterns because for one year
320:46 - that might be the case but for another
320:48 - year it might just be
320:50 - completely uh the opposite so you might
320:53 - want to consider a larger um sales line
320:57 - graph for example maybe for 5 years and
321:00 - then you can see if that's the case and
321:02 - if that's the case then this might
321:04 - suggest a seasonal Trend and if not then
321:07 - you will act accordingly all right so we
321:10 - have covered um the sales Trends we can
321:13 - move on to the next chapter which is all
321:16 - right let's move on to the next chapter
321:18 - which is mapping so we want to create a
321:20 - map out of uh sales per state so for
321:26 - each state we want to color it um
321:28 - according to the amount of sales so if
321:31 - you are if there is a high amount of
321:33 - sales then it should be colored yellow
321:35 - and if there is low amount of sales then
321:37 - it should be colored
321:39 - uh blue so the question is why would
321:42 - someone want to do this now companies
321:44 - looking to expand into new Geographic
321:47 - areas face the challenge of identifying
321:49 - the most promising States and regions
321:53 - for their products or Services now for
321:55 - example how do you know if um your
321:59 - product will sell in a certain state so
322:03 - one of the one of the tactics that
322:06 - people often use is by seeing if there
322:08 - is a similar store like them operating
322:10 - in that state or in that City and so if
322:13 - there is a similar store um working
322:16 - there and it's not a saturated Market
322:19 - meaning there are still plenty of amount
322:22 - of people who might buy your product
322:27 - then it's a good idea to go there for
322:29 - example a company that manufactures
322:31 - Athletic Apparel is considering
322:34 - expanding its retail footprint by
322:37 - analyzing total sales data by US state
322:40 - they can see that states with a high
322:41 - concentration of fitness centers and
322:43 - active population like for example
322:46 - California Texas or Florida might be
322:50 - good candidates for new stores and if
322:52 - there are currently no
322:54 - uh um sports stores there then it's even
322:58 - better or for example if um you're a
323:02 - business and you want to strategically
323:06 - allocate your Market budget and sales
323:08 - team and so you have uh stores all over
323:11 - the States you you want to optimize it
323:14 - for each state maybe one state is
323:17 - performing well and another state is not
323:19 - performing so you might then from the
323:21 - map you might see which state is not
323:23 - performing well and allocate your
323:25 - resources accordingly to be able to um
323:29 - maximize your return on investment by
323:33 - optimizing certain strategies but if you
323:36 - don't know which state is not uh
323:38 - performing well then if you don't know
323:40 - which ST which state is performing well
323:44 - then you have no information on where
323:47 - you have to optimize like for example a
323:50 - n a national Pizza chain wants to
323:52 - optimize his marketing spend now sales
323:55 - data reveals that their P pit areas in
323:58 - the midwest consistently outperform
324:00 - those on the west coast now this data
324:02 - suggest that they might need to allocate
324:04 - more marketing budget to increase the
324:06 - brand awareness and sales in the
324:08 - Westland States and you might also want
324:10 - to do this because of competitor
324:13 - analysis so staying ahead of the
324:15 - competition means understanding where
324:17 - your competitors are having the most
324:19 - success analyzing their sales patterns
324:22 - across states can reveal their
324:23 - Geographic strengths and weaknesses now
324:26 - for example a Coffee Roasting Company
324:27 - notices a competitive coffee brand is
324:30 - experiencing High sales in the Pacific
324:32 - Northwest State now this could indicate
324:34 - the competitor has established a strong
324:36 - partnership with local grow grocery
324:39 - stores or launch successful marketing
324:41 - campaigns in that region now the company
324:43 - can use this information to Target
324:46 - similar grocery stores or develop
324:49 - competitive marketing strategies for the
324:52 - Pacific Northwest so without further do
324:56 - let's get started with it now I will
324:59 - walk you through the code and instead of
325:00 - writing it instead of writing it we will
325:04 - I will just walk you through it so let's
325:06 - first import the clely graph function or
325:10 - the library so we initialize the ploto
325:13 - jupit notebook and we also want to
325:16 - create a map for all 50 states which we
325:19 - do this right here and add the ab
325:22 - abbreviation column to the data frame
325:25 - because right now there is not we add we
325:29 - initialize the variable and calculate
325:33 - the amount of sales for each state and
325:35 - grou it by State and this is exactly
325:37 - what we need and then we add the
325:39 - abbreviation to some of sales we do that
325:41 - here and finally we plot it and this is
325:44 - how the map looks like so the Blue Area
325:47 - are the ones with low amount of sales
325:50 - and the yellow area which California has
325:53 - high amount of sales so from this map
325:56 - you can see which
325:57 - areas um main sales come from and
326:01 - according to it you can op you can
326:03 - optimize accordingly so let's say you
326:05 - have a pizza chain or pizza chain of
326:08 - stores and you want to see which one of
326:12 - your States is performing the best and
326:14 - which one is performing the poorest so
326:16 - you want to spend your most energy on
326:19 - optimizing what doesn't work so from
326:22 - this you can see clear forign is great
326:23 - so you can leave it alone but for
326:25 - example Texas is not performing that
326:27 - well so you might allocate more
326:29 - marketing budget or more uh resources
326:32 - there to have a to start making um or to
326:37 - start getting get more sales because in
326:39 - Texas there are still many amount of
326:41 - people who consume pieras but they are
326:44 - not buying so why is that and you can
326:47 - also see so for example if this is a
326:50 - completely another store this is like
326:53 - relates to for example a retail store
326:56 - for Sport Goods and all of this States
327:01 - they've got um a store in from this You
327:03 - can conclude that uh California is
327:06 - performing real well so it it's probably
327:09 - not a good idea to go there since it
327:12 - might have since there might be like
327:15 - Market situation there but you can go
327:18 - for example to for example Florida you
327:21 - might go to Florida and start selling um
327:24 - similar uh Sport Goods there because
327:28 - like you see the market is still uh new
327:32 - or the market is still not
327:34 - saturated all right so that was that so
327:37 - we can also create a bar graph out of it
327:40 - now from this you can see that most of
327:42 - the the total sales by state California
327:45 - is doing the best and North theota is
327:49 - doing the worst and of course now as you
327:51 - remember we previously categorized or
327:55 - showed how large each of our categories
327:57 - are and we did the same for sub our
328:03 - subcategories but we never did it in the
328:05 - same uh plot so so here we display our
328:11 - main category of products which are
328:12 - Furniture office supplies and technology
328:15 - and for each uh category we have
328:17 - subcategories and based on their size
328:20 - it's uh and it's organized based on
328:22 - their size so chairs it seems that
328:25 - chairs is the largest or sells the most
328:29 - in our furniture category then it goes
328:31 - to tables and we have then small amount
328:33 - of book cases and
328:35 - finishings and in our office supplies
328:37 - you can see that the best uh storage um
328:41 - product is performing the best and
328:43 - envelopes and labels are performing the
328:45 - worst and for our Tech category phones
328:48 - are performing the best and the machines
328:51 - accessories and copies and from this you
328:53 - can see that phones is overall the best
328:56 - subcategory it's even I think it's even
328:58 - larger
328:59 - than um chairs yeah little larger so
329:04 - this is a much better way to display um
329:07 - if you're trying to make an
329:09 - argument um to display the plot and of
329:12 - course you can also do it this way all
329:13 - right and hope you guys enjoy this
329:17 - project I definitely did and I will see
329:21 - you guys in the next video this video
329:23 - was sponsored by lunarch at lunarch we
329:26 - are all about making you ready for your
329:29 - dream job in Tech making data signs and
329:32 - AI accessible to everyone with is data
329:36 - science artificial intelligence or
329:38 - engineering at lunar Tech Academy we
329:41 - have courses and boot camps to help you
329:43 - become a job ready professional we are
329:46 - here to help also businesses and schools
329:48 - and universities with a top notot
329:50 - training modernization with data science
329:53 - and AI corporate training including the
329:56 - latest topics like generative AI with
329:59 - lunar Tech learning is easy fun and
330:02 - super practical we care about providing
330:05 - an endtoend learning experience that is
330:07 - both practical and grounded in
330:09 - fundamental knowledge our community is
330:12 - all about supporting each other making
330:15 - sure you get where you want to go ready
330:17 - to start your Tech Journey lunner Tech
330:20 - is where you begin for students or
330:22 - aspiring data science and AI
330:24 - professionals visit Lun Tech Academy
330:27 - section to explore our courses and boot
330:29 - camps and just in general our programs
330:32 - businesses in Need for employee training
330:35 - upscaling or data science and AI
330:37 - solution
330:38 - should head to the technology section on
330:40 - the lunch. page Enterprises looking for
330:44 - corporate training curriculum
330:46 - modernization and customize AI tools to
330:49 - enhance education please visit the
330:52 - lunarch Enterprises section at lunch.
330:55 - for a free consultation and customize
330:58 - estimate join lunch and start building
331:01 - your future one data point at a time