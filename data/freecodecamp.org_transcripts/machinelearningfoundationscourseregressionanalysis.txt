00:00 - welcome to this machine learning course
00:01 - for beginners taught by hayu Singh he is
00:05 - a data scientist and popular instructor
00:07 - this course is your gateway to mastering
00:09 - the intricacies of regression analysis
00:12 - in the realm of machine learning IUS has
00:15 - designed this course to provide you with
00:16 - in-depth knowledge practical examples
00:19 - and Hands-On projects that will set you
00:22 - up for success with the curriculum that
00:24 - covers everything from the basics to
00:26 - Advanced Techniques this course offers a
00:29 - unique opportunity to not only learn the
00:32 - fundamentals of machine learning but
00:34 - also explore the often overlooked power
00:36 - of regression analysis so get ready for
00:40 - a deep dive into the world of machine
00:42 - learning what if I told you that there's
00:45 - a core machine learning course that does
00:46 - not have only detained explanation of
00:48 - regression analysis but also have bday
00:51 - lecture notes as assignments with
00:54 - personalized feedback on that and a
00:56 - scheduled learning plan for you well
00:58 - guess what I'm offering all of this but
01:01 - absolutely free hi I'm ayush and I'm
01:04 - currently working as a data scientist at
01:05 - Epic and has demonstrated working
01:07 - experience as an envelopes engineer at
01:09 - xenomen and a data scientist at artifact
01:11 - and a founder of Anton and I welcome you
01:14 - to our core comprehensive machine
01:16 - learning course out here the course is
01:18 - designed to give you a solid foundation
01:20 - of machine learning and tradition
01:22 - analysis which is often overlooked in
01:24 - most of the courses available right now
01:26 - we beg it by linking the downward for
01:28 - machine learning so even if you're
01:29 - beginner you are able to understand it
01:31 - you talk about what is learning we talk
01:32 - about generalization we talk about
01:34 - inductive bias and a lot of things
01:36 - initially and then we gradually move on
01:38 - regression analysis and recover as much
01:40 - as we go in depth and to ensure that we
01:43 - are quite equipped with the Practical
01:44 - applications as well I'm going to
01:46 - introduce two projects by the end of the
01:48 - course which you have never seen like
01:49 - that project before as well because I
01:52 - have worked personally at several
01:53 - companies and I know how we deliver data
01:56 - science process to them and the same way
01:57 - I'm gonna do it in this course as well
01:59 - but that's not all if you want to
02:01 - maximize your learning opportunity from
02:03 - this course then we are offering free
02:05 - updates to the course regular
02:06 - assignments with personalized feedback
02:08 - to you and a tailored learning plan for
02:11 - you to complete this course in just one
02:13 - month you can simply go on a GitHub page
02:15 - for the course and then get access to
02:16 - the lbs directly from there and all of
02:19 - this were absolutely free one of the
02:21 - major issues which are on a highlight
02:22 - you might be asking here use just for a
02:24 - question most of the instructor takes
02:26 - around only one audio of taking freaking
02:28 - so much of our super teacher I tell you
02:30 - that people forget the foundation people
02:32 - forget the basics of it and they'll just
02:34 - just strictly goes on Advanced topics
02:36 - like deep learning and then they are not
02:38 - able to secure the job and then they
02:39 - complain me that hey I used data science
02:41 - has no jobs plus actually if you think
02:43 - in a way that the talent is less than
02:46 - the actual drops because they are not
02:48 - going to pay you thousands of dollars
02:49 - for just three lines of code which you
02:51 - ride inside but anyone can do it and
02:53 - then crowd is increasing but the talent
02:55 - is decreasing they're forgetting what's
02:57 - the base of machine learning what's the
02:59 - base of data science before getting the
03:01 - solid foundation which was used to be
03:03 - given in 90s and all so that's what the
03:06 - course focuses on we make sure that we
03:08 - emphasize the core concept of machine
03:09 - learning and regression analysis so that
03:11 - you can even learn further things on
03:13 - your own our course is designed to put
03:15 - emphasis on core concept of machine
03:18 - learning and traditional analysis so
03:20 - that you can tackle critical business
03:22 - problem and solve it the way real data
03:25 - scientists solve so that's enough of it
03:27 - now let's get started with the course
03:28 - you can directly go to the LMS and then
03:30 - start from there by seeing the tailored
03:32 - learning plan or you can get started
03:33 - from here let me know if you have any
03:35 - notes in the comment section and find
03:36 - any errors or irata in this course
03:38 - please feel free to make an issue on the
03:40 - GitHub page for the course hey everyone
03:42 - uh so let's get started with our machine
03:44 - learning content and actually we'll get
03:46 - started with
03:48 - um a bit of discussion on learning the
03:51 - machines learning intelligence and give
03:53 - you some of the concepts which is
03:55 - required eventually to build up your
03:57 - ideas right so let's get started
04:06 - so I would suggest you to open up your
04:08 - notes if you have got it so once you
04:10 - have the notes that you can see on the
04:11 - front of the screen so we're going to
04:13 - start off with what is learning right so
04:16 - let's eventually talk about what is
04:18 - learning and slowly we'll get into the
04:21 - get into the concepts of you know a very
04:23 - very good examples of learning and
04:24 - intelligence Etc
04:26 - so the first one is that what is
04:29 - learning so I'll suggest you to write
04:31 - your answer in the comment box if you
04:33 - know so according to what exactly uh
04:35 - what exactly you know about in terms of
04:37 - machine learning we have the training
04:38 - data we take some sort of you know
04:41 - um the data don't worry what data looks
04:43 - like and everything will cover that so
04:45 - we have the data right and then we feed
04:47 - to a sort of a black box and then we get
04:50 - a trained algo that is trained on that
04:52 - data for example assume that you have a
04:55 - man right you trained that man with the
04:57 - with that particular skill and then
04:59 - whatever output after six or seven
05:01 - months you will be getting a trained man
05:03 - specialized in that skill right so this
05:06 - is how your model model gets trained
05:07 - your model gets the data gets the
05:09 - training from the data and then get it
05:12 - it gets trained by some instructor right
05:15 - or assume that in this case it's an
05:17 - algorithm and then after after after
05:19 - some time we get a trained algorithm
05:21 - right that in terms of a very simple or
05:24 - very you know mind understanding
05:26 - standing behind any algorithm but let's
05:29 - talk about a very nice example very nice
05:32 - example which is called which is known
05:34 - as rats learning to avoid poisonous
05:37 - dates so what is baits baits are some
05:40 - sort of you know food or Etc so the
05:42 - formal definition of beets is that it is
05:45 - it is some sort of you know
05:48 - um it is a type of prey that is used to
05:51 - attract when hunting so
05:53 - um sometimes you we use that baits to
05:55 - attract you know poisonous thing it it
05:58 - is a poisonous thing that you use to
05:59 - attract uh food right uh sorry a mouse
06:03 - is or any aerial things whatever for
06:05 - example fish we also have the beets
06:07 - right so in this way we have the baits
06:09 - so basically the problem statement the
06:10 - problem statement is that the rats are
06:13 - learning to avoid the poisonous baits so
06:16 - how do lats rats learn to avoid that
06:18 - poisonous Spades right so rats assume
06:21 - that rat came and then uh assume that
06:24 - rat game a rat game and that and then
06:27 - they given a no they there was a food
06:30 - available where this was a bait this was
06:32 - a bait right uh so it got they gone and
06:35 - then they um it was like smelling good
06:37 - and Etc right eight rat ate this food
06:41 - rat eat this and there's an ill effect
06:43 - on that game right there's an ill effect
06:46 - of that game there's a effect of rat
06:49 - game when they eat this food and and
06:52 - then rats will not eat this food because
06:55 - this is this is the learning mechanism
06:57 - which happened right now where where
07:00 - according to the past experience
07:02 - according to the history or the mistakes
07:03 - in the in in the past this rat did and
07:07 - he's not on rats will no more gone to
07:09 - repeat this step again
07:12 - this is one sort of a learning mechanism
07:13 - another learning mechanism which you can
07:15 - think of is assume that you you have
07:18 - done some sort of mistake in your past
07:20 - or a very simple example assume that uh
07:23 - you go for someone for guidance right
07:25 - why why they are capable of giving items
07:27 - because they have gone through the
07:29 - problems they have already gone through
07:31 - the problems so you go to them so that
07:33 - you get an easy path so that you get an
07:35 - easy uh guidance from them you you so to
07:38 - achieve something in your career right
07:39 - so in that way I'm saying that they
07:42 - already have an experience of something
07:43 - now they're capable of you to guide
07:46 - right just like I do I guide so many
07:48 - people right so we I got so so many of
07:51 - books right now so I am a capable
07:53 - because I've already gone through those
07:55 - problems now I'm capable because of my
07:57 - past experiences so whole point to take
07:59 - out right now I have learned what
08:02 - mistakes I what mistakes and what things
08:04 - I did that leaded me to success and
08:06 - failure right that I've
08:09 - well to learned that so now that's a
08:11 - learning mechanism for example you learn
08:14 - you know you you study some maths
08:16 - concept right you study quadratic
08:17 - equations or maybe some calculus concept
08:19 - so when you study that when you study
08:22 - that
08:23 - um what eventually happens is that uh
08:26 - you you take a look at that particular
08:28 - particular concept and once you take a
08:30 - look at the particular concept now you
08:31 - understand with the helpful examples and
08:33 - then you do a problem and if you did any
08:35 - sort of mistakes you you change it
08:37 - according to you know um Lex ask ask for
08:39 - help or see the solution but to when you
08:41 - s but but when you saw similar questions
08:43 - you will be able you you will be able to
08:46 - answer that why because you have learned
08:49 - from the past mistakes which you did
08:51 - right because you are not going to
08:53 - repeat the mistakes which you did in the
08:55 - past even while solving problems
08:56 - sometimes it happens as well sometimes
08:58 - it does not right so you you look that's
09:00 - on the learning mechanism that's why we
09:02 - say the failure is a part of success
09:04 - because it gives you the learning phases
09:06 - the learning of opportunities to
09:08 - actually go to grow forward
09:10 - right so there are several types of
09:12 - approaches which we go through the first
09:14 - approach is learning by memorization
09:17 - approach so this is something that's
09:19 - very interesting uh where we we say in
09:22 - Indian language it's ratification and in
09:25 - ratification what what we do we memorize
09:27 - things whatever we we we we want to
09:30 - study so for example I've seen students
09:32 - to literally memorize you know signs
09:34 - answers SST answers even computer
09:37 - science answers because they literally
09:38 - memorize code and they just go and write
09:40 - in the sheet of paper in uh and then
09:42 - they they memorize literally like even
09:45 - mathematical questions however it's so
09:47 - obvious that daily not will perform well
09:49 - but but but over here assume that if you
09:52 - memorization every concept each and
09:54 - every line of your answers will you be
09:56 - able to perform good in examination so
09:57 - even if one line you forget you'll mess
10:00 - up with all the answers you'll start
10:01 - integrating first answer to second
10:03 - answer second answer to first right so
10:05 - you have to apply so over here
10:08 - memorization literally fails in your
10:10 - daily life you can not literally
10:12 - memorize everything and then go and set
10:15 - an examination and you expect your that
10:17 - you'll score good it it works till class
10:20 - six or seven but it does doesn't work
10:21 - when you actually grow right so you can
10:24 - actually can't literally memorize each
10:26 - and everything whether it be answers
10:27 - whether it be mathematical Solutions or
10:29 - literally anything
10:31 - so I hope that that giving a good sense
10:33 - about what exactly I want to talk on is
10:36 - um
10:37 - learning by memorization approach where
10:38 - we memorize we learn some things by
10:41 - memorizing uh that sort of thing right
10:43 - so now so now what so now what what
10:46 - exactly I'm going to do is talk about
10:48 - some of the uh examples which we have
10:51 - listed out here for you to better
10:53 - understand things cool so assume that
10:56 - you have you you have a you want to
10:58 - build a system so you might have seen in
11:00 - your email you know uh Gmail where you
11:03 - where you have several photos promotions
11:05 - folders social folder Spam folders and
11:08 - it's an updates foldering Etc so you
11:11 - have these folders and this one folder
11:13 - called spam where all the spam goes in
11:15 - right where all the spam goes in so what
11:17 - are what are those spams right that's
11:19 - question that's a question so now now
11:21 - we're going to build a system you want
11:23 - to build a system you want to build a
11:24 - system that given the past data right
11:27 - first of all we if if you're learning
11:30 - something to identify a spam you need to
11:32 - have the data you need to have the you
11:33 - need to have some some sort of you know
11:35 - content to understand tantrum right so
11:38 - in this case if you want to build a
11:41 - system you'll build a system that given
11:43 - an email given an email it classifies
11:46 - that email is a spam or not spam okay
11:49 - that given an email it classifies
11:51 - whether that email is a spam or not spam
11:53 - right so now that's a question right so
11:56 - you you so now what you so now what you
11:59 - will do is that the first step is for
12:03 - anything for learning literally anything
12:04 - you need content for this so we need the
12:06 - data for it right so we are we are going
12:09 - to collect the collect the collect
12:11 - collect collect the data
12:12 - um we want to collect the data which is
12:15 - where where we have the spam emails
12:17 - right so we will collect the data which
12:19 - is spam emails out here so we can we
12:21 - collected the data which is spam emails
12:23 - out here now
12:24 - we have the content now we are now we
12:26 - want our machine to learn from this
12:28 - content so there will be two types of
12:30 - guys first who will memorize this so the
12:32 - four we'll talk with the first guy the
12:34 - first level will memorize this will
12:37 - memorize each and every email each and
12:39 - every email so machine will memorize
12:42 - each and every spam email
12:43 - right and now now you're now that
12:47 - machine is capable now with that machine
12:50 - is even if if that is 100 correct even
12:52 - if that memorized a very hundred percent
12:54 - correctly right so machine will be able
12:57 - to class will will be will will be able
12:59 - to memorize each and everything now if
13:02 - our an email comes em email comes
13:05 - through this machine and now this a
13:07 - machine is stored to classify this email
13:09 - to be as a spam
13:12 - or to be as a not spam right whether
13:14 - that email is a spam or not spam so you
13:16 - you go to new email and you classify it
13:18 - so how machine will classify machine
13:21 - will search this email into its memory
13:24 - whether this email is available into end
13:27 - to its memory or not right whether the
13:30 - words of this email is available to in
13:33 - in my memory or not right it if not then
13:37 - we say that it is non-spam but because
13:38 - the data is but because the the content
13:41 - which we learned is about spam right so
13:43 - if the if this is not available in that
13:45 - content in his memory what he what the
13:47 - model has memorized from the data then
13:50 - he then the model will be able to see
13:51 - that it is non-spam but if that email is
13:53 - available then it will be spam so what's
13:56 - the problem with this approach what's
13:58 - the problem with this approach right
13:59 - that's a good question what exactly the
14:02 - problem with this approach that's a good
14:04 - question to actually go forward to the
14:06 - problem with this approach the problem
14:08 - with this approach it is not
14:10 - generalizable it so why it is not
14:13 - generalizable so let's talk about a very
14:15 - instance example assume that you have
14:17 - got a new email you you've got a
14:19 - completely new email which where your
14:21 - model has not uh you know memorized for
14:25 - example you sat in your mathematics uh
14:27 - let's say Science examination you got
14:29 - your questions which was in a textbook
14:30 - which you memorize but you also got a
14:32 - tweaked question but you also got a very
14:34 - different thread set of questions so
14:36 - where you have to apply logic and you
14:37 - know apply several skills in that so he
14:39 - there you will fail right because you
14:41 - memorize only the consequences which
14:42 - came but you're not able to answer the
14:44 - questions which are new which are unseen
14:46 - by you right so in this case as well
14:48 - model will fail to gen to to to predict
14:51 - for examples or predict for emails which
14:55 - are unseen to buy the model which are
14:57 - not seen by the model
15:00 - right so that's that's the problem with
15:03 - uh with the approach right now right so
15:06 - that's one of the approach the problem
15:07 - is that the machine which we have does
15:10 - not have the ability to email any unseen
15:14 - emails to to predict for any unseen
15:18 - email so for example if any any email
15:20 - comes and let's assume that it is a spam
15:22 - but this is not available in this memory
15:24 - content so that will be flat as non-spam
15:27 - but actually it was a Spam right so this
15:29 - is one of one of the problem so we need
15:32 - what we need to do we need to figure out
15:33 - a solution to further generalize it
15:36 - further make into to to generalize means
15:39 - to have to give to give this machine an
15:42 - ability to give this machine an ability
15:44 - to further classify even unseen emails
15:48 - by logical thinking so I'm going to make
15:50 - this guy so the second number of guy
15:52 - wear it also understands analyzes the
15:54 - data and applies logical thinking in
15:57 - further to predict emails so that's
15:59 - where to generalize and this is called
16:01 - something known as inductive inference
16:04 - okay this is not nothing but called
16:06 - inductive inference
16:09 - so we will talk we'll talk let's uh
16:11 - let's talk about inductive inference uh
16:14 - in the in detail in the next lecture so
16:17 - now we'll talk about something known as
16:19 - a generalization which we had had a talk
16:21 - and then we'll talk about who and what's
16:23 - the issue in this as well right so we'll
16:25 - talk about that so the we we call that
16:28 - generalizing our model is known as
16:30 - inductive inference right please read
16:33 - the if you want to know more examples
16:35 - and all I have linked the very nice book
16:37 - the chapters and every chapters is also
16:39 - included in the in in the LMS so please
16:41 - go over there you won't be able to see
16:42 - everything uh um each and everything
16:44 - with the resources and everything uh and
16:46 - the reading materials regarding if if
16:49 - you want more examples but that's it you
16:51 - will stay here if you have understood it
16:53 - so generalization generalization is it's
16:56 - an ability to
16:58 - um to add the feature in a machine to
17:01 - actually improve right to to actually uh
17:03 - work on even unseen examples but to
17:05 - apply logical thinking
17:07 - so now let's take an example over here
17:09 - is that the the problem the problem
17:12 - which we had the problem the very basic
17:15 - problem which we had so I'm just going
17:16 - to take a very very nice example right
17:18 - now
17:19 - so now now over here over here this is
17:24 - right so in the end in this example This
17:27 - Thread if we have the generalization
17:29 - available if we have the generalization
17:32 - available if we have the generalization
17:34 - uh available where your rat will be able
17:38 - to decide whether to eat this food or
17:40 - not be whether to eat this food or not
17:42 - based on its smell right so rat will be
17:45 - able to decide to to eat this food or
17:48 - not
17:48 - with the help of its milk
17:52 - right so generalization how does this
17:55 - help how does this generalization have
17:57 - this had this right it helps it to
18:00 - decide and answer examples which is
18:01 - similar to the bad food so what is this
18:03 - saying that rat may have seen the
18:06 - similar similar food right so that may
18:09 - have seen the similar food so
18:10 - generalization is that that even that's
18:14 - even a similar something comes in that
18:16 - will be able to refuse okay this is a
18:18 - bad Pro this is a good food right so
18:20 - that should generalization gives an
18:22 - ability to even rat to decide on this
18:25 - particular food which has net never
18:27 - written before based on the based on the
18:29 - similar things which everybody eat which
18:31 - he has written in the past
18:33 - another example which which we have seen
18:35 - that the that we have a machine which
18:38 - scans the emails we'll scans the emails
18:40 - it extracts the words that appears to be
18:42 - spammy Words which is spammy words and
18:45 - then it says spam or Not by seeing the
18:47 - spammy word so basically that email
18:49 - comes and then see what proportion of
18:50 - the spammy votes is in that email and
18:52 - then classify it based on that
18:55 - but there's an issue so now by this if
18:57 - you have the if you have the if we have
18:59 - the words that is that is spammy and
19:02 - then we check the proportion in that or
19:03 - we give the machine the ability to even
19:05 - classify uh based on The Logical
19:07 - understanding of that and or a
19:10 - generalized understanding of okay this
19:12 - is this this is how the spam email looks
19:14 - like right so your model will is able to
19:16 - generalize accordingly according to the
19:19 - things right that's one of them but here
19:22 - your system might be able to perform
19:24 - their event right so in this case you
19:26 - might be thinking that you will be your
19:29 - system will be able to perform OMG right
19:31 - but actually it's not true
19:34 - um you have to give the ability to the
19:37 - model that to identify your nonsense
19:38 - examples and then you think okay it's
19:40 - over right it's not over so inductive
19:43 - inference might lead us to false
19:45 - predictions misleading predictions let
19:47 - us see how so this uh prison super
19:49 - session is an example business
19:52 - supercision is an example
19:55 - um I will which I have read in a machine
19:57 - learning theory from algorithms's Theory
19:59 - and also this is a very famous example
20:00 - on learning conducted by some sort of
20:02 - you know University which I don't
20:03 - remember they have written it very
20:05 - nicely is about how this inductive
20:09 - inference might lead us to bad incidents
20:12 - how this inductive inference might lead
20:14 - us to bad inference so let's talk about
20:16 - the very nice stuff as of now is this
20:21 - this particular thing pigeon
20:22 - superstition so you will be seeing a way
20:25 - you'll you'll be seeing a very nice uh
20:27 - I'll be linking a very nice video which
20:29 - you which I suggest you to go over there
20:32 - and watch that video first right so go
20:35 - over there watch that video first or
20:38 - I'll be giving a short
20:40 - um short experience on what exactly is
20:42 - going on so also you can actually pause
20:44 - the video right now go over there and
20:46 - understand what exactly is going in that
20:49 - video but I'll be explaining you in
20:51 - simple English terms so you can actually
20:53 - go over there and understand it by your
20:54 - own terms but I'll be explaining you in
20:56 - the very same simple English terms so
21:00 - you have um what what thought what the
21:01 - experiment was experiment was the you
21:04 - have you have set up you know you have
21:06 - set of pigeons you have set up pigeons
21:07 - which was contained in a cage so you
21:09 - have you you've taken a set of pigeons
21:11 - and what they did they have
21:13 - they've kept it in a cage right and that
21:16 - cage had a small hole has had a small
21:19 - hole where the food was coming and going
21:21 - at a certain interval of a Time assume
21:23 - that every three seconds four seconds
21:25 - five seconds ten seconds 12 seconds 14
21:27 - seconds and all these pigeons are super
21:30 - angry okay so so we have a cage and that
21:33 - we have a pigeons which is super hungry
21:35 - and then we there's a hole where the
21:37 - food is coming at a certain interval of
21:40 - a Time
21:41 - okay at a certain interval of a time now
21:43 - now when smooth is delivered now they
21:47 - are angry right so sorry there the
21:49 - patients are hungry right so if the
21:51 - patients are hungry they'll keep on
21:52 - doing some things they'll try to Peak no
21:54 - they'll they'll try to peek and try to
21:56 - find you know some sort of
21:58 - um Foods you know so that they try to
22:00 - Peak or they try to you know wings as
22:03 - like spread their feathers and then try
22:06 - to uh go from here to the other Peak
22:08 - right so in this case as an experiment
22:10 - I'm taking example if the pigeons if one
22:12 - of the pages F2 two three are peaking
22:14 - right so they're they're searching for
22:16 - them they're they're active okay they're
22:18 - active and they're peeking like this
22:19 - which is my like searching for food uh
22:21 - they're peeking now so when food is
22:24 - delivered when food is delivered at
22:26 - every interval of a time they are
22:28 - engaged in some sort of activity that
22:30 - activity can be peaking or that can be
22:32 - Feathering up right anything literally
22:35 - anything so if you see the video we'll
22:36 - be seeing that some birds are peeking
22:37 - some birds are
22:39 - um this is only one body which the
22:50 - they're engaged in some activities
22:52 - now now now when the food was delivered
22:55 - they were engaged in some activity right
22:57 - so when food is delivered they go on and
22:59 - eat in that right and then again start
23:02 - doing the same action
23:04 - again started doing the same action in a
23:07 - hope that food will deliver it once
23:09 - again right so if they would like pigeon
23:13 - is doing some peaking and the food is
23:15 - delivered by doing that action so you're
23:17 - gonna need and again started peaking
23:19 - so he thought that if I pick I'll be
23:23 - getting
23:24 - um
23:25 - food to me right so so basically pigeons
23:28 - started doing the same action in a hope
23:30 - of food arrival but I'll come to that
23:33 - part later on
23:34 - so the pigeon Association of the
23:37 - delivery of the food with whatever
23:39 - actions they do
23:41 - peeking Feathering flying literally
23:44 - anything actions it with whatever
23:47 - transactions they had been performing
23:49 - when when it was first delivered so if
23:51 - the first time it were delivered so
23:52 - there were beacons so they continue to
23:53 - do peaking in order for the food arrival
23:56 - because they hope for it because they
23:59 - hope for it so what's the problem in
24:01 - this right what's the problem in this
24:03 - right so let's talk about what exactly
24:06 - the
24:07 - um uh problem which which comes so
24:10 - basically pigeon build an association
24:14 - pigeon built an association with pigeon
24:18 - pattern Association of the delivery of
24:20 - the food the delivery of the food with
24:23 - whatever chance or actions they had been
24:26 - performing right so they built the
24:28 - association between delivery of food
24:31 - with the actions like peaking or flying
24:35 - they did so they they tend to do peaking
24:38 - in Hope of food arrival
24:40 - right so very interesting so your your
24:43 - vision learned that this that they
24:46 - learned this right they learned it but
24:49 - what exactly but but but now
24:52 - now this now let's try to figure out
24:54 - what exactly the problem in this problem
24:56 - is that human Learners the we can rely
24:59 - on Common Sense filter right to fit to
25:02 - filter out meaningless sense for example
25:03 - for example assume that uh every time I
25:07 - throw this pen over here every every
25:09 - time I throw throw this pen over here
25:10 - right every time I throw throw this pen
25:13 - over here my food will be coming right
25:16 - like it is at a certain a certain in the
25:18 - end of a time in the morning and
25:20 - afternoon in the night it does not
25:22 - matter how much I do this I'll be
25:24 - getting that interval of time in this
25:25 - case as well it was delivered on the
25:28 - interval of a second it was not
25:29 - dependent on what actions patients are
25:31 - doing it was just lost intervals three
25:34 - seconds four seconds three seconds six
25:36 - seconds eight nine seconds 12 seconds
25:38 - um I mean 12 seconds and then it is
25:40 - going on the at a certain end Tower over
25:42 - time it is not associated with what
25:45 - actions patients are doing doesn't
25:47 - really matter
25:48 - this is so now we we can even we can
25:53 - rely on it is a meaningless right
25:54 - whatever pressure has understood it is a
25:56 - meaningless right this is a meaningless
25:57 - and this is something which is not so
25:59 - human Learners can filter out
26:01 - meaningless things we can filter out
26:03 - that's a coincidence or that's something
26:05 - that that's something super meaningless
26:07 - and meaningless learning conclusions I
26:09 - by doing this if I get money like it's
26:12 - one time assume that you you are
26:14 - throwing this pen out here right you're
26:16 - throwing this pen out here and you you
26:18 - any and you got money on your bank now
26:20 - you are now you cannot keep on doing
26:22 - this to get money till the end of Hope
26:24 - of money arrival money will come at a
26:26 - certain amount of time right
26:28 - um so this is something which this is
26:30 - meaningless conclusion but there are
26:31 - some humans who cannot apply the common
26:32 - sense filters to not take out the
26:38 - but never mind so and now
26:41 - now we now uh now over here region also
26:45 - learned a very bad meaningless
26:46 - conclusion where is where it he thought
26:49 - that peaking will get me a food
26:51 - right that's a bad so we also have to
26:53 - give this Builder to our machines
26:55 - right so in this generalization approach
26:58 - the problem was that your model may end
27:01 - up learning a Minix meaningless learning
27:03 - conclusions so we must give the well
27:07 - defined crisp principles again I'm
27:10 - reading we must give well defined crisp
27:15 - principles so that will protect our
27:18 - program that from reaching in senseless
27:20 - like What patients reached
27:22 - that will protect our machines to reach
27:25 - that to to to to reach that conclusion
27:28 - this excerpt is taken from understanding
27:30 - from ml from Theory to algorithm this is
27:32 - very nice that's why I've taken it from
27:33 - there what exactly it tells it tells
27:37 - that your that we need to beautiful
27:40 - gives filters to our model so that our
27:42 - model is able to identify the
27:43 - meaningless conclusions like this right
27:46 - meaningless conclusions so we have to
27:48 - give the well defined meaningless sorry
27:51 - well-defined Tris principles crisp
27:54 - principles to a model and those
27:56 - principles are only called and those
27:59 - well
28:00 - well defined
28:02 - crisp
28:05 - principles
28:07 - are called
28:09 - what that are called nothing but ah
28:13 - development it's it's that's what we
28:16 - learned that's what the theory of ml
28:17 - comes in theory of ml that's what the
28:20 - theory of ml where we give the
28:22 - well-defined principles to our model I
28:24 - hope that makes sense to you uh that
28:26 - give it a very nice understanding
28:28 - about whatever we want to have a talk on
28:31 - now there is a very interesting concept
28:34 - which is known as inductive bias
28:38 - inductive bias so let's talk about that
28:41 - and then we can end this video and then
28:42 - we can go to the next step which is
28:44 - exactly learning about what is machine
28:45 - learning and then now we know about
28:47 - learning and then we can go over a bit
28:48 - off you know
28:49 - learnings so look cool
28:52 - so assume that uh so sometimes
28:55 - um uh you you might be noticing that a
28:58 - new cat that a new cat that that a new
29:02 - sorry new rat whatever I'm saying a new
29:05 - rat might refrain from eating a food
29:09 - because they think that that this food
29:12 - is is is poisonous why because of the
29:17 - prior knowledge inherited from the
29:19 - previous generations so they have some
29:21 - sort of genes that tells us don't need
29:23 - this food this is poisonous because of
29:25 - the past Generations because of the
29:28 - habitual understanding if you've studied
29:30 - the biology then you have studied that
29:33 - there are there Evolution or the or the
29:36 - evolution is necessary for human beings
29:38 - if there is no Evolution if there is no
29:40 - you know chain of you know
29:43 - um for a whole food chain so assume that
29:46 - you have a you you have a lot lots of
29:49 - humans and they died from the disease
29:51 - but but there might be some some newborn
29:54 - newborn humans where or babies which
29:57 - later they would develop in humans where
30:00 - they are capable of fighting with those
30:02 - diseases they are capable of that right
30:04 - they have inherited they have built a
30:07 - very nice combination so fighting with
30:08 - the diseases we so in this case this rat
30:11 - has inherited from the previous
30:12 - generations previous generations of
30:15 - their ads to further to to further say
30:18 - that okay I'm not going to eat this food
30:20 - I'm not going to eat this food because
30:21 - it gives some sort of sense
30:24 - so you might be thinking why rat so this
30:27 - so why rat why rat was able to
30:31 - successfully classify to not to do not
30:34 - eat that food whether pigeons was not
30:37 - able to they just was reaching to a
30:38 - meaningless conclusion because drats
30:41 - were having the prior knowledge and
30:44 - prior knowledge is super important where
30:45 - the rats were having the prior knowledge
30:47 - so pigeons learning was not more
30:50 - successful as compared to what rats said
30:52 - because of the prior knowledge and prior
30:54 - knowledge
30:56 - is also called inductive bias prior
31:00 - knowledge is also called inductive bias
31:02 - I hope that makes sense
31:04 - um now I hope that that every very
31:06 - nicely introduction to learning in the
31:09 - next lecture what I eventually going to
31:11 - try to achieve is talk about a very nice
31:15 - introduction to machine learning with
31:17 - applications and will also do several
31:19 - examples as well that's something
31:22 - um
31:23 - isn't necessary for you right so let's
31:25 - go to the next lecture and then try to
31:27 - understand machine learning with a very
31:28 - nice definition with the very nice set
31:29 - of examples so yeah so now what I'm
31:33 - going to do is actually talk about uh
31:35 - start talking about you know machine
31:37 - learning stuff and that's something
31:39 - which is super important for you to
31:40 - actually go forward with it right
31:42 - let's talk about that so now what we're
31:45 - going to do we're going to actually talk
31:46 - about what is machine learning I'm going
31:48 - to talk about some of the applications
31:49 - of it and then we'll see some of the
31:51 - formal definition which comes in machine
31:53 - learning now you know what exactly
31:54 - learning is right now you know exactly
31:56 - what learning is and then you'll start
31:58 - referring that we have solved some of
32:00 - the problem now to div now just just to
32:02 - conduct the dots to connect the dots the
32:05 - dots how we're going to connect it the
32:07 - dot is first is there first where are
32:11 - the where we learned about learning
32:12 - mechanism where our rats were learning
32:15 - two are two two to further classify
32:17 - where the whether to eat this food food
32:19 - or not right so we've gone through
32:21 - memorization approach where um by taking
32:23 - a simple spammer ham examples and for
32:25 - him memorization we thought that
32:28 - generalization that we have to
32:30 - generalize it so that is inductive
32:32 - inference so we've got another another
32:34 - issue which we see in inductive
32:36 - inference which is the misleading or
32:38 - reaching to a senseless conclusions and
32:40 - for defining that senseless curve and
32:42 - for for refraining or protecting our
32:44 - programs for senseless conclusions or
32:46 - the principal conclusions we have
32:48 - something known as machine learning
32:50 - wherein this machine learning we Define
32:51 - a well defined well-defined crisp
32:54 - preface principles so that our model or
32:56 - model does not reach us to uh senseless
32:59 - conclusions
33:01 - cool
33:03 - um so let's get started so I suggest you
33:05 - to maybe have a coffee with you because
33:07 - that's something is super important
33:09 - to refrain from it so correctly I'm
33:11 - recording at 3am because I have boats as
33:14 - well and I have to prepare uh so uh I
33:18 - know how I'm managing this because I
33:20 - have a class 10 both class 10
33:21 - examination in 20 24 days I guess and
33:24 - have to prepare a lot and uh
33:27 - I'm doing my day and night on that but
33:29 - eventually I have to also do for
33:30 - community and I'm doing it because I
33:32 - like it not something who's forcing me
33:35 - to do this but coming back to what
33:37 - exactly I want to do it is what tells
33:39 - machine learning question so whatever
33:42 - you know about machine learning write
33:43 - that in a comment box edit your comment
33:46 - or whatever you can do and just just
33:48 - write that in the comment box what is
33:49 - machine learning right so what is
33:52 - machine learning it's an artificial
33:54 - intelligence domain it an artificial
33:57 - intelligence domain where we extract
34:00 - patterns from the data and analyzes the
34:03 - data and make intelligent predictions on
34:06 - the new data according to the pattern
34:08 - your machine has learned a very nice and
34:11 - influential statements so let's try to
34:14 - talk about uh the very highly
34:16 - highlighted statement it's an artificial
34:18 - intelligence domain where we extract
34:21 - patterns from the data
34:23 - analyze the data make intelligent
34:26 - predictions on the new data this new is
34:29 - very nice keyword according to the
34:31 - pattern your machine has learned so
34:33 - these uh these you know keywords if you
34:35 - understand you know machine learning so
34:37 - let's try and understand each and every
34:39 - keyword first it's a domain of
34:41 - artificial intelligence so if anyone
34:43 - calls you know MLS AI say ml is AI of
34:49 - course it is but it is but it is a
34:51 - domain of Nei it is a subset of an AI
34:54 - domain right
34:55 - our way to extract patterns from the
34:57 - data where the exact patterns are data
34:59 - so I want to explain this to you with a
35:01 - very nice you know practical example
35:03 - let's forget you know bookish examples
35:05 - so assume that you um you wanna you are
35:08 - doing something like um yeah so you're
35:11 - starting some sort of mathematical
35:12 - Concepts assume that you're studying
35:13 - quadratic equations so you're studying
35:16 - about quadratic equations and in that
35:18 - quadratic equations you had something
35:20 - known as taking out the you know uh by
35:22 - you have some sort of quadratic
35:24 - equations from there's a form formula
35:27 - for taking out using
35:29 - loses
35:30 - um like the concept right so there are
35:32 - so many word problems regarding ages uh
35:36 - distance and speed Etc so now many
35:38 - actually when you actually solve those
35:40 - problems when you actually solve those
35:42 - problems you learn some sort of patterns
35:44 - that this is how I should solve it when
35:47 - you learn those chapters we expect those
35:49 - chapters when you actually understand
35:50 - those chapters you learn
35:53 - how to what what what patterns are there
35:56 - what patterns of questions are there and
35:58 - how the solutions how the solutions are
36:00 - framed right so in this way in this way
36:04 - we have to figure out in this way you
36:07 - are learning patterns from the from from
36:09 - the chapter and by learning that you're
36:12 - able to reach to a conclusion right so
36:14 - once you have the pattern now if the new
36:16 - question comes replies that whatever
36:18 - patterns which you have learned are
36:20 - analyzed your your your content now you
36:23 - use that analyzer whatever whatever you
36:25 - learned to answer those or predict
36:28 - predict them cqs right so answer those
36:31 - new questions right and what from
36:34 - whatever you have learned till now so if
36:36 - you if you didn't find the pattern well
36:38 - you will be not able to you will be not
36:40 - able to you know answer that well but if
36:42 - you know but you will be able to answer
36:43 - very well right
36:45 - good so that's exactly machine learning
36:47 - work so we're going to understand in
36:48 - that way so it extract patterns from the
36:51 - data so for example if you take an
36:52 - example of a Spam or ham example spam or
36:55 - ham example it extracts patterns from
36:58 - the data it extracts patterns from the
37:01 - data where how spam email looks like how
37:04 - hammy looks like it learns analyzes that
37:07 - whole data data means the collection of
37:09 - information in this case you have a Spam
37:12 - and Es spam and have emails that's the
37:14 - collection of information you extract
37:16 - the patches you understand that house
37:18 - fam looks like how ham looks like now
37:20 - once you have that I analyzed it now now
37:22 - when new spaniel comes in you use the
37:25 - model uses that analyzed or patterns to
37:28 - make intelligent predictions on the data
37:30 - which you have right according to the
37:32 - question your according to the pattern
37:34 - which a machine has learned that's what
37:35 - exactly machine stores so now you might
37:38 - have several questions by now I know it
37:40 - first is again how to extract patterns
37:43 - how machine extract patterns from the
37:45 - data how machines analyzes the data how
37:47 - machines make intelligent predictions
37:49 - from the data and Etc
37:51 - right so these are questions and in the
37:54 - whole course we'll answer this we'll
37:56 - answer this question answer this how
37:59 - machines extract patterns how we analyze
38:01 - the data how machines make predictions
38:04 - from your data right so there are set of
38:07 - mathematical as of now the answer is
38:09 - their set of mathematical algorithms
38:11 - that helps us to extract patterns
38:13 - mathematical or statistical algorithms
38:15 - it's from the data which we'll study
38:17 - throughout the course okay
38:20 - uh so that's an answer so let's go to a
38:23 - more formal definition which I really
38:25 - like by Tom Michelle and that's here's a
38:28 - nice book I recommend you to read that
38:29 - book really nice book
38:31 - if Tom is still is seeing this I don't
38:33 - expect him to be seeing this but hi to
38:35 - him I'm just kidding uh here's my one of
38:38 - my big inspiration and Machinery field
38:39 - to get started with his books on machine
38:41 - learning is really really influential
38:43 - and his lectures as well so nice
38:47 - a computer program is safe to learn from
38:49 - experience a with respect to some class
38:52 - of a task T and some performance P if
38:55 - it's performance p on on task t as
38:57 - measured by P improves with experience e
39:00 - if you got this very nice if you know if
39:03 - you don't that's totally all right
39:04 - because I was also the one even in in my
39:07 - middle of the faces I was not able to
39:08 - understand this definition
39:11 - this is by Tom Michelle okay so let's
39:13 - understand this definition by this
39:15 - flowchart which I prepared for you so
39:17 - you have a computer program assume that
39:19 - this is your machine learning machine
39:22 - learning uh whatever you can come to
39:24 - program what is model I've been taking
39:26 - model name right so assume that model is
39:29 - your machines as of now okay machines
39:31 - assume that as of now I'm just taking
39:33 - model just assume that as a machines
39:35 - this is not having so much of jargons
39:38 - so you have this computer program you
39:39 - have this computer program that computer
39:41 - program learns
39:43 - from The Experience e and in this case
39:46 - experience e is your data your computer
39:49 - program they learn or analyze extract
39:52 - patterns from the data for some task
39:55 - the data is for for some tasks right so
39:57 - for example if you have a Spam or ham
39:59 - data which is a collection of
40:01 - information of the spam emails and ham
40:02 - emails so that's the experiences
40:04 - available now the task was to identify
40:06 - spam so that's what for that task
40:09 - and that performance on the task is
40:12 - measured by P so we measured the
40:13 - performance and once we have the
40:15 - performance we improve and that
40:17 - performance is in being improved by
40:20 - adding more data or adding more
40:22 - experiences in that so that's what
40:24 - exactly it says
40:26 - hide highlighting the important terms
40:28 - it's safe to the learn from the
40:29 - experience e with respect to some task T
40:32 - and some performance measured P if the
40:34 - performance on T the task spam and ham
40:36 - measured by performance if the measure
40:39 - by this it improves to the experience if
40:41 - if you increase the experiment
40:43 - experience it will also be it it is
40:45 - going to be improved so this is a
40:47 - definition by Tom Witcher
40:48 - I hope that makes sense another example
40:51 - is Indian house price prediction Indian
40:53 - house price prediction system so here a
40:55 - task is to predict the house prices of
40:57 - Indian states where you want to build
40:59 - any ml system you need one of the key
41:02 - ingredients which is data which is in
41:04 - this case new experience
41:06 - so your trained model which is your
41:08 - computer program on the experience and
41:11 - this is the EXP on the pass data which
41:13 - is experience because password is
41:14 - experience right and here past data is
41:16 - your experience so the performance p is
41:19 - measured how well your model is
41:20 - performing on that particular task and
41:22 - if you add more experience in the the
41:24 - performance will be improved that's it
41:25 - that's definition very easy now I hope
41:28 - that you understood do it for other
41:30 - applications very very nice practice
41:33 - however if you if you want to do the
41:35 - practice all along please enroll in our
41:38 - official ml course as well as well as
41:40 - you can also go um and review some of
41:43 - you know uh
41:46 - um courses available the the three LMS
41:49 - which we ask you to enroll yeah
41:53 - good so what are the steps done by your
41:55 - machine Learning System so that's
41:57 - something what are the what are the
41:58 - steps for machine learning follows to
42:00 - actually do this I know this is early
42:02 - phase to talk about that that's
42:04 - something is super important which I
42:05 - want to have a talk hurt to her talk
42:07 - with my viewers on this however if it's
42:09 - okay if you don't understand it but I
42:10 - won't just want to have a do a talk with
42:12 - a very detailed explanation of
42:14 - everything and then we'll talk about
42:16 - data and then and after that we'll talk
42:19 - about some of the applications and then
42:21 - we'll
42:21 - um talk about a very nice application or
42:25 - the type of machine learning which is
42:27 - supervised learning we'll talk about
42:29 - unsupervised later on uh but as of now
42:32 - I'll talk on supervise only so hey folks
42:35 - welcome to this video in this video what
42:38 - I'm going to do is to talk about some of
42:40 - the steps uh which is done by usually
42:43 - done by Machine Learning System so what
42:45 - is the procedure what is the process for
42:48 - going through uh for for building a a
42:51 - good machine Learning System what is
42:53 - machine Learning System the system in
42:55 - which we give an ability we given
42:57 - ability to a two two two to a machine to
43:00 - be able to to learn from the data which
43:03 - which means to be able to learn from the
43:05 - data or extract patterns from the data
43:07 - and make intelligent prediction so we're
43:09 - going to make a machine a system what is
43:11 - the process of that system where we
43:13 - extract the data we analyze the data and
43:16 - make intelligent predictions according
43:17 - to what it has learned
43:20 - so um now so what the like everything
43:23 - for example while while creation of you
43:26 - know maggies or any sort of short sort
43:28 - of fools you have a processes so just
43:31 - like that like well creation of machine
43:32 - Learning System there's a processes
43:34 - created or recommended processes which I
43:38 - really want to talk to you guys so let's
43:41 - get started actually with that uh so it
43:43 - may happen some of you may not
43:44 - understand the fully thing that's
43:46 - totally all right totally all right if
43:49 - you don't if you don't understand it
43:51 - because I feel like it's it's something
43:53 - which you might focus on things like
43:56 - um definitions and the understanding
43:58 - core and a Crux of what exactly each
44:00 - step is doing as as there is a section
44:04 - in cs01 course so the the the course
44:07 - which I teach on Anton so there's a
44:10 - section called ml Ops where I cover
44:12 - these these things in great detail and
44:14 - talking about several case studies and
44:16 - all so if you're interested in something
44:17 - you know learning in depth and all I
44:19 - recommend uh see ESO one if you want to
44:21 - learn something fully so now coming back
44:24 - to this you have something known as
44:26 - scoping so you have something like that
44:28 - scoping and
44:30 - what that's what does this scoping means
44:32 - scoping is something like uh the first
44:35 - step in your machine Learning System is
44:37 - scope out the problem you're trying to
44:39 - solve scope out the problem you're
44:40 - trying to solve the first step is
44:42 - scoping where you where what you do you
44:45 - understand the problem statement and
44:47 - write down or jot down the key
44:48 - requirements for your ml project so
44:50 - basically whenever you start off with
44:52 - any sort of project or machine learning
44:54 - project machine learning any sort of
44:56 - machine learning project so what you do
44:58 - over there
44:59 - um you simply first of all understand
45:00 - the problem statement right you you
45:02 - understand the for example let's say
45:05 - you're building a Spam or a ham
45:06 - classifier so you have a system they
45:08 - have a machine M you want to build a
45:10 - machine M that that classifies whether
45:12 - that is a spam or ham right so the first
45:15 - step is scoping where you understand
45:16 - what exactly this problem statement is
45:18 - what what sort of data which you have
45:21 - right what sort of data which you have
45:22 - what are the key requirements like
45:25 - requirements what is useful analysts do
45:27 - you understand the problem statement
45:29 - according to given by the state
45:30 - stakeholders and the second step is you
45:33 - understand key requirements like do I
45:36 - require the how many data scientists to
45:38 - work in this what what should be the
45:39 - compute power of this what should the
45:41 - infrastructure required for this
45:43 - Etc so scoping out means getting a brief
45:45 - understanding what this project will be
45:48 - about and creating a brief plan of go to
45:50 - strategy for working on this project
45:52 - then after you have a brief plan and
45:54 - everything now what you do you simply go
45:55 - to the data part where you do where the
45:58 - examples of the steps is now now once
46:00 - you have the data you have to eventually
46:02 - clean it we'll talk about data cleaning
46:04 - and all in one of our project so when
46:06 - you have the data
46:08 - when you have the data you have to clean
46:10 - this data you have to clean the data
46:11 - your processes you have to process the
46:14 - data in a good way and Etc so basically
46:17 - what I'm trying to sell over here what
46:19 - I'm trying to tell over here that given
46:21 - the data you know you you want to you
46:23 - know uh understand it you wanna to uh
46:27 - clean it Etc so I think the and then you
46:30 - have the data then you cross collect and
46:33 - validate the data accordingly right and
46:36 - then you clean the data where uh where
46:38 - you have removing of missing values Etc
46:40 - so I know that this say this will not
46:42 - make sense if you're a beginner so it is
46:44 - something like you have the very bad
46:46 - data you convert that to a good data by
46:48 - data cleaning and Etc you validate the
46:50 - data whether that data is from truthful
46:52 - resources or not whether we can trust
46:54 - the data information or not
46:57 - now we have the data now now what now
47:01 - what we need to do we need to extract
47:02 - patterns from the data and for
47:03 - extracting patterns from the data we
47:05 - have something like modeling
47:06 - not something which actors do on a ramp
47:09 - work but I think on this modeling
47:11 - requires something like you know uh some
47:14 - something like yo your your your shifts
47:17 - your system tries to extract patterns
47:20 - from the clean data which you have right
47:23 - and then once you have the ones who
47:24 - wants once you're done with modeling you
47:27 - have now you're gonna put this you want
47:28 - to put this so that your users can make
47:30 - predictions from this model M so once
47:32 - you have the trained model get it low
47:34 - with learn to extract patterns so now we
47:36 - want to make predictions for it for
47:37 - example if this machine is of spam and
47:39 - ham classifier so I'm going to put this
47:41 - in production so that it is able to
47:43 - classify so that every email can get to
47:45 - this and the model is able to classify
47:47 - in couple of stuffs which is spammer ham
47:49 - so you know put this in production so
47:51 - that it can be used by folks um in real
47:54 - time and also we Monitor and maintain
47:57 - our system we see like how our system is
48:00 - performing what is the like whether a
48:02 - system whether a performance of a system
48:04 - is degrading or what what is the
48:05 - possible errors which are coming what is
48:07 - the etc etc so what is the wrong
48:09 - predictions which are coming Etc so we
48:11 - have to monitor our system as well so
48:13 - there's four steps for any uh this photo
48:15 - four to five steps of recipes of machine
48:17 - learning where you first of all plan out
48:19 - what exactly you need to do by
48:21 - understanding the problem statement and
48:23 - then you get the data and while getting
48:25 - the data you have to validate whether
48:26 - that data is from truthful resources and
48:28 - clean the data in a good data and once
48:30 - you have that good data now you model it
48:32 - which means that you extract you learn
48:35 - to extract patterns from the data and
48:37 - now you have the model which which has
48:38 - the patterns from the data it should be
48:40 - able to successfully use it in the
48:43 - production right and then and then you
48:45 - have to continuously monitor it so these
48:48 - are the steps for your machine learning
48:49 - applications now
48:52 - now let's talk about uh I hope that this
48:55 - this makes sense but now now let's talk
48:58 - about something known as data so what is
49:00 - data so data is info is in information
49:03 - contained in an structured or
49:05 - unstructured format so um so data is set
49:09 - of Records so you you might have seen in
49:11 - your you know machine learning lectures
49:13 - or sorry uh in your class eight or nine
49:15 - lectures of computer science where you
49:18 - might have gone through a data is a set
49:20 - of Records right it's a records or set
49:23 - of information available for the
49:24 - particular uh product so for example you
49:27 - have Amazon listing you have Amazon
49:29 - listing let's say you have you have
49:31 - Mouse right you have Mouse so that Mouse
49:34 - might have several several data right
49:37 - several data like what is the title what
49:40 - is the description what is the price so
49:41 - these are the
49:43 - uh set of records for the particular day
49:46 - Air Products so just just like that data
49:48 - is in information
49:50 - which is in structured and unstructured
49:53 - format okay and
49:56 - um what we'll talk about what is
49:57 - structure and unstructured in a bit but
50:00 - here's here's an example of a data where
50:02 - this is the example of a structured data
50:04 - where it is contained in a table where
50:06 - this data contains the the it is the
50:09 - data of house price prediction where we
50:11 - are given the some of the information
50:13 - about the house prices uh House housing
50:16 - price data where we given the sum of the
50:18 - information about house prices which is
50:20 - first one is floor what is the floor
50:23 - space how many number of rooms in that
50:25 - apartment how what is the lot size is
50:27 - there any other apartment attached to it
50:29 - right Row House Corner House detached so
50:32 - these are the information for a
50:33 - particular house right
50:35 - and this is the price and one thousand
50:38 - dollars right so this is a price for the
50:41 - for for this feature but this feature
50:43 - this is the price uh for that particular
50:45 - house then so we have the second data
50:47 - point this is the this is the this is
50:49 - the record for the second house this is
50:51 - the record for the third house so you
50:52 - got for the um this first house second
50:54 - house third house fourth house fifth
50:56 - house sixth house seven hours six eight
50:58 - and nine house so you have the nine
51:00 - thousand nine houses where for every
51:02 - house is you have your own infos using
51:04 - uh including pricing floor space rooms
51:08 - Etc so these are nothing but uh uh these
51:12 - uh these are nothing but your your data
51:14 - where we actually have something uh uh
51:18 - information contained right
51:23 - cool so now what we do now what we know
51:26 - what what you want to do is talk about
51:30 - a bit of application uh in machine
51:33 - learning right the first application
51:35 - which we really want to talk on is a
51:37 - loan protection system so if it it is an
51:41 - application so loan protection system is
51:42 - an application in which of machine
51:44 - learning so assume that you are in a
51:46 - banking sector you're you're in a
51:48 - banking sector and and Bank provides
51:51 - loans to the customers right so you
51:53 - might have visited loans and also your
51:55 - bank might provide loans and also the
51:57 - customers who require uh rights so now
52:01 - so now you assume that you are working
52:03 - in a bank and then that that bank gives
52:05 - loans to the customers who requires the
52:07 - loans so when providing loans there's a
52:09 - high risk that that that the person may
52:12 - or may not return the loan so this case
52:15 - that the person will not uh return the
52:18 - loan back with interest right it it may
52:22 - return loan back but will it are they
52:25 - going to return back with interests or
52:28 - not so given the bag down information
52:31 - about the particular customer what is
52:33 - the history of them like my salary
52:36 - etc etc we've given those infos we're
52:39 - gonna predict we're going to predict
52:40 - whether we should give loan to that
52:42 - person or not this is one example
52:44 - another example is the spam hand
52:46 - detection system where you promote
52:48 - another example is Spam ham detection
52:50 - system uh where you have where you given
52:53 - an email you know a classifier that that
52:55 - email is a spam or have
52:57 - recommendation engine you might have
52:58 - searches something like this which
53:00 - you're seeing in front of you so this is
53:02 - a recommendation engine uh in Google
53:05 - searches or maybe recommendation engine
53:07 - in your YouTube so these are a couple of
53:09 - applications which you might have
53:10 - already seen I don't intend to you know
53:12 - just just to give you a feel about it so
53:14 - I hope that you understood uh in a nice
53:16 - way about a very good processes of
53:19 - machine learning what I want to achieve
53:21 - in the next set of video is talk about
53:22 - the last thing of this uh to talk about
53:25 - the last thing of this lecture is to
53:27 - talk about supervised learning and
53:30 - we try to learn super supervised
53:32 - learning with a very Visual and very
53:34 - storyline of examples so that anyone
53:36 - even if a class 5 you will be able to
53:38 - understand very easily hello everyone
53:42 - um so now we'll talk about something
53:43 - known as supervised learning and hello
53:47 - everyone hello everyone we'll talk about
53:49 - supervise so now we'll talk about
53:52 - supervised learning and
53:55 - uh we don't want to directly get started
53:58 - with you know definitions and all I want
54:00 - to tell you a very nice storyline a very
54:02 - nice example relating into the Practical
54:04 - stuff
54:05 - so let's get started with that the first
54:07 - is for is to understand the concept so
54:11 - now so now whatever you're saying in
54:13 - front of me is to understand the concept
54:16 - for example for example you have assume
54:20 - that you are studying a mathematics book
54:22 - right so you're studying a mathematics
54:24 - book and there you understand some
54:27 - concept you understand some examples
54:30 - a date so you understand something so so
54:32 - you have a mathematics book you
54:34 - understand you know some concept you but
54:36 - you for example you have a quadratic
54:37 - equation as a chapters you do some
54:39 - examples you want to analyze some
54:40 - formulas you want to analyze some
54:42 - patterns for solving a particular
54:44 - questions and Etc so now you have once
54:46 - have the questions now what you do
54:48 - you're trained with the text to your
54:50 - train so now you understand the concepts
54:52 - you're trained with the from the
54:53 - examples and train for the exercise
54:55 - questions with the help of Supervisors
54:57 - so when doing exercise we are doing
54:58 - examples you're analyzing the patterns
55:00 - so you're getting trained with examples
55:03 - and exercise questions with the help of
55:05 - our supervisors so of course you have a
55:07 - question and this you're also seeing the
55:09 - solution if you're not knowing if you
55:11 - are if you're getting error in that uh
55:14 - question right you're not able to solve
55:15 - that question so you seek help from
55:17 - supervisors or whoever the instructor is
55:19 - out there
55:20 - so by now that our storyline is that you
55:23 - understand some concept you get trained
55:26 - from that examples you do also do the
55:28 - back exercises exercise questions with
55:30 - the help of your instructor and whoever
55:32 - now once you're trained with everything
55:34 - now you sit for an examination you sit
55:36 - for an examination and in that
55:38 - examination you're not you do you you
55:39 - don't have any help of Supervisors you
55:41 - don't have any help of exercise
55:43 - Solutions you don't have any help of
55:45 - answer sheets Etc right so you sit for
55:48 - an examination and if you were trained
55:51 - well then you will be performing well
55:53 - otherwise you'll be not performing well
55:55 - okay so this this was your uh this this
55:58 - will be condition after giving the
56:00 - examination so if you're if you are
56:01 - trained well with the help of your
56:03 - supervisors and everything you will be
56:04 - able to answer it very well so now
56:08 - now you can see that how you are trained
56:11 - you will train the examples questions
56:12 - where you're given question as well as
56:15 - the answer to it so that you can
56:16 - understand the pattern right in exercise
56:19 - questions you have a question as well
56:20 - the help of Super Soap supervisors which
56:22 - is question around so you have a
56:23 - possibility to see the answer right so
56:26 - just like this you the your train with
56:28 - examples and the questions now you sit
56:30 - for an examination you sit and give the
56:32 - examination and then that will predict
56:33 - whether you're going to really
56:36 - um pass the examination or not if you
56:37 - were trained well analyze the question
56:39 - well you will be able to pass and the
56:41 - whole concept is nothing but called
56:44 - supervised learning so let's let's talk
56:47 - about what exactly supervised learning
56:49 - is so in supervised learning you're
56:52 - given the question as well as the answer
56:54 - so that your model learns from it and
56:57 - get trained from that
57:00 - right so in supervised learning you're
57:02 - given the question just like this just
57:04 - like in this case you're in what for for
57:07 - training for this particular child you
57:09 - are given the examples and in that
57:11 - example of the questions as well as the
57:12 - answers right to uh to understand the
57:15 - pattern so in supervised learning as
57:16 - well you're given the question as well
57:18 - as the answer so that your model learns
57:20 - to analyze pattern from it
57:22 - and then whenever you set for an
57:24 - examination which is the new examples
57:25 - you and if you if you were analyzed well
57:29 - or if you're trained well you'll be able
57:31 - to solve it right but now but now what I
57:34 - want to do but now what I would really
57:36 - want to talk is on something which is
57:39 - which you which you might notice is that
57:43 - assume that you have an example over
57:45 - here for house price protection system
57:47 - so you so you have an example of housing
57:50 - price price protection system where you
57:52 - want to predict the house prices based
57:54 - on the given features so you know you
57:56 - know you know you know invest in real
57:58 - estate and you predict the house price
57:59 - in the coming 10 years by giving the
58:01 - following requirements for that
58:03 - reports for that particular house so
58:05 - this house number one is the house
58:06 - number two it's house number three so
58:08 - given this characteristics of the house
58:09 - we are we under predict the price of the
58:12 - house right given the characteristics of
58:14 - this house we want to predict the price
58:16 - of the house characteristics means what
58:18 - is the floor space rooms and all given
58:20 - all of this you want to predict what is
58:21 - the possible price if you have this so
58:24 - so now first of all we have to analyze
58:27 - it right so any for every way before
58:28 - sitting in any examination you have to
58:30 - learn it right so you're in supervised
58:32 - learning we are given the question which
58:34 - is a predict as well as the target which
58:36 - is the solution which from where we have
58:38 - to learn it so your model sees the
58:40 - question okay this is this is the
58:41 - question this is the information given
58:42 - in a question so any provide problems
58:44 - has the wrong information right and they
58:46 - ask us to predict something and also we
58:48 - are given the solutions as well so
58:50 - your model will find the relationship
58:52 - between the price floors your
58:55 - relationship between Solutions and
58:56 - problems and analyze patterns from the
58:58 - data right uh and then and then get
59:01 - trained get get trained so over here
59:04 - this solution the solution is acting as
59:06 - a supervisor for this model for this
59:09 - question so basically uh this will be
59:12 - this is this is this is helping this is
59:14 - helping your model to learn right if you
59:16 - don't have solutions for example if you
59:18 - sit for mathematic examination and you
59:20 - want to prepare for it if you don't have
59:21 - the solutions how will you understand
59:22 - the pattern right so over here you
59:24 - should have the solution you should that
59:26 - that that that is called Target variable
59:28 - right so you give some features based on
59:31 - that the target which is price and then
59:33 - algorithms learns from this and then you
59:35 - give new examples which will be without
59:37 - Target so now so now your model trained
59:40 - on this now
59:43 - you you want your model to set an
59:44 - examination where you only give this
59:47 - information you only give this
59:48 - particular a question and then say Okay
59:51 - predict the house prices based on the
59:52 - given info say for example this now your
59:55 - model will predict this price you don't
59:56 - have to give this because now now now
59:58 - it's our and now something will will
60:01 - introduce to evaluation system where it
60:03 - will check yes or wrong right which
60:05 - which is the which is the task of an
60:08 - examiner or a test Checker but as of now
60:11 - uh once the model is trained we can give
60:13 - only this question we'll have the answer
60:16 - later on where the algorithm tries to
60:18 - predict the answer
60:20 - so supervised learning is the type of
60:22 - machine learning in which models are
60:24 - trained using well labeled training data
60:27 - so labeled means every data is labeled
60:31 - well right so this this particular
60:33 - information is labeled well out here
60:36 - this particular information is labeled
60:38 - well outer right so you have the
60:40 - question as well
60:41 - the solution and machine learning tries
60:43 - to predict the output
60:45 - so here we mean labeled me so here we
60:48 - mean labeled means what does labeled
60:50 - means it means that let's say you're
60:52 - working on spam or ham protection system
60:54 - so for every email you every email for
60:57 - example email number one you have the
60:58 - label related so so that your model
61:01 - learns right so you have the you have
61:03 - the data you have something like this
61:04 - data and for every email you have the
61:06 - weather for the foreign first of all you
61:10 - have to make it learn by the historical
61:12 - data so you have the email number two it
61:14 - is ham even number three is Spam so the
61:16 - the the the label that label order or
61:19 - the thing which are to predict
61:21 - is given over here so this is what the
61:24 - label is in machine learning where it is
61:25 - label means that for every given input
61:28 - feature in this case the email
61:30 - we have to we have the correct we have
61:32 - the correct uh label for that whether
61:34 - that email was that and then we ask a
61:36 - model to learn from this data right and
61:38 - then and then after after more learned
61:40 - we just say we we just only give emails
61:43 - and then tries to predict what will the
61:45 - possible outer from that
61:47 - okay so uh super supervised learning
61:50 - problems can be classified into two ways
61:53 - so first ways is regression and second
61:56 - ways is classification so we'll talk
61:59 - about a classification regression in
62:01 - detail this course focuses on regression
62:04 - but uh if you if you want to learn more
62:06 - about you know complete ml with earning
62:08 - opportunities I recommend CSO one
62:11 - you can go and watch that but uh in
62:13 - regression in regression here here and
62:17 - supervised learning problems can be
62:18 - classified into two set of problems the
62:21 - first problem is the first problem the
62:23 - first problem is that your target
62:25 - variable which you predict is continuous
62:27 - values so assume that applications like
62:30 - predict prediction
62:33 - of house prices so in this house prices
62:35 - you have your target variable your
62:37 - target variable which is the house
62:38 - prices which are continuous which are
62:41 - continuous value so if you don't know
62:42 - what is continuous discrete Etc you can
62:45 - skip to the data fundamentals lecture
62:47 - where I'll be talking about data
62:49 - fundamentals in detail right so over
62:52 - here here a Target variable which you
62:54 - want to predict is continuous so for
62:55 - example the house price your pricing
62:57 - example is continuous right a revenue
62:59 - prediction sales prediction Etc so these
63:02 - are the examples of your regression
63:04 - values right there you're very the the
63:06 - problems where we want to predict are
63:08 - the uh when you predict where the target
63:10 - variable is of continuous value but in
63:12 - classification our Target value isn't
63:14 - discrete whether a person has a cancer
63:15 - yes or no whether the person whether a
63:18 - answer is correct or not yes or no
63:20 - whether this is this true or not yes or
63:23 - no so this is called sort of
63:24 - classification problems in this course
63:26 - we'll only deal with regression problems
63:27 - if you want to know more about CM things
63:30 - in real you can go to classification
63:31 - problems however we'll talk about each
63:34 - things in detail as we go
63:36 - now I think uh this is something which
63:39 - which we are done with whole machine
63:40 - learning now you may think here use
63:41 - various unsupervised learning where is
63:44 - reinforcement learning this course is
63:47 - intended to make you inclined towards
63:49 - the learning the core stuff of it so
63:52 - I'll just give you the idea what is
63:54 - unsupervised learning is and so while
63:56 - learning in which you don't have the
63:58 - solution you only have two you have only
64:00 - have to make this loss any sort of sense
64:03 - of logic from out of the data and then
64:06 - get the prediction anyway so in
64:07 - unsupervised learning you don't have any
64:09 - sort of supervisor in this case in this
64:11 - case in this case you don't get any
64:13 - examples you don't get any questions you
64:15 - just don't get any help of Supervisors
64:16 - you just strictly sell an examination
64:18 - and then you have to learn from it you
64:20 - have to analyze the questions and then
64:21 - make a pattern sort of it right so
64:23 - that's what answer revised which is the
64:25 - most challenging today and most of the
64:26 - work is being done in supervised
64:28 - learning but uh but unsupunt
64:30 - supervisoring is very challenging but uh
64:32 - that's something we'll take a look at
64:34 - later on right that's something which
64:37 - we'll take a look at later on at the end
64:38 - of this course but I want you to stick
64:41 - with as of now is with supervised
64:43 - learning and let's try to learn the core
64:45 - of supervised learning and master it hey
64:47 - everyone welcome to the another module
64:50 - which is linear regression so what we'll
64:52 - try to achieve in this module is we'll
64:55 - try to learn in depth about a linear
64:58 - regression we'll try to understand each
65:01 - and every concept so the table of
65:02 - contents and everything is already
65:03 - introduced to you so I hope that you
65:06 - know what you're going to learn so
65:07 - before starting up this video I would
65:09 - like to talk about the two terms right
65:12 - now is to talk about linear and then
65:14 - talk about regression as this course is
65:17 - only based out of you know machine
65:19 - learning code and linear regression so
65:21 - we'll talk about only these two terms
65:23 - which is linear and regression
65:26 - separately so what is linear linear is
65:29 - something which
65:30 - which is which which is like a straight
65:33 - line like you know so you you might have
65:35 - seen linear graphs where it is just a
65:38 - straight line right and then what is
65:40 - regression regression is or a sub
65:44 - problem of your machine learning you
65:45 - know it's it's it's it's it's a sub
65:47 - problem of a supervised machine learning
65:49 - uh machine learning task where your
65:52 - output variable is by output variable
65:54 - where your output variable of your so
65:57 - the output variable or the Target baby
65:59 - build which you want to predict is
66:01 - continuous what is continuous we have
66:03 - already seen in our previous lecture on
66:05 - data fundamentals so continuous means
66:08 - the height of the person if you're
66:09 - predict the height of a person you
66:10 - predict the stock or a stock of a stock
66:13 - of the particular company or if you want
66:14 - to predict anything which is which is
66:16 - which is uncountable or I would say
66:18 - non-finite or
66:21 - something which is continuous like
66:22 - height a uh height age Etc but there's
66:26 - this discrete where if your target
66:28 - variable discrete where you have a
66:30 - finite number of possible outcomes over
66:32 - there then that's a classification
66:33 - problem so we are going to approve
66:35 - approach regression problem outer
66:38 - regression problem out here using one of
66:40 - the one of the algorithms called line
66:42 - iteration so now one of the now you
66:44 - might have seen in our definition of
66:46 - machine learning that we use
66:48 - um that that we how we how we extract
66:51 - patterns from the data so this is one of
66:53 - the ways we are going to study one of
66:55 - the ways for extracting patterns from
66:58 - our data and then this sets a very nice
67:00 - base so once you have this you you'll be
67:03 - easily able to learn logistic regression
67:05 - and other things but if you fully want
67:07 - to learn I suggest cs01 cores for you
67:10 - which is the best for you maybe
67:12 - yes so let's get started eventually and
67:15 - let's try to talk about what are the
67:16 - things which is needed out here cool so
67:19 - over here you're seeing something known
67:21 - as
67:22 - um we use ml for estimating or
67:24 - establishing relationships between two
67:27 - variables so that's something that's
67:28 - very confusing so I would like to take a
67:30 - time to make you understand what exactly
67:31 - I mean by that so over here over here
67:35 - what are what I exactly mean by that is
67:38 - that assume that you have
67:40 - um you have that you're working on
67:42 - something like let's let's take an
67:43 - example that you are working on cancer
67:47 - you're you're not you know predict you
67:49 - know predict whether the person has a
67:51 - cancer or not you know right so so you
67:53 - know predict whether person has a cancer
67:54 - or not so you're given them you're given
67:56 - some of the information about the person
67:57 - so you're given about the the B BMI info
68:01 - person which is blood which is BMI and
68:03 - then you're given the weight of the
68:04 - person you're given the height of the
68:06 - person so you're given these three
68:08 - features these three features you're
68:09 - gonna predict
68:11 - he ought to predict whether the person
68:13 - has a cancer or not I know this these
68:15 - features are not enough but bass assume
68:17 - that based on this hypothetical
68:19 - situation based on these given features
68:21 - you're going to predict whether that
68:22 - person has a cancer or not
68:25 - okay so how we're going to approach this
68:27 - and how exactly we are going to uh take
68:29 - up from here so one thing which I just
68:31 - want to tell you is how how we are going
68:34 - to approach this like why how can we
68:37 - relate this to establishing
68:38 - relationships so you have these feature
68:40 - you want to predict whether the person
68:42 - has a cancer or not so over here you
68:44 - want to establish a relationship between
68:46 - every feature you have so you know to be
68:48 - honest you establish a relationship
68:50 - between these these two things whether
68:52 - there is any relationship between BMI
68:54 - and cancer whether BMI has any effect on
68:58 - the person is having cancer so if BMI is
69:00 - high whether the person is having cancer
69:01 - ba BMI is low the person is having a
69:04 - cancer so we have to you want to
69:05 - identify the relationships out of it
69:07 - identify the relationships or something
69:09 - like I identify any sort of you know and
69:12 - any sort of
69:14 - um uh you know the
69:17 - any sort of relationship between uh over
69:21 - there
69:21 - out there so you know establish the
69:24 - relationships in
69:25 - um in various like you know the BMI and
69:29 - the cancer or maybe establish a
69:30 - relationship between feature number two
69:32 - which is weight of the person and the
69:33 - cancer so does weight of the person
69:36 - affect whether the person has a cancer
69:38 - or not does does this particular info
69:41 - affect whether a person is going to have
69:43 - a cancer so over here what exactly we we
69:46 - use ml for exactly identifying that
69:49 - exactly the same thing we use ml for
69:51 - establishing the relationship with this
69:54 - particular feature is going to affect my
69:57 - output variable in this case where my
69:59 - age or weight or BMI will affect whether
70:02 - I have a cancer or not okay individually
70:05 - like this feature the effects or not
70:07 - this feature if it's if yes then how by
70:11 - what units Etc so these these questions
70:13 - are answered so we'll take a very nice
70:15 - problem statement to make you understand
70:17 - this particular statement
70:20 - which would be easy for you so let's
70:22 - take an example your marketing
70:23 - strategist a data scientist for a
70:25 - company and basically this is what the
70:26 - data scientists do eventually have the
70:28 - marketing team would have the decision
70:29 - team to take certain decisions or
70:31 - marketing team to take some certain
70:32 - marketing steps so you prepare a
70:35 - marketing plan for a car company so you
70:38 - have a car company and then you prepare
70:39 - the marketing plan how does marketing
70:41 - plan should be there for car company or
70:43 - to help the marketing strategies out
70:45 - there with your data driven decisions
70:48 - so manifold of the first is a
70:50 - manufacturer of the car so so you're
70:53 - given these particular infos about the
70:55 - car the features about the car right so
70:58 - you saw you're given these one two three
71:00 - four four information about the car four
71:03 - features of the cars and based on this
71:06 - how to make a marketing plan okay so
71:08 - your make a marketing plan so in what
71:11 - way the marketing strategist asked you
71:13 - as a data scientist asked you that tell
71:16 - me which info we should highlight out of
71:19 - all these infos in our main ad so when
71:21 - you make our when we make our main ad
71:23 - which info out of all of this we should
71:26 - highlight so that we make maximum of
71:29 - sales for that car understanding what
71:31 - I'm trying to say what I'm trying to say
71:33 - that there is a marketing team sitting
71:34 - out there you're a data scientist out
71:36 - there they ask you that out of all these
71:39 - features can you recommend me which info
71:42 - which info I should I should take and
71:46 - show it in ad so that I have a most of
71:48 - the market uh most of the sales okay
71:52 - and just I don't want the prediction I
71:54 - also want to know the following
71:56 - questions so marketing's team is setting
71:59 - and asking asking this question so what
72:00 - are those questions question says is is
72:02 - there any relationship between N4 that
72:05 - particular information the car so is
72:07 - there any relationship between the
72:08 - manufacturer of the cars and the sales
72:10 - so of course as you think that let's
72:14 - assume that manufacturer is BMW
72:18 - but but it may have like there might be
72:21 - a very nice relation some it may happen
72:23 - that the brand manufacturer of the car
72:25 - is high but the sales might be low
72:27 - because the brand is super highly
72:28 - expensive right but if you have
72:30 - something wagonal or you know low budget
72:32 - the brand is Tata you know Tata Motors
72:35 - and all so they produce less expensive
72:37 - so the sales will be also very high so
72:39 - the manufacturer of the so we have to
72:41 - understand is there any relationship if
72:42 - yes then what a relationship is
72:44 - available between both of them and that
72:46 - answer is how strong is your
72:48 - relationship between that particular
72:50 - manufacturer and the sales you do for
72:53 - every model of the car engine size
72:54 - horsepower right so what is the
72:56 - relationship between a horsepower and
72:58 - the sales so it horsepower is high what
73:00 - is the sales if low what is the sales
73:01 - right if if engine size is highs size is
73:05 - highs what is the sales if the inner
73:07 - sizes though what is the sales
73:09 - Etc
73:10 - which info contributes to the sale so
73:12 - which information sometimes it may
73:14 - happen that this that that any
73:16 - particular information may not
73:17 - contribute to may not have any effect
73:19 - that does not really make sense just
73:21 - like in prison pigeon example which is
73:23 - senseless
73:24 - right how accurately can we estimate the
73:26 - effect how accurate we are in estimating
73:28 - the effect of each individual features
73:31 - each individual features on our output
73:33 - variable y
73:36 - and is the relationship linear between
73:38 - both of them and what exactly we say
73:40 - that the relationship is linear which
73:43 - means do they follow the linearity or do
73:46 - they follow the the straight line
73:48 - convention so what is linear things
73:50 - which are you know in straight line or
73:52 - something in any in any straight line it
73:54 - should be linear it should not be like
73:56 - this right it should be uh linear so you
73:59 - might have studied new lower classes
74:00 - about that so I hope that this gives you
74:03 - a very nice sense about you know uh we
74:05 - use ml for estimating relationships in
74:07 - the next video what I'm what I'll try to
74:09 - achieve in the next video what I'm
74:11 - trying to achieve is I don't know what I
74:13 - want to do is to work about geometrical
74:16 - understanding is to work about
74:17 - geometrical understanding geometrical
74:20 - understanding right sort of work about
74:22 - geometrical understanding of our
74:24 - regression so we'll actually go ahead
74:26 - and then talk about how we can estimate
74:28 - the relationship between features and
74:30 - outcome variables in easy way so let's
74:32 - go ahead and talk about that
74:36 - so now we'll talk about something which
74:40 - is now now once we and now we have
74:42 - studied about you know he this is how we
74:45 - asked this is what we have to do this is
74:47 - what ml is about estimating
74:48 - relationships so let's start talking
74:50 - about how we estimate relationships
74:53 - between variables
74:55 - right so let's let's get started and
74:58 - actually talk about that right
75:00 - um so over here what uh so now uh before
75:03 - going on that let's talk about what what
75:06 - it What does it means to say that data
75:08 - is linear right data is linear what is
75:12 - it what does it mean so data that can be
75:15 - represented on a live line graph right
75:17 - it's a data which can be represented on
75:19 - a line graph and there's a clear
75:21 - relationship between two variables
75:23 - between two variables is a clear
75:25 - relationship between two variables that
75:28 - the that that the graph can be shown on
75:31 - a straight line so there's a clear
75:33 - relationship with you which which you're
75:34 - saying and one unit one unit increase in
75:37 - X is showing a constant increase in y
75:40 - right that's what the data is said to be
75:43 - linear you might have studied in lower
75:45 - classes Just For Those Who would know
75:46 - who who just want to recap I just told
75:48 - about that
75:49 - this is an example of a linear data this
75:52 - is an example for linear data this is an
75:54 - example of a linear ignore the lines as
75:56 - of now okay ignore the lines as of now
75:58 - just go just go about the just go about
76:00 - the data which you have out here just
76:02 - just go about the data so you see that
76:04 - the data is linear if the data would
76:07 - have something like this you know very
76:08 - uh very random you know there's not
76:11 - showing any clear relationship there's
76:13 - not doing any clear relationships
76:15 - between between variables then it might
76:18 - have not been this is called a nonlinear
76:20 - so data is linear over here
76:23 - and over here over here assume that this
76:26 - is your data assume that this is your
76:28 - data so now assume that this is your
76:31 - data we're on x axis we're on x-axis you
76:35 - have something like the number of years
76:37 - of experience so you're going to build a
76:39 - system so uh let's let's formulate a
76:41 - problem right now you want to build a
76:43 - system you want to build a system that
76:45 - predicts the salary of a person given
76:47 - the given the
76:49 - um
76:50 - years of experience of a person so given
76:53 - the the years of experience of a person
76:55 - you know predict what to what will be
76:57 - the possible salary so if the person has
76:59 - a two years experience you have such
77:01 - such number of salary so you are given a
77:03 - data in a CSE and a CSV file load you
77:05 - give give given a data in an Excel sheet
77:07 - and in that data and in that data you
77:10 - have something like this uh record and
77:13 - your you have one information available
77:15 - which is the years of experience and
77:18 - assume three and there is the label a
77:20 - Target variable we're going to predict
77:21 - the salary which is in let's say five
77:23 - right which is in 5K dollars five k five
77:26 - thousand dollars to ten thousand dollars
77:27 - whatever per year
77:29 - according to Indian market
77:32 - um so over here you have years
77:33 - experience and the salary of that person
77:37 - right and this isn't a 5k okay so if
77:40 - there's five over here that means five
77:42 - thousand okay so uh and then you have
77:44 - the data and then you plot this data
77:46 - over here now you plot this particular
77:48 - data over here because you have only X
77:50 - variable you have only X variable and
77:53 - then you have y so you know you're given
77:55 - X you're gonna predict y out here right
77:58 - you're given X you're given X you're
78:00 - gonna predict why right so I just want
78:02 - to talk about one of the thing which is
78:04 - super important right now is is this
78:07 - something known as functions
78:08 - conscription to functions so in
78:10 - functions what we do in functions what
78:12 - what we do in functions what what we do
78:14 - say for example that this is a function
78:16 - which X squares the number which square
78:18 - is the number so this function takes the
78:20 - input value of x take the input value of
78:22 - x and outputs y
78:25 - and outputs Y which is a square of X
78:28 - right so when you plot this function
78:30 - you're going to plot this function this
78:32 - looks like a parabola this this this
78:34 - looks like a pattern this this is a
78:36 - parabola okay so X and then f of x then
78:39 - it takes the it takes the input value
78:41 - does some processing and Returns the
78:43 - square of it Returns the square of it
78:45 - which is a square right so in this case
78:48 - we want to tell we're gonna We want to
78:50 - build a function f so we're going to
78:51 - build a function f we're going to build
78:52 - a function f we're going to build a
78:54 - function f that takes up your Builder
78:57 - function f that takes up years to
78:59 - experience of a person that takes up
79:00 - years to experience of person and map
79:03 - this which we call this X in this case
79:05 - the input value the the thing which
79:06 - you're going to input to the model like
79:08 - give this input for example in cancer
79:11 - prediction we will give the input like
79:13 - BMI weight agent to give input and then
79:15 - as output we're gonna We want to predict
79:17 - what will the salary of the person
79:19 - that's the output out there that's the
79:22 - output out there which is the salary of
79:24 - a person so you this this does something
79:27 - so this does something with this input
79:28 - value and then returns you the salary so
79:31 - now so now we're gonna build a function
79:33 - now I'm going to build this black box
79:35 - thing this this this this thing out here
79:37 - this particular whatever processing that
79:39 - does with the user experience so we're
79:41 - gonna build that you know for for
79:43 - building that we wanted to learn it's
79:45 - the algorithm called linear regression
79:47 - where it helps us to building that
79:49 - particular processing that takes up your
79:51 - input feature X and does some processing
79:53 - and returns some values file
79:55 - okay so so how do we learn that function
79:58 - f that takes the value y um es expense
80:01 - and Returns the Y so let's start off
80:03 - with the geometrical understanding so
80:04 - assume that you have the data plotted
80:06 - over here on x-axis you have years
80:08 - experience and one y-axis you have uh
80:11 - salary of the person so this is the data
80:13 - which you plotted this is the data which
80:14 - you plotted I hope so that you know data
80:16 - plotting at least given the tables
80:18 - you're given the tables out there you
80:20 - should you should be able to uh plot
80:23 - data out here very easily right so when
80:25 - you plot this data out here when you
80:27 - when you plot this data out here we've
80:29 - brought this data out here now you look
80:31 - the ignore the black line as of now you
80:33 - plot this data now now your what what
80:37 - you do you find a straight line you find
80:40 - a straight line that best fits the data
80:43 - that best fits the data you try to try
80:46 - to find the line that best fits the data
80:49 - so why do we find the line that best
80:51 - fits the data right that's a good
80:53 - question right so assume that assume
80:55 - that you wanna you have
80:58 - um
80:58 - four years of experience your four years
81:01 - experience so what what will be the
81:03 - possible uh salary of yours so if you
81:05 - have the fourths four years of
81:06 - experience forced to experience the
81:09 - actual value states that the actual
81:10 - value states that you will be having
81:13 - somewhat like you know
81:15 - um I think eight eight thousand dollars
81:18 - for a new right but your model so so
81:21 - basically the intersection from X and
81:25 - the in the intersection point so
81:27 - basically the intersection point is this
81:29 - is this point so this is the
81:30 - intersection point and this intersection
81:33 - point is your is your is your model
81:36 - approximation model approximation so
81:39 - let's talk about what exactly this model
81:41 - approximation means model like
81:43 - proximation is something like your model
81:46 - approximated so this is there is a there
81:49 - as as we say for example assume that you
81:52 - have you have you have your examination
81:54 - and is the examination similar questions
81:56 - comes or exactly same question comes
81:58 - right so it might happen that your
82:01 - approach might be different from the
82:02 - books approach
82:03 - it may be similar question it may be
82:05 - similar question so you answer
82:07 - differently right so you over here don't
82:09 - exactly answer the same thing you
82:10 - written differently for example in
82:12 - science exam in science exam you don't
82:14 - write exact exact the same question
82:16 - exact the same answer there is some of
82:17 - course the your you have a proximate
82:20 - answer right not exactly similar to the
82:22 - original answer notice actually or
82:24 - similar to the bookish answer but
82:26 - similar to that so if someone asks what
82:27 - is photosynthesis you might ask you you
82:30 - might write your answer in different
82:31 - language but the original may be
82:32 - different so there's there should be
82:34 - some difference between both of the
82:35 - answers okay so in the same way if
82:37 - someone says that someone asked a
82:39 - question what is the sale what is the
82:42 - salary of person with the US experience
82:44 - for where they ask they give the years a
82:46 - four
82:46 - we have some processing done and then it
82:49 - results in some values
82:51 - okay and as of now we are trying to
82:54 - understand the geometrically how it
82:55 - should work if someone asks what is the
82:57 - what is this idea if if the person has
82:59 - the four years of experience that says
83:02 - that if if if I ask what is this year's
83:05 - experience when when when when the years
83:07 - experience is four
83:09 - it says that the the actual answer is
83:12 - this the when actual answer is this
83:14 - actual answer is this with original but
83:16 - there is the force the difference but
83:18 - there's of course the but the actual
83:20 - model approximated this this straight
83:22 - line as of now this is straight line so
83:25 - as this is a straight line so we found
83:27 - the straight line that best fits the
83:29 - data okay so we found a straight line
83:31 - and then we go gone and see okay this is
83:34 - the point of intersection and this is
83:36 - your model prediction this is your
83:38 - function approximated value not exactly
83:40 - same but similar to that the small
83:42 - difference between bulbots and then
83:43 - right
83:44 - okay so now you may may have served
83:46 - several questions please don't worry
83:48 - we'll tackle each and every question in
83:50 - detail
83:51 - okay
83:52 - so let's start tackling each and every
83:54 - question in detail now over here what
83:56 - you've seen is that assume that this
83:58 - particular information and this
84:00 - particular information so there is of
84:01 - course the difference there's a model
84:02 - approximation and your actual actual
84:04 - value so what is the difference between
84:06 - model approximation to Natural value as
84:08 - I again told in Science examination for
84:10 - a particular question there might be
84:12 - different answer but concept is same but
84:15 - different answer right in their own
84:16 - words in bookish language but you might
84:17 - have different answers in any other
84:19 - language so there's of course a
84:21 - difference so if the difference is high
84:22 - that means the concept is not same so it
84:25 - will so your your examiner will match
84:27 - your answer with the original answer and
84:29 - if the difference is similar then that's
84:31 - good but if the difference is high this
84:33 - this the the dissimilarity is high then
84:36 - the answer is wrong because you might
84:37 - have written in when you when when you
84:40 - were told to write the photosynthesis
84:41 - you you came and write written something
84:44 - like
84:45 - um respiration right so that's that that
84:48 - will be wrong right so I think that's
84:51 - that's a very very good example State
84:53 - over here so now your the actual answer
84:55 - though someone asked about
84:56 - photosynthesis so actual answer is the
84:58 - bookish answer which is so correct which
85:00 - is 100 correct we call that thing as a
85:02 - ground truth and then you you return
85:04 - that is a student approximated answer
85:06 - that that student might have written
85:08 - some answer so we we match both of them
85:10 - but take out the difference how how how
85:13 - similar they are if the similarity is if
85:16 - the Sim if if the similarity is if this
85:19 - if the similarity is less that means
85:22 - that uh that the answer is correct but
85:24 - if the simulator is high that means oh
85:26 - that something is away right that
85:28 - something is wrong that they have
85:30 - written as respiration instead of
85:31 - photosynthesis uh chemistry is the
85:34 - something never mind so this is what the
85:37 - difference between model approximation
85:38 - so in this case only your model says
85:40 - that for this much for this much of
85:42 - experience this is the particular uh
85:44 - value of this particular salary but your
85:46 - actual value states that this but
85:47 - there's a difference so higher the
85:49 - difference
85:52 - but why did I say that best fit line but
85:55 - why did I say this specific line so why
85:57 - did I say this best fit line because for
85:59 - example assume that your line is like
86:01 - this which does not best fits the data
86:03 - which does not fits the data well so now
86:06 - if you have four years to experience if
86:08 - your four years of experience you have
86:10 - four years of experience
86:12 - the model approximate is that
86:14 - intersection point which is nothing but
86:16 - which is nothing but four four thousand
86:19 - dollar but actually but actually it was
86:21 - six rights so the difference is high so
86:24 - the difference is super high right but
86:26 - but actually if you see that this is
86:29 - your actual value so this is your actual
86:30 - value right this is your this is your
86:32 - correct answer but this is your model
86:34 - approximated value this is a moral
86:36 - approximate value right so what there's
86:39 - a difference between so over here
86:41 - there's a small difference but over here
86:43 - it is a bit large difference so that's
86:45 - why we want to find the line which best
86:48 - fits the data so now for example
86:50 - so now for this so now for example
86:52 - assume that you go to this line or or
86:55 - the and this is the best fit line which
86:57 - is out there which is in over here but
86:58 - assume this this line we have four years
87:00 - experience we have four years of
87:02 - experience this is but actually the the
87:04 - correct the correct uh the correct is
87:07 - this one so of course there's difference
87:08 - but this particular but this particular
87:11 - line which best fits the data right
87:13 - which best fits the data uh where we are
87:16 - using that uh to make prediction right
87:18 - so now I think you gotta you got a way
87:21 - to
87:22 - um approximate your values you got a way
87:25 - to approximate your values now now let's
87:28 - worry about let's talk about
87:30 - um
87:31 - in detail okay let's talk about in
87:33 - detail what exactly I'm trying to tell
87:35 - is why exactly how exactly we got to our
87:38 - answer so what our problem was our
87:41 - problem was that we wanted to extract
87:43 - patterns from the data basically going
87:44 - to learn from the data and make
87:46 - intelligent predictions so we're gonna
87:48 - find the best fit line that means
87:49 - learning that find that best fits the
87:51 - data for example you want to learn your
87:54 - you know to train your mind that best
87:55 - fits the uh
87:57 - the concept the best fits the concept
88:00 - and around that concept you utilize that
88:02 - concept and that's called hypothesis you
88:04 - utilize that step to further make
88:07 - prediction to further make predictions
88:09 - or to further solve the question so in
88:10 - that end just in this way only it first
88:13 - of all it first of all makes the best
88:15 - fits the data best fits the data by a
88:17 - straight line it should be linear right
88:19 - it should be linear in a straight line
88:21 - and then it best fits the data and then
88:24 - we can use this best fit to see okay if
88:27 - we have currently we have only one input
88:29 - value if there is more then there's a
88:30 - several things but yeah
88:34 - so given that you can just go go and
88:36 - then this is the point of intersection
88:38 - so this is your y value which is in this
88:41 - case the Y values on Y axis so we simply
88:43 - predict that I hope that it makes sense
88:45 - if it does not please review the lecture
88:47 - again and again and if you still don't
88:49 - please go and review the concepts of
88:51 - Class 8 and 9.
88:53 - okay uh why to find the best fit line
88:56 - that's a good question about why exactly
88:57 - we want to find the best bit line
89:00 - um so I have given a very very nice
89:02 - example out here which I've just
89:04 - described to you
89:06 - so now uh you might you might have
89:09 - question on how do we find this best
89:14 - straight line how do we find this best
89:15 - fit line so let's try let's go ahead and
89:19 - let's try to find this
89:21 - um how exactly we go ahead and find the
89:23 - best fit line so so now you might have
89:26 - question okay now we have the data like
89:28 - how can we find the best fit like the
89:30 - first the first solution in your mind
89:32 - might come that we can plot our data on
89:34 - X and Y excess and then use our drawing
89:37 - skills to draw best fit lines so you can
89:39 - simply say and draw like this and then
89:41 - draw best fit line but the major defect
89:44 - is we cannot really make predictions
89:46 - from that so how can you make exact
89:47 - predictions from it tell me this won't
89:49 - defect how can you tell give me the
89:50 - exact protection you cannot give me the
89:52 - possible exact prediction you can just
89:53 - still approach I've just told over here
89:54 - proximated right not exactly the same
89:57 - what model approximate just approach are
89:59 - you sure approximated that's the first
90:02 - effect and the second effect is what if
90:04 - if you have more than one features in
90:06 - information for example currently it's X
90:09 - and Y only where you have years
90:10 - experience to predict the salary but
90:12 - what if your ears experience the
90:13 - designation the skills the ratings Etc
90:16 - so for Access four input values you want
90:19 - to predict one salary so this is a
90:21 - multi-dimensional thing you cannot
90:22 - really plot a straight line that's the
90:24 - first that's the first solution cut
90:25 - where you cannot use the joining skills
90:27 - to draw a straight line which the best
90:29 - fits the data so how can we get this a
90:32 - straight line so the second solution is
90:35 - that as of now let's go ahead and talk
90:37 - about only one only one thing which is
90:40 - which is you know you you you're only
90:42 - given only one information which is X
90:44 - which is the year's experience when you
90:46 - predict the Salvage that is only one
90:49 - so how can how can we uh do this so
90:52 - second solution might arise that you
90:55 - might have seen this in your early
90:56 - classes Y is equals to MX plus b
90:59 - why is it this is the equation for any
91:01 - straight line so y equals to MX plus b
91:04 - is the mathematical equation for a
91:06 - straight line where M indicates the
91:09 - slope of the line and B B indicates the
91:12 - intercept of that line slope means how
91:15 - much y changes when X changes and
91:18 - intercept B which is the y-intercept
91:21 - which is the y-intercept where your line
91:23 - cuts at y right if you don't don't know
91:26 - about the concept don't worry I'll just
91:27 - speak I'll just recapsulate uh a little
91:30 - a little bit on this so what exactly
91:33 - slope in this example so slope in this
91:35 - example states that how much my salary
91:37 - will change
91:39 - how much my salary will change if my
91:43 - years of experience will change that's
91:45 - the slope that's the slope which says
91:47 - how much your y changes the salary
91:49 - changes when X changes that's a slope so
91:52 - M over here is your slope and M is also
91:54 - the coefficient of your x and x in this
91:57 - case is a years experience plus the Y
92:00 - intercept where at exactly your y
92:02 - intersects and y y intercept is one of
92:05 - the major thing because we will talk
92:07 - about bias term later on but over here
92:10 - if you have this Y intercept over here
92:13 - if you've then white recept also plays a
92:15 - very nicer so over here this is the wine
92:17 - this is a varied white cuts on the
92:18 - y-axis
92:20 - so now there's one interpretation which
92:23 - we can get from this that if we are able
92:25 - so this particular line is fully
92:27 - dependent on this in these two
92:30 - parameters only these two parameters can
92:32 - change this line right only slope
92:34 - because X will be anything right but but
92:37 - m and b determines how this line should
92:41 - be
92:42 - the slope and the volume step so if Y
92:44 - intercept is different it should be like
92:46 - this it should be like this and slope is
92:47 - different it should be like different
92:49 - different shapes so that your your
92:52 - straight line your best bet line depends
92:54 - on the slope and the B which is the bias
92:57 - though so a straight line is dependent
93:00 - upon slope and the Y intercept so if you
93:03 - are able to find if you are able to find
93:06 - the slope and the best interferent in
93:09 - intercept that best fits the data we got
93:12 - a new line so very interesting very
93:15 - interesting uh um an analogy
93:18 - is why I am saying that they are
93:20 - dependent why I am saying that we are
93:22 - depend that our straight line is
93:24 - dependent for example so you have you
93:26 - have you have this data where I plotted
93:27 - the very small small amount of data and
93:29 - in this example uh and in this example
93:33 - assume that we have this this particular
93:34 - assume that we have three three
93:36 - different best bits lines let's go with
93:38 - the first one let's go to the first one
93:39 - this over here our slope is different
93:42 - and y intercept is different that's why
93:44 - R it is not perfectly fitting our data
93:46 - so now we tweaked in a way that we
93:48 - changed we changed the slope and The
93:50 - Intercept we got to this it okay okay
93:52 - fits the data
93:54 - but over here which again changed and
93:56 - seen that it it is also it fits the data
93:58 - very well right so so you're all use a
94:02 - straight line is dependent upon m and b
94:04 - so if you're able to find the best slope
94:07 - and the best y-intercept for the
94:09 - particular data which you have you will
94:11 - be able to to
94:13 - um
94:14 - to have the very to to have the very
94:17 - good line to have the very good line
94:19 - right that's a that's a nice analogy I
94:22 - hope so that you're getting whatever
94:23 - time you tell is I'm just trying to tell
94:26 - you that you want to get the best M
94:30 - which is the slope
94:31 - and the best bee because it determines
94:35 - your values
94:37 - it determines your X values
94:41 - right so I hope that this this gives a
94:44 - sense that our so if you're able to find
94:46 - m and b we will be easily able to get
94:47 - the best fed line I hope that this makes
94:49 - sense
94:50 - okay
94:52 - I don't need to again repeat it again
94:54 - and again so this is a very dummy
94:55 - example which which which you're seeing
94:57 - in front of me uh is about is is about
95:01 - horsepower and sales so so you're you're
95:04 - given the horsepower of a car you're
95:07 - given only one information as of now as
95:08 - I said will only go with one information
95:10 - so you're given the horsepower of the
95:12 - car the RSI based on horsepower what is
95:14 - the sales for the car right you know you
95:17 - are identify that so to find the best
95:19 - fit line that best fits this data so if
95:21 - you plot this data it should look
95:22 - something like this so if you plot the
95:24 - data we have to best fit data so
95:26 - sometimes you don't need to really draw
95:28 - make your grass you to you know draw you
95:30 - know find the tuition you'll find a
95:32 - straight line or linear equation
95:34 - um not linear equation yeah so the the
95:36 - straightly straight line equation that
95:38 - best fits the data okay
95:41 - so your build is function f your build
95:43 - this function app that takes in
95:44 - horsepower
95:46 - and then and then thus and then multiply
95:49 - and then you have multi and then you
95:50 - have the straight line equation which is
95:52 - used for use which is used for making uh
95:54 - predictions so we need to build a
95:55 - function f that Maps your X which is the
95:58 - horsepower to the sales establish a
96:00 - relationship between both of them so
96:03 - here we are multiplying with M with X so
96:05 - that we can say that m is will will come
96:08 - will come to that but over here we want
96:10 - to we want to give the function
96:11 - horsepower and multiply a horsepower
96:13 - with a slope which is how much how much
96:16 - how much how much uh why how much house
96:19 - horsepower how much sales changes when
96:22 - horsepower changes plus Y which is your
96:25 - y-intercept which is y-intercept where
96:27 - it intersects on Y axis so now you might
96:30 - have questioned that how the why we
96:32 - multiply and how does this affect the
96:34 - output so when we multiply what effects
96:36 - does it goes on output variable what
96:38 - effect is Y intercept gives to the
96:40 - output variable that's something we
96:41 - should talk about in interpretation of
96:43 - our coefficients later on but as of now
96:45 - you can get that that this is the
96:46 - equation where you put in the values of
96:48 - X you'll get the particular value
96:50 - okay you can see over here that let's
96:52 - let's assume your slope is three and
96:54 - intercept is 4 which is dummy as of now
96:56 - if you take our function so now this is
96:57 - a function like this now you have built
96:59 - your function I've built your function
97:00 - you built your function built your
97:03 - function so you simply give the value of
97:04 - x and then get your answer
97:07 - and the regular prediction which is
97:08 - number of sales which you'll get
97:10 - okay
97:13 - so um now you might have service now now
97:16 - you might have uh several question right
97:19 - now like how can we find our best mnb
97:22 - like currently we have just taken a
97:24 - dummy example so how can we find best
97:26 - mnb what exactly this mnb contributes to
97:29 - the uh sales what why exactly we use
97:32 - that first of all at first place why
97:34 - exactly we use it Etc that that can be
97:37 - will that will be answered in later
97:38 - phases as you go but as of now what I
97:41 - just want to tell you you have got to
97:42 - know the very nice geometrical
97:44 - understanding of linear regression and I
97:45 - hope that you will really uh utilize
97:47 - this to recapsulate the concept so if
97:49 - you want this nose you can get that from
97:51 - my website you can also enroll in the
97:54 - course which I'm teaching in fully ml
97:55 - Pro which is from teaching from baby
97:58 - Basics to earning opportunities
98:00 - Etc so I hope that you will enroll in
98:02 - that let's catch up in the next video
98:04 - so folks we are going to continue our
98:07 - journey on regression analysis so in the
98:10 - previous lecture we had a talk on the
98:12 - some example and we have shown that how
98:15 - each like we have m and b and how this m
98:19 - how horsepower has its own weight and
98:21 - then we have an intercept we have talked
98:23 - about this if you haven't seen my
98:24 - previous video please go and see about
98:26 - that so our main idea was how can we
98:29 - find this main M and this intercept
98:32 - which we usually call as B okay so my
98:35 - question was how can we find how to find
98:37 - these two terms because your best fit
98:39 - line depends on this m and b only right
98:43 - so basically that that was the basic
98:45 - question that your best foot line
98:47 - depends on this so for if you want to
98:49 - get the good fit line then you need to
98:51 - find this m and intercept which is a
98:53 - slope of the particular variable how
98:56 - horsepower and The Intercept of that
98:59 - particular variable so I hope that that
99:00 - makes sense now what what you exactly
99:03 - going to do we're going to study about
99:04 - how can we find best m and b for your
99:08 - data which is the eventually in other
99:11 - words best Straight best fit straight
99:13 - line in your data right so what was the
99:16 - POS what is the sum of the possible way
99:17 - which we can think of so over here what
99:20 - is one of the possible way is to hit and
99:22 - trial different different values of m
99:24 - and b and see what works well right so
99:27 - for example for example in the in the in
99:30 - this example you understand you're gonna
99:32 - use the you know build a function f that
99:35 - takes the horsepower as an input that
99:37 - takes the horsepower as an input and
99:38 - predicts the number of a sales so if you
99:40 - want to promote your car with that
99:42 - horsepower what is the possible number
99:44 - of us is where you're gonna get right so
99:46 - you're going to build a function f that
99:48 - establishes the relationship between
99:49 - your horsepower of a car and the sales
99:52 - right so you know predict how how much
99:55 - sales would be done if the if we
99:57 - actually use the horsepower as an or how
100:00 - much sales are based on horsepower what
100:02 - is the relationship between them right
100:03 - that is the one question out there but
100:06 - what I want to convey is that over here
100:09 - for for building let's assume that X is
100:11 - a horsepower you have MX plus B and M
100:15 - for for finding the relationship right
100:17 - so here over here m and b we need we
100:19 - need to find out so over here you you
100:22 - cannot just plug anything like we have
100:23 - seen that if you plug anything we are
100:24 - going to we're not going to get a best
100:26 - fit line so what can we do we can we can
100:28 - try out several values of m and b so we
100:30 - can try at 1.1 or the and then some 1.2
100:33 - and then with these values given the x
100:36 - value we took in test how well these
100:38 - values are by plotting it on a straight
100:40 - line like this so um we plot on a
100:42 - straight line and see how it performs by
100:44 - hit and trial right we first of all
100:46 - perform like this and then the next what
100:48 - we do is we conduct another we try
100:50 - another value and see if you're getting
100:52 - the best fit line so you've got to what
100:54 - eventually will do you will try several
100:56 - set of m and b right and then you will
100:58 - try to plot the best fit line or the
101:00 - plot that equation and once you plot it
101:02 - you will be able to see okay this is the
101:03 - best way I know this is not so you'll
101:04 - plot and see how is it the best fit line
101:06 - right so head and trial different values
101:08 - and see what works well but it has
101:10 - several problems the first problem is in
101:13 - it will be very hectic it will be very
101:15 - hectic and time consuming and time
101:18 - consuming okay first thing second is
101:21 - what if if you have a 3D or 4D um in
101:24 - higher dimension of data right you might
101:27 - have you might have higher dimension of
101:29 - data and you may not be able to plot
101:30 - every time and then see right so in that
101:33 - in that real world data will not look
101:35 - like this it will have a millions of
101:37 - examples millions of input features
101:40 - right so uh we exactly want to talk on
101:42 - uh that so we cannot really plot and
101:45 - then see against right so that second
101:47 - one is it will be very hard for us to
101:50 - eventually plot the data and see every
101:52 - time So eventually hectic and time
101:54 - consuming so we need to cut that
101:56 - approach out what's the next approach we
101:58 - have an optimization algorithm which is
102:01 - based on this hit and trial only which
102:03 - is based on this hit and trial only but
102:05 - we do it very strategically you learn
102:07 - how we do it very strategically using
102:10 - mathematical process right so I hope
102:12 - that will make make more sense when I
102:14 - actually go into this now so let's let's
102:18 - see let's see how to see what works well
102:21 - right so how to see what works really
102:23 - well so over here as of now we have seen
102:25 - that we can use optimal optimization
102:26 - algorithm to come up with that so uh but
102:28 - before that we need to see how what
102:30 - works will and what not then we can come
102:32 - to this optimization problem so if we
102:35 - want to see the graphical problem like
102:37 - over here we have two sets of data so
102:40 - the first set of data has this this this
102:41 - data and then you plot the best fit line
102:43 - and this is the best fit line available
102:45 - for your data and over here you you you
102:48 - don't have the best line so this this
102:51 - particular this this particular equation
102:52 - is wrong but this particular equation is
102:54 - correct but why because in this
102:57 - particular equation has a best fit line
102:58 - or this particular line uh fits the data
103:00 - best and this particular line does not
103:02 - fits the data valve right so that is one
103:04 - of them now over here you can see that
103:08 - over here you can see that it is not
103:11 - fitting the best fit line so what is is
103:13 - that and what is the conclusion we're
103:15 - trying to make is if you want to see the
103:16 - best headline which I've already seen is
103:18 - will will be this but by now we are only
103:22 - seeing or we are only telling by seeing
103:25 - the data we are only seeing uh we are
103:27 - only telling how is this the best fit
103:29 - line is the best fit line by only seeing
103:31 - the data but data will be you know you
103:33 - have only the horsepower as of now now
103:35 - there will be lot of a lot of input
103:37 - variables for example you may want to
103:39 - establish relationships you know you may
103:41 - want to establish a relationship between
103:43 - maybe horsepower and then you know on a
103:46 - on another one which is the brand of the
103:49 - car you know the social media you know
103:51 - engagement of the car and blah blah blah
103:53 - I'm going to find the relationship where
103:55 - all of these three to a particular
103:56 - variable sales so we have to find you
103:58 - will be getting three Deals we'll learn
104:00 - about this uh multiple uh multiple
104:02 - linear regression soon but you will see
104:04 - that we will have three dimensional data
104:06 - even four dimensional five dimension of
104:07 - data right but it's not always possible
104:10 - to plot it will always not get only some
104:12 - 100 200 300 or you'll get millions of
104:14 - data points right you cannot plot
104:16 - everything right you cannot plot and
104:18 - then see that how it's performing that
104:20 - is a graphical method for General
104:21 - understanding it's not for something
104:23 - which you have to do in real world but
104:25 - sometimes it also comes in handy if you
104:27 - have a small amount of data but
104:29 - eventually you'll have to deal with
104:31 - numerical way right so for example when
104:34 - you plot the when you when you plot the
104:36 - line you can for example assume that you
104:38 - have a equation x f f of x is equals to
104:41 - x squared so the the the graphical will
104:44 - look like this the graphical is Parabola
104:46 - the graphical is Parabola but you cannot
104:48 - eventually use this Parabola nicely to
104:51 - get your answers for larger values you
104:54 - know for larger way for subset you can
104:55 - do for two or three you can do but what
104:57 - if one two five nine nine and nine so
104:59 - you cannot even do that you cannot even
105:01 - try to use this Parable to do that you
105:03 - can eventually do that but it's very
105:04 - hectic and very not possible sometimes
105:06 - it's also not possible right so in that
105:09 - case what you will do you'll utilize is
105:11 - numerical way x squared f 2x is equal to
105:14 - the function numerical this is the
105:15 - function this is f of x graphical way
105:18 - and this is the numerical way this is
105:21 - the numerical way right so we'll go with
105:22 - the numerical way your plugins go one
105:24 - two five nine nine and then you just one
105:25 - two five nine nine times one two five
105:27 - and nine you just squared this and you
105:29 - get it exact value and then you'll get
105:31 - it exact value right so just in the case
105:34 - of linear regression okay we have the
105:36 - graphical best fit line to from at our
105:39 - data but this graphical line is
105:41 - represented by a numerical value which
105:43 - is MX plus b we need to utilize the
105:46 - numerical as well I hope that you
105:48 - understood why the importance of
105:49 - numerical is given over graphical
105:51 - graphical for understanding as well so
105:53 - now let's come to numerical
105:55 - representation that works really really
105:58 - well over here which is which you're
106:00 - seeing in front of me is that you have
106:03 - some data points out here so assume that
106:05 - this is your one data point is another
106:06 - this is another this another this is
106:08 - another so you have some data points out
106:10 - here if some data points out here right
106:12 - you have this blue points are data
106:14 - points in with labeled examples so this
106:16 - data point has x value this and Y value
106:18 - this this report so it has the labeled
106:20 - example y as well y as well please note
106:23 - that we're dealing with supervised
106:24 - learning algorithm and we should know
106:26 - that this is the part this has y right
106:29 - cool so over here assume that assume
106:33 - that your X is equals to 24. okay your X
106:36 - is equals to 24. your predicted would be
106:39 - assume that this is a straight line you
106:40 - plotted the best fit line you plotted
106:42 - the best fit line so this is the best
106:43 - fit line which you have in your data
106:44 - right and then X is equals to 24. this x
106:47 - is equal to 24 if you just see if you go
106:50 - above it will this is actually the this
106:53 - is actually the ground truth this is
106:55 - actually your label value but over here
106:57 - this is what your model predicted
106:59 - because model will predict at the point
107:01 - of intersection the point of an
107:02 - intersection is a model prediction right
107:04 - for example if we say x equals to 20 x
107:07 - equal to 20 would be gone gone gone gone
107:09 - gone and here it intersects right
107:12 - because we don't have any example we
107:14 - don't have any x equals to 20 a label a
107:17 - layer or data point in your data but
107:19 - over here the reason why you best fitted
107:21 - the line so that we can predict like
107:22 - this and then if we if we see the Y
107:23 - value there are 45 as the prediction
107:26 - right this is how we make sense but over
107:28 - here but something but let us assume
107:31 - that this value 24 for 24 your true
107:34 - value your ground truth means true value
107:36 - for example what is ground truth you
107:38 - have a score you have scored 45 out of
107:41 - 15 years test scores this is a 45 which
107:43 - is the correct value which is the truth
107:45 - but your model predicts that you have
107:47 - scored 57 out of 50. so this is your
107:50 - this is a predicted value or
107:51 - approximated value and that was your uh
107:55 - 45 out of 50 is your ground truth 45 is
107:58 - the ground truth so over here as well
108:00 - for this particular Podium X1 for this
108:02 - particular data point x equals to 24 x
108:05 - is equals to 24 so X is equals to 20 4X
108:08 - x is equal to 2024 your predicted value
108:11 - your ground truth is 38 right for
108:15 - therefore x equals to 24 but the model
108:17 - prediction Lies over here at the point
108:19 - of intersection so when you just go over
108:20 - go above you see the point of an
108:22 - intersection over here and the model
108:24 - predicted value is nothing but 47.
108:27 - right so actually the correct value for
108:31 - x is equal to 24 is 38 but your model
108:34 - approximated or predicted 47. so of
108:37 - course there's there's some sort of
108:39 - Errors you know for example you might
108:41 - have noticed that when you're actually
108:42 - worth in class 10 DNA you know when
108:45 - actually DNA copying is done there are
108:47 - variations right there's some there are
108:49 - some errors so when model approximates
108:51 - it comes with errors as well right so
108:53 - those errors between approximator and
108:56 - correct value 38 is the correct value is
108:59 - and third 47 is the approximated or the
109:01 - model predicted value the model what
109:03 - what model thinks what model thinks x
109:05 - equals to 24 and what model thinks
109:08 - decided by the straight line right uh so
109:10 - that's why we say that best with line
109:12 - freely works for best fit line we need
109:14 - to have for a line we need to have a
109:16 - best fit in the data then it will
109:18 - perform very well right so the
109:20 - difference between the predictive is
109:22 - approximated and the actual value the
109:24 - correct value which you call as a ground
109:25 - truth is called Nothing but residual or
109:28 - difference of Errors right so it's
109:29 - called residual or difference of Errors
109:31 - let's take a look at another example by
109:33 - taking a look over here so assume that
109:36 - assume that you have this particular
109:38 - data point you have this particular data
109:40 - point assume that this one this one so
109:43 - over here you have X is equals to maybe
109:45 - 12 and when X is equal to 12 your your
109:49 - your correct value your correct value as
109:52 - well as your correct value the blue
109:53 - point is ground truth as well as your
109:56 - moral prediction lie on the same point
109:57 - which means y i which is a ground truth
110:01 - which is ground Truth for x equals to 12
110:03 - and Y hat I which is your which is your
110:06 - model predictive value are on the same
110:08 - point which means that they both are
110:11 - equal and assume that the models is that
110:13 - you will have 1200 samples 1200 sales
110:17 - your model predicted to 100 sales and
110:20 - actually 200 Sales was done which means
110:22 - that a model was perfectly accurate your
110:26 - model is perfectly accurate right so in
110:28 - this case the error will be equals to
110:31 - zero error will be equals to zero
110:33 - right so I'll take one example to make
110:36 - you understand assume that you're
110:37 - working on you're working in a date as a
110:39 - data scientist and ask him stop and over
110:41 - there you want to predict you're going
110:43 - to predict based on past data you're
110:45 - going to predict what will be the number
110:46 - of a sales Fund in the next day so your
110:48 - model says that there will be 200 Sales
110:50 - next day right but actually 200 Sales
110:53 - was done which means they are on the
110:54 - same point so over here with x equals to
110:57 - 12 then a y hat is equal to 37 and when
111:01 - y i is with 37 and Y hat I also has got
111:04 - 37 which means both are equal and the
111:06 - residual between them is equal to zero
111:08 - let's take a look at another example
111:10 - when your X is equals to 24 when your X
111:13 - is equals to 24 over here your the
111:16 - actual the blue point as well as the as
111:19 - well as the predicted value is on the
111:20 - same point which means that you predict
111:22 - it as well approximated is r equal
111:26 - let's take a look at another example
111:27 - when your X is equals to let's say 29
111:30 - you have
111:32 - you have this as the predicted value
111:34 - sorry this has a correct value which is
111:36 - a ground throat and this as your
111:39 - approximated value this this has an
111:41 - approximate a value this has a correct
111:43 - value I'm not so the this the difference
111:45 - between both of them is a residual so
111:48 - your correct value is little bit more
111:50 - than the actual predicted value so what
111:52 - predict value is nothing but the point
111:54 - of intersection when we drop on the
111:55 - x-axis I hope you are getting what I'm
111:57 - trying to say I'm just trying to justify
112:00 - that that the difference between the
112:03 - ground truth as well as the approximated
112:06 - value nothing else I'm doing I'm just
112:08 - trying to justify that okay let's read
112:11 - something for x equals to 24
112:13 - approximated more operative is 47 but
112:15 - the ground rule is 38 so the error term
112:17 - on error between Y and Y hat which is
112:20 - why hat means which is approximated in
112:22 - ground truth is nothing but 9. so the
112:24 - error term is nine higher your error is
112:27 - better approximation is what does this
112:29 - mean high order error is in bad Deb
112:31 - approximate is for example assume that
112:34 - you are in an ice cream shop and then
112:36 - you pre and your model predicted 1200
112:38 - sales which which will happen the next
112:40 - day since the 200 Sales will happen but
112:42 - actually only 400 sales happen so the
112:45 - difference between you know for example
112:48 - you're giving a match test right you
112:50 - write a very different steps right to
112:52 - different different times a very
112:54 - different answer rather than the actual
112:56 - answer so you're absolutely wrong right
112:58 - you should be at least close to it when
113:00 - you're giving a Science examination you
113:02 - written something you written some texts
113:03 - which is the actual which is the your
113:05 - answer which you think is answer and
113:08 - there is a correct answer in a marking
113:10 - scheme right there's a correct answer so
113:12 - your your what you what your teacher
113:14 - will do your teacher will match with
113:16 - this if the error is so much in this is
113:19 - so much the the difference between so
113:20 - much then the error is bad so that the
113:23 - bit difference between the why why I and
113:26 - Y hat or sorry y hat i and y i which is
113:29 - the moral which is the actual value and
113:31 - there's a more applicable value if the
113:33 - error between in them is high then
113:34 - battery approximation is why I have
113:36 - given you a very simple example one
113:38 - example is ice cream shop which is 1300
113:40 - and 400 so error between them is very
113:42 - very high
113:43 - right and that's why we don't have
113:45 - that's why the higher the error will be
113:47 - that your approximation will be right so
113:50 - I hope that makes sense about this
113:51 - higher error and bad approximation is
113:53 - but what we can do we can for evaluating
113:56 - how well our lines are but why are we
113:59 - learning all of these things the reason
114:01 - why we are learning you know R and Phi
114:03 - how well this line is for based on
114:06 - numerical representation by seeing if
114:08 - you can tell okay this is good but
114:09 - numerically we should be also be
114:11 - verified right so what we do we do for
114:14 - every sample which we which you have in
114:16 - your data right so what you do for all
114:19 - these Blue Points you take out the
114:21 - difference of Errors for all these data
114:23 - points for example for example y1 which
114:26 - is this y one my y hat one minus y1
114:30 - which is equals to 9 right which is
114:32 - equal to 9. so you taken out for the
114:34 - first for the first one and then you do
114:35 - for the size second one y two y two so
114:38 - if you see over here Y2 where is Y2 yeah
114:42 - this is Y2 so this is your predicted
114:45 - value this is your predicted value and
114:46 - this is your ground truth so the
114:49 - difference between them is 44 which is a
114:51 - predicted value minus 45 you get minus 1
114:54 - right so this is the this is the
114:56 - difference between predictor and the
114:58 - actual value right and then you have per
115:01 - Y3 for Y3 you have difference of Errors
115:04 - where you have y 3 which is the actual
115:06 - value and the predicted value which is
115:07 - minus 4 and then you have y 4 and where
115:11 - the predicted actual value are on the
115:12 - same point you have 0 and you do for all
115:15 - the data points okay this this blue
115:17 - lines and blue points are nothing but
115:18 - the data points these blue points are
115:20 - nothing but the data points right so you
115:22 - do for you calculate the for example
115:24 - what you do eventually so let's take a
115:27 - scenario to make it little bit more easy
115:28 - to understand right
115:30 - when you actually let us assume you have
115:33 - 10 problems in an exercise 10 problems
115:36 - you have the problems one two three four
115:38 - five six as well as you have the
115:39 - solutions so what you do what you do you
115:43 - make your own 10 Solutions you make your
115:45 - own 10 Solutions based on the questions
115:46 - what you have learned you you make your
115:47 - own 10 Solutions right and these
115:50 - Solutions let us assume that this is
115:51 - your name is model and you approximated
115:54 - this or predicted these Solutions now
115:58 - what you do you match it with this you
116:00 - match with the correct Solutions given
116:02 - in your book right so higher the
116:04 - difference is bad your answer is right
116:07 - well that's what we are doing over here
116:08 - to evaluate how well our paper gone is
116:11 - we we match each and every Patriot value
116:13 - with that respective correct value for
116:16 - example question number four we match
116:18 - the question number four with question
116:19 - number four the correct answer as well
116:20 - as the so we just match and check how
116:22 - where it is and we give our mark space
116:24 - on that so just like this for evaluating
116:27 - how well the best vet line our are we
116:29 - just have this straight line line and
116:31 - given a straight line employee we
116:33 - evaluate more operations minus the
116:36 - correct value for all data points
116:37 - available in your data right we do not
116:40 - do for data points for data points which
116:42 - we don't have any label for that for
116:44 - example we cannot do for uh assume that
116:46 - uh we cannot do for let's say two we
116:50 - cannot do do for let's say two right we
116:52 - cannot because we do not have data point
116:54 - for that but we can do for the maybe
116:56 - even seven right we can do this because
116:58 - we have the prediction over here and
117:00 - then we have the model practice the
117:01 - correct value over here okay
117:04 - I hope that makes sense and then you
117:06 - then you take out for and then you have
117:08 - a n number of errors or we can say e
117:11 - number of Errors Eis so you have e
117:14 - number of Errors e will be equal to the
117:15 - number of data points which you have in
117:17 - your data right so it will have the E
117:19 - number of Errors how N1 minus N2 and the
117:22 - one denotes y hat I and N2 denotes y i
117:26 - and then you have negative values as
117:28 - well somewhere so there are two issues
117:30 - which we come we have m n we have
117:32 - several number of Errors how can we
117:34 - combine all of this into one single
117:36 - number to better describe our model and
117:38 - then second one is some of our error
117:40 - terms are negative so you can see that
117:41 - they some are negative right so what we
117:44 - do to take care of this so we can solve
117:47 - the problem we have the negative sign we
117:49 - can get rid of by two processes by
117:51 - squaring the number or by taking out the
117:54 - absolute absolute of that in absolute of
117:57 - that number absolute of that so for
118:00 - example you can we could simply take out
118:01 - the absolute obvious whatever the answer
118:03 - which integer the absolute or the
118:05 - squared absolute or this Square so
118:08 - Square will just cancel out as well as
118:10 - absolutely we will just cancel out so we
118:12 - can we have two approaches either Square
118:14 - to the differences or absolute
118:16 - differences so what we usually we what
118:19 - what we will usually do is we are going
118:22 - to use the square of the number I'll
118:23 - talk about why we are not using absolute
118:25 - but there is an apps we also use
118:27 - absolute as well but I'll tell you later
118:29 - on we'll just wait for my confirmation
118:30 - for some for some maybe next set of
118:32 - sections and then the first problem is
118:34 - all negative we can either use any any
118:36 - of the approach to fix the negative sign
118:39 - but but even after fix it we have
118:41 - certain number of M number of Errors m m
118:44 - is nothing but equals to the number of
118:45 - data points and every as I said your
118:48 - error should be equal to number of data
118:50 - points because the difference is where
118:51 - because you'll be only to your only ask
118:53 - your model to take out the answer you
118:55 - ask your model to take out the answer as
118:57 - uh for the questions which we already
118:59 - have the correct answers so that we can
119:01 - match and test how a model performs so
119:04 - what we can do we can take that average
119:05 - of M terms we can have take out the
119:07 - average of all the errors E1 E2 all over
119:09 - e m but then what we can do we can put
119:11 - that in a submission format so that we
119:13 - come up with the formula so formula is
119:16 - this 1 by m summation I equals to zero
119:18 - all the one or the M4 every Eis we take
119:21 - other every errors hence the expanded
119:23 - Formula 1 by m i is equals to zero all
119:26 - the way out of the m so you do so you go
119:28 - from I equals to all the way out of the
119:29 - M right y i y hat I minus y i squared so
119:34 - over here you're taking out the
119:35 - differences and differences you're
119:37 - squaring that up and adding the another
119:39 - I equals to 1 then I equals to 2 and
119:42 - then I equals to three so please note
119:43 - that you have already studied the
119:44 - summation format right so how it will
119:47 - perform for example assume that you have
119:49 - a date you have a data point you have a
119:51 - data point something like this where you
119:52 - have
119:53 - one two three four five right you have
119:57 - one two three four five so I equals to
119:58 - one is equals to I equals to I equals to
120:01 - zero I equals to one other M right so
120:04 - you have y one y one minus y a y one
120:08 - that's quick right so you do for y
120:10 - equals to one I is equals to two I is
120:12 - equals to three I is equals to four is
120:14 - equals to five so you do for every data
120:16 - point for all the and for all the detail
120:18 - points in your data you add a squared to
120:21 - remove the negative sign and then you
120:23 - app and then you have the approximation
120:25 - value by uh sorry the difference of
120:27 - errors this is called the residuals
120:29 - which you will get and we divide the
120:30 - value by by all the summation all the uh
120:34 - sum by m right I hope that this makes
120:37 - sense this this formula makes sense so
120:39 - this formula should make sense to you
120:41 - you know people usually learn these type
120:43 - of formulas but it's eventually what
120:45 - you're doing you're just thinking of the
120:46 - differences of the multiplicative value
120:48 - and the actual value is squaring it up
120:50 - to remove the negative sign and then
120:52 - you're taking and then you're taking the
120:54 - average of all of them you're taking the
120:56 - average of all of them
120:58 - and this comes up with the name of cost
121:01 - function or we can say mean squared
121:04 - error for evaluating how well our best
121:07 - fit line is
121:08 - how well our best fit line is we will
121:11 - work on a worked example
121:13 - so why is it useful first it is way to
121:16 - evaluate the best fit line it measures
121:19 - the performance of our model so higher
121:21 - the MSC is badder model is why because
121:24 - higher the MSC will indicate that
121:26 - differences are also higher so as I said
121:29 - that if the differences if the
121:30 - differences are higher then it's not a
121:33 - good model right we had to talk on that
121:35 - right so this is one of them this is one
121:37 - of the key criteria which you should
121:39 - know so it is way to evaluate the best
121:41 - fit line and this is way to measure the
121:43 - performance how well so higher the error
121:44 - is battery so you have to make sure that
121:46 - your cost function is approximately or
121:48 - is or around zero
121:50 - so now we come up with the more
121:52 - conventional formula which is J of m and
121:55 - b so basically we're evaluating our m
121:57 - and b is equals to 1 by 2 m i is equals
122:01 - to zero all the way out of the m y I
122:03 - have minus y i which is the difference
122:05 - of headers off you take the average but
122:07 - what is 2 over here what is 2 over here
122:09 - what is 2 over here it's because of the
122:12 - convention but usually people say
122:14 - because of convention but eventually
122:16 - when you learn about this optimization
122:17 - algorithm which I was talking about
122:19 - which I was talking about this too
122:21 - really helps in getting written when you
122:24 - actually do the calculation stuff you
122:27 - know sometimes researchers find an easy
122:29 - method to easy to calculations and these
122:31 - two really helps in that optimization
122:33 - algorithm we'll see how this helps when
122:35 - you actually derive the formula but as
122:37 - of now you can just remember it okay now
122:40 - to recapsulate what what we have studied
122:43 - recapsulate what you have studied is
122:46 - what we can eventually do is that we can
122:49 - take out the differences between more
122:52 - addicted and actual value moral
122:54 - predicted and actual value and then
122:56 - Square it up to get rid of negative sign
122:57 - and we have several values now now we
123:00 - have to Simply divide the term or by the
123:02 - number of values in your data set which
123:04 - is take out the average of them higher
123:05 - the error is that your model is because
123:07 - the higher the different it will
123:08 - showcase the higher the differences is
123:10 - right but there are two questions now
123:12 - why we don't use absolute
123:15 - and what is 2 what is the 2 over here
123:18 - what is the use of two over here and we
123:21 - will talk about the real reason of why
123:23 - we don't take out absolute and why we
123:26 - don't use two over here right why why we
123:29 - are not using ingredient descent or
123:30 - optimization section tool so let's
123:32 - quickly work on a verb example of mean
123:34 - square error that will make more sense
123:36 - so you have your height over here you
123:39 - have your height over here and then you
123:41 - have your weight over here so basically
123:43 - given height of a person you're gonna
123:45 - predict you want to establish a function
123:46 - f that takes the height of person and
123:49 - then predicts what's the weight of that
123:51 - person what's the weight of that person
123:53 - right so given this height and predict
123:55 - the weight
123:56 - right so now over here you came up with
123:59 - this you came up with 0.8 X which is M
124:02 - plus 9.0 which is B so you came up with
124:04 - these with these formula so as I told as
124:07 - I told that you have your uh now what
124:10 - you do you will put in the values of X
124:12 - for the known for the only axis for
124:15 - which you have the known label right or
124:17 - the data points only and you may figure
124:20 - how we came up this zero point is 9.2
124:22 - it's because I just randomly written it
124:24 - because I want to show you not how we
124:26 - get this but how we evaluate its
124:29 - performance okay
124:30 - so what we do we have 0.8 we have we
124:33 - have zero zero zero point eight which is
124:35 - over here multiplied by this 43 plus 9.2
124:38 - which is you just exchange the value of
124:40 - x every time 43.6 44.5 for every values
124:43 - for every excess values now now what we
124:46 - do we compare the correct value and the
124:48 - model approximated by the correct value
124:50 - in a model approximated value correct
124:51 - value and model like rocks correct model
124:53 - correct model we take out the
124:55 - differences errors okay we have these
124:57 - errors out here now what we do first of
124:59 - all we Square all the terms and then we
125:01 - sum it all up and then divide by m which
125:04 - is one two three four five five and then
125:07 - 6.08 is your square error so when you
125:09 - apply this formula you will get that
125:11 - okay so basically what we are doing over
125:14 - here is nothing but taking out the
125:17 - answer for every axis and then comparing
125:19 - with actual answer
125:21 - okay cool I hope that this worked
125:24 - example of Ms really helped and
125:26 - everything is pretty much Crystal Clear
125:27 - let me know if you have any questions in
125:29 - the descript Community or in the comment
125:31 - box and try to answer each and every
125:32 - question now what you're eventually
125:34 - going to do is talk about the
125:36 - relationships talk about relationships
125:39 - in terms of mathematics not in terms of
125:41 - life because I pretty much stash up in
125:45 - relationships and life so please ignore
125:47 - I'm not good advisor on relationship in
125:49 - life but I'm good advisor on how can we
125:52 - establish relationships okay we'll talk
125:54 - about the definition of relationships
125:56 - we'll talk about some rotational changes
125:58 - and then we talk about how to find that
126:01 - beta0 and beta 1 which I've been waiting
126:04 - for a long time we have several ideas
126:05 - they build from a plain English language
126:07 - and then we go to a level but prior to
126:10 - this I would suggest that you're pretty
126:11 - much comfortable with derivatives the
126:13 - geometrical understanding of derivatives
126:15 - as well as a bit of you know formula and
126:17 - rules and derivatives it will really
126:18 - really cool so now what we need to do is
126:22 - get started with a bit of next sections
126:26 - about relationships till then I'll talk
126:28 - later
126:29 - so now what we're going to do is talk a
126:32 - bit about relationships relationship is
126:35 - one of the most uh important thing to
126:37 - talk about because usually people tell
126:38 - you know establish a relationship or
126:40 - establish a relationship and blah blah
126:42 - blah and eventually you you get confused
126:44 - what exactly this term the relationships
126:47 - means
126:48 - so we have even just clear about that
126:51 - what exactly this relationship means so
126:54 - that it makes much more sense out of it
126:56 - right so over here whatever what I'll
126:59 - try to do is tell you what exactly the
127:02 - relationships are tell you what is that
127:04 - relationships are and then I'll I'll
127:06 - tell about two types of relationships
127:08 - which is deterministic relationship and
127:11 - second is a statistical relationship and
127:13 - why we want to focus only on statistical
127:15 - relationship over the course right so
127:18 - let's talk about what exactly the
127:20 - current thing which is studying right
127:21 - now the current thing which is starting
127:22 - right now is a linear regression which
127:25 - is a statistical method that allows us
127:27 - to summarize data and study
127:29 - relationships between two quantitative
127:31 - uh which is the quantitative variables
127:34 - which is that type of continuous
127:36 - variables so we have seen it that
127:38 - supervised learning supervised learning
127:40 - has two types of problems one is
127:41 - regression problems and another one is
127:43 - classification problem so we have two
127:45 - types of problem classification and
127:47 - regression problem in terms of
127:49 - regression problem how can we identify a
127:51 - problem is regression problem if the
127:53 - target variable or the thing which you
127:55 - want to predict the thing which you want
127:56 - to predict the thing we're going to
127:58 - predict is continuous is continuous data
128:01 - or is continuous data or some sort of a
128:03 - non-countable or non-finite or something
128:06 - which is continuous like the age of a
128:07 - person the height of person the salary
128:10 - of a person they are the con their
128:12 - example for continuous value so if the
128:14 - target variable is is is of con is of
128:17 - continuous data then
128:21 - um then the then it then we known then
128:23 - it's it's known as the regression
128:24 - problems then it's known as the
128:25 - regression problem right
128:27 - cool so linear regression is a
128:29 - statistical method we'll talk about what
128:31 - exactly the statistical method means
128:33 - that helps to summarize data and study
128:36 - relationship between two variables now
128:38 - you you may ask hey ayush what do you
128:40 - mean by study relationships and
128:42 - summarize data so you'll study you'll
128:44 - you'll understand more practically later
128:46 - on but assume that you have a function
128:48 - here function f of x your function f of
128:50 - x that tells you that tells you about
128:52 - the the price of a house in a certain
128:55 - area given its size right so you are
128:57 - given a size of the house so you're
128:59 - given the size of the house it will make
129:01 - you it will make you it it will predict
129:03 - it will predict the price of the house
129:05 - this is the output variable which which
129:07 - you wanna get right so given the size
129:10 - you know predict the price of the house
129:12 - right so what does this F tells F will
129:15 - tell you f will establish the
129:17 - relationship as F will tell you about
129:19 - what is the relationship between these
129:21 - two right it will tell you what is the
129:24 - relationship between these two right
129:25 - that's that's the first thing which the
129:28 - tries to answer this and second it will
129:31 - summarize by how if we increase size by
129:33 - this much by some unit then what will be
129:36 - increase in price so if we make price if
129:39 - if we increase size by 10 units what
129:42 - will be the increase in price right
129:44 - that's what it tells it's it tells you
129:47 - about the relationship it tells you
129:49 - about if one increases what will happen
129:51 - to another right they will study about
129:54 - interpreting the coefficients soon
129:57 - interpreting the coefficient so but
129:59 - that's what the that's that's that's
130:00 - what it uh conveys that you have
130:04 - um
130:05 - uh relations between two quantity
130:07 - variables where you have a particular
130:09 - over here which is it tells you about
130:12 - the two uh quantitative continuous
130:14 - variables or the features okay so assume
130:17 - that assume that you wanna predict the
130:20 - sales of a particular car given these
130:22 - particular features so you know build a
130:24 - function f
130:26 - that takes the manufacturer that takes
130:28 - the model that takes the engine size and
130:30 - that takes the horsepower that takes all
130:32 - these four field input features and
130:35 - predicts the sales of a particular car
130:38 - it predicts the sales of a particular
130:40 - car so basically we we we tell okay this
130:43 - is a manufacturer this is more of the
130:44 - guy this is the engine size is a
130:45 - horsepower and predicts the sales so how
130:48 - this function f which it will eventually
130:50 - help us the function f will tell that is
130:53 - if if we how this manufacturer is
130:57 - affecting the sales how this model car
131:00 - is affecting the sales how does engine
131:02 - size is affecting the sales and
131:04 - horsepower is affecting the sales so we
131:06 - can figure out which of the following is
131:09 - affecting more of the sales like if we
131:11 - focus more on manufacture then for
131:13 - example assume that model of the car
131:15 - affects the sales the most so we can
131:17 - focus on model of the car a model of the
131:19 - car to Brand it up to get more sales
131:21 - right because this is this is generating
131:23 - more so the sales for us or if this is
131:25 - generating then we can focus on
131:27 - showcasing engine size or begin to go so
131:29 - focus on showcasing device power or we
131:31 - can focus on showcasing manufacture
131:32 - according to our analysis that's why I
131:36 - say it helps us to summarize and study
131:38 - relations between these variable and
131:39 - output variable y
131:41 - okay
131:42 - so these variable which we give as an
131:45 - input are called explanatory variable or
131:47 - independent or predictor variable okay
131:50 - or value so why we call this as an
131:52 - independent independent or predictor or
131:55 - explanatory first of all every each
131:58 - individual feature each into each
132:00 - individual feature or variable are
132:03 - independent of each other for example
132:05 - this particular variable is not
132:08 - dependent on this manufacturers is not
132:11 - dependent on this for its own existence
132:13 - model the car is not dependent on the
132:17 - engine size for its existence horsepower
132:19 - is not dependent on the engine size for
132:21 - this that's why you call the independent
132:23 - but this particular sales variable is
132:25 - called the dependent because this this
132:27 - is something which you're trying to
132:28 - predict we cannot predict sales if any
132:31 - of these variables are not here
132:33 - right so sales is dependent on
132:36 - manufacturers sales is dependent on
132:39 - model the car for its own existence
132:41 - right
132:43 - um you know I I hope so that that is
132:46 - clear there's something to be removed
132:48 - over here that this is not the predictor
132:51 - let me just remove this maybe it makes
132:52 - more more sense yeah so now this is this
132:56 - is a response or dependent variable this
132:58 - is a response or dependent variable
133:00 - explanatory variable means it itself
133:03 - explains it like it it is explanatory by
133:06 - itself manufacturers we know that what
133:08 - are the card that's why it goes
133:09 - explanatory variable right or
133:11 - independent I hope so that tip mixture
133:13 - you can see our data fundamentals
133:14 - lecture for more info
133:16 - cool so the algorithm let's first talk
133:20 - about what is an algorithm algorithm is
133:23 - in step-by-step procedure is in
133:25 - step-by-step procedure to solve a
133:28 - particular problem and we and we have an
133:31 - algorithm in machine learning to study
133:33 - relationships and summarize data so I
133:35 - hope that this gives you good sense of
133:37 - what this various examples we have a
133:39 - talk okay so there are several types of
133:41 - relationships which you can come across
133:43 - first type is deterministic relationship
133:46 - deterministic relationship so you might
133:48 - have coded in Python that you have
133:50 - forecoded a function in Python that
133:52 - converts Fahrenheit in so your DF code
133:54 - in fudge that converts Fahrenheit to
133:56 - Celsius right so you have code in Python
133:59 - that converts Fahrenheit to Celsius
134:00 - right so that is the deterministic
134:03 - region why I am saying that is a
134:04 - determinist relationship the formula for
134:07 - converting your Celsius to Fahrenheit so
134:10 - we can build a function f that takes in
134:12 - the value of
134:14 - and that uh for the that that function f
134:17 - is a fatter nine that dates in the value
134:18 - of c and nine by five C plus 32. so this
134:22 - is this is an example of a deterministic
134:24 - why it is deterministic because you can
134:27 - simply plug any value of C you'll get
134:29 - exact for X you'll just you it will just
134:32 - be converted into fahrenheit so there is
134:34 - a defined relationship already there for
134:38 - F and C so if Celsius is this much then
134:41 - it will just calculate things up and get
134:42 - you out of f if you know the value of C
134:46 - if you get the value of C you can easily
134:48 - get the value of f exactly okay
134:52 - because this is already defined formula
134:54 - which we have this is already defined
134:57 - relationship which which we already have
134:58 - and it is linear sort of thing for
135:00 - example if if your cells is 0 then your
135:04 - Fahrenheit will well well with this if
135:06 - your Celsius is 10 then your uh then
135:08 - your value will be also on Y axis which
135:10 - is the Fahrenheit so you if you know the
135:12 - value of C you plug in the value of c
135:13 - and then you get exact F other examples
135:16 - are if you want to calculate the if you
135:19 - if you calculate the the
135:22 - the area of a circle which is pi r
135:25 - squared right or Pi d right so this is
135:28 - one this is the area this is a defined
135:30 - you plug in the value of pi and diameter
135:32 - you get the area right 1 by 2 times BH
135:36 - Define relationship you plug in the
135:38 - value of B and H you get the value so
135:40 - these are defined relationships right
135:42 - this equation exactly describes this
135:45 - exact these equations it exactly
135:47 - describes the relationships right plug
135:49 - in the value it will multiply with some
135:51 - factor and then except and then are you
135:55 - it to a a to 8 will multiply the C with
135:57 - some factor nine by five and then add 32
136:00 - to it right and is the exact or defined
136:03 - relationship between the two variables
136:05 - and this also defined relation between
136:07 - the two variables right but in machine
136:11 - learning we are interested in
136:13 - statistical relationship right in
136:15 - statistical relationship with the
136:17 - relationship between two variables are
136:18 - not perfect please know that
136:19 - deterministic candidate and statistical
136:21 - indeterministic your relationships
136:23 - perfectly defines the particular uh
136:26 - variable between two variables but the
136:30 - statistical it does not perfectly
136:32 - defines it okay
136:34 - I hope that it that that is much more
136:37 - clear so you know find so one one such
136:40 - example is you're gonna find a
136:42 - relationship between skin cancer
136:43 - mortality and latitude right so you want
136:47 - to find you know find what is the what
136:48 - is the what is the relationships if the
136:51 - if what is the ratio between latitude of
136:53 - a particular country and skin cancer
136:55 - mortality so if latitude increases
136:58 - whether it is skin cancer decreases or
137:00 - increases this is a question right this
137:03 - is a question so this is the data which
137:05 - we have this is the data which we have
137:07 - on x-axis we have the latitude at the
137:09 - center of a state on our y-axis one on
137:12 - y-axis we have mortality deaths per
137:15 - that's per 10 million this is the
137:17 - mortality rate this is a mortality rate
137:19 - and this is your latitude
137:21 - and all the blue points are the data
137:23 - points now what you did you you you
137:25 - model the your straight line on this
137:28 - which best fits the data and that
137:30 - straight line is of this where your
137:32 - intercept is 389.2 minus
137:35 - um I think yeah yeah that this this this
137:38 - this particular thing is bit wrong over
137:41 - here this this should be yeah so which
137:44 - is three three point three eighty nine
137:45 - point two minus 5.98 X where this is uh
137:49 - assume that this is your
137:51 - um straight line this is your straight
137:52 - line okay this is your is straight line
137:55 - out there this is your equation for this
137:57 - straight line this equation for this
137:58 - straight line I hope that it makes sense
138:01 - and this is the negative relationship
138:03 - this is a negative relationship first of
138:04 - all we'll talk about the positive
138:06 - negative relationship and further on
138:08 - later on but uh but as of now you can
138:10 - see that this this equation is defined
138:12 - by this
138:13 - um this to stay straight line defined by
138:14 - this particular equation now how can we
138:17 - interpret what is going on over here how
138:19 - can we interpret what is what is going
138:21 - on here so assume that you can see the
138:24 - higher lat altitudes of Northern us the
138:27 - less exposed you did due to the harmful
138:29 - rays of the sun therefore the Lesser
138:31 - risk
138:32 - nevertheless risk so let's uh let's
138:34 - figure out it is saying the higher
138:36 - latitudes of the northern us higher
138:39 - latitude means oh over here High higher
138:42 - latitude which which are going to the
138:44 - the right or the side then you then this
138:46 - 45
138:47 - uh 45 to 50 so higher latitudes higher
138:51 - latitudes means your legs exposed to the
138:53 - harmful days of the Sun so when you go
138:56 - over when you when you go this side in
138:58 - the highlights so if you see there is no
139:00 - data which suggests which suggest that
139:03 - any any uh values of cancer mortality
139:06 - right cancer mortality is it there's a
139:09 - less or even for Town whatever there is
139:11 - a less not too much not too much 150
139:14 - there is nothing but on so if you go to
139:16 - the right side road or the more the
139:18 - higher the altitude is less the more
139:20 - mortality therefore less the risk okay
139:24 - but over if you go over here down 35
139:27 - there is a lot you know there's a whole
139:29 - chunk of uh information out there or the
139:32 - mortality rate out here okay so I hope
139:34 - that this gives a good sense about it so
139:36 - but over here you can see the
139:38 - relationships are not perfect sometimes
139:40 - over here there is this much right but
139:43 - the relationships are not perfect
139:45 - because we don't get exact value we get
139:47 - approximated value but will not get
139:49 - exact value so if you if if if you have
139:51 - seen over here if you have seen over
139:53 - here that we have that that we were
139:55 - getting some sort of residuals that that
139:57 - we were getting some sort of residuals
139:59 - right so your model has approximated but
140:02 - didn't got the exact value this is
140:04 - called the exact value but your model is
140:06 - approximated and the model has
140:08 - approximated it
140:10 - ok so I hope that this gives a good
140:12 - sense about what is why exactly it's not
140:14 - perfect it shows some Trend it is
140:16 - showing some Trend as we go to the SVS
140:19 - increase the latitude your cancer
140:21 - mortality date is it in decreasing okay
140:24 - and it also shows the scattering
140:26 - relationships you can see that all the
140:28 - data is a scattered and it's the
140:29 - scattered relationships so as you can
140:31 - see that this is this is what the
140:32 - statistical and over here over the
140:35 - course we are going to talk about state
140:36 - of modeling the
140:38 - statistical relationships okay cool so
140:42 - if you are interested in finding the
140:43 - statistical relationships so now now uh
140:46 - we'll some we will summarize what
140:48 - exactly we studied after changing some
140:49 - rotational changes so that it makes
140:51 - sense in the next lecture what exactly
140:54 - we are going to do if we are going to
140:55 - talk about how can we estimate the
140:58 - coefficient how can we estimate that m
141:00 - and b in that equation so you have X is
141:03 - y equals to MX plus b how can we
141:05 - estimate m and b by giving you some
141:07 - rotational changes and then we'll talk
141:10 - about how what is the interpretation of
141:12 - that m and b in different terms and then
141:15 - we just and then we'll just recapsulate
141:17 - whatever we have studied so that it
141:19 - makes much more sense and then we
141:21 - finally go on understanding how we can
141:23 - get that in simplest terms
141:26 - so everyone uh now what you're going to
141:28 - do is we're going to talk about a bit
141:30 - about how can we estimate m and b what
141:34 - are its interpretation what is this
141:36 - interpretation what is this
141:38 - interpretation like what exactly it
141:40 - means and Etc after some rotational
141:42 - changes
141:44 - so yeah so first of all what are we
141:47 - given with so you so you're given the
141:50 - data so you're given the data X1 then
141:52 - you have y1 so basically it is a
141:55 - supervised learning problem so you have
141:56 - input value as well as its label y1 X2
142:00 - Y2 X3 Y3 all the way down to the x m y n
142:04 - so you have particular input value
142:06 - according with with with these labels X2
142:09 - with this label X3 with this label so
142:11 - you so you're given the data and we try
142:14 - to find the best fit line based on the
142:16 - given data which we have that best fits
142:18 - the data that's our end goal so why best
142:22 - fit line a very nice question to tackle
142:25 - why do we want the best bit line right
142:27 - so line which best fits the data have n
142:31 - errors and they're small as possible in
142:34 - overall sense okay so be on a fine so we
142:38 - wanna find a line which have n number of
142:41 - Errors which are n number of errors and
142:44 - there's small as much as possible
142:46 - overall sense well so what does this
142:47 - mean overall sense in overall sense
142:49 - means that we have n number of Errors N1
142:52 - N2 N3 N4 N5 n6 N7 n8n9 right so this is
142:58 - the this is the so over you can see that
143:01 - we have one two three four five six
143:03 - seven eight nine ten eleven twelve so we
143:06 - have a total number of error 12 errors
143:08 - so 12 means when we take an average it
143:11 - should be very small so overall means
143:12 - average so it should be as small as
143:15 - possible and then and then we can simply
143:19 - and then we can simply approximate by
143:21 - going off from the x-axis at the point
143:23 - of intersection that is the Y that is
143:25 - the that is the coordinate which
143:26 - contains y as well and then we get a
143:28 - production y okay so we have a way to
143:31 - achieve it by least quiz criteria so
143:34 - least release quiz criteria is something
143:35 - which which will study as well later on
143:37 - but we have a way to achieve it which
143:39 - will study later on but what you can
143:41 - ignore this as of now so this forms the
143:44 - best fit lines so our equation for best
143:46 - fit line X1 is m x i plus b so let me
143:50 - just make sure that this is something
143:52 - this is something which we should ignore
143:54 - yeah uh so which is H of X is equals to
143:57 - MX plus b m x plus b this is the this is
144:01 - an equation you plug in the value of x
144:04 - and then you get a prediction and then
144:05 - you could get a prediction for the
144:06 - straight line so our goal is to find
144:08 - that m and b right
144:10 - right so we can evaluate our best fit
144:12 - line using our MSC only Square criteria
144:15 - so we can simply evaluate evaluate our
144:18 - best value line one by two m
144:21 - I equals to zero all run out of the m h
144:23 - of x minus y minus y squared Why are we
144:27 - squaring because to to avoid the
144:29 - negative as we are taking the average
144:31 - and adding 2 as a convention but will
144:33 - but we'll see why this two really helps
144:36 - while uh in latest pages of gradient
144:39 - descent we'll talk about that but you
144:40 - can assume that as a convention but
144:42 - we'll talk about really we will talk
144:44 - about that why is that two because I
144:45 - think this is important to talk
144:47 - um so you can use simply use this
144:48 - equation to take out some errors and
144:50 - higher the error is where your model is
144:52 - so you use MSE to calculate how your how
144:56 - you are performed so how you're good at
144:58 - errors if error is small so errors
145:01 - depends on the straight line
145:03 - so if the straight line is bad like this
145:06 - is like this is a this is something
145:07 - straight line this is your assume that
145:09 - this is your data and this is a straight
145:11 - line so this is this straight line is
145:13 - bad right so your errors depends on
145:15 - straight line and your straight line
145:18 - depends on two values m and b right
145:20 - because this particular this particular
145:23 - value has its own Y intercept and has
145:27 - its own M so B and M right it has all so
145:30 - it is dependent on M which is the slope
145:32 - and a y intercept B the slope and The
145:34 - Intercept so what we want to do we're
145:37 - gonna We want to reduce our errors right
145:39 - so if you want to reduce our errors what
145:41 - we should work on we should work on
145:42 - straight line we should work on getting
145:43 - good straight line and if you want to
145:45 - get a good straight line we should work
145:46 - on what we should work on in getting the
145:48 - good values of m and b we should we
145:50 - should find good slope and Intercept in
145:52 - order to minimize m and b that with you
145:55 - in order to minimize your cost function
145:56 - to find mnu so in a fine m and b that
145:59 - minimizes your cost version which is yo
146:01 - you have a cross version J of M B which
146:04 - is equals to 1 by 2 m i equals so J of M
146:09 - comma B uh 1 by 2 m i equals zero all
146:12 - the one out of the m h of x minus y
146:14 - squared so Square for negative sign
146:16 - average Tower and 2 4 as a convention
146:19 - so to find mnb that minimizes the cost
146:21 - quench so what are its notational
146:24 - changes so what are its notational
146:25 - changes so again to to to repeat your
146:29 - summary we we have a hypothesis
146:31 - hypothesis give some error like to now
146:34 - here but now we say it how how the
146:36 - hypothesis is this we get some error and
146:38 - then we and then the error depends on
146:40 - straight line the straight line depends
146:42 - on m and b so the final good slope m and
146:43 - b together and if you get the good
146:45 - straight line it will automatically
146:46 - reduce the error a whole story is
146:49 - represented on a screen
146:51 - I hope that this gives a good sense
146:53 - about whatever I'm talking about so
146:55 - let's do our goal is to minimize this
146:57 - particular thing which is J it's it
146:58 - should be J I'm so sorry I'm very sorry
147:01 - some of the um I'm I'm very sorry about
147:04 - this about sometimes you get you you
147:06 - have a bit of you know on errors and all
147:09 - you can definitely put in a Errata page
147:11 - we can fix this up right it will be much
147:13 - more helpful if you help us to complete
147:15 - it okay so what are some of the
147:18 - notational changes so over here right
147:20 - now we are using MX plus b as of now h
147:23 - of x h of x equals to MX plus b as if
147:27 - now for as for our prediction function
147:29 - what we can do we can assume we can
147:32 - assume B to be beta0 and M to beta1 okay
147:36 - these are the parameters so why are we
147:39 - doing this the reason why we are doing
147:41 - this in industry in Industry you will
147:44 - see mostly this beta 1 and beta0 and
147:46 - also it will help us to be bit more
147:48 - clear about you know
147:50 - industry or the online articles so when
147:53 - you go to online article you make sure
147:55 - you're comfortable with the beta 1 and
147:56 - beta0 sometimes instead of beta 1 and
147:58 - Theta 1 and Theta 0 or sometimes you
148:01 - have some other notation but we'll use
148:02 - this beta 1 and beta0 so over here you
148:05 - might assume why are not we taking M as
148:07 - a m s beta1 Ms beta0 beta0 but as as a
148:13 - convention we always take we always
148:14 - start off with our bias term B B beta
148:18 - which is B plus m x so B is assume beta0
148:22 - plus beta 1 times x right
148:25 - okay I'll just make sure about this so
148:28 - your M becomes beta 1 and B becomes
148:30 - beta0 okay so we can write beta 0 plus
148:33 - beta 1 x that resolves that is equal
148:35 - equivalent to B plus MX
148:41 - these are the parameters but the meaning
148:43 - stays the same beta 1 is nothing but the
148:46 - slope and beta0 is nothing but your
148:49 - intercept so now there are other names
148:51 - as well so over here over here when you
148:54 - multiply 2x 2 is 2 over here it scales
148:59 - somewhere it scales your X right so 2
149:01 - over here is a phase of weight given to
149:03 - this x just like that X has a weight we
149:08 - have a feature which is X for example
149:09 - the horsepower horsepower has got in
149:12 - weight what's first gotten beta 1 or we
149:14 - can see even in this case horsepower X
149:17 - has gotten weight M so in this case the
149:19 - horsepower Garden uh weight beta 1 and
149:21 - beta0 is intercept or biased we'll talk
149:24 - about what is this bias term means later
149:26 - on but this is a y-intercept okay from
149:29 - where it should intersect at y axis
149:33 - I hope it it uh gives you a good sense
149:36 - about it cool so I just want to talk a
149:39 - bit more about uh these things a bit
149:42 - more on your uh hypothesis uh so your
149:45 - hfx is equals to beta0 plus beta 1 x
149:48 - right and beta beta 1 is a feature
149:51 - weight and X is a nothing but your uh
149:54 - but your uh but but beta1 is nothing but
149:57 - a coefficient of your feature or the
149:59 - feature weight or a parameter beta0 is
150:01 - also intercept we core by convention we
150:04 - have beta0 x 0 plus beta 1 x 1 or we can
150:07 - say beta B times x 0 plus M times X1
150:12 - okay so what does this mean they both
150:15 - are equivalent but but we are going to
150:17 - use a new notation which is beta0 X 0
150:20 - plus beta 1 x 1 okay or you can say b x
150:24 - 0 plus m a m x 1 both means the same so
150:28 - I'm going with the beta 0 x 0 plus beta
150:31 - 1 x 1 by conversion x z is not but
150:33 - equals to 1. so whenever you multiply
150:35 - anything with one it just remains that
150:37 - number so beta0 so that's why we write
150:38 - beta0 alone plus beta 1 x 1 okay so we
150:42 - have so we have x 0 to be equals to 1
150:45 - right so that's why we don't usually
150:47 - write x 0 okay so I hope that you
150:50 - understood will uh it will be more handy
150:52 - when we'll talk about multiple linear
150:53 - regression we will finally understand it
150:55 - but as of now just keep in mind that we
150:58 - have did the bit of rotational change
150:59 - everything Remains the Same just our
151:00 - notational changes introduce beta 1
151:03 - instead of M and beta0 instead of B
151:07 - all sorry makes sense I hope so that you
151:10 - that that you understood about
151:11 - relationships in detail uh and now you
151:14 - can you can easily ignore uh there's a
151:16 - direct formula but I just want you to
151:18 - ignore it because I don't want you to
151:20 - focus on that uh I hope so that you will
151:23 - not focus on that now what we will do
151:25 - we'll talk about we'll talk word so we
151:27 - have a hypothesis h of x h of x equals
151:30 - to beta 1 times X1 Plus beta0 so this is
151:35 - your hypothesis so what does this beta1
151:37 - represents what this is beta0 represents
151:40 - in terms of you know predict the price
151:41 - of a house given a size you're given the
151:43 - size of the house so size has beta 1
151:46 - times the size of the house plus beta0
151:47 - so what this beta 1 represents or how
151:50 - does this beta 1 affects your output
151:51 - variable if you introduce B beta 1 how
151:54 - does it affect on your output variable
151:56 - and what is beta0 effect on your output
151:58 - variable we'll talk about that later on
152:00 - so everyone we come back to our next
152:03 - section of our video which is talking
152:05 - about uh which is talking about
152:08 - um interpretation
152:11 - enter
152:12 - production of coefficients of
152:16 - coefficients parameters or slope and
152:19 - intercept both all mean the same so
152:22 - usually just just to make a remark just
152:25 - to make a remark over here your f i i
152:28 - sometimes use f of x is equals to V
152:29 - beta0 plus beta1 so function of baby on
152:32 - a builder function f f that that Maps
152:35 - your X to your output variable uh h f x
152:37 - means the same and Y so they both they
152:39 - all mean the same eventually at then
152:41 - you're taking out just an output
152:42 - variable y right this you're taking the
152:44 - dependent variable right so all mean the
152:46 - same value interchangeably just to make
152:48 - sure that you are on the same page so we
152:50 - are going to use this uh this this the
152:52 - particular formula uh eventual equation
152:55 - uh where it's going to take out the Y
152:57 - which is the dependent which is a
152:58 - dependent variable and beta0 is an
153:00 - intercept term and you have a beta one
153:03 - is a coefficient of x or feature weight
153:06 - of X or we can say slow okay
153:10 - and so now so now so now let's start
153:13 - talking about so now let's start talking
153:15 - about about what is the significance of
153:19 - your beta0 and beta 1 in terms of
153:23 - problem statements that that will make
153:25 - much more sense that that will make much
153:28 - more sense so let's get started now so
153:31 - assume that so assume that let's start
153:33 - talking about beta0 so beta0 is nothing
153:35 - but call The Intercept the Y intercept
153:39 - the Y intercept term the Y intercept
153:42 - term or buy a storm
153:45 - by the septum and the biostore so this
153:48 - is what the beta0 is so what this is
153:50 - beta 0 represents what does this beta0
153:52 - affect the output variable how does it
153:55 - affect the output variable right so
153:58 - beta0 represents the value of y when X
154:02 - is equals to zero when X1 is equals to
154:05 - zero so your beta0 comes into play when
154:08 - represents the value of y beta0
154:11 - represents the value of y when X is
154:13 - equals to zero so when 0 multiplied with
154:16 - beta 1 that will be nothing but beta0
154:18 - plus 0 so Y is nothing but equals to
154:21 - beta0 that's the first scenario when
154:23 - beta beta0 really helps is when x is by
154:27 - chance if when X is not there whenever
154:29 - when you don't have an information we
154:31 - just go with the Baseline this is what
154:32 - called This is known as a baseline this
154:35 - is known as Baseline so when X is equal
154:37 - to 0 when X is equals to 0 then when X
154:40 - is equal to 0 then your Y is equals to
154:43 - beta0 so that in other words The
154:45 - Intercept of the regression line with
154:48 - the y-axis okay so which means which
154:51 - means this comes into play when X as X
154:53 - is equals to zero okay
154:55 - and it's also The Intercept it's also
154:59 - The Intercept it's also The Intercept of
155:01 - the regression line on y-axis so it's a
155:04 - y intercept right so if beta0 is
155:07 - positive if beta if beta0 is positive so
155:09 - let me just make it clear so if beta0 is
155:12 - positive is beta0 is positive if beta if
155:16 - beta0 is positive it means that it's it
155:19 - means that the equation line starts
155:21 - above the y-axis above the y-axis which
155:24 - means it's it's above the y axis of over
155:26 - here something like that but but if it
155:29 - is negative then it says below the
155:31 - y-axis so you know uh it's it's like um
155:34 - if if your y if if your uh intercept is
155:38 - positive if your inters a bit positive
155:41 - it will stay some somewhere over here
155:42 - but if it's negative then it will stay
155:43 - somewhere over here right so that's one
155:46 - of the that's one of the property so
155:48 - let's take an example to make sure that
155:50 - we better understand this like uh to
155:52 - what exactly when x equals zero so
155:55 - suppose suppose was going to build a
155:57 - simple linear regression model where y
155:59 - represents the price of a house and X
156:00 - represents the size of the house so you
156:02 - know build a function f that takes in
156:04 - the size of the house and and predicts
156:06 - the price of the house so the Builder
156:07 - version F right uh you know window
156:09 - function f that takes a sign and
156:11 - predicts the price so beta0 beta0 would
156:14 - be the price of the house so so this
156:16 - equation will be modelized beta 0 plus
156:18 - beta1 X1 so beta 0 would be the price of
156:22 - the house would be the price of the
156:23 - house when X is X is equal to zero so
156:26 - beta0 plus beta 1 x 0 which is which is
156:30 - beta zero zero multiplier beta 1 is also
156:32 - of course zero so beta0 would be the
156:35 - price of a particular house
156:37 - when price of particular house when when
156:41 - the size of when the size of the house
156:43 - is zero when the size of the house is
156:44 - zero or nothing okay that's when the
156:47 - beta0 comes in play that's when the
156:49 - beta0 comes at play so let's take a very
156:51 - simple example let's take a very simple
156:53 - example to make you understand one more
156:55 - thing so assume that that that that you
156:57 - want to predict uh the the application
156:59 - is predicting
157:01 - exams course exam scores predicting exam
157:06 - scores so in a build a function f that
157:08 - takes in the number of hours you study
157:10 - number of hours you study now what was
157:12 - the r study and predicts the exam
157:16 - predicts this exam scores of that
157:18 - particular child so that's that's the
157:21 - basic uh that's the basic function and
157:23 - you and you build and you build a
157:25 - fortune we'll talk about how we come up
157:26 - with this mnb later on but to build the
157:29 - function f of x which is nothing but
157:30 - equals to beta0 plus beta 1 x so f of x
157:34 - number of our study which is equals to
157:36 - assume that your beta0 is 60 plus
157:39 - um plus beta 1 is 5 and X so we have
157:41 - just taken random one we are just taking
157:43 - random number for a second for example
157:44 - talk about how can we take out this
157:45 - beta0 beta one later on but just assume
157:48 - that you're taking the random one right
157:49 - so over here over here your your
157:52 - intercept term your intercept term is a
157:54 - number 60 and your and your coefficient
157:57 - is nothing but uh uh uh the five as is
158:01 - as this five so so your beta0 is 60 this
158:05 - means that if if this should if the if
158:08 - this shouldn't have been studied
158:09 - anything if the student haven't started
158:11 - ending which means it goes to 0 60 plus
158:13 - 5 times 0 is another 60 so if storing
158:16 - has not studied anything at all which is
158:19 - a in with that is equals to zero the
158:21 - predicted X exam score would be nothing
158:23 - but 60 would be nothing but 60 which
158:27 - represents the average which which is
158:30 - nothing but the average score which is
158:31 - nothing but average scores of students
158:34 - who do not study who don't don't study
158:36 - so 60 is nothing but the average score
158:39 - of the students who don't sell that it's
158:41 - a bit higher but that's what the beta0
158:44 - represent beta0 represents the situation
158:46 - when your x0 is nothing but equals to x
158:50 - x 1 is in nothing what equals to zero
158:52 - which means that that in this example
158:54 - the person has not studied at all or in
158:56 - on a previous example the size of the
158:59 - house is zero which means the average of
159:01 - the size average house of the price when
159:03 - size of the house is zero okay I hope
159:06 - that you understood what exactly this
159:07 - beta0 means if this beta0 means and it
159:12 - uh yeah it it makes sense but in case of
159:15 - sometimes video that does not make sense
159:18 - but it acts as a biased term but it acts
159:21 - as a buyer's term the average term uh
159:24 - that's very well this is a bias as well
159:26 - Okay cool so we have studied about beta0
159:30 - now let's talk about beta1 now now let's
159:32 - talk about beta 1. let's talk about beta
159:36 - 1 in in predicted so beta1 so beta0
159:39 - represents so when X is X x is zero but
159:42 - beta 1 is the coefficient of x right
159:44 - beta 1 is a coefficient of x so we are
159:46 - multiplying x with something beta 1 that
159:49 - means it represents that how the how
159:52 - this beta 1 will affect this output
159:54 - variable y so it says that it is
159:57 - repeater one represents the change in y
160:00 - for one unit increase in X so if X
160:02 - increases by one unit what will be the
160:05 - change in y that's what the beta 1
160:07 - represents that's what the beta 1
160:09 - represents beta 1 represents what the
160:12 - change in X the change in X the change
160:15 - in X for a word sorry the change in y
160:18 - the change in y what how how much y will
160:21 - change if we increase X by one unit
160:24 - let's take a very simple example let's
160:27 - take a very simple example to make you
160:29 - understand about this for example if we
160:32 - have a simple linear regression model
160:34 - where y represents where your particular
160:36 - y represents the number of hours of
160:38 - sleep a person gets okay the number of
160:41 - hours you're going to build a function f
160:42 - you know you want to build you want to
160:44 - build a function f that you're gonna
160:45 - build a function f
160:47 - now in a bit of the Builder function f
160:50 - so let me just clear this so over here
160:52 - we build a linear version vary on a
160:55 - predict the number of hours a person
160:57 - will sleep the number of hours person
160:59 - will sleep uh the number of hours the
161:03 - person given given that then given beta0
161:06 - plus beta 1 the number of a cop then the
161:09 - number of coffee cups the number of
161:12 - coffee cups like the the problem
161:14 - statement is going to predict the number
161:15 - of hours the person will sleep given how
161:18 - how many number of a coffee cups he has
161:20 - drank okay that's that's the function
161:23 - which you know uh me so if you have a
161:25 - simple linear Vision model where y
161:27 - represents the number of R so y
161:29 - represents the number of R's a
161:30 - particular man's lead we divided this is
161:32 - the number of ask and gets it with the
161:35 - sleep a person gets and X represents
161:38 - your X is nothing but the number of a
161:40 - copy cops a person had right then the
161:43 - beta one represents the beta one how
161:45 - this is how what does beta 1 Test B data
161:48 - 1 represents the change in hours of
161:50 - sleep for 1 cup increase in a coffee
161:52 - consumption so if we increase the coffee
161:55 - consumption by one hour what will be the
161:57 - change in number of hours of sleep of a
161:58 - particular person okay I hope that your
162:01 - understanding over here the number of
162:04 - coffee cups assume that number of how if
162:06 - we increase if we drank one more coffee
162:08 - what will be the effect on this output
162:11 - variable that's what beta 1 says about
162:13 - it that's what beta1 says about it let's
162:15 - take a look at the previous example
162:17 - which which we had so the example was
162:20 - the predicting service exams course
162:21 - we're going to build a function f that
162:22 - takes in the number of study hours the
162:24 - particular person predicts so beta0 plus
162:26 - beta 1X 60 plus 5x so now we have now
162:29 - now we don't have x equals to Z now we
162:31 - don't have that that is worth a beta0
162:34 - but now assume that assume that um
162:37 - what this is beta1 Tells over here beta
162:40 - was tells that for every additional hour
162:42 - of a study if we increase X by 1 or
162:46 - which is one unit which is every
162:47 - additional hour of study
162:50 - X increases by 1 the predicted score
162:53 - exam score also increases by 5 increases
162:56 - by 5. so this P this 5 represents the
162:59 - rate of change in Y which is the exam
163:02 - score the rate of change in exam score
163:04 - what will the change in exam score given
163:07 - the given there is one unit increase in
163:10 - x one unit one unit increase one
163:12 - additional hour of study so in this case
163:15 - for every additional hour with study
163:17 - which is X increases by 1 the predicted
163:20 - exam score increases by five increase by
163:25 - five this is the rate of change in y
163:27 - change in y with respect to change in x
163:30 - by one unit increase in X with respect
163:32 - to X okay so assume that if a student
163:35 - studied if a student studied three hours
163:37 - so now F of 3 which is nothing about 60
163:39 - plus 5 times 3 which is 60 plus 5 times
163:44 - 3 which is 15 which is 75 which is 75 so
163:47 - student has studied three units and
163:49 - three three hours extra three hours
163:51 - additional hours so three times five so
163:54 - it will increase by 15 times the
163:57 - Baseline will be increased by 15 times
163:59 - so we can study if a student has studied
164:01 - for three hours the predict exam score
164:04 - will be 75 which is 15 points higher
164:07 - than the Baseline if this one has not
164:09 - studied at all
164:11 - which means that that does the if your X
164:15 - is equals 3 then there's a then there is
164:17 - increase in X okay then then if for
164:20 - every additional hour whatever is
164:22 - additional hour the his or her score
164:24 - increases by 5. you can see clearly that
164:27 - how this uh how this 5 is affecting your
164:30 - output variable how this 5 is affecting
164:32 - your output variable how this 5 is
164:34 - affecting your output variable how this
164:36 - five is affecting your output available
164:37 - oops
164:39 - 5 5 is affecting the output variable out
164:41 - there I hope that it makes sense to you
164:43 - as well okay
164:47 - currently it was positive but sometimes
164:49 - you you have negative as well sometimes
164:51 - you have negative as well okay sometimes
164:53 - you get negative uh scores as well so
164:56 - let's talk about negativity as well uh
164:58 - which will make much more sense and
165:01 - we'll uh I'll give you a sheet where we
165:03 - have much more examples of
165:04 - interpretation that will make much more
165:06 - sense as well right so assume that uh
165:09 - assume that if you have a linear
165:11 - regression model where y represents the
165:13 - number of R study and X represents the
165:16 - age of a student okay so you you want to
165:19 - predict you want to predict the the
165:22 - number for us a particular student will
165:24 - study given the age of a age of students
165:27 - so you know given the age of person to
165:28 - predict the number password student will
165:30 - study you know given like higher dates
165:32 - higher than high higher different study
165:34 - and the coefficient for X is minus two
165:37 - so you have got beta0 plus beta 1 x and
165:40 - x over here is
165:42 - the the age of a person right so if your
165:45 - beta 1 is nothing at minus 2 which means
165:47 - it's different so what does this
165:49 - indicates this means that every one year
165:52 - increase in X so X is nothing but age
165:54 - right so every one year increase in X
165:56 - the predicted number of hours of study
165:59 - decreases by two you can see our
166:01 - misconception was that it is that that
166:05 - higher the age higher the study hours
166:06 - but over here it decreases so minus 2
166:09 - represents so if your beta is positive
166:13 - this means your If X increases then beta
166:16 - absorbs increases in this case a
166:18 - predicting stress exams course if the
166:20 - study hours increases then the exam also
166:22 - because the beta was positive but in
166:24 - this case if beta is negative my beta 1
166:26 - is negative which is minus 2 so which
166:28 - means for every additional hour which if
166:30 - X increases sorry if beta 1 sorry if if
166:34 - if if if X increases your output value
166:37 - decreases
166:40 - but in this case if X increases outer
166:43 - values also increases when beta 1 is
166:44 - positive but in this case in beta 1 is
166:46 - negative when X increases output output
166:50 - decreases
166:51 - one such example was that that the the
166:55 - the the in this case beta 1 is -2 which
166:58 - means for every one year increase in age
167:00 - the predicted number of study will
167:02 - decrease by this factor which is minus
167:04 - 2. okay
167:06 - I hope that this makes sense so we have
167:08 - studies we have written over there here
167:09 - as well here as well that if there is a
167:14 - positive relationship there's a positive
167:15 - relationship X increases leads to Y
167:18 - increase as well as well okay
167:22 - so you can see increase in size also
167:24 - also in reason uh price so I hope that
167:27 - this gives a good sense about whatever
167:28 - we are talking about I'll take one more
167:30 - example maybe to make you understand uh
167:33 - in much more detail so that we are on a
167:35 - good page
167:37 - okay so what exactly what I what I want
167:39 - to do is uh take an example of
167:42 - predicting sales you know to take an
167:45 - example of predicting take an example of
167:48 - predicting sales and this is this is a
167:50 - nice example to start talking on so
167:53 - suppose you have a data set that
167:55 - contains information on Advertising
167:56 - expenses and the sales of a company so
168:00 - so you know you know make a function f
168:01 - you know make a function f that that
168:03 - you're going to predict the the
168:05 - advertising expenses of the you know
168:08 - predict the revenue you know predicted a
168:09 - venue the the sales of a person based on
168:12 - the like you know give the advertising
168:14 - expenses like how much you expect how
168:16 - much you spend on advertising and and
168:18 - and predict what and you know predict
168:20 - why which is the sales you know you know
168:22 - establish a relationship between
168:23 - advertising expenses and the sales
168:25 - advertising expenses means the the money
168:28 - you spent on Advertising of of a company
168:30 - okay so you have only one feature X and
168:32 - you're gonna predict to buy and predict
168:34 - why so your so your uh f of x will be
168:36 - nothing but X is because advertising
168:37 - expenses which is equals to beta0 plus
168:39 - beta1 X so assume that your beta0 is
168:42 - nothing but one thousand plus beta 1
168:44 - which is 0.8 x 0.8 X so what does this
168:48 - mean what does this signify so this
168:51 - signify that your beta0 is thousand beta
168:55 - 0 is thousand what does this mean it is
168:56 - thousand if the company does not spend
168:59 - any money on sales if the company does
169:02 - not spend any money on sales which means
169:04 - X is equals to zero your sales with the
169:07 - the revenue the revenue there's the
169:10 - sales revenue would be one thousand
169:13 - which is the average which is the
169:14 - Baseline when your Baseline of Y which
169:17 - represents the average revenue of
169:18 - company when they do not spend without
169:21 - any advertising okay
169:23 - let's beta0 but what about beta 1 but
169:26 - what about beta 1 beta 1 over here is
169:28 - 0.8 so this says that for every
169:31 - additional dollar we spend for every
169:33 - additional dollar dollar wage we spend
169:36 - which is one unit increase in X will
169:39 - lead to increase uh the total will lead
169:42 - to the sales revenue increase by 0.8
169:45 - dollars so if increase one dollar on
169:47 - Revenue then 0.8 then we'll or will get
169:50 - a raise of 0.8 dollars in Revenue
169:52 - so this is the rate of change so this is
169:55 - a positive relationship so it means the
169:57 - the the the the advertising uh revenue
170:00 - is in coupled increases uh then the
170:03 - sales also increases
170:06 - okay so assume that your company spends
170:08 - five thousand dollars on Advertising so
170:11 - your if five thousand okay this is
170:14 - equals to beta0 1000 plus 0.8 5000.
170:20 - so this is nothing but your your total
170:22 - answer would be tot your total so your
170:25 - total answer would be five thousand
170:27 - which is five thousand so your total uh
170:30 - your your your answer would be let me
170:31 - just calculate so it will much worse
170:33 - 1000
170:35 - plus 0.8 times 5
170:39 - 000.
170:40 - 5000 which is nothing but five thousand
170:43 - uh dollar is the five five thousand
170:46 - which would would be the sales so
170:49 - basically if your company has a spend
170:51 - five thousand dollar on Advertising the
170:54 - predicted the predicted sales or the
170:56 - revenue would be five thousand which is
170:59 - four thousand higher with the five
171:01 - thousand or with the final sales if you
171:03 - if if you uh spend five five thousand
171:05 - dollars the the five five thousand
171:08 - advertising which means that there's a
171:10 - four thousand higher four thousand
171:12 - higher than the Baseline Revenue so this
171:14 - is four thousand higher than the
171:15 - Baseline Revenue right so this is a
171:17 - positive positive guy so it says that
171:19 - for every increase in for every in for
171:21 - everyone you increase in the
171:23 - advertisement every one dollar increase
171:25 - in the advertising your Revenue will
171:28 - increase by 0.8 dollars
171:32 - I hope that this makes sense now I'm now
171:34 - I'm not going to talk more in that uh
171:36 - will you be I've given you in enough
171:38 - examples for you to explore by your own
171:40 - uh I hope that this really helped I'll
171:42 - be catching up in the next lecture when
171:43 - we'll talk about how can we estimate our
171:46 - ex how can we estimate our beta0 and
171:49 - beta 1 in 3D let's get started in the
171:51 - next lecture hey everyone welcome back
171:53 - to this lecture in this particular
171:55 - lecture what exactly we are going to
171:56 - cover is uh we'll just recapitulate
171:59 - whatever we have learned and then we'll
172:02 - talk about how can we find beta0 and
172:04 - beta1 in the in like our whole you know
172:08 - discussion is uh if we don't have better
172:11 - beta bet Azure and beta1 we'll be not
172:13 - getting our best line eventually we'll
172:14 - be failing to get the good predictions
172:16 - so that was one of our one of our uh
172:18 - goal so let's recapsulate whatever uh we
172:21 - have done so we haven't our hypothesis
172:24 - function or a prediction function our
172:26 - hypothesis function or prediction
172:27 - function and over here the hypothesis or
172:29 - prediction function states that you have
172:30 - a hfx and a builder function or maybe um
172:32 - FX that that you're in a better version
172:34 - F that takes in the value of x and then
172:36 - and then uh taking value of x and map it
172:39 - to Y and Y can be any any problem for
172:42 - example you want to predict the sales
172:43 - given uh given the advertising expenses
172:46 - or you want to predict the exam scores
172:47 - given the number of hours study so you
172:49 - have input value of only one in one
172:51 - input feature as of now you have only
172:53 - one input feature X you have only one
172:55 - input feature X and given uh and and
172:57 - using this one input feature X sort of
172:58 - predicting what is the uh y right but it
173:01 - may happen that you have several input
173:03 - features X2 X3 all that are out of the X
173:05 - and you have several input features and
173:07 - based on the several input features you
173:08 - will predict the Y but we'll take a look
173:10 - at that several one later on but first
173:12 - of all let's build our Baseline so
173:13 - whenever you work on any type of problem
173:15 - the first you should think about how can
173:18 - we how can we build the Baseline build
173:22 - the Baseline and a working solution then
173:25 - after building a baseline we can extend
173:27 - it okay usually I have seen a lot of
173:29 - people you know initially they started
173:30 - focusing on Advanced things and
173:32 - eventually they they end up losing time
173:34 - money and Etc so what I recommend is
173:36 - start with the very basic or same
173:38 - Baseline and then build upon that so
173:40 - just like this we are doing over here we
173:42 - are currently taking only one feature as
173:43 - of now only one feature and then we are
173:45 - trying to predict and then and then you
173:46 - will see that how will extend it
173:48 - streamlining it very nicely to other
173:50 - features as well two several features
173:52 - several input features as well okay
173:56 - cool so
173:57 - um let's get started so over here which
173:59 - you which which you're seeing over here
174:01 - which is the hypothesis function which
174:02 - is the prediction function you put in
174:03 - the value of x and we get a prediction
174:04 - and you have to learn your beta0 and
174:06 - beta 1.
174:07 - you have to find USB resident beta 1 and
174:10 - then you have your MSC which is which is
174:12 - used to calculate which is used to
174:14 - calculate your uh how well your best fit
174:17 - line is how well your best fit line is
174:19 - which is 1 over 2m or I equals to
174:21 - alternate the m h of x i minus y i where
174:25 - we are taking the difference of
174:26 - residuals and the squaring to to remove
174:28 - the negative sign and then we are
174:30 - averaging it up so that we we get an
174:32 - overall sense and we're adding 2 as a
174:34 - convention or you will see that how this
174:36 - two will help us later on but as of now
174:38 - assume that this is a convention so
174:40 - what's what's our goal is our goal is to
174:42 - minimize a mean squared error which is
174:44 - mean squared or Y divided mean square
174:46 - error because it's we are taking mean of
174:47 - the squares of the error because we are
174:50 - taking squares of the error because
174:51 - these are called errors because these
174:52 - are residual the differences are errors
174:54 - and they're squaring this up and then
174:55 - you're taking them in that's that's
174:56 - where it's called the mean squared error
174:58 - and our end goal is to minimize this
175:01 - mean square error minimize this mean
175:02 - square error in order to find your bl0
175:04 - and beta 1 because our best fit line
175:06 - depends on this blog beta0 and beta1 so
175:10 - we'll see approach that is commonly
175:12 - finding these so we have to find this
175:14 - beta 0 and beta 1. so what are what are
175:16 - your initial ideas right so let's talk
175:18 - about what are initial ideas I would
175:20 - like to give you some seconds five
175:21 - seconds from now on to think about what
175:25 - what ideas what idea you can come up
175:27 - with what idea you can come up with to
175:30 - actually to actually get this beta0 and
175:33 - beta1 so you can take five seconds from
175:36 - now on to think about what exactly you
175:39 - want to achieve and you can actually
175:40 - take a break as well if you are just
175:42 - feeling overwhelmed but I'm trying to
175:43 - make an explanation so easier
175:46 - so I hope that you have understood now
175:48 - over here in I what is the idea number
175:50 - one the idea number one is to try out
175:54 - several values of beta0 and beta 1 and
175:57 - see if with that particular parameter
175:59 - whether your cost function is decreasing
176:01 - or not so what I'm trying to say that
176:03 - you can try out several first first what
176:06 - you what the idea is that you can try
176:09 - out random say for example you guess
176:10 - beta0
176:12 - oops what happened to me
176:15 - OMG
176:17 - or give it uh yeah so you can try
176:19 - several values let's let us assume beta0
176:21 - to E1 and beta1 to be zero you've taken
176:24 - these values and then you plug in the
176:26 - value of over your beta0 beta 1 and then
176:28 - you evalu and then you take in the
176:29 - predictions for every X's X is out here
176:31 - for every predictions and then you've
176:33 - taken out the MSC and then we have taken
176:35 - we have done this please don't uh
176:37 - interfere if you don't please see my
176:39 - past lectures okay so we have we will
176:41 - take out the MSC now we'll take out the
176:43 - MSC so if with the 0 and 1 if our MSC is
176:47 - decreasing first of all if if our MSC is
176:50 - decreasing if our cost is decreasing if
176:52 - our error is decreasing then what we did
176:54 - then then we'll update our values of
176:56 - beta0 from for example assume that the
176:59 - first in the when when you get started
177:01 - you say every to be 0 and 0 you
177:03 - initialize beta 1 and beta 0 to be
177:05 - random values let us assume bit 0 and
177:07 - beta0 will be initialize the zero and
177:09 - beta 1 is 1. so first of all initialize
177:11 - and then you try a different value okay
177:13 - and then you take out the cost function
177:14 - with this m a mean square error the
177:16 - error from this particular uh betas and
177:19 - then what you do you oh you you randomly
177:21 - take another value you just take another
177:23 - value the X 0.25 or beta 1 0 1.75 you
177:27 - take the value and then you check MSC
177:29 - and then you check your MSC if the MSC
177:31 - is less than the previous one then you
177:33 - update the beta value okay then you
177:35 - update the beta value
177:37 - right so you update the beta value if
177:40 - with this with this new value if your Ms
177:42 - dick is is less than the previous one if
177:45 - your error is less than the previous one
177:46 - right and then you update your beta
177:48 - value to be the new one whose error was
177:49 - less than the previous one okay and then
177:51 - after this you have beta0 to be 0.25
177:53 - beta 1 episode so it's 1.75 now this
177:56 - updated one right where MSC something
177:58 - MSC C okay now you update we now again
178:01 - update 0. maybe 95 and beta1 to be zero
178:05 - Now Beta 0 is 0.95 and beta1 to be 1.95
178:08 - so you take to you you take these two
178:10 - values and then you check okay now if I
178:13 - check our MSE is less than previous one
178:16 - if yes then we have the then we again
178:18 - update the value then we use the not
178:20 - previously but the current which is 1.95
178:22 - and beta beta 1 equals one point uh 995
178:25 - beta 0 is equal to 0.95 okay so we use
178:27 - the updated one so what exactly we are
178:29 - doing we are trying out several values
178:31 - of beta0 and beta 1 and see if with that
178:34 - particular value of the parameter our
178:35 - cost one should decreasing as compute
178:37 - previous one or not if it is then you
178:40 - update and if it is not we just stick to
178:41 - the previous one okay
178:44 - so our first step in this idea what we
178:46 - would initialize your beta0 and beta 1
178:48 - and we evaluate with these uh how do we
178:51 - evaluate using this cost function where
178:52 - you predict on every samples of x i and
178:54 - then you compare it with the y i and y
178:56 - hat I and then you simply predict it so
178:58 - the cost function will be super duper
179:00 - high right so the cost function will be
179:02 - super duper High because when you have
179:03 - beta0 to be 0 and beta 1 because so it's
179:05 - the cost function will be super super
179:06 - high now you try different values of
179:08 - beta0 and beta 1 like 1.4 and 2.5
179:10 - irrespectively which is beta04 1.5 beta
179:13 - 1 through 2.4 and see if the cost wasn't
179:16 - decreasing so if the cost function is
179:18 - decreasing as compared to previous one
179:20 - you update the beta value from 0 to
179:22 - these values and keep on doing this
179:25 - until and unless you get low error
179:27 - you get low error right so you have to
179:30 - try out several values of beta and beta
179:31 - B beta 1 beta zero and then until next
179:33 - to get a lower error so now what's the
179:36 - big deal in this the big deal in this is
179:39 - that we can keep on trying
179:42 - so what to do like this is something
179:44 - which you have to think of right so we
179:46 - need a strategy right so we can keep on
179:49 - trying but we should know a strategy to
179:51 - keep on trying we cannot just think
179:53 - about any any value and then just go
179:55 - with it right it will definitely not
179:57 - make sense right so what exactly we want
180:01 - we want to think about in terms of we
180:04 - want a strategy for us or we want an
180:07 - algorithm to try for us this is a good a
180:10 - good good uh this is a good strategy but
180:12 - with an algorithm to try this strategy
180:14 - for us to try this strategy for us in an
180:17 - efficient and timely manner within
180:20 - strategy so what's that strategy the
180:22 - strategies we want we want a system
180:25 - which automatically tries out several
180:27 - values without any human intervention
180:29 - tries out every values every in every
180:32 - iteration so what is iteration in this
180:34 - case you might be confused in this so
180:36 - when your beta 1 is equal to zero and
180:38 - beta0 is equal to zero so you have these
180:40 - betas right this is now when you update
180:43 - now you now you update the value so the
180:45 - first iteration would be now you update
180:47 - the value and then check the Mac then a
180:49 - second iteration you update this the
180:51 - value asked about the previous one and
180:52 - then check MSC is less than if it is
180:54 - then you this is second iteration the
180:56 - third iteration means the third time
180:57 - you're changing the value and seeing if
180:58 - the msc's so that's iteration okay so we
181:01 - want a system which automatically tries
181:03 - out the values every iteration in such a
181:06 - way that your iteration at nth value you
181:09 - iteration at the last parameter value
181:12 - which is the last iteration or something
181:14 - like that would assume that you want to
181:15 - make a 20 iteration so at 20 the
181:17 - iteration is better or maybe this your
181:20 - second iteration at the the the error at
181:23 - the second iteration where you have
181:24 - updated beta 1 and beta0 should be less
181:27 - than the previous one okay it is what is
181:29 - telling the iteration at the nth value
181:31 - where for example third iteration in the
181:33 - in the third iteration the parameters
181:35 - value of the third iteration should be
181:36 - better than the second iteration
181:38 - and how to check betterness by MSC
181:41 - it's better than that before iteration
181:43 - oh so that you got it this is a strategy
181:45 - you want now how can we come up with
181:48 - this strategy this is something you know
181:49 - this this is something which you should
181:51 - be able to uh confront by yourself so
181:53 - how what is the exact strategy for this
181:56 - to occur so the strategy is the strategy
182:00 - is uh the strategy is we can make use of
182:02 - something known as uh let's not go to
182:05 - what exactly I have a very different
182:06 - mind to teach you all so assume that
182:08 - assume that uh we we want to know so
182:11 - what exactly we we want to know so how
182:13 - can we check how can we check we want to
182:15 - check that that that whether by cost
182:19 - function or whether my mean square error
182:21 - is decreasing if I change beta0 us um if
182:26 - if I change the beta 0. so what exactly
182:29 - you wanna know whether whether my cost
182:34 - function cost function is decreasing if
182:38 - I change my what if I change my
182:43 - beta0 or and beta1 so I want to check
182:46 - this right I want to check this so you
182:48 - might have heard about uh let's let's
182:50 - let's start going again a mathematical
182:51 - term so we wanna we wanna check this
182:53 - right so what is the week and we can
182:55 - check this so you might have heard about
182:58 - um slope okay so let's talk about change
183:01 - in y over change in X how much y changes
183:04 - how much y changes when X changes that's
183:07 - that's what is change in whatever change
183:09 - in X so how much y changes when X
183:11 - changes so what if I can write how much
183:14 - J changes how much your cost function
183:16 - changes when you change when you change
183:19 - your beta0 and how much your cost
183:21 - function changes when you change your
183:23 - beta1 so can we get our change the rate
183:26 - of change
183:27 - right so if my cost function decreasing
183:29 - as compared to be as cash come to the
183:31 - previous one this is good we can check
183:33 - the rate of change by nothing but the
183:37 - slope by nothing but the slope uh we
183:40 - which which you have heard now now here
183:43 - comes an interesting part now over here
183:45 - how can we take out the slope of this
183:47 - because we eventually want to take out
183:49 - the slope of this particular uh function
183:51 - how can we take out this a very
183:53 - interesting question so the though let's
183:56 - let's let's worry about first of all
183:58 - tell me what is the if you if you can
184:00 - applaud this function f of x how what is
184:03 - the what what will the plot looks like
184:04 - so I would recommend you to plot this
184:07 - function on a graph right now where you
184:10 - plug in the values of x and x squared
184:12 - plug in the values one one two four
184:14 - three nine plug in the several values
184:16 - and then plot it the plot which you're
184:18 - going to get the plot which you're going
184:20 - to get is a nothing it's nothing but
184:24 - something like this
184:27 - but something like this okay uh parabola
184:33 - okay or something like the uh some the
184:36 - so if not not exactly Parabola but uh
184:39 - you if you plot this on if I I just
184:42 - plotted it on terms of you know a
184:44 - positive one but if you plot this uh but
184:47 - the but the parabola if you if you
184:49 - eventually plot this up if you
184:50 - eventually plot this up so let me let me
184:52 - let me just plot this up
184:54 - since the plotting would look like this
184:55 - which is the function x squared this is
184:58 - a this is uh this is a function this is
185:00 - a graph of your x squared this is a
185:03 - graph of your x squared now this is the
185:06 - graph of this x its weight so can you
185:08 - tell me how can you take out slope in
185:10 - this because at every point every Point
185:12 - uh your slope changes so there is not a
185:15 - constant slope right so assume that you
185:19 - wanna take a slope at this particular
185:20 - point so at this particular Point what
185:23 - does it tell what what does a slope will
185:25 - tell how much uh that how much it will
185:28 - how what what is at this particular
185:30 - point will will tell how much your x
185:32 - square will change when you change this
185:34 - a little bit let's forget it you can
185:37 - though let's say you wanted to take a
185:40 - slope at this particular point you want
185:42 - to take a slope at this particular point
185:43 - because slope pattern is not is not
185:45 - constant right it's not same all over
185:48 - the graph it keeps on changing right it
185:51 - keeps on changing so now now it is not
185:55 - possible so what we do there's a concept
185:57 - of derivatives which takes out slope of
186:00 - a curved line okay of a curved line I
186:04 - hope that this gives a good sense now
186:05 - this Parabola why does I introduce this
186:07 - Parabola to you interesting question
186:09 - right observe carefully observe
186:12 - carefully can you see this
186:15 - this is something assume that this is
186:17 - something a x and then you're squaring
186:19 - this up isn't your cost function will
186:21 - look something like parabola
186:24 - right because we're squaring directly
186:25 - squaring this up right it will look like
186:28 - something like Parabola so when you plot
186:30 - this equation up and you plot this
186:31 - equation up you will get you will get
186:34 - something like this something like this
186:39 - something like this okay
186:43 - you will get something like that now on
186:46 - x-axis you will have your betas on
186:49 - y-axis you have your errors so now again
186:51 - it might can confuse you for every point
186:55 - of the betas for example let let us
186:56 - assume this is a beta0 okay so in beta0
186:59 - beta0 over here if beta0 is uh if beta0
187:03 - point is over here so your error at this
187:05 - point where error with this beta0 is
187:07 - very high
187:08 - error with that particular beta0 is very
187:10 - very high if beta0 is over here if beta0
187:13 - is over here
187:14 - beta0 over over here error is bit low
187:18 - the previous one if beta0 is over here
187:20 - error is bit lower than so this
187:22 - represents this uh the x axis represents
187:25 - the beta the values of beta 1 beta sorry
187:28 - beta0 and Y represents the error y
187:30 - represents the error
187:32 - okay so over here what our goal should
187:36 - be our goal should be to to get the beta
187:40 - to get that beta value to get that beta
187:43 - value over we want to get that beta from
187:46 - here to here so that our error is very
187:50 - very less approximately equals to zero
187:54 - So eventually we're gonna get so assume
187:57 - that this is a beta 1 this is a beta1
187:59 - and then over when when the beta1 is
188:02 - over let us assume beta1 is something
188:04 - like
188:05 - um four
188:07 - is something like 4 okay beta1 is
188:11 - something like four so what will be the
188:13 - error error will be super duper High
188:14 - beta 1 so now you have beta 1 at a
188:18 - little bit less than or maybe uh
188:20 - something like that so b b beta 1 is in
188:22 - this case if beta 1 is less than the
188:24 - previous one and the error is also less
188:26 - so 80 I'm just taking an example it can
188:28 - be more as well it can be more as well
188:31 - don't worry don't worry about anything
188:32 - currently it's a very simplest example
188:34 - which I can plot for you okay which is
188:37 - now beta1 you try you wanna reach to a
188:39 - global Minima you wanna reach to a
188:41 - global Minima not local Global Minima
188:43 - where your error is very very less where
188:46 - is negligible so you're going to start
188:48 - with the beta value whose error is very
188:51 - very high and then you have to learn the
188:53 - beta value on a chain keep on changing
188:55 - the beta value so you're seeing that we
188:56 - are changing this is definitely beta
188:58 - values aren't in unless our error is
189:00 - nothing but equals to zero
189:03 - okay I hope that you're getting what I'm
189:05 - trying to send me I'm just trying to
189:07 - convey that you're gonna keep on
189:08 - changing beta value you keep on changing
189:10 - the middle value that will help us that
189:13 - will help us to get your beta that that
189:16 - will help you that that that will help
189:18 - you to get beta 1 equals two so beta 1
189:21 - the error of beta 1 is equals to zero
189:23 - okay
189:24 - I hope that you that that you're getting
189:26 - so how can we take out the rate of
189:28 - change how can we take out the rate of
189:30 - change so how much your error changes so
189:32 - when when your beta beta0 changes how
189:35 - much you're at how much your error
189:37 - changes when betas are at this
189:38 - particular Point changes beta0 will be
189:40 - saying will will be not same it will be
189:42 - changing right so what will the what
189:43 - will the change in error when that at
189:46 - this particular point so what is what is
189:47 - the derivative of your error with
189:49 - respect to this beta0 at this particular
189:51 - point
189:52 - okay
189:54 - so let's uh uh so it will be more clear
189:57 - once we uh understand more in
190:00 - mathematical perspective but eventually
190:02 - what our derivative is telling it is
190:04 - telling that how mature Dairy how mature
190:07 - uh cost functional error will change if
190:10 - you change your beta a little bit
190:13 - okay
190:15 - let me change your beta a little bit so
190:17 - we use the concept of differentiation
190:19 - which is how much y changes when X
190:21 - changes Matlab key how much your error
190:23 - will change when your beta changes so in
190:26 - a nutshell we are looking for this
190:28 - strategy one we are just looking for the
190:30 - strategy where we can take a look if
190:32 - this something increases or decreases
190:33 - we're just looking for that strategy and
190:35 - here we go here we go so here we go
190:38 - where it tests the d y by DX where
190:40 - you're taking out that where where we
190:42 - are when when you take out the start
190:43 - when you you have to do for individually
190:45 - for different different parameters
190:46 - because you have two parameters beta0
190:48 - and beta 1 right beta beta 01 is slope
190:51 - and one is Pierson but it's not
190:52 - something like it's the best of now but
190:54 - over here it's the stock the
190:56 - optimization problem so you eventually
190:57 - take out the derivative of your cost
191:00 - function with respect to beta0 and
191:01 - derivative of the COS function with
191:03 - respect to beta 1 because we have to do
191:04 - so it tells you it this tells you how
191:07 - much your cost function will change if
191:08 - you change beta0 a little bit how much
191:10 - this cross function will change when you
191:12 - change beta 1 a little bit so if that
191:14 - changes if this it it it will happen in
191:16 - every iteration so the second iteration
191:18 - assume that it changes and it decreases
191:20 - then it then it if and then compares to
191:23 - from the previous one and see if there's
191:24 - something low yes then you update the
191:26 - value of beta0
191:27 - so there's a change in a when beta0
191:30 - changes same here as well same here as
191:32 - well okay
191:34 - I hope that you're getting what exactly
191:37 - I'm trying to convey now here's your
191:40 - algorithm here's your algorithm here's
191:42 - your algorithm
191:44 - um for updation like you get a change
191:46 - now you get a change now you have to
191:48 - update no you have to update your values
191:50 - as well and then you look for the
191:51 - another beta value right
191:53 - so you update the this is this is called
191:56 - the update rule this is called the
191:58 - update update rule where you first of
192:01 - all update beta0 with old beta so
192:04 - basically you have a new new beta0 new
192:06 - beta0 and then we'll assign this we this
192:09 - is assignment operator where you have
192:10 - new beta Nu beta is equals to Old beta
192:13 - minus the learning return I'll talk
192:15 - about that times the change in X with
192:18 - respect to the the the change in your
192:21 - cost function how much your day of the
192:23 - how much error changes when beta0
192:25 - changes
192:27 - we'll expand this will will take out the
192:29 - derivative in just a second but assume
192:31 - that so we multiply the learning rate
192:32 - for this so what is learning rate
192:35 - what is learning rate so but but before
192:38 - that uh we'll we'll talk about that
192:40 - learning rate later on but let's let's
192:42 - stick at some point
192:44 - over here beta0 is is the old beta0 like
192:48 - over here if you have beta 0 to be 0 and
192:50 - beta 1 is equal to 0 then this is the O
192:52 - this now we have the second iteration
192:53 - second side second iteration here beta 1
192:55 - uh beta uh so you have been new beta and
192:58 - you have to take out the new beta so you
193:00 - have the when you actually convert beta
193:01 - 1 is you assume that you have a zero
193:04 - minus the linear Alpha with respect so
193:06 - basically this is the old old beta has
193:08 - covered the previous one the pre the
193:09 - previous beta okay the old beta minus
193:12 - the learning with Alpha and this change
193:13 - change of rate of change of rate of cost
193:16 - function with respect to this beta0 and
193:18 - then you do for beta 1 as well I'm so
193:19 - sorry this this
193:21 - should be beta1
193:24 - this should be the one okay this should
193:27 - be beta1 uh now over here over here so
193:30 - now you may have several questions the
193:31 - first question is would be why are we
193:33 - subtracting our old beta the reason why
193:36 - we are subtracting because we're going
193:37 - to minimize our function right one way
193:39 - to think about it it's it's it's it's
193:41 - not something uh uh legal but we can
193:45 - think about that we want to minimize our
193:47 - function right when we want to go down
193:49 - and minimize our function so that our
193:51 - error is less so we're gonna minimize
193:53 - that's why we subtract I mean if
193:55 - anything you need to minimize sort of
193:56 - subtract and then it will tell you it
193:59 - will get this this change this this
194:01 - calculates the change it calculates the
194:04 - exact points using derivatives at the
194:06 - current points okay
194:09 - and of course there's the change and
194:11 - then you were learning rate Alpha which
194:13 - is nothing which is nothing which is
194:15 - nothing which is nothing but the
194:19 - learning rate Alpha so over here over
194:21 - here you are going down at a certain
194:24 - speed right at a certain rate
194:26 - at a certain rate you're going down at a
194:30 - certain rate so the rate of going down
194:32 - the rate of going down the rate of
194:34 - reaching to a global Minima is nothing
194:36 - but called The
194:38 - Learning rate okay it's nothing but
194:41 - called The Learning rate the rate or by
194:42 - which we go in we we learn with the the
194:45 - read by which we learn is also known as
194:47 - learning rate we'll take a look at we'll
194:49 - take a we'll have a separate video on
194:51 - learning rate we'll talk about some of
194:53 - the best cases and cases of learning in
194:55 - great detail but you can assume as of
194:57 - now it's case the back it scales the
194:59 - change by the factor of Alpha and that
195:01 - is the rate of learning in a geometrical
195:03 - perspective
195:06 - I hope your understanding
195:08 - cool
195:09 - let's get started
195:13 - top so the what are the steps to to
195:16 - summarize you initialize your beta0 to
195:18 - band beta 1 equals to zero I assume that
195:20 - U is ratio of beta 1 to any number any
195:22 - random values and we there is a
195:24 - initialization techniques we'll study in
195:26 - deep learning not as of now but you can
195:27 - ignore it you calculate and gradient at
195:30 - the current point with the current
195:31 - current value of course the error will
195:33 - be high you scale it a factor of X when
195:36 - minimize it by subtracting you subtract
195:38 - the old betas here the update rule app
195:39 - happens and they repeat two and three
195:41 - points two and three points right you
195:43 - repeat you calculate you you and in the
195:46 - first iteration you calculate the
195:47 - gradient with the previous now in the
195:49 - second iteration now your betas are
195:50 - updated now it's not the initial but
195:52 - it's the updated one which you which you
195:53 - which which you get after applying this
195:55 - update rule right and then you apply the
195:57 - derivative and then you uh get the and
196:00 - and then you get the new beta and then
196:02 - you go to third iteration and then you
196:03 - now you it now you beta R of the second
196:05 - iteration so it's okay so you repeat the
196:07 - two and three points how much by how
196:09 - much we're going to repeat first of all
196:11 - we can do it for like Beyond a repeat
196:13 - for 20 iteration third iteration for the
196:15 - iteration or space we can we can learn
196:18 - this when you're when you're uh when
196:20 - you're uh when you actually converge
196:22 - when you actually when your beta when
196:24 - your beta is actually when you're uh
196:25 - when you're when your algorithm is
196:27 - converged so what what we say can burst
196:29 - is that your step size is smaller like a
196:32 - step size smaller than the tolerance and
196:34 - then the learning rate stops when it
196:35 - reaches the global Minima okay
196:38 - and you will learn about this you can
196:40 - ignore this as of now we'll have a
196:41 - separate video on learning rate to make
196:43 - you better understand about that
196:45 - okay so and then we do for beta0 and
196:48 - then we do for and then we do for beta 1
196:50 - as well and then we do for beta 1 as
196:52 - well okay doing the same thing we are
196:54 - updating the beta1 the new beta1 the old
196:56 - beta minus learning refer the change
196:58 - okay and then you repeat until some
197:00 - conditions what is the number of hitters
197:02 - should be 24 or 20 iterations where you
197:04 - I for beta I equals to 0 and 1 we apply
197:06 - the folder for beta0 and there is update
197:08 - with simply simply do for all the betas
197:11 - out there whatever beta 1 and beta0 you
197:13 - do for both one okay
197:15 - now now your question would be how can
197:19 - we calculate the derivative of this So
197:23 - eventually you don't use usually use the
197:25 - derivative you use the partial
197:26 - derivative of jio bi with respect to bi
197:30 - b b i so this this exactly means that
197:33 - the in both means exactly the same but
197:35 - this is a partial derivative which is
197:36 - applied in a vector Calculus if you want
197:38 - to learn more about you know uh calculus
197:40 - and everything if you haven't yet I
197:43 - recommend you to watch this playlist
197:45 - which I created over a year ago which is
197:47 - quite famous playlist uh not by so much
197:49 - of people but eventually it is very nice
197:51 - if you if you wanted to learn about
197:53 - calculus and great detail I've tried
197:54 - after I've considered everything which I
197:56 - know about calculus over here
198:00 - so how can we take out this partial
198:02 - derivative of J of uh how can we take
198:04 - the partial derivative of your cost
198:05 - function with respect to a particular
198:07 - parameter this exactly means the same
198:08 - then how much it cost function changes
198:09 - when you'll be the beta changes
198:13 - okay I hope that this makes sense so you
198:15 - see that they do the same sort of work
198:17 - overall you seem simply same sort of
198:19 - work but this is all the vector calculus
198:20 - so how individual parameter like
198:23 - eventually how individual parameter
198:25 - affects your MSE and over here I have
198:29 - taken out the derivation I have taken
198:31 - out the derivation I have taken out the
198:33 - derivation for your model now you might
198:35 - be confused how this derivation came
198:37 - we'll talk about that for a single
198:39 - training example uh in a free form so
198:42 - I'll just talk about when when will I
198:44 - should do a real world example no then
198:46 - then I'll show you the derivation ignore
198:48 - this as of now this is just for notes
198:50 - purposes I'll show you a worked example
198:53 - Hands-On so that it will it is more
198:56 - clear so now when you when you actually
198:58 - calculate when this will yield to this
199:01 - formula to this formula we'll learn how
199:04 - to utilize this formula in for uh we'll
199:06 - now you plug in the values now over here
199:08 - this this will give you the rate of
199:10 - change
199:10 - okay listen to the rate of change so
199:13 - we'll talk about what is the
199:15 - interpretation of this what why do we
199:17 - use derivative what is the
199:18 - interpretation of this number and
199:20 - various things this is just a starting
199:21 - point my aim was to make you understand
199:24 - that we have an update Rule and then we
199:26 - have this uh give a storyline for what
199:29 - exactly how do we learn and this is the
199:31 - story behind the the learning point
199:33 - okay so you can see that we are having
199:36 - the this is the derivation when you
199:38 - calculate the derivative of the cost
199:40 - function the cost function is which I've
199:41 - seen with respect to particular
199:42 - parameter you get this particular
199:43 - equation
199:44 - okay so we'll put we'll talk we'll talk
199:47 - about this in just just in next section
199:50 - we'll talk when we'll work on work the
199:52 - example please ignore others now but
199:53 - just remember that this is the derived
199:55 - if you don't want to do the calculator
199:57 - stuff you can actually ignore the next
199:58 - section as well but I really really
199:59 - recommend if you want to uh fully
200:01 - understand greatness and you should be
200:03 - able to learn about it
200:05 - okay now I have listed some examples of
200:08 - learning rate so over here which which
200:10 - I've seen learning rate Alpha is a
200:12 - learning rate which decides by how much
200:15 - you should Converse the rate of learning
200:17 - okay so we have learning is 0.1 starts
200:20 - with this and then it slows down and
200:22 - then it converges slowly so if you see
200:24 - that at this point at this point it is
200:26 - very very slow why because it is at as
200:29 - it is learning the end and and it no you
200:31 - can understand this today it knows that
200:33 - the con it knows that it is a my my
200:36 - conversion rate is coming so I have to
200:37 - be very cautious because I am so I have
200:39 - to be a bit slow because when you have a
200:41 - when you drive a car you don't drive at
200:44 - a same speed right you don't have it
200:46 - stays same speed you strive same speed
200:48 - you drive that fast bits whenever
200:49 - required the breaker comes you slow it
200:51 - down right or whenever the stop is going
200:53 - to slow it down that's why the stoppage
200:55 - is slows but if but if your error if you
201:00 - if your learning rate is too high you
201:02 - this is the learning rate which you have
201:03 - to choose according to your problems
201:05 - statement according to the data which
201:07 - will talk about that later on will
201:08 - actually the practical but there's
201:10 - something which you have to tune like
201:11 - try out several and see what works well
201:12 - so if you're learning that it's too high
201:14 - it will mostly diverge so for example if
201:17 - if your car is going on okay with a high
201:20 - speed so if you push a break if you push
201:23 - like 100 kilometer per hour so push a
201:25 - break car is most likely to fall down
201:27 - right well like it it will most likely
201:30 - to fall down it will most like to crash
201:31 - like hell so if the learning result will
201:34 - diverge like this it will never converge
201:36 - to the Minima it will simply keep on
201:38 - diverging you'll it will never reach to
201:39 - a stoppage
201:41 - and if your learning rate is too small
201:44 - then it will never converge okay because
201:46 - it will be two percent super duper so if
201:48 - you drive at one two one meter per se
201:51 - per per kilometer you know it's it's uh
201:54 - I mean I I just mess it up so if you try
201:56 - slowly you'll not able to reach okay
201:58 - cool this these are some of the learning
202:00 - rate variations uh we'll talk about that
202:03 - later on but I hope that you understood
202:04 - what grade is it in upper level now
202:06 - let's go and do the work example and the
202:08 - derivations and all to make you more uh
202:10 - understandable about all of this thing
202:12 - so hey everyone I'm back uh with work
202:16 - example of trading descent learning and
202:19 - we will eventually take a sample data
202:21 - and perform a real time learning on that
202:24 - and then I'll show you how it is super
202:26 - simple to understand query in descent
202:28 - even with mathematical reasoning uh if
202:31 - you're if you're not comfortable even
202:32 - with math I would recommend you to watch
202:34 - it right it will be much more better for
202:36 - you
202:37 - so let's get started
202:39 - um over here I wanna I wanna take a
202:41 - sample data so sample data which I'm
202:43 - which I'm going to take is you have a
202:44 - one input feature well whoops my mind is
202:47 - so diverted I'm using yellow color and
202:49 - white so X and Y and in this X and Y you
202:53 - have 1 2 2 4 3 6 4 8 please note I'm not
203:01 - going to take a huge data otherwise it
203:03 - will take me years but I'm going to take
203:05 - a sample data but I'm going to take a
203:07 - sample data this is a sample data where
203:09 - you have only one independent feature
203:10 - independent
203:13 - independent
203:15 - feature and then you have your output
203:17 - variable output
203:19 - uh variable or Target variable okay you
203:24 - have one independent feature and then
203:26 - you have output variable and Target
203:27 - feature Target variable which predicts
203:30 - in a builder function f in a bill of
203:31 - function f or takes the value of this x
203:34 - is the values of this and Maps it to Y
203:37 - so to build a function f that RFI is any
203:39 - sort of relationship between these two
203:40 - right
203:42 - so basically this for for you it may be
203:45 - super simple right to understand the
203:46 - relationships but eventually I'm going
203:48 - to take an example to show you how our
203:49 - machine learning algorithm will
203:50 - interpret this how our machine algorithm
203:52 - will go on understanding the
203:54 - relationship okay because I am always a
203:56 - Believer or of understanding from the
203:58 - Baseline so that you want to send a core
204:00 - of it you will understand even the big
204:02 - things of it
204:04 - so your f of x is uh when you when
204:06 - you're taking a value of when you put
204:08 - put in the value of x you'll predict y
204:10 - so build a function f so your Builder
204:12 - function f so the equation it's equation
204:14 - so sorry so your equation for this line
204:16 - so equation for this line or the Y
204:18 - equals to MX plus b or Y is equals to
204:22 - Beta is 1 times X Plus beta0 so your
204:25 - point is to take out beta 1 and beta 0
204:28 - out of this using great in this set so
204:30 - what's that first step the first step of
204:33 - ours is nothing step number one step
204:36 - number one initialize initialize
204:40 - your beta 0 and beta 1. so I'm going to
204:44 - initialize beta 1 to B1 and beta 0 to be
204:48 - zero so this is two two things which I'm
204:50 - going to initialize initially okay
204:51 - because an N thing you have to
204:53 - initialize first then the second step
204:55 - which which you have to do the Second
204:57 - Step which you have to do the Second
204:59 - Step which you have to do is calculate
205:01 - the error is calculate the MSC calculate
205:05 - the Mac with the initialized with the
205:09 - initialized beta
205:12 - or values
205:14 - with initialized beta values and so how
205:17 - how can we take out first of all let's
205:18 - say it's very simple you have your uh Y
205:21 - is equals to Beta what what was the beta
205:23 - 1 beta 1 with 1 times X plus zero this
205:27 - Tesla's equation you plug in the values
205:29 - of every X over here you plug in the
205:30 - value one two three four right so uh the
205:34 - Y bread one so y red one which is equals
205:38 - to nothing but uh one times one plus
205:41 - zero which is nothing but equals to one
205:43 - so you keep keep on doing for every
205:44 - sample so I press two wiper three the
205:46 - prediction which is one times two plus
205:48 - zero equals to 2 1 times 3 plus 0 equals
205:51 - to three and one times four plus zero
205:53 - equals to four okay
205:56 - so now you have to calculate the step
205:58 - now you have to after taking the
206:00 - predictions now what you have to do is
206:02 - calculate MSE is calculate is calculate
206:05 - MSC calculate MSE by nothing but 1 by m
206:10 - I is equals to 1 all the way out of the
206:11 - M as well all the all the way around to
206:13 - the m so what exactly we can do we can
206:16 - simply uh say that one thing is we can
206:19 - also use our formula which which we had
206:21 - of that 1 by 2m which is nothing but 7.5
206:24 - divided by 2 which is something 3 plus
206:28 - 35 yeah so what exactly we can do what
206:30 - exactly we can do is we can either use 2
206:32 - over here but as of now let's ignore 2
206:34 - as of now what do you say let's let's
206:36 - ignore two as of now uh because of uh
206:39 - and just just just for sick of
206:42 - Simplicity more but eventually we'll use
206:44 - this too so maybe let's add 2 over here
206:46 - as well 1 by 2 m but uh but eventually
206:50 - we are we're just going to take the
206:51 - squares nothing else more so let's stick
206:54 - to this only what do you say let's stick
206:56 - to this one it's much more uh better so
206:59 - 1 by
207:01 - M of course we will change it later but
207:04 - my point is not to make it perfect
207:06 - accurate but as of now uh if if you see
207:09 - in your machine machine learning
207:11 - libraries they're consistent enough but
207:12 - let us use Simple it's very simple
207:14 - without using 2R and anything which is
207:17 - one by uh and you have your edge of x i
207:19 - h of x i minus y i it's great for the
207:23 - spirit for removing and the difference
207:24 - is it's great for removing the negative
207:26 - sign so now it will yield to 1 minus 2
207:29 - squared plus 2 minus 4 squared plus 3
207:33 - minus 6 squared plus 4 minus 8 squared
207:36 - divided by
207:38 - divided by 4 and that will yield that
207:40 - will yield to nothing at one plus four
207:43 - one plus one plus four plus nine plus
207:47 - sixteen divided by 4 and that will the
207:49 - third P plus 4 which is nothing but 7.5
207:52 - okay this is your error this is your
207:55 - error if you utilize these two values
207:57 - now
207:59 - uh let's add one more page let's add one
208:02 - one one one more page now once you have
208:05 - the once once you have the MSC now you
208:08 - have to use gradient descent now a step
208:10 - is you have to use use Grid in descent
208:13 - use gradient descent to minimize to
208:17 - minimize your uh your cost function
208:20 - which is G of beta which is G of beta so
208:23 - to use Grid in descent to minimize that
208:25 - cool so let's eventually talk about how
208:28 - can we eventually do it so your cost
208:30 - function is so I'm going to write the
208:32 - conventional cost function to show you
208:33 - what exactly this 2 means in uh how how
208:37 - exactly this two eventually help us
208:39 - so uh this it it will make you more
208:42 - clear so 1 by 2 m your I is equals to 1
208:46 - all the amount of the m
208:48 - h of x i minus y i
208:52 - squared so this is your cost function so
208:54 - basically what our goal was to take out
208:56 - the partial derivative of this of this
208:58 - because our what was our grain initial
209:01 - formula formula was this formula right
209:03 - so we have to take out this we will take
209:04 - will take out the derivation we'll we'll
209:06 - take out the we'll do the calc will do
209:08 - the calculation of this how how we even
209:10 - do you will see it right now so we are
209:12 - going to take a derivative of J of beta
209:15 - with respect to with respect to beta0
209:18 - first of all let's let's do for beta 0
209:20 - okay so what this is split will tell it
209:23 - will tell how much will cost until
209:24 - changes when your beta0 changes so it is
209:27 - nothing it is nothing so what what will
209:29 - be undertake of the derivative of this
209:30 - whole set of functions this whole
209:32 - function so how how does it sell so
209:35 - which is nothing but derivative of of
209:37 - the whole functions we will write that
209:39 - with respect to beta0 so the whole
209:40 - function is one by two m
209:43 - I is equals to one all the one out of
209:45 - the m h of x i minus y i squared okay
209:50 - this is your uh this this is what which
209:52 - shown us all so what I'm going to do I'm
209:54 - going to take out the sum rule uh some
209:56 - sum sum outside so I'm just some some
209:57 - rule outside which is 1 by 2 m
210:03 - then we take out the derivative then we
210:05 - take out the derivative of your of your
210:08 - uh this function which is over here
210:10 - you're going to apply the change audio
210:11 - over here you're going to apply the
210:13 - chain rule on this particular function
210:14 - so X of I minus y i squared okay
210:19 - now you're going to take out the
210:20 - derivative of this how you're going to
210:22 - do it we have a two Function One inner
210:24 - one out Outlet so you will take out the
210:27 - 1 by 2 m 1 by 2m I is equals to 1 all
210:30 - the way out of the M derivative of of
210:32 - this particular pass function that
210:34 - derivative of this this particular over
210:36 - here you was the outer function
210:38 - multiplied this is over here we're going
210:41 - to apply the chain rule over here I'm
210:42 - going to apply the chain rule of
210:43 - calculus so we are going to take out the
210:45 - derivative of outer function leaving as
210:48 - it is inside times the derivative of the
210:50 - in inner function the derivative of
210:52 - outer function is nothing but equals to
210:54 - 2 comes over here by a by the power rule
210:56 - and then we subtract minus 1 from here
210:58 - so 2 uh and we leave the inside as it is
211:01 - as it is for outside derivative and over
211:04 - here 2 minus 1 is nothing but equals to
211:05 - one well times your derivative of your
211:08 - inside function which is beta0 and say
211:10 - and say inside function is nothing but
211:12 - minus y i okay so this is your uh this
211:15 - is your uh equation that that you're
211:18 - trying to solve Now 1 by 2 m 1 by 2 m i
211:22 - is equals to 1 all the way out of the M
211:24 - all all different out of the m and then
211:26 - you simply solve for it which is nothing
211:29 - but your 2 and 2 cuts out so you see
211:31 - that 2 and 2 cuts out so now we will not
211:34 - write two and then your final answer
211:36 - would be 1 by m h of x i h of x i minus
211:40 - y i this is your derivation this is your
211:43 - derivation you might think yayush what
211:45 - happened to this this becomes equals to
211:47 - 1 this this becomes equals to 1. why do
211:50 - we say that this is equals to 1. so
211:52 - let's talk about why exactly this is
211:54 - equals to one so when you take out the
211:55 - partial derivative of this function h of
211:57 - x i h of X I A minus y I with respect to
212:01 - beta0 with respect to beta0 what is that
212:03 - if we expand this h of x i is nothing
212:05 - but
212:06 - is nothing but let us assume the beta0
212:10 - plus beta 1 x i minus y I with respect
212:15 - to beta0 right so over here we're going
212:17 - to take the with respect to beta0 so
212:19 - except beta0 every except this except
212:22 - this everything is constant everything
212:24 - is constant so that is that that that
212:28 - that that will be nothing but equals to
212:29 - zero so it will everything is constant
212:31 - and that derivative is nothing but equal
212:33 - to so everything is zero okay now now
212:36 - over here now over here now over here
212:39 - now if if you go uh and if you see that
212:42 - everything will be 0 because it is
212:44 - except zero which is nothing but oh we
212:47 - and the derivative of a constant is you
212:49 - know it's it's zero so now now you see
212:52 - now you add then derivative and now this
212:54 - this one because this is respect this
212:56 - this the whole equation will come P
212:58 - equals to 1 y because because
213:01 - when taking the partial derivative with
213:03 - respect to all the letters except the
213:06 - except beta0 are treated as constants
213:10 - and their derivative constant is nothing
213:12 - but zero and then you have beta0 when
213:14 - you take out the derivative of that is
213:15 - something which equals to one and we
213:16 - have one over here and when you actually
213:18 - do that because it's it it does it does
213:20 - not have a constant and even if it as as
213:23 - we know that a we it it has a constant x
213:25 - 0 and x 0 is nothing but equals to 1.
213:27 - right so I hope that did make sense to
213:30 - you as well uh if it not please see my
213:32 - calculus lectures please see my calculus
213:35 - lectures out here it will make much more
213:37 - sense if you see I'm not going to go in
213:38 - depth explanation but you can understand
213:40 - this way that we come up with when we
213:42 - actually take out the partial derivative
213:44 - of jio beta with respect to beta0 is
213:48 - nothing but 1 by m one by m
213:52 - I is equals to 1 all there are the m y
213:55 - hat I minus y i and y hat is the regular
213:58 - model predictions plus okay so this is
214:01 - your a beta0 now we'll do for beta1
214:04 - and now we'll do for beta1 okay beta1
214:07 - exactly the same the only thing changes
214:09 - instead of one noun so when you actually
214:12 - uh every Everything Remains the Same so
214:14 - whenever when you actually take out the
214:15 - derivative of beta with respect to beta
214:18 - 1 is nothing but equals to everything
214:20 - Remains the Same everything Remains the
214:22 - Same over here everything Remains the
214:24 - Same 1 over m one over M uh I was to one
214:28 - other the m y hat I minus y I multiply
214:33 - multiplied by x i multiplied by x i y
214:35 - multiplied by x i because now in this
214:38 - case now in this case when you actually
214:39 - take out the derivative of this take out
214:41 - that derivative of this you will notice
214:43 - that now h of x i is supposed to Beta 0
214:45 - plus beta 1 x i minus y i right so now
214:50 - take the derivative of this everything
214:52 - except this one is constant this one
214:54 - this one everything is constant now over
214:56 - here we actually have beta 1 times x so
214:59 - the constant is the this is the
215:01 - coefficient of the curve x i so this is
215:03 - X x i would be over Dash
215:07 - okay so X I will be written over there
215:10 - please see my lectures uh if you are
215:12 - confused like how this x again this is a
215:13 - rule of calculus and all if you don't
215:15 - know please ignore uh
215:18 - now over here you have you you got this
215:21 - for beta1 as well right
215:24 - now what you need to do we had an update
215:26 - Rule now we have to update beta0
215:28 - it all beta minus learning that Alpha
215:31 - the derivative of beta with respect to
215:34 - beta0
215:35 - and then we have beta 1
215:37 - beta 1 minus learning Alpha with respect
215:39 - to
215:41 - beta y right you you have this you have
215:43 - this so let's calculate this one let's
215:45 - calculate the first one the partial
215:48 - derivative of beta with respect to beta0
215:50 - is nothing but we have already taken out
215:52 - I've already taken out the equation this
215:55 - was our equation right this this was our
215:57 - equation so first of all our prediction
215:59 - our prediction one minus 2 multiplied by
216:03 - multiplied by uh one minus sorry yeah 1
216:07 - 1 minus 2 V and we are multiplied by one
216:09 - so it's it's we we should not write
216:11 - anything 1 minus two and and then you
216:14 - have 2 minus 4 which is your prediction
216:17 - minus the actual Value Plus 3 minus 6
216:20 - plus 4 minus 8 and then you divide by
216:23 - the total number of values which is 4
216:24 - which is nothing but which is nothing
216:27 - but minus 2.5 which is the rate of
216:30 - change okay you do the same for J of
216:33 - beta with respect to beta 1 which is
216:35 - nothing you will get nothing but 1 minus
216:38 - 2 but this time you're multiplying with
216:40 - x i and the first of the X5 which is X1
216:42 - and this is for X1 right the X X1 is
216:45 - that nothing but equals to one so if you
216:46 - go and Z your X Y is nothing but it's
216:48 - now you now you do for SEC six second
216:49 - one you do for second one which is a 2
216:53 - minus four usually simply add plus 2
216:55 - minus 4 multiplied by two plus three
216:58 - minus 6 multiplied by 3 plus 4 minus
217:01 - eight four minus eight multiplied by 4
217:03 - and then you divide the total by four
217:04 - you get nothing but minus seven point
217:08 - five you get that thing but minus seven
217:10 - point five minus seven point five when
217:13 - you calculate so your uh with the
217:16 - current value of beta0 to be zero and
217:17 - beta 1 to B1 is only about minus two
217:19 - point five and minus 7 which is the rate
217:20 - of change now once we get it now once we
217:23 - get it we can simply apply our update
217:25 - rule our update rule was first of all
217:27 - let's let's do for beta0 our old beta0
217:29 - which we initialize was 0 minus 30 let
217:33 - us assume that your learning rate is
217:34 - Alpha we'll talk about learning the
217:36 - alpha in great detail uh multi
217:38 - multiplied by learning will also
217:40 - multiply it with the rate of change and
217:42 - the rate of change for beta0 was minus
217:43 - two point five right and the beta 0 the
217:46 - updated bit as beta0 is 0.25
217:50 - and then your beta1 you do the same you
217:52 - have one in this case because the old
217:54 - beta was one or minus 0.1 multiplied by
217:57 - minus 7.5 you get nothing but 1.75
218:02 - that's an output okay this will be down
218:04 - this is updated B doesn't updated beta 1
218:06 - after in the first iteration okay as
218:09 - because it now you what now what you do
218:11 - now this this is now you're done with
218:13 - the baby beta0 and beta 1 in the first
218:15 - iteration in the next it's done in the
218:17 - first iteration you calculate the MSE
218:19 - and the updated beta0 and beta 1.
218:21 - calculate the message
218:23 - you'll be noticing that your Mac
218:26 - is low is low so when when you
218:30 - calculated when you calculated the MSC
218:32 - with
218:34 - uh with this uh with beta0 to B with
218:37 - beta0 to be one and zero it was minus it
218:40 - it was seven seven point five but when
218:43 - you'll calculate your MSE with 0.25 and
218:46 - 1.75 it will be less as compared to
218:48 - previous one please note that you can
218:49 - try it out as well
218:52 - and then you do the next iteration
218:54 - number two maybe an iteration number two
218:56 - you do the same step now what you do you
218:59 - calc you calculate them you see you plug
219:01 - in the values of you plug you up again
219:03 - you again make the predictions you again
219:05 - make the predictions with new beta value
219:07 - so again with make the predictions with
219:09 - the numerator value and then you uh take
219:11 - out the error and after taking out the
219:12 - error you simply apply the update rule
219:14 - after calculating this derivatives once
219:17 - again how this derivative is calculated
219:19 - you have your new predictions right if
219:22 - your new predictions and new predictions
219:24 - you will get once again and then you
219:25 - have to calculate that but once again
219:26 - you're getting new derivatives and then
219:28 - you can and then you have the iteration
219:30 - number two iteration number two as well
219:31 - iteration number three you do the same
219:33 - and all until unless you reach to 20 or
219:35 - whatever number of iterations which you
219:36 - have fixed up
219:37 - okay I hope that you're getting what I
219:39 - mean whatever I'm trying to tell you can
219:41 - try out this priori you may be confused
219:43 - about iteration number two so let me
219:44 - just be clear about it what exactly I
219:45 - want to do what you can do you can first
219:48 - of all you be you you have got new betas
219:49 - right you make your wipe red once again
219:51 - once again now you get your four
219:53 - predictions after getting four
219:55 - prediction you take out MSC with these
219:56 - four predictions you've taken out good
219:58 - now what you do you have you all do
220:00 - already have your uh partial derivative
220:02 - of cos function with respect to beta 1
220:04 - and beta 2 you have some equation you
220:06 - plug in the values now the now that
220:08 - derivative you'll get is very different
220:10 - why because your predictions will change
220:12 - because you have because you're using
220:13 - new beta value new parameter value your
220:15 - predictions will change of course so
220:17 - beta that derivative also change and
220:20 - then you plug in the derivative over
220:21 - here with now beta0 and beta1 with a
220:23 - different bear beta0 is 0.25 and 0 beta
220:25 - 1 is
220:26 - 1.75 minus 2.5 and beta 1 to be 1.75
220:30 - right
220:31 - now again you will be getting new betas
220:34 - where your MSC will be low as compared
220:36 - to previous one I hope that this gives
220:37 - you good sense and I really hope that
220:39 - you understood this as well
220:43 - and now I'm just going to uh now we'll
220:46 - ask you to take ring because this was
220:47 - something you know uh very tiring as
220:49 - well for me and it may be tiring as well
220:52 - for you in the next lecture we'll talk
220:53 - about uh why does gradient recent use
220:56 - derivative what is the significance of
220:58 - it and I'll give you a Google sheet
221:00 - where we have we have one more solved
221:02 - problem of it and then we are finally
221:04 - done with learning and then I'll
221:05 - recapsulate you everything make your
221:07 - summary so that you understand it and
221:09 - then you go to other forms of
221:10 - integration to study more about a lot of
221:12 - things so let's catch up in the next
221:13 - lecture hey everyone welcome to this
221:15 - lecture uh so basically what I'm going
221:17 - to do today is talk about some of the
221:19 - key terms which may be uh useful for you
221:22 - and then we'll talk a bit have a bit of
221:24 - discussion around
221:26 - um some of the types of regression
221:28 - algorithm which is a uni varied or
221:30 - simple integration or multivariate and
221:33 - multiple linear equations we'll talk
221:34 - about the other differences and all in
221:36 - great detail okay
221:38 - and then we'll get started with
221:39 - extension form which is quite easy to
221:41 - understand if if you have understood the
221:43 - previous one so first of all let let us
221:45 - understand why does gradient descent use
221:47 - derivative of the cost function so first
221:49 - of all let's figure out that and so any
221:52 - so basically in linear regression so in
221:54 - linear regression what was what is your
221:57 - eventual board what is your eventual
221:59 - goal that's a nice question to start
222:01 - forward so your eventual goal is nothing
222:04 - but to minimize but to minimize your
222:06 - cost function or your MSE so that that
222:09 - is a goal and and and your error between
222:11 - all to minimize the error between your
222:14 - predicted and actual Target value
222:16 - so that we take that partial derivative
222:19 - partial derivative of that cost function
222:21 - with respect to a particular parameter
222:23 - or a coefficient assume that in this
222:25 - case beta0 Okay so particular parameter
222:28 - coefficient and which so what does that
222:30 - derivative gives us is of course that
222:32 - gives us the rate of change but what in
222:34 - terms of how does this help us to go
222:36 - down or how how does it help us to reach
222:38 - to Global Minima that's a very nice
222:40 - question to ask the first it gives the
222:42 - first is the direction to move your data
222:44 - be right whether to increase the beta or
222:47 - whether to increase decrease the beta
222:49 - right so it also gives whether we should
222:51 - increase the beta increase the value
222:53 - from 1 to 12 increase or decrease the
222:55 - beta that's what it gives so direction
222:58 - to move your coefficients or parameters
223:01 - in in order to minimize your error so
223:04 - for example if you have a current zero
223:06 - should we minimize or macro so should we
223:09 - move should we decrease or should we
223:10 - increase this so This exactly the
223:12 - derivative tells and it tells in such a
223:15 - way that it minimizes the error
223:16 - minimizes our MSC okay so the derivative
223:20 - of your mean squared error with respect
223:21 - to a point square with respect to a
223:23 - coefficient or a parameter value or any
223:26 - kind of beta 0 and beta2 sorry beta 1
223:29 - and beta0 provides the rate of change of
223:32 - the MSE with respect to particular so
223:34 - how much that changes when this changes
223:35 - and M so for example if your derivative
223:38 - if if
223:40 - uh derivative if your derivative is
223:43 - positive if the derivative is positive
223:45 - which is the rate of change is positive
223:46 - then increasing the value of coefficient
223:49 - will decrease the error okay first
223:52 - please note that whatever I'm telling if
223:55 - the derivative is positive with respect
223:57 - to this so it tells that if we increase
224:00 - the value of beta if we increase the
224:03 - value of beta if beta is increased then
224:06 - the error will decrease so if it is
224:09 - positive then we increase the value of
224:11 - beta please note that this is the most
224:12 - famous question when when we talk about
224:14 - the interpretation because this this
224:17 - derivative also gives the direction to
224:18 - movie whether to increase or decrease uh
224:20 - degree of theta in order to minimize the
224:22 - error right so if your derivative is
224:24 - positive then if you increase the beta
224:26 - your error will decrease with means if
224:28 - the if it is positive if the derivative
224:30 - positives it increases your beta okay
224:33 - and if your derivative is negative then
224:36 - increasing the values so for example
224:39 - um yeah so just just just for a note uh
224:42 - over here I think guys I I had a very
224:45 - long interpretation I'm going to retake
224:47 - it I'm going to retake it okay so now
224:50 - the derivative of MSE with respect to a
224:54 - particular coefficient provides the rate
224:56 - of change how much your cost function
224:58 - will change when you change the
224:59 - coefficient so if your derivative is
225:02 - positive if your derivative is positive
225:04 - so whatever the derivative which are
225:05 - taken whatever the derivative derivative
225:07 - which you which which I have taken if
225:09 - this is positive which means that if
225:12 - increase the value of beta so basically
225:14 - derivative what is it gives it gives the
225:16 - direction to move to increase or
225:18 - decrease the data that's that's what it
225:20 - gets right so if you increase if if the
225:22 - derivative is positive the derivative is
225:24 - positive so if we that that means that
225:28 - if we increase the value if we increase
225:29 - the value of beta our error will also
225:32 - increase so we'll have if the so so in
225:36 - this test if increase the value of beta
225:37 - a revelation 3 so so in so over here we
225:40 - decrease it
225:41 - okay but if your derivative is negative
225:43 - if your derivative is a negative which
225:46 - means that if you increase the value of
225:48 - a coefficient if you increase beta by
225:50 - some Factor by 3 is beta and you can
225:52 - increase beta from the previous one it
225:54 - will decrease the error it will decrease
225:57 - the error right so over here we just go
226:00 - with this change okay so basically this
226:02 - derivative gives us the direction to
226:04 - move your beta in if if the derivative
226:06 - is positive that means if we increase
226:08 - the beta our error will also increase
226:09 - but if the derivative is negative it
226:11 - means to increase the beta then the
226:13 - derivative will decrease okay
226:16 - I hope so if the derivative is positive
226:18 - then we decrease the beta so that our
226:21 - error also decreases
226:23 - okay I hope that that this gives you a
226:26 - good sense so by Computing the
226:27 - derivative of these uh of this MSC with
226:30 - respect to every individual parameter we
226:33 - can determine the direction in which
226:35 - each coefficient should be changed like
226:38 - to increase the degrees in order to
226:40 - minimize our error okay I hope so that
226:42 - you got it and now our gradient descent
226:45 - uses this derivative advertis uses this
226:47 - derivative of the of the mean squared
226:50 - error to update the parameters in the
226:53 - direction in the direction to update the
226:55 - parameters in the direction it minimizes
226:56 - with this we will use the graduation
226:58 - tool in the direction that it minimizes
227:00 - the particular error so at every
227:03 - iteration to at every iteration the the
227:06 - gradient of a m the the gradient of mean
227:08 - square error is computed and the
227:11 - coefficients are updated in the opposite
227:14 - direction of the gradient as we have
227:17 - already seen how exactly gradient
227:19 - descent works and and and it keeps on it
227:22 - keeps on you know changing the direction
227:24 - increasing or decreasing keeps happening
227:26 - a positive of the gradient opposite of
227:29 - the gradient and keeps on the come come
227:31 - coming up until the unless we converge
227:33 - to the global minimum so basically the
227:36 - whole idea behind first it is the rate
227:38 - of change derivative is the rate of
227:39 - change second it tells you the direction
227:41 - and second like how big the step is in
227:45 - this case if you see that it is starting
227:46 - in the big big step but eventually it is
227:48 - started small right so if your slope is
227:50 - large if your slope is large we want to
227:53 - take a large step because we are far
227:55 - from the minimum we are farm so if this
227:57 - over here slope is large right so over
227:58 - here it is far from the minimum so so we
228:01 - take the big step but as we go towards
228:03 - the minimum our slope becomes small and
228:05 - that's why we have to take a little
228:07 - little step so the your derivative tells
228:10 - that how big this step we should take
228:13 - I hope that this gives you a very clear
228:15 - sense about what exactly our derivative
228:18 - means and I hope that this gives a good
228:21 - sense about why do we use study where if
228:23 - for the cost function
228:24 - cool
228:26 - now we'll talk about a couple of types
228:29 - of regression problems in great detail I
228:32 - think this is important to talk and then
228:33 - we'll end the end about this section so
228:36 - there are two types of so now in the
228:38 - next in in the next video we'll talk
228:40 - about two types of regression which is
228:42 - univarial submultivated multiple there
228:44 - are a lot of regression we'll talk about
228:46 - that it's one and then you put the uh in
228:48 - you you make a you you make a vector X
228:51 - you make a vector X which is which
228:53 - contains the information vector and then
228:56 - and then and and then you have a X1
228:59 - X1 x0 X1 and X2 and then you make a and
229:03 - and then you make a parameter vector and
229:06 - then you make a parameter vector and
229:08 - then you make a parameter Vector beta0
229:09 - beta 1 and beta 2 and then you take the
229:12 - dot product between both of them
229:14 - dot product between both of them dot
229:17 - product between both of them so when you
229:19 - take the dot product between both of
229:21 - them what does it does X zero beta 0
229:23 - plus x 1 beta 1 x 2 Beta beta2 so we
229:26 - started the linear algebra this heavily
229:30 - reduces the computational time because
229:32 - it follows something when it's
229:33 - broadcasting where it's simply multi
229:35 - multiplies and uh adds at the same time
229:38 - and the calculation the dot product
229:40 - which will perform the way of taking the
229:42 - vector x x 0 x 1 and X2 and then beta 0
229:44 - beta 1 beta 2 they take the dot product
229:46 - it will be super duper efficient so we
229:49 - have several Frameworks to do this which
229:51 - is numpy framework you know tensorflow
229:53 - which which provides the optimized
229:55 - versions for the vectorized option this
229:57 - is called the vectorized operations
229:58 - that's why linear algebra comes into
230:00 - place okay
230:01 - now how can we write the dot product so
230:04 - if you have the beta if you have the
230:06 - beta which is the parameter Vector which
230:08 - contains the beta0 beta1 all the way
230:09 - around to the beta n number of a bit the
230:11 - number of beta is nothing because the
230:12 - number of features which you have and x
230:14 - x my X Vector contains your X1 out of
230:18 - the uh x 0 all done or the X N then you
230:20 - take the dot product by Beta transpose X
230:24 - this is this is how you can write or you
230:25 - can say beta dot X okay you can write in
230:29 - this way so you basically take the dot
230:30 - product but what will come to dot
230:31 - product we'll come to dot product later
230:33 - on when we solve a real world example
230:35 - but as of now this is how it works now
230:37 - let's get started with more detailing
230:41 - thing okay first is let's prepare our
230:44 - design Matrix let's prepare our design
230:46 - Matrix so this is left repair design
230:48 - design Matrix is nothing but your data
230:51 - how can you convert your data into a
230:52 - matrix a CSV data into Matrix so in this
230:55 - case in this case you have your design
230:57 - Matrix X okay X contains your data okay
231:01 - so assume that you have X1 X2 X3 all the
231:05 - number of features so extra size of the
231:06 - house price the number of bedrooms above
231:09 - the house the number of Earth so it it
231:11 - can have any number of p number of
231:12 - information so here we have written p in
231:15 - the K instead of hence we have p number
231:16 - of information okay and the for and the
231:19 - first data point can be represented as
231:21 - if you have seen The Matrix lecture you
231:23 - could have understood this very easily
231:24 - so the first data point in your the
231:27 - first data point in in your data is X11
231:30 - which represents that it is the first
231:33 - row and First Column which means is it's
231:36 - the first it is the first it is the uh
231:38 - for it it is available in the first row
231:41 - which of of the first information for
231:43 - example if there's the size of the house
231:45 - this is the first okay now this is the
231:47 - second row first information this is the
231:49 - second data point of the first
231:50 - information which is assume that this is
231:52 - the size of the house the first
231:54 - information of the size of the the house
231:55 - second information of the size of the
231:57 - house so you have same information but
231:59 - different different samples then you
232:01 - have price then you have number of
232:03 - bedrooms so first information of the
232:04 - second second feature which is the
232:06 - number of bedrooms second information
232:08 - about the bedroom so there's several
232:10 - samples for the same right so you have
232:12 - so on over here you have different
232:14 - different features or as a number of
232:16 - columns and then on the number of rows
232:18 - has an N number of a number of rows okay
232:21 - first data point First Column first row
232:23 - First Column second first row second
232:25 - column so First Row Third column all you
232:27 - know the first row P column okay
232:30 - and then at last you have an nth row
232:33 - First Column and a throw second column
232:34 - and a throw Peak column okay so it says
232:38 - that it can have p number of columns or
232:39 - n number for an N number of rows okay so
232:42 - this is no nothing known as design
232:44 - Matrix this is nothing known as
232:46 - design Matrix okay and then you have
232:49 - this Y and Y then the the why why n will
232:55 - be equals to X N okay so the number of
232:58 - rows then the number of elements which
233:00 - you have in your parameter y Vector is
233:02 - equals to the number of rows which you
233:03 - have in a design major because it is a
233:05 - super supervised learning problem and
233:06 - every individual row has its y1 Y2 all
233:11 - the way around to the Y N
233:13 - every has its own label so it it it it
233:16 - may be it not be smaller than it x n is
233:19 - equals to Y and the number of elements
233:21 - in y n should be equals to the number of
233:22 - their data points in x n okay
233:26 - cool now what we can actually take out
233:29 - we can actually take out our y hat i y
233:34 - hat I which is the predict prediction so
233:36 - now you might be thinking that okay
233:39 - every individual parameter has beta0
233:41 - beta1 all there are the beta P okay p
233:44 - number four now should we are we going
233:46 - to multiply beta 1 with all the
233:48 - variables over here beta beta beta is
233:51 - the two with all the variables and how
233:52 - exactly the hypothesis function how
233:54 - exactly is will this work
233:58 - a very very interesting question so your
234:01 - prediction your prediction why had I why
234:03 - had I was a prediction first of all you
234:06 - have beta0 beta0 by default comes in
234:08 - beta0 times x zero so basically
234:11 - sometimes you know what we do we
234:13 - actually have a n times P plus 1 10 and
234:17 - N times P plus one in this case we add
234:20 - another column of 1 1 1 1 1 1. we'll
234:23 - talk about that why do we add one okay
234:25 - so your you get a predict you you
234:28 - actually have this prediction y hat I
234:29 - which is beta0 plus beta1 x i One beta 2
234:33 - x i 2 plus beta all that are beta p x i
234:36 - p what does this mean this will you
234:38 - you'll only have to understand if you
234:40 - understand this thing okay so you have
234:42 - your Y and Y how you can get your
234:45 - prediction you can get your prediction
234:46 - which is y hat I which is why hat I you
234:49 - can get your prediction you can get your
234:50 - prediction you you can get your
234:51 - prediction from your model by for
234:54 - example eventually you have to make you
234:55 - you have to make a prediction for
234:56 - calculating how well your equation is
234:59 - right so you actually have a parameter
235:01 - Vector which is beta0 beta one all done
235:03 - or beta P which is the which is of size
235:04 - of p plus one p plus 1 because there's a
235:07 - p number of features plus one which is
235:09 - beta0 okay
235:11 - so you multiply it so basically in this
235:13 - case we also add there of one over here
235:15 - so I'm just going to add to make sure
235:17 - one one one all the way around to be one
235:20 - so only one so this this indicates X
235:22 - zero okay so that beta0 multiple gets
235:25 - multiplied with this we'll talk we'll
235:26 - talk about that uh in a bit so we have
235:28 - this particular and now when you
235:30 - multiply this Matrix with a vector it
235:33 - should lead you to a vector of your
235:36 - predictions so every individual sample
235:38 - have will have it because uh because a
235:42 - row is a data point second row is
235:44 - another data point so basically it's a
235:46 - column columnar difference you know
235:50 - what I'm speaking about is assume that
235:53 - assume that you know make you you have a
235:54 - data like this you have a data like this
235:56 - you have data like this okay data like
235:59 - this and data is mostly you know like
236:02 - this so you have uh so this particular
236:04 - Point has this label Y and now you have
236:06 - to make the prediction for this y hat I
236:08 - so that you can compare the both and
236:10 - then the take out the differences so
236:12 - take out the Y hat for this particular
236:14 - data point and then for this particular
236:15 - data points so for this part because
236:17 - every row is a data point every row is a
236:20 - data point okay what is it that that
236:22 - will eventually help us in taking our
236:24 - this or learning from our data okay hope
236:27 - so that it makes clear
236:30 - we'll talk about this uh this in a bit
236:32 - but before that whatever exactly want to
236:35 - convey to you all is you can actually
236:37 - convert this you can take this X1 you
236:40 - can take this X1 you can take this X1
236:42 - which is all the uh
236:46 - this this column into into a a row
236:51 - Vector you can actually convert this so
236:52 - X1 transpose which means eventually x x
236:56 - one or S like like click this we can
236:58 - convert like this and then you have X2
237:00 - transpose X and transpose you take this
237:02 - and convert this into a row Vector take
237:04 - this converts into a row Vector this
237:06 - this this has its own benefits we'll
237:08 - talk about that and your cost must
237:10 - become like this we'll talk about this
237:11 - all of this in retail just will will
237:13 - come come to this please ignore as of
237:15 - now
237:16 - cool so you have X1 x2ox X3 all the way
237:20 - it's it it has some feature we have the
237:22 - x0 which is beta0 and then you get a
237:23 - prediction y using nothing okay
237:25 - categorize as I've already talked about
237:27 - this your p number of samples are
237:29 - generalized to P samples and the
237:31 - parameter Vector p and we have y Vector
237:33 - Y which is the ground truth okay you
237:36 - have a matrix distribution which is
237:37 - nothing but which is also the data
237:39 - Matrix and the design Matrix now based
237:41 - on this now based on the every
237:42 - individual data point will have its own
237:44 - prediction uh will have its own
237:46 - prediction and then we compare with the
237:47 - ground through the actual prediction and
237:49 - then take out the error and then take
237:50 - out the error okay so every data date
237:52 - data point has its own prediction and
237:54 - then and every column has its own B2
237:57 - which is the beta values
238:00 - okay
238:01 - understood in this case you are seeing
238:03 - that exorcism being ultimately
238:04 - aggregating of the information now let's
238:07 - get started let's get started in how can
238:10 - we actually how can we actually take out
238:13 - the Y hat one y hat two y hat three y
238:16 - hat I all the way around the number of
238:18 - features which we have let's exactly
238:20 - work on that so y hat one which is
238:23 - equals to beta0 which is equals to beta0
238:26 - which is uh equals to beta0 so where I
238:29 - am going to write it up I don't know
238:31 - yeah in this case see this example uh
238:34 - now over here I think I read I written
238:36 - sorry now you want to make now you know
238:38 - now you want to get a uh predicted
238:40 - prediction you know a prediction vector
238:42 - and then you have your y uh and and then
238:45 - you have a ground truth over here put
238:46 - your prediction vector and first of all
238:48 - you have this one you you made a a
238:50 - column where you have one one filter
238:52 - okay why you'll notice and then a
238:55 - parameter Vector B okay when you
238:57 - multiply this Matrix this is this should
238:59 - be equals to this should be equals to so
239:01 - when you multiply this Matrix with a
239:03 - multiply with this Matrix of the product
239:05 - The Matrix is n times P plus 1 y p
239:08 - number of columns plus one column which
239:10 - we added right now times you have P plus
239:13 - 1 times 1 is B number to columns which
239:15 - is one and will there is only one column
239:17 - so when you when you actually we under
239:19 - take out the take out the multiplication
239:22 - between both of them the dot product
239:24 - between both of them multiplication with
239:25 - the product okay
239:27 - in this case what we'll do you have
239:29 - beta0 as I said that every individual
239:31 - Point has its own prediction okay it
239:33 - should not be like this it should be
239:35 - like this every data point is is is a
239:38 - row okay so beta0 times 1 beta 1 times X
239:42 - One One beta 2 times X12 all the way out
239:46 - of the beta P times x 1 P so basically
239:48 - you take the first row you take the
239:52 - first row of the vector multi take the
239:55 - take the dot product between the first
239:58 - row of the design Matrix or data Matrix
240:01 - with the first element of the vector
240:07 - and multiply with every element of the
240:10 - first uh row of that every element sorry
240:14 - sorry I I think I just mess it up I
240:16 - think I just made up you take the first
240:18 - row of the I just I'm so sorry let's you
240:23 - take the first row of the design Matrix
240:25 - and then you take the vector so
240:27 - basically you have your you you have
240:30 - your row Vector you multiply with the
240:33 - you you you multiply with the column
240:36 - Vector so you take the dot product
240:37 - between the row vector and the column
240:39 - Vector row vector and the column Vector
240:41 - which is nothing you'll come up with One
240:44 - X One One X one two all that are the uh
240:48 - this X
240:49 - 1p so X11 x 1 2 all X1 P all another X
240:54 - and there is one so take this and then
240:56 - you have a column Vector which is beta0
240:57 - beta 1 beta take the dot product between
241:00 - both of them so dot product will yield
241:02 - to something like this so beta0 This
241:04 - beta0 multiplied with this one beta 1
241:06 - multiplies this then plus of course beta
241:09 - 3 beta 3 multiplied with the third
241:10 - information beta 4 multiply the fourth
241:12 - information fourth fourth information
241:14 - the I did uh fourth information a
241:16 - particular data point in that fourth
241:18 - information okay I enter I in ith index
241:21 - and then you have got your prediction
241:22 - you just take out this and then go to
241:24 - your prediction now you go to Y2 which
241:25 - is the which is the prediction for the
241:28 - second row okay for this second row so
241:30 - basically you do the set you take the
241:32 - second row you take the second row and
241:34 - then you take the uh and then you take
241:36 - this Vector do the dot product with both
241:38 - of them it is beta 0 times 1 beta 1
241:40 - times this is the same information this
241:42 - is the same information it is the x21 is
241:45 - it is saying that is the second data
241:47 - point of the first ever of the same this
241:48 - if the size of the how does the second
241:50 - data point okay second data point of the
241:51 - first information so beta1 times x21
241:53 - beta 2 times X2 beta 3 times 6 2 x 2 3
241:56 - so you have this b y two all different
241:58 - out of the y n all that are the y n you
242:00 - you do for all the round of y n which is
242:01 - a y uh n number of uh predictions which
242:04 - is beta0 times beta 1 x and one which is
242:06 - the nth data point for the first and for
242:09 - any data point for the first information
242:10 - the beta 2 times x 2 x and 2 which is
242:13 - nth information for the second sorry NF
242:16 - data point for the second information
242:17 - any data point for the third information
242:19 - and nth and for n and a data point for
242:23 - the piece information okay and between
242:25 - you have y i which says that you have
242:27 - beta0 times x times of course one times
242:29 - of course one because everything is one
242:31 - in the end and that first case beta 1
242:33 - times x i one I three data point for the
242:36 - first information either a point for the
242:38 - second information either a point for
242:39 - the third information and either point
242:41 - for the beat information right and this
242:43 - is nothing but your prediction function
242:46 - for nothing but your simply
242:50 - uh multiple linear regression hypothesis
242:52 - function okay you can write this in a
242:54 - coefficient in a vectorized format which
242:56 - is y hat is equals to X B and this will
242:58 - yield to n times 1 vector
243:02 - I hope that this makes sense okay
243:06 - now over here now once once we have this
243:09 - now you can I actually uh now once we
243:11 - have the multiple you have the
243:12 - hypothesis function uh we'll do a verb
243:15 - example just like we did for linear
243:17 - regression simple line regression we do
243:18 - a simple work example for you to be
243:19 - better comfortable in
243:21 - um okay
243:25 - cool
243:27 - yes so now you have this now you have
243:29 - this now you now what you have to do now
243:31 - you've got your buy now if you compare
243:32 - this y hat I with Y how well your how
243:35 - well your this particular uh equation is
243:38 - so this can be which you have you you
243:40 - have this J of PETA which is your cos
243:43 - function which is nothing but equals to
243:45 - this which is for defined for simple
243:46 - animation but eventually you will notice
243:48 - one thing you will notice one thing you
243:50 - have your vector in this case you have
243:53 - your vector in this case you have Vector
243:54 - in this case you have Vector in this
243:56 - case right of course you can write your
243:58 - vector not a big build you can actually
243:59 - what you can do you can actually you
244:01 - know take out uh something you can take
244:03 - out something like this you can have the
244:05 - Y hat and Y and then subtract it and
244:07 - then you're getting another vector and
244:09 - then you simply add all the elements in
244:11 - that vector and divide by the number num
244:13 - number of terms you can actually do this
244:15 - but this is computationally
244:17 - computationally expensive and
244:19 - computational expensive we avoid in
244:21 - machine learning because it's already
244:22 - two not too much you know
244:24 - um what do you say but you know very
244:27 - every time taking so what we do we have
244:30 - a vectorized as we have the vectorized
244:34 - hypothesis function if someone is
244:35 - vectorized which is computationally good
244:37 - we have convex vectorized cost function
244:39 - as well which is defined by this which
244:43 - is Geo beta which is 1 by 2 em X beta
244:46 - which is your prediction function which
244:48 - is the prediction which is why you had
244:49 - eventually y hat minus y transpose X
244:52 - beta minus y hat well sorry y not this
244:55 - is your prediction and this is a ground
244:56 - truth prediction ground through so you
244:58 - have this Vector this this also use the
245:00 - same Vector this this yields the vector
245:01 - this yields the vector this is the
245:03 - vector this is the vector okay so how do
245:06 - we how do we derived this cost function
245:09 - so let's let's talk about how do we
245:10 - derived that
245:12 - okay so let's let's break it down let's
245:15 - break let's let's not take this let's
245:17 - take a very simple example where you
245:18 - have very where you have a vector X
245:20 - where you have one one and then you have
245:22 - first first information first first data
245:24 - point of the first information so first
245:26 - second item one for the first and if you
245:28 - have only one predictor which is an
245:29 - independent variable and then you multi
245:31 - and as you have only one uh information
245:33 - you have beta 1 and beta0 as as as a
245:36 - simple integration but let's frame this
245:38 - on a multiple things okay so when you
245:40 - when you want to take the prediction
245:41 - it's nothing but X beta so you take the
245:43 - beta0 and beta 1 this this the first row
245:46 - multi take the dot product between these
245:48 - two the the vector which is the
245:49 - parameter vector and the first row uh
245:51 - and then when you take the dot product
245:53 - and then the and then you write the
245:55 - equation over there and then take this
245:56 - this particular vector and then multiply
245:58 - with this Vector sorry row Vector then
246:01 - you dot the dot print between both of
246:02 - them the dot rate between the row vector
246:04 - and this Vector so you'll get the three
246:06 - data points out here now what do you do
246:08 - now you have this now the what will so
246:10 - you have the prediction for you have the
246:12 - pre action from your model now what you
246:14 - have to do as I said that you want to
246:15 - you want to identify what is the
246:17 - differences between both of them right
246:19 - as please note that if you want to know
246:20 - the geometrical perspective of course
246:22 - Solution please see my previous lectures
246:23 - because I've already told this I'm not
246:25 - going to tell again and again so you
246:27 - have now you take out the errors the
246:28 - residuals how how apart from they are so
246:31 - you simply have the prediction function
246:32 - the printed the prediction which which
246:33 - you got X like XB which is equals to
246:36 - nothing but the back Vector minus y hat
246:38 - which which is nothing but what which is
246:40 - nothing but a ground truth okay
246:43 - so when you see this when you see this
246:45 - you will simply have this and now we
246:47 - have to as as as to as to remove the you
246:51 - know the squared sign here sorry the
246:54 - negative sign we Square we Square so
246:56 - this is going to be square so if this is
246:58 - going to be square so every individual
247:00 - parameter is going to be squared so when
247:02 - every individual parameter is going to
247:03 - be scaled now you can see that this is
247:05 - going to stay so you can actually you
247:06 - have X beta minus y beta sorry X beta
247:10 - which is the which is nothing but which
247:12 - will yield the prediction function which
247:14 - Vector minus the ground root transfer so
247:17 - basically let's remove transfers as of
247:19 - now so you have a couple of times x beta
247:21 - minus y so you say simply take the
247:23 - transpose for better multiplication of
247:26 - both of them and eventually you will be
247:28 - getting a very nice so you have noticed
247:29 - a pattern you know using a vectorization
247:31 - it will eventually helping you it is
247:33 - helping you out
247:35 - it is helping you out very very easily
247:38 - it is helping you out so basically you
247:40 - you you're simply multiplying You're
247:42 - simply multiplying this two times the
247:43 - this square of this so basically you
247:45 - have this so you have this transpose for
247:47 - better computation you can see for that
247:49 - if you a field we'll do one sample
247:50 - example just to see how does it work uh
247:53 - so now rather than doing this extensive
247:55 - computation you can simply you know
247:56 - multiply the subtract the vector and
247:58 - then take take out the dot productivity
248:00 - in both of them to actually get the
248:02 - answer
248:03 - okay
248:05 - hope so it makes sense
248:07 - okay yeah the dot for a bit in both of
248:09 - them to actually take out the answer and
248:10 - then you divide by 2m or 2 over here is
248:12 - added because of the green decent
248:14 - convention but you can actually remove
248:15 - these two as well to take out the what
248:17 - average
248:19 - I hope that this gives a good sense
248:20 - about how to how do we came with this uh
248:22 - cross function uh efficient cost
248:24 - function
248:25 - um apart from that I think we covered
248:27 - pretty much very very well about this
248:28 - multiple line regression uh and then now
248:31 - once we have this cost function now
248:32 - you're able to evaluate it now what
248:34 - we'll do now we want to minimize
248:35 - minimize the cost function you'll see
248:37 - that how I do that how I do it using
248:39 - again gradient descent I'll choose I'll
248:41 - showcase you the difference between the
248:43 - partial data as we are using the partial
248:45 - derivative why we are using a partial
248:46 - derivative a very interesting question
248:48 - but I'm just I just wanted to be with
248:50 - the convention I just don't want to use
248:52 - that d d is used for scalar programs but
248:55 - this partial that depth derivative is
248:57 - used for what
248:59 - is used for
249:02 - um
249:02 - Vector formula but but basically uh I
249:05 - think uh I'll just note it down for you
249:08 - for for you all that okay here we are
249:11 - using uh in terms of future notation the
249:14 - the reason why I use that uh partial
249:16 - derivative in that uh when actually
249:18 - we're solving green descent is for so
249:21 - that it so so that it works perfectly
249:23 - for a fusion notation which we'll study
249:25 - right now so it makes you be a bit handy
249:27 - otherwise I could have introduced video
249:29 - so you might be confused so that's why I
249:31 - didn't introduced over there
249:33 - okay so cool I think it's now in the
249:37 - next video we'll talk about how can we
249:38 - optimize our cost function and apart
249:40 - from that I'll show you an analytical
249:42 - analytical solution how can we without
249:44 - any procedure we can just take out our
249:46 - beta's value in matter of no second in
249:48 - in a matter of no time with Pro with
249:50 - calculating and then we'll talk about
249:51 - how can we evaluate uh and then we'll
249:54 - talk about uh some of the hypothesis
249:55 - testing uh which is t-test f-test and a
249:59 - lot of things going to come up and you
250:00 - will be eventually enjoying all of this
250:02 - thing and after that what we'll do we'll
250:04 - talk about something known as uh
250:07 - assumptions of linear regression some of
250:09 - the assumptions and then we'll do
250:10 - another project as well uh you'll be
250:13 - super duper enjoying that as well I if
250:15 - I'm enjoying to teach you all of this so
250:17 - let's catch up in the next lecture
250:18 - London bye-bye hey everyone welcome back
250:20 - to this lecture uh so basically uh we
250:22 - are at linear regression where we'll be
250:25 - talking about multiple linear regression
250:27 - I already have the notes for you but
250:29 - that's bit less so I just want to
250:30 - elaborate with you uh to you with some
250:33 - exam which may help you to understand
250:35 - much more better so we'll do a we will
250:37 - have a set of worked set of examples for
250:40 - you to better understand all of these
250:41 - things okay so let's get started now so
250:44 - basically uh let's have a let's have an
250:46 - example so that I can show you the
250:48 - vectorized operation and everything so
250:49 - assume that example over here is X1
250:52 - uh X2 X in X1 you have one two three and
250:58 - four and X2 you have two three one and
251:04 - three and then you have the you have a
251:06 - two independent variables and then you
251:07 - have one dependent variables which is
251:10 - three the four point five four and six
251:13 - so you have the data available you have
251:15 - a data available like this so let me
251:17 - just draw a straight line you have a
251:19 - date you have a data available where
251:21 - these two uh these two variables which
251:23 - is out here these two variables are
251:25 - independent
251:27 - Independence this is no dependent output
251:29 - variable and this is nothing but your
251:31 - white or your dependent variable or your
251:34 - dependent variable okay so this is the
251:37 - data now so now so now you have the so
251:40 - now you can make a design Matrix so you
251:41 - know so now you can make a design Matrix
251:43 - first of all uh first of all we'll we
251:45 - will calculate the we'll have the
251:47 - hypothesis function out there okay so we
251:49 - are now we're gonna train our linear
251:51 - regression model on top of it so how
251:52 - will train first we will uh put put that
251:55 - in form of a matrix which is nothing but
251:57 - um
251:58 - one two two three three one and four
252:04 - eight and then we have one one one I'll
252:07 - show you what is the use case of this
252:08 - one just in some just in some you know
252:11 - uh moments which you will see that why
252:13 - we are adding one more over here so let
252:15 - me just have this in a nice way if x
252:18 - equals to one one one
252:22 - so with the we have the design Matrix
252:24 - we'll talk about this later on we'll
252:26 - talk about this later on and then you
252:28 - have beta and then you have beta so
252:30 - basically every individual feature has
252:32 - its own beta right so beta1 and beta2
252:36 - beta 1 and beta2 right beta 1 but but we
252:40 - also have something else beta0 right so
252:43 - how beta0 basically when we do the
252:45 - multiplication so basically we take the
252:47 - first row multiply with the uh take the
252:50 - dot predicate in the first row and and
252:53 - the vector B so basically the here is
252:55 - here we have only two for example assume
252:57 - that we haven't taken this particular
252:59 - one one one then we would have only one
253:01 - two and this this would have been left
253:03 - so we have to multiply beta 0 to 1 as
253:05 - well right because beta 0 times x 0 x 0
253:09 - is equals to one that that's why so you
253:12 - assume that you that you have
253:13 - initialized your beta
253:15 - with uh maybe a zero so but before that
253:20 - you have the beta over there and then
253:21 - you have your ground through ground
253:23 - throat Vector which is three four point
253:26 - five four and six three four point five
253:29 - four and six okay so you so you have
253:31 - this uh ground Truth uh over here now
253:35 - what you do now let's now with the
253:38 - available settings now let's let's do
253:39 - one thing let's form a hypothesis
253:41 - function so you can get your prediction
253:43 - vector by multiplying Your Design Matrix
253:45 - with the with the parameter vector and
253:47 - this will yield to to
253:50 - um this will lead to a vector this will
253:52 - yield to a vector which is equals what
253:54 - what was our design Matrix design Matrix
253:56 - was one one two one two three one three
253:59 - one one four eight and then let's
254:02 - initialize our beta value beta0 beta 1
254:05 - beta two all the all the way to zero
254:07 - because we have to you have to find the
254:09 - best setup so in the first iteration
254:11 - what you do you just initialize
254:13 - okay I hope that this really makes sense
254:16 - uh to you
254:18 - now once we have that now once we have
254:20 - that what we will do after initializing
254:23 - our beta values after initializing our
254:25 - beta values to zero what we will do
254:27 - we'll simply have this zero zero zero
254:29 - and now we'll take the field we will
254:32 - take the dot review the first row and
254:34 - the column of that the first row The
254:36 - Dropper when both of them will yield
254:37 - nothing but Zero The Dropper between the
254:40 - second row in this Vector will lead to
254:41 - zero this and this is zero and zero so
254:45 - predictions your predictions are zero
254:47 - which is a y hat so once you have the
254:49 - predictions from your model when your
254:51 - betas are all set to zero you'll be
254:53 - having that okay
254:55 - and then and then now we have to compute
254:58 - the cost once you have the model
254:59 - prediction I have to estimate how well
255:01 - how well so when you when you have the H
255:04 - of X in this case is nothing but beta0
255:06 - plus beta 1 x 1 plus beta 2 x 2. so with
255:11 - B resor is nothing but zero times X1
255:13 - plus 0 times X2 so how well this this
255:15 - equation or best fit line is of course
255:17 - this is super bad because we have
255:19 - initialized as this is the first
255:20 - iteration so let's compute the cost so
255:23 - when we compute the cost when we compute
255:25 - the cost of it
255:28 - cost
255:30 - function so cos function is computed by
255:34 - this so we have already had a talk on
255:36 - the derivation of our cost function
255:38 - which is nothing but 1 by 2 m
255:41 - X beta
255:43 - minus y Vector transpose X beta minus y
255:49 - okay so this this was our equation so
255:52 - let's let's first of all compute let's
255:54 - first of all compute X beta minus y okay
255:58 - so when you how can a cow can you come
256:01 - come compute this we have model
256:03 - prediction minus the ground truth moral
256:05 - position was 0 minus 3 0 minus 4.5 0
256:09 - minus 6 and 0 minus uh sorry 4 and 6.
256:13 - okay this was 4 and 6 so you have your
256:15 - model friction minus the ground truth
256:17 - which will yield to a vector which is
256:20 - minus 3 minus 4.5 minus 4 and minus 6
256:26 - this will be your uh this this this will
256:29 - be your when you calculate this now what
256:31 - we will do we'll take this we have
256:33 - calculated now we'll take the transpose
256:35 - of this so when you take the transpose
256:37 - of
256:39 - the transpose of this which is nothing
256:41 - with equals to minus 3 minus 4.5 minus 4
256:46 - and minus 6. we have this this
256:48 - particular transpose Vector now we have
256:51 - the transpose Vector now what you have
256:52 - to transverse Vector multiplied that
256:54 - multiplied with this particular Vector
256:57 - then you will have Simply Now
257:00 - you have uh one by e 2 m so here you
257:04 - have here you have four examples m is
257:06 - just one two three four four date four
257:09 - data points one two two by four is
257:11 - equals to eight one by eight times one
257:14 - by eight times minus 3 minus 4.5 minus 4
257:18 - minus 6 and then you have the non
257:20 - transverse Vector which is minus 3 minus
257:23 - 4.5 minus 4 and minus 6 when you
257:26 - actually calculate this up and you can
257:27 - actually calculate this up which is
257:29 - nothing but which is sorry
257:32 - so this is nothing but
257:34 - um nine plus uh 25 so 9 plus 20.5 plus
257:41 - 16 plus 36 as you multiply the element
257:44 - wise operation
257:45 - and then what do you do and then you
257:47 - simply add it up and that will jio beta
257:49 - will heal to
257:51 - 10.156 so you'll have this
257:54 - um
257:56 - okay so you have seen that how we have
257:59 - taken an account of this hypothesis
258:01 - function after that we have had this
258:03 - taken out the and and we have taken out
258:05 - the cost pension out there I hope that
258:07 - this gives you pretty much a good sense
258:08 - about how exactly we cover up things now
258:11 - this was our vectorized operation out
258:13 - there right so how this vectorization
258:16 - really helps so this is a very important
258:18 - topic to talk on maybe it's very useful
258:20 - as well to talk on like how
258:21 - vectorization question how vectorization
258:24 - how vectorization helps how
258:28 - vectorization helps so if you were to do
258:30 - this normally you know if you if you
258:32 - were to do this without any vectorized
258:34 - operations first of all let us assume
258:36 - you know you don't want to do it with
258:37 - any vectorized operations first of all
258:39 - even with calculation of hypothesis will
258:42 - take too much of time no nested Loops
258:44 - will come in will take too much of time
258:45 - so when you have this particularly your
258:47 - Zen general form which is the word h of
258:48 - a h of X is equals beta 0 plus beta 1 uh
258:52 - X1 plus beta 2 x 2 right so you put in
258:55 - the value of x you would have put in the
258:57 - value of x and then calculate you put in
258:59 - the value of X2 calculator okay
259:02 - and put it now that's for that's our
259:04 - first data point and then you do first
259:06 - second date I'm going to third port so
259:08 - you do for all the four data points okay
259:09 - now after this now after this what you
259:12 - what what you will do you'd have take
259:14 - out the diff that that you have to take
259:16 - out the errors for every for every
259:17 - individual right so that would take a
259:19 - ton of time right so that's why we we
259:22 - over here you have seen that how
259:24 - Simplicity by at the only one time it
259:26 - just calculates our error okay
259:29 - cool so um so what are the what are the
259:32 - operations involved what are the
259:33 - operations
259:36 - involved which you have studied right
259:38 - now so operations the first operation
259:40 - which is involved is compute the
259:43 - compute your hypothesis or compute XB
259:46 - which is nothing your X is nothing but M
259:49 - times n plus 1 Matrix where you have one
259:52 - included for 1 1 column and then you
259:54 - have a beta which is nothing but n plus
259:57 - 1 Matrix and and then you have nothing
259:59 - but n plus 1 Matrix okay and then you
260:03 - have a beta which is nothing but n plus
260:04 - 1 Matrix uh sorry uh n plus 1 times 1
260:08 - Matrix or the vector where you have
260:10 - betas are also included that's why we we
260:12 - have this plus one okay so compute this
260:14 - so computation takes the the complexity
260:18 - will take uh some order so that's the
260:20 - first and operation is involved second
260:22 - operation which is involved which is the
260:24 - which is taking out the error like this
260:26 - and then what you do and then we take
260:28 - the transpose of this and then we take
260:30 - the transpose of this and then we
260:32 - multiply it okay so this is the second
260:34 - method information and after you
260:36 - multiply it you will be getting your
260:37 - answer so the total time takes it's very
260:39 - less as compared to the you can also
260:42 - take out the runtime complexity you want
260:44 - so now I hope that this gives a good
260:45 - sense about what exactly we trying we're
260:47 - trying to offer now what we'll do now
260:50 - we'll go forward and talk about uh how
260:52 - can we perform a gradient descent how
260:55 - can we perform gradient descent outer
260:57 - okay so that's the first question to
261:00 - tackle so let's go ahead
261:02 - and have this cutting template
261:05 - and let's get started how can we perform
261:07 - gradient descent okay so basically so
261:10 - basically
261:12 - um how can we perform limits right
261:15 - gradient
261:17 - descent
261:19 - amazing question right so how can we
261:22 - perform so you have J of beta your cost
261:25 - function is nothing but 1 by 2 m
261:28 - X beta minus y transpose X beta minus y
261:33 - so this was a cost function right so now
261:35 - this is a COS function as I've studied
261:37 - that you have to take the partial
261:39 - derivative of a COS with the gradient of
261:41 - a COS version so there's a difference
261:42 - between partial derivative and
261:44 - derivative so there's a difference
261:45 - between partial derivative of cos
261:47 - function with respect to a parameter and
261:50 - uh derivative of so previously so
261:54 - basically this we use if we have the
261:57 - scalar program if you have a scalar
261:59 - program just just like we saw in our uh
262:02 - previous simple simple a linear
262:05 - regression but in this case but in this
262:07 - case but in this case
262:09 - um this particular thing we have a
262:12 - vector program which we saw in a multi
262:14 - multiple linear regression but I just
262:15 - introduce you this into the first only
262:17 - so that you'll be comfortable with what
262:19 - exactly both mean the exact same thing
262:21 - but this is for Vector programs this is
262:23 - what Vector programs and this is for
262:25 - scalar programs so basically whenever
262:28 - you have whenever you have betas in
262:30 - Vector you actually use this and then
262:32 - when you have the same scalar you
262:33 - actually use this okay
262:35 - so what you do so what you do you take
262:38 - out the you you take the debt you take
262:40 - the dairy partial derivative of G of
262:42 - beta with respect to a particular beta
262:44 - why we take out and everything we have
262:46 - talked that it gives the rate of change
262:48 - it gives about everything the whole
262:49 - story of geometrical we have already
262:51 - talked about so you take the partial
262:52 - derivative of it so that you can
262:54 - identify the rate of change how much the
262:55 - cost which is changing with a beta is
262:57 - changing so that when you take out the
262:58 - derivatives which actually gives 1 by m
263:00 - x transpose X beta minus y so when you
263:04 - actually take out the derivative I'm not
263:06 - going to talk about the
263:07 - um how can we derive it it's not
263:09 - required maybe for you if you're not a
263:10 - calculator student but we of course have
263:12 - the reading materials for you to get
263:13 - started with it okay so so what is what
263:17 - is the Grid in descent algorithm so once
263:19 - we have the gradient once once we have
263:21 - the gradient calculated out here what
263:24 - you can do we have the algorithm
263:25 - following so go for I for I
263:31 - in for I in range
263:33 - number of iterations you want to perform
263:36 - you wanna perform you want to change the
263:37 - variable so that you so so so that you
263:40 - can converge you take out first of all
263:42 - the hypo if using hypothesis function
263:44 - you take the doctor between NP dot dot X
263:46 - and beta and then once you have the
263:48 - ground truth now what you do you take
263:50 - out the error now you take out the error
263:51 - which is why you had minus y this y hat
263:54 - is nothing but your modification and Y
263:56 - is right and then you have a gradient
263:59 - now you now you take out the derivative
264:00 - of the gradient at that particular point
264:02 - and over here it will sort of vector
264:04 - will do one solved examples to better
264:06 - for you so we take out this so we take
264:08 - out the derivative of the gradient and
264:10 - that particular point which tells us how
264:11 - much how much uh change we are getting
264:13 - in a cost function if we change the beta
264:15 - a little bit which which was nothing but
264:17 - which we have derived this is nothing
264:19 - but 1 by m
264:21 - times the doctor between the dot product
264:24 - between your
264:26 - your views so basically this is the
264:28 - docker between the error this is the
264:30 - error and it's the transpose let's take
264:32 - out the top between the transpose of
264:36 - your X which is this and the error which
264:39 - is this so just simply multiply okay at
264:42 - now once you have the editor once you
264:44 - have once once we have that once you
264:45 - have the gradient we can actually use
264:46 - the update rule so your beta new will
264:49 - contain beta old minus the learning rate
264:51 - Alpha which is nothing we're learning it
264:53 - Alpha multiplied by the gradient
264:55 - multiply with the gradient which is okay
264:57 - right this is the derivative which here
264:58 - another partial derivative constantial
265:01 - with respect to particular uh variable
265:03 - okay
265:04 - so I hope that this the this is the
265:06 - algorithm which is for multiple line
265:08 - regression let's try to do one thing
265:10 - let's try to solve solve a very simple
265:12 - example of using a grid in descent okay
265:14 - so assume that I'm going to take a very
265:17 - simple data set which is X1
265:20 - X2 and Y which is 1 2 2 3 3 and 4.5 So
265:26 - based on this on a predict the output
265:28 - variable y so your design Matrix will
265:30 - conclude f 1 1 okay because this is
265:33 - important x 0 and then one two and two
265:36 - and three this is a design Matrix this
265:38 - is a design Matrix
265:40 - okay and then you have a output the
265:42 - ground root Vector which is 3 and 4.5
265:45 - and then you have parameter Vector which
265:47 - is 0 and then you have bit 0 0 so beta0
265:52 - beta 1 and beta dou beta0 as a proper
265:54 - bias term beta1 is the parameter of X1
265:57 - and we advise the parameter of term of
265:58 - X2 okay assume that your learning rate
266:01 - Alpha is 0.01
266:03 - and then now what you do now you compute
266:07 - the gradient so now what to know because
266:10 - but before that what you do but before
266:12 - that what you do you compute the you you
266:15 - run this algorithm the first step in
266:17 - this uh when one running this algorithm
266:19 - you're going to calculate your y hat and
266:21 - the calculator y hat which is nothing
266:22 - but X beta which is nothing but 0 0 okay
266:26 - you calculated now you calculate the
266:28 - error which is nothing over there which
266:30 - is nothing but your 0 minus 3 and 0
266:32 - minus 4.5 which is nothing but Z minus
266:35 - Three N minus 4.5 which is nothing but
266:38 - this is nothing but the error the the
266:41 - difference now once you have the
266:42 - difference you calculate the gradient at
266:43 - that particular point where you
266:45 - calculate the gradient in that
266:46 - particular point which is nothing but
266:47 - one by two one by two which is over here
266:50 - m is equals to 2 that's why one by two
266:52 - multiplied by your your dot product
266:54 - between your X transpose the design
266:56 - Matrix and the error the doctor between
266:58 - the design Matrix which is the design
267:00 - Matrix over here is one one okay which
267:03 - is one once you attempts the transpose
267:05 - of it will be one one
267:07 - um
267:07 - one two sorry sorry I guess I did a bit
267:10 - wrong over here it should be one two
267:13 - okay so one two and two three okay so
267:16 - one one one two
267:18 - and two three so when you take out the
267:20 - transpose of this particular Matrix
267:21 - you'll get this multiplied by multiplied
267:24 - by oops
267:27 - and multiplied by your uh error error or
267:32 - difference Vector which is this okay so
267:35 - when you calculate the gradient you
267:36 - calculate the gradient you will end up
267:39 - being well you'll end up being with uh
267:42 - this particular variable which is
267:43 - nothing but
267:45 - it'll have c c let us assume that you
267:48 - have variant now what you do you have a
267:50 - gradient so when you actually work on it
267:52 - when you actually work on it so this in
267:53 - the new set of beta so I'm going to come
267:55 - to the gradient here this is a good this
267:56 - one even when you solve for it you'll
267:57 - you'll automatically get it okay you'll
268:01 - automatically get it just when you solve
268:03 - for it okay so you get all the values
268:05 - and you solve for it and then you have
268:06 - the new beta is equals to Old beta minus
268:09 - learnings and grad okay this this
268:11 - particular grad and this is also a
268:12 - vector okay so your new beta will be a
268:15 - Vic Vector which is a three elements
268:17 - which is the old beta is also three zero
268:19 - zero zero okay minus 0.01 multiplied by
268:23 - your vector multiplier by your vector
268:25 - which is the C1 which is C2 and C3 in
268:29 - this case if we exactly want to focus on
268:33 - exact numbers this will be nothing but
268:35 - minus 3.75 minus 6 and minus 9.
268:40 - okay
268:43 - so this is the basically uh the the the
268:46 - C will when calculate will come out
268:48 - which is nothing but your new set of
268:50 - betas new set of betas will be zero zero
268:54 - zero minus 0.01 times minus 3.75 minus
269:00 - 0.01 times minus 6 minus 0.01 multiplied
269:04 - by minus 9 so you have this one and then
269:07 - you actually calculate this 0.0375
269:10 - 0.06 and then 0.09 so this is your new
269:14 - set of vector after first iteration okay
269:16 - and this is the first iteration now now
269:18 - you now after calculating the new betas
269:21 - now betas are updated now you what you
269:23 - do you you you run this once again with
269:25 - the new B test calc with symmetrical
269:26 - gradient and then get the new beta and
269:28 - then do the same thing unless you the
269:30 - number of iterations reached I hope that
269:33 - this gives a good sense about how
269:34 - exactly we work through and
269:37 - um
269:38 - for geometrical perspective you have
269:39 - already talked a lot
269:41 - cool um so I'm just trying to make sure
269:43 - that you have every worked example into
269:45 - your hand so that it's much more better
269:47 - for you to understand it next what we
269:49 - have next we are done with mostly um
269:51 - this we'll talk about a bit about
269:52 - analytical solution of linear regression
269:54 - and then we'll talk about how can we
269:56 - evaluate our model uh specifically and
269:59 - then after that we have a lot of things
270:01 - layer so let me just go there
270:03 - so after the calculation we'll talk
270:05 - about hypothesis testing which is one of
270:07 - the most important thing over here which
270:09 - is the how can we test our model the
270:11 - significance and everything and then we
270:13 - go to the next step which is nothing but
270:16 - um actually talk about assumptions of
270:17 - the model and then we'll do one project
270:19 - and they'll wrap up this course in this
270:21 - video what I what exactly I'm going to
270:22 - do is talk about something else
270:24 - analytical solution of linear regression
270:27 - so basically what exactly we'll talk
270:29 - about the anal what exactly we will talk
270:31 - about in the analytical solution of
270:33 - linear regression is you have direct
270:35 - formula you have direct formula for
270:37 - taking out your beta values by putting
270:40 - your X and y's into it so how this is
270:42 - derived and everything I'll be linking a
270:45 - very nice resources in this particular
270:46 - reading section of this or you or you
270:48 - can see in the resources section for
270:49 - this particular lecture I'll I'll give
270:52 - you how do we derive this formula but
270:54 - it's pretty easy if you get to it you'll
270:55 - have the assignments as well for on it
270:57 - so basically we have the direct formula
271:00 - when we plug in the values of X and Y
271:01 - and then we get our output variable y so
271:03 - sorry uh betas right so we have an
271:05 - optimized beta which is converged
271:07 - because it's the best beta right so
271:08 - there's a direct formula which is which
271:10 - we call analytical solution that so
271:12 - basically
271:13 - uh so basically the formula is X
271:15 - transpose X then we take the inverse
271:16 - multiply with X transpose y so so let's
271:19 - talk about let's let's talk about
271:22 - um
271:22 - and let's try let's talk about by taking
271:25 - one particular example so assume that
271:27 - this is a particular example you have
271:28 - one one one one as X zero and then you
271:29 - have one independent feature and one x0
271:32 - which is nothing but for beta0 and then
271:34 - you have output and then you have Target
271:35 - variable uh which is Y which is the
271:37 - ground truth now we utilize both of them
271:40 - to actually take out the analytics to
271:42 - take out the beta's values right so
271:44 - basically so the first step is to
271:46 - calculate the X transpose x x transpose
271:48 - X so when you take out the sodium take
271:49 - out the transpose of this so This column
271:51 - this column becomes a row and row
271:53 - becomes the column so basically one one
271:54 - and five seven two zero and then you
271:56 - have the particular Matrix Matrix out
271:58 - here which is two uh two by five Matrix
272:00 - multiplied by five by two that will
272:02 - yield to two by two Matrix so when you
272:03 - multiply this up you will get a two by
272:05 - two Matrix which is nothing but this one
272:07 - okay now the next step what you do is
272:09 - take out the inverse of it so what is
272:12 - the inverse of it or inverse of any
272:13 - particular thing is one by this okay one
272:16 - with determinant of the inner value
272:17 - which is one by determinant and when you
272:19 - take out the determinant of this
272:20 - particular which is nothing but which is
272:22 - nothing but multiplied by this minus
272:24 - multiple minus 54 times 54. so this is
272:27 - determinant this is which is not not
272:29 - nothing but the area under a
272:31 - parallelogram we have already talked
272:32 - about the terminal everything in great
272:33 - detail please see the linear algebra
272:36 - lectures so when you take out a
272:37 - determinant you will have 674 and then
272:40 - what you do you take out the adju gate
272:41 - of that and then and then you take out
272:43 - the aggregate of that so educate of that
272:45 - would be nothing but
272:47 - um this one this particular Matrix and
272:49 - then you multiply with 1 by this with
272:52 - the the as you get Matrix where you will
272:53 - get the your inverse of a particular
272:55 - Matrix now once you have the inverse of
272:58 - a particular Matrix now what you do you
273:00 - compute you compute X transpose Y which
273:02 - is this one you now we compute X
273:04 - transpose I you'll get this particular
273:05 - thing and then finally multiply this
273:07 - particular Matrix this particular Matrix
273:09 - and this particular vector by far
273:10 - getting beta0 and beta 1 and that's the
273:12 - required beta0 and beta 1 which is your
273:15 - regression equation okay
273:18 - I hope that this makes much more sense
273:20 - uh now uh so uh as the as this was very
273:24 - simple example uh for a calculation of
273:26 - this but I actually want you to just
273:28 - don't uh remember how to how exactly
273:30 - those this is something which is uh
273:32 - softwares are made to do it for you so
273:34 - you don't need to worry about this what
273:35 - you need to worry about is uh uh is this
273:38 - particular thing like what exactly this
273:40 - trust tells is here we have the formula
273:42 - which directly gives our beta values
273:44 - okay uh if this if this is not if you if
273:49 - your this meter is not invertible
273:51 - invertible it cannot be inverted then
273:53 - you need to remove the extra features to
273:55 - ensure that your D is smaller than or
273:58 - equals to n so here the number of
273:59 - features is less okay so uh this is a
274:03 - very general General thing which you
274:05 - need to know but uh but let's talk about
274:07 - what is the difference between this this
274:08 - is important this is usually asked in
274:10 - interviews what is the difference
274:11 - between gradient descent and closed form
274:12 - Solutions gradient descent requires
274:14 - multiple iterations okay so you have one
274:17 - iteration second and it requires time
274:19 - and you need to choose the alpha value
274:20 - as well and it extremely works well when
274:23 - you have when you have a large number of
274:24 - features it is not it does not works
274:27 - well when you have a large number of
274:28 - features okay it supports the
274:30 - incremental learning it suppose the
274:32 - incremental learning okay it is a non we
274:36 - can close from which is non iterative
274:37 - it's just plug in the values and then we
274:39 - get up values and then it's no need for
274:41 - Alpha values we don't need that it is
274:43 - slow if n is large if a number of
274:45 - features large it is slow because it
274:47 - will compute all the operations which is
274:48 - linear algeb operation which is quite
274:49 - expensive and then you have this
274:52 - particular thing which is nothing but a
274:54 - you can you compute X transpose x uh
274:56 - which is roughly order of NQ which is
274:58 - extremely extremely time taking so if
275:01 - your if your data is small and is small
275:03 - then I suggest to go close form but if
275:05 - data is you know large then I support
275:07 - incremental learning which is great
275:09 - indecent okay
275:11 - I hope that this this gives a very great
275:14 - sense what exactly is an analytical
275:16 - solution means now what we do now what
275:18 - we'll do we'll talk about how can we
275:20 - evaluate our machine learning model by
275:23 - going on to the next slide so let me
275:24 - just make a template let me just make a
275:27 - template
275:28 - uh over here so toilet paper and over
275:32 - here what I'll do I'll talk about some
275:33 - of the methods how we can eventually uh
275:37 - talk about how can I evaluate our
275:39 - machine learning model in a right way
275:40 - okay so uh so the first uh the first
275:44 - measure which we have already have seen
275:46 - earlier is nothing but known as mean
275:49 - square error so the first measure for
275:51 - evaluating how well your model fits the
275:54 - data which is nothing but mean squared
275:57 - error so what exactly mean a square
275:59 - error is is the average of this great
276:02 - difference between the predicted value
276:03 - and the actual values and emphasizes the
276:07 - impact of larger errors on overall error
276:09 - so basically what exactly this test
276:11 - tells you have the MSC which is nothing
276:14 - but 1 by m some sometimes you write 2
276:16 - over here as well but let's ignore two
276:17 - ways of now so 1 by m multiplied by I
276:21 - equals to 1 all the way around to the
276:23 - end all day round to the end all the way
276:25 - around to the m you do you you you do
276:28 - for every example for every example for
276:30 - every examples you take out their
276:31 - prediction take out their prediction uh
276:34 - sorry you you you take out more
276:36 - prediction minus y i which is minus your
276:39 - ground truth and square to remove the
276:41 - negative to remove the negative sign
276:43 - once if so to remove the negative error
276:46 - if if there's any right so basically
276:48 - what does it tell what does it do it
276:50 - gives you it gives you
276:52 - it gives you the the the difference is
276:57 - the overall differences the overall
276:59 - difference is the overall
277:02 - the overall differences the overall
277:04 - differences
277:07 - in your model but that's a one note to
277:10 - be taken out here one note to be taken
277:12 - out here you see that we are taking mean
277:13 - you see that you are taking mean so mean
277:16 - is highly affected by outliers so assume
277:19 - that you have a 2 3
277:21 - 2.56 and then 99 you understand this
277:25 - this can easily affect your energy
277:28 - however models performing best value one
277:30 - example is a perform very worse right so
277:32 - over here your MSC will yield to very
277:35 - bad example to to a very misleading you
277:37 - know misleading uh error because it is
277:40 - it is waiting eventually taken a mean
277:42 - and this is something giving us a
277:43 - misleading outcome right this is
277:45 - something which is to give giving out
277:46 - the misleading outcome so basically we
277:49 - say that it emphasizes the impact of
277:52 - larger errors on the overall sense so if
277:55 - you've got the if you if you if if
277:57 - you're getting that a large MSC that
277:59 - means that it is not something like
278:01 - every error is performing it emphasizes
278:03 - it emphasizes what is emphasizes the
278:07 - impact of large errors the impact of
278:10 - large errors like this like this in
278:14 - overall sense in overall in overall
278:17 - sense but I just told about the MSC so
278:20 - that's one of them another one is
278:23 - another one is is about mean absolute
278:27 - error mean absolute error so it's the
278:29 - average of the absolute difference
278:31 - between the predicted and the actual
278:33 - values it measures the average magnitude
278:36 - of errors in a set of predictions
278:38 - without considering the direction so
278:40 - basically so basically what it does
278:43 - tells that you have mean absolute error
278:46 - and what does it tells that the first of
278:49 - all the formula for this is a nothing
278:50 - but 1 by m
278:53 - I is equals to 1 all the way down to the
278:55 - m and take an absolute value of y i
278:57 - minus y i okay and not squared over here
279:01 - please note that we have that you don't
279:02 - need to square so basically what what
279:04 - we're doing and rather than you doing
279:06 - Square to remove the negative sign
279:08 - you're taking the absolute value and so
279:09 - that we get rid of negative terms so
279:11 - basically that means that if we get rid
279:13 - of negative terms if we if we get rid of
279:16 - negative terms if you did get rid of
279:17 - negative terms what does it mean it
279:20 - means that if you don't have if you
279:22 - don't have any direction it it does not
279:24 - if while you take out this then it does
279:26 - not a negative a positive reduction it
279:27 - just says we don't have Direction so if
279:29 - we get the rate of negative Direction
279:30 - which also means that we get rid of the
279:32 - direction without the considering the
279:33 - factor of whether it is positive or
279:34 - negative and let's not focuses on that
279:36 - okay
279:37 - um so uh it does not consider detections
279:40 - but it takes the average of the apps
279:41 - when you take the average you'd be
279:42 - positive positive B positive C take the
279:45 - average okay it is not taking out the
279:47 - differences that consider that is going
279:48 - in other direction it all in the like
279:50 - consistent like it's it's like we're not
279:52 - considering Direct in this case so
279:54 - that's mean absolute error that's one of
279:56 - the way to think about it okay
279:59 - and that and another one which we have
280:01 - is nothing but called root mean squared
280:05 - error which is nothing but called root
280:07 - mean square error so let me let me just
280:09 - write root
280:11 - mean mean squared error or somewhere we
280:15 - call rmse so what does rmse means this
280:19 - is a very important uh thing to talk on
280:20 - rmsc is nothing but a square of your
280:23 - mean square error square root of your
280:25 - mean Square so when you take out the
280:26 - square root of Mac that will give you
280:28 - rmse which is root mean squarer okay
280:31 - that is the square root of that it has
280:33 - the same units as the dependent variable
280:36 - so why does we
280:38 - um
280:38 - why do we take out MSE because the units
280:41 - are same as the depend the target
280:43 - variable that's why it is easier to
280:45 - interpret you know some sometimes we
280:47 - take out the MSE because we want to come
280:51 - back at this format where it was okay
280:54 - you can understand in that way here we
280:57 - squared it no we're going to come back
280:58 - at where it was without considering the
281:00 - negative sign so here it tells you the
281:03 - it it converts back to the units which
281:04 - was previously in okay cool though so
281:07 - these are very small errors which I
281:09 - really want to talk on uh one of the
281:11 - important and most important is nothing
281:13 - but called are
281:16 - r squared error r squared error and in
281:21 - that we have SST which is uh which is
281:24 - total sum of squares and uh residual sum
281:26 - of squares which we'll talk about that
281:27 - today okay
281:28 - Okay cool so let's talk about these both
281:31 - in great details so that
281:33 - um you have a very nice understanding of
281:36 - what exactly you need to do okay cool so
281:39 - um let me just start off with one
281:41 - example so let's say let's consider a
281:44 - very simple example to illustrate the
281:46 - the the the the R square error so assume
281:49 - that you have a data set with one
281:50 - independent feature okay and one
281:53 - dependent feature okay so let's consider
281:55 - your the index number the observation
281:58 - number which is observation number is
281:59 - one so we have X which is the in index
282:02 - which is 1 and Y which is also which is
282:05 - the target which is three okay so two
282:08 - two 5 and then you have three three
282:12 - seven and four four nine so you have the
282:16 - index number the the OBS the observation
282:18 - number the observation number and then
282:21 - you have and then you have the
282:22 - independent feature and the dependent
282:23 - feature so basically on a map of
282:25 - function x to the output variable y okay
282:27 - so now so now you've built a linear
282:29 - regression model on top of it when you
282:31 - built a linear equation or model on top
282:33 - of it uh basically we need to do the
282:35 - hypothesis test we sorry we know we need
282:37 - to predict we need to predict the Y hat
282:39 - right so that we can calculate so that
282:41 - we can so that so let's take an example
282:43 - beta0 is something there 0.1 and beta 1
282:46 - is also 0.2 then you want to test it
282:48 - right so you have to take you have to
282:49 - say you have to perform the predictions
282:50 - on the X and then compare the prediction
282:52 - y hat with Y right so your y would be
282:55 - nothing but so your your actual y was 3
282:59 - 5 7 and 9 and your model predicted 3.5
283:04 - 5.5 7.5 and 9.5 which is the model which
283:09 - richer model has predicted okay now we
283:12 - have to compute the r squared we have to
283:14 - compute the r squared so the formula for
283:16 - compute we'll talk about the geometrical
283:17 - meaning in just in some minutes but the
283:19 - formula for computing the r is squared
283:21 - the formula for computing the aspect is
283:22 - nothing but 1 minus SSR divided by s s t
283:26 - okay this is the formula for calculating
283:28 - the
283:30 - um
283:30 - what formula for calculating your r
283:33 - squared but let's talk about let's break
283:35 - down what is SSR and what is SST so SST
283:39 - is nothing but it's the total sum of
283:41 - squares so SST is nothing but total sum
283:46 - of squares which is total sum of squares
283:50 - what does it tells SST represents the
283:53 - total variation in the dependent which
283:55 - is the variation in this so let me just
283:57 - highlight this the variation
284:00 - the total variation between the
284:03 - dependent variable is represents the
284:05 - total total variation in the dependent
284:07 - variable uh difference is what what
284:09 - difference is what variation it shows
284:11 - between whom so it shows between the it
284:13 - shows that the total variation how much
284:15 - variation you get into this okay how it
284:18 - shows that by showcasing the difference
284:19 - between the actual value and the mean so
284:23 - your SST your SSD formula is nothing
284:26 - you'd go for ISO soon all around the m y
284:29 - i minus y which is the mean device and
284:33 - and and squared okay to get rid of
284:35 - negative sign so what does it tell it is
284:37 - explaining about your why it is
284:38 - explaining about a Target variable it is
284:41 - where the where we take out the
284:42 - differences between the differences
284:44 - between your uh prediction your your
284:46 - action not a prediction ground explains
284:49 - the variation between the ground and the
284:51 - mean of this so what is the average so
284:53 - assume that average is six so the what
284:56 - is the so three three minus 6 which is
284:58 - minus three right so the variation
285:00 - between your variation between the
285:02 - actual values and the means what is the
285:03 - most common value value in this and what
285:07 - is the What is the who who are in beside
285:09 - of them what is the variation is over
285:11 - there so that's what it tells so
285:14 - basically it is saying the very we have
285:16 - already talked about variance a lot I
285:18 - don't know if this this customer once
285:19 - again so it measures the variability of
285:22 - the target variable without the
285:24 - considering the influence of the
285:26 - independent variables over here are you
285:27 - seeing I'm taking y hat I'm not taking
285:29 - my hat I'm just taking out I'm just
285:32 - taking out the variability of my target
285:34 - there how much my why is spread out how
285:37 - much my why is Target variable which is
285:39 - dependent variable is varying out right
285:41 - it's wearing out without considering our
285:43 - betas wouldn't beta1 because over here
285:45 - this is this is our data right we are
285:46 - not considering anything we're just
285:47 - taking the Y uh which is we are just
285:49 - taking the Y uh which is the mean and
285:51 - then we are just trying to take out the
285:53 - variance we are just trying to take out
285:54 - the variance between uh with with that
285:56 - mean so basically and then we are not
285:58 - doing any sort of if there is no any
286:00 - influence there is no influence no
286:02 - influence of your independent variables
286:06 - we are not used there is no influence of
286:07 - the these independent variables or in
286:09 - literally anything over there
286:11 - when we calculate so so your formula for
286:14 - that was SSD was this where to summarize
286:16 - that you're taking out the differences
286:17 - between your predict your your not
286:20 - predictive the ground truths values the
286:22 - target where the the actual Target
286:23 - values and the mean of that to take out
286:25 - the difference uh so to take out the
286:27 - variation in that particular or the
286:29 - variability in that particular variable
286:31 - okay which is why then you have nothing
286:33 - but something known as SSR is nothing
286:35 - but SSR so which is which is the full
286:38 - form is nothing but residual sum of
286:40 - squares so the formula for this is
286:42 - nothing but SSR is equals to uh I is
286:45 - equals to 1 all the one out of the n y i
286:48 - minus y hat I squared so this is
286:50 - something which if I'm which you're
286:51 - familiar with here we are taking out the
286:53 - taking of the the variability of the
286:56 - dependent variable so basically we're
286:59 - taking out the variability of the
287:00 - dependent variable considering our y hat
287:03 - considering our ads but you can take out
287:05 - the variability of this here we are
287:06 - adjusting out a variability without
287:07 - considering the Y head here we are over
287:10 - here taking over Liberty whether
287:12 - considering the y-hat because we are
287:13 - going to definitely taking the residuals
287:15 - through it explains it represents the
287:18 - unexp uh the it so basically higher the
287:21 - variability May higher the variability
287:23 - method more the difference and more the
287:25 - difference back the model because your
287:26 - model could not be able to predict so
287:28 - higher the variability explained in this
287:30 - which means that the more the model is
287:32 - not able to capture which means that
287:34 - more is not to explain again I'll
287:36 - summarize in very simple terms the SSR
287:39 - Is Nothing But whichever studied so I'm
287:41 - just going to reframe in different terms
287:42 - that you're taking out the variability
287:44 - of your dependent variable taking the
287:46 - variability of your dependent variable
287:47 - considering the effect of Y at how
287:50 - you're doing that previously in SST what
287:52 - you're doing you're taking on that
287:53 - variability of Y hat without considering
287:55 - of Y hat just sticking out the mean of
287:57 - that and then just taking out the the
287:58 - way the very with the variation but of
288:00 - yeah you're thinking the variability
288:02 - with the considering so higher the
288:03 - variability means that your three is far
288:06 - away from 3.5 that's what it means you
288:08 - know so if the variability is high that
288:10 - means the errors
288:13 - this how spread they are from the mean
288:16 - so basically the the variability is also
288:18 - which means the way the variability is
288:20 - high the model could not capture it the
288:23 - model could not learn it in a nice way
288:25 - okay so SSR over there is indicates that
288:28 - this is the formula so now once you
288:30 - about know about SSR and SST now you can
288:33 - easily come back to formula and what
288:35 - does it tell let's let's talk about that
288:38 - so your formula tells me your formula
288:40 - tells me so assume that we want to take
288:42 - out of R square so first of all let's
288:43 - let's take out wire y uh which is let's
288:46 - let's put let's take out the r squared
288:48 - for this position number three point
288:49 - three plus five plus seven plus nine
288:51 - divided by four which is number six and
288:53 - then you have s s t you take out the SST
288:55 - which is which you'll get a 20 and SSR
288:57 - which is residual which is nothing but
288:59 - two okay now we can easily compute it
289:01 - now you can easily compute it which is r
289:03 - squared which is nothing but 1 minus 2
289:05 - by 20 2 by 20 which which will get 1
289:08 - minus 0.1 which is 0.9 which is when you
289:11 - convert in percentage term which is 90
289:12 - so the r squared value is 0.9 which
289:16 - means that 90 of the variation in the
289:19 - dependent variable y can you explained
289:22 - by the independent feature X using
289:24 - linear regression so if we are able to
289:27 - explain if if our if if there is a 90
289:30 - variation in the dependent variable that
289:33 - means that 90 of the answers are able to
289:37 - correctly are your email generation
289:38 - means model is able to give 90 of the
289:40 - correct answers you this is ninety
289:43 - percent this this is your model is able
289:44 - to correctly identify ninety percent of
289:46 - the examples you'll understand in this
289:47 - way ninety percent of examples are
289:49 - correctly identified okay but it's not
289:51 - exactly what I'm saying is something
289:52 - like 90 variation ninety percent
289:54 - variation is explained by your model
289:56 - which means the ninety percent of your
289:58 - variation which is which is perfectly
290:00 - explained so the if if the variation is
290:02 - low if for example it's 20 20 so this is
290:05 - pretty pretty much low so your model is
290:06 - not able to uh exp your model is not a
290:08 - best fit because it is not able to
290:10 - explain your model if your 99 90 by 90
290:13 - you model your line vision is able to
290:15 - explain the X explain the in the is
290:19 - explained okay I hope that this gives
290:21 - you goods and what exactly this is
290:23 - higher the R square value good the moral
290:25 - is because it's able to explain more
290:27 - variations in the dependent variable y
290:29 - given or explained by the independent
290:32 - variable X so X is able to uh which is
290:35 - your linearity model is able to uh
290:38 - explain your why okay but one thing to
290:41 - be noted over here there is
290:43 - one thing which I really want to write
290:45 - one thing which I really want to write
290:47 - over here
290:48 - over here maybe let's write it out so
290:51 - the r squared which means that the X
290:54 - percentage of X percentage of the
290:59 - variation of the variation
291:02 - in the dependent variable in the
291:04 - dependent variable
291:06 - y can be explained
291:09 - can be explained
291:12 - by
291:13 - X using LR so this is what is said so if
291:17 - the variation is large then then your
291:19 - model is good but if the variation is
291:21 - less than your model is not but it
291:22 - depends on problem to problem as well
291:23 - okay but but over here see over here
291:27 - that you have X so what is it tells then
291:29 - there's if there's 90 percent of the
291:31 - video for in this example the 90 the 90
291:33 - of the variation in this particular Y in
291:36 - ninety percent of the variation ninety
291:38 - percent of the variation in this
291:40 - particular Y is explained by X is
291:43 - explained by x x seems to be useful
291:44 - feature is explained by X using our
291:47 - linear regression okay so if 90 of
291:49 - variation is explained you can
291:51 - understand your your you are able to
291:53 - your you've given the information you're
291:55 - able to correctly under 90 of them okay
291:57 - so this is this is one way to understand
291:59 - it but yeah this is a very messy thing
292:02 - to help you understand r squared we'll
292:04 - have one practical example to have a one
292:06 - one more lecture on square and there is
292:08 - something whereas adjusted R square
292:09 - we'll talk about that buckets and the
292:11 - end of this lecture I really hope that
292:12 - you really enjoyed this and now we are
292:15 - coming to the ends of our lectures
292:17 - specifically cool so uh we'll talk about
292:21 - uh hypothesis testing which is one of
292:23 - our last phases and then we'll talk
292:24 - about assumptions uh which is one of the
292:26 - my favorite Topic in the world do
292:28 - project and then we'll end up this
292:30 - lecture hey everyone welcome to this
292:31 - video is uh in this lecture what exactly
292:34 - I'm going to do I'm going to talk about
292:35 - one of the second last topic of whole
292:38 - regression analysis course is about
292:40 - hypothesis testing and hypothesis
292:43 - testing is one of the most you know
292:44 - ignored as well as one of the most asked
292:47 - questions
292:48 - um that the topics from which the
292:50 - interviewers asks integration analysis
292:52 - and we'll talk about this in pretty much
292:54 - very detail with several examples as
292:57 - well like how do we use hypothesis
292:59 - testing a linear regression we'll have
293:01 - the Practical implementation of it as
293:03 - well in various terms and uh and then
293:06 - after and then after uh working on the
293:08 - hypothesis testing we'll go forward and
293:11 - talk about uh I will go and forward and
293:14 - talk about something called as some
293:16 - something known as assumptions of linear
293:19 - regression where we'll talk about
293:20 - something but that's for later purposes
293:21 - currently our aim should be finished
293:24 - this hypothesis testing in very detail
293:27 - now one thing which I recommend over
293:29 - here is sometimes you will see formulas
293:31 - I would suggest you to completely
293:34 - ignore those formulas okay um sometimes
293:37 - you know there is a some some formulas
293:38 - will come up the main idea from which
293:41 - which you have to take up is how you can
293:43 - understand the concept the core of it
293:46 - formulas can be seen later on as well so
293:48 - how can you understand the core of it by
293:50 - understanding the Crux by several
293:51 - examples which I'll try to take Okay
293:54 - cool so now what so now what we will do
293:56 - we'll try to start off with this but
293:58 - before that I just want to clarify some
294:00 - of the things that uh as as I say
294:02 - formulas will come please ignore if you
294:04 - if you're not able to understand it and
294:06 - at any point if you're not able to
294:07 - understand I suggest go to a Discord
294:08 - server go to a Discord server and ask
294:10 - directly by giving the time stamp you
294:12 - should tag the help or create a ticket
294:14 - directly out there and then ask the
294:16 - question uh in a machine learning
294:18 - section about uh any such questions if
294:20 - you have so let's get started eventually
294:22 - uh with this with this
294:25 - um
294:26 - basically uh you you have you must have
294:29 - studied about hypothesis testing in
294:31 - statistical and probability lectures uh
294:34 - if you haven't please I consider to
294:36 - watch a little bit of lectures around
294:37 - hypothesis testing because I assume that
294:39 - you know a little bit about hypothesis
294:42 - is testing however I'll I'll I'll also
294:44 - describe over here if you don't know but
294:46 - uh I if if you're not comfortable with
294:48 - Statistics and probability I would
294:51 - suggest the highly suggest to go and
294:53 - watch our stats and prop lecture stats
294:56 - and prop lecture which is one of the
294:58 - most important you know uh lectures but
295:00 - it is available on our course which is
295:01 - cs01 or you can say machine learning
295:04 - course machine learning course by Anton
295:06 - you will be able to see everything out
295:08 - there but let's get started with
295:09 - actually this this lecture so basically
295:12 - what is hypothesis testing hypothesis
295:14 - means some sort of statements or to be
295:16 - made some thought some sort of
295:18 - statements which is not yet means this
295:20 - is the hypothesis which we want to test
295:21 - we see now this is a hypothesis which
295:23 - you want to test so the formal
295:25 - definition if we go to hippo this is the
295:27 - formal definition if you go to a
295:28 - hypothesis so hypothesis states that the
295:31 - the hypothes states that is what that
295:35 - the statement or the explanation like
295:38 - for a statement is made based on limited
295:42 - evidence or maybe small amount of
295:43 - evidence for example for a for example
295:46 - one or one of the example that that one
295:48 - of the example can be if you stay up
295:50 - late then you feel tired the next day
295:53 - that's that's one of her hypothesis
295:55 - that's one of our hypothesis right
295:57 - um so these are some of the hypothesis
295:59 - so how can we test that hypo how can we
296:01 - validate that hypothesis is valid or not
296:03 - how can we validate how can we valid the
296:06 - current example which you have taken the
296:07 - current exam what example which you have
296:09 - taken the example which you have taken
296:10 - to see we say we we are saying that if
296:13 - you stay up late then you feel tired the
296:16 - next day that's one of the example so
296:18 - how can he add if I stay up late if you
296:20 - stay up late how you'll find like how
296:22 - can we validate it so hypothesis testing
296:24 - deals with validating your hypothesis
296:26 - yes
296:27 - um using several tests you know you
296:29 - conduct tests for identifying that but
296:31 - but never mind let's actually talk about
296:33 - uh let's keep talking about hypothesis
296:35 - testing so it is a statistical method in
296:37 - terms of statistical maths it's just a
296:41 - statistical method which helps to make
296:43 - decisions about the data eventually for
296:45 - example that statement what was that if
296:47 - you stay up late if you stay up late you
296:50 - feel tired if you you you feel tired
296:53 - okay
296:56 - um that's one of them another one is
296:58 - let's say let's say for the sake of the
297:00 - example another hypothesis can be daily
297:03 - exposure to Sun leads to increased level
297:06 - of Happiness that's one of the
297:07 - hypothesis so previous hypothesis was
297:09 - that the previous hypothesis was if you
297:12 - stay up late if you
297:14 - stay late if you stay late you feel
297:17 - tired you feel tired you feel tired
297:20 - second example of a hypothesis exposure
297:24 - to sun exposure to Sun what does it
297:27 - leads it leads to increased level of
297:30 - Happiness increased level of Happiness
297:34 - increased level of Happiness so this is
297:36 - one of them so these these are some
297:38 - examples so basically hypothesis testing
297:40 - what does it do what does it do it
297:42 - validates it validates whether this
297:44 - statement is true or not based on the
297:46 - data which takes the decision whether
297:48 - this statement is true or not based on
297:49 - the available data based on the variable
297:52 - evidences okay so there are eventually
297:54 - two types of hypothesis which we
297:56 - generally know something else a null
297:58 - hypothesis and alternative hypothesis or
298:01 - or you can say H1 so s0 and H1
298:05 - so what does it mean like null uh or an
298:08 - alternative and blah blah blah so let's
298:10 - talk about uh both of them in great
298:12 - detail so that you you you could
298:14 - understand in pretty simple language
298:17 - um so let's take an example that he that
298:20 - uh that that your hypothesis is that
298:22 - that your hypothesis what is your
298:24 - hypothesis which you're not validate
298:25 - you're gonna validate whether whether
298:29 - you're let's take in a pharmaceutical
298:31 - pharmaceutical company uh invented a new
298:34 - drug of weight loss drug okay so they
298:37 - they invented a weight loss drug they
298:39 - invented a weight loss drug and they
298:41 - want to test they want to test this
298:43 - hypothesis that the weight weight loss
298:45 - is effective okay and if the hypothesis
298:48 - is true then we say that if this this is
298:50 - the hypothesis what is hypothesis weight
298:52 - loss drug is effective okay so we're
298:55 - gonna test this happens whether this
298:57 - weight loss drug the company invented
298:58 - with the new weight loss which they have
299:00 - invented is it effective or not so this
299:02 - is a hypothesis which we want to test
299:04 - which we're gonna test okay so there are
299:06 - two types of hypothesis the first one is
299:08 - no line second one is alternative so
299:10 - what is the null hypothesis in this
299:11 - particular example the null hypothesis
299:13 - in the in this particular example the
299:15 - new drug the new weight drug which is
299:17 - which is which is published or which is
299:19 - evented has no effect on decreasing your
299:22 - weight okay so we say that so we say
299:25 - that your null hypothesis the in the in
299:27 - the population this is the population
299:29 - mean which is that that indicates in the
299:31 - pop in the pop with the drug effect on
299:33 - the population is zero means the drug
299:36 - effect on a lot of people is zero okay
299:38 - and that's null hypothesis what exactly
299:39 - no hypothesis States it says that a drug
299:42 - has no effect but an alternative
299:44 - hypothesis what is it states it states
299:47 - that your drug has an effect your drug
299:49 - has an effect on decreasing the weight
299:51 - loss which means that a population the
299:54 - the in in the population this is working
299:56 - which is not equals to zero what does it
299:59 - mean ecosystem and what does it mean to
300:00 - mean the population mean equals to zero
300:03 - which means that it has no effect
300:04 - population which is not 0 which means
300:06 - that it has some effect okay because if
300:09 - the mean is zero the on the number of
300:11 - people for example this this is going to
300:12 - assume the number of people which for
300:13 - which this drug if affected is zero
300:15 - which means that the drug does not
300:17 - affect but over here it is so the drug
300:19 - eventually affected the population okay
300:23 - so how can we value how can we test it
300:26 - so basically so basically we want to
300:28 - test we want to test whether whether
300:31 - whether we will consider null hypothesis
300:33 - or alternate and uh a particular example
300:36 - this particular
300:38 - can be either classified in null or
300:40 - hypothesis or alternate hypothesis so in
300:43 - a context we want to conduct a test
300:45 - we're going to conduct a test we're
300:46 - going to conduct a test to identify
300:48 - whether it is null hypothesis which
300:50 - means the drug has no effect and an
300:52 - alternate hypothesis which means the
300:53 - drug has an effect okay so this is two
300:56 - of them which one to go forward so we
300:58 - have several tests in our statistical
301:00 - tools to conduct whether whether the
301:02 - drug has effect or not some of them are
301:04 - t-test f-test and there are several
301:07 - other as well so we'll study about
301:08 - t-test in real detail F test will be
301:11 - given to you as in homework uh to
301:13 - actually work on your by yourself
301:14 - because we want you to be not even spin
301:17 - food like this we just want you to be
301:18 - export and everything by yourself as
301:20 - well so you now now you're at a journey
301:22 - where you have to explore things by your
301:24 - own right if you have came over game
301:25 - over here this means that you have
301:26 - achieved a lot in your career as of now
301:28 - in terms of learning okay so now so now
301:32 - as you so of course when you conduct a
301:34 - test it will give some sort of number
301:35 - assume that c right so in in what and
301:38 - then what we do we call for example
301:40 - you've got C so so a week so we know
301:43 - you're going to classify you know you're
301:44 - going to classify whether whether this
301:46 - is a null or hypothesis so if if that I
301:50 - assume that you conducted a test as in
301:52 - though don't worry we'll talk about
301:53 - Easter in the great detail you conducted
301:55 - a test and then you got some number c
301:57 - okay you got some number c so we check
301:59 - we check with the significance we three
302:01 - with a threshold and we say that if your
302:04 - if your if your test value is is smaller
302:08 - than is smaller than is it smaller than
302:10 - this particular 0.00 which is five
302:12 - percent which is five percent smaller
302:14 - than five percent then what we do we
302:16 - reset the null hypothesis and then we
302:18 - accept the alternate hypothesis which
302:19 - means the drug then we re then the drug
302:21 - has an effect so if if your if your TV
302:25 - if your value from the test which you
302:27 - conducted is smaller than the
302:28 - significance value which is 0.05 this is
302:31 - a standard threshold which we keep
302:32 - foreign
302:35 - test will give you some number and then
302:37 - you and then you compare with it if if
302:39 - the test value is smaller than this then
302:41 - we say that the drug has an effect why
302:43 - we'll talk about that later on great
302:44 - detail as well about this don't worry
302:46 - okay and then if if it is if if it is
302:49 - greater if it is great out your T value
302:50 - is greater than this then we say that it
302:52 - is a alternate hypothesis sorry if we
302:54 - resect the alternate hypothesis and then
302:56 - accept so basically anything a widget
302:58 - anything above the five percent and your
303:01 - your validity value or the test value is
303:03 - greater than five percent then we say
303:05 - that the drug has no effect and then we
303:08 - say the drug has no effect it is smaller
303:10 - than five five percent the drug has an
303:12 - effect okay
303:14 - and that that was one of one of the
303:15 - example let's talk about in great detail
303:17 - this this was just an example to
303:19 - Showcase you the hypothesis testing but
303:21 - how we use the hypothesis testing in
303:23 - linear regression that's what that's one
303:24 - of the most famous questions to talk on
303:27 - um I just I just was uh getting a Red
303:30 - Bull I'm so sorry for that okay so uh
303:34 - how can we think about in terms of
303:36 - linear regression and this is something
303:38 - which everyone needs to know about this
303:40 - this is something which every everyone
303:41 - needs to eventually know about so let's
303:43 - get started with that as well yes
303:46 - cool so how can we use that in uh linear
303:51 - regression in linear regression so
303:54 - basically
303:55 - so basically
303:57 - um I'll just go through a past example
303:59 - so that you could be understandable in
304:01 - the easy way I rather prefer all of this
304:03 - thing okay so this is this is one of
304:06 - them this is one of them where we talk
304:08 - about F tests Yeah so basically how does
304:11 - we formulate our hypothesis in terms of
304:13 - linear regression so in a case of linear
304:15 - regression the claim is made that there
304:17 - is an a relationship so what is an
304:20 - hypothesis hypothesis is that there is
304:22 - there's exist a relationship between a
304:25 - response and a predictive variables now
304:27 - you might be thinking why do we need
304:28 - hypothesis testing why do we need
304:30 - hypothesis testing so basically how and
304:33 - how does it helps and how does it helps
304:35 - now this all of this might be in your
304:37 - question but first of all I I understand
304:39 - that your question is so let's go ahead
304:41 - and talk about why exactly we need
304:43 - hypothesis testing so assume that assume
304:46 - that
304:46 - um you you want to build you want to
304:48 - build you want to build a function f
304:50 - that takes in the number of hours of
304:53 - study number of hours and the attendance
304:55 - per percentage of a particular child and
304:57 - then predict what what will the exams go
304:59 - so so it takes the X1 and X2 and
305:02 - predicts Y which is the exam score okay
305:04 - so it takes X1 and X2 and predicts Y and
305:07 - this is a nice uh this is the function
305:10 - which you have to build this is the
305:11 - function which you have to build up okay
305:13 - so you have um now now now
305:16 - it may happen now it may happen now what
305:19 - what do you want to do you want to
305:21 - interpret right is this significant to
305:23 - outward variable is this information is
305:27 - this particular independent X1 is
305:30 - significant what does that mean
305:31 - significant significant means important
305:34 - okay
305:35 - like you can understand is is it
305:38 - significant is it even reliable for this
305:40 - so basically what we do
305:43 - and the first hypothesis formulation is
305:45 - is there exist any relationship between
305:47 - both of them yes or no how can we
305:50 - identify a relationship between both of
305:52 - them how can we identify is there any
305:53 - relationship between both of them uh the
305:55 - input variables and router variables by
305:57 - calculating the significance okay how
305:59 - much this value X1 is significant to
306:02 - this output variable y okay because this
306:04 - is important in terms of the the reason
306:06 - why it is important the reason why it it
306:09 - is important see example X1 and X2 and
306:12 - you're getting the Y so so basically you
306:14 - identify the relation between both of
306:15 - them so if there is no relationship then
306:17 - your machine learning model will be not
306:19 - able to perform well so you can tell to
306:20 - stakeholders that there is no
306:21 - relationship it with the in into the
306:24 - number of hours a particular student
306:25 - study that that whatever number of hours
306:27 - to do in a particular study it will not
306:30 - affect exams course exams course will be
306:32 - the According to some of the factors
306:33 - okay so this can be one of the possible
306:35 - factors so if we should first of all uh
306:37 - assure ourselves that there is a
306:39 - relationship okay this is this is one of
306:41 - them so there is a relation how can we
306:42 - identify there's a relationship between
306:43 - input variables and output variables we
306:45 - can identify the relationship between
306:47 - the input variables and output variable
306:48 - by identifying the significant say the
306:51 - significance of our input variables to
306:53 - outward variables okay so we can say
306:55 - that okay okay so this is uh is this
306:57 - significant to our output variable yes
306:59 - or no yes or no is this significant to
307:02 - our output variable yes or no X1 is
307:04 - significant so if a is is so question is
307:07 - is your is your number for our study is
307:10 - significant to your exams course
307:12 - so this is the this is the hypothesis
307:14 - formulation hypothesis formulation what
307:16 - does it says that there is a
307:18 - relationship there is a relationship
307:20 - there is a relationship between input
307:21 - and outer variable between between input
307:23 - and output variables or independent
307:25 - features and dependent features okay and
307:27 - then we go to the X2 and then say okay
307:29 - is this something related to is this
307:31 - something is the X2 which is the
307:33 - attendance is also is is also
307:35 - significant talk about yes or no okay so
307:38 - this is how we this is this this is why
307:40 - important because we want to distribute
307:41 - into identify is this very even
307:42 - relationship and assume that X2 does not
307:45 - have a significant so we can drop this
307:47 - column we can drop this feature if you
307:49 - drop this feature it will definitely
307:50 - help us in getting the robust model
307:52 - because this is just unnecessarily over
307:54 - here because it's not contributing to
307:55 - your output variable does not have any
307:57 - relationship with output variable
307:58 - understanding what I'm trying to say I'm
308:00 - just trying to convince you just I'm
308:02 - trying to convince you that there is a
308:04 - relationship there is a relationship
308:05 - there is a relationship if there is a
308:07 - relationship that's that's the build of
308:09 - a hypothesis formulation
308:12 - we'll talk about all of this in great
308:14 - detail but you can understand this
308:15 - exists a relationship between the
308:17 - Institute between the response which is
308:19 - the Y and the predictor so if that's the
308:21 - hypothesis formulation okay so what the
308:24 - the formulation of hypothesis times says
308:27 - that if the claim is represented using
308:30 - the non-zero then then the alternative
308:31 - have this is the uh this is Alter
308:34 - alternate hypothesis and if your beta
308:36 - value are zero then this is a null
308:38 - hypothesis I know you didn't understood
308:40 - anything let's talk about uh what is
308:42 - what exactly that space statement says
308:45 - so what will be our null hypothesis in
308:47 - this claim and what will be our
308:48 - alternate hypothesis null hypothesis
308:50 - null hypothesis means that there are no
308:54 - relationship between whom between your
308:56 - independent X1 X2 and
309:00 - um y there's a row no relationship
309:02 - between X1 and X to Y and what is the
309:04 - alternate hypothesis alternate means
309:06 - completely opposite okay that there is a
309:08 - relationship there is a relationship
309:10 - between X1 and X2 and Y okay so in so we
309:14 - now now how can we how can we say that
309:16 - how can we even make it more
309:18 - mathematical so null hypothesis we can
309:20 - can we say that null hypothesis where
309:23 - your beta1 and beta 2 are equal to 0. or
309:27 - we can say all your betas are equal to
309:29 - zero or your all your beta 1 and beta 2
309:32 - are are equal to zero null hypothesis
309:35 - means so if so for example let you I
309:38 - know it's it's very simple to understand
309:39 - the concept please apply your mind now
309:41 - over here that you have your hypothesis
309:44 - which is beta0 or um beta 1 beta X1 plus
309:49 - beta 2 x 2 okay you're understanding so
309:52 - if we say that if we say that that if if
309:55 - if there is if there is no effect if
309:58 - there is no effect of this an output
310:00 - variable which means that this is equal
310:02 - to zero this is equal to zero all right
310:04 - bro I'm just trying to say that if if
310:07 - this is equal to 0 that means that will
310:09 - do yield to 0 and that will it does not
310:11 - that will not affect your output
310:13 - variable but whatever eventual null
310:14 - hypothesis is saying that your X1 X does
310:16 - not have any significance right so where
310:18 - beta 0 is equal to zero that means that
310:20 - X1 does not have any significance if
310:22 - your beta 2 is not is equal to 0 that
310:24 - means X2 is also does not have any
310:25 - significance in output variable okay so
310:28 - here all betas will be equal to zero a
310:30 - null hypothesis all betas are equal to
310:33 - zero in a null hypothesis an alternate
310:35 - hypothesis says that alternate
310:37 - hypothesis which means that all the
310:39 - betas are not equal to zero are not
310:41 - equal to zero that's your hypo this is
310:43 - your hypothesis now you might be
310:45 - thinking a bit of off key how this is
310:47 - how this is coming again I'll summarize
310:49 - it for you
310:50 - that your h of X is nothing but equal to
310:53 - beta0 plus beta 1 x 1 and beta 2 x 2. so
310:57 - so null hypothesis what does it say is
310:59 - that there's no relationship if there's
311:00 - no relationship that beta will be down
311:02 - if we make beta 1 to beat a beta 2 to b
311:04 - equals zero that means it will hold
311:06 - yield to zero and this will hold you to
311:07 - zero which means it's not affecting any
311:09 - output variable right but what if if
311:12 - there's if if the if there's something
311:13 - out of zero that is alternative which
311:15 - means that that it is affecting some
311:17 - sort of outer it is affecting some sort
311:18 - of output a variable so a null
311:21 - hypothesis we say that all R beta value
311:23 - are nothing but zero but alternate all
311:25 - our value is nothing but not equal to
311:27 - zero this is our hypothesis for
311:29 - emulation in linear regression I hope
311:32 - that this gives you a very good
311:34 - understanding about what exactly we aim
311:37 - to start solving about uh this okay
311:40 - so so now you can see that if everything
311:43 - is equals to zero if everything is equal
311:45 - to 0 that means this is a null
311:47 - hypothesis which means that does not
311:48 - have any output that does not have any
311:50 - consequence or output variable but if
311:51 - there's not even result that means they
311:52 - have a consequences on output variables
311:54 - so there are three types of tests we
311:57 - should uh talk on uh which is test for
312:00 - significance of regression is it a
312:02 - regression problem or not maybe it can
312:04 - be easily classified when we'll talk
312:05 - about linearity assumption it can be
312:08 - easier talked about a linearity
312:09 - assumption which will see that later on
312:12 - one is test t-test which checks the
312:15 - significance of individual to
312:16 - coefficient regression coefficient so
312:18 - what is it tells so what does it tells I
312:21 - think I did a little bit wrong yeah yeah
312:23 - so it what is the T Test us T Test as I
312:26 - said that we have a hypothesis where our
312:28 - s0 where what does null null hypothesis
312:31 - indicates there are beta values nothing
312:32 - but equals zero and an alternate
312:34 - hypothesis what does it indicates either
312:35 - beta value is not equal to zero is not
312:37 - equals to zero this is our formal
312:39 - formulation apart is testing which on a
312:42 - test original test so in a test we want
312:44 - to test whether there's exist
312:45 - relationship or not so we have a test
312:47 - right we conduct a test and then it gets
312:49 - at some certain value let's assume C so
312:51 - if that c is a smaller than 0.05 it's
312:53 - more than five percent we'll talk about
312:55 - that this this again significance just
312:58 - in a second but if this model then then
313:00 - we say that we say that we reject the
313:02 - null hypothesis that means the there's a
313:04 - no release except that there is a
313:05 - relationship but if it is greater than
313:08 - 0.05 that means that we reject the
313:10 - alternative with that there is no
313:11 - relationship between the variables okay
313:13 - reject this in order and then we accept
313:15 - this uh we'll talk about the
313:16 - significance just in a second so this
313:18 - test check the significance of
313:20 - individual regression coefficients
313:21 - individually okay not something like
313:24 - over it's it checks that whether it be
313:26 - whether this X1 is is six is significant
313:29 - to Output variable or not or whether
313:30 - it's X2 is significant to Output
313:32 - variable not how does it checks that if
313:34 - your beta 2 is equal to zero so your
313:35 - your hypothesis will be a zero where
313:37 - beta 2 is equals to zero NH hypothesis
313:40 - with beta 2 is is not equal to zero so
313:42 - this is a hypothesis it checks
313:43 - individually it checks individually and
313:45 - once it checks individually we can
313:46 - easily take out the overall overall
313:48 - significance okay so so so if we take if
313:51 - we take out individual uh significance
313:53 - then we can easily is okay this X1 is
313:55 - not in the not significant then we can
313:57 - remove this X1 how how can if the beta 1
313:59 - of X1 is a zero that that means that it
314:02 - is not significance and then we have
314:04 - something known as t-test and then we
314:06 - okay uh sorry F test and F test is used
314:09 - for taking out the overall significance
314:13 - the overall significance overall
314:15 - significance of your regression barriers
314:17 - overall which means whether this overall
314:19 - equation is significant or not okay
314:22 - significant means overall so the
314:24 - hypothesis formulation would be the null
314:25 - hypothesis would be beta of all the way
314:27 - devices and an alternative which is all
314:29 - the video value is not not equals to so
314:30 - this is the hypothesis form formulation
314:32 - three test let's talk about the first
314:35 - test t-test def test will be left left
314:37 - upon you to explore I hope that I'm
314:39 - giving you a very great knowledge is out
314:41 - of it and I really hope that you'll
314:42 - utilize this information because for me
314:44 - it takes a ton of time you know to
314:46 - develop this and I hope that you will uh
314:49 - that you will that that you're enjoying
314:51 - this course out till here and if your
314:53 - till here that means that's something
314:55 - you're a special guy you know uh that's
314:57 - you're a special guy and you're always
314:59 - up for very big challenges out there
315:01 - okay cool so now now I've been talking
315:05 - about something known as significance
315:07 - value so we'll conduct the t-test so how
315:10 - can we how can we conduct the t-test so
315:13 - there are some steps for conduct for
315:15 - conduction of a t-test
315:18 - I'm directly going to the you know
315:20 - t-test uh formulas so that it is my it
315:23 - it will make much more sense to you only
315:24 - when we'll do one example
315:27 - okay so let's go to uh
315:31 - one T Test example yes so you're a so
315:35 - your first so you're basically again
315:36 - I'll just write the t-test example so
315:38 - that it makes much more sense yeah so
315:41 - your so your hypothesis formulation is
315:43 - hypothesis for emulation is so assume
315:45 - that let's take an example because I
315:47 - really like to take examples in every
315:49 - bit whatever I'm explaining because that
315:51 - something is makes me motivated to like
315:54 - your and you're able to understand by
315:55 - the simple examples so example what this
315:57 - example says example says that you want
316:00 - to build a simple linear regression
316:02 - model we build a simple linear
316:04 - regression model build a simple linear
316:06 - regression model and what is that same
316:08 - simple integration more model it will
316:09 - build the function f build a function f
316:11 - that takes in the the text and the
316:14 - square footage the the the the the
316:17 - square footage or or or you can see the
316:19 - size of the house size of the house and
316:22 - predicts your price and predicts your
316:25 - price so given the x value is going to
316:26 - predict y I'm just taking a simple
316:28 - linear equation to make you understand
316:29 - so in this case what zero
316:41 - okay and Alternatives means that your
316:43 - beta1 which means the beta1 which is the
316:45 - beta 1 is the say is the coefficient of
316:47 - this X1 right beta1 is not equal 0 which
316:50 - means that there is a this this means
316:52 - there is there is no relationship no
316:56 - relationship relationship between X1 and
317:01 - Y which means that no religion resizes
317:02 - the house and price the size of that
317:03 - does not affect a price and this there
317:07 - is this alternate means there is a
317:09 - relationship between X and Y that means
317:12 - that that means that there's the release
317:13 - the size of the house definitely affects
317:16 - this price okay so when you fit when you
317:18 - fetch the linearization model and you
317:20 - fit the linear version model you have
317:21 - the following equation now what you do
317:24 - you calc you do the test now you do the
317:26 - T Test how will do will talk about just
317:29 - in some sentence but the formula for a
317:31 - calculate for the formula for doing the
317:34 - for the formula for doing for for for
317:36 - doing this test is we as we do
317:39 - individually for every parameters right
317:41 - if we do the individually for every
317:43 - parameters so the T we calculate the T
317:46 - statistic the t-st statistic for whom
317:49 - these statistical for the size of the
317:51 - house because it does individually
317:52 - assume that it assumed so we'll take the
317:54 - take out the T for every individual
317:56 - parameter So currently I'm going for
317:57 - only one so it takes a t statistic for
318:00 - the size of the house which is estimated
318:02 - coefficient estimated coefficient the
318:05 - coefficient value so in this case it
318:07 - will be
318:08 - beta 1 minus zero beta 1 minus 0 divided
318:13 - by the standard error of that beta 1. a
318:17 - very nice equation of very confusing to
318:19 - a lot of people now let's talk about why
318:22 - does it confusing and let's talk about
318:24 - each and every Integrity of it okay each
318:28 - and every Integrity of it so uh we'll
318:32 - talk about why are we subtracting it why
318:35 - are we dividing with sound error what is
318:37 - a standard error and yeah so and and and
318:40 - and after conduct after conduction of
318:42 - this test what we do so that it makes
318:44 - much more uh sense to you in your
318:47 - initial stages as well okay so let me
318:50 - just uh let me just open my uh from a
318:53 - calculator because this is something
318:54 - which I'll be required which will be
318:55 - required so let's first of all talk
318:57 - about let's first of all talk about the
318:58 - general formula so the general formula
319:00 - for calculating the t t value to
319:03 - statistic for a particular beta for a
319:05 - particular beta is nothing but estimated
319:07 - coefficient that is coefficient which is
319:08 - estimated because we have to take every
319:10 - this happens stress testing only can be
319:12 - tested if you if you have a fully
319:14 - trained model so that you can test your
319:15 - hypothesis okay estimate coefficient
319:19 - minus 0 divided by standard error of
319:23 - that coefficient standard error of that
319:26 - coefficient so what does it measures
319:28 - what does it measures so what we do what
319:31 - we are doing over here so basically
319:33 - we're calculating the T statistic and
319:35 - then we are subtracting the hypothesized
319:38 - hypothesized value from the official
319:42 - from the estimated coefficient so so you
319:44 - might be thinking what is hypothesized
319:47 - value you this is something which is uh
319:49 - confusing to most of the people so
319:52 - hypothesize the value is the value of a
319:54 - coefficient under the null hypothesis so
319:57 - we we first of all assume okay in uh
320:01 - basically what basically what we do we
320:04 - hypothesize something we make a we make
320:07 - something like true like there's a no
320:08 - ratio we prior to assume that there is
320:11 - no relationship okay and then we
320:13 - disprove that and then we disprove that
320:15 - so hypothesized value is zero in this
320:18 - case because your hypothesized value is
320:20 - zero okay so estimated minus the
320:22 - hypothesized value which means the value
320:24 - of the coefficient under the null
320:27 - hypothesis okay this is the hypothesized
320:30 - value which is
320:31 - if the beta 1 is equal to five that that
320:34 - means I'll write a five over here
320:43 - already assume that there's no
320:45 - relationship now we disprove that uh
320:47 - with with with t statistics so if you're
320:49 - not even disprove that that means there
320:50 - is no relationship but if you're able to
320:51 - disprove then we accept the alternative
320:54 - hypothesis okay so over here this
320:58 - actually usually that there's no
320:59 - relationship so basically we subtract
321:01 - with the hypothesized value and then we
321:03 - divide by the standard error so what
321:06 - does a standard error means what does
321:09 - standard error means so let's just go
321:11 - and talk about uh let's just go and talk
321:14 - about a very nice explanation of
321:16 - standard error because this is something
321:18 - also also people you know uh worry a lot
321:21 - about so standard error is nothing is
321:24 - nothing but
321:26 - um uh it's it's nothing but accuracy the
321:29 - accuracy of an estimated value the
321:32 - accuracy or how much uncertain or how
321:35 - much uncertain your regression
321:37 - coefficients are so I'll take a very
321:39 - simple example to make you understand
321:41 - about this
321:42 - um so basically assume that uh so assume
321:45 - that you built a linear regression model
321:46 - and the standard error is the measure of
321:50 - uncertainty of the estimated
321:52 - coefficients and then describe the ratio
321:54 - basically uncertain uncertain
321:55 - uncertainty means the student standard
321:58 - error so I'll just write it over here so
322:00 - standard error it measures the
322:02 - uncertainty uncertainty uncertainty or
322:07 - the accuracy
322:08 - accuracy or preciseness or the
322:12 - preciseness of the estimated
322:14 - coefficients of the estimated
322:16 - coefficients this is a very important uh
322:19 - to talk on that that the standard is
322:21 - nothing but the accuracy uncertainty
322:24 - preciseness so that lower the
322:27 - uncertainty means better your
322:28 - coefficiency so basically it evaluates
322:30 - basically it evaluates how your
322:33 - coefficients are uncertain how how your
322:36 - estimated coefficients are accurate how
322:38 - your uh how your uh how your estimated
322:41 - coefficients are precised and all so
322:43 - these are certain uh certain words to
322:46 - talk about so basically so basically you
322:49 - calculate so basically you calculate and
322:51 - say that okay this is something is used
322:53 - this is something it is used to indicate
322:55 - how precise or or precise or accurate
322:59 - your estimated value are okay so again
323:02 - you might be your might be thinking so
323:03 - you might have beta 1 equals to two beta
323:06 - is equal to 2 by estimated by your
323:07 - gradient descent okay by estimated by
323:10 - your gradient recent that's one of them
323:12 - so when you estimate a beta1 so how
323:13 - precise how much how much precise this
323:16 - is how much precise is how much accurate
323:19 - this is is how much earns how much
323:21 - uncertainty it contains how much
323:22 - unsureness how much it we are sure about
323:24 - this
323:25 - we quantify that so we quantify how much
323:29 - we are sure about that we quantify how
323:31 - much we are accurate about a particular
323:32 - beta value that's what the standard
323:34 - error means standard error means how
323:36 - much we are sure about that particular
323:38 - beta value okay how much you're sure
323:41 - about that I hope that it that I hope
323:43 - that it gives you so if your standard
323:45 - error is high if your standard error if
323:48 - it's time if you if your standard error
323:49 - is high that indicates that there is
323:52 - more runs under uncertainty more
323:54 - uncertainty a less accuracy less
323:57 - preciseness
323:59 - okay so if your uncertainty is high
324:01 - which means not sure about that that
324:03 - means that is bad no if you're not sure
324:05 - about if you're not too much sure about
324:06 - the particular coefficient that means
324:07 - that that that means that there's
324:09 - something wrong with it if it's super
324:11 - accurate with that and you're also not
324:12 - accurate so if your standard is small
324:14 - that means that your alternate is low
324:16 - which means you are too sure but you are
324:18 - able to quantify things you are able to
324:19 - quantify how much sure and unsure you
324:21 - are so in this case if you are sure if
324:23 - your unsureness is too low which means
324:25 - that you're sure of that beta values to
324:27 - that that means that is uh that is good
324:30 - so your assignable error should be less
324:33 - so basically what we are doing so
324:35 - basically what what we are doing we are
324:37 - simply subtracting we are simply
324:38 - subtracting sorry we are simply first of
324:40 - all the now now you understand this full
324:42 - equation you have you calculate the T
324:44 - value calculate the T value which is
324:46 - beta 1 minus 0 divided by the standard
324:49 - error so now what does the division
324:52 - gives you what does the division gives
324:55 - you this is something which is usually
324:57 - taught in the statistics class classes
324:59 - and I usually recommend you to not even
325:02 - uh which is the which is the which is
325:05 - not even recommended for you even if you
325:07 - if you if you're just curious you can
325:09 - just go ahead and talk about things uh
325:11 - but what exactly what I'm gonna do is
325:12 - talk about my interpretation from what I
325:15 - under understood because that's
325:17 - something you know I I also have to take
325:19 - care whether whether I understood it
325:21 - nicely or not right
325:24 - um so uh let's talk about that uh
325:27 - otherwise it will be bit off to me as
325:29 - well so the T value which you're going
325:33 - to get over here the p-value which is
325:35 - going to get over here which will give
325:36 - you which will give you a number so that
325:38 - we can compare with the threshold we'll
325:40 - talk about to the threshold just in some
325:42 - second just just after this topic so the
325:45 - T this is called the T statistic this is
325:47 - called the T statistic is that is what
325:50 - is the what is this test it helps us to
325:53 - assess the significance of the
325:55 - coefficient relative to the variability
325:58 - or the the estimate or the uncertainty
326:01 - of our estimates so what does it tell
326:03 - again I'm going to repeat it tells you
326:05 - please remember the significance it
326:08 - tells you I'm just taking in same very
326:10 - I'm not taking very mathy you know if
326:11 - you if you go online no everybody is
326:13 - taking so mathematic things I'm not able
326:15 - to even understand why they are taking
326:17 - mathematic things but it tells you the
326:20 - significance it tells you the
326:22 - significance of a coefficient of a
326:25 - coefficient relative relative to the
326:29 - uncertainty uncertainty uncertainty of
326:33 - the estimate of estimate that's exactly
326:37 - what it what what what is it t-test
326:40 - gives you in geometrically if you
326:43 - understand it geometrically it can be
326:45 - thought of as how many standard errors
326:47 - the how many standard errors the
326:50 - estimated coefficient is away from the
326:52 - hypothesized value so again
326:55 - geometrically meaning
326:57 - and geometrically meaning I know this is
327:00 - something uh this is something
327:01 - unconfusing as well I can I can ask you
327:03 - to ignore it as well but uh but if you
327:05 - just want to understand it much more
327:06 - depth it will be easy for you so it can
327:09 - be thought of as a it can be thought it
327:13 - can be thought as uh it can be thought
327:16 - of as how many standard errors how many
327:18 - standard errors as standard errors are
327:21 - away are the the the the coefficients
327:24 - the coefficients the standard errors of
327:26 - the coefficients is away from the
327:28 - hypothesized value is are we from what
327:31 - is hypothesized value which means that
327:32 - we already assume it which is
327:35 - beta is nothing which is which
327:37 - hypothesized value which is equal to
327:38 - zero so what does it get now this is
327:40 - something which which how many standard
327:42 - errors are away from that away from that
327:45 - okay so now what does it give the taste
327:49 - test the the what does it give is it
327:51 - gives uh so if one once you calculate
327:54 - your T once you calculate the T
327:55 - statistic so we're gonna understand two
327:57 - ways either by this particular example
327:58 - of this particular what does it give is
328:01 - significance of a coefficient relative
328:03 - to the uncertainty unsureness what is
328:05 - the significance of particular
328:06 - coefficient with respect to the relative
328:08 - to the there is uncertain uncertainty of
328:10 - the estimate higher than certainty like
328:12 - it's not good right so the last T
328:15 - statistic so now let's talk about what
328:17 - does it mean if you have a large t
328:21 - statistic if you have a large D
328:23 - statistic what does it mean if you have
328:26 - a large D statistic so if you have a
328:28 - large D statistic you it indicates that
328:32 - your estimated coefficient is different
328:35 - from the hypothesized value
328:38 - is different so last is statistic means
328:42 - that your that your estimated
328:44 - coefficient that your estimated
328:45 - coefficient is different is different
328:48 - from the hypothesized value is different
328:50 - from hypothesized value which is s0
328:52 - which means that there is a relationship
328:54 - that there is a relationship between
328:57 - your h j which this is between your beta
329:00 - 1 and y okay
329:02 - so this would last the statistic mean
329:04 - you will will be talking about it
329:06 - usually it depends whether if you have
329:08 - the P values we have to compare with our
329:09 - P P value but as of now why do we do why
329:13 - do we divide we divide it because
329:15 - because it tells you the significance of
329:17 - our equation now once we have the
329:18 - significance now how can we say that
329:20 - okay this is the perfect like how can we
329:21 - identify whether to accept the null
329:23 - hypothesis or whether you expect the
329:25 - hypothesized value which means that the
329:27 - pre-assumed thing which is there's no
329:28 - relationship or should we accept it or
329:30 - should we reject that there is that we
329:32 - have an Evidence to say that that there
329:35 - is a relationship between that okay uh
329:37 - let's talk about that this is something
329:39 - important to talk on as well I'm just
329:40 - talking uh too much and so sorry for
329:42 - that but it's actually 30 minutes but
329:44 - that but this is something which is
329:46 - important as well uh in terms of
329:48 - understanding level as well okay so
329:51 - let's talk about uh P value let's talk
329:53 - about p-value so p-value over here what
329:56 - is it p-value means so so once we have
329:59 - the t t statistic we once we have the T
330:01 - to if we compare with the threshold
330:04 - value usually
330:06 - this threshold value 0.05 or 5 and we
330:10 - say that if you T statistic is less than
330:13 - five percent is less than five percent
330:15 - then there is a then we can resect our
330:18 - null hypothesis and then we can reject a
330:19 - null hypothesis and say that there is if
330:21 - it is greater than five percent then we
330:23 - say that okay this is something
330:25 - um
330:26 - we can reject the alternative that there
330:30 - is no relationship then we can say that
330:31 - there's no relationship if your T
330:33 - statistic is greater than like the three
330:35 - critical value the T critical value okay
330:37 - uh currently T statistic is different
330:39 - from T critical value D critical
330:42 - valuables will talk about it just in a
330:44 - second but assume that you got a some
330:45 - critical value and then you compare with
330:47 - a 0.05
330:49 - okay
330:50 - okay so uh you can take a break as of
330:53 - now even I'm speaking for the last 40
330:55 - minutes but that's all right this is
330:56 - something which we have to do if you
330:58 - want to understand uh hypothesis testing
331:00 - in detail
331:02 - um so so basically what is the so so how
331:05 - can we say that how can we say that that
331:08 - if your T critical value the critical
331:10 - value the the value which you will get
331:12 - the probability the the the critical
331:13 - value which you'll get is if it's more
331:15 - than five percent then only if it is how
331:17 - can you support that statement we can
331:19 - say that how can we say that how can we
331:22 - say that as I said as I say that the
331:26 - null hypothesis is a statement that
331:28 - assumes that there is no effect there is
331:31 - no effect on the relationship so there's
331:33 - there we introduce the concept of
331:35 - something known as P value and threshold
331:38 - value odd is also called nothing but
331:40 - p-value so what does p-value means it
331:44 - says that it says that the observed
331:46 - result is unlikely to have occurred by
331:49 - chance alone and so basically what does
331:52 - it mean I know it's something uh chance
331:53 - and all it's something which is also
331:55 - very great to understand it
331:58 - uh but I'll take a very simple example
332:00 - uh so that you have uh a nice
332:03 - interpretation so assume that you want a
332:05 - 300 test if new weight loss supplement
332:07 - is effective on weight loss or not again
332:10 - the same example which I've taken up so
332:12 - you conduct experiment on two groups of
332:14 - people so group number one and group
332:16 - number two conduct on two groups of
332:18 - people and then once you are one groups
332:20 - that takes the supplement and other
332:22 - takes the uh place which is the other
332:25 - takes the supply once one group takes a
332:27 - supplement other does not fix that
332:28 - supplement okay so hypothesis what does
332:30 - null hypothesis States null hypothesis
332:32 - is that there is no effect of that
332:33 - supplement
332:34 - okay that there's no no effect and and
332:37 - no difference there's no effect so
332:39 - basically there is a two groups again
332:41 - there's a two group group number one
332:42 - group number two so each taken their own
332:44 - treatment affect the supplement those
332:46 - the supplement number one and supplement
332:47 - number two okay so the null hypothesis
332:49 - states that there is no difference
332:51 - between these two so in identify if
332:53 - there's any difference that's a
332:54 - hypothesis and uh the the hypothesis uh
332:58 - the alter alternate hypothesis says that
333:00 - there is a difference which means that
333:01 - if there's a difference that means there
333:03 - is something cooking in that so we can
333:04 - of course use that for treatable effect
333:06 - but I bought the null hypoall state that
333:08 - there is no difference so after you
333:09 - conduct the experiment you calculate
333:11 - your critical value and then what you
333:13 - have a significance you value and then
333:15 - and then your critical value comes
333:17 - critique critical value 0.02 0.02 okay
333:21 - and 0.02 so your value is 0.02 which is
333:24 - the T critical value by by doing the
333:27 - bike taking all the T statistic you
333:29 - calculate 0.02 now now your threshold
333:32 - was 0.05 0.05 right so so what this
333:36 - means that that means that there is
333:38 - enough evidence to support that the
333:41 - weight loss supplement has an effect on
333:43 - the weight loss so there is a Soviet X
333:45 - we reject the null hypothesis we reject
333:47 - the null hypothesis
333:48 - represent so if if your critical value
333:53 - is is a smaller than your significance
333:56 - value then we say then we reject then we
333:59 - reject our null hypothesis and accept
334:01 - our uh alternate hypothesis
334:04 - now you might be thinking why the why
334:06 - does it happen like that why does it
334:08 - happen like that
334:09 - uh this is something uh this this is
334:12 - something which also you need to
334:13 - understand so this is a general thing if
334:16 - a critical value is smaller than this
334:17 - then we say that okay reject the null
334:19 - hypothesis and then we accept that
334:20 - there's a so but if but if you got 0.07
334:23 - that means that you have to resect but
334:25 - you have to reject your alternate
334:27 - headphones except the null hypothesis
334:28 - okay I know this is something hard to
334:30 - understand as well in the initial level
334:32 - but you have to understand it anyway
334:34 - assuming that you have the name for this
334:36 - cool so how can we connect this how can
334:39 - how should we connect this P value the
334:40 - concept of p-value to linear regression
334:43 - so the so in linear regression your
334:45 - known hypothesis states that the
334:46 - independent variables and dependent
334:48 - variables has no effect your independent
334:50 - variable has no effect on the depending
334:52 - which means so all your betas are equal
334:54 - to zero and the P value associated with
334:57 - every coefficients help us to determine
334:59 - uh that has a significance with us so
335:01 - basically you have a critical value in
335:03 - this case you will you'll be having
335:04 - critical value for every uh betas okay
335:07 - so correct currently we'll be having
335:09 - critical value or the P P value for this
335:12 - size which states that which states the
335:14 - significance of our size of the house on
335:17 - pre on the price
335:18 - and so in this example in this example
335:21 - as we have seen so you obtain so you
335:24 - obtain the p-value or we can see so
335:26 - there are P value or we can say critical
335:28 - value we can say critical value by
335:31 - taking the T test and then and then
335:33 - comparing with the t-test distribution
335:34 - we'll talk about that we'll talk about
335:36 - this you calculate the T test after
335:38 - calculating the test over here after
335:40 - calculating this after calculating this
335:42 - you compare after you take out you know
335:44 - after you take out some number you
335:46 - compare with the table you compare with
335:48 - the table and the table you it is a
335:50 - table which is already uh published on
335:52 - the internet okay you you don't even
335:54 - remember everything you don't need to
335:55 - worry about how this came you compare
335:56 - okay this this is something over here
335:58 - now this is uh we have to we'll we'll
336:00 - select the how do we select the p p
336:02 - values as well later on but you you
336:04 - calculate how much significant how what
336:06 - is the probability that this is that is
336:08 - that size of the house is significant
336:10 - okay you calculate that P you calculate
336:12 - the P value for the coefficient of what
336:14 - over the coefficient of your size of the
336:17 - house and now we say that if if your P
336:19 - value is less than 0.05 is a threshold
336:21 - which is the threshold you reject the
336:24 - null hypothesis where you say that there
336:25 - is not enough evidence to accept the
336:27 - null hypothesis so you might be thinking
336:29 - what does it mean bro why do we why do
336:31 - we don't don't cities if it is if your
336:33 - baby value like if your probability is
336:35 - higher that means that it is something
336:37 - significance that is not okay p-value is
336:42 - always related to the null hypothesis so
336:43 - higher the p-value is the more you're
336:46 - providing evidence that there's no
336:47 - relationship Lord is that there is an
336:50 - Evidence to reject them there is
336:52 - evidence for alternative that there is a
336:54 - lower chance that there is a lower
336:56 - chance of
336:58 - um of there's a lower chance that it
337:00 - occur that it is a a null hypothesis
337:04 - okay
337:05 - I hope that you are understanding what
337:07 - I'm trying to say if you are not then
337:09 - please watch the lectures once again so
337:11 - that uh you are able to understand
337:14 - in much more easy way
337:18 - um so if if you want I can actually take
337:20 - an example I can actually take an
337:22 - example to help you understand there's a
337:24 - what is chance and all but let me know
337:26 - if you want to understand about that
337:28 - like how can we there's a support to
337:30 - statement how can we say
337:32 - how how can we say how can we say
337:36 - that why not greater why not greater uh
337:39 - currently you can understand this way
337:40 - that if the P value is smaller if the P
337:42 - value is smaller than 0.05 you reject
337:44 - the null hypology except the ordinary
337:45 - hypothesis okay
337:48 - if you if you wanna know please let me
337:50 - know about that as well because I have a
337:51 - very nice explanation but that is bit of
337:53 - out of the context of the course but
337:55 - it's all right I can actually understand
337:57 - like uh I can actually understand this
337:59 - this something which people face as well
338:10 - cool
338:12 - I hope that this gives a good sense now
338:13 - let's do a very nice example a numerical
338:16 - work example
338:17 - so that you understand it nicely and
338:21 - then uh I will be done with the t-test
338:23 - you'll be left with f-test by your own
338:26 - and we're left with the F test which you
338:28 - have to explore by your own I'm not
338:30 - going to talk about that so let's talk
338:32 - about uh so let's continue our previous
338:34 - work example our previously worked
338:36 - example was that your you have a square
338:38 - footage you have a square footage which
338:41 - is one thousand two hundred to one
338:43 - thousand two hundred one thousand two
338:45 - hundred and then you have 1500 1700
338:49 - 2100 2300 2600 and 3000 okay and then
338:56 - you have nothing but the Y where we have
338:58 - the price of the house you can
338:59 - understand some sort of price okay I'm
339:01 - not writing the data but I'm just making
339:03 - it like this okay so your equation will
339:06 - be beta 1 plus beta0 sorry beta0 sorry I
339:09 - think I did it wrong beta Y is equal to
339:12 - beta0 plus beta 1 x okay x x y so assume
339:16 - that your beta0 as you assume that your
339:18 - beta0 is nothing but 5 50 000 okay fifty
339:22 - thousand and your beta 1 is nothing but
339:24 - equals two hundred okay so we can so
339:27 - first so let's State our Nolan uh
339:30 - alternate so no less as H 0 is
339:33 - H should do is where all your beta
339:35 - values H1s the Alternatives they are not
339:37 - equals to zero that there is a
339:39 - relationship existence you calculate the
339:41 - T statistic you calculate the T
339:43 - statistic T statistic how how it is
339:46 - calculated estimated we under calculate
339:48 - for beta 1 okay okay because we're gonna
339:50 - take out the significance of this x one
339:52 - you calculate the beta 100 minus this is
339:54 - the estimated minus zero why because
339:56 - because the knowledge because minus the
339:58 - hypothesized value in this case it is
339:59 - equal to zero divided by divided by the
340:02 - standard error the standard error of
340:03 - that coefficient the standard error is
340:05 - also is a it's it's a nice formula for a
340:07 - calculating of a standard error but in
340:09 - real world you don't need to worry about
340:10 - the formula you can actually look on the
340:11 - internet to calculate the standard error
340:14 - formula so the standard error which
340:15 - which you're gonna get the standard
340:17 - error which which you're gonna get is
340:19 - nothing but the the whole calculation
340:21 - will yield the whole the whole
340:24 - calculation will yield to 100 divided by
340:26 - 10 which will not nothing but equal to
340:28 - 10 so your t t statistic is ten now you
340:31 - might be thinking here use this is the
340:32 - greater than this but in most of the
340:34 - case this will be greater right no once
340:36 - you calculate the T statistic now you
340:38 - take now you convert this into you're
340:39 - going to understand you convert this
340:41 - into a probabilistic value
340:43 - understanding this way I used to
340:44 - understand this whether you convert this
340:46 - into some sort of proper some sort of
340:48 - probability value so that we can compare
340:49 - with a threshold
340:51 - okay so the four calorie for calculating
340:54 - the threshold for practic for uh for
340:56 - calc for calculating the threshold of
340:58 - critical value for calendar in the
341:00 - threshold critical value so that we can
341:01 - compare with our significance so which
341:04 - is 0.0 so that it can compare because we
341:05 - cannot compare this we should compare
341:07 - properties values we calculate something
341:09 - else degrees of freedom degrees of
341:11 - freedom degrees of freedom is calculated
341:13 - by n minus 2 N minus 2 the number of a
341:17 - fluid number of rows which you have in a
341:19 - column the number of rows which you have
341:21 - in your data minus
341:23 - um which is number of parameters of your
341:25 - model in this case it is seven minus 2
341:27 - which is nothing but 5. now you may be
341:29 - thinking what is degrees of freedom
341:30 - again new question asked so degrees of
341:33 - freedom is is nothing so if you if you
341:35 - also review the notes I have written a
341:37 - very nice and detailed uh notes on what
341:40 - is degrees of freedom so that you also
341:42 - understand it so if you go down so
341:45 - degrees of freedom degrees of freedom is
341:47 - nothing so assume that your data set
341:49 - contains 10 observations and regression
341:52 - model has two parameters the intercept
341:53 - and the slope okay in this case as well
341:55 - it was intercept and the slope the
341:56 - degrees of freedom is equal to eight so
341:59 - what is degrees of freedom his use
342:01 - derives the number of independent ways a
342:04 - system can change number of independent
342:07 - ways the system can change this is used
342:09 - for calculating if you're not able to
342:11 - understand I I would say ignore it but
342:15 - eventually even a but eventually this
342:16 - has a statistical meaning where it says
342:19 - that number of a number of uh
342:21 - independent values or the independent
342:23 - values in a data set so basically beta 1
342:25 - and beta of course is dependent right
342:27 - number of independent values of your
342:28 - model the number of independent why we
342:30 - say that this number of independent we
342:32 - can search online about more about
342:33 - degrees of freedom but this number of
342:35 - independent value which uh so basically
342:36 - the two are dependent because it's slope
342:38 - and intercept which is dependent so we
342:39 - can subtract it from our whole data
342:41 - number of observations will eventually
342:44 - degrees of freedom
342:47 - degrees of freedom so once you calculate
342:50 - the degrees of freedom in this case is 5
342:52 - okay now now so now now your
342:56 - significance value is 0.05 degrees of
342:59 - freedom is uh 5 degrees of freedom is 5
343:05 - degrees of freedom is 5 okay so your
343:10 - significance is this degrees of freedom
343:12 - is this now let's go ahead now let's go
343:14 - ahead and talk about something known as
343:17 - uh
343:19 - now let's talk about something known as
343:21 - uh T table distribution so ACD
343:27 - open up the T table distribution
343:31 - to do that you'll need to be online oops
343:34 - I'm not online that's pretty bad
343:37 - so yeah so I'll put up the I'll I'll put
343:40 - up you you just you should open it
343:42 - online by your by your own I'm not
343:44 - online as of now so I suggest you should
343:46 - open by your own so that you understand
343:48 - it by yourself as well open that online
343:50 - and then just go to a table and then if
343:53 - you see and then if you see on x axis on
343:55 - the top of it yeah you have nothing but
343:58 - uh you have nothing but T values the the
344:01 - the
344:02 - probability which is you have you have
344:04 - to go to two tails in in regression you
344:06 - always use two tests not one test the
344:08 - difference is listed in the reading
344:09 - materials please see that so in two
344:11 - tails you see that 0.05 you see that
344:12 - there is a 0.05 and then if you see the
344:15 - fifth the degrees of freedom is five
344:16 - that is the 2.57 2.571 2.571 is your is
344:22 - your critical T value is
344:25 - 2.571 is a critical T value is a
344:28 - critical T value now you compare the Cal
344:31 - the T statistic T test statistic
344:33 - statistic with the critical value with
344:36 - the critical value you so it shows that
344:39 - it shows that if you your of course T is
344:42 - greater than 2.571 these gradient of
344:45 - course 2.571 so we resect the null
344:47 - hypothesis and because this means that
344:49 - there's a significant relationship
344:51 - between that so you you you might be
344:54 - saying that here you should just some
344:55 - some seconds ago you told that that it
344:57 - should be smaller than the critical T
344:59 - value so as not not about that but in
345:02 - this a t test of course we do in several
345:03 - examples but in t-test we simply do this
345:06 - is we simply reject the null hypothesis
345:08 - in the favor if it is if a test
345:09 - statistic if a significance a level is
345:13 - greater than the approximated uh
345:16 - critical value because of course it
345:17 - changes no it changes so as I said your
345:20 - T statistics should be greater than
345:22 - should be sorry uh should be greater if
345:24 - the if the significance is greater than
345:26 - your approximated value then we say that
345:28 - okay this is something to be predicted
345:30 - nicely so I hope that this gives a good
345:32 - sense uh if you want to know more about
345:34 - all of these things I would really
345:35 - suggest uh go forward and talk about
345:37 - have also given several examples above
345:40 - so if you go and see several examples of
345:42 - mine of t-test of t-test you can go over
345:46 - here and talk about like for every
345:48 - individual predictor variable for every
345:50 - individual predictor variable you should
345:52 - be able to view it nicely but my point
345:55 - was that you can easily use the
345:58 - materials to actually uh t-test to
346:00 - calculate t-test statistics and then
346:02 - approximate the critical value and then
346:03 - compare the and if the T Test with an
346:05 - approximately critical value then you go
346:07 - forward and accept that okay
346:10 - cool but uh but you know but you but you
346:12 - have to also make sure that uh your
346:14 - Trail but but it depends on several
346:16 - other problems from the test to test uh
346:18 - so that's that that was pretty much
346:20 - about uh these statistics statistic
346:23 - um now what we'll do now we'll now we
346:25 - are done with almost hypothesis testing
346:27 - now we'll go ahead and talk about
346:28 - assumptions of linear regression and
346:30 - then we'll wrap up our whole course with
346:33 - the one project hey everyone welcome to
346:35 - this one of the our last reflectures I'm
346:37 - super excited that we came along over
346:39 - here and I hope that you really
346:40 - understood each and every Integrity of
346:42 - everything which should which you should
346:44 - know uh we have talked a lot of things
346:46 - as of now and it's still not ended here
346:48 - let's say we still have a lot of things
346:50 - to explore we still have a lot of things
346:52 - to cover up and one of the things to
346:54 - cover up which is one of the most famous
346:55 - interviews kind of thing is assumptions
346:58 - of linear regression so basically what
347:00 - I'll do is I have tried to make it in a
347:02 - unique way or structural way okay so
347:05 - basically uh we have several sub topics
347:08 - to cover in this that's our sub topics
347:09 - are linearity then you have you know
347:12 - Independence then you have normality
347:14 - assumption and there are several other
347:15 - assumptions listed out here so what I'll
347:19 - try to do what I'll try to do I'll make
347:21 - sure that you understand each of the
347:23 - substation great detail and I'll make
347:25 - sure that you also do your substantial
347:27 - research by yourself okay
347:29 - cool so basically uh let's start off
347:32 - with the first assumption let's start
347:33 - with the first example these are the
347:34 - notes for you to you know review quickly
347:36 - but I prefer using my own hand right now
347:40 - so let me just open my new hand which is
347:42 - uh this one let's open a dotted paper uh
347:46 - once you open the dotted paper let's get
347:47 - started now so the first uh so the first
347:49 - assumption but but what exactly
347:50 - assumption is now you might be thinking
347:52 - hey are you showcase this is assumption
347:54 - but what exactly assumption is and why
347:55 - do we need to study about this so
347:57 - assumptions assumptions so what is the
347:59 - definition of assumption so tell me the
348:01 - definition of assumptions so first of
348:04 - all let's get it clear about what
348:05 - exactly the formal definition on
348:07 - dictionary says because anything which I
348:09 - learn about anything I just make sure
348:10 - that I know that definition of the topic
348:12 - so uh thing is the the what is
348:15 - assumptions the the thing that is
348:17 - accepted as true or as certain to happen
348:20 - without any proof a very nice uh
348:23 - definition so what this says when you
348:26 - apply a linear regression model this
348:28 - linear regression model assumes a
348:31 - several thing without any proof it
348:33 - assumes that your that your data should
348:35 - be linear it should follow linearity
348:37 - assumption almost sky or no constant
348:39 - variance assumption are there a lot of
348:41 - assumptions so you're so your assumes
348:43 - all of these things and assumes and
348:45 - assumes all of these things and then and
348:47 - then you can expect your model to work
348:48 - fine so if you're so if any of those
348:51 - assumptions fail like if any of the
348:53 - assumptions fails then you then you
348:54 - cannot explain the LR model will only
348:57 - perform best you'll only perform best if
349:00 - and only if these assumptions are met
349:01 - assumptions means certain things are met
349:04 - like for example for example uh when
349:07 - when we have something like let's let's
349:09 - take an example when you prove something
349:10 - or uh or whenever when you say now
349:13 - assume that this is something and then
349:15 - you put an argument so when assumption
349:16 - is that not true then how can you say
349:18 - that particular thing uh will work right
349:21 - and so uh assume that uh assume that we
349:25 - have some certain uh assumptions to be
349:27 - there and then we expect these
349:29 - assumptions to be true for our LR model
349:32 - to work perfectly fine okay so this is
349:36 - this is one of the definition of
349:37 - assumptions in a great detail now what
349:39 - I'll do now what I'll do I'll I'll talk
349:41 - about there are several so so as as I
349:43 - talk there are several assumptions so
349:45 - I'll talk about some of the most
349:46 - important assumptions for you to explore
349:48 - it out right away and then we can
349:50 - further get started on uh onwards okay
349:53 - so the first assumption which is which
349:54 - which I'm going to talk about is
349:56 - something known as linearity assumption
349:58 - linearity linearity assumption please
350:01 - ignore my handwriting I have to write it
350:03 - super quickly that's who that's why I'm
350:05 - writing it super super duper quickly so
350:07 - linearity assumption says that it what
350:09 - what does it say is that it says that
350:11 - the relationship between the dependent
350:14 - variable independent variable is linear
350:16 - okay so what does it says the the
350:19 - relationship between as this linear
350:21 - regression and so it expects the
350:23 - linearity Assumption where it says the
350:25 - dependent the the depend the
350:27 - relationship between the independent and
350:30 - the dependent variable are linear so
350:32 - what exactly linear is as I've already
350:34 - talked about it means that we can have
350:36 - the we can have this straight line which
350:38 - can represent or approximate our data so
350:41 - we can have a straight line that can a
350:42 - reasonably approximate our data okay so
350:46 - uh over here this is a very nice example
350:48 - of the linearity Assumption this data
350:51 - where we have only one paint put feature
350:52 - in the output variable you plot this y
350:54 - onto this and X onto this now over here
350:57 - it you it can be your data is linear why
350:59 - because it can be approximately on the
351:01 - straight line data but in this case this
351:03 - is not a linear over here you cannot
351:05 - make a straight line to say to data you
351:08 - it it forms something like this which is
351:10 - not linear okay it cannot be reasonably
351:12 - approximated as a straight line so
351:14 - that's what the linearity Assumption
351:16 - says okay so this is an important why
351:19 - this is important or by why this is
351:21 - important because accuracy of the linear
351:23 - equation model relies on the underlying
351:26 - relationship which is nothing but linear
351:28 - okay so if your model is not linear
351:30 - assume that in this example assume that
351:31 - in this example the so the errors will
351:33 - be super super high as and if the errors
351:36 - are super high which means that your
351:37 - model is not performing well and it can
351:39 - fit in any way you're not able to
351:40 - perform well you have to use extensions
351:42 - of linear education to work on this data
351:44 - but we'll talk about that later on but
351:46 - over here it will not definitely not
351:48 - work well pretty well okay so this is
351:50 - something which you should know about
351:51 - like this is an important assumption
351:52 - because it will lead to unaccurate
351:54 - predictions because of high error Okay
351:56 - cool so once we have those assumptions
351:58 - like once we have those assumptions so
352:01 - which which you can see if you plot the
352:02 - data on a graph the point the point
352:04 - should show that it should be reasonably
352:06 - represented in straight line and if does
352:08 - not form the straight line this means
352:09 - that they are not significantly
352:11 - appropriate or we should use as other
352:13 - significant model which is a probably
352:15 - normal regression or other regression to
352:18 - actually make it more appropriate Okay
352:20 - cool so this was on another example now
352:22 - as I told once we have these assumptions
352:25 - once you to okay this is the Assumption
352:27 - now how can you test how can you test if
352:30 - the if if that particular data set
352:33 - follows linearity assumption so no not
352:35 - linearity assumptions or not we have to
352:37 - test no so we have another sub so how
352:39 - can we test it over here what we did we
352:42 - just did a visual inspection we just did
352:44 - a visual inspection okay this is
352:45 - something which you can draw a straight
352:46 - line but what if you have a higher
352:48 - dimensional dimensional for data so
352:50 - there are several ways to test your
352:51 - model okay the first test which you uh
352:54 - which is uh which is very popular is
352:57 - nothing but visual inspection visual
353:00 - inspection so in that visual inspection
353:02 - in that visual inspection what you do
353:04 - what you do the first thing which you do
353:06 - is Scatter Plots the Scatter Plots the
353:09 - first way to test if that if your model
353:11 - for if your data follows linearity
353:13 - assumption or not is apply scatter plot
353:16 - so what exactly Scatter Plots does so
353:18 - what exactly Scatter Plots does what is
353:20 - it does please we simply take and add
353:23 - independent and or dependent variable we
353:25 - have to of course do it in a 2d plane of
353:27 - course so when you have the on y-axis
353:29 - you have the dependent variable x axis
353:31 - we have independent variables and you
353:33 - plotted so if the relationship shows the
353:34 - straight line or the or the linear data
353:36 - that that means it is volume but if it
353:37 - does but it does not then then it is not
353:40 - near assumption is not satisfied so
353:42 - visual inspection is one of the most
353:43 - common ways as well but over here the
353:45 - only downside though is if you have more
353:46 - number of variables large number of
353:47 - variables input features independent
353:49 - features here to plotted against every
353:50 - individual and then see if it works but
353:52 - but you know it's sometimes not good as
353:54 - well but eventually it helps you to get
353:55 - a better understanding but this is a
353:57 - Scatter Plots to uh eventually help okay
354:00 - so uh so this is this is one of them
354:03 - another one is which is more common like
354:06 - over here we have to plot for every
354:07 - individual independent feature X 2 and
354:09 - then Y and X3 and then Y and all we have
354:12 - to do for Scatter Plots but in but
354:14 - another example is let's say how can you
354:16 - test with residuals plots residual plots
354:19 - so residual plants plots is one of the
354:21 - most common ways to
354:23 - um to test those assumptions what you do
354:26 - you simply put you simply
354:28 - um
354:29 - you simply take the you simply take the
354:31 - residuals and then put it on a y-axis
354:34 - against the predicted or the uh
354:36 - predicted values or the independent
354:38 - variable so basically you take you on
354:40 - y-axis you have the residuals on by
354:42 - sorry on accesses we have the prediction
354:44 - you have the prediction okay on why
354:46 - because you have the residual so why uh
354:48 - on Accessory Y axis will prediction
354:49 - sorry other situles and put on the over
354:52 - here prediction so you might be thinking
354:54 - here how does it even make sense right
354:55 - so let's uh let me show you a very nice
354:58 - example of how what exactly residual
355:00 - plots looks like so that it makes
355:01 - perfect sense to you so I just going to
355:04 - uh take a very nice example maybe that
355:07 - will uh make much more sense uh so I'm
355:10 - just uh drawing out a very nice example
355:12 - so that I can actually send it from Mac
355:15 - to my iPad that really works you know uh
355:18 - sometimes it works and sometimes it is
355:20 - not so let me just open up a very nice
355:23 - so open up a safari where is my Safari
355:26 - yeah so yeah so how can I download load
355:29 - this cool so I send this on my laptop
355:33 - from laptop to over here now I should be
355:35 - able to show it to you all so this is
355:37 - the example of a residual plot this is
355:39 - an example of a residual plot and what
355:41 - does this plot says on on X on on x-axis
355:45 - you have the temperature and then y-axis
355:47 - you have the residuals now if you see
355:49 - that now if you see the when you when
355:51 - you plot the related values against the
355:53 - residuals against the residuals the odd
355:56 - or even attempt or even the independent
355:58 - variables against the residuals it
356:00 - should show what it should show it
356:02 - should show a very Cur a very random
356:06 - scattered point so over here you know
356:08 - you see they are very randomly scattered
356:10 - there's that that's not the residual
356:11 - plots there is the on y-axis you have
356:14 - residual and x axis you have that
356:15 - independent variable so this does not
356:17 - forms any sort of you know a pattern
356:19 - like this uh what what patterns I'm
356:22 - talking about like this or like this or
356:24 - like this it is not forming it is it is
356:27 - a very random one you cannot even fit
356:29 - any sort of models is very very random
356:31 - and they're around zero only they're
356:32 - around zero okay so so if if if if the
356:37 - linearity Assumption holds if the
356:38 - linearity Assumption satisfies the
356:40 - residuals are the residuals are
356:43 - scattered randomly are scattered
356:45 - randomly okay are scattered randomly and
356:48 - they're around zero they're around zero
356:50 - okay I hope that you you are unable to
356:52 - understand this pretty well uh this is
356:54 - this is the visual inspection this this
356:56 - is what you do for visual inspection
356:57 - another way is something known as uh is
357:00 - is something known as partial regression
357:03 - plot partial regression plot partial
357:07 - regression plot and this is something
357:09 - which you have to explore by your own
357:11 - this is something explored by your own
357:12 - I'm not going to teach teach about that
357:14 - but you have to explore by your own uh
357:16 - because this um because now you are at a
357:18 - stage where you have to explode by your
357:19 - own as well I cannot spoon video
357:21 - everything but this this is one of the
357:23 - way which which you should explode right
357:24 - now by yourself another way to test
357:27 - another way to test it is is nothing is
357:30 - nothing but something always statistical
357:33 - tests statistical tests so we have seen
357:36 - our our hypothesis testing in that
357:38 - hypothesis testing we have several tests
357:39 - which is t-test f-test you know now so
357:42 - there are several test for States
357:43 - testing the significance of our
357:45 - um coefficients or independent features
357:47 - so just like that you have a tests for
357:49 - testing the linearity assumptions for
357:52 - example one of the test is nothing but a
357:54 - rainbow test is nothing but rainbow test
357:56 - the rainbow test I'm not going to go
357:57 - into mathematical details of it because
357:59 - this is something uh you're not studying
358:00 - statistics in detail if you want to
358:02 - study statistics just go and search more
358:04 - if you're interested you know but I
358:05 - don't suggest you to go in so mathy
358:07 - details of it because nobody usually
358:08 - asks you just need to know okay there's
358:09 - something just available which we have
358:11 - to use for testing is linearity or maybe
358:14 - something known as har we call it test
358:16 - which is one of our one of the most like
358:17 - uh statistical tests which is often used
358:20 - for testing this other like how can we
358:23 - eventually uh talk about things how can
358:25 - we eventually test if if the model is
358:28 - linear or not okay uh so just you can
358:30 - you can explore the mathematical
358:31 - diseases how do we test exactly but it
358:34 - just follows the same thing which I've
358:35 - studied the test assesses we have a null
358:37 - hypothesis the null hypothesis states
358:39 - that the null hypothesis states that
358:40 - what is a test it states that the
358:42 - there's a the relationship between the
358:45 - relationship between your X and Y is
358:47 - linear so it it assumes that it already
358:50 - assumes now we have an alternative it
358:52 - says that it is not the relationship is
358:55 - not so so so what we do we conduct the
358:58 - test and once we have the critical uh
359:00 - the p-value if the P P value which we
359:03 - get from after conducting the rainbow or
359:04 - the these test if it's smaller than 0
359:07 - 0.05 which means which means that uh
359:11 - which means that you can reject the null
359:13 - hypothesis uh if if it is more than you
359:16 - can reject the null hypothe which means
359:17 - the resumptions is is not satisfied
359:19 - because you are saying that it is not
359:20 - linear okay so this is one of the this
359:22 - is the the same same concept holds for
359:25 - every hypothesis testing another another
359:26 - example is another example so these are
359:29 - some of the some of the ways to
359:31 - summarize up linear regression
359:32 - assumptions holds for you know there
359:35 - should be linear significant
359:36 - relationship between flow two variables
359:38 - uh we have scatter Scatter Plots then we
359:41 - have residual plots and then we have
359:43 - partial regression plot and statistical
359:45 - plot and that we have uh hypothesis and
359:47 - tests uh sorry there are several
359:49 - hypothesis tests available to test with
359:51 - the darts whether it follows statistical
359:54 - or not okay I hope that this gives a
359:56 - good sense about our linearity
359:58 - assumption now what we'll do we'll talk
360:00 - about something known as another
360:01 - assumption which is nothing but called
360:02 - as uh the remedies of it yeah yeah this
360:06 - is this is one of the way to think about
360:08 - it I just forgot like we have some
360:10 - Revenue remedies about how can we fix so
360:13 - once our assumption so assume that your
360:14 - assumption is violated so assume that
360:18 - your assumptions is assumptions are
360:19 - violated assumptions are violated now if
360:22 - the if your if your model does not
360:24 - satisfy linearization which means that
360:25 - your model is not going to work well
360:27 - right so so we keep so
360:30 - you told about that this will not work
360:32 - well so what we should do if this does
360:34 - not work so I will say that fix that
360:36 - assumption make it the make make it from
360:38 - non-violated to non-violated assumption
360:40 - okay
360:42 - and this is what you have to do so there
360:44 - are several remedies for it you know
360:46 - remedies means something to fix if the
360:48 - if it is if it is being violated we have
360:50 - to fix that as well hashtag fix extract
360:54 - hashtag fix is also very famous thing so
360:56 - you have to fix that assumption so there
360:58 - are several remedies the first remedies
361:01 - is nothing but called transforming
361:03 - variables the first remedy is we call
361:06 - transforming of our variables
361:08 - transforming of our variables over here
361:10 - you can see that we transform our
361:11 - variables so um this is one of the
361:14 - Transformer variables so what does that
361:15 - mean transformation of our variables
361:17 - which means that if the relationship
361:18 - between the predictor and the response
361:20 - is appears to be you can try
361:21 - transforming the predictor or to see for
361:23 - example you can take the square root or
361:26 - log or or Square of a variable for
361:28 - example assume that you have a house
361:30 - price square and then you want to
361:31 - predict based on the so you know based
361:33 - on the size and predict the house price
361:34 - you have uh you have something to one
361:36 - thousand then you have 200 000 uh then
361:39 - 250 sorry 1 to 200 square feet and you
361:43 - have to 250 000 so use what you can do
361:45 - you can apply the
361:47 - log you can apply the log or the square
361:51 - root you can apply the log or a square
361:53 - root on you know on maybe on uh on on
361:56 - variables okay on on variables so that
361:59 - it comes on a it it becomes linear okay
362:02 - so you apply the transformation on the
362:04 - variables you apply the transformational
362:05 - variables used to study more about
362:07 - feature in genetics if you want to study
362:08 - more in degree level or all of these
362:09 - things I suggest you take the course
362:11 - which is cs01 bo4 which is one of our
362:14 - famous sports available online
362:16 - um so you can actually uh talk about
362:18 - this so we get we we take this log
362:20 - square root or Square of the predictor
362:22 - variable so independent variables out
362:23 - there so that it works perfectly so that
362:25 - you bring that bring them back into the
362:27 - linear way so when I take the log it of
362:29 - course comes in linear uh so this is a
362:33 - very nice uh note to make sure to so
362:36 - that's one of the transformation of
362:37 - Remedy uh variables another one is ADD
362:41 - higher order terms had higher order
362:44 - terms or interaction terms which means
362:46 - that if you have your X variable then
362:49 - you make another variable X but you
362:50 - square that but you squared that okay so
362:53 - you add the so you add the cubic per
362:55 - Cube cubic terms or quadratic to help
362:57 - capture the even the non-linear
362:59 - relationship So eventually this
363:01 - introduces a concept of polynomial
363:03 - regression in this Century entries
363:04 - concept of polynomial regression but
363:07 - that's something for later on where you
363:08 - introduce the higher order of you know
363:10 - uh the variables where you just take the
363:12 - X and then take the x squared and then
363:13 - you have x 2 x 1 take the X1 squared so
363:16 - you just introduce the higher order uh
363:18 - terms for example quadratic or cubic
363:21 - which helps to capture the non-linear
363:23 - relationship another one is like if
363:25 - nothing is working then you should start
363:26 - going on to take out the different model
363:29 - okay
363:30 - different model means you can consider
363:32 - using Gams or decision trees models like
363:35 - random Forest which can better capture
363:37 - if you're if if you data is non-linear
363:39 - too much in order to satisfy the
363:41 - Assumption then I think you should go to
363:43 - random forest or somewhere like that to
363:45 - actually uh cover up our assumptions of
363:47 - uh random Forest you know okay cool so I
363:50 - hope that gives you a very nice
363:52 - understanding about a linearity
363:54 - assumption what the next switch we'll
363:56 - talk about is something known as uh
363:58 - Independence assumption which is also a
364:00 - very nice assumption and then after that
364:01 - we'll talk about Homo scarcity and
364:03 - normality assumption and then we'll wrap
364:06 - up the lecture on this
364:08 - what's up
364:10 - hey everyone welcome to this video in
364:13 - this video what exactly I'm going to do
364:14 - is going to talk about independence
364:15 - assumption this is one of the again the
364:17 - second most famous assumption available
364:19 - right now which is Independence which is
364:23 - independence
364:25 - assumption which is again I said the
364:27 - second most Independence assumption
364:29 - which is like important so the what does
364:31 - independence assumption States
364:32 - independence assumption states that what
364:35 - is it states it states that that the
364:38 - errors or the residuals which you have
364:40 - the residuals which you have the
364:43 - residuals which you have there is the
364:46 - the residuals which you have are
364:49 - independent of each other okay so for
364:51 - example you have X1 and then you have y
364:54 - and then you have a b c d and you have y
364:58 - one y two y three y four and then you
365:01 - have y hat which you've taken from which
365:03 - is y hat one y hat two y so your error
365:06 - will be will be Error 1 is equals to Y
365:08 - hat one minus y one error 2 is equals to
365:10 - Y it had two minus so so the error
365:12 - number one should be independent of area
365:14 - number two or error number two two
365:16 - should be independent of error number
365:18 - one okay so errors or residuals
365:20 - associated with the in this the data or
365:23 - the observation should be should be
365:25 - uninfluenced by any other observation
365:27 - out there so error number two should be
365:29 - uninfluenced by any other so error
365:30 - number two should not get affected by
365:32 - the error number one okay or error
365:33 - number two should not get affected by an
365:35 - animal else so I guess the the the
365:37 - individual assumption states that the
365:39 - Assumption switches holds that that your
365:42 - observations associated with one
365:43 - observations should not influence the
365:45 - error associated with the another
365:47 - observation
365:48 - okay if you don't fix this you will end
365:51 - up with biased results uh unaccurate
365:54 - estimates unreliable predictions and a
365:57 - lot of things can happen uh so this is
365:59 - the this is what the assumptions say
366:00 - that you're uh that your error should be
366:02 - independent so let's take a very nice
366:04 - example so example states that that you
366:07 - have you have something known as
366:08 - electricity so you want to predict the
366:09 - electricity uh based on the uh based on
366:12 - the temperature in a city based on the
366:14 - hour okay so we collect the hourly data
366:16 - for entire month so you have hourly data
366:21 - which is and then you have temperature
366:23 - okay and then you have your electricity
366:26 - demand temperature electricity and
366:28 - demand so one two three dot dot dot 15
366:33 - 16 17 dot dot dot and 80 85 and 90 dot
366:39 - dot so this is this is one of our you
366:41 - know uh this is one of our
366:44 - um this is one of our data which we have
366:47 - in our annual test whether this
366:48 - assumption has a wholesale Independence
366:50 - assumption or not so if you see that the
366:52 - demand for electricity at one hour at
366:56 - one hour at one hour might be related to
366:59 - the demand of the previous one right so
367:01 - so basically so basically what what is
367:04 - it trying to say you it's it's something
367:05 - which is trying to tell you that assume
367:07 - that the electricity demand at on up at
367:11 - our two might be related to the previous
367:13 - hour right might be related to previous
367:15 - hour electricity at three might be
367:18 - related to the previous hour hour number
367:20 - three multiple to the previous hour so
367:22 - you see that there is some sort of
367:23 - relation and as we said that there
367:25 - should be not any relation of course if
367:27 - this is written then of course errors
367:28 - will be also related okay so electricity
367:31 - uses strengths to show a pattern so when
367:33 - you when as we are using electricity it
367:36 - is being so that we have it is greater
367:38 - than the previous one that's what it is
367:39 - showing okay it is as the hours
367:41 - increases your electricity demands also
367:42 - in which it is showing such some sort of
367:44 - pack pattern over the course of a date
367:47 - so it is of course showing the pattern
367:49 - over the course of the day because it is
367:51 - related to each other so what is it
367:53 - showing for example over here higher the
367:55 - demand is higher in the day and lower
367:57 - demand at night or lower demand at night
368:00 - so this is something which is which
368:01 - which can be showcasing the patterns so
368:04 - over here this is showcasing the trends
368:06 - which is not good right this means that
368:08 - our that our points are independent
368:11 - violating the independence assumption
368:12 - and that's one of our important major
368:14 - issue so that's this this is what
368:16 - assumption indicates how can we test our
368:18 - assumptions for testing our assumptions
368:20 - for test testing and assumptions we have
368:22 - couple of things again we can talk about
368:24 - residual plots
368:26 - and residual
368:27 - plots so again you you can take your
368:30 - residuals and plot it and plot it or
368:33 - like like this you take the residuals
368:35 - and plot in on a y-axis and you take the
368:36 - fitted value the prediction value
368:38 - product on x-axis right and when you
368:40 - plot it and when you plot it you will
368:42 - see you will see that you will see that
368:44 - uh you you'll see that uh
368:47 - you will see that there is no patterns
368:49 - or trends of the residual so there is no
368:51 - patterns there is this there's no such
368:53 - certain patterns uh of over here this
368:55 - over the line they are distributed
368:57 - randomly or scatteredly if there's some
368:59 - sort of patterns like this you know like
369:01 - this so we can say that that's that
369:04 - something does not hold true but over
369:05 - here this does not have pattern but but
369:08 - in the case of this it will hold a
369:09 - particular pattern when you when in this
369:11 - particular example to hold a pattern
369:12 - okay uh so you this is a visual
369:15 - inspection uh from which you can see the
369:17 - another one is nothing but call
369:18 - statistical tests statistical tests and
369:23 - statistical tests what does it say is
369:25 - that you have couple of tests which is
369:27 - Durbin Watson test the urban Watson test
369:30 - which will give you some sort of number
369:31 - and then we say that okay that that
369:33 - number is greater than the significance
369:34 - to 0.05 which is a P value then we say
369:37 - okay we can reject the null hypothesis
369:38 - and say okay this something is correlate
369:40 - so we'll talk these are tests which have
369:42 - already seen the way we conduct tests
369:43 - and all in previous lectures but Darwin
369:45 - Watson test and uh versus got Freight
369:48 - rest God Freight test so let's talk
369:51 - about a bit about uh Durbin but your
369:54 - your stance should be that you are
369:56 - making it researching a bit more about
369:58 - it okay
370:00 - and so how can we so that so there is
370:02 - one of them is
370:04 - um is what we can do so what exactly the
370:07 - Durbin Watson test means the Durbin
370:09 - Watson uh is specifically was introduced
370:12 - what Hawaii why it was introduced it was
370:14 - introduced to detect the auto
370:16 - correlation Auto correlation Auto
370:19 - correlation it it was it was came on to
370:22 - reduce the auto correlations in the
370:24 - residuals in the residuals in the
370:27 - residuals of Allah so what is means
370:29 - autocorrelation the residual so which
370:31 - which directly means what is auto
370:33 - correlation first of all autocorrelation
370:34 - nothing but the correlation between the
370:36 - residences correlation means is there
370:38 - any relationship between both of them so
370:39 - if they're highly highly correlated
370:40 - which means what they are indeed they're
370:42 - dependent on each other so so basically
370:44 - it tells you the correlation between two
370:46 - and the two residuals at different time
370:48 - points so basically it's it it means
370:51 - that it checks with the errors are
370:52 - correlated across observations or not so
370:55 - if they are that means they don't they
370:57 - don't they don't satisfy this that the
370:59 - independence assumption and if they are
371:01 - not correlated then then they satisfy
371:03 - the Assumption okay so dermin Watson the
371:06 - Durbin Watson test which you have it
371:08 - lies between zero to four zero to four
371:10 - and if you turbine Watson gives test of
371:13 - a 2 value 2 which means that there is no
371:15 - autocorrelation and if there is no
371:17 - autocorrelation between residuals that
371:19 - means the Assumption satisfies because
371:20 - there is there is no barriers correlated
371:22 - with each other okay so but if the
371:24 - values are less than if the values are
371:27 - less than two that means there is a
371:28 - positive correlations which means
371:30 - there's a positive quarter we have
371:31 - already talked about positive and
371:32 - negative correlation please see the
371:33 - lectures of probability if you haven't
371:35 - bought the course please now go and just
371:36 - enroll in a course if you haven't
371:38 - studied about problem statistics so
371:40 - value less than two suggests that the
371:42 - positive correlations are nothing but
371:44 - are nothing but um the positive
371:48 - correlations which you have is nothing
371:50 - but uh a value less than two if your
371:52 - turbine was to test statistics give less
371:54 - than two that means they have a they
371:55 - hold a positive correlation but uh which
371:58 - means the residuals are positively
372:01 - correlated across observations but your
372:03 - value is greater than two suggest
372:04 - negative correlation which means that
372:07 - residuals are negatively correlated so
372:09 - your value should be two if they want to
372:11 - satisfy this assumption if they're less
372:12 - than two that means they are positively
372:14 - correlated if they're greater than two
372:15 - then they are negatively correlated
372:17 - across the observations okay
372:21 - um yeah so uh how can we perform some of
372:23 - the steps so what you do you first of
372:25 - all fit your data and then you calculate
372:27 - the residuals you calculate the
372:29 - residuals
372:31 - you calculate the residuals after
372:34 - calculating the residuals what you do
372:35 - you take out D in in T Test you're
372:38 - taking out the T value over here you can
372:39 - take out the test statistic which is D
372:41 - um you you can search online for the
372:43 - formula but formula is not even required
372:44 - you know if there are Auto automatically
372:46 - tools which does for this now once we
372:48 - have this D now you what you do you
372:50 - compare the test statistic to the script
372:52 - critical values for the Durbin WhatsApp
372:55 - the the the table which we have okay the
372:58 - tables are statistical software come you
373:00 - compare it and if your D is less than
373:03 - that lower critical value or greater
373:05 - than that then we resent the null
373:06 - hypothesis of no autocorrelation which
373:08 - means assumptions is likely violated for
373:12 - example what does it says what does it
373:14 - say that you conduct the test and then
373:15 - if and then we again our null hypothesis
373:17 - and Alternate hypothesis comes into play
373:19 - and if it's greater than that and then
373:20 - we reject it and then so this resection
373:22 - automation will eventually happen Okay
373:25 - so but uh but mostly what you do if you
373:28 - could be statistic gives two that means
373:30 - there is no autocorrelation but however
373:31 - you can perform test as well for
373:33 - detecting autocorrelation but one of the
373:35 - few more drawbacks of it the Durbin
373:37 - Watson test is is more for you know what
373:39 - it's more for time series data it's more
373:42 - for time series data okay so that's why
373:46 - uh we have to use a you have to use
373:48 - proper you know tests for all of this
373:50 - thing I hope that you understood about
373:52 - how can we test our independence
373:54 - resentment so there are several remedies
373:55 - for it so there are several remedies if
373:57 - the assumptions are violated how can we
374:00 - fix it so one of the way to fix it is
374:03 - add additional variables so you can
374:05 - actually add additional variables you
374:07 - can actually add the first one is add
374:09 - additional vars add variables so for
374:12 - example if there's any underlying factor
374:14 - which is causing for for example in this
374:15 - case which you have taken example that
374:17 - your this this this has a this boil it's
374:20 - independent y because it might be
374:21 - related to previous one
374:23 - so we can find a factor which is causing
374:25 - we can try to find a factor which is
374:27 - causing that dependence and then we can
374:29 - you know add this Factor as an
374:30 - independent variable okay to uh we can
374:33 - add this Factor so you're adding more
374:34 - variables definitely helps definitely
374:37 - helps for for example in this example
374:39 - what we can do we can introduce another
374:41 - variable we can enter introduce another
374:43 - variable we can introduce another
374:45 - variable what variable time of the day
374:47 - time of the day in this case the time of
374:49 - time time of the race we will introduce
374:51 - another variable so time of the week
374:53 - which is night night morning so how does
374:56 - it help how does it help this helps how
374:59 - does it help if if we convert we can
375:02 - actually take it that we can actually
375:03 - understand the pattern your model could
375:06 - understand the pattern okay this is
375:07 - something is
375:08 - um uh daily practice you want to be able
375:10 - to capture the daily patterns that's
375:12 - that that was the factor so you can
375:13 - consider adding more variables according
375:15 - to what what is causing this this Factor
375:17 - so basically at night time so we can
375:19 - actually explicitly add this to actually
375:21 - help you so that your model does not be
375:22 - depend so this is one of them you can
375:25 - add more variables uh we'll do one
375:27 - proper example as well don't don't worry
375:29 - you can use time series model
375:31 - for time series model drill relatively
375:34 - works well in these type of examples and
375:37 - then at last you have nothing but called
375:39 - mixed effect models but that's something
375:41 - it's like that's something you can use
375:43 - hierarchical
375:45 - hierarchical linear models but I
375:47 - suggested to export by that alone it's
375:48 - not too much use but it's exactly that's
375:50 - what people actually use sometimes if if
375:52 - you have hierarchical data but I suggest
375:54 - you to learn about hierarchical linear
375:57 - models HLN a very nice uh concept around
376:01 - it okay so adding more variables you
376:04 - know uh time series using another
376:05 - algorithm all of this really helps
376:08 - good so I hope that you understood about
376:10 - independence assumption now we'll talk
376:12 - about uh homo schedule assumption uh
376:14 - which is also known as non-uh constant
376:16 - constant variance constant variance
376:19 - across it
376:20 - so I'll talk about that uh where where
376:23 - your variance of all the residuals are
376:26 - constant across all the levels so we'll
376:27 - talk about that homo this no constant
376:30 - very uh constant various assumption uh
376:32 - and then we'll take one example and
376:34 - after that we'll have our simple last
376:36 - one which is normality and then we have
376:38 - no multiple linearity which is left upon
376:40 - you for you to explore so let's get
376:42 - started with
376:43 - um what homo scarcity assumption or even
376:46 - say no constant variance assumption no
376:49 - okay cool
376:50 - so what does this homo scarcity
376:52 - assumption states that I I really have a
376:54 - hard time understanding you know
376:56 - pronouncing this so please ignore if I
376:58 - sometimes pronounce it wrong so most
377:00 - characteristic assumption is a
377:02 - statistical assumption that states that
377:03 - the variance of the eritream also with
377:05 - this spread or very what does it means
377:07 - the spread of the values variance means
377:09 - spread of the values okay uh how how
377:11 - does it spread across the mean so the
377:13 - mean of that how does the variance is uh
377:16 - in a regression model is constant the
377:18 - residuals or the variants are constant
377:20 - okay they're constant across all the all
377:22 - the predictor y variables you may be
377:24 - thinking yeah use what the
377:27 - okay so what what what just you told
377:30 - okay so uh you have uh you you you might
377:33 - be confused how what exactly does it
377:35 - mean so let's talk about what exactly
377:37 - does it mean so you can assume that it
377:40 - is a it states that there are variance I
377:43 - assume that you know variance the spread
377:44 - of values of the researchables is
377:46 - constant across all levels they do not
377:48 - change or you can say this spread of
377:51 - irresiduals should be approximately same
377:54 - for all values of the independent so you
377:57 - have
378:05 - so they should be constant across all
378:08 - independent values okay this should be
378:11 - constant if they are not if the variance
378:13 - of the residuals increases or decreases
378:15 - transmitter Dr domestically for example
378:18 - at at Road row number one error is
378:20 - something C and over row number two
378:23 - error is C plus hundred that that means
378:25 - that is that increases and at around row
378:28 - number three C minus 100 that means it
378:30 - decreases you know so it increases this
378:32 - decrease dramatically which means that
378:33 - it is not following that assumption that
378:35 - is not constant the variance is not of
378:37 - constant across the individuals of your
378:39 - samples then we say that it is not
378:40 - following our assumption that voltage
378:42 - the Assumption okay so uh I hope that
378:46 - really helps about that again over here
378:48 - for testing it we have nothing but
378:50 - called residual plots we have nothing
378:52 - but called residual plots which really
378:54 - helps to test the homoscadicity
378:56 - Assumption so what is uh so let me let
378:59 - me just uh again if there's too many
379:00 - patterns then you can conclude that
379:02 - there's no such assumption so just going
379:04 - to uh make you aware about like a very
379:07 - nice example maybe for you which will be
379:09 - super helpful if you understand it a
379:11 - nice way so just going to have it over
379:13 - here the PNG file uh uploaded right here
379:17 - share done
379:20 - and then I'm going to share again
379:21 - airdrop
379:23 - and iPad
379:25 - oops Yeah so this is this is one of the
379:29 - nice image which I usually use to
379:32 - explain it so this is this is called a
379:34 - residual plots we have a residual plots
379:35 - we plot and the y-axis residuals on
379:37 - x-axis we have some sort of uh fitted
379:40 - values you have to put if we have fitted
379:43 - values or the predicted values or the
379:45 - model pretty values on on y-axis you
379:46 - have the uh on y-axis you have what on
379:49 - y-axis you have nothing but uh residuals
379:51 - or Nexus we have fitted values and when
379:53 - you plot it if you it shows some sort of
379:55 - patterns over here you see that it is
379:58 - showing some sort of patterns over here
379:59 - it is showing some sort of patterns but
380:01 - over here it is not showing any sort of
380:03 - patterns right so in this example if if
380:05 - your model shows something like this
380:07 - that means the assumption is satisfied
380:09 - you can go ahead with it but if this
380:10 - like if there's some sort of showing
380:12 - patterns that means that assumptions
380:13 - does not hold this is this is example of
380:15 - a heterostasis heterostaticity and this
380:18 - is also example heteroscadicity but this
380:20 - is an example of homoscadicity which
380:22 - means that it was a homoseconditional
380:24 - assumption is true which is consider
380:26 - your variance your errors are constant
380:27 - across the uh
380:29 - across the independent variables okay
380:32 - and there's no Trends or there's no
380:34 - Trends over here it is increasing over
380:36 - here it is decreasing now where there is
380:37 - it is constant across all variables okay
380:41 - uh this is what this is a visual
380:43 - inspection another one is nothing but
380:45 - called uh we have several you know uh
380:48 - statistical tests just like as pre
380:50 - previous one which is called the Pagan
380:52 - test Pagan test
380:55 - or a white test white test and all of
380:58 - this you don't really need to you know
381:00 - understand what exactly uh it does but
381:02 - again the hypothesis testing formula
381:03 - looks in you have the and null
381:06 - hypothesis alternate and then you
381:07 - perform the test and then checking the
381:08 - significance value and then you say okay
381:10 - there's something greater than then we
381:11 - reject or accept statistical test for
381:13 - all the statistical tests which you have
381:15 - to understand by your own by searching
381:16 - online I'm not going to do with it
381:17 - because it is out of the scope of the
381:19 - course as well it is not required as
381:20 - well for for initial steps at least
381:24 - so and once you once you assume that
381:26 - your your model does not satisfy so what
381:28 - are the remedies how can we fix it so
381:30 - fixing can be happen in two or three
381:32 - ways two or three ways okay so uh two or
381:36 - three ways so let's talk about what are
381:37 - those two or three ways the first way is
381:40 - we can transform our variables transform
381:44 - transformation can happen transformation
381:46 - can happen so you can add you know you
381:48 - can apply transformation or dependent on
381:49 - an independent like a log transformation
381:51 - or square root transformation or inverse
381:54 - transformation and then see if your
381:55 - assumption is being fixed or not another
381:57 - one what you can do which is weighted
382:00 - linear regression weighted linear
382:02 - regression what is weighted linear
382:04 - regression says that instead of using
382:05 - ordinary which you use right now you can
382:07 - use weighted which what it does it
382:09 - assigns weights towards you have a nine
382:11 - observation so it assigns the weights to
382:13 - every observation okay based on their
382:15 - variance based on the variance but
382:17 - weighted linear regression is something
382:18 - which is which which you should learn in
382:20 - the extensions of linear regression
382:22 - um which is upon you to understand it
382:23 - much more graduated I'm just doing it
382:25 - because I want you to and make explore
382:27 - by yourself so not empty and giving you
382:29 - that nobody gives all of these names but
382:31 - I'm giving you to explore by your own
382:32 - and the another one which is we should
382:34 - explore which is uh more sort of you
382:36 - know uh used to remedy is something
382:38 - known as hover regression or quantile
382:41 - regression quantile
382:43 - regression content regression or hover
382:46 - regression and these two are used you
382:49 - know for uh techniques for you know what
382:52 - it is used for uh checking whether
382:55 - whether uh and and this this really
382:57 - really helps in fixing those quantile
382:59 - regression and hover so I suggest you to
383:01 - explore these three types of extensions
383:03 - of linear regression which helps to
383:05 - satisfy this assumption so I really want
383:07 - you to go ahead and then stop this
383:09 - lecture and then go over and search
383:10 - about this learn about this make a note
383:12 - and tag me on LinkedIn
383:15 - Okay cool so I hope that you're done
383:17 - with this homo scarcity assumption uh
383:19 - now at last we'll talk about nothing but
383:21 - call normality assumption which is our
383:24 - most you know as well as it's not you
383:26 - know what most it's something which is
383:27 - required for everyone you know cool so
383:31 - let's get started with normality
383:32 - assumption so what is it normality
383:34 - assumption states that normal resumption
383:37 - states that normality assumption states
383:39 - that that the residuals now previously
383:42 - in Independence and the constant and
383:44 - homoscope is the constant you know
383:46 - Independence independent of each other
383:47 - uh constant and now we talk about they
383:49 - should be normally distributed you know
383:51 - that word is normally distributed you
383:53 - know what is said to be write me in the
383:54 - comment box what is it said to be a
383:56 - normally distributed a very nice uh so
383:58 - I've just wait for five seconds what
384:00 - does it means
384:02 - so tell me what is it means your
384:05 - residuals to be normally to be normally
384:08 - distributed
384:12 - so let me let me just write the question
384:13 - so that at least I can check for it and
384:16 - you and in the comment box uh yeah so
384:19 - please go ahead and invite it
384:21 - um so that I could understand
384:25 - foreign
384:32 - so what does it say to be a normally
384:35 - distributed which means which means that
384:38 - your mean is that that that your
384:40 - distribution is following a normal
384:43 - gaussian distribution okay your mean is
384:46 - following a normal gaussian distribution
384:48 - that your residuals are following the
384:51 - normal when you plot the residuals they
384:52 - should follow something like this where
384:55 - most of the values should lie between
384:57 - this range okay should right should uh
385:01 - this should be symmetrical this should
385:03 - be symmetrical most of the value should
385:04 - be around the mean your mean is
385:06 - something that nothing mean is equals to
385:07 - zero and then you have the bell shaped
385:09 - curve which is symmetric which is which
385:11 - should be which where the mean should be
385:14 - close to zero in the case of the
385:15 - residuals in the case of residuals your
385:17 - mean should be close to zero in the case
385:19 - because residuals are of course zero
385:21 - right because we don't want to be
385:21 - residuals to be high so it should
385:23 - satisfy our normality assumption it
385:25 - should satisfy our normality assumption
385:29 - I hope that so basically uh normative
385:31 - assumption states that and what is
385:33 - normality assumption say it's that that
385:35 - your normality assumptions refers to
385:36 - assumptions that the errors are normally
385:38 - distributed in other words that your uh
385:40 - it should be well shaped or belt shaped
385:42 - curved with a symmetrical positions with
385:45 - most of the errors clustered around the
385:47 - mean and fewer errors so most of the
385:48 - errors so even even plotted so most of
385:50 - the errors should be over here so it
385:52 - should be constant so you see that it is
385:54 - related to the previous one this
385:55 - constant show here most of the error
385:57 - should be constant it should be
385:57 - independent and there should be around
385:59 - they should be similar it should not be
386:01 - this it should be very one one or two
386:04 - error should be at extreme like very
386:05 - different but it should be most most of
386:07 - them should should be uh between like it
386:10 - should be around the mean okay
386:12 - uh over over here you can see that if
386:15 - you violate this it can affect the
386:16 - validity and the conclusions your model
386:18 - is going to make and it can also often
386:20 - result in bias and inflated standard
386:22 - deviation errors so again which can
386:24 - generally affects the accuracy
386:27 - so there are several ways to test the
386:28 - and the easiest way is to plot a
386:31 - distilled plot is a normal plot which is
386:33 - nothing but a histogram or something
386:35 - like you know histogram with a belt
386:37 - shape curve uh which which you usually
386:39 - do so let me just introduce you with uh
386:43 - a very nice image for this as well maybe
386:45 - for you which will help you we have two
386:47 - two types of plot which we can use the
386:49 - first one is quantile plots the second
386:51 - one is uh histogram plot so assume that
386:54 - I just want to take a very nice example
386:56 - in which example I should take I don't
386:58 - even get it
387:00 - um so I just gonna use this
387:03 - so assume that assume that you have
387:07 - this particular example
387:12 - where you plot the residuals on a y-axis
387:15 - where you plot the residuals on a y axis
387:17 - and then see okay your eventually your
387:20 - your your bell shape will shut form like
387:23 - this it should it should form like a
387:24 - gaussian distribution that's one of them
387:26 - when you plot the where you plot the
387:28 - histogram where you plot the histogram
387:29 - and then what you do when the plot the
387:31 - histogram with the Bell shape curve and
387:33 - then and then you apply the distal plot
387:34 - on top of it so that it looks like a a
387:37 - density plot you know the density plot
387:38 - and it should look like a gaussian
387:40 - distribution or you can what you can do
387:42 - you can create a quantile quantile plots
387:44 - what does quantile content plot means it
387:46 - means that what is content quantile
387:48 - means that it means that if the
387:49 - residuals are normally distributed
387:51 - should fall approximately along a
387:53 - straight line now this may be a bit of
387:55 - you know a bit off to you so let's just
387:57 - go ahead and talk about it so what I'm
388:01 - going to do is have it like over here
388:04 - quantile plots I'm just going to send it
388:06 - out to you over here
388:10 - you usually you know what I do I usually
388:12 - teach along with it I I just don't like
388:15 - you know to teach it like uh like like
388:17 - like just having a preparation because I
388:20 - feel like having a preparation really
388:21 - affects you know uh the way we teach
388:24 - so of course I have a preparation but
388:27 - not too much
388:28 - because I like to be very natural so
388:30 - over here you can see that you have a QQ
388:32 - plot and UV plot if so what is QQ plot
388:35 - indicates the QP plot sees that that if
388:39 - your distribution are now if your errors
388:41 - if your residuals are normally
388:42 - distributed then the points on the the
388:44 - point it should lie approximately which
388:45 - is approximately over here this normal
388:48 - distribution their lies approximately on
388:50 - the straight line but over here this is
388:51 - not a normal distribution log normal
388:53 - distribution so your points are so
388:55 - points are in a diverse the points are
388:57 - not on the straight line over here this
388:58 - is also not on a straight line but over
389:00 - here this is on a straight line so if if
389:02 - you get the content plot like this then
389:04 - you say okay this satisfies our
389:05 - assumption and then you have several
389:07 - other statistical tests just like
389:10 - um previous one like which is over here
389:13 - we have sapiro will test or you can say
389:15 - Smith Smirnoff test where it is used but
389:18 - will what will perform the test one of
389:20 - the test in practical but you there's
389:22 - that something is according to you if
389:24 - you want to do it but I like to do it by
389:25 - normality assumption which is just my
389:27 - visual inspection
389:29 - what are the what are the remedies for
389:32 - it transformation is one of the most
389:34 - popular remedies removing the variables
389:36 - which is which is causing to be it in
389:38 - non-normal so you can remove the
389:39 - variables which is causing to be
389:40 - non-normal or you can talk about or you
389:43 - can use different other algorithms such
389:45 - as quantile regression as I told that
389:47 - you should be comfortable with that
389:49 - um and actually in the course you know
389:50 - the course which which we have CSO one
389:52 - we teach about all of these extension as
389:54 - well but as of now I feel like as we
389:56 - made it for fee we just wanted to
389:57 - explore by your own as well okay so this
390:01 - this is about normality assumption where
390:03 - we where we are ask our residuals to
390:04 - follow a normal distributed plot
390:07 - Okay cool so I hope that really makes
390:09 - sense uh now what I'll do I'll just wrap
390:12 - up so now we are we are done with almost
390:14 - all the assumptions now what is left is
390:16 - something known as no multi-collinearity
390:18 - assumption which is for you as a
390:20 - homework to uh understand it's very easy
390:22 - and very very tricky as well if you
390:24 - don't understand the right way so but
390:25 - that's that's something which you have
390:27 - to deal by yourself or talked about 10
390:29 - hours of lecture that's something which
390:30 - you have to do by your own as well right
390:32 - and we have a talk in very things in
390:33 - great detail and I really hope that you
390:35 - understand all of these things you have
390:36 - what we have came to an end actually so
390:39 - we are done with the assumptions and
390:40 - everything now what's next the next is
390:43 - nothing but we'll go and do a sample
390:45 - project one simple linear regression and
390:48 - one multiple linear question to Showcase
390:50 - you several other things as well so you
390:52 - don't need to worry about a lot of
390:53 - things let's get started with the final
390:55 - section of the course and then we'll say
390:57 - goodbye
391:01 - foreign
391:05 - welcome to the mostly the last section
391:08 - I've been telling in the Practical
391:10 - session so we'll so uh so we'll now
391:13 - start off with the practicality stuff
391:15 - get you comfortable with what we have
391:17 - learned and how to implement that as
391:19 - well okay so we'll we'll definitely do
391:22 - that but but prior to this you have a
391:25 - programming assignment for you so that
391:27 - you could you could Implement linear
391:29 - regression from very Scratch by your own
391:31 - I'm not going to do that what I'm going
391:32 - to do I'm going to utilize libraries
391:34 - teach you the core and Concepts and
391:36 - cruts which nobody else teaches that's
391:38 - my aim so uh we'll be we'll be doing a
391:41 - short a couple of lending regression
391:43 - project one of the one of them is using
391:45 - simple linear regression to give you a
391:47 - basic idea about how we go about
391:49 - building a very nice report how do how
391:52 - we go about interpreting the results of
391:54 - linear regression and Etc and as my as
391:58 - as my strategy suggests we'll start off
392:00 - with the Baseline and then we will
392:02 - slowly extend to the to the multiple
392:05 - variables as well right so so what so
392:09 - what we can do we can actually uh do
392:11 - that in terms of uh having more sort of
392:14 - you know adding more sort of uh projects
392:17 - in future if you want to go with the
392:19 - free one but as of now let's let's get
392:21 - started with the very simple example on
392:24 - how we can use linear regression for
392:27 - identifying predict advertising
392:29 - prediction but before that I'm pretty
392:31 - much sure that you might be confused
392:32 - about what exactly the project is and
392:34 - how we're going to go forward with it so
392:37 - basically let me show you what exactly
392:38 - the project I'm going to start over the
392:40 - problem statement and then after the
392:42 - problem statement I'm going to make you
392:43 - familiar with something known as uh
392:47 - with with some with something known as
392:52 - um like what a simple and linear
392:54 - regression and then we'll do a couple of
392:56 - stuffs like 40 for example data
392:58 - ingestion and then we'll talk about how
392:59 - we will do the model how do you test the
393:01 - assumptions and Etc so I hope that that
393:04 - really makes sense uh apart from it and
393:07 - uh and on the other other hand uh let's
393:10 - get started I eventually talking about
393:12 - first of all what exactly the data is so
393:15 - first of all we have the advertising
393:17 - data set as they have already seen a lot
393:19 - uh and you'll also seen the programming
393:21 - assignment which you'll be doing uh
393:23 - please see the course website where we
393:26 - have listed for the same so over here of
393:28 - over here you have three variables you
393:31 - have three variables which is TV a radio
393:34 - and newspaper and these are listed num
393:38 - listed listed it's listed
393:41 - um expenses which which a particular
393:43 - company has put on for example they have
393:45 - they have spent around 200 that 230.1 uh
393:49 - on uh on on expense on TV at advertising
393:54 - on TV 37.8 000 on Radio and 669.2 on
393:58 - newspaper advertising and based on
394:00 - Advertising they got 20 20 22.1 this
394:04 - much sales okay so so basically we had
394:07 - this is over here we have the expenses
394:11 - in which every company has did on TV
394:13 - radio and newspaper and we are going to
394:15 - predict uh what is now what is the
394:17 - significant number of a sales so now
394:19 - your task as a data scientist is to
394:22 - identify which which in which mode of
394:25 - advertising is much more liable for is
394:30 - which which a company should increase
394:32 - the expense to get the sales and which
394:35 - one
394:37 - um which which one source which should
394:39 - completely vanish it like we should
394:41 - completely ignore it for example in some
394:43 - cases okay you uh you you might say okay
394:45 - for this particular example you might
394:48 - want to go with TV because it is more
394:50 - significant in getting more sales as
394:52 - compared to newspaper or radio right so
394:55 - so your comp so so that's that's what
394:57 - your work should be as a data scientist
394:59 - it's helped to take help the
395:01 - stakeholders to take better decisions
395:03 - out there right so what decisions they
395:05 - they have to take they have to they have
395:07 - to take the decisions up about like
395:09 - whether to use TV whether to expand and
395:12 - spend more on TV radio or newspaper or
395:16 - couple of them so we have to identify we
395:18 - have to identify which variable
395:20 - independent variable is giving most of
395:22 - our most amount of contribution to sales
395:24 - which particular medium of source is
395:26 - giving most amount of informations to
395:27 - the sales and that's our problem
395:29 - statement so what I'll do
395:32 - for for being super simplistic as as I
395:35 - told it's not a full-fledged project
395:37 - it's just for the learning purposes not
395:40 - for the putting on a portfolio so
395:43 - basically uh over here what exactly
395:45 - gonna do I want to take this TV and I
395:49 - want to take one of the I'll take one of
395:51 - the advertising source and understand
395:54 - his impact understand its impact on the
395:58 - awkward variable sales for example I
396:00 - could take TV and then understand its
396:02 - impact on sales so I can build a linear
396:05 - regression on top of it we're given the
396:07 - TV the expense curve given on TV it will
396:10 - protect me the sales so that I can and
396:12 - if I open this much and then I can do
396:13 - for separate variables and then I can do
396:16 - for separate variables and then see
396:18 - which is which is having the most amount
396:20 - of contribution however this is not veto
396:22 - it we actually use the multiple linear
396:24 - regression where we'll use a three of
396:26 - them to actually predict the sales but
396:29 - that's for you to try out not for me as
396:31 - a programming assignment we have listed
396:33 - detailed programming assignment for you
396:35 - where we will do a small thing and then
396:37 - you have to extend it to further okay so
396:40 - over here or over here what we'll be
396:43 - doing we will be making the data into a
396:45 - simple linear equation which is a simple
396:46 - PDF where you take the only one column
396:49 - as of now so that you have X1 you have
396:51 - one a one into an independent feature
396:52 - and then one dependent feature and over
396:54 - here I'm going to RFI the significance
396:56 - of TV onto the sales the significance of
396:59 - TV or to the sales however you can also
397:02 - include other variables if you want to
397:04 - understand the significance but as of
397:05 - now let's let's get started with it uh
397:07 - you'll be also doing a very nice
397:08 - programming assignment where you'll be
397:10 - actually understanding answering that
397:12 - question of which one to go forward with
397:16 - which one of the medium source to go
397:17 - forward with so let's get started with
397:19 - actually talking about uh the how how
397:21 - can you code in a computer
397:28 - so uh everyone back to this lecture uh
397:32 - to this project so I'm going to start
397:33 - off with the data injection so the first
397:36 - step of any Machinery project is to
397:38 - ingest the data and I really like it to
397:40 - be in certain sort of class and
397:42 - everything so basically over here over
397:45 - here we have
397:47 - um we have some we have a class and we
397:50 - have a little data ingestion class which
397:51 - ingests the data advertising and then
397:53 - lows it down to only one to uh to a
397:57 - simple for to a data set where we can
397:59 - use Simple linear regression on where we
398:01 - have only one independent feature and
398:03 - one dependent feature so the first step
398:04 - is to initialize the class for the file
398:06 - path so we required a file path when
398:08 - someone instantiates this cloth class
398:10 - and then what and then what we do and
398:12 - then we have a method called load data
398:14 - which reads the CSV which reads the CSV
398:17 - from the specified file path and then we
398:19 - get the and then we get our specified as
398:21 - I said that it that we want the specific
398:23 - TB and sales only so that it is for
398:25 - learning purposes and then you
398:27 - concatenate both of the X and Y
398:29 - variables and then we return that data
398:31 - frame okay so this is a basic data
398:33 - ingestion class not a big deal to
398:35 - understand it okay now we can actually
398:37 - use this now we can actually use this
398:39 - over here
398:41 - over here which is uh where where we are
398:43 - importing so ignore everything just just
398:46 - go over here you can you can say from
398:49 - source which is the source folder dot
398:52 - data ingests with the file name import
398:53 - the data ingestion class and then you
398:55 - say okay this is inside instantiator
398:58 - class and then um and then Source it to
399:00 - the or the make it to the
399:02 - advertising.csv and that gives your
399:04 - simple DF that that that will give you
399:07 - the data frame which is a simple DF
399:09 - which we want to perform on okay now in
399:12 - data ingestion is done so let's run our
399:14 - file to actually see okay that will make
399:18 - much more sense and I'm going to use
399:19 - something like uh very nice over here so
399:24 - I'm just oops
399:28 - a little clear LS and then I'm just
399:31 - going to have this
399:33 - uh if I have actually an environment
399:36 - fiction so I'll just uh environment
399:39 - variable so what was the name LR Raj
399:42 - maybe I'm not sure yeah uh so Honda
399:45 - deactivate
399:47 - so that's clear okay so uh I have
399:50 - activated my environment if you don't
399:51 - know how to activate you will also
399:52 - seeing the reading materials for that or
399:54 - the other otherwise in the next set of
399:55 - projects I have showed you how to do
399:57 - this okay so I'm just going to remove
400:01 - this because it's not required yes over
400:04 - here now what we'll do over here now
400:06 - what we what we will do is we will
400:09 - simply uh run this python dot python
400:13 - main dot pi and I'll just I just want to
400:16 - run that uh just gonna make it comment
400:19 - and I I just want to take a look at what
400:21 - exactly the data frame looks like so
400:23 - it's going to print that data frame
400:24 - print that data frame it's better it may
400:28 - seems good to you so that you can see
400:29 - what exactly it's working on so so when
400:32 - you when you run it you'll be eventually
400:33 - getting a date a data frame where where
400:35 - you have only one column which is an X
400:37 - and Y over there okay so it eventually
400:40 - takes time initially and then it works
400:42 - fine so you have the TV which is that
400:44 - expenses on TV and then the sales which
400:46 - happen if you have advertised on TV so
400:50 - that's a data ingestion is done now the
400:52 - next step is I'm going to do a basic
400:53 - data processing please note that in the
400:56 - next project which will do we'll have
400:57 - the very extensive data processing thing
400:59 - but as of now I would suggest you can
401:01 - ignore this as if now okay uh maybe uh
401:04 - you which it it just for information
401:06 - purposes if you want to know how we are
401:09 - performing basically what we're doing we
401:11 - are we are importing a class known as
401:13 - data processing class and in that we are
401:15 - instant sharing uh instantiating with
401:17 - the data frame object sorry uh ends with
401:20 - which which takes an input which is data
401:22 - frame and then performs couple of
401:23 - methods the first method it applies it
401:25 - identifies if there's any outlier the
401:27 - box plot we box plot and see if there's
401:29 - any outlier in that particular TV's
401:31 - variable because linear regression is
401:33 - it's sensitive to outliers so we have
401:35 - the TV and then we just see if there's
401:37 - any outliers if there's any we identify
401:40 - uh who who are those outliers
401:43 - numerically okay and there are several
401:45 - ways to deal with it capping trimming
401:47 - and several ways to identify as well we
401:49 - should be seeing in the next lecture
401:50 - please ignore that as of now
401:53 - so when you're going to go to main.pi
401:55 - now uh now you can just import the same
401:58 - thing you can simply import the same
402:00 - thing which is the data processing from
402:01 - that data source folder in the data data
402:03 - preprocess file and then you simply run
402:06 - it so let's when when we run it it is
402:08 - okay let's have a fi if there's any
402:09 - outlier so it says that there's no one
402:11 - here apply so let's print it out let's
402:13 - let's print out books I haven't printed
402:15 - that so outliers so if I just print it
402:18 - out you will be able to see that there's
402:21 - zero outliers in that trigger column
402:23 - okay of uh using z-score but you will
402:26 - identify your method to actually uh NF
402:29 - which are outliers and I would recommend
402:30 - to if you don't know about outliers you
402:32 - should wait for the next lecture as of
402:34 - now you can ignore this this is not the
402:35 - scope of this uh process the scope of
402:38 - this process is focus on model building
402:40 - only so now once we have the bit of
402:43 - Crossing I just want to show you the
402:44 - process nothing else like okay after
402:46 - data ingestion we process the data in a
402:48 - good way and then what we do we go on
402:50 - how can we build a linear regression
402:52 - model so how can we build that so we'll
402:54 - be using stats model API because I
402:56 - really like that as computer as a
402:58 - scikit-learn because
403:00 - um I don't know why but yeah it it is
403:02 - more interpretable it gives more results
403:03 - it gives more explanations when we off
403:06 - after after building up the model okay
403:08 - so basically we'll build a class which
403:11 - takes in which takes an X and Y which is
403:13 - the independent and dependent variable
403:15 - and then we and and then and then what
403:17 - we do we add a constant which is one one
403:19 - terms you know in X is a design Matrix
403:21 - and then one one one one one uh a column
403:24 - of one so that we multiply with beta0
403:26 - and 1 which is SM dot add constant we
403:28 - have SM as an allies for this a for for
403:31 - this library and then what we do and
403:34 - then we make a method called fit and
403:36 - that fit what it does we are calling or
403:38 - we are calling orderly squares which is
403:40 - equivalent to linear regression and then
403:42 - we're calling dot fit and when we're
403:44 - calling dot fit by giving your
403:46 - independent variable and dependent sorry
403:48 - dependent variable independent variable
403:49 - where we were saying okay this is
403:51 - something as a dependent independent and
403:52 - this something is dependent so more of
403:55 - the relationship using linear regression
403:56 - first first what this fit fit will do it
403:59 - will perform all the steps First Step
404:00 - hypothesis second calculate the cost
404:03 - third is perform the gradient descent on
404:05 - top of it until unless your model is
404:07 - come first we have already talked about
404:08 - what exactly fit method does it fits the
404:10 - method for example we have already seen
404:12 - what the algorithm is take out the
404:13 - hypothesis take out the error take out
404:15 - the dead take out the derivative and
404:17 - then perform the gradient descent n
404:19 - number of times just after you updating
404:21 - and updating beta's value so that's what
404:23 - fit does okay fit means it trains your
404:26 - linear regression model by the number of
404:28 - iterations so when you actually go and
404:29 - see what it whatever exactly isn't that
404:31 - our OLS is you will be seeing all this
404:33 - stuff but but basically it trains your
404:35 - using gradient descent algorithm
404:38 - cool and then return the model and then
404:41 - we can print the summary of our model
404:43 - what exactly after training our model
404:44 - what exactly the summary of our model
404:46 - looks like what exactly the summary of
404:49 - our model looks like and if you if
404:51 - you're getting confused what this dot
404:52 - fit method does please take a look at
404:54 - online from for a scratch implementation
404:56 - of limitation if you're still not sure
404:57 - about how do we programming it up but uh
405:00 - if but you have to attempt a programming
405:01 - assignment for this for better
405:03 - understanding you print the summary of
405:05 - the line regression model which is model
405:06 - is equal to self.fit first of all you
405:08 - train the model and after you sell those
405:10 - that you train the model and then print
405:12 - the summary of the model and return the
405:13 - trained model so that anyone can use it
405:15 - for prediction so over here when you
405:17 - actually uh when you actually uh use
405:20 - that so let's let's have a basic simple
405:23 - integration model over here and let's
405:25 - print it out let's print print that
405:28 - stuff out this model summary so
405:31 - um basically we have we're just calling
405:32 - dot summary we are just calling dot
405:34 - summary and when you're calling dot
405:36 - summary it first of all trains the model
405:37 - prints out somebody and then return the
405:39 - model so how does that summary looks
405:41 - like which is the most important part of
405:42 - this lecture is we have this OLS
405:45 - regression results which is telling okay
405:47 - this is the dependent variable what
405:48 - model we are using and a lot of things
405:49 - we shall talk about that in just a
405:51 - second about how we can interpret the
405:53 - OLS results regression and then once we
405:55 - have our uh oh what is results now we
405:59 - are we have a model trained with a full
406:00 - report of how it is performing how our
406:02 - model is performing how they're
406:04 - performing in different different tests
406:05 - and Etc
406:07 - and Etc so this is something which will
406:09 - which which we'll talk about okay uh
406:12 - each and every Integrity what exactly
406:14 - this what exactly this what exactly this
406:15 - report says and everything in Greater
406:17 - detail so let's talk about how can we
406:19 - interpret each of them in a nice way so
406:22 - I would suggest if you go to
406:23 - interpret.md so when you when you go to
406:25 - interpret.md you have pretty much
406:26 - everything listed out there so I'm just
406:28 - going to focus on important you know
406:30 - explanation of which is required for you
406:32 - as of now and which is in the scope of
406:34 - the course uh like confidence the
406:35 - interval switcher which we haven't had a
406:37 - talk so I'm not going to talk about that
406:38 - but we are going to talk about most of
406:40 - the things which is the starter which is
406:41 - also very very important
406:43 - cool so let's talk about that so first
406:45 - of all let's let's assess the results so
406:48 - now we have trained the model that can
406:50 - take in the TV expenses and then can
406:52 - predict and that can predict what that
406:54 - what what will be the possibly the
406:56 - number of the sales which are which
406:57 - you're gonna get so what you can do you
406:59 - can simply go over here you can simply
407:01 - go over here and then say okay here and
407:03 - then say you will model dot predict
407:05 - model dot predict you given though you
407:07 - you you're given the value of TV sales
407:10 - and then that will give you you're given
407:12 - the value of TV sales and the previous
407:13 - year through the particular variable
407:15 - which is the uh which are TV expenses
407:18 - and that that will give you the sales
407:19 - okay so this is what what you're going
407:22 - to model or predict but that's something
407:25 - for you know I just like you can easily
407:27 - do that I assume that because that's
407:29 - something which I will also explore by
407:30 - your own okay
407:32 - cool so over here you have the first
407:35 - first and the first one is dependent
407:37 - variable and dependent variable is a
407:39 - dependent variable in the model where in
407:41 - this case it's the sales going to
407:42 - predict the sales given the TBE expenses
407:44 - second one is r squared as you know we
407:47 - had a talk on r squared right in Greater
407:49 - detail how do we evaluate our how do we
407:52 - evaluate our linear regression model so
407:54 - over here so over here our r squared our
407:57 - r squared which is nothing but 0.812
407:59 - which means that 81.2 variations of the
408:02 - sales is explained in our TV that means
408:05 - that is pretty nice which is 81 of
408:08 - variations in the sales whatever the
408:10 - variations comes in in the sales is
408:12 - being described by your TV expenses and
408:15 - that's pretty much put it's good so
408:17 - higher r squared indicates the model is
408:18 - better fit to the data where lower is
408:21 - not good okay
408:23 - uh but but it also depends on problem to
408:25 - problem so you'll so you might have to
408:27 - worry about that as well but but
408:29 - basically over here it explains pretty
408:30 - well uh what is adjusted r squared
408:33 - adjusted our r squared in this case is
408:35 - almost the same is almost the same which
408:37 - is 0.811 and what has what exactly
408:41 - adjusted R square is it's just it's
408:43 - nothing but a modified version of your R
408:45 - square modified version of R square so
408:47 - what is modified version of r squared
408:49 - for example it is a measure of a
408:51 - goodness of fit for the linear equation
408:54 - model that adjusts for the number of
408:55 - predicts now you now you might be
408:57 - confused in this so let's consider very
408:58 - very simple example
409:00 - consider that linearized model that
409:02 - predicts the sales of a product for the
409:04 - amount of money spent on Advertising
409:06 - okay so r squared will give us an idea
409:08 - about how more how well the model fits
409:11 - on the data how well your sales is being
409:12 - described by your independent variable
409:14 - okay now let's see now let's say that
409:17 - you add under additional independent
409:19 - variable such as how much your how much
409:21 - your company spent on radio spending
409:23 - okay so the r squared value will
409:25 - increase if the radio for for example r
409:29 - squared which which we have will
409:31 - increase if the radio is pending
409:34 - improves the model OKAY like if if the
409:38 - radio spending improves the model R
409:39 - square will also increase right so
409:42 - basically so basically it will increase
409:45 - even if one of the predictor is not
409:47 - improving the model okay so you put
409:49 - another variable X and then if it is if
409:51 - if it is not improving but a radio is
409:53 - improving then increases so r squared
409:57 - might not be significant if you have a
409:58 - more lots lots of predict independent
410:01 - variables okay
410:03 - lots of independent variables so I just
410:06 - R square is pretty much same because we
410:08 - have only one variable and they are
410:09 - almost same because over here we have
410:12 - only one to check the the two to check
410:15 - how much they're explaining to the
410:16 - output variable y but for example if you
410:18 - introduce to one of the variables like
410:20 - radio newspaper then you will see a
410:21 - massive difference because because if
410:23 - you have a large large number of even if
410:25 - one of them is increa is improving the
410:27 - model you're asked whether it will
410:28 - increase but r squared just increase the
410:31 - RS but I just ask it what does it do it
410:34 - only increase R square if and only if if
410:37 - and only if if your overall it just if
410:40 - your overall model is improving okay so
410:43 - that is our adjusted R square you can
410:45 - take a look if you want to know more
410:46 - about it
410:48 - method what method which you're using B
410:49 - squares method where we are using
410:51 - greatness uh grain is almost same but
410:53 - little bit of difference and all but we
410:55 - are using gradient descent algorithm to
410:56 - optimize it and then we have something
410:58 - else F statistic and so F statistic
411:00 - which you have already talked about in
411:01 - hypothesis testing and F statistic which
411:04 - tests the overall significance of the
411:06 - regression model in this case the F
411:09 - statistic is this much which tests how
411:10 - much your regression model is good so
411:13 - basically uh so basically of over here
411:15 - which you see the app statistic which
411:17 - does overall significance of a
411:19 - particular model and if you convert that
411:21 - to a probability probability of say F
411:23 - statistic which is around
411:25 - 7.93-74 which means the model is highly
411:28 - significant how can we say that the
411:30 - model is highly significant so you might
411:32 - be thinking here you 7.93.4 is values
411:35 - very very small like very very large
411:37 - right but it's not actually if if if you
411:40 - know math then you'll be able to
411:41 - understand this that actually a very
411:43 - very small number okay so so what we'll
411:46 - do we'll compare that value the
411:47 - probability will compare that value
411:50 - against our uh against our uh PP value
411:54 - and then we can see use and and and then
411:56 - we can see over here and then what in
411:59 - the in the p-value we can we can we can
412:01 - test okay in this case we we can have a
412:03 - null hypothesis that the co if that the
412:06 - model coefficients are equal to zero
412:08 - okay
412:09 - a low P value indicates that the
412:12 - evidence again so we can so P value
412:14 - which is the P value P value lower than
412:17 - 0.05 which so that will if if it is then
412:20 - will be resected so over here it's
412:21 - actually lower than
412:22 - 0.0.05 then we'll say okay this let's
412:25 - reject our null hypothesis the null
412:26 - hypothesis was that all our betas are
412:28 - equals to zero so that is not so so we
412:30 - can see that it's pretty very zero that
412:32 - model is highly significant to our uh so
412:35 - your your DV is highly significant in
412:38 - predicting that but but uh but the this
412:41 - this F statistic can be for several
412:44 - other values as well it tests the
412:45 - overall sense so if you also see the
412:47 - highly significant then if you use three
412:49 - of the variables of radio newspaper and
412:51 - uh what this TV then if it is
412:53 - significant you say Okay three other
412:55 - variables are very highly significant as
412:56 - well in predicting the sales
412:59 - okay uh number of observations there are
413:01 - 200 observations as of now as I said
413:03 - there's degrees of freedoms so degrees
413:05 - of freedoms in this case it's 198
413:07 - because you have 200 200 number of rows
413:10 - and then you simply have number of rows
413:12 - or number of data points minus the
413:14 - number of parameters in this case only
413:15 - beta 1 and beta 0 beta 1 is related to
413:18 - TV and bitter is the constant term uh
413:21 - it's there's the biased term so what
413:23 - exactly degrees of freedom is so what
413:26 - exactly this does is it represents the
413:29 - number of independent values in your
413:31 - data set number of independent values
413:33 - okay this actually helps in calculation
413:35 - this actually helps in calculation of T
413:38 - statistic or F statistic and lot of
413:40 - other way but this is what the DF
413:42 - residuals is it's saying they are DF
413:44 - residuals it's nothing but here 198.
413:49 - okay so covariance type so what is
413:51 - covariance type covariance type over
413:53 - here is nothing but uh non-robist we'll
413:57 - talk about that what does that what does
413:59 - that mean what does it mean but the
414:01 - covariance type is the type of the
414:03 - covariance used to compute the standard
414:05 - errors of the coefficients again the
414:06 - blah blah blah it's very difficult to
414:08 - understand but I'll I'll explain in a
414:10 - nice way so that you understand it so
414:12 - over here what is a telling that is your
414:14 - standard matters us first of all what is
414:16 - standard errors we have all drugged the
414:18 - other how efficient how efficient or
414:20 - accurate or how reliable our
414:22 - coefficients are for example a very nice
414:24 - example which will destroy or say you
414:26 - want to predict the mean height of all
414:29 - adult males in the United States of
414:30 - America we can take a sample of adult
414:32 - males and calculate the mean height of
414:34 - the samples will be from the sample we
414:36 - to calculate the mean height so how much
414:38 - so we say that because the sample is
414:39 - only small portion of the whole POS
414:41 - population so the mean height of the
414:43 - sample will not directly match the
414:45 - population sample that's true right
414:48 - that's true this is mean height will of
414:49 - course not matter so the standard and
414:51 - what is it tells it tells the measure of
414:54 - the variability of the height of the
414:56 - sample which provides what is the what
414:58 - is the it provides an estimate of the
415:00 - uncertainty so how much we are sure
415:02 - about how much our sample mean is
415:04 - accurate how much a sample mean is
415:06 - accurate accordance through the
415:07 - population mean so small on me is
415:09 - smaller than standard error is more
415:11 - precise the estimate for your population
415:13 - estimate so you're based on Sample mean
415:15 - you're predicting the population mean
415:17 - which means that you're using uh there
415:20 - is some sort of standard error switch
415:21 - tells that okay how much precise we are
415:23 - in getting the population mean given we
415:25 - have the uh sample mean
415:28 - and in this case covariance type is non
415:30 - robust which means we calculate the
415:33 - covariance of our Matrix without any
415:35 - elimination of the data without any
415:39 - elimination of the data so what exactly
415:40 - does this tells what exactly does this
415:43 - sense I'm just just going to go over
415:44 - here and then tell you about that so
415:46 - over here if you read read this out
415:48 - as typically non robust where there is
415:50 - no elimination of the data to calculate
415:52 - covariance between two features but what
415:54 - exactly covariance is covariance is a
415:57 - nothing but it tells you the
415:58 - relationships between two random
416:00 - variables so for example you have two
416:02 - stocks which is S P 500 and ABC
416:05 - Corporation you're going to assess the
416:07 - directional relation what is directional
416:08 - directional relationship so if SP is
416:11 - increasing what is happening to ABC
416:13 - Corporation so what is the covariance
416:15 - between SP and ABC so it tells either
416:17 - one is squeezing or one is decreasing so
416:18 - basically this is the data is an example
416:20 - and it's extra and the variable is y
416:22 - random variables over here both are
416:23 - increasing that means they have a
416:24 - positive covariance this one it tells
416:27 - you the direction of the relationship
416:30 - not like if one is the wood then what is
416:32 - happening to another so if both of the
416:34 - stroke decreases then we have certain
416:36 - things so that's for the covalences it
416:37 - tells you the relational uh it tells you
416:40 - the relationships between two random
416:42 - variables okay
416:44 - uh cool so I hope let me just ensure
416:47 - that yeah it's running cool so go
416:51 - variant shows two videos between uh how
416:53 - to move with respect to each other and
416:54 - blah blah blah it just tells you about
416:56 - everything over here okay
416:58 - uh no that's known robots which means
417:00 - that we don't eliminate any data okay
417:02 - which is most common way coefficient so
417:05 - coefficient is coefficient is nothing
417:07 - but coefficient is nothing but for
417:08 - constant which is the bias term which is
417:09 - beta0 is 6.978 which is the average when
417:12 - X is your X is equals to zero and TV is
417:15 - 0.000.055 and constant for the Santa
417:18 - error for the constant is 0.323 which is
417:21 - how reliable how reliable our uh our
417:25 - constant or or the our how accurate or
417:28 - precise our estimate is for bias term
417:30 - and how to estimate how good I estimate
417:33 - for TV which is pretty much very low for
417:34 - both of them
417:36 - and then you calculate the T statistic
417:38 - which we had a talk which which we had a
417:41 - talk which is T statistic which is a
417:42 - look at each statistic for constant like
417:44 - how because this is what is it it tells
417:46 - you it tells you the a signal the
417:48 - significance of the individual
417:49 - parameters it it tells you this it it
417:52 - tells you for individual parameters okay
417:54 - so we just calc which is calculated by
417:56 - dividing the coefficient by the standard
417:58 - error of that coefficient so how so
418:01 - basically that's the T statistic now
418:02 - what you do you compare the T statistic
418:04 - so this is the basically you have the
418:06 - p-value uh you have the p-value for each
418:08 - coefficient which measures now if uh and
418:12 - you can see that both of the P values uh
418:14 - both of the P values are close to zero
418:15 - which means that they are that they are
418:18 - pretty nice you know they are less than
418:19 - 0.05 and they're both significant to the
418:22 - model okay we have Omnibus test which
418:25 - test though now now over here we are
418:27 - done with a couple of things which is
418:28 - the testing significance of a model now
418:31 - what we do we have the Omnibus test what
418:33 - exactly Omnibus test does it tells us
418:35 - that we have absorption which is
418:37 - normality assumption in a linear
418:38 - regression which tells whether our data
418:40 - follow our residuals are normally
418:41 - distributed or not so Omnibus test
418:44 - statistic gives you exactly the uh to
418:46 - that helps to validate the Assumption
418:48 - that's why I really like stats model API
418:50 - because it tells you everything which
418:51 - you need you don't need to calculate
418:52 - further on so Omnibus statistic which
418:55 - gives you 0.013 and the probability
418:58 - which get the prob the P P value which
419:00 - is 0.993 which suggests that residuals
419:03 - are normally distributed how can we say
419:05 - that they are normally distributed so
419:07 - over here which which which you can see
419:09 - the the the the the probability value
419:12 - the probability of Omnibus value is the
419:14 - p-value associated with the test
419:16 - statistic which was calculated okay so t
419:19 - t statistic you know T statistic
419:20 - statistic which we can calculate and
419:23 - other things so here the probability
419:25 - common Omnibus is the if your if your
419:28 - value of the prop the probability of
419:29 - Omnibus value types of the test is close
419:32 - to one that means our likely normally
419:34 - distributed and there if they're less
419:36 - than that they are if they're less than
419:38 - 0.05 which means they're not normal
419:40 - distributed right so over here which is
419:43 - 0.993 which means that are close to one
419:45 - that means it's a normal distributed
419:47 - another one is Durban words and test
419:49 - which was used for testing the
419:50 - independence of residuals whether one
419:52 - into one one error doesn't not affect
419:54 - another which we had a talk so it
419:56 - usually ranges between 0 to 4 and the
419:58 - value indicating two which means they'd
420:00 - have they they follow the assumption but
420:02 - less than they are you know uh positive
420:04 - and greater than they are greater than
420:06 - to their negative
420:08 - correlation right so um so uh but so in
420:13 - this case your double Watson test is
420:15 - around two which means that they also
420:16 - follow the independence assumption now
420:18 - we don't need to check for too much and
420:20 - then value close to as you can see that
420:22 - you can uh read over here and then we
420:24 - have another test which is another again
420:26 - for normality assumption which is Jack
420:28 - beta test you know there is a lot of
420:30 - pronunciation uh for the same like harik
420:34 - beta you know so it depends on what you
420:37 - use so you you have another test you
420:39 - don't need to understand how exactly the
420:41 - mathematical working was setting it's
420:42 - just for statistics purposes so which
420:44 - tests assumptions of a normality of the
420:46 - residuals in this case your beta
420:48 - statistic as you know first first you
420:49 - have a t value and then the critical
420:51 - value right so first of all calculate
420:52 - the beta statistic which is this much
420:54 - and then you calculate the probability
420:55 - of that which is from the table
420:57 - distribution which is 0.979 which says
421:00 - that residuals are normally this what is
421:02 - the probability which is the so so that
421:04 - we have the problem so that we can
421:05 - compare with the significance right
421:07 - Alpha value otherwise how can you
421:08 - compare this this is this is not a
421:09 - probability convert that probability so
421:12 - we suggest that the residuals are
421:13 - normally distributed how can we say that
421:15 - you can read that how can we see that we
421:17 - have actually written you everything
421:18 - which is over here
421:20 - uh that that also tests suggests now now
421:23 - we have skewness is skewness means that
421:26 - skewness means so how can I explain your
421:27 - skewness is skewness is a statistical
421:29 - you know uh I think that you have to
421:32 - enroll in my course for learning about
421:33 - all of these things but but basically
421:36 - um but but basically skewness means that
421:38 - excuse suggests some normality
421:40 - assumption okay uh so basically it has
421:42 - also also helps to calculate our
421:44 - normality so over here in this case is
421:45 - minus 0.08 which means the residuals are
421:48 - close to being normal distributed I
421:49 - suggest you to you know have a look at
421:51 - in detail about skewness and kotoris
421:53 - because this is something which you have
421:55 - to and it won't indicates the validates
421:58 - our normality assumption over there but
422:00 - I suggest you to explore by your own
422:02 - because something is takes a long time
422:03 - to explain that is that that I explained
422:06 - in my probability course
422:09 - cool and this is the condition number uh
422:11 - this is not important you can actually
422:12 - ignore but actually condition number if
422:14 - you if you want to know about like uh
422:17 - and so you have a condition number over
422:18 - here condition number what is it states
422:20 - it represents the sensitivity of the
422:23 - predictions of the model predictions to
422:24 - small changes in the value of
422:26 - independent variables so how much your
422:29 - model prediction will change if you
422:31 - change the training data and then
422:32 - retrain the model okay so if you reach
422:34 - if you change the training a little bit
422:36 - and then how much that will change so
422:38 - higher the higher the condition number
422:40 - is better uh sorry uh small kind of it's
422:43 - a small condition number indicates that
422:45 - the model is a relatively insensitive to
422:48 - changes which suggests are relatively
422:50 - stable okay so they are not sensitive so
422:52 - basically smaller condition number which
422:54 - means that even if you change the data
422:56 - your predictions will be remained stable
422:57 - it will not drastically change I will
423:00 - not trust it it will not drastically
423:02 - change so I hope that that really helps
423:06 - um that really helps now uh I hope that
423:08 - this this provides a very nice
423:10 - explanation about our condition number
423:12 - and the summary of everything now what
423:15 - you can do now you can have now now we
423:17 - have now we are done with the
423:18 - interpretation now I can actually go to
423:20 - something known as buy your own which is
423:22 - explanation and explanation consists of
423:25 - everything which we had talked on a
423:27 - lecture like I had uh of over here again
423:30 - what exactly the report is what is the
423:32 - small summary of the model over here I
423:34 - have interpreted I've interpreted every
423:36 - the important you know tests and
423:38 - everything I've also validated the
423:40 - assumptions I've also validated the
423:42 - assumptions if you you can also go on uh
423:44 - images and then see figure one figure
423:46 - two figure three and all which which
423:48 - will give you a very nice as uh for for
423:50 - every assumption by your own okay if you
423:53 - want you can actually tell me in a
423:55 - comment box if you want to if you want
423:56 - me to develop for it but you can see
423:58 - that I've written a very nice blog for
423:59 - this so that you don't need to worry
424:01 - about too much and also just what are
424:02 - the further improvements or limitations
424:04 - which can be listed out there for you to
424:06 - work on and that's pretty much it about
424:08 - this project I'll be catching up the
424:09 - next project till then bye
424:13 - hey everyone welcome back to another
424:15 - lecture and section which where we are
424:17 - going to eventually talk about one of
424:19 - the last projects for the midterm so on
424:22 - regressions on on regression part and
424:25 - then we'll move on to classification
424:26 - projects so basically what this project
424:29 - indicates this this project says that
424:31 - that in this uh in this we'll try to
424:33 - predict cancer mortality rates for the
424:36 - U.S countries right so what exactly this
424:39 - uh what exactly the problem statement is
424:41 - ETC so here's the walkthrough of the
424:43 - project which will go through it first
424:45 - we'll start off with the introduction
424:46 - which is this lecture so we'll start off
424:48 - with the introduction lecture and then
424:50 - give you some motivation to start with
424:51 - it and then we'll uh go through through
424:54 - the Rick what are some of the required
424:56 - installations and what is the virtual
424:59 - environment setting up the virtual
425:00 - environment setting up your workspace
425:02 - Etc
425:03 - and then we'll go forward in
425:05 - understanding exploratory data analysis
425:08 - so inex in exploratory date and Analysis
425:12 - we'll try to do some you know a Basics
425:14 - Ed of the data set and we'll try to
425:16 - understand uh more about the data and
425:19 - then after that we'll go through uh
425:21 - preparation of the data what is
425:23 - preparation of the Dead data means which
425:25 - means cleaning up and then feature
425:27 - engineering Etc
425:29 - and then we'll model our work uh where
425:31 - we'll Implement our linear regression
425:33 - model to predict what is the death rate
425:36 - for that particular country in that
425:38 - particular country for the different
425:40 - different states
425:41 - I hope that makes sense now now uh what
425:44 - I'll do is
425:46 - make you make you make you go through
425:48 - all the the first part of this project
425:50 - work through is Introduction section
425:53 - where we let's talk about uh what
425:55 - exactly we are going to solve in today's
425:57 - lectures
425:58 - so basically you can go to the uh given
426:00 - link so Wireless regression challenge so
426:03 - you'll be having the link in the readme
426:04 - of your project and so if you go over
426:07 - there so basically this is a challenge
426:09 - listed by data.word which is one of the
426:12 - good challenges which I've ever seen and
426:14 - basically this challenge says that that
426:16 - you want to predict that your task is to
426:18 - build a model to predict what is the
426:21 - rate of cancer mortality what is the
426:23 - rate of the the person dying with the
426:26 - cancer for U.S countries okay so if you
426:30 - go and see the data so the data the data
426:32 - dictionary where you have several
426:34 - independent features and then you have
426:36 - one dependent features so what is let's
426:38 - let's talk a bit about on this is you
426:41 - have the data dictionary let's let me
426:43 - show you first of all the data the data
426:45 - which you'll have is over here this is
426:47 - the data which is which is in CSV file
426:50 - okay so you have a total of
426:52 - 3047 rows and 34 columns in which in
426:56 - which there is one column which is a
426:58 - Target variable so let's talk about what
427:00 - is which is the color which is Target
427:01 - variable which is Target death rate with
427:04 - the target is the death rate which is a
427:07 - dependent variable it says that mean per
427:10 - capita which is what is the death rate
427:12 - uh in per capita which is 100 000 cancer
427:15 - mortalities and then you have again the
427:18 - same thing average mean number of
427:19 - reported cases of cancer diagonal
427:21 - diagnosed annually so you have these
427:24 - features which is average counts if you
427:26 - go down you see average count and then
427:28 - you have average dates per year which is
427:30 - mean number of reported mortalities due
427:32 - due to the cancer average death per year
427:35 - then you talk about incident rate mean
427:37 - per capita and the median income per
427:40 - country then you have population of that
427:43 - country percent of population country so
427:45 - you have the features out here you have
427:47 - the features out here and every feature
427:49 - has its own description listed out here
427:53 - so I suggest without doing anything go
427:56 - through the understand every feature and
427:59 - try to first of all understand what
428:00 - exactly each feature is trying to tell
428:02 - you okay
428:04 - um so basically the the years of data it
428:06 - contains is from 2010 to 2016 and
428:10 - between that you have several number of
428:11 - information out there right for example
428:14 - you can just go and see
428:16 - um some birth rate which is the number
428:18 - of live births relative to number of
428:21 - country women in the country which is
428:24 - the person of married households person
428:26 - of country listening who I don't find a
428:27 - category which is not white so basically
428:29 - these are the information for a
428:30 - particular observations
428:32 - okay so let's go with that example so
428:35 - first example is where your average
428:36 - Union count is this much and then you
428:38 - have a death rate where given all of
428:40 - these information you have to predict
428:42 - this particular information which is the
428:43 - death rate so where is the death rate
428:46 - um death rate
428:48 - yes so you saw you have to predict this
428:50 - target variable so this is this
428:52 - particular is the target variable where
428:54 - you have to predict this given all the
428:56 - information out there given all the
428:58 - independent features so these are
429:00 - independent features and you have a one
429:01 - which is a dependent feature I hope that
429:03 - makes sense now
429:05 - um so what exactly we will do let's uh
429:07 - let's go to this now given the data so
429:10 - basically we need to predict the rates
429:12 - so cancer mortality rates for the USPS
429:14 - come countries out there right
429:17 - um which is nothing but how many people
429:19 - died uh we have to predict the death
429:22 - rate using of uh death rate which will
429:25 - happen uh using cancer okay with the
429:29 - cancer diseases so build the task is to
429:32 - build a multivariate which is multiple
429:35 - um Lane regression model to predict the
429:39 - death rate for the US countries right so
429:42 - what what should be the deliverables
429:43 - when when you when you will complete
429:45 - this task what should be your following
429:47 - deliverable it's like what you're going
429:49 - to deliver to the task master whoever is
429:52 - checking your assignment is first is
429:55 - yours should be model equation where
429:57 - you're where you should have a model you
429:59 - should have a statistical software where
430:00 - you're telling about R square mean
430:02 - squared your code file and the model
430:04 - Diagnostics whether your model follows
430:06 - the linearity Assumption Independence
430:09 - assumption Etc and then your
430:11 - interpretation of the model and there
430:13 - are other factors to also consider in
430:15 - this okay so I hope that this this will
430:18 - tell you a lot and now I hope that this
430:21 - gives you good sense about what exactly
430:22 - we have to do now we you you may be
430:25 - thinking we'll be covering each and
430:26 - every part as you might have already
430:27 - noticed that we have covered this part
430:30 - which is assessing them which is a model
430:33 - assumption test as well as the
430:35 - evaluation part so these two things are
430:37 - in your assignment however we'll do the
430:40 - big chunk of portion of this project to
430:42 - help you better understand mainly the
430:44 - data processing steps and all so I hope
430:46 - that this gives you good sense about
430:48 - what exactly because we are going to do
430:51 - um so I as I told our our aim we have
430:55 - come we have given you the short
430:56 - introduction about the problem statement
430:58 - problem statement is nothing we have to
430:59 - predict the death rate per me uh which
431:04 - is which which is the uh death rate and
431:07 - what is the forecast using which
431:09 - happened with cancer mortalities okay
431:11 - now uh you might have some issues like
431:14 - Understanding Variables so here's you
431:16 - have to do if you're having a very uh
431:18 - height a hard time and in understanding
431:21 - cancer mode uh whatever the independent
431:23 - features then suggest you research that
431:26 - particular term online on Google and
431:28 - then you'll be automatically
431:29 - understanding what exactly you need to
431:31 - do right but as of now we want to
431:33 - predict what what will the death rate in
431:35 - that particular country given that in
431:37 - that sort of information so basically if
431:39 - you go out here
431:41 - if you go out here you will see a
431:43 - geography in that geography in in that
431:46 - geography you have the geography as well
431:48 - as for for that particular place and for
431:51 - that particular year you have all the
431:53 - information available I hope that this
431:56 - gives you a good sense what exactly the
431:58 - problem statement is now in the next set
432:00 - of lectures what I'll try to achieve is
432:02 - I'll make I'll start off with a very
432:05 - basic introduction of what is the
432:07 - require required installations which you
432:09 - need to do what is environment variables
432:12 - so we'll start off with the basic
432:13 - introduction to environment variables
432:15 - and then we'll move forward accordingly
432:18 - hello everybody
432:20 - um now we'll talk about one of the
432:21 - theory theoretical concept today is a
432:24 - bit about on programming basis but this
432:26 - should already be covered in your python
432:27 - if but if you don't know I widely use
432:30 - this uh virtual environment uh in this
432:33 - project so if you if you don't about
432:36 - virtual environment at all uh this
432:38 - lecture is for you maybe uh but if you
432:40 - don't know completely python this this
432:42 - lecture is not for you please complete
432:44 - your python stuff click in Python you
432:46 - you're taught you taught in these all
432:48 - these things so please make sure that
432:50 - you have some idea what exactly these
432:52 - things are but if even if you don't if
432:53 - you have basics of stuff not done then
432:56 - you will be able to solve things up cool
432:58 - so let's get started uh actually is
433:02 - first is I will to what our agenda today
433:05 - to talk about a virtual
433:07 - environment
433:09 - variables in Python
433:12 - our agenda is to talk about this
433:15 - especially in this particular lecture
433:17 - and uh first of all let's so let's talk
433:20 - about uh what exactly they are right so
433:23 - virtual environment in is an isolated
433:26 - working copy of your of your python
433:29 - version that allows you to install any
433:31 - sort of packages or dependencies which
433:33 - you have for a particular project so I
433:35 - know it's something which is a very big
433:37 - definition let's let's say one thing
433:39 - you're working on Project number a
433:41 - you're you're working on Project number
433:43 - a and also you're working on Project
433:45 - number B project number B you have a
433:48 - python you have python installed you
433:49 - have a python installed right and then
433:52 - in that python you have several
433:53 - libraries installed such as numpy
433:55 - installed panda is installed
433:58 - matplotlib installed blah blah blah so
434:01 - in that python version you have several
434:03 - dependencies and packages are also
434:05 - installed right and now now now you now
434:10 - now you have this package packages and
434:12 - all so your project a also downloads in
434:15 - your base version your pro in Project a
434:17 - you download your whatever first of all
434:19 - down download your package is required
434:21 - for project from these um which these
434:24 - and every library has its own version
434:25 - right they have the V1 V2 V3 Etc and
434:29 - Project B might also have the uh might
434:33 - also have some some sort of similar
434:35 - libraries or different different
434:36 - packages installed okay but but but you
434:39 - may be thinking but over here but over
434:42 - here project a is utilizing my it might
434:45 - happen that project a might have
434:48 - different versions requirement different
434:50 - versions requirement and Project B might
434:53 - have different versions requirement
434:54 - right that's one of the way to think
434:57 - about that if your project a requires if
435:00 - you project a runs on numpy 1.0 then
435:04 - project if Project B works on numpy 2.0
435:06 - then that's an issue right you you
435:09 - cannot uh run Project B on that first
435:12 - library right on the other hand you
435:15 - might be having questions like
435:18 - um so so so so we need uh so you you may
435:21 - you might be thinking how can we solve
435:23 - this there's one way to solve this is
435:26 - that we can have every project
435:28 - separately so we can have a project
435:30 - project number a we can have a project
435:32 - number a with with its own python
435:34 - version with its own python version with
435:37 - its own libraries version so its own
435:39 - libraries versions with its own
435:40 - dependencies
435:42 - right so for example numpy can be
435:44 - dependent on several other uh packages
435:48 - to work right pandas can be dependent
435:51 - upon numpy matplotlab right so a single
435:54 - Library can be dependent upon several
435:56 - other uh packages so one way to think
435:59 - about why do we need first of all what
436:01 - is virtual first of all the the problem
436:03 - which arised that a particular project
436:05 - project number a might have different
436:07 - requirements of your python version they
436:10 - require 3.8 but Project B requires 3.9
436:12 - and they might require several other
436:14 - other set of versions of your libraries
436:17 - and maybe there there might be different
436:20 - different dependencies so for example
436:22 - numpy can be dependent on pandas and
436:24 - password but pandas can may not be
436:27 - dependent on matplotlab right so this is
436:30 - dependency conflicts which means which
436:32 - may can happen and Etc so what is the
436:34 - ideal solution
436:36 - we can have a a container we can have a
436:39 - container sort of thing we can have a
436:40 - container sort of thing or the variable
436:42 - sort of thing a variable sort of thing
436:44 - or an environment sort of thing for
436:47 - every project so we have two projects so
436:49 - project number a so we create an
436:51 - environment for that particular project
436:52 - number a and in that project number a
436:55 - you might install your own python
436:58 - version your own python version you
437:00 - might also install your own uh libraries
437:03 - your own libraries so now you can run
437:07 - that project a using this and whatever
437:10 - versions you have and then you make this
437:12 - another environment for project number B
437:15 - for a project B and in that project B
437:18 - you can have different different
437:20 - versions so you've given a numpy 1.2
437:22 - number one 1.3 and then you can activate
437:24 - this environment if you're running this
437:26 - particular project you can have project
437:28 - C with different versions with different
437:31 - versions of your libraries or Python and
437:33 - all right and then if you want to if
437:35 - you're into runs from number three you
437:37 - can activate this environment
437:39 - if you have project number D then you
437:43 - can have several other versions and then
437:45 - you can actually if you want to run this
437:47 - project then you can activate this
437:48 - particular environment
437:50 - okay so this is one of the way to to
437:53 - think about it right uh so basically you
437:56 - can you can you can have a different
437:58 - different versions of a packages and
437:59 - different different for the different
438:01 - different projects for the different
438:02 - different uh environments
438:05 - and and and and it will and we have and
438:07 - we can create this and this will not
438:09 - affect anything on a global global means
438:12 - where you have a globe global winds
438:14 - which is which is out of which is the
438:16 - which is setting over here in that
438:18 - Global you have certain packages
438:20 - installed python installed on a path
438:21 - environment but in that a global one you
438:24 - have several of the virtual environments
438:25 - available
438:26 - right there are different different
438:28 - tools that use to create these sort of
438:30 - environments one of them is when another
438:33 - one is conda another one is uh one is
438:37 - when another one is conda another one is
438:40 - uh let's let's say for us for a second
438:42 - example uh pinev
438:45 - so we have these three where you can
438:47 - simply create your version bar and
438:50 - environments out of these tools right
438:53 - so let's talk about a bit about you know
438:56 - a bit of reasons why exactly we need a
439:00 - virtual environment in our daily life as
439:02 - is something which is super important so
439:04 - uh there are several reasons which we
439:06 - might use for virtual environment the
439:08 - first one is package package
439:12 - management
439:16 - so this is the first one which is in
439:18 - package management you can you what you
439:20 - can do you can install your packages you
439:23 - can install your
439:25 - packages you can install and manage your
439:28 - packages or libraries libraries for
439:31 - particular projects you don't need to
439:33 - install in such a way that it should be
439:35 - applicable on every project it can be
439:37 - for a particular project and this like
439:40 - you install it like you install you
439:41 - manage your project you you have a
439:44 - project and then you have several other
439:45 - dependencies so you have to only in
439:47 - practice this you can package this
439:48 - without affecting the project number two
439:50 - on which you're working so you might
439:52 - have noticed in my previous projects I
439:54 - always create a budget environment so
439:55 - that it does not affect my other
439:56 - projects and dependencies right another
439:59 - way is we isolate our projects which is
440:02 - another one is
440:04 - um is isolation another one is ISO
440:08 - isolation
440:10 - isolation means that if you have lots of
440:14 - environments you have lots of virtual
440:16 - environments for all for project number
440:18 - a project number B project number c so
440:21 - you can have these virtual environments
440:22 - so changes in the project number a does
440:25 - not affect the changes in Project number
440:27 - two even they if they have a same line
440:29 - because they have all installed
440:31 - separately they haven't all are
440:33 - installed separately by creating a
440:35 - folder so everyone has their own folders
440:37 - and in that they have their own packages
440:38 - so if every environment has their own
440:42 - different different versions or it may
440:44 - be same as well but uh one is that one
440:47 - does not affect others so in covid-19
440:49 - used to isolate
440:51 - people so in just way over here as well
440:54 - we isolate you guys
440:56 - okay now uh you might have another one
440:59 - is which is the another one is
441:02 - reproducibility
441:07 - okay so this is another reason of
441:10 - reposibility which means that let's take
441:13 - an example you run a particular project
441:15 - number a you run a project number a and
441:17 - that there has own requirements you know
441:19 - that has short should have numpy should
441:21 - have pandas in that project should have
441:23 - C born should have uh this this
441:25 - particular requirements all the all the
441:27 - technical requirements right
441:29 - so when you have when you package this
441:31 - up you can exactly set up in such a way
441:34 - that it will work on other systems as
441:36 - well you can also deploy to production
441:38 - so whatever you change change it over
441:40 - here changes the production as well so
441:42 - you can actually have a very good
441:43 - reproducible pipelines out here and I
441:45 - hope that this gives you good sense
441:47 - about what exactly uh you need to you
441:49 - know you should be able to but the
441:50 - reproducibility is not nothing but
441:53 - sharing of your uh project to others or
441:56 - deploying your project to a production
441:58 - that's why because it may happen that
442:01 - you have a global slate safe say for
442:02 - example of global environment and in
442:04 - that you have hosted project number a so
442:06 - even if a single dependency error occurs
442:09 - in the system which is running for users
442:11 - and production will fail right so you
442:13 - have the separately so that your model
442:16 - is also reproduction
442:18 - is contained in a single container or a
442:20 - one environment I hope that this gives
442:23 - you good sense about what exactly we
442:25 - wanted to talk on packages and all now
442:28 - what we'll do we'll try to get into bit
442:30 - of coding part so now we are compared to
442:32 - complete a bit of theory which which was
442:34 - needed before it now what we'll do we'll
442:36 - try to install our packages now and then
442:38 - we'll get started with a basics of you
442:41 - know exploratory data analysis and try
442:43 - to understand from there hey everyone
442:45 - welcome to this uh another video on what
442:50 - what you're going to do we are going to
442:51 - set up our project and make sure that
442:53 - you're also following you have to
442:56 - downloaded materials and all so you're
442:57 - also evolving to that throughout the
442:59 - course
443:00 - um so what I'll do today is make you a
443:02 - bit comfortable with
443:04 - um a bit of you know setting up the
443:06 - environments and installing required
443:08 - libraries getting it through the
443:09 - notebooks which we have getting it
443:11 - through the important steps out there
443:13 - right so let's start with that first of
443:15 - all I want to show you one thing which
443:17 - is a documentation of python weight
443:19 - where it says that we'll be using van
443:21 - we'll be using when to actually create
443:23 - the environment variable out here so um
443:26 - so now if if you want to create the
443:30 - virtual environment if so if you if you
443:32 - if you really want to create the virtual
443:33 - environment then what we can do we can
443:35 - simply have this maybe this one which is
443:37 - python mvnb which which is that so
443:41 - basically I have opened my UK when when
443:44 - you unzip your file when you unzip your
443:46 - file you will be having this stuff okay
443:48 - so now you open your terminal you open
443:50 - your terminal and go to that folder so
443:52 - when you clone that when you when you
443:54 - when you'll be downloading the zip file
443:56 - when you'll be downloading this ZIP file
443:58 - open that zip file and open go in that
444:01 - folder so it should be easy right so CD
444:03 - and write the go go into this file and
444:06 - open your vs code in it right and once
444:09 - you open your vs code
444:10 - um write python
444:12 - um Dash M and when it's in the case that
444:15 - we're going to create a virtual
444:15 - environment and let's name that as a OLS
444:18 - regression challenge so just write OLS
444:21 - which will be the name of our
444:22 - environment and then let's click on
444:24 - enter and that creates and that creates
444:28 - uh we noticed do you want to select it
444:31 - for the workspace folder yes I want to
444:34 - select it for workspace folder now we
444:36 - have created a virtual environment we
444:39 - have created a virtual environment named
444:40 - OLS where it says that you can see that
444:43 - a new folder has been created where all
444:45 - our files will be installed or our
444:47 - packages will be installed now it's
444:49 - containerized in a particular
444:50 - environment so now project will only run
444:52 - if that environment is activated if this
444:55 - environment is activated okay so how can
444:58 - we act how can we activate this this is
445:01 - this is pretty much uh like like I'm I'm
445:04 - totally you know uh trying to make sure
445:06 - that how eventually if I'm if I'm uh you
445:09 - know in initial versions how I'm going
445:12 - to make sure that you're able to
445:15 - um get it so so basically uh if you go
445:18 - to the you know so if you go to the bin
445:20 - for bin file and then you have something
445:22 - which is activate which is something
445:25 - which is activate so you can either go
445:27 - there and then uh confirm it right
445:30 - either you can go there go there like CD
445:32 - and blah blah blah you just go there or
445:34 - you can just have a sets like let's say
445:37 - let's say uh you want to go at
445:41 - uh in the folder of CD OLS Ed OLS CD OLS
445:47 - eventually I I actually use pi and V to
445:50 - be honest because
445:52 - of my organization because in previous
445:54 - organizations they were using pi EnV but
445:57 - I recommend you to use when because it's
445:59 - just initial version of yourself that's
446:00 - why they come in to use that then I go
446:02 - to select go to bin and then click on
446:05 - source
446:06 - Source activate
446:08 - or maybe you can run simple simply the
446:11 - activate as well it should run very fine
446:12 - so now when you see over here OLS which
446:15 - means that now you're particular now you
446:17 - can run your project with that
446:19 - particular environment okay
446:21 - now you can go out of that however you
446:24 - can also write source and then you write
446:26 - you know OLS
446:28 - OLS and then you simply write our bin
446:32 - and then you simply write activate Okay
446:34 - it should work perfectly fine right so I
446:37 - actually gone and showed you this step
446:39 - by step uh solution of that right
446:42 - let me just zoom in a bit maybe it may
446:45 - be beneficial for you uh to notice which
446:48 - is out here so let me just zoom in a bit
446:51 - Yeah so now we have now we have
446:53 - activated environment now what we need
446:55 - to do now as I said now once we have
446:58 - activated our environment now what we
447:00 - have to do we have to
447:02 - um let's let's go through the uh project
447:05 - let us go through all the files right
447:08 - that's much more better rather than
447:10 - showing it like this so now you have the
447:12 - now you now will see something like this
447:14 - after every in after we have activated
447:15 - your environment so though so I'll
447:18 - recommend you to go to readme.md and
447:20 - it's in readme.md you'll find everything
447:22 - which you need which is what exactly the
447:24 - problem statement Etc now now what we
447:27 - need to do we need to record and we need
447:28 - to install the required libraries which
447:31 - will be going to use in this project so
447:33 - we'll be going to use this this project
447:35 - as well as we have some other libraries
447:36 - to install so we'll install that for
447:38 - sure so uh as of now we'll install these
447:41 - libraries frequent which which have
447:43 - created the requirements which says that
447:45 - these are the requirements for this
447:47 - particular project which means these are
447:49 - the libraries which are packages which
447:51 - needs to be installed for this project
447:53 - so when you will go to there and then
447:55 - what you can do you can simply add first
447:57 - of all which python so let's type think
447:59 - about which is the python which which
448:01 - you're using over here we are using the
448:03 - python which is created for this
448:05 - environment and then let's talk about
448:07 - which pip you're using using pip for
448:09 - this particular you can see that OLS bin
448:11 - and pep so pip is also for this
448:13 - particular environment now what we can
448:15 - do we can simply install pip install R
448:18 - Dash R requirements so what what this
448:22 - does what this does so it you you can
448:25 - simply name all the file name all the
448:27 - libraries with with the specified
448:29 - versions which you need specified
448:31 - versions which you need for that
448:32 - particular Library will come to that how
448:34 - we got how we created this at the end
448:36 - but as as of now you can simply have
448:39 - this pip install and this this indicates
448:41 - that we want to uh you can also install
448:43 - it individually but I was I have added
448:45 - in a file so so I can install it all at
448:48 - the ones by a single command which is in
448:50 - the requirements file and then it goes
448:51 - in the first install the first second
448:53 - third fourth all around the end and then
448:56 - simply click enter once we click enter
448:58 - it takes it takes a bit of you know
449:00 - um time to do that and then you will see
449:03 - that it is being all the packages are
449:06 - being installed so all the packages are
449:08 - I think uh being installed over here
449:10 - where they're installing matplotlib
449:11 - they're installing numpy they're
449:13 - installing scikit loan they're
449:15 - installing stats model and then we are
449:17 - done so we got a bit of warning over
449:19 - here what that warning says that that
449:20 - the version which we are using the paper
449:23 - version which we are using it's not
449:26 - upgraded so let's upgrade our pip
449:28 - version so let's let's upgrade our pip
449:30 - version so we can simply add Python and
449:33 - then use this particular command so it
449:36 - is saying that you used to use this
449:37 - python which is a python the the
449:39 - absolute the the path of the Python
449:42 - python is over here but you can actually
449:43 - absolutely use that python because you
449:45 - have just checked that python the which
449:47 - python the python is like the
449:48 - environment python only and then you're
449:50 - installing uh install and then you
449:53 - upgrade the PIP you simply upgrade your
449:55 - pip and then click enter now it upgrades
449:58 - your pep to the latest version which is
450:00 - 23.0 now that warning can be removed so
450:03 - I hope that this gives you good sense uh
450:06 - about what exactly we wanted to wanted
450:08 - to do now we have the working we have
450:12 - the working uh available things out here
450:15 - where we set it up our project now we
450:17 - are we now we are ready to start off
450:19 - with our explanation of the project so
450:21 - uh if you if you open our notes if you
450:24 - open our notes we had this notes out
450:26 - here we had this notes out here let me
450:28 - just move it a bit you have the
450:30 - introduction we have done with the
450:32 - introduction all the setting up
450:33 - everything however we haven't covered
450:35 - this setting up get repository which
450:37 - we'll do at the end node right now so
450:39 - now we have the required instincts now
450:41 - what do will now we have the data so
450:44 - when you when you when you open this
450:46 - data folder you have you have a lot of
450:47 - things available out here you you know
450:50 - you ignore this these two as of now
450:52 - because
450:53 - this is something which will be built in
450:54 - the project only you will see that is
450:56 - the data and this is the original data
450:58 - right so I suggest to go in this data
451:00 - and this data you will find out the data
451:03 - which is available out there so let me
451:04 - just open that so I think uh
451:08 - when you open this in CSV file it will
451:10 - look much more better I have already
451:12 - shown you in the introduction session
451:14 - very very if you go over here you will
451:17 - be having cancer regression dot CSV you
451:19 - will be able to see the data out here
451:21 - okay now over here the first step is
451:23 - will the first step we should see a very
451:25 - simple way where we had a class where we
451:27 - are taking out adjusting the data and
451:30 - second what we'll do we'll start off
451:31 - with understanding our data okay so I've
451:35 - created a very nice notebook where we
451:37 - did a very extensive Ada dealt with you
451:40 - know outliers we deal with a lot of
451:43 - things out here so we'll cover that in
451:46 - re-read it so I hope that and then we'll
451:48 - after the after the after the Eda will
451:50 - cover some pre-processing techniques
451:52 - we'll cover some feature engineering
451:54 - techniques then we'll cover some
451:55 - pre-processing steps Etc
451:58 - I hope that this this guy too super well
452:00 - let's catch up in the next set of
452:02 - lectures to understand the Ed and
452:03 - everything out there so everyone welcome
452:05 - back to another lecture especially on uh
452:09 - some theory part which we'll try to
452:10 - discuss you know some I will start off
452:12 - with this basic Eda and then with the
452:15 - basic idea we'll try to figure out what
452:17 - is the possible you know uh data uh
452:21 - cleaning techniques and data preparation
452:22 - techniques which we have to go forward
452:24 - with right so let's let's start talking
452:27 - about actually about all of these things
452:29 - but before that we're going to review a
452:31 - quick concept which we have to go
452:33 - through is about quartiles right you can
452:36 - read about deciles percentiles in the
452:38 - previous in the lecture start in the
452:40 - probability and statistics part I would
452:42 - like to go through only one concept of
452:44 - over here which is quartiles so what is
452:47 - quartile quartile is something which
452:49 - basically quartal is a part of quantiles
452:51 - where we divide our data into some sort
452:53 - of distribution which are equal sized
452:55 - right or subgroups okay so we divide our
452:59 - data into subgroups right so we have
453:01 - already studied that it divides our
453:02 - distribution into four equal parts so
453:05 - quartiles divides are in distribution to
453:06 - four equal parts
453:08 - uh and there are three quartiles so here
453:11 - assume that you have this data this data
453:13 - so to assume that this is a column two
453:15 - four five six seven eight so divide this
453:17 - data into equal equal parts where you
453:20 - have first first second third and fourth
453:25 - so you'll see that the all are equal
453:27 - equal parts and this is the quartile
453:29 - number one this is quartile number two
453:30 - and quarter number three so quartiles
453:32 - are at the cuts where we divide our uh
453:35 - equal parts right so the first step of
453:37 - is for taking all the quartiles so what
453:39 - is quartile quartile is where we divide
453:41 - a distribution four four equal parts and
453:43 - quartiles are like Q one Q two Q three
453:46 - are the quartiles where which are
453:48 - present at the cuts and they first of
453:50 - all we have to put our data in a ordered
453:52 - format it should not be unordered
453:54 - otherwise it will not work right over
453:57 - here over here you have for your quarter
453:59 - number two is the median quartile number
454:01 - two is the is the medium so let's talk
454:03 - about quarter number one and quarter
454:05 - number three in great detail
454:07 - uh so basically you're seeing and you
454:09 - you're seeing in front of you that you
454:10 - have a date that your distribution of
454:12 - your particular data right and this is
454:14 - hotel number one where and this quarter
454:16 - number two and this quarter number three
454:17 - so given this particular information q1
454:20 - is the central point between your
454:23 - smallest value and the median of your
454:25 - column okay Q3 is the median the Q3 is
454:29 - the median and Q2 is the highest score
454:31 - so basically what given this example so
454:34 - assume that this is this this is an
454:35 - example of your marks you got right this
454:38 - is this is an example so we put in order
454:40 - format now you take out q1 by this
454:42 - formula you take out q1 by this formula
454:44 - you take out um Q3 or sorry uh wait you
454:49 - take out Q3 you take out Q3 which is the
454:51 - middle value by this formula and you
454:53 - take out q1 Q2 Q3 which we have already
454:55 - taken out right and now what q1 tells
454:58 - you q q 1 tells you that 25 of this
455:00 - course are less than 68. so if we have
455:03 - the quarter number one if we have the
455:06 - quarter number one which which is at the
455:07 - position number five which is 68 so we
455:09 - say that there is 25 percent of the
455:11 - values there's 25 percent of values
455:13 - which are less than 68. quarter number
455:16 - two tells there are 50 percent of values
455:19 - which are smaller than which which are
455:21 - smaller which is which are smaller than
455:25 - um you know
455:27 - so whatever median is there so there is
455:29 - 50 value which is smaller than this
455:31 - particular median like 50 values like
455:33 - this 50 and the fifty percent who are
455:35 - greater than this so that's why uh Q2 is
455:37 - called a median which is the middle
455:39 - value we're 50 greater than this and 50
455:40 - smaller smaller than this you have Q3
455:43 - where we say that Q3 which says that
455:46 - which is a 75th percentile or you visit
455:48 - where it contains the top 25 percent of
455:51 - this course which are greater than 84
455:53 - right so basically it is saying that uh
455:56 - whatever is called which are greater
455:57 - than 84 that's what the uh greater than
456:00 - 84 are the uh 25 of the values which are
456:04 - 20 last 25 top 25 percent of the values
456:06 - right so this is also called the 75th
456:09 - percentile where it says that 25 percent
456:11 - of the scores are greater than 84 and 75
456:15 - percent less than 24. so basically out
456:17 - here out this we say that there is 25
456:19 - percent of the score which are greater
456:22 - than 20 which are which are greater than
456:23 - 84 because that's the quarter number
456:25 - three for this particular example
456:26 - greater greater than 20 84 and there's
456:28 - 75 percent of the examples which are
456:30 - less than this okay
456:32 - again I'll repeat quartile number one
456:34 - says that the 25 of values are smaller
456:37 - than this q1 whatever the value over
456:39 - here 50 of the values are small and and
456:42 - why once is the 25 of values are smaller
456:45 - than this particular value and 75 sorry
456:49 - and 75 percent of values which are
456:51 - greater than this particular quartile
456:53 - and you how you can take out you can
456:54 - take out by this formula okay
456:57 - I hope that you're that you're getting
456:59 - one way to interpret it q1 is the middle
457:02 - point q1 is the middle point between the
457:04 - lowest value in your data and the median
457:07 - of your data right median of a data that
457:09 - that that is the because 50 what is the
457:12 - half of 50 because the medium is called
457:15 - the 50th percentile so what is the half
457:16 - is two which is a 25th so you have you
457:19 - have to take the median this is the
457:20 - central point between both of them
457:22 - and Q3 is a central point between median
457:25 - and the largest value maximum value in
457:27 - your data okay this is the one way to
457:29 - interpret it another one which which you
457:32 - are going to talk today is IQR what is
457:34 - IQR IQR it tells us how far apart your
457:38 - first and third quartiles are okay
457:41 - indicating how spread out your 50 of
457:44 - your data is so what it is telling so
457:46 - what it is telling that
457:48 - um
457:49 - um what it is telling that you IQR is
457:53 - the difference between your quarter
457:55 - number three and quartile number one
457:56 - okay where it says that because most of
457:59 - the most of the data lies between these
458:01 - two right because most of most of the
458:03 - data Lies over here right so we're gonna
458:05 - take out the range how how do you take
458:08 - out the range first of all is the simple
458:10 - range is taken out by maximum minus
458:12 - minimum right but that is that is that
458:16 - is that is
458:17 - um not resistance to outliers out yeah
458:20 - we have we have a talk we we had a talk
458:22 - please see the problem lectures please
458:24 - again I'll repeat it is if we use our
458:27 - formal range
458:28 - where we take out the maximum value and
458:31 - maximum and minimum value then it can be
458:34 - told Liars y you can see over here which
458:36 - you have listed it out for example you
458:38 - have this data out here right now you
458:40 - have portal number one 3.5 quarter
458:42 - number two six and quarter number three
458:43 - eight okay which are the cuts so what so
458:47 - we can say that there's 25 percent of
458:49 - the values which are less than 3.5 fifty
458:51 - percent of values which are less than
458:52 - six and eight um with 25 of values which
458:56 - are greater than negative okay so so
458:58 - assume that assume that you can take out
459:00 - IQR IQR which is which is the quarter
459:03 - number three which is the which is the
459:04 - range of the range of the range of the
459:08 - values where your most of the data are
459:10 - present okay
459:12 - where your most of the data present you
459:14 - should think logically as well in this
459:16 - over here
459:17 - it is we are taking out the range of
459:20 - this particular where are most of the
459:22 - data line right where are most of the
459:24 - data lines so they are not likely all
459:26 - outliers outlier are the one where they
459:29 - are out of the distribution where
459:30 - they're out of the distribution right so
459:33 - IQR is the it tells it tells it tells
459:37 - the range it tells the it it it
459:39 - describes the range between these two uh
459:42 - quarter number one and quarter number
459:43 - three where it says that how much your
459:46 - data is
459:48 - um uh you know spread out just like the
459:50 - range where you talk about number three
459:52 - minus quarter number one because we just
459:54 - take out the range of this particular
459:56 - part of the data that's why score number
459:57 - three minus quarter number one V2
460:00 - so you get 4 and range S7 but why we
460:03 - don't use range why we don't use a range
460:07 - because let's take an example we
460:09 - introduce 100 over here 100 over here so
460:12 - 100 minus 2 which is 98 right but but we
460:16 - if we take a but quarter but quarters
460:18 - will not change over here the portals
460:20 - will not change so our IQR will be
460:22 - resistant to even these outliers but
460:24 - range will be affected right so that's
460:27 - why so that's why where are most of the
460:28 - data are present we take the range of
460:30 - that that's what the interquartile range
460:33 - I hope that that gives a good good sense
460:36 - of what exactly we wanted to have a talk
460:38 - on now what I'll do in the next uh after
460:41 - the review of this we'll talk about a
460:44 - very nice stuff which is box plots so uh
460:48 - let's go into the next lecture and talk
460:49 - about something known as box plots
460:52 - so hey Vox welcome to the another
460:55 - lecture on box plots so uh eventually we
460:58 - have I think we haven't had a good talk
460:59 - on box plus and all but it's very easy
461:02 - relatively easy if you know about
461:04 - quartiles and all right so assume that
461:07 - assume that you have a data assume that
461:09 - you have a uh data out here so let let
461:12 - me just first of all give the title as
461:14 - box plots right box plots I know my
461:17 - handwriting is not good but that's
461:19 - something which you have to bear with
461:20 - and we'll extensively use this box plot
461:22 - to do video night to to do a nice thing
461:24 - okay
461:26 - um so basically assume that you have
461:27 - this data 18
461:29 - 34
461:31 - and then you have 76 and then you have
461:35 - 29 then you have 15 then you have 41 46
461:40 - 25 54.
461:43 - 38 20 32 43 and 22. so you have this
461:49 - data available out here now the first
461:52 - step as I told the first step
461:54 - the first step is to sort it all right
461:59 - in an order so when you sort it in an
462:02 - order you'll have 15 okay let me let me
462:05 - use another pen
462:07 - um assume that you'll sort it on another
462:09 - order which is 15 18 then you have uh 15
462:15 - 18 is 20 then you have 22 then you have
462:19 - 25 then you have 29 then you have 32 and
462:24 - then 34 38 41 43 and 46.
462:29 - right so this is our assorted data now
462:32 - so now once we have the sorted data now
462:35 - um and then we have 54 sorry uh then you
462:38 - have 50 equivalent 76 so now we have
462:41 - this sorted data now what we need to do
462:43 - right so now once you have that sorted
462:45 - data we're going to take out the
462:45 - quartile number one quarter number two
462:47 - and quarter number three right that's
462:49 - our goal to take out so how can we take
462:52 - out this as I said divide your data into
462:55 - equal equal parts first is divide your
462:58 - data into equal equal parts so first of
463:00 - all let's find the middle value so
463:02 - middle value so first of all let's let's
463:03 - count the number of values one two three
463:05 - okay so let me just one two three four
463:09 - five one two three four five six seven
463:11 - eight nine ten eleven twelve thirteen
463:13 - fourteen so you have an even number of
463:15 - values right so that's something so
463:18 - basically I've already conducted it so
463:19 - when you take out the median of even
463:21 - then it then then I think you should
463:24 - tell me the formula in your Discord so
463:26 - go over there and tell me the formula
463:27 - for median you should you should be able
463:29 - to have already covered this
463:31 - um so the the median will come out will
463:33 - come which which is over here which is
463:35 - the quarter number two which is around
463:37 - 33. so the values over here are the the
463:41 - values at this side on the left left
463:43 - side are the 50 of values which are
463:47 - smaller than 33 and the value switches
463:49 - over here are greater than 33 which is
463:51 - the 50 of the values I hope that that
463:54 - makes sense now so now once we have this
463:57 - quartile quarter number two now we have
463:59 - to go forward to quartile number finding
464:01 - one so quarter quartile number one we
464:03 - have to find we can use that formula to
464:05 - find the index I have already used it so
464:07 - this particular is the quartile number
464:09 - one this particular is quarter number
464:11 - one this particular squatting number one
464:12 - right and now and now we we want to find
464:16 - the Portal number three which is over
464:19 - here which is over here which is
464:20 - volatile number three so now you see the
464:24 - that we have divided our data set into
464:26 - four equal parts one two three and four
464:29 - right one two three and four so we have
464:33 - already divided our data into four equal
464:35 - parts so now your portal number one is
464:37 - quarter number one is twenty two quarter
464:39 - number two is which is the median which
464:41 - is 33 and quarter number three which is
464:43 - 43. so you have fifty percent of the
464:45 - values which are smaller than twenty two
464:47 - sorry twenty five percent of values
464:49 - which are smaller than twenty two fifty
464:51 - percent values are smaller than 33 uh
464:54 - and then and then there's 43 or sorry 25
464:57 - percent of the values which are greater
464:59 - than 43 okay so currently you have this
465:01 - particular example uh with its statistic
465:03 - we which you have now what what you want
465:06 - to do as I told let's take out the IQR
465:09 - as well IQR right uh what is IQR IQR is
465:12 - the range where all the data lies in
465:14 - right which is Q3 and minus q1 where all
465:17 - your data lies in which is 50 of the
465:19 - data which is 43 minus 22 which will
465:22 - give 21 okay so that's your IQR so
465:24 - that's your IQR now what we need to do
465:27 - now what what you need to do okay first
465:31 - is we can go forward we can go forward
465:34 - with taking out the maximum you know we
465:37 - can pick the minimum and then take out
465:38 - the maximum and then make a box plot out
465:41 - of it but for outlier identification for
465:44 - outlier identification we need to make
465:46 - whisker whisker okay we're going to make
465:49 - whiskers so what is Whiskers Whiskers
465:52 - gives you that a maximum a maximum and
465:56 - the minimum value in your data
465:58 - and a maximum and the minimum minimum
466:01 - value data according to the IQR range
466:03 - right according to the IQR range so it
466:06 - gives the maximum and the minimum value
466:09 - in the data according to your list value
466:12 - 21 according to this Mass so basically
466:14 - we have the maximum and then if you say
466:17 - if any Delta points go above this
466:19 - maximum then that's an outlier or any
466:22 - data points which go below this minimum
466:24 - then that's also an outlier so for
466:26 - outlier and the identification with
466:28 - respect to our IQR because IQR is
466:30 - resistant to outliers so we're going to
466:32 - take out so we have a viscous which
466:34 - gives us the
466:36 - um
466:37 - uh the the maximum minimum values which
466:41 - are also resistant to our data which
466:42 - which helps under the detection you will
466:44 - get to know just wait for a second
466:46 - second now we want to take out the
466:48 - maximum and a minimum value so minimum
466:50 - value can be taken out by the minimum
466:52 - value can be taken out by Q um I think
466:56 - we can take out q1 minus 1.5 times IQR
467:02 - right that's the minimum value that's
467:05 - the minimum value and then maximum value
467:08 - can be taken out Q3 Q3 plus 1.5 IQR okay
467:13 - so this is the formula for minimum value
467:16 - for taking the minimum value in the date
467:18 - of with respect to this out IQR range
467:20 - and this is for maximum now you may ask
467:24 - how did how does this came this is
467:26 - something which you can go forward even
467:27 - if you want to research how does this
467:28 - derived but that's a very general uh
467:30 - explanation behind this but that's not a
467:32 - scope on this my scope on this to tell
467:34 - you that we want to find out the maximum
467:36 - minimum values with respect to the
467:38 - outlier range if there was simply a
467:40 - range or something you would have just
467:41 - taken the minimum and maximum values
467:43 - right but that cannot be used as the
467:45 - outlier I'll tell you the at last why
467:47 - why we cannot use the minimum maximum
467:48 - why we need to take out the minimum
467:50 - Maxima with respect to IQR so if we do
467:52 - the calculation to 21 minus 1.5 times
467:56 - your IQR which is 21 and then comma
467:59 - because that's for minimum and this for
468:01 - Q3 which is 43 plus 1.5 multiplied by 21
468:05 - you get you get nine points minus nine
468:08 - point minus 9.5 and then you get 74.5 so
468:12 - this is your minimum value and this is
468:14 - your maximum value so if anything goes
468:16 - above this sorry if anything goes below
468:18 - this minimum if any data points go low
468:20 - go below this data point we say that 9.5
468:22 - minus 9.5 if any goes above this this is
468:25 - called the upper limit and this is
468:26 - called the lower limit so if any any
468:27 - goes above this we say that that's
468:30 - thoughts on outlier so let's try to do
468:31 - with the help of a diagram so let's try
468:34 - to do with the help of a diagram so I'll
468:35 - make a very nice diagram right now
468:38 - uh so assume that you have a diagram
468:40 - over here you have the diagram over here
468:42 - a number line so let me just make a
468:44 - diagram like this and let me just use
468:47 - another pen yeah so we have zero then
468:49 - you have 20
468:52 - uh 30 40.
468:55 - 50 60. let's make a let's extend this
469:02 - uh 70
469:04 - 80 right so you have time so you have
469:07 - this num num number line out here now
469:10 - now basically what what was our outlier
469:13 - what was the outlaw our outlier our
469:15 - sorry what was our quartile the quartile
469:17 - number one was 23. number two was what
469:20 - 33 and quarter number three was 43 so it
469:24 - is slightly over here so you simply make
469:26 - something like this
469:29 - okay I know this something is not great
469:31 - but this is this is what it looks like
469:32 - and this is your portal number so now
469:35 - this particular is quarter number one
469:37 - this particular is quarter number two
469:39 - and this particular is quartile number
469:42 - three as it makes it it is 22 it is I
469:46 - think uh 33 and this is 43 okay so you
469:49 - have these three outer quad uh quartiles
469:52 - as of now which I've already seen
469:54 - now we wanna we wanna now one thing
469:57 - which we can do we can simply say that
469:59 - something like this we take out the
470:01 - minimum and a maximum and then just add
470:03 - it for example in this case the minimum
470:05 - is in this case the minimum is a 15 so
470:09 - we can have over here you know we can
470:11 - have like this minimum case and then
470:13 - what is the maximum maximum is 76
470:16 - maximum is 76 so we can have uh
470:19 - something like this okay
470:22 - we can have something like this but it
470:24 - but I'll tell you why it will not work
470:26 - first of all you see that 76 is out a
470:28 - bit bit out of the distribution is bit
470:30 - outlier just by seeing it right so so
470:33 - I'll tell you why why it will not work
470:35 - first as we said as we say that IQR is
470:39 - resistant to outliers and IQ represents
470:41 - the 50 of our data so if we take out the
470:44 - maximum minimum value using IQR that
470:46 - makes makes more sense because we'll be
470:48 - able to identify where are most of data
470:49 - lies so if any of this if any of the
470:51 - data goes out of that light spreading
470:54 - out we'll consider that as an outlier
470:56 - right so now so now I think it will
470:59 - makes more sense if you just try to now
471:00 - mix and make make it more sense sense
471:02 - sensible yeah so now we wanna we have
471:05 - this minus 9.5 we have this minus 9.5
471:08 - where we say that where we say that we
471:11 - have we make a whisker we make a whisker
471:13 - we make a whisker which is minus 9.5
471:15 - which is minus 9.5 so and then you have
471:18 - the 770 I think what was that yeah
471:22 - it will be something over here so
471:24 - anything anything which is uh now now
471:27 - anything which is which this is for this
471:29 - is for the maximum it's for the maximum
471:31 - value and this is what the minimum value
471:33 - okay so maximum and this is for minimum
471:35 - value okay so anything anything any art
471:38 - data so how can we use this to remove
471:40 - our data how can we use this to remove
471:43 - outliers from our data so we say that
471:45 - anything below minus 9.5 anything which
471:47 - below minus 1.5 is outlier ending above
471:50 - minus above 74.5 is outlier because this
471:53 - is the distribution this is the range of
471:55 - our of our maximum data where are most
471:57 - of the data so anything above this so if
471:59 - we see our data anything below minus
472:01 - sign to 9.5 there's no anything below my
472:03 - n minus but there is
472:05 - um above 74.5 which is 76 so we simply
472:08 - say that we consider this particular
472:10 - thing as an outlier this particular
472:11 - thing as an outlier okay this particular
472:13 - thing is an outlier
472:16 - this particular thing is outlier and we
472:18 - remove this data from our plot right
472:21 - from our data so we remove that outlier
472:24 - right there you know the linear
472:26 - regression that other that lots of
472:28 - algorithms that are sensitive to
472:29 - outliers okay so you have to remove
472:31 - these outliers we'll we should study
472:33 - later on as well so we'll remove this
472:35 - outlier by a box plot where I'll some
472:37 - summarize the concept you take out the
472:39 - you take an order you take out the
472:40 - quartiles you take out the IQR you take
472:42 - the maximum minimum value by this
472:44 - formula by this formula which is which
472:46 - is out here q1 minus 115 Times Square
472:48 - IQR Q3 plus 1. maximum is for minimum
472:51 - anything above this is bad anything
472:53 - above this is outlier anything below
472:54 - this is outlier I hope that makes sense
472:56 - let's catch up in the next lecture to
472:58 - actually see the Practical effect and
472:59 - how we actually do it to identify
473:01 - outliers in your data set hey everyone
473:03 - welcome back to another video
473:05 - um so now what we'll do we'll start off
473:07 - with you know Eda stuff and uh
473:10 - processing stuff which we eventually
473:12 - need for a project and I'll make sure
473:14 - that you understand each and every
473:15 - Concept in great detail
473:17 - so you can actually read all of this
473:19 - thing which which you have done done it
473:20 - for you but let's start off with uh we
473:23 - have we are done with the injection of
473:24 - the data now let's start off with this
473:25 - file don't know what is what's this file
473:27 - don't worry we'll figure it out
473:30 - the first line of the code states that
473:31 - the first line of the code says that we
473:33 - ingest the data from our data ingest
473:36 - file so it goes in data and just file it
473:38 - ingests this class so if you click over
473:40 - here if you click over here you'll be
473:41 - going to this ingest data class and over
473:43 - here in this class you have the init
473:45 - method which is a Constructor and then
473:46 - you have a method which is get data
473:48 - which will where we ask for the path of
473:50 - the data where the data is present in
473:52 - your folder and then we read the CSV if
473:54 - it is a CSV file we read that okay and
473:57 - then we return the data frame right so
474:00 - let's see it goes and changes data it's
474:02 - it just Imports that class
474:05 - then it goes in processing data it goes
474:09 - in processing file and then it imposes
474:11 - the following columns which we'll take a
474:13 - look in a bit you have to just wait for
474:15 - a sec right which will take a look in a
474:17 - bit
474:18 - and then you have for from future
474:19 - engineering we are import where there is
474:21 - another feature engineering file where
474:23 - importing some functions which is
474:25 - important to us as of now right which is
474:27 - important to us as of now right so now
474:29 - you see over here we have ingest data we
474:31 - have ingest data and it's we are uh we
474:35 - are you know what what we are doing we
474:36 - are simply
474:38 - um
474:38 - instantiating a class and then using
474:41 - that class as we said we are using the
474:43 - method of the class if you see that
474:44 - method methods get data over there so
474:46 - using the method in just database is the
474:48 - object which is the blueprint of the
474:49 - class we say that we will get the data
474:51 - from the following uh from from the
474:53 - following folder so we go to OLS we go
474:56 - to data and then simply copy the
474:58 - relative path of it right and then we
475:01 - import that and then now once we have
475:02 - the data now it's ready to start with
475:04 - some of the processing which is required
475:05 - as of now but before processing uh what
475:08 - what I usually like to do I usually like
475:10 - to you know go through all the basic
475:12 - data you know see how how's the data
475:15 - looks like you know see some of the
475:16 - missing values if if I have any and talk
475:19 - about distribution right so let's let's
475:21 - before going on the distribution one
475:23 - let's before going on a processing one I
475:25 - would like to take a time to explore a
475:27 - bit of you know explorated data analysis
475:30 - and Analysis and all so what's what was
475:32 - our project our project was to identify
475:34 - the number of debts which was due to the
475:37 - cancer in a specified population for
475:39 - over a period of a Time typically one
475:41 - year so mortality rates are nothing but
475:43 - a number of deaths are good for a period
475:44 - of time in this case we have we have
475:46 - given already in this competition you
475:48 - have to build a multivariate least
475:50 - squares competition to predict the
475:52 - cancer mortality rates for U.S countries
475:54 - right the first step is of course we are
475:56 - going to import the libraries of course
475:58 - sometimes I have imported in a bit
475:59 - between as well so I'll do it as we go
476:01 - along so we import our libraries as of
476:03 - now we need pandas and numpy we pfpd as
476:06 - an ally so that we don't have right
476:07 - pandas dot every time to use Panda so we
476:09 - have PD where we can use this PD now one
476:11 - thing that you which would do using a
476:13 - way we are not importing that class just
476:14 - for sake of you know Simplicity I just
476:16 - added it you know we can give it to any
476:17 - people which is for Simplicity however
476:20 - we can use that we can simply import the
476:22 - you know from data from data ingest from
476:26 - data ingest and then we simply import
476:28 - the simply import the ingest data file
476:31 - and then we sing assembly maker you know
476:34 - um interest data which is I'm just going
476:36 - to comment it out you know ingest data
476:39 - which is in Jets data and then same
476:41 - simply say key uh data is equals to
476:44 - ingestinal so you simply have this to
476:47 - actually import the data but as of now
476:48 - just important that because of
476:49 - simplicity so that you you could be on
476:51 - the same page rather than going over
476:52 - this so if anyone is seeing ADH they
476:54 - should be able to understand it very
476:55 - well so now once we have the data so now
476:58 - let me start running it so before that I
477:00 - would like to just tell you that we have
477:02 - to over here you have to activate your
477:04 - environment in jupyter Notebook as well
477:06 - in in uh in vs code we have this we have
477:10 - this select selecting of our kernels but
477:12 - in vs code you can simply write you can
477:14 - simply write like this
477:17 - a source and then you simply have the
477:21 - following command which we have used
477:22 - earlier so now you see that we are using
477:25 - source and then it is activating it so
477:27 - you can simply use this as well you can
477:29 - simply use the this as well source and
477:31 - then use simply give the activity it
477:32 - will be able to activate for that
477:34 - particular environment however you can
477:35 - change the kernel as well or call in
477:38 - your vs code if you have issues if
477:39 - you're having issues over there please
477:41 - let me know I'm just deactivating
477:43 - because sometimes you know conda I have
477:45 - also a conda installed so I have to um
477:47 - it's my system thing which I have to
477:48 - deactivate it doesn't really matter it
477:50 - is a base thing but I deactivate for a
477:53 - safer thing but as of now you just
477:55 - simply Source then you have uh which
477:57 - which you have your source and then you
477:59 - simply activate it okay and then also
478:02 - this uh base base also gets activated
478:04 - but it will it mean it may be not Vision
478:06 - but but it is not going from here it is
478:08 - my system things I don't know what's
478:10 - what what happens but I think I need to
478:12 - uninstall this Honda but never mind on
478:15 - that but uh you can you can first of all
478:17 - will activate that environment whenever
478:19 - you enter the project First Step
478:20 - activate that environment because that's
478:22 - what the project right once you have
478:24 - activated the environment now you should
478:26 - be able to run it so let me run the
478:27 - first first cell I usually use vs code
478:30 - because it's much more simpler so it
478:32 - says that for vs code we are going to
478:34 - download VR4 for this particular
478:35 - environment we have to also download the
478:37 - IPI ipinel IPI kernel however however if
478:41 - you're using vs code you should you
478:43 - don't really need this okay you don't
478:44 - really need
478:45 - um to download every time but I don't
478:48 - know but yeah for free score we have to
478:50 - now it runs now it runs now you see that
478:54 - it takes some seconds to run it and then
478:56 - we simply say achar now let's let me
478:58 - import my data so this is what your data
479:00 - looks like this is what your data looks
479:02 - like where you have the 34 columns and
479:05 - five rows where it where we are asking
479:06 - to show the five columns you can also
479:07 - specify on a series 10 or 5 it is just
479:10 - for the top five columns
479:15 - is a target variable the death rate in
479:18 - that particular span incident rate
479:20 - median income rate uh Power type study
479:23 - case study recap bind dink which is and
479:26 - media range so you see that this is
479:27 - something unusual you have to work on
479:29 - this so I'll come back like it's
479:30 - something you have Nan values OMG you
479:32 - have missing values available you have
479:34 - you also have you know
479:36 - um what you also have if you've explored
479:38 - the data if you explore the data you
479:41 - also have you know over here which is
479:42 - the English which is the categorical
479:45 - values which is this text value so how
479:47 - we are going to deal with that right
479:48 - missing values categorical values in in
479:52 - type of interval in type of interval
479:54 - type of thing how are we going to deal
479:56 - with this because we cannot just view it
479:57 - to them because model will make because
479:58 - this this acts as a string right now so
480:01 - that model will not make sense so
480:03 - there's couple of things which have to
480:04 - take care missing values right the first
480:07 - is missing values second is over here
480:09 - which we see that this is something
480:11 - unusual which is something unusual where
480:13 - you are the lists in every written
480:14 - column and then you have you know um
480:17 - text variable okay object variable we
480:21 - can use simply data dot info to take out
480:23 - things out here to take out who
480:25 - geography is in a string and then you
480:27 - have enough all all values are float
480:29 - Target there is float and then you can
480:31 - take out the null and then it gives you
480:32 - good information
480:34 - so now let's take out so now once we are
480:36 - done with exploration you can also take
480:38 - out describe you can also write describe
480:40 - so
480:43 - [Music]
480:44 - um
480:45 - data
480:47 - dot describe
480:52 - and then over here when you do the
480:53 - describe then what it does for every
480:56 - column it describes some of the useful
480:59 - statistical analysis for average a n
481:01 - there are 3047 count with the number of
481:04 - values in that particular column what is
481:06 - the mean mean is 606 is standard
481:09 - deviation 14 and 16 minimum value in
481:11 - that column 6 25 of the values which is
481:14 - 76 where we see that there is this 25
481:16 - percent of the percent of the value
481:18 - which is smaller than 76. 50 value which
481:21 - is more than 100 171 and 75 percent of
481:24 - the and less rest 20 and more than and
481:27 - so and 25 percent of the values which
481:29 - are greater than 518 or we can say 20 75
481:31 - percent of values which is lesser than
481:33 - 518 all right and maximum value in that
481:36 - column is 385150 so you have now for
481:39 - every column list list it out here
481:42 - and then now now once we have the basic
481:45 - things and now what we do we simply go
481:47 - ahead and simply take out some of the
481:49 - ask because we have noted on three three
481:52 - things that bin uh that binwala thing
481:54 - and then you have Nan values right and
481:57 - the categorical so let's take out let's
481:58 - take a look what number of missing
482:01 - values which we have in which we switch
482:02 - columns so if you run this if you run
482:04 - this we say that null values we we put
482:07 - the null values and then we say data dot
482:09 - is null dot sum so what does this gives
482:12 - um
482:13 - uh this gives
482:15 - um you know when when you print out
482:16 - normal values this should give you know
482:18 - this every column with the number of a
482:20 - null values present out there with the
482:22 - number of present or not there are only
482:23 - three columns where your null values are
482:24 - present there are only three columns
482:26 - which are where your
482:28 - um null values are present so we say
482:30 - that you print the null value you print
482:32 - the null from from the null values from
482:34 - that series take the null values take
482:36 - the null values whose known as great
482:39 - greater than zero right so we simply
482:40 - print that
482:42 - and then we see there are three columns
482:43 - post ECT and over here so now now there
482:48 - are several ways to deal with missing
482:50 - values which we'll talk about in later
482:51 - videos so I'll give you quick solution
482:53 - in this but we'll also talk about there
482:54 - are several ways jnn computation simple
482:57 - imputed blah blah blah so there are
482:58 - several techniques for dealing with
483:00 - missing values with their pros and cons
483:01 - which will deal with in very detail that
483:04 - we have now once once we have this let's
483:06 - worry about other things let's let's
483:08 - start exploring each and every variables
483:10 - so once you have the basic you know
483:12 - missing values exploration of the data
483:14 - now let's go ahead and talk about how we
483:17 - can
483:18 - um uh now you have to analyze hnl
483:21 - variables in your data to further
483:23 - analyze your data so now you may be
483:25 - thinking hey ayush how are you coming up
483:26 - with these these things the first step
483:28 - is an in anything is to do a basic
483:31 - exploration okay have a template basic
483:33 - exploration where you talk about what
483:35 - are the number of there is there any
483:37 - missing values is there what is the data
483:39 - types available for every column what is
483:41 - the maximum minimum values what what
483:43 - values are greater than this odd value
483:44 - is smaller than smaller than and the
483:46 - next step should be to analyze each and
483:48 - every variable okay each and every
483:50 - variable out here that will give you a
483:52 - very good set of thing first is we are
483:54 - going to analyze our Target death rate
483:57 - we're going to analyze our Target
483:59 - variable right target variable in which
484:01 - we say that
484:03 - plot.histogram you're plotting the
484:04 - histogram bins should be 20 color should
484:07 - be blue and H should h of the S2
484:09 - histogram should be black and then we
484:10 - say on x-axis label that as a death rate
484:13 - on y-axis river that frequency and then
484:15 - title should be histogram of the day
484:17 - three and then we show that Plot show me
484:19 - the plot right now that's what it is
484:21 - saying and I'll talk about what is the
484:23 - entity interpretation of this so when
484:25 - you when you actually take out the
484:26 - histogram of this target death rate you
484:27 - take out the histogram of this
484:29 - particular column A of this particular
484:31 - column this will result in this
484:32 - particular histogram so you may have
484:34 - high time in uh interpretation of it so
484:36 - let's talk about interpretation of your
484:38 - histograms
484:40 - the values which are grouped into bins
484:43 - along the x-axis so the values the
484:45 - values are grouped along with bins so
484:47 - you see over here you see over here on
484:50 - the values on x-axis they're grouped in
484:52 - bins they're grouped in bends so uh we
484:55 - will talk we'll draw we'll talk about
484:57 - the bins first of all uh as of from
484:59 - which you see over here in this
485:00 - histogram it shows all the threads which
485:02 - are date divided in the bins and y-axis
485:04 - on the in the counties you know the
485:06 - frequencies in that bill okay so we say
485:09 - that zero two hundred zero two hundred
485:11 - is the I think uh fifty two hundred not
485:14 - zero two hundred uh zero two hundred is
485:16 - the sorry 5200 is one bin it's 500 to
485:19 - 150 is another but over here you might
485:22 - have noticed that we have 20 bins as of
485:23 - now but if you notice it very nicely
485:26 - that is this the the the range of this
485:30 - the you see you see the mouse right now
485:32 - you see the mouse they are where I'm
485:35 - indicating so this is 20 you know 20
485:37 - bits so we say that that in this
485:39 - particular bin what are the the number
485:41 - of frequencies in this for this
485:43 - particular bin what is the frequency
485:45 - right for this particular bin what is
485:46 - the frequency so that's what it is
485:48 - telling that it shows all the trades in
485:50 - x-axis with the bins over here we have
485:52 - 20 20 20 bins uh 20 20 and then another
485:57 - 20 right and y-axis is the counties in
486:00 - that bend so for the for this particular
486:02 - bin this will be the counties on WE
486:04 - simply have this 100 we have this this
486:06 - bounties for this we have this much
486:08 - versus 550 around for this particular
486:10 - okay so for for 150 to maybe let's take
486:14 - an example of this particle bill so we
486:17 - have this uh
486:18 - but if you have a type of thing so now
486:20 - this is the simple interpretation of
486:22 - histogram which you've already seen but
486:24 - but it is but now once you have the
486:25 - basic understanding of histogram what
486:28 - will how this will help us right first
486:31 - it help us to identify ourselves how
486:33 - this help us to understand a particular
486:34 - variable in terms of statistical format
486:37 - first it will help us to identify what
486:39 - is the center of the data what is the
486:40 - spread of the data and what is the verb
486:42 - is eventually what is the range and what
486:44 - is the shape of the data right so let's
486:47 - talk let's answer these three questions
486:48 - what is the center of the data but but
486:50 - before that lets to note something
486:52 - number line spams from the minimum value
486:55 - to maximum value the number line is
486:58 - broken into equally sized intervals you
487:01 - see that all are equally sized these are
487:03 - nothing but called bins covering the
487:05 - range of values over here it is 150 150
487:10 - to this will be uh maybe we have 20
487:13 - bands so in this particular we have all
487:16 - the values all the values which lie in
487:18 - this range page right which lies in this
487:20 - range okay
487:22 - um there will be where is the number
487:23 - line is broken equal called bins and it
487:25 - have the every win has the range of
487:27 - values in that a histogram shows how
487:30 - frequent a values occur in that
487:31 - particular bin so his histogram it shows
487:34 - how what is the how frequent the the
487:37 - range of values occurs in that
487:39 - particular how frequent on bikes is for
487:41 - this particular bin there is uh 6780
487:44 - that is the frequency right so that says
487:47 - the frequency the height of each bar
487:48 - represents the number of values in the
487:50 - data set that fall within a particular
487:52 - bin and when the the higher you know the
487:56 - higher number of you know thing is and
487:59 - then you have y-axis which is the
488:00 - counter number though which is to be
488:02 - discrete which which represents the
488:03 - amount of data Falls in that frequency
488:05 - of course and the histogram make it easy
488:07 - to see which values are common and which
488:09 - values are less common or which range of
488:11 - values are less common in your data set
488:13 - okay yes now now once now once we have
488:17 - that let's go to some of the
488:19 - observations so I've linked the resource
488:20 - which is good for you uh first
488:23 - observation is it is slightly positively
488:25 - skewed data set so if it is not
488:27 - completely normal it is very very
488:29 - slightly positively skewed data set
488:31 - right and then there are outliers as
488:34 - well yeah so you see that over here you
488:37 - see over here you see over here you see
488:40 - over here say this these are out of
488:41 - distributions and this outliers nice so
488:44 - we got an idea this is an SQ data set or
488:47 - slightly and you have the outliers and
488:50 - this is also not this is not a
488:52 - completely normal distributed this is it
488:54 - is not completely normally distributed
488:56 - and is mean is greater than median in
488:59 - slightly positively skewed data set your
489:00 - mean is greater than median and you have
489:03 - three peaks in the data set so your buy
489:04 - model data where you have three peaks in
489:06 - your data set right for that particular
489:08 - range this is the high this is one two
489:10 - three these These are three picks that's
489:12 - why we say buy model data but the most
489:14 - important key takeaway which I'm going
489:15 - to take from here which I'm going to
489:16 - trans translate is the is we have this
489:19 - The Columns which is slightly secured we
489:21 - have the columns which is normally
489:23 - distributed we have the outlier in that
489:25 - column as well
489:27 - like however
489:29 - um we are not going to uh we have we we
489:31 - will just take care later on about the
489:33 - outlines but so that that is for this
489:34 - particular variable so we do this for
489:36 - every variables but that for example if
489:39 - a thousand variables how are you going
489:40 - to do it that something is a uh talk to
489:42 - talk on specifically but I hope that
489:45 - this histogram gives you a nice sense
489:47 - about what exactly we're going to deal
489:48 - with the next so next what we're going
489:50 - to do as we've identified we about lies
489:52 - we're going to use box plot to see
489:54 - whether we really have outliers in a
489:56 - great sense right so we're going to use
489:57 - that and then have a have a very nice
489:59 - understanding of that and for every
490:01 - variable we'll do the nice exploration
490:03 - and then we'll learn about outlier
490:05 - detection removing and lot of things to
490:07 - talk on you know there's a very lot of
490:09 - things to talk about so let's go and go
490:10 - to the next lecture and talk about that
490:13 - so everyone let's get started with our
490:16 - data processing techniques which we
490:18 - wanted to so you will find a file uh
490:21 - which is data processing test and data
490:23 - processing data processing test is a
490:26 - file where we are testing our all our
490:28 - data processing you know
490:30 - um our methods out there so we'll try to
490:33 - uh go through go through a step by step
490:35 - what exactly we are doing and why we are
490:38 - doing that
490:39 - right so first of all let's get started
490:41 - with what exactly things which you're
490:43 - going to use first we ingested the data
490:46 - we get the data we imported data using
490:48 - CSV file and then what you're doing
490:50 - you're finding the constant column so
490:52 - this is not appropriate for every case
490:54 - so basically this fine consists constant
490:56 - columns what this does this function
490:59 - takes a data frame and it turns the
491:01 - columns that contains a single value so
491:03 - if your data frame if you if your column
491:06 - has only one single value that's the
491:07 - very very bad thing which can happen
491:09 - right so it will remove it will find the
491:12 - columns which is constant right which
491:14 - has the single values which can indicate
491:16 - either of categorical variables or the
491:19 - variables which does not provide any
491:21 - sort of values to your um to your data
491:24 - right so we find the constant values
491:27 - constant which is that a column column
491:29 - set contains only a single value either
491:30 - a11 either 222 or whatever
491:33 - so constant columns you make a v we we
491:36 - make a list to empty less constant
491:38 - columns and we iterate through the the
491:40 - column state of range rate data frame
491:42 - dot column where we ask for the data
491:43 - frame as an input to the function and
491:45 - then we iterate to the columns and then
491:46 - we say that find the number of unique
491:48 - values in that column by using using
491:50 - unique method and then if that unique
491:53 - method is greater than one then append
491:55 - that particular column in this constant
491:56 - columns which are which which we are
491:58 - doing over here and then we are
491:59 - returning at the end the the constant
492:02 - columns right so that's the first step
492:04 - which which we are doing over here so it
492:06 - says the columns that contains a single
492:07 - value and then print out the constant
492:09 - columns and then let's try to run it so
492:12 - uh whatever what exactly what I'm going
492:14 - to do is Maybe
492:16 - go to run my you know uh interactive
492:21 - window so I'll just run my interactive
492:22 - window so we have OLS activated in this
492:25 - as well so you see our act in Virtual
492:28 - environment which is OLS is activated in
492:31 - this now so we have that let's wait for
492:34 - a few seconds then it should work really
492:36 - fine so over here which you're seeing in
492:38 - front of me is it is running uh the 30
492:41 - lines of code which I've selected you
492:43 - might use Jupiter notebooks or you might
492:45 - run the whole notebook step by step the
492:48 - the way I like is interactive you can
492:50 - also go online and search about virtual
492:52 - uh Visual Studio code interactive
492:54 - notebook you will be able to find out so
492:56 - if if you go and then search out uh
492:59 - virtue sorry Visual Studio code
493:03 - interactive
493:04 - interactive notebook
493:08 - then if you go and see then you'll be
493:10 - able to see that python interactive
493:12 - notebooks where you can actually
493:14 - download each and everything and then
493:15 - run the run that thing interactively by
493:19 - select selecting the number of uh cells
493:21 - so now it says the columns that contain
493:23 - the single value is nothing so you have
493:25 - a so so you have a column which has at
493:27 - least more than one value so at least it
493:29 - has a good spread so that's that's
493:30 - something which you don't have to worry
493:31 - about sometimes it may happen that your
493:33 - columns can be a single value and then
493:35 - you want to take a look if your data set
493:37 - is too large enough
493:38 - now you go to columns which contain the
493:40 - thing a very few few number of values so
493:42 - I've also written delete constant
493:44 - columns so these uh this what what this
493:46 - particular method does this drops the
493:48 - columns which are constant but in this
493:50 - case we don't have any constant columns
493:52 - now we search for columns which is
493:54 - um with few unique values where we see
493:56 - that function takes in a data frame and
493:58 - a threshold as an input like what number
494:00 - of constant values and the return so for
494:02 - example some columns may have few and
494:04 - few fewer number of
494:06 - values in that column so we say that we
494:09 - what we do we simply say that we make an
494:11 - empty list and then we Loop then we
494:13 - iterate through all the columns you know
494:14 - date data set and then Define the unique
494:16 - values for every uh data frame and then
494:19 - we say if the unique value is smaller
494:20 - than threshold so for example you say
494:22 - that you want to take a look at the
494:24 - columns which has which has less than
494:26 - three less than three unique values in
494:29 - that particular column right so if if
494:31 - that is smaller than threshold then it
494:33 - says that append that column so you
494:35 - basically have the columns which has few
494:37 - fewer fewer unique column so it should
494:39 - also give you a empty slide which you're
494:43 - seeing if you print this out
494:46 - so I can just print it print this out it
494:47 - works like a Jupiter notebooks so it
494:49 - says that there is there is no such
494:50 - column so the even few unique values
494:52 - with a threshold 10 okay with that
494:55 - threshold then you can you can change
494:57 - your threshold so similarly I have
494:59 - several methods for as a template for
495:01 - you to make this work in your further
495:03 - projects like find duplicate the rows so
495:06 - it may it may sometimes happen that you
495:07 - have a duplicated rows as well so you
495:09 - can actually delete the duplicate rows
495:12 - by using these two methods where you
495:13 - first of all find the duplicated rows by
495:15 - finding dot duplicated and then you
495:17 - simply drop the duplicates by keeping
495:19 - the first duplicate so so you might have
495:22 - first duplicate second duplicate third
495:24 - where the second and third are the same
495:25 - as first then it keeps the force and
495:27 - drops the second and three right so you
495:30 - delete the duplicate Etc so you have
495:32 - couple of methods now here but but we
495:34 - are not but we are not using them so now
495:36 - you have the basic you know basic things
495:38 - which is just for us exploring things
495:40 - there is lot to left in data processing
495:42 - now we have all of this left now what
495:44 - what we'll do we'll do some or sort of a
495:47 - basic feature engineering or feature
495:48 - construction okay that's called feature
495:50 - construction rights feature splitting
495:52 - and construction this whatever I'm going
495:54 - to teach right now is called feature
495:55 - construction feature is splitting so
495:58 - let's do that in the next video
496:02 - so now what we're going to do we're
496:03 - going to talk about something known as
496:05 - feature construction and features
496:06 - splitting in our data so one thing which
496:08 - I would just want to highlight for you
496:09 - is if you if if you see our data which
496:12 - we have if you see our data which we
496:14 - have is let me let me just go to desktop
496:16 - and then let's let me go to my OLS
496:18 - regression Challenge and then you have
496:20 - the data now let's go to data and then
496:22 - let's open this data and then worry
496:23 - about two of the columns out here the
496:25 - First Column out here is our Bend you
496:28 - know you we have seen where the columns
496:30 - were in a in terms of interval format so
496:32 - you've seen build uh bending where it
496:35 - says that your column is an interval
496:36 - format 6194 so this is your lower bound
496:39 - and this is your upper bound right this
496:41 - is a lower bound and this is your upper
496:42 - bound right so you have this column so
496:44 - so it will it is of object type and the
496:47 - model will not make any sense out of it
496:49 - if it is left alone so what we need to
496:51 - do we're going to take the first element
496:52 - and you we store the first element in
496:55 - another column and take the second
496:57 - element and store that in another column
496:59 - this is called the first lower bound and
497:01 - it's called the upper bound right so
497:03 - what we'll do we'll split out this we'll
497:04 - split out the the values from this
497:06 - column into different different columns
497:08 - by making different different columns
497:09 - right so let's see how we are going to
497:11 - do it so we're going to use uh so first
497:13 - of all let me show you first of all what
497:15 - exactly that uh we are doing we are
497:17 - using DF data frame and then assessing
497:18 - that column and the assessing the first
497:20 - value from that and the first value is
497:22 - this if you see the type of it if you
497:24 - see the type of it so type of it is
497:26 - nothing but your string I guess so if
497:28 - you see the type of it it is nothing but
497:30 - a string so now you have that string now
497:32 - to make the column so that it should
497:34 - make sense otherwise it will not it the
497:36 - model will not accept it
497:38 - foreign
497:56 - so this function is made in feature
497:58 - engineering.pi file and in this what
498:01 - we're doing we are making a empty file
498:03 - bin dink where we are iterating in the
498:06 - in that column so we are iterating for I
498:08 - in that particular column so it will go
498:10 - through first value second value third
498:12 - value Etc and then first of all it will
498:15 - remove all the parentheses my point is
498:16 - that it will it will remove the
498:18 - parentheses and the square bracket so it
498:20 - is saying I to strip and then
498:23 - um and then this particular
498:26 - parenthesis and squared brackets so it
498:28 - will it it will remove it so if you see
498:30 - dot strip function in
498:40 - if you see dot strip function in Python
498:42 - if you see dot strip function
498:44 - I mean um dot split which helps to
498:47 - remove the characters from the beginning
498:48 - or the end of the string right so it
498:50 - strips out it removes the white space
498:52 - and any sort of values which we wanted
498:54 - to remove so it's play it removes the
498:56 - parenthesis and brackets in this case
498:58 - the parenthesis and brackets now we have
499:00 - removed the parenthesis in Brackets now
499:03 - we have this
499:05 - um and then what it does it split the
499:08 - string into a list so we can actually uh
499:10 - split that string into a list when where
499:13 - you see so it is it says whenever you
499:15 - see this column split that split that
499:18 - list split that string into a list
499:21 - buy this so when when when the when the
499:24 - push program sees this particular column
499:26 - it says okay this is the first element
499:28 - and then splits these two differently
499:29 - right and then what it does it converts
499:32 - the list to a tuple um considering that
499:34 - we should have a unique stuff and then
499:36 - it converts the converts individual
499:38 - elements to float right so it from the
499:41 - Tuple it Maps every individual to float
499:44 - so each and each individual is mapped to
499:46 - float and then finally convert that list
499:48 - to a tuple to a list and then you append
499:51 - that particular Uh I that particular
499:54 - value into this so if I could show you
499:56 - if I could show you what exactly each
499:58 - steps looks like its steps looks like so
500:01 - let me just show you each each and every
500:02 - step this should make much more sense I
500:04 - guess so
500:06 - let me just print each and every eyes
500:09 - uh whenever you actually uh go ahead and
500:12 - then see that right
500:14 - that should make much more sense for you
500:18 - um let me just go to data in chest state
500:20 - of test and then let's run it
500:24 - first of all we have to you know save
500:26 - that it will not work if we don't save
500:28 - it so we have to save that
500:34 - feature engineering and then just save
500:35 - it first of all and go to data test and
500:38 - then now run it from here to there in an
500:40 - interactive window
500:42 - it will take some time to run it
500:45 - so now you see now you see the for the
500:48 - first it was this it was this it was
500:52 - this then it then it converted into this
500:56 - particular thing that then it converted
500:58 - into this particular thing by removing
501:00 - all the parentheses by removing all the
501:02 - parentheses right now by removing all
501:04 - the parentheses over there so basically
501:06 - you can see now
501:08 - um where I was Data engineering
501:18 - so you can see over here we are first
501:20 - printing I the first printing I with the
501:23 - removal of all the values out here with
501:27 - all the values out here and then we are
501:29 - getting the value of I we are getting
501:31 - the value of I which is you know
501:33 - um
501:34 - specified value right which is the which
501:37 - which we wanted now you can simply say
501:39 - you can simply what what you can do once
501:42 - you have the value once you have the
501:43 - values which is remove all the
501:45 - processing stuff which is converted into
501:46 - a list which is converted into a list
501:48 - which you can see over here which is
501:50 - converted into a list now that list okay
501:52 - now that list can be added instead of
501:54 - those strings right and then what we do
501:57 - we say that we we make a new column data
502:00 - which is of lower bound make a new
502:02 - column and then we say that for I in
502:04 - that bend deck take the first element in
502:07 - that list take the first element in that
502:08 - list and then you in up in Upper bound
502:11 - we say that take the second with the
502:12 - zero the first element in the in the
502:15 - lower bound zeroth and the first one
502:16 - first so we say for every each and every
502:19 - values right we also make a column
502:21 - called median where we say may lower
502:24 - bound plus upper bound divided by 2 that
502:26 - will give us median and then we finally
502:28 - drop that column which was useless in
502:30 - this case so we simply drop this column
502:32 - bending right
502:34 - so this is how uh basically what you're
502:37 - doing we're simply taking this value
502:38 - putting this another in one column named
502:41 - lower bound taking this value putting
502:42 - this on another column main named old
502:44 - upper bound and then we are taking the
502:46 - median of both of this like this the
502:49 - first element plus second element
502:51 - divided by two uh and by storing that in
502:54 - another column that might be useful
502:55 - right so now this will start making
502:58 - sense so what we did we splitted the
503:00 - feature we splitted the intervals and
503:02 - then we used accordingly according to a
503:04 - statement and then we drop the columns
503:05 - which are useless so now we have the
503:07 - information which is good
503:09 - so this is what what we did now if you
503:11 - go and see our data which we have so let
503:13 - me let me just show you the data which
503:14 - we have
503:18 - you should wait for a second
503:23 - still opening that's very big to be
503:25 - honest
503:26 - yes and then and then we'll go to
503:28 - categorical to column and then we'll
503:29 - talk about uh categorical encoding in
503:32 - the next set of lectures so if you see
503:33 - out here that spreadsheet now if you see
503:35 - over here now if you see over here you
503:38 - have something known as upper bound and
503:40 - lower bound so let me just show you to
503:41 - you first lower bound so that lower
503:44 - bound have this particular values upper
503:46 - bound has this particular values and
503:47 - median is this right so we have we we
503:49 - removed that because that was we're not
503:51 - picking sense because of interval that
503:53 - was a string and model will not make
503:54 - sense out of it we made something like
503:56 - this which will make sense to our model
503:58 - right
503:59 - now we have something else categorical
504:01 - so now another issue which we had
504:03 - another issue which we had I just want
504:05 - to show it to you what issue which we
504:07 - had is this particular column
504:10 - where is that column
504:14 - uh data and then we're just going to
504:16 - show it to you that's much more better
504:17 - yeah so see this now we are done with
504:19 - this now you see the geography column in
504:21 - geography column we have the county and
504:24 - the state right County and the state so
504:27 - both are the same point right so can't
504:29 - we have can't we split this feature
504:32 - again and then have County in another
504:34 - column and state in another column that
504:37 - will make much more sense right so what
504:39 - we do we simply say we simply say County
504:42 - we split that with that comma because
504:44 - comma is there so we split we're using
504:46 - the comma and then we say that uh for
504:48 - for for every column in geography and
504:50 - then we say that take in in this case we
504:52 - asked to take the zeroth element because
504:55 - this will be converted into a list now
504:57 - now we take the zeroth one because it
504:58 - was splitted in the enter in with the
505:01 - help of comma so it it will be converted
505:02 - to list and then we take the first Score
505:04 - first first element
505:12 - and then we take the first element and
505:14 - store that in a county column and then
505:16 - we take the second element of which is
505:18 - of course worse because it starts from
505:20 - zero one so let's take the second
505:21 - element store that in a state column and
505:23 - then we drop the geography so now you
505:25 - have another column for kids app for
505:27 - counties and states
505:28 - right so that that is feature splitting
505:31 - and reconstruction this is this is the
505:33 - processes called feature splitting and
505:35 - reconstruction I hope that you got it
505:36 - very nicely and that's it for this right
505:39 - now um so what I'll do in the next set
505:41 - of lectures is actually talk about we
505:43 - have this something now we have the
505:44 - columns which is splitted now we now how
505:46 - can we convert how can we make use of
505:48 - the state and the counties and how can
505:50 - we convert that how can we make sense
505:52 - out of it by different different methods
505:54 - of categorical encoding that's also
505:56 - super useful for you to understand let's
505:57 - go back and let's go to the next lecture
506:03 - so hey everyone so now uh in this what
506:06 - I'll try to achieve is uh just what I'm
506:09 - going to achieve is try to give you a
506:11 - very nice and complete feature
506:13 - engineering techniques which is uh till
506:17 - the previous lecture we have completed
506:18 - this categorical uh sorry to the in
506:21 - geographical column we converted the
506:22 - geographical column into two sets of
506:24 - color which is county and state what I'm
506:26 - going to do next it is talk about one
506:28 - hot encoding which is
506:31 - um so you have seen the the the the the
506:33 - data where we had the values which is
506:36 - categorical so now we have the two
506:38 - categorical values which is this County
506:40 - and state in County we have some other
506:42 - end State we have some other set of
506:43 - values now we have to come convert this
506:45 - into some sort of integer or numeric
506:48 - that should make sense to the model
506:49 - right that's your your model should be
506:51 - able to interpret it right so now what
506:54 - we need to do we need to
506:56 - um uh you encode those categorical
506:58 - variables right so we learn about
507:00 - several other encoding techniques later
507:02 - on but we'll encode those category
507:04 - variables or try to make sense out of
507:06 - that English otherwise it will give at a
507:08 - very a good error because your model
507:10 - will not accept the English stuff it
507:11 - will accept only the numeric and you
507:13 - have to make in such a way that it
507:14 - should make sense
507:16 - Okay so
507:19 - so what you what exactly I'm doing it
507:21 - over here whatever you seeing in front
507:22 - of your screen is you have a categorical
507:24 - columns in that categorical categorical
507:26 - columns you are selecting if in that
507:29 - data table so selecting the data types
507:31 - which are of object and then selecting
507:33 - columns so basically what this category
507:35 - column states that this is a list of
507:37 - category columns whose object is data
507:39 - type so most of the categorical
507:40 - categorical columns whose data type is
507:43 - object right so that's what we first of
507:46 - all select the categorical columns and
507:48 - then what we do we had the one hot
507:50 - encoder you know uh instance we create
507:52 - an instance with the following
507:54 - parameters so let's try to go ahead and
507:56 - then talk about what is what exactly one
507:57 - hot encoder so I'm just going to go
508:00 - ahead and then talk about uh show you a
508:02 - very nice visual a quick uh
508:04 - understanding of that so if whatever
508:07 - you're seeing in front of me is my
508:08 - screen so see this you have the
508:10 - categorical vary variable you have a
508:12 - categorical variable which is color and
508:15 - then in that you have red blue green uh
508:17 - blue so as of now instead
508:20 - right so we in in this case we have a
508:22 - categorical variable which is color so
508:24 - what this one hot encoding will do one
508:27 - what will coding will create the three
508:29 - other the the the the number of unique
508:32 - um values in that particular column will
508:34 - be the equals to the number of new
508:36 - columns which will be created and now
508:39 - for this assume that color is uh for for
508:42 - example in ID number one ID number one
508:45 - we have color red where it says color
508:47 - red is present but color blue is absent
508:49 - green absent and zero a blue is absent
508:51 - right same goes with color blue and then
508:54 - when first article a blue is absent but
508:56 - you see the color blue is friend so
508:57 - that's why it's one so basically what
508:59 - you what what we are doing eventually is
509:01 - removing uh is is converting that into a
509:05 - dummy variable type of thing we're
509:06 - creating creating some sort of dummy
509:08 - variable for every values and then we
509:10 - are seeing if that particular variable
509:11 - is available or not in that right so uh
509:14 - I'll tell you a very nice example in
509:16 - there so you can also go and see the
509:18 - other set of examples so let us assume
509:20 - that you go and see this one
509:22 - assume that you have seen this example
509:24 - and in the in this example you have I
509:25 - land as a categorical variables and then
509:29 - for every categorical variables you have
509:31 - the you have the columns created for
509:33 - every means the uni categorical uh
509:35 - counts in that so Biscoe and then in
509:38 - bisco the first ID is one because it is
509:40 - present but tungsten is not present
509:41 - that's why zero in the first ID in the
509:43 - in the first ID right and see my mouse
509:46 - so in the first ID bisco is only present
509:48 - then a second ID at the last column
509:50 - dream is present and the third we have
509:52 - second tongue strain is present right so
509:54 - this is how we create new set of this
509:56 - this is how we convert this particular
509:58 - one column to the number of occurred to
510:00 - the number of a column which is equal to
510:02 - the number of unique values in that
510:03 - particular categorical variable and in
510:06 - that what and and how we do it we have
510:08 - the particular ID and then we see that
510:10 - ID is present in that particular column
510:12 - or not right so I hope that this makes
510:14 - sense I'll be giving you a very very
510:16 - nice example as well so this is what the
510:18 - M1 hot encoding will do which over here
510:21 - what is this parts is equal to true and
510:23 - then handle unknown is equals to true so
510:25 - what is this uh variables so handle
510:27 - unknown is nothing but
510:29 - um
510:29 - I think that there is a very nice set of
510:34 - yeah so handle and uh an unknown which
510:37 - is this specifies the way I think there
510:39 - should be some uh
510:43 - yes so when giving an input so when
510:45 - given an input uh if there is any
510:48 - unknown values comes in so ignore that
510:50 - for example for example in that
510:52 - categorical variable apart from this
510:54 - apart from red blue green you apart from
510:56 - this some other you know a voided color
510:58 - comes in but that's not present in your
510:59 - data so we have to it is asking to
511:01 - ignore that part okay
511:04 - and the sparsity we are seeing that as a
511:06 - false where we we don't have a sparse
511:09 - data right so now we instantiate then we
511:11 - say fit transform fit transformative
511:13 - fits and transform so basically fit the
511:16 - Transformer to X and Y uh which is a fit
511:19 - the data and then transform it like
511:20 - first of all fit the data like have the
511:23 - uh all the all the cat all the things
511:26 - out here fit the data and then transform
511:27 - it like convert that into the
511:29 - categorical variables over here right uh
511:32 - for all the category variables in your
511:33 - data set and you have one hot encoded
511:35 - which is the data set which is one
511:37 - hotter encoded now you convert that data
511:39 - into a data frame but and then you name
511:41 - that feature by ticket getting the get
511:43 - features names out using the one hot
511:45 - encoder um object and then using get
511:47 - features names out by giving that
511:49 - particular categorical column right
511:51 - you'd name that as a columns and then
511:54 - what you do you drop the categorical
511:55 - columns which are now you have the
511:57 - geography now you don't need the English
511:59 - column you already have the numeric
512:01 - converter new encoded column so you drop
512:03 - that and then you concatenate the
512:05 - numeric which which was there and then
512:07 - the official data set
512:09 - okay so I know it it might make sense to
512:12 - for you to maybe understand it but let
512:14 - me just make you clear with things so
512:16 - let me just open that OLS
512:19 - and then let's go to uh processed
512:23 - let's go over there it will take some
512:24 - time because it is a very huge data set
512:26 - now which is which is being created and
512:28 - I'll explain you every part which is
512:30 - happening over here
512:32 - so let's let let's just wait for opening
512:39 - yes so you're seeing in front of you're
512:41 - you're seeing in front you're seeing in
512:43 - front of me which is uh your first
512:45 - categorical column which is that
512:47 - geography that geography is now
512:49 - converted so now you see that we have
512:51 - County uh abelia County and that first
512:55 - ID it is not present so every unique ID
512:57 - of that particular County uh that's that
513:00 - every unique value of that County
513:01 - variable is converted into the columns
513:03 - now if they converted into a columns is
513:07 - now if you see the if that I I believe
513:09 - is present in the first row or dot it
513:12 - says no it's not present but if you see
513:14 - now this is there there are so many unit
513:17 - values that's why there's so many
513:18 - counties are created though so many
513:20 - variables are created and there's a
513:21 - downfall of it that's a downfall it but
513:24 - we cannot do anything that's for the one
513:26 - hot encoding says then you have state
513:28 - then you have you know State I think it
513:30 - it it is not shows full but you have a
513:32 - state and for State as well for state
513:34 - Washington States Los Angeles and Etc it
513:37 - is showing off as well and then you see
513:39 - that that it is it is mostly you see by
513:41 - zero but at some point you will see that
513:43 - there is one as well this this one as
513:46 - well where it indicates that that
513:47 - particular county is present in that
513:49 - that particular variable
513:51 - okay I hope that makes sense I don't
513:53 - want to repeat it once again so this is
513:54 - how it got converted this is a very huge
513:56 - data set which is being formed right
513:58 - there's a 1935 columns and only few rows
514:00 - right but we'll deal with that so this
514:03 - is how what a one out encoding is where
514:04 - you convert our numeric to this one hot
514:07 - encoding but but we'll take a look at
514:09 - other encoders like you know
514:11 - um there are other in encoders as well
514:14 - where we'll take a look at that and then
514:16 - now what you can do you can simply say
514:17 - that one hot encoding and then you
514:19 - simply encode the variables now you have
514:22 - that now you have also seen now you have
514:24 - converted the to the numeric now what
514:25 - you want to do you want to
514:27 - um
514:28 - you want to fill out the missing values
514:30 - so missing values in the data set
514:32 - however we'll be having another section
514:34 - of dealing with missing values but
514:36 - currently what I'm explaining you see
514:38 - first is first of all we take take the
514:40 - columns which which we need to drop
514:42 - right so here's my plan what I'm going
514:44 - to do if the missing values exceeds 50
514:46 - percentage of the whole data then we say
514:49 - that remove that particular column right
514:51 - but if that is below 50 we'll say that
514:54 - let's deal with it with our uh
514:56 - techniques which we have so the first of
514:58 - all calls to drop first the variable
515:00 - contains the call columns which we have
515:01 - to draw so we select the columns from
515:04 - the data frame where there are null
515:06 - values and where the null value is the
515:07 - mean of that null value the in that
515:09 - particular column the mean is greater
515:12 - than 0.5
515:13 - if that percentage is greater than 0.5
515:16 - then we simply drop if that's what the
515:18 - particular columns with that particular
515:19 - column in the data frame is if the known
515:22 - values in that particular column is
515:23 - greater than 50 percent we say that drop
515:26 - it which is drop X is equal to 1 because
515:28 - it is a column if if this was a row then
515:31 - we would have written X is equal to zero
515:32 - then if the values which are smaller
515:36 - than 0.5 the missing values are less
515:38 - than 50
515:40 - then we say that fill the missing values
515:43 - with that mean of that particular column
515:45 - so for example you have one two three
515:47 - four Nan and then six seven so now you
515:51 - have only one value so you fill this by
515:53 - the mean of all the values in that
515:55 - particular column right you have other
515:57 - methods to simple imputer K and an
515:59 - imputer there are several other
516:01 - techniques for imputing missing values
516:02 - which is a whole set of interview
516:04 - questions but we'll deal with that in
516:07 - the next section uh like we'll study in
516:09 - detail right so we don't have to worry
516:11 - about that now this is something which
516:13 - is dealing with missing values now now I
516:16 - hope that now you now what you can do
516:18 - you can simply use this uh in one hot
516:20 - encoding drop and fill and then print
516:21 - out the shape and then you can simply
516:22 - convert that now we have the process
516:24 - data where what we did we first adjusted
516:27 - the data we found basic values which is
516:29 - converted that intervals to bins
516:31 - categories called categorical some some
516:33 - call it splitted the feature
516:35 - reconstruction feature splitting feature
516:37 - engineering of converted categorical
516:39 - variables into numeric and then we
516:40 - recently dropped and filled the missing
516:42 - values now we have the perfect data to
516:45 - do the analysis on it to do the analysis
516:48 - on it where now we'll deal with
516:50 - something known as outliers so that's
516:53 - something is the whole set of discussion
516:54 - for the next set of videos so let's
516:56 - catch up in the next video okay bye
517:11 - oh
517:12 - so now what we'll do now we have done
517:15 - with the preparation of a data set now
517:17 - once we are done with the preparation of
517:19 - the data set now we'll go and explore
517:21 - some of the most amazing things in your
517:23 - data set which is working or dealing
517:25 - with outliers so this project I would
517:28 - really like to focus on dealing with
517:31 - outliers so uh let's go let's go ahead
517:34 - and let's try to figure out lot of
517:36 - things out here
517:38 - right so um in outliers we have seen a
517:41 - very nice explanation of the outliers
517:43 - but
517:44 - um we'll talk about some of the methods
517:46 - for dealing with outliers and I think
517:49 - this will be a very long video uh so
517:51 - bear bear with me up right so first is
517:54 - what we're doing we're taking out the
517:55 - box plot of our Target death rate where
517:58 - we say that we I'm using plotly so I'm
518:01 - using plotly where we say that we're
518:03 - going to create a figure in that figure
518:05 - you know on a y-axis we should have the
518:07 - target death rate and then it should a
518:09 - point pose Jitter box points to be
518:11 - outliers Etc so this is a I'm using
518:13 - plotly to plot my box plots so out here
518:17 - out here you can see my first quartile
518:20 - my first quartile which is
518:23 - um zusic
518:25 - 161.2 and then my second quartile is 178
518:29 - and my third quartile is 194. right
518:32 - and then you have something upper fence
518:34 - up upper fence is something which is
518:36 - your maximum and there you have lower
518:38 - fence which is this much so whatever
518:40 - goes above this are are say to be an
518:44 - outlier and whatever goes beyond this
518:46 - whatever goes
518:48 - um below the minimum I'll say it'll be
518:49 - outliers but this is this is what we
518:51 - have learned right but this is something
518:53 - which we'll take a look at later on but
518:55 - as of now let's go ahead and let's talk
518:57 - about uh let's analyze rather than a
518:59 - Target column right let's analyze other
519:02 - The Columns as well rather than that
519:05 - so first of all what I'll do what I'll
519:08 - do is we have a column known as average
519:10 - a n count right so this is a column in
519:13 - our data set now with what we're doing
519:14 - and think of the mean of that column and
519:16 - then taking the standard deviation of
519:18 - that column right and then we're
519:20 - creating a this histogram to visualize
519:22 - what is the distribution of that average
519:24 - a n and then we plotting the box plot
519:28 - using uh plotly so uh for line by lining
519:31 - ice I'll be giving a resources for
519:33 - learning plotly okay so I suggest you to
519:35 - first of all learn broadly because
519:37 - that's very pretty much easy so you can
519:39 - first of all I suggest to learn plotly
519:41 - where you're saying create a figure for
519:43 - that data and then that data I'm into
519:45 - we're going to have the box plot of this
519:47 - particular column and the power it
519:48 - should be outlined Zeta is equal to 0.3
519:51 - and Point post is equals to 1.8 however
519:53 - you don't need to remember all of this
519:55 - what I usually do I just copy and paste
519:57 - things so even I even sometimes I forget
519:59 - lot of parameters so it's organization
520:02 - eventually so what I'm doing right now
520:03 - I'm taking out the now I am going each
520:06 - now why I'm doing this first of all
520:08 - let's understand why I'm doing this the
520:10 - reason why I'm doing this I want to
520:11 - identify outliers in my columns so just
520:14 - for explanation sake of thing I am
520:16 - giving the tools how I go about learning
520:19 - more about how my individual variable
520:21 - looks like how my how this average
520:24 - airline columns are is what is the
520:26 - distribution what is the mean what is
520:29 - the standard deviation what is the what
520:32 - is the correlation coefficient
520:33 - correlation with my target variable what
520:36 - is the number of outliers present etc
520:38 - etc etc so in this this is a step which
520:42 - I follow is a framework which I follow
520:44 - is the reason why we are doing this we
520:46 - need to summarize this is some summarize
520:48 - this column average a n and count
520:50 - statistically
520:51 - so what are you seeing in front of me we
520:53 - we have peer print printing the mean of
520:55 - that particular column and then you're
520:56 - plotting the histogram bills is equals
520:58 - to 20 which you have seen the way in
521:00 - interpretation of the histogram then
521:02 - over here we calc and then you simply
521:04 - box plot it and then over here you
521:06 - calculate the correlation between the
521:08 - averaging and count and Target which is
521:10 - we are saying correlation is equals to
521:12 - this particular column this is assume
521:14 - that is a vector dot correlation with
521:17 - whom Target death rate right so into
521:19 - identify what is the what is this the
521:21 - direction the strength between your
521:23 - average and account and the number of
521:25 - death threats in that particular country
521:28 - so the correlation it will be given by
521:29 - this and then you create a scatter plot
521:32 - to visualize the relationship so uh yes
521:35 - what I here's why I did first of all I
521:37 - care first of all I did two basic
521:38 - statistical analysis of this numeric
521:40 - column mean standard deviation I I I
521:43 - wanted to know the distribution that's
521:45 - why I added a histogram I already know
521:47 - the outliers that's why I added you know
521:49 - box slot I want to know the correlation
521:51 - between both of them that's why I added
521:53 - this now you may think are we going to
521:55 - do this for every column if you wish to
521:57 - analyze any particular column you should
521:59 - do it right if you wish to analyze if
522:02 - you wish to know more about the data you
522:04 - should do all of this stuff for each and
522:05 - every numeric variables right that will
522:08 - give you a better understanding about
522:10 - holistic understanding about data
522:15 - plotting the scatter plot to visualize
522:17 - the relationship between AF averaging
522:19 - and governance Target their threat so
522:21 - let's first of all print this out from
522:22 - starting then it will make more sense so
522:25 - let me just run it eventually
522:30 - so it will take some time to run it so
522:32 - we'll just give it give it some time to
522:35 - um run it
522:37 - I think yes
522:43 - plotly no module name plotly so
522:46 - basically it is saying the plotly is not
522:47 - installed so let me quickly install
522:49 - plotly so plotly install pip and then
522:54 - I'll just install plotly by using pip
522:57 - install plotly
522:58 - by making a nice pip install plotly
523:02 - let's wait a few seconds to get it for
523:04 - getting it installed I think it's get
523:06 - start so once it gets stalled we'll just
523:09 - run this
523:13 - you should wait for some time
523:16 - whoops it is saying a minimum type of
523:21 - rendering requires an NB format 4.2 but
523:24 - it is not installed
523:26 - interesting question so let's go and
523:28 - search for it so this is how you should
523:29 - search if you I don't know so currently
523:32 - I don't know the solution to this
523:33 - problem okay now let's try to debug it
523:34 - so it is saying that there's something
523:36 - to do with this format right there's
523:38 - something to do do this format I think
523:40 - that you have to update your kernel you
523:43 - have to upgrade your kernel I have to
523:45 - upgrade your jupyter notebook that's why
523:46 - it was saying you can simply go and then
523:49 - simply run it and then you will see that
523:50 - it will work completely fine then you
523:53 - see that it will completely fine so let
523:55 - me run this out and then you see that
523:58 - and then you first of all restart the
523:59 - kernel so you have to also restart the
524:01 - kernel to be honest
524:04 - restart because once you install the
524:07 - libraries you also have to restart so
524:09 - that it makes sense
524:17 - yes now we are done with that now what
524:20 - we do we simply have this
524:22 - um so let me just first of all run run
524:24 - it out so it says that what it says it
524:27 - says that for a particular average a n
524:29 - account you have mean of average count
524:31 - standard deviation histogram you see it
524:35 - is a positively skewed it's not normal
524:36 - it's a positively skewed data set where
524:39 - most of the data lies between your zero
524:40 - to five thousand because there's some
524:42 - outliers also present you see that there
524:43 - are a lot of outliers present you see
524:46 - that that this is your upper upper phase
524:49 - lower fins and in this upper fence you
524:51 - have 117 is the Upper Front so whatever
524:52 - values Above This are outliers and
524:54 - whatever values below the six are
524:56 - outliers but you see the most of the
524:58 - value slice between uh you see these are
525:00 - the values which are OMG these are out
525:02 - outliers
525:03 - and then correlation between the target
525:05 - is negative correlation where one
525:06 - increases then other decreases so it is
525:09 - saying that
525:10 - um the the the the negative correlation
525:13 - what it is one increases and other
525:16 - decreases
525:17 - right
525:18 - right so as uh assume that
525:22 - um so now you see over here now we see
525:24 - now this is scatter plot where on on
525:26 - x-axis we have average in encounter Y
525:28 - axis we have a Target death rate you see
525:30 - that there is some sort of a giving a
525:32 - very nice stuff but you see over here
525:34 - that uh but but let's let's in let's
525:38 - interpret it so it is saying the mean of
525:40 - average a n and count is 606 which tells
525:43 - us on an average there were 606 cases of
525:46 - cancer diagnosed annually Per County
525:50 - right six and average the standard
525:53 - deviation tells how spread out your data
525:55 - is indicates it is quite spread what in
525:58 - 216 is quite spread with counties having
526:01 - relatively low low numbers of with count
526:04 - some counties having relatively low
526:06 - number of cancer diagnosis annually and
526:08 - some count is very relatively High
526:10 - that's where the standard deviation is
526:11 - high because the spread is high in this
526:13 - case and this is further confirming
526:15 - which you can see that is further
526:16 - confirmed in your histogram or box plot
526:18 - you see the correlation is minus 0.14
526:21 - which tells us the strength and then
526:23 - strength of a linear relationship
526:25 - correlation of negative one indicates
526:27 - that the perfect linear relationship
526:29 - which means that one increases then
526:31 - other variable decreases
526:33 - a correlation of one means that there is
526:35 - a perfect positive relationship between
526:36 - both of them right so this is a very
526:38 - correlation about how can we interpret
526:41 - it so if in this case if one increase
526:43 - then other other decreases but it does
526:46 - not imply cause a cause I should please
526:48 - see my lectures initially on correlation
526:50 - does not imply causation but only thing
526:52 - which I want to tell is to this is how
526:54 - you can visualize or summarize your
526:56 - particular variables by dealing with all
526:58 - all of the shutsticks
527:06 - you have several missing values as well
527:09 - um I think you have several several
527:11 - missing values however we are going to
527:13 - import the process data to deal with
527:14 - anything literally anything right so you
527:17 - can actually ignore this as of now so
527:18 - what you're going to do we're going to
527:20 - import our process data but now over
527:22 - here but now over here you will be a bit
527:24 - shocked to uh see the outlier detection
527:27 - so let's try to learn about outlier
527:29 - detection in the next set of lectures uh
527:32 - on how we can utilize how you can
527:34 - utilize how we can utilize several tests
527:37 - for outlier detection how we can
527:39 - eventually utilize other things like
527:41 - um uh like SQ how can we perform outlier
527:45 - detection skewed data sets how can we
527:47 - remove it right how can we remove our uh
527:50 - out outliers from the data set by and
527:53 - how can we use our trimming capping this
527:55 - over several the techniques for dealing
527:57 - with it we'll deal with that and then at
527:59 - the end we will just save it and then
528:01 - we'll go to model building which is
528:03 - super duper easy part in this where we
528:06 - send we have our model to eventually
528:09 - train a model but the most important
528:11 - part of this project would be the
528:13 - outlier dealing with outliers and
528:14 - working with data so let's meet in the
528:17 - next lecture talking about things which
528:18 - you have not imagined bye
528:23 - so hey everyone uh what I'll do today is
528:26 - talk about
528:28 - um outlier detection which is the most
528:30 - important you know uh lecture till now
528:33 - which we which which you're going to
528:35 - deal with today is that the most
528:37 - important lecture and we'll try to
528:39 - achieve uh talking about outlier
528:41 - detection using z-score method we'll try
528:43 - to achieve outlier detection using uh
528:45 - IQR you know uh interquartile ranges
528:48 - we'll also try several other techniques
528:50 - like and how can we how what is
528:52 - streaming what is capping
528:54 - Etc in these sort of lectures so so
528:57 - let's get started actually with all of
528:59 - this
529:00 - so whatever you're using in front of
529:02 - your screen is for normally distributed
529:06 - data so
529:08 - um uh so first of all we'll try it off
529:10 - try and try it try it try out outline
529:12 - detection using outlier detection using
529:15 - um
529:16 - z-score and over here first of all we
529:19 - import a process data the one which you
529:21 - process very nicely and then we take a
529:23 - look at the head of this and then we
529:24 - take a look at the tail of this so let's
529:26 - try to print out our head our head of
529:30 - the data and then let's first of all
529:32 - head of the data and we print out the
529:33 - hair of the data this is how it look it
529:34 - looks like but I think which we want
529:36 - this to be in a form of rather than
529:38 - printing let's print out something like
529:40 - this
529:41 - it should look like this where you have
529:43 - five rows nine out of four columns right
529:45 - because why why this is nine over four
529:47 - columns because every value of that
529:49 - categorical values B has become a new
529:52 - categorical column
529:54 - now now uh now for z-score where we are
529:58 - going to use out we are going to use Z
530:00 - scores we have studied z-scores in our
530:02 - probability uh lectures so please go
530:04 - there and review it back so we also
530:07 - these scores are you and we are going to
530:09 - use z-scode for outlier detection right
530:11 - we are going to use Z score for outlier
530:14 - detection
530:16 - so basically so basically what what we
530:18 - are seeing over here is that
530:22 - um there's one condition for z-score
530:24 - that your particular column or data
530:27 - should be normally distributed so we on
530:30 - a deal so we first of all want to select
530:32 - the columns which are okay normally
530:35 - distributed okay it is not like Hi-Fi
530:37 - and all so you can go and see over here
530:39 - as well how do I know if my distribution
530:41 - is a gaussian if you want to go in math
530:44 - depth but as of now we don't need to go
530:46 - in it you can just use tests like you
530:48 - know normal tests from sci-fi to
530:50 - understand it so first of all we select
530:52 - the numerical columns because why you
530:54 - want to select the numerical columns you
530:55 - want to select the numerical columns so
530:59 - that only because you want to deal with
531:01 - numerical columns and then you have
531:02 - gaussian columns and non-gaussian
531:03 - columns we make a two empty list and
531:05 - then we say for column numerical columns
531:07 - for polynomial columns we're taking out
531:09 - the stat and P P value for normal test
531:13 - which is which is conducted on that
531:15 - particular column which is going to on
531:16 - that particular column and then you're
531:18 - printing out statistics and p-value and
531:21 - if now this now our PSR significant
531:24 - alpha alpha 0.05 so if a P value is
531:26 - greater than that Alpha they're greater
531:28 - than the alpha we we append our column
531:30 - in that uh so it it so if so what we say
531:33 - we resect our null hypothesis we reject
531:36 - our null hypothesis and then we say okay
531:38 - that if that P there's a significance of
531:41 - that particular column is greater than
531:42 - 0.05 we say that that the particular
531:45 - variable has the is the caution called
531:48 - variable and otherwise it is
531:49 - non-gaussian so uh we we we run it then
531:53 - we run it it will take some time to run
531:55 - it so you see that we ran it now if you
531:58 - take a look at the length of the
531:59 - gaussian columns there is only one
532:01 - column which is normally distributed
532:04 - there is one only one column which is
532:06 - normally distributed which is PCT public
532:09 - coverage
532:11 - now with this what you now we have now
532:13 - let's check with that's really you know
532:15 - uh gaussian columns out here so when you
532:18 - subplot it and then here's the
532:20 - explanation of this code so what you're
532:22 - doing you're creating a subplot with 10
532:23 - by 10 size so this is what your figure
532:25 - is create is of 10 height and 10 width
532:27 - using a subplots function from the matte
532:29 - library and there's Androids and
532:32 - Androids and and callers parameter so
532:34 - you see there is something known as you
532:35 - see a few this is how so you have and
532:38 - what is the number of rows and what is
532:39 - the number of columns so if there is
532:40 - more columns if there is more columns
532:42 - which are gaussian gaussian or normally
532:44 - distributed a normal distributor we most
532:47 - likely go with this you know specify the
532:49 - number of rows and columns and there is
532:51 - only one column in one row because there
532:52 - is only one value of gaussian columns
532:54 - and then we go to your use for Loop to
532:56 - iterate over the colors there is only
532:58 - one column in the gaussian calls that's
532:59 - why we just iterate once time and then
533:01 - we plot the density function then
533:03 - density plot of that function right
533:05 - density plot of that function the X
533:07 - parameter is set to access which is
533:09 - which allows the plots to be displayed
533:10 - on the same plot and the subplots
533:13 - parameter set to True which specifies
533:14 - that each iteration of the loop it
533:17 - should create a new plot right that
533:19 - should create a new plot that's why
533:21 - subplot is equals to true and then a
533:23 - share X parameter is set to false which
533:25 - means that each plot should have its own
533:28 - x axis so each plot should have own axis
533:30 - that's why share X which is sharing the
533:32 - x-axis should be false right and then we
533:36 - tightly out tight layout means to
533:38 - automatically adjust to fit nicely
533:40 - within the figure so it it ought it
533:42 - should automatically adjust so it fit
533:44 - nicely on the computer
533:46 - then we run it now we see that this is
533:48 - the only color which is normally
533:49 - distributed which is normally
533:51 - distributed
533:53 - now now what now what we're gonna do now
533:56 - we're gonna take now we have the column
533:57 - now we have the money the column right
533:59 - now so we print now I'm going to take
534:00 - the whatever data present in that column
534:02 - right because we want to identify
534:03 - outliers in that particular column right
534:06 - so we are going to take out the data
534:07 - from that column and then we're going to
534:09 - describe that data so when you now you
534:12 - have the now you have the gaussian data
534:13 - if you print out the gaussian data so
534:16 - you have only one column pcdi which is
534:18 - gaussian because we want to process data
534:20 - and then we just have taken the coil
534:21 - caution columns out there right so it is
534:23 - just giving the column columns which are
534:25 - gaussian gaussian so now we have the
534:27 - gaussian data now we can perform our
534:28 - outlier detection on this and basic info
534:31 - which is basic info which is mean
534:32 - standard deviation count you know 25
534:35 - percentile so 50 Etc which is the good
534:37 - statistic which you'll use later on
534:40 - then what you then what you're doing we
534:42 - are taking the basic info so that we
534:44 - should which we'll be using in our Bay
534:46 - info which is means standard deviation
534:47 - minimum and maximum
534:49 - and then what you're doing and then and
534:52 - then over here now here the part come as
534:54 - I told you that in as I told you that we
534:57 - need to identify the highest and the
534:59 - lowest right highest allowed in the
535:01 - lowest allowed and we'll be using the Z
535:03 - score and that right so what what
535:06 - eventually we are doing what eventually
535:07 - we we are using z-score for eventually
535:10 - taking out you know uh highest and
535:12 - lowest so what what you're doing on this
535:15 - case we are taking that particular
535:16 - column and then what that mean because
535:18 - this is a basic info basic information
535:20 - taking the mean of that minus sorry plus
535:23 - that's the maximum thing plus three
535:25 - times the basic input then the standard
535:27 - deviation okay this is the the currently
535:30 - we're using z-score that's why we have
535:31 - this formula why this three three can be
535:34 - changed we give into uh no no the you
535:36 - know values which go outside of that so
535:39 - if you go and see you know uh z-score
535:41 - outlier detection Z score
535:45 - outlier detection
535:49 - let's go to this so look you can see
535:52 - that anything which goes and anything
535:54 - which goes beyond this three then that's
535:56 - outlier beyond beyond minus three below
535:59 - minus three that outlier and Beyond
536:01 - minus B Beyond three and that's also my
536:03 - outlier right so the formula what's this
536:06 - formula was this but you can see over
536:08 - here anything Z score anything below
536:10 - minus three anything above 3 above
536:13 - standard division three they are
536:14 - outliers right so if you see over here
536:16 - that this that the nice any Z score
536:19 - greater than plus three which is
536:20 - standard deviation or minus 3 is
536:22 - considered as a less than sand minus is
536:24 - considered either as the outlier
536:26 - okay so which is pretty much same as
536:28 - this random deviation this is also
536:29 - called the standard deviation method
536:30 - some people call it but okay we're not
536:32 - going to call that so we have the we are
536:34 - going to take out you know uh for PVC
536:37 - column not foreign
536:39 - this column right uh this is the this is
536:43 - the highest allowed so anything which
536:45 - goes up above this anything which goes
536:46 - beyond below this so this is for plus
536:49 - three values and minus three standard
536:51 - deviation so anything which goes this is
536:52 - the my this is the top of the plus three
536:54 - and this is the value of the Tom below
536:57 - of the minus three so we say that
536:59 - highest allowed in that column is 59
537:02 - which is the plus three which is the
537:03 - z-score and this is plus three and this
537:06 - in this case a minus three lowest and
537:08 - where we subtract it and then what
537:10 - you're doing we are saying we are saying
537:12 - he in process day now we are going to
537:14 - print out so we say the process data
537:16 - so you can see out here that what
537:18 - exactly we're doing we are using the
537:20 - function uses Boolean indexing to select
537:23 - only rows in the data frame where the
537:25 - values in the column are either greater
537:27 - or the low or greater than the highest
537:29 - allowed or less than the lowest allowed
537:31 - and this results in the new data frame
537:33 - operation and this is called the data
537:35 - wrangling this is called the data
537:37 - wrangling and in this case you have Pub
537:40 - processed and then we go go to go to
537:42 - that particular column and therefore
537:43 - Loop iterate to every rows and then say
537:45 - any rows which is greater than that
537:46 - print out or any rows which is smaller
537:49 - than this particular value print out
537:50 - that and that prints out very nicely
537:52 - your columns and there's only five rows
537:55 - which are which are acting as outlier
537:57 - there's only five rows which are acting
537:59 - as a outlier
538:01 - so what we do we make a function we make
538:03 - a function we make a function where we
538:06 - talked or where we make a function that
538:08 - checks that deals with outliers that
538:10 - checks whether that for that function is
538:11 - outlier just implement the advanced
538:13 - version of it where anyone can put your
538:15 - own column name and then he'll be able
538:16 - to able to get your the the data frame
538:20 - right so over here you have deal with
538:22 - outlier so it deals with outliers where
538:24 - it Returns the out there it Returns the
538:26 - data frame which which is the Outlook
538:28 - outline data frame in that particular
538:30 - column okay it is for that particular
538:32 - column so it is highest allowed lowest
538:33 - loud as we talked about and then the
538:35 - data frame which we where that we select
538:37 - the columns the rows where the highway
538:39 - the where the values are greater than
538:41 - high slot and select the rows which are
538:42 - smaller than high lowest allowed and
538:44 - then we return the data frame okay now
538:47 - be honest we want to get the columns
538:49 - from a data set so we have the gaussian
538:50 - columns so now you have the gaussian
538:52 - columns where all all our columns which
538:54 - are normally distributed as of now we
538:56 - have the gaussian columns which are
538:57 - there's only one column in that which is
538:59 - normally distributed but we're going to
539:00 - go forward and talk about to the all the
539:03 - there might be other columns as well
539:04 - when you're working on a project right
539:06 - so iterate through every column in that
539:07 - gaussian columns
539:09 - and then you say deal with outliers you
539:12 - call that function and then you say
539:13 - whatever what are the what is the data
539:14 - data frame you have is gaussian data
539:16 - which you have right now is the gaussian
539:18 - data where we have only one column so
539:19 - you have to also convert that to uh
539:21 - gaussian data which should be the
539:22 - gaussian data and in that gaussian data
539:24 - you have these columns these uh this
539:26 - this particular column because we're
539:27 - iterating to gaussian columns and the
539:29 - basic info which which we wanna guess
539:31 - right the basic info which we have to
539:34 - give as of now we have already taken out
539:36 - all these three and when we take out the
539:38 - shape and if that shape is greater than
539:40 - zero then we see that that callers have
539:42 - the I have the outliers so as of now the
539:45 - as we expect PCD public coverage has
539:47 - outliers
539:48 - okay so what so now if you print out
539:51 - this data frame now if you print out
539:52 - this data frame so let's print out that
539:55 - it uh let's print out the data to the
539:57 - frame eventually
539:59 - so you see that we have the data frame
540:01 - we have the data frame which is a five
540:03 - rows and five rows columns so you have
540:06 - this particular data frame okay in that
540:09 - particular column you have this this
540:11 - value which is acting as outliers which
540:14 - is acting as a outliers
540:16 - but over here you see process data is
540:19 - showing all the values but we are
540:20 - interested in only this put this
540:22 - particular you know uh
540:24 - columns so we just remove we just remove
540:27 - those trimmed trimmed those values from
540:30 - our origin so we removed this outlier
540:32 - this outlier this outlier which means
540:34 - removing whole rows so there are couple
540:36 - of methods while dealing with outliers
540:38 - first one is streaming second word is
540:40 - capping trimming is a technique which is
540:43 - used to handle outliers in a data set
540:45 - which means that outliers have values
540:48 - that are significantly different from
540:49 - majority of the values in a data set
540:51 - when dealing with it is it is we're
540:53 - dealing with outliers the goal is to
540:55 - identify or remove them in order to more
540:57 - accurate representation of the data
540:59 - otherwise there are there are algorithms
541:01 - which are prone to this thing so
541:03 - trimming means removing the values that
541:05 - are far the removing the removing the
541:07 - outliers simply removing that's called
541:08 - trimming trimming means removing the
541:11 - outliers
541:12 - so when it is useful where it is useful
541:15 - when it is useful when it provides uh
541:18 - when when when there's a when we know uh
541:20 - it is easy to implement to be honest and
541:23 - it and for interpretation of it it
541:25 - provides a simple way to handle outliers
541:27 - without having to make any sort of
541:28 - assumptions just about removing like any
541:30 - missing values
541:31 - so here's what you do we go to the now
541:33 - we have the curve now we have the calls
541:35 - that have outliers right and then we
541:36 - print the data frame where it has the
541:38 - outliers right where we can clearly see
541:40 - then it be iterate through all the
541:42 - columns which has outliers then we go to
541:44 - highest allowed and lowest loud and when
541:47 - it trim the data trimmed data what does
541:49 - it mean trim data which means we go to
541:51 - the process data right and then we say
541:53 - that and then and then and and then we
541:55 - say that
541:59 - yeah so over here over here we say that
542:02 - that particular column is more than
542:03 - highest allowed or I think this should
542:06 - be or
542:09 - this should be or or
542:12 - um that particular is smaller than
542:15 - lowest allowed is Greg is greater than
542:16 - slowest allowed so if if that happens
542:18 - removed remove the column right it is a
542:21 - trim data where you're only selecting
542:23 - rows only says only selecting rows which
542:25 - satisfies these two conversations which
542:27 - we already seen which is great greater
542:29 - than values and uh smaller than values
542:32 - right
542:33 - so if you see the process data
542:38 - dot shape
542:41 - you're 47 so I think we we have to have
542:43 - this both the conditions right we have
542:45 - to add this both of the conditions to
542:47 - further successfully do it very now it
542:49 - removes all the values which are you
542:51 - know uh satisfies these both conditions
542:54 - it has to satisfy with this this both
542:57 - condition right
542:59 - now that's streaming trimming means
543:01 - removing of the outliers by by your data
543:03 - wrangling conditions where we see if
543:05 - that particular value in that particular
543:07 - column is greater than highest load for
543:08 - that particular column we remove that
543:10 - and then if that particular is less than
543:12 - that lower allowed then remove that as
543:13 - well
543:15 - gapping capping is another technique
543:17 - which is used to handle outliers which
543:19 - means that uh which means that to handle
543:21 - out Lies by replacing the values that
543:22 - are farther away with the maximum or
543:25 - minimum value so we just remove the you
543:28 - know value values of that particular one
543:30 - with the maximum or minimum so you can
543:32 - see that highest and then we create a
543:34 - copy and then we see that in that we
543:36 - block loc we go to the values in that
543:39 - particular column as we saw that column
543:41 - in this case a public PC to average then
543:44 - go to this column and then go to the if
543:46 - there's anything which is greater than
543:47 - high is allowed and then we simply go to
543:49 - that column ABC is equals to the highest
543:52 - that's a maximum and the same goes for
543:54 - minimum so if you simply have this
543:56 - capped data very simply remove
543:58 - are we simply remove our uh you know
544:01 - what our outliers with the maximum or
544:04 - minimum accordingly
544:08 - you can see uh anything which is greater
544:10 - than highest loud which is go is going
544:13 - going to be Mac going going to be
544:14 - replaced with the maximum anything which
544:16 - is smaller lower slot is going to be
544:17 - replaced with the lowest allowed
544:19 - now we have the capped now we have the
544:21 - capped data now we have the capped data
544:24 - now you can see that this Gap now this
544:27 - cap the data will not have any such
544:31 - um outliers in this any such outliers in
544:35 - this
544:36 - right and always you you can see we do
544:39 - we are not removing any call and any
544:41 - rows we just capping it so this is
544:43 - useful when you don't want to lose or
544:45 - lose lose of your information
544:48 - okay
544:49 - I hope that really makes sense now I
544:51 - hope that you want you you understood
544:53 - everything which I want to teach about
544:54 - outliers and in the next video what what
544:56 - you're going to do I think it's pretty
544:58 - much understandable from now on but I'll
544:59 - again make it for those who didn't
545:01 - understood again so I'll make another
545:02 - lecture on outlier detection if there is
545:05 - a skewness in your column
545:08 - so folks welcome to the next lecture on
545:12 - um whenever many many of you have
545:13 - requested me to work to actually
545:15 - complete this stuff and that's why I'm
545:17 - just going to complete this and you
545:19 - might have questions around like uh why
545:22 - the hell
545:23 - um like uh on outlier detection like uh
545:25 - first of all we have used you know for
545:27 - we have separated our normally
545:29 - distributed data so you so you might be
545:31 - seeing over here we have separated our
545:33 - normally distributed data for example in
545:35 - this which is which which we had
545:38 - outlier detection I think there should
545:40 - be one
545:42 - yeah uh which is outlier outlier
545:45 - detection using on normal normal
545:46 - distributed data so what you what we
545:48 - were eventually doing in this
545:50 - um approach
545:52 - is we were trying to uh from our from
545:54 - our uh whole data we were trying to
545:56 - extract columns we were trying to
545:58 - extract columns which are of uh which
546:01 - are of which are not normally
546:03 - distributed okay so uh for example there
546:06 - are several columns that from that we
546:07 - were trying to extract columns which are
546:09 - normally distributed
546:10 - um and you if you don't know what normal
546:12 - distribution please see the lecture so
546:13 - normal distribution which I prepared in
546:15 - a very nice way so then we use some
546:17 - tests to to determine uh we use some
546:20 - statistical tests to determine whether
546:22 - that is the whether that variable is
546:25 - normal distributed or not and we have
546:28 - used and then we use you know the thing
546:29 - which is p values to to say to to
546:32 - identify it that particular variable is
546:34 - a normal distributor or not using the
546:35 - certain test however you don't need to
546:38 - go in depth about that how do we
546:39 - identify that is for more like who wants
546:42 - to go in in depth but usually in
546:44 - interviews they don't ask like in depth
546:46 - all of these things but it's good if you
546:48 - if you know if you know about that but
546:49 - that's something for statistics majors
546:51 - and all but not for uh I guess for
546:53 - machine machine learning
546:56 - so now I'm so only we have we only we
546:58 - have analyzed only one column so the the
547:00 - reason why we are categorizing it first
547:02 - is for um for the for for example for
547:06 - normal normal distributed we use z-score
547:10 - um for z-score for outlier detection for
547:12 - normal distributed data right but but
547:15 - there are several columns which are not
547:17 - normally distributed as well as they are
547:20 - um like they they follow other the
547:23 - distribution so what should what what
547:24 - should we do how should we handle should
547:26 - we go to Every individual variables and
547:28 - then identify uh and then identify what
547:30 - is there in what is the
547:33 - uh outliers in that particular column or
547:36 - or we or
547:38 - um what or what we do there is something
547:40 - called as IQR interquartile range which
547:42 - we use that for outlier detection IQR
547:45 - for outlier detection IQR for outlier
547:48 - reduction so let's let me just tell you
547:49 - the IQR for the outlier detection yeah
547:52 - so we use outlier for uh iqr4 outlier
547:55 - detection and then uh that IQR works on
547:58 - skewed data skewed means a positive
548:01 - shifter or negative is negatively
548:02 - shifted we have already talked we have
548:05 - we have already had a talk on this
548:06 - please make sure that you actually
548:10 - um
548:10 - go towards it now now coming back to uh
548:13 - one of the questions which may arise wow
548:16 - like
548:17 - like a student asked me like hey ayush
548:19 - how can we identify which columns to
548:21 - test with them whether that column has
548:23 - outlier or not just I'll let you know
548:25 - one thing is that this is very very you
548:28 - know a big problem this is a big problem
548:30 - in um like you you can't just go to
548:33 - Every variable and then do things
548:34 - according to your own pace every each
548:37 - one column here might follow normal
548:39 - distribution and another column might
548:40 - fall is Q distribution right so they
548:43 - have two different distribution and
548:44 - normal normally their z-score works and
548:47 - on a skew there is other method works
548:49 - right so that's a defined set of rules
548:51 - which you already have
548:53 - and and what we do is simply classify in
548:56 - different different categories and then
548:58 - we apply the required all outlier
549:00 - detection techniques for the same and
549:02 - mostly our data has this skewed data so
549:04 - first of all what what we do uh you you
549:06 - might have seen our data has 1900
549:08 - columns
549:09 - our data has
549:11 - 1900 columns so what we do so what we do
549:15 - is we simply say that um like 19 why why
549:19 - they have 1900 columns because we have
549:21 - encoded our categorical data that's why
549:24 - it went from you know from small amount
549:26 - to large amount so first of all we
549:28 - identify columns which are categorical
549:29 - because they are of course cute right
549:31 - they are of course at once or another
549:32 - side so we don't want to touch them
549:34 - because they are categorical so we
549:36 - separate the categorical values how what
549:38 - we do we simply say calls to remove we
549:40 - simply make a list and then we iterate
549:42 - through our data columns and then we say
549:44 - what is the number of between in that
549:47 - particular column we say if the unique
549:50 - number of values in that in that column
549:52 - is less than 10 which means they're
549:53 - categorical however there is only two
549:55 - two unique values if they're categorical
549:57 - but still I for a safe journey I just
549:59 - added 10 and then I added cost remove a
550:01 - pen now I'm just using that list to drop
550:04 - that so there is 1870 columns which we
550:06 - are going to going to drop because their
550:08 - categorical columns that's it's a nice
550:10 - strategy that you might have other
550:11 - strategies to remove the categorical
550:13 - column because now they are not catech
550:15 - now they are not string type or object
550:16 - type they are numerical type and I'll
550:18 - deal with nicely to numerical types data
550:20 - but make sure that you don't apply the
550:23 - basic processing like remove one single
550:24 - values of few unique values don't apply
550:26 - that because you have to know that this
550:28 - is the categorical data
550:31 - like that's for numeric not for category
550:33 - now over here the code defines as you
550:35 - can see that uh over here else now what
550:37 - we do now what we do we identify as
550:39 - skewed columns as I said now we have the
550:41 - normal normal distributor now we
550:43 - identify these cubed columns so whenever
550:44 - you try to solve for outlier detection I
550:46 - did recommend you do couple of things is
550:49 - normal normal distributed data and
550:51 - skewed data right so there are different
550:53 - different distributions which I might
550:54 - start it right binomial uniform Etc I
550:58 - suggest to to take out to to research on
551:01 - that okay this is the distribution this
551:02 - is distribution of my data what outlayer
551:04 - detection techniques which I can apply
551:05 - to that so currently I have choose a
551:08 - skewed data I have choose a skewed data
551:09 - as Q distribution and skewd you might
551:11 - have it shifted positively or negatively
551:13 - anyway right so we identify that that
551:16 - sort of data and then what we do we
551:18 - simply say the uh over here what we over
551:20 - here we have a function that takes in
551:21 - the processed process data frame and
551:23 - then the columns so what does that
551:25 - columns form columns means the columns
551:27 - in the data right so call our columns in
551:29 - there that's it which is Wiki May we
551:31 - make a list and then we iterate through
551:33 - columns we take our dots Cube using dot
551:35 - skew method we just take out the now the
551:38 - skewness score and if that skewness
551:41 - score is greater than 1 or if that
551:43 - screen score is negative one then we
551:46 - append that uh then we append that
551:49 - particular column in that skewed calls
551:51 - and if this and if there and if that
551:54 - does not passes this condition then we
551:55 - won't append right
551:57 - um so you might have questions like how
551:59 - how exactly is skewness and well all
552:01 - works I did recommend to see how the
552:03 - reading materials which are which we
552:04 - have released for you please see that
552:06 - that's the best thing which you can do
552:07 - right now and then you visibly uh run
552:10 - this one to add in five skewed columns
552:11 - giving the data for skewness and the
552:12 - data for skewness columns and you have
552:14 - total number 16 columns 16 columns such
552:17 - as Q data right and then what you do you
552:21 - simply uh you simply make it just like
552:22 - you created the gaussian data you create
552:24 - the skewed data and then what you do you
552:27 - simply create the to see the
552:29 - distribution so over here you see over
552:30 - here you've subplotted it where we said
552:32 - we where we have I think for 16 and
552:36 - weight columns so you have to sub plots
552:37 - 16 right so what we do 4 by 4 16 so we
552:41 - can have four by four plots to put two
552:43 - two to plot every 16 variables
552:46 - which you can see and I've written this
552:48 - explanation of the code for the same
552:49 - please see that which you can see now if
552:51 - you just uh if you just maintain it let
552:54 - me just say plot image
552:57 - output
553:00 - open it please yeah
553:02 - so first of all see this first of all
553:04 - see this this is the distribution of
553:07 - averaging and conditioner distribution
553:08 - of average death so you have one two
553:10 - three four four columns one two three
553:12 - four four rows and you have all the 16
553:14 - variables which you had now now you can
553:16 - see that there they are at this their
553:18 - their skewed data which you can see over
553:21 - here possibly negatively and you can see
553:23 - they are putting this is negatively this
553:25 - is positively this is this is slightly
553:27 - positively sorry negatively I'm sorry uh
553:30 - I think I did wrong it is slightly
553:31 - positively and this is positively so it
553:34 - negatively positively and and you can
553:36 - actually see that we have positive and
553:38 - negative if it is going from this side
553:39 - then this negative if this is going to
553:41 - this side this is positive
553:43 - right so uh and then you type layout so
553:46 - that it adjusts Auto automatically and
553:47 - then show it now you can see that you're
553:49 - confirmed okay these these these are my
553:51 - columns just skewed and then what you do
553:53 - you simply box plot so that you see that
553:55 - if they did they do have outliers by
553:57 - using this particular code where you're
553:59 - saying uh figure where you're giving the
554:01 - column and the Box points outliers and
554:03 - then your title is box plot what were
554:06 - the number of columns to be added and
554:07 - then you say okay go to and then you
554:09 - apply this function to each and every
554:10 - variable for that particular data for
554:13 - skin skewed columns and then a column
554:15 - and then skewed columns where you have
554:16 - this cute data and here you can see that
554:18 - OMG we do have this queued we do have
554:21 - outliers we can see maximum and then
554:23 - these are the information which are of
554:24 - outside the way and this is you can see
554:27 - outside the way
554:28 - right and we have seen the
554:30 - interpretation of the box plots as well
554:31 - now you can actually see that this is
554:33 - also the outside the these are the
554:35 - variables Etc so we have this now how
554:38 - can we deal with this the what what we
554:41 - can do let's take a particular column
554:42 - from this let's take a particular column
554:44 - I am going to take
554:46 - um this cute column which is uh for an
554:48 - example as of now study recap and then
554:50 - we take out the quantile range 25th cup
554:52 - percentile the 25th percentile and uh
554:56 - and then we take out 75th percentile
554:58 - okay we take a 25th percentile and then
555:01 - we can take out 75th percentile which is
555:03 - a quarter q1 quartile 1 and quartile
555:05 - three and then we take a IQR from that
555:07 - okay and then we have upper limit like
555:10 - but the maximum value which which which
555:12 - we should have and anything goes above
555:14 - this our outlier and lower limit which
555:16 - is minimum and and things so you can see
555:18 - plus minus over here we are multiplying
555:20 - this and this is the Strand is strand
555:21 - deviation oh upper limit and lower limit
555:23 - and then you can simply say the process
555:25 - data where you have it where you just
555:26 - trim the data out here where you just
555:28 - say take out the columns here which are
555:29 - the outlier which you're seeing our
555:31 - upper limit or they're of lower limit
555:33 - they are below the lower limit upper
555:35 - limit for this particular column is 209
555:37 - and lower limit for this is minus one
555:39 - through five and these are the columns
555:40 - which you 504 rows in that particular
555:42 - column has outliers
555:45 - and then you simply uh and then you
555:47 - simply apply this and then you simply
555:49 - apply this column where we just had
555:51 - listed this particular all the code in
555:52 - through a sorts of the function a Define
555:54 - and column for that particular element
555:56 - is checking the quantiles q1 Q3 IQR and
555:59 - then upper limit lower limit and then a
556:00 - and then taking out that and then what
556:02 - you what you're doing you're iterating
556:03 - through all columns and then applying
556:05 - this function on that giving the process
556:06 - data and that particular column and then
556:08 - you simply have the uh count then you
556:12 - simply have the columns which has the
556:13 - outliers right
556:15 - now now you have the column switch which
556:17 - has the outlier so you have uh one two
556:20 - three four five six seven eight nine ten
556:23 - eleven twelve thirteen fourteen fifteen
556:25 - sixteen all 16 variables have their
556:28 - outliers so what we've what we do first
556:31 - of all we take out trimming there's two
556:33 - approaches we have to talk trimming and
556:34 - capping trimming removing capping means
556:38 - um updating updating my uh values like
556:41 - you know what what values updating my
556:43 - outliers with the maximum one and the
556:45 - minimum right so if that a maximum then
556:48 - we if if if the values above the maximum
556:51 - then you updated that with maximum if
556:52 - the value is up lower then minimum then
556:54 - we update that with the minimum you
556:56 - might be clear if you have seen the
556:58 - previous lecture otherwise I'm not going
556:59 - to repeat once and once again right so
557:01 - uh going to this code on trimming first
557:03 - of all identify what rows to remove and
557:06 - that can be done by this data wrangling
557:08 - or the data extraction where you're
557:10 - saying in that particular column in that
557:11 - particular column
557:13 - if that is uh that if that is greater
557:15 - than lower upper limit and if that is
557:17 - smaller than lower limit then these are
557:19 - the rows to remove and then this is a
557:21 - trimmed data where we actually this is
557:24 - where we actually talker the trim our
557:26 - whole data and over here we are tapping
557:28 - it so here we are capping it out there
557:31 - and uh and over here what this is just
557:34 - an example as of now just to Showcase
557:35 - we'll convert that to a function right
557:37 - now and then we simply lock loc to
557:39 - identify the rows and then we identify
557:42 - the rows where your upper limit uh for
557:44 - that particular column right for that
557:45 - particular column for that the the upper
557:48 - up any any value in that particular
557:50 - column is greater than upper limit
557:51 - update that with upper limit and any
557:53 - value below that lower limit update that
557:55 - with lower limit which is the minimum
557:57 - value you see that your data is
557:59 - maintained the same but over here your
558:00 - data is trimmed
558:02 - gapping gapping means
558:05 - um you see over here again the same
558:07 - thing you have converted the code to see
558:08 - over here and over here we will cap will
558:11 - not remove cap by removing we lose whole
558:14 - chunk of lot of information that's why
558:16 - we are tapping it over here and now you
558:18 - can see we are simply say go to the
558:19 - particular column sorry Row in that
558:22 - particular column if that is greater
558:23 - than upper limit for that particular
558:25 - column update that with upper limit and
558:28 - if there is more than lower limit update
558:29 - that with lower limit okay
558:32 - I I hope that that you're getting and
558:34 - then we apply this capping this function
558:36 - on everywhere every variables we have
558:38 - outliers and then we see that we have
558:41 - the data without trimming and we are
558:42 - just going to cap it and then you can
558:45 - now box plot on cap data now your capped
558:47 - data has that now if your box plot on
558:49 - that for every columns now you see there
558:51 - there are no outliers and that's right
558:55 - you want it you want the challenge
558:57 - I hope that you get it and then we say
558:59 - now we have we are going to use the cap
559:00 - data only we are not going to use
559:01 - something then we save our capped the
559:03 - data into something else okay
559:08 - I hope that you I hope that you got one
559:10 - point there is one as high uh assignment
559:12 - related to it there are lot lots of
559:14 - things to try out you can actually try
559:15 - out outlier detection using percentile
559:17 - method that's your thing now I think we
559:20 - are mostly done with the outline
559:21 - detection on regression model on on
559:23 - regression model uh let's talk about uh
559:26 - in the in the next set of videos around
559:27 - a regression model and then we will
559:29 - actually talk about that in great detail
559:31 - let's let's go in the further video and
559:33 - let's talk about that
559:39 - cool uh so we'll uh now we'll come to
559:42 - now we have done with the data data part
559:45 - and my whole point from this project is
559:47 - to tell you about data stuff now we'll
559:49 - just go and quickly review because we
559:51 - have already talked a lot on you know
559:53 - how to build regression model and all
559:54 - and that's not my point on talking in
559:56 - this project my point was different to
559:58 - teach you several techniques feature
559:59 - engineering collection and all
560:01 - techniques to actually go forward with
560:03 - that
560:04 - now uh what now what we'll do we'll try
560:08 - to model it very basic you know uh just
560:10 - to let you know you know we have already
560:12 - tried it out and then now it's now in
560:14 - the assignment you'll be given to model
560:17 - it accordingly so I'm just just going to
560:19 - give you a very small Baseline and it's
560:21 - a assignment will actually further
560:22 - update this uh stuff now which which
560:25 - you're seeing in front of me is we have
560:28 - the first function but ignore all all of
560:30 - that let's go to the let's go to over
560:32 - here so what what I did instead of
560:34 - classes or everything I just created
560:36 - functions right I just created function
560:38 - so that it is easy for you to understand
560:39 - but in but usually I suggest you to work
560:41 - with classes
560:43 - first of all we are importing our Gap
560:45 - data from the cap data which we saved so
560:47 - you can actually use the df.2 to CSV and
560:50 - give that particular path you'll be
560:51 - saving that your file and then if you go
560:54 - to the data data folder you have a cap
560:56 - data over there and cap data is where
560:59 - your where you have dealt dealt with you
561:01 - know missing values and everything out
561:02 - there outliers and everything
561:05 - now uh what you will do is you simply uh
561:09 - first is you have this particular thing
561:11 - where you have the correlation first of
561:13 - all you have the read read it and then
561:15 - you and then the second thing which I
561:16 - which I really like to do is identify
561:19 - the correlation among numeric features
561:22 - among numeric features and then right
561:25 - right now it is eight it has you know
561:27 - 1900 columns and it will take so much of
561:29 - time and the reason why I want to
561:31 - identify the numeric correlation because
561:33 - there is one assumption which you have
561:35 - saw that um the code the code they
561:38 - should not be correlated to too much if
561:39 - they're correlated then they're not
561:40 - providing any sort of information to the
561:42 - output variable
561:44 - and when I apply my correlation
561:46 - techniques when I when I apply my
561:47 - correlation techniques and and not have
561:49 - in not in every process you have to do
561:50 - you have to do this in exploratory data
561:52 - analysis part where you are
561:53 - understanding the correlation between
561:54 - variables this should be done in an Eda
561:57 - part right I'm doing it right now
561:58 - because I think it's much for me I think
562:00 - as of right I've just added added over
562:03 - there just to make sure that we are on a
562:04 - page like like why we are using this we
562:07 - are using just to know that we can
562:09 - ignore our highly coded correlation
562:10 - coefficient and then you see our result
562:12 - right so I just add it over here for for
562:14 - a for a sake of learning but this is a
562:16 - part of your EDM
562:19 - um so you have a correlation among
562:20 - numeric features where you are going to
562:21 - the where you're given the data frame
562:23 - and The Columns of the data frame and
562:25 - then in a numeric column you select all
562:26 - the columns here numeric and then
562:29 - to be honest every column is to American
562:31 - this right now and then you take out the
562:33 - correlation it builds a correlation
562:34 - Matrix with we have already talked about
562:36 - correlation please we don't want to get
562:38 - it but I'll tell you a small uh stuff so
562:41 - let me let me just go to correlation
562:44 - Matrix and then let's talk about that a
562:48 - bit so so this is a correlation Matrix
562:51 - so this is a correlation Matrix so I'll
562:52 - just open that in another tab so this is
562:55 - a correlation Matrix and in this
562:56 - correlational Matrix you you find heat
562:58 - map like this you find heat map like
563:00 - this where where on on on on over here
563:05 - on y-axis you have the variables on
563:07 - x-axis you also have the variables let's
563:09 - survive the correlation between the same
563:11 - columns is one they are highly correct
563:12 - highly coded with each other but the
563:14 - correlation between survive and P class
563:16 - is negative the correlation between
563:18 - survive and this variable is this much
563:20 - the correlation between servers so you
563:21 - have the Matrix for every VR caviar
563:23 - taking out the correlation between every
563:25 - variables out there
563:27 - right so it goes to it takes to the
563:29 - correlation and then it's sick it it it
563:31 - it takes it it has the empty set and
563:34 - then what we are doing we're going to
563:35 - for I in range for that correlation all
563:38 - the coral and then we go to J and then
563:40 - we are saying if the correlation of two
563:43 - variables so the correlation between for
563:45 - example survived and P class is greater
563:48 - than 0.8 is 80 greater than 80 percent
563:52 - we simply append that it's highly
563:54 - correlated with each other right we
563:56 - simply append that we simply append that
563:58 - column so that uh we could know that
564:00 - they're highly correlated features
564:02 - so we were using that I when when I when
564:05 - I ran that method when I ran that method
564:07 - by
564:09 - um
564:11 - uh by applying this so basically what
564:13 - you're seeing over here
564:15 - is I had the columns and then uh over
564:18 - here I had the columns and then over
564:21 - here you can see correlation among
564:22 - numeric features Gap data and cap data
564:25 - dot columns so I've just given the cap
564:26 - data and then Columns of that capped
564:28 - data and then I I was I was able to take
564:31 - out the correlation features and these
564:32 - these were the most highly correlated
564:34 - highly highly correlated feature uh
564:37 - features among themselves and now what
564:40 - now what what we can do we can we can
564:42 - first of all uh remove the remove the
564:45 - columns from the data first of all but I
564:46 - don't really like removing it so what
564:48 - I'm doing is I'm making a list where I'm
564:51 - telling call for call in cap data dot
564:53 - columns if the call is not in hike
564:55 - highly coded so we are just saying key
564:58 - for every column in your cap data which
565:01 - is your data which you've imported for
565:02 - every column make sure take give me that
565:04 - column names but also make sure that
565:07 - these are not in these high in highly
565:09 - query if the if the call also not in
565:11 - this so this is a list comprehension
565:12 - which you might have learned in your
565:14 - Python programming language
565:16 - and I said please see the resampling
565:19 - methods for this and we're going to
565:21 - explain the single word diet right right
565:22 - now please see the resampling method
565:24 - what we are doing you're dividing our
565:25 - data into training and testing I've
565:27 - already uploaded lectures on resampling
565:29 - please go and see that first before
565:30 - coming to this
565:31 - split the data and then we train our
565:33 - model I'm not I'm not going to go with
565:36 - this we are adding the constant why
565:37 - we're either adding the constant because
565:39 - we have to multiply with that bias term
565:40 - that one you know x 0 SEC 0 is equal to
565:42 - one so we are adding one to every rows
565:44 - so that whenever b0 multiplies with one
565:46 - only right which is the biister and then
565:49 - we have
565:50 - um and in the constant and then we have
565:52 - LL model we are training OLS from the
565:54 - the stats model API uh with the Y train
565:58 - and X train within to intercept
566:01 - and then you simply take your lard or
566:03 - summary and then it is printing out the
566:05 - summary and then you have the same goes
566:07 - for significant variables so what we are
566:09 - doing we are identifying the significant
566:12 - variables so um let's talk let's talk
566:16 - about that like I'll talk in a bit but
566:17 - let's run this whatever we have as of
566:19 - now so just to make sure that you're on
566:21 - the right page
566:24 - so so it will take some time to run
566:26 - we'll just chill we'll just chill out
566:28 - right now uh so it will take some time
566:30 - to run
566:31 - and so we should just stay calm stay
566:35 - nice
566:36 - do whatever is going to happen right now
566:38 - because it because the columns are super
566:40 - high and you and you and first of all it
566:43 - might be not as much valuable to deal
566:46 - with that categorical but to to have
566:48 - those categorical columns so could the
566:50 - correlation between categorical columns
566:51 - I'll be giving you a very nice article
566:53 - which talks about correlation between
566:54 - categorical columns which tells you
566:56 - whether that whether like and and and
566:59 - will also uh worry about key what are
567:01 - the variables which are significant
567:02 - enough to make our model better like we
567:04 - cannot go with these much variables to
567:06 - be honest
567:08 - because it will lead to several other
567:10 - problems as well so first of all Let's
567:12 - uh let's let's go with this now now if
567:15 - you see over here it printed out the
567:16 - columns which is of highly correlated so
567:18 - if you see out here that we print out
567:20 - the highly correlated features and then
567:22 - now it goes and training of the model so
567:23 - it is training of the linear regression
567:25 - model and it will take extreme y train
567:29 - this training on and then it is going to
567:30 - print out the summary so let's try to
567:32 - take out what exactly summary looks like
567:36 - oops yeah this is the summary and the
567:39 - summary is too big because you have 1900
567:42 - columns bro you you understand your 1900
567:44 - columns and to be honest we'll and and
567:47 - the most of the night allow around 99
567:50 - percent are of out of encoded columns
567:53 - that's why uh that's why in this case we
567:56 - but that's why we in this case you have
567:57 - to remove some of them so for first of
567:59 - all see that we have dependent variable
568:00 - the the thing in which you want to
568:01 - predict R square at just a r Square F
568:04 - statistic you know number of
568:05 - observations residuals you have moral
568:08 - covariance type and then for constant
568:10 - which is the bias term we have the B
568:12 - beta0 is 94 with instant deviation the
568:15 - TV and the P value for that average and
568:17 - for every variable you have this this
568:19 - much information a left a for every
568:22 - variable you have this much information
568:23 - left and there is so much of information
568:24 - and it is not feasible to go through
568:27 - every information so what I do I just go
568:29 - through what are the features which are
568:30 - numeric which are important which are
568:32 - not categorical as of now so what I'm
568:33 - doing I just could go on through you can
568:35 - go through numeric and then talk out
568:36 - talk some things but before that what I
568:39 - can do is talk about things which is you
568:41 - know signal we can take out the
568:43 - significant variables we have learned
568:44 - about that
568:46 - so let's let's do one thing let's print
568:48 - out the significance variable so let's
568:51 - print out the variables which are only
568:52 - significant enough so from 1900 we got
568:56 - to 144 columns which are significant
568:58 - enough we have done a lot of tests we're
569:00 - using p values you can take out the
569:01 - significancing significance of a
569:03 - particular variable so we we using t t
569:07 - statistics using T statistic we are
569:09 - taking out the significance of each and
569:10 - every variable over here so we have we
569:12 - have using LR dot P values and that P
569:15 - value is this is this is this particular
569:17 - thing
569:20 - is greater than this particular
569:21 - information
569:22 - these statistic we have t-test which you
569:25 - which we are using right now this
569:27 - particular thing which you're seeing
569:28 - right now p is greater than test so we
569:30 - are using for that if P value is smaller
569:32 - than p-value threshold and threshold is
569:34 - 0.05.
569:36 - okay I hope that you're getting that we
569:38 - are taking on the money columns which
569:39 - are which are which are which which are
569:41 - greater than that so that uh sorry I
569:43 - think uh so the values which are smaller
569:46 - than zero zero zero point zero five so
569:48 - that we can resect our null hypothesis
569:50 - and say that okay these these variables
569:52 - are important so we have only 144 or
569:54 - 1900 to 144 we've reduced the complexity
569:57 - of the model
569:58 - understanding what I'm trying to say now
570:01 - what you're doing you're simply training
570:03 - the model now with significant variables
570:05 - though the reason why we are removing
570:07 - this I'll tell you the reason why we are
570:08 - removing this but first of all uh you
570:11 - will get to know first of all let's run
570:13 - this you'll get to know
570:14 - you got an error you got an error what
570:17 - you got an error in that significant
570:19 - variables you also have the constant
570:20 - column a constant means Constitution
570:23 - which has one right so whatever what I
570:26 - am going to do I'm just going to remove
570:27 - it as of now there is actually a lot of
570:30 - fixes right now but I don't want to
570:31 - extend that you can fix it by your own
570:34 - so what you can eventually do you can
570:36 - create uh you can create actually X in
570:37 - Extreme as well you can create a
570:39 - constant column as well so that it
570:41 - appears that so you can actually what do
570:42 - what you can do
570:44 - let me do one thing right now I think
570:46 - that's something which which we can
570:47 - eventually do
570:49 - extreme and then just go ahead and then
570:52 - add X strain and then constant I guess
570:56 - what we can eventually do in this we can
570:58 - you can add the significant you know
571:00 - constant columns rather than removing
571:03 - that so I think what you can go and then
571:06 - add significant verbs and then
571:08 - significant Vats we can add constant
571:12 - yeah so you can actually do this but how
571:14 - however that significant natural action
571:17 - should be having your favorite you know
571:19 - what you that should be having
571:22 - um constant columns or you can what you
571:24 - can do you can simply have this SM dot
571:27 - you know SM dot add constant and then
571:30 - you can simply add that constant as well
571:32 - over there right so it is very easy to
571:34 - do do things you can actually go to uh
571:36 - xtrain.act constant and then the
571:39 - constant will be also added right so you
571:42 - can actually go over there and then
571:44 - simply add SM dot add constant
571:48 - right SM Dot
571:50 - add cons because in extremely 8080 it is
571:52 - not added right so we can actually go
571:54 - ahead and then add the constant audios
571:56 - as SM dot at constant and then we simply
571:59 - have this x string so it adds the
572:01 - constant you can actually use for loops
572:03 - and all to also added but this is the
572:04 - best way to actually do do that rather
572:06 - than removing that so you actually
572:07 - rather than moving that you can actually
572:08 - add this and then once you add this so
572:11 - let's run this so you might be having a
572:13 - nice set of information so now it it is
572:15 - complete now you run this now you run
572:18 - this you should be able to see a very
572:20 - nicer of info so now let's go and let's
572:22 - print out the summary which you can see
572:23 - so now if you run this now if you run
572:26 - this you have less information as
572:28 - compared to the previous one you have
572:30 - just only few set of information to go
572:32 - forward to
572:34 - so yeah this is the one and then you can
572:36 - also see the the R square is 0.68
572:40 - however it is it is decreased but it is
572:42 - a it it is a model which we looked for
572:45 - right or however it reduced the
572:47 - complexity of our model right our
572:49 - complexity of our model the more
572:51 - important is not to get the accuracy but
572:53 - also the robust model the model which is
572:55 - able to perform in most of the cases
572:56 - very nicely
572:57 - so this was it for the project and I
572:59 - hope that you really like this
573:01 - particular project and
573:03 - um you now you'll be given assignments
573:05 - for the for the same for rest of the
573:07 - things which you have to do in the
573:08 - project uh now you can get started with
573:10 - making your own projects Havoc try to
573:12 - explanation everything from very scratch
573:13 - now I suggest to watch resampling
573:16 - methods video regularization videos
573:17 - which is going to come so please watch
573:19 - that and thank you for the lectures