00:00 - if you are looking for a way to apply
00:01 - python and machine learning to a real
00:04 - world application this is the course for
00:06 - you you will learn all about
00:08 - bioinformatics for drug discovery using
00:11 - python
00:12 - shannon is a professor of bioinformatics
00:15 - and he knows how to break things down in
00:17 - a way that is simple to understand
00:22 - welcome to the bioinformatics from
00:24 - scratch course my name is cenin
00:27 - and i'm an associate professor of
00:29 - bioinformatics in this course you'll be
00:32 - learning about bioinformatics through
00:34 - the lens of drug discovery
00:36 - no prior knowledge of bioinformatics or
00:38 - biology is needed although if you have
00:41 - some it will be helpful and so we're
00:43 - starting from the basics which means
00:45 - that we're going to start from
00:46 - collecting data sets to pre-processing
00:49 - the data set to performing exploratory
00:52 - data analysis or eda and also to build
00:55 - machine learning models in order to make
00:57 - prediction as well as to obtain
00:59 - data-driven insights that will be useful
01:02 - for drug discovery and then you're also
01:04 - going to be learning how you could
01:06 - compare machine learning models and then
01:09 - selecting a suitable one for your use
01:11 - case and then finally we're going to be
01:14 - deploying the model as a web application
01:16 - and actually i've created an infographic
01:19 - where i will summarize all of the
01:21 - content of this particular video in an
01:24 - infographic style and so let's have a
01:26 - look so this is the infographic of the
01:28 - bioinformatics from scratch series that
01:31 - was on my youtube channel called data
01:33 - professor and so in this collaboration
01:36 - with free codecamp we're going to
01:38 - combine all of the six part series into
01:40 - one video course and so let's have a
01:42 - quick overview of what you'll be
01:44 - learning in this course so in part one
01:47 - you're going to be learning about target
01:49 - protein search which means that you're
01:51 - going to select a target protein of your
01:53 - interest that you will focus on for
01:55 - example if you want to have a machine
01:57 - learning model for discovering breast
01:59 - cancer drugs then you're going to select
02:02 - aromatase as your target protein search
02:04 - or if you're looking into alzheimer you
02:07 - might want to search for acetylcholine
02:09 - estrase which we'll be using in this
02:11 - particular series and then we're going
02:13 - to collect the data set from the chambo
02:15 - database using the python library from
02:18 - temple and then we get the bioactivity
02:20 - data and then we're going to pre-process
02:22 - that dropping the missing data dropping
02:25 - duplicate data and then we're going to
02:27 - label compounds according to their
02:29 - bioactivity thresholds in order to
02:31 - obtain a created data set and in part
02:34 - two we're going to perform exploratory
02:36 - data analysis whereby we're going to
02:38 - firstly clean the smiles notation which
02:41 - represent the chemical structure of the
02:43 - compound in the dataset that we're going
02:45 - to analyze in the video series and then
02:48 - we export that out as files number four
02:51 - and five which will be available to you
02:54 - in the github repo provided in the video
02:56 - description of this video and so the
02:58 - number four five here will be labeled in
03:01 - the description and so what it
03:03 - essentially mean is that they have two
03:05 - class or three class which is the y
03:08 - variable and then once we have cleaned
03:10 - the smiles notation we're going to
03:13 - calculate descriptors in order to
03:15 - perform eda and for eda we're going to
03:18 - use visual part and also we're going to
03:21 - perform statistical analysis as well and
03:23 - in part three we're going to calculate
03:25 - additional descriptor which we will be
03:28 - using in order to build machine learning
03:31 - models in the subsequent part which is
03:33 - part four and so in part four we're
03:35 - going to build a random forest model for
03:38 - performing prediction on quantitative
03:41 - data which is the pic50 and therefore it
03:44 - is called the regression model and
03:46 - finally we'll make a scatter plot in
03:48 - order to see the distribution or the
03:50 - goodness of fits of the actual value and
03:52 - also the predicted values and then in
03:55 - part five we're going to compare several
03:58 - machine learning models and then we're
04:00 - going to make a performance comparison
04:02 - plot as you see here
04:04 - and in the last part part six we're
04:07 - going to deploy the model by making it
04:09 - into a web application meaning that the
04:12 - user could input the molecule of their
04:15 - interest and then the web app will be
04:17 - making the prediction and so we have a
04:19 - lot to cover here in the six part series
04:21 - but no worries because i'm going to
04:23 - provide a step-by-step guide from the
04:25 - basics and so without further ado let's
04:28 - get started
04:33 - welcome back to the data professor
04:35 - youtube channel
04:36 - if you're new here my name is chenin
04:38 - nantan ahmad and i'm an associate
04:41 - professor of bioinformatics on this
04:43 - youtube channel we cover about data
04:45 - science concepts and practical tutorials
04:49 - so if you're into this type of content
04:51 - please consider subscribing
04:55 - so in the previous video i have shown
04:57 - you how you can apply machine learning
04:59 - on a computational drug discovery
05:01 - project particularly we downloaded a
05:04 - data set derived from the study of
05:06 - dilani and the data set is essentially a
05:10 - collection of compounds along with their
05:13 - molecular solubility value which is a
05:16 - important physical chemical property of
05:18 - compounds on whether it can be
05:21 - solubilized in water to what extent
05:24 - and so some of you might be wondering
05:26 - what if you want to collect original
05:28 - data
05:29 - let's say that you want to create a new
05:30 - data science project for your portfolio
05:33 - and you want something that is new
05:34 - original has never been done before then
05:37 - this video is for you
05:39 - because in biology there is a lot of
05:42 - unknown that is waiting to be researched
05:45 - about
05:46 - and so in this video i'm going to show
05:48 - you how you can retrieve and download
05:50 - biological activity data of compounds
05:53 - from the chamber database which you can
05:55 - subsequently use to construct machine
05:58 - learning models which is also
05:59 - technically known as quantitative
06:01 - structure activity relationship and so
06:04 - the development of such qsar or q-star
06:08 - models holds great value for drug
06:11 - discovery efforts particularly it allows
06:13 - us to understand the origins of the
06:16 - biological activity and the
06:18 - interpretation of the model will allow
06:20 - us to understand how we can design a
06:23 - better drug and so such data that you're
06:26 - going to collect and download today by
06:29 - following along with this video not only
06:31 - will allow you to build your data
06:33 - science portfolio but may also initiate
06:36 - or scratch the surface towards the
06:38 - development of novel therapeutic agents
06:40 - or novel drugs and so without further
06:43 - ado let's get started
06:45 - okay so the first thing that you want to
06:46 - do now is head over to the github of the
06:49 - data professor
06:50 - and then click on the code repository
06:55 - and then scroll down
06:57 - click on python
07:00 - scroll down and click on the cdd ml part
07:04 - 1 bioactivity data
07:10 - and then right click on the raw link
07:12 - and you want to save it into your
07:14 - computer
07:16 - or if you would like to follow along on
07:18 - the google code lab you are more than
07:20 - welcome to do so so what you want to do
07:22 - is go to colab and then click on the
07:25 - file open notebook and then click on the
07:27 - github tab
07:29 - and then type for data professor
07:33 - enter
07:35 - and then it's going to be the first file
07:36 - that you see here
07:38 - cddml part one okay and then you want to
07:40 - click on that and then it should open up
07:42 - a new notebook for you but i already
07:44 - have that so i'm going to follow the one
07:46 - that i have open right here so the
07:48 - exciting part of this video is that
07:50 - you're going to collect original data so
07:52 - it's going to be the same data that
07:54 - researchers in the field are collecting
07:57 - and publishing about and so today you're
07:59 - going to have the opportunity to
08:01 - contribute to computational drug
08:03 - discovery okay so the database that
08:05 - we're going to use is called chambo
08:07 - database and it is a database comprising
08:10 - of more than 2 million compounds and it
08:13 - is compiled from more than 76 000
08:15 - documents
08:17 - and the version as of march 25 2020 is
08:21 - chambo version 26 okay and so the first
08:24 - thing that you want to do here is to
08:26 - install the jumbo web resource client
08:29 - and so we're going to use the pip
08:30 - install here
08:31 - so this library will allow you to
08:33 - download the biological activity data
08:36 - directly from the jumbo database but
08:38 - before we do that let me show you how
08:40 - the tempo database actually looks like
08:43 - so you could search on google for chambo
08:46 - c-h-e-m-l
08:50 - okay so let's say that we're going to
08:52 - search for coronavirus
08:58 - and then we're going to go with the
09:06 - search for coronavirus in all targets
09:10 - we're going to click on that
09:14 - and so the targets here refers to the
09:17 - targeted proteins or target organism
09:20 - that the drug will act on so
09:21 - biologically these compounds will come
09:24 - into contact with the protein or the
09:26 - organism and induce a modulatory
09:29 - activity towards it it could either be
09:31 - to activate the protein or the organism
09:34 - or to inhibit it okay
09:37 - and so this will give us seven targets
09:39 - here
09:40 - and if we scroll down we're gonna see
09:41 - the type of the target would be
09:43 - comprising of organism and single
09:45 - protein
09:47 - and the single protein will be sars
09:49 - coronavirus 3c like proteinase and the
09:52 - replicase polyprotein 1ab and so these
09:56 - are for the saris coronavirus 1. and so
09:58 - as you can see that the sars coronavirus
10:01 - 2 is not yet deposited in this database
10:05 - and so we're going to work with what we
10:07 - have here okay so let's head back to the
10:09 - notebook and the chambo web resource
10:11 - client should have already been
10:13 - installed and let's proceed with
10:15 - importing the library so here we're
10:17 - going to import the pandas spd and we're
10:20 - going to use from jumbo
10:23 - webresourceclient.new client import new
10:25 - client
10:28 - okay
10:30 - and then in this section we're going to
10:31 - search for the target protein and so
10:33 - it's essentially going to be the same
10:35 - process that we're searching right here
10:38 - in the search bar we type in coronavirus
10:40 - so we're going to do exactly in the code
10:42 - but first we're going to assign new
10:45 - target to the target variable and then
10:47 - we're going to create a variable called
10:50 - target underscore query equals to target
10:53 - dot search and then the search keyword
10:56 - will go here and then we're going to
10:57 - create a target data frame and then
10:59 - we're gonna assign the target query
11:01 - inside the argument using the from
11:03 - dictionary function okay and then
11:05 - finally we're gonna display the contents
11:07 - of the data frame and then
11:09 - we're gonna run this in order to see
11:11 - that
11:12 - okay and so here we see seven results
11:15 - and it's the same thing that we see here
11:17 - seven targets okay so seven results here
11:20 - and then notice that there are two
11:21 - single protein right here and the rest
11:24 - are organism so same thing right here we
11:26 - have
11:28 - two single protein
11:30 - and the rest being organism okay
11:35 - and so in this tutorial we're going to
11:37 - use the single protein for our further
11:40 - investigation okay and so let's go to
11:42 - the next step so in this section we're
11:44 - going to select and retrieve bioactivity
11:46 - data for sars coronavirus 3c like
11:49 - proteinase which is the fourth entry
11:52 - right here
11:56 - or actually it's the fifth entry but it
11:58 - has the index number of four
12:01 - okay so it actually is fifth entry let's
12:04 - call it the fifth entry
12:10 - okay and so let's run the cell and so
12:13 - notice that the chamber id here is
12:16 - chambo 3927 so this is the target id so
12:20 - it's a unique identification of the
12:22 - target
12:23 - okay and so here we're going to define a
12:25 - variable called activity and we're going
12:27 - to use new client.activity and then
12:30 - afterward we're going to define a
12:31 - variable called res and then we're going
12:34 - to assign it the block of code here
12:35 - which is activity dot filter and then in
12:38 - the argument here we're going to use the
12:40 - target chamber id equals to the selected
12:43 - target and then we're going to have the
12:44 - closing parenthesis as part of the
12:47 - filter function and then we're going to
12:48 - apply another filter which is to select
12:51 - only the values containing ic50 for the
12:55 - column called standard type okay and so
12:58 - i'm going to show you that in just a
12:59 - moment
13:04 - and let's show the contents of the data
13:06 - frame
13:09 - because there are so many columns
13:12 - why don't i
13:14 - show only the first three
13:16 - because the font is rather big and we
13:19 - need to access the scroller here
13:23 - okay so let's find the column that i was
13:25 - talking about
13:28 - the standard type
13:30 - so here standard type ic50
13:33 - okay
13:34 - so we're gonna select only the ic50 here
13:39 - and so let me show you what are the
13:41 - unique values in the standard type
13:45 - column
13:50 - okay and so we see that there are only
13:51 - ic50 here
13:53 - okay so for this particular data set it
13:56 - wouldn't matter because all of the value
13:58 - here are the same and they are ic 50.
14:01 - but in cases of other data set there
14:04 - might be a combination of other
14:06 - bioactivity unit types so it might be
14:08 - ic50 it might be ec 50 or it could be
14:12 - percent activity so when we define a
14:15 - particular standard type here it will
14:17 - make our data set more uniform and so we
14:20 - won't have a mixture of different
14:22 - bioactivity units okay so we're going to
14:25 - use only the ic50 type and the standard
14:29 - value is the potency of the drug and so
14:32 - the number here represents the potency
14:34 - and so the lower the number the better
14:37 - the potency of the drug becomes okay
14:40 - and likewise the higher the number the
14:43 - worse the potency becomes
14:46 - okay so ideally we want to find a number
14:48 - of the standard value to be as low as
14:50 - possible meaning that the inhibitory
14:52 - concentration at 50 percent will have a
14:55 - low concentration meaning that in order
14:57 - to elicit 50
15:00 - of the inhibition of a target protein
15:02 - you would need lower concentration of
15:04 - the drug let's think of it this way the
15:06 - number here reflects the concentration
15:09 - of the drug and so the lower the
15:11 - concentration that is required the
15:13 - better it is because if you have higher
15:15 - number it means that you require more
15:17 - amount of the drug in order to produce
15:20 - the same inhibition at 50
15:22 - and so analogously let's say that if you
15:25 - could take five milliliter of a
15:27 - medication versus five liter of
15:30 - medication right but which is impossible
15:32 - in order to produce the same effect
15:34 - which one would you choose right okay so
15:37 - something to think about
15:39 - so let's go back to here all right and
15:42 - so finally we're going to write out the
15:45 - data frame into a csv file and we're
15:48 - going to call it the bioactivitydata.csv
15:52 - and we're going to have the index number
15:54 - to be false because we don't want the
15:56 - index number to be in the resulting csv
15:59 - file okay so let's write that out and
16:02 - let me mount my google drive into the
16:05 - notebook and so i'm gonna click here
16:09 - okay and so i'm gonna paste in the
16:12 - authorization code enter
16:18 - all right so it's mounted
16:20 - i think i might have already run this
16:22 - because the data folder has probably
16:25 - been created but let me check
16:41 - okay so it's right here data
16:44 - so it has already been created
16:47 - okay but for you guys let's say i create
16:50 - data too okay
16:53 - so for you guys creating a new folder
16:56 - called data in your codelab notebook
16:58 - folder for the first time would probably
17:00 - work so let's
17:02 - continue and so we're going to copy the
17:05 - bioactivity data here into the folder
17:10 - and let's ls that
17:14 - and we're going to see the bioactivity
17:15 - data so let me also add the dash l
17:18 - function here
17:19 - so i can also see the time at which it
17:22 - is created and it is created on april
17:25 - 29th so it's right now
17:27 - and so let's see the content of the csv
17:30 - let's list this again in our current
17:32 - working directory and we're going to
17:34 - take a glimpse of the contents of the
17:37 - bioactivity data and it looks like this
17:40 - so it's a csv data
17:42 - okay
17:44 - and then we're going to proceed to the
17:45 - next step we're going to do some
17:47 - handling of the missing data if there is
17:50 - any and then we're going to drop
17:52 - compounds with missing standard value so
17:55 - the thing is we have already dropped any
17:58 - missing values here okay so apparently
18:01 - for this data set there is no missing
18:03 - data however this code might come in
18:05 - handy for other data sets where there is
18:09 - missing data okay so let's proceed to
18:11 - the next step and so here we're going
18:12 - gonna do some data pre-processing of the
18:15 - bioactivity data so for the benefit of
18:18 - creating
18:19 - machine learning models where we could
18:21 - classify compounds into three categories
18:24 - as either being a active compound an
18:27 - inactive compound or an intermediate
18:29 - compound and so the active compound will
18:31 - be defined as drugs that have ic50 of
18:35 - less than one micromolar and one
18:37 - micromolar is equal to 1000 nanomolar
18:41 - and so a drug having ic50 value of less
18:44 - than a thousand nanomolar will be
18:46 - classified as active and a drug having
18:49 - ic50 value greater than 10 000 will be
18:52 - classified as inactive and drugs having
18:55 - value in between 1000 and 10 000 will be
18:58 - called intermediate so in some of the
19:00 - research projects that we have normally
19:03 - done we either use the two class or the
19:05 - three class okay and so we're going to
19:07 - use the conditions as defined here that
19:10 - i have already told you about and so
19:12 - we're going to run this block of code
19:14 - and then we're going to iterate over the
19:16 - molecule chamber id column let's go back
19:19 - here
19:23 - let me show you
19:26 - molecule
19:30 - molecule chamber id okay so this data
19:33 - set is comprised of many compounds and a
19:37 - compound is a drug a molecule a molecule
19:40 - is a chemical structure that produces a
19:43 - modulatory activity or in other words it
19:46 - exerts some effect on the target protein
19:48 - kind of like when you take medication
19:51 - and the medication exerts some effect on
19:53 - you like you might feel drowsy you might
19:55 - feel thirsty which are the side effects
19:58 - but the drug will directly act on the
20:00 - target protein in order to produce the
20:02 - desired biological effect which
20:05 - ultimately cures your symptoms and that
20:08 - is why you're taking drugs right or
20:10 - medication right and so you see that
20:12 - this is the chambo molecule id so each
20:15 - compound will be described by a molecule
20:18 - chamber id and so each row represents
20:21 - one compound and there might be a
20:22 - possibility that multiple rows will
20:25 - contain the same molecule chamber id and
20:27 - if that is the case for simplicity we're
20:30 - going to keep only one of them okay
20:32 - because we don't want any redundancy in
20:34 - the data set
20:36 - right so for the molecule chamber id let
20:39 - me show you before we iterate
20:41 - so df2 dot molecule tempo id
20:45 - so it essentially contains the chambo id
20:48 - as i have mentioned so they are the
20:50 - unique identification number of each
20:52 - molecule and so we're going to iterate
20:54 - through each of them right
20:56 - but first we're going to create a empty
20:59 - variable called mole cid and so in each
21:02 - iteration of the for loop here we're
21:03 - going to append the molecule chamber id
21:06 - into this empty variable so let's run
21:08 - that and then we're going to see that
21:10 - the mole cid contains
21:14 - the molecule tempo id again
21:20 - okay and here we're going to do the same
21:22 - thing we're going to define an empty
21:24 - variable called canonical smiles and
21:26 - then we're going to iterate over the
21:28 - canonical smiles and then we're gonna
21:30 - append it to the empty variable here and
21:33 - then we're gonna do the same for the
21:34 - standard value which is the ic50 but
21:37 - actually
21:38 - this is only one way of doing things so
21:41 - this might actually be a complicated way
21:45 - actually another way would be to simply
21:56 - so in df2 dot
21:59 - molecule
22:01 - tempo okay so we just
22:04 - call selection equals to whatever we
22:07 - want so we want the molecule
22:11 - jumbo id
22:12 - and we want the canonical
22:16 - smiles
22:19 - and we want the standard
22:22 - value
22:24 - and then df2
22:27 - we have the
22:28 - selection
22:31 - and then we're going to assign this
22:34 - to
22:34 - the f3
22:36 - actually this might be a easier way
22:40 - the f3
22:42 - right so we get the data frame here
22:44 - containing the three columns that we
22:47 - needed
22:48 - and actually we could do the same here
22:50 - as well
22:52 - canonical smiles okay i have to run it
22:54 - first
22:56 - probably have to run this
23:00 - okay
23:04 - and so it gives you the same thing so
23:08 - i'm not sure
23:10 - and we also want the bioactivity class
23:12 - as well which do we have yeah we have it
23:15 - here bioactivity class and so we're
23:17 - gonna just append to it and set to df3
23:23 - i'm going to have pd concat and then the
23:25 - f3 with the
23:27 - bioactivity
23:29 - class
23:30 - and then
23:31 - axis equals to 1.
23:36 - no
23:38 - oh so there are series and data frame
23:41 - objects
23:43 - okay so it's a list so this needs to be
23:46 - made into a data frame or a series that
23:50 - would work all right so it works so same
23:52 - thing here does it look the same yes it
23:55 - looks the same okay so actually this
23:57 - might be a easier way so let me copy
24:00 - this
24:06 - okay
24:07 - so i'm offering an alternative method
24:10 - here as well and i'm gonna move it below
24:13 - this okay so select either way
24:17 - okay so now we're gonna create a csv
24:20 - file for the pre-processed spiral
24:22 - activity data and we're assigning df3
24:25 - dot to csv and then the name of the file
24:28 - and then the index will be funnels and
24:31 - that's all so let's check the file
24:34 - ls
24:36 - okay so here it is the pre-processed
24:39 - data so let's copy that into the google
24:42 - drive
24:48 - pre-processed
24:50 - data
24:56 - let's have a look
25:03 - all right so we have both of them here
25:05 - so let me annotate this a bit
25:18 - all right so congratulations you have
25:20 - successfully downloaded the biological
25:23 - activity data from the chamber database
25:26 - and so now we can use this for
25:27 - subsequent machine learning model
25:29 - building and i'm going to cover that in
25:31 - a future video and so please stay tuned
25:33 - to that while in the meantime you could
25:35 - also use this data set that you have
25:38 - created on a data science project of
25:40 - your own or you could also modify the
25:43 - search query at the beginning
25:46 - so let me show you instead of using
25:48 - coronavirus you could use another
25:51 - keyword let's say
25:52 - aromatase
25:55 - so the aromatase
25:56 - is an enzyme as part of the cytochrome
25:58 - p450 which is responsible for breast
26:02 - cancer and so the goal of drug discovery
26:04 - effort is to find a compound or a
26:07 - molecule that will be able to inhibit
26:09 - the function of the aromatase enzyme
26:12 - okay and so here is the human aromatase
26:15 - enzyme so as you can see try out
26:17 - different keywords and see what protein
26:19 - you have and then you could use this
26:22 - novel data set in your own data science
26:25 - project
26:26 - so the possibilities are endless
26:29 - and so now you have original data that
26:31 - you could play around with that no one
26:33 - else in the world might have because you
26:35 - guys might be using different keywords
26:37 - right and so this will be a novelty in
26:39 - itself and so if this video was helpful
26:42 - to you please give it a thumbs up and if
26:44 - you haven't yet subscribed please
26:46 - subscribe to the channel for more
26:47 - awesome content on data science and as
26:50 - always the best way to learn data
26:52 - science is to do data science and so
26:54 - please enjoy the journey
27:00 - okay so today's part two of the
27:03 - bioinformatics project series where we
27:06 - will show you how to apply data science
27:08 - for drug discovery
27:11 - and in the previous video i've covered
27:13 - about how you can collect data directly
27:16 - from the bioactivity database chambo and
27:20 - so in today's video we're going to take
27:22 - a step further by computing molecular
27:25 - descriptors and we're going to then
27:27 - perform exploratory data analysis on the
27:31 - computed descriptors and so without
27:33 - further ado
27:34 - let's get started okay so the first
27:36 - thing that you want to do is head over
27:38 - to the github of the data professor and
27:41 - you want to click on the code repository
27:45 - and then scroll down and find python
27:50 - and so before we proceed with doing part
27:53 - two
27:53 - exploratory data analysis we're going to
27:56 - do a recap of part one and we're not
27:58 - going to do a ordinary recap but we're
28:01 - going to do a concise version and so in
28:03 - this concise version
28:06 - let's have a look we've trimmed down the
28:08 - code and it will be a bit more
28:10 - lightweight okay so i'll show you that
28:13 - in a moment and so what you want to do
28:14 - now is you want to scroll down and right
28:17 - click on the raw link and then save link
28:20 - as
28:21 - and then you want to save it into your
28:23 - computer
28:25 - okay and then you can follow along on
28:27 - your local computer using jupyter
28:29 - notebook however if you want to use
28:31 - google colab we can do that as well and
28:34 - so let's go to the codelab right now and
28:36 - so in colab you can click on the file
28:39 - open notebook and then click on the
28:41 - github tab and then you want to type in
28:43 - data professor
28:47 - and then go to the code repository so
28:49 - make sure that it is data professor
28:51 - slash code and then you want to click on
28:54 - the
28:54 - cddml part 1 bioactivity data concise
29:03 - okay but i'm going to use the one
29:06 - already in my collab
29:09 - and aside from part one we're gonna also
29:11 - do part two which is also in the python
29:14 - repository under code and then you wanna
29:17 - click on the cddml part two exploratory
29:20 - data analysis
29:24 - and then you to right click on the raw
29:26 - link and save link as and then also save
29:29 - it into your computer
29:31 - and so we could do the same thing right
29:32 - inside collab by file open notebook okay
29:36 - and then you want to click on the github
29:38 - and type in data professor
29:41 - slash code
29:43 - enter
29:45 - and then you want to find cdd ml part 2
29:49 - exploratory data analysis and click on
29:51 - that one okay but since i already have
29:53 - it locally i'm gonna use it okay so
29:55 - let's start with part one so you wanna
29:57 - click on the connect
30:00 - and then give it some time to load up
30:05 - okay so let's go over what i have done
30:07 - in this concise version so essentially
30:10 - there's two things so the first thing is
30:12 - that
30:14 - redundant code cells were deleted
30:17 - and the second part is that code cells
30:19 - for saving files to google drive has
30:22 - also been deleted
30:26 - and so at the end of the notebook
30:28 - we can simply download it by clicking on
30:31 - the files button on the left hand side
30:35 - of the panel and then we could download
30:37 - a copy of the zip file of the curated
30:40 - data okay so i'm going to show you that
30:42 - in just a moment alright so the notebook
30:44 - is loaded and you want to install the
30:48 - chambo web resource client
30:51 - so go ahead and run the cell
30:54 - all right so it's installed
30:56 - and let's go ahead and import the
30:58 - libraries
31:00 - and let's run the
31:02 - code cell for searching for coronavirus
31:05 - and this is the result
31:08 - and so we're gonna select the fifth
31:10 - entry which has the index number of four
31:12 - right here
31:14 - and so let's run that code cell
31:17 - so a detailed explanation of all of this
31:20 - has already been given in the previous
31:22 - video so i'm just gonna run the code
31:24 - cell one by one
31:26 - so if you want further information
31:29 - please check out the previous video of
31:31 - part one
31:35 - okay and so this is the bioactivity
31:38 - class label
31:41 - combine the data frames
31:44 - and writing out the output file
31:49 - okay and so let's download the
31:50 - pre-processed file
31:54 - and there you go
31:55 - right so a recap in order to download
31:58 - this file make sure that you hover your
32:00 - mouse over the three dotted line on the
32:02 - far right of the name of the file and
32:04 - then click on it and then choose
32:06 - download all right so we're done with
32:08 - the part one and so now let's continue
32:11 - with part two and so it should be noted
32:14 - that explanation for all of the code
32:16 - cells in this part one has already been
32:19 - given in the previous video the part one
32:22 - of the bioinformatics project series
32:25 - okay and so let's proceed to the
32:27 - contents that are intended for this part
32:30 - two of the bioinformatics project series
32:34 - so let's close this notebook and now
32:37 - let's go to the part two and make sure
32:39 - to click on the connect and then you
32:41 - want to run the code cell for installing
32:43 - conda and rd kit and so what rdkit
32:46 - essentially will allow you to do is it
32:49 - will allow you to compute the molecular
32:51 - descriptors for the compounds in the
32:54 - dataset that we have compiled from part
32:56 - one okay so let me explain again in part
32:59 - one we have already downloaded the data
33:02 - set of the biological activity from the
33:04 - chambo database and so the data set will
33:07 - comprise of the molecule names and the
33:10 - corresponding smiles notation which is
33:12 - the information about the chemical
33:14 - structure which we will use in this part
33:17 - too in order to compute the molecular
33:19 - descriptors and the data from part one
33:22 - also contains the ic50 which in part one
33:25 - we have already performed the binning
33:28 - into the bioactivity class active
33:31 - inactive and intermediate okay and so in
33:33 - this part two we're going to select only
33:37 - two by activity class which are the
33:39 - active and the inactive so that we can
33:42 - easily compare between the active
33:44 - compounds and the inactive compounds
33:46 - okay so without further ado let's have a
33:48 - look at the code so now conda and rdkit
33:50 - has already been installed
33:53 - and so let's load up the pandas library
33:56 - and make sure to click on the file
33:59 - button here on the left hand panel and
34:01 - then you want to upload and then choose
34:04 - the
34:04 - bioactivity data that we have prepared
34:08 - from the previous part one okay and so
34:11 - it's right here now it has already been
34:13 - uploaded and then we can close the panel
34:16 - here okay and so let's load up the csv
34:19 - file
34:20 - so the following block of codes we're
34:22 - going to compute the lipinski rule of 5
34:25 - descriptors or simply lipinski
34:28 - descriptors you might be wondering what
34:30 - is lepinsky descriptors well lipinski
34:33 - descriptor originates from the fact that
34:36 - christopher lipinski who is a scientist
34:39 - at pfizer came up with a set of rules
34:42 - called the rule of five which was used
34:44 - to evaluate the drug likeness of
34:46 - compounds and so the drug likeness is
34:49 - based on the
34:50 - key pharmacokinetic properties
34:53 - comprising of absorption distribution
34:56 - metabolism excretion which has an
34:58 - acronym of acne and this is also known
35:01 - as the pharmacokinetic profiles and so
35:03 - what essentially admin will tell us is
35:06 - that it will tell us the relative drug
35:08 - likeness of the compound whether it can
35:11 - be absorbed into the body distributed to
35:14 - the proper tissue and organs and become
35:16 - metabolized and eventually become
35:19 - excreted from the body and so in order
35:21 - to derive the rule of five christopher
35:24 - lipinski collected a set of fda approved
35:27 - drug that are normally administered
35:30 - orally and then based on his analysis he
35:33 - observed that the four descriptors that
35:35 - was used for his analysis had
35:38 - corresponding values in multiples of
35:40 - five as follows so the molecular weight
35:43 - should be less than 500 dalton the
35:46 - octanol water partition coefficient or
35:48 - log p has to be less than five hydrogen
35:51 - bond donors is less than five hydrogen
35:54 - bond acceptors is less than ten and so
35:56 - as you can see all of the values are
35:58 - multiples of five okay and so let's
36:01 - proceed with computing the descriptors
36:04 - so let's load up the library
36:06 - and then compute the descriptors so this
36:09 - is a custom function that was inspired
36:12 - from this link here and it was modified
36:15 - to include the descriptors for this
36:16 - analysis
36:18 - all right and so we have the lipinski
36:21 - descriptors in this data frame and in
36:23 - order to get that we're going to apply
36:25 - the custom function called lipinski
36:27 - which was the custom function here which
36:30 - takes in as input the smiles notation so
36:33 - the smile sensation contains the
36:35 - chemical information and so what the
36:37 - chemical information tells us is the
36:40 - exact atomic details of the molecule and
36:43 - so it's going to use that as the input
36:45 - to compute the molecular descriptors all
36:47 - right and so let's continue
36:50 - and so let's run that
36:51 - and let's have a look at the data frame
36:54 - all right so we can see that there are
36:56 - four descriptors that we have previously
36:59 - covered including molecular weight log p
37:02 - which will tell us the size log p will
37:04 - tell us the solubility and so this is
37:06 - the relative number of the hydrogen bond
37:09 - donors and acceptors and so we can see
37:12 - that there are a total of 133 rolls and
37:15 - four columns and as a recall the data
37:18 - frame that we have read directly from
37:20 - the curated file from part 1 is shown in
37:23 - the df data frame and so we're going to
37:26 - combine the df data frame and the
37:28 - lipinski data frame together because we
37:30 - want to have the standard value and the
37:32 - bioactivity class columns
37:36 - and so we're going to use the pd concat
37:38 - function in order to combine the df and
37:41 - df lipinski data frame and then we're
37:43 - going to put it into the df combined
37:45 - variable and then let's have a look at
37:46 - the new data frame all right and so you
37:48 - can see that the last four columns are
37:51 - integrated into the df data frame here
37:55 - all right and so the dimensions of the
37:57 - data frame is correct 133 rows and then
38:00 - the number of columns has been expanded
38:02 - to be b8
38:04 - okay and so now we're going to convert
38:05 - the standard value which is the ic50 to
38:09 - the pic50 scale and so the reason for
38:12 - doing the ic50 to pic50 transformation
38:15 - which is essentially the negative
38:17 - logarithmic transformation from the ic50
38:20 - value is that the original ic50 value
38:24 - has uneven distribution of the data
38:26 - points and so in order to make the
38:28 - distribution more even we will have to
38:30 - apply negative logarithmic
38:32 - transformation okay and so let me give
38:34 - you a challenge let me know how the
38:36 - distribution of the original ic50 looks
38:39 - like versus the pic50 that you have
38:43 - performed the transformation so let me
38:45 - know in the comments after you have
38:46 - tried this and so a hint is that what
38:49 - you can do is perform a simple scatter
38:52 - plot okay so let me know in the comments
38:54 - if you see any difference in the
38:55 - distribution of ic50 versus the pi350
38:59 - okay so let's do the actual
39:01 - transformation
39:02 - by running this custom function oh and
39:04 - one point here which is worthy to note
39:07 - is the ic50 value which is contained
39:10 - inside the standard value column has
39:13 - large numbers and the large number here
39:16 - will after performing negative logarithm
39:19 - it will become a negative value and in
39:21 - order to prevent that we're going to
39:23 - need to cap the maximum value right here
39:27 - to be here 100 million so we need to cap
39:30 - the value to be 100 million so that the
39:33 - resulting pic50 would not be less than
39:36 - 1.0 otherwise it will have negative
39:39 - values and that will make interpretation
39:41 - a bit more difficult okay so we're going
39:43 - to cap the values to a hundred million
39:46 - by creating a custom function called
39:49 - norm value and so what essentially the
39:51 - nor value function would do is that it
39:53 - will read through the individual values
39:55 - in the standard value column and if the
39:58 - value is greater than 100 million it
40:00 - will cap the value to be 100 million so
40:03 - that the value will not exceed 100
40:05 - million and so therefore after
40:07 - performing negative logarithmic
40:09 - transformation it will not be less than
40:11 - 1.0
40:12 - okay and so let's perform the norm value
40:15 - here
40:17 - let's describe the value again and
40:19 - notice that the maximum value is 1 times
40:22 - 10 to the eighth power so it is a
40:24 - hundred million whereas previously
40:28 - the value is rather big
40:31 - okay
40:34 - okay and so we're gonna apply the pic50
40:37 - function to the normalized data frame
40:39 - and then we're gonna call the new data
40:41 - frame to be df final okay and so notice
40:44 - that we have now created a new column
40:46 - called pic50 and we have already deleted
40:50 - the original ic50 column
40:54 - notice that the standard value here
40:56 - column has now been deleted and it is
40:59 - converted to be pic50 which is the
41:02 - negative logarithmic form of the ic50
41:06 - and so let's describe the data frame all
41:09 - right and so now the maximum value is
41:12 - 7.3 and the minimum value is 1.0
41:17 - and what we want to do now is to allow
41:20 - simple comparison between the two
41:22 - bioactivity classes therefore we're
41:25 - going to delete the intermediate class
41:28 - and we're going to call the new data
41:29 - frame to be df2 class
41:32 - all right and so we have now 119 rolls
41:35 - by eight columns
41:37 - and so let's perform exploratory data
41:40 - analysis using the lipinski descriptors
41:43 - and so in tem informatics or draft
41:45 - discovery we're gonna call the
41:47 - exploratory data analysis to be chemical
41:49 - space analysis because what it
41:51 - essentially does is it allows us to look
41:54 - at the chemical space and the chemical
41:56 - space is kind of like a chemical
41:58 - universe right so as jose medina franco
42:01 - post said each chemical compound could
42:03 - be thought of as like stars okay and so
42:05 - the active molecules would be compared
42:08 - to a constellation and it will be
42:10 - referred to as constellation and so he
42:12 - developed a approach which he termed
42:15 - constellation plot whereby you could
42:17 - perform chemical space analysis and
42:20 - create the constellation plot whereby
42:22 - the active molecule would be
42:23 - correspondingly have larger size in
42:26 - comparison with the less active molecule
42:29 - and so we're going to apply a similar
42:30 - concept in our plot here as i will show
42:33 - you in the next few moments here and so
42:35 - what we want to do first is import the
42:37 - library seaborn and the matplotlib as
42:40 - plt and so now we're going to create a
42:42 - simple frequency plot of the two
42:45 - bioactivity classes so using this block
42:47 - of code we're going to create a
42:49 - frequency plot comparing the inactive
42:52 - and the active molecules and in doing so
42:56 - we're also going to save it as a pdf
42:59 - file right so the x and y labels are
43:02 - obtained using these two lines of code
43:05 - and the frequency plot is using the
43:08 - count plot function where we use x
43:11 - variable to be bioactivity class right
43:13 - here and so as you can see there is no
43:15 - need to define the y variable because
43:18 - the y variable here is the frequency and
43:20 - so the edge color is black which means
43:22 - that the bar will have a black outline
43:25 - okay and so being able to save it as the
43:27 - pdf file will allow you to use the
43:30 - resulting files for your report for your
43:33 - publication for your project and as i
43:36 - have already mentioned in the part one
43:38 - of this bioinformatics project series
43:41 - these two notebooks are crafted based on
43:44 - actual research protocol that we use in
43:46 - our own research group okay so let's
43:49 - proceed
43:51 - all right so now we're going to make a
43:52 - scatter plot of the molecular weight
43:54 - versus the log p or the solubility of
43:57 - the molecules and we're first going to
43:59 - start by defining the figure size to be
44:02 - 5.5 by 5.5 and we're going to use the
44:05 - scatter plot function here whereby the x
44:08 - variable will be mw or molecular weight
44:11 - and the y variable will be the log p and
44:14 - the data will be df2 class and the hue
44:17 - here would refer to the color and so the
44:20 - color will be defined on the basis of
44:22 - the bioactivity class and because there
44:24 - are two classes you will see that the
44:26 - color comprises of blue and orange
44:29 - whereby blue will refer to the inactive
44:32 - molecule and orange will refer to the
44:34 - active molecule and the size of the data
44:36 - points here will be according to the
44:38 - pic50 values and we define the edge
44:41 - color to be black which is the edge of
44:44 - the circles and the alpha transparency
44:46 - is defined to be 0.7 and the x label and
44:50 - y label are custom here mw and log p
44:53 - with a font size of 14 and we have the
44:56 - font weight to be bold and in this line
44:58 - we're gonna define that we want the
45:00 - figure legend to be outside the plot
45:02 - otherwise it will be embedded inside
45:04 - which will make it very difficult to see
45:07 - so we opted to have the figure legend
45:10 - outside and then finally we're gonna
45:11 - save it into the pdf file so let's run
45:13 - this block of code here
45:15 - all right so it's finished and so let's
45:17 - do the same thing for the psc 50 value
45:21 - so the same concept applies just
45:22 - changing the name of the variables
45:25 - and so here we see the distribution of
45:27 - the inactive class and the active class
45:30 - and so this is to be expected because we
45:32 - use the threshold to define active and
45:35 - inactive and so the threshold that we
45:37 - used was five and six right so if the
45:39 - pxc 50 value is greater than six it will
45:42 - be active and if the psc 50 value is
45:45 - less than 5 it will be inactive and so
45:47 - you can see that the distribution of the
45:49 - inactive is rather vast in comparison to
45:52 - the distribution of the active molecules
45:55 - which is between 6 and 7 whereas the
45:57 - inactive is between 1 and five
46:00 - okay and so we're going to perform mann
46:02 - whitney u test in order to look at the
46:05 - difference between the two bioactivity
46:08 - class active and inactive and so we're
46:11 - going to apply this man whitney u test
46:13 - to test the statistical significance of
46:16 - the difference whether they are
46:17 - different or not different
46:19 - and so the code for performing the
46:21 - manually u test was modified from
46:23 - machine learningmastery.com
46:25 - and we made it into a function all right
46:27 - and so let's run it
46:29 - and let's apply the man whitney function
46:32 - to the pic50 and what it will do is it's
46:35 - going to compare the active class and
46:37 - the inactive class to see whether there
46:40 - is a statistical significance for the
46:42 - pic50 variable and so based on this
46:45 - analysis the p-value is rather low and
46:49 - therefore we reject the null hypothesis
46:52 - and therefore we can say that it is
46:55 - having different distribution meaning
46:57 - that active and inactive okay and so
47:00 - we're gonna apply the same plots and
47:03 - statistical analysis for the other four
47:06 - lipinski descriptors as well and so
47:08 - let's breeze through this
47:12 - box plot man whitney
47:14 - box plot
47:16 - man whitney
47:18 - box plot
47:19 - man whitney
47:20 - oh
47:21 - again okay
47:23 - man whitney
47:24 - box plot
47:26 - man whitney
47:28 - and so make a note that all of the files
47:30 - from the mad whitney and the box plot
47:32 - are saved as files and so the mad
47:36 - whitney has its own csv file and the box
47:39 - plot has its own pdf file and so we can
47:42 - download all of this at the end in order
47:45 - to use it for your own project and
47:48 - research
47:50 - and so let's have a look but before we
47:51 - do that let's do some interpretation of
47:54 - the results okay so let's make sense of
47:56 - the results here
47:58 - so let's start with the psg50 values so
48:00 - taking a look at the psc50 values the
48:03 - actives and inactive displayed
48:06 - statistically significant difference
48:08 - which is to be expected because the
48:10 - threshold value was already defined at
48:12 - six and five okay as i have already
48:15 - explained of the four lipinski
48:17 - descriptors only log p exhibited no
48:20 - difference between the active and the
48:23 - inactive while the other three
48:25 - descriptors comprising of mw number of
48:28 - hydrogen bond owner and acceptor shows
48:31 - statistically significant difference
48:34 - between the active and inactive okay and
48:37 - so
48:38 - okay so let's continue
48:41 - all right and so finally we're going to
48:43 - sip up all of the files comprising of
48:46 - the csv files and pdf files which was
48:49 - generated in this notebook and so all of
48:51 - the manually you test and the box plot
48:53 - will now be zipped up and we can
48:55 - conveniently download it into our
48:57 - computer
48:58 - so let's zip up the file
49:01 - and click on the file button on the left
49:03 - panel and then hover your mouse on the
49:06 - three dot alliance click on it and click
49:08 - on download all right and so it will
49:10 - download into your computer
49:13 - and so you will see
49:15 - the plots that we have generated in the
49:17 - notebook and the resulting
49:20 - men whitney you test okay all of them
49:22 - are downloaded as csv file
49:26 - alright so if you find value in this
49:28 - video please give it a thumbs up and if
49:30 - you haven't yet subscribed please
49:32 - subscribe to the channel and as always
49:34 - the best way to learn data science is to
49:36 - do data science and please enjoy the
49:39 - journey
49:44 - welcome back if you're new here my name
49:46 - is
49:48 - and i'm an associate professor of
49:49 - bioinformatics and this is the data
49:51 - professor youtube channel
49:55 - okay so in this video i'm going to
49:57 - continue with the part 3 of the
49:59 - bioinformatics project series where i go
50:02 - through how you can implement a
50:04 - bioinformatics project from scratch so a
50:07 - short recap in part one i showed you how
50:09 - you can retrieve the bioactivity data
50:12 - directly from the chambo database
50:14 - followed by a quick data pre-processing
50:17 - in part two i've shown you how you can
50:19 - calculate the lipinski descriptor and
50:21 - perform exploratory data analysis and
50:24 - this video is part three where i'm gonna
50:26 - show you how you can calculate molecular
50:28 - descriptors followed by preparing the
50:30 - data set that we will be using for the
50:32 - next part which is part four and in part
50:35 - four we're going to do some model
50:37 - building and so without further ado
50:39 - let's get started okay so the first
50:41 - thing that you want to do is head over
50:43 - to the github of the data professor
50:46 - and click on the code repository
50:50 - scroll down and click on python
50:53 - scroll down and notice that i've created
50:56 - three additional files which has the
50:59 - prefix of acetylcholine esterase which
51:02 - is the name of the target protein that
51:04 - our research group have previously
51:06 - published and the great thing about this
51:08 - target protein is that there are an
51:10 - abundance of bioactivity data and
51:12 - therefore it will be a great starting
51:14 - point for model building so essentially
51:16 - what i have done is change the name of
51:18 - the target protein in part one and then
51:21 - perform all of the code cells and did
51:23 - the same thing with part two by taking
51:26 - in the output from part one and then i
51:28 - finally exported the files from part two
51:31 - and then used it for part three which is
51:33 - today and so the export of data from
51:36 - part one and part two will be provided
51:39 - on the github of the data professor so
51:42 - it will be provided here in the data
51:44 - directory so you'll be noticing that
51:46 - there are six additional files
51:49 - containing the name essentia calling
51:50 - series 0 1 until 0 6.
51:55 - okay so let's download the part 3 asset
51:58 - coiling esterase so you could click on
52:00 - it and then
52:02 - you could right click on the raw link
52:07 - and then save it into your computer
52:11 - because i already have it in my google
52:13 - code lab so i'm going to use this one
52:15 - okay so let's begin so you're gonna see
52:17 - here that in this part three we will be
52:20 - calculating molecular descriptors and
52:22 - then we're going to prepare the data set
52:24 - which will be used for the next part
52:26 - part four and in part four we're going
52:28 - to perform some model building
52:30 - so we are going to need to download the
52:33 - paddle descriptor software which is
52:35 - provided on the github of data professor
52:40 - and i'm going to provide the link to the
52:42 - original website of the developers of
52:44 - the paddle descriptors along with the
52:46 - link to the original research paper okay
52:49 - so the paddle dot zip file has been
52:51 - downloaded along with the paddle.sh file
52:54 - which is the shell script file
52:56 - containing instructions on how to run
52:58 - the paddle calculation because here
53:00 - we're going to use paddle to calculate
53:02 - the molecular descriptors
53:04 - okay so we're going to unzip the folder
53:09 - okay and so we're going to download the
53:11 - acetylcholine
53:13 - file which is containing the pic50 along
53:16 - with the three bioactivity class
53:21 - all right and so we're going to import
53:23 - pandas as pd
53:25 - and we're going to import the csv file
53:29 - and assign it the df3 variable name
53:33 - so let's have a look at this data frame
53:36 - okay so we're going to select only the
53:37 - canonical smiles column along with the
53:40 - molecule temple id column and we're
53:42 - going to put it in the selection
53:43 - variable and then we're going to subset
53:45 - the data by using df3 bracket selection
53:49 - which contains the name of the precise
53:51 - columns that we wanted and then we're
53:53 - going to assign the name of df3
53:55 - underscore selection and then we're
53:57 - gonna save it as molecule.smi
54:01 - let's run it
54:03 - let's have a look at the file using the
54:05 - bash
54:06 - okay so it contains the
54:08 - smiles notation here and the name of the
54:12 - molecule
54:13 - so a quick recap
54:15 - these smiles notation
54:18 - represents the chemical information
54:21 - that pertain to the chemical structure
54:24 - so the c here represents the carbon atom
54:28 - the o represents the oxygen the n
54:30 - represents nitrogen okay and so let's
54:33 - continue
54:34 - and so in this line we're going to see
54:36 - how many lines of molecules do we have
54:38 - and we have 4695
54:41 - which matches this number here four six
54:44 - nine five so we wanted to check that all
54:46 - of the rows are coming in the
54:49 - molecule.smi
54:50 - file okay and so we're gonna perform the
54:53 - descriptor calculation by running bash
54:56 - paddle.sh okay so maybe you're wondering
54:59 - what is inside the paddle file
55:03 - let's have a look
55:07 - paddle.sh
55:10 - so it contains the instruction so we're
55:12 - going to use java and then we're going
55:13 - to use one gigabyte of memory
55:16 - and
55:17 - we're going to use this option because
55:20 - we don't have a display on the google
55:23 - code lab so we're going to use the java
55:25 - awt headless equal to true and then
55:28 - we're going to specify the jar function
55:30 - because we're going to use the paddle
55:32 - descriptor.jar file and then here we're
55:35 - going to remove the salt and the salts
55:37 - are the sodium and the chloride which
55:40 - are in the chemical structure and so
55:42 - this program will automatically remove
55:44 - all salts and also small organic acid
55:47 - from the chemical structure so if that
55:50 - sounded gibberish then so it essentially
55:52 - means that we are cleaning the chemical
55:55 - structure so that there are no impurity
55:57 - okay and here are the other options
55:59 - pertaining to how we also clean the
56:01 - chemical structure and then this option
56:04 - tells the program that we're going to
56:05 - compute the molecular fingerprint and
56:08 - the fingerprint type will be pubchem
56:10 - fingerprint okay and so finally it will
56:13 - output the descriptors into the file
56:15 - called descriptors output.csv
56:18 - and let's run it
56:22 - some molecules are taking 5 seconds some
56:25 - are 0.6
56:27 - okay some are fairly quick 0.3
56:30 - okay because we have 4695
56:34 - it's going to take you about 18 to 19
56:37 - minutes to complete
56:38 - so why don't we just stop this and then
56:43 - we can download directly the computed
56:45 - file which is here descriptors output
56:48 - dot csv so why don't you head over to
56:51 - the data professor github click on the
56:53 - data repository
56:56 - and then you will see this page and then
56:57 - scroll down and find descriptors output
57:00 - dot csv click on it
57:03 - and then right click on the download
57:04 - button and save link as and then
57:07 - download it into your computer
57:10 - so we're going to save it into the
57:11 - desktop and i'm going to change the save
57:13 - as type to be all files and i'm going to
57:16 - change.txt to be csv
57:22 - okay so it's automatically opening it
57:23 - for me
57:27 - okay so we're gonna go back to this
57:30 - notebook and then
57:32 - as you see it's barely
57:34 - up to about 200 so i'm gonna stop this
57:39 - if it allows me to
57:41 - all right so it stopped so let's see if
57:44 - there's any generated file so it has
57:47 - generated some output and so i'm going
57:49 - to delete this because i'm going to
57:51 - upload the completed version
57:54 - so here i'm going to click on the upload
57:56 - and then this is the desktop and i'm
57:58 - going to click on the descriptors output
58:00 - dot csv
58:02 - okay and it's currently uploading
58:04 - wait one
58:06 - moment so let's list the files so
58:10 - okay so it's uploading
58:12 - so notice the file size
58:14 - okay so it's increasing that's a good
58:16 - sign
58:17 - wait one moment it's fairly big file
58:21 - almost hang in there
58:24 - all right so it's finished so let's
58:27 - have a look again
58:29 - so it's about 8.3 megabytes
58:32 - and so we're going to read the
58:33 - descriptor's output into df3 underscore
58:36 - x
58:37 - and so here we're going to prepare the x
58:39 - and y data matrices and the x data
58:41 - matrix will comprise of the molecular
58:43 - descriptors which are the pubchem
58:46 - fingerprints so let's have a look
58:49 - and so we're going to delete the first
58:51 - column here the name
58:53 - because we want only the molecular
58:55 - features
58:57 - so let's drop it using the dot drop
58:59 - function
59:00 - and the name of the column that we want
59:02 - it to be dropped and then we see that
59:04 - the name column has been dropped and
59:06 - then we reassign it back to df3
59:08 - underscore x
59:10 - all right so now let's create the y data
59:13 - matrix
59:14 - and here we're going to take the pic 50
59:17 - column directly from the f3 data frame
59:20 - which is the initially loaded data frame
59:22 - and then we're going to assign it to the
59:24 - df3 underscore y
59:26 - and then here we're going to combine x
59:28 - and y together so this really depends
59:31 - but for portability we're gonna combine
59:33 - it and then we're gonna output it into a
59:35 - csv file which we will be uploading to
59:38 - the github and then we're gonna use that
59:40 - for part four
59:41 - okay and so here we're gonna output this
59:44 - data set 3 data frame into a csv file
59:48 - and notice that the name here is fairly
59:50 - long and the purpose for having such a
59:52 - long name is to allow less confusion and
59:55 - to allow us to easily see what is the
59:57 - purpose of this file and so the first
59:59 - segment of this name is the name of the
60:01 - target protein which is the
60:03 - acetylcholine esterase 0 6 is just the
60:06 - sequential order number and so
60:08 - bioactivity data 3 class pxc50
60:11 - essentially tell us that it contains the
60:13 - bioactivity data information along with
60:16 - three categorical class comprising of
60:19 - active inactive and intermediate and it
60:21 - also contains the psc 50 values and then
60:24 - the last segment here is pubchem fp
60:27 - signifying that it contains the pubchem
60:30 - fingerprint so this will become handy
60:32 - when we have more than one fingerprint
60:34 - type and so paddle allows you to compute
60:36 - more than 10 different fingerprint types
60:39 - and so let's make it your homework to
60:40 - try to compute other different
60:42 - fingerprint types so you want to play
60:45 - around with the options and see what
60:47 - other molecular fingerprints are
60:49 - available and then you could rename the
60:51 - file accordingly all right so let's see
60:53 - if we have already written out the file
60:55 - and it's right here
60:58 - okay and then we're gonna write out the
61:00 - file so let's run the code cell here
61:03 - and let's save it into our computer all
61:06 - right so it's finished and it's gonna
61:08 - open up for us to see all right let's
61:11 - see so these are the fingerprints
61:14 - of pubchem
61:15 - and the last column is the pic50 so
61:18 - we're going to use this file to perform
61:21 - model building in the next episode so
61:23 - please stay tuned to that so support
61:25 - this channel by smashing the like button
61:27 - subscribe if you haven't yet done so and
61:30 - click on the notification bell in order
61:32 - to be notified of the next video and if
61:34 - you have come this far in the video
61:36 - please give yourself a big clap and
61:38 - comment down below that you have watched
61:40 - until the end and big kudos to you guys
61:42 - and as always the best way to learn data
61:45 - science is to do data science and please
61:48 - enjoy the journey
61:55 - okay so welcome back to this part four
61:58 - of the bioinformatics from scratch
62:00 - series where i show you how to do a
62:02 - bioinformatics project using machine
62:04 - learning in a step-by-step manner so in
62:07 - this video we're going to build a simple
62:09 - regression model based on the random
62:11 - forest algorithm and the dataset that
62:13 - we're using is based on the
62:14 - acetylcholine esterase inhibitors which
62:16 - is derived from the previous tutorial
62:18 - videos and so without further ado let's
62:21 - get started so the first thing that you
62:22 - want to do is head over to the github of
62:24 - the data professor and then you want to
62:26 - click on the code link
62:29 - and then click on python
62:31 - and then you want to find
62:33 - cddml part four and so if you haven't
62:35 - yet gone through the previous three
62:38 - episodes here please make sure to go
62:40 - through that in the provided playlist up
62:42 - and below
62:44 - and so you wanna click on the part four
62:47 - and then
62:48 - right click on the raw link and then
62:50 - save link as and then save it into your
62:52 - computer
62:56 - all right so let's get started
62:59 - so let's connect to the google code lab
63:03 - all right so for those of you new here
63:06 - you could open the notebook directly by
63:08 - clicking on the open notebook click on
63:10 - github
63:12 - and then you type in data professor
63:17 - and then find cddml part four
63:21 - right here and then you click on that
63:22 - one
63:24 - okay so i'm going to use the one
63:25 - provided here
63:26 - so let's begin
63:28 - so the first block of code here is to
63:31 - import the necessary libraries
63:34 - so we're going to simply run that
63:37 - and then we're going to load in the data
63:38 - set that we have prepared from the prior
63:41 - videos
63:43 - so we're going to load it in
63:45 - and so the data set here is based on the
63:47 - pubchem fingerprint
63:49 - and it's going to contain the
63:51 - bioactivity data for the acetylcholine
63:53 - esters inhibitors so one of you asked a
63:56 - very great question in the prior video
63:58 - in part three we have prepared a pop can
64:01 - fingerprints and in part two we have
64:03 - prepared lipinski descriptors and then
64:05 - the question was what's the difference
64:07 - between these two so that's a very great
64:09 - question so firstly the lipinski
64:11 - descriptor will provide us with a set of
64:14 - simple molecular descriptors that
64:17 - essentially will be giving us a quick
64:20 - overview of the drug-like properties of
64:22 - the molecule and so historically
64:24 - christopher lipinski created a set of
64:27 - four descriptors that he had
64:30 - investigated in his research that are
64:32 - responsible for drug-like properties
64:35 - whereby he analyzed a set of orally
64:37 - active drugs and then he came up with
64:39 - this rule of five whereby compounds that
64:42 - are passing the rule of five will make
64:44 - good oral drugs and so for the pubchem
64:47 - fingerprints which we will be using
64:49 - today as well for the model building it
64:51 - is describing the local features of the
64:54 - molecules so the lipinski descriptor
64:57 - will be describing the global features
64:59 - of the molecule particularly the
65:01 - molecular size of the molecule the
65:03 - solubility of the molecule and also the
65:05 - number of hydrogen bond donor and
65:08 - acceptor which is the propensity to
65:10 - accept and donate hydrogen bonds and by
65:13 - local features for the pubchem i mean
65:15 - that each molecule will be described by
65:18 - the unique building blocks of the
65:20 - molecule so if we think of molecules as
65:23 - kind of like a lego building blocks so
65:25 - each molecule will be comprised of
65:27 - several lego building blocks and the way
65:29 - at which the lego building blocks are
65:31 - connected it will create a unique
65:34 - properties for the drug and that is the
65:36 - essence of drug discovery and also the
65:38 - essence of drug design so essentially
65:40 - the connectivity of the lego blocks are
65:43 - giving rise to the unique structure of
65:45 - the molecule and also the unique
65:47 - molecular properties and so therefore we
65:49 - have to find a way to rearrange the lego
65:52 - building block in such a way that the
65:54 - molecule provides the most potency
65:57 - toward the target protein that it wants
65:59 - to interact while also being safe and
66:02 - not so toxic right because if the
66:04 - molecule is toxic then you have side
66:06 - effects happening all right so we have
66:07 - already downloaded the data set now and
66:10 - so let's have a look at the input
66:11 - features so the pubchem fingerprint has
66:14 - 881
66:16 - input features so let's think of the
66:18 - input features for the pubchem
66:19 - fingerprints as kind of like a unique as
66:22 - the name implies fingerprints so each
66:24 - molecule will be given a unique
66:26 - fingerprint kind of like each of us
66:28 - humans have a unique fingerprint right
66:30 - and so the unique fingerprints of each
66:32 - molecule will allow the machine learning
66:34 - algorithm to learn from the unique
66:36 - properties in terms of the molecular
66:38 - properties of the compound and then
66:40 - create a model that will be able to
66:43 - distinguish between compounds that are
66:45 - active compounds that are inactive right
66:48 - because this is the goal of our model
66:50 - building we want to see which functional
66:52 - group or fingerprints are essential for
66:55 - designing a good drug or a potent drug
66:57 - and so the target variable that we are
66:59 - using for our prediction is called pic50
67:02 - which is the minus negative logarithm of
67:05 - the ic50 value ic50 is the inhibition
67:08 - concentration at 50
67:10 - okay so let's have a look further in 3.1
67:13 - the input features so
67:16 - notice that
67:18 - okay let me increase the font size
67:22 - it might be a bit too small here
67:26 - okay there you go so for those of you
67:28 - who are using mobile phones to look at
67:31 - this video so i'm going to increase the
67:33 - font size so let's continue so the input
67:35 - feature here x equals to df dot drop so
67:39 - we're going to drop the pic50 in order
67:41 - to create the x variable matrix okay
67:44 - let's see so the df here is reading in
67:48 - the downloaded data set file which is
67:51 - comprised of the fingerprint and the
67:53 - pic50 value okay so it's in the df data
67:56 - frame
67:57 - okay so in order to create the input
67:58 - features we're going to drop the pic50
68:01 - column because the psd50 column will be
68:03 - used as the y variable so upon dropping
68:06 - the ps350 we will have only the pubchem
68:09 - fingerprints and so we will call this x
68:12 - and then for y we're going to use df dot
68:15 - pse 50.
68:16 - okay so let's run the blocks of code
68:19 - here
68:20 - oh okay i have to run the top one here
68:22 - first
68:23 - all right and then run the x
68:26 - all right run the y
68:29 - right so x and y are loaded in and then
68:31 - we're going to have a look at the shape
68:32 - of the data
68:34 - so we have 4
68:35 - 695 rows or compounds and we have 888
68:40 - and then we have 881 pubchem
68:42 - fingerprints
68:44 - so here we're going to remove the low
68:45 - variance features
68:48 - and then we're going to have a look
68:50 - so we have 137 fingerprints left which
68:53 - is from the 881
68:56 - so variables having low variance will be
68:59 - removed
69:00 - and then we're going to split the data
69:01 - in a 80 20 fashion
69:04 - and then we're gonna look at the data
69:06 - dimension again
69:09 - all right so let's build a simple
69:11 - regression model using random forest
69:14 - and so we're gonna use an estimator to
69:16 - be a hundred and then upon building the
69:18 - model we get about 0.50
69:25 - all right so we did not set the seat
69:27 - number so it is varying over time
69:29 - because of the random features that it
69:31 - is taking to build the model all right
69:34 - so why don't we set seed here okay so
69:36 - let's set the seed number
69:39 - import numpy snp
69:44 - and then let's build the model
69:49 - five one two let's run that again
69:53 - five one two try again
69:56 - all right so you see that if we don't
69:58 - set the c number the seat number will be
70:00 - randomized and then we get different
70:02 - results so here we're setting the c to
70:04 - 100 and we're getting the same results
70:07 - so let's make the prediction
70:09 - and now here in this block of code we're
70:11 - gonna make a scatter plot of the
70:13 - experimental versus the predicted pic50
70:16 - values
70:18 - and then here you go we have a scatter
70:20 - plot of experimental and predicted
70:24 - alright so if you're finding value in
70:26 - this video please give it a thumbs up
70:28 - subscribe if you haven't yet done so hit
70:31 - on the notification bell in order to be
70:33 - notified of the next video and as always
70:35 - the best way to learn data science is to
70:38 - do data science and please enjoy the
70:40 - journey
70:45 - welcome back to part five of the
70:47 - bioinformatics project from scratch
70:50 - series where i show you how you could
70:52 - build your own computational drug
70:54 - discovery model using the machine
70:57 - learning algorithm
70:58 - in today's episode i will be showing you
71:01 - how you could compare several machine
71:03 - learning algorithms for building
71:04 - regression models of the acetylcholine
71:07 - esterase inhibitors and today we're
71:10 - going to be using a lazy and efficient
71:12 - way of building several machine learning
71:14 - algorithms and this was shown in a
71:16 - recent video using the lazy predict
71:19 - python library and so we're going to be
71:21 - using that for today's tutorial and
71:24 - before proceeding further let's do a
71:26 - quick recap
71:29 - so in part one i have shown you how you
71:32 - could collect original data set in
71:34 - biology that you could use in your own
71:37 - data science project particularly i have
71:39 - demonstrated to you how you could
71:41 - download and pre-process the biological
71:44 - activity data from the chambo database
71:46 - and the data set is comprised of
71:48 - compounds and molecules that have been
71:50 - biologically tested for their activity
71:53 - toward the target organism or protein of
71:56 - interest then
71:58 - in part two i have shown to you how you
72:00 - could calculate the lapinski descriptors
72:03 - which are descriptors used for
72:05 - evaluating the likelihood of being a
72:08 - drug-like molecule and then i've shown
72:10 - to you how you could perform some basic
72:13 - exploratory data analysis
72:18 - on these lapinski descriptors
72:20 - particularly the eda are based on making
72:23 - simple box plot and scatter plot in
72:25 - order to visualize the differences of
72:27 - the active and inactive subset of the
72:30 - compound
72:33 - in part 3 i have made some changes to
72:35 - the target protein and then we're using
72:37 - the acetylcholine esterase as it
72:40 - provides a larger data set to work with
72:42 - and so in this part we have already
72:44 - computed the molecular descriptors using
72:47 - the paddle descriptor software and then
72:50 - we prepared the data set comprising of
72:52 - the x and y data frames and then we used
72:55 - that to build a prediction model in the
72:57 - subsequent parts which is part four
73:01 - where we use the descriptors generated
73:03 - from part 3 in order to build a
73:05 - regression model using the random forest
73:07 - algorithm
73:16 - and now to today's episode let's get
73:18 - started
73:21 - so here we're going to be comparing
73:23 - several machine learning algorithms
73:25 - using the lazy predict library and so
73:27 - the first thing that you need to do is
73:29 - install the lazy predict and so in a
73:33 - prior video i've shown you how you could
73:35 - use the lazy predict to do a quick and
73:37 - rapid model building of classification
73:40 - and also regression models in just a few
73:42 - lines of code and so let's start by
73:45 - installing the library
73:51 - okay and so we have already installed it
73:54 - and then we're going to be importing the
73:56 - necessary libraries and so here we're
73:58 - using the pandas seaborn and also the
74:01 - second learn library specifically we're
74:04 - importing the train test split function
74:06 - and then we're going to be importing the
74:08 - lazy predict and also the lazy regressor
74:12 - function
74:12 - and so now we're going to be loading up
74:14 - the data set and we're going to be
74:16 - directly downloading it from the github
74:19 - of data professor
74:21 - and so the links is here wget to
74:23 - download it and now we're going to be
74:25 - reading in the file and then we're going
74:27 - to be assigning it to the df data frame
74:31 - then we're going to be splitting it up
74:33 - into the x and y variables
74:37 - and let's take a look at the dimension
74:39 - of the x variable
74:41 - and so here we see that it has a total
74:43 - of 4 695
74:46 - rows or the number of compounds in the
74:48 - data set and it has a total of 881
74:51 - descriptors or the features or the
74:54 - number of columns and so the first thing
74:56 - that we need to do is we're going to be
74:58 - removing the low variance features so
75:01 - those that have low variance
75:04 - and let's take a look at the dimension
75:05 - of the data set again and so we have a
75:07 - reduced subset from 881 to be 137
75:12 - variables now we're going to be
75:13 - performing a data split using the 80 20
75:16 - ratio
75:18 - all right now comes the fun part so as
75:20 - you can see here we're going to be
75:21 - building more than 20 machine learning
75:24 - models and so we're using only two lines
75:27 - of code here so the first one is like
75:29 - any other scikit-learn functions for
75:31 - building the model is to assign the
75:34 - machine learning algorithm into a
75:36 - classifier variable and then we're going
75:38 - to be assigning the results from the
75:41 - prediction after we built the model and
75:43 - then we're assigning it to the train and
75:45 - test variables so the train and test
75:48 - variables here will be containing the
75:50 - performance of the model's prediction
75:53 - and so let's build the model
75:57 - so here it has 39 models 39 machine
76:02 - learning algorithms
76:04 - so this might take some time because the
76:06 - data is relatively big at almost 5000
76:09 - rows
76:12 - and so it should be noted here that
76:13 - model building is using default
76:15 - parameters for all of the 39 algorithms
76:18 - used
76:21 - and so if you want to perform hyper
76:23 - parameter optimization that will be a
76:25 - topic for another video right and so
76:27 - models have been built and let's have a
76:29 - look at the train
76:37 - okay so lg bm is the best model here
76:44 - so from our prior tutorials random
76:47 - forest was used for the model building
76:49 - and so here it had
76:51 - slightly better performance
76:57 - let's have a look at the test set
77:05 - lgbm regressor
77:07 - random forest also at third place here
77:13 - but the thing is they're roughly the
77:14 - same okay 0.57 and 0.56
77:20 - let's have a look
77:24 - at the data visualization of the model
77:27 - performance
77:32 - and so the bar plot of the r squared
77:35 - values is provided here
77:37 - and we're going to have a look at the
77:39 - rmse values here
77:46 - and then we're also going to have look
77:47 - at the calculation time
77:57 - provided here
78:04 - so the longer the bars become the longer
78:07 - it takes to build the model all right
78:09 - and so congratulations we have already
78:11 - built several machine learning models
78:13 - for comparison
78:18 - in prior videos of the bioinformatics
78:21 - from scratch series you have learned how
78:23 - to compile your very own bio activity
78:26 - data set directly from the channel
78:28 - database how to perform exploratory data
78:32 - analysis on the computed lipinski
78:34 - descriptors you have also learned how to
78:36 - build random forest model as well as
78:39 - building several machine learning models
78:41 - for comparing the model performance
78:43 - using the lazy predict library and so in
78:46 - this video we will be taking a look at
78:48 - how we can take that machine learning
78:50 - model of the bioactivity data set and
78:53 - convert it into a web application that
78:55 - you could deploy on the cloud that will
78:57 - allow users to be able to make
79:00 - predictions on your machine learning
79:02 - model for the target protein of your
79:05 - interest and so without further ado
79:07 - we're starting right now
79:11 - okay so the first thing that you want to
79:13 - do is
79:15 - go to the bioactivity prediction app
79:18 - folder
79:19 - and so this folder will be provided in
79:22 - the github link in the video description
79:24 - and so before we start let me show you
79:27 - how the app looks like
79:32 - so i'm going to activate my condy
79:34 - environment
79:36 - and for you please make sure to activate
79:38 - your own content environment as well so
79:40 - on my computer i'm using the data
79:42 - professor environment so i'm going to
79:45 - activate it by typing in conda activate
79:48 - data professor
79:50 - and i'm going to go to the desktop
79:52 - because that is where the streamlet
79:54 - folder resides
80:01 - and then we're going to go to the
80:03 - bioactivity folder
80:09 - let's have a look at the contents so the
80:11 - app.py will be the application and so
80:15 - we're going to type in streamlit run
80:18 - app.py in order to launch this
80:21 - bioactivity prediction app
80:27 - okay and so this is the bioactivity
80:29 - prediction app that i'm going to be
80:31 - showing you today how you could build
80:33 - one so let's have a look at the example
80:35 - input file
80:38 - so this is the example input file
80:41 - so in order to proceed with using this
80:44 - app
80:45 - we're going to have to upload the file
80:48 - drag and drop right here or browse files
80:50 - and select the input file and so while
80:53 - waiting for a input file to be uploaded
80:55 - you can see here that the blue box will
80:58 - be giving us a waiting message so it's
81:01 - saying upload input data in the sidebar
81:04 - to start
81:05 - so essentially the input file contains
81:08 - the smiles notation and the channel id
81:12 - and so the tempo id you can think of it
81:14 - as kind of like the name of the molecule
81:17 - here and particularly the tempo id is a
81:20 - unique identification number of this
81:22 - particular molecule that chambo database
81:25 - has assigned to it and the smiles
81:28 - notation here is a one-dimensional
81:31 - representation of this particular
81:33 - chemical structure and so this smiles
81:36 - notation will be used by the paddle
81:39 - descriptor software that we're going to
81:41 - be using here today in the app in order
81:43 - to generate molecular fingerprint which
81:46 - describe the unique chemical features of
81:48 - the molecule and then such molecular
81:51 - fingerprints will then be used by the
81:53 - machine learning model to make a
81:55 - prediction
81:57 - okay and so the prediction will be the
81:59 - pic50 values that you see here and the
82:02 - psc 50 value is the bioactivity against
82:06 - the target protein of interest and so in
82:08 - this application the target protein is
82:11 - acetyl cholinesterase and this target
82:14 - protein is a target for the alzheimer's
82:17 - disease okay and so this app is built in
82:20 - python using the streamlight library and
82:23 - molecular fingerprints are calculated
82:26 - using the paddle descriptor
82:41 - and so
82:42 - back in 2016 we have published a paper
82:45 - describing the development of a q star
82:48 - model for predicting the bioactivity of
82:52 - the acetylcholine series and so if
82:55 - you're interested in this article please
82:56 - feel free to read it so i'm going to
82:58 - provide you the link in the video
83:00 - description as well
83:04 - okay so let's
83:06 - drag and drop the input file
83:09 - so
83:10 - example as it took one in series i'm
83:13 - going to drag a drop here
83:16 - and then in order to initiate the
83:18 - prediction i'm going to press on the
83:21 - predict button
83:23 - and as you see here the input file is
83:26 - giving you this data frame
83:29 - and then it's calculating the descriptor
83:32 - and the calculated descriptor is
83:34 - provided here
83:35 - in this particular data frame so you're
83:37 - going to see here that there are a total
83:39 - of five input molecules and there are
83:42 - 882
83:44 - columns and you're going to see here
83:46 - that the first column is the chamber id
83:50 - so in reality you're going to have a
83:52 - total of
83:53 - 881 molecular fingerprints and the
83:56 - molecular fingerprints that we're using
83:58 - today is the pubchem fingerprint and
84:01 - because we have previously built a
84:03 - machine learning model which i will be
84:05 - showing you using this file the jupyter
84:07 - notebook file
84:09 - we had reduced the number of descriptors
84:12 - from 881 to 217. no actually 218 because
84:16 - we have already deleted the first column
84:18 - the name of the the symbol id column and
84:21 - so we have reduced from 881 columns to
84:24 - 218 columns okay and so in the code
84:28 - we're going to be selecting the same 218
84:31 - columns that you see here which
84:33 - corresponds to the descriptor subsets
84:36 - from the initially full set of 881 okay
84:40 - so we're going to use the 218 as the x
84:43 - variables in order to predict the psa 50
84:46 - and finally we have the prediction
84:48 - output and the last data frame here and
84:51 - we have the corresponding tempo id and
84:53 - then we could also download the
84:55 - prediction by pressing on this link
85:04 - and then the prediction is provided here
85:06 - in the csv file
85:09 - okay
85:11 - so the data is provided here
85:14 - all right and so
85:16 - let's get started shall we
85:25 - okay so we have to first build our
85:27 - prediction model using the jupyter
85:29 - notebook and then we're going to
85:31 - save the model as a pickle file right
85:34 - here okay so let me show you in
85:37 - which will take just a moment so let me
85:40 - open up a new terminal
85:43 - and then i'm going to
85:45 - activate jupiter
85:46 - typing in jupyter notebook
85:50 - okay so i have to first activate condy
85:53 - environment
85:55 - kind of activate data professor so it's
85:57 - the same environment and then jupyter
85:59 - notebook
86:03 - all right there you go and then i'm
86:05 - going to open up the jupyter notebook
86:09 - all right and here we go so actually
86:11 - this was adapted from one of the prior
86:14 - tutorials in this bioinformatic from
86:16 - scratch series and essentially we're
86:19 - going to just download the calculated
86:22 - fingerprints from the github of data
86:24 - professor using this url link
86:27 - and so we're importing pandas as pd
86:30 - and then we're downloading and reading
86:32 - it in using pandas and the resulting
86:35 - data frame looks like this and so you're
86:37 - going to see here that we have
86:40 - all of this so one column the last
86:42 - column is pic50 and then we have 881
86:46 - columns for the pubchem fingerprints
86:50 - and then the next cell here is we're
86:52 - going to be
86:53 - dropping the last column or the pic50
86:56 - column
86:57 - in order to assign it to the x variable
87:01 - and then we're going to just select the
87:04 - last column denoted here by -1
87:08 - and assigning it to the y variable
87:11 - and so now that we have the x and y
87:13 - separated
87:14 - we're going to next remove the low
87:16 - variance feature from the x variable so
87:20 - initially we have 881
87:23 - and so applying a threshold of 0.1 this
87:26 - resulted in 218 columns and then we're
87:30 - going to be saving it into a descriptor
87:33 - list dot csv file so let me show you
87:36 - that
87:39 - descriptor lists the csv file
87:42 - okay and then you're going to see here
87:43 - that the first
87:45 - row will contain the names of the
87:47 - fingerprints that are retained in other
87:50 - words the name of the descriptors of the
87:53 - 218 columns here
87:55 - we here can see that top can fingerprint
87:58 - 0 1 2 has been removed and we have
88:01 - fingerprint 3 and fingerprints 4 until
88:04 - 11 has been removed
88:06 - fingerprint 14 has been removed
88:09 - fingerprint 17 has also been removed
88:12 - so more than 600 fingerprints have been
88:16 - deleted from the x variable and so the
88:18 - removal of excessive redundant features
88:21 - will allow us to build the model much
88:24 - quicker
88:25 - okay and so in just a few moments i will
88:27 - be telling you how we're going to be
88:29 - making use of this descriptor list in
88:32 - order to select the subsets from the
88:35 - computed descriptors that we obtain from
88:38 - the input query
88:41 - right here let me show you
88:43 - that we get from the input query right
88:45 - here
88:46 - so out of this smile citation
88:49 - we generated 881 columns
88:53 - and then we're going to be selecting a
88:55 - subset of 218
88:58 - from the initially 881 by using this
89:01 - particular list of descriptors okay
89:08 - and let's go back to the jupiter
89:11 - notebook
89:13 - all right
89:16 - let's save it
89:21 - and then we're going to be building the
89:23 - model we're in the forest model
89:26 - we're setting here the random states to
89:28 - be 42 the number of estimators to be 500
89:32 - and we're using the random force
89:33 - regressor and we fit the model here in
89:36 - order to train it and then we're going
89:38 - to be calculating the score which is the
89:41 - r2 score
89:43 - and then we're assigning it to the r2
89:45 - variable and then finally we're going to
89:48 - be applying the trained model to make a
89:51 - prediction on the x variable which is
89:53 - the training sets also and then we're
89:56 - assigning it to the wide red variable
90:02 - okay so here we see that the r squared
90:04 - value is 0.86
90:07 - and then let's print out the performance
90:11 - mean squared error of 0.34
90:13 - and let's make the scatter plot of the
90:15 - actual and predicted values
90:19 - okay so we get this plot here
90:22 - and then finally we're going to be
90:23 - saving the model by
90:25 - dumping it using the pickle function
90:28 - pico dot dump and then as input argument
90:30 - we're going to have model and then we're
90:32 - going to save it as essential calling
90:34 - series model dot pkl
90:38 - and there you go we have already saved
90:40 - the model okay so i'm going to go ahead
90:42 - and close this stupid notebook
90:52 - and let's help over back
90:54 - and
90:55 - let's take a look at the app.py file
91:00 - okay so let's have a brief look you're
91:03 - going to see here that the app.py is
91:06 - less than
91:07 - 90 lines of code and about 87 to be
91:10 - exact and you're going to see that there
91:12 - are some white spaces so even if we
91:15 - delete all of the white space it might
91:17 - be even less maybe 80 lines of code
91:20 - okay so the first seven lines of code
91:23 - will be importing the necessary
91:25 - libraries and so we're making use of
91:28 - streamlit as the web framework and we're
91:30 - using pandas in order to display the
91:33 - data frame and the image function from
91:35 - the pil library is used to display this
91:39 - illustration and the descriptor
91:41 - calculation will be made possible by
91:43 - using the subprocess library so that
91:46 - will allow us to compute the paddle
91:48 - descriptor via the use of java and we're
91:51 - using the os library in order to perform
91:54 - file handling so here you're going to
91:56 - see that we're using the os dot remove
91:59 - in order to remove the molecule.smi
92:02 - file so i'm going to explain to you that
92:03 - in just a moment base64 will be used for
92:07 - encoding decoding of the file
92:10 - when we will make the file available for
92:12 - download the prediction results and the
92:15 - pickle library will be used for loading
92:17 - up the pickled file of the model okay
92:21 - and so you're going to be seeing here
92:23 - that we're making three custom functions
92:26 - so lines 10 through 15 the first custom
92:29 - function will be our molecular
92:31 - descriptor calculator
92:33 - so we're defining a function called desk
92:37 - calc and then the statement underneath
92:39 - it will be the bash command and so this
92:42 - bash command is what we're normally
92:44 - using when we type into the command line
92:48 - okay and so this
92:50 - option here will allow us to run the
92:52 - code in the command line without
92:54 - launching a gui version of paddle
92:56 - descriptor and so without this option
92:59 - here it will launch a gui version but
93:02 - since we don't want that to happen we're
93:04 - going to use this option
93:06 - okay and so we're using the jar file to
93:09 - make the calculation of the fingerprints
93:11 - and then you're gonna see here that we
93:13 - have additional options such as removing
93:16 - salt standardizing the nitro group of
93:18 - the molecule and then we're using the
93:20 - fingerprint to be the pubchem
93:22 - fingerprint using the xml file here and
93:26 - then finally we're generating the
93:28 - molecular descriptor file by saving it
93:30 - to the descriptors underscore output.csv
93:33 - file
93:34 - and so this batch command will be
93:36 - serving as input right here
93:39 - in the subprocess dot p open function
93:42 - okay
93:44 - and then finally after the descriptor
93:46 - has been calculated we're removing the
93:49 - molecule.smi file and so the
93:51 - molecule.smi file will be generated in
93:54 - another function so i will be discussing
93:56 - that in just a moment
93:59 - and the second custom function that
94:00 - we're generating here is file download
94:03 - so after making the prediction we're
94:05 - going to be encoding decoding the
94:08 - results and then the output will be
94:09 - available as a file for downloading
94:12 - using this link and the third function
94:14 - that we're creating is called build
94:17 - model so it will be accepting the input
94:20 - argument which is the input data
94:22 - and then it will be
94:24 - loading up the pickle file which is the
94:27 - built model into a load model variable
94:31 - and then the model which we have loaded
94:34 - will be
94:34 - used for making a prediction on the
94:37 - input data which is specified here
94:40 - and after a prediction has been made
94:42 - we're going to be assigning it to the
94:44 - prediction variable then we're going to
94:46 - be printing out the header called
94:48 - prediction output which is right here
94:52 - and underneath it we're going to create
94:54 - a variable called prediction output and
94:56 - we're going to be creating a pd dot
94:58 - series so essentially it is a column
95:01 - using pandas and so the first column is
95:04 - prediction and then we're naming it
95:06 - pic50 which is here
95:11 - and then we're going to create another
95:13 - variable called molecule name and the
95:16 - column that we're creating
95:18 - is the chamber id or the molecule name
95:21 - which is right here
95:23 - the first column
95:25 - and then we're going to be combining
95:27 - these two columns given by the
95:30 - individual variables called prediction
95:33 - output
95:34 - and molecule name so we're using the
95:36 - pd.concat function
95:39 - and then in bracket we're using molecule
95:41 - name which is the first column
95:43 - prediction output which is the second
95:45 - column and then we're using an axis
95:47 - equals to one in order to tell it to
95:49 - combine the two
95:51 - variables or the two columns in a
95:53 - side-by-side manner okay so axis one
95:56 - will allow us to have the two columns
95:58 - side by side otherwise it will be
96:00 - stacked underneath it so psv50 column
96:03 - will be stacked underneath the molecule
96:05 - name if the axis was to be zero
96:09 - okay and finally we're writing out the
96:10 - data frame which is here and then we're
96:13 - allowing it to
96:14 - generate the download link which is
96:17 - right here and we're making use of the
96:20 - file download function described earlier
96:22 - on here okay and then aligns number 38
96:27 - we're generating this or displaying this
96:30 - image of the web app
96:33 - okay and lines number 43 until 51 or 52
96:39 - is the header here the bioactivity
96:41 - prediction app title and then the
96:43 - description of the app and then the
96:45 - credits of the app and this is written
96:48 - in markdown language
96:50 - all right and so let's have a look
96:52 - further
96:53 - lines 55 until 59 will be displaying the
96:58 - sidebar
96:59 - right here so 55
97:01 - will be displaying the header number one
97:04 - upload your csv data and then we're
97:06 - creating a variable called uploaded file
97:09 - and here we're using the st.sidebar dot
97:12 - file loader file uploader and then as
97:15 - input argument we're displaying the text
97:18 - upload your input file which is also
97:20 - right here
97:22 - and then the type of the file will be
97:24 - the txt file so right here
97:28 - and then we're creating a link using
97:30 - markdown language to the example file
97:33 - provided here to the example essential
97:35 - coding series so it's going to be the
97:37 - exact same file that we have selected as
97:40 - input
97:42 - okay so that's the sidebar
97:45 - function that you see here all right and
97:47 - so let's have a look for
97:49 - so here you can see that from line 61
97:52 - until 87 we have the if and else
97:55 - condition so if we click on the predict
97:58 - button which is right here using the
98:01 - st.sidebar dot button function with
98:04 - input argument of predict so if we click
98:06 - on it it will make the descriptor
98:08 - calculation and apply the machine
98:11 - learning model to make a prediction and
98:13 - finally displaying the results of the
98:16 - prediction right here and allow the user
98:18 - to download the predictions however if
98:21 - we didn't click anything whereby we
98:24 - loaded up the web page from the
98:26 - beginning as i will show you right now
98:28 - you will see a blue box
98:30 - displaying the message of upload input
98:33 - data in the sidebar to start okay so two
98:36 - conditions if the predict button is
98:38 - clicked it will make a prediction
98:40 - otherwise it will just display the text
98:42 - here that it is waiting for you to
98:45 - upload the input data okay so let's have
98:47 - a look under the if condition so upon
98:50 - clicking on the predict button
98:52 - as you have guessed it will load the
98:54 - data that you had just drag and dropped
98:56 - and then it will be saving it as a
98:59 - molecule.smi file and this very same
99:02 - file here molecule.smi
99:04 - will be used by the desk calculation
99:07 - function that we have discussed earlier
99:10 - on particularly the molecule.smi file
99:13 - will be used by the paddle descriptor
99:15 - software for the molecular descriptor
99:18 - calculation and after the descriptors
99:21 - have been calculated we will assign it
99:23 - as the x variable
99:26 - it's right here
99:27 - okay so i'm going to tell you in just a
99:29 - moment lines number 65 will be printing
99:32 - out the header right here
99:34 - so let me make a prediction first so
99:36 - that we can see
99:38 - let's drag and drop the input file
99:43 - press on the predict button
99:45 - it's right here original input data
99:49 - line number 65.
99:51 - line number 66 will be printing out the
99:53 - data frame of the input file
99:56 - so you're going to see here two columns
99:58 - the smile citation
100:00 - which represent the chemical structure
100:02 - information and the jumbo id column
100:05 - lines number 68 will be displaying a
100:08 - spinner so upon
100:10 - loading up this
100:12 - results here by pressing on the predict
100:14 - button you saw earlier on that they had
100:16 - a yellow message box saying calculating
100:20 - descriptor and so underneath we have the
100:22 - desk calculation function and after it
100:24 - is calculated it will be displaying the
100:27 - following content the calculated
100:29 - molecular descriptor which follows here
100:32 - on lines number 72
100:35 - right here calculated molecular
100:36 - descriptor and then it will be reading
100:39 - in the calculated descriptor from the
100:41 - descriptor output.csv file it will be
100:44 - assigning it to the desk variable then
100:47 - we're going to be writing out right here
100:50 - and showing the data frame of the
100:52 - descriptors that have been calculated
100:54 - and then we're going to be printing out
100:56 - the shape of the descriptor and so we
100:58 - see here that it has five rows or five
101:01 - molecules
101:02 - 880
101:04 - molecular fingerprints
101:06 - and then in lines number 78 until 82 is
101:10 - going to be the subset of descriptor
101:12 - that is read from the previously built
101:15 - model from the file descriptor list dot
101:18 - csv and so you can see here that we're
101:21 - going to create a variable called x list
101:23 - and then we're reading in the columns
101:26 - okay and then we're going to be from the
101:28 - initial descriptor of 881
101:32 - we're going to be selecting a subset
101:35 - provided in the x list and then we
101:37 - assign the subset of descriptor which is
101:40 - 218 descriptors selected from the
101:44 - initially set of 881 and then we
101:47 - assigned that to the desk subset
101:49 - variable and then finally we print it
101:51 - out as a data frame and we also print
101:54 - out the dimension as well so we see here
101:56 - that there are five molecules and 218
102:00 - columns or 218 fingerprints and finally
102:04 - we make use of this calculated molecular
102:06 - descriptor subset and use it as an input
102:09 - argument to the build model function and
102:12 - then as i have mentioned earlier on it
102:15 - will be building the model and then
102:17 - finally it will be displaying the model
102:19 - prediction result right here so users
102:22 - can download it into their own computer
102:25 - thank you for watching until the end of
102:27 - this video and if you enjoy
102:29 - bioinformatics tutorial then you might
102:31 - want to also check out my youtube
102:33 - channel where i have several other
102:35 - bioinformatics tutorial and content
102:37 - where i show you how you could use
102:39 - python or r to make sense of biological
102:42 - data sets and i like to end my videos by
102:45 - saying the best way to learn data
102:47 - science is to do data science and please
102:50 - enjoy the journey