00:00 - prepare for a job interview about deep
00:01 - learning with this course from tdiv she
00:04 - goes over 50 common interview questions
00:07 - related to deep learning she's an
00:09 - experienced data science professional
00:11 - and has published papers in machine
00:13 - learning scientific
00:16 - journals I'm DF from luner Tech in this
00:19 - course I'm going to teach you everything
00:21 - that you need to know from the topic of
00:24 - deep learning I'm going to help you
00:26 - prepare for your deep learning
00:28 - interviews we are going to answer answer
00:30 - 15 most popular deep learning questions
00:32 - out there that you can definitely expect
00:35 - if you are going to participate in data
00:38 - science machine learning engineering but
00:40 - alsoo AI engineering and research
00:42 - scientist interviews deep learning is
00:45 - one of the most popular topics this days
00:48 - because it forms the Cornerstone of
00:50 - topics such as large language models but
00:52 - also the generative AI it contains many
00:55 - complex Concepts it combines linear
00:58 - algebra mathematics differental ition
01:00 - Theory and also uh Advanced algorithms
01:03 - and they all come together to form this
01:05 - field of AI which is the fundamental
01:08 - part of generative AI the large language
01:11 - models so if you want to get a job in
01:13 - large language models or geni then
01:16 - definitely uh prepare for this type of
01:18 - interviews because deep learning is
01:20 - going to be a concept that your
01:21 - interviews will test you for this course
01:24 - I have specifically designed in such way
01:27 - that we will go from very basic
01:29 - questions to more advanced one so let's
01:31 - first look into the first 50 questions
01:33 - that we will be covering as part of this
01:35 - course the first 10 will be the basic
01:37 - concept so we will start uh very basic
01:40 - we will talk about the concept of deep
01:42 - learning uh what is the difference
01:44 - between deep learning and machine
01:45 - learning we'll also talk about the
01:47 - concept of neural network we will look
01:49 - into its architecture also what is the
01:51 - concept behind neurons what is back
01:53 - propagation how does it work how it is
01:56 - different from GD what is GD what are
01:59 - those different op optimizers we will
02:01 - also be uh covering different activation
02:03 - functions which are really important and
02:06 - we will be covering them in detail to
02:08 - also understand the concept of
02:10 - saturation how this is causing Ving
02:12 - gradient problem and also we'll be
02:14 - covering the um different aspects of
02:17 - just the entire training and testing
02:20 - process behind neural
02:21 - networks and then we will move towards
02:24 - more complex uh topics and those will be
02:27 - the next 10 questions so we will be
02:30 - covering the concept of back propagation
02:32 - in more detail the relationship with the
02:35 - gradient descent and we'll be covering
02:37 - the vanishing as well as the exploring
02:39 - gradient uh problem we will also be
02:42 - covering various uh question specific
02:44 - and uh um problems related to deep
02:48 - learning models that you will be asked
02:50 - during your interviews we will also be
02:52 - covering the loss function and a various
02:55 - type of uh popular loss functions that
02:57 - uh you can expect questions about during
03:00 - your deep learning
03:01 - interviews in the next 10 questions we
03:04 - will be covering the following 10
03:06 - questions so we will be covering the uh
03:09 - SGD algorithm so more Advan version of
03:12 - GD we will be covering the SGD with
03:14 - momentum what is the role of this
03:16 - momentum we will also be covering the
03:18 - different versions and variants of GD
03:21 - something that comes uh time and time
03:23 - again during the Deep learning
03:25 - interviews such as the badge GD mini
03:27 - badge GD and the SGD
03:30 - what is the difference between them what
03:32 - is the function of bed size another very
03:35 - popular question you can expect we are
03:37 - also going to cover the idea of uh
03:40 - Haitian uh how Haitian is being used as
03:43 - part of different optimization
03:44 - algorithms also what is the RMS prop and
03:48 - what is the U Infamous Adam
03:52 - algorithm then as part of the next 10
03:54 - questions we will be covering bit more
03:56 - complex uh topics we will be covering
03:59 - the concept of Adam W and then we'll be
04:02 - covering the batch normalization what is
04:04 - the difference between batch and layer
04:07 - normalization uh how layer
04:09 - normalizations are used as part of
04:11 - Transformers what is the residual
04:13 - connection uh what is the function of
04:15 - residual connections and we'll also be
04:17 - talking about gradient clipping cier
04:20 - initialization as well as the uh the
04:22 - ways that we can solve vention gradient
04:24 - problem and the exploding gradient
04:26 - problem then we will also be covering
04:28 - the concept of overfitting definitely
04:31 - something that you can uh expect during
04:33 - your deep learning interviews then as
04:36 - part of the uh final 10 questions of
04:39 - this part of the course we will be
04:41 - covering the concept of Dropout and the
04:44 - regularization in neural networks we
04:46 - will be covering the uh overfeeding
04:48 - problem and how it can be solved in
04:50 - neural networks also what is the
04:52 - difference between drop out and random
04:54 - Forest so different tricky questions
04:57 - that you can expect that have the goal
04:59 - to confuse you but we will clear out
05:01 - everything such that you don't need to
05:04 - uh confuse anything during your
05:06 - interviews then we'll be covering the
05:08 - concept of uh the training versus
05:11 - testing the adjustment when it comes to
05:13 - Dropout we'll be covering the L1
05:15 - regularization L2 regularization what is
05:18 - the difference between them and the
05:20 - impact on the weights uh the
05:22 - penalization impact and difference how
05:25 - severe they impact the uh moral
05:28 - performance
05:30 - we'll also be covering the concept of uh
05:33 - curse of dimensionality and how deep
05:35 - learning is solving this
05:37 - problem then we will be covering the uh
05:40 - concept of generative models honestly
05:43 - more and more people are getting into
05:45 - deep learning so whenever you want to
05:47 - get into AI or in machine learning you
05:51 - just want to start from the fundamentals
05:53 - and to understand those more complex
05:55 - models such as Transformers uh GPT
05:58 - series T5 in order to understand those
06:01 - mechanisms you need to know the basics
06:04 - and those are the Deep learning
06:05 - fundamentals something that we are going
06:07 - to cover as part of this course because
06:10 - once you know the fundamentals then you
06:12 - will also be prepared to answer more
06:13 - advanced questions and uh also to be
06:16 - able to answer the follow-up questions
06:18 - because during the Deep learning
06:19 - interviews you are not only going to
06:22 - answer the questions quickly and in a
06:24 - scripted way but you will most likely
06:27 - get those follow-up questions and I'm
06:29 - going to help you with my detailed
06:31 - explanations in such a way that you will
06:33 - not only be prepared to a those uh basic
06:37 - questions but also the corresponding
06:38 - followup more advanced uh questions do
06:42 - note that the idea of this course is not
06:44 - to learn all the concepts from scratch
06:46 - but instead to either quickly refresh
06:49 - your memory from Deep learning or to uh
06:52 - prepare for your deep learning
06:53 - interviews and you already know the most
06:55 - of these Concepts there are uh quite a
06:58 - lot of complex uh topics that we will be
07:00 - covering as part of deep learning and uh
07:03 - I would suggest you to First refresh
07:05 - your memory when it comes to statistics
07:08 - machine learning and also various type
07:11 - of mathematical Concepts such as the uh
07:13 - matrix multiplication with linear
07:15 - algebra differentiation Theory and then
07:18 - get into uh this interview preparation
07:21 - course when it comes to the uh
07:23 - fundamentals to statistics if you want
07:25 - to refresh your memory in this topic uh
07:28 - before getting into this course I will
07:30 - put a link uh that will forward you to
07:33 - the fundamentals to statistics course as
07:35 - part of lunar Tech where you will be
07:37 - able to learn the statistical Concepts
07:39 - or refresh your memory and when it comes
07:42 - to machine learning my fundamental to
07:44 - machine learning handbook uh from free
07:46 - Cod camp or uh fundamentals to machine
07:49 - learning course as part of ltech boot
07:52 - camp will be able to help you to either
07:55 - learn or refresh your machine learning
07:57 - memory and once you have done that all
08:00 - then you are ready to conquer your deep
08:03 - learning interviews I hope you guys will
08:05 - stick around cuz this course is going to
08:07 - be very helpful for anyone who wants to
08:09 - prepare for deep learning interviews so
08:12 - if you're ready I'm really excited so
08:15 - let's get started so the first question
08:17 - is what is deep learning so in this
08:20 - question your interviewer wants to
08:22 - understand whether do you have an
08:23 - understanding where the Deep learning
08:25 - comes in comparison to machine learning
08:27 - in comparison to AI and you can briefly
08:30 - mention about the idea of neural
08:32 - networks and how it is related to human
08:34 - brain so deep learning is a subset of
08:38 - machine learning which is then a subset
08:40 - of AI so a branch of artificial
08:42 - intelligence which involves training
08:44 - artificial neural networks on large
08:47 - amount of data in order to identify and
08:50 - learn those hidden patterns nonlinear
08:53 - relationships in a data which is
08:56 - something that the traditional machine
08:58 - learning models like linear regression
09:00 - or logistic regression or random forest
09:03 - and exus are not doing so well as the
09:07 - Deep learning models and the the heart
09:10 - of deep learning is the concept of
09:12 - layers because the artificial neuron
09:14 - networks and just in general the Deep
09:16 - learning tries to replicate uh the way
09:19 - human brains work and the way we
09:22 - intercept the and take the input signals
09:25 - from outside world when we see something
09:27 - or hear something or smell something and
09:30 - then use that information in order to
09:32 - learn and in order to whether to make
09:34 - conclusions or to do something and
09:38 - that's exactly what deep learning tries
09:40 - to do it tries to use this idea of naral
09:44 - networks which are deep learning models
09:47 - in order to take the input data in the
09:50 - input layer and then transform this
09:53 - through the activation functions to
09:55 - activate these neurons and then
09:58 - transform it into into activations and
10:01 - this will be in our hidden layers and
10:03 - the magic happens in a deep learning
10:05 - model to identify the nonlinear
10:07 - relationships in the input data and then
10:10 - what it does is that it produces then
10:12 - the output so this is at a very high
10:15 - level the interviewer doesn't want you
10:17 - to explain the idea of Weights the idea
10:19 - of parameters which we will talk about
10:21 - later but then the intervie wants to
10:23 - know whether you have higher level
10:25 - understanding what this deep learning is
10:28 - and maybe also to keep a few examples of
10:30 - deep learning models definitely mention
10:32 - this relationship between the way the
10:34 - human brains and the and the neural
10:36 - networks you specify this idea of input
10:39 - data input signals and activations and
10:41 - hidden layer don't go too much into
10:44 - detail of the architecture of neural
10:46 - networks and do briefly mention that the
10:49 - that these deep learning models examples
10:51 - of deep learning models are for instance
10:54 - artificial neural networks uh reur
10:56 - neural networks lstms the different
10:59 - sorts of more advanced architecture such
11:01 - as variational outo encoders they're
11:03 - also based on neural networks you get
11:06 - the idea so if I were to answer this
11:10 - question very short I would just say
11:12 - that deep learning models are a subset
11:14 - of machine learning models on its turn a
11:16 - branch of artificial intelligence that
11:18 - tries to learn complex data and large
11:21 - amount of data and discover hm patterns
11:23 - and complex structure in this pattern
11:26 - for various type of tasks including
11:29 - computer vision including a machine
11:31 - translation task lot of NLP models um
11:34 - the INF famous Chad gbt so gbt models or
11:38 - T5 all these large language models they
11:40 - are all based on Neal networks and they
11:42 - are deep learning models so the next
11:46 - question is how does de learning differ
11:48 - from the traditional machine learning
11:50 - models here your interview wants to
11:52 - understand whether you know what are the
11:55 - main advantages of the deep learning
11:57 - models compared to machine learning
11:58 - model models so do mention a few
12:01 - examples of traditional machine learning
12:03 - models and then mention a few examples
12:06 - of deep learning models and try to put
12:09 - the focus on one or two advantages of
12:12 - the deep learning models so a few
12:15 - examples of machine learning models are
12:17 - linear regression logistic regression
12:19 - the support Vector machines KN viation
12:22 - algorithm K means or clust string so as
12:25 - an unsupervised learning model theb scan
12:28 - do mention few boosting algorithms such
12:30 - as GBM XG boost you can also mention
12:33 - random Forest very popular for
12:35 - regression as well as classification
12:37 - type of tasks so in this way you will
12:39 - show to your interviewer that you know
12:42 - quite quite well those machine learning
12:44 - algorithms then do mention the Deep
12:47 - learning models which fundamentally
12:49 - differ from the machine learning models
12:52 - and I mention examples such ASN RNN LS
12:56 - grus and outter encoders all base or
12:59 - neural networks do also mention CNN so
13:02 - convolutional neural networks very much
13:05 - popular in the field of computer vision
13:07 - then do point out that the first biggest
13:10 - difference is that uh traditional
13:13 - machine learning models they rely on
13:15 - manual feature extraction while deep
13:18 - learning models they do this
13:20 - automatically and they automatically
13:23 - take the input data as it is we don't
13:26 - need to perform feature selection we
13:28 - don't need to worry about that because
13:30 - our model represented by neural networks
13:32 - especially with this D players is able
13:35 - to identify this patterns in all this
13:38 - data and then extract for itself the
13:42 - most important features that describe
13:44 - our data and then use it for different
13:46 - type of tasks whether it's for
13:48 - regression for classification it
13:51 - actually performs very well so deep
13:53 - learning models perform very well on Big
13:56 - Data something that we have seen machine
13:59 - learning models suffer so ml models are
14:02 - known for becoming worse in terms of
14:05 - their performance when the size of the
14:07 - data increases when the number of
14:10 - features increases they start to
14:11 - overfeed and they start also to become
14:15 - unstable and not accurately predict the
14:18 - output when we increase the number of
14:21 - observations in our data also the
14:23 - include complex features in the data
14:25 - that's something that definitely helped
14:28 - the de learning models outperform the
14:30 - traditional machine learning models
14:31 - especially in the test such as computer
14:34 - vision or speech recognition machine
14:37 - translation and the idea of deep
14:39 - learning models you can even see why
14:42 - they are better than a machine learning
14:44 - models by even looking at the most
14:47 - recent different sorts of applications
14:49 - in the across different Tech Fields you
14:52 - can see that most of them are based on
14:54 - some sort of Naro network based
14:56 - algorithm rather than machine learning
14:58 - models and this is mainly because of
15:01 - this various abilities of deep learning
15:03 - models including ability to handle large
15:05 - data ability to handle complex features
15:08 - and being able to do feature extraction
15:11 - on itself instead of doing it outside of
15:14 - the model or um instead of suffering
15:17 - from various type of problems like the
15:19 - traditional machine learning models do
15:21 - so the next question is what is a neural
15:24 - network so here your interior wants to
15:27 - understand the relationship ship between
15:29 - the neural networks to to the human
15:32 - learning the way we learn and whether
15:34 - you know this components and how the
15:37 - components uh flow so in terms of the
15:40 - training process whether do you know
15:42 - what the weights are what the parameters
15:44 - are whether you know what this hidden
15:46 - layers are whether they they are just
15:49 - one or multiple you understand the
15:51 - concept of deep neural networks and how
15:53 - this flow of information happens and
15:57 - whether you mention all the these
15:59 - elements of the neural networks so I
16:02 - would mention as an answer to this
16:03 - question that neural networks is a
16:06 - foundational Concept in the field of
16:07 - deep learning and in the field of
16:10 - artificial intelligence just in general
16:12 - and it's a computational model that is
16:14 - inspired by biological human brains and
16:17 - it tries to replicate the way humans
16:19 - learn the way they interpret information
16:22 - they take the input data and they they
16:25 - process that information so they do
16:27 - training of the on the input data and
16:31 - they also consistently update on this
16:33 - information so they learn something new
16:35 - they add new information and then they
16:39 - update the the possible outcome and the
16:42 - end result is to minimize the amount of
16:45 - error we are making when we are learning
16:48 - and that's exactly what we are doing in
16:50 - neural networks so the core of neuron
16:53 - network is made up of this units which
16:55 - we are calling neurons like in our human
16:57 - brains and uh these neurons are together
17:01 - together forming the layers we have
17:04 - input layer which is in our which
17:06 - represent our input data then we have
17:09 - this the the weights which showcase how
17:12 - much importance we need to put on
17:14 - different input signals and this weights
17:18 - are what we are calling parameter in our
17:20 - model which we need to consistently
17:23 - update when we are performing the
17:26 - training so the learning process
17:29 - so these layers include the input layer
17:31 - one or more hidden layers which is the
17:34 - middle part of our naral Network and
17:36 - that helps to learn these different
17:38 - patterns helps to understand activate
17:41 - certain neurons and deactivate or
17:44 - activate less the other input neurons
17:47 - and then transforms this information
17:50 - after this activations are done we go
17:53 - through this hidden layer and in after
17:55 - this hidden layer then we can have
17:57 - another hidden layer which will then
18:00 - receive the information from the
18:02 - previous hidden layer and will then
18:04 - continue the learning process on it
18:06 - certain it will be able to learn and
18:09 - understand the previous hidden layers
18:11 - information and as an example of this
18:14 - process think of that in the input we
18:16 - have the different pixels of the images
18:19 - and we want to learn these images and
18:22 - for the model for the nural network to
18:25 - learn it should be able to First learn s
18:29 - some sort of objects in the input data
18:32 - so in the first hidden layer it might
18:34 - discover certain signals certain edges
18:37 - and certain information and then in the
18:39 - next hidden layer it will be able to
18:41 - identify specific objects for instance
18:45 - the the ears of a cat if we want to
18:47 - identify where the image is of a cat or
18:49 - a dog and then passes this information
18:51 - onto your next layer onto the next layer
18:53 - and then we go on to the final layer
18:55 - when we produce the output now now if
18:58 - you are familiar with machine learning
19:00 - and if you are looking at this tutorial
19:03 - I assume that you are familiar with the
19:04 - idea of classification regression on
19:06 - supervised learning supervised learning
19:09 - so the in case of the traditional neural
19:11 - networks we then produce the output and
19:14 - it can be for instance identifying a
19:17 - probability a value between zero and one
19:20 - let's say for our classification task
19:23 - and then this value should be compared
19:26 - uh to the actual class so if we have for
19:29 - instance probability and we transform it
19:31 - to the class then we can update the
19:34 - corresponding so we have the output of
19:36 - our neuron Network in each Epoch and
19:40 - then we use this information to compare
19:42 - to the true labels in our supervised
19:44 - learning process and then we compute the
19:47 - error so we are trying to understand how
19:50 - well we did our prediction and then this
19:53 - information is being used to then
19:56 - understand how moral is being how
19:58 - accurately is the model predicting the
20:01 - results and this then this is what we
20:03 - are calling loss so this loss function
20:06 - is then used in order to understand how
20:10 - much we need to change the weights so
20:13 - how much importance we put on the
20:15 - neurons how much we need to update these
20:18 - weights as well as the other parameters
20:20 - in our model in order to reduce the
20:23 - amount of error moral is making and
20:25 - that's the underlying idea of neural
20:27 - networks to take the input put it
20:29 - through those hidden layers to learn
20:31 - from this input data activate the
20:33 - neurons and have either one hidden layer
20:36 - or multiple hidden layers continuously
20:39 - activate those neurons get the output
20:42 - understand what is the loss that our
20:44 - model is making the overall cost
20:46 - function and then compute the what we
20:49 - are calling gradients and do briefly
20:51 - mention here the idea of gradients but
20:53 - do not go too much into detail of it and
20:57 - then using this grip radians we
20:59 - understand how much we need to update
21:01 - this weights in order to improve our
21:05 - prameters of our model because at the
21:07 - end of the day what we care about for
21:10 - our final result is this model that has
21:13 - the parameters the weight parameters the
21:16 - paramet the bias parameters that will
21:19 - end up generating the most accurate
21:22 - predictions for us and for that we need
21:24 - to continuously learn go through this
21:26 - Loop in order to properly learn and in
21:30 - order to produce a model that makes the
21:33 - minimum amount of error so it's able to
21:35 - properly learn from the input data the
21:38 - next question is explain the concept of
21:41 - neon in deep learning in this question
21:44 - the interview wants to hear about the
21:46 - concept of Weights the concept of bias
21:49 - factors how they are used in order to
21:52 - uptain the so-called Z scores and how
21:55 - the activations are activ ation
21:58 - functions are used to obtain activations
22:01 - and what is the point of this weights
22:04 - and Bas vectors and relationship to the
22:06 - learning process so a neuron in a deep
22:09 - learning it is referred as sometime
22:12 - sometimes referred as artificial neuron
22:15 - which mimics the function of the human
22:17 - brain's neuron but it does so in
22:20 - mathematic and simple way and the idea
22:23 - is that in a neural
22:25 - network the moral gets different
22:28 - inputs and here this input signals we
22:31 - are calling neurons it takes the input
22:35 - data which we represent and call it as
22:38 - an X and we use this x to multiply it by
22:43 - weights because this weights can then
22:46 - tell us how much importance this neuron
22:49 - should have how much attention we need
22:51 - to pay to this neuron and these weights
22:54 - are then multiplied by this input data
22:58 - which you call X those are the neurons
23:01 - and we add a bias Factor because there
23:04 - can be always a bias introduced when we
23:07 - are performing our learning process and
23:10 - the the way it's multiplied by this
23:12 - input data X we are adding then on the
23:15 - top of that the bias factors and this
23:18 - will be our Z scor but then in the next
23:21 - stage before approaching the hidden
23:24 - layer we use what we are calling
23:26 - activation functions and activation
23:29 - function such as sigmoid function
23:32 - Rectify linear unit or
23:35 - leyu all this activation functions what
23:38 - they are doing is that they are
23:40 - mathematically introducing nonlinearity
23:43 - into our neural network and these
23:46 - activation functions are applied to all
23:49 - these different Z scores which is based
23:52 - on the weights and then input data and
23:54 - bias vectors in order to understand how
23:57 - much each neuron should be activated it
24:00 - can be that we want in certain when when
24:03 - looking at the object and trying to
24:05 - understand what is the object in that
24:06 - image based on the pixels it can be that
24:09 - we need to activate certain neurons
24:11 - related to that specific area or that
24:13 - specific object and we should not
24:16 - activate the other neurons and that's
24:19 - exactly what we are doing with this
24:20 - activations of this neurons so here then
24:24 - we are using these activations to
24:26 - introduce non larity for neural network
24:29 - to learn this complex structures in the
24:32 - input data so the next question is
24:35 - explain architecture of neural networks
24:38 - in simple way given that we have already
24:41 - answered the previous questions all
24:43 - mentioning some parts of the
24:45 - architecture and the training process of
24:47 - Naro networks this one will be easy to
24:50 - answer and I will use here a simple
24:53 - visualization for simple Naro networks
24:57 - that consist of just one hidden layer to
25:00 - just summarize everything that we
25:02 - mentioned as part of previous interview
25:04 - questions so the nural network is this
25:08 - multi-layer structured model and each
25:11 - layer is transforming the input data
25:14 - step by step think of it like an
25:17 - assembly line so assembly line where
25:20 - every stage adds more complexity on the
25:24 - preview stage and the detail and the
25:28 - complexity is adding more value to the
25:31 - ability of the model to perform
25:33 - predictions so in the beginning the the
25:37 - model has this input layers as you can
25:39 - see here the input layers are
25:41 - represented by this Z1 Z2 Z Tre up until
25:45 - z n and in here you can think of this
25:49 - elements this inputs as part of input
25:51 - layer as features describing the data so
25:55 - it can be for instance that we want to
25:57 - use use neural network to uh estimate
26:00 - the price of a house we need to have
26:03 - certain features to estimate that price
26:05 - we need to learn and in order to
26:07 - understand how we can differentiate
26:09 - different houses and how these features
26:12 - influence the house price so Z1 can be
26:15 - for instance the number of bedrooms of
26:17 - this house Z2 can be the the number of
26:20 - levels of this house that tree can be
26:24 - the whether the house has a swimming
26:26 - pool or not not a can be uh whether the
26:30 - house was built so what is the year that
26:32 - the house was built what is the age of
26:34 - the house all these features they
26:36 - describe a certain element or certain uh
26:40 - characteristic of the house and this
26:42 - will help us and this will help the
26:44 - moral to learn from this feature and to
26:47 - understand what will be possible
26:50 - accurate price price for this house and
26:53 - then we have these different weights as
26:55 - you can see uh we have weight one one
26:58 - weight one two weight 1 three and then
27:01 - we have the corresponding weights for
27:02 - the second input the corresponding
27:04 - weights for the for our third feature so
27:07 - corresponding to Z3 such as w31 w32 and
27:12 - w33 Etc as you can see this weights they
27:17 - show a pile that goes an arrow that goes
27:20 - onto the next layer which is our hidden
27:23 - layer so in this simple architecture we
27:26 - have just we got just one hidden layer
27:29 - but it can also be that you have more
27:31 - hidden layers which is usually CA the
27:33 - case with the traditional deep neural
27:36 - networks hence also the name deep so
27:39 - what this weights do is that they help
27:41 - us to understand how much this input
27:46 - feature should contribute to First
27:49 - hidden unit second hidden unit and third
27:52 - hiding unit so as you can see in our
27:54 - hidden layer we have the three circles
27:57 - these three circles describe the hidden
28:00 - units in our hidden layer and we have
28:03 - the simple structure so we got just
28:05 - three hidden units but this is something
28:07 - that you can decide for yourself when
28:09 - training your model it's also what we
28:11 - are calling hyper parameter that you can
28:13 - tune so in here that you can see that
28:16 - the w11 goes from Z1 to H1 so this is
28:20 - the importantance weights that tell us
28:24 - and tell to the narrow Network how much
28:26 - the input layer should get attention
28:29 - when should get importance when it when
28:32 - it helps you learn the first heing unit
28:35 - so for H1 what is the weight that we
28:39 - need to put on Z one so our first
28:41 - feature then we have the W12 which
28:44 - describes the amount of weight that we
28:47 - need to put on the first input feature
28:50 - when identifying this when learning and
28:53 - putting the information into our second
28:55 - hidden unit H2 so you get the idea so
29:00 - then this this weights and the
29:03 - corresponding bias factors and the input
29:06 - data helps us to compute what we are
29:10 - calling Z scores and Z score so Z can be
29:13 - represented as the W so the weight
29:16 - multipli by X which is the previous in
29:19 - this case the input data plus bias
29:22 - Vector which we sometime refer as B then
29:25 - once we have the Z scores we are then
29:27 - ready to add an nonlinearity so the
29:30 - activations which will then help us to
29:33 - understand how we need to perform
29:35 - aggregation so you can see that when it
29:37 - comes to the first hitting unit H1 there
29:40 - are four different piles so arrows that
29:43 - comes to H1 it is the w11 W uh 21 w31
29:51 - and
29:52 - wn1 what this basically means is that we
29:56 - have this different input features and
29:59 - per input feature we have the
30:01 - corresponding weight that helps us to
30:03 - understand how much attention we need to
30:06 - pay on these different input features
30:08 - when we are obtaining the H1 so our
30:11 - hidden layers first hidden unit and we
30:15 - can aggregate these different Z scores
30:19 - that we compute based on different
30:20 - weights different input signals and this
30:23 - activation function can significantly
30:25 - change on the way we calculate our
30:28 - activations in the H1 and that's exactly
30:32 - what the activation does so the
30:33 - activation functions then once we have
30:36 - this information we have computed our
30:38 - activation we are ready to compute our
30:41 - output and the output is done after we
30:44 - have computed our H1 H2 and H3 those
30:47 - hidden units have learned based on the
30:50 - weights and bi factors and activation
30:52 - function the information in our input
30:55 - data in the previous layer then we use
30:57 - the final so the final weights so uh wh1
31:03 - wh2 wh3 in order to understand how much
31:07 - each hidden unit need to contribute to
31:11 - the output layer and in this output
31:13 - layer what we are doing is that we are
31:15 - getting the output in the format that is
31:18 - desired so it can be that we are
31:20 - performing classification and we want to
31:22 - get a
31:23 - probability uh therefore we need to
31:26 - obtain a value between 0 and one then in
31:29 - this output layer we will then have this
31:31 - volue we will transform it to a class
31:33 - and we will compare it to the true label
31:36 - such as zeros and ones you understand
31:38 - how well our simple neural network is
31:40 - performing so the next question is what
31:42 - is an activation function in neural
31:45 - networks so activation function in
31:48 - neural network plays a crucial role and
31:51 - the choice of activation function will
31:53 - uh Define the performance of your neural
31:56 - network we by now have already spoken a
31:59 - lot about the weights and how we compute
32:02 - the Z scores using the weights the input
32:05 - data or the previous layers input a
32:07 - previous layers output we add the bias
32:09 - vectors to obtain the Z scores and then
32:13 - we need to apply activation function to
32:16 - obtain the activation so how much each
32:19 - neuron should be activated when
32:22 - Computing the next layer so that is
32:26 - exactly what activations do so
32:28 - activation functions they uh in neural
32:31 - network they introduce the idea of
32:34 - nonlinearity if we were to not use
32:37 - specific type of activation functions
32:39 - they will our model will be similar to
32:42 - the uh linear regression model we will
32:44 - have a plain linear type of model that
32:48 - will be able to uncover the linear
32:50 - patterns and will not be able to uh
32:52 - discover these complex hidden patterns
32:54 - in the data and we have seen that in the
32:57 - true world the data most of the time
33:00 - contains nonlinear relationships
33:02 - therefore we introduce the activation
33:05 - functions which help us to introduce
33:07 - nonlinearity into our neural network and
33:10 - they are like aggregation functions that
33:13 - help us to understand how we need to
33:15 - combine this different input this this
33:17 - different neurons corresponding Z scores
33:20 - in order to obtain final so single
33:22 - activation volume so in here if you look
33:26 - at this figure here you can see that we
33:28 - are using the X1 X2 up until xn which
33:31 - are our input points and then we have
33:34 - the corresponding weights which tell us
33:35 - how much each of these inputs need to be
33:38 - activated and then we need to aggregate
33:40 - these values we need to obtain one
33:42 - single activation which will be then
33:45 - based on the activation function it will
33:48 - it basically defines how much we need to
33:52 - add a value that corresponds to that
33:55 - specific input when Computing the hidden
33:59 - units volume that we saw before and
34:01 - there are different sorts of activation
34:03 - functions do mention briefly that the
34:06 - four popular activation functions are
34:08 - sigmoid activation function the hyperb
34:11 - hyperbolic tank function uh which
34:13 - shortly is referred as Tang function
34:16 - also definitely mentioned about
34:18 - rectifier linear unit activation
34:20 - function so the REO and also mention the
34:24 - Leaky relo so the Leaky of the rectifier
34:27 - linear unit activation function you can
34:30 - also if you have time you can also
34:32 - specify that activation functions they
34:35 - act like a Gatekeepers so they decide
34:38 - how much information needs to be used
34:40 - from this from this state to be passed
34:43 - upon to onto the next state so that's
34:46 - basically the idea behind activation
34:48 - functions which enable the neural
34:50 - networks to properly learn nonlinear
34:53 - relationships in the data the next
34:55 - question is name few popular activation
34:58 - functions and describe them so we
35:01 - briefly spoke about activation functions
35:03 - in the question number six so here I
35:06 - will go into much more detail about
35:08 - these different activation functions and
35:11 - we'll also make a categorization between
35:13 - them when you are getting asked this
35:15 - question you don't need to give all
35:17 - these different sorts of examples just
35:20 - one example of out of this four and then
35:23 - provide this detailed explanation of
35:26 - this specific activ ation function but
35:28 - in this case what I would do is that I
35:30 - will provide you all four four popular
35:33 - activation functions largely used in the
35:35 - industry and then it will be up to you
35:37 - to decide which one you will give as an
35:40 - example so I tend to make this
35:43 - categorization behind this four
35:45 - activation function so sigmoid
35:47 - activation function Tang activation
35:49 - function rectifier linear unit or short
35:52 - relu activation function and the leak
35:54 - key rectifier linear unit Rel the
35:57 - activation function so uh I tend to say
36:01 - that sigmoid function and Tang function
36:04 - they are they can form a single category
36:07 - and then the rectifier linear unit and
36:10 - its adjusted version Le youru they can
36:13 - form the second category of activation
36:16 - functions and we will soon see why so
36:19 - let's now start with the sigmoid
36:21 - activation function so if you are
36:23 - familiar with the logistic regression
36:25 - then you most likely will recognize this
36:27 - activation function because sigo
36:29 - function is also used in logistic
36:31 - regression when we are trying to produce
36:35 - values that are like probabilities so
36:38 - values between zero and one so as you
36:41 - can see in case of sigmoid activation
36:43 - function we are getting the this s
36:46 - shaped curve when it comes to the
36:48 - diagram that describes the activation
36:50 - functions and so what goes into
36:52 - activation function we are Computing the
36:55 - Z scores which are them based on the uh
36:58 - on the previous layers data so output it
37:01 - can be for instance described by X and
37:04 - then we are multiplying this x by the
37:06 - weights let's say w and then we are
37:08 - adding bias Factor let's say B and then
37:11 - we this helps us to get the Z scores
37:13 - then the Z scores needs to be used and
37:15 - put given us an input into the
37:18 - activation function and this is exactly
37:21 - then what transforms the Z scores into
37:25 - activation function and here it is the
37:27 - sigmoid activation function so sigmoid
37:30 - activation function then takes the input
37:33 - let's say Z and then what it does is
37:36 - that it transforms into this value where
37:38 - the value is 1 / to 1 + e to the power
37:42 - minus Z so similar to what we do in
37:44 - logistic regression and as you can see
37:48 - what this graph represents which is a
37:50 - sigo function in the x axis we have all
37:53 - the Z scores basically and in the y AIS
37:57 - we have the corresponding activations
37:59 - and you can see that for all the values
38:01 - from minus infinity till plus infinity
38:04 - for the exis so for the Z scores the
38:07 - corresponding Activation so on the Y AIS
38:09 - we have values between zero and one so
38:12 - the minimum is zero and the maximum is
38:14 - one so you can see that the inflation
38:17 - point so the point where we go when we
38:20 - flip the uh the curve is at
38:23 - 0.5 and this activation function uh you
38:27 - can see that whenever we have a negative
38:29 - values then the corresponding
38:31 - activations will be between zero and 0.5
38:35 - it is all these areas and whenever we
38:38 - have the Z scores which are positive
38:42 - scores the corresponding so if if we
38:45 - have positive scores for our Z scores
38:46 - then the corresponding activations will
38:48 - be between 0.5 and then one what is
38:51 - really important to remember and
38:53 - mentioned during your answer as part of
38:55 - the interview question is that sigmoid
38:58 - function is suffering from what we call
39:01 - saturation so what is saturation so
39:04 - saturation happens when when we take a
39:07 - very large negative values or very large
39:09 - positive values um then the activation
39:13 - or the volue in this case of a function
39:16 - converges to the extremes of the
39:18 - corresponding function and this is
39:21 - exactly what happens here if we look
39:23 - here if we take let's say the x equals
39:26 - to minus 1,000 then the corresponding y
39:30 - will be most likely very close to zero
39:33 - and which is the minimum so it's the
39:35 - extreme of the of this activation
39:37 - function and if we take the X as a very
39:41 - large positive number like plus 1,000
39:44 - then the corresponding activation value
39:47 - will be very close to one which is then
39:49 - the global maximum of this function so
39:51 - this is exactly what we refer as
39:53 - saturation so it means that if we take
39:56 - very large negative or very large
39:58 - positive number for our Z scores this
40:01 - activation function will then provide us
40:04 - an output a value that is very close to
40:07 - the extreme so two extremes the uh zero
40:10 - and the one of the corresponding
40:12 - activation function and later on we will
40:14 - see why this can be problematic if we
40:16 - use this function as part of our
40:19 - activations for the hidden layers but of
40:22 - course when it comes to getting a value
40:24 - an output between 0 and one
40:26 - which is exactly what the sigmoid
40:28 - function is doing then the sigmoid
40:30 - function would be the goto activation
40:33 - function so the next activation function
40:36 - that we will talk about is the
40:37 - hyperbolic tank activation function ins
40:40 - shorted tank function so as you can see
40:44 - the hyperbolic tank function is quite
40:46 - similar in its shape to the sigmoid
40:49 - activation function only what it does
40:51 - different is that it transforms all the
40:54 - values to value between minus one and 1
40:58 - instead of 0 and one like the sigmoid
41:01 - function was doing so the tank function
41:04 - as the name suggests it's based on the
41:07 - idea that comes from uh geometric topics
41:10 - if you are familiar with the idea of
41:11 - sinus cosine is T tangent and cotangent
41:15 - then this will be familiar to you
41:17 - because this function does exactly that
41:19 - so it's uses this tank function to
41:22 - transform all the Z scores to volue that
41:26 - is between minus1 and one so the F set
41:30 - which represents the dis activation
41:32 - function is a function that is equal to
41:35 - uh in denominator we have e^ Z minus e^
41:39 - minus Z / to e to^ z + h^ minus Z so
41:45 - what it basically does is that it
41:47 - transforms it takes the Z sores and if
41:50 - the Z scores are negative it transforms
41:53 - to the values between minus one and zero
41:56 - and then if the Z SES are positive
41:59 - numbers from zero to plus infinity then
42:02 - the corresponding activations will be
42:04 - between zero and one and as in case of
42:07 - sigmoid function tank function also is
42:10 - s-shaped uh only the inflation point so
42:13 - the point where it changes its pattern
42:16 - the way the graph behaves is not at the
42:19 - 0.5 but it's at a point of zero like
42:22 - sigmoid function tank function also
42:24 - suffers from saturation so for large
42:27 - negative and large positive numbers the
42:29 - corresponding activations will be very
42:31 - close to the extreme point so the minus
42:34 - one and + one uh for this function which
42:38 - can be problematic like in case of
42:40 - sigmoid function so don't use these two
42:43 - functions for your hidden layers but use
42:45 - them for the output layer and we use
42:48 - them for the output layer simply because
42:51 - the the value that we are getting from
42:53 - these two functions is very convenient
42:55 - to to transform it to the corresponding
42:58 - value whether it's for instance
42:59 - classification case because if we get a
43:01 - value between 0o and one for instance as
43:04 - in case of sigo activation function then
43:07 - we can interpret this as probability and
43:09 - this probability can be then used for
43:12 - getting a class so class zero and one
43:14 - for for instance and this then can be
43:17 - used for evaluating our model by
43:20 - comparing it to the actual classes and
43:22 - compute the cross entropy or any other
43:25 - uh valuation metrics can be computed
43:28 - based on this and later on we will see
43:30 - why this two activation functions should
43:33 - not be actually used in the hidden
43:35 - layers especially for deep neural
43:37 - networks that have many hidden layers
43:40 - but for now as this is out of the scope
43:42 - of this question I don't want to go into
43:44 - details as we have already covered quite
43:46 - a lot so the next two activation
43:50 - functions that you can mention when
43:52 - answering this question uh can be the
43:54 - rectifier linear unit and the Ley
43:57 - rectifier linear unit activation
43:59 - functions the two are very similar
44:01 - especially when it comes to the positive
44:04 - Z scores but they are bit different when
44:07 - it comes to the negative Z scores so
44:09 - what do I mean by this when we look at
44:11 - the rectifier linear unit so re
44:14 - activation function you can see that uh
44:17 - it looks like this so it takes for all
44:19 - the cases when Z so the
44:22 - activation if the Z score is small than
44:26 - zero so it's a negative number then the
44:28 - corresponding activation will be zero so
44:31 - basically uh this activation function
44:34 - will not be activating any of the
44:36 - negative
44:37 - neurons whereas the for for the cases
44:41 - when we have the Z scores positive then
44:44 - the corresponding activations will be
44:48 - exactly equal to the Z scores so hence
44:51 - the name linear so if you look at the Y
44:54 - is equal to X line and you visualize
44:57 - that line you can see that the positive
45:01 - part so for the part where the x is is
45:03 - positive from zero to plus infinity you
45:06 - can see that that Y is equal to X line
45:08 - is exactly like that exactly what we
45:10 - have here and that's where the the name
45:12 - linear comes from so it's like the
45:15 - linear function representation but this
45:17 - only holds for the positive numbers so
45:21 - for positive numbers we will get the the
45:23 - Z scores and we will activate them by
45:26 - their exact amount whereas in case of
45:29 - negative Z scores or negative neural
45:31 - then the corresponding activation will
45:34 - not happen and we will not be activating
45:36 - these negative values and for all these
45:39 - cases the activation function will set
45:41 - the activation equal to zero so this can
45:45 - be uh problematic for the cases when we
45:48 - do want to have the output we do want to
45:50 - take into account these negative values
45:52 - and we want to uh consider these
45:54 - negative values and perform the
45:56 - predictions based on them too in those
45:58 - cases we need to adjust this re so we
46:01 - can then better use what we are calling
46:03 - leaky relo and what this leaky relu
46:06 - activation function does is that it not
46:08 - only activates the positive sores but it
46:11 - also activates the negative ones it does
46:15 - that at a lesser extreme than the for
46:18 - the cases when d z is positive of course
46:20 - as you can see from this from this
46:22 - negative side so for from this third
46:25 - quarter of our graph but it still does
46:28 - it and you can see it also from the
46:30 - function itself because the activation
46:31 - function corresponding to Liu can be
46:34 - represented by this F set which is equal
46:37 - to
46:37 - 0.01 if Z is smaller than zero and is
46:40 - equal to Z if Z is larger than equal
46:43 - zero so for all the positive numbers the
46:45 - liid ru acts exactly the same as for Ru
46:49 - as you can see here but for the negative
46:52 - values the corresponding activation is
46:55 - simply equal to
46:56 - 0.01 so you can see that it is this part
46:59 - of the figure so both of this activation
47:03 - functions do not suffer from saturation
47:06 - something that is definitely different
47:08 - compared to the sigmoid and tank
47:09 - function and that's why it is
47:12 - recommended and it has been proven that
47:15 - this two activation functions perform
47:17 - much better when we use them as part of
47:19 - our uh hidden layers activations and
47:22 - they are not so much used when it comes
47:25 - to our output layer so think of like
47:27 - using the Leaky Rel and re for your
47:30 - hidden layers but not to use them for
47:33 - your output layer and the other way
47:35 - around for the sigo function tank
47:37 - function use them for for your output
47:39 - layer but not for your hidden layers so
47:42 - the next question is what happens if you
47:44 - do not use any activation functions in a
47:47 - neural network so the answer for this
47:50 - question can be very short because it is
47:53 - quite obvious so the AB of activation
47:56 - functions will reduce the narrow Network
47:59 - to a common machine learning model like
48:01 - linear regression something that removes
48:05 - the entire idea of using neural networks
48:07 - in the first place therefore in order to
48:10 - be able to utilize the architecture of
48:14 - neural networks and to be able to
48:16 - discover hidden patterns in our data
48:19 - nonlinear patterns we can use the
48:22 - activation functions and nonlinear
48:25 - activation
48:26 - function the next question is describe
48:29 - how training of basic neural networks
48:31 - work so in this case your interviewer
48:34 - wants to know whether you know the idea
48:36 - of a forward pass backward pass what is
48:39 - back propagation how this processes are
48:42 - connected to each other and how neural
48:45 - networks utilize these layers in order
48:47 - to learn these complex patterns in your
48:50 - data so you can start by uh describing
48:53 - the training process of naral network a
48:55 - very basic one uh by the process of what
48:59 - we are calling forward path forward path
49:02 - takes the input data and processes
49:05 - through neurons which we saw before and
49:08 - uses this weighted sum and activation
49:11 - functions to produce an output so we
49:15 - take the input data we multiply it by
49:18 - the corresponding weights and then we
49:21 - add a bias Factor then those are our Z
49:24 - scores then we apply an activation
49:27 - function to introduce
49:29 - nonlinearity and this activated values
49:32 - the activation scores then are used when
49:35 - we are passing information onto the next
49:37 - layer because then we are using this as
49:40 - our input we are multiplying it by the
49:42 - corresponding weights we are adding the
49:44 - bias factors and then uh we are using
49:47 - activation function for that hidden
49:49 - layer in order to learn from this
49:53 - complexities this process continues on
49:55 - until we reach the output layer where we
49:58 - compute our output so this output is
50:01 - then used to compare it to the True
50:04 - Values that we have for our data so it
50:06 - can be for instance the true labels of
50:09 - our observations in a classification
50:11 - problem then we compute what we are
50:14 - calling a loss function or cost function
50:16 - it can be for instance the rmsc or MSC
50:20 - for regression type of problem it can be
50:23 - for instance cross entropy for class
50:25 - ification type of problem and then using
50:27 - this loss function we are then doing
50:30 - what we are calling back propagation
50:32 - back propagation is then process of
50:34 - computing the gradients so the first
50:36 - order derivative of this loss function
50:39 - with respect to the weights and the bias
50:41 - Vector so with respect to the parameters
50:43 - of our model that we want to improve and
50:46 - once we have computed this gradient
50:48 - which tell us how much our L function
50:51 - will change if we make a certain change
50:54 - to our our weights and to our parameters
50:57 - so the bias vectors then we can use this
51:00 - gradients in order to update our
51:04 - parameter in our model and the way we
51:06 - are doing that is by the process of
51:08 - backward pass so this is the opposite of
51:12 - what we are doing in the forward pass in
51:15 - the forward pass we go from left to the
51:17 - right till the end of the model to get
51:19 - the output in the backward pass we do
51:21 - the opposite so we basically we're going
51:23 - backwards we have identified the
51:26 - gradients by doing back propagation then
51:30 - we are using these gradients we are
51:32 - giving it as an input tune optimization
51:35 - technique it can be SGD GD SGD with
51:38 - momentum RMS probe Adam you name it and
51:41 - then using that approach in order to
51:44 - update our weights and our bias factors
51:48 - uh layer by layer so then what we are
51:51 - basically doing is that uh we are taking
51:54 - the gradients we are taking the
51:57 - optimization algorithm that we will use
51:59 - let's say Adam and then we are updating
52:03 - the weights corresponding to the last
52:05 - layer we have computed that then we go
52:07 - on to the next layer that comes before
52:11 - so in this way we go from right from
52:13 - very deep layers to the left one so from
52:17 - using the multiplications between the
52:19 - weights and in this way we are then
52:22 - using the gradients and we are updating
52:25 - the weights and the bias factors in such
52:27 - way that we can adjust the weights and
52:30 - then reduce the loss function so this is
52:34 - very important to mention and to show
52:36 - the interviewer that you understand why
52:38 - we are updating the weight and why we
52:40 - are updating the bias vectors because
52:42 - mathematically the gradients show how
52:45 - much change will be in the loss function
52:47 - when we change a certain weight or
52:49 - certain bias Factor by certain amount it
52:52 - can be small amount and that is
52:54 - something that we are using the
52:56 - gradients uh in order to update the
52:59 - previous weight parameters and bias
53:02 - parameters such that we can reduce the
53:05 - loss that our motor is making because
53:07 - this will mean that we are one step
53:09 - closer to having more accurate model the
53:13 - smaller amount of error we are making
53:15 - the smaller is our loss the better is
53:17 - our neural network performing so that's
53:20 - the idea behind training of the neural
53:23 - networks and this Pro continues on and
53:26 - on we have forward pass back propagation
53:29 - backward pass and on and on until we
53:31 - have reached some sort of uh end
53:34 - criteria so stopping criteria let's say
53:36 - the number of iterations number of epo
53:39 - or some other stopping criteria the next
53:42 - question is what is gradient descent so
53:44 - gradient descent is an optimization
53:46 - algorithm that we are using in both
53:48 - machine learning and in deep learning in
53:51 - order to minimize the loss function of
53:53 - our model which which means that we are
53:56 - iteratively improving the model
53:58 - parameters in order to minimize the cost
54:01 - function and to end up with a set of
54:03 - model parameters that will that will
54:06 - optimize our model and the model will be
54:09 - producing highly accurate predictions so
54:12 - in order to understand the gradient
54:14 - descent we need to understand what is
54:16 - the loss function what is the cost
54:18 - function which is another way of
54:19 - referring to the loss function uh we
54:22 - need to also understand the the flow of
54:25 - nural network how the training process
54:27 - works uh which we have seen as part of
54:29 - the previous questions and then we need
54:31 - to understand the idea of iteratively
54:34 - improving the model and why we are doing
54:37 - that so let's start from the very
54:40 - beginning so we have just learned that
54:42 - during the training process of naral
54:44 - network we first do the forward pass
54:46 - which means that we are iteratively
54:49 - Computing our activations so we are
54:52 - taking our input data we then are
54:54 - passing it with the corresponding weight
54:57 - parameters and the bias vectors through
54:59 - the hidden layers and we are activating
55:02 - those neurons by using activation
55:04 - functions and then we are going through
55:06 - those multiple hidden layers up until we
55:08 - end up with uh Computing the output for
55:12 - that specific forward path so the uh
55:15 - predictions why hat so once we perform
55:19 - this in the for our very initial
55:22 - iteration of training the neural network
55:24 - we need to have a set of model
55:26 - parameters that we can start uh training
55:29 - process in the first place so we
55:31 - therefore need to initialize those
55:33 - parameters in our model and we have
55:35 - specifically two type of model
55:38 - parameters in the naral network we have
55:40 - the weights and we have the bias factors
55:42 - as we have seen in the previous
55:43 - questions so then the question is well
55:47 - how much error are we making if we are
55:49 - using this specific set of ways and bias
55:52 - vectors cuz those are the parameters
55:54 - that we we can change in order to
55:57 - improve the accuracy of our model so
56:00 - then the question is well if we use this
56:03 - very initial version of the model
56:05 - parameters so the weights and the B
56:07 - vectors and we compute the output so the
56:10 - Y hat uh then we need to understand how
56:13 - much error is the model making based on
56:15 - the set of model parameters that's the
56:17 - loss function so loss function or the
56:20 - cost function which means the average
56:22 - error that we are making when we are
56:24 - using this weight and the bias factors
56:25 - in order to perform the
56:27 - predictions and uh as you know already
56:31 - from the machine learning we have
56:32 - regression type of tasks and
56:34 - classification type of tasks based on
56:36 - the problem that you are solving you can
56:38 - also decide what kind of loss functions
56:40 - you will be using in order to measure
56:42 - how well your model is doing and the
56:45 - idea behind narrow Network training
56:47 - process is that you want to iteratively
56:50 - improve this moral parameters so the
56:52 - weights and bias factors such that uh
56:55 - you will end up with the set of best and
56:57 - most optimal waste and the bias factors
57:01 - that will result in the smallest amount
57:03 - of error that the model is making which
57:05 - means that you came up with an algorithm
57:08 - and with neural network that is
57:10 - producing highly accurate predictions
57:12 - which is our goal our entire goal by
57:15 - using neural networks so loss functions
57:18 - if you are dealing with classification
57:19 - type of problems can be the cross
57:22 - entropy which is usually the go to
57:24 - Choice when it comes to the
57:26 - classification type of tasks but you can
57:28 - also use the F1 score so F beta score
57:32 - you can use the Precision Recall now
57:34 - beside this in case you have a
57:36 - regression type of task you can also use
57:38 - the mean Square the MSC you can use the
57:41 - rmsc you can use the MAA and those are
57:44 - all the ways that you can measure the
57:47 - performance of your model every time
57:49 - when you are changing your model
57:51 - parameters so we have also seen as part
57:54 - of the training of of neural network
57:55 - that there is one fundamental algorithm
57:57 - that we need to use which we called and
57:59 - referred as a back propagation that we
58:02 - use in order to understand how much is
58:05 - there a change in our loss function when
58:07 - we apply a small change in our
58:09 - parameters so this is what we were
58:11 - referring as gradients and this came
58:13 - from mathematics and as part of the back
58:16 - prop what we were doing is that we were
58:19 - Computing the first order partial
58:21 - derivative of the loss function with
58:23 - respect to each of our model parameters
58:26 - in order to understand how much we can
58:28 - change each of those parameters in order
58:30 - to decrease our loss function so then
58:34 - the question is how exactly gradient
58:35 - descent is performing the optimization
58:38 - so the gradient descent is using the
58:41 - entire training data when going through
58:44 - one pass and one iteration as part of
58:46 - the training process so for each update
58:50 - of the parameters so every time it wants
58:52 - to update the weight factors and bias
58:55 - vectors it is using the entire training
58:57 - data which means that in one go in one
59:00 - forward path we are using all the
59:03 - training observations in order to
59:05 - compute our predictions and then compute
59:08 - our loss function and then perform back
59:10 - propagation compute our first order
59:13 - derivative of the loss function with
59:15 - respect to each of those model
59:16 - parameters and then use that in order to
59:19 - update those parameters so the way that
59:22 - the GD is performing the optimization in
59:25 - updating the model parameters is taking
59:28 - the output of the back prop which is the
59:31 - first order partial derivative of the
59:33 - loss function with respect to the moral
59:36 - parameters and then multiplying it by
59:38 - the Learning rate or the step size and
59:41 - then subtracting this amount from the
59:43 - original and current model parameters in
59:46 - order to get the updated version of the
59:49 - model parameters so as you can see here
59:52 - this comes from the previously showcased
59:55 - simple example from neural network and
59:58 - here when we compute the predictions we
60:00 - take the gradients from the back propop
60:04 - and then we are using this DV which is
60:06 - the first order gradient of the loss
60:09 - function with respect to the weight
60:10 - parameter and then multiply this with
60:13 - the step size the EA and then we are
60:16 - subtracting this from V which is the
60:19 - current weight parameter in order to get
60:22 - the new updated weight parameter
60:25 - and the same we also do for our second
60:28 - parameter which is the bias Factor so
60:31 - one thing you can see here is that we
60:33 - are using this step size the learning
60:35 - rate which can be also considered a
60:38 - separate topic we can go into details
60:40 - behind this but for now uh think of the
60:43 - learning rate as a step size which
60:46 - decides how much of this size of the
60:50 - step should be when we are performing
60:52 - the updates because we know know exactly
60:55 - how much the change there will be in the
60:57 - L function when we make a certain change
60:59 - in our parameters so we know the
61:01 - gradient size and then it's up to us to
61:05 - understand how much of this entire
61:07 - change we need to apply so do we want to
61:09 - make a big jump or we want to make a
61:12 - smaller jumps when it comes to
61:14 - iteratively improving the model
61:17 - parameters if we take this learning rate
61:20 - very large it means that we will apply a
61:23 - bigger change which means the algorithm
61:26 - will make a bigger step when it comes to
61:29 - moving towards the global Optimum and
61:32 - later on we will also see that it might
61:34 - become problematic when we are making
61:36 - too big of a jumps especially if those
61:39 - are not accurate uh jumps so we need to
61:42 - therefore ensure that we optimize this
61:45 - learning parameter which is a hyper
61:47 - parameter and we can tune this in order
61:49 - to find the best learning rate that will
61:52 - be minimizing the loss function and will
61:55 - be optimizing our neural network and
61:58 - when it comes to the gradient descent
62:00 - the quality of this algorithm is very
62:02 - high it is known as a good Optimizer
62:05 - because it's using the entire training
62:07 - data when performing the gradients so
62:10 - performing the back propop and then
62:11 - taking this in order to update the model
62:13 - parameters and the gradients that we got
62:16 - based on the entire training data is the
62:20 - represents the true gradients so we are
62:22 - not estimating it we are not making an
62:24 - error but uh instead we are using the
62:27 - entire training data when calculating
62:29 - those gradients which means that we have
62:32 - a good Optimizer that will be able to
62:35 - make accurate steps towards finding the
62:39 - global Optimum therefore GD is also
62:41 - known as a good Optimizer and it's able
62:44 - to find with higher likelihood the
62:47 - global Optimum of the loss function so
62:50 - the problem of the gradient descent is
62:52 - that when it is using the entire
62:54 - training data for every time updating
62:57 - the model parameters it is just
62:59 - sometimes computationally not visible or
63:02 - super expensive because training a lot
63:05 - of observations taking the entire
63:07 - training data to perform Just One update
63:11 - in your model parameters and every time
63:13 - stor storing that large data into the
63:16 - memory performing those iterations on
63:19 - this large data it means that when you
63:22 - have a very large data you using this
63:24 - algorithm might take hours to optimize
63:28 - in some cases even days or years when it
63:31 - comes to using very large data or using
63:34 - very complex data therefore GD is known
63:37 - to be a good Optimizer but in some cases
63:40 - it just not feasible to use it because
63:42 - it's just not
63:48 - efficient the next question is what is
63:50 - the role of Optimizer in deep learning
63:53 - so when it comes to to training a narrow
63:55 - network uh we want to find what is the
63:58 - set of hyperparameters and what is the
64:00 - set of parameters that we can use as
64:03 - part of our model that will end up
64:05 - making the least amount of error because
64:08 - when our algorithm is making the
64:09 - smallest amount of error it means that
64:11 - our predictions are most likely accurate
64:15 - and when using it on a new unseen data
64:18 - it will also perform in a much better
64:20 - way so it means that we can rely on this
64:23 - algor and we can apply it across
64:26 - different applications without worrying
64:28 - that it's making too much of an error so
64:31 - in order to perform this optimization we
64:33 - are making use of different optimization
64:36 - algorithms creating in descent that we
64:38 - just discussed is one of such algorithms
64:40 - that is used in order to update the
64:43 - model parameters in order to minimize
64:45 - the amount of error that the model is
64:47 - making in this case the amount of cost
64:50 - or the loss that a Mot is making by
64:52 - minimizing the loss function
64:55 - so that is basically the primary goal of
64:57 - the optimizers in not only deep learning
65:00 - but also in general in machine learning
65:02 - and in deep learning models so the idea
65:05 - is that we will iteratively adjust the
65:08 - model parameters in this case the weight
65:10 - parameters and the bias parameters in
65:13 - our model in order to end up with the
65:16 - scenario and with the model where the uh
65:19 - where the algorithm is making the
65:21 - minimum amount of error or in some case
65:24 - say it can be the objective function is
65:26 - of the nature that should be maximized
65:28 - it means that we will need to come up
65:30 - with a set of parameters of the model
65:32 - that will maximize that objective
65:34 - function so depending on the type of
65:37 - objective objective that you have as
65:39 - part of your algorithm you will need to
65:42 - then decide how your optimization
65:44 - algorithm should be applied you will
65:45 - need to minimize or you will need to
65:48 - maximize your objective function so when
65:51 - it comes to the optimization algorithms
65:53 - we have the very original optimization
65:56 - algorithms but there are also many other
65:58 - variants of GD that have been developed
66:01 - over the years in order to combat some
66:03 - of the disadvantages that the GD has but
66:06 - at the same time also to try to
66:08 - replicate the benefits uh of the
66:11 - gradient descent so the SGD is one
66:14 - example of optimization algorithm beside
66:18 - of GD uh there is this minib badge GD we
66:22 - also have S GD with momentum when the
66:24 - momentum was introduced to improve the
66:26 - optimization algorithm such as HGD uh we
66:29 - also have adaptive learning based type
66:32 - of optimization techniques such as RMS
66:35 - prop uh Adam and adamw also adagrad and
66:40 - those are all different sorts of
66:41 - optimization algorithms that are used in
66:43 - deep learning in order to optimize the
66:46 - algorithm so another aspect that we need
66:49 - to keep in mind is that the goal of the
66:52 - optimization algorithm is to iteratively
66:55 - improve the model parameters such that
66:58 - at the end we will end up finding the
67:00 - global opum so during various type of
67:03 - explanations when explaining the
67:06 - variance of gradient descend and auto
67:08 - optimizers I will be continuously making
67:10 - use of this term Global Optimum so what
67:13 - Global Optimum means is that it is the
67:16 - actual minimum or actual maximum of the
67:20 - objective function that we are using to
67:22 - measure the error that the motor is
67:24 - making because we can have a local
67:28 - minimum or local maximum versus global
67:31 - minimum or Global maximum what this
67:33 - means is that when we have this area
67:36 - consisting of many values that the error
67:40 - can take in some cases when the
67:43 - optimization is making those movements
67:46 - in order to reach that minimum it might
67:49 - confuse and end up discovering the local
67:52 - minimum or local maximum Maxum instead
67:54 - of finding the global minimum and Global
67:56 - maximum what this means is that for some
68:00 - area when the optimization algorithm is
68:02 - moving in order to understand how it
68:05 - should improve its direction to identify
68:07 - that minimum in some cases it might
68:10 - discover that well this is the minimum
68:12 - that we are looking so the algorithm
68:14 - will converge and it will decide that
68:16 - this is the set of hyperparameters and
68:18 - parameters that we need to use in order
68:20 - to optimize our model but in the reality
68:23 - it has confused the global minimum with
68:26 - with the local minimum and it has
68:28 - falsely declared that the algorithm has
68:30 - converged and it has found the actual
68:32 - minimum but it is actually the local
68:35 - minimum not the global one and Global
68:37 - one a global minimum it means that it is
68:41 - actually the minimum cuz the local
68:43 - minimum can seem as of it is the minimum
68:46 - of the loss function but actually it is
68:48 - just a volue that it is much smaller
68:51 - than the surrounding values but it's
68:53 - actually not the smallest one in the
68:55 - entire set of values so ideally we want
68:59 - to end up with a convergence and with a
69:01 - state of the model where our all the
69:04 - weights and the bias factors we found
69:06 - they result in the actual minimum of the
69:09 - loss function and not just the value
69:12 - that for a certain set of Valu seems the
69:14 - minimum therefore the goal is to
69:17 - identify the global minimum and not the
69:20 - local one next question is what is back
69:23 - propagation and why it is important in
69:26 - deep learning so back propagation is a
69:30 - fundamental and essential part of
69:32 - training of a neural network it helps
69:35 - you learn from the errors that the model
69:37 - is making in each iteration in order to
69:40 - understand how the change in the certain
69:44 - parameters of the model such as weight
69:46 - factors and bias factors uh will result
69:50 - in a change in the loss function so the
69:54 - back propagation algorithm is the middle
69:57 - part of the entire training process and
70:00 - what I mean here is that we first
70:02 - perform our forward path we go through
70:05 - all this transformation when fetting the
70:09 - uh input data into our Network and then
70:12 - based on this Transformations we then
70:14 - end up with the final output layer and
70:17 - we then can perform our prediction so we
70:20 - compute the Y hat once the Y hat is
70:23 - computed we can then compute the loss
70:25 - function or for that one specific
70:27 - iteration after that forward pass once
70:30 - we have the loss function then we can
70:33 - understand how we need to change our
70:36 - weights and our bias factors such that
70:38 - we can make less errors that in the
70:41 - current iteration the moral is making so
70:44 - the errors the average errors that the
70:45 - moral is making we are measuring it by
70:47 - the loss function now then the next step
70:49 - as part of this algorithm it is to
70:52 - obtain
70:54 - specific amount of change in the loss
70:56 - function when we are making a small
70:58 - change in the weights parameters and in
71:02 - the bias parameters and this is what we
71:04 - are also referring as gradients the
71:07 - entire process of computing those
71:09 - gradients is called back propagation so
71:12 - back propagation is basically a fancy
71:14 - way of referring to the process of
71:16 - calculating the first order partial
71:19 - derivative of the loss function with
71:21 - respect to each of our modal parameters
71:24 - including various weight parameters and
71:26 - the bias parameters so this back
71:29 - propagation is fundamental because it
71:31 - helps the algorithm to understand how
71:34 - much a change in L function we will
71:37 - observe if we change each of those
71:40 - different weight parameters because this
71:42 - is the way that we need to uh Supply
71:45 - this information to the optimization
71:48 - algorithm such as G Adam in order for
71:50 - the optimization algorithm to be able to
71:53 - to update the model parameter so uh
71:56 - basically the output of the back propop
72:00 - is the input for the optimization
72:02 - algorithm such as GD HGD adom Etc so if
72:06 - we look at the specific example that we
72:09 - have looked into as part of the previous
72:11 - questions we saw this uh very simple
72:13 - narrow Network where we had a small
72:16 - amount of hidden layers and small amount
72:18 - of hidden units and we had just couple
72:20 - of input signals so as part of the
72:23 - forward pass we then performed all the
72:25 - Transformations we have activated the
72:27 - neurons and then we end up with this y
72:29 - hat which is the prediction of the
72:32 - response variable so once we have the Y
72:35 - hat then we also have cached our final
72:40 - uh values for our Z scores so the Z
72:43 - scores were equal to the X multip by the
72:46 - weight parameters plus the bias
72:48 - parameters so once we have this then we
72:51 - can apply a various differentiation
72:54 - rules specifically the change rule but
72:57 - also we can make use of the
72:59 - differentiation rule of the summations
73:01 - and add one applied to the constant in
73:04 - order to update the gradients of the L
73:08 - function with respect to the activation
73:10 - so the da and then we can use this in
73:13 - order to obtain the gradients of the L
73:16 - function with respect to the Z and then
73:18 - we can make use of this da and then DZ
73:21 - to obtain the gradients of the loss
73:23 - function with respect to the weight so
73:25 - that is DV and then finally in the
73:28 - similar way we can make use of Da and DZ
73:30 - in order to obtain the gradients of the
73:33 - loss function with respect to the Beats
73:35 - so our bias factors what is important
73:38 - here is to keep in mind without going
73:41 - too much into details of mathematical
73:43 - derivations that in order to update uh
73:47 - the model parameters we need to know the
73:51 - uh gradients of the loss function with
73:52 - respect to the weight parameter so DV as
73:55 - well as the gradients of the bias
73:57 - factors which is the first order partial
73:59 - derivative of the loss function with
74:01 - respect to the B so the bias factors but
74:04 - then in order to compute the DV and the
74:07 - DB mathematically we also need to First
74:10 - compute the gradients of the activations
74:14 - and the gradients of the the z s and the
74:17 - reason for that is because the the
74:20 - weight factors and the bias factors they
74:23 - are part of the Z scor so we have this
74:25 - chain of Transformations we applied
74:28 - which means that when we are performing
74:30 - differentiation we then also need to
74:32 - perform this chain of differentiations
74:35 - in order to be able to obtain the
74:37 - gradients of the weights and gradients
74:38 - of the Bas factors in the first place
74:41 - which means that first we need to unpack
74:43 - what is the gradients of the activations
74:46 - then we need to understand what is the
74:47 - gradients of the Z scor we need to then
74:50 - use this applying the chain rule in
74:52 - order to obtain this DV as you can see
74:55 - here Z is equal to W mtip with the
74:58 - activations a plus b to be more specific
75:02 - the transpose of the weight Matrix
75:04 - multiplied with the activations Matrix
75:07 - then we are adding the bias vors which
75:10 - forms our Zs and then to compute the
75:12 - gradients of the loss function with
75:14 - respect to the weight so DV we first
75:17 - need to take the partial derivative of
75:19 - the loss function J with respect to the
75:21 - Z sores and then we need to
75:23 - multiply with the partial derivative of
75:26 - the Z scores with respect to the weight
75:29 - Matrix in order to be able to compute
75:32 - this partial derivative DV and then this
75:35 - is simply equal to the DZ multiply by
75:38 - the transpose of the activations Matrix
75:42 - and something similar we are also
75:43 - performing for obtaining the gradients
75:46 - of the bias vectors I will not go too
75:49 - much into detail of the mathematical
75:51 - derivations I will not go too much into
75:53 - details of the mathematical derivations
75:56 - of this gradients cuz you will need to
75:59 - dive deep into this various partial
76:01 - derivatives we need to also refresh our
76:03 - memory when it comes to using chain
76:05 - rules the differentiation rules when it
76:08 - comes to the sum of the values when it
76:10 - comes to constant multiplied with the
76:12 - target value and those are things that I
76:15 - would definitely recommend you to
76:17 - refresh when you are going through deep
76:19 - learning topics but just to say in scope
76:23 - of the interview question I just wanted
76:25 - to share this just to refresh your
76:27 - memory that um in every time when we are
76:30 - trying to optimize the algorithm in
76:33 - every step we are performing back
76:35 - propagation which includes calculating
76:37 - all these various gradients and in order
76:40 - to compute the gradient with respect to
76:43 - the weight and the gradient with respect
76:45 - to the bias vectors we first also need
76:47 - to calculate the gradients with respect
76:49 - to activations and the gradients with
76:51 - respect to the Z scores in order to then
76:54 - use them to calculate the gradients of
76:57 - the weights and then gradients of the
76:58 - bias vectors and then to finalize this
77:02 - the back propagation is simply this
77:05 - process of computing the uh partial
77:08 - derivatives of the loss function with
77:10 - respect to the weights and with respect
77:12 - to the bias factors in order to supply
77:15 - them this as an input for our
77:18 - optimization algorithm which is actually
77:20 - our next question the next question is
77:23 - how is back propagation different from
77:26 - gradient descent so during your deep
77:29 - learning interviews you might get a
77:31 - question regarding the difference
77:33 - between back propagation and gradient
77:35 - descent this is usually a question a
77:38 - tricky one to confuse you because
77:40 - sometimes it's very intuitive to
77:42 - describe the training process and to say
77:44 - that we first perform our forward path
77:48 - we then compute our loss function to see
77:51 - how well the model is training and the
77:53 - amount of air that is making on average
77:55 - and then we compute the gradients to
77:58 - understand how much we need to how much
78:00 - of a change we have in the loss function
78:02 - when we apply the uh change in various
78:04 - uh parameters and then we use this in
78:07 - order to uh Supply it to the backward
78:10 - path to continuously then update the
78:13 - model parameters up until the earlier
78:15 - layers in order to end up with a model
78:18 - that is performing better by coming from
78:21 - the deeper layers to the earlier layers
78:24 - so this sounds very intuitive and you
78:27 - might be still you might be still be
78:30 - asked a question but how is this then
78:33 - different from the gradient descent what
78:36 - is this different specifically when it
78:37 - comes to B propagation and GD well the
78:41 - answer to this question actually is very
78:43 - simple one and this is just in my
78:45 - opinion a question to trick the
78:47 - interview but I would just say that the
78:50 - back propagation is the actual process
78:52 - of Computing the gradients to understand
78:56 - how much a change in the L function is
78:58 - there when we are changing the model
79:00 - parameters and then the output of the
79:03 - back propagation is simply used as an
79:05 - input for the gradient descent or any
79:08 - other optimization algorithm in order to
79:11 - update the model parameters so the
79:13 - gradient descent is simply using the
79:16 - output of the back propagation as an
79:18 - input because it's taking the computed
79:21 - gradients from from the back prop as an
79:24 - input in order to then update the model
79:27 - parameters so the back propagation
79:29 - happens in the middle once we end up
79:31 - with our forward pass and then the back
79:34 - propagation is done and then we are
79:36 - performing our backward pass so we are
79:38 - using the back prop and then we are
79:40 - continuously iteratively updating our
79:42 - model parameters from deeper layers to
79:44 - the earlier layers and then this is what
79:47 - is done by the optimization algorithm
79:49 - the next question is describe what
79:51 - Vanishing gradient problem is and it
79:53 - impact on narrow networks so vention
79:56 - gradients happens when the gradients of
79:58 - a network so loss functions which
80:00 - respect to the modal parameters such as
80:02 - weights and the bias parameters they
80:05 - become very small and in some cases they
80:07 - become entirely close to zero which
80:10 - means they start to vanish as they
80:13 - propagate back through very deep layers
80:15 - to the earlier layers and the result of
80:19 - this Vanishing gradients is that the
80:21 - network is no longer able to learn
80:23 - dependencies in the data effectively and
80:27 - the model is no longer able to update
80:28 - the model effectively which means that
80:31 - the algorithm will end up not being
80:33 - optimized and we will end up with a
80:36 - model that is unable and was not able to
80:38 - learn the actual dependencies in the
80:41 - data of course that's something that we
80:43 - want to avoid and we want to have a
80:46 - proper amount of gradients that we will
80:48 - use as part of the optimization
80:50 - algorithms like GD in order to obtain
80:52 - the moral parameters and then do this
80:55 - iteratively and continuously such that
80:57 - we will end up minimizing our L function
81:00 - and our model will be at a state that it
81:02 - will provide highly accurate
81:05 - predictions so how do we get this
81:07 - Vanishing gradient problem and what is
81:09 - the reason of that so as part of
81:12 - training of a neural network we saw that
81:15 - in each training iteration as part of
81:17 - the gradient descent algorithm for
81:19 - instance we would be using the entire
81:21 - training data to train our Network and
81:25 - then perform these different
81:26 - Transformations from very earlier layers
81:28 - to up until the last output layer to
81:31 - take the activations and then
81:33 - multiplying it with the weight Matrix
81:36 - and then adding the bias parameters
81:38 - comput the Z scores in this way and then
81:41 - apply the activation function in order
81:43 - to activate those neurons and then after
81:46 - this is done in that specific hidden
81:48 - layer the network is able to learn those
81:51 - different dependencies in the D data and
81:53 - then learn the structure and then move
81:55 - on on to the next layer and then to the
81:57 - next layer up until reaching the output
82:00 - layer and then when we have our output
82:03 - layer we then are able to calculate our
82:06 - prediction so y hat and then we are able
82:09 - to compare the Y hat to the true labels
82:12 - or True Values and then understand what
82:15 - is this loss function what is the
82:16 - average error that the model is making
82:18 - with this set of parameters and then we
82:20 - are performing the back propagation to
82:22 - comput gradients then supplying it to
82:24 - the optimization algorithm in order to
82:27 - update the model parameters and the way
82:29 - the optimization algorithm is doing it
82:31 - is uh opposite of the forward path so it
82:34 - is Computing and taking the gradients
82:37 - Computing the corresponding updates and
82:40 - then updating the weights and the bias
82:42 - factors from Deep layers to the earlier
82:45 - layers and the problem with this is that
82:47 - we are performing all these
82:49 - Transformations and then we are
82:51 - cumulatively every time time multiplying
82:53 - those values what this means is that in
82:57 - some cases especially when we have deep
82:59 - Network when we have many hidden layers
83:02 - by the time the network approaches from
83:04 - Deep layers the middle layers and then
83:06 - earlier layers this after this
83:09 - multiplication of many of these weights
83:11 - and this gradients start to become very
83:13 - close to zero when the gradients become
83:16 - very close to zero it means that we have
83:19 - nothing than to update our weights and
83:22 - update our bias parameters when we can
83:25 - no longer update our weights in our
83:27 - model it means that the network is no
83:29 - longer able to properly learn especially
83:32 - from the earlier layers when the
83:34 - gradient is Vanishing and this is then
83:37 - the problem because we want our model to
83:39 - continuously learn and continuously
83:42 - update those weights such that we will
83:44 - end up with the best set of parameters
83:46 - for the Ws and the bias factors in order
83:49 - to uh minimize the loss function and
83:51 - then provide highly accurate predictions
83:55 - therefore ideally we want to ensure that
83:57 - those gradients do not vanish so we
84:00 - combat the vention gradient problem and
84:02 - our network is able to properly learn
84:05 - and understand this uh dependencies in
84:08 - our data independent whether it's in the
84:10 - Deep layers or in the earlier layers so
84:13 - when it comes to description of this
84:15 - problem try to focus on the weights
84:17 - because that's the biggest challenge
84:19 - when the gradients of the weight start
84:21 - to vanish as especially for earlier
84:23 - layers in a deep neural network and
84:25 - that's something we want to avoid
84:27 - because then the morel in case of
84:29 - Vanishing gradients problem is no is not
84:32 - able to effectively learn another
84:34 - important thing that you can mention as
84:36 - part of your answer is that there are
84:39 - certain architectures that inherently
84:41 - are subject to this problem and those
84:44 - are especially the RNs or reur Naro
84:47 - networks as well as lstms grus which are
84:51 - all inherently deep in their nature
84:54 - because unlike the original neural
84:56 - networks like artificial neural networks
84:58 - that have a single input layer they
85:01 - provide the input and then the neurons
85:03 - are being activated and they provide a
85:05 - single output and the hidden layers they
85:09 - are not interconnected so they are not
85:14 - like the RNN that take the input and
85:16 - then for each input they provide the
85:18 - corresponding output and then this
85:20 - hidden layer uh and then hid unit is
85:22 - then being used as part of the next step
85:25 - and the next step so they are not
85:27 - sequential based like rnn's then they
85:30 - are less likely to to be prone to this
85:33 - Vanishing gradient problem than the RNs
85:35 - or because rnms and other sequence type
85:38 - of neural networks they are inherently
85:41 - uh deep which means that they have this
85:44 - multiple layers uh based on the amount
85:47 - of time steps that they have which then
85:50 - makes uh this algorithms more prone to
85:52 - the vening gradient problem the next
85:54 - question is what is the connection
85:56 - between various activation functions and
85:58 - the vening gradient problem so Vishing
86:01 - gradient problem is highly related to
86:04 - the choice of activation functions
86:07 - because there are certain activation
86:09 - functions that automatically result in
86:12 - vention gradient problem because of
86:13 - their inherit characteristics and there
86:16 - are also activation functions which are
86:18 - known to not cause the Vishing gradient
86:20 - problem and of course we want to be
86:22 - aware of such activation functions and
86:25 - then not use them whenever we are aware
86:28 - that there can be Vanishing gradient
86:30 - problem so here I tend to make a
86:33 - distinction between the sigmoid
86:35 - activation function and the tank
86:37 - activation function versus the reu so
86:40 - rectifier linear unit and the Leaky relu
86:43 - activation functions and the reason for
86:45 - that is because of their inherit
86:47 - property what we are calling saturation
86:50 - what this means is that in case case of
86:52 - sigmoid activation function for instance
86:55 - when the activation is FZ is equal to 1
86:58 - / to 1 + e^ minus Z it means that
87:02 - whenever the Z or the Z course is very
87:06 - large negative number let's say minus
87:08 - 1000 then the corresponding activation
87:12 - the F set will then be very close to
87:16 - zero as you can see here which is
87:18 - considered as a minimum for this sigmoid
87:22 - function and then when the Z so the Z
87:26 - score in this case from this graph the X
87:28 - will be very large positive number so it
87:31 - will be around 1,000 then the
87:33 - corresponding function the activation
87:35 - will be around one so very close to one
87:38 - and as you can see at some point after
87:41 - certain level of x uh for the positive
87:44 - values independent how large this x will
87:47 - be the corresponding activation will be
87:50 - very close to one and at some point
87:52 - there will not even be a change in this
87:54 - area this is exactly what we are
87:56 - referring as saturation which means that
87:59 - when we have very large negative or very
88:01 - large positive number then there will no
88:04 - longer be any change in the
88:07 - corresponding activation we will just
88:09 - end up approaching the maximum or the
88:11 - minimum of this function in this case
88:14 - for very large positive ex's the Y will
88:16 - be around one and for very large
88:19 - negative numbers the corresponding activ
88:22 - will be around zero therefore in case of
88:25 - sigmoid activation function we are
88:27 - saying that the sigmoid activation
88:29 - function suffers from
88:30 - saturation and it is prone to causing
88:33 - the vanishing gradient problem so it
88:35 - causes the gradients to vanish
88:38 - especially when we have this very large
88:41 - values as a z score so it can be large
88:45 - positive or large negative and actually
88:48 - the same holds for the tank activation
88:51 - function because like like you can see
88:52 - here Tang activation function like
88:55 - sigmoid activation function is this
88:57 - s-shaped activation function and like
89:00 - sigmoid function the tank also suffers
89:03 - from saturation and in here you can see
89:06 - that when we have X which is very large
89:09 - positive number let's say 1,000 then the
89:12 - corresponding activation will be around
89:14 - one the same will hold if we change this
89:17 - X and we make it 2,000 or 3,000 so so
89:22 - independent how much we will increase
89:25 - this large number for X the
89:27 - corresponding activation will be the
89:29 - same it will be close to one which is
89:31 - the maximum of this function and then
89:33 - the same holds for the very large
89:36 - negative numbers for this activation
89:37 - function so let's say the x is around-
89:41 - 1,000 then the corresponding activation
89:43 - will be around minus1 but the same holds
89:46 - also when the X will be around - 2000 or
89:50 - - 10K so this means that in case of tank
89:54 - activation function as in case of sigo
89:56 - activation function when we have very
89:58 - large positive or very large negative
90:01 - values and we want to see how much we
90:03 - can activate them using this activation
90:05 - function then the corresponding
90:06 - activation value is around the extreme
90:09 - of this function so minus one and one in
90:12 - case of tank function and the zero and
90:15 - one in case of sigmoid activation
90:17 - function so therefore like in case of
90:19 - sigmoid activation function we are
90:21 - saying that tank activation function
90:24 - suffers from saturation and this means
90:27 - that tank activation function inherently
90:30 - causes the vanishing gradient problem
90:32 - because it's causing the gradients to
90:34 - vanish and that's something that we want
90:36 - to avoid so one thing that we can keep
90:40 - in mind is that the sigmoid activation
90:42 - function and Tang activation functions
90:44 - are great when it comes to the output
90:46 - layer because they have this nice
90:48 - properties in transforming the values
90:51 - between certain C ranges like in case of
90:53 - sigmoid function it transforms the
90:55 - values any values between Z and one
90:57 - which is great when we want to have an
90:59 - output in the form of probabilities
91:02 - great for classification type of
91:04 - problems but when it comes to using this
91:06 - activation functions for hidden layers
91:09 - that's something that we want to avoid
91:11 - so my suggestion would be to use sigmoid
91:14 - activation function or hyperbolic tank
91:16 - activation function for your output
91:19 - layers but then do not use it for your
91:21 - hidden layers because they are prone to
91:24 - Vanishing gradient problem the next set
91:26 - of activation functions that we will
91:28 - discuss today are the rectifier linear
91:31 - unit and the Leaky Rel so leaky
91:33 - rectifier linear unit unlike the sigmoid
91:36 - activation function and the Tang
91:38 - activation function reu or Liu they both
91:41 - do not suffer from saturation this means
91:44 - that they are then no longer causing the
91:47 - vening gradient problem and they are
91:49 - great for using it as part of the Hidden
91:51 - layer
91:52 - it you can see that the for the cases
91:55 - when the we take very large positive X
91:59 - then the corresponding activations will
92:01 - change and when we go from let's say
92:04 - 1,000 to 2,000 from 2,000 till 10K we
92:08 - will have the corresponding activation
92:10 - values also change a lot which means
92:13 - that the activation function will not
92:16 - result in the gradients coming very
92:19 - close to zero which is what we are
92:21 - describing as a Vishing gradients so as
92:23 - part of this REO will not be causing the
92:26 - vening gradient problem which is
92:29 - something that is great especially when
92:31 - we are using them as part of our hidden
92:33 - layers the same also holds for the Le
92:37 - which you can see here because the two
92:39 - are very similar to each other and with
92:41 - only difference that Liu does consider
92:44 - activating at some extent the negative
92:47 - neurons which means that when ex is
92:50 - negative value so for from minus
92:52 - infinity to 0 you can see that the
92:53 - corresponding activations are not zero
92:56 - but they are equal to
92:58 - 0.01 so this means that unlike the
93:02 - sigmoid and tank activation function
93:04 - those two are not suffering from
93:06 - saturation so they activation values
93:08 - will change and will not be close to the
93:12 - same extreme of the function uh when we
93:15 - make very when we provide as an input
93:19 - very large positive or very large
93:21 - negative vales which means that they are
93:23 - ideal for using as part of the Hidden
93:25 - layers such that the network can
93:28 - continuously effectively learn those
93:30 - hidden dependencies in the data
93:32 - therefore we are saying use reu and
93:35 - leaky reu as part of your hidden layers
93:37 - to activate those neurons and whenever
93:40 - you approach this output layer for the
93:42 - output layer use the sigmoid function or
93:44 - the Hang activation function the next
93:47 - question is there is a neuron in the
93:49 - hidden layer that always results results
93:51 - in a large error in back propagation and
93:54 - what could be the reason for that so
93:57 - there can be many reasons that can
94:00 - result in this specific neuron or some
94:03 - of the neurons to consistently result in
94:06 - large error as part of the back
94:08 - propagation in the na Network so the
94:10 - first reason could be poor
94:11 - initialization of the weights and what
94:14 - this means is that in the very beginning
94:16 - when we are starting the training
94:17 - process we need to initialize the weight
94:21 - PR
94:22 - and the bias parameters such that the
94:23 - network can uh take this initial values
94:27 - and then perform the forward pass
94:30 - compute the loss and then use this as an
94:32 - input for the optimization algorithm
94:35 - like GD SGD in order to then update
94:38 - those weight parameters
94:39 - iteratively but if we do this
94:42 - initialization of the we weights not in
94:44 - a proper way which means that we are not
94:47 - using the right distribution to random
94:49 - from it to randomly sample from from it
94:52 - or we are just providing a wrong or not
94:57 - proper value to this weight and bias
95:00 - parameters it means that this will right
95:03 - from the beginning skew off the learning
95:06 - process for those specific neurons and
95:09 - if we right from the beginning perform
95:11 - the learning process in an improper way
95:14 - and we are sking those learning
95:17 - processes in every iteration this means
95:20 - that the next iteration that will be
95:22 - based in the first iteration that
95:24 - started wrongly from the first pass it
95:27 - will also be done in the wrong way and
95:29 - then the next time and then next time
95:31 - this process will continue to result
95:33 - every time in an improper learning and
95:36 - in the large error as part of the
95:39 - calculation of the loss and then back
95:41 - propagation and another reason for this
95:44 - can be the V Vanishing or the exploding
95:46 - gradients so if the gradients become
95:49 - very small uh and the weight of the
95:51 - neurons especially in deep network no
95:54 - longer start to update those different
95:57 - parameters this means that uh we can
95:59 - persistently make those large errors in
96:02 - specific Neons because we are no longer
96:05 - able to effectively learn those
96:07 - dependencies in the model and uh that
96:10 - specific name will not be correctly
96:12 - activated and will not be the
96:14 - corresponding weights will not be
96:16 - updated in order to improve the accuracy
96:19 - of the predictions for those specific
96:21 - neurons and of course the same holds for
96:23 - the opposite problem which is the
96:25 - exploding gradient problem if we are
96:27 - making too much of an updates and right
96:30 - and the neurons are not being properly
96:32 - activated the corresponding weights are
96:34 - not being properly updated this means
96:37 - that we will be consistently making
96:39 - those large errors for those Neons
96:42 - another reason for this can be
96:44 - inadequate learning rate which means
96:46 - that if we have too high learning rate
96:49 - or too small learning rate we might be
96:52 - overshooting the optimal values during
96:55 - the training or undershooting so
96:57 - insufficiently updating those weight
96:59 - parameters and those bias parameters and
97:02 - this means that we will then
97:04 - continuously make large errors for those
97:06 - specific Neons therefore it's really
97:08 - important to optimize these hyper
97:10 - parameters in our model such that the
97:13 - model can make consistent updates and
97:16 - consistently improve the model
97:18 - parameters which means also uh improve
97:21 - the learning process another reason can
97:23 - be improper activation function because
97:26 - activation functions can significantly
97:28 - impact the learning process the way we
97:31 - activate our neurons will be a defining
97:34 - factor for the neuron Network to
97:36 - properly learn those dependencies in
97:38 - data and then understand how the weight
97:42 - parameters and the parameters need to
97:44 - updated such then the such that the
97:46 - learning will be optimized the next
97:49 - question is what do understand by a
97:50 - computation graph so computational graph
97:53 - is a way to visualize complex operations
97:56 - that we are using as part of uh training
98:00 - models as part of various type of
98:03 - optimization processes and it's a great
98:05 - way to Showcase all the steps that we
98:08 - are taking when going from very simple
98:11 - variables or objects till applying
98:14 - various Transformations and then getting
98:16 - to the very complex functions so one way
98:20 - that we are using comp ational graphs
98:22 - are when we are visualizing the neural
98:25 - networks so we saw this a very basic
98:28 - representation and a basic neural
98:30 - network in the earlier interview
98:32 - questions and this one is a vivid
98:35 - example of applying computational graph
98:37 - in here you can see that we are using
98:40 - the the not and the edges in order to
98:43 - describe the computational graph and the
98:46 - in the computational graph we usually
98:48 - have the nodes representing objects or
98:51 - variables and then the edges usually
98:54 - represent the type of operations or
98:56 - Transformations that we are applying to
98:59 - those objects or variables in this case
99:02 - you can see that we have input signals
99:04 - X1 X2 X3 which are simply the variables
99:07 - those are the nodes at the initial layer
99:11 - and then we are showcasing that we are
99:13 - applying those edges this arrows that go
99:15 - onto our first hidden layer in this
99:19 - hidden layer you can see that we have
99:21 - the Z scores pair hidden unit and then
99:24 - we have the corresponding Activation so
99:26 - Z1 A1 corresponding to the first hidden
99:29 - layers first hidden unit and then we
99:32 - have the Z2 A2 and a Z3 A3 Z4 A4 because
99:38 - we have four different hidden unit as
99:40 - part of our single hidden layer then we
99:44 - apply another
99:46 - transformation uh when we go from the
99:48 - first hidden layer to the uh to to the
99:51 - output layer what you can see here is
99:53 - that we have this arrows that showcase
99:56 - how we go from X1 X2 X3 so our input
100:00 - data uh to the Z scores and then we
100:03 - apply the activation on the Z scores to
100:05 - compute our activation scores and then
100:08 - we use this activation scores from the
100:10 - final layer in order to go onto our uh
100:14 - predictions which is the Y hat you can
100:17 - see here that we are using this final
100:19 - output layer to compute the Y hat
100:22 - and uh this is something that we are
100:24 - doing to visualize this process of
100:27 - computing first the Z scores based on
100:30 - the XI the input data multiplied with
100:33 - the transpose of the weight Matrix W
100:35 - plus b so this set the observation layer
100:38 - observation level and then the uh y I
100:41 - had is then simply equal to AI equal to
100:44 - Sigma zi and what we are doing here is
100:48 - that we are uh Computing the Z scores
100:51 - and then after Z scores we are applying
100:53 - this Sigma function which is the
100:55 - activation function that we are getting
100:58 - and then after the activation function
101:01 - is applied on the Z scores we are then
101:03 - Computing and getting the activation
101:06 - scores and then using this activation
101:08 - scores to compute our Yi head and then
101:12 - once we have our Yi head which is our
101:15 - prediction then we are comparing it to
101:18 - the True Values so the y i and then we
101:21 - are Computing our loss so the amount of
101:24 - airor that the motor is making so you
101:26 - can see what are those steps that we are
101:28 - making and when we have more hidden
101:30 - layers not just one but let's say three
101:33 - four or billions of those layers then we
101:38 - will end up having a very hard time
101:41 - understanding those different steps let
101:43 - alone to visualize it or to Showcase how
101:47 - many layers we have and each time using
101:50 - different IND is each time performing
101:52 - the same
101:53 - Transformations from input data to Z
101:56 - scor and then to activation function for
101:58 - after activation function we get the
102:00 - activation scores you get the idea so
102:03 - it's then much easier to Showcase all
102:07 - these calculations using this
102:09 - computational graph so basically we
102:11 - instead of using all these
102:13 - Transformations these matrices that can
102:15 - be hard to understand uh we are using
102:18 - the computational graph to simplify the
102:21 - process and to visualize it to Showcase
102:23 - how we go from the input variables um
102:26 - and then we apply all these
102:28 - Transformations and then we end up with
102:30 - the
102:31 - predictions and then this computational
102:33 - graph can be made bit more difficult by
102:36 - also showcasing the the other side of
102:39 - the story so getting the uh predictions
102:41 - and then applying back propagation and
102:43 - then from back propagation to the actual
102:46 - gradiate calculations and then the
102:48 - updates of the parameters but then just
102:51 - for Simplicity I thought I will just use
102:54 - this simple neural network to Showcase
102:57 - this but of course the idea behind
103:00 - computational graph is not just to
103:01 - Showcase a forward path but also to
103:03 - showcase the entire network including
103:06 - the back propagation so calculation of
103:09 - the gradient what is gradient clipping
103:11 - and their impact on Naro Network so we
103:15 - just saw the residual connections and we
103:18 - saw that residual connections especially
103:20 - for deep neural network and specifically
103:22 - for the cases like RNN lstms when we
103:25 - have the sequential nature with too many
103:27 - layers uh we can have this Vanishing
103:30 - gradient problem and residual
103:31 - connections can help to solve this
103:33 - problem now gradient clipping is solving
103:36 - the opposite problem so it's solving the
103:39 - exploring gradient problem whenever we
103:42 - have certain deep naral networks like
103:44 - RNN lstms because we are each time using
103:49 - these Transformations and we are using a
103:51 - cumulative nature and multiplying all
103:54 - these weights towards each other through
103:56 - this process and we are using the
103:58 - previous hidden state to update the the
104:01 - new hidden state so next hidden State
104:03 - and next one it also means that when we
104:06 - are going through these layers during
104:09 - the uh optimization process after back
104:12 - propop is done we need to perform all
104:14 - this Transformations and by the time we
104:16 - come through the earlier layers it can
104:18 - be that we have exploding gradients so
104:22 - the gradients become so large that it
104:25 - starts to impact the entire performance
104:27 - of the model we are starting to make two
104:30 - big over jumps and the updates that we
104:33 - are starting to get and optimization
104:35 - process is starting to make too much of
104:38 - this oscilations towards different
104:40 - directions so we are seeing this tic
104:42 - Behavior as part of our optimization
104:45 - process and that's definitely not
104:47 - something that we want to have because
104:49 - exploring great Radiance means that we
104:51 - have unstable naral network and uh this
104:55 - also means that the update of the
104:57 - weights will be too large and this will
105:00 - then result in not well performing and
105:03 - not properly trained neural network now
105:06 - to avoid all these problems the way we
105:09 - can solve this exploding gradient uh
105:11 - problem is by using the uh gradient
105:14 - clipping what gradient clipping does is
105:17 - that it basically Clips the gradients at
105:20 - a certain level so there is a threshold
105:24 - which is the parameter that we are using
105:25 - for gradient clipping and whenever the
105:28 - gradient becomes too large so higher
105:30 - than the threshold then we will be
105:32 - clipping that gradient to ensure that
105:34 - this gradient is not too large and we
105:37 - are not updating our weight parameters
105:39 - or bios sectors too much so in this way
105:44 - what we are doing is that we are kind of
105:46 - stabilizing our neural network which is
105:48 - especially important for architectures
105:50 - like lstms RNN grus uh which have this
105:54 - sequential nature of the data with too
105:56 - many layers and uh in this way we will
105:59 - ensure that we do not have this ertic
106:02 - jumps in our naral Network optimization
106:05 - process and we will end up with a stable
106:09 - Network that is able to properly learn
106:11 - the
106:19 - interdependencies
107:21 - the next question is what is cross
107:23 - entropy and why it is preferred as a
107:25 - cost function for multiclass
107:27 - classification type of problems so cross
107:30 - entropy which is also known as log loss
107:33 - it measures the performance of a
107:35 - classification model that has an output
107:38 - in the terms of probabilities which are
107:41 - values between zero and one so whenever
107:44 - you are dealing with classification type
107:46 - of problem let's say you want to
107:48 - classify whether an image is of a cat or
107:51 - a dog or and the house can be classified
107:55 - as a old house versus a new house in all
107:59 - those cases when you have these labels
108:02 - and you want the morel to provide a
108:05 - probability uh to each of those classes
108:08 - per observation such that you will have
108:10 - as an output of your model that the
108:13 - house a has 50% probability of being
108:17 - classified as new 50% probability of
108:19 - being class ified as old or the cat has
108:23 - 70% probability of U uh being a cat
108:27 - image or this image has 30% probability
108:30 - of of being a dog image in all those
108:34 - cases when you are dealing with this
108:35 - type of problems you can apply the cross
108:38 - entropy as a loss function and the cross
108:42 - entropy is measured as this negative of
108:45 - the sum of the Y log p + 1 - Y and then
108:49 - log 1 minus P where Y is the actual
108:53 - label so in binary classification this
108:55 - can be for instance one and zero and
108:58 - then p is the predicted probability so
109:01 - in this case the P will be then the
109:02 - value between 0 and one and then the Y
109:05 - is the corresponding label so let's say
109:07 - a label zero when you are dealing with
109:10 - cat image and label one when you are
109:12 - dealing with a dog image and the
109:15 - mathematical explanation behind this
109:17 - formula is out of the scope of this
109:19 - question so I will not going into that
109:21 - details but if you are interested in
109:23 - that make sure to check out the logistic
109:25 - regression model this is part of my
109:28 - machine learning fundamentals handbook
109:30 - which you can check out and this one
109:32 - includes also logistic regression which
109:34 - explains step by step how we end up with
109:37 - this log likelihood function and how
109:39 - then we go from the products to
109:42 - summations after applying the
109:44 - logarithmic function so we get the log
109:46 - ODS and then we multiply it with the
109:49 - minus because this is the uh negative of
109:53 - the likelihood function given that we
109:56 - want to ideally minimize the loss
109:59 - function and this is the opposite of the
110:02 - likelihood function um and in this case
110:06 - what the showcases is that we will end
110:08 - up getting a volue that tells how well
110:12 - the model is performing in terms of
110:15 - classification so the cross entropy then
110:18 - will tell us whether the model is doing
110:20 - a good job in terms of classifying the
110:22 - observation to a certain class the next
110:24 - question is what kind of loss function
110:26 - we can apply when we are dealing with
110:28 - multiclass
110:30 - classification so in this case when
110:32 - dealing with multiclass classification
110:35 - we can use the multiclass crow entropy
110:38 - which is often referred as a softmax
110:40 - function so softmax loss function it's a
110:44 - great way to measure the performance of
110:46 - a model that wants to classify
110:49 - observation to one of the multiple
110:51 - classes which means that we are no
110:54 - longer dealing with binary
110:55 - classification but we are dealing with
110:57 - multic class
110:59 - classification so one example of such
111:01 - case is when we want to classify an
111:04 - image to be from Summer theme to be from
111:08 - Spring theme or from winter theme given
111:11 - that we have three different possible
111:13 - classes we are no longer dealing with
111:15 - binary classification but we are dealing
111:17 - with multiclass classification which
111:19 - means that that we also need to have a
111:22 - proper way to measure the performance of
111:24 - the model that will do this
111:26 - classification and soft Max is doing
111:29 - exactly this so instead of getting the
111:32 - pair observation two different values
111:34 - which will say what is the probability
111:36 - of that observation belonging to class
111:38 - one or class two instead we will have a
111:41 - larger Vector pair observation depending
111:45 - on the number of classes you will be
111:46 - having in this specific example we will
111:48 - end up having three different values so
111:51 - one vector with three different entries
111:53 - per observation saying what is the
111:56 - probability that this picture is from
111:58 - Winter seene what is the probability of
112:01 - this observation coming from Summer
112:03 - theme and the third one what is the OBS
112:05 - what is the probability that the
112:06 - observation comes from a spring theme in
112:09 - this way we will then have all the
112:12 - classes with the corresponding
112:14 - probabilities so as in case of the Cross
112:17 - entropy also in case of the soft Max
112:20 - when we are when we have a small value
112:23 - for the softmax it means that the model
112:26 - is performing a good job in terms of
112:28 - classifying observations to different
112:30 - classes and we have well separated
112:32 - classes and one thing to keep in mind
112:34 - when we are comparing cross entropy and
112:36 - multi class cross entropy or the softmax
112:40 - is that we are usually using this
112:43 - whenever we have more than two classes
112:46 - and you might recall from the uh
112:48 - Transformer um model introduction from
112:51 - the paper tension is all you need that
112:54 - as part of this architecture of
112:56 - Transformers a soft Max layer is also
112:59 - applied um as part of the multiclass
113:02 - classification so when we are uh
113:05 - Computing our activation course and also
113:08 - at the end when we want to transform our
113:11 - output to a values that make sense and
113:13 - to measure the performance of the
113:15 - Transformer the Transformer the next
113:18 - question is what is a s GD and why it is
113:21 - used in training naral networks so SGD
113:25 - is like GD an optimization algorithm
113:27 - that is used in deep learning in order
113:29 - to optimize the performance of a deep
113:31 - learning model and to find a set of
113:34 - model parameters that will minimize the
113:36 - loss function by iteratively improving
113:39 - the parameters of the model including
113:41 - the weight parameters and the bias
113:43 - parameters so the SGD the way it
113:47 - performs the update of model parameters
113:50 - is by using a randomly selected single
113:53 - or just few training observations so
113:56 - unlike the GD which was using the entire
113:59 - training data to update the model
114:02 - parameters in one iteration in case of
114:05 - SGD the SGD is using just single uh
114:09 - randomly selected training observation
114:12 - to perform the update so what this
114:14 - basically means is that instead of using
114:17 - the entire training data for each up dat
114:20 - SGD is making those updates in the model
114:23 - parameters per training observation and
114:27 - there is also an importance of this
114:29 - random component so the stochastic
114:31 - element in this algorithm hence also the
114:34 - name stochastic gradient decent because
114:37 - SGD is randomly sampling from training
114:40 - observations a single or just couple of
114:43 - training data points and then using that
114:46 - it performs the forward path so it
114:49 - computes the Z scores and then computes
114:52 - the activation scores after applying
114:54 - activation function then reaches the end
114:57 - of the forward path and the network
115:00 - computes the output so the Y hat and
115:02 - then computes the loss and then we
115:04 - perform the back prop only on those few
115:07 - data points um and then we are getting
115:11 - the gradients which are then no longer
115:13 - the exact gradient so in SGD given that
115:16 - we are using only a randomly Selected
115:19 - Few data point or a single data point
115:22 - instead of having the actual gradients
115:24 - we are estimating those true gradients
115:27 - because the true gradients are based on
115:29 - the entire training data and in SGD for
115:32 - this optimization we are using only few
115:36 - data points what this means is that we
115:39 - are getting an imperfect estimate of
115:42 - those gradients as part of the back
115:44 - propagation which means that the
115:45 - gradients will contain this noise and
115:48 - the result result of this is that we are
115:51 - making the optimization process much
115:53 - more efficient because we are making
115:55 - those uh uh parameter updates very
115:58 - quickly based on pass by using only a
116:02 - few data points U and training a Neal
116:05 - Network on just a few data points is
116:08 - much faster uh and easier than using an
116:11 - entire training data for a single update
116:14 - but this comes at the cost of the
116:16 - quality of the SGD because when we are
116:19 - using only a few data points to train
116:21 - the model and then compute gradients
116:24 - which are the estimate of the true
116:26 - gradients then this gradients will be
116:28 - very noisy they will be imperfect and
116:31 - most likely far off from the actual
116:33 - gradients which also means that uh we
116:36 - will make a less accurate update to our
116:39 - model parameters and this means that
116:42 - every time when the optimization
116:44 - algorithm is trying to find that Global
116:46 - Optimum and make those movements per
116:49 - each r
116:50 - to move one step closer towards that
116:53 - Optimum most of the time it will end up
116:55 - making wrong decisions and will pick the
116:59 - wrong direction given that the gradient
117:01 - is the source of that choice of what
117:04 - direction it needs to take and every
117:07 - time it will make those uh oscilations
117:10 - those movements which will be very
117:14 - erratic and it will end up most of the
117:17 - time discovering the Lo opum instead of
117:20 - the global opum because every time when
117:23 - it's using just very small part of the
117:25 - training data it's estimating the
117:28 - gradients which are noisy which means
117:31 - that the direction it will take will
117:33 - most likely be also a wrong one and when
117:35 - you make those wrong directions and
117:38 - wrong moves every time you will start to
117:41 - ciliate and this is exactly what HD is
117:43 - doing it's making those wrong decision
117:46 - Choice when it comes to direction of the
117:48 - optimization and it will end up
117:51 - discovering a local optim instead of the
117:53 - global one and therefore the HGD is also
117:56 - known to be a bad Optimizer it is
117:59 - efficient it is great in terms of
118:01 - convergence time in terms of the memory
118:04 - usage CU storing moral and that is based
118:07 - on a very small data and storing that
118:10 - small data into the memory is not
118:13 - computationally heavy and memory heavy
118:16 - but this comes at the cost of this
118:18 - quality of the Optimizer and in the
118:21 - upcoming interview questions we will
118:22 - learn how we can adjust this SGD
118:25 - algorithm in order to improve the
118:28 - quality of this optimization technique
118:31 - the next question is why does sastic
118:33 - gradient Des sense or the SGD oate
118:35 - towards local minimum so there are a few
118:39 - reasons why this oscilation happens but
118:42 - first let's discuss what oscilation is
118:45 - so oscilation is the movement that we
118:48 - have when we're trying trying to find
118:50 - the global Optimum so uh whenever we are
118:53 - trying to optimize the algorithm by
118:55 - using an optimization method like GD SGD
118:59 - RMS probe Adam we are trying to minimize
119:02 - the loss function and ideally we want to
119:05 - change iteratively our model parameters
119:08 - so much that we will end up with the set
119:11 - of parameters resulting in the minimum
119:15 - so Global minimum of the loss function
119:18 - not just local minimum but the global
119:20 - one and the difference between the two
119:22 - is that the local minimum might appear
119:23 - as of it's the minimum of the loss
119:25 - function but it holds only for a certain
119:28 - area when we are looking at this
119:30 - optimization process whereas the global
119:32 - Optimum is really the mean the real
119:36 - minimum of the loss function and that's
119:39 - exactly uh that we are trying to chase
119:43 - when we have too much oscilations which
119:45 - means too much movements when we are
119:48 - trying to find the direction towards the
119:51 - global Optimum then this might become
119:54 - problematic because we then are making
119:56 - too many movements every time and if we
119:59 - are making those movements that are
120:01 - opposite or they are towards the wrong
120:03 - direction then this will end up
120:06 - resulting in discovering local opum
120:08 - instead of global opum something that we
120:10 - are trying to avoid and the oscilations
120:15 - happen much more often in the SGD
120:17 - compared to GD because in case of GD we
120:20 - are using the entire training data uh in
120:23 - order to compute the gradient so the
120:26 - partial derivative of the loss function
120:27 - with respect to the parameters of the
120:29 - model whereas in case of SGD we learned
120:32 - that we are using just randomly SLE
120:35 - single or few training data points in
120:38 - order to update the gradients and to use
120:41 - these gradients to update the model
120:43 - parameters this then for SGD results in
120:47 - having too many of this oscilation
120:49 - because the the random subsets that we
120:52 - are using they are much smaller than
120:53 - training data they do not contain all
120:55 - the information in training data and
120:58 - this means that the gradients that we
120:59 - are calculating in each step when we are
121:02 - using entirely different and very small
121:05 - data can defer significantly one time we
121:08 - uh can have One Direction the other time
121:11 - an entirely uh different direction uh
121:14 - for our movement in our optimization
121:17 - process and uh this huge difference
121:20 - there's variability in the direction
121:23 - because of the huge difference in the
121:25 - gradients can result in too often of
121:29 - this oscilation so too many of uh
121:32 - bouncing around towards the area to find
121:36 - the right direction towards the global
121:38 - Optimum in this case the minimum of the
121:40 - loss function so that's the first reason
121:42 - the random subsets the second reason why
121:45 - in HGD we have too many of those
121:47 - oscilations those movements
121:49 - uh is the step size so step size the
121:52 - learning rate can Define how much we
121:56 - need to update the the weights and or
121:59 - the bias parameters and the magnitude of
122:02 - this updates is determined by this
122:04 - learning rate which then also plays a
122:06 - role how many of this how different this
122:09 - movements will be and how large uh the
122:13 - the jumps will be when we are looking at
122:16 - the
122:17 - oscilations so the the third reason why
122:20 - the SGD will suffer from too many of
122:23 - oscilations which is a bad thing because
122:25 - it will result in finding a local
122:27 - Optimum instead of the global Optimum
122:29 - too many times is the imperfect estimate
122:33 - so when we are Computing the gradients
122:35 - of the loss function with respect to the
122:37 - weight parameters or the bias factors
122:40 - then if this is done on a small sample
122:42 - of the training data then the gradients
122:44 - will be noisy whereas if we were to use
122:47 - the entire training data that contains
122:48 - all all the information about the
122:50 - relationships between the features and
122:53 - just in general in the data then the
122:55 - gradients will be much less noisy they
122:57 - will be much more accurate therefore
123:00 - because we are using this the gradients
123:04 - of based on small data as estimate of
123:07 - the actual gradients which is based on
123:09 - the entire training data this introduces
123:12 - a noise so imperfection when it comes to
123:15 - estimating this true gradient and this
123:18 - imperfection can result in updates that
123:21 - do not always Point directly towards the
123:23 - global opum and this will then cause
123:25 - this oscilations in the
123:28 - HGD so at higher level I would say that
123:31 - there are three reasons why HGD will
123:33 - have too many of these oscilations the
123:36 - first one is the random subsets the
123:38 - second one is the step size and the
123:40 - third one is definitely the imperfect
123:42 - estimate of the
123:47 - gradients the next next question is how
123:50 - is GD different from HGD so what is the
123:53 - difference between the gradient descent
123:55 - and the stochastic gradient descent so
123:58 - by now given that we have gone too much
124:00 - into details of HGD I will just give you
124:03 - a higher level summary of the
124:05 - differences of the two so for this
124:07 - question I would answer by making use of
124:11 - four different factors that CA a
124:13 - difference between the GD and HGD so the
124:16 - first factor is the data usage the
124:18 - second one is the update frequency the
124:20 - third one is the computational
124:22 - efficiency and the fourth one is the
124:24 - convergence pattern so let's go into one
124:27 - of this into each of these factors one
124:29 - by one so gradient descent uses the
124:33 - entire training data when uh training
124:36 - the model and Computing the gradients
124:38 - and using this gradients as part of back
124:40 - propagation process to update the model
124:43 - parameters however SGD unlike GD is not
124:47 - using the entire training data when
124:49 - performing the training process and
124:51 - updating the model parameters in one go
124:54 - instead what SGD does is that it uses
124:57 - just a randomly sample single or just
125:00 - two training data points when performing
125:02 - the training and when using the
125:05 - gradients based on the Su points in
125:08 - order to update the model parameters so
125:10 - that's the data usage and the amount of
125:12 - data that SGD is using versus the GD so
125:15 - the second difference is the update
125:17 - frequency so given that GD updates the
125:21 - model parameters based on the entire
125:22 - training data every time it makes much
125:26 - less of this updates compared to the HGD
125:28 - because HGD then very frequently every
125:31 - time for this single data point or just
125:34 - few training data points it updates the
125:37 - model parameters unlike the GD that has
125:40 - to use the entire training data for just
125:43 - one single set of update so this causes
125:46 - then SGD to make those update much more
125:49 - frequently uh when using just a very
125:52 - small data so that's about the
125:53 - difference in terms of update frequency
125:56 - then another difference is the
125:57 - computational efficiency so GD is less
126:00 - computationally efficient than HGD
126:03 - because GD has to use this entire
126:05 - training data uh make the computation so
126:08 - back propagation and then update the
126:10 - model parameters based on this entire
126:12 - training data which can be
126:14 - computationally heavy especially if you
126:16 - are dealing with a very large data and
126:19 - very complex data and unlike GD SGD is
126:23 - much more efficient and very fast
126:25 - because it's using a very small amount
126:27 - of data to perform the updates which
126:30 - means that it is it requires less amount
126:32 - of memory to sort data it uses small
126:35 - data and it will then take much less
126:38 - amount of time to find a global optim or
126:42 - at least it thinks that it finds the
126:44 - global opum so the convergence is much
126:47 - faster in case of SGD compared to GD
126:49 - which makes it much more efficient than
126:51 - the GD then the final factor that I
126:53 - would mention as part of this question
126:55 - is the converence pattern so GD is known
126:59 - to be smoother and of higher quality as
127:02 - an optimization algorithm than SGD SGD
127:05 - is known to be a bed Optimizer and the
127:07 - reason for this is because that the
127:09 - efficiency of HGD comes at a cost of the
127:12 - quality of it of finding the global
127:14 - Optimum so SGD makes all the all this
127:17 - oscilation
127:19 - given that it's using a very small part
127:20 - of the training data when estimating the
127:23 - true gradients and unlike SGD GD is
127:26 - using the entire training data so it
127:28 - doesn't need to estimate the gradients
127:30 - it's able to determine the exact
127:33 - gradients and this causes a lot of
127:35 - oscilations Ina in case of SGD and in
127:38 - case of GD we don't need to make all
127:40 - these oscilations so the amount of
127:42 - movements that the algorithm is making
127:44 - is much smaller and that's why it takes
127:48 - much less amount of time for HG to find
127:50 - the global optim but unfortunately most
127:53 - of the time it confuses the global optim
127:55 - with the local opum so SGD ends up
127:58 - making this many movements and it end up
128:01 - discovering the local opum and confuses
128:03 - it with the global opum which is of
128:06 - course not desirable because we would
128:07 - like to have the actual Global opum so
128:10 - the set of parameters that will actually
128:12 - minimize and find the minimum value of
128:14 - the loss function and SGD is the
128:17 - opposite because it's using the true
128:19 - gradients and it is most of the time
128:22 - able to identify the true Global optim
128:25 - so the next question is how can we use
128:28 - optimization methods like GD uh but in a
128:31 - more improved way so how we can improve
128:34 - the GD and what is the role of the
128:36 - momentum term so whenever you hear
128:39 - momentum and then GD uh try to
128:41 - automatically focus on the HGD with
128:44 - momentum because SGD with momentum is
128:48 - basically Al the improved version of HGD
128:50 - and as far as you know the difference
128:51 - between HGD and GD it will be much
128:54 - easier for you to explain what is the
128:56 - HGD with
128:57 - momentum so uh we just discussed that
129:01 - the HD suffers from oscilation so too
129:04 - many of those movements and a lot of
129:07 - time because we are using a small amount
129:09 - of training data to estimate the true
129:11 - gradients this will result in having
129:14 - entirely different gradients and too
129:17 - much of the different sorts of updates
129:20 - in the weights and of course that's
129:22 - something that we want to avoid because
129:24 - we saw and we explained that too many of
129:27 - those movements will end up causing the
129:29 - optimization algorithm to mistakenly
129:33 - confuse uh the global opum and local
129:36 - opum so it will pick the local opum
129:38 - think that it's a global opum but it's
129:40 - not the case so to solve this problem
129:43 - and to improve the uh SGD algorithm
129:47 - while taking into to account that SGD in
129:50 - many aspects is much more much better
129:52 - than the GD we came up with this SGD
129:55 - with momentum algorithm where HGD with
129:57 - momentum will take basically the
129:58 - benefits of the HGD and then it will
130:01 - also try to address the biggest
130:03 - disadvantage of HGD which is this too
130:05 - many of these
130:07 - oscilations and the way SGD with
130:09 - momentum does is that it uses this
130:12 - momentum and it introduces this idea of
130:15 - momentum so momentum is basically a way
130:19 - to find and put the optimization
130:22 - algorithm towards better Direction and
130:24 - reduce the amount of oscilations so the
130:26 - amount of all this random movements and
130:29 - the way that it does is that it tries to
130:31 - add a fraction of this previous updates
130:34 - that we made on the motor parameters
130:37 - which then we assume will be a good
130:40 - indication of the more accurate
130:42 - Direction in this specific time step so
130:46 - imagine that we are at time time step T
130:49 - and we need to make the update then uh
130:51 - the what momentum does is that uh it
130:54 - looks at all the previous updates and
130:56 - uses the more recent updates more
130:58 - heavily and says that the more recent
131:01 - updates most likely uh will be better
131:04 - representation of the direction that we
131:06 - need to take versus the very old updates
131:09 - and this updates in the optimization
131:12 - process these very recent ones when we
131:15 - take them into account then we can have
131:17 - a better uh a better way of and more
131:21 - accurate way of updating the moral
131:24 - parameters so let's look into
131:26 - mathematical representation just for a
131:28 - quick refreshment so what the SGD with
131:30 - momentum tries to do is to accelerate
131:33 - this conversion process and instead of
131:37 - having too many of the movements towards
131:39 - the different direction and having two
131:41 - different ofen gradients and updates it
131:43 - tries to stabilize this process and have
131:46 - more constant updates and in here you
131:49 - can see that as part of the momentum we
131:52 - are obtaining this momentum term which
131:55 - is equal to VT +1 for the update at the
131:58 - time step of t+1 what it does is that it
132:02 - takes this this gamma multiplies it by
132:05 - VT plus the learning rate ITA and then
132:08 - the gradient where you can see that this
132:11 - inflated triangle and then underneath
132:14 - the Theta and then J Theta T simply
132:17 - means the gradient of the loss function
132:20 - with respect to the parameter TAA and
132:22 - what what is basically doing is that it
132:25 - says we are Computing this momentum term
132:27 - for the time step of t+ one which is
132:30 - based on the previous updates uh through
132:33 - this term GMA multip by VT plus the the
132:37 - common term that we saw before for the
132:39 - SGD and for GD which is basically uh
132:42 - using this ITA learning rate multiplied
132:46 - by the first order partial derivative of
132:48 - the loss function with respect to the
132:49 - parameter TAA so we then are using this
132:53 - momentum term to Simply subtract it from
132:55 - our current parameter TAA T in order to
132:59 - uptain and the new version so the
133:01 - updated version which is TAA t + one
133:04 - where TAA is simply the model parameter
133:07 - so in this way what we are doing is that
133:09 - we are performing more the updates in
133:12 - more consistent way so we are
133:14 - introducing consistency into the
133:16 - direction by waiting the recent
133:19 - adjustments more heavily and it builds
133:22 - up the momentum hence the name moment
133:26 - momentum so the momentum builds up this
133:28 - speed towards the direction of the
133:31 - global opum in more consistent gradients
133:34 - enhancing the uh movement towards This
133:37 - Global Optimum so the global minimum of
133:39 - the loss function and this then on its
133:41 - turn will improve of course the quality
133:43 - of the optimization algorithm and we
133:45 - will end up discovering the global
133:47 - Optimum rather than local Optimum so to
133:51 - summarize what this SGD with momentum
133:53 - does is that it basically takes the HGD
133:56 - algorithm so it again uses a small
133:58 - training data when performing the model
134:00 - parameter updates but unlike the SGD
134:04 - what SGD with momentum does is that it
134:06 - tries to replicate the gd's quality when
134:09 - it comes to the finding the actual
134:12 - Global optim and the way it does that is
134:14 - by introducing this momentum term which
134:16 - helps also to reduce consistency in the
134:19 - updates and to reduce the oscilations
134:21 - the algorithm is making by having much
134:23 - more smoother path towards discovering
134:26 - the actual Global Optimum of the L
134:29 - function the next question is compare
134:31 - badge gradient descent to mini badge
134:34 - gradient descent and to stochastic
134:36 - gradient descent so here we have three
134:40 - different versions of the gradient
134:42 - descent algorithm the uh traditional
134:44 - badge gradient descent uh often referred
134:47 - as GT simply uh the second algorithm is
134:50 - the mini badge gradient descent and the
134:53 - third algorithm is the SGD or the
134:55 - stochastic gradient descent so the three
134:58 - algorithm are algorithms are very close
135:01 - to each other they do differ in terms of
135:03 - their efficiency and the amount of data
135:05 - that they are using when performing each
135:08 - of this model training and the model
135:11 - parameters update so let's go through
135:13 - them one by one so the bch gradient
135:16 - descent this is the orig
135:18 - GD uh this method involves the
135:20 - traditional approach of using the entire
135:22 - training data for each iteration when
135:26 - Computing the gradients so doing the
135:27 - back prop and then taking this gradients
135:30 - as an input for the optimization
135:32 - algorithm to perform a single update for
135:35 - these model parameters then on to the
135:37 - next iteration when again using the
135:39 - entire training data to compute the
135:42 - gradients and to update the model
135:44 - parameters so here for the badge
135:46 - gradient descent we are we are not
135:48 - estimating the true gradients but we are
135:50 - actually Computing the gradients because
135:52 - we have the entire training data now B
135:54 - gradient descent thanks to this quality
135:56 - of using the entire training data uh has
135:59 - a very high quality so it's very stable
136:01 - it's able to identify the actual Global
136:04 - Optimum however this comes at a cost of
136:07 - efficiency because the bch gradient
136:09 - descent uses the entire training data it
136:11 - needs to every time put this entire
136:13 - training data into the memory and it is
136:15 - very slow when it comes to per
136:17 - performing the optimization especially
136:20 - when dealing with large and complex data
136:22 - sets now next we have the Other Extreme
136:25 - of the bch gradient descent which is SGD
136:29 - so SGD unlike the GD and we saw this
136:31 - previously when discussing the previous
136:33 - intrview questions that SGD is using uh
136:37 - stochastically so randomly sampled
136:39 - single or just few training observations
136:42 - in order to perform the training so
136:44 - Computing the gradients Performing the
136:46 - backrop and then using optimization to
136:49 - update the model parameters in each
136:51 - iteration which means that we actually
136:54 - we do not compute the actual gradients
136:57 - but we actually are estimating the true
137:00 - gradients because we are using just a
137:03 - small part of the training data so uh
137:06 - this of course comes at a cost of the
137:07 - quality of the algorithm although it's
137:10 - efficient to use only small sample uh
137:12 - from the training data when doing the
137:14 - back prop the training we need you don't
137:16 - need to store
137:18 - uh the entire training data into the
137:20 - memory but just a very small portion of
137:22 - it and we perform the model updates
137:25 - quickly but then uh we find the
137:27 - so-called Optimum much quicker compared
137:30 - to the GD but this comes at a cost of
137:33 - the quality of the algorithm because
137:35 - then it starts to make too many of these
137:37 - oscilations due to this noisy gradients
137:41 - which then ends up confusing the global
137:44 - opum with the local opum and then
137:46 - finally we have our third optimization
137:48 - algorithm uh which is the mini badge
137:51 - gradient descent and this mini badge
137:53 - gradient descent is basically the Silver
137:55 - Lining between the badge gradient
137:57 - descent and the original HGD so sastic
138:00 - gradient descent and the way mini badge
138:03 - works is that it tries to strike this
138:05 - balance between the traditional GD and
138:08 - the SGD it tries to take the advantages
138:11 - of the SGD when it comes to uh the
138:14 - efficiency and combine it with the
138:17 - advance of GD when it comes to stability
138:19 - and consistency of the updates and
138:21 - finding the actual Global Optimum and
138:23 - the way that it does that is by randomly
138:26 - sampling the training observations into
138:28 - two batches where the batch is much
138:30 - bigger compared to SGD and it then uses
138:34 - this smaller portions of training data
138:37 - in each iteration to to do the back
138:39 - propop and then to update the moral
138:41 - parameters so think of this like the
138:44 - kfold cross validation when we are
138:46 - sampling our training data into this K
138:49 - different folds in this case batches and
138:52 - then we are using this in order to train
138:54 - the model and then in case of naral
138:56 - networks to use the mini mini badge
138:58 - gradient descent to update the model
139:01 - parameters such as weights and bias
139:03 - vectors so the tree have have a lot of
139:06 - similarities but they also have
139:07 - differences and in this interview
139:09 - question your interview is trying to
139:11 - test whether do you understand the
139:13 - benefits of one and two and what is the
139:15 - purpose of having mini badge grade in
139:17 - descent so the next question is how to
139:20 - decide the badge size in a deep learning
139:22 - considering both twoo small and twoo
139:25 - large sizes this question is a very
139:28 - important question where the interviewer
139:30 - is trying to test whether you understand
139:32 - what is the impact of the badge size uh
139:35 - on your entire algorithm the quality of
139:38 - it so here your interviewer wants to
139:40 - know your understanding of the best size
139:44 - impact on the gradient noise on the bias
139:47 - of your model on the variance of the
139:48 - model on the generalization of the model
139:52 - uh the convergence of the model the
139:54 - efficiency especially the memory usage
139:57 - so uh we just discussed and compared the
140:00 - traditional badge gradient descent to
140:02 - the mini badge gradient descent and to
140:03 - the HGD and here we have already touched
140:07 - on this idea of using uh very large
140:10 - training data using small part of the
140:13 - training data like mini batches versus
140:16 - using even smaller like a single or very
140:19 - few observations randomly sampled from
140:21 - training data and this question is
140:24 - exactly related to that so let's say we
140:26 - have this mini badge gradient descent
140:28 - and we want to understand what is this
140:30 - badge size that we need to use what are
140:33 - the amount of observations that we need
140:35 - to put in single batch when performing
140:38 - the uh training process performing the
140:40 - back propop and then updating the model
140:42 - parameters in each
140:43 - iteration so let's compare the uh small
140:47 - bed size versus large bed size uh with
140:50 - respect to all these factors that I just
140:51 - mentioned so uh when it comes to the
140:54 - small bed sizes which means that let's
140:56 - say we have uh two up to 32 as the bed
141:00 - size so we have one till 32 observations
141:05 - uh that have that are in our batch so in
141:08 - this case the uh gradient noise will be
141:11 - of course very high and the reason for
141:13 - that is because we are using a very
141:16 - small part of the training data when
141:18 - estimating the gradients of our model
141:21 - the true gradients and this of course
141:23 - means that this estimate will Pro most
141:26 - likely not be the proper estimate for
141:29 - the true gradient because we are using
141:31 - just a very small part of the training
141:33 - data and this then introduces a lot of
141:36 - noise into our model versus when we have
141:39 - a large bed size then the gradient noise
141:41 - will be lower because we are using a
141:43 - larger training part uh training data
141:46 - part
141:47 - when performing the estimation of the
141:49 - true gradient then we have the
141:51 - convergence so when it comes to the
141:53 - small bed sizes the convergence tends to
141:56 - tends to be the quality of the
141:58 - convergence tends to be worse because
142:01 - it's like the SGD it tends to make all
142:04 - of these oscilations and though the
142:06 - convergence will be Speedy so it will be
142:09 - quick but it will most likely discover
142:11 - the local optim versus the global optim
142:13 - while when we have large bed sites then
142:16 - the AL it will often converge to sharper
142:19 - minimum so the the actual Global Optimum
142:22 - and the process might take slower but
142:26 - the quality will be higher then we have
142:28 - the generalization Factor uh
142:31 - generalization basically refers to the
142:33 - idea of overfitting how how much the
142:36 - model is actually memorizing the
142:37 - training data versus being able to
142:41 - perform equally well on the Unseen data
142:45 - now whenever we are using the entire
142:47 - training data which a lot of times will
142:50 - contain noise then the model by every
142:54 - time using this large portion or the
142:56 - entire training data when training the
142:58 - model it will be more likely to use this
143:03 - noise in order to obtain gradients and
143:06 - then perform modal parameter updates
143:09 - what this basically means is that for
143:11 - these large bed sizes then the
143:13 - generalization will be potentially worse
143:16 - because the model will be memorizing the
143:19 - training data it will be it will most
143:21 - likely uh catch this noise as a normal
143:25 - points and it will be much more focused
143:28 - on the training data uh whereas when we
143:31 - have a smaller bed size then we are
143:34 - randomly selecting a much smaller
143:36 - portion of the training data and the
143:38 - likelihood of having a noise in that
143:41 - small part of the training data in the
143:43 - small batch is lower which means that
143:46 - the like lihood of the morel following
143:49 - this noise and overfitting and
143:52 - Performing very worse on the Unseen new
143:55 - training data a new test data uh is
143:58 - lower so just to summarize when it comes
144:00 - to generalization because it's really
144:02 - important when we have small bed size
144:05 - then we have potentially better
144:07 - generalizing model because we will then
144:10 - using a small portion of the training
144:13 - data and the model will be less inclined
144:15 - to memorize the training data and it
144:17 - will perform uh equally well on the new
144:20 - unseen data versus the large B size
144:23 - which will intend and will have higher
144:25 - likelihood of following the noise in the
144:27 - data and then not generalizing well on
144:30 - the Unseen data then we have the bias so
144:33 - small bad size case then the bias will
144:36 - be potentially lower because the model
144:39 - will be less likely to overfit to the
144:42 - training pattern whenever we have a
144:44 - moral overfitting then the bias will be
144:46 - higher if we have lower chance for
144:48 - overfitting then the bias will be lower
144:50 - this also means that generalization and
144:52 - the bias are very much related as we
144:55 - also know in the traditional machine
144:56 - learning and the the opposite holds for
144:59 - the large bad size because for the large
145:01 - bed size we saw that it generalizes
145:04 - worse uh at the lesser extent which also
145:07 - means that the bias will most likely be
145:10 - higher in case of large B size so uh
145:14 - when it comes to the variance then in
145:16 - case of small small bed size the
145:17 - variance is higher because we are every
145:20 - time using a different part of the
145:22 - training data and small part so there
145:25 - will be much more of the oscilations and
145:27 - the exploration when it comes to finding
145:30 - the global Optimum whereas in case of
145:32 - larger bad size then we have a variance
145:34 - which is much lower because we are
145:37 - making less explorations to find the
145:40 - solution of the optimization process
145:43 - then we have the computational cost and
145:45 - the memory usage for large bed sizes
145:48 - when we have large amount of data then
145:50 - we will be storing much more data into
145:53 - the memory it also means that the
145:55 - convergence will be slower and the
145:58 - computational cost will be higher and
146:01 - the smaller is the bed size the lower is
146:03 - computational cost and the lower is the
146:06 - memory usage so the next question is how
146:09 - does the bedge size impact the
146:10 - performance of a deep learning model so
146:13 - bed size which is a number of training
146:15 - samples that we use pair iteration when
146:19 - performing the training of the model
146:21 - Computing the gradients as part of back
146:23 - propop and then updating our model
146:25 - parameters per iteration uh plays a
146:29 - significant role in the performance and
146:31 - the final outcome of the deep learning
146:33 - model there are few things that I would
146:36 - advise to mention as part of your
146:38 - interview answer to this question and
146:41 - the first one is definitely the training
146:43 - time and here it's important to to anal
146:46 - ize this with respect to the training
146:49 - time per iteration training time per
146:51 - Epoch and the overall training time
146:54 - because the two the Tre are actually
146:56 - very much different when we compare the
146:59 - bed size and here we would like to
147:02 - compare the small badge versus the large
147:05 - badge so when it comes to this smaller
147:08 - badge sizes which means that we are
147:10 - using smaller portion of the training
147:11 - data per iteration to compute the
147:14 - gradients and to perform the updates
147:16 - this means that per Epoch so it means
147:19 - when we want to pass the entire training
147:21 - data which is one Epoch it means that we
147:25 - need to have many of these iterations
147:27 - because per iteration we need to have we
147:30 - we just use a small portion of the
147:32 - training data and the smaller is the bir
147:34 - size it means the higher will be number
147:36 - of iterations per EPO to perform to
147:39 - ensure that the entire training data has
147:42 - been used for updating the model
147:44 - parameters so this also means that the
147:47 - training time will be longer per Epoch
147:49 - compared to the larger bed sizes where
147:52 - we will go through the entire training
147:54 - data so per Epoch we will have less
147:56 - number of iteration so it will take much
147:58 - less time per Epoch to train the model
148:02 - but the problem is that in case of
148:04 - larger bed sizes it can be Memory
148:07 - intensive and may not always even be
148:10 - feasible when we have a very small
148:12 - machine not a powerful one in order to
148:14 - perform the training uh so it comes also
148:17 - with Hardware
148:18 - limitations then another Factor you can
148:20 - mention is the memory usage so a larger
148:23 - bed size requires more memory whereas
148:25 - the a smaller bed size requires less
148:28 - memory so this is another factor that
148:30 - comes into the overall performance of
148:32 - your model and then we have the
148:34 - convergence quality this we have spoken
148:36 - a lot about so I will just briefly
148:38 - mention uh when we have smaller bed size
148:40 - the smaller is the badge size the higher
148:43 - is usually the generalization of the
148:46 - model but usually the lower is the
148:48 - convergence quality because it's more
148:50 - likely to catch the local Optimum
148:52 - instead of global Optimum then when it
148:54 - comes to the learning Dynamics then the
148:57 - bch size will impact the motor's ability
148:59 - to escape the local opum if we have the
149:04 - uh smaller bed sizes and we are using
149:06 - them when to estimate the the true
149:09 - gradient then this this gradients will
149:11 - be noisy which also means that the
149:14 - learning will be inconsistent and it is
149:17 - more likely to impact the quality of the
149:21 - optimization process and to catch the
149:24 - local optim inert Global Optimum and
149:26 - overall if your batch size is small then
149:28 - usually the algorithm is much more
149:31 - instable rather than when we have large
149:33 - badge size and the algorithm is much
149:35 - more consistent in the update of model
149:38 - parameters the next question is what is
149:41 - Haitian and how it can be used for
149:43 - faster training what are it
149:45 - disadvantages so if you got a course in
149:49 - differential Theory then you most likely
149:51 - know what Haitian means uh here I would
149:54 - just give you a very high level summary
149:56 - of what haian is and I will go in much
149:59 - more detail into the impact of using
150:01 - haian in deep learning so Haitian comes
150:06 - from a differential Theory and it is
150:09 - basically a matrix that contains the
150:12 - second order partial derivative of a
150:15 - function which back to the parameters in
150:18 - case of deep learning and as part of the
150:21 - optimization process Haitian can play a
150:24 - role of getting better and more accurate
150:27 - estimates of the gradients because it
150:30 - then will uh compute the second order
150:33 - partial derivative of the loss function
150:35 - with respect to the model parameters and
150:38 - the reason why we are using Haan in some
150:41 - cases in some adaptive learning
150:43 - optimization algorithms which are more
150:45 - improved version of SGD or HGD with
150:48 - momentum is to have a better estimate
150:51 - less noisy estimat of the true gradients
150:54 - and you might recall that when we were
150:57 - discussing the HGD and HGD with momentum
151:00 - we were saying that our primary goal is
151:04 - to reduce the number of oscilations as
151:06 - much as possible because when we are
151:09 - using a small bed size or just few
151:11 - training observations to perform our
151:13 - back prop and obtain the estimate of the
151:16 - true gradients then usually this ends up
151:19 - introducing instability in our model
151:21 - because this gradients this estimate of
151:23 - the gradients are not proper estimat of
151:26 - the true gradients and to improve this
151:29 - uh when we use Haitian Haitians are not
151:32 - to be much more accurate estimate of the
151:35 - gradients and they can help the
151:37 - algorithm the optimization algorithm to
151:40 - move towards more correct direction when
151:43 - we are trying to update the moral
151:45 - parameters so so using the same amount
151:47 - of data but having less noisy gradients
151:52 - and making more correct updates and take
151:55 - a smoother direction towards the global
151:58 - Optimum now this comes with certain
152:02 - disadvantages because it's not that easy
152:05 - to compute the haian so if we look at
152:07 - this example this is an example of an
152:10 - Haan Matrix and you can see that on the
152:12 - diagonal we have simply the the second
152:14 - order derivative of the function f with
152:17 - respect to the parameter X1 but then
152:21 - when it comes to multiple parameters so
152:23 - when we have multiple parameters in our
152:25 - model we then need to obtain the partial
152:28 - derivative of the function in our case
152:30 - the loss function with respect to the
152:33 - model parameters and when you do it this
152:36 - computation manually just as an example
152:39 - just for fun you will discover that in
152:42 - some cases when you have many parameters
152:45 - in your model then it will be sometimes
152:48 - super difficult to compute this second
152:50 - order partial derivatives beside of that
152:53 - this comes also at the cost of the
152:55 - computational resources when it comes to
152:58 - using hati as part of your optimization
153:00 - algorithm because this results in uh
153:04 - having this complicated functions
153:07 - putting into the memory and storing this
153:10 - information into the memory and this
153:13 - haian Matrix can be extremely large for
153:16 - or deep neural network models and this
153:19 - will then result in the high number of
153:21 - parameters making it sometimes
153:23 - computationally invisible or very
153:26 - expensive to calculate and to then
153:29 - invert the uh gradients then the other
153:32 - disadvantage of this haian usage is that
153:35 - we might have a risk of overfitting
153:38 - because when we are using this second
153:39 - order derivative the second order
153:41 - partial derivative um as way to
153:44 - estimated to gradients we might might be
153:46 - over relying on this haian for training
153:50 - accelerations and this might result in
153:53 - the moral memorizing the training data
153:56 - and overly relying on the training data
153:58 - when performing the parameter updates
154:00 - which will mean that the moral will be
154:03 - able will less likely generalize well on
154:07 - nonen data which is of course a problem
154:10 - because then our moral is overfitting
154:12 - and it will be able to generalize well
154:15 - on an in data which is our end goal the
154:18 - next question is discuss the concept of
154:21 - an Adaptive learning rate describe
154:23 - adoptive learning methods so here we
154:26 - need to talk both about the concept of
154:28 - adaptive learning rate and we also need
154:31 - to give you examples of optimization
154:33 - algorithms that are known to be of
154:35 - adaptive nature so when it comes to the
154:39 - traditional GD HGD or HGD with momentum
154:42 - so all the optimization algorithms that
154:44 - we spoke about before all these
154:46 - algorithms have single quality uh in
154:50 - common that they are using exactly the
154:52 - same learning rate when updating all
154:55 - model parameters independent whether
154:58 - those are weight parameters whether
154:59 - those are uh bias factors or whether
155:03 - they are different weight factors so
155:05 - weight parameters for different hidden
155:07 - layers now the Adaptive learning process
155:10 - and adaptive learning rate uh in neural
155:13 - network is quite different from this
155:16 - constant learning rate so this learning
155:19 - rate which we saw before defined by ITA
155:22 - in deep learning can play a crucial role
155:26 - when it comes to defining the step size
155:29 - so how much we need to update the weight
155:33 - parameters how much we need to update
155:35 - the bias factors and it basically
155:38 - defines the uh the amount of update we
155:42 - need to make so what learning the
155:44 - Adaptive learning process does is that
155:47 - it says we should not use exactly the
155:50 - same learning rate across all these
155:52 - parameters instead we need to look at
155:54 - the training data and we need to look at
155:57 - the feature and based on that understand
156:00 - and adopt the learning rate so the step
156:04 - size that we need to use when updating
156:06 - the model parameters because it can be
156:08 - that we have this different features and
156:11 - one feature can require a different
156:13 - amount of updates such as learning uh
156:15 - learning rate the step size versus the
156:17 - other features that will require
156:19 - different sorts of updates and that's
156:21 - exactly what this adaptive learning
156:23 - optimization process does so it handles
156:27 - diverse data it's able to look into
156:30 - these complex data sets and look into
156:32 - this features and it then tries to
156:35 - update the different model parameters
156:39 - based on the corresponding information
156:41 - that it's getting so from the learning
156:43 - process from Computing the gradients and
156:44 - then from this patterns in the different
156:47 - features so in this way we are adopting
156:51 - the learning rate accordingly and each
156:55 - uh model parameter will have its own
156:58 - learning rate so this will then avoid
157:02 - having a stagnation and will definitely
157:04 - uh reduce the oscilations because it
157:07 - helps you prevent getting stuck in a
157:09 - local minimum and not being able to
157:11 - discover the global opum which is a
157:14 - common issue when we have a F learning
157:16 - rate now when it comes to different
157:18 - examples of adaptive learning rate when
157:20 - we have different learning rate and the
157:22 - learning rate is being adopted based on
157:25 - the information that it's getting from
157:27 - the algorithm and training data there
157:29 - are a few that you can mention and one
157:31 - of the most popular optimization
157:33 - algorithms that is also of adaptive
157:35 - nature is the Adam optimization
157:38 - algorithm So Adam that specifically
157:41 - stands for adaptive moment estimation uh
157:45 - which is then the algorithm that adopts
157:47 - the learning rate accordingly and
157:50 - therefore is also known to have much
157:52 - better performance when it comes to
157:54 - optimization so another example is the
157:57 - RMS prop RMS prop stands for the root
158:00 - mean squar propagation and this
158:03 - algorithm has been introduced as one of
158:05 - the cuser courses about deep learning
158:09 - and from then on many people have been
158:11 - using it and experimenting with it
158:13 - another adaptive optimization Al Alm is
158:16 - the Adaptive gradient algorithm which is
158:19 - the aagr so this also adjust the
158:22 - learning rate based on what we are
158:24 - calling cumulative squared gradients and
158:28 - it's particularly useful when you are
158:29 - dealing with spars data so this is
158:32 - basically at high level the summary of
158:34 - the Adaptive optimization process what
158:36 - it means to adapt the learning rate uh
158:39 - the impact of it on the Deep learning
158:42 - model performance as well as the
158:44 - examples of few adapted optimization
158:46 - algorithm largely used across the
158:49 - industry the next question is what is
158:51 - RMS prop and how does it work so we just
158:55 - saw that RMS prop is one of the examples
158:59 - that that can be defined as an Adaptive
159:02 - optimization process and RMS probe
159:05 - stands for the root mean squared
159:07 - propagation and it is like GD HGD and
159:10 - HGD with momentum an optimization
159:13 - algorithm that tries to minimize the
159:15 - loss function of your deep learning
159:17 - model to find the set of moral
159:20 - parameters that that will minimize the
159:22 - loss function so uh what RMS probe does
159:26 - is that it tries to address some of the
159:29 - shortcomings of the traditional gradient
159:31 - descent algorithm and it is especially
159:34 - useful when we are dealing with
159:36 - Vanishing gradient problem or exploring
159:38 - gradient problem so we saw before that a
159:43 - very big problem during the train
159:45 - meaning of deep neural network is this
159:47 - concept of Vanishing gradient or
159:49 - exploring gradient so when the gradients
159:52 - start to converge towards zero uh so
159:55 - they become very small they almost
159:57 - vanish uh or when the gradients are so
160:00 - big that they are exploding so they are
160:03 - becoming very large and they result in a
160:06 - large amount of oscilations now uh to
160:10 - avoid this what RMS prop is doing is
160:12 - that it is using an Adaptive learning
160:15 - rate it's adjusting the learning rate
160:18 - and it is using for this for this
160:20 - process this idea of running running
160:23 - average of the second order gradients so
160:28 - this is related to this concept of haian
160:31 - and it is also using this DK parameter
160:34 - uh which takes into account and
160:36 - regulates uh what is the magnitude the
160:40 - average of the magnitudes of the recent
160:42 - gradients that we need to use where when
160:45 - we are updating the model parameters so
160:49 - basically what is the amount of
160:50 - information that we need to take into
160:52 - account from the recent adjustments so
160:54 - in this case this means that parameters
160:57 - with large gradients will have their
161:01 - effective learning rate to be reduced so
161:05 - whenever we have uh large gradients for
161:08 - parameter we will be then reducing the
161:11 - gradients this means that we will then
161:13 - control the exploring gradient
161:15 - effect uh and of course the other way
161:18 - around holds true in case of RMS probe
161:20 - for the parameters that will have a
161:22 - small gradients we will be then
161:24 - controlling this and we will be
161:26 - increasing their learning rate to ensure
161:29 - that the gradient will not Vish and in
161:33 - this way we will be then controlling and
161:36 - smoothing the process so the RMS prob
161:39 - uses this DEC rate which you can see
161:42 - here too this beta which is that a
161:45 - number usually around 0.9 and it
161:49 - controls how quickly this running
161:52 - average forgets the all these gradients
161:54 - so as you can see here uh we have this
161:57 - running average VT which is equal to
162:00 - Beta * VT minus 1 + 1 - beta * GT ^ 2 so
162:06 - this is basically our uh second order
162:10 - gradient so then what we are doing is
162:13 - that we are taking this running average
162:16 - and then we are using this to adapt and
162:19 - to adjust our learning rate so you can
162:22 - see here in our second expression the
162:24 - Theta t + 1 so the updated version of
162:27 - the parameter is equal to the Theta T so
162:29 - the current parameter minus so we're
162:32 - subtracting the learning rate divided to
162:35 - square root of this VT square root of
162:38 - this running average and we are adding
162:40 - some Epsilon which is usually a small
162:42 - number just to ensure that we are not
162:44 - dividing this ITA to0 in case our
162:47 - running average is equal to zero so we
162:49 - are ensuring that this number still
162:51 - exists and we do not divide a number to
162:53 - zero and then we are simply multiplying
162:56 - this to our gradient so as you can see
163:00 - depending on our parameter we will then
163:03 - have a different learning rate and we
163:06 - will then be updating this learning so
163:08 - by adapting this learning rate in case
163:10 - of RMS prop we are then stabilizing this
163:13 - optimization process we are then
163:15 - preventing all these random movements
163:18 - these
163:18 - oscilations and at the same time we are
163:21 - ensuring smoother convergence we are
163:23 - also ensuring that our Network
163:25 - especially for deep neural networks it
163:28 - doesn't suffer from this Vanishing
163:30 - radient problem and from the exploding
163:32 - gradient problem which can be a serious
163:35 - problem when we are trying to optimize
163:38 - our deep neural network the next
163:41 - question is what is atam and why is it
163:43 - used most of the time in nural networks
163:47 - So Adam is an Adaptive optimization
163:50 - algorithm that is known for its Dynamic
163:53 - learning adjustments based on the first
163:56 - and the second moments of the gradients
163:59 - So Adam like RMS prop is another
164:02 - adaptive optimization algorithm which
164:05 - adapts the learning rate accordingly in
164:09 - order to reduce the amount of
164:11 - oscilations and to improve the
164:13 - optimization algorithms quality what
164:15 - Adam tries to do is that it tries to
164:18 - bring this benefits of two different
164:21 - algorithms together it tries to combine
164:23 - the idea of momentum from SGD with
164:26 - momentum with the idea of the RMS probe
164:29 - by using the running average of the
164:31 - second order derivative so the second
164:34 - moment and by combining the two it tries
164:37 - then to take the benefits of the Two
164:40 - Worlds so when it comes to the Adam it
164:43 - can be represented by this mathematical
164:46 - representation you don't need to know it
164:48 - but I always think it helps to bring
164:50 - perspective to how it combines the Two
164:53 - Worlds the RMS probe and SGD with
164:55 - momentum and why it is actually so
164:59 - useful so when we look in here we can
165:02 - see that the first one is the momentum
165:04 - term so the Mt is equal to beta 1 * Mt
165:07 - minus 1 plus 1 - beta 1 * GT where the
165:12 - beta 1 is the firste which is simply a
165:16 - constant uh and the beta 2 is also a
165:19 - constant a hyper parameter both of which
165:22 - we can actually tune so in here you can
165:25 - see GT and GT squ which are simply the
165:28 - first moment and the second moment of
165:30 - our gradients where second moment is
165:33 - known to be better estimate less noisy
165:36 - compared to the first moment so what we
165:38 - are trying to do is that we are on one
165:41 - hand using this idea of
165:43 - momentum in order to reduce the amount
165:46 - of ciliation that the optimization
165:48 - algorithm is making and in this way we
165:51 - are improving the optimization process
165:53 - and getting a better quality and on the
165:56 - other hand we are also introducing this
165:59 - idea of velocity which comes from the
166:01 - RMS prob which then will result in
166:05 - better less noisy estimate of the
166:08 - gradients so by doing this what we are
166:10 - trying to do is that we are adjusting
166:12 - the learning rate throughout the
166:14 - training process and we are making it
166:17 - more effective across different
166:19 - scenarios so for scenarios where GD
166:22 - would momentum would have been better
166:24 - Adam will incorporate this and where it
166:27 - would be beneficial to use RMS prop then
166:29 - Adam would again the a optimization
166:31 - algorithms and that's why Adam has been
166:35 - proven across the entire industry to be
166:38 - very effective and people are using it
166:41 - across the industry uh for wide range of
166:43 - problems and applications
166:45 - so as you can see once we have our
166:47 - running average for the first order
166:50 - derivative and the running average for
166:52 - our second moment gradient then we are
166:55 - also adjusting so we are correcting this
166:59 - momentum and the velocity uh by dividing
167:03 - this Mt so we are taking the Mt and we
167:05 - are dividing it to 1 minus beta 1 T
167:08 - because we want to adjust for the bias
167:10 - when it comes to this momentum and then
167:13 - we are also adjust the velocity so VT
167:17 - head is then the adjusted version of the
167:18 - VT because we take the VT and we divide
167:21 - it to 1 minus beta 2 to the^ T by using
167:25 - the beta 2 as our hyper parameter as our
167:27 - second bias vector and then we are using
167:30 - this adjusted running average for the
167:33 - first moment and the adjusted version of
167:35 - the running average of our second order
167:38 - gradient in order to update and adapt
167:42 - our learning rate and as you can see
167:44 - here what we are doing we are taking the
167:46 - learning rate Alpha we are multiplying
167:49 - it by the adapted and adjusted momentum
167:53 - term and then we are dividing it to the
167:55 - square root of this velocity term or VT
167:58 - hat plus some Epsilon which is usually a
168:01 - small number just to ensure that we are
168:02 - not dividing this number to zero and we
168:05 - are using this to subtract from the
168:07 - Theta T so the current parameter to get
168:10 - our new updated parameter Theta t + one
168:14 - so so when it comes to the benefits of
168:17 - using the Adam first of all Adam
168:19 - incorporates this benefits from multiple
168:22 - optimization algorithms to ensure that
168:25 - the quality is in place and the
168:26 - efficiency is in place and the the
168:29 - optimization algorithm is able to handle
168:31 - different
168:32 - scenarios uh the efficiency with spse
168:35 - gradients is another advantage of Adam
168:37 - Optimizer because Adam is particularly
168:40 - efficient when dealing with this sparse
168:43 - gradient and uh the other advantage of
168:46 - Adam is that it's able to balance the
168:49 - speed and the stability So Adam
168:52 - generally converges much faster when it
168:55 - comes to the converging to the global
168:57 - opum compared to algorithm such as HGD
169:01 - and at the same time it also tends to be
169:03 - more stable because it's able to control
169:05 - the it's able to control the learning
169:07 - process compared to the algorithms that
169:10 - only use the first moment uh whereas the
169:13 - the atom is is also using the uh second
169:16 - order gradients so the second moment and
169:20 - the final advantage of Adam is that it
169:22 - is robust so it's known for its
169:24 - robustness in various conditions and
169:26 - various scenarios uh which also means
169:29 - that it is less sensitive to
169:32 - hyperparameter choices and this is
169:35 - especially important when it comes to
169:37 - the initial learning rate and this makes
169:39 - it much more easy and user friendly for
169:43 - large range of applications especially
169:45 - when you are someone who is just
169:47 - learning deep learning then it can be a
169:50 - bit difficult to tune your
169:53 - hyperparameters it can be difficult to
169:55 - identify this initial values that you
169:57 - need to use for your algorithm and Adam
170:00 - is then less sensitive to this choice of
170:03 - learning rate which can be really
170:04 - important what is Adam W and why it is
170:07 - preferred over Adam so here we are of
170:10 - course referring to the atam
170:11 - optimization algorithm that we just
170:13 - discussed and the Adam W is simply the
170:17 - adjusted version of the traditional Adam
170:20 - Optimizer so like Adam Adam W is an
170:24 - Adaptive optimization algorithm that is
170:26 - used to optimize the training of a
170:29 - neural network and to find the minimal
170:33 - point of the loss function by
170:35 - continuously adopting the learning rate
170:37 - and updating the model parameters so the
170:40 - biggest difference in Adam W and Adam is
170:43 - the way they perform this
170:45 - regularization so the traditional Adam
170:49 - has one specific disadvantage which
170:51 - usually comes uh when dealing with deep
170:54 - neural networks so unlike the uh
170:58 - traditional SGD with momentum Adam is
171:01 - unable to properly generalize the moral
171:05 - and most of the time when we are
171:07 - performing regularization with Adam so
171:09 - let's say we are using L2 re
171:12 - regularization uh with Adam in order to
171:16 - penalize uh large weights and in order
171:18 - to address the overfitting problem of
171:21 - the algorithm then Adam has shown in the
171:25 - industry when training different deep
171:26 - neural network to not generalize very
171:29 - well and to solve this problem of
171:31 - overfitting so that's exactly what Adam
171:34 - W tries to do it tries to address the
171:36 - specific problem of Adam such that we
171:39 - can have improved generalization and it
171:42 - can address the shortcomings of the
171:44 - traditional Adam making it one of the
171:47 - best algorithms out there and it's being
171:50 - used more and more in across Varian
171:53 - applications and this is especially true
171:56 - when it comes to fine-tuning already
171:58 - pre-trained model because those type of
172:00 - models are known to have this problem of
172:04 - overfitting and we need to have General
172:07 - model when the problem of overfitting
172:09 - will not be S so absent so present and
172:13 - we will be able to to pre-train the
172:15 - model find H the model this pre uh based
172:18 - on this pre-trained model and then use
172:21 - this on an unseen data and the model
172:23 - will still perform well so this
172:25 - basically solving the problem of
172:27 - overfitting and ensuring that we have
172:29 - improved
172:30 - generalization that's what adamw does
172:33 - and the way it does that is that like
172:35 - you can see here unlike the traditional
172:38 - Adam with L2 regularization for instance
172:42 - what it does is that it adds this DC so
172:45 - this penalization term right in the
172:48 - update process when updating the model
172:51 - parameters so the traditional Adam
172:54 - algorithm when combined with L2
172:56 - regularization where Adam is the
172:58 - optimization algorithm and L2 is the
173:01 - regularization algorithm to solve the
173:03 - overfitting uh what we do is that we add
173:07 - this Lambda multiplied by Theta T minus
173:10 - one where Lambda is this uh penalization
173:13 - parameter we use to understand how much
173:16 - we need to penalize certain parameters
173:19 - and this is to ensure that we that the
173:22 - model doesn't memorize the training data
173:25 - and it ensures that uh the model the
173:27 - model is more
173:29 - generalizable now when we uh do this
173:32 - sometimes this doesn't help us that much
173:36 - to solve the overfeeding problem and the
173:40 - weight Decay so decoupling this weight
173:43 - decay from this gradient updates and
173:46 - instead of using this as part of this
173:49 - gradient update if we incorporate this
173:51 - as part of the directly as part of the
173:54 - model parameter updates then it has
173:57 - proven to have much better impact in
173:59 - terms of generalization of the model so
174:01 - when we add this regularization term
174:04 - Lambda multiply by Theta T minus one not
174:08 - to the gradient but we add it directly
174:11 - when updating the model parameter as
174:13 - part of this uh entire uh expression as
174:17 - you can see multiplied with the learning
174:19 - rate then this has shown to have much
174:22 - better impact in terms of solving the
174:24 - overfitting problem and making the
174:26 - entire training trained model more
174:29 - generalizable which will then ensure
174:32 - that uh we have better performance in
174:34 - specific scenarios especially when we
174:36 - have uh deep naral networks or when we
174:39 - are fire tuning pre-trained model and
174:41 - this also means that the algorithm uh
174:44 - will perform equally well on an unseen
174:47 - data unlike when we are using the
174:49 - traditional Adam which in some scenarios
174:52 - doesn't perform generalization very well
174:54 - and It suffers from overfitting the next
174:57 - question is what is badge normalization
174:59 - and why it is used in neural networks so
175:03 - a b normalization to understand this
175:06 - concept you need to know what is a
175:07 - normal distribution so the standard
175:10 - normal distribution is a probability
175:12 - distribution function that has a mean
175:14 - equal to zero and a variance equal to
175:18 - one and what bch normalization does is
175:21 - that it normalizes the activations per
175:26 - batch hence the name bch normalization
175:29 - so bch normalization is this technique
175:32 - that is introduced to address the issue
175:35 - of uh this internal covariant shift uh
175:38 - in the neural networks and what this
175:41 - refers is that uh there will be a change
175:44 - in the distribution of the network
175:47 - activations due to the changes in the
175:50 - parameters of a naral network so every
175:52 - time when we are using the back prop we
175:54 - are Computing the gradients then we are
175:56 - using the optimization algorithm to
175:58 - update those model parameters this then
176:00 - will introduce this change in the
176:03 - distribution of activations which can
176:06 - then destabilize our naral Network and
176:08 - it's something that we don't want to
176:10 - have uh for better performance of our
176:13 - algorithm and our narrow Network we want
176:16 - our batches to have certain preferred
176:20 - distributional properties so we want it
176:22 - to be uh for instance normally
176:24 - distributed which is known to be one of
176:27 - the most go-to uh distribution functions
176:30 - in statistics in machine learning and by
176:34 - uh normalizing the activation so once we
176:37 - compute our Z course we apply our
176:39 - activation function and we get our
176:41 - activations then we can normalize this
176:44 - activations by Computing the mean and by
176:47 - Computing the variance of these
176:50 - activations across the batch so for all
176:52 - the observations perir batch and then we
176:55 - can use this in order to normalize the
176:59 - activations now you you will most likely
177:01 - be familiar with this concept of
177:03 - normalization if you have used
177:05 - traditional machine learning models but
177:07 - let's quickly look into the mathematical
177:09 - definitions just in case you you also
177:12 - need to provide this as part of your
177:13 - answer
177:15 - so when we look in here you can see that
177:17 - we are Computing the mean of all the
177:20 - activations where XI defines the
177:22 - activations and M is the number of
177:25 - observations per batch we are taking the
177:28 - sum of all these activations and we are
177:30 - dividing it to the number of
177:31 - observations per batch this then gives
177:34 - us our uh batch mean so mu batch and
177:38 - then we can also compute the
177:41 - corresponding variance of the batch by
177:44 - taking this activations XI and
177:47 - subtracting from this the mean of the
177:49 - batch and taking the square of this so
177:51 - the square difference of the
177:54 - activations uh and the mean of the batch
177:57 - and we are taking the sum of them for
177:59 - all the badge observations so M elements
178:01 - and dividing it to the number of all
178:04 - observations per batch in this way we
178:07 - can find an estimate of variance of this
178:10 - batch and we can then use this to
178:13 - normalize
178:14 - the activations of this batch so as you
178:17 - can see here we are taking the XI so
178:20 - activations we are subtracting the mub
178:22 - the mean of the batch and dividing it to
178:25 - the square root of the sigma B squ so
178:27 - the variance of the batch and we are
178:29 - adding some noise so the Epsilon to
178:33 - ensure that we we do not divide this
178:37 - number to zero and then once we have
178:39 - this activations we can then use this
178:42 - normalized activations to obtain our
178:46 - predictions so this picture is a common
178:49 - picture that is used to visualize this
178:51 - idea of bch normalization because here
178:54 - we have all the observations of the bch
178:57 - so we have n different observations
178:59 - according to this picture and then we
179:02 - need to normalize this pair feature so
179:06 - you can see that we are normalizing pair
179:07 - feature all the observations perir batch
179:10 - so we are using all the observations per
179:12 - batch to find the main for that feature
179:15 - and to find a variance of that feature
179:17 - and then we are Computing the normalized
179:20 - activations uh now you might be thinking
179:23 - but why do we even normalize it beside
179:25 - of stabilizing the neural network and
179:28 - how does that even affect the the
179:31 - stability of the neural network so when
179:34 - we perform normalization using this bch
179:37 - Norm uh bch Norm introduces this two
179:40 - trainable parameters per Activation so
179:43 - one one for the scale and one for the
179:46 - shift so the scale is then this SMU and
179:49 - then the shift is then the variance so
179:52 - once we have this the step ensures that
179:55 - the normalization process doesn't
179:57 - diminish the representational capacity
180:01 - of the network so in this way what we
180:04 - are doing is that we are stabilizing the
180:07 - distribution of the activations across
180:11 - different layers so in each layer we
180:14 - don't have entirely different
180:15 - distribution for activations and when we
180:18 - have similar distributions and this is
180:20 - just normal then we will have more
180:23 - stable learning process and this will
180:26 - then allow for Higher Learning rates uh
180:29 - which will then accelerate the training
180:31 - process of a neural network and when we
180:33 - are accelerating the training process we
180:35 - are making a lesser amount of this
180:37 - oscilations which we saw was a big
180:39 - problem as part of the traditional sgds
180:42 - and the Lesser we are making making
180:44 - oscilations the higher is the chance
180:46 - that we will quickly and smoothly go
180:50 - towards the global Optimum and we will
180:52 - find the global Optimum of our loss
180:55 - function the next benefit of using bch
180:57 - normalization is that it reduces the
181:00 - sensitivity of the algorithm to the uh
181:03 - initialization so when we are training
181:05 - our Nal Network we need to have uh
181:08 - initialization for our weights and for
181:11 - our bias sectors and this weight
181:14 - initialization can influence
181:16 - significantly the final outcome of the
181:19 - naral network and it might even entirely
181:22 - change and put the algorithm towards
181:25 - wrong direction so towards local minimum
181:28 - when we are trying to optimize the
181:30 - algorithm and when we perform bch
181:33 - normalization then the algorithm will be
181:36 - less sensitive to this initial values of
181:38 - the weights which means also that we
181:41 - will have higher chance to find a global
181:43 - op op instead of the local opum then the
181:46 - final indirect impact of this bch
181:49 - normalization is the regularization
181:51 - impact so bch normalization is usually
181:54 - done for stabilizing the neural network
181:56 - but the indirect impact of the
181:59 - circularization indirect impact of this
182:01 - normalization is that it has a
182:03 - regularization impact so what then this
182:07 - algorithm does is that it slightly
182:10 - regularizes the neural network and it
182:14 - reduces the need for using other
182:16 - regularization algorithms that would uh
182:18 - try to solve the overfitting problem and
182:21 - the way moral does this is that by
182:24 - normalizing the activations it ensures
182:27 - that some of the neurons doesn't get too
182:29 - high
182:31 - activations uh and this will then be
182:33 - some noise points which will end up
182:35 - impacting the outcome of the entire
182:38 - neural network because if the uh
182:41 - training happens on some noise data
182:44 - points as part of a training data and
182:47 - the the morel focuses on this noise
182:49 - points and learns this memorizes the
182:52 - data points which are not supposed to be
182:54 - memorized then the updates that the
182:57 - model will make will be based on this to
183:01 - high but noisy points when we are
183:04 - performing normalization and we are
183:06 - reducing the impact of this noise points
183:09 - it means also that we are also reducing
183:11 - the risk of the moral following this
183:14 - noise too heavily and impacting the
183:17 - entire moral update which then on its
183:20 - turn reduces the risk of overfitting so
183:22 - it has a regularization impact for naral
183:26 - Network the next question is what is
183:28 - layer normalization and why it is used
183:30 - in naral networks so a layer
183:33 - normalization uh like veg normalization
183:36 - tries to stabilize the network and
183:38 - reduce the internal covariate shift of
183:40 - the activations to ensure that the
183:43 - neural network is able to properly learn
183:47 - the in a stable way dependencies in the
183:50 - data so we just saw that batch
183:52 - normalization uses the entire training
183:56 - data of one batch when Computing the
184:00 - mean and the variance and Performing
184:03 - normalization per feature now unlike the
184:06 - bch normalization the way layer
184:08 - normalization does this normal
184:10 - normalization process is that it
184:13 - computes the mean across all features
184:16 - not one but all features for a single
184:19 - training observation and does it the
184:22 - same for the variant so it computes the
184:24 - variant across all features but single
184:26 - training observation and then uses this
184:29 - to normalize these
184:31 - activations so the reason for this
184:33 - change is that in some cases in certain
184:36 - neural network architectures it is pH
184:39 - the feasibility is not there so we
184:41 - cannot even calculate it just doesn't
184:44 - make sense to perform normalization
184:47 - across all training observations because
184:49 - of the structure of the our architecture
184:52 - and also the uh input data nature for
184:55 - instance when we have CNN based on this
184:58 - architecture we can then use the bch
185:00 - normalization easily but when we have
185:02 - this architecture such as RNN or lstms
185:06 - or grus then using bch normalization is
185:09 - not effective and layer normalization
185:12 - has proven to be much more effective in
185:15 - those cases so therefore for this
185:18 - different sorts of architectures we can
185:20 - then use layer normalization whenever it
185:23 - is not possible to use B normalization
185:26 - layer normalization is largely used also
185:29 - in the state-of-the-art Transformers
185:31 - which are a part of the trendy large
185:34 - language model such as gpts which are
185:36 - encoder only but also the decoder only
185:40 - architectures uh such as bird
185:43 - and if you're wondering why layer Norm
185:46 - is particularly used in architectures
185:48 - like RNN or lstms or grus or
185:51 - Transformers it is because in those kind
185:53 - of architectures the bed size is either
185:57 - small or it can vary and whenever we
186:01 - have this varying or small bch sizes
186:04 - then bch Norm usually is not effective
186:07 - and unlike V Norm layer Norm in those
186:09 - cases is particularly effective so like
186:13 - in RNN or in grus or lstms where
186:17 - regarding of the P size which can be
186:20 - different uh the layer Norm will still
186:22 - perform effectively when it comes to
186:25 - stabilizing the network the next
186:27 - question is what are residual
186:28 - connections and their function in naral
186:32 - networks so residual connections have a
186:35 - very important function in naral
186:37 - networks and specifically they try to
186:39 - optimize the performance of naral
186:41 - networks by combating the infamous
186:45 - Vanishing gradient problem the residual
186:47 - connections they play a key role in deep
186:50 - learning architectures like rest net
186:53 - Transformers uh the um the GPT series
186:57 - large language models as model so if you
187:01 - look at the paper called attention is
187:04 - all you need then you'll most likely see
187:07 - as part of the architecture in each of
187:10 - these layers beside of having the actual
187:12 - layer you will also see this additional
187:15 - layer which says at and then Norm so
187:18 - this is exactly where the layer
187:20 - normalization comes and the residual
187:23 - connections are added so what the
187:26 - residual connection does is that in this
187:28 - unlike the typical naral network uh
187:31 - where the input is simply transformed
187:33 - through the weights and the bias vectors
187:35 - into the corresponding activations to
187:37 - produce the output in case of residual
187:41 - connections we are adding the the
187:43 - original input directly on the output of
187:47 - this layer and this is often referred to
187:49 - us as a short cat or a skip connection
187:53 - and the reason why it is called a short
187:56 - cat or a skip connection is because the
187:59 - gradients are simply skipping through
188:02 - this transformation of this
188:03 - computational graph of being transformed
188:06 - towards the multiplication with the
188:08 - weights adding bias factors and then uh
188:10 - using the activation function as
188:12 - transform
188:13 - and instead of that it has also a direct
188:16 - shortcut towards contributing to the
188:19 - final outcome so as you can see here
188:22 - with residual connections we have the
188:23 - output equal to the FX which is a
188:26 - transformation so it's the output of
188:28 - that layer plus the actual X which is
188:31 - the input so going back to this example
188:35 - of the Transformer model in the original
188:37 - paper attention is All You Need You
188:40 - Might Recall from the Transformer
188:42 - architecture this part which is the left
188:45 - part so the encoder part of the
188:47 - Transformer model it's also used as part
188:49 - of the GPT Series so the general
188:52 - pre-trained model uh where we are so the
188:55 - Transformer model where we are using the
188:58 - input uh so the input embeddings on the
189:01 - top of that positional encodings uh
189:03 - based on the sinusoids and cinos oids
189:05 - and then on that after that we are
189:08 - transforming this into cues keys and
189:11 - values if you are familiar with
189:13 - Transformer model and then this goes us
189:16 - an input to the multi-ad attention layer
189:20 - so if you're are not familiar with this
189:23 - architecture of Transformers just forget
189:25 - about that don't mind the details just
189:27 - think of like this multi-ad attention
189:30 - layer as a layer in your neural network
189:32 - that tries to learn the uh inter
189:35 - dependences between elements of your
189:38 - sequence so what we are doing here is
189:40 - that beside of providing this input
189:43 - those positional encodings added on the
189:46 - top of the input encodings beside of
189:49 - giving that as an input to the multi had
189:51 - ATT tension layer which then gives us an
189:53 - output the transformed version uh we are
189:57 - also giving this value this input you
190:00 - can see with this Arrow going to the on
190:03 - the top of that output so we are getting
190:05 - the output from the multi tension layer
190:07 - and on the top of that we are performing
190:09 - layer normalization and we are adding
190:12 - the input on this output and this is
190:15 - exactly what we are doing as part of
190:17 - residual connections we are adding the
190:19 - input on the top of that
190:21 - output now uh the reason why it is
190:24 - called shortcut it is because when
190:27 - Computing the gradients uh the gradient
190:31 - will then go through this
190:33 - Transformations we need to compute the
190:35 - partial derivative of the loss function
190:37 - with respect to the weights and for that
190:39 - we first need to calculate the partial
190:41 - derivative of the loss function with
190:42 - respect to the Z scores and then we need
190:44 - to compute the partial derivative of the
190:46 - Z of the Z scores with respect to the
190:48 - weights and then use the two and use the
190:50 - chain rule to compute the partial
190:52 - derivative of the loss function with
190:54 - respect to weights now this entire
190:57 - transformation of this gradients will
190:59 - result in many cases the gradient coming
191:02 - very close to zero and Vanishing so this
191:06 - is the INF famous Vanishing gradient
191:08 - problem and to avoid this what we are
191:11 - doing with residual connection is that
191:13 - we have also this shortcut so you can
191:16 - see that we are not only performing this
191:19 - different transformation so we have X
191:21 - then we have the weight layer then we
191:23 - have the rectifier linear unit
191:25 - activation then we have weight layer and
191:27 - then we are getting the output which is
191:29 - the FX and on the top of that we are
191:32 - also adding this x so the initial input
191:35 - on the top of this
191:36 - output this is what we are calling
191:39 - residual connections and the reason why
191:42 - you can see that we have this idea of
191:44 - shortcut so the gradient is able to flow
191:48 - into network not only through this
191:50 - Transformations but also directly
191:52 - towards the gradient is because of this
191:55 - mathematical derivation here so if we
191:58 - look at the output Y is equal to x + FX
192:01 - where X is the input that we are adding
192:03 - on the top of the output FX you can see
192:06 - that if we take the partial derivative
192:09 - of this function uh X+ FX with respect
192:13 - to the X then what we need to do is we
192:16 - need to apply the chain so we need to
192:19 - take the partial derivative of the E
192:21 - with respect to the Y then multiply it
192:24 - with the partial derivative of y with
192:26 - respect to the X which is equal to the
192:29 - partial derivative of e with respect to
192:31 - Y so exactly the same as we have here
192:34 - but then multiplied with 1 + FX or F
192:38 - first order derivative X I have to say
192:41 - uh and the reason why it is is this
192:43 - value because as you can see here we
192:45 - need to take the partial derivative of y
192:47 - with respect to X and as y isal to x +
192:50 - FX when we take the partial derivative
192:53 - of X Plus FX with respect to x uh using
192:56 - the um uh the rule for summation the
193:00 - differentiation of sums then this is
193:02 - simply equal to the derivative of the X
193:05 - with respect to X which is one plus the
193:08 - derivative of FX with respect to X which
193:11 - is f and then this not X which basically
193:14 - is the first order derivative of FX with
193:17 - respect to X so as you can see here when
193:20 - we open this parenthesis this is the
193:22 - same as taking the partial derivative of
193:25 - a with respect to Y and then adding to
193:29 - this the partial derivative of e with
193:31 - respect to y * f x and this is simply
193:36 - opening parenthesis so one multiply with
193:38 - this value which gives us the first
193:40 - order derivative of e with respect to
193:42 - one y plus and then we have this second
193:44 - part of the within the parenthesis which
193:46 - is the FX multiplied with the uh partial
193:50 - derivative of a with respect to Y which
193:52 - is simply the partial derivative of e
193:55 - with respect to Y multiplied by f x so
193:58 - what we can see here and the reason why
194:00 - I am explaining this mathematical
194:02 - derivation is that you can see for
194:04 - yourself that the gradient in this case
194:08 - uh with respect to X will be able to not
194:11 - only flow through this part which is the
194:16 - multiplied by
194:17 - FX which has gone through all these
194:20 - transformations of the weight and in
194:21 - activation function but also directly
194:24 - because we have here this term partial
194:27 - derivative of e with respect to Y and
194:30 - why this is helpful and how this is
194:32 - reducing thetion gradient problem is
194:35 - because when we have this multiple
194:37 - layers by the time we come from this
194:39 - very deep layers to the very earlier
194:41 - layers our gradient goes through all
194:44 - these transformations of the weights
194:46 - activation functions and then it ends up
194:49 - becoming very close to zero so it's
194:50 - Vanishing and if we have this
194:52 - opportunity for the gradient to go
194:55 - through the shortcut and flip through
194:57 - and Skip some of the layers we will have
195:00 - higher chance of this gradient to not
195:02 - vanish and to actually reach from deeper
195:05 - layers to the initial layers and that's
195:07 - exactly what we want to do and this
195:09 - solves the problem of Vanishing gradient
195:11 - what is gradient clipping and their
195:13 - impact on neural network so we just saw
195:17 - the residual connections and we saw that
195:20 - residual connections especially for deep
195:22 - neural network and specifically for the
195:24 - cases like RNN lstms when we have the
195:27 - sequential nature with too many layers
195:30 - uh we can have this Vanishing gradient
195:32 - problem and residual connections can
195:33 - help to solve this problem now gradient
195:36 - clipping is solving the opposite problem
195:39 - so it's solving the exploring gradient
195:42 - problem
195:43 - whenever we have certain deep neural
195:45 - networks like RNN lstms because we are
195:49 - each time using these Transformations
195:52 - and we are using a cumulative nature and
195:54 - multiplying all these weights towards
195:56 - each other through this process and we
195:59 - are using the previous hidden state to
196:02 - update the the new hidden state so next
196:05 - hidden State and next one it also means
196:07 - that when we are going through these
196:09 - layers during the uh optimization
196:12 - process after backrop is done we need to
196:15 - perform all these Transformations and by
196:17 - the time we come through the earlier
196:19 - layers it can be that we have exploding
196:22 - gradients so the gradients become so
196:25 - large that it starts to impact the
196:28 - entire performance of the model we are
196:30 - starting to make two big over jumps and
196:33 - the updates that we are starting to get
196:36 - and optimization process is starting to
196:38 - make too much of this oscilations
196:40 - towards different directions so we are
196:42 - seeing this ertic Behavior as part of
196:45 - our optimization process and that's
196:48 - definitely not something that we want to
196:50 - have because exploring gradients means
196:52 - that we have unstable naral network and
196:56 - uh this also means that the update of
196:58 - the weights will be too large and this
197:02 - will then result in not well performing
197:04 - and not properly train neural network
197:08 - now to avoid all these problems the way
197:10 - we can solve this explode gradient uh
197:13 - problem is by using the uh gradient
197:16 - clipping what gradient clipping does is
197:19 - that it basically Clips the gradients at
197:22 - a certain level so there is a threshold
197:25 - which is the parameter that we are using
197:27 - for gradient clipping and whenever the
197:29 - gradient becomes too large so higher
197:32 - than the threshold then we will be
197:34 - clipping that gradient to ensure that
197:36 - this gradient is not too large and we
197:38 - are not updating our weight parameters
197:41 - or bias sectors too much so in this way
197:45 - what we are doing is that we are kind of
197:47 - stabilizing our neural network which is
197:50 - especially important for architectures
197:52 - like lstms RNN grus uh which have this
197:56 - sequential nature of the data with too
197:58 - many layers and uh in this way we will
198:01 - ensure that we do not have this ertic
198:04 - jumps in our naral Network optimization
198:06 - process and we will end up with a stable
198:10 - Network that is able to properly learned
198:14 - interdependencies so the key ID behind
198:16 - cavier initialization is to keep the
198:19 - variance of the activations and the
198:22 - gradients consistent across layers the
198:25 - way it is done it is by setting the
198:28 - initial weights based on the number of
198:32 - input and the output neurons in a neural
198:36 - network so the way it works is that it
198:39 - looks into the n in which is is simply
198:42 - the number of uh neurons uh as the
198:46 - inputs that go into that layer and the N
198:49 - output which is the number of neurons or
198:51 - the output as part of that layer because
198:54 - in each layer we have neurons that comes
198:56 - in and the neurons that come out and
199:00 - This n in that comes in and N Out that
199:03 - describes the number of neurons that go
199:04 - out is being used in order to define the
199:09 - distribution of this weights so Ty so
199:12 - one way that the cier initialization
199:15 - tries to stabilize the network and keep
199:19 - the gradients as well as the activations
199:22 - consistent and stable is by using uh
199:25 - certain statistical distributions uh
199:28 - with certain qualities that will ensure
199:31 - that the variance of the weights is
199:33 - constant so we want to come up with a
199:36 - distribution from which we will sample
199:38 - our initial weights using the n in which
199:42 - is the number of neurons that go into
199:44 - that layer and N Out which is the number
199:46 - of neurons that go out of that layer
199:49 - using that combined with the statistical
199:52 - distribution so one way of doing that is
199:54 - by using the uniform distribution if
199:57 - you're familiar with the statistical
199:59 - distribution function uh the the way
200:02 - uniform distribution works is that it
200:04 - always has the two parameters it has a
200:06 - lower bound a and the upper bound B and
200:09 - the distribution always lies within this
200:11 - specific area it's like a rectangular
200:15 - area uh and the corresponding values so
200:18 - the upper bound is the probability is 1
200:21 - / B minus a so the probability for all
200:23 - the values that lie in within that range
200:26 - is constant it's equal to 1 / B minus a
200:30 - now why this is important in our case it
200:33 - is because if we use certain A and B in
200:37 - this case you can see when using square
200:39 - root of 6 / to square root of n plus n
200:42 - out as a lower bound and is positive
200:45 - version so square root of 6 / to a sare
200:48 - root of n in plus N Out then the
200:51 - variance corresponding to this random
200:53 - variable that follows uniform
200:55 - distribution with this lower bound a and
200:56 - lower B the upper bound B will be then
200:59 - equal to 2 / 2 square root of n in plus
201:03 - N Out so the variance of this random
201:05 - distribution will be this amount and
201:09 - this means that every time we have this
201:11 - weight weight this weight will then be
201:14 - constant so the variation of it will be
201:16 - constant and what does it mean if the
201:18 - weight has a constant variance it means
201:21 - that it will always change in a similar
201:25 - amount of with similar amount so it will
201:28 - not change too much with too high
201:30 - gradients so exploring gradients it will
201:33 - also not change too low so the gradients
201:36 - will not be too small so we will also
201:39 - combat The Vanishing gradient problem
201:41 - and and that's why cavier initialization
201:43 - can be so important and so significant
201:47 - because it can help to stabiliz the
201:51 - entire network it can bring consistency
201:53 - into the variance of these activations
201:56 - and the gradients across different
201:58 - layers and it will also reduce this risk
202:01 - of Vanishing and exploding gradients
202:03 - which will promote stability and it will
202:06 - also improve the efficiency of your
202:09 - training process and this
202:12 - process it is particularly relevant for
202:15 - the sigmoid activation function and the
202:17 - tank activation
202:19 - function let's now look into the next
202:21 - question so what are the different ways
202:23 - to solve Vanishing gradient problem so
202:26 - by now we have discussed many topics and
202:29 - many approaches where I have briefly
202:31 - mentioned that this helps you solve the
202:34 - venish gradient problem for example so
202:37 - here I will just summarize at higher
202:38 - level very quickly what are the
202:40 - approaches that you can mention that
202:42 - solve the vanishing gradient problem so
202:45 - the Vishing gradient problem firstly can
202:48 - be solved by using appropriate
202:50 - activation functions so by using
202:52 - activation functions such as rectifier
202:55 - linear unit function or leaky rectifier
202:57 - linear unit function that do not suffer
202:59 - from
203:00 - saturation uh this activation functions
203:03 - can help to solve the vening gradient
203:05 - problem and ensure that your gradients
203:08 - will not vanish in your in those deep
203:10 - networks by the time you come from this
203:12 - very deep layers to the early
203:15 - layers uh if you use the so don't use
203:18 - the sigmoid don't use the tank but use
203:20 - the Leaky reu or reu in order to combat
203:24 - the vention gradient problem another way
203:27 - that you can solve this problem is what
203:29 - we just discussed in the previous
203:30 - question which is the cavier
203:32 - initialization so choosing an
203:34 - appropriate weight initialization
203:36 - technique can help you to combat The
203:39 - Vanishing gradient
203:40 - problem
203:42 - another way to solve this problem is by
203:44 - performing bch normalization so bch
203:47 - normalization indirectly will introduce
203:50 - stability into the network and by
203:53 - normalizing the activations it will also
203:56 - result in your gradients which will be
203:59 - more consistent and if your gradients
204:01 - are more consistent it also means that
204:03 - you have lesser chance of having a
204:06 - problem of Vanishing
204:09 - gradients then another way that you can
204:11 - solve the gradient problem especially
204:15 - when it comes to sequence-based
204:17 - architectures like RNN lsms
204:20 - grus uh in all those cases you can use
204:23 - residual connections so residual
204:26 - connections will help you to open the
204:28 - door for the shortcut for your gradients
204:30 - to flip through your network without per
204:34 - without going through all this
204:35 - Transformations and this will then on
204:37 - its turn help you reduce the risk of
204:40 - Vanishing gradients
204:44 - then the final way of combating the
204:47 - vening gradient problem is by using an
204:50 - appropriate architecture so whenever you
204:53 - have you have a choice of choosing
204:55 - certain architectures then try to make
204:58 - use of the ones that are more
205:01 - appropriate for your data more
205:03 - appropriate for your problem and this
205:05 - will avoid at least at some extent the
205:08 - chance of having Vanishing gradient
205:10 - problem if you are for instance making
205:12 - use of the Transformer architecture then
205:15 - in here we are automatically also adding
205:18 - this residual connections and layer
205:19 - normalizations after each layer and this
205:22 - helps you combat The Vanishing gradients
205:25 - problem at some extent and if we are for
205:28 - instance using the Adam V this also
205:30 - helps to regularize the network and in
205:33 - some way also solve this problem the
205:35 - next question is what are the ways to
205:37 - solve exploding gradients so one way of
205:40 - solving EXP floating gradient like we
205:42 - just spoke about is the gradient
205:44 - clipping so gradient clipping can help
205:47 - to keep this threshold and ensure that
205:50 - every time the gradients are checked
205:53 - with respect to this threshold and every
205:55 - time when we have gradients that are
205:56 - about to explode so they go beyond the
205:59 - threshold it will then clip the
206:01 - gradients and it will ensure that the
206:03 - gradients will not go over this level
206:06 - and this will then solve the problem of
206:08 - exploding
206:09 - gradients another way you can solve this
206:12 - problem is by using the weight
206:14 - initialization in a proper way so as we
206:17 - saw before cavier initialization will
206:20 - help us to not only solve the problem of
206:22 - Vanishing gradients but also the problem
206:24 - of exploring gradients because by
206:27 - keeping the variance of the weight
206:29 - constant we ensure that the gradi
206:31 - gradients will not go not explode so not
206:35 - grow too much also not grow too little
206:37 - so too vanish so in this way weight
206:40 - regularization and weight initialization
206:43 - will also help to combat the exploding
206:46 - gradient problem the next question is
206:50 - what happens if the neural network is
206:52 - suffering from overfitting uh relate to
206:55 - weights in neural network so there are
206:57 - two portions of this in this question
207:00 - and the first part refers to the
207:03 - overfitting problem so you need to show
207:05 - that you understand how overfitting
207:07 - relates to neural network usually this
207:09 - is similar to the overfitting of machine
207:11 - learning models uh and the second part
207:14 - is to relate the overfitting to the ways
207:17 - in narrow Network so let's start with
207:20 - the first part so what is overfitting
207:23 - overfitting happens when the model that
207:26 - we are using in this case a deep uh
207:29 - learning model uh it starts to memorize
207:31 - the training data and when training the
207:34 - model it tries to ensure that the model
207:37 - performs well specifically on the
207:39 - training data even though training data
207:41 - might might contain some noise points
207:43 - not General points some outliers and
207:46 - when we train our model on this type of
207:48 - data and model starts to memorize this
207:52 - and we use this fitted data a fitted
207:54 - model to train on and unseen data then
207:57 - what we will see is that this
207:59 - overfitting model is not performing well
208:01 - on an unseen data whereas our entire
208:04 - goal is to train the model on a training
208:08 - data in such a way that this fin version
208:11 - the fitted version of the model will
208:14 - properly perform on an unseen data
208:17 - that's our entire goal otherwise we
208:19 - would not be using predictions in the
208:21 - first place we want to have a proper
208:23 - prediction with high accuracy for a
208:26 - future data or unseen new data and
208:30 - that's something that overfitting causes
208:32 - a problem with because it makes the
208:34 - model less generalizable it will not
208:37 - generalize well on unseen data now now
208:41 - how it is related to the weights of the
208:43 - model so large weights in a neural
208:47 - network will make the model very
208:49 - sensitive to its input data which might
208:52 - also include outliers or noise data and
208:56 - when the model follows training data too
208:58 - closely which also includes these
209:00 - unusual points then this model will not
209:03 - generalize well to the new data and
209:05 - therefore that's something that we don't
209:07 - want to have we want to solve the
209:09 - problem of overfitting to ensure sure
209:11 - that our model is generalizable and will
209:14 - also perform equally well or at least at
209:17 - some extent very similar to the
209:19 - performance of the training data when we
209:22 - introduce the model to the new data the
209:25 - next question is what is drop out and
209:27 - how does it work so Dropout is a
209:30 - regularization technique commonly used
209:33 - in deep learning specifically in order
209:35 - to solve the problem of
209:37 - overfitting so we just saw in the
209:40 - previous interview question question
209:41 - that overfitting can cause a lot of
209:43 - problems uh when training neural
209:46 - networks and we want to have ideally a
209:49 - model that generalizes well on an unseen
209:51 - data and one way to make our model more
209:54 - generalizable and improve the
209:56 - performance of our model is by using a
209:59 - regularization technique and one very
210:02 - popular regularization technique in
210:03 - naral networks is Dropout so what
210:07 - Dropout regularization does is that it
210:09 - randomly drop out or deactivates a
210:14 - subset or of neurons in your neural
210:16 - network during each training
210:20 - iteration and the way it does is that it
210:22 - takes this probability or you can also
210:26 - say proportion and this p is simply a
210:30 - value between 0 and one and this refers
210:33 - to the amount of or proportions of
210:37 - neurons that needs to be randomly
210:40 - deactivated so p is the dropout rate and
210:44 - we are saying that the rgl which is
210:47 - simply the activations in the layer l
210:52 - that this random variable follows barol
210:54 - distribution with one minus P
210:57 - probability uh where 1 minus p is simply
211:00 - equal to the amount of OBS amount of
211:03 - neons that should not be deactivated so
211:06 - amount of errors that should be
211:07 - activated and if you have p as the
211:09 - deactivation r then 1 minus P will be
211:12 - the activation rate and why baroli
211:16 - because if you are familiar with some
211:18 - very popular statistical probability
211:20 - distributions you will know that barol
211:23 - distribution helps us to uh Express and
211:27 - Define random variables that have two
211:30 - possible outcomes there can be either
211:32 - success or failure so there can be
211:35 - either dropping out or not dropping out
211:37 - we can either activate or not activate
211:40 - so deactivate neurons and that's why we
211:44 - are saying that our activations in that
211:48 - specific layer let's say layer l can be
211:50 - described by a random variable that
211:52 - follows bare noly distribution with
211:55 - activation rate of one minus P where p
211:58 - is the dropout rate so think of it like
212:01 - if we want to randomly deactivate or
212:04 - drop during the training iteration 20%
212:08 - of our activation of our neurons then
212:12 - this P will be equal to 0.2 which means
212:15 - that our activation rate will be equal
212:17 - to 0.8 so our layer will follow a baroli
212:22 - distribution with 0.8 as 0.8 being equal
212:26 - the success
212:28 - probability so then what we will doing
212:31 - is that during only the training process
212:34 - in each tion we will randomly deactivate
212:37 - and drop out some of the neurons and in
212:40 - this this way we will ensure that every
212:43 - time our model sees only 80% of the
212:48 - activations and how does this help to
212:51 - solve the overfitting is because we
212:55 - reduce the uh chance of our model seeing
212:59 - the noise points and memorizing the
213:02 - training data because if our model sees
213:05 - every time 80% of the neurons and every
213:08 - time it's changing from iteration to
213:10 - iteration
213:11 - this 80% cuz the process is random then
213:15 - we have smaller chance for our Network
213:18 - to every time see the same noise and to
213:22 - memorize training data cuz we are kind
213:24 - of decorrelating the training
213:27 - process and what this then will help to
213:30 - do is that it will make our neural
213:34 - network more as table and less dependent
213:37 - on certain neurons and it also means
213:39 - that it will generally ize better it
213:42 - will reduce the chance of
213:44 - overfitting there is however a
213:47 - catch um which comes that uh we are
213:50 - performing the Dropout only at the
213:54 - training data and later on in the
213:56 - upcoming questions we will see how we
213:58 - can answer those type of questions what
214:00 - exactly we need to do given that we are
214:02 - performing Dropout only on the training
214:05 - data the next question is how does
214:07 - Dropout prevent overfitting in naral
214:10 - Networks so we already answered majority
214:13 - of this question so I will just at high
214:15 - level summarize what I just said as part
214:17 - of the question 41 so the way Dropout
214:21 - prevents the overfitting in the network
214:23 - is that it randomly deactivates p% of
214:27 - the neurons during the training process
214:29 - which then reduces the dependency of the
214:31 - neural network on certain data points
214:34 - and reduces the chance of the network to
214:37 - memorize the training data in this way
214:39 - the network uh encourages the feature
214:43 - redundancy and also helps to generalize
214:46 - the model better the next question is is
214:49 - drop out like random Forest so in this
214:52 - question your interviewer wants to test
214:55 - to very tricky way whether do you
214:58 - understand the differences between
214:59 - Dropout and random Forest as well as
215:01 - their
215:02 - similarities now let's first start with
215:05 - the common goal that the two algorithms
215:09 - have so the both drop out and the random
215:12 - Forest they aim to improve the
215:14 - robustness of your model and they reduce
215:16 - the chance and the risk of overfitting
215:19 - by introducing Randomness and diversity
215:23 - in the training
215:25 - process you might recall the random
215:28 - Forest algorithm which is an emble
215:31 - machine learning algorithm can be used
215:33 - for both classification and for
215:35 - regression and the way algorithm works
215:37 - is that it tries to use the boot
215:40 - bootstrapped B different bootstrap
215:42 - samples which are copies of the original
215:45 - training data only with
215:47 - replacement and uses this B different uh
215:51 - training copies in order to build B
215:55 - different trees but it does this by
215:58 - decorrelating the trees so making them
216:01 - uncorrelated unlike the backing uh by
216:05 - using randomly selected M features when
216:10 - performing the splitting of the trees so
216:13 - when the trees in random Forest are
216:16 - built because in this recursive uh
216:20 - building process in each step the
216:22 - algorithm randomly selects M out of all
216:26 - P features when splitting the tree so
216:29 - when making this decision uh of where we
216:31 - need to split the tree and to what
216:34 - region the observation needs to be
216:35 - assigned this introduces Randomness to
216:39 - the algorithm and this Randomness helps
216:42 - to decorrelated trees and then the
216:44 - algorithm ends up having B uncorrelated
216:48 - different decision trees and then it
216:51 - combines them all therefore it's called
216:53 - also emble algorithm and then takes the
216:56 - average across all these different B
216:59 - trees in order to get the final
217:02 - prediction so in case of classification
217:04 - that's the majority class in case of
217:06 - regression that's the average of all the
217:08 - outputs now this helps the random Forest
217:12 - to combat the overfitting unlike the
217:14 - backing and as the um average of
217:18 - uncorrelated variables has a lower
217:21 - variance than the average of correlated
217:23 - variables in this way we help it helps
217:26 - to reduce the variance and this then on
217:28 - its turn helps to reduce the risk of
217:31 - overfitting and makes the model more
217:34 - generalizable so Dropout also tries to
217:39 - kind of decorrelate process only the way
217:42 - it does it is by uh deactivating certain
217:45 - neurons and in each iteration use a
217:49 - different uh portion of training data in
217:52 - order to train the model and on his turn
217:54 - this helps to avoid overfitting and
217:57 - avoid the network memorizing the
218:00 - training data so in both ways and in
218:03 - both approaches the modals are
218:05 - introducing this random component
218:07 - Dropout by randomly dropping so certain
218:10 - neurons random Forest by randomly
218:13 - selecting M different features when
218:15 - performing the recursive splits and
218:17 - building decision trees in order to make
218:21 - the modals more generalizable and reduce
218:23 - the risk of
218:24 - overfitting now the key differences so
218:27 - how Dropout and random Forest are
218:29 - different well Dropout is a technique
218:32 - with a single model we have just single
218:35 - narrow Network we are using exactly the
218:38 - same training data and we are randomly
218:41 - dropping neurons in order to in each
218:44 - iteration have a different uh portion of
218:47 - training neurons in order to make the
218:50 - morel more
218:51 - generalizable unlike the random Forest
218:54 - which is an emble technique so it uses B
218:59 - different copies from the original data
219:02 - that samples it with replacement and
219:04 - then uses this B different data sets in
219:08 - order to build B different trees
219:10 - and then combined this B different
219:14 - models to create single model
219:18 - so this means that the two algorithms
219:21 - work entirely different also one is more
219:24 - a regularization technique the other one
219:26 - is an actual algorithm that can be used
219:28 - on its own the next question is what is
219:31 - the impact of Dropout on a training
219:33 - versus testing when we were discussing
219:36 - the Dropout I briefly mentioned that
219:39 - dropout is only applied on a training
219:42 - data which means that we are only
219:44 - randomly deactivating the
219:47 - activations uh as part of our training
219:49 - process but this also means that the
219:53 - neurons have smaller and specifically
219:57 - one minus P probability of being
220:00 - activated during the training process so
220:03 - we are reducing the probability of a
220:06 - neuron to be selected for being
220:09 - activated
220:11 - and this will then introduce
220:13 - inconsistency when it comes to the
220:15 - testing process because we are applying
220:17 - the drop out only during the training
220:20 - this also means that we need to address
220:23 - this reduce in the activation
220:26 - probability when we are performing the
220:28 - testing so given that p% of the neurons
220:33 - during the training are dropped or
220:35 - deactivated we need to compensate for
220:38 - this and we need to ensure that we have
220:41 - consistency in the input size and for
220:44 - that what we need to do is that we need
220:46 - to scale the activations in the testing
220:49 - by 1 minus p and in this way we will
220:53 - then compensate for the small
220:56 - probability difference that we were
220:58 - making when we were dropping out certain
221:02 - activations so just as an example if the
221:05 - dropout rate is 20% so
221:07 - 0.2 this means that during the training
221:10 - process the neurons have 80% probability
221:13 - of being activated and 20% chance of not
221:17 - being activated it means that during the
221:20 - testing we need to scale the activations
221:23 - by using 0.8 so 1 minus P the next
221:27 - question is what are L2 or L1
221:29 - regularizations and how do they prevent
221:31 - overfitting in their own network so both
221:35 - L1 and L2 regularizations are shrinkage
221:38 - or regularization techniques that are
221:40 - both used both in traditional machine
221:42 - learning and in deep learning in order
221:45 - to prevent the moral overfitting so
221:48 - trying to make the model more
221:49 - generalizable like the
221:51 - Dropout so you might know from the
221:54 - traditional machine learning models what
221:56 - does L1 and L2 regularization do L2
221:59 - regularization is also referred as Rich
222:02 - regression and L1 regularization is also
222:04 - referred as l or regression so what L1
222:07 - regularization does is that it adds as a
222:11 - normalization or as a regularization or
222:14 - as a penalization factor that is based
222:17 - on this penalization parameter Lambda
222:20 - multiplied by this term which is based
222:24 - on the absolute value of the weights so
222:29 - this is different from the L2
222:31 - regularization which is the reach
222:33 - regularization and this regularization
222:36 - adds to our loss function the
222:39 - regularization ation term which is based
222:41 - on the Lambda so the penalization
222:43 - parameter multiplied by the square of
222:46 - the weight so you can see how the two
222:49 - are different one is based on what we
222:51 - are calling the L1 norm and the other
222:53 - one is based what we are calling L2 Norm
222:56 - hence the names L1 and L2
222:59 - regularization so both of them are used
223:02 - with the same motivation to prevent
223:05 - overfitting what L1 does different from
223:07 - L2 is that L1 can set the weight of
223:12 - certain neurons exactly equal to zero so
223:15 - in some way also performing feature
223:17 - selection whereas L2 regularization it
223:21 - shrinks the weights towards zero but it
223:23 - never sets them exactly equal to zero so
223:27 - in this aspect L2 doesn't perform
223:29 - feature selection and it only performs
223:32 - regularization whereas L1 can be used
223:35 - not only for uh shrinking the weights
223:38 - and regularizing the network but also
223:39 - also for performing feature selection
223:41 - when you have too many
223:43 - features so you might be wondering but
223:45 - how does this help to prevent
223:47 - overfitting well when you uh shrink the
223:50 - weights towards zero and you are trying
223:53 - to regularize this small or large
223:56 - weights then this methods such as L1 or
224:00 - L2 regularization they will ensure that
224:03 - the moral doesn't overfit to the
224:05 - training data so you will then regular
224:08 - regularize the weights and this will
224:10 - then on its turn regularize the network
224:13 - because the weights will Define how much
224:17 - of this Behavior ertic Behavior will be
224:20 - prevented if you have two large weights
224:23 - and you reduce them and you regularize
224:25 - them it will ensure that you don't have
224:28 - exploring gradients it will also uh
224:31 - ensure that the network doesn't heavily
224:34 - rely on certain neurons and this will
224:37 - then ensure that your moto is not
224:39 - overfitting and not memorizing straining
224:42 - data which might also include noise and
224:45 - outlier points now what is the
224:47 - difference between L2 and L1
224:49 - regularization approaches so I did
224:52 - briefly mention the difference of the
224:54 - two now let's look into more detailed
224:57 - version of it so L2 regularization also
225:00 - known as Rich regularization at squared
225:04 - weight values to the loss function so
225:07 - it's also called L2 Norm so as you can
225:09 - see we are taking the weights wi we are
225:12 - squaring them we are summing them up for
225:15 - that specific layer and we are
225:16 - multiplying it by a regularization
225:18 - parameter also referred as penalization
225:21 - parameter Lambda hence the name ll2
225:25 - which basically means that this is the
225:27 - regularization term for the L2
225:29 - regularization or the reach
225:31 - regularization now when it comes to the
225:34 - impact of this L2 regularization is that
225:37 - it will shrink the weight weights
225:40 - towards zero but it does this in a very
225:43 - proportional way so it tries to make the
225:46 - weights more equal towards each other uh
225:49 - so it will have a bigger impact on the
225:53 - large weights because it will try to
225:55 - make it equal to the smaller weights but
225:58 - it will have lesser impact on the
225:59 - smaller weights in this way it will try
226:02 - to proportionally spread over the ways
226:05 - towards different neurons and it will
226:07 - try to ensure that all this different
226:09 - neurons have kind of equal
226:12 - proportionally equal uh impact uh on the
226:15 - training process whereas when it comes
226:18 - to L1 regularization which is based on
226:20 - L1 Norm also referred as ler
226:23 - regularization uh it adds to the loss
226:26 - function the added so the sum of the
226:28 - absolute value of the
226:30 - weights so you can see here that it's
226:33 - equal to Lambda multiplied by the sum of
226:36 - all the absolute values of w w i we of
226:40 - our neural network so the way L1
226:43 - regularization works is that it adds
226:45 - these absolute values which then shrinks
226:49 - the weights toward zero and in some
226:52 - cases it sets certain weights and
226:55 - specifically those small weights exactly
226:57 - equal to zero so in this way L1
227:00 - regularization punishes the small
227:03 - weights so it's heavily shrinking those
227:06 - weights and set them equal to exactly
227:08 - zero and it has less of a punishment
227:11 - effect and is less harsh on this large
227:14 - weights and in this way it also performs
227:18 - feature selection because all the
227:19 - neurons so the features that have a
227:22 - weight of zero so they have uh their
227:25 - weight become zero it means that they
227:27 - are no longer being no longer
227:29 - contributing to the network let's
227:30 - briefly look at the difference between
227:32 - the two in this table to clarify so when
227:35 - it comes to the feature selection
227:37 - process L1 performs feature selection L2
227:40 - doesn't because L1 said certain weights
227:42 - exactly equal to zero when it comes to
227:45 - the smoothing process L1 it has a less
227:47 - smoothing effect because it's very harsh
227:50 - on the small weights it sets them
227:52 - exactly equal to zero and less on the
227:54 - large weights L2 is bit different it's
227:57 - less harsh of a method and it is more
228:00 - harsh on the large weights and much less
228:02 - on the small weights it kind of
228:04 - proportionally spreads over the weights
228:06 - across all the Neons it it then ensures
228:09 - that process is
228:10 - smoother so then finally the sparsity
228:14 - aspect when it comes to L1 the sparcity
228:17 - is high because the model also performs
228:20 - feature selection so it sets certain
228:22 - weights equal to zero which means that
228:25 - it also automatically perform swi
228:27 - selection and it then reduces that the
228:29 - dimension of the model and makes the
228:32 - network
228:33 - sparser whereas in case of a two or re
228:37 - regularization the sparcity is low
228:40 - because the regularization process is
228:43 - not performing future selection and it
228:46 - has no zero
228:48 - weights the next question is how do L1
228:52 - and L2 regularization impact the weights
228:55 - in narrow network uh when it comes to
228:58 - comparing the impact on large versus
229:00 - small weight
229:02 - penalizations so in here we can just
229:04 - summarize at high level given that we
229:06 - went into detail of this as part of
229:08 - previous questions that L2 as the square
229:12 - of the weights as a penalty and as a
229:14 - result the large weights are heavily
229:17 - punished so they are reduced versus the
229:21 - small weights which are not so heavily
229:22 - and harshly reduced this means that the
229:26 - L2 will distribute the error along and
229:31 - among all the weights and this will lead
229:33 - to smaller but nonzero weights and it
229:37 - will kind of proportionally spread over
229:39 - all the weights across the
229:41 - neurons now when it comes to the L1
229:44 - regularization L1 significantly reduces
229:47 - the magnitude of the weights so large
229:49 - versus small weights the reduction of
229:52 - the of the weights when it comes to the
229:55 - large weights is much smaller so the L2
229:59 - L1 regularization is less harsh on the
230:02 - large weights compared to thisal weights
230:06 - so given that we are using this absolute
230:09 - vol Val of the weights it means that it
230:12 - will the algorithm will be effectively
230:14 - remove the features from the model by
230:18 - setting certain weights exactly equal to
230:20 - zero and those are usually the weights
230:22 - that have already been small so it will
230:25 - penalize more harshly those small
230:29 - weights and it will be very harsh on
230:31 - them but not so much on the large
230:33 - weights and in this way we will end up
230:36 - removing those small weights by setting
230:38 - them equal to zero and keeping only
230:40 - those large weights the next question is
230:43 - what is the curse of dimensionality in
230:45 - machine learning or in AI so the curse
230:48 - of dimensionality is a nonn phenomena in
230:51 - machine learning especially when we are
230:53 - dealing with this distance-based
230:55 - neighborhood based models like KNN or
230:59 - cines and we need to compute this
231:01 - distance using distance measures such as
231:03 - aine distance cosign distance Manhattan
231:06 - distance and whenever we have have uh
231:10 - High dimensional data so we have many
231:13 - features in our data then the model
231:15 - starts to really suffer from the Cur of
231:19 - dimensionality so the complexity Rises
231:23 - when the model needs to compute these
231:25 - distances between the the set of pairs
231:29 - but given that we have so many features
231:31 - it becomes problematic and sometimes
231:34 - even invisible to obtain those distances
231:38 - and
231:39 - and obtaining and calculating these
231:42 - distances in some cases doesn't even
231:44 - make sense because they no longer
231:46 - reflect the actual pairwise relationship
231:49 - or the distance between those two pair
231:52 - of observations when we have so many
231:55 - features and that's what we are calling
231:57 - the curse of dimensionality so we have a
231:59 - curse on our ml or AI model when we have
232:04 - high dimension and when we want to
232:06 - compute this distance between pair of
232:08 - observ ations and this can introduce
232:12 - data sparsity this can introduce
232:14 - computational challenges can introduce a
232:17 - risk of overfitting for our problem the
232:19 - model becomes less
232:21 - generalizable and it will also become
232:23 - problem in terms of picking a distance
232:26 - measure that can handle this high
232:28 - dimension of our data how deep learning
232:31 - models tackle the course of
232:33 - dimensionality so when it comes to the
232:36 - Deep learning models unlike the machine
232:38 - learning learning models deep learning
232:40 - models do not suffer from this problem
232:43 - so machine learning models they need to
232:45 - for them to work properly with high
232:47 - dimensional data they need to perform
232:49 - feature selection they sometimes
232:52 - recommend to use different sorts of
232:54 - feature selection techniques such as
232:56 - forward stepwise selection backward
232:57 - stepwise selection mix selections or
233:01 - other type of feature selection
233:02 - techniques or dimensionality reduction
233:05 - techniques such as PCA or factor
233:07 - analysis in order to to reduce the
233:09 - dimension of our features before even
233:13 - providing them as an input to our
233:14 - machine learning model this is not
233:17 - necessarily for our deep learning models
233:19 - Because deep learning models are able to
233:21 - counter this course of dimensionality by
233:25 - learning this useful feature
233:26 - representations and reducing this data
233:29 - Dimension that we have uh applying
233:32 - regularization and also using
233:34 - architecture specifically designed for
233:37 - high dimension data so
233:40 - basically we don't need to perform
233:42 - feature selection when it comes to uh
233:45 - using this complex and high dimensional
233:47 - data in the Deep learning Because deep
233:50 - learning models are specifically
233:52 - designed to be able to learn complex
233:54 - dependencies in the features nonlinear
233:56 - patterns so they do not need feature
233:59 - selection techniques to perform that for
234:02 - them and instead they just use
234:04 - techniques regularization techniques
234:05 - such as L1 regularization so L of
234:08 - regular ation to not only perform
234:10 - regularization but also to perform
234:12 - future selection as part of the deep
234:15 - learning models itself so few ways that
234:19 - the Deep learning models can prevent
234:22 - this curse of dimensionality and the way
234:24 - that they do not suffer from this curse
234:26 - of
234:27 - dimensionality is by performing feature
234:29 - learning as part of the actual original
234:32 - Deep learning process um so it also uses
234:36 - this dimensionality reduction techniques
234:39 - for instance uses outo encoders
234:41 - variational outo encoders to compress
234:44 - the input
234:46 - data and represent the input data with
234:49 - this compressed form like the
234:51 - dimensionality reduction technique but
234:54 - now as part of the actual architecture
234:56 - of deep learning model and then learn
234:58 - the hidden factors from this already
235:01 - compressed data with lower Dimension and
235:04 - then reconstruct from this to get the
235:07 - final output
235:09 - now another way that it does is by using
235:12 - regularization methods and I briefly
235:14 - spoke about this uh just before by for
235:18 - instance using L1 regularization because
235:20 - L1 regularization reduces the sparcity
235:24 - uh and introduces sparcity by reducing
235:26 - the dimension of the of the features by
235:30 - setting some of the weights equal to
235:32 - zero it indirectly performs feature
235:35 - selection and finally if we use model
235:38 - such as CNN and we use pulling layers so
235:42 - pulling layers in convolution neural
235:44 - networks for instance they automatically
235:47 - perform feature selection and they
235:49 - reduce the dimensionality of the problem
235:52 - and in this way they automatically
235:54 - combat the curse of dimensionality the
235:57 - next question is what are generative
235:59 - models give examples so let's first
236:02 - start with the generative models so
236:05 - generative models aim to model how data
236:08 - is
236:09 - generated they want to learn the uh
236:12 - joint probability distribution what we
236:13 - are often referring as P X and Y which
236:17 - is the joint probability distribution of
236:19 - two random variables X and Y where X
236:22 - represent the features of our data and
236:25 - why represent the
236:27 - labels so what generative models do is
236:30 - that they try to model the underlying
236:33 - data distribution so they want to figure
236:35 - out how data was generated in the first
236:37 - place and in this way they can then
236:41 - generate new data instances and they can
236:45 - also predict
236:46 - probabilities and beside this generative
236:49 - models are also great when it comes to
236:51 - unsupervised learning tasks so from
236:54 - machine learning you are most likely
236:56 - familiar with this idea of supervised
236:58 - versus unsupervised when in case of
237:01 - supervised we do have the label so the Y
237:03 - values but in case of the unsupervised
237:06 - case we don't have labels to supervisor
237:09 - learning process which means that we
237:11 - need to perform the learning only using
237:14 - the features think about clustering
237:17 - think about outlier detection or
237:20 - dimensionality reduction or some data
237:24 - Generation all these cases can be
237:26 - considered as unsupervised learning
237:28 - tasks and generative models are great in
237:32 - doing
237:33 - so now when it comes to the generative
237:36 - models they are particularly useful in
237:38 - scenarios when we want to understand and
237:40 - we need to understand the underlying
237:43 - data distribution or when we need to
237:46 - generate new data points so let's say
237:48 - you want to generate synthetic data or
237:51 - you have lot of images and you want to
237:53 - generate new images images that do make
237:56 - sense and they are similar to the input
237:58 - data but they are new they are noic they
238:01 - are fresh and this can be done by using
238:04 - generative models and all the models
238:07 - that these days you hear about
238:09 - for instance the GPT
238:11 - series uh the variational auto encoders
238:15 - the gens so generative adval series uh
238:19 - networks all these models they are
238:21 - generative models and they are largely
238:23 - used for generating new images
238:26 - generating synthetic data um for
238:29 - generating new speech for instance when
238:32 - there is a data that is there is a model
238:34 - that is trained on the large amount of
238:36 - of the speech you provide for examples
238:39 - of your speech and then the model is
238:41 - able to generate new voice so recording
238:45 - of based on the text based on your own
238:48 - voice but you haven't yet spoken that
238:51 - that's generative model that is doing
239:07 - that