00:01 - what's up guys in this video we're going
00:03 - to introduce the concept of client-side
00:05 - artificial neural networks which will
00:07 - lead us to deploying and running models
00:10 - along with our full deep learning
00:11 - applications in the browser to implement
00:14 - this cool capability we'll be using
00:16 - tensorflow.js
00:17 - tensorflow's javascript library which
00:19 - will allow us to well build and access
00:22 - models in javascript
00:23 - i'm super excited to cover this so let's
00:26 - get to it
00:33 - whether you're a complete beginner to
00:35 - neural networks and deep learning or you
00:37 - just want to up your skills in this area
00:39 - you'll definitely want to check out all
00:41 - the resources available on the deep
00:42 - lizard youtube channel as well as
00:44 - deepblizzard.com there you'll find
00:47 - several complete video series and blogs
00:49 - where you'll learn everything from
00:51 - absolute machine learning basics all the
00:53 - way to developing and deploying your own
00:55 - deep learning projects from scratch
00:57 - here's a quick breakdown of the sections
00:59 - that make up this video go ahead and
01:02 - take a look here to get an idea of the
01:04 - content we'll be covering together
01:06 - so client-side neural networks running
01:09 - models in the browser to be able to
01:11 - appreciate the coolness factor of this
01:13 - we're going to need some context so that
01:15 - we can contrast what we've historically
01:17 - been able to do from a deployment
01:19 - perspective to what we can now do with
01:21 - client-side neural networks
01:23 - alright so what are we used to being
01:25 - able to do
01:26 - as we've seen in our previous series on
01:28 - deploying neural networks in order to
01:30 - deploy a deep learning application we
01:32 - need to bring two things together the
01:34 - model and the data
01:40 - to make this happen we'll normally see
01:42 - something like this
01:43 - we have a front-end application say a
01:45 - web app running in the browser that a
01:47 - user can interact with to supply data
01:50 - then we have a back-end application a
01:52 - web service where our model is loaded
01:54 - and running when the user supplies the
01:56 - data to the front-end application that
01:59 - web app will make a call to the back-end
02:01 - application and post the data to the
02:03 - model the model will then do its thing
02:05 - like make a prediction and then it will
02:07 - return its prediction back to the web
02:08 - application which will then be supplied
02:10 - to the user
02:12 - so that's the usual story of how we
02:14 - bring the model and the data together
02:16 - and we've already gone over all the
02:17 - details for how to actually implement
02:19 - this type of deployment so be sure to
02:21 - check out that series i mentioned
02:22 - earlier if you haven't already links in
02:24 - the description
02:26 - now with client-side neural network
02:27 - deployment we have no back-end the model
02:30 - isn't sitting on a server somewhere
02:31 - waiting to be called by front-end apps
02:33 - but rather the model is embedded inside
02:36 - of the front-end application what does
02:38 - this mean
02:39 - well for a web app that means the model
02:41 - is running right inside the browser
02:44 - so in our example scenario that we went
02:46 - over a couple moments ago rather than
02:48 - the data being sent across the network
02:50 - or across the internet from the front
02:51 - end application to the model in the back
02:54 - end the data in the model are together
02:56 - from the start within the browser
02:59 - okay so this is cool and everything but
03:01 - what's the
03:02 - point for one users get to keep their
03:05 - data local to their machines or devices
03:08 - since the model is running on the local
03:09 - device as well
03:11 - if user data isn't having to travel
03:12 - across the internet we can say that's a
03:14 - plus and there's more on this concern in
03:16 - the previous series i mentioned
03:18 - additionally when we develop something
03:20 - that runs in a front-end application
03:22 - like in the browser that means that
03:24 - anyone can use it anyone can interact
03:26 - with it as long as they can browse to
03:28 - the link our application is running on
03:30 - there's no prerequisites or installs
03:32 - needed
03:33 - and in general a browser application is
03:35 - highly interactive and easy to use so
03:37 - from a user perspective it's really
03:39 - simple to get started and engage with
03:41 - the app
03:42 - so this is all great stuff but just as
03:44 - there's considerations to address with
03:46 - the traditional deployment
03:47 - implementation there are a couple
03:49 - caveats for the client-side deployment
03:50 - implementation as well
03:55 - but don't let that run you off there's
03:57 - just a few things to consider
03:59 - as we're talking about these i'm going
04:01 - to play some demos of open source
04:02 - projects that have been created with
04:04 - tensorflow.js just so you can get an
04:06 - idea of some of the cool things we can
04:07 - do with client-side neural networks
04:09 - links to all of them are in the
04:11 - description
04:12 - now on to the caveats
04:14 - for one we have to consider the size of
04:16 - our models since our models will be
04:18 - loaded and running in the browser you
04:19 - can imagine that loading a massive model
04:22 - into our web app might cause some issues
04:24 - tensorflow.js suggests to use models
04:26 - that are 30 megabytes in size or less
04:29 - to give some perspective the vgg 16
04:32 - model is over 500 megabytes so what
04:35 - would happen if we tried to run that
04:36 - sucker in the browser we're actually
04:38 - going to demo that in a future section
04:40 - so stay tuned all right then what types
04:43 - of models are good to run in the browser
04:46 - well smaller ones ones that have been
04:48 - created with the intent to run on
04:50 - smaller or lower powered devices like
04:52 - phones and what types of models have we
04:54 - seen that are incredibly powerful for
04:56 - this
04:56 - mobile nets
04:58 - so we'll also be seeing how a mobilenet
05:00 - model holds up to being deployed in the
05:01 - browser as well
05:03 - now once we have a model up and running
05:05 - in the browser can we do anything we'd
05:07 - ordinarily otherwise do with the model
05:09 - using tensorflow.js pretty much
05:12 - but would we want to do everything with
05:14 - the model running in the browser that's
05:16 - another question and the answer is
05:18 - probably not
05:19 - having our models run in the browser is
05:21 - best suited for tasks like fine-tuning
05:23 - pre-trained models or most popularly
05:25 - inference so using a model to get
05:28 - predictions on new data
05:30 - and this task is exactly what we saw
05:32 - with the project we worked on in the
05:33 - keras model deployment series using
05:35 - flask
05:36 - while building new models and training
05:38 - models from scratch can also be done in
05:39 - the browser these tasks are usually
05:41 - better addressed using other apis like
05:43 - keras or standard tensorflow in python
05:46 - so now that we have an idea of what it
05:48 - means to deploy our model to a
05:50 - client-side application why we'd want to
05:52 - do this and what types of specific
05:54 - things we'd likely use this for let's
05:56 - get set to start coding and implementing
05:58 - applications for this in the next
06:00 - sections of this video let me know in
06:02 - the comments if you plan to follow along
06:04 - and i'll see it in the next section
06:07 - in this section we'll continue getting
06:09 - acquainted with the idea of client-side
06:11 - neural networks and we'll kick things
06:13 - off by seeing how we can use
06:15 - tensorflow's model converter tool to
06:17 - convert keras models into tensorflow.js
06:20 - models
06:21 - this will allow us to take models that
06:22 - have already been built and trained with
06:24 - keras and make use of them in the
06:26 - browser with tensorflow.js so let's get
06:28 - to it
06:30 - [Music]
06:36 - tensorflow.js has what they call the
06:38 - layers api which is a high level neural
06:41 - network api inspired by keras and we'll
06:44 - see that what we can do with this api
06:46 - and how we use it is super similar to
06:48 - what we've historically been able to do
06:50 - with keras
06:51 - so given this it makes sense that we
06:53 - should be able to take a model that we
06:55 - built in keras or that we trained in
06:57 - keras and port it over to tensorflow.js
06:59 - and use it in the browser with the
07:01 - layers api right
07:02 - otherwise the alternative would be to
07:04 - build a model from scratch and train it
07:06 - from scratch in the browser and as we
07:08 - discussed in the last section that's not
07:10 - always going to be ideal so having the
07:12 - ability and the convenience to convert
07:14 - pre-built or pre-trained keras models to
07:17 - run in the browser is definitely going
07:18 - to come in handy
07:20 - alright now let's see how we can convert
07:22 - a keras model to a tensorflow.js model
07:25 - first we need to install the
07:26 - tensorflow.js model converter tool so
07:29 - from a python environment probably one
07:31 - where keras is already installed we run
07:33 - pip install tensorflow.js from the
07:35 - terminal
07:36 - and once we have this we can convert a
07:38 - keras model into a tensorflow.js model
07:41 - there are two ways to do the conversion
07:43 - and we'll demo both
07:45 - the first way is making use of the
07:46 - converter through the terminal or
07:48 - command line
07:49 - we'd want to use this method for keras
07:51 - models that we've already saved to disk
07:53 - as an h5 file
07:54 - if you've watched the d blizzard keras
07:56 - series you know we have multiple ways we
07:58 - can save a model or save parts of a
08:00 - model like just the weights or just the
08:02 - architecture to convert a keras model
08:04 - into a tensorflow.js model though we
08:06 - need to have saved the entire model with
08:09 - the weights the architecture everything
08:11 - in an h5 file
08:12 - currently that's done using kara's
08:14 - model.save function so given this i
08:17 - already have a sample model i've created
08:19 - with keras and save to disk
08:21 - if you don't already have access to
08:23 - these keras model files don't worry i've
08:25 - included links to keras github where you
08:27 - can just download these files once you
08:30 - have them you can follow this conversion
08:32 - process using those h5 files when
08:34 - they're needed later in this video i'm
08:36 - in the terminal now where we'll run the
08:38 - tensorflow.js converter program
08:40 - so we run tensorflow.js converter and
08:43 - specify what kind of input the converter
08:45 - should expect so we supply dash dash
08:47 - input format keras
08:49 - then we supply the path to the saved h5
08:52 - file and the path to the output
08:54 - directory where we want our converted
08:55 - model to be placed
08:57 - and the output directory needs to be a
08:59 - directory that's solely for holding the
09:00 - converted model there will be multiple
09:03 - files so don't just specify your desktop
09:05 - or something like that
09:06 - so when we run this we get this warning
09:08 - regarding a deprecation but it's not
09:10 - hurting us for anything we're doing here
09:12 - and that's it for the first method we'll
09:14 - see in a few moments the format of the
09:15 - converted model but before we do that
09:18 - let's demo the second way to convert a
09:20 - keras model
09:21 - this is going to be done directly using
09:23 - python and this method is for when we're
09:25 - working with a keras model and we want
09:27 - to go ahead and convert it on the spot
09:28 - to a tensorflow.js model without
09:30 - necessarily needing to save it to an h5
09:32 - file first
09:34 - so we're in a jupiter notebook where
09:36 - we're importing keras in the
09:37 - tensorflow.js library and i'm going to
09:40 - demo this with the vgg16 model because
09:42 - we'll be making use of this one in a
09:44 - future section anyway but this
09:46 - conversion will work for any model you
09:48 - build with keras so we have this vgg g16
09:50 - model that's created by calling
09:52 - carous.applications.bgg16.vgg16
09:57 - and then we call
09:58 - tensorflowjs.converters.save
10:01 - keras model
10:02 - and to this function we supply the model
10:04 - that we're converting as well as the
10:06 - path to the output directory where we
10:08 - want the converted tensorflow.js model
10:10 - to be placed
10:11 - and that's it for the second method
10:13 - so let's go check out what the output
10:15 - from these conversions look like we're
10:17 - going to look at the smaller model that
10:18 - we converted from the terminal
10:21 - so we're inside of this directory called
10:22 - simplemodel which is the output
10:24 - directory i specified whenever we
10:26 - converted the first model and we have a
10:28 - few files here
10:29 - we have this one file called model.json
10:32 - which contains the model architecture
10:33 - and metadata for the weight files
10:36 - and those corresponding weight files are
10:37 - these sharded files that contain all the
10:39 - weights from the model and are stored in
10:41 - binary format
10:43 - the larger and more complex the model is
10:45 - the more weight files there will be
10:47 - this model was small with only a couple
10:49 - dense layers and about 640 learnable
10:51 - parameters but the vgg16 model we
10:54 - converted on the other hand with over
10:56 - 140 million learnable parameters has 144
11:00 - corresponding weight files
11:01 - all right so that's how we can convert
11:03 - our existing keras models into
11:05 - tensorflow.js models we'll see how these
11:07 - models and their corresponding weights
11:09 - are loaded in the browser in a future
11:11 - section when we start building our
11:12 - browser application to run these models
11:15 - i'll see you there
11:17 - in this section we'll go through the
11:19 - process of getting a web server setup to
11:21 - host deep learning web applications and
11:23 - serve deep learning models with express
11:25 - for node.js so let's get to it
11:30 - [Music]
11:35 - to build deep learning applications that
11:37 - run in the browser we need a way to host
11:39 - these applications and a way to host the
11:41 - models so then really we just need a way
11:44 - to serve static files if you followed
11:46 - the d blizzard youtube series on
11:48 - deploying keras models then you know
11:50 - that we already have a relatively easy
11:52 - way of hosting static files and that's
11:54 - with flask blast though is written in
11:57 - python and while it would work perfectly
11:59 - fine to host the tensorflow.js
12:01 - applications we'll be developing it
12:02 - makes sense that we might want to use a
12:04 - javascript based technology to host our
12:06 - apps since we're kind of breaking away
12:08 - from python and embracing javascript in
12:11 - this series
12:12 - so enter express for node.js
12:15 - express is a minimalist web framework
12:18 - very similar to flask but is for node.js
12:21 - not python and if you're not already
12:23 - familiar with node.js then you're
12:25 - probably wondering what it is as well
12:28 - node.js which we'll refer to most of the
12:30 - time as just node is an open source
12:32 - runtime environment that executes
12:34 - javascript on the server side
12:36 - see historically javascript has been
12:38 - used mainly for client-side applications
12:41 - like browser applications for example
12:43 - but node allows us to write server-side
12:46 - code using javascript
12:48 - we'll specifically be making use of
12:50 - express to host our web applications and
12:52 - serve our models
12:54 - so let's see how we can do that now
12:57 - first things first we need to install
12:59 - node.js i'm here on the downloads page
13:02 - of node's website so you just need to
13:04 - navigate to this page choose the
13:06 - installation for your operating system
13:07 - and get it installed
13:09 - i've installed node on a windows machine
13:11 - but you'll still be able to follow the
13:12 - demos we'll see in a few moments even if
13:14 - you're running another operating system
13:17 - alright after we've got node installed
13:19 - we need to create a directory that will
13:20 - hold all of our project files so we have
13:23 - this directory here i've called
13:24 - tensorflow.js
13:26 - within this directory we'll create a
13:28 - subdirectory called local server which
13:30 - is where the express code that will run
13:32 - our web server will reside
13:34 - and we'll also create a static directory
13:37 - which is where our web pages and
13:38 - eventually our models will reside
13:41 - within this local server we create a
13:43 - package.json file which is going to
13:46 - allow us to specify the packages that
13:48 - our project depends on
13:50 - let's go ahead and open this file
13:52 - i've opened this with visual studio code
13:54 - which is a free open source code editor
13:56 - developed by microsoft that can run on
13:58 - windows linux and mac os
14:01 - this is what we'll be using to write our
14:02 - code so you can download it and use it
14:04 - yourself as well or you can use any
14:06 - other editor that you'd like
14:08 - alright back to the package.json file
14:11 - within package.json we're going to
14:12 - specify a name for our project which
14:15 - we're calling tensorflow.js all
14:17 - lowercase per the requirements of this
14:19 - file
14:20 - we'll also specify the version of our
14:21 - project
14:22 - there's some specs that the format of
14:24 - this version has to meet but most
14:26 - simplistically it has to be in an x.x.x
14:28 - format so we're just going to go with
14:30 - the default of 1.0.0
14:33 - all right name and version are the only
14:35 - two requirements for this file but there
14:37 - are several other optional items we can
14:39 - add like a description the author and a
14:41 - few others we're not going to worry
14:43 - about this stuff but we are going to add
14:45 - one more thing the dependencies
14:48 - this specifies the dependencies that our
14:50 - project needs to run
14:52 - we're specifying express here since
14:54 - that's what we'll be using to host our
14:56 - web apps and we're also specifying the
14:58 - version
14:59 - now we're going to open powershell and
15:01 - we have the ability to open it from
15:02 - right within this editor by navigating
15:04 - to view and then integrated terminal
15:08 - and you should have the ability to open
15:10 - the terminal of your choice that's
15:11 - appropriate for your operating system if
15:13 - you're running on linux for example and
15:14 - don't have powershell
15:16 - otherwise you can just open the terminal
15:18 - outside of the editor if you'd like
15:20 - all right so from within powershell we
15:22 - make sure we're inside of the local
15:23 - server directory where the package.json
15:26 - file is and we're going to run npm
15:29 - install
15:30 - npm stands for node package manager and
15:32 - by running npm install npm will download
15:35 - and install the dependencies listed in
15:37 - our package.json file so let's run npm
15:41 - install and we'll see it installs
15:43 - express
15:44 - and when this is finished you can see
15:46 - that we now have an added node modules
15:48 - directory that contains the downloaded
15:50 - packages and we additionally have this
15:52 - packagelock.js
15:54 - file that we didn't have before
15:56 - it contains information about the
15:57 - downloaded dependencies don't delete
15:59 - these things
16:01 - alright so at this point we have node we
16:04 - have express now we need to write a node
16:06 - program that will start the express
16:08 - server and will host the files that we
16:10 - specify
16:12 - let's see that makes sense
16:15 - to do this we'll create this file called
16:17 - server.js
16:20 - inside of server js we first import
16:22 - express using require express
16:26 - using require like this we'll import the
16:27 - express module and give our program
16:29 - access to it
16:31 - you can think of a module and node as
16:33 - being analogous to a library in
16:35 - javascript or python just a group of
16:37 - functions that we want to have access to
16:39 - from within our program
16:40 - and then we create an express
16:42 - application using the express module
16:44 - which we assigned to app
16:46 - an express app is essentially a series
16:48 - of calls to functions that we call
16:50 - middleware functions
16:52 - middleware functions have access to the
16:54 - http request and response objects as
16:57 - well as the next function in the
16:59 - application's request response cycle
17:01 - which just passes control to the next
17:03 - handler
17:04 - so within this app when a request comes
17:06 - in we're doing two things we're first
17:09 - logging information about the request to
17:11 - the terminal where the express server is
17:12 - running and we then pass control to the
17:15 - next handler which will respond by
17:17 - serving any static files that we've
17:19 - placed in this directory called static
17:21 - that's right within the root directory
17:22 - of our tensorflow.js project
17:25 - so in our case the middleware functions
17:27 - i mentioned are here and here
17:31 - note that the calls to app.use are only
17:33 - called once and that's when the server
17:35 - is started
17:36 - the app.use calls specify the middleware
17:38 - functions and calls to those middleware
17:41 - functions will be executed each time a
17:43 - request comes into the server
17:46 - lastly we call app.listen to specify
17:48 - what port express should listen on i've
17:51 - specified port 81 here but you can
17:53 - specify whichever unused port you'd like
17:56 - when the server starts up and starts
17:57 - listening on this port this function
17:59 - will be called which will log this
18:01 - message letting us know that the server
18:02 - is up and running
18:04 - all right we're all set up let's drop a
18:06 - sample html file into our static
18:08 - directory then start up the express
18:10 - server and see if we can browse to the
18:12 - page
18:13 - we're going to actually just place the
18:15 - web application called predict.html that
18:17 - we created in the keras deployment
18:19 - series into this directory as a proof of
18:21 - concept so we place that here you can
18:24 - use any html file you'd like though to
18:26 - test this now to start express we use
18:29 - powershell let's make sure we're inside
18:31 - of the local server directory and we run
18:34 - node server js
18:37 - we get our output message letting us
18:38 - know that express is serving files from
18:40 - our static directory on port 81.
18:44 - so now let's browse to localhost or
18:46 - whatever the ip address is that you're
18:48 - running express on
18:49 - port 81 slash predict.html
18:53 - which is the name of the file we put
18:55 - into the static directory
18:57 - and here we go this is indeed the web
18:59 - page we wanted to be served
19:02 - we can also check out the output from
19:04 - this request in powershell to view the
19:06 - logging that we specified
19:08 - so good we now have node and express set
19:11 - up to be able to serve our models and
19:13 - host our tensorflow.js apps that we'll
19:15 - be developing coming up
19:17 - give me a signal in the comments if you
19:18 - are able to get everything up and
19:20 - running and i'll see it in the next
19:21 - section
19:24 - in this section we're going to start
19:26 - building the ui for our very first
19:28 - client-side neural network application
19:30 - using tensorflow.js so let's get to it
19:35 - [Music]
19:41 - now that we have express setup to host a
19:43 - web app for us let's start building one
19:45 - the first app we'll build is going to be
19:47 - similar in nature to the predict app we
19:49 - built in the flask series with keras
19:52 - recall this was the app we built in that
19:54 - previous series
19:56 - we had a fine-tuned vgg g16 model
19:58 - running in the back end as a web service
20:00 - and as a user we would select an image
20:02 - of a cat or dog submit the image to the
20:05 - model and receive a prediction
20:08 - now the idea of the app will develop
20:09 - with tensorflow.js will be similar but
20:12 - let's discuss the differences can you
20:14 - see the source code that's generating
20:16 - the responses
20:18 - um yeah we can and we will but first
20:22 - know that our model will be running
20:24 - entirely in the browser
20:25 - our app will therefore consist only of a
20:27 - front-end application developed with
20:29 - html and javascript
20:31 - so here's what the new app will do
20:34 - the general layout will be similar to
20:36 - the one we just went over where a user
20:37 - will select an image submit it to the
20:39 - model and get a prediction we won't be
20:42 - restricted to choosing only cat and dog
20:44 - images though because we won't be using
20:46 - fine-tuned models this time
20:48 - instead we'll be using original
20:50 - pre-trained models that were trained on
20:51 - imagenet so we'll have a much wider
20:54 - variety of images we can choose from
20:56 - once we submit our selected image to the
20:58 - model the app will give us back the top
21:00 - five predictions for that image from the
21:03 - imagenet classes
21:04 - so which model will we be using
21:07 - well remember how we discussed earlier
21:09 - that models best suited for running in
21:11 - the browser are smaller models and how
21:13 - tensorflow recommends using models that
21:15 - are 30 megabytes or less in size well
21:18 - we're first going to go against this
21:19 - recommendation and use vgg16 as our
21:22 - model which is over 500 megabytes in
21:25 - size
21:26 - nice priorities
21:28 - we'll see how that works out for us but
21:30 - you can imagine that it may be
21:32 - problematic
21:34 - no worries though we'll have mobilenet
21:35 - to the rescue coming in at only about 16
21:38 - megabytes so we'll get to see how these
21:40 - two models compare to each other
21:41 - performance wise in the browser it'll be
21:44 - interesting
21:45 - all right let's get set up
21:48 - from within the static directory we
21:49 - created last time we need to create a
21:51 - few new resources
21:53 - we need to create a file called
21:54 - predictwithtfjs.html
21:57 - which will be our web app
21:59 - then we also need to create a file
22:01 - called predict js which will hold all
22:03 - the javascript logic for our app
22:06 - then we need a directory to hold our
22:08 - tensorflow.js models so we have this one
22:11 - which we're calling tfjs models
22:14 - navigating inside we have two
22:15 - subdirectories one for mobilenet and one
22:18 - for vgg16 since these are the two models
22:21 - we'll be using
22:22 - each of these directories will contain
22:24 - the model.json and the corresponding
22:26 - weight files for each model
22:28 - navigating inside of vgg 16 we can see
22:31 - that
22:32 - to get these files here i simply went
22:34 - through the conversion process in python
22:36 - of loading bg g16 and mobilenet with
22:38 - keras and then converting the models
22:41 - with the tensorflow.js converter we
22:43 - previously discussed so follow that
22:45 - earlier section to get this same output
22:47 - to place in your model directories
22:50 - alright navigating back to the static
22:51 - directory the last resource is this
22:53 - imagenet class js file this is simply a
22:56 - file that contains all the imagenet
22:58 - classes which we'll be making use of
23:00 - later you can also find all of these
23:02 - ordered imagenet classes on the
23:03 - tensorflow.js blogs at deeplizzer.com
23:07 - let's open it up and take a look at the
23:09 - structure
23:10 - so we just have this javascript object
23:12 - called imagenet classes that contains
23:14 - the key value pairs of the imagenet
23:16 - classes with associated ids
23:19 - all right now let's open the predict
23:20 - with tfjs.html file and jump into the
23:23 - code
23:24 - we're starting off in the head by
23:26 - specifying the title of our web page and
23:28 - importing the styling from this css file
23:31 - for all the styling on the page we'll be
23:33 - using bootstrap which is an open source
23:35 - library for developing html css and
23:38 - javascript that uses design templates to
23:40 - format elements on the page
23:42 - bootstrap is really powerful but we'll
23:44 - simply be using it just to make our app
23:46 - look a little nicer
23:48 - now bootstrap uses a grid layout where
23:51 - you can think of the webpage having
23:52 - containers that can be interpreted as
23:55 - grids and then ui elements on the page
23:58 - are organized into the rows and the
23:59 - columns that make up those grids
24:02 - by setting the element's class
24:04 - attributes that's how bootstrap knows
24:06 - what type of styling to apply to them
24:09 - so given this that's how we're
24:10 - organizing our ui elements
24:12 - embedded within the body we're putting
24:14 - all the ui elements within this main tag
24:17 - you can see that our first div is what's
24:19 - considered to be a container on the page
24:21 - and then within the container we have
24:23 - three rows and each row has columns
24:27 - the columns are where our actual ui
24:28 - elements reside
24:30 - our ui elements are the image selector
24:33 - the predict button
24:34 - the prediction list and the selected
24:37 - image
24:38 - we'll explore this grid layout
24:39 - interactively in just a moment but first
24:41 - let's finish checking out the remainder
24:43 - of the html
24:45 - all that we have left to do is import
24:46 - the required libraries and resources
24:48 - that our app needs
24:50 - first we import jquery
24:52 - then we import tensorflow.js with this
24:54 - line
24:55 - so this single line is all it takes to
24:57 - get tensorflow.js into our app
25:00 - then we import the imagenet class js
25:02 - file we checked out earlier
25:04 - and lastly we import our predict.js file
25:07 - which as mentioned earlier contains all
25:09 - the logic for what our app does when a
25:11 - user supplies an image to it
25:13 - alright so that's it for the html let's
25:15 - check out the page and explore the grid
25:17 - layout first we start up our express
25:20 - server which we learned how to do in the
25:22 - last section
25:23 - then in our browser we'll navigate to
25:25 - the ip address where our express server
25:27 - is running port 81 predict with
25:30 - tfjs.html
25:33 - and here's our page it's pretty empty
25:35 - right now because we haven't selected an
25:36 - image but once we write the javascript
25:38 - logic to handle what to do when we
25:40 - select an image then the name of the
25:42 - selected image file will be displayed
25:43 - here
25:44 - the image will be displayed in the image
25:46 - section and upon clicking the predict
25:48 - button the predictions for the image
25:50 - from the model will be displayed in this
25:52 - prediction section
25:53 - if we open the developer tools by right
25:55 - clicking on the page and then clicking
25:57 - inspect then from the elements tab we
25:59 - can explore the grid layout
26:02 - let's expand the body
26:04 - then main
26:07 - then this first div that acts as the
26:09 - container
26:10 - and hovering over this div you can see
26:12 - that the blue on the page is what's
26:14 - considered to be the container or the
26:16 - grid
26:18 - so now that we've expanded this div we
26:20 - have access to all the rows
26:22 - so hovering over the first row we can
26:24 - see what that maps to in the ui from
26:26 - this blue section
26:28 - and we can do the same for the second
26:30 - and third rows as well
26:34 - then if we expand the rows we have
26:36 - access to the columns that house the
26:38 - individual ui elements
26:40 - so hovering over this first column in
26:42 - the first row we can see that the image
26:43 - selector is here
26:45 - and the predict button is within the
26:47 - second column in the first row
26:49 - and the same idea applies for the
26:50 - remaining elements on the page as well
26:53 - so hopefully that sheds a bit of light
26:55 - on the grid layout that bootstrap is
26:56 - making use of
26:58 - all right in the next section we'll
27:00 - explore all of the javascript that
27:02 - handles the predictions and actually
27:04 - makes use of tensorflow.js i'll see you
27:06 - there
27:08 - in this section we'll continue the
27:10 - development of the client-side deep
27:12 - learning application we started last
27:14 - time so let's get to it
27:21 - [Music]
27:24 - in the last section we built the ui for
27:26 - our image classification web app now
27:28 - we'll focus on the javascript that
27:30 - handles all the logic for this app
27:32 - we'll also start getting acquainted with
27:34 - the tensorflow.js api
27:36 - without further ado let's get right into
27:39 - the code
27:39 - [Music]
27:42 - recall in the last section we created
27:44 - this predict.js file within the static
27:46 - directory but left it empty this file
27:49 - now contains the javascript logic that
27:51 - handles what will happen when a user
27:53 - submits an image to the application so
27:55 - let's look at the specifics for what's
27:57 - going on with this code
27:59 - we first specify what should happen when
28:01 - an image file is selected with the image
28:03 - selector
28:04 - when a new image is selected the change
28:06 - event will be triggered on the image
28:08 - selector and when this happens we first
28:11 - create this file reader object called
28:13 - reader to allow the web app to read the
28:15 - contents of the selected file
28:18 - we then set the onload handler for
28:20 - reader which will be triggered when
28:21 - reader successfully reads the contents
28:23 - of a file
28:25 - when this happens we first initialize
28:27 - the data url variable as reader.result
28:29 - which will contain the image data as a
28:31 - url that represents the files data as a
28:34 - base64 encoded string
28:37 - we then set the source attribute of the
28:39 - selected image to the value of data url
28:42 - lastly within the onload handler we need
28:44 - to get rid of any previous predictions
28:46 - that were being displayed for previous
28:48 - images and we do this by calling empty
28:50 - on the prediction list element
28:53 - next we get the selected file from the
28:55 - image selector and load the image by
28:57 - calling read as data url on reader and
29:00 - passing in the selected image file
29:03 - we then instantiate this model variable
29:05 - and we're going to define it directly
29:07 - below
29:08 - now this below section may look a little
29:11 - freaky if you're not already a
29:12 - javascript whiz so let's see what the
29:15 - deal is
29:16 - here we have what's called an iife or
29:19 - immediately invoked function expression
29:22 - an iife is a function that runs as soon
29:25 - as it's defined
29:27 - we can see this is structured by placing
29:29 - the function within parentheses and then
29:32 - specifying the call to the function with
29:34 - these parentheses that immediately
29:36 - follow
29:37 - within this function we load the model
29:39 - by calling the tensorflow.js function
29:42 - tf.loadmodel which accepts a string
29:45 - containing the url to the model.json
29:47 - file
29:48 - recall from the last section we showed
29:50 - how the model.json file and
29:52 - corresponding weight files should be
29:53 - organized within our static directory
29:55 - that's being served by express
29:58 - we're first going to be working with
29:59 - vgg16 as our model so i've specified the
30:02 - url to where the model.json file for
30:04 - vgg16 resides
30:07 - now a tf.load model returns a promise
30:11 - meaning that this function promises to
30:12 - return the model at some point in the
30:14 - future
30:16 - this await keyword pauses the execution
30:18 - of this wrapping function until the
30:21 - promise is resolved and the model is
30:23 - loaded
30:24 - this is why we use the async keyword
30:27 - when defining this function because if
30:29 - we want to use the await keyword then it
30:31 - has to be contained within an async
30:34 - function
30:35 - now i've added a progress bar to the ui
30:37 - to indicate to the user when the model
30:40 - is loading
30:41 - as soon as the promise is resolved we're
30:43 - then hiding the progress bar from the ui
30:45 - which indicates the model is loaded
30:48 - before moving on let's quickly jump over
30:50 - to the html we developed last time so i
30:53 - can show you where i inserted this
30:54 - progress bar
30:56 - so here we are in predict with tfjs.html
31:00 - and you can see that right within this
31:01 - first div the container i've inserted
31:04 - this row where the progress bar is
31:05 - embedded
31:06 - we'll see it in action within the ui at
31:08 - the end of this video
31:10 - alright jumping back over to the
31:12 - javascript we now need to write the
31:14 - logic for what happens when the predict
31:16 - button is clicked when a user clicks the
31:19 - predict button we first get the image
31:20 - from the selected image element
31:23 - then we need to transform the image into
31:25 - a rank 4 tensor object of floats with
31:28 - height and width dimensions of 224 by
31:31 - 224 since that's what the model expects
31:34 - to do this we create a tensor object
31:36 - from the image by calling the
31:38 - tensorflow.js function tf dot from
31:41 - pixels and passing our image to it
31:44 - we then resize the image to 224 by 224
31:48 - cast the tensor's type to float 32
31:50 - and expand the tensor's dimensions to be
31:53 - of rank 4.
31:54 - we're doing all of this because the
31:56 - model expects the image data to be
31:58 - organized in this way and note that all
32:00 - of these transformations are occurring
32:02 - with calls to functions from the
32:04 - tensorflow.js api
32:06 - all right we have the tensor object of
32:08 - image data that the model expects
32:11 - now vgg16 actually wants the image data
32:14 - to be further pre-processed in a
32:16 - specific way beyond the basics we just
32:18 - completed
32:20 - there are transformations to the
32:21 - underlying pixel data that need to
32:23 - happen for this pre-processing that
32:24 - bgg16 wants
32:27 - in other libraries like keras
32:29 - pre-processing functions for specific
32:31 - models are included in the api
32:33 - currently though tensorflow.js does not
32:35 - have these pre-processing functions
32:37 - included so we need to build them
32:39 - ourselves
32:41 - we're going to build a preprocessing
32:42 - function in the next section to handle
32:44 - this
32:45 - so for right now what we'll do is pass
32:47 - in the image data contained in our
32:49 - tensor object as is to the model
32:52 - the model will still accept the data as
32:54 - input it just won't do a great job with
32:56 - its predictions since the data hasn't
32:58 - been processed in the same way as the
33:00 - images that bgg16 was originally trained
33:03 - on
33:04 - so we'll go ahead and get this app
33:06 - functional now
33:07 - and then we'll circle back around to
33:09 - handle the pre-processing in the next
33:10 - section and insert it appropriately then
33:13 - all right so a user clicks the predict
33:15 - button we transform the image data into
33:17 - a tensor and now we can pass the image
33:19 - to the model to get a prediction
33:21 - we do that by calling predict on the
33:23 - model and passing our tensor to it
33:26 - predict returns a tensor of the output
33:28 - predictions for the given input
33:31 - we then call data on the prediction
33:33 - sensor which asynchronously loads the
33:35 - values from the tensor and returns a
33:37 - promise of a typed array after the
33:39 - computation completes
33:41 - notice the await and async keywords here
33:44 - that we discussed earlier
33:46 - so this predictions array is going to be
33:48 - made up of 1000 elements each of which
33:51 - corresponds to the prediction
33:52 - probability for an individual imagenet
33:55 - class
33:56 - each index in the array maps to a
33:59 - specific imagenet class
34:01 - now we want to get the top 5 highest
34:03 - predictions out of all of these since
34:05 - that's what we'll be displaying in the
34:07 - ui
34:08 - we'll store these top five predictions
34:10 - in this top five variable top five top
34:12 - five top five
34:13 - before we sort and slice the array to
34:16 - get the top five we need to map the
34:18 - prediction values to their corresponding
34:20 - imagenet classes
34:22 - for each prediction in the array we
34:24 - return a javascript object that contains
34:27 - the probability and the imagenet class
34:29 - name
34:30 - notice how we use the index of each
34:32 - prediction to obtain the class name from
34:34 - the imagenet classes array that we
34:36 - imported from the imagenet classes
34:38 - javascript file
34:40 - we then sort the list of javascript
34:42 - objects by prediction probability in
34:44 - descending order and obtain the first
34:47 - five from the sorted list using the
34:49 - slice function
34:51 - lastly we iterate over the top five
34:53 - predictions and store the class names
34:55 - and corresponding prediction
34:57 - probabilities in the prediction list of
34:59 - our ui
35:01 - and that's it let's now start up our
35:03 - express server and browse to our app
35:08 - all right we're here and we've got
35:09 - indication that our model is loading
35:16 - so i paused the video while this model
35:18 - was continuing to load and it ended up
35:20 - taking about 40 seconds to complete
35:23 - not great it may even take longer for
35:26 - you depending on your specific computing
35:28 - resources remember though i said we'd
35:31 - run into some less than ideal situations
35:33 - with running such a large model like vgg
35:36 - 16 in the browser
35:38 - i warned you
35:39 - now
35:41 - so the time it takes to load the model
35:43 - is the first issue
35:45 - we've got over 500 megabytes of files to
35:47 - load into the browser for this model
35:49 - hence the long loading time
35:52 - alright well our model is loaded so
35:53 - let's choose an image and predict on it
36:08 - hmm about a five second wait time to get
36:10 - a prediction on a single image
36:16 - again not great
36:18 - oh and yeah the display prediction isn't
36:20 - accurate but that doesn't have anything
36:22 - to do with the model size or anything
36:24 - like that it's just because we didn't
36:26 - include the pre-processing for bgg16
36:28 - remember
36:29 - we're going to handle that in the next
36:31 - section there we'll get further exposure
36:33 - to the tensorflow.js api by exploring
36:36 - the tensor operations we'll need to work
36:37 - with to do the pre-processing
36:40 - all right so we've got that coming up
36:42 - and then afterwards we'll solve all
36:43 - these latency issues attributed to using
36:45 - a large model by substituting mobilenet
36:48 - in for vgg16
36:50 - let me know in the comments if you were
36:51 - able to get your app up and running and
36:53 - i'll see it in the next section
36:55 - in this section we're going to explore
36:57 - several tensor operations by
36:59 - pre-processing image data to be passed
37:01 - to a neural network running in our web
37:03 - app so let's get to it
37:11 - [Music]
37:12 - recall that last time we developed our
37:14 - web app to accept an image pass it to
37:17 - our tensorflow.js model and obtain a
37:19 - prediction
37:20 - for the time being we're working with
37:22 - vgg16 as our model and in the previous
37:25 - section we temporarily skipped over the
37:27 - image pre-processing that needed to be
37:29 - done for vgg16
37:31 - we're going to pick up with that now
37:33 - so we're going to get exposure to what
37:35 - specific pre-processing needs to be done
37:37 - for vgg16 yes but perhaps more
37:40 - importantly we'll get exposure to
37:42 - working with and operating on tensors
37:44 - we'll be further exploring the
37:46 - tensorflow.js library in order to do
37:48 - these tensor operations all right let's
37:51 - get into the code
37:52 - we're back inside of our predict js file
37:54 - and we're going to insert the vgg16
37:56 - pre-processing code right within the
37:58 - handler for the click event on the
38:00 - predict button
38:01 - we're getting the image in the same way
38:03 - we covered last time converting it into
38:05 - a tensor object using tf.from pixels
38:08 - resizing it to the appropriate 224x224
38:11 - dimensions and casting the type of the
38:13 - tensor to float32 no change here so far
38:17 - all right now let's discuss the
38:19 - pre-processing that needs to be done for
38:21 - vgg16
38:23 - this paper authored by the creators of
38:25 - vgg16 discusses the details the
38:28 - architecture and the findings of this
38:30 - model
38:31 - we're interested in finding out what
38:32 - pre-processing they did on the image
38:35 - data
38:36 - jumping to the architecture section of
38:37 - the paper the authors state quote the
38:40 - only pre-processing we do is subtracting
38:42 - the mean rgb value computed on the
38:44 - training set from each pixel let's break
38:47 - this down a bit
38:48 - we know that imagenet was the training
38:50 - set for vgg16 so imagenet is the data
38:53 - set for which the mean rgb values are
38:55 - calculated to do this calculation for a
38:58 - single color channel say red we compute
39:01 - the average red value of all the pixels
39:04 - across every imagenet image
39:06 - the same goes for the other two color
39:08 - channels green and blue
39:10 - then to pre-process each image we
39:12 - subtract the mean red value from the
39:14 - original red value in each pixel we do
39:17 - the same for the green and blue values
39:19 - as well
39:20 - this technique is called zero centering
39:23 - because it forces the mean of the given
39:25 - data set to be zero
39:27 - so we're zero centering each color
39:29 - channel with respect to the imagenet
39:31 - data set
39:32 - now aside from zero centering the data
39:34 - we also have one more pre-processing
39:36 - step not mentioned here the authors
39:39 - trained vgg16 using the cafe library
39:42 - which uses a bgr color scheme for
39:44 - reading images rather than rgb
39:47 - so as a second preprocessing step we
39:49 - need to reverse the order of each pixel
39:51 - from rgb to bgr
39:54 - alright now that we know what we need to
39:56 - do let's jump back in the code and
39:57 - implement it
39:59 - we first define a javascript object mean
40:01 - imagenet rgb which contains the mean red
40:05 - green and blue values from imagenet
40:07 - we then define this list we're calling
40:09 - indices the name will make sense in a
40:12 - minute
40:13 - this list is made up of one-dimensional
40:15 - tensors of integers created with
40:17 - tf.tensor1d
40:19 - the first tensor in the list contains
40:21 - the single value zero the second tensor
40:24 - contains the single value 1 and the
40:26 - third tensor contains the single value
40:28 - 2.
40:29 - we'll be making use of these sensors in
40:30 - the next step
40:32 - here we have this javascript object
40:34 - we're calling centered rgb which
40:36 - contains the centered red green and blue
40:38 - values for each pixel in our selected
40:40 - image
40:42 - let's explore how we're doing this
40:43 - centering
40:44 - recall that we have our image data
40:46 - organized now into a 224 by 224x3 tensor
40:50 - object so to get the centered red values
40:52 - for each pixel in our tensor we first
40:55 - use the tensorflow.js function tf.gather
40:58 - to gather all the red values from the
41:00 - tensor
41:01 - specifically tf.gather is gathering each
41:04 - value from the zeroth index along the
41:06 - tensor's second axis
41:09 - each element along the second axis of
41:10 - our 224 by 224 by 3 tensor represents a
41:14 - pixel containing a red green and blue
41:16 - value in that order so the zeroth index
41:19 - in each of these pixels is the red value
41:22 - of the pixel
41:24 - after gathering all the red values we
41:25 - need to center them by subtracting the
41:27 - mean imagenet red value from each red
41:30 - value in our tensor
41:32 - to do this we use the tensorflow.js sub
41:35 - function which will subtract the value
41:37 - passed to it from each red value in the
41:39 - tensor
41:40 - it will then return a new tensor with
41:42 - those results
41:44 - the value we're passing to sub is the
41:46 - mean red value from our mean imagenet
41:48 - rgb object but we're first transforming
41:51 - this raw value into a scalar object by
41:53 - using the tf.scalar function
41:57 - alright so now we've centered all the
41:58 - red values but at this point the tensor
42:01 - we've created that contains all of these
42:02 - red values is of shape 224 by 224 by 1.
42:06 - we want to reshape this tensor to just
42:08 - be a one-dimensional tensor containing
42:10 - all 50
42:12 - 176 red values so we do that by
42:15 - specifying this shape to the reshape
42:17 - function
42:19 - great now we have a one-dimensional
42:21 - tensor containing all the centered red
42:22 - values from every pixel in our original
42:25 - tensor
42:26 - we need to go through this same process
42:27 - now again to get the centered green and
42:29 - blue values
42:31 - at a brief glance you can see the code
42:33 - is almost exactly the same as what we
42:34 - went through for the red values
42:36 - the only exceptions are the indices
42:38 - we're passing to tf.gather and the mean
42:41 - imagenet values we're passing to
42:42 - tf.scalar
42:44 - at this point we now have this centered
42:46 - rgb object that contains a
42:48 - one-dimensional tensor of centered red
42:50 - values a one-dimensional tensor of
42:52 - centered green values and a
42:54 - one-dimensional tensor of centered blue
42:56 - values
42:57 - we now need to create another tensor
42:59 - object that brings all of these
43:00 - individual red green and blue tensors
43:02 - together into a 224 by 224x3 tensor this
43:06 - will be the pre-processed image
43:09 - so we create this process tensor by
43:12 - stacking the centered red centered green
43:14 - and centered blue tensors along axis one
43:17 - the shape of this new tensor is going to
43:19 - be of 50
43:20 - 176 by 3.
43:23 - this sensor represents 50 176 pixels
43:26 - each containing a red green and blue
43:28 - value
43:29 - we need to reshape this sensor to be in
43:31 - the form that the model expects which is
43:33 - 224 by 224 by 3.
43:36 - now remember at the start we said that
43:38 - we need to reverse the order of the
43:39 - color channels of our image from rgb to
43:42 - bgr so we do that using the
43:44 - tensorflow.js function reverse to
43:46 - reverse our tensor along the specified
43:48 - axis
43:50 - lastly we expand the dimensions to
43:52 - transform the tensor from rank 3 to rank
43:54 - 4 since that's what the model expects
43:58 - okay now we have our pre-processed image
44:01 - data in the form of this pre-processed
44:03 - tensor object so we can pass this
44:05 - pre-processed image to our model to get
44:07 - a prediction
44:08 - before we do that though note that we
44:11 - handled these tensor operations in a
44:13 - specific way and a specific order to
44:15 - pre-process the image it's important to
44:17 - know though that this isn't the only way
44:19 - we could have achieved this
44:21 - in fact there's a much simpler way
44:23 - through a process called broadcasting
44:25 - that could achieve the same process
44:27 - tensor at the end
44:28 - you gotta be kidding me don't worry
44:30 - we're going to be covering broadcasting
44:32 - in a future section but i thought that
44:34 - for now doing these kind of exhaustive
44:36 - tensor operations would be a good
44:38 - opportunity for us to explore the
44:39 - tensorflow.js api further and get more
44:42 - comfortable with tensors in general
44:45 - checking out our app using the same
44:47 - image as last time we can now see that
44:48 - the model gives us an accurate
44:50 - prediction on the image since the image
44:52 - has now been processed appropriately
44:55 - now i don't know about you but tensor
44:57 - operations like the ones we worked with
44:58 - here are always a lot easier for me to
45:00 - grasp when i can visualize what the
45:02 - tensor looks like before and after the
45:04 - transformation
45:05 - so in the next section we're going to
45:07 - step through this code using the
45:09 - debugger to visualize each tensor
45:11 - transformation that occurs during
45:13 - pre-processing i'll see you there
45:16 - in this section we're going to continue
45:18 - our exploration of tensors here we'll be
45:21 - stepping through the code we developed
45:22 - last time with the debugger to see the
45:24 - exact transformations that are happening
45:26 - to our tensors in real time so let's get
45:29 - to it
45:35 - [Music]
45:38 - last time we went through the process of
45:39 - writing the code to pre-process images
45:42 - for vgg16 through that process we gained
45:45 - exposure to working with tensors
45:47 - transforming and manipulating them
45:49 - we're now going to step through these
45:51 - tensor operations with the debugger so
45:53 - that we can see these transformations
45:54 - occur in real time as we interact with
45:56 - our app
45:58 - if you're not already familiar with
45:59 - using a debugger don't worry you'll
46:01 - still be able to follow we'll first go
46:03 - through this process using the debugger
46:05 - and visual studio code then we'll demo
46:07 - the same process using the debugger
46:09 - built into the chrome browser
46:11 - we're here within our predict.js file
46:13 - within the click event for the predict
46:15 - button where all the preprocessing code
46:17 - is written
46:18 - we're placing a breakpoint in our code
46:20 - where our first sensor is defined
46:22 - remember this is where we're getting the
46:23 - selected image and transforming it into
46:26 - a tensor using tf.from pixels
46:29 - the expectation around this breakpoint
46:31 - is when we browse to our app the model
46:33 - will load we'll select an image and
46:36 - click the predict button once we click
46:38 - predict this click event will be
46:40 - triggered and will hit this breakpoint
46:42 - when this happens the code execution
46:44 - will be paused until we tell it to
46:46 - continue to the next step
46:48 - this means that while we're paused we
46:50 - can inspect the tensors we're working
46:51 - with and see how they look before and
46:53 - after any given operation
46:55 - let's see
46:57 - we'll start our debugger in the top left
46:59 - of the window which will launch our app
47:00 - in chrome
47:05 - alright we can see our model is loading
47:09 - okay the model is loaded let's select an
47:11 - image
47:16 - now let's click the predict button and
47:18 - when we do this we'll see our break
47:19 - point will get hit and the app will
47:21 - pause
47:24 - and here we go our code execution is now
47:26 - paused we'll minimize the browser and
47:28 - expand our code window now since this is
47:30 - where we'll be debugging
47:32 - we're currently paused at this line
47:34 - where we define our tensor object we're
47:36 - going to click this step over icon which
47:38 - will execute this code where we're
47:40 - defining tensor and we'll pause at the
47:42 - next step let's see
47:46 - all right we're now paused at the next
47:47 - step
47:48 - now that tensor has been defined let's
47:50 - inspect it a bit first we have this
47:52 - variables panel over in the left where
47:54 - we can check out information about the
47:56 - variables in our app and we can see our
47:58 - tensor variable is here in this list
48:00 - clicking tensor we can see we have all
48:03 - types of information about this object
48:06 - for example we can see the d type is
48:08 - float32 the tensor is of rank three the
48:11 - shape is 224 by 224 by three and the
48:14 - size is a hundred and fifty thousand and
48:16 - five twenty eight so we get a lot of
48:18 - information describing this guy
48:20 - additionally in the debug console we can
48:22 - play with this sensor further for
48:24 - example let's print it using the
48:25 - tensorflow.js print function
48:30 - and we'll scroll up a bit and we can see
48:32 - that this kind of lets us get a summary
48:34 - of the data contained in this tensor
48:37 - remember we made this tensor have shape
48:39 - 224 by 224x3
48:41 - so looking at this output we can
48:43 - visualize this tensor as an object with
48:46 - 224 rows each of which is 224 pixels
48:49 - across and each of those pixels contains
48:52 - a red green and blue value
48:54 - so what's selected here represents one
48:56 - of those 224 rows and each one of these
49:00 - are one of the
49:01 - pixels in this row
49:03 - and each of these pixels contains first
49:05 - a red then a green then a blue value
49:08 - so make sure you have a good grip on
49:09 - this idea so you can follow all the
49:11 - transformations this tensor is about to
49:13 - go through
49:14 - all right our debugger is paused on
49:16 - defining the mean imagenet rgb object
49:19 - let's go ahead and step over this so
49:20 - that it gets defined
49:23 - again we can now inspect this object
49:24 - over in the local variables panel
49:28 - we're not doing any tensor operations
49:29 - here so let's go ahead and move on
49:33 - we're now paused on our list of rank one
49:35 - tensors called indices which we'll make
49:37 - use of later so let's execute this
49:41 - we can see indices now shows up in our
49:43 - local variable panel
49:45 - let's inspect this one a bit from the
49:46 - debug console
49:49 - if we just print out this list using
49:51 - console.log indices
49:58 - get back that this is an array with
49:59 - three things in it we know that each
50:01 - element in this array is a tensor so
50:03 - let's access one of them let's get the
50:06 - first
50:10 - tensor and it might help if we spell
50:13 - indices correctly so let's try that
50:14 - again
50:19 - we get back that this object is a tensor
50:21 - and we can see what it looks like just a
50:23 - one-dimensional tensor with a single
50:24 - value zero and we can easily do the same
50:27 - thing for the second and third elements
50:28 - in the list too
50:30 - all right we're going to minimize this
50:31 - panel on the left now and scroll up some
50:33 - in our code
50:36 - we're now paused where we're defining
50:38 - the centered rgb object and from last
50:40 - time we know that's where the bulk of
50:41 - our tensor operations are occurring
50:44 - so if we execute this block then we'll
50:46 - skip over being able to inspect each of
50:48 - these transformations so what we'll do
50:50 - is we'll stay paused here but in the
50:52 - debugger console we'll mimic each of
50:54 - these individual transformations one by
50:56 - one so we can see the before and after
50:59 - version of the tensor
51:00 - so for example we're first going to
51:02 - mimic what's happening here with the
51:04 - creation of the tensor that contains all
51:06 - the centered red values within our
51:08 - centered rgb object
51:10 - in the console we'll create this
51:11 - variable called red and set it equal to
51:13 - just the first call to tf.gather and see
51:16 - what it looks like
51:18 - so we'll go ahead and copy this call
51:21 - and we'll create a variable red and set
51:23 - it equal to that
51:26 - before we do any other operations let's
51:28 - see what this looks like let's first
51:30 - check the shape of red
51:34 - okay 224 by 224 by one so similar to
51:38 - what we saw from the original tensor of
51:40 - 224 by 224 by 3 but rather than the last
51:43 - dimension containing all three pixel
51:45 - values red green and blue our new red
51:47 - tensor only contains the red pixel
51:49 - values
51:50 - let's print red
51:54 - and let's scroll up so that we can see
51:56 - the start of the tensor
51:57 - and just to hit the point home let's
51:59 - compare this to the original tensor so
52:02 - the first three values in red are 56 58
52:05 - and 59
52:06 - now let's scroll up and check out the
52:08 - original tensor to see if this lines up
52:10 - so 56 58 59
52:14 - scrolling up to our original tensor and
52:16 - yep our original tensor has the red
52:18 - values of 56 58 and 59 in the first
52:21 - three zeroth indices along the second
52:24 - axis so red is just made up of each of
52:27 - these values
52:28 - all right let's scroll back down in our
52:30 - debug console and let's see what the
52:33 - next operation on red is
52:35 - this is where we're centering each red
52:37 - value by subtracting the mean red value
52:39 - from imagenet using this sub function
52:42 - let's make a new variable called
52:43 - centered red and mimic this operation
52:46 - so we'll define centered red
52:51 - equal to red
52:52 - and then call this sub function
52:55 - now let's print centered red
53:00 - and scroll up to the top
53:03 - okay so about minus 67 minus 65 and
53:07 - minus 64 for the first three values
53:09 - along the second axis let's compare this
53:11 - to the original red tensor now by
53:13 - scrolling up to look at that
53:15 - and these are 56 58 and 59 as the first
53:19 - three values along the second axis
53:21 - so if we do the quick math of
53:22 - subtracting the mean red value of
53:25 - 123.68 and remember we can see that by
53:28 - looking here
53:30 - 123.68 as our mean red value in the mean
53:33 - imagenet rgb object subtracting this
53:35 - number from the first three values of
53:37 - our original red tensor we do indeed end
53:40 - up with the centered red values in the
53:41 - new centered red tensor we just looked
53:43 - at
53:44 - now centered red still has the same
53:46 - shape as red which recall is 224 by 224
53:49 - by 1. the next step is to reshape the
53:52 - sensor to be a rank one tensor of size
53:54 - fifty 50176
53:57 - so we just want to bring all the
53:59 - centered red values together which are
54:00 - currently each residing in their own
54:02 - individual tensors
54:04 - so to mimic this reshape call we'll make
54:06 - a new variable called reshaped red
54:08 - so we'll scroll back down in our
54:10 - debugger console
54:12 - and we'll copy this reshape call
54:15 - and we'll define reshaped red
54:18 - equal to centered red
54:21 - and then call reshape on that
54:24 - all right let's check the shape on this
54:26 - new object to get confirmation
54:29 - and we see it is indeed the shape that
54:31 - we specified
54:33 - let's now look at the printout of
54:34 - reshaped red
54:38 - okay and we see all the red values are
54:40 - now listed out here in this
54:42 - one-dimensional tensor
54:44 - all right so that's it for getting all
54:45 - the centered red values as mentioned
54:47 - last time we go through the same process
54:49 - to gather all the blues and greens as
54:51 - well so we're not going to go through
54:52 - that in the debugger
54:54 - we'll now execute this block of code to
54:56 - create this centered rgb object and move
54:58 - on to the next step
55:01 - this is where we're bringing our
55:02 - centered red green and blue values all
55:05 - together into a new processed tensor
55:08 - so from the console let's run this first
55:10 - stack operation by creating a variable
55:12 - called stacked tensor
55:14 - so i'll create stack tensor
55:17 - set that equal to this stack call
55:23 - remember we just saw that reshaped red
55:25 - ended up being a rank 1 tensor of shape
55:27 - 50
55:28 - 176
55:29 - the green and blue tensors have the same
55:31 - shape and size so when we stack them
55:33 - along axis 1 we should now have a 50 176
55:37 - by 3 tensor you may think the result of
55:40 - the stack operation would look like this
55:42 - where we have the centered red tensor
55:44 - with its 50 176 values stacked on top of
55:48 - the green tensor stacked on top of the
55:50 - blue tensor
55:51 - and that's how it would look if we were
55:53 - stacking along axis zero
55:55 - because we're stacking along axis one
55:57 - though we'll get something that looks
55:59 - like this where we have fifty 50 176
56:02 - rows each of which is made up of a
56:04 - single pixel with a red green and blue
56:07 - value
56:08 - let's check the shape now in the console
56:10 - to be sure we get the 50 0176 by 3 we
56:13 - expect
56:17 - yep we do let's also print it to get a
56:19 - visual
56:23 - okay so we have 50 176 rows each
56:27 - containing a red green and blue value
56:30 - now we need to reshape this guy to be of
56:32 - shape 224 by 224 by 3 before we can pass
56:35 - it to the model so let's do that now
56:37 - with a new variable we'll call reshaped
56:39 - tensor
56:40 - so we'll copy the reshape call from over
56:43 - here and define reshaped tensor
56:46 - [Applause]
56:48 - equal to
56:51 - our stacked tensor dot reshape
56:55 - okay let's print this reshape tensor
57:00 - and scroll up to the top
57:02 - again this shape means we have 224 rows
57:05 - each containing 224 pixels which each
57:08 - contain a red green and blue value
57:10 - now we need to reverse the values in
57:12 - this tensor along the second axis from
57:14 - rgb to bgr for the reasons we mentioned
57:17 - last time
57:18 - so we'll copy this reverse call here
57:21 - and we'll make a new object called
57:22 - reversed tensor
57:27 - and set that equal to our reshape tensor
57:31 - dot reverse
57:34 - and we need to scroll down in our debug
57:36 - console
57:37 - and let's print this one out
57:42 - and scroll up to the top of it
57:44 - okay so we see the first bgr values now
57:47 - let's scroll up to our last sensor to
57:49 - make sure this is the reverse of the rgb
57:51 - values we had there so minus 99 minus 87
57:55 - minus 67
57:56 - scrolling up we have minus 99 minus 87
58:00 - minus 67.
58:02 - so indeed our new tensor has the
58:03 - reversed rgb values
58:06 - let's scroll back down in our debugger
58:08 - and our last operation is expanding the
58:10 - dimensions of our tensor to make it go
58:12 - from rank 3 to a rank 4 tensor which is
58:14 - what our model requires
58:16 - so we'll create a new tensor called
58:18 - expanded
58:20 - tensor and set that equal to reverse
58:23 - tensor
58:25 - and we'll copy the expand dim call from
58:28 - over here
58:29 - and call that on our reverse sensor
58:32 - all right now let's check the shape of
58:33 - this guy to make sure it's what we
58:35 - expect
58:38 - so we have this inserted dimension at
58:40 - the start now making our tensor rank
58:42 - four with shape one by two twenty four
58:45 - by two twenty four by three rather than
58:47 - just two twenty four by two twenty four
58:48 - by 3 that we had last time
58:51 - and if we print this out
58:56 - and scroll up to the start
58:58 - we can see this extra dimension added
59:00 - around our previous tensor
59:02 - and that sums up all the tensor
59:04 - operations
59:05 - quickly though in case you're not using
59:07 - visual studio code i did want to also
59:09 - show this same setup directly within the
59:12 - chrome browser so that you can do your
59:13 - debugging there instead if you'd prefer
59:16 - in chrome we can right click on our page
59:18 - click inspect
59:20 - and then go to the sources tab
59:22 - here we have access to the source code
59:24 - for our app predict.js is currently
59:27 - being shown in the open window so now we
59:29 - have access to the exact code we were
59:31 - displaying in visual studio code and we
59:33 - can insert breakpoints here in the same
59:35 - way as well
59:36 - let's go ahead and put a breakpoint in
59:38 - the same place as we did earlier
59:42 - now let's select an image and click the
59:43 - predict button
59:48 - we see that our app is paused at our
59:50 - breakpoint and then we can step over the
59:52 - code just as we saw earlier
60:00 - and we have our variables all showing in
60:02 - this panel here
60:05 - and we also have our console down here
60:08 - so i can do
60:09 - indices
60:11 - zero dot print for example
60:15 - to get that same output that we got in
60:17 - visual studio debugger and from this
60:19 - console i can run all the same code that
60:21 - we ran in visual studio code as well
60:24 - alright hopefully now you have a decent
60:26 - grasp on tensors and tensor operations
60:28 - let me know what you thought of going
60:30 - through this practice in the debugger to
60:31 - see how the tensor's changed over time
60:33 - with each operation and i'll see it in
60:35 - the next section
60:38 - in this section we'll learn about
60:39 - broadcasting and illustrate its
60:41 - importance and major convenience when it
60:43 - comes to tensor operations so let's get
60:46 - to it
60:46 - [Music]
60:52 - [Music]
60:54 - over the last couple of sections we've
60:56 - immersed ourselves in tensors and
60:58 - hopefully now we have a good
61:00 - understanding of how to work with
61:01 - transform and operate on them if you
61:04 - recall a couple sections back i
61:05 - mentioned the term broadcasting and said
61:08 - that we would later make use of it to
61:09 - vastly simplify our vgg g16
61:11 - pre-processing code
61:13 - before we get into the details about
61:15 - what broadcasting is though let's get a
61:17 - sneak peek of what our transform code
61:19 - will look like once we've introduced
61:21 - broadcasting
61:23 - because i'm using git for source
61:24 - management i can see the diff between
61:26 - our original predict.js file and the
61:29 - modified version of this file that uses
61:31 - broadcasting
61:32 - on the left we have our original
61:34 - predict.js file within the click event
61:37 - recall this is where we transformed our
61:39 - image into a tensor then the rest of
61:41 - this code was all created to do the
61:43 - appropriate pre-processing for vgg16
61:45 - where we centered and reversed the rgb
61:48 - values
61:49 - now on the right this is our new and
61:51 - improved predict.js file that makes use
61:53 - of broadcasting in place of all the
61:56 - explicit one by one tensor operations on
61:58 - the left so look all of this code in red
62:01 - has now been replaced with what's shown
62:03 - in green
62:04 - that's a pretty massive reduction of
62:06 - code
62:07 - before we show how this happened we need
62:08 - to understand what broadcasting is
62:11 - broadcasting describes how tensors with
62:13 - different shapes are treated during
62:15 - arithmetic operations
62:17 - for example it might be relatively easy
62:19 - to look at these two rank two tensors
62:22 - and figure out what the sum of them
62:24 - would be
62:25 - they have the same shape so we just take
62:27 - the element y sum of the two tensors
62:29 - where we calculate the sum element by
62:30 - element and here we go we have our
62:33 - resulting tensor
62:34 - now since these two tensors have the
62:36 - same shape one by three no broadcasting
62:39 - is happening here
62:40 - remember broadcasting comes into play
62:42 - when we have tensors with different
62:44 - shapes
62:45 - alright so what would happen if our two
62:47 - rank two tensors instead looked like
62:49 - this and we wanted to sum them
62:51 - we have one with shape one by three and
62:53 - the other was shaped three by one
62:56 - well here's where broadcasting will come
62:57 - into play
62:58 - before we cover how this is done go
63:00 - ahead and pause the video and just see
63:02 - intuitively what comes to mind as the
63:04 - resulting tensor from adding these two
63:05 - together
63:06 - give it a go write it down and keep what
63:08 - you write handy because we'll circle
63:10 - back around to what you wrote later in
63:11 - the video
63:13 - all right we're first going to look at
63:14 - the result and then we'll go over how we
63:16 - arrive there
63:18 - our result from summing these two
63:20 - tensors is a three by three tensor
63:22 - so here's how broadcasting works
63:25 - we have two tensors with different
63:26 - shapes the goal of broadcasting is to
63:28 - make the tensors have the same shape so
63:30 - we can perform element-wise operations
63:32 - on them
63:33 - first we have to see if the operation
63:35 - we're trying to do is even possible
63:37 - between the given tensors
63:39 - based on the tensor's original shapes
63:41 - there may not be a way to reshape them
63:43 - to force them to be compatible and if we
63:45 - can't do that then we can't use
63:46 - broadcasting
63:48 - the rule to see if broadcasting can be
63:50 - used is this we compare the shapes of
63:52 - the two tensors starting at their last
63:55 - dimensions and working backwards
63:57 - our goal is to determine whether or not
63:59 - each dimension between the two tensor
64:02 - shapes is compatible
64:04 - in our example we have shapes three by
64:06 - one and one by three so we first compare
64:09 - the last dimensions
64:10 - the dimensions are compatible when
64:12 - either a they're equal to each other or
64:15 - b one of them is one
64:18 - comparing the last dimensions of the two
64:19 - shapes we have a one and a three
64:22 - are these compatible well let's check
64:24 - the rule are they equal no one doesn't
64:28 - equal three
64:29 - is one of them one
64:30 - yes
64:31 - great the last dimensions are compatible
64:34 - working our way to the front for the
64:36 - next dimension we have a three and a one
64:39 - similar story just switched order right
64:42 - so are these compatible
64:44 - yes
64:45 - okay that's the first step we've
64:47 - confirmed each dimension between the two
64:49 - shapes is compatible
64:51 - if however while comparing the
64:53 - dimensions we confirmed that at least
64:55 - one dimension wasn't compatible then we
64:57 - would cease our efforts there because
64:59 - the arithmetic would not be possible
65:01 - between the two
65:02 - now since we've confirmed that our two
65:04 - tensors are compatible we can sum them
65:06 - and use broadcasting to do it
65:09 - when we sum two tensors the result of
65:11 - this sum will be a new tensor
65:13 - our next step is to find out the shape
65:15 - of this resulting tensor
65:17 - we do that by again comparing the shapes
65:20 - of the original tensors let's see
65:22 - exactly how this is done
65:24 - comparing the shape of one by three to
65:26 - three by one we first calculate the max
65:29 - of the last dimension
65:30 - the max of three and one is three
65:33 - three will be the last dimension of the
65:35 - shape of the resulting tensor
65:38 - moving on to the next dimension
65:40 - again the max of one and three is three
65:43 - so three will be the next dimension of
65:45 - the shape of the resulting tensor
65:47 - we've now stepped through each dimension
65:49 - of the shapes of the original tensors
65:51 - and we can conclude that the resulting
65:53 - tensor will have shape three by three
65:56 - the original tensors of shape one by
65:58 - three and three by one will now be
66:00 - expanded to shape three by three also in
66:02 - order to do the element-wise operation
66:05 - broadcasting can be thought of as
66:07 - copying the existing values within the
66:10 - original tensor and expanding that
66:12 - tensor with these copies until it
66:14 - reaches the required shape
66:18 - the values in our 1x3 tensor will now be
66:21 - broadcast to this 3x3 tensor
66:24 - and the values in our 3x1 tensor will
66:26 - now be broadcast to this 3x3 tensor
66:30 - we can now easily take the element-wise
66:32 - sum of these two to get this resulting
66:35 - three by three tensor
66:36 - let's do another example
66:39 - what if we wanted to multiply this rank
66:41 - two tensor of shape one by three with
66:43 - this rank zero tensor better known as a
66:45 - scalar we can do this since there's
66:48 - nothing in the broadcasting rules
66:49 - preventing us from operating on two
66:51 - tensors of different ranks
66:53 - let's see
66:54 - we first compare the last dimensions of
66:56 - the two shapes
66:57 - when we're in a situation where the
66:59 - ranks of the two tensors aren't the same
67:01 - like what we have here then we simply
67:03 - substitute a one in for the missing
67:05 - dimensions of the lower ranked tensor
67:08 - in our example we substitute a one here
67:11 - then we ask are these two dimensions
67:13 - compatible and the answer will always be
67:15 - a yes in this type of situation since
67:17 - one of them will always be a one
67:20 - all right all the dimensions are
67:21 - compatible
67:23 - so what will the resulting tensor look
67:24 - like from multiplying these two together
67:27 - again go ahead and pause here and try
67:29 - yourself before getting the answer
67:31 - well the max of three and one is three
67:34 - and the max of one and one is one so our
67:37 - resulting tensor will be of shape one by
67:39 - three
67:40 - our first sensor is already this shape
67:42 - so it gets left alone
67:44 - our second tensor is now expanded to
67:46 - this shape by broadcasting its value
67:48 - like this
67:50 - now we can do our element-wise
67:51 - multiplication to get this resulting one
67:54 - by three tensor
67:55 - let's do one more example
67:57 - what if we wanted to sum this rank 3
68:00 - tensor of shape 1 by 2 by 3 and this
68:03 - rank 2 tensor of shape 3x3
68:06 - before covering any of the incremental
68:07 - steps go ahead and give it a shot
68:09 - yourself and see what you find out
68:11 - alright assuming you've now paused and
68:13 - resumed the video the deal with these
68:14 - two tensors is that we can't operate on
68:17 - them
68:17 - why
68:18 - well comparing the second to last
68:20 - dimensions of the shapes they're not
68:22 - equal to each other and neither one of
68:24 - them is one so we stop there
68:27 - all right and now we should have a good
68:28 - grip on broadcasting let's go see how
68:31 - we're able to make use of it in our vgg
68:32 - g16 pre-processing code
68:35 - first we can see we're changing our mean
68:38 - imagenet rgb object into a rank 1 tensor
68:41 - which makes sense right because we're
68:43 - going to be making use of broadcasting
68:44 - which is going to require us to work
68:46 - with tensors not arbitrary javascript
68:48 - objects
68:50 - alright now get a load of this remaining
68:52 - code
68:53 - all of this code was written to handle
68:55 - the centering of the rgb values
68:57 - this has now all been replaced with this
68:59 - single line which is simply the result
69:01 - of subtracting the mean imagenet rgb
69:03 - tensor from the original tensor okay so
69:06 - why does this work and where is the
69:08 - broadcasting let's see
69:10 - our original tensor is a rank 3 tensor
69:12 - of shape 224 by 224x3
69:16 - our mean imagenet rgb tensor is a rank 1
69:18 - tensor of shape 3.
69:21 - our objective is to subtract each mean
69:23 - rgb value from each rgb value along the
69:26 - second axis of the original tensor
69:29 - from what we've learned about
69:30 - broadcasting we can do this really
69:32 - easily
69:33 - we compare the dimensions of the shapes
69:35 - from each tensor and confirm they're
69:36 - compatible the last dimensions are
69:38 - compatible because they're equal to each
69:40 - other
69:41 - the next two dimensions are compatible
69:43 - because we substitute a one in for the
69:44 - missing dimensions in our rank one
69:46 - tensor taking the max across each
69:49 - dimension our resulting tensor will be
69:51 - of shape 224 by 224 by 3.
69:54 - our original tensor already has that
69:56 - shape so we leave it alone
69:58 - our rank 1 tensor will be expanded to
70:00 - the shape of 224 by 224 by 3 by copying
70:03 - its three values along the second axis
70:07 - so now we can easily do the element-wise
70:09 - subtraction between these two tensors
70:12 - exiting out of this diff and looking at
70:14 - the modified predict js file alone we
70:17 - have this
70:18 - so the reversing and the expanding of
70:20 - the dims at the end is still occurring
70:21 - in the same way after the centering
70:24 - now actually if we wanted to make this
70:26 - code even more concise rather than
70:28 - creating two tensor objects our original
70:30 - one and our pre-processed one we can
70:32 - chain all these calls together to
70:34 - condense these two separate tensors into
70:36 - one
70:37 - we would first need to bring our mean
70:39 - imagenet rgb definition above our tensor
70:41 - definition
70:43 - then we need to move our sub reverse and
70:45 - expand dim calls up and change them to
70:48 - the original tensor
70:50 - lastly we replace this reference to
70:51 - process tensor with just tensor and
70:54 - that's it
70:55 - so if you took the time to truly
70:57 - understand the tensor operations we went
70:58 - through step by step in the last couple
71:00 - of sections then you should now be
71:02 - pretty blown away by how much easier
71:04 - broadcasting can make our lives and our
71:06 - code given this do you see the value in
71:09 - broadcasting let me know in the comments
71:12 - oh also remember all those times i asked
71:14 - you to pause the video and record your
71:15 - answers to the examples we were going
71:17 - through let me know what you got and
71:19 - don't be embarrassed if you were wrong i
71:21 - was wrong when i tried to figure out
71:22 - examples like these when i first started
71:24 - learning broadcasting so no shame let me
71:26 - know and i'll see you in the next
71:28 - section
71:30 - in this section we'll be adding new
71:32 - functionality to our deep learning web
71:34 - application to increase its speed and
71:36 - performance specifically we'll see how
71:39 - we can do this by switching models so
71:41 - let's get to it
71:44 - [Music]
71:49 - we currently have a web app that allows
71:51 - users to select and submit an image and
71:54 - subsequently receive a prediction for
71:55 - the given image up to this point we've
71:58 - been using vgg16 as our model
72:00 - bgg16 gets the job done when it comes to
72:02 - giving accurate predictions on the
72:04 - submitted images however as we've
72:06 - previously discussed a model of its size
72:08 - over 500 megabytes is not ideal for
72:11 - running in the browser
72:12 - because of this we've seen a decent time
72:14 - delay in both loading the model as well
72:16 - as obtaining predictions from the model
72:18 - well we're in luck because we'll now
72:20 - make use of a much smaller model
72:22 - mobilenet which is pretty ideal size
72:24 - wise for running in the browser coming
72:26 - in at around 16 megabytes
72:28 - with mobilenet we'll see a vast decrease
72:30 - in time for both loading the model and
72:32 - obtaining predictions
72:34 - let's go ahead and get into the code to
72:35 - see what modifications we need to make
72:38 - all right we're here in our predict with
72:40 - tfjs.html file and we're going to make a
72:43 - model selector where the user has the
72:45 - ability to choose which model to use for
72:47 - now we'll have vgg16 and mobilenet as
72:50 - available options currently the call to
72:52 - load the model occurs immediately when
72:54 - the web page is requested but now we'll
72:57 - change that functionality so that the
72:58 - model will be loaded once a user selects
73:00 - which model they'd like to use
73:02 - our model selector will take on the form
73:04 - of an html select element so the first
73:07 - thing we need to do is add this element
73:09 - to our html within the same row as the
73:11 - image selector and the predict button
73:13 - we're adding this new select element
73:15 - within a column to the left of both of
73:16 - the previously mentioned elements
73:19 - when a user shows up to the page the
73:20 - model selector will be set to the option
73:22 - that states select model and they'll
73:24 - have the option to choose either
73:26 - mobilenet or vgg16
73:28 - now also recall how we mentioned that
73:30 - until now the model was being loaded
73:32 - immediately when a user arrived at the
73:34 - page and during that time the progress
73:36 - bar would show to indicate the loading
73:38 - since we'll be changing the
73:39 - functionality so that the model isn't
73:41 - loaded until a user chooses which model
73:43 - they want to use we won't need the
73:45 - progress bar to show until that model is
73:46 - selected
73:48 - so navigating to the progress bar
73:50 - element we're going to set the display
73:51 - style attribute to none which will hide
73:53 - the progress bar until we explicitly
73:55 - instruct it to be shown in the
73:56 - javascript code
73:58 - alright that's it for the changes to our
74:00 - html jumping to predict.js will now
74:03 - specify what should happen once a user
74:05 - selects a model when a model is selected
74:07 - this will trigger a change event on the
74:09 - model selector we're handling this event
74:11 - by calling a new function which we'll
74:13 - discuss in a moment called load model
74:15 - load model essentially does what it
74:17 - sounds like it does
74:19 - we pass this function the value from
74:20 - model selector which is either going to
74:22 - be mobilenet or vgg16
74:25 - do you remember how previously we were
74:27 - loading the model using an immediately
74:29 - invoked function expression or iife
74:31 - well now that we don't want to load the
74:33 - model until we explicitly call load
74:35 - model like we just specified we no
74:37 - longer want this loading to happen
74:38 - within an iife
74:40 - the code for load model is actually
74:42 - super similar to the iife we had before
74:45 - just with some minor adjustments
74:47 - load model accepts the name of the model
74:49 - to be loaded
74:50 - once called the progress bar will be
74:52 - shown to indicate the model is loading
74:54 - we initially set the model to undefined
74:56 - so that in case we're in a situation
74:57 - where we're switching from one model to
74:59 - another the previous model can be
75:01 - cleared from memory
75:02 - afterwards we set model to the result of
75:05 - calling the tensorflow.js function
75:07 - tf.loadmodel
75:08 - remember this function accepts the url
75:10 - to the given model's model.json file the
75:13 - models reside in folders that were given
75:15 - the names of the actual models
75:16 - themselves
75:18 - for example the vgg16 files reside
75:20 - within a directory called vgg16
75:23 - and the mobilenet files reside within a
75:25 - directory called mobilenet
75:27 - so when we give the url to the
75:28 - model.json we use the name of the
75:30 - selected model to point to the correct
75:32 - location for where the corresponding
75:34 - json file resides
75:36 - once the model is loaded we then hide
75:38 - the progress bar all right now let's
75:40 - navigate to the click event for the
75:41 - predict button
75:43 - previously within this handler function
75:45 - we would get the selected image and then
75:47 - we would do all of the pre-processing
75:48 - for vgg16 and get a prediction
75:51 - well now since we have two different
75:53 - models that pre-process images
75:54 - differently we're putting the
75:55 - pre-processing code into its own
75:57 - function called pre-process image so now
76:00 - once a user clicks the predict button we
76:02 - get the selected image we get the model
76:04 - name from the value of the model
76:06 - selector and then we create a tensor
76:08 - which is set to the result of our new
76:10 - pre-process image function
76:12 - we pass the function both the image and
76:13 - the model name
76:15 - let's go check out this function
76:17 - all right as just touched on pre-process
76:19 - image accepts an image and the model
76:21 - name
76:22 - it then creates a tensor using tf.from
76:24 - pixels passing the given image to it
76:27 - resizes this tensor to have height and
76:29 - width dimensions of 224x224 and cast the
76:32 - tensor's type to float
76:34 - all of this should look really familiar
76:35 - because we had this exact same code
76:37 - within the predict buttons click event
76:38 - before this code won't change regardless
76:41 - of whether we're using vgg16 or
76:43 - mobilenet
76:44 - now in case later we want to add another
76:46 - model and say we only want the base
76:48 - generic preprocessing that we just
76:49 - covered then in that case we won't pass
76:52 - a model name and we'll catch that case
76:54 - with this if statement that just returns
76:55 - the tensor with expanded dimensions
76:58 - if vgg16 is the selected model then we
77:01 - need to do the remaining pre-processing
77:03 - that we went over together in earlier
77:05 - sections
77:06 - so we have our mean imagenet rgb tensor
77:08 - that we defined last time here
77:10 - and we subtract the mean imagenet rgb
77:12 - tensor from the original tensor reverse
77:15 - the rgb values and expand the dimensions
77:17 - of the tensor
77:19 - we then return this final tensor as the
77:20 - result of this function
77:22 - if mobilenet is selected on the other
77:24 - hand then our preprocessing will be a
77:26 - bit different unlike vgg16 the images
77:29 - that mobilenet was originally trained on
77:30 - were pre-processed so that the rgb
77:32 - values were scaled down from a scale of
77:34 - 0 to 255 to a scale of minus 1 to 1.
77:39 - we do this by first creating this scalar
77:41 - value of 127.5 which is exactly one half
77:44 - of 255
77:46 - we then subtract the scalar from the
77:48 - original tensor and divide that result
77:50 - by the scalar
77:52 - this will put all the values on a scale
77:53 - of minus one to one but notice the use
77:56 - of broadcasting that's going on with
77:58 - these operations behind the scenes
78:00 - lastly we again expand the dimensions
78:02 - and then return this resulting tensor
78:05 - also in this last case if a model name
78:07 - is passed to this function that isn't
78:08 - one of the available ones already here
78:10 - then we'll throw this exception
78:12 - alright we've made all the necessary
78:14 - code changes let's now browse to our app
78:16 - and see the results
78:19 - we've arrived at our application and we
78:20 - now have the new model selector we added
78:22 - in
78:23 - clicking on the selector we can select
78:25 - either mobilenet or vgg16
78:27 - let's go ahead and select mobilenet
78:31 - and you can see that loaded pretty fast
78:33 - remember when we loaded vgg16 in
78:35 - previous sections i had to pause and
78:37 - resume the video since it took so long
78:39 - to load but mobilenet was speedy
78:42 - alright cool now we'll select an image
78:44 - click predict
78:46 - and again mobilenet was super fast
78:48 - relative to vgg16 and returning a
78:50 - prediction to us
78:52 - so hopefully this exercise has
78:54 - illustrated the practicality of using
78:55 - mobile nets in situations like these
78:58 - and that's it congratulations for making
79:00 - it all the way to the end give yourself
79:02 - a pat on the back if you're looking to
79:04 - learn more be sure to check out deep
79:06 - blizzard on youtube and the available
79:08 - resources on deeplister.com thanks for
79:10 - watching and i hope to see you later
79:17 - [Music]
79:35 - you