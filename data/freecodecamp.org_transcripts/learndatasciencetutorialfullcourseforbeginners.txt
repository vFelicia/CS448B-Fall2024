00:02 - Welcome to Data Science: An Introduction.
I'm Barton Poulson and what we are going to
00:07 - do in this course is We are going to have
a brief, accessible and non-technical overview
00:13 - of the field of Data Science. Now, some people
when they hear Data Science, they start thinking
00:18 - things like: Data and think about piles of
equations and numbers and then throw on top
00:25 - of that Science and think about people working
in their lab and they start to say eh, that's
00:31 - not for me. I'm not really a technical person
and that just seems much too techy. Well,
00:37 - here's the important thing to know. While
a lot of people get really fired up about
00:40 - the technical aspects of Data Science the
important thing is that Data Science is not
00:45 - so much a technical discipline, but creative.
And, really, that's true. The reason I say
00:53 - that is because in Data Science you use tools
that come from coding and statistics and from
00:59 - math But you use those to work creatively
with data. The idea is there's always more
01:06 - than one way to solve a problem or answer
a question But most importantly to get insight
01:12 - Because the goal, no matter how you go about
it, is to get insight from your data. and
01:18 - what makes Data Science unique, compared to
so many other things is that you try to listen
01:23 - to all of your data, even when it doesn't
fit in easily with your standard approaches
01:29 - and paradigms you're trying to be much more
inclusive in your analysis and the reason
01:35 - you want to do that is because everything
signifies. everything carries meaning and
01:41 - everything can give you additional understanding
and insight into what's going on around you
01:47 - and so in this course what we are trying to
do is give you a map to the field of Data
01:51 - Science and how you can use it and so now
you have the map in your hands and you can
01:56 - get ready to get going with Data Science.
Welcome back to Data Science: An Introduction.
02:03 - And we're going to begin this course by defining
data science. That makes sense. But, we are
02:08 - going to be doing it in kind of a funny way.
The first thing I am going to talk about is
02:12 - the demand for data science. So, let's take
a quick look. Now, data science can be defined
02:20 - in a few ways. I am going to give you some
short definitions. Take one on my definition
02:25 - is that data science is coding, math, and
statistics in applied settings. That's a reasonable
02:31 - working definition. But, if you want to be
a little more concise, I've got take two on
02:36 - a definition. That data science is the analysis
of diverse data, or data that you didn't think
02:42 - would fit into standard analytic approaches.
A third way to think about it is that data
02:48 - science is inclusive analysis. It includes
all of the data, all of the information that
02:53 - you have, in order to get the most insightful
and compelling answer to your research questions.
03:01 - Now, you may say to yourself, "Wait... that's
it?" Well, if you're not impressed, let me
03:10 - show you a few things. First off, let's take
a look at this article. It says, "Data Scientist:
03:16 - the Sexiest Job of the 21st Century." And
please note, this is coming from Harvard Business
03:21 - Review. So, this is an authoritative source
and it is the official source of this saying:
03:27 - that data science is sexy! Now, again, you
may be saying to yourself, "Sexy? I hardly
03:34 - think so." Oh yeah, it's sexy. And the reason
data science is sexy is because first, it
03:42 - has rare qualities, and second it has high
demand. Let me say a little more about those.
03:49 - The rare qualities are that data science takes
unstructured data, then finds order, meaning,
03:54 - and value in the data. Those are important,
but they're not easy to come across. Second,
04:00 - high demand. Well, the reason it's in high
demand is because data science provides insight
04:06 - into what's going on around you and critically,
it provides competitive advantage, which is
04:12 - a huge thing in business settings. Now, let
me go back and say a little more about demand.
04:19 - Let's take a look at a few other sources.
So, for instance the McKinsey Global Institute
04:24 - published a very well-known paper, and you
can get it with this URL. And if you go to
04:29 - that webpage, this is what's going to come
up. And we're going to take a quick look at
04:33 - this one, the executive summary. It's a PDF
that you can download. And if you open that
04:38 - up, you will find this page. And let's take
a look at the bottom right corner. Two numbers
04:43 - here, I'm going to zoom in on those. The first
one, is they are projecting a need in the
04:50 - next few years for somewhere between 140 and
190,000 deep analytical talent positions.
04:57 - So, this means actual practicing data scientists.
That's a huge number; but almost ten times
05:04 - as high is 1.5 million more data-savvy managers
will be needed to take full advantage of big
05:12 - data in the United States. Now, that's people
who aren't necessarily doing the analysis
05:17 - but have to understand it, who have to speak
data. And that's one of the main purposes
05:22 - of this particular course, is to help people
who may or may not be the practicing data
05:27 - scientists learn to understand what they can
get out of data, and some of the methods used
05:32 - to get there. Let's take a look at another
article from LinkedIn. Here is a shortcut
05:36 - URL for it and that will bring you to this
webpage: "The 25 hottest job skills that got
05:41 - people hired in 2014." And take a look at
number one here: statistical analysis and
05:47 - data mining, very closely related to data
science. And just to be clear, this was number
05:52 - one in Australia, and Brazil, and Canada,
and France, and India, and the Netherlands,
05:59 - and South Africa, and the United Arab Emirates,
and the United Kingdom. Everywhere. And if
06:06 - you need a little more, let's take a look
at Glassdoor, which published an article this
06:11 - year, 2016, and it's about the "25 Best Jobs
in America." And look at number one right
06:20 - here, it's data scientist. And we can zoom
on this information. It says there is going
06:28 - to be 1,700 job openings, with a median base
salary of over $116,000, and fabulous career
06:35 - opportunities and job scores. So, if you want
to take all of this together, the conclusion
06:39 - you can reach is that data science pays. And
I can show you a little more about that. So
06:44 - for instance, here's a list of the top ten
highest paying salaries that I got from US
06:50 - News. We have physicians (or doctors), dentists,
and lawyers, and so on. Now, if we add data
06:57 - scientist to this list, using data from O'Reilly.com,
we have to push things around. And goes in
07:03 - third with an average total salary (not the
base we had in the other one, but the total
07:09 - compensation) of about $144,000 a year. That's
extraordinary. So in sum, what do we get from
07:17 - all this? First off, we learn that there is
a very high demand for data science. Second,
07:23 - we learn that there is a critical need for
both specialists; those are the sort of practicing
07:28 - data scientists; and for Generalists, the
people who speak the language and know what
07:33 - can be done. And of course, excellent pay.
And all together, this makes Data Science
07:39 - a compelling career alternative and a way
of making you better at whatever you are doing.
07:45 - Back here in data science, we're going to
continue our attempt to define data science
07:51 - by looking at something that's really well
known in the field; the Data Science Venn
07:55 - Diagram. Now if you want to, you can think
of this in terms of, "What are the ingredients
08:00 - of data science?" Well, we're going to first
say thanks to Drew Conway, the guy who came
08:07 - up with this. And if you want to see the original
article, you can go to this address. But,
08:14 - what Drew said is that data science is made
of three things. And we can put them as overlapping
08:19 - circles because it is the intersection that's
important. Here on the top left is coding
08:24 - or computer programming, or as he calls it:
hacking. On the top right is stats or, stats
08:30 - or mathematics, or quantitative abilities
in general. And on the bottom is domain expertise,
08:36 - or intimate familiarity with a particular
field of practice: business, or health, or
08:40 - education, or science, or something like that.
And the intersection here in the middle, that
08:46 - is data science. So it's the combination of
coding and statistics and math and domain
08:52 - knowledge. Now, let's say a little more about
coding. The reason coding is important is
08:58 - because it helps you gather and prepare the
data. Because a lot of the data comes from
09:02 - novel sources and is not necessarily ready
for you to gather and it can be in very unusual
09:08 - formats. And so coding is important because
it can require some real creativity to get
09:14 - the data from the sources to put it into your
analysis. Now, a few kinds of coding that
09:22 - are important; for instance, there is statistical
coding. A couple of major languages in this
09:28 - are R and Python. Two open-source free programming
languages. R, specifically for data. Python
09:34 - is general-purpose, but well adapted to data.
The ability to work with databases is important
09:40 - too. The most common language there is SQL,
usually pronounced "Sequel," which stands
09:45 - for Structured Query Language, because that's
where the data is. Also, there is the command
09:51 - line interface, or if you are on a Mac, people
just call it "the terminal." Most common language
09:56 - there is Bash, which actually stands for Bourne-again
shell. And then searching is important and
10:04 - regex, or regular expressions. While there
is not a huge amount to learn there (it's
10:10 - a small little field), it's sort of like super-powered
wildcard searching that makes it possible
10:15 - for you to both find the data and reformat
it in ways that are going to be helpful for
10:19 - your analysis. Now, let's say a few things
about the math. You're going to need things
10:25 - like a little bit of probability, some algebra,
of course, regression (very common statistical
10:31 - procedure). Those things are important. And
the reason you need the math is: because that
10:35 - is going to help you choose the appropriate
procedures to answer the question with the
10:40 - data that you have. And probably even more
importantly; it is going to help you diagnose
10:43 - problems when things don't go as expected.
And given that you are trying to do new things
10:48 - with new data in new ways, you are probably
going to come across problems. So the ability
10:52 - to understand the mechanics of what is going
on is going to give you a big advantage. And
10:58 - the third element of the data science Venn
Diagram is some sort of domain expertise.
11:02 - Think of it as expertise in the field that
you're in. Business settings are common. You
11:07 - need to know about the goals of that field,
the methods that are used, and the constraints
11:12 - that people come across. And it's important
because whatever your results are, you need
11:16 - to be able to implement them well. Data science
is very practical and is designed to accomplish
11:21 - something. And your familiarity with a particular
field of practice is going to make it that
11:27 - much easier and more impactful when you implement
the results of your analysis. Now, let's go
11:34 - back to our Venn Diagram here just for a moment.
Because this is a Venn, we also have these
11:38 - intersections of two circles at a time. At
the top is machine learning. At the bottom
11:44 - right is traditional research. And on the
bottom left hand is what Drew Conway called,
11:49 - "the danger zone." Let me talk about each
of these. First off, machine learning, or
11:53 - ML. Now, you think about machine learning
and the idea here is that it represents coding,
11:59 - or statistical programming and mathematics,
without any real domain expertise. Sometimes
12:05 - these are referred to as "black box" models.
They kind of throw data in and you don't even
12:10 - necessarily have to know what it means or
what language it is in, and it will just kind
12:13 - of crunch through it all and it will give
you some regularities. That can be very helpful,
12:18 - but machine learning is considered slightly
different from data science because it doesn't
12:23 - involve the particular applications in a specific
domain. Also, there's traditional research.
12:31 - This is where you have math or statistics
and you have domain knowledge; often very
12:35 - intensive domain knowledge but without the
coding or programming. Now, you can get away
12:39 - with that because the data that you use in
traditional research is highly structured.
12:44 - It comes in rows and columns, and is typically
complete and is typically ready for analysis.
12:49 - Doesn't mean your life is easy, because now
you have to expand an enormous amount of effort
12:54 - in the methods and the designing of the project
and the interpretation of the data. So, still
13:01 - very heavy intellectual cognitive work, but
it comes from a different place. And then
13:06 - finally, there is what Conway called, "the
danger zone." And that's the intersection
13:11 - of coding and domain knowledge, but without
math or statistics. Now he says it is unlikely
13:17 - to happen, and that is probably true. On the
other hand, I can think of some common examples,
13:21 - what are called "word counts," where you take
a large document or a series of documents,
13:25 - and you count how many times a word appears
in there. That can actually tell you some
13:29 - very important things. And also, drawing maps
and showing how things change across place
13:33 - and maybe even across time. You don't necessarily
have to have the math, but it can be very
13:38 - insightful and helpful. So, let's think about
a couple of backgrounds where people come
13:45 - from here. First, is coding. You can have
people who are coders, who can do math, stats,
13:51 - and business. So, you get the three things
(and this is probably the most common), most
13:55 - the people come from a programming background.
On the other hand, there is also stats, or
14:00 - statistics. And you can get statisticians
who can code and who also can do business.
14:05 - That's less common, but it does happen. And
finally, there is people who come into data
14:09 - science from a particular domain. And these
are, for instance, business people who can
14:14 - code and do numbers. And they are the least
common. But, all of these are important to
14:19 - data science. And so in sum, here is what
we can take away. First, several fields make
14:26 - up Data Science. Second, diverse skills and
backgrounds are important and they are needed
14:33 - in data science. And third, there are many
roles involved because there are a lot of
14:37 - different things that need to happen. We'll
say more about that in our next movie. The
14:45 - next step in our data science introduction
and our definition of data science is to talk
14:50 - about the Data Science Pathway. So I like
to think of this as, when you are working
14:56 - on a major project, you have got to do one
step at a time to get it from here to there.
15:01 - In data science, you can take the various
steps and you can put them into a couple of
15:05 - general categories. First, there are the steps
that involve planning. Second, there's the
15:11 - data prep. Third, there's the actual modeling
of the data. And fourth, there's the follow-up.
15:18 - And there are several steps within each of
these; I'll explain each of them briefly.
15:23 - First, let's talk about planning. The first
thing that you need to do, is you need to
15:27 - define the goals of your project so you know
how to use your resources well, and also so
15:32 - you know when you are done. Second, you need
to organize your resources. So you might have
15:37 - data from several different sources; you might
have different software packages, you might
15:42 - have different people. Which gets us to the
third one: you need to coordinate the people
15:46 - so they can work together productively. If
you are doing a hand-off, it needs to be clear
15:51 - who is going to do what and how their work
is going to go together. And then, really
15:56 - to state the obvious, you need to schedule
the project so things can move along smoothly
16:00 - and you can finish in a reasonable amount
of time. Next is the data prep, where you
16:06 - are taking like food prep and getting the
raw ingredients ready. First of course, is
16:10 - you need to get the data. And it can from
many different sources and be in many different
16:15 - formats. You need to clean the data and, the
sad thing is, this tends to be a very large
16:22 - part of any data science project. And that
is because you are bringing in unusual data
16:27 - from a lot of different places. You also want
to explore the data; that is, really see what
16:33 - it looks like, how many people are in each
group, what the shape of the distributions
16:37 - are like, what is associated with what. And
you may need to refine the data. And that
16:42 - means choosing variables to include, choosing
cases to include or exclude, making any transformations
16:49 - to the data you need to do. And of course
these steps kind of can bounce back and forth
16:53 - from one to the other. The third group is
modeling or statistical modeling. This is
16:59 - where you actually want to create the statistical
model. So for instance, you might do a regression
17:04 - analysis or you might do a neural network.
But, whatever you do, once you create your
17:10 - model, you have to validate the model. You
might do that with a holdout validation. You
17:15 - might do it really with a very small replication
if you can. You also need to evaluate the
17:22 - model. So, once you know that the model is
accurate, what does it actually mean and how
17:27 - much does it tell you? And then finally, you
need to refine the model. So, for instance,
17:33 - there may be variables you want to throw out;
maybe additional ones you want to include.
17:38 - You may want to, again, transform some of
the data. You may want to get it so it is
17:43 - easier to interpret and apply. And that gets
us to the last part of the data science pathway.
17:49 - And that's follow up. And once you have created
your model, you need to present the model.
17:54 - Because it is usually work that is being done
for a client, could be in house, could be
17:59 - a third party. But you need to take the insights
that you got and share them in a meaningful
18:04 - way with other people. You also need to deploy
the model; it is usually being done in order
18:10 - to accomplish something. So, for instance,
if you are working with an e-commerce site,
18:15 - you may be developing a recommendation engine
that says, "people who bought this and this
18:19 - might buy this." You need to actually stick
it on the website and see if it works the
18:23 - way that you expected it to. Then you need
to revisit the model because a lot of the
18:28 - times, the data that you worked on is not
necessarily all of the data, and things can
18:33 - change when you get out in the real world
or things just change over time. So, you have
18:39 - to see how well your model is working. And
then, just to be thorough, you need to archive
18:45 - the assets, document what you have, and make
it possible for you or for others to repeat
18:50 - the analysis or develop off of it in the future.
So, those are the general steps of what I
18:56 - consider the data science pathway. And in
sum, what we get from this is three things.
19:01 - First, data science isn't just a technical
field, it is not just coding. Things like,
19:06 - planning and presenting and implementing are
just as important. Also, contextual skills,
19:13 - knowing how it works in a particular field,
knowing how it will be implemented, those
19:18 - skills matter as well. And then, as you got
from this whole thing, there are a lot of
19:23 - things to do. And if you go one step at a
time, there will be less backtracking and
19:28 - you will ultimately be more productive in
your data science projects. We'll continue
19:34 - our definition of data science by looking
at the roles that are involved in data science.
19:39 - The way that different people can contribute
to it. That's because it tends to be a collaborative
19:45 - thing, and it's nice to be able to say that
we are all together, working together towards
19:48 - a single goal. So, let's talk about some of
the roles involved in data science and how
19:53 - they contribute to the projects. First off,
let's take a look at engineers. These are
19:58 - people who focus on the back end hardware.
For instance, the servers and the software
20:03 - that runs them. This is what makes data science
possible, and it includes people like developers,
20:10 - software developers, or database administrators.
And they provide the foundation for the rest
20:16 - of the work. Next, you can also have people
who are Big Data specialists. These are people
20:22 - who focus on computer science and mathematics,
and they may do machine learning algorithms
20:29 - as a way of processing very large amounts
of data. And they often create what are called
20:34 - data products. So, a thing that tells you
what restaurant to go to, or that says, "you
20:39 - might know these friends," or provides ways
of linking up photos. Those are data products,
20:45 - and those often involve a huge amount of very
technical work behind them. There are also
20:51 - researchers; these are people who focus on
domain-specific research. So, for instance,
20:57 - physics, or genetics, or whatever. And these
people tend to have very strong statistics,
21:03 - and they can use some of the procedures and
some of the data that comes from the other
21:07 - people like the big data researchers, but
they focus on the specific questions. Also
21:14 - in the data science realm, you will find analysts.
These are people who focus on the day-to-day
21:18 - tasks of running a business. So for instance,
they might do web analytics (like Google analytics),
21:23 - or they might pull data from a SQL database.
And this information is very important and
21:31 - good for business. So, analysts are key to
the day-to-day function of business, but they
21:37 - may not be, exactly be Data Science proper,
because most of the data they are working
21:43 - with is going to be pretty structured. Nevertheless,
they play a critical role in business in general.
21:49 - And then, speaking of business. You have the
actual business people; the men and women
21:54 - who organize and run businesses. These people
need to be able to frame business-relevant
22:00 - questions that can be answered with the data.
Also, the business person manages the project
22:06 - and the efforts and the resources of others.
And while they may not actually be doing the
22:11 - coding, they must speak data; they must know
how the data works, what it can answer, and
22:18 - how to implement it. You can also have entrepreneurs.
So, you might have a data startup; they are
22:25 - starting their own little social network,
their own little web search platform. An entrepreneur
22:31 - needs data and business skills. And truthfully,
they have to be creative at every step along
22:37 - the way. Usually because they are doing it
all themselves at a smaller scale. Then we
22:44 - have in data science something known as "the
full stack unicorn." And this is a person
22:49 - who can do everything at an expert level.
They are called a unicorn because truthfully,
22:53 - they may not actually exist. I will have more
to say about that later. But for right now,
22:58 - we can sum up what we got out of this video
by three things. Number one, data science
23:04 - is diverse. There's a lot of different people
who go into it, and they have different goals
23:09 - for their work, and they bring in different
skills and different experiences and different
23:14 - approaches. Also, they tend to work in very
different contexts. An entrepreneur works
23:20 - in a very different place from a business
manager, who works in a very different place
23:23 - from an academic researcher. But, all of them
are connected in some way to data science
23:29 - and make it a richer field. The last thing
I want to say in "Data Science: An Introduction"
23:36 - where I am trying to define data science,
is to talk about teams in data science. The
23:42 - idea here is that data science has many different
tools, and different people are going to be
23:49 - experts in each one of them. Now, you have,
for instance, coding and you have statistics.
23:55 - Also, you have what feels like design, or
business and management that are involved.
24:00 - And the question, of course, is: "who can
do all of it? Who's able to do all of these
24:05 - things at the level that we need?" Well, that's
where we get this saying (I have mentioned
24:09 - it before), it's the unicorn. And just like
in ancient history, the unicorn is a mythical
24:17 - creature with magical abilities. In data science,
it works a little differently. It is a mythical
24:23 - Data Scientist with universal abilities. The
trouble is, as we know from the real world,
24:28 - there are really no unicorns (animals), and
there are really not very many unicorns in
24:34 - data science. Really, there are just people.
And so we have to find out how we can do the
24:39 - projects even though we don't have this one
person who can do everything for everybody.
24:44 - So let's take a hypothetical case, just for
a moment. I am going to give you some fictional
24:47 - people. Here is my fictional person Otto,
who has strong visualization skills, who has
24:55 - good coding, but has limited analytic or statistical
ability. And if we graph his stuff out, his
25:01 - abilities... So, here we have five things
that we need to have happen. And for the project
25:07 - to work, they all have to happen at least,
a level of eight on the zero-to-ten. If we
25:13 - take his coding ability, he is almost there.
Statistics, not quite halfway. Graphics, yes
25:19 - he can do that. And then, business, eh, alright.
And project, pretty good. So, what you can
25:26 - see here is, in only one of these five areas
is Otto sufficient on his own. On the other
25:32 - hand, let's pair him up with somebody else.
Let's take a look at Lucy. And Lucy has strong
25:37 - business training, has good tech skills, but
has limited graphics. And if we get her profile
25:43 - on the same thing that we saw, there is coding,
pretty good. Statistics, pretty good. Graphics,
25:50 - not so much. Business, good. And projects,
OK. Now, the important thing here is that
25:56 - we can make a team. So let's take our two
fictional people, Otto and Lucy, and we can
26:02 - put together their abilities. Now, I actually
have to change the scale here a little bit
26:06 - to accommodate the both of them. But our criterion
still is at eight; we need a level of eight
26:11 - in order to do the project competently. And
if we combine them: oh look, coding is now
26:18 - past eight. Statistics is past eight. Graphics
is way past. Business way past. And then the
26:25 - projects, they are too. So when we combine
their skills, we are able to get the level
26:31 - that we need for everything. Or to put it
another way, we have now created a unicorn
26:37 - by team, and that makes it possible to do
the data science project. So, in sum: you
26:43 - usually can't do data science on your own.
That's a very rare individual. Or more specifically:
26:50 - people need people, and in data science you
have the opportunity to take several people
26:56 - and make collective unicorns, so you can get
the insight that you need in your project
27:01 - and you can get the things done that you want.
In order to get a better understanding of
27:07 - data science, it can be helpful to look at
contrasts between data science and other fields.
27:13 - Probably the most informative is with Big
Data because these two terms are actually
27:17 - often confused. It makes me think of situations
where you have two things that are very similar,
27:23 - but not the same. Like we have here in the
Piazza San Carlo here in Italy. Part of the
27:29 - problem stems from the fact that data science
and big data both have Venn Diagrams associated
27:34 - with them. So, for instance, Venn number one
for data science is something we have seen
27:39 - already. We have three circles and we have
coding and we have math and we have some domain
27:47 - expertise, that put together get data science.
On the other hand, Venn Diagram number two
27:53 - is for Big Data. It also has three circles.
And we have the high volume of data, the rapid
28:00 - velocity of data, and the extreme variety
of data. Take those three v's together and
28:06 - you get Big Data. Now, we can also combine
these two if we want in a third Venn Diagram,
28:12 - we call Big Data and Data Science. This time
it is just two circles. With Big Data on the
28:17 - left and Data Science on the right. And the
intersection in the middle, there is Big Data
28:22 - Science, which actually is a real term. But,
if you want to do a compare and contrast,
28:28 - it kind of helps to look at how you can have
one without the other. So, let's start by
28:33 - looking at Big Data without Data Science.
So, these are situations where you may have
28:39 - the volume or velocity or variety of data
but don't need all the tools of data science.
28:44 - So, we are just looking at the left side of
the equation right now. Now, truthfully, this
28:51 - only works if you have Big Data without all
three V's. Some say you have to have the volume,
28:55 - velocity, and variety for it to count as Big
Data. I basically say anything that doesn't
29:01 - fit into a standard machine is probably Big
Data. I can think of a couple of examples
29:06 - here of things that might count as Big Data,
but maybe don't count as Data Science. Machine
29:13 - learning, where you can have very large data
sets and probably very complex, doesn't require
29:19 - very much domain expertise, so that may not
be data science. Word counts, where you have
29:24 - an enormous amount of data and it's actually
a pretty simple analysis, again doesn't require
29:30 - much sophistication in terms of quantitative
skills or even domain expertise. So, maybe/maybe
29:37 - not data science. On the other hand, to do
any of these you are going to need to have
29:41 - at least two skills. You are going to need
to have the coding and you will probably have
29:46 - to have some sort of quantitative skills as
well. So, how about data science without Big
29:54 - Data? That's the right side of this diagram.
Well, to make that happen you are probably
30:00 - talking about data with just one of the three
V's from Big Data. So, either volume or velocity
30:07 - or variety, but singly. So for instance, genetics
data. You have a huge amount of data and it
30:16 - comes in very set structure and it tends to
come in at once. So, you have got a lot of
30:20 - volume and it is a very challenging thing
to work with. You have to use data science,
30:25 - but it may or may not count as Big Data. Similarly,
streaming sensor data, where you have data
30:31 - coming in very quickly, but you are not necessarily
saving it; you are just looking at these windows
30:36 - in it. That is a lot of velocity, and it is
difficult to deal with, and it takes Data
30:41 - Science, the full skill set, but it may not
require Big Data, per se. Or facial recognition,
30:48 - where you have enormous variety in the data
because you are getting photos or videos that
30:53 - are coming in. Again, very difficult to deal
with, requires a lot of ingenuity and creativity
30:59 - may or may not count as Big Data, depending
on how much of a stickler you are about definitions.
31:05 - Now, if you want to combine the two, we can
talk about Big Data Science. In that case,
31:12 - we are looking right here at the middle. This
is a situation where you have volume, and
31:17 - velocity, and variety in your data and truthfully,
if you have the three of those, you are going
31:22 - to need the full Data Science skill set. You
are going to need coding, and statistics,
31:28 - and math, and you are going to have to have
domain expertise. Primarily because of the
31:33 - variety you are dealing with, but taken all
together you do have to have all of it. So
31:38 - in sum, here is what we get. Big Data is not
equal to, it is not identical to data science.
31:44 - Now, there is common ground, and a lot of
people who are good at Big Data are good at
31:49 - data science and vice versa, but they are
conceptually distinct. On the other hand,
31:53 - there is the shared middle ground of Big Data
Science that unifies the two separate fields.
32:02 - Another important contrast you can make in
trying to understand data science is to compare
32:06 - it with coding or computer programming. Now,
this is where you are trying to work with
32:11 - machines and you are trying to talk to that
machine, to get it to do things. In one sense
32:16 - you can think of coding as just giving task
instructions; how to do something. It is a
32:21 - lot like a recipe when you're cooking. You
get some sort of user input or other input,
32:26 - and then maybe you have if/then logic, and
you get output from it. To take an extremely
32:31 - simple example, if you are programming in
Python version 2, you write: print, and then
32:36 - in quotes, "Hello, world!" will put the words
"Hello, world!" on the screen. So, you gave
32:42 - it some instructions and it gave you some
output. Very simple programming. Now, coding
32:50 - and data gets a little more complicated. So,
for instance, there is word counts, where
32:56 - you take a book or a whole collection of books,
you take the words and you count how many
33:01 - there are in there. Now, this is a conceptually
simple task, and domain expertise and really
33:08 - math and statistics are not vital. But to
make valid inferences and generalizations
33:17 - in the face of variability and uncertainty
in the data you need statistics, and by extension,
33:24 - you need data science. It might help to compare
the two by looking at the tools of the respective
33:31 - trades. So for instance, there are tools for
coding or generic computer programming, and
33:37 - there are tools that are specific for data
science. So, what I have right here is a list
33:43 - from the IEEE of the top ten programming languages
of 2015. And it starts at Java and C and goes
33:51 - down to Shell. And some of these are also
used for data science. So for instance, Python
33:57 - and R and SQL are used for data science, but
the other ones aren't major ones in data science.
34:06 - So, let's, in fact, take a look at a different
list of most popular tools for data science
34:12 - and you see that things move around a little
bit. Now, R is at the top, SQL is there, Python
34:17 - is there, but for me what is the most interesting
on the list is that Excel is number five,
34:22 - which would never be considered programming,
per se, but it is, in fact, a very important
34:27 - tool for data science. And that is one of
the ways that we can compare and contrast
34:31 - computer programming with data science. In
sum, we can say this: data science is not
34:39 - equal to coding. They are different things.
On the other hand, they share some of the
34:43 - tools and they share some practices specifically
when coding for data. On the other hand, there
34:49 - is one very big difference in that statistics,
statistical ability is one of the major separators
34:56 - between general purpose programming and data
science programming. When we talk about data
35:05 - science and we are contrasting with some fields,
another field that a lot of people get confused
35:09 - and think they are the same thing is data
science and statistics. Now, I will tell you
35:15 - there is a lot in common, but we can talk
a little bit about the different focuses of
35:20 - each. And we also get into the issue of definitionalism
that data science is different because we
35:25 - define it differently, even when there is
an awful lot in common between the two. It
35:31 - helps to take a look at some of the things
that go on in each field. So, let's start
35:35 - here about statistics. Put a little circle
here and we will put data science. And, to
35:41 - borrow a term from Steven J. Gould, we can
call these non-overlapping magisteria; NOMA.
35:48 - So, you think of them as separate fields that
are sovereign unto themselves with nothing
35:53 - to do with each other. But, you know, that
doesn't seem right; and part of that is that
35:59 - if we go back to the Data Science Venn Diagram,
statistics is one part of it. There it is
36:05 - in the top corner. So, now what do we do?
What's the relationship? So, it doesn't make
36:11 - sense to say these are totally separate areas,
maybe data science and statistics because
36:18 - they share procedures, maybe data science
is a subset or specialty of statistics, more
36:23 - like this. But, if data science were just
a subset or specialty within statistics then
36:32 - it would follow that all data scientists would
first be statisticians. And interestingly
36:39 - that's just not so. Say, for instance, we
take a look at the data science stars, the
36:47 - superstars in the field. We go to a rather
intimidating article; it's called "The World's
36:53 - 7 Most Powerful Data Scientists" from Forbes.com.
You can see the article if you go to this
36:58 - URL. There's actually more than seven people,
because sometimes he brings them up in pairs.
37:04 - Let's check their degrees, see what their
academic training is in. If we take all the
37:09 - people on this list, we have five degrees
in computer science, three in math, two in
37:16 - engineering, and one each in biology, economics,
law, speech pathology, and one in statistics.
37:24 - And so that tells us, of course, these major
people in data science are not trained as
37:32 - statisticians. Only one of them has formal
training in that. So, that gets us to the
37:37 - next question. Where do these two fields,
statistics and data science, diverge? Because
37:42 - they seem like they should have a lot in common,
but they don't have a lot in training. Specifically,
37:48 - we can look at the training. Most data scientists
are not trained, formally, as statisticians.
37:55 - Also, in practice, things like machine learning
and big data, which are central to data science,
38:03 - are not shared, generally, with most of statistics.
So, they have separate domains there. And
38:10 - then there is the important issue of context.
Data scientists tend to work in different
38:16 - settings than statisticians. Specifically,
data scientists very often work in commercial
38:21 - settings where they are trying to get recommendation
engines or ways of developing a product that
38:27 - will make them money. So, maybe instead of
having data science a subset of statistics,
38:34 - we can think of it more as these two fields
have different niches. They both analyze data,
38:39 - but they do different things in different
ways. So, maybe it is fair to say they share,
38:46 - they overlap, they have analysis in common
of data, but otherwise, they are ecologically
38:53 - distinct. So, in sum: what we can say here
is that data science and statistics both use
39:01 - data and they analyze it. But the people in
each tend to come from different backgrounds,
39:07 - and they tend to function with different goals
and contexts. And in that way, render them
39:13 - to be conceptually distinct fields despite
the apparent overlap. As we work to get a
39:20 - grasp on data science, there is one more contrast
I want to make explicitly, and that is between
39:26 - data science and business intelligence, or
BI. The idea here is that business intelligence
39:33 - is data in real life; it's very, very applied
stuff. The purpose of BI is to get data on
39:42 - internal operations, on market competitors,
and so on, and make justifiable decisions
39:48 - as opposed to just sitting in the bar and
doing whatever comes to your mind. Now, data
39:54 - science is involved with this, except, you
know, really there is no coding in BI. There's
40:02 - using apps that already exist. And the statistics
in business intelligence tend to be very simple,
40:09 - they tend to be counts and percentages and
ratios. And so, it's simple, the light bulb
40:15 - is simple; it just does its one job there
is nothing super sophisticated there. Instead
40:20 - the focus in business intelligence is on domain
expertise and on really useful direct utility.
40:30 - It's simple, it's effective and it provides
insight. Now, one of the main associations
40:36 - with business intelligence is what are called
dashboards, or data dashboards. They look
40:41 - like this; it is a collection of charts and
tables that go together to give you a very
40:46 - quick overview of what is going on in your
business. And while a lot of data scientists
40:51 - may, let's say, look down their nose upon
dashboards, I'll say this, most of them are
40:56 - very well designed and you can learn a huge
amount about user interaction and the accessibility
41:03 - information from dashboards. So really, where
does data science come into this? What is
41:09 - the connection between data science and business
intelligence? Well, data science can be useful
41:15 - to BI in terms of setting it up. Identifying
data sources and creating or setting up the
41:21 - framework for something like a dashboard or
a business intelligence system. Also, data
41:28 - science can be used to extend it. Data science
can be used to get past the easy questions
41:33 - and the easy data, to get the questions that
are actually most useful to you; even if they
41:37 - require really sometimes data that is hard
to wrangle and work with. And also, there
41:42 - is an interesting interaction here that goes
the other way. Data science practitioners
41:46 - can learn a lot about design from good business
intelligence applications. So, I strongly
41:54 - encourage anybody in data science to look
at them carefully and see what they can learn.
41:59 - In sum: business intelligence, or BI, is very
goal oriented. Data science perhaps prepares
42:07 - the data and sets up the form for business
intelligence, but also data science can learn
42:14 - a lot about usability and accessibility from
business intelligence. And so, it is always
42:21 - worth taking a close look. Data science has
a lot of real wonderful things about it, but
42:29 - it is important to consider some ethical issues,
and I will specifically call this "do no harm"
42:35 - in your data science projects. And for that
we can say thanks to Hippocrates, the guy
42:40 - who gave us the Hippocratic Oath of Do No
Harm. Let's specifically talk about some of
42:45 - the important ethical issues, very briefly,
that come up in data science. Number one is
42:52 - privacy. That data tells you a lot about people
and you need to be concerned about the confidentiality.
42:59 - If you have private information about people,
their names, their social security numbers,
43:04 - their addresses, their credit scores, their
health, that's private, that's confidential,
43:10 - and you shouldn't share that information unless
they specifically gave you permission. Now,
43:15 - one of the reasons this presents a special
challenge in data science because, we will
43:20 - see later, a lot of the sources that are used
in data science were not intended for sharing.
43:25 - If you scrape data from a website or from
PDFs, you need to make sure that it is ok
43:30 - to do that. But it was originally created
without the intention of sharing, so privacy
43:35 - is something that really falls upon the analyst
to make sure they are doing it properly. Next,
43:41 - is anonymity. One of the interesting things
we find is that it is really not hard to identify
43:46 - people in data. If you have a little bit of
GPS data and you know where a person was at
43:52 - four different points in time, you have about
a 95% chance of knowing exactly who they are.
43:57 - You look at things like HIPAA, that's the
Health Insurance Portability and Accountability
44:01 - Act. Before HIPAA, it was really easy to identify
people from medical records. Since then, it
44:07 - has become much more difficult to identify
people uniquely. That's an important thing
44:12 - for really people's well-being. And then also,
proprietary data; if you are working for a
44:19 - client, a company, and they give you their
own data, that data may have identifiers.
44:23 - You may know who the people are, they are
not anonymous anymore. So, anonymity may or
44:28 - may not be there, but major efforts to make
data anonymous. But really, the primary thing
44:33 - is even if you do know who they are, that
you still maintain the privacy and confidentiality
44:38 - of the data. Next, there is the issue about
copyright, where people try to lock down information.
44:46 - Now, just because something is on the web,
doesn't mean that you are allowed to use it.
44:52 - Scraping data from websites is a very common
and useful way of getting data for projects.
44:58 - You can get data from web pages, from PDFs,
from images, from audio, from really a huge
45:06 - number of things. But, again the assumption
that because it is on the web, it's ok to
45:11 - use it is not true. You always need to check
copyright and make sure that it is acceptable
45:16 - for you to access that particular data. Next,
and our very ominous picture, is data security
45:23 - and the idea here is that when you go through
all the effort to gather data, to clean up
45:29 - and prepare for analysis, you have created
something that is very valuable to a lot of
45:33 - people and you have to be concerned about
hackers trying to come in and steal the data,
45:38 - especially if the data is not anonymous and
it has identifiers in it. And so, there is
45:43 - an additional burden to place on the analyst
to ensure to the best of their ability that
45:48 - the data is safe and cannot be broken into
and stolen. And that can include very simple
45:53 - things like a person who is on their project
but is no longer, but took the data on a flash
45:57 - drive. You have to find ways to make sure
that that can't happen as well. There's a
46:01 - lot of possibilities, it's tricky, but it
is something that you have to consider thoroughly.
46:06 - Now, two other things that come up in terms
of ethics, but usually don't get addressed
46:12 - in these conversations. Number one is potential
bias. The idea here is that the algorithms
46:18 - or the formulas that are used in data science
are only as neutral or bias-free as the rules
46:25 - and the data that they get. And so, the idea
here is that if you have rules that address
46:33 - something that is associated with, for instance,
gender or age or race or economic standing,
46:41 - you might unintentionally be building in those
factors. Which, say for instance, say for
46:46 - title nine, you are not supposed to. You might
be building those into the system without
46:50 - being aware of it, and an algorithm has this
sheen of objectivity, and people say they
46:57 - can place confidence in it without realizing
that it is replicating some of the prejudices
47:01 - that may happen in real life. Another issue
is overconfidence. And the idea here is that
47:08 - analyses are limited simplifications. They
have to be, that is just what they are. And
47:13 - because of this, you still need humans in
the loop to help interpret and apply this.
47:20 - The problem is when people run an algorithm
to get a number, say to ten decimal places,
47:25 - and they say, "this must be true," and treat
it as written-in-stone absolutely unshakeable
47:31 - truth, when in fact, if the data were biased
going in; if the algorithms were incomplete,
47:38 - if the sampling was not representative, you
can have enormous problems and go down the
47:45 - wrong path with too much confidence in your
own analyses. So, once again humility is in
47:52 - order when doing data science work. In sum:
data science has enormous potential, but it
47:58 - also has significant risks involved in the
projects. Part of the problem is that analyses
48:04 - can't be neutral, that you have to look at
how the algorithms are associated with the
48:11 - preferences, prejudices, and biases of the
people who made them. And what that means
48:17 - is that no matter what, good judgment is always
vital to quality and success of a data science
48:25 - project. Data Science is a field that is strongly
associated with its methods or procedures.
48:32 - In this section of videos, we're going to
provide a brief overview of the methods that
48:38 - are used in data science. Now, just as a quick
warning, in this section things can get kind
48:44 - of technical and that can cause some people
to sort of freak out. But, this course is
48:50 - a non-technical overview. The technical hands
on stuff is in the other courses. And it is
48:57 - really important to remember that tech is
simply the means to doing data science. Insight
49:05 - or the ability to find meaning in your data,
that's the goal. Tech only helps you get there.
49:11 - And so, we want to focus primarily on insight
and the tools and the tech as they serve to
49:17 - further that goal. Now, there's a few general
categories we are going to talk about, again,
49:23 - with an overview for each of these. The first
one is sourcing or data sourcing. That is
49:29 - how to get the data that goes into data science,
the raw materials that you need. Second is
49:34 - coding. That again is computer programming
that can be used to obtain and manipulate
49:40 - and analyze the data. After that, a tiny bit
of math and that is the mathematics behind
49:47 - data science methods that really form the
foundations of the procedures. And then stats,
49:53 - the statistical methods that are frequently
used to summarize and analyze data, especially
49:58 - as applied to data science. And then there
is machine learning, ML, this is a collection
50:05 - of methods for finding clusters in the data,
for predicting categories or scores on interesting
50:11 - outcomes. And even across these five things,
even then, the presentations aren't too techie-crunchy,
50:18 - they are basically still friendly. Really,
that's the way it is. So, that is the overview
50:26 - of the overviews. In sum: we need to remember
that data science includes tech, but data
50:32 - science is greater than tech, it is more than
those procedures. And above all, that tech
50:39 - while important to data science is still simply
a means to insight in data. The first step
50:48 - in discussing data science methods is to look
at the methods of sourcing, or getting data
50:54 - that is used in data science. You can think
of this as getting the raw materials that
50:59 - go into your analyses. Now, you have got a
few different choices when it comes to this
51:04 - in data science. You can use existing data,
you can use something called data APIs, you
51:09 - can scrape web data, or you can make data.
We'll talk about each of those very briefly
51:16 - in a non-technical manner. For right now,
let me say something about existing data.
51:22 - This is data that already is at hand and it
might be in-house data. So if you work for
51:26 - a company, it might be your company records.
Or, you might have open data; for instance,
51:32 - many governments and many scientific organizations
make their data available to the public. And
51:39 - then there is also third party data. This
is usually data that you buy from a vendor,
51:44 - but it exists and it is very easy to plug
it in and go. You can also use APIs. Now,
51:51 - that stands for Application Programming Interface,
and this is something that allows various
51:56 - computer applications to communicate directly
with each other. It's like phones for your
52:02 - computer programs. It is the most common way
of getting web data, and the beautiful thing
52:06 - about it is it allows you to import that data
directly into whatever program or application
52:11 - you are using to analyze the data. Next is
scraping data. And this is where you want
52:18 - to use data that is on the web, but they don't
have an existing API. And what that means,
52:24 - is usually data that's in HTML web tables
and pages, maybe PDFs. And you can do this
52:32 - either with using specialized applications
for scraping data or you can do it in a programming
52:38 - language, like R or Python, and write the
code to do the data scraping. Or another option
52:44 - is to make data. And this lets you get exactly
what you need; you can be very specific and
52:51 - you can get what you need. You can do something
like interviews, or you can do surveys, or
52:56 - you can do experiments. There is a lot of
approaches, most of them require some specialized
53:00 - training in terms of how to gather quality
data. And that is actually important to remember,
53:05 - because no matter what method you use for
getting or making new data, you need to remember
53:10 - this one little aphorism you may have heard
from computer science. It goes by the name
53:14 - of GIGO: that actually stands for "Garbage
In, Garbage Out," and it means if you have
53:20 - bad data that you are feeding into your system,
you are not going to get anything worthwhile,
53:25 - any real insights out of it. Consequently,
it is important to pay attention to metrics
53:30 - or methods for measuring and the meaning - exactly
what it is that they tell you. There's a few
53:36 - ways you can do this. For instance, you can
talk about business metrics, you can talk
53:40 - about KPIs, which means Key Performance Indicators,
also used in business settings. Or SMART goals,
53:47 - which is a way of describing the goals that
are actionable and timely and so on. You can
53:53 - also talk about, in a measurement sense, classification
accuracy. And I will discuss each of those
53:59 - in a little more detail in a later movie.
But for right now, in sum, we can say this:
54:06 - data sourcing is important because you need
to get the raw materials for your analysis.
54:11 - The nice thing is there's many possible methods,
many ways that you can use to get the data
54:16 - for data science. But no matter what you do,
it is important to check the quality and the
54:21 - meaning of the data so you can get the most
insight possible out of your project. The
54:29 - next step we need to talk about in data science
methods is coding, and I am going to give
54:34 - you a very brief non-technical overview of
coding in data science. The idea here is that
54:39 - you are going to get in there and you are
going to King of the Jungle/master of your
54:43 - domain and make the data jump when you need
it to jump. Now, if you remember when we talked
54:49 - about the Data Science Venn Diagram at the
beginning, coding is up here on the top left.
54:53 - And while we often think about sort of people
typing lines of code (which is very frequent),
54:58 - it is more important to remember when we talk
about coding (or just computers in general),
55:03 - what we are really talking about here is any
technology that lets you manipulate the data
55:09 - in the ways that you need to perform the procedures
you need to get the insight that you want
55:14 - out of your data. Now, there are three very
general categories that we will be discussing
55:19 - here on datalab. The first is apps; these
are specialized applications or programs for
55:24 - working with data. The second is data; or
specifically, data formats. There's special
55:30 - formats for web data, I will mention those
in a moment. And then, code; there are programming
55:36 - languages that give you full control over
what the computer does and how you interact
55:40 - with the data. Let's take a look at each one
very briefly. In terms of apps, there are
55:46 - spreadsheets, like Excel or Google Sheets.
These are the fundamental data tools of probably
55:50 - a majority of the world. There are specialized
applications, like Tableau for data visualization,
55:57 - or SPSS, it is a very common statistical package
in the social sciences and in businesses,
56:02 - and one of my favorites, JASP, which is a
free open source analog of SPSS, which actually
56:08 - I think is a lot easier to use and replicate
research with. And, there are tons of other
56:13 - choices. Now, in terms of web data, it is
helpful to be familiar with things like HTML,
56:21 - and XML, and JSON, and other formats that
are used to encapsulate data on the web, because
56:28 - those are the things that you are going to
have to be programming about to interact with
56:32 - when you get your data. And then there are
actual coding languages. R is probably the
56:39 - most common, along with Python; general purpose
language, but it has been well adapted for
56:43 - data use. There's SQL, the structured query
language for databases, and very basic languages
56:49 - like C, C++, and Java, which are used more
in the back-end of data science. And then
56:56 - there is Bash, the most common command line
interface, and regular expressions. And we
57:01 - will talk about all of these in other courses
here at datalab. But, remember this: tools
57:09 - are just tools. They are only one part of
the data science process. They are a means
57:16 - to the end, and the end, the goal is insight.
You need to know where you are trying to go
57:23 - and then simply choose the tools that help
you reach that particular goal. That's the
57:28 - most important thing. So, in sum, here's a
few things: number one, use your tools wisely.
57:35 - Remember your questions need to drive the
process, not the tools themselves. Also, I
57:39 - will just mention that a few tools is usually
enough. You can do an awful lot with Excel
57:45 - and R. And then, the most important thing
is: focus on your goal and choose your tools
57:53 - and even your data to match the goal, so you
can get the most useful insights from your
57:58 - data. The next step in our discussion of data
science methods is mathematics, and I am going
58:05 - to give a very brief overview of the math
involved in data science. Now, the important
58:11 - thing to remember is that math really forms
the foundation of what we're going to do.
58:15 - If you go back to the Data Science Venn Diagram,
we've got stats up here in the right corner,
58:19 - but really it's math and stats, or quantitative
ability in general, but we'll focus on the
58:25 - math part right here. And probably the most
important question is how much math is enough
58:33 - to do what you need to do? Or to put it another
way, why do you need math at all, because
58:39 - you have got a computer to do it? Well, I
can think of three reasons you don't want
58:45 - to rely on just the computer, but it is helpful
to have some sound mathematical understanding.
58:50 - Here they are: number one, you need to know
which procedures to use and why. So you have
58:57 - your question, you have your data and you
need to have enough of an understanding to
59:00 - make an informed choice. That's not terribly
difficult. Two, you need to know what to do
59:07 - when things don't work right. Sometimes you
get impossible results. I know that statistics
59:14 - you can get a negative adjusted R2; that's
not supposed to happen. And it is good to
59:18 - know the mathematics that go into calculating
that so you can understand how something apparently
59:22 - impossible can work. Or, you are trying to
do a factor analysis or principal component
59:27 - and you get a rotation that won't convert.
It helps to understand what it is about the
59:32 - algorithm that's happening, and why that won't
work in that situation. And number three,
59:38 - interestingly, some procedures, some math
is easier and quicker to do by hand than by
59:44 - firing up the computer. And I'll show you
a couple of examples in later videos, where
59:49 - that can be the case. Now, fundamentally there
is a nice sort of analogy here. Math is to
59:56 - data science as, for instance, chemistry is
to cooking, kinesiology is to dancing, and
60:02 - grammar is to writing. The idea here is that
you can be a wonderful cook without knowing
60:07 - any chemistry, but if you know some chemistry
it is going to help. You can be a wonderful
60:11 - dancer without know kinesiology, but it is
going to help. And you can probably be a good
60:16 - writer without having an explicit knowledge
of grammar, but it is going to make a big
60:21 - difference. The same thing is true of data
science; you will do it better if you have
60:26 - some of the foundational information. So,
the next question is: what kinds of math do
60:31 - you need for data science? Well, there's a
few answers to that. Number one is algebra;
60:36 - you need some elementary algebra. That is,
the basically simple stuff. You can have to
60:41 - do some linear or matrix algebra because that
is the foundation of a lot of the calculations.
60:46 - And you can also have systems of linear equations
where you are trying to solve several equations
60:50 - all at once. It is a tricky thing to do, in
theory, but this is one of the things that
60:55 - is actually easier to do by hand sometimes.
Now, there's more math. You can get some Calculus.
61:03 - You can get some big O, which has to do with
the order of a function, which has to do with
61:07 - sort of how fast it works. Probability theory
can be important, and Bayes' theorem, which
61:14 - is a way of getting what is called a posterior
probability can also be a really helpful tool
61:20 - for answering some fundamental questions in
data science. So in sum: a little bit of math
61:28 - can help you make informed choices when planning
your analyses. Very significantly, it can
61:35 - help you find the problems and fix them when
things aren't going right. It is the ability
61:40 - to look under the hood that makes a difference.
And then truthfully, some mathematical procedures,
61:46 - like systems of linear equations, that can
even be done by hand, sometimes faster than
61:52 - you can do with a computer. So, you can save
yourself some time and some effort and move
61:56 - ahead more quickly toward your goal of insight.
Now, data science wouldn't be data science
62:03 - and its methods without a little bit of statistics.
So, I am going to give you a brief statistics
62:09 - overview here of how things work in data science.
Now, you can think of statistics as really
62:15 - an attempt to find order in chaos, find patterns
in an overwhelming mess. Sort of like trying
62:21 - to see the forest and the trees. Now, let's
go back to our little Venn Diagram here. We
62:28 - recently had math and stats here in the top
corner. We are going to go back to talking
62:31 - about stats, in particular. What you are trying
to do here; one thing is to explore your data.
62:38 - You can have exploratory graphics, because
we are visual people and it is usually easiest
62:43 - to see things. You can have exploratory statistics,
a numerical exploration of the data. And you
62:49 - can have descriptive statistics, which are
the things that most people would have talked
62:52 - about when they took a statistics class in
college (if they did that). Next, there is
62:58 - inference. I've got smoke here because you
can infer things about the wind and the air
63:03 - movement by looking at patterns in smoke.
The idea here is that you are trying to take
63:09 - information from samples and infer something
about a population. You are trying to go from
63:15 - one source to another. One common version
of this is hypothesis testing. Another common
63:21 - version is estimations, sometimes called Confidence
Intervals. There are other ways to do it,
63:25 - but all of these let you go beyond the data
at hand to making larger conclusions. Now,
63:33 - one interesting thing about statistics is
you're going to have to be concerned with
63:36 - some of the details and arranging things just
so. For instance, you get to do something
63:41 - like feature selection and that's picking
variables that should be included or combinations
63:45 - and there are problems that can come up that
are frequent problems and I will address some
63:50 - of those in later videos. There's also the
matter of validation. When you create a statistical
63:56 - model you have to see if it is actually accurate.
Hopefully, you have enough data that you can
64:01 - have a holdout sample and do that, or you
can replicate the study. Then, there is the
64:06 - choice of estimators that you use; how you
actually get the coefficients or the combinations
64:12 - in your model. And then there's ways of assessing
how well your model fits the data. All of
64:17 - these are issues that I'll address briefly
when we talk about statistical analysis at
64:22 - greater length. Now, I do want to mention
one thing in particular here, and I just call
64:28 - this "beware the trolls." There are people
out there who will tell you that if you don't
64:33 - do things exactly the way they say to do it,
that your analysis is meaningless, that your
64:39 - data is junk and you've lost all your time.
You know what? They're trolls. So, the idea
64:46 - here is don't listen to that. You can make
enough of an informed decision on your own
64:52 - to go ahead and do an analysis that is still
useful. Probably one of the most important
64:57 - things to think about in this is this wonderful
quote from a very famous statistician and
65:00 - it says, "All models or all statistical models
are wrong, but some are useful." And so the
65:08 - question isn't whether you're technically
right, or you have some sort of level of intellectual
65:13 - purity, but whether you have something that
is useful. That, by the way, comes from George
65:19 - Box. And I like to think of it basically as
this: as wave your flag, wave your "do it
65:26 - yourself" flag, and just take pride in what
you're able to accomplish even when there
65:31 - are people who may be criticizing it. Go ahead,
you're doing something, go ahead and do it.
65:36 - So, in sum: statistics allow you to explore
and describe your data. It allows you to infer
65:43 - things about the population. There is a lot
of choices available, a lot of procedures.
65:49 - But no matter what you do, the goal is useful
insight. Keep your eyes on that goal and you
65:55 - will find something meaningful and useful
in your data to help you in your own research
66:01 - and projects. Let's finish our data science
methods overview by getting a brief overview
66:08 - of Machine Learning. Now, I've got to admit
when you say the term "machine learning,"
66:13 - people start thinking something like, "the
robot overlords are going to take over the
66:17 - world." That's not what it is. Instead, let's
go back to our Venn Diagram one more time,
66:23 - and in the intersection at the top between
coding and stats is Machine Learning or as
66:29 - it's commonly called, just ML. The goal of
Machine Learning is to go and work in data
66:36 - space so you can, for instance, you can take
a whole lot of data (we've got tons of books
66:40 - here), and then you can reduce the dimensionality.
That is, take a very large, scattered, data
66:47 - set and try to find the most essential parts
of that data. Then you can use these methods
66:52 - to find clusters within the data; like goes
with like. You can use methods like k-means.
66:59 - You can also look for anomalies or unusual
cases that show up in the data space. Or,
67:06 - if we go back to categories again, I talked
about like for like. You can use things like
67:11 - logistic regression or k-nearest neighbors,
KNN. You can use Naive Bayes for classification
67:18 - or Decision Trees or SVM, which is Support
Vector Machines, or artificial neural nets.
67:25 - Any of those will help you find the patterns
and the clumping in the data so you can get
67:29 - similar cases next to each other, and get
the cohesion that you need to make conclusions
67:35 - about these groups. Also, a major element
of machine learning is predictions. You're
67:41 - going to point your way down the road. The
most common approach here; the most basic
67:45 - is linear regression, multiple regression.
There is also Poisson regression, which is
67:50 - used for modeling count or frequency data.
And then there is the issue of Ensemble models,
67:55 - where you create several models and you take
the predictions from each of those and you
67:59 - put them together to get an overall more reliable
prediction. Now, I will talk about each of
68:05 - these in a little more detail in later courses,
but for right now I mostly just want you to
68:10 - know that these things exist, and that's what
we mean when we refer to Machine Learning.
68:15 - So, in sum: machine learning can be used to
categorize cases and to predict scores on
68:21 - outcomes. And there's a lot of choices, many
choices and procedures available. But, again,
68:26 - as I said with statistics, and I'll also say
again many times after this, no matter what,
68:31 - the goal is not that "I'm going to do an artificial
neural network or a SVM," the goal is to get
68:37 - useful insight into your data. Machine learning
is a tool, and use it to the extent that it
68:43 - helps you get that insight that you need.
In the last several videos I've talked about
68:49 - the role in data science of technical things.
On the other hand, communicating is essential
68:56 - to the practice, and the first thing I want
to talk about there is interpretability. The
69:02 - idea here is that you want to be able to lead
people through a path on your data. You want
69:08 - to tell a data-driven story, and that's the
entire goal of what you are doing with data
69:14 - science. Now, another way to think about this
is: when you are doing your analysis, what
69:19 - you're trying to do is solve for value. You're
making an equation. You take the data, you're
69:23 - trying to solve for value. The trouble is
this: a lot of people get hung up on analysis,
69:29 - but they need to remember that analysis is
not the same thing as value. Instead, I like
69:35 - to think of it this way: that analysis times
story is equal to value. Now, please note
69:43 - that's multiplicative, not additive, and so
one consequence of that is when you go back
69:49 - to, analysis times story equals value. Well,
if you have zero story you're going to have
69:55 - zero value because, as you recall, anything
times zero is zero. So, instead of that let's
70:01 - go back to this and say what we really want
to do is, we want to maximize the story so
70:07 - that we can maximize the value that results
from our analysis. Again, maximum value is
70:13 - the overall goal here. The analysis, the tools,
the tech, are simply methods for getting to
70:19 - that goal. So, let's talk about goals. For
instance, an analysis is goal-driven. You
70:26 - are trying to accomplish something that's
specific, so the story, or the narrative,
70:31 - or the explanation you give about your project
should match those goals. If you are working
70:36 - for a client that has a specific question
that they want you to answer, then you have
70:41 - a professional responsibility to answer those
questions clearly and unambiguously, so they
70:46 - know whether you said yes or no and they know
why you said yes or no. Now, part of the problem
70:52 - here is the fact the client isn't you and
they don't see what you do. And as I show
70:58 - here, simply covering your face doesn't make
things disappear. You have to worry about
71:02 - a few psychological abstractions. You have
to worry about egocentrism. And I'm not talking
71:08 - about being vain, I'm talking about the idea
that you think other people see and know and
71:13 - understand what you know. That's not true;
otherwise, they wouldn't have hired you in
71:17 - the first place. And so you have to put it
in terms that the client works with, and that
71:22 - they understand, and you're going to have
to get out of your own center in order to
71:27 - do that. Also, there's the idea of false consensus;
the idea that, "well everybody knows that."
71:34 - And again, that's not true, otherwise, they
wouldn't have hired you. You need to understand
71:38 - that they are going to come from a different
background with a different range of experience
71:42 - and interpretation. You're going to have to
compensate for that. A funny little thing
71:46 - is the idea about anchoring. When you give
somebody an initial impression, they use that
71:51 - as an anchor, and then they adjust away from
it. So if you are going to try to flip things
71:55 - over on their heads, watch out for giving
a false impression at the beginning unless
72:00 - you absolutely need to. But most importantly,
in order to bridge the gap between the client
72:06 - and you, you need to have clarity and explain
yourself at each step. You can also think
72:14 - about the answers. When you are explaining
the project to the client, you might want
72:19 - to start in a very simple procedure: state
the question that you are answering. Give
72:24 - your answer to that question, and if you need
to, qualify as needed. And then, go in order
72:31 - top to bottom, so you're trying to make it
as clear as possible what you're saying, what
72:36 - the answer is, and make it really easy to
follow. Now, in terms of discussing your process,
72:42 - how you did this all. Most of the time it
is probably the case of they don't care, they
72:47 - just want to know what the answer is and that
you used a good method to get that. So, in
72:52 - terms of discussing processes or the technical
details, only when absolutely necessary. That's
72:57 - something to keep in mind. The process here
is to remember that analysis, which means
73:02 - breaking something apart. This, by the way,
is a mechanical typewriter broken into its
73:07 - individual component. Analysis means to take
something apart, and analysis of data is an
73:13 - exercise in simplification. You're taking
the overall complexity, sort of the overwhelmingness
73:19 - of the data, and you're boiling it down and
finding the patterns that make sense and serve
73:24 - the needs of your client. Now, let's go to
a wonderful quote from our friend Albert Einstein
73:29 - here, who said, "Everything should be made
as simple as possible, but not simpler." That's
73:36 - true in presenting your analysis. Or, if you
want to go see the architect and designer
73:41 - Ludwig Mies van der Rohe, who said, "Less
is more." It is actually Robert Browning who
73:47 - originally said that, but Mies van der Rohe
popularized it. Or, if you want another way
73:53 - of putting a principle that comes from my
field, I'm actually a psychological researcher;
73:59 - they talk about being minimally sufficient.
Just enough to adequately answer the question.
74:05 - If you're in commerce you know about a minimal
viable product, it is sort of the same idea
74:10 - within analysis here, the minimal viable analysis.
So, here's a few tips: when you're giving
74:17 - a presentation, more charts, less text, great.
And then, simplify the charts; remove everything
74:23 - that doesn't need to be in there. Generally,
you want to avoid tables of data because those
74:28 - are hard to read. And then, one more time
because I want to emphasize it, less text
74:33 - again. Charts, tables can usually carry the
message. And so, let me give you an example
74:39 - here. I'm going to give a very famous dataset
at Berkeley admissions. Now, these are not
74:44 - stairs at Berkeley, but it gives the idea
of trying to get into something that is far
74:48 - off and distant. Here's the data; this is
graduate school admissions in 1973, so it's
74:55 - over 40 years ago. The idea is that men and
women were both applying for graduate school
75:00 - at the University of California Berkeley.
And what we found is that 44 percent of the
75:05 - men who applied were admitted, that's their
part in green. And of the women, only 35 percent
75:12 - of women were admitted when they applied.
So, really at first glance this is bias, and
75:18 - it actually led to a lawsuit, it was a major
issue. So, what Berkeley then tried to do
75:24 - was find out, "well which programs are responsible
for this bias?" And they got a very curious
75:29 - set of results. If you break the applications
down by program (and here we are calling them
75:34 - A through F), six different programs. What
you find, actually, is that in each of these
75:40 - male applicants on the left female applicants
are on the right. If you look at program A,
75:46 - women actually got accepted at a higher rate,
and the same is true for B, and the same is
75:52 - true for D, and the same is true for F. And
so, this is a very curious set of responses
75:59 - and it is something that requires explanation.
Now in statistics, this is something that
76:03 - is known as Simpson's Paradox. But here is
the paradox: bias may be negligible at the
76:12 - department level. And in fact, as we saw in
four of the departments, there was a possible
76:16 - bias in favor of women. And the problem is
that women applied to more selective programs,
76:24 - programs with lower acceptance rates. Now,
some people stop right here and say therefore,
76:29 - "nothing is going on, nothing to complain
about." But you know, that's still ending
76:33 - the story a little bit early. There are other
questions that you can ask, and as producing
76:39 - a data-driven story, this is stuff that you
would want to do. So, for instance, you may
76:43 - want to ask, "why do the programs vary in
overall class size? Why do the acceptance
76:49 - rates differ from one program to the other?
Why do men and women apply to different programs?"
76:56 - And you might want to look at things like
the admissions criteria for each of the programs,
77:00 - the promotional strategies, how they advertise
themselves to students. You might want to
77:04 - look at the kinds of prior education the students
have in the programs, and you really want
77:08 - to look at funding level for each of the programs.
And so, really, you get one answer, at least
77:15 - more questions, maybe some more answers, and
more questions, and you need to address enough
77:19 - of this to provide a comprehensive overview
and solution to your client. In sum, let's
77:26 - say this: stories give value to data analysis.
And when you tell the story, you need to make
77:34 - sure that you are addressing your client's'
goals in a clear, unambiguous way. The overall
77:41 - principle here is be minimally sufficient.
Get to the point, make it clear. Say what
77:48 - you need to, but otherwise be concise and
make your message clear. The next step in
77:55 - discussing data science and communicating
is to talk about actionable insights, or information
78:02 - that can be used productively to accomplish
something. Now, to give sort of a bizarre
78:07 - segue here, you look at a game controller.
It may be a pretty thing, it may be a nice
78:13 - object, but remember: game controllers exist
to do something. They exist to help you play
78:19 - the game and to do it as effectively as possible.
They have a function, they have a purpose.
78:25 - Same way data is for doing. Now, that's a
paraphrase for one of my favorite historical
78:31 - figures. This is William James, the father
of American Psychology, and pragmatism is
78:37 - philosophy. And he has this wonderful quote,
he said, "My thinking is first and last and
78:43 - always for the sake of my doing." And the
idea applies to analysis. Your analysis and
78:50 - your data is for the sake of your doing. So,
you're trying to get some sort of specific
78:56 - insight in how you should proceed. What you
want to avoid is the opposite of this from
79:01 - one of my other favorite cultural heroes,
the famous Yankees catcher Yogi Berra, who
79:06 - said, "We're lost, but we're making good time."
The idea here is that frantic activity does
79:11 - not make up for lack of direction. You need
to understand what you are doing so you can
79:16 - reach the particular goal. And your analysis
is supposed to do that. So, when you're giving
79:21 - your analysis, you're going to try to point
the way. Remember, why was the project conducted?
79:28 - The goal is usually to direct some kind of
action, reach some kind of goal for your client.
79:33 - And that the analysis should be able to guide
that action in an informed way. One thing
79:42 - you want to do is, you want to be able to
give the next steps to your client. Give the
79:46 - next steps; tell them what they need to do
now. You want to be able to justify each of
79:52 - those recommendations with the data and your
analysis. As much as possible be specific,
79:58 - tell them exactly what they need to do. Make
sure it's doable by the client, that it's
80:02 - within their range of capability. And that
each step should build on the previous step.
80:08 - Now, that being said, there is one really
fundamental sort of philosophical problem
80:14 - here, and that's the difference between correlation
and causation. Basically, it goes this way:
80:22 - your data gives you correlation; you know
that this is associated with that. But your
80:29 - client doesn't simply want to know what's
associated; they want to know what causes
80:33 - something. Because if they are going to do
something, that's an intervention designed
80:36 - to produce a particular result. So, really,
how do you get from the correlation, which
80:41 - is what you have in the data, to the causation,
which is what your client wants? Well, there's
80:46 - a few ways to do that. One is experimental
studies; these are randomized, controlled
80:51 - trials. Now, that's theoretically the simplest
path to causality, but it can be really tricky
80:56 - in the real world. There are quasi-experiments,
and these are methods, a whole collection
81:01 - of methods. They use non-randomized data,
usually observational data, adjusted in particular
81:08 - ways to get an estimate of causal inference.
Or, there's the theory and experience. And
81:15 - this is research-based theory and domain-specific
experience. And this is where you actually
81:19 - get to rely on your client's information.
They can help you interpret the information,
81:25 - especially if they have greater domain expertise
than you do. Another thing to think about
81:31 - are the social factors that affect your data.
Now, you remember the data science Venn Diagram.
81:37 - We've looked at it lots of times. It has got
these three elements. Some proposed adding
81:41 - a fourth circle to this Venn diagram, and
we'll kind of put that in there and say that
81:46 - social understanding is also important, critical
really, to valid data science. Now, I love
81:55 - that idea, and I do think that it's important
to understand how things are going to play
82:00 - out. There are a few kinds of social understanding.
You want to be aware of your client's mission.
82:05 - You want to make sure that your recommendations
are consistent with your client's mission.
82:10 - Also, that your recommendations are consistent
with your client's identity; not just, "This
82:14 - is what we do," but, "This is really who we
are." You need to be aware of the business
82:19 - context, sort of the competitive environment
and the regulatory environment that they're
82:22 - working in. As well as the social context;
and that can be outside of the organization,
82:27 - but even more often within the organization.
Your recommendations will affect relationships
82:32 - within the client's organization. And you
are going to try to be aware of those as much
82:37 - as you can to make it so that your recommendations
can be realized the way they need to be. So,
82:43 - in sum: data science is goal focused, and
when you're focusing on that goal for your
82:49 - client you need to give specific next steps
that are based on your analysis and justifiable
82:55 - from the data. And in doing so, be aware of
the social, political, and economic context
83:02 - that gives you the best opportunity of getting
something really useful out of your analysis.
83:09 - When you're working in data science and trying
to communicate your results, presentation
83:13 - graphics can be an enormously helpful tool.
Think of it this way: you are trying to paint
83:19 - a picture for the benefit of your client.
Now, when you're working with graphics there
83:24 - can be a couple of different goals; it depends
on what kind of graphics you're working with.
83:29 - There's the general category of exploratory
graphics. These are ones that you are using
83:34 - as the analyst. And for exploratory graphics,
you need speed and responsiveness, and so
83:40 - you get very simple graphics. This is a base
histogram in R. And they can get a little
83:46 - more sophisticated and this is done in ggplot2.
And you can break it down into a couple other
83:52 - histograms, or you can make it a different
way, or make it see-through, or split them
83:56 - apart into small multiples. But in each case,
this is done for the benefit of you as the
84:01 - analyst understanding the data. These are
quick, they're effective. Now, they are not
84:06 - very well-labeled, and they are usually for
your insight, and then you do other things
84:11 - as a result of that. On the other hand, presentation
graphics which are for the benefit of your
84:18 - client, those need clarity and they need a
narrative flow. Now, let me talk about each
84:24 - of those characteristics very briefly. Clarity
versus distraction. There are things that
84:29 - can go wrong in graphics. Number one is color.
Colors can actually be a problem. Also, three-dimensional
84:37 - or false dimensions are nearly always a distraction.
One that gets a little touchy for some people
84:43 - is interaction. We think of interactive graphics
as really cool, great things to have, but
84:48 - you run the risk of people getting distracted
by the interaction and start playing around
84:52 - with it. Going, like, "Ooh, I press here it
does that." And that distracts from the message.
84:56 - So actually, it may be important to not have
interaction. And then the same thing is true
85:01 - of animation. Flat, static graphics can often
be more informative because they have fewer
85:08 - distractions in them. Let me give you a quick
example of how not to do things. Now, this
85:14 - is a chart that I made. I made it in Excel,
and I did it based on some of the mistakes
85:19 - I've seen in graphics submitted to me when
I teach. And I guarantee you, everything in
85:24 - here I have seen in real life, just not necessarily
combined all at once. Let's zoom in on this
85:29 - a little bit, so we can see the full badness
of this graphic. And let's see what's going
85:34 - on here. We've got a scale here that starts
at 8 goes to 28% and is tiny; doesn't even
85:40 - cover the range of the data. We've got this
bizarre picture on the wall. We've got no
85:43 - access lines on the walls. We come down here;
the labels for educational levels are in alphabetical
85:49 - order, instead of the more logical higher
levels of education. Then we've got the data
85:55 - represented as cones, which are difficult
to read and compare, and it's only made worse
85:59 - by the colors and the textures. You know,
if you want to take an extreme, this one for
86:04 - grad degrees doesn't even make it to the floor
value of 8% and this one for high school grad
86:10 - is cut off at the top at 28%. This, by the
way, is a picture of a sheep, and people do
86:17 - this kind of stuff and it drives me crazy.
If you want to see a better chart with the
86:21 - exact same data, this is it right here. It
is a straight bar chart. It's flat, it's simple,
86:28 - it's as clean as possible. And this is better
in many ways. Most effective here is that
86:35 - it communicates clearly. There's no distractions,
it's a logical flow. This is going to get
86:40 - the point across so much faster. And I can
give you another example of it; here's a chart
86:47 - previously about salaries for incomes. I have
a list here, I've got data scientist in it.
86:52 - If I want to draw attention to it, I have
the option of putting a circle around it and
86:56 - I can put a number next to it to explain it.
That's one way to make it easy to see what's
87:01 - going on. We don't even have to get fancy.
You know, I just got out a pen and a post-it
87:06 - note and I drew a bar chart of some real data
about life expectancy. This tells the story
87:13 - as well, that there is something terribly
amiss in Sierra Leone. But, now let's talk
87:18 - about creating narrative flow in your presentation
graphics. To do this, I am going to pull some
87:23 - charts from my most cited academic paper,
which is called, A Third Choice: A Review
87:29 - of Empirical Research on the Psychological
Outcomes of Restorative Justice. Think of
87:32 - it as mediation for juvenile crimes, mostly
juvenile. And this paper is interesting because
87:39 - really it's about fourteen bar charts with
just enough text to hold them together. And
87:44 - you can see there's a flow. The charts are
very simple; this is judgments about whether
87:49 - the criminal justice system was fair. The
two bars on the left are victims; the two
87:54 - bars on the right are offenders. And for each
group on the left are people who participated
87:59 - in restorative justice, so more victim-offender
mediation for crimes. And for each set on
88:05 - the right are people who went through standard
criminal procedures. It says court, but it
88:10 - usually means plea bargaining. Anyhow, it's
really easy to see that in both cases the
88:15 - restorative justice bar is higher; people
were more likely to say it was fair. They
88:20 - also felt that they had an opportunity to
tell their story; that's one reason why they
88:24 - might think it's fair. They also felt the
offender was held accountable more often.
88:29 - In fact, if you go to court on the offenders,
that one's below fifty percent and that's
88:33 - the offenders themselves making the judgment.
Then you can go to forgiveness and apologies.
88:39 - And again, this is actually a simple thing
to code and you can see there's an enormous
88:44 - difference. In fact, one of the reasons there
is such a big difference is because instead
88:47 - of court preceding, the offender very rarely
meets the victim. It also turns out I need
88:53 - to qualify this a little bit because a bunch
of the studies included drunk driving with
88:57 - no injuries or accidents. Well, when we take
them out, we see a huge change. And then we
89:04 - can go to whether a person is satisfied with
the outcome. Again, we see an advantage for
89:08 - restorative justice. Whether the victim is
still upset about the crime, now the bars
89:11 - are a little bit different. And whether they
are afraid of revictimization and that is
89:14 - over a two to one difference. And then finally
recidivism for offenders or reoffending; and
89:20 - you see a big difference there. And so what
I have here is a bunch of charts that are
89:24 - very very simple to read, and they kind of
flow in how they're giving the overall impression
89:30 - and then detailing it a little bit more. There's
nothing fancy here, there's nothing interactive,
89:35 - there's nothing animated, there's nothing
kind of flowing in seventeen different directions.
89:40 - It's easy, but it follows a story and it tells
a narrative about the data and that should
89:46 - be your major goal with the presentation graphics.
In sum: presenting, or the graphics you use
89:52 - for presenting, are not the same as the graphics
you use for exploring. They have different
89:56 - needs and they have different goals. But no
matter what you are doing, be clear in your
90:01 - graphics and be focused in what you're trying
to tell. And above all create a strong narrative
90:07 - that gives different level of perspective
and answers questions as you go to anticipate
90:13 - a client's questions and to give them the
most reliable solid information and the greatest
90:18 - confidence in your analysis. The final element
of data science and communicating that I wanted
90:25 - to talk about is reproducible research. And
you can think of it as this idea; you want
90:30 - to be able to play that song again. And the
reason for that is data science projects are
90:35 - rarely "one and done;" rather they tend to
be incremental, they tend to be cumulative,
90:41 - and they tend to adapt to these circumstances
that they're working in. So, one of the important
90:47 - things here, probably, if you want to summarize
it very briefly, is this: show your work.
90:52 - There's a few reasons for this. You may have
to revise your research at a later date, your
90:56 - own analyses. You may be doing another project
and you want to borrow something from previous
91:01 - studies. More likely you'll have to hand it
off to somebody else at a future point and
91:05 - they're going to have to be able to understand
what you did. And then there's the very significant
91:09 - issue in both scientific and economic research
of accountability. You have to be able to
91:16 - show that you did things in a responsible
way and that your conclusions are justified;
91:22 - that's for clients funding agencies, regulators,
academic reviewers, any number of people.
91:27 - Now, you may be familiar with the concept
of open data, but you may be less familiar
91:32 - with the concept of open data science; that's
more than open data. So, for instance, I'll
91:37 - just let you know there is something called
the Open Data Science Conference and ODSC.com.
91:43 - And it meets three times a year in different
places. And this is entirely, of course, devoted
91:49 - to open data science using both open data,
but making the methods transparent to people
91:55 - around them. One thing that can make this
really simple is something called the Open
91:59 - Science Framework, which is at OSF.io. It's
a way of sharing your data and your research
92:05 - with an annotation on how you got through
the whole thing with other people. It makes
92:10 - the research transparent, which is what we
need. One of my professional organizations,
92:16 - the Association for Psychological Science
has a major initiative on this called open
92:21 - practices, where they are strongly encouraging
people to share their data as much as is ethically
92:27 - permissible and to absolutely share their
methods before they even conduct a study as
92:32 - a way of getting rigorous intellectual honesty
and accountability. Now, another step in all
92:39 - of this is to archive your data, make that
information available, put it on the shelf.
92:43 - And what you want to do here is, you want
to archive all of your datasets; both the
92:48 - totally raw before you did anything with it
dataset, and every step in the process until
92:53 - your final clean dataset. Along with that,
you want to archive all of the code that you
92:58 - used in the process and analyzed the data.
If you used a programming language like R
93:02 - or Python, that's really simple. If you used
a program like SPSS you need to save the syntax
93:07 - files, and then that can be done that way.
And again, no matter what, make sure to comment
93:12 - liberally and explain yourself. Now, part
of that is you have to explain the process,
93:18 - because you are not just this lone person
sitting on the sofa working by yourself, you're
93:23 - with other people and you need to explain
why you did it the way that you did. You need
93:28 - to explain the choices, the consequences of
those choices, the times that you had to backtrack
93:33 - and try it over again. This also works into
the principle of future-proofing your work.
93:40 - You want to do a few things here. Number one;
the data. You want to store the data in non-proprietary
93:45 - formats like a CSV or Comma Separated Values
file because anything can read CSV files.
93:50 - If you stored it in the proprietary SPSS.sav
format, you might be in a lot of trouble when
93:56 - somebody tries to use it later and they can't
open it. Also, there's storage; you want to
94:00 - place all of your files in a secure, accessible
location like GitHub is probably one of the
94:06 - best choices. And then the code, you may want
to use something like a dependency management
94:12 - package like Packrat for R or Virtual Environment
for Python as a way of making sure that the
94:19 - packages that you use; that there are always
versions that work because sometimes things
94:23 - get updated and it gets broken. This is a
way of making sure that the system that you
94:28 - have will always work. Overall, you can think
of this too: you want to explain yourself
94:34 - and a neat way to do that is to put your narrative
in a notebook. Now, you can have a physical
94:40 - lab book or you can also do digital books.
A really common one, especially if you're
94:45 - using Python, is Jupyter with a "y" there
in the middle. Jupyter notebooks are interactive
94:51 - notebooks. So, here's a screenshot of one
that I made in Python, and you have titles,
94:58 - you have text, you have the graphics. If you
are working in R, you can do this through
95:02 - something called RMarkdown. Which works in
the same way you do it in RStudio, you use
95:07 - Markdown and you can annotate it. You can
get more information about that at rmarkdown.rstudio.com.
95:14 - And so for instance, here's an R analysis
I did, and as you can see the code on the
95:20 - left and you see the markdown version on the
right. What's neat about this is that this
95:24 - little bit of code here, this title and this
text and this little bit of R code, then is
95:31 - displayed as this formatted heading, as this
formatted text, and this turns into the entire
95:37 - R output right there. It's a great way to
do things. And if you do RMarkdown, you actually
95:42 - have the option of uploading the document
into something called RPubs; and that's an
95:48 - online document that can be made accessible
to anybody. Here's a sample document. And
95:53 - if you want to go see it, you can go to this
address. It's kind of long, so I am going
95:57 - to let you write that one down yourself. But,
in sum: here's what we have. You want to do
96:05 - your work and archive the information in a
way that supports collaboration. Explain your
96:11 - choices, say what you did, show how you did
it. This allows you to future-proof your work,
96:16 - so it will work in other situations for other
people. And as much as possible, no matter
96:21 - how you do it, make sure you share your narrative
so people understand your process and they
96:26 - can see that your conclusions are justifiable,
strong and reliable. Now, something I've mentioned
96:33 - several times when talking about data science,
and I'll do it again in this conclusion, is
96:38 - that it's important to give people next steps.
And I'm going to do that for you right now.
96:42 - If you're wondering what to do after having
watched this very general overview course,
96:46 - I can give you a few ideas. Number one, maybe
you want to start trying to do some coding
96:51 - in R or Python; we have courses for those.
You might want to try doing some data visualization,
96:58 - one of the most important things that you
can do. You may want to brush up on statistics
97:02 - and maybe some math that goes along with it.
And you may want to try your hand at machine
97:07 - learning. All of these will get you up and
rolling in the practice of data science. You
97:14 - can also try looking at data sourcing, finding
information that you are going to do. But,
97:18 - no matter what happens try to keep it in context.
So, for instance, data science can be applied
97:25 - to marketing, and sports, and health, and
education, and the arts, and really a huge
97:32 - number of other things. And we will have courses
here at datalab.cc that talk about all of
97:38 - those. You may also want to start getting
involved in the community of data science.
97:44 - One of the best conferences that you can go
to is O'Reilly Strata, which meets several
97:48 - times a year around the globe. There's also
Predictive Analytics World, again several
97:53 - times a year around the world. Then there's
much smaller conferences, I love Tapestry
97:59 - or tapestryconference.com, which is about
storytelling in data science. And Extract,
98:06 - a one-day conference about data stories that
is put on by import.io, one of the great data
98:13 - sourcing applications that's available for
scraping web data. If you want to start working
98:18 - with actual data, a great choice is to go
to Kaggle.com and they sponsor data science
98:25 - competitions, which actually have cash rewards.
There's also wonderful data sets you can work
98:30 - with there to find out how they work and compare
your results to those of other people. And
98:36 - once you are feeling comfortable with that,
you may actually try turning around and doing
98:40 - some service; datakind.org is the premier
organization for data science as humanitarian
98:47 - service. They do major projects around the
world. I love their examples. There are other
98:52 - things you can do; there's an annual event
called Do Good Data, and then datalab.cc will
98:58 - be sponsoring twice-a-year data charrettes,
which are opportunities for people in the
99:03 - Utah area to work with the local nonprofits
on their data. But above all of this, I want
99:10 - you to remember this one thing: data science
is fundamentally democratic. It's something
99:16 - that everybody needs to learn to do in some
way, shape or form. The ability to work with
99:21 - data is a fundamental ability and everybody
would be better off by learning to work with
99:27 - data intelligently and sensitively. Or, to
put it another way: data science needs you.
99:34 - Thanks so much for joining me in this introductory
course and I hope it has been good and I look
99:38 - forward to seeing you in the other courses
here at datalab.cc. Welcome to "Data Sourcing".
99:46 - I'm Barton Poulson and in this course, we're
going to talk about Data Opus or that's Latin
99:52 - for Data Needed. The idea here is that no
data, no data science; and that is a sad thing.
100:00 - So, instead of leaving it at that we're going
to use this course to talk about methods for
100:05 - measuring and evaluating data and methods
for accessing existing data and even methods
100:11 - for creating new, custom data. Take those
together and it's a happy situation. At the
100:17 - same time, we'll do all of this still at an
accessible, conceptual and non-technical level
100:24 - because the technical hands-on stuff will
happen in later other courses. But for now,
100:31 - let's talk data. For data sourcing, the first
thing we want to talk about is measurement.
100:38 - And within that category, we're going to talk
about metrics. The idea here is that you actually
100:44 - need to know what your target is if you want
to have a chance to hit it. There's a few
100:50 - particular reasons for this. First off, data
science is action-oriented; the goal is to
100:56 - do something as opposed to simply understand
something, which is something I say as an
101:01 - academic practitioner. Also, your goal needs
to be explicit and that's important because
101:07 - the goals can guide your effort. So, you want
to say exactly what you are trying to accomplish,
101:11 - so you know when you get there. Also, goals
exist for the benefit of the client, and they
101:17 - can prevent frustration; they know what you're
working on, they know what you have to do
101:21 - to get there. And finally, the goals and the
metrics exist for the benefit of the analyst
101:27 - because they help you use your time well.
You know when you're done, you know when you
101:32 - can move ahead with something, and that makes
everything a little more efficient and a little
101:37 - more productive. And when we talk about this
the first thing you want to do is try to define
101:44 - success in your particular project or domain.
Depending on where you are, in commerce that
101:50 - can include things like sales, or click-through
rates, or new customers. In education it can
101:56 - include scores on tests; it can include graduation
rates or retention. In government, it can
102:02 - include things like housing and jobs. In research,
it can include the ability to serve the people
102:08 - that you're to better understand. So, whatever
domain you're in there will be different standards
102:15 - for success and you're going to need to know
what applies in your domain. Next, are specific
102:22 - metrics or ways of measuring. Now again, there
are a few different categories here. There
102:28 - are business metrics, there are key performance
indicators or KPIs, there are SMART goals
102:34 - (that's an acronym), and there's also the
issue of having multiple goals. I'll talk
102:37 - about each of those for just a second now.
First off, let's talk about business metrics.
102:44 - If you're in the commercial world there are
some common ways of measuring success. A very
102:51 - obvious one is sales revenue; are you making
more money, are you moving the merchandise,
102:56 - are you getting sales. Also, there's the issue
of leads generated, new customers, or new
103:01 - potential customers because that, then, in
turn, is associated with future sales. There's
103:07 - also the issue of customer value or lifetime
customer value, so you may have a small number
103:13 - of customers, but they all have a lot of revenue
and you can use that to really predict the
103:19 - overall profitability of your current system.
And then there's churn rate, which has to
103:24 - do with, you know, losing and gaining new
customers and having a lot of turnover. So,
103:29 - any of these are potential ways for defining
success and measuring it. These are potential
103:34 - metrics, there are others, but these are some
really common ones. Now, I mentioned earlier
103:39 - something called a key performance indicator
or KPI. KPIs come from David Parmenter and
103:46 - he's got a few ways of describing them, he
says a key performance indicator for business.
103:51 - Number one should be nonfinancial, not just
the bottom line, but something else that might
103:56 - be associated with it or that measures the
overall productivity of the association. They
104:01 - should be timely, for instance, weekly, daily,
or even constantly gathered information. They
104:07 - should have a CEO focus, so the senior management
teams are the ones who generally make the
104:13 - decisions that affect how the organization
acts on the KPIs. They should be simple, so
104:19 - everybody in the organization, everybody knows
what they are and knows what to do about them.
104:23 - They should be team-based, so teams can take
joint responsibility for meeting each one
104:29 - of the KPIs. They should have significant
impact, what that really means is that they
104:35 - should affect more than one important outcome,
so you can do profitably and market reach
104:41 - or improved manufacturing time and fewer defects.
And finally, an ideal KPI has a limited dark
104:49 - side, that means there's fewer possibilities
for reinforcing the wrong behaviors and rewarding
104:55 - people for sort of exploiting the system.
Next, there are SMART goals, where SMART stands
105:02 - for Specific, Measurable, Assignable to a
particular person, Realistic (meaning you
105:10 - can actually do it with the resources you
have at hand), and Time-bound, (so you know
105:14 - when it can get done). So, whenever you form
a goal you should try to assess it on each
105:19 - of these criteria and that's a way of saying
that this is a good goal to be used as a metric
105:25 - for the success of our organization. Now,
the trick, however, is when you have multiple
105:31 - goals, multiple possible endpoints. And the
reason that's difficult is because, well,
105:37 - it's easy to focus on one goal if you're just
trying to maximize revenue or if you're just
105:42 - trying to maximize graduation rate. There's
a lot of things you can do. It becomes more
105:48 - difficult when you have to focus on many things
simultaneously, especially because some of
105:53 - these goals may conflict. The things that
you do to maximize one may impair the other.
105:58 - And so when that happens, you actually need
to start engaging in a deliberate process
106:03 - of optimization, you need to optimize. And
there are ways that you can do this if you
106:08 - have enough data; you can do mathematical
optimization to find the ideal balance of
106:13 - efforts to pursue one goal and the other goal
at the same time. Now, this is a very general
106:20 - summary and let me finish with this. In sum,
metrics or methods for measuring can help
106:26 - awareness of how well your organization is
functioning and how well you're reaching your
106:31 - goals. There are many different methods available
for defining success and measuring progress
106:37 - towards those things. The trick, however,
comes when you have to balance efforts to
106:41 - reach multiple goals simultaneously, which
can bring in the need for things like optimization.
106:49 - When talking about data sourcing and measurement,
one very important issue has to do with the
106:54 - accuracy of your measurements. The idea here
is that you don't want to have to throw away
106:59 - all your ideas; you don't want to waste effort.
One way of doing this in a very quantitative
107:05 - fashion is to make a classification table.
So, what that looks like is this, you talk
107:12 - about, for instance, positive results, negative
results... and in fact let's start by looking
107:18 - at the top here. The middle two columns here
talk about whether an event is present, whether
107:22 - your house is on fire, or whether a sale occurs,
or whether you have got a tax evader, whatever.
107:28 - So, that's whether a particular thing is actually
happening or not. On the left here, is whether
107:34 - the test or the indicator suggests that the
thing is or is not happening. And then you
107:40 - have these combinations of true positives;
where the test says it's happening and it
107:44 - really is, and false positives; where the
test says it happening, but it is not, and
107:50 - then below that true negatives, where the
test says it isn't happening and that's correct
107:54 - and then false negatives, where the test says
there's nothing going on, but there is in
107:58 - fact the event occurring. And then you start
to get the column totals, the total number
108:03 - of events present or absent, then the row
totals about the test results. Now, from this
108:11 - table what you get is four kinds of accuracy,
or really four different ways of quantifying
108:16 - accuracy using different standards. And they
go by these names: sensitivity, specificity,
108:24 - positive predictive value, and negative predictive
value. I'll show you very briefly how each
108:30 - of them works. Sensitivity can be expressed
this way, if there's a fire does the alarm
108:36 - ring? You want that to happen. And so, that's
a matter of looking at the true positives
108:42 - and dividing that by the total number of alarms.
So, the test positive means there's an alarm
108:49 - and the event present means there's a fire;
you want it to always have an alarm when there's
108:53 - a fire. Specificity, on the other hand, is
sort of the flip side of this. If there isn't
108:59 - a fire, does the alarm stay quiet? This is
where you're looking at the ratio of true
109:05 - negatives to total absent events, where there's
no fire, and the alarms aren't ringing, and
109:12 - that's what you want. Now, those are looking
at columns; you can also go sideways across
109:18 - rows. So, the first one there is positive
predictive value, often abbreviated as PPV,
109:25 - and we flip around the order a little bit.
This one says, if the alarm rings, was there
109:30 - a fire? So, now you're looking at the true
positives and dividing it by the total number
109:36 - of positives. Total number of positives is
any time the alarm rings. True positives are
109:40 - because there was a fire. And negative predictive
value, or NPV, says of the alarm doesn't ring,
109:48 - does that in fact mean that there is no fire?
Well, here you are looking at true negatives
109:54 - and dividing it by total negatives, the time
that it doesn't ring. And again, you want
109:58 - to maximize that so the true negatives account
for all of the negatives, the same way you
110:02 - want the true positives to account for all
of the positives and so on. Now, you can put
110:07 - numbers on all of these going from zero percent
to a 100% and the idea is to maximize each
110:12 - one as much as you can. So, in sum, from these
tables we get four kinds of accuracy and there's
110:20 - a different focus for each one. But, the same
overall goal, you want to identify the true
110:27 - positives and true negatives and avoid the
false positives and the false negatives. And
110:31 - this is one of way of putting numbers on,
an index really, on the accuracy of your measurement.
110:41 - Now data sourcing may seem like a very quantitative
topic, especially when we're talking about
110:45 - measurement. But, I want measure one important
thing here, and that is the social context
110:50 - of measurement. The idea here really, is that
people are people, and they all have their
110:56 - own goals, and they're going their own ways.
And we all have our own thoughts and feelings
111:00 - that don't always coincide with each other,
and this can affect measurement. And so, for
111:05 - instance, when you're trying to define your
goals and you're trying to maximize them you
111:09 - want to look at things like, for instance,
the business model. An organization's business
111:14 - model, the way they conduct their business,
the way they make their money, is tied to
111:19 - its identity and its reason to be. And if
you make a recommendation and it'scontrary
111:25 - to their business model, that can actually
be perceived as a threat to their core identity,
111:28 - and people tend to get freaked out in that
situation. Also, restrictions, so for instance,
111:35 - there may be laws, policies, and common practices,
both organizationally and culturally, that
111:41 - may limit the ways the goals can be met. Now,
most of these make a lot of sense, so the
111:47 - idea is you can'tjust do anything you want,
you need to have these constraints. And when
111:52 - you make your recommendations, maybe you'll
work creatively in them as long as you're
111:56 - still behaving legally and ethically, but
you do need to be aware of these constraints.
112:02 - Next, is the environment. And the idea here
is that competition occurs both between organizations,
112:09 - that company here is trying to reach a goal,
but they're competing with company B over
112:13 - there, but probably even more significantly
there is competition within the organization.
112:18 - This is really a recognition of office politics.
And when you, as a consultant, make a recommendation
112:24 - based on your analysis, you need to understand
you're kind of dropping a little football
112:27 - into the office and things are going to further
one person's career, maybe to the detriment
112:32 - of another. And in order for your recommendations
to have maximum effectiveness they need to
112:37 - play out well in the office. That's something
that you need to be aware of as you're making
112:42 - your recommendations. Finally, there's the
issue of manipulation. And a sad truism about
112:49 - people is that any reward system, any reward
system at all, will be exploited and people
112:55 - will generally game the system. This happens
especially when you have a strong cut off;
113:01 - you need to get at least 80 percent, or you
get fired and people will do anything to make
113:08 - their numbers appear to be eighty percent.
This happens an awful lot when you look at
113:12 - executive compensation systems, it looks a
lot when you have very high stake school testing,
113:18 - it happens in an enormous number of situations,
and so, you need to be aware of the risk of
113:23 - exploitation and gaming. Now, don't think,
then, that all is lost. Don't give up, you
113:29 - can still do really wonderful assessment,
you can get good metrics, just be aware of
113:35 - these particular issues and be sensitive to
them as you both conduct your research and
113:40 - as you make your recommendations. So, in sum,
social factors affect goals and they affect
113:46 - the way you meet those goals. There are limits
and consequences, both on how you reach the
113:52 - goals and how, really, what the goal should
be and that when you're making advice on how
113:57 - to reach those goals please be sensitive to
how things play out with metrics and how people
114:04 - will adapt their behavior to meet the goals.
That way you can make something that's more
114:08 - likely to be implemented the way you meant
and more likely to predict accurately what
114:14 - can happen with your goals. When it comes
to data sourcing, obviously the most important
114:20 - thing is to get data. But the easiest way
to do that, at least in theory, is to use
114:26 - existing data. Think of it as going to the
bookshelf and getting the data that you have
114:32 - right there at hand. Now, there's a few different
ways to do this: you can get in-house data,
114:37 - you can get open data, and you can get third-party
data. Another nice way to think of that is
114:43 - proprietary, public, and purchased data; the
three Ps I've heard it called. Let's talk
114:50 - about each of these a little bit more. So,
in-house data, that's stuff that's already
114:55 - in your organization. What's nice about that,
it can be really fast and easy, it's right
114:59 - there and the format may be appropriate for
the kind of software in the computer that
115:05 - you are using. If you're fortunate, there's
good documentation, although sometimes when
115:10 - it's in-house people just kind of throw it
together, so you have to watch out for that.
115:14 - And there's the issue of quality control.
Now, this is true with any kind of data, but
115:18 - you need to pay attention with in-house, because
you don't know the circumstances necessarily
115:23 - under which people gathered the data and how
much attention they were paying to something.
115:27 - There's also an issue of restrictions; there
may be some data that, while it is in-house,
115:31 - you may not be allowed to use, or you may
not be able to publish the results or share
115:36 - the results with other people. So, these are
things that you need to think about when you're
115:42 - going to use in-house data, in terms of how
can you use it to facilitate your data science
115:48 - projects. Specifically, there are a few pros
and cons. In-house data is potentially quick,
115:54 - easy, and free. Hopefully it's standardized;
maybe even the original team that conducted
115:59 - this study is still there. And you might have
identifiers in the data which make it easier
116:04 - for you to do an individual level analysis.
On the con side however, the in-house data
116:10 - simply may not exist, maybe it's just not
there. Or the documentation may be inadequate
116:16 - and of course, the quality may be uncertain.
Always true, but may be something you have
116:21 - to pay more attention to when you're using
in-house data. Now, another choice is open
116:27 - data like going to the library and getting
something. This is prepared data that's freely
116:32 - available, consists of things like government
data and corporate data and scientific data
116:38 - from a number of sources. Let me show you
some of my favorite open data sources just
116:44 - so you know where they are and that they exist.
Probably, the best one is data.gov here in
116:49 - the US. That is the, as it says right here,
the home of the US government's open data.
116:55 - Or, you may have a state level one. For instance,
I'm in Utah and we have data.utah.gov, also
117:02 - a great source of more regional information.
If you're in Europe, you have open-data.europa.eu,
117:07 - the European Union open data portal. And then
there are major non-profit organizations,
117:16 - so the UN has unicef.org/statistics for their
statistical and monitoring data. The World
117:23 - Health Organization has the global health
observatory at who.int/gho. And then there
117:32 - are private organizations that work in the
public interest, such as the Pew Research
117:37 - Center, which shares a lot of its data sets
and the New York Times, which makes it possible
117:43 - to use APIs to access a huge amount of the
data of things they've published over a huge
117:49 - time span. And then two of the mother loads,
there's Google, which at google.com has public
117:56 - data which is a wonderful thing. And then
Amazon at aws.amazon.com/datasets has gargantuan
118:04 - datasets. So, if you needed a data set that
was like five terabytes in size, this is the
118:08 - place that you would go to get it. Now, there's
some pros and cons to using this kind of open
118:14 - data. First, is that you can get very valuable
datasets that maybe cost millions of dollars
118:19 - to gather and to process. And you can get
a very wide range of topics and times and
118:24 - groups of people and so on. And often, the
data is very well formatted and well documented.
118:31 - There are, however, a few cons. Sometimes
there's biased samples. Say, for instance,
118:36 - you only get people who have internet access,
and that can mean, not everybody. Sometimes
118:42 - the meaning of the data is not clear or it
may not mean exactly what you want it to.
118:47 - A potential problem is that sometimes you
may need to share your analyses and if you
118:51 - are doing proprietary research, well, it's
going to have to be open instead, so that
118:57 - can create a crimp with some of your clients.
And then finally there are issues with privacy
119:02 - and confidentiality and in public data that
usually means that the identifiers are not
119:07 - there and you are going to have to work at
a larger aggregate level of measurement. Another
119:12 - option is to use data from a third-party,
these go by the name Data as a Service or
119:19 - DaaS. You can also call them data brokers.
And the thing about data brokers is they can
119:22 - give you an enormous amount of data on many
different topics, plus they can save you some
119:27 - time and effort, by actually doing some of
the processing for you. And that can include
119:32 - things like consumer behaviors and preferences,
they can get contact information, they can
119:37 - do marketing identity and finances, there's
a lot of things. There's a number of data
119:42 - brokers around, here's a few of them. Acxiom
is probably the biggest one in terms of marketing
119:49 - data. There's also Nielsen which provides
data primarily for media consumption. And
119:55 - there's another organization Datasift, that's
a smaller newer one. And there's a pretty
120:01 - wide range of choices, but these are some
of the big ones. Now, the thing about using
120:05 - data brokers, there's some pros and there's
some cons. The pros are first, that it can
120:10 - save you a lot of time and effort. It can
also give you individual level data which
120:16 - can be hard to get from open data. Open data
is usually at the community level; they can
120:21 - give you information about specific consumers.
They can even give you summaries and inferences
120:27 - about things like credit scores and marital
status. Possibly even whether a person gambles
120:31 - or smokes. Now, the con is this, number 1
it can be really expensive, I mean this is
120:37 - a huge service; it provides a lot of benefit
and is priced accordingly. Also, you still
120:43 - need to validate it, you still need to double
check that it means what you think it means
120:47 - and that it works in with what you want. And
probably the real sticking point here is the
120:53 - use of third-party data is distasteful to
many people, and so you have to be aware that
120:59 - as you're making your choices. So, in sum,
as far as data sourcing existing data goes
121:05 - obviously data science needs data and there's
the three Ps of data sources, Proprietary
121:12 - and Public and Purchased. But no matter what
source you use, you need to pay attention
121:17 - to quality and to the meaning and the usability
of the data to help you along in your own
121:23 - projects. When it comes to data sourcing,
a really good way of getting data is to use
121:30 - what are called APIs. Now, I like to think
of these as the digital version of Prufrock's
121:37 - mermaids. If you're familiar with the love
song on J. Alfred Prufrock by TS Eliot, he
121:42 - says, "I have heard the mermaids singing,
each to each," that's TS Eliot. And I like
121:48 - to adapt that to say, "APIs have heard apps
singing, each to each," and that's by me.
121:54 - Now, more specifically when we talk about
an API, what we're talking about is something
121:59 - called Application Programming Interface,
and this is something that allows programs
122:04 - to talk to each other. Its most important
use, in terms of data science, is it allows
122:09 - you to get web data. It allows your program
to directly go to the web, on its own, grab
122:15 - the data, bring it back in almost as though
it were local data, and that's a really wonderful
122:20 - thing. Now, the most common version of APIs
for data science are called REST APIs; that
122:27 - stands for Representational State Transfer.
That's the software architectural style of
122:32 - the world wide web and it allows you to access
data on web pages via HTTP, that's the hypertext
122:39 - transfer protocol. They, you know, run the
web as we know it. And when you download the
122:45 - data that you usually get its in JSON format,
that stands for Javascript Object Notation.
122:51 - The nice thing about that is that's human
readable, but it's even better for machines.
122:56 - Then you can take that information and you
can send it directly to other programs. And
123:01 - the nice thing about REST APIs is that they're
what is called language agnostic, meaning
123:07 - any programming language can call a REST API,
can get data from the web, and can do whatever
123:13 - it needs to with it. Now, there are a few
kinds of APIs that are really common. The
123:18 - first is what are called Social APIs; these
are ways of interfacing with social networks.
123:23 - So, for instance, the most common one is Facebook;
there's also Twitter. Google Talk has been
123:28 - a big one and FourSquare as well and then
SoundCloud. These are on lists of the most
123:32 - popular ones. And then there are also what
are called Visual APIs, which are for getting
123:37 - visual data, so for instance, Google Maps
is the most common, but YouTube is something
123:42 - that accesses YouTube on a particular website
or AccuWeather which is for getting weather
123:48 - information. Pinterest for photos, and Flickr
for photos as well. So, these are some really
123:53 - common APIs and you can program your computer
to pull in data from any of these services
124:00 - and sites and integrate it into your own website
or here into your own data analysis. Now,
124:08 - there's a few different ways you can do this.
You can program it in R, the statistical programming
124:13 - language, you can do it in Python, also you
can even use it in the very basic BASH command
124:20 - line interface, and there's a ton of other
applications. Basically, anything can access
124:25 - an API one way or another. Now, I'd like to
show you how this works in R. So, I'm going
124:30 - to open up a script in RStudio and then I'm
going to use it to get some very basic information
124:35 - from a webpage. Let me go to RStudio and show
you how this works. Let me open up a script
124:41 - in RStudio that allows me to do some data
sourcing here. Now, I'm just going to use
124:47 - a package called JSON Lite, I'm going to load
that one up, and then I'm going to go to a
124:53 - couple of websites. I'm going to getting historical
data from Formula One car races and I'm going
124:59 - to be getting it from Ergast.com. Now, if
we go to this page right here, I can go straight
125:04 - to my browser right now. And this is what
it looks like; it gives you the API documentation,
125:12 - so what you're doing for an API, is you're
just entering a web address and in that web
125:17 - address it includes the information you want.
I'll go back to R here just for a second.
125:22 - And if I want to get information about 1957
races in JSON format, I go to this address.
125:29 - I can skip over to that for a second, and
what you see is it's kind of a big long mess
125:38 - here, but it is all labeled and it is clear
to the computer what's going on here. Let's
125:43 - go back to R. And so what I'm going to do
is, I am going to save that URL into an object
125:52 - here, in R, and then I'm going to use the
command from JSON to read that URL and save
125:58 - it into R. And which it has now done. And
I'm going to zoom in on that so you can see
126:04 - what's happened. I've got this sort of mess
of text, this is actually a list object in
126:09 - R. And then I'm going to get just the structure
of that object, so I'm going to do this one
126:16 - right here and you can see that it's a list
and it gives you the names of all the variables
126:21 - within each one of the lists. And what I'm
going to do is, I'm going to convert that
126:26 - list to a data frame. I went through the list
and found where the information I wanted was
126:32 - located, you have to use this big long statement
here, that will give me the names of the drivers.
126:38 - Let me zoom in on that again. There they are.
And then I'm going to get just the column
126:46 - names for that bit of the data frame. So,
what I have here is six different variables.
126:52 - And then what I'm going to do is, I'm going
to pick just the first five cases and I'm
126:55 - going to select some variables and put them
in a different order. And when I do that,
127:02 - this is what I get. I will zoom in on that
again. And the first five people listed in
127:07 - this data set that I pulled from 1957, are
Juan Fangio, makes sense one of the greatest
127:14 - drivers ever, and other people who competed
in that year. And so what I've done is by
127:19 - using this API call in R, a very simple thing
to do, I was able to pull data off that webpage
127:25 - in a structured format, and do a very simple
analysis with it. And let's sum up what we've
127:31 - learned from all this. First off, APIs make
it really easy to work with web data, they
127:37 - structure, they call it for you, and then
they feed it straight into the program for
127:41 - you to analyze. And they are one of the best
ways of getting data and getting started in
127:46 - data science. When you're looking for data,
another great way of getting data is through
127:55 - scraping. And what that means is pulling information
from webpages. I like to think of it as when
128:00 - data is hiding in the open; it's there, you
can see it, but there's not an easy, immediate
128:06 - way to get that data. Now, when you're dealing
with scraping, you can get data in several
128:11 - different formats. You can get HTML text from
webpages, you can get HTML tables from the
128:17 - rows and columns that appear on webpages.
You can scrape data from PDFs, and you can
128:21 - scrape data from all sorts of data from images
and video and audio. Now, we will make one
128:27 - very important qualification before we say
anything else: pay attention to copyright
128:32 - and privacy. Just because something is on
the web, doesn't mean you're allowed to pull
128:38 - it out. Information gets copyrighted, and
so when I use examples here, I make sure that
128:43 - this is stuff that's publicly available, and
you should do the same when you are doing
128:47 - your own analyses. Now, if you want to scrape
data there's a couple of ways to do it. Number
128:52 - one, is to use apps that are developed for
this. So, for instance, import.io is one of
128:57 - my favorites. It is both a webpage, that's
its address, and it's a downloadable app.
129:01 - There's also ScraperWiki. There's an application
called Tabula, and you can even do scraping
129:07 - in Google Sheets, which I will demonstrate
in a second, and Excel. Or, if you don't want
129:12 - to use an app or if you want to do something
that apps don't really let you do, you can
129:15 - code your scraper. You can do it directly
in R, or Python, or Bash, or even Java or
129:22 - PHP. Now, what you're going to do is you're
going to be looking for information on the
129:29 - webpage. If you're looking for HTML text,
what you're going to do is pull structured
129:34 - text from webpages, similar to how a reader
view works in a browser. It uses HTML tags
129:41 - on the webpage to identify what's the important
information. So, there's things like body,
129:46 - and h1 for header one, and p for paragraph,
and the angle brackets. You can also get information
129:52 - from HTML tables, although this is a physical
table of rows and columns I am showing you.
129:57 - This also uses HTML table tags, that is like
table, and tr for table row, and td for table
130:04 - data, that's the cell. The trick is when you're
doing this, you need the table number and
130:08 - sometimes you just have to find that through
trial and error. Let me give you an example
130:11 - of how this works. Let's take a look at this
Wikipedia page on the Iron Chef America Competition.
130:19 - I'm going to go to the web right now and show
you that one. So, here we are in Wikipedia,
130:24 - Iron Chef America. And if you scroll down
a little bit, you see we have got a whole
130:28 - bunch of text here, we have got our table
of contents, and then we come down here, we
130:32 - have a table that lists the winners, the statistics
for the winners. And let's say we want to
130:38 - pull that from this webpage into another program
for us to analyze. Well, there is an extremely
130:44 - easy way to do this with Google Sheets. All
we need to do is open up the Google Sheet
130:50 - and in cell A1 of that Google Sheet, we paste
in this formula. It's IMPORTHTML, then you
130:58 - give the webpage and then you say that you
are importing a table, you have to put that
131:01 - stuff in quotes, and the index number for
the table. I had to poke around a little bit
131:05 - to figure out this was table number 2. So,
let me go to Google Sheets and show you how
131:10 - this works. Here I have a Google Sheet and
right now it's got nothing in it. But watch
131:15 - this; if I come here to this cell, and I simply
paste in that information, all the stuff just
131:21 - sort of magically propagates into the sheet,
makes it extremely easy to deal with, and
131:27 - now I can, for instance, save this as a CSV
file, put it in another program. Lots of options.
131:32 - And so this is one way that I'm scraping the
data from a webpage because I didn't use an
131:37 - API, but I just used a very simple, one-link
command to get the information. Now, that
131:44 - was a HTML table. You can also scrape data
from PDFs. You have to be aware of if it's
131:50 - a native PDF, I call that a text PDF, or a
scanned or imaged PDF. And what it does with
131:55 - native PDFs, it looks for text elements; again
those are like code that indicates this is
132:00 - text. And you can deal with Raster images,
that's pixel images, or vector, which draws
132:05 - the lines, and that's what makes them infinitely
scalable in many situations. And then in PDFs,
132:11 - you can deal with tabular data, but you probably
have to use a specialized program like Scraper,
132:16 - Wiki, or Tabula in order to get that. And
then finally media, like images and video
132:22 - and audio. Getting images is easy; you can
download them in a lot of different ways.
132:27 - And then if you want to read data from them,
say for instance, you have a heat map of a
132:31 - country, you can go through it, but you will
probably have to write a program that loops
132:34 - through the image pixel-by-pixel to read the
data and them encode it numerically into your
132:40 - statistical program. Now, that's my very brief
summary and let's summarize that. First off,
132:47 - if the data you are trying to get at doesn't
have an existing API, you can try scraping
132:52 - and you can write code in a language like
R or Python. But, no matter what you do, be
132:58 - sensitive to issues of copyright and privacy,
so you don't get yourself in hot water, but
133:03 - instead, you make an analysis that can be
of use to you or to your client. The next
133:10 - step in data sourcing is making data. And
specifically, we're talking about getting
133:14 - new data. I like to think of this as, you're
getting your hands on and you're getting "data
133:19 - de novo," new data. So, can't find the data
that you need for your analysis? Well, one
133:26 - simple solution is, do it yourself. And we're
going to talk about a few general strategies
133:32 - used for doing that. Now, these strategies
vary on a few dimensions. First off is the
133:37 - role. Are you passive and simply observing
stuff that's happening already, or are you
133:41 - active where you play a role in creating the
situation to get the data? And then there's
133:47 - the "Q/Q question," and that is, are you going
to get quantitative, or numerical, data, or
133:53 - are you going to get qualitative data, which
usually means text, paragraphs, sentences
133:58 - as well as things like photos and videos and
audio? And also, how are you going to get
134:03 - the data? Do you want to get it online, or
do you want to get it in person? Now, there's
134:08 - other choices than these, but these are some
of the big delineators of the methods. When
134:13 - you look at those, you get a few possible
options. Number one is interviews, and I'll
134:17 - say more about those. Another one is surveys.
A third one is card sorting. And a fourth
134:24 - one is experiments, although I actually want
to split experiments into two kinds of categories.
134:29 - The first one is laboratory experiments, and
that's in-person projects where you shape
134:34 - the information or an experience for the participants
as a way of seeing how that involvement changes
134:41 - their reactions. It doesn't necessarily mean
that you're a participant, but you create
134:46 - the situation. And then there's also A/B testing.
This is automated, online testing of two or
134:52 - more variations on a webpage. It's a very,
very simple kind of experimentation that's
134:58 - actually very useful for optimizing websites.
So, in sum, from this very short introduction
135:06 - make sure you can get exactly what you need.
Get the data you need to answer your question.
135:11 - And if you can't find it somewhere, then make
it. And, as always, you have many possible
135:17 - methods, each of which have their own strengths
and their own compromises. And we'll talk
135:21 - about each of those in the following sections.
The first method of data sourcing where you're
135:28 - making new data that I want to talk about
is interviews. And that's not because it's
135:33 - the most common, but because it's the one
you would do for the most basic problem. Now,
135:38 - basically an interview is nothing more than
a conversation with another person or a group
135:43 - of people. And, the fundamental question is,
why do interviews as opposed to doing a survey
135:49 - or something else? Well, there's a few good
reasons to do that. Number one: you're working
135:56 - with a new topic and you don't know what people's
responses will be, how they'll react. And
136:01 - so you need something very open-ended. Number
two: you're working with a new audience and
136:06 - you don't know how they will react in particular
to what it is you're trying to do. And number
136:11 - three: something's going on with the current
situation, it's not working anymore, and you
136:15 - need to find what's going on, and you need
to find ways to improve. The open-ended information
136:21 - where you get past you're existing categories
and boundaries can be one of the most useful
136:25 - methods for getting that data. If you want
to put it another way, you want to do interviews
136:30 - when you don't want to constrain responses.
Now, when it comes to interviews, you have
136:36 - one very basic choice, and that's whether
you do a structured interview. And with a
136:41 - structured interview, you have a predetermined
set of questions, and everyone gets the same
136:47 - questions in the same order. It gives a lot
of consistency even though the responses are
136:52 - open-ended. And then you can also have what's
called an unstructured interview. And this
136:57 - is a whole lot more like a conversation where
you as the interviewer and the person you're
137:02 - talking to - your questions arise in response
to their answers. Consequently, an unstructured
137:09 - interview can be different for each person
that you talk to. Also, interviews are usually
137:15 - done in person, but not surprisingly, they
can be done over the phone, or often online.
137:22 - Now, a couple of things to keep in mind about
interviews. Number one is time. Interviews
137:27 - can range from just a few minutes to several
hours per person. Second is training. Interviewing's
137:34 - a special skill that usually requires specific
training. Now, asking the questions is not
137:40 - necessarily the hard part. The really tricky
part is the analysis. The hardest part of
137:45 - interviews by far is analyzing the answers
for themes, and way of extracting the new
137:50 - categories and the dimensions that you need
for your further research. The beautiful thing
137:57 - about interviews is that they allow you to
learn things that you never expected. So,
138:02 - in sum: interviews are best for new situations
or new audiences. On the other hand, they
138:08 - can be time-consuming, and they also require
special training; both to conduct the interview,
138:13 - but also to analyze the highly qualitative
data that you get from them. The next logical
138:19 - step in data sourcing and making data is surveys.
Now, think of this: if you want to know something
138:26 - just ask. That's the easy way. And you want
to do a survey under certain situations. The
138:31 - real question is, do you know your topic and
your audience well enough to anticipate their
138:36 - answers? To know what the range of their answers
and the dimensions and the categories that
138:47 - are going to be important. If you do, then
a survey might be a good approach. Now, just
138:52 - as there were a few dimensions for interviews,
there are a few dimensions for surveys. You
138:57 - can do what is called a closed-ended survey;
that is also called a forced choice. It is
139:04 - where you give people just particular options,
like a multiple choice. You can have an open-ended
139:07 - survey, where you have the same questions
for everybody, but you allow them to write
139:10 - in a free-form response. You can so surveys
in person and you can also do them online
139:14 - or over the mail or phone or however. And
now, it is very common to use software when
139:17 - doing surveys. Some really common applications
for online surveys are SurveyMonkey, and Qualtrics,
139:21 - or at the very simple end there is Google
Forms, and the simple and pretty end there
139:28 - is Typeform. There is a lot more choices,
but these are some of the major players and
139:34 - how you can get data from online participants
in survey format. Now, the nice thing about
139:41 - surveys is, they are really easy to do, they
are very easy to set up and they are really
139:48 - easy to send out to large groups of people.
You can get tons of data really fast. On the
139:54 - other hand, the same way that they are easy
to do, they are also really easy to do badly.
139:58 - The problem is that the questions you ask,
they can be ambiguous, they can be double-barreled,
140:03 - they can be loaded and the response scales
can be confusing. So, if you say, "I never
140:09 - think this particular way" and the person
puts strongly disagree, they may not know
140:13 - exactly what you are trying to get at. So,
you have to take special effort to make sure
140:20 - that the meaning is clear, unambiguous, and
that the rating scale, the way that people
140:26 - respond, is very clear and they know where
their answer falls. Which gets us into one
140:32 - of the things about people behaving badly
and that is beware the push poll. Now, especially
140:37 - during election time; like we are in right
now, a push poll is something that sounds
140:41 - like a survey, but really what it is is a
very biased attempt to get data, just fodder
140:47 - for social media campaigns or I am going to
make a chart that says that 98% of people
140:52 - agree with me. A push poll is one that is
so biased, there is really only one way to
140:58 - answer to the questions. This is considered
extremely irresponsible and unethical from
141:02 - a research point of view. Just hang up on
them. Now, aside from that egregious violation
141:09 - of research ethics, you do need to do other
things like watch out for bias in the question
141:14 - wording, in the response options, and also
in the sample selection because any one of
141:19 - those can push your responses off one way
or another without you really being aware
141:27 - that it is happening. So, in sum, let's say
this about surveys. You can get lots of data
141:34 - quickly, on the other hand, it requires familiarity
with the possible answers in your audience.
141:41 - So, you know, sort of, what to expect. And
no matter what you do, you need to watch for
141:46 - bias to make sure that your answers are going
to be representative of the group that you
141:54 - are really concerned about understanding.
An interesting topic in Data Sourcing when
141:59 - you are making data is Card Sorting. Now,
this isn't something that comes up very often
142:03 - in academic research, but in web research,
this can be a really important method. Think
142:08 - of it as what you are trying to do is like
building a model of a molecule here, you are
142:12 - trying to build a mental model of people's
mental structures. Put more specifically,
142:16 - how do people organize information intuitively?
And also, how does that relate to the things
142:20 - that you are doing online? Now, the basic
procedure goes like this: you take a bunch
142:25 - of little topics and you write each one on
a separate card. And you can do this physically,
142:29 - with like three by five cards, or there are
a lot of programs that allow you to do a digital
142:32 - version of it. Then what you do is you give
this information to a group of respondents
142:37 - and the people sort those cards. So, they
put similar topics with each other, different
142:43 - topics over here and so on. And then you take
that information and from that you are able
142:50 - to calculate what is called, dissimilarity
data. Think of it as like the distance or
142:55 - the difference between various topics. And
that gives you the raw data to analyze how
143:01 - things are structured. Now, there are two
very general kinds of card sorting tasks.
143:03 - There are generative and there's evaluative.
A generative card sorting task is one in which
143:07 - respondents create their own sets, their own
piles of cards using any number of groupings
143:12 - they like. And this might be used, for instance,
to design a website. If people are going to
143:17 - be looking for one kind of information next
to another one, then you are going to want
143:25 - to put that together on the website, so they
know where to expect it. On the other hand,
143:31 - if you've already created a website, then
you can do an evaluative card sorting. This
143:34 - is where you have a fixed number or fixed
names of categories. Like for instance, the
143:41 - way you have set up your menus already. And
then what you do is you see if people actually
143:46 - put the cards into these various categories
that you have created. That's a way of verifying
143:54 - that your hierarchical structure makes sense
to people. Now, whichever method you do, generative
143:59 - or evaluative, what you end up with when you
do a card structure is an interesting kind
144:04 - of visualization called a Dendrogram. That
actually means branches. And what we have
144:09 - here is actually a hundred and fifty data
points; if you are familiar with the Fisher's
144:15 - Iris data, that's what's going on here. And
it groups it from one giant group on the left
144:17 - and then splits it in pieces and pieces and
pieces until you end up with lots of different
144:21 - observations, well actually, individual-level
observations at the end. But you can cut things
144:25 - off into two or three groups or whatever is
most useful for you here, as a way of visualizing
144:32 - the entire collection of similarity or dissimilarity
between the individual pieces of information
144:38 - that you had people sort. Now, I will just
mention very quickly if you want to do a digital
144:44 - card sorting, which makes your life infinitely
easier because keeping track of physical cards
144:50 - is really hard. You can use something like
Optimal Workshop, or UserZoom or UX Suite.
144:56 - These are some of the most common choices.
Now, let's just sum up what we've learned
145:02 - about card sorting in this extremely brief
overview. Number one, card sorting allows
145:08 - you to see intuitive organization of information
in a hierarchical format. You can do it with
145:10 - physical cards or you can also have digital
choices for doing the same thing. And when
145:16 - you are done, you actually get this hierarchical
or branched visualization of how the information
145:24 - is structured and related to each other. When
you are doing your Data Sourcing and you are
145:30 - making data, sometimes you can't get what
you want through the easy ways, and you've
145:34 - got to take the hard way. And you can do what
I am calling laboratory experiments. Now of
145:42 - course, when I mention laboratory experiments
people start to think of stuff like, you know,
145:47 - doctor Frankenstein in his lab, but lab experiments
are less like this and in fact they are a
145:51 - little more like this. Nearly every experiment
I have done in my career has been a paper
145:55 - and pencil one with people in a well-lighted
room and it's not been the threatening kind.
146:01 - Now, the reason you do a lab experiment is
because you want to determine cause and effect.
146:05 - And this is the single most theoretically
viable way of getting that information. Now,
146:07 - what makes an experiment an experiment is
the fact that researchers play active roles
146:11 - in experiments with manipulations. Now, people
get a little freaked out when they hear manipulations,
146:14 - think that you are coercing people and messing
with their mind. All that means is that you
146:19 - are manipulating the situation; you are causing
something to be different for one group of
146:26 - people or for one situation than another.
It's a benign thing, but it allows you to
146:30 - see how people react to those different variations.
Now, you are going to want to do an experiment,
146:36 - you are going to want to have focused research,
it is usually done to test one thing or one
146:43 - variation at a time. And it is usually hypothesis-driven;
usually you don't do an experiment until you
146:51 - have done enough background research to say,
"I expect people to react this way to this
146:55 - situation and this way to the other." A key
component to all of this is that experiments
146:59 - almost always have random assignment regardless
of how you got your sample, when they are
147:03 - in your study, you randomly assign them to
one condition or another. And what they does
147:08 - is it balances out the pre-existing differences
between groups and that's a great way of taking
147:13 - care of confounds and artifacts. The things
that are unintentionally associated with differences
147:18 - between groups that provide alternate explanations
for your data. If you have done good random
147:24 - assignment and you have a large enough group
of people than those confounds and artifacts
147:30 - are basically minimized. Now, some places
where you are likely to see laboratory experiments
147:36 - in this version are for instance are eye tracking
and web design. This is where you have to
147:41 - bring people in front of a computer and you
stick a thing there that sees where they are
147:48 - looking. That's how we know for instance that
people don't really look at ads on the side
147:56 - of web pages. Another very common place is
research in medicine and education and in
148:02 - my field, psychology. And in all of these,
what you find is that experimental research
148:07 - is considered the gold standard for reliable
valid information about cause and effect.
148:11 - On the other hand, while it is a wonderful
thing to have, it does come at a cost. Here's
148:18 - how that works. Number 1, experimentation
requires extensive, specialized training.
148:22 - It is not a simple thing to pick up. Two,
experiments are often very time consuming
148:25 - and labor intensive. I have known some that
take hours per person. And number three, experiments
148:31 - can be very expensive. So, what that all means
is that you want to make sure that you have
148:38 - done enough background research and you need
to have a situation where it is sufficiently
148:42 - important to get really reliable cause and
effect information to justify these costs
148:48 - for experimentation. In sum, laboratory experimentation
is generally considered the best method for
148:53 - causality or assessing causality. That's because
it allows you to control for confounds through
148:57 - randomization. On the other hand, it can be
difficult to do. So, be careful and thoughtful
149:02 - when considering whether you need to do an
experiment and how to actually go about doing
149:07 - it. There's one final procedure I want to
talk about in terms of Data Sourcing and Making
149:12 - New Data. It's a form of experimentation and
it is simply called A/B testing and it's extremely
149:17 - common in the web world. So, for instance,
I just barely grabbed a screenshot of Amazon.com's
149:21 - homepage and you're got these various elements
on the homepage and I just noticed, by the
149:25 - way, when I did this that this woman is actually
an animated gif, so she moves around. That
149:31 - was kind of weird; I have never seen that
before. But the thing about this, is this
149:37 - entire layout, how things are organized and
how they are on there, will have been determined
149:43 - by variations on A/B testing by Amazon. Here's
how it works. For your webpage, you pick one
149:49 - element like what's the headline or what are
the colors or what's the organization or how
149:53 - do you word something and you create multiple
versions, maybe just two version A and version
149:57 - B, why you call it A/B testing. Then when
people visit your webpage you randomly assign
150:01 - these visitors to one version or another,
you have software that does that for you automatically.
150:07 - And then you compare the response rates on
some response. I will show you those in a
150:12 - second. And then, once you have enough data,
you implement the best version, you sort of
150:17 - set that one solid and then you go on to something
else. Now, in terms of response rates, there
150:21 - are a lot of different outcomes you can look
at. You can look at how long a person is on
150:28 - a page, you can actually do mouse tracking
if you want to. You can look at click-throughs,
150:33 - you can also look at shopping cart value or
abandonment. A lot of possible outcomes. All
150:37 - of these contribute through A/B testing to
the general concept of website optimization;
150:40 - to make your website as effective as it can
possibly be. Now, the idea also is that this
150:46 - is something that you are going to do a lot.
You can perform A/B tests continually. In
150:50 - fact, I have seen one person say that what
A/B testing really stands for is always be
150:56 - testing. Kind of cute, but it does give you
the idea that improvement is a constant process.
151:02 - Now, if you want some software to do A/B testing,
two of the most common choices are Optimizely
151:08 - and VWO, which stands for Visual Web Optimizer.
Now, many others are available, but these
151:11 - are especially common and when you get the
data you are going to use statistical hypothesis
151:15 - testing to compare the differences or really
the software does it for you automatically.
151:24 - But you may want to adjust the parameters
because most software packages cut off testing
151:32 - a little too soon and the information is not
quite as reliable as it should be. But, in
151:38 - sum, here is what we can say about A/B testing.
It is a version of website experimentation;
151:43 - it is done online, which makes it really easy
to get a lot of data very quickly. It allows
151:50 - you to optimize the design of your website
for whatever outcome is important to you.
151:56 - And it can be done as a series of continual
assessments, testing, and development to make
151:59 - sure that you're accomplishing what you want
to as effectively as possible for as many
152:05 - people as possible. The very last thing I
want to talk about in terms of data sourcing
152:10 - is to talk about the next steps. And probably
the most important thing is, you know, don't
152:15 - just sit there. I want you to go and see what
you already have. Try to explore some open
152:21 - data sources. And if it helps, check with
a few data vendors. And if those don't give
152:27 - you what you need to do your project, then
consider making new data. Again, the idea
152:32 - here is get what you need and get going. Thanks
for joining me and good luck on your own projects.
152:42 - Welcome to "Coding in Data Science". I'm Bart
Poulson and what we are going to do in this
152:47 - series of videos is we're going to take a
little look at the tools of Data Science.
152:52 - So, I am inviting you to know your tools,
but probably even more important than that
152:57 - is to know their proper place. Now, I mention
that because a lot of the times when people
153:03 - talk about data tools, they talk about it
as though that were the same thing as data
153:09 - science, as though they were the same set.
But, I think if you look at it for just a
153:15 - second that is not really the case. Data tools
are simply one element of data science because
153:21 - data science is made up of a lot more than
the tools that you use. It includes things
153:24 - like, business knowledge, it includes the
meaning making and interpretation, it includes
153:29 - social factors and so there's much more than
just the tools involved. That being said,
153:36 - you will need at least a few tools and so
we're going to talk about some of the things
153:41 - that you can use in data science if it works
well for you. In terms of getting started,
153:47 - the basic things. #1 is spreadsheets, it is
the universal data tool and I'll talk about
153:51 - how they play an important role in data science.
#2 is a visualization program called Tableau,
153:57 - there is Tableau public, which is free, and
there's Tableau desktop and there is also
154:02 - something called Tableau server. Tableau is
a fabulous program for data visualization
154:07 - and I'm convinced for most people provides
the great majority of what they need. And
154:12 - though while it is not a tool, I do need to
talk about the formats used in web data because,
154:17 - you have to be able to navigate that when
doing a lot of data science work. Then we
154:22 - can talk about some of the essential tools
for data science. Those include the programming
154:27 - language R, which is specifically for data,
there's the general purpose programming language
154:33 - Python, which has been well adapted to data.
And there's the database language sequel or
154:38 - SQL for structured query language. Then if
you want to go beyond that, there are some
154:44 - other things that you can do. There are the
general purpose programming languages C, C++,
154:50 - and Java, which are very frequently used to
form the foundation of data science and sort
154:55 - of high level production code is going to
rely on those as well. There's the command
155:01 - line interface language Bash, which is very
common, a very quick tool for manipulating
155:07 - data. And then there's the, sort of wild card
supercharged regular expressions or Regex.
155:16 - We'll talk about all of these in separate
courses. But, as you consider all the tools
155:21 - that you can use, don't forget the 80/20 rule.
Also known as the Pareto Principle. And the
155:29 - idea here is that you are going to get a lot
of bang for your buck out of small number
155:33 - of things. And I'm going to show you a little
sample graph here. Imagine that you have ten
155:38 - different tools and we'll call them A through
B. A does a lot for you, B does a little bit
155:44 - less and it kind of tapers down to, you have
got a bunch of tools that do just a little
155:49 - of stuff that you need. Now, instead of looking
at the individual effectiveness, look at the
155:54 - cumulative effectiveness. How much are you
able to accomplish with a combination of tools?
155:59 - Well, the first ones right here at 60% where
the tools started and then you add on the
156:04 - 20% from B and it goes up and then you add
on C and D and you add up little smaller,
156:10 - smaller pieces and by the time you get to
the end, you have got 100% of effectiveness
156:14 - from your ten tools combined. The important
thing about this is, you only have to go to
156:20 - the 2nd tool, that is two out of ten, that's
B, that's 20% of your tools and in this made
156:27 - up example, you have got 80% of your output.
So, 80% of the output from 20% of the tools,
156:35 - that's a fictional example of the Pareto Principle,
but I find in real life it tends to work something
156:40 - approximately like that. And so, you don't
necessarily have to learn everything and you
156:45 - don't have to learn how to do everything in
everything. Instead you want to focus on the
156:50 - tools that will be most productive and specifically
most productive for you. So, in sum, let's
156:58 - say these three things. Number 1, coding or
simply the ability to manipulate data with
157:03 - programs and computers. Coding is important,
but data science is much greater than the
157:11 - collection of tools that's used in it. And
then finally, as you're trying to decide what
157:16 - tools to use and what you need to learn and
how to work, remember the 80/20, you are going
157:20 - to get a lot of bang from a small set of tools.
So, focus on the things that are going to
157:24 - be most useful for you in conducting your
own data science projects. As we begin our
157:31 - discussion of Coding and Data Science, I actually
want to begin with something that's not coding.
157:35 - I want to talk about applications or programs
that are already created that allow you to
157:40 - manipulate data. And we are going to begin
with the most basic of these, spreadsheets.
157:45 - We're going to do the rows and columns and
cells of Excel. And the reason for this is
157:50 - you need spreadsheets. Now, you may be saying
to yourself, "no no no not me, because you
157:57 - know what I'm fancy, I'm working in my big
set of servers, I've got fancy things going
158:03 - on." But, you know what, you too fancy people,
you need spreadsheets as well. There's a few
158:09 - reasons for this. Most importantly, spreadsheets
can be the right tool for data science in
158:15 - a lot of circumstances; there are a few reasons
for that. Number one, spreadsheets, they're
158:20 - everywhere, they're ubiquitous, they're installed
on a billion machines around the world and
158:25 - everybody uses them. They probably have more
data sets in spreadsheets than anything else,
158:31 - and so it's a very common format. Importantly,
it's probably your client's format; a lot
158:38 - of your clients are going to be using spreadsheets
for their own data. I've worked with billion
158:42 - dollar companies that keep all of their data
in spreadsheets. So, when you're working with
158:46 - them, you need to know how to manipulate that
and how to work with it. Also, regardless
158:51 - of what you're doing, spreadsheets are specifically
csv - comma separated value files - are sort
158:57 - of the lingua franca or the universal interchange
format for data transfer, to allow you to
159:02 - take it from one program to another. And then,
truthfully, in a lot of situations they're
159:07 - really easy to use. And if you want a second
opinion on this, let's take a look at this
159:13 - ranking. There's a survey of data mining experts,
it's the KDnuggets data mining poll, and these
159:20 - are the tools they most use in their own work.
And look at this: lowly Excel is fifth on
159:27 - the list, and in fact, what's interesting
about it is it's above Hadoop and Spark, two
159:33 - of the major big data fancy tools. And so,
Excel really does have place of pride in a
159:40 - toolkit for data analyst. Now, since we're
going to sort of the low tech end of things,
159:46 - let's talk about some of the things you can
do with a spreadsheet. Number one, they are
159:50 - really good for data browsing. You really
get to see all of the data in front of you,
159:54 - which isn't true if you are doing something
like R or Python. They're really good for
159:58 - sorting data, sort by this column then this
column then this column. They're really good
160:02 - for rearranging columns and cells and moving
things around. They're good for finding and
160:07 - replacing and seeing what happens so you know
that it worked right. Some more uses they're
160:13 - really good for formatting, especially conditional
formatting. They're good for transposing data,
160:19 - switching the rows and the columns, they make
that really easy. They're good for tracking
160:23 - changes. Now it's true if you're a big fancy
data scientist you're probably using GitHub,
160:28 - but for everybody else in the world spreadsheets
and the tracking changes is a wonderful way
160:33 - to do it. You can make pivot tables, that
allows you to explore the data in a very hands-on
160:39 - way, in a very intuitive way. And they're
also really good for arranging the output
160:45 - for consumption. Now, when you're working
with spreadsheets, however, there's one thing
160:50 - you need to be aware of: they are really flexible,
but that flexibility can be a problem in that
160:56 - when you are working in data science, you
specifically want to be concerned about something
160:58 - called Tidy Data. That's a term I borrowed
from Hadley Wickham, a very well-known developer
161:03 - in the R world. Tidy Data is for transferring
data and making it work well. There's a few
161:10 - rules here that undo some of the flexibility
inherent in spreadsheets. Number one, what
161:15 - you want to do is have a column be equivalent
to the same thing as a variable; columns,
161:21 - variables, they are the same thing. And then,
rows are equal - exactly the same thing as
161:27 - cases. That you have one sheet per file, and
that you have one level of measurement, say,
161:34 - individual, then organization, then state
per file. Again, this is undoing some of the
161:39 - flexibility that's inherent in spreadsheets,
but it makes it really easy to move the data
161:44 - from one program to another. Let me show you
how all this works. You can try this in Excel.
161:51 - If you have downloaded the files for this
course, we simply want to open up this spreadsheet.
161:56 - Let me go to Excel and show you how it works.
So, when you open up this spreadsheet, what
162:02 - you get is totally fictional data here that
I made up, but it is showing sales over time
162:08 - of several products at two locations, like
if you're selling stuff at a baseball field.
162:14 - And this is the way spreadsheets often appear;
we've got blank rows and columns, we've got
162:18 - stuff arranged in a way that makes it easy
for the person to process it. And we have
162:24 - got totals here, with formulas putting them
all together. And that's fine, that works
162:29 - well for the person who made it. And then,
that's for one month and then we have another
162:34 - month right here and then we have another
month right here and then we combine them
162:37 - all for first quarter of 2014. We have got
some headers here, we've got some conditional
162:43 - formatting and changes and if we come to the
bottom, we have got a very busy line graphic
162:49 - that eventually loads; it's not a good graphic,
by the way. But, similar to what you will
162:54 - often find. So, this is the stuff that, while
it may be useful for the client's own personal
163:01 - use, you can't feed this into R or Python,
it will just choke and it won't know what
163:05 - to do with it. And so, you need to go through
a process of tidying up the data. And what
163:11 - this involves is undoing some of the stuff.
So, for instance, here's data that is almost
163:17 - tidy. Here we have a single column for date,
a single column for the day, a column for
163:23 - the site, so we have two locations A and B,
and then we have six columns for the six different
163:29 - things that are sold and how many were sold
on each day. Now, in certain situations, you
163:33 - would want the data laid out exactly like
this if you are doing, for instance, a time
163:37 - series, you will do something vaguely similar
to this. But, for true tidy stuff, we are
163:43 - going to collapse it even further. Let me
come here to the tidy data. And now what I
163:48 - have done is, I have created a new column
that says what is the item being sold. And
163:52 - so, by the way, what this means is that we
have got a really long data set now, it has
163:56 - got over a thousand rows. Come back up to
the top here. But, what that shows you is
164:03 - that now it's in a format that's really easy
to import from one program to another, that
164:09 - makes it tidy and you can re-manipulate it
however you want once you get to each of those.
164:13 - So, let's sum up our little presentation here,
in a few lines. Number one, no matter who
164:19 - you are, no matter what you are doing in data
science you need spreadsheets. And the reason
164:24 - for that is that spreadsheets are often the
right tool for data science. Keep one thing
164:30 - in mind though, that is as you are moving
back and forth from one language to another,
164:35 - tidy data or well-formatted data is going
to be important for exporting data into your
164:40 - analytical programmer language of choice.
As we move through "Coding and Data Science,"
164:47 - and specifically the applications that can
be used, there's one that stands out for me
164:51 - more than almost anything else, and that's
Tableau and Tableau Public. Now, if you are
164:57 - not familiar with these, these are visualization
programs. The idea here is that when you have
165:02 - data, the most important thing you can do
is to first look and see what you have and
165:08 - work with it from there. And in fact, I'm
convinced that for many organizations Tableau
165:15 - might be all that they really need. It will
give them the level of insight that they need
165:21 - to work constructively with data. So, let's
take a quick look by going to tableau.com.
165:28 - Now, there are a few different versions of
Tableau. Right here we have Tableau Desktop
165:35 - and Tableau Server, and these are the paid
versions of Tableau. They actually cost a
165:40 - lot of money, unless you work for a nonprofit
organization, in which case you can get them
165:46 - for free. Which is a beautiful thing. What
we're usually looking for, however, is not
165:52 - the paid version, but we are looking for something
called Tableau Public. And if you come in
165:57 - here and go to products and we have got these
three paid ones, over here to Tableau Public.
166:05 - We click on that, it brings us to this page.
It is public.tableau.com. And this is the
166:12 - one that has what we want, it's the free version
of Tableau with one major caveat: you don't
166:18 - save files locally to your computer, which
is why I didn't give you a file to open. Instead,
166:24 - it saves them to the web in a public form.
So, if you are willing to trade privacy, you
166:31 - can get an immensely powerful application
for data visualization. That's a catch for
166:37 - a lot of people, which is why people are willing
to pay a lot of money for the desktop version.
166:41 - And again, if you work for a nonprofit you
can get the desktop version for free. But,
166:45 - I am going to show you how things work in
Tableau Public. So, that's something that
166:50 - you can work with personally. The first thing
you want to do is, you want to download it.
166:55 - And so, you put in your email address, you
download; it is going to know what you are
166:59 - on. It is a pretty big download. And once
it is downloaded, you can install and open
167:03 - up the application. And here I am in Tableau
Public, right here, this is the blank version.
167:08 - By the way, you also need to create an account
with Tableau in order to save your stuff online
167:14 - to see it. I will show you what that looks
like. But, you are presented with a blank
167:18 - thing right here and the first thing you need
to do is, you need to bring in some data.
167:23 - I'm going to bring in an Excel file. Now,
if you downloaded the files for the course,
167:28 - you will see that there is this one right
here, DS03_2_2_TableauPublic.excel.xlsx. In
167:33 - fact, it is the one that I used in talking
about spreadsheets in the first video in this
167:40 - course. I'm going to select that one and I'm
going to open it. And a lot of programs don't
167:47 - like bringing in Excel because it's got all
the worksheets and all the weirdness in it.
167:51 - This one works better with it, but what I'm
going to do is, I am going to take the tidy
167:54 - data. By the way, you see that it put them
in alphabetical order here. I'm going to take
168:00 - tidy data and I'm going to drag it over to
let it know that it's the one that I want.
168:05 - And now what it does is it shows me a version
of the data set along with things that you
168:13 - can do here. You can rename it, I like that
you can create bin groups, there's a lot of
168:17 - things that you can do here. I'm going to
do something very, very quick with this particular
168:22 - one. Now, I've got the data set right here,
what I'm going to do now is I'm going to go
168:28 - to a worksheet. That's where you actually
create stuff. Cancel that and go to worksheet
168:33 - one. Okay. This is a drag and drop interface.
And so what we are going to do is, we are
168:44 - going to pull the bits and pieces of information
we want to make graphics. There's immense
168:49 - flexibility here. I'm going to show you two
very basic ones. I'm going to look at the
168:55 - sales of my fictional ballpark items. So,
I'm going to grab sales right here and I'm
169:02 - going to put that as the field that we are
going to measure. Okay. And you see, put it
169:06 - down right here and this is our total sales.
We're going to break it down by item and by
169:12 - time. So, let me take item right here, and
you can drag it over here, or I can put it
169:18 - right up here into rows. Those will be my
rows and that will be how many we have sold
169:22 - total of each of the items. Fine, that's really
easy. And then, let's take date and we will
169:28 - put that here in columns to spread it across.
Now, by default it is doing it by year, I
169:34 - don't want to do that, I want to have three
months of data. So, what I can do is, I can
169:38 - click right here and I can choose a different
time frame. I can go to quarter, but that's
169:42 - not going to help because I only have one
quarter's worth of data, that's three months.
169:47 - I'm going to come down to week. Actually,
let me go to day. If I do day, you see it
169:52 - gets enormously complicated, so that's no
good. So, I'm going to back up to week. And
169:58 - I've got a lot of numbers there, but what
I want is a graph. And so, to get that, I'm
170:03 - going to come over here and click on this
and tell it that I want a graph. And so, we're
170:09 - seeing the information, except it lost items.
So, I'm going to bring item and put it back
170:15 - up into this graph to say this is a row for
the data. And now I've got rows for sales
170:21 - by week for each of my items. That's great.
I want to break it down one more by putting
170:26 - in the site, the place that it sold. So, I'm
going to grab that and I'm going to put it
170:31 - right over here. And now you see I've got
it broken down by the item that is sold and
170:41 - the different sites. I'm going to color the
sites, and all I've got to do to do that is,
170:45 - I'm going to grab site and drag it onto color.
Now, I've got two different colors for my
170:51 - sites. And this makes it a lot easier to tell
what is going on. And in fact, there is some
170:56 - other cool stuff you can do. One of the things
I'm going to do is come over here to analytics
171:01 - and I can tell it to put an average line through
everything, so I'll just drag this over here.
171:08 - Now we have the average for each line. That's
good. And I can even do forecasting. Let me
171:16 - get a little bit of a forecast right here.
I will drag this on and if you can go over
171:22 - here. I will get this out of the way for a
second. Now, I have a forecast for the next
171:28 - few weeks, and that's a really convenient,
quick, and easy thing. And again, for some
171:32 - organizations that might be all that they
really need. And so, what I'm showing you
171:38 - here is the absolute basic operation of Tableau,
which allows you to do an incredible range
171:44 - of visualizations and manipulate the data
and create interactive dashboards. There's
171:49 - so much to it and we'll show that in another
course, but for right now I want to show you
171:53 - one last thing about Tableau Public, and that
is saving the files. So now, when I come here
171:59 - and save it, it's going to ask me to sign
into Tableau Public. Now, I sign in and it
172:08 - asks me how I want to save this, same name
as the video. There we go, and I'm going to
172:22 - hit save. And then that opens up a web browser,
and since I'm already logged into my account,
172:31 - see here's my account and my profile. Here's
the page that I created. And it's got everything
172:39 - that I need there; I'm going to edit just
a few details. I'm going to say, for instance,
172:45 - I'm going to leave its name just like that.
I can put more of a description in there if
172:48 - I wanted. I can allow people to download the
workbook and its data; I'm going to leave
172:53 - that there so you can download it if you need
to. If I had more than one tab, I would do
172:58 - this thing that says show the different sheets
as tabs. Hit save. And there's my data set
173:09 - and also it's published online and people
can now find it. And so what you have here
173:15 - is an incredible tool for creating interactive
visualizations; you can create them with drop-down
173:19 - menus, and you can rearrange things, and you
can make an entire dashboard. It's a fabulous
173:24 - way of presenting information, and as I said
before, I think that for some organizations
173:28 - this may be as much as they need to get really
good, useful information out of their data.
173:35 - And so I strongly recommend that you take
some time to explore with Tableau, either
173:39 - the paid desktop version or the public version
and see what you can do to get some really
173:44 - compelling and insightful visualizations out
of your work in data science. For many people,
173:51 - their first experience of "Coding and Data
Science" is with the application SPSS. Now,
173:59 - I think of SPSS and the first thing that comes
to my mind is sort of life in the Ivory tower,
174:04 - though this looks more like Harry Potter.
But, if you think about it the package name
174:10 - SPSS comes from Statistical Package for the
Social Sciences. Although, if you ask IBM
174:17 - about it now, they act like it doesn't stand
for anything. But, it has its background in
174:22 - social science research which is generally
academic. And truthfully, I'm a social psychologist
174:26 - and that's where I first learned how to use
SPSS. But, let's take a quick look at their
174:31 - webpage ibm.com/spss. If you type that in,
that will just be an alias that will take
174:39 - you to IBM's main webpage. Now, IBM didn't
create SPSS, but they bought it around version
174:44 - 16, and it was very briefly known as PASW
predictive analytic software, that only lasted
174:50 - briefly and now it's back to SPSS, which is
where it's been for a long time. SPSS is a
174:56 - desktop program; it's pretty big, it does
a lot of things, it's very powerful, and is
175:01 - used in a lot of academic research. It's also
used in a lot of business consulting, management,
175:07 - even some medical research. And the thing
about SPSS, is it looks like a spreadsheet
175:13 - but has drop-down menus to make your life
a little bit easier compared to some of the
175:17 - programming languages that you can use. Now,
you can get a free temporary version, if you're
175:23 - a student you can get a cheap version, otherwise
SPSS costs a lot of money. But, if you have
175:29 - it one way or another, when you open it up
this is what it is going to look like. I'm
175:35 - showing SPSS version 22, now it's currently
on 24. And the thing about SPSS versioning
175:45 - is, in anything other than software packaging,
these would be point updates, so I sort of
175:48 - feel like we should be on 17.3, as opposed
to 23 or 24. Because the variations are so
175:55 - small that anything you learn from the early
ones, is going to work on the later ones and
175:59 - there is a lot of backwards and forwards compatibility,
so I'd almost say that this one, the version
176:05 - I have practically doesn't matter. You get
this little welcome splash screen, and if
176:10 - you don't want to see it anymore you can get
rid of it. I'm just going to hit cancel here.
176:14 - And this is our main interface. It looks a
lot like a spreadsheet, the difference is,
176:18 - you have a separate pane for looking at variable
information and then you have separate windows
176:23 - for output and then an optional one for something
called Syntax. But, let me show you how this
176:28 - works by first opening up a data set. SPSS
has a lot of sample data sets in them, but
176:33 - they are not easy to get to and they are really
well hidden. On my Mac, for instance, let
176:39 - me go to where they are. In my mac I go to
the finder, I have to go to Mac, to applications,
176:46 - to the folder IBM, to SPSS, to statistics,
to 22 the version number, to samples, then
176:52 - I have to say I want the ones that are in
English, and then it brings them up. The .sav
176:58 - files are the actual data files, there are
different kinds in here, so .sav is a different
177:03 - kind of file and then we have a different
one about planning analyses. So, there are
177:07 - versions of it. I'm going to open up a file
here called "market values .sav," a small
177:14 - data set in SPSS format. And if you don't
have that, you can open up something else;
177:18 - it really doesn't matter for now. By the way,
in case you haven't noticed, SPSS tends to
177:26 - be really really slow when it opens. It also,
despite being version 24, it tends to be kind
177:32 - of buggy and crashes. So, when you work with
SPSS, you want to get in the habit of saving
177:37 - your work constantly. And also, being patient
when it is time to open the program. So, here
177:42 - is a data set that just shows addresses and
house values, and square feet for information.
177:48 - This, I don't even know if this is real information,
it looks artificial to me. But, SPSS lets
177:55 - you do point and click analyses, which is
unusual for a lot of things. So, I am going
178:00 - to come up here and I am going to say, for
instance, make a graph. I'm going to make
178:05 - a- I'm going to use what is called a legacy
dialogue to get a histogram of house prices.
178:11 - So, I simply click values. Put that right
there and I will put a normal curve in top
178:17 - of it and click ok. This is going to open
up a new window, and it opened up a microscopic
178:23 - version of it, so I'm going to make that bigger.
This is the output window, this is a separate
178:28 - window and it has a navigation pane here on
the side. It tells me where the data came
178:35 - from, and it saves the command here, and then,
you know, there's my default histogram. So,
178:41 - we see most of the houses were right around
$125,000, and then they went up to at least
178:48 - $400,000. I have a mean of $256,000, a standard
deviation of about $80,000, and then there
178:56 - is 94 houses in the data set. Fine, that's
great. The other thing I can do is, if I want
179:01 - to do some analyses, let me go back to the
data just for a moment. For instance, I can
179:09 - come here to analyze and I can do descriptive
and I'm actually going to do one here called
179:14 - Explore. And I'll take the purchase price
and I'll put it right here and I'm going to
179:21 - get a whole bunch just by default. I'm going
to hit ok. And it goes back to the output
179:27 - window. Once again made it tiny. And so, now
you see beneath my chart I now have a table
179:36 - and I've got a bunch of information. A stem
and leaf plot, and a box plot too, a great
179:40 - way of checking for outliers. And so this
is a really convenient way to save things.
179:45 - You can export this information as images,
you can export the entire file as an HTML,
179:52 - you can do it as a pdf or a PowerPoint. There's
a lot of options here and you can customize
179:56 - everything that's on here. Now, I just want
to show you one more thing that makes your
180:02 - life so much easier in SPSS. You see right
here that it's putting down these commands,
180:08 - it's actually saying graph, and then histogram,
and normal equals value. And then down here,
180:13 - we've got this little command right here.
Most people don't know how to save their work
180:20 - in SPSS, and that's something you kind of
just have to do it over again every time,
180:23 - but there's a very simple way to do this.
What I'm going to do is, I'm going to open
180:28 - up something called a Syntax file. I'm going
to go to new, Syntax. And this is just a blank
180:35 - window that's a programming window, it's for
saving code. And let me go back to my analysis
180:43 - I did a moment ago. I'll go back to analyze
and I can still get at it right here. Descriptives
180:50 - and explore, my information is still there.
And what happens here is, even though I set
180:56 - it up with drop-down menus and point and click,
if I do this thing, paste, then what it does
181:02 - is, it takes the code that creates that command
and it saves it to this syntax window. And
181:06 - this is just a text file. It saves it as .spss,
but it is a text file that can be opened in
181:13 - anything. And what's beautiful about this
is, it is really easy to copy and paste, and
181:17 - you can even take this into Word and do a
find and replace on it, and it's really easy
181:23 - to replicate the analyses. And so for me,
SPSS is a good program. But, until you use
181:30 - Syntax you don't know the true power of it
and it makes your life so much easier as a
181:35 - way of operating it. Anyhow, this is my extremely
brief introduction to SPSS. All I want to
181:41 - say is that it is a very common program, kind
of looks like a spreadsheet, but it gives
181:45 - you a lot more power and options and you can
use both drop-down menus and text-based Syntax
181:52 - commands as well to automate your work and
make it easier to replicate it in the future.
181:59 - I want to take a look at one more application
for "Coding and Data Science", that's called
182:03 - JASP. This is a new application, not very
familiar to a lot of people and still in beta,
182:08 - but with an amazing promise. You can basically
think of it as a free version of SPSS and
182:14 - you know what, we love free. But, JASP is
not just free, it's also open source, and
182:22 - it's intuitive, and it makes analyses replicable,
and it even includes Bayesian approaches.
182:30 - So, take that all together, you know, we're
pretty happy and we're jumping for joy. So,
182:35 - before we move on, you just may be asking
yourself, JASP, what is that? Well, the creator
182:43 - has emphatically denied that it stands for
Just Another Statistics Program, but be that
182:48 - as it may, we will just go ahead and call
it JASP and use it very happily. You can get
182:53 - to it by going to jasp-stats.org. And let's
take a look at that right now. JASP is a new
183:00 - program, they say a low fat alternative to
SPSS, but it is a really wonderful great way
183:05 - of doing statistics. You're going to want
to download it, by supplying your platform;
183:11 - it even comes in Linux format, which is beautiful.
And again, it's beta so stay posted, things
183:17 - are updating regularly. If you're on Mac,
you're going to need to use Xquartz, that's
183:23 - an easy thing to install and it makes a lot
of things work better. And it's the wonderful
183:27 - way to do analyses. When you open up JASP,
it's going to look like this. It's a pretty
183:33 - blank interface, but it's really easy to get
going with it. So for instance, you can come
183:38 - over here to file and you can even choose
some example data sets. So for instance, here's
183:45 - one called Big 5 that's personality factors.
And you've got data here that's really easy
183:51 - to work with. Let me scroll this over here
for a moment. So, there's our five variables
183:55 - and let's do some quick analyses with these.
Say for instance, we want to get descriptives;
184:02 - we can pick a few variables. Now, if you're
familiar with SPSS, the layout feels very
184:09 - much the same and the output looks a lot the
same. You know, all I have to do is select
184:14 - what I want and it immediately pops up over
here. Then I can choose additional statistics,
184:20 - I can get core tiles, I can get the median.
And you can choose plots; let's get some plots,
184:26 - all you have to do is click on it and they
show up. And that's a really beautiful thing
184:31 - and you can modify these things a little bit,
so for instance, I can take the plot points.
184:35 - Let's see if I can drag that down and if I
make it small enough I can see the five plots,
184:41 - I went a little too far on that one. Anyhow,
you can do a lot of things here. And I can
184:51 - hide this, I can collapse that and I can go
on and do other analyses. Now, what's really
184:57 - neat though is when I navigate away, so I
just clicked in a blank area of the results
185:02 - page, we are back to the data here. But if
I click on one of these tables, like this
185:07 - one right here, it immediately brings up the
commands that produced it and I can just modify
185:12 - it some more if I want. Say I want skewness
and kurtosis, boom they are in there. It is
185:18 - an amazing thing and then I can come back
out here, I can click away from that and I
185:23 - can come down to the plots expand those and
if I click on that it brings up the commands
185:29 - that made them. It's an amazingly easy and
intuitive way to do things. Now, there's another
185:35 - really nice thing about JASP and that is that
you can share the information online really
185:40 - well through a program called osf.io. That
stands for the open science foundation, that's
185:47 - its web address osf.io. So, let's take a quick
look at what that's like. Here's the open
185:53 - science framework website and it's a wonderful
service, it's free and it's designed to support
185:59 - open, transparent, accessible, accountable,
collaborative research and I really can't
186:06 - say enough nice things about it. What's neat
about this is once you sign up for OSF you
186:13 - can create your own area and I've got one
of my own, I will go to that now. So, for
186:20 - instance, here's the datalab page in open
science framework. And what I've done is i
186:25 - created a version of this JASP analysis and
I've saved it here, in fact, let's open up
186:33 - my JASP analysis in JASP and I'll show you
what it looks like in osf. So, let's first
186:39 - go back to JASP. When we're here we can come
over to file and click computer and I just
186:48 - saved this file to the desktop. Click on desktop,
and you should have been able to download
186:56 - this with all the other files, DS03_2_4_JASP,
double click on that to open it and now it's
187:05 - going to open up a new window and you see
I was working with the same data set, but
187:09 - I did a lot more analyses. I've got these
graphs; I have correlations and scatter plots.
187:15 - Come down here, I did a linear regression.
And we just click on that and you can see
187:21 - the commands that produce it as well as the
options. I didn't do anything special for
187:25 - that, but I did do some confidence intervals
and specified that and it's really a great
187:31 - way to work with all this. I'll click back
in an empty area and you see the commands
187:34 - go away and so I've got my output here in
JASP, but when I saved it though, I had the
187:41 - option of saving it to OSF, in fact if you
go to this webpage osf.io/3t2jg you'll actually
187:54 - be able to go to the page where you can see
and download the analyses that I conducted,
187:58 - let's take a look. This is that page, there's
the address I just barely gave you and what
188:04 - you see here is the same analysis that I conducted,
it's all right here, so if you're collaborating
188:10 - with people or if you want to show things
to people, this is a wonderful way to do it.
188:14 - Everything is right there, this is a static
image, but up at the top people have the option
188:20 - of downloading the original file and working
with it on their own. In case you can't tell,
188:26 - I'm really enthusiastic about JASP and about
its potential, still in beta, still growing
188:31 - rapidly. I see it really as an open source
free and collaborative replacement to SPSS
188:39 - and I think it is going to make data science
work so much easier for so many people. I
188:44 - strongly recommend you give JASP a close look.
Let's finish up our discussion of "Coding
188:50 - and Data Science" the applications part of
it by just briefly looking at some other software
188:56 - choices. And I'll have to admit it gets kind
of overwhelming because there are just so
189:01 - many choices. Now, in addition to the spreadsheets,
and Tableau, and SPSS, and JASP, that we have
189:09 - already talked about, there's so much more
than that. I'm going to give you a range of
189:14 - things that I'm aware of and I'm sure I've
left out some important ones or things that
189:18 - other people like really well, but these are
some common choices and some less common,
189:22 - but interesting ones. Number one, in terms
of ones that I haven't mentioned is SAS. SAS
189:28 - is an extremely common analytical program,
very powerful, used for a lot of things. It's
189:33 - actually the first program that I learned
and on the other hand it can be kind of hard
189:39 - to use and it can be expensive, but there's
a couple of interesting alternatives. SAS
189:43 - also has something called the SAS University
Edition, if you're a student this is free
189:49 - and it's slightly reduced in what it does,
but the fact that it's free. And also it runs
189:56 - in a virtual machine which makes it an enormous
download, but it's a good way to learn SAS
190:02 - if it's something that you want to do. SAS
also makes a program that I really love were
190:06 - it not so extraordinarily expensive and that
is called JMP and its visualization software.
190:15 - Think a little bit of Tableau, how we saw
it, you work with it visually and this one
190:18 - you can drag things around, it's really wonderful
program. I personally find it prohibitively
190:24 - expensive. Another very common choice among
working analysts is Stata and some people
190:30 - use Minitab. Now, for mathematical people,
there's MATLAB and then of course there's
190:36 - Mathematica itself, but it is really more
of a language than a program. On the other
190:41 - hand, Wolfram; who makes Mathematica, is also
the people who give us Wolfram Alpha, most
190:47 - people don't think of this a stats application
because you can run it on your iPhone. But,
190:51 - Wolfram Alpha is an incredibly capable and
especially if you pay for the pro account,
190:58 - you can do amazing things in this, including
analyses, regression models, visualizations
191:04 - and so it's worth taking a little closer look
at that. Also, because it provides a lot of
191:09 - the data that you need so Wolfram Alpha is
an interesting one. Now, several applications
191:15 - that are more specifically geared towards
data mining, so you don't want to do your
191:20 - regular, you know, little t tests and stuff
on these. But, there's RapidMiner and there's
191:26 - KNIME and Orange and those are all really
nice to use because they are control languages
191:33 - where you drag notes onto a screen and you
connect them with lines and you can see how
191:37 - things run through. All three of them are
free or have free versions and all three of
191:43 - them work in pretty similar manners. There's
also BigML, which is for machine learning
191:49 - and this is unusual because it's browser based,
it runs on their servers. There's a free version,
191:54 - though you can't download a whole lot, it
doesn't cost a lot to use BigML and it's a
191:59 - very friendly, very accessible program. Then
in terms of programs you can actually install
192:04 - for free on your own computer, there's one
call SOFA Statistics, it means statistics
192:08 - open for all, it's kind of a cheesy title,
but it's a good program. And then one with
192:13 - a web page straight out of 1990 is Past 3,
this is paleontological software, on the other
192:22 - hand does do very general stuff, it runs on
many platforms and it's a really powerful
192:27 - thing and it's free, but it is relatively
unknown. And then speaking of relatively unknown,
192:33 - one that's near and dear to my heart is a
web application called Statcrunch, it costs,
192:37 - but it costs like $6 or $12 a year, it's really
cheap and it's very good, especially if for
192:44 - basic statistics and for learning, I used
in some of the classes that I was teaching.
192:49 - And then if you're deeply wedded to Excel
and you can't stand to leave that environment,
192:53 - you can purchase add-ons like XLSTAT, which
give you a lot of statistical functions within
193:00 - the Excel environment itself. That's a lot
of choices and the most important thing here
193:06 - is don't get overwhelmed. There's a lot of
choices, but you don't even have to try all
193:11 - of them. Really the important question is
what works best for you and the project that
193:16 - you're working on? Here's a few things you
want to consider in that regard. First off
193:21 - is functionality, does it actually do what
you want or does it even run on your machine?
193:25 - You don't need everything that a program can
do. When you think about the stuff Excel can
193:30 - do, people probably use five percent of what's
available. Second is ease of use. Some of
193:37 - these programs are a lot easier to use than
the others and I personally find that the
193:43 - ones that are easier to use, I like them,
so you might say, "No, I need to program because
193:47 - I need custom stuff". But I'm willing to bet
that 95% of what people do does not require
193:51 - anything custom. Also, the existence of a
community. Constantly when you're working
193:58 - you come across problems and don't know how
to solve it and being able to get online and
194:03 - do a search for an answer and have enough
of a community that there are people there
194:07 - who have put answers up and discuss these
things. Those are wonderful. Some of these
194:12 - programs are very substantial communities
and some of them it is practically nonexistent
194:15 - and it is to you to decide how important it
is to you. And then finally of course there
194:18 - is the issue of cost. Many of these programs
I mentioned are free, some of them are very
194:24 - cheap, some of them run some sort of premium
model and some of them are extremely expensive.
194:28 - So, you don't buy them unless somebody else
is paying for it. So, these are some of the
194:32 - things that you want to keep in mind when
you're trying to look at various programs.
194:36 - Also, let's mention this; don't forget the
80/20 rule. You're going to be able to do
194:42 - most of the stuff that you need to do with
only a small number of tools, one or two,
194:47 - maybe three, will probably be all that you
ever need. So, you don't need to explore the
194:52 - range of every possible tool. Find something
that you need, find something you're comfortable
194:58 - with and really try to extract as much value
as you can out of that. So, in sum, in our
195:04 - discussion of available applications for coding
and data science. First remember applications
195:10 - are tools, they don't drive you, you use them.
And that your goals are what drive the choice
195:17 - of your applications and the way that you
do it. And the single most important thing
195:21 - is to remember, what works for you, may work
well for somebody else, if you're not comfortable
195:26 - with it, if it's not the questions you address,
then it's more important to think about what
195:32 - works for you and the projects that you're
working on as you make your own choices for
195:37 - tools, for working in data science. When you're
"Coding in Data Science," one of the most
195:43 - important things you can do is be able to
work with web data. And if you work with web
195:47 - data you're going to be working with HTML.
And in case you're not familiar with it, HTML
195:53 - is what makes the World Wide Web go ‘round.
What it stands for is HyperText Markup Language
196:00 - - and if you've never dealt with web pages
before, here's a little secret: web pages
196:07 - are just text. It is just a text document,
but it uses tags to define the structure of
196:15 - the document and a web browser knows what
those tags are and it displays them the right
196:20 - way. So, for instance, some of the tags, they
look like this. They are in angle brackets,
196:26 - and you have an angle bracket and then the
beginning tag, so body, and then you have
196:31 - the body, the main part of your text, and
then you have in angle brackets with backslash
196:36 - body to let the computer know that you are
done with that part. You also have p and backslash
196:42 - p for paragraphs. H1 is for header one and
you put it in between that text. TD is for
196:50 - table data or the cell in a table and you
mark it off that way. If you want to see what
196:55 - it looks like just go to this document: DS03_3_1_HTML.txt.
I'm going to go to that one right now. Now,
197:05 - depending on what text editor you open this
up, it may actually give you the web preview.
197:10 - I've opened it up in TextMate and so it actually
is showing the text the way I typed it. I
197:17 - typed this manually; I just typed it all in
there. And I have HTML to see what a document
197:23 - is, I have an empty header, but that sort
of needs to be there. This, I say what the
197:27 - body is, and then I have some text. li is
for list items, I have headers, this is for
197:33 - a link to a webpage, then I have a small table.
And if you want to see what this looks like
197:41 - when displayed as a web page, just go up here
to window and show web preview. This is the
197:47 - same document, but now it is in a browser
and that's how you make a web page. Now, I
197:53 - know this is very fundamental stuff, but the
reason this is important is because if you're
197:57 - going to be extracting data from the web,
you have to understand how that information
198:02 - is encoded in the web, and it is going to
be in HTML most of the time for a regular
198:07 - web page. Now, I will mention something that,
there's another thing called CSS. Web pages
198:13 - use CSS to define the appearance of a document.
HTML is theoretically there to give the content
198:20 - and CSS gives the appearance. And that stands
for Cascading Style Sheets. I'm not going
198:25 - to worry about that right now because we're
really interested in the content. And now
198:30 - you have the key to being able to read web
pages and pull data from web pages for your
198:36 - data science project. So, in sum; first, the
web runs on HTML and that's what makes the
198:43 - web pages that are there. HTML defines the
page structure and the content that is on
198:48 - the page. And you need to learn how to navigate
the tags and the structure in order to get
198:53 - data from the web pages for your data science
projects. The next step in "Coding and Data
199:00 - Science" when you're working with web data
is to understand a little bit about XML. I
199:06 - like to think of this as the part of web data
that follows the imperative, "Data, define
199:11 - thyself". XML stands for eXtensible Markup
Language, and what it is XML is semi-structured
199:20 - data. What that means is that tags define
data so a computer knows what a particular
199:25 - piece of information is. But, unlike HTML,
the tags are free to be defined any way you
199:31 - want. And so you have this enormous flexibility
in there, but you're still able to specify
199:36 - it so the computer can read it. Now, there's
a couple of places where you're going to see
199:40 - XML files. Number one is in web data. HTML
defines the structure of a web page, but if
199:46 - they're feeding data into it, then that will
often come in the form of an XML file. Interestingly,
199:54 - Microsoft Office files, if you have .docx
or .xlsx, the X-part at the end stands for
200:01 - a version of XML that's used to create these
documents. If you use iTunes, the library
200:07 - information that has all of your artists,
and your genre's, and your ratings and stuff,
200:12 - that's all stored in an XML file. And then
finally, data files that often go with particular
200:19 - programs can be saved as XML as a way of representing
the structure of the data to the program.
200:25 - And for XML, tags use opening and closing
angle brackets just like HTML did. Again,
200:32 - the major difference is that you're free to
define the tags however you want. So for instance,
200:37 - thinking about iTunes, you can define a tag
that's genre, and you have the angle brackets
200:42 - in genre to begin that information, and then
you have the angle brackets with the backslash
200:48 - to let it know you're done with that piece
of information. Or, you can do it for composer,
200:53 - or you can do it for rating, or you can do
it for comments, and you can create any tags
200:58 - you want and you put the information in between
those two things. Now, let's take an example
201:03 - of how this works. I'm going to show you a
quick dataset that comes from the web. It's
201:09 - at ergast.com and API, and this is a website
that stores information about automobile Formula
201:16 - One racing. Let's go to this webpage and take
a quick look at what it's like. So, here we
201:22 - are at Ergast.com, and it's the API for Formula
One. And what I'm bringing up is the results
201:28 - of the 1957 season in Formula One racing.
And here you can see who the competitors were
201:33 - in each race, and how they finished and so
on. So, this is a dataset that is being displayed
201:39 - in a web page. If you want to see what it
looks like in XML, all you have to do is type
201:44 - XML onto the end of this: .XML. I've done
that already, so I'm just going to go to that
201:49 - one. And as you see, it's only this bit that
I've added: .XML. Now, it looks exactly the
201:54 - same because the web page is structuring XML
data by default but if you want to see what
201:58 - it looks like in its raw format, just do an
option, click on the web page, and go to view
202:05 - page source. At least that's how it works
in Chrome, and this is the structured XML
202:12 - page. And you can see we have tags here. It
says Race Name, Circuit Name, Location, and
202:19 - obviously, these are not standard HTML tags.
They are defined for the purposes of this
202:23 - particular dataset. But we begin with one.
We have Circuit Name right there, and then
202:29 - we close it using the backslash right there.
And so this is structured data; the computer
202:35 - knows how to read it, which is exactly, this
is how it displays it by default. So, it's
202:40 - a really good way of displaying data and its
a good way to know how to pull data from the
202:46 - web. You can actually use what is called an
API, an Application Programming interface
202:50 - to access this XML data and it pulls it in
along with its structure which makes working
202:55 - with it really easy. What's even more interesting
is how easy it is to take XML data and convert
203:02 - it between different formats, because it's
structured and the computer knows what you're
203:06 - dealing with. So for example, one it's really
easy to convert XML to CSV or comma separated
203:14 - value files (that's the spreadsheet format)
because it knows exactly what the headings
203:18 - are; what piece of information goes in each
column. Example two: it's really easy to convert
203:24 - HTML documents to XML because you can think
of HTML with its restricted set of tags as
203:32 - sort of a subset of the much freer XML. And
three, you can convert CSV, or your spreadsheet
203:39 - comma separated value, to XML and vice versa.
You can bounce them all back and forth because
203:44 - the structure is made clear to the programs
you're working with. So in sum, here's what
203:50 - we can say. Number one, XML is semi-structured
data. What that means is that it has tags
203:57 - to tell the computer what the piece of information
is, but you can make the tags whatever you
204:01 - want them to be. And, XML is very common for
web data and it's really easy to translate
204:09 - the format XML/HTML/CSV so on and so forth.
It's really easy to translate them back and
204:15 - forth which gives you a lot of flexibility
in manipulating data so can get into the format
204:20 - you need for your own analysis. The last thing
I want to mention about "Coding and Data Science"
204:28 - and web data is something called JSON. And
I like to think of it as a version of smaller
204:36 - is better. Now, what JSON stands for is JavaScript
Object Notation, although JavaScript is supposed
204:43 - to be one word. And what it is, is that like
XML, JSON is semi-structured data. That is,
204:51 - you have tags that define the data, so the
computer knows what each piece of information
204:55 - is, but like XML the tags can vary freely.
And so there's a lot in common between XML
205:02 - and JSON. So XML is a Markup Language (that's
what the ML stands for), and that gives meaning
205:09 - to the text; it lets the computer know what
each piece of information is. Also, XML allows
205:14 - you to make comments in the document, and
it allows you to put metadata in the tags
205:18 - so you can actually put information there
in the angle brackets to provide additional
205:22 - context. JSON, on the other hand, is specifically
designed for data interchange and so it's
205:28 - got that special focus. And the structure;
JSON corresponds with data structures, you
205:34 - know it directly represents objects and arrays
and numbers and strings and booleans, and
205:39 - that works really well with the programs that
are used to analyze data. Also, JSON is typically
205:46 - shorter than XML because it does not require
the closing tags. Now, there are ways to do
205:51 - that with XML, but that's not typically how
it's done. As a result of these differences,
205:58 - JSON is basically taking XML's place in web
data. XML still exists, it's still used for
206:04 - a lot of things, but JSON is slowly replacing
it. And we'll take a look at the comparison
206:09 - between the three by going back to the example
we used in XML. This is data about Formula
206:15 - One car races in 1957 from ergast.com. You
can just go to the first web page here, then
206:23 - we will navigate to the others from that.
So this is the general page. This is if you
206:28 - just type in without the .XML or .JSON or
anything. So it's a table of information about
206:35 - races in 1957. And we saw earlier that if
you add just add .XML to the end of this,
206:41 - it looks exactly the same. That's because
this browser is displaying XML properly by
206:47 - default. But, if you were to right click on
it, and go to view page source, you would
206:54 - get this instead, and you can see the structure.
This is still XML, and so everything has an
206:59 - opening tag and a closing tag and some extra
information in there. But, if you type in
207:06 - .JSON what you really get is this jumbled
mess. Now that's unfortunate because there
207:13 - is a lot of structure to this. So, what I
am going to do is, I am actually going to
207:17 - copy all of this data, then I'm going to go
to a little web page; there's a lot of things
207:22 - you can do here, and it's a cute phrase. It's
called JSON Pretty Print. And that is, make
207:28 - it look structured so it's easier to read.
I just paste that in there and hit Pretty
207:33 - Print JSON, and now you can see hierarchical
structure of the data. The interesting thing
207:41 - is that the JSON tags only have tags at the
beginning. It says series in quotes, then
207:49 - a colon, then it gives the piece of information
in quotes, and a comma and it moves on to
207:53 - the next one. And this is a lot more similar
to the way data would be represented in something
207:58 - like R or Python. It is also more compact.
Again, there are things you can do with XML
208:06 - but this is one of the reasons that JSON is
becoming preferred as a data carrier for websites.
208:14 - And as you may have guessed, it's really easy
to convert between the formats. It's easy
208:19 - to convert between XML, JSON, CSV, etc. You
can get a web page where you can paste a version
208:27 - in and you get the other version out. There
are some differences, but for the vast majority
208:32 - of situations, they are just interchangeable.
In Sum: what did we get from this? Like XML,
208:39 - JSON is semi-structured data, where there
are tags that say what the information is,
208:43 - but you define the tags however you want.
JSON is specifically designed for data interchange
208:50 - and because it reflects the structure of the
data in the programs, that makes it really
208:54 - easy. Also, because it's relatively compact
JSON is replacing gradually XML on the web,
209:03 - as the container for data on web pages. If
we are going to talk about "Coding and Data
209:10 - Science" and the languages that are used,
then first and foremost is R. The reason for
209:15 - that is, according to many standards, R is
the language of data and data science. For
209:21 - example, take a look at this chart. This is
a ranking based on a survey of data mining
209:26 - experts of the software they use in doing
their work, and R is right there at the top.
209:32 - R is first, and in fact that's important because
there's Python which is usually taken hand
209:39 - in hand with R for Data Science. But R sees
50% more use than Python does, at least in
209:46 - this particular list. Now there's a few reasons
for that popularity. Number one, R is free
209:52 - and it's open source, both of which make things
very easy. Second, R is specially developed
209:58 - for vector operations. That means it's able
to go through an entire list of data without
210:01 - having to write ‘for' loops to go through.
If you've ever had to write ‘for' loops,
210:04 - you know that would be kind of disastrous
having to do that with data analysis. Next,
210:10 - R has a fabulous community behind it. It's
very easy to get help on things with R, you
210:15 - Google it, you're going to end up in a place
where you're going to be able to find good
210:17 - examples of what you need. And probably most
importantly, R is very capable. R has 7,000
210:28 - packages that add capabilities to R. Essentially,
it can do anything. Now, when you are working
210:35 - with R, you actually have a choice of interfaces.
That is, how you actually do the coding and
210:39 - how you get your results. R comes with it's
own IDE or Interactive Development Environment.
210:46 - You can do that, or if you are on a Mac or
a Linux you can actually do R through the
210:50 - Terminal through the command line. If you've
installed R, you just type R and it starts
210:54 - up. There is also a very popular development
environment called RStudio.com, and that's
211:00 - actually the one I use and the one I will
be using for all my examples. But another
211:03 - new competitor is Jupyter, which is very commonly
used for Python; that's what I use for examples
211:09 - there. It works in a browser window, even
though its locally installed. And RStudio
211:14 - and Jupyter there's pluses and minus to each
one of them and I'll mention them as we get
211:17 - to each one of them. But no matter which interface
you use, R's command line, you're typing lines
211:22 - of code in order to get the commands. Some
people get really scared about that but really
211:28 - there are some advantages to that in terms
of the replicability and really the accessibility,
211:34 - the transparency of your commands. So for
instance, here's a short example of some of
211:39 - the commands in R. You can enter them into
what is called a console, and that's just
211:44 - one line at a time and that's called an interactive
way. Or you can save scripts and run bits
211:49 - and pieces selectively and that makes your
life a lot easier. No matter how you do it,
211:54 - if you are familiar with programming other
languages then you're going to find that R's
211:57 - a little weird. It has an idiosyncratic model.
It makes sense once you get used to it, but
212:03 - it is a different approach, and so it takes
some adaptation if you are accustomed to programming
212:09 - in different languages. Now, once you do your
programming to get your output, what you're
212:14 - going to get is graphs in a separate window.
You're going to get text and numbers, numerical
212:20 - output in the console, and no matter what
you get, you can save the output to files.
212:24 - So that makes it portable, you can do it in
other environments. But most importantly,
212:30 - I like to think of this: here's our box of
chocolates where you never know what you're
212:34 - going to get. The beauty of R is in the packages
that are available to expand its capabilities.
212:40 - Now there are two sources of packages for
R. One goes by the name of CRAN, and that
212:46 - stands for the Comprehensive R Archive Network,
and that's at cran.rstudio.com. And what that
212:54 - does is takes the 7,000 different packages
that are available and organizes them into
213:00 - topics that they call task views. And for
each one if they have done their homework,
213:05 - they have datasets that come along with the
package. You have a manual in .pdf format,
213:10 - and you can even have vignettes where they
run through examples of how to do it. Another
213:16 - interface is called Crantastic! And the exclamation
point is part of the title. And that is at
213:21 - crantastic.org. And what this is, is an alternative
interface that links to CRAN. So if you find
213:27 - something you like in Crantastic! and you
click on the link, it's going to open in CRAN.
213:32 - But the nice thing about Crantastic! is it
shows the popularity of packages, and it also
213:37 - shows how recently they were updated, and
that can be a nice way of knowing you're getting
213:41 - sort of the latest and greatest. Now from
this very abstract presentation, we can say
213:46 - a few things about R: Number one, according
to many, R is the language of data science
213:53 - and it's a command line interface. You're
typing lines of code, so that gives it both
213:58 - a strength and a challenge for some people.
But the beautiful thing is that for the thousands
214:03 - and thousands of packages of additional code
and capability that are available for R, that
214:08 - make it possible to do nearly anything in
this statistical programming language. When,
214:15 - talking about "Coding and Data Science" and
the languages, along with R, we need to talk
214:19 - about Python. Now, Python the snakes is a
general-purpose program that can do it all,
214:27 - and that's its beauty. If we go back to the
survey of the software used by data mining
214:35 - experts, you see that Python's there and it's
number three on the list. What's significant
214:39 - about that, is that on this list, Python is
the only general purpose programming language.
214:45 - It's the only one that can be theoretically
used to develop any kind of application that
214:50 - you want. That gives it some special powers
compared to all the others, most of which
214:54 - are very specific to data science work. The
nice things about Python are: number one,
215:01 - it's general purpose. It's also really easy
to use, and if you have a Macintosh or Linux
215:08 - computer, Python is built into it. Also, Python
has a fabulous community around it with hundreds
215:15 - of thousands of people involved, and also
python has thousands of packages. Now, it
215:21 - actually has 70 or 80,000 packages, but in
terms of ones that are for data, there are
215:26 - still thousands available that give it some
incredible capabilities. A couple of things
215:33 - to know about Python. First, is about versions.
There are two versions of Python that are
215:38 - in wide circulation: there's 2.x; so that
means like 2.5 or 2.6, and 3.x; so 3.1, 3.2.
215:47 - Version 2 and version 3 are similar, but they
are not identical. In fact, the problem is
215:53 - this: there are some compatibility issues
where code that runs in one does not run in
215:59 - the other. And consequently, most people have
to choose between one and the other. And what
216:06 - this leads to is that many people still use
2.x. I have to admit, in the examples that
216:11 - I use, I'm using 2.x because so many of the
data science packages that are developed with
216:18 - that in mind. Now let me say a few things
about the interfaces for Python. First, Python
216:24 - does come with its own Interactive Development
Learning Environment and they call it IDLE.
216:29 - You can also run it from the Terminal, or
command line interface, or any IDE that you
216:33 - have. A very common and a very good choice
is Jupyter. Jupyter is a browser-based framework
216:42 - for programming and it was originally called
IPython. That served as its initial, so a
216:49 - lot of the time when people are talking about
IPython, what they are really talking about
216:52 - is this Python in Jupyter and the two are
sometimes used interchangeably. One of the
216:57 - neat things you can do, there are two companies:
Continuum and Enthought. Both of which have
217:02 - made special distributions of Python with
hundreds and hundreds of packages preconfigured
217:10 - to make it very easy to work with data. I
personally prefer Continuum Anaconda, it's
217:15 - the one that I use, a lot of other people
use it, but either one is going to work and
217:19 - it's going to get you up and running. And
like I said with R, no matter what interface
217:23 - you use, all of them are command line. You're
typing lines of code. Again, there is tremendous
217:29 - strength to that but, it can be intimidating
to some people at first. In terms of the actual
217:34 - commands of Python, we have some examples
here on the side, and the important thing
217:38 - to remember is that it's a text interface.
On the other hand, Python is familiar to millions
217:44 - of people because it is very often a first
programming language people learn to do general
217:50 - purpose programming. And there are a lot of
very simple adaptations for data that make
217:55 - it very powerful for data science work. So,
let me say something else again: data science
218:01 - loves Jupyter, and Jupyter is the browser-based
framework. It's a local installation, but
218:08 - you access it through a web browser that makes
it possible to really do some excellent work
218:14 - in data science. There's a few reasons for
this. When you're working in Jupyter you get
218:19 - text output and you can use what's called
Markdown as a way of formatting documents.
218:24 - You can get inline graphics for the graphics
to show up directly beneath the code that
218:28 - you did it. It's also really easy to organize,
present, and to share analyses that are done
218:35 - in Jupyter. Which makes it a strong contender
for your choices in how you do data science
218:41 - programming. Another one of the beautiful
things about Python, like R, is there are
218:46 - thousands of packages available. In Python,
there is one main repository; it goes by the
218:51 - name PyPI. Which is for the Python Package
Index. Right here it says there are over 80,000
218:57 - packages and 7 or 8,000 of those are for data-specific
purposes. Some of the packages that you will
219:04 - get to be very familiar with are NumPy and
SciPy, which are for scientific computing
219:10 - in general; Matplotlib and a development of
it called Seaborn are for data visualization
219:18 - and graphics. Pandas is the main package for
the doing statistical analysis. And for machine
219:25 - learning, almost nothing beats scikit-learn.
And when I go through hands-on examples in
219:32 - Python, I will be using all of these as a
way of demonstrating the power of the program
219:37 - for working with data. In sum we can say a
few things: Python is a very popular program
219:45 - very familiar to millions of people and that
makes it a good choice. Second, of all the
219:49 - languages we use for data science on a frequent
basis, this is the only one that's general
219:54 - purpose. Which means it can be used for a
lot of things other than processing data.
219:59 - And it gets its power, like R does, from having
thousands of contributed packages which greatly
220:05 - expand its capabilities especially in terms
of doing data science work. A choice for "Coding
220:12 - in Data Science," one of the languages that
may not come immediately to mind when they
220:16 - think data science, is Sequel or SQL. SQL
is the language of databases and we think,
220:23 - "why do we want to work in SQL?" Well, to
paraphrase the famous bank robber Willie Sudden
220:29 - who apparently explained why he robbed banks
and said: "Because that's where the money
220:33 - is." The reason we would with SQL in data
science is because that's where the data is.
220:39 - Let's take another look at our ranking of
software among data mining professionals,
220:44 - and there's SQL. Third on the list, and also
of this list, its also the first database
220:50 - tool. Other tools, for instance, get much
fancier, and much new and shinier, but SQL
220:56 - has been around for a while as very very capable.
There's a few things to know about SQL. You
221:02 - will notice that I am saying Sequel even though
it stands for Structured Query Language. SQL
221:10 - is a language, not an application. There's
not a program SQL, it's a language that can
221:16 - be used in different applications. Primarily,
SQL is designed for what are called relational
221:21 - databases. And those are special ways of storing
structured data that you can pull in. You
221:28 - can put things together, you can join them
in special ways, you can get summary statistics,
221:33 - and then what you usually do is then export
that data into your analytical application
221:39 - of choice. The big word here is RDBMS - Relational
Database Management System; that is where
221:50 - you will usually see SQL as a query language
being used. In terms of Relational Database
221:57 - Management System, there are a few very common
choices. In the industrial world where people
222:03 - have some money to spend, there's Oracle database
is a very common one and Microsoft SQL Server.
222:10 - In the open source world, two very common
choices are MySQL, even though we generally
222:16 - say Sequel, when it's here you generally say
MySQL. Another one is PostgreSQL. These are
222:23 - both open source, free versions of the language;
sort of dialects of each, that make it possible
222:30 - for you to working with your databases and
for you to get your information out. The neat
222:34 - thing about them, no matter what you do, databases
minimize data redundancy by using connected
222:39 - tables. Each table has rows and columns and
they store different levels or different of
222:45 - abstraction or measurement, which means you
only have to put the information one place
222:49 - and then it can refer to lots of other tables.
Makes it very easy to keep things organized
222:54 - and up to date. When you are looking into
a way of working with a Relational Database
222:59 - Management System, you get to choose in part
between using a graphical user interface or
223:05 - GUI. Some of those include SQL Developer and
SQL Server Management Studio, two very common
223:11 - choices. And there are a lot of other choices
such as Toad and some other choices that are
223:16 - graphical interfaces for working with these
databases. There are also text-based interfaces.
223:23 - So really, any command line interface, and
any interactive development environment or
223:29 - programming tool is going to be able to do
that. Now, you can think of yourself on the
223:34 - command deck of your ship and think of a few
basic commands that are very important for
223:38 - working with SQL. There are just a handful
of commands that can get you where you need
223:44 - to go. There is the Select command, where
you're choosing the cases that you want to
223:49 - include. From: says what tables are you going
to be extracting them from. Where: is a way
223:57 - of specifying conditions, and then Order By:
obviously is just a way of putting it all
224:02 - together. This works because usually when
you are in a SQL database you're just pulling
224:08 - out the information. You want to select it,
you want to organize it, and then what you
224:13 - are going to do is you are going to send the
data to your program of choice for further
224:17 - analysis, like R or Python or whatever. In
sum here's what we can say about SQL: Number
224:24 - one, as a language it's generally associated
with relational databases, which are very
224:31 - efficient and well-structured ways of storing
data. Just a handful of basic commands can
224:36 - be very useful when working with databases.
You don't have have to be a super ninja expert,
224:41 - really a handful. Five, 10 commands will probably
get you everything you need out of a SQL database.
224:48 - Then once the data is organized, the data
is typically exported to some other program
224:52 - for analysis. When you talk about coding in
any field, one of the languages or one of
225:00 - the groups of languages that come up most
often are C, C++, and Java. These are extremely
225:07 - powerful applications and very frequently
used for professional, production level coding.
225:13 - In data science, the place where you will
see these languages most often is in the bedrock.
225:19 - The absolute fundamental layer that makes
the rest of data science possible. For instance,
225:26 - C and C++. C is from the ‘60s, C++ is from
the ‘80s, and they have extraordinary wide
225:33 - usage, and their major advantage is that they're
really really fast. In fact, C is usually
225:39 - used as the benchmark for how fast is a language.
They are also very, very stable, which makes
225:44 - them really well suited to production-level
code and, for instance, server use. What's
225:50 - really neat is that in certain situations,
if time is really important, if speeds important,
225:55 - then you can actually use C code in R or other
statistical languages. Next is Java. Java
226:03 - is based on C++, it's major contribution was
the WORA or the Write Once Run Anywhere. The
226:11 - idea that you were going to be able to develop
code that is portable to different machines
226:16 - and different environments. Because of that,
Java is the most popular computer programming
226:22 - language overall against all tech situations.
The place you would use these in data science,
226:30 - like I said, when time is of the essence,
when something has to be fast, it has to get
226:35 - the job accomplished quickly, and it has to
not break. Then these are the ones you're
226:39 - probably going to use. The people who are
going to use it are primarily going to be
226:43 - engineers. The engineers and the software
developers who deal with the inner workings
226:50 - of the algorithms in data science or the back
end of data science. The servers and the mainframes
226:56 - and the entire structure that makes analysis
possible. In terms of analysts, people who
227:04 - are actually analyzing the data, typically
don't do hands-on work with the foundational
227:09 - elements. They don't usually touch C or C++,
more of the work is on the front end or closer
227:14 - to the high-level languages like R or Python.
In sum: C, C++ and Java form a foundational
227:24 - bedrock in the back end of data and data science.
They do this because they are very fast and
227:30 - they are very reliable. On the other hand,
given their nature that work is typically
227:36 - reserved for the engineers who are working
with the equipment that runs in the back that
227:41 - makes the rest of the analysis possible. I
want to finish our extremely brief discussion
227:48 - of "Coding in Data Sciences" and the languages
that can be used, by mentioning one other
227:53 - that's called Bash. Bash really is a great
example of old tools that have survived and
228:01 - are still being used actively and productively
with new data. You can think of it this way,
228:07 - it's almost like typing on your typewriter.
You're working at the command line, you're
228:11 - typing out code through a command line interface
or a CLI. This method of interacting with
228:17 - computers practically goes back to the typewriter
phase, because it predates monitors. So, before
228:23 - you even had a monitor, you would type out
the code and it would print it out on a piece
228:26 - of paper. The important thing to know about
the command line is it's simply a method of
228:32 - interacting. It's not a language, because
lots of languages can run at the command line.
228:38 - For instance, it is important to talk about
the concept of a shell. In computer science,
228:43 - a shell is a language or something that wraps
around the computer. It's a shell around the
228:48 - language, that is the interaction level for
the user to get things done at the lower level
228:54 - that aren't really human-friendly. On Mac
computers and Linux, the most common is Bash,
229:01 - which is short for Bourne Again Shell. On
Windows computers, the most common is PowerShell.
229:13 - But whatever you do there actually are a lot
of choices, there's the Bourne Shell, the
229:17 - C shell; which is why I have a seashell right
here, the Z shell, there's fish for Friendly
229:22 - Interactive Shell, and a whole bunch of other
choices. Bash is the most common on Mac and
229:28 - Linux and PowerShell is the most common on
Windows as a method of interacting with the
229:32 - computer at the command line level. There's
a few things you need to know about this.
229:38 - You have a prompt of some kind, in Bash, it's
a dollar sign, and that just means type your
229:43 - command here. Then, the other thing is you
type one line at a time. It's actually amazing
229:50 - how much you can get done with a one-liner
program, by sort of piping things together,
229:56 - so one feeds into the other. You can run more
complex commands if you use a script. So,
230:03 - you call a text document that has a bunch
of things in it and you can get much more
230:08 - elaborate analyses done. Now, we have our
tools here. In Bash we talk about utilities
230:17 - and what these are, are specific programs
that accomplish specific tools. Bash really
230:22 - thrives on "Do one thing, and do it very well."
There are two general categories of utilities
230:28 - for Bash. Number one, is the Built-ins. These
are the ones that come installed with it,
230:34 - and so you're able to use it anytime by simply
calling in their name. Some more common ones
230:39 - are: cat, which is for catenate; that's to
put information together. There's awk, which
230:47 - is it's own interpreted language, but it's
often used for text processing from the command
230:52 - line. By the way, the name 'Awk' comes from
the initials of the people who created it.
230:57 - Then there's grep, which is for Global search
with a Regular Expression and Print. It's
231:04 - a way of searching for information. And then
there's sed, which stands for Stream Editor
231:10 - and its main use is to transform text. You
can do an enormous amount with just these
231:15 - 4 utilities. A few more are head & tail, display
the first or last 10 lines of a document.
231:24 - Sort & uniq, which sort and count the number
of unique answers in a document. Wc, which
231:31 - is for word count, and printf which formats
the output that you get in your console. And
231:38 - while you can get a huge amount of work done
with just this small number of built-in utilities,
231:44 - there are also a wide range of installable.
Or, other command line utilities that you
231:49 - can add to Bash, or whatever programming language
you're using. So, since some really good ones
231:56 - that have been recently developed are jq:
which is for pulling in JSON or JavaScript,
232:01 - object notation data from the web. And then
there's json2csv, which is a way of converting
232:07 - JSON to csv format, which is what a lot of
statistical programs are going to be happy
232:12 - with. There's Rio which allows you to run
a wide range of commands from the statistical
232:18 - programming language R in the command line
as part of Bash. And then there's BigMLer.
232:25 - This is a command line tool that allows you
to access BigML's machine learning servers
232:31 - through the command line. Normally, you do
it through a web browser and it accesses their
232:35 - servers remote. It's an amazingly useful program
but to be able to just pull it up when you're
232:40 - in the command line is an enormous benefit.
What's interesting is that even though you
232:45 - have all these opportunities, all these different
utilities, you can do all amazing things.
232:50 - And there's still an active element of utilities
for the command line. So, in sum: despite
232:58 - being in one sense as old as the dinosaurs,
the command line survives because it is extremely
233:03 - well evolved and well suited to its purpose
of working with data. The utilities; both
233:10 - the built-in and the installable are fast
and they are easy. In general, they do one
233:15 - thing and they do it very, very well. And
then surprisingly, there is an enormous amount
233:20 - of very active development of command line
utilities for these purposes, especially with
233:26 - data science. One critical task when you are
Coding in Data Science is to be able to find
233:33 - the things that you are looking for, and Regex
(which is short of Regular Expressions) is
233:39 - a wonderful way to do that. You can think
of it as the supercharged method for finding
233:44 - needles in haystacks. Now, Regex tends to
look a little cryptic so, for instance, here's
233:50 - an example. As something that's designed to
determine if something is a valid email address,
233:56 - and it specifies what can go in the beginning,
you have the at sign in the middle, then you've
234:00 - got a certain number of letters and numbers,
then you have to have a dot something at the
234:04 - end. And so, this is a special kind of code
for indicating what can go where. Now regular
234:11 - expressions, or regex, are really a form of
pattern matching in text. And it's a way of
234:17 - specifying what needs to be where, what can
vary, and how much it can vary. And you can
234:22 - write both specific patterns; say I only want
a one letter variation here, or a very general
234:27 - like the email validator that I showed you.
And the idea here is that you can write this
234:32 - search pattern, your little wild card thing,
you can find the data and then once you identify
234:38 - those cases, then you export them into another
program for analysis. So here's a short example
234:44 - of how it can work. What I've done is taken
some text documents, they're actually the
234:48 - texts to Emma and to Pygmalion, two books
I got off of Project Gutenberg, and this is
234:57 - the command. Grep ^l.ve *.txt - so what I'm
looking for in either of these books are lines
235:04 - that start with ‘l', then they can have
one character; can be whatever, then that's
235:10 - followed by ‘ve', and then the .txt means
search for all the text files in the particular
235:17 - folder. And what it found were lines that
began with love, and lived, and lovely, and
235:23 - so on. Now in terms of the actual nuts and
bolts of regular expressions, there are some
235:29 - certain elements. There are literals, and
those are things that are exactly what they
235:34 - mean. You type the letter ‘l', you're looking
for the letter ‘l'. There are also metacharacters,
235:40 - which specify, for instance, things need to
go here; they're characters but are really
235:46 - code that give representations. Now, there
are also escape sequences, which is normally
235:53 - this character is used as a variable, but
I want to really look for a period as opposed
235:58 - to a placeholder. Then you have the entire
search expression that you create and you
236:03 - have the target string, the thing that it
is searching through. So let me give you a
236:07 - few very short examples. ^ this is the caret.
This is the sometimes called a hat or in French,
236:14 - a circonflexe. What that means, you're looking
for something at the beginning of the search
236:18 - you are searching. For example, you can have
^ and capital M, that means you need something
236:24 - that begins with capital M. For instance the
word "Mac," true, it will find that. But if
236:31 - you have iMac, it's a capital M, but it's
not the first letter and so that would be
236:35 - false, it won't find that. The $ means you
are looking for something at the end of the
236:41 - string. So for example: ing$ that will find
the word ‘fling' because it ends in ‘ing',
236:49 - but it won't find the word ‘flings' because
it actually ends with an ‘s'. And then the
236:55 - dot, the period, simply means that we are
looking for one letter and it can be anything.
236:59 - So, for example, you can write ‘at.'. And
that will find ‘data' because it has an
237:06 - ‘a', a ‘t', and then one letter after
it. But it won't find ‘flat', because ‘flat'
237:12 - doesn't have anything after the ‘at'. And
so these are extremely simple examples of
237:16 - how it can work. Obviously, it gets more complicated
and the real power comes when you start combining
237:22 - these bits and elements. Now, one interesting
thing about this is you can actually treat
237:26 - this as a game. I love this website, it's
called Regex golf and it's at regex.alf.nu.
237:33 - And what it does is brings up lists of words;
two columns, and your job is to write a regular
237:41 - expression in the top, that matches all the
words on the left column and none of the words
237:46 - on the right. And uses the fewest characters
possible, and you get a score! And it's a
237:51 - great way of learning how to do regular expressions
and learning how to search in a way that is
237:58 - going to get you the data you need for your
projects. So, in sum: Regex, or regular expressions,
238:05 - help you find the right data for your project,
they're very powerful and they're very flexible.
238:10 - Now, on the other hand, they are cryptic,
at least when you first look at them but at
238:14 - the same time, it's like a puzzle and it can
be a lot of fun if you practice it and you
238:18 - see how you can find what you need. I want
to thank you for joining me in "Coding in
238:26 - Data Science" and we'll wrap up this course
by talking about some of the specific next
238:31 - steps you can take for working in data science.
The idea here, is that you want to get some
238:36 - tools and you want to start working with those
tools. Now, please keep in mind something
238:41 - that I've said at another time. Data tools
and data science are related, they're important
238:48 - but don't make the mistake of thinking that
if you know the tools that you have done the
238:52 - same thing as actually conducted data science.
That's not true, people sometimes get a little
238:57 - enthusiastic and they get a little carried
away. What you need to remember is the relationship
239:01 - really is this: Data Tools are an important
part of data science, but data science itself
239:06 - is much bigger than just the tools. Now, speaking
of tools remember there's a few kinds that
239:13 - you can use, and that you might want to get
some experience with these. #1, in terms of
239:17 - just Apps, specific built applications Excel
& Tableau are really fundamental for both
239:25 - getting the data from clients or doing some
basic data browsing and Tableau is really
239:31 - wonderful for interactive data visualization.
I strongly recommend you get very comfortable
239:36 - with both of those. In terms of code, it's
a good idea to learn either ‘R' or ‘Python'
239:43 - or ideally to learn both. Ideally because
you can use them hand in hand. In terms of
239:49 - utilities, it's a great idea to work with
Bash, the command line utility and to use
239:54 - regular expression or regex. You can actually
use those in lots and lots of programs; regular
239:59 - expressions. So they can have a very wide
application. And then finally, data science
240:05 - requires some sort of domain expertise. You're
going to need some sort of field experience
240:10 - or intimate understanding of a particular
domain and the challenges that come up and
240:16 - what constitutes workable answers and the
kind of data that's available. Now, as you
240:20 - go through all of this, you don't need to
build this monstrous list of things. Remember,
240:25 - you don't need everything. You don't need
every tool, you don't need every function,
240:30 - you don't need every approach. Instead remember,
get what's best for your needs, and for your
240:37 - style. But no matter what you do, remember
that tools are tools, they are a means to
240:42 - an end. Instead, you want to focus on the
goal of your data science project whatever
240:48 - it is. And I can tell you really, the goal
is in the meaning, extracting meaning out
240:53 - of your data to make informed choices. In
fact, I'll say a little more. The goal is
240:58 - always meaning. And so with that, I strongly
encourage you to get some tools, get started
241:04 - in data science and start finding meaning
in the data that's around you. Welcome to
241:10 - "Mathematics in Data Science". I'm Barton
Poulson and we're going to talk about how
241:14 - Mathematics matters for data science. Now,
you maybe saying to yourself, "Why math?",
241:22 - and "Computers can do it, I don't need to
do it". And really fundamentally, "I don't
241:28 - need math I am just here to do my work". Well,
I am here to tell you, No. You need math.
241:35 - That is if you want to be a data scientist,
and I assume that you do. So we are going
241:41 - to talk about some of the basic elements of
Mathematics, really at a conceptual level
241:45 - and how they apply to data science. There
are few ways that math really matters to data
241:51 - science. #1, it allows you to know which procedures
to use and why. So you can answer your questions
241:59 - in a way that is the most informative and
the most useful. #2, if you have a good understanding
242:05 - of math, then you know what to do when things
don't work right. That you get impossible
242:10 - values or things won't compute, and that makes
a huge difference. And then #3, an interesting
242:17 - thing is that some mathematical procedures
are easier and quicker to do by hand then
242:23 - by actually firing up the computer. And so
for all 3 of these reasons, it's really helpful
242:28 - to have at least a grounding in Mathematics
if you're going to do work in data science.
242:34 - Now probably the most important thing to start
with in Algebra. And there are 3 kinds of
242:39 - algebra I want to mention. The first is elementary
algebra, that's the regular x+y. Then there
242:45 - is Linear or matrix algebra which looks more
complex, but is conceptually it is used by
242:50 - computers to actually do the calculations.
And then finally I am going to mention Systems
242:54 - of Linear Equations where you have multiple
equations simultaneously that you're trying
242:59 - to solve. Now there's more math than just
algebra. A few other things I'm going to cover
243:04 - in this course. Calculus, a little bit of
Big O or order which has to do with the speed
243:11 - and complexity of operations. A little bit
of probability theory and a little bit of
243:17 - Bayes or Bayes theorem which is used for getting
posterior probabilities and changes the way
243:21 - you interpret the results of an analysis.
And for the purposes of this course, I'm going
243:27 - to demonstrate the procedures by hand, of
course you would use software to do this in
243:32 - the real world, but we are dealing with simple
problems at conceptual levels. And really,
243:37 - the most important thing to remember is that
even though a lot of people get put off by
243:41 - math, really You can do it! And so, in sum:
let's say these three things about math. First
243:47 - off, you do need some math to do good data
science. It helps you diagnose problems, it
243:52 - helps you choose the right procedures, and
interestingly you can do a lot of it by hand,
243:57 - or you can use software computers to do the
calculations as well. As we begin our discussion
244:04 - of the role of "Mathematics and Data Science",
we'll of course begin with the foundational
244:08 - elements. And in data science nothing is more
foundational than Elementary Algebra. Now,
244:14 - I'd like to begin this with really just a
bit of history. In case you're not aware,
244:20 - the first book on algebra was written in 820
by Muhammad ibn Musa al-Khwarizmi. And it
244:26 - was called "The Compendious Book on Calculation
by Completion and Balancing". Actually, it
244:31 - was called this, which if you transliterate
that comes out to this, but look at this word
244:37 - right here. That's the algebra, which means
Restoration. In any case, that's where it
244:43 - comes from and for our concerns, there are
several kinds of algebra that we're going
244:47 - to talk about. There's Elementary Algebra,
there's Linear Algebra and there are systems
244:53 - of linear equations. We'll talk about each
of those in different videos. But to put it
244:58 - into context, let's take an example here of
salaries. Now, this is based on real data
245:04 - from a survey of the salary of people employed
in data science and to give a simple version
245:09 - of it. The salary was equal to a constant,
that's sort of an average value that everybody
245:15 - started with and to that you added years,
then some measure of bargaining skills and
245:21 - how many hours they worked per week. And that
gave you your prediction, but that wasn't
245:25 - exact there's also some error to throw into
it to get to the precise value that each person
245:29 - has. Now, if you want to abbreviate this,
you can write it kind of like this: S + C
245:35 - + Y + B + H + E, although it's more common
to write it symbolically like this, and let's
245:44 - go through this equation very quickly. The
first thing we have is outcome,; we call that
245:50 - y the variable y for person i, "i" stands
for each case in our observations. So, here's
245:58 - outcome y for person i. This letter here,
is a Greek Beta and it represents the intercept
246:07 - or the average, that's why it has a zero,
because we don't multiply it times anything.
246:11 - But right next to it we have a coefficient
for variable 1. So Beta, which means a coefficient,
246:18 - sub 1 for the first variable and then we have
variable 1 then x 1, means variable 1, then
246:26 - i means its the score on that variable for
person i, whoever we are talking about. Then
246:30 - we do the same thing for variables 2 and 3,
and at the end, we have a little epsilon here
246:36 - with an i for the error term for person i,
which says how far off from the prediction
246:40 - was their actual score. Now, I'm going to
run through some of these procedures and we'll
246:46 - see how they can be applied to data science.
But for right now let's just say this in sum.
246:51 - First off, Algebra is vital to data science.
It allows you to combine multiple scores,
246:57 - get a single outcome, do a lot of other manipulations.
And really, the calculations, their easy for
247:01 - one case at at time. Especially when you're
doing it by hand. The next step for "Mathematics
247:07 - for Data Science" foundations is to look at
Linear algebra or an extension of elementary
247:13 - algebra. And depending on your background,
you may know this by another name and I like
247:17 - to think welcome to the Matrix. Because it's
also known as matrix algebra because we are
247:23 - dealing with matrices . Now, let's go back
to an example I gave in the last video about
247:27 - salary. Where salary is equal to a constant
plus years, plus bargaining, plus hours plus
247:33 - error, okay that's a way to write it out in
words and if you want to put it in symbolic
247:38 - form, it's going to look like this. Now before
we get started with matrix algebra, we need
247:43 - to talk about a few new words, maybe you're
familiar with them already. The first is Scalar,
247:49 - and this means a single number. And then a
vector is a single row or a single column
247:57 - of numbers that can be treated as a collection.
That usually means a variable. And then finally,
248:05 - a matrix consists of many rows and columns.
Sort of a big rectangle of numbers, the plural
248:12 - of that by the way is matrices and the thing
to remember is that Machines love Matrices.
248:19 - Now let's take a look at a very simple example
of this. Here is a very basic representation
248:25 - of matrix algebra or Linear Algebra. Where
we are showing data on two people, on four
248:33 - variables. So over here on the left, we have
the outcomes for cases 1 and 2, our people
248:40 - 1 and 2. And we put it into the square brackets
to indicate that it's a vector or a matrix.
248:45 - Here on the far left, it's a vector because
it's a single column of values. Next to that
248:52 - is a matrix, that has here on the top, the
scores for case 1, which I've written as x's.
248:59 - X1 is for variable 1, X2 is for variable 2
and the second subscript is indicated that
249:05 - it's for person 1. Below that, are the scores
for case 2, the second person. And then over
249:12 - here, in another vertical column are the regression
coefficients, that's a beta there that we
249:18 - are using. And then finally, we've got a tiny
little vector here which contains the error
249:23 - terms for cases 1 and 2. Now, even though
you would not do this by hand, it's helpful
249:29 - to run through the procedure, so I'm going
to show it to you by hand. And we are going
249:34 - to take two fictional people. This will be
fictional person #1, we'll call her Sophie.
249:38 - We'll say that she's 28 years old and we'll
say that she's has good bargaining skills,
249:42 - a 4 on a scale of 5, and that she works 50
hours a week and that her salary is $118,000.00.
249:49 - Our second fictional person, we'll call him
Lars and we'll say that he's 34 years old
249:55 - and he has moderate bargaining skills 3 out
of 5, works 35 hours per week and has a salary
250:01 - of $84,000.00. And so if we are trying to
look at salaries, we can look at our matrix
250:08 - representation that we had here, with our
variables indicated with their Latin and sometimes
250:14 - Greek symbols. And we will replace those variables
with actual numbers. We have the salary for
250:20 - Sophie, our first person. So why don't we
plug in the numbers here and let's start with
250:26 - the result here. Sophie's salary is $118,000.00
and here's how all these numbers all add up
250:32 - to get that. The first thing here is the intercept.
And we just multiply that times 1, so that's
250:37 - sort of the starting point, and then we get
this number 10, which actually has to do with
250:42 - years over 18. She's 28 so that's 10 years
over 18, we multiply each year by 1395. Next
250:49 - is bargaining skills. She's got a 4 out of
5 and for each step up you get $5,900.00.
250:54 - By the way, these are real coefficients from
study of survey of salary of data scientists.
251:01 - And then finally hours per week. For each
hour, you get $382.00. Now you can add these
251:08 - up, and get a predicted value for her but
it's a little low. It's $30,00.00 low. Which
251:14 - you may be saying that's pretty messed up,
well that's because there's like 40 variables
251:17 - in the equation including she might be the
owner and if she's the owner then yes she's
251:21 - going to make a lot more. And then we do a
similar thing for the second case, but what's
251:27 - neat about matrix algebra or Linear Algebra
is this means the same stuff and what we have
251:36 - here are these bolded variables. That stand
in for entire vectors or matrices. So for
251:44 - instance; this Y, a bold Y stands for the
vector of outcome scores. This bolded X is
251:52 - the entire matrix of values that each person
has on each variable. This bolded beta is
252:01 - all of the regression coefficients and then
this bolded epsilon is the entire vector of
252:07 - error terms. And so it's a really super compact
way of representing the entire collection
252:14 - of data and coefficients that you use in predicting
values. So in sum, let's say this. First off,
252:22 - computers use matrices. They like to do linear
algebra to solve problems and is conceptually
252:27 - simpler because you can put it all in there
in this type formation. In fact, it's a very
252:32 - compact notation and it allows you to manipulate
entire collections of numbers pretty easily.
252:38 - And that's that major benefit of learning
a little bit about linear or matrix algebra.
252:43 - Our next step in "Mathematics for Data Science
Foundations" is systems of linear equations.
252:52 - And maybe you are familiar with this, but
maybe you're not. And the idea here is that
252:55 - there are times, when you actually have many
unknowns and you're trying to solve for them
253:01 - all simultaneously. And what makes this really
tricky is that a lot of these are interlocked.
253:08 - Specifically that means X depends on Y, but
at the same time Y depends on X. What's funny
253:14 - about this, is it's actually pretty easy to
solve these by hand and you can also use linear
253:19 - matrix algebra to do it. So let's take a little
example here of Sales. Let's imagine that
253:27 - you have a company and that you've sold 1,000
iPhone cases, so that they are not running
253:31 - around naked like they are in this picture
here. Some of them sold for $20 and others
253:35 - sold for $5. You made a total of $5,900.00
and so the question is "How many were sold
253:43 - at each price?" Now, if you were keeping our
records, but you can also calculate it from
253:50 - this little bit of information. And to show
you I'm going to do it by hand. Now, we're
253:56 - going to start with this. We know that sales
the two price points x + y add up to 1,000
254:02 - total cases sold. And for revenue, we know
that if you multiply a certain number times
254:08 - $20 and another number times $5, that it all
adds up to $5,900.00. Between the two of those
254:15 - we can figure out the rest. Let's start with
sales. Now, what I'm going to do is try to
254:21 - isolate the values. I am going to do that
by putting in this minus y on both sides and
254:29 - then I can take that and I can subtract it,
so I'm left with x is equal to 1,000 - y.
254:35 - Normally I solve for x, but I solve for y,
you'll see why in just a second. Then we go
254:39 - to revenue. We know from earlier that our
sales at these two prices points, add up to
254:45 - $5,900.00 total. Now what we are going to
do is take the x that's right here and we
254:49 - are going to replace it with the equation
we just got, which is 1,000 - y. Then we multiply
254:56 - that through and we get $20,000.00 minus $20y
plus $5 y equals $5,900.00. Well, we can subtract
255:02 - these two because they are on the same thing.
So, $20y then we get $15y, and then we subtract
255:10 - $20,000.00 from both sides. So there it is,
right there on the left, and that disappears,
255:17 - then I get it over on the right side. And
then I do the math there, and I get minus
255:22 - $14, 100.00. Well, then I divide both sides
by negative $15.00 and when we do that we
255:30 - get y equals 940. Okay, so that's one of our
values for sales. Let's go back to sales.
255:37 - We have x plus y equals 1,000. We take the
value we just got, 940, we stick that into
255:42 - the equation, then we can solve for x. Just
subtract 940 from each side, there we go.
255:49 - We get x is equal to 60. So, let's put it
all together, just to recap what happened.
255:56 - What this tells us is that 60 cases were sold
at $20.00 each. And that 940 cases were sold
256:04 - at $5 each. Now, what's interesting about
this is you can also do this graphically.
256:09 - We're going to draw it. So, I'm going to graph
the two equations. Here are the original ones
256:15 - we had. This one predicts sales, this one
gives price. The problem is, these aren't
256:19 - in the economical form for creating graphs.
That needs to be y equals something else,
256:23 - so we're going to solve both of these for
y. We subtract x from both sides, there it
256:29 - is on the left, we subtract that. Then we
have y is equals to minus x plus 1,000. That's
256:34 - something we can graph. Then we do the same
thing for price. Let's divide by 5 all the
256:39 - way through, that gets rid of that and then
we've got this 4x, then let's subtract 4x
256:46 - from each side. And what we are left with
is minus 4x plus 1,180, which is also something
256:52 - we can graph. So this first line, this indicates
cases sold. It originally said x plus y equals
257:01 - 1000, but we rearranged it to y is equal to
minus x plus 1000. And so that's the line
257:07 - we have here. And then we have another line,
which indicates earnings. And this one was
257:13 - originally written as $20.00 times x plus
$5.00 times y equals $5,900.00 total. We rearranged
257:20 - that to y equals minus 4x plus 1,180. That's
the equation for the line and then the solution
257:28 - is right here at the intersection. There's
our intersection and it's at 60 on the number
257:34 - of cases sold at $20.00 and 940 as the number
of cases sold at $5.00 and that also represents
257:40 - the solution of the joint equations. It's
a graphical way of solving a system of linear
257:47 - equations. So in sum, systems of linear equations
allow us to balance several unknowns and find
257:56 - unique solutions. And in many cases, it's
easy to solve by hand, and it's really easy
258:03 - with linear algebra when you use software
to do it at the same time. As we continue
258:09 - our discussion of "Mathematics for Data Science"
and the foundational principles the next thing
258:14 - we want to talk about is Calculus. And I'm
going to give a little more history right
258:19 - here. The reason I'm showing you pictures
of stones, is because the word Calculus is
258:24 - Latin for stone, as in a stone used for tallying.
Where when people would actually have a bag
258:29 - of stones and they would use it to count sheep
or whatever. And the system of Calculus was
258:35 - formalized in the 1,600s simultaneously, independently
by Isaac Newton and Gottfried Wilhelm Leibniz.
258:43 - And there are 3 reasons why Calculus is important
for data science. #1, it's the basis for most
258:50 - of the procedures we do. Things like least
squares regression and probability distributions,
258:55 - they use Calculus in getting those answers.
Second one is if you are studying anything
259:00 - that changes over time. If you are measuring
quantities or rates that change over time
259:05 - then you have to use Calculus. Calculus is
used in finding the maxima and minima of functions
259:11 - especially when you're optimizing. Which is
something I'm going to show you separately.
259:15 - Also, it is important to keep in mind, there
are two kinds of Calculus. The first is differential
259:22 - Calculus, which talks about rates of change
at a specific time. It's also known as the
259:27 - Calculus of change. The second kind of Calculus
is Integral Calculus and this is where you
259:32 - are trying to calculate the quantity of something
at a specific time, given the rate of change.
259:39 - It's also known as the Calculus of Accumulation.
So, let's take a look at how this works and
259:44 - we're going to focus on differential Calculus.
So I'm going to graph an equation here, I'm
259:50 - going to do y equals x2 a very simple one
but it's a curve which makes it harder to
259:55 - calculate things like the slope. Let's take
a point here that's at minus 2, that's the
260:02 - middle of the red dot. X is equal to minus
2. And because y is equal to x2 , if we want
260:08 - to get the y value, all we got to do is take
that negative 2 and square it and that gives
260:13 - us 4. So that's pretty easy. So the coordinates
for that red point are minus 2 on x, and plus
260:19 - 4 on the y. Here's a harder question. "What
is the slope of the curve at that exact point?"
260:25 - Well, it's actually a little tricky because
the curve is always curving there's no flat
260:30 - part on it. But we can get the answer by getting
the derivative of the function. Now, there
260:36 - are several different ways of writing this,
I am using the one that's easiest to type.
260:41 - And let's start by this, what we are going
to do is the n here and that is the squared
260:47 - part, so that we have x2 . And you see that
same n turns into the squared, and then we
260:52 - come over here and we put that same value
2 in right there, and we put the two in right
260:59 - here. And then we can do a little bit of subtraction.
2 minus 1 is 1 and truthfully you can just
261:05 - ignore that then then you get 2x. That is
the derivative, so what we have here is the
261:10 - derivative of x2 is 2x. That means, the slope
at any given point in the curve is 2x. So,
261:19 - let's go back to the curve we had a moment
ago. Here's our curve, here's our point at
261:24 - x minus 2, and so the slope is equal to 2x,
well we put in the minus 2, and we multiply
261:31 - it and we get minus 4. So that is the slope
at this exact point in the curve. Okay, what
261:37 - if we choose a different point? Let's say
we came over here to x is equal to 3? Well,
261:42 - the slope is equal to 2x so that's 2 times
3, is equal to 6. Great! And on the other
261:49 - hand, you might be saying to yourself "And
why do I care about this?" There's a reason
261:55 - that this is important and what it is, is
that you can use these procedures to optimize
262:00 - the decisions. And if that seems a little
to abstract to you, that means you can use
262:05 - them to make more money. And I'm going to
demonstrate that in the next video. But for
262:10 - right now in sum, let's say this. Calculus
is vital to practical data science, it's the
262:17 - foundation of statistics and it forms the
core that's needed for doing optimization.
262:27 - In our discussion about Mathematics and data
science foundations, the last thing I want
262:31 - to talk about right here is calculus and how
it relates to optimization. I like to think
262:36 - of this, in other words, as the place where
math meets reality, or it meets Manhattan
262:40 - or something. Now if you remember this graph
I made in the last video, y is equal to x2,
262:48 - that shows this curve here and we have the
derivative that the slope can be given by
262:53 - 2x. And so when x is equal to 3, the slope
is equal to 6, fine. And this is where this
263:00 - comes into play. Calculus makes it possible
to find values that maximize or minimize outcomes.
263:08 - And if you want to think of something a little
more concrete here, let's think of an example,
263:13 - by the way that's Cupid and Psyche. Let's
talk about pricing for online dating. Let's
263:19 - assume you've created a dating service and
you want to figure out how much can you charge
263:23 - for it that will maximize your revenue. So,
let's get a few hypothetical parameters involved.
263:32 - First off, let's say that subscriptions, annual
subscriptions cost $500.00 each year and you
263:37 - can charge that for a dating service. And
let's say you sell 180 new subscriptions every
263:43 - week. On the other hand, based on your previous
experience manipulating prices around, you
263:49 - have some data that suggests that for each
$5 you discount from the price of $500.00
263:55 - you will get 3 more sales. Also, because its
an online service, lets make our life a little
264:02 - more easier right now and assume there is
no increase in overhead. It's not really how
264:06 - it works, but we'll do it for now. And I'm
actually going to show you how to do all this
264:09 - by hand. Now, let's go back to price first.
We have this. $500.00 is the current annual
264:17 - subscription price and you're going to subtract
$5.00 for each unit of discount, that's why
264:23 - I'm giving D. So, one discount is $5.00, two
discounts is $10.00 and so on. And then we
264:29 - have a little bit of data about sales, that
you're currently selling 180 subscriptions
264:33 - per week and that you will add 3 more for
each unit of discount that you give. So, what
264:42 - we're going to do here is we are going to
find sales as a function of price. Now, to
264:48 - do that the first thing we have to do is get
the y intercept. So we have price here, is
264:54 - $500.00, is the current annual subscription
price minus $5 times d. And what we are going
265:00 - to do is, is we are going to get the y intercept
by solving when does this equal zero? Okay,
265:06 - well we take the $500 we subtract that from
both sides and then we end up with minus $5d
265:12 - is equal to minus $500.00. Divide both sides
by minus $5 and we are left with d is equal
265:19 - to 100. That is, when d is equal to 100, x
is 0. And that tells us how we can get the
265:26 - y intercept, but to get that we have to substitute
this value into sales. So we take d is equal
265:32 - to 100, and the intercept is equal to 180
plus 3; 180 is the number of new subscriptions
265:38 - per week and then we take the three and we
multiply that times our 100. So, 180 times
265:45 - 3 times 100,[1] is equal to 300 add those
together and you get 480. And that is the
265:52 - y intercept in our equation, so when we've
discounted sort of price to zero then the
265:59 - expected sale is 480. Of course that's not
going to happen in reality, but it's necessary
266:04 - for finding the slope of the line. So now
let's get the slope. The slope is equal to
266:10 - the change in y on the y axis divided by the
change in x. One way we can get this is by
266:18 - looking at sales; we get our 180 new subscriptions
per week plus 3 for each unit of discount
266:26 - and we take our information on price. $500.00
a year minus $5.00 for each unit of discount
266:32 - and then we take the 3d and the $5d and those
will give us the slope. So it's plus 3 divided
266:40 - by minus 5, and that's just minus 0.6. So
that is the slope of the line. Slope is equal
266:47 - to minus 0.6. And so what we have from this
is sales as a function of price where sales
266:54 - is equal to 480 because that is the y intercept
when price is equal to zero minus 0.6 times
267:04 - price. So, this isn't the final thing. Now
what we have to do, we turn this into revenue,
267:10 - there's another stage to this. Revenue is
equal to sales times price, how many things
267:16 - did you sell and how much did it cost. Well,
we can substitute some information in here.
267:22 - If we take sales and we put it in as a function
of price, because we just calculated that
267:26 - a moment ago, then we do a little bit of multiplication
and then we get that revenue is equal to 480
267:34 - times the price minus 0.6 times the price.
Okay, that's a lot of stuff going on there.
267:42 - What we're going to do now is we're going
to get the derivative, that's the calculus
267:45 - that we talked about. Well, the derivative
of 480 and the price, where price is sort
267:51 - of the x, the derivative is simply 480 and
the minus 0.6 times price? Well, that's similar
268:00 - to what we did with the curve. And what we
end up with is 0.6 times 2 is equal to 1.2
268:08 - times the price. This is the derivative of
the original equation. We can solve that for
268:14 - zero now, and just in case you are wondering.
Why do we solve it for zero? Because that
268:21 - is going to give us the place when y is at
a maximum. Now we had a minus squared so we
268:29 - have to invert the shape. When we are trying
to look for this value right here when it's
268:35 - at the very tippy top of the curve, because
that will indicate maximum revenue. Okay,
268:41 - so what we're going to do is solve for zero.
Let's go back to our equation here. We want
268:48 - to find out when is that equal to zero? Well,
we subtract 480 from each side, there we go
268:56 - and we divide by minus 1.2 on each side. And
this is our price for maximum revenue. So
269:05 - we've been charging $500.00 a week, but this
says we'll have more total income if we charge
269:09 - $400.00 instead. And if you want to find out
how many sales we can get, currently we have
269:16 - 480 and if you want to know what the sales
volume is going to be for that. Well, you
269:22 - take the 480 which is the hypothetical y intercept
when the price is zero, but then we put in
269:29 - our actual price of $400.00, multiply that,
we get 240, do the subtraction and we get
269:36 - 240 total. So, that would be 240 new subscriptions
per week. So let's compare this. Current revenue,
269:45 - is 180 new subscriptions per week at $500.00
per year. And that means our current revenue
269:51 - is $90,000.00 per year, I know it sounds really
good, but we can do better than that. Because
269:59 - the formula for maximum value is 240 times
$400.00, when you multiply those you get $96,000.00.
270:07 - And so the improvement is just a ratio of
those two. $96,000.00 divided by $90,000.00
270:11 - is equal to 1.07. And what that means is a
7% increase and anybody would be thrilled
270:19 - to get a 7% increase in their business simply
by changing the price and increasing the overall
270:25 - revenue. So, let's summarize what we found
here. If you lower the cost by 20%, go from
270:31 - $500.00 year to $400.00 per year, assuming
all of our other information is correct, then
270:38 - you can increase sales by 33%; that's more
than the 20 that you had and that increases
270:43 - total revenue by 7%. And so we can optimize
the price to get the maximum total revenue
270:51 - and it has to do with that little bit of calculus
and the derivative of the function. So in
270:56 - sum, calculus can be used to find the minima
and maxima of functions including prices.
271:02 - It allows for optimization and that in turn
allows you to make better business decisions.
271:10 - Our next topic in "Mathematics and Data Principals",
is something called Big O. And if you are
271:16 - wondering what Big O is all about, it is about
time. Or, you can think of it as how long
271:23 - does it take to do a particular operation.
It's the speed of the operation. If you want
271:28 - to be really precise, the growth rate of a
function; how much more it requires as you
271:35 - add elements is called its Order. That's why
it's called Big O, that's for Order. And Big
271:41 - O gives the rate of how things grow as the
number of elements grows, and what's funny
271:46 - is there can be really surprising differences.
Let me show you how it works with a few different
271:52 - kinds of growth rates or Big O. First off,
there's the ones that I say are sort of one
271:57 - the spot, you can get stuff done right away.
The simplest one is O1, and that is a constant
272:04 - order. That's something that takes the same
amount of time, no matter what. You can send
272:08 - an email out to 10,000 people just hit one
button; it's done. The number of elements,
272:13 - the number of people, the number of operations,
it just takes the same amount of time. Up
272:19 - from that is Logarithmic, where you take the
number of operations, you get the logarithm
272:23 - of that and you can see it's increased, but
really it's only a small increase, it tapers
272:29 - off really quickly. So an example is finding
an item in a sorted rate. Not a big deal.
272:34 - Next, one up from that, now this looks like
a big change, but in the grand scheme, it's
272:40 - not a big change. This is a linear function,
where each operation takes the same unit of
272:46 - time. So if you have 50 operations, you have
50 units of time. If you're storing 50 objects
272:50 - it takes 50 units of space. So, find an item
in an unsorted list it's usually going to
272:56 - be linear time. Then we have the functions
where I say you know, you'd better just pack
273:01 - a lunch because it's going to take a while.
The best example of this is called Log Linear.
273:07 - You take the number of items and you multiply
that number times the log of the items. An
273:12 - example of this is called a fast Fourier transform,
which is used for dealing for instance with
273:17 - sound or anything that sort of is over time.
You can see it takes a lot longer, if you
273:21 - have 30 elements your way up there at the
top of this particular chart at 100 units
273:26 - of time, or 100 units of space or whatever
you want to put it. And it looks like a lot.
273:30 - But really, that's nothing compared to the
next set where I say, you know you're just
273:34 - going to be camping out you may as well go
home. That includes something like the Quadratic.
273:39 - You square the number of elements, you see
how that kind of just shoots straight up.
273:44 - That's Quadratic growth. And so multiplying
two n-digit numbers, if you're multiplying
273:50 - two numbers that have 10 digit numbers it's
going to take you that long, it's going to
273:53 - take a long time. Even more extreme is this
one, this is the exponential, two raised to
274:00 - the power to the number of items you have.
You'll see, by the way, the red line does
274:04 - not even go all the way to the top. That's
because the graphing software that I'm using,
274:09 - doesn't draw it when it goes above my upper
limit there, so it kind of cuts it off. But
274:14 - this is a really demanding kind of thing,
it's for instance finding an exact solution
274:18 - for what's called the Travelling Salesman
Problem, using dynamic programming. That's
274:23 - an example of exponential rate of growth.
And then one more I want to mention which
274:27 - is sort of catastrophic is Factorial. You
take the number of elements and you raise
274:32 - that to the exclamation point Factorial, and
you see that one cuts off very soon because
274:36 - it basically goes straight up. You have any
number of elements of any size, it's going
274:42 - to be hugely demanding. And for instance if
you're familiar with the Travelling Salesman
274:47 - Problem, that's trying to find the solution
through the brute force search, it takes a
274:51 - huge amount of time. And you know before something
like that is done, you're probably going to
274:55 - turn to stone and wish you'd never even started.
The other thing to know about this, is that
275:01 - not only do something's take longer than others,
some of these methods and some functions are
275:05 - more variable than others. So for instance,
if you're working with data that you want
275:10 - to sort, there are different kinds of sort
or sorting methods. So for instance, there
275:15 - is something called an insertion sort. And
when you find this on its best day, it's linear.
275:21 - It's O of n, that's not bad. On the other
hand the average is Quadratic and that's a
275:30 - huge difference between the two. Selection
sorts on the other hand, the best is quadratic
275:35 - and the average is quadratic. It's always
consistent, so it's kind of funny, it takes
275:41 - a long time, but at least you know how long
it's going to take versus the variability
275:45 - of something like an insertion sort. So in
sum, let me say a few things about Big O.
275:51 - #1, You need to know that certain functions
or procedures vary in speed, and the same
275:57 - thing applies to making demands on a computer's
memory or storage space or whatever. They
276:02 - vary in their demands. Also, some are inconsistent.
Some are really efficient sometimes and really
276:07 - slow or difficult the others. Probably the
most important thing here is to be aware of
276:13 - the demands of what you are doing. That you
can't, for instance, run through every single
276:16 - possible solution or you know, your company
will be dead before you get an answer. So
276:23 - be mindful of that so you can use your time
well and get the insight you need, in the
276:27 - time that you need it. A really important
element of the "Mathematics and Data Science"
276:33 - and one of its foundational principles is
Probability. Now, one of the things that Probability
276:39 - comes in intuitively for a lot of people is
something like rolling dice or looking at
276:44 - sports outcomes. And really the fundamental
question of what are the odds of something.
276:48 - That gets at the heart of Probability. Now
let's take a look at some of the basic principles.
276:53 - We've got our friend, Albert Einstein here
to explain things. The Principles of Probability
276:58 - work this way. Probabilities range from zero
to 1, that's like zero percent to one hundred
277:03 - percent chance. When you put P, then in parenthesis
here A, that means the Probability of whatever
277:11 - is in parenthesis. So P(A), means the Probability
of A. and then P(B) is the Probability of
277:19 - B. When you take all of the probabilities
together, you get what is called the probability
277:23 - Space. And that's why we have S and that all
adds up to 1, because you've now covered 100
277:29 - % of the possibilities. Also you can talk
about the compliment. The tilde here is used
277:34 - to say the probability of not A is equal to
1 minus the probability of A, because those
277:41 - have to add up. So, let's take a look at something
also that conditional probabilities, which
277:46 - is really important in statistics. A conditional
probability is the probability that something
277:52 - if something else is true. You write it this
way: the probability of, and that vertical
277:57 - line is called a Pipe and it's read as assuming
that or given that. So you can read this as
278:02 - the probability of A given B, is the probability
of A occurring if B is true. So you can say
278:11 - for instance, what's the probability if something's
orange, what's the probability that it's a
278:15 - caret given this picture. Now, the place that
this comes in really important for a lot of
278:20 - people is the probability of type one and
type two errors in hypothesis testing, which
278:24 - we'll mention at some other point. But I do
want to say something about arithmetic with
278:29 - probabilities because it does not always work
out the way people think it will. Let's start
278:33 - by talking about adding probabilities. Let's
say you have two events A and B, and let's
278:40 - say you want to find the probabilities of
either one of those events. So that's like
278:43 - adding the probabilities of the two events.
Well, it's kind of easy. You take the probability
278:48 - of event A and you add the probability of
event B, however you may have to subtract
278:55 - something, you may have to subtract this little
piece because maybe there are some overlap
279:00 - between the two of them. On the other hand
if A and B are disjoined, meaning they never
279:04 - occur together, then that's equal to zero.
And then you can subtract zero which is just,
279:11 - you get back to the original probabilities.
Let's take a really easy example of this.
279:15 - I've created my super simple sample space
I have 10 shapes. I have 5 squares on top,
279:22 - 5 circles on the bottom and I've got a couple
of red shapes on the right side. Let's say
279:26 - we want to find the probability of a square
or a red shape. So we are adding the probabilities
279:34 - but we have to adjust for the overlap between
the two. Well here's our squares on top. 5
279:39 - out of the 10 are squares and over here on
the right we have two red shapes, two out
279:44 - of 10. Let's go back to our formula here and
let's change a little bit. Change the A and
279:49 - the B to S and R for square and red. Now we
can start this way, let's get the probability
279:55 - that something is a square. Well, we go back
to our probability space and you see we have
280:00 - 5 squares out of 10 shapes total. So we do
5 over 10, that reduces to .5. Okay, next
280:08 - up the probability of something red in our
sample space. Well, we have 10 shapes total,
280:15 - two of them on the far right are red. That's
two over 10, and you do the division get.2.
280:21 - Now, the trick is the overlap between these
two categories, do we have anything that is
280:26 - both square and red, because we don't want
to count that twice we have to subtract it.
280:31 - Let's go back to our sample space and we are
looking for something that is square, there's
280:35 - the squares on top and there's the things
that are red on the side. And you see they
280:39 - overlap and this is our little overlapping
square. So there's one shape that meets both
280:45 - of those, one out of 10. So we come back here,
one out of 10, that reduces to .1 and then
280:51 - we just do the addition and subtraction here.
.5 plus .2 minus .1, gets us .6. And so what
280:58 - that means is, there is a 60% chance of an
object being square or red. And you can look
281:07 - at it right here. We have 6 shapes outlined
now and so that's the visual interpretation
281:13 - that lines up with the mathematical one we
just did. Now let's talk about multiplication
281:19 - for Probabilities. Now the idea here is you
want to get joint probabilities, so the probability
281:25 - of two things occurring together, simultaneously.
And what you need to do here, is you need
281:31 - to multiply the probabilities. And we can
say the probability of A and B, because we
281:35 - are asking about A and B occurring together,
a joint occurrence. And that's equal to the
281:41 - probability of A times the probability of
B, that's easy. But you do have to expand
281:47 - it just a little bit because you can have
the problem of things overlapping a little
281:51 - bit, and so you actually need to expand it
to a conditional probability, the probability
281:58 - of B given A. Again, that's that vertical
pipe there. On the other hand, if A and B
282:03 - are independent and they never co-occur, or
B is no more likely to occur if A happens,
282:10 - then it just reduces to the probability of
B, then you get your slightly simpler equation.
282:14 - But let's go and take a look at our sample
space here. So we've got our 10 shapes, 5
282:20 - of each kind, and then two that are red. And
we are going to look at originally, the probability
282:26 - of something being square or red, now we are
going to look at the probability of it being
282:29 - square and red. Now, I know we can eyeball
this one real easy, but let's run through
282:33 - the math. The first thing we need to do, is
get the ones that are square. There's those
282:39 - 5 on the top and the ones that are red, and
there's those two on the right. In terms of
282:44 - the ones that are both square and red, yes
obviously there's just this one red square
282:49 - at the top right. But let's do the numbers
here. We change our formula to be S and R
282:54 - for square and red, we get the probability
of square. Again that's those 5 out of 10,
283:00 - so we do 5/10, reduce this to .5. And then
we need the probability of red given that
283:06 - it's a square. So, we only need to look at
the squares here. There's the squares, 5 of
283:11 - them, and one of them is red. So that's 1
over 5 . That reduces to .2. You multiply
283:19 - those two numbers; .5 times .2, and what you
get is .10 or 10% chance or 10 percent of
283:26 - our total sample space is red squares. And
you come back and you look at it and you say
283:30 - yeah there's one out of 10. So, that just
confirms what we are able to do intuitively.
283:35 - So, that's our short presentation on probabilities
and in sum what did we get out of that? #1,
283:41 - Probability is not always intuitive. And also
the idea that conditional values can help
283:47 - in a lot of situations, but they may not work
the way you expect them to. And really the
283:51 - arithmetic of Probability can surprise people
so pay attention when you are working with
283:56 - it so you can get a more accurate conclusion
in your own calculations. Let's finish our
284:03 - discussion of "Mathematics and Data Science"
and the basic principles by looking at something
284:09 - called Bayes' theorem. And if you're familiar
with regular probability and influential testing,
284:15 - you can think of Bayes' theorem as the flip
side of the coin. You can also think of it
284:22 - in terms of intersections. So for instance,
standard inferential tests and calculations
284:24 - give you the probability of the data; that's
our d, given the hypothesis. So, if you assume
284:31 - a known hypothesis is true, this will give
you the probability of the data arising by
284:37 - chance. The trick is, most people actually
want the opposite of that. They want the probability
284:41 - of the hypothesis given the data. And unfortunately,
those two things can be very different in
284:46 - many circumstances. On the other hand, there's
a way of dealing with it, Bayes does it and
284:56 - this is our guy right here. Reverend Thomas
Bayes, 18th Century English minister and statistician.
284:59 - He developed a method for getting what he
called posterior probabilities that use as
285:05 - prior probabilities. And test information
or something like base rates, how common something
285:08 - overall to get the posterior or after the
fact Probability. Here's the general recipe
285:15 - to how this works: You start with the probability
of the data given the hypothesis which is
285:22 - what you get from the likelihood of the data.
You also get that from a standard inferential
285:28 - test. To that, you need to add the probability
to the hypothesis or the cause of being true.
285:32 - That's called the prior or the prior probability.
To that you add the D; the probability of
285:40 - the data, that's called the marginal probability.
And then you combine those and in a special
285:47 - way to get the probability of the hypothesis
given the data or the posterior probability.
285:52 - Now, if you want to write it as an equation,
you can write it in words like this; posterior
285:56 - is equal to likelihood times prior divided
by marginal. You can also write it in symbols
286:00 - like this; the probability of H given D, the
probability of the hypothesis given the data,
286:04 - that's the posterior probability. Is equal
to the probability of the data given the hypothesis,
286:09 - that the likelihood, multiplied by the probability
of the hypothesis and divided by probability
286:14 - of the data overall. But this is a lot easier
if we look at a visual version of it. So,
286:17 - let's go this example here. Let's say we have
a square here that represents 100% of all
286:24 - people and we are looking at a medical condition.
And what we are going to say here is that
286:32 - we got this group up here that represents
people who have a disease, so that's a portion
286:40 - of all people. And that what we say, is we
have a test and people with the disease, 90%
286:45 - of them will test positive, so they're marked
in red. Now it does mean over here on the
286:51 - far left people with the disease who test
negative that's 10%. Those are our false negatives.
286:58 - And so if the test catches 90% of the people
who have the disease, that's good right? Well,
287:04 - let's look at it this way. Let me ask y0u
a basic question. "If a person tests positive
287:09 - for a disease, then what is the probability
they really have the disease?" And if you
287:15 - want a hint, I'm going to give you one. It's
not 90%,. Here's how it goes. So this is the
287:24 - information I gave you before and we've got
90% of the people who have the disease; that's
287:30 - a conditional probability, they test positive.
But what about the other people, the people
287:34 - in the big white area below, ‘of all people'.
We need to look at them and if any of them
287:41 - ever test positive, do we ever get false positives
and with any test you are going to get false
287:46 - positives. And so let's say our people without
the disease, 90% of them test negative, the
287:51 - way they should. But of the people who don't
have the disease, 10% of them test positive,
287:55 - those are false positives. And so if you really
want to answer the question, "If you test
288:00 - positive do you have the disease?", here's
what you need. What you need is the number
288:07 - of people with the disease who test positive
divided by all people who test positive. Let's
288:12 - look at it this way. So here's our information.
We've got 29.7% of all people are in this
288:18 - darker red box, those are the people who have
the disease and test positive, alright that's
288:24 - good. Then we have 6.7% of the entire group,
that's the people without the disease who
288:29 - test positive. So we want to do, we want the
probability of the disease what percentage
288:34 - have the disease and test positive and then
divide that by all the people that test positive.
288:37 - And that bottom part is made up of two things.
That's made up of the people who have the
288:47 - disease and test positive, and the people
who don't have the disease and test positive.
288:53 - Now we can take our numbers and start plugging
them in. Those who have the disease and test
288:59 - positive that's 29.7% of the total population
of everybody. We can also put that number
289:04 - right here. That's fine, but we also need
to look at the percentage that do not have
289:11 - the disease and test positive; of the total
population, that's 6.7%. So, we just need
289:16 - to rearrange, we add those two numbers on
the bottom, we get 36.4% and we do a little
289:21 - bit of division. And the number we get is
81.6%, here's what that means. A positive
289:26 - test result still only means a probability
of 81.6% of having the disease. So, the test
289:31 - is advertised at having 90% accuracy, well
if you test positive there's really only a
289:36 - 82% chance you have the disease. Now that's
not really a big difference. But consider
289:41 - this: what if the numbers change? For instance,
what if the probability of the disease changes?
289:46 - Here's what we originally had. Let's move
it around a little bit. Let's make the disease
289:51 - much less common. And so now what we do, we
are going to have 4.5% of all people are people
289:57 - who have the disease and test positive. And
then because there is a larger number of people
290:02 - who don't have the disease, we are going to
have a relatively larger proportion of false
290:06 - positives. Again, compared to the entire population
it's going to be 9.5% of everybody. So we
290:10 - are going to go back to our formula here in
words and start plugging in the numbers. We
290:14 - get 4.5% right there, and right there. And
then we add in our other number, the false
290:20 - positives that's 9.5%. Well, we rearrange
and we start adding things up, that's 14%
290:26 - and when we divide that, we get 32.1%. Here's
what that number means. That means a positive
290:34 - test result; you get a positive test result,
now means you only have a probability of 32.1%
290:41 - of having the disease. That's ? less than
the accuracy of 90%, and in case you can't
290:45 - tell, that's a really big difference. And
that's why Bayes theorem matters, because
290:48 - it answers the questions that people want
and the answer can be dramatically different
290:51 - depending on the base rate of the thing you
are talking about. And so in sum, we can say
290:56 - this. Bayes theorem allows you to answer the
right question, people really want to know;
290:59 - what's the probability that I have the disease.
What's the probability of getting a positive
291:02 - if I have the disease. They want to know whether
they have the disease. And to do this, you
291:10 - need to have prior probabilities, you need
to know how common the disease is, you need
291:16 - to know how many people get positive test
results overall. But, if you can get that
291:23 - information and run them through it can change
your answers and really the emotional significance
291:28 - of what you're dealing with dramatically.
Let's wrap up some of our discussion of "Mathematics
291:34 - and Data Science" and the data principles
and talk about some of the next steps. Things
291:39 - you can do afterwards. Probably the most important
thing is, you may have learned about math
291:43 - a long time ago but now it's a good time to
dig out some of those books and go over some
291:51 - of the principles you've used before. The
idea here is that a little math can go a long
291:55 - way in data science. So, things like Algebra
and things like Calculus and things like Big
292:02 - O and Probability. All of those are important
in data science and its helpful to have at
292:04 - least a working understanding of each. You
don't have to know everything, but you do
292:07 - need to understand the principles of your
procedures that you select when you do your
292:13 - projects. There are two reasons for that very
generally speaking. First, you need to know
292:16 - if a procedure will actually answer your question.
Does it give you the outcome that you need?
292:21 - Will it give you the insight that you need?
Second; really critical, you need to know
292:28 - what to do when things go wrong. Things don't
always work out, numbers don't always add
292:32 - up, you got impossible results or things just
aren't responding. You need to know enough
292:37 - about the procedure and enough about the mathematics
behind it, so you can diagnose the problem,
292:41 - and respond appropriately. And to repeat myself
once again, no matter what you're working
292:44 - on in data science, no matter what tool you're
using, what procedure you're doing, focus
292:49 - on your goal. And in case you can't remember
that, your goal is meaning. Your goal is always
292:54 - meaning. Welcome to "Statistics in Data Science".
I'm Barton Poulson and what we are going to
293:00 - be doing in this course is talking about some
of the ways you can use statistics to see
293:06 - the unseen. To infer what's there, even when
most of it's hidden. Now this shouldn't be
293:11 - surprised. If you remember the data science
Venn Diagram we talked about a while ago,
293:16 - we have math up here at the top right corner,
but if you were to go to the original description
293:22 - of this Venn Diagram, it's full name was math
and stats. And let me just mention something
293:28 - in case it's not completely obvious about
why statistics matters to data science. And
293:30 - the idea is this; counting is easy. It's easy
to say how many times a word appears in a
293:37 - document, it's easy to say how many people
voted for a particular candidate in one part
293:40 - of the country. Counting is easy, but summarizing
and generalizing those things hard. And part
293:45 - of the problem is there's no such thing as
a definitive analysis. All analyses really,
293:50 - depend on the purposes that you're dealing
with. So as an example, let me give you a
293:58 - couple of pairs of words and try to summarize
the difference between them in just two or
294:03 - three words. In a word or two, how is a souffle
different from a quiche, or how is an Aspen
294:09 - different from a Pine tree? Or how is Baseball
different from Cricket? And how are musicals
294:14 - different from opera? It really depends on
who you are talking to, it depends on your
294:20 - goals and it depends on the shared knowledge.
And so, there's not a single definitive answer,
294:28 - and then there's the matter of generalization.
Think about it again, take music. Listen to
294:32 - three concerti by Antonio Vivaldi, and do
you think you can safely and accurately describe
294:36 - all of his music? Now, I actually chose Vivaldi
on purpose because even Igor Stravinsky said
294:44 - you could, he said he didn't write 500 concertos
he wrote the same concerto 500 times. But,
294:52 - take something more real world like politics.
If you talk to 400 registered voters in the
294:57 - US, can you then accurately predict the behavior
of all of the voters? There's about 100 million
295:05 - voters in the US, and that's a matter of generalization.
That's the sort of thing we try to take care
295:09 - of with inferential statistics. Now there
are different methods that you can use in
295:12 - statistics and all of them are described to
give you a map; a description of the data
295:18 - you're working on. There are descriptive statistics,
there are inferential statistics, there's
295:20 - the inferential procedure Hypothesis testing
and there's also estimation and I'll talk
295:22 - about each of those in more depth. There are
a lot of choices that have to be made and
295:31 - some of the things I'm going to discuss in
detail are for instance the choice of Estimators,
295:40 - that's different from estimation. Different
measures of fit. Feature selection, for knowing
295:46 - which variables are the most important in
predicting your outcome. Also common problems
295:51 - that arise when trying to model data and the
principles of model validation. But through
295:56 - this all, the most important thing to remember
is that analysis is functional. It's designed
295:59 - to serve a particular purpose. And there's
a very wonderful quote within the statistics
296:02 - world that says all models are wrong. All
statistical descriptions of reality are wrong,
296:06 - because they are not exact depictions, they
are summaries but some are useful and that's
296:12 - from George Box. And so the question is, you're
not trying to be totally, completely accurate,
296:15 - because in that case you just wouldn't do
an analysis. The real question is, are you
296:22 - better off not doing your analysis than not
doing it? And truthfully, I bet you are. So
296:31 - in sum, we can say three things: #1, you want
to use statistics to both summarize your data
296:40 - and to generalize from one group to another
if you can. On the other hand, there is no
296:44 - "one true answer" with data, you got to be
flexible in terms of what your goals are and
296:47 - the shared knowledge. And no matter what your
doing, the utility of your analysis should
296:49 - guide you in your decisions. The first thing
we want to cover in "Statistics in Data Science"
296:54 - is the principles of exploring data and this
video is just designed to give an exploration
296:58 - overview. So we like to think of it like this,
the intrepid explorers, they're out there
297:03 - exploring and seeing what's in the world.
You can see what's in your data, more specifically
297:08 - you want to see what your dataset is like.
You want to see if your assumptions are right
297:13 - so you can do a valid analysis with your procedure.
Something that may sound very weird, but you
297:20 - want to listen to your data. Something's not
work out, if it's not going the way you want,
297:24 - then you're going to have to pay attention
and exploratory data analysis is going to
297:30 - help you do that. Now, there are two general
approaches to this. First off, there's a graphical
297:35 - exploration, so you use graphs and pictures
and visualizations to explore your data. The
297:37 - reason you want to do this is that graphics
are very dense in information. They're also
297:45 - really good, in fact the best to get the overall
impression of your data. Second to that, there
297:50 - is numerical exploration. I make it very clear,
this is the second step. Do the visualization
297:54 - first, then do the numerical part. Now you
want to do this, because this can give greater
297:59 - precision, this is also an opportunity to
try variations on the data. You can actually
298:03 - do some transformations, move things around
a little bit and try different methods and
298:07 - see how that effects the results, see how
it looks. So, let's go first to the graphical
298:11 - part. They are very quick and simple plots
that you can do. Those include things like
298:15 - bar charts, histograms and scatterplots, very
easy to make and a very quick way to getting
298:19 - to understand the variables in your dataset.
In terms of numerical analysis; again after
298:21 - the graphical method, you can do things like
transform the data, that is take like the
298:26 - logarithm of your numbers. You can do Empirical
estimates of population numbers, and you can
298:33 - use robust methods. And I'll talk about all
of those at length in later videos. But for
298:39 - right now, I can sum it up this way. The purpose
of exploration is to help you get to know
298:45 - your data. And also you want to explore your
data thoroughly before you start modelling,
298:49 - before you build statistical models. And all
the way through you want to make sure you
298:55 - listen carefully so that you can find hidden
or unassumed details and leads in your data.
299:02 - As we move in our discussion of "Statistics
and Exploring Data", the single most important
299:07 - thing we can do is Exploratory Graphics. In
the words of the late great Yankees catcher
299:14 - Yogi Berra, "You can see a lot by just looking".
And that applies to data as much as it applies
299:22 - to baseball. Now, there's a few reasons you
want to start with graphics. #1, is to actually
299:29 - get a feel for the data. I mean, what's it
distributed like, what's the shape, are there
299:37 - strange things going on. Also it allows you
to check the assumptions and see how well
299:42 - your data match the requirements of the analytical
procedures you hope to use. You can check
299:48 - for anomalies like outliers and unusual distributions
and errors and also you can get suggestions.
299:53 - If something unusual is happening in the data,
that might be a clue that you need to pursue
299:58 - a different angle or do a deeper analysis.
Now we want to do graphics first for a couple
300:03 - of reasons. #1, is they are very information
dense, and fundamentally humans are visual.
300:07 - It's our single, highest bandwidth way of
getting information. It's also the best way
300:10 - to check for shape and gaps and outliers.
There's a few ways that you can do this if
300:15 - you want to and the first is with programs
that rely on code. So you can use the statistical
300:22 - programming language R, the general purpose
language Python. You can actually do a huge
300:27 - amount in JavaScript, especially D3JS. Or
you can use Apps, that are specifically designed
300:29 - for exploratory analysis, that includes Tableau
both the desktop and public versions, Qlik
300:32 - and even Excel is a good way to do this. And
finally you can do this by hand. John Tukey
300:36 - who's the father of Exploratory Data Analysis,
wrote his seminal book, a wonderful book where
300:39 - it's all hand graphics and actually it's a
wonderful way to do it. But let's start the
300:44 - process for doing these graphics. We start
with one variable. That is univariate distributions.
300:49 - And so you'll get something like this, the
fundamental chart is the bar chart. This is
300:53 - when you are dealing with categories and you
are simply counting however many cases there
300:58 - are in each category. The nice thing about
bar charts is they are really easy to read.
301:03 - Put them in descending order and may be have
them vertical, maybe have them horizontal.
301:08 - Horizontal could be nice to make the labels
a little easier to read. This is about psychological
301:15 - profiles of the United States, this is real
data. We have most states in the friendly
301:17 - and conventional, a smaller amount in the
temperamental and uninhibited and the least
301:21 - common of the United States is relaxed and
creative. Next you can do a Box plot, or sometimes
301:27 - called a box and whiskers plot. This is when
you have a quantitative variable, something
301:29 - that's measured and you can say how far apart
scores are. A box plot shows quartile values,
301:34 - it also shows outliers. So for instance this
is google searches for modern dance. That's
301:39 - Utah at 5 standard deviations above the national
average. That's where I'm from and I'm glad
301:45 - to see that there. Also, it's a nice way to
show many variables side by side, if they
301:50 - are on proximately similar scales. Next, if
you have quantitative variables, you are going
301:54 - to want to do a histogram. Again, quantitative
so interval or ratio level, or measured variables.
301:58 - And these let you see the shape of a distribution
and potentially compare many. So, here are
302:03 - three histograms of google searches on Data
Science, and Entrepreneur and Modern Dance.
302:07 - And you can see, mostly for the part normally
distributed with a couple of outliers. Once
302:12 - you've done one variable, or the univariate
analyses, you're going to want to do two variables
302:18 - at a time. That is bivariate distributions
or joint distributions. Now, one easy way
302:25 - to do this is with grouped plots. You can
do grouped bar charts and box plots. What
302:30 - I have here is grouped box plots. I have my
three regions, Psychological Regions of the
302:38 - United States and I'm showing how they rank
on openness that's a psychological characteristic.
302:41 - As you can see, the relaxed and creative are
high and the friendly conventional tend to
302:45 - go to the lowest and that's kind of how that
works. It's also a good way of seeing the
302:47 - association between a categorical variable
like region of the United States psychologically,
302:49 - and a quantitative outcome, which is what
we have here with openness. Next, you can
302:53 - also do a Scatterplot. That's where you have
quantitative variables and what you're looking
302:58 - for here is, is it a straight line? Is it
linear? Do we have outliers? And also the
303:04 - strength of association. How closely do the
dots all come to the regression line that
303:10 - we have here in the middle. And this is an
interesting one for me because we have openness
303:19 - across the bottom, so more open as you go
to the right and agreeableness. And what you
303:26 - can see is there is a strong downhill association.
The states and the states that are the most
303:33 - open are also the least agreeable, so we're
going to have to do something about that.
303:40 - And then finally, you're going to want to
go to many variables, that is multivariate
303:45 - distributions. Now, one big question here
is 3D or not 3D? Let me make an argument for
303:50 - not 3D. So, what I have here is a 3D Scatterplot
about 3 variables from Google searches. Up
303:55 - the left, I have FIFA which is for professional
soccer. Down there on the bottom left, I have
304:00 - searches for the NFL and on the right I have
searches for NBA. Now, I did this in R and
304:08 - what's neat about this is you can click and
drag and move it around. And you know that's
304:15 - kind of fun, you kind of spin around and it
gets kind of nauseating as you look at it.
304:21 - And this particular version, I'm using plotly
in R, allows you to actually click on a point
304:25 - and see, let me see if I can get the floor
in the right place. You can click on a point
304:33 - and see where it ranks on each of these characteristics.
You can see however, this thing is hard to
304:39 - control and once it stops moving, it's not
much fun and truthfully most 3D plots I've
304:44 - worked with are just kind of nightmares. They
seem like they're a good idea, but not really.
304:52 - So, here's the deal. 3D graphics, like the
one I just showed you, because they are actually
304:58 - being shown in 2D, they have to be in motion
for you to tell what is going on at all. And
305:05 - fundamentally they are hard to read and confusing.
Now it's true, they might be useful for finding
305:10 - clusters in 3 dimensions, we didn't see that
in the data we had, but generally I just avoid
305:16 - them like the plague. What you do want to
do however, is see the connection between
305:20 - the variables, you might want to use a matrix
of plots. This is where you have for instance
305:26 - many quantitative variables, you can use markers
for group membership if you want, and I find
305:31 - it to be much clearer than 3D. So here, I
have the relationship between 4 search terms:
305:37 - NBA, NFL, MLB for Major League Baseball and
FIFA. You can see the individual distributions,
305:43 - you can see the scatterplots, you can get
the correlation. Truthfully for me this is
305:47 - a much easier chart to read and you can get
the richness that we need, from a multidimensional
305:54 - display. So the questions you're trying to
answer overall are: Number 1, Do you have
305:59 - what you need? Do you have the variables that
you need, do you have the ability that you
306:06 - need? Are there clumps or gaps in the distributions?
Are there exceptional cases/anomalies that
306:10 - are really far out from everybody else, spikes
in the scores? And of course are there errors
306:15 - in the data? Are there mistakes in coding,
did people forget to answer questions? Are
306:19 - there impossible combinations? And these kinds
of things are easiest to see with a visualization
306:23 - that really kind of puts it there in front
of you. And so in sum, I can say this about
306:30 - graphical exploration of data. It's a critical
first step, it's basically where you always
306:33 - want to start. And you want to use the quick
and easy methods, again. Bar charts, scatter
306:37 - plots are really easy to make and they're
very easy to understand. And once you're done
306:41 - with the graphical exploration, then you can
go to the second step, which is exploring
306:44 - the data through numbers. The next step in
"Statistics and Exploring Data" is exploratory
306:49 - statistics or numerical exploration of data.
I like to think of this, as go in order. First,
306:55 - you do visualization, then you do the numerical
part. And a couple of things to remember here.
307:01 - #1, you are still exploring the data. You're
not modeling yet, but you are doing a quantitative
307:06 - exploration. This might be an opportunity
to get empirical estimates, that is of population
307:11 - parameters as opposed to theoretically based
ones. It's a good time to manipulate the data
307:15 - and explore the effect of manipulating the
data, looking at subgroups, looking at transforming
307:18 - variables. Also, it's an opportunity to check
the sensitivity of your results. Do you get
307:22 - the same general results if you test under
different circumstances. So we are going to
307:26 - talk about things like Robust Statistics,
resampling data and transforming data. So,
307:30 - we'll start with Robust Statistics. This by
the way is Hercules, a Robust mythical character.
307:35 - And the idea with robust statistics is that
they are stable, is that even when the data
307:39 - varies in unpredictable ways you still get
the same general impression. This is a class
307:43 - of statistics, it's an entire category, that's
less affected by outliers, and skewness, kurtosis
307:48 - and other abnormalities in the data. So let's
take a quick look. This is a very skewed distribution
307:54 - that I created. The median, which is the dark
line in the box, is right around one. And
308:00 - I am going to look at two different kinds
of robust statistics, The Trimmed Mean and
308:07 - the Winsorized Mean. With the Trimmed mean,
you take a certain percentage of data from
308:11 - the top and the bottom and you just throw
it away and compute for the rest. With the
308:15 - Winsorized, you take those and you move those
scores into the highest non-outlier score.
308:18 - Now the 0% is exactly the same as the regular
mean and here it's 1.24, but as we trim off
308:24 - or move in 5%, the mean shifts a little bit.
Then 10 % it comes in a little bit more to
308:29 - 25%, now we are throwing away 50% of our data.
25% on the top and 25% on the bottom. And
308:36 - we get a trimmed mean of 1.03 and a winsorized
of 1.07. When we throw away 50% or we trim
308:42 - 50%, that actually means we are leaving just
the median, only the middle scores left. Then
308:47 - we get 1.01. What's interesting is how close
we get to that, even when we have 50% of the
308:51 - data left, and so that's an interesting example
of how you can use robust statistics to explore
308:55 - data, even when you have things like strong
skewness. Next is the principle of resampling.
308:59 - And that's like pulling marbles repeatedly
from the jar, counting the colors, putting
309:02 - them back in and trying again. That's an empirical
estimate of sampling variability. So, sometimes
309:06 - you get 20% red marbles, sometimes you get
30, sometimes you get 22 and so on. There
309:10 - are several versions for this, they go by
the name jackknife, the bootstrap the permutation.
309:16 - And the basic principle of resampling is also
key to the process of cross-validation, I'll
309:21 - have more to say about validation later. And
then finally there's transforming variables.
309:24 - Here's our caterpillars in the process of
transforming into butterflies. But the idea
309:28 - here, is that you take a difficult data set
and then you do what's called a smooth function.
309:34 - There's no jumps in it, and something that
allows you to preserve the order and work
309:40 - on the full dataset. So you can fix skewed
data, and in a scatter plot you might have
309:45 - a curved line, you can fix that. And probably
the best way to look at this is probably with
309:52 - something called Tukey's ladder of powers.
I mentioned before John Tukey, the father
309:57 - of exploratory data analysis. He talked a
lot about data transformations. This is his
310:01 - ladder, starting at the bottom with the -1,
over x2, up to the top with x3. Here's how
310:06 - it works, this distribution over here is a
symmetrical normally distributed variable,
310:10 - and as you start to move in one direction
and you apply the transformation, take the
310:13 - square root you see how it moves the distribution
over to one end. Then the logarithm, then
310:15 - you get to the end then you get to this minus
1 over the square of the score. And that pushes
310:24 - it way way, way over. If you go the other
direction, for instance you square the score,
310:30 - it pushes it down in the one direction and
then you cube it and then you see how it can
310:35 - move it around in ways that allow you to,
you can actually undo the skewness to get
310:42 - back to a more centrally distributed distribution.
And so these are some of the approaches that
310:47 - you can use in the numerical distribution
of data. In sum, let's say this: statistical
310:52 - or numerical exploration allows you to get
multiple perspectives on your data. It also
310:55 - allows you to check the stability, see how
it works with outliers, and skewness and mixed
310:59 - distributions and so on. And perhaps most
important it sets the stage for the statistical
311:04 - modelling of your data. As a final step of
"Statistics and Exploring Data", I'm going
311:08 - to talk about something that's not usually
exploring data but it is basic descriptive
311:14 - statistics. I like to think of it this way.
You've got some data, and you are trying to
311:22 - tell a story. More specifically, you're trying
to tell your data's story. And with descriptive
311:28 - statistics, you can think of it as trying
to use a little data to stand in for a lot
311:36 - of data. Using a few numbers to stand in for
a large collection of numbers. And this is
311:40 - consistent with the advice we get from good
ole Henry David Thoreau, who told us Simplify,
311:46 - Simplify. If you can tell your story with
more carefully chosen and more informative
311:52 - data, go for it. So there's a few different
procedures for doing this. #1, you'll want
311:56 - to describe the center of your distribution
of data, that is if you're going to choose
312:01 - a single number, use that. # 2, if you can
give a second number give something about
312:06 - the spread or the dispersion of the variability.
And #3, give something about the shape of
312:10 - the distribution. Let me say more about each
of these in turn. First, let's talk about
312:16 - center. We have the center of our rings here.
Now there are a few very common measure of
312:20 - center or location or central tendency of
a distribution. There's the mode, the median
312:25 - and there's the mean. Now, there are many,
many others but those are the ones that are
312:31 - going to get you most of the way. Let's talk
about the mode first. Now, I'm going to create
312:36 - a little dataset here on a scale from 1 to
11, and I'm going to put individual scores.
312:39 - There's a one, and another one, and another
one and another one. Then we have a two, two,
312:43 - then we have a score way over at 9 and another
score over at 11. So we have 8 scores, and
312:48 - this is the distribution. This is actually
a histogram of the dataset. The mode is the
312:50 - most commonly occurring score or the most
frequent score. Well, if you look at how tall
312:55 - each of these go, we have more ones than anything
else, and so one is the mode. Because it occurs
313:01 - 4 times and nothing else comes close to that.
The median is a little different. The median
313:05 - is looking for the score that is at the center
if you split it into two equal groups. We
313:09 - have 8 scores, so we have to get one group
of 4, that's down here, and the other group
313:15 - of four, this really big one because it's
way out and the median is going to be the
313:19 - place on the number line that splits those
into two groups. That's going to be right
313:24 - here at one and a half. Now the mean is going
to be a little more complicated, even though
313:30 - people understand means in general. It's the
first one here that actually has a formula,
313:34 - where M for the mean is equal to the sum of
X (that's our scores on the variable), divided
313:38 - by N (the number of scores). You can also
write it out with Greek notation if you want,
313:46 - like this where that's sigma - a capital sigma
is the summation sign, sum of X divided by
313:52 - N. And with our little dataset, that works
out to this: one plus one plus one plus one
313:58 - plus two plus two plus nine plus eleven. Add
those all up and divide by 8, because that's
314:03 - how many scores there are. Well that reduces
to 28 divided by 8, which is equal to 3.5.
314:10 - If you go back to our little chart here, 3.5
is right over here. You'll notice there aren't
314:19 - any scores really exactly right there. That's
because the mean tends to get very distorted
314:27 - by its outliers, it follows the extreme scores.
But a really nice, I say it's more than just
314:36 - a visual analogy, is that if this number were
a sea saw, then the mean is exactly where
314:43 - the balance point or the fulcrum would be
for these to be equal. People understand that.
314:47 - If somebody weighs more they got to sit in
closer to balance someone who less, who has
314:50 - to sit further out, and that's how the mean
works. Now, let me give a bit of the pros
314:54 - and cons of each of these. Mode is easy to
do, you just count how common it is. On the
314:59 - other hand, it may not be close to what appears
to be the center of the data. The Median it
315:05 - splits the data into two same size groups,
the same number of scores in each and that's
315:11 - pretty easy to deal with but unfortunately,
it's pretty hard to use that information in
315:16 - any statistics after that. And finally the
mean, of these three it's the least intuitive,
315:20 - it's the most effective by outliers and skewness
and that really may strike against it, but
315:25 - it is the most useful statistically and so
it's the one that gets used most often. Next,
315:29 - there's the issue of spread, spread your tail
feathers. And we have a few measures here
315:34 - that are pretty common also. There's the range,
there are percentiles and interquartile range
315:39 - and there's variance and standard deviation.
I'll talk about each of those. First the Range.
315:45 - The Range is simply the maximum score minus
the minimum score, and in our case that's
315:50 - 11 minus 1, which is equal to 10, so we have
a range of 10. I can show you that on our
315:53 - chart. It's just that line on the bottom from
the 11 down to the one. That's a range of
315:57 - 10. The interquartile range which is actually
usually referred to simply as the IQR is the
316:02 - distance between the Q3; which is the third
quartile score and Q1; which is the first
316:09 - quartile score. If you're not familiar with
quartiles, it's the same the 75th percentile
316:13 - score and the 25th percentile score. Really
what it is, is you're going to throw away
316:18 - some of the some of the data. So let's go
to our distribution here. First thing we are
316:21 - going to do, we are going to throw away the
two highest scores, there they are, they're
316:25 - greyed out now, and then we are going to throw
away two of the lowest scores, they're out
316:30 - there. Then we are going to get the range
for the remaining ones. Now, this is complicated
316:34 - by the fact that I have this big gap between
2 and 9, and different methods of calculating
316:38 - quartiles do something with that gap. So if
you use a spreadsheet it's actually going
316:41 - to do an interpolation process and it will
give you a value of 3.75, I believe. And then
316:46 - down to one for the first quartile, so not
so intuitive with this graph but that it is
316:51 - how it works usually. If you want to write
it out, you can do it like this. The interquartile
316:57 - range is equal to Q3 minus Q1, and in our
particular case that's 3.75 minus 1. And that
317:03 - of course is equal to just 2.75 and there
you have it. Now our final measure of spread
317:07 - or variability or dispersion, is two related
measures, the variance and the standard deviation.
317:11 - These are little harder to explain and a little
harder to show. But the variance, which is
317:16 - at least the easiest formula, is this: the
variance is equal to that's the sum, the capital
317:22 - sigma that's the sum, X minus M; that's how
far each score is from the mean and then you
317:27 - take that deviation there and you square it,
you add up all the deviations, and then you
317:31 - divide by the number. So the variance is,
the average square deviation from the mean.
317:35 - I'll try to show you that graphically. So
here's our dataset and there's our mean right
317:39 - there at 3 and a half. Let's go to one of
these twos. We have a deviation there of 1.5
317:45 - and if we make a square, that's 1.5 points
on each side, well there it is. We can do
317:48 - a similar square for the other score too.
If we are going down to one, then it's going
317:53 - to be 2.5 squared and it's going to be that
much bigger, and we can draw one of these
318:00 - squares for each one of our 8 points. The
squares for the scores at 9 and 11 are going
318:05 - to be huge and go off the page, so I'm not
going to show them. But once you have all
318:13 - those squares you add up the area and you
get the variance. So, this is the formula
318:20 - for the variance, but now let me show the
standard deviation which is also a very common
318:26 - measure. It's closely related to this, specifically
it's just the square root of the variance.
318:32 - Now, there's a catch here. The formulas for
the variance and the standard deviation are
318:37 - slightly different for populations and samples
in that they use different denominators. But
318:41 - they give similar answers, not identical but
similar if the sample is reasonably large,
318:46 - say over 30 or 50, then it's really going
to be just a negligible difference. So let's
318:53 - do a little pro and con of these three things.
First, the Range. It's very easy to do, it
319:02 - only uses two numbers the high and the low,
but it's determined entirely by those two
319:06 - numbers. And if they're outliers, then you've
got really a bad situation. The Interquartile
319:09 - Range the IQR, is really good for skewed data
and that's because it ignores extremes on
319:13 - either end, so that's nice. And the variance
and the standard deviation while they are
319:18 - the least intuitive and they are the most
affected by outliers, they are also generally
319:22 - the most useful because they feed into so
many other procedures that are used in data
319:27 - science. Finally, let's talk a little bit
about the shape of the distribution. You can
319:32 - have symmetrical or skew distribution, unimodal,
uniform or u-shaped. You can have outliers,
319:37 - there's a lot of variations. Let me show you
a few of them. First off is a symmetrical
319:43 - distribution, pretty easy. They're the same
on the left and on the right. And this little
319:48 - pyramid shape is an example of a symmetrical
distribution. There are also skewed distributions,
319:55 - where most of the scores are on one end and
they taper off. This here is a positively
320:03 - skewed distribution where most of the scores
are at the low end and the outliers are on
320:08 - the high end. This is unimodal, our same pyramid
shape. Unimodal means it has one mode, really
320:12 - kind of one hump in the data. That's contrasted
for instance to bimodal where you have two
320:17 - modes, and that usually happens when you have
two distributions that got mixed together.
320:18 - There is also uniform distribution where every
response is equally common, there's u-shaped
320:22 - distributions where people tend to pile up
at one end or the other and a big dip in the
320:29 - middle. And so there's a lot of different
variations, and you want to get those, the
320:34 - shape of the distribution to help you understand
and put the numerical summaries like the mean
320:43 - and like the standard deviation and put those
into context. In sum, we can say this: when
320:47 - you use this script of statistics that allows
you to be concise with your data, tell the
320:53 - story and tell it succinctly. You want to
focus on things like the center of the data,
321:00 - the spread of the data, the shape of the data.
And above all, watch out for anomalies, because
321:06 - they can exercise really undue influence on
your interpretations but this will help you
321:10 - better understand your data and prepare you
for the steps to follow. As we discuss "Statistics
321:16 - in Data Science", one of the really big topics
is going to be Inference. And I'll begin that
321:23 - with just a general discussion of inferential
statistics. But, I'd like to begin unusually
321:27 - with a joke, you may have seen this before
it says "There are two kinds of people in
321:31 - the world. 1) Those you can extrapolate from
incomplete data and, the end". Of course,
321:35 - because the other group is the people who
can't. But let's talk about extrapolating
321:39 - from incomplete data or inferring from incomplete
data. First thing you need to know is the
321:44 - difference between populations and samples.
A population represents all of the data, or
321:52 - every possible case in your group of interest.
It might be everybody who's a commercial pilot,
321:58 - it might be whatever. But it represents everybody
in that or every case in that group that you're
322:06 - interested in. And the thing with the population
is, it just is what it is. It has its values,
322:12 - it has it's mean and standard deviation and
you are trying to figure out what those are,
322:22 - because you generally use those in doing your
analyses. On the other hand, samples instead
322:28 - of being all of the data are just some of
the data. And the trick is they are sampled
322:31 - with error. You sample one group and you calculate
the mean. It's not going to be the same if
322:37 - you do it the second time, and it's that variability
that's in sampling that makes Inference a
322:45 - little tricky. Now, also in inference there
are two very general approaches. There's testing
322:51 - which is short for hypothesis testing and
maybe you've had some experience with this.
322:56 - This is where you assume a null hypothesis
of no effect is true. You get your data and
323:02 - you calculate the probability of getting the
sample data that you have if the null hypothesis
323:09 - is true. And if that value is small, usually
less than 5%, then you reject the null hypothesis
323:16 - which says really nothings happen and you
infer that there is a difference in the population.
323:21 - The other most common version is Estimation.
Which for instance is characterizing confidence
323:26 - intervals. That's not the only version of
Estimation but it's the most common. And this
323:36 - is where you sample data to estimate a population
parameter value directly, so you use the sample
323:42 - mean to try to infer what the population mean
is. You have to choose a confidence level,
323:49 - you have to calculate your values and you
get high and low bounds for you estimate that
323:56 - work with a certain level of confidence. Now,
what makes both of these tricky is the basic
324:02 - concept of sampling error. I have a colleague
who demonstrates this with colored M&M's,
324:07 - what percentage are red, and you get them
out of the bags and you count. Now, let's
324:12 - talk about this, a population of numbers.
I'm going to give you just a hypothetical
324:16 - population of the numbers 1 through 10. And
what I am going to do, is I am going to sample
324:25 - from those numbers randomly, with replacement.
That means I pull a number out, it might be
324:28 - a one and I put it back, I might get the one
again. So I'm going to sample with replacement,
324:36 - which actually may sound a little bit weird,
but it's really helpful for the mathematics
324:42 - behind inference. And here are the samples
that I got, I actually did this with software.
324:48 - I got a 3, 1, 5, and 7. Interestingly, that
is almost all odd numbers, almost. My second
324:58 - sample is 4, 4, 3, 6 and 10. So you can see
I got the 4 twice. And I didn't get the 1,
325:09 - the 2, the 5, 7, or 8 or 9. The third sample
I got three 1's! And a 10 and a 9, so we are
325:18 - way at the ends there. And then my fourth
sample, I got a 3, 9, 2, 6, 5. All of these
325:24 - were drawn at random from the exact same population,
but you see that the samples are very different.
325:34 - That's the sampling variability or the sampling
error. And that's what makes inference a little
325:41 - trickier. And let's just say again, why the
sampling variability, why it matters. It's
325:45 - because inferential methods like testing and
like estimation try to see past the random
325:48 - sampling variation to get a clear picture
on the underlying population. So in sum, let's
325:54 - say this about Inferential Statistics. You
sample your data from the larger populations,
326:01 - and as you try to interpret it, you have to
adjust for error and there's a few different
326:05 - ways of doing that. And the most common approaches
are testing or hypothesis testing and estimation
326:11 - of parameter values. The next step in our
discussion of "Statistics and Inference" is
326:15 - Hypothesis Testing. A very common procedure
in some fields of research. I like to think
326:20 - of it as put your money where your mouth is
and test your theory. Here's the Wright brothers
326:28 - out testing their plane. Now the basic idea
behind hypothesis testing is this, and you
326:33 - start out with a question. You start out with
something like this: What is the probability
326:37 - of X occurring by chance, if randomness or
meaningless sampling variation is the only
326:41 - explanation? Well, the response is this, if
the probability of that data arising by chance
326:45 - when nothing's happening is low, then you
reject randomness as a likely explanation.
326:49 - Okay, there's a few things I can say about
this. #1, it's really common in scientific
326:55 - research, say for instance in the social sciences,
it's used all the time. #2, this kind of approach
327:01 - can be really helpful in medical diagnostics,
where you're trying to make a yes/no decision;
327:05 - does a person have a particular disease. And
3, really anytime you're trying to make a
327:11 - go/no go decision, which might be made for
instance with a purchasing decision for a
327:16 - school district or implementing a particular
law, You base it on the data and you have
327:21 - to make a yes/no. Hypothesis testing might
be helpful in those situations. Now, you have
327:26 - to have hypotheses to do hypothesis testing.
You start with H0, which is shorthand for
327:34 - the null hypothesis. And what that is in larger,
what that is in lengthier terms is that there
327:41 - is no systematic effect between groups, there's
no effect between variables and random sampling
327:45 - error is the only explanation for any observed
differences you see. And then contrast that
327:50 - with HA, which is the alternative hypothesis.
And this really just says there is a systematic
327:55 - effect, that there is in fact a correlation
between variables, that there is in fact a
328:00 - difference between two groups, that this variable
does in fact predict the other one. Let's
328:05 - take a look at the simplest version of this
statistically speaking. Now, what I have here
328:09 - is a null distribution. This is a bell curve,
it's actually the standard normal distribution.
328:14 - Which shows z-scores in relative frequency,
and what you do with this is you mark off
328:17 - regions of rejection. And so I've actually
shaded off the highest 2.5% of the distribution
328:22 - and the lowest 2.5%. What's funny about this
is, is that even though I draw it +/- 3, it
328:27 - looks like 0. It's actually infinite and asymptotic.
But, that's the highest and lowest 2.5% collectively
328:31 - leaves 95% in the middle. Now, the idea is
then that you gather your data, you calculate
328:37 - a score for you data and you see where it
falls in this distribution. And I like to
328:46 - think of that as you have to go down one path
to the other, you have to make a decision.
328:51 - And you have to decide to whether to retain
your null hypothesis; maybe it is random,
328:56 - or reject it and decide no I don't think it's
random. The trick is, things can go wrong.
329:06 - You can get a false positive, and this is
when the sample shows some kind of statistical
329:13 - effect, but it's really randomness. And so
for instance, this scatterplot I have here,
329:22 - you can see a little down hill association
here but this is in fact drawn from data that
329:29 - has a true correlation of zero. And I just
kind of randomly sampled from it, it took
329:35 - about 20 rounds, but it looks negative but
really there's nothing happening. The trick
329:41 - about false positives is; that's conditional
on rejecting the null. The only way to get
329:48 - a false positive is if you actually conclude
that there's a positive result. It goes by
329:54 - the highly descriptive name of a Type I error,
but you get to pick a value for it, and .05
329:59 - or a 5% risk if you reject the null hypothesis,
that's the most common value. Then there's
330:03 - a false negative. This is when the data looks
random, but in fact, it's systematic or there's
330:10 - a relationship. So for instance, this scatterplot
it looks like there's pretty much a zero relationship,
330:16 - but in fact this came from two variables that
were correlated at .25, that's a pretty strong
330:21 - association. Again, I randomly sampled from
the data until I got a set that happened to
330:25 - look pretty flat. And a false negative is
conditional on not rejecting the null. You
330:32 - can only get a false negative if you get a
negative, you say there's nothing there. It's
330:38 - also called a Type II error and this is a
value that you have to calculate based on
330:46 - several elements of your testing framework,
so it's something to be thoughtful of. Now,
330:50 - I do have to mention one thing, big security
notice, but wait. The problem with Hypothesis
330:56 - Testing; there's a few. #1, it's really easy
to misinterpret it. A lot of people say, well
331:01 - if you get a statistically significant result,
it means that it's something big and meaningful.
331:06 - And that's not true because it's confounded
with sample size and a lot of other things
331:09 - that don't really matter. Also, a lot of other
people take exception with the assumption
331:12 - of a null effect or even a nil effect, that
there's zero difference at all. And that can
331:17 - be, in certain situations can be an absurd
claim, so you've got to watch out for that.
331:21 - There's also bias from the use of cutoff.
Anytime you have a cut off, you're going to
331:26 - have problems where you have cases that would
have been slightly higher, slightly lower.
331:27 - It would have switched on the dichotomous
outcome, so that is a problem. And then a
331:32 - lot of people say, it just answers the wrong
question, because "What it's telling you is
331:40 - what's the probability of getting this data
at random?" That's not what most people care
331:45 - about. They want it the other way, which is
why I mentioned previously Bayes theorem and
331:49 - I'll say more about that later. That being
said, Hypothesis Testing is still very deeply
331:55 - ingrained, very useful in a lot of questions
and has gotten us really far in a lot of domains.
332:05 - So in sum, let me say this. Hypothesis Testing
is very common for yes/no outcomes and is
332:07 - the default in many fields. And I argue it
is still useful and information despite many
332:15 - of the well substantiated critiques. We'll
continue in "Statistics and Inference" by
332:20 - discussing Estimation. Now as opposed to Hypothesis
Testing, Estimation is designed to actually
332:22 - give you a number, give you a value. Not just
a yes/no, go/no go, but give you an estimate
332:29 - for a parameter that you're trying to get.
I like to think of it sort of as a new angle,
332:38 - looking at something from a different way.
And the most common, approach to this is Confidence
332:44 - Intervals. Now, the important thing to remember
is that this is still an Inferential procedure.
332:49 - You're still using sample data and trying
to make conclusions about a larger group or
332:54 - population. The difference here, is instead
of coming up with a yes/no, you'd instead
332:59 - focus on likely values for the population
value. Most versions of Estimation are closely
333:03 - related to Hypothesis Testing, sometimes seen
as the flip side of the coin. And we'll see
333:08 - how that works in later videos. Now, I like
to think of this as an ability to estimate
333:14 - any sample statistic and there's a few different
versions. We have Parametric versions of Estimation
333:19 - and Bootstrap versions, that's why I got the
boots here. And that's where you just kind
333:23 - of randomly sample from the data, in an effort
to get an idea of the variability. You can
333:27 - also have central versus noncentral Confidence
Intervals in the Estimation, but we are not
333:35 - going to deal with those. Now, there are three
general steps to this. First, you need to
333:42 - choose a confidence level. Anywhere from say,
well you can't have a zero, it has to be more
333:45 - than zero and it can't be 100%. Choose something
in between, 95% is the most common. And what
333:52 - it does, is it gives you a range a high and
a low. And the higher your level of confidence
333:57 - the more confident you want to be, the wider
the range is going to be between your high
334:03 - and your low estimates. Now, there's a fundamental
trade off in what' happening here and the
334:08 - trade off between accuracy; which means you're
on target or more specifically that your interval
334:12 - contains the true population value. And the
idea is that leads you to the correct Inference.
334:17 - There's a tradeoff between accuracy and what's
called Precision in this context. And precision
334:22 - means a narrow interval, as a small range
of likely values. And what's important to
334:26 - emphasize is this is independent of accuracy,
you can have one without the other! Or neither
334:30 - or both. In fact, let me show you how this
works. What I have here is a little hypothetical
334:37 - situation, I've got a variable that goes from
10 to 90, and I've drawn a thick black line
334:43 - at 50. If you think of this in terms of percentages
and political polls, it makes a very big difference
334:49 - if you're on the left or the right of 50%.
And then I've drawn a dotted vertical line
334:57 - at 55 to say that that's our theoretical true
population value. And what I have here is
335:01 - a distribution that shows possible values
based on our sample data. And what you get
335:05 - here is it's not accurate, because it's centered
on the wrong thing. It's actually centered
335:10 - on 45 as opposed to 55. And it's not precise,
because it's spread way out from may be 10
335:17 - to almost 80. So, this situation the data
is no help really at all. Now, here's another
335:23 - one. This is accurate because it's centered
on the true value. That's nice, but it's still
335:30 - really spread out and you see that about 40%
of the values are going to be on the other
335:34 - side of 50%; might lead you to reach the wrong
conclusion. That's a problem! Now, here's
335:39 - the nightmare situation. This is when you
have a very very precise estimate, but it's
335:43 - not accurate; it's wrong. And this leads you
to a very false sense of security and understanding
335:52 - of what's going on and you're going to totally
blow it all the time. The ideal situation
335:59 - is this: you have an accurate estimate where
the distribution of sample values is really
336:03 - close to the true population value and it's
precise, it's really tightly knit and you
336:11 - can see that about 95% of it is on the correct
side of 50 and that's good. If you want to
336:20 - see all four of them here at once, we have
the precise two on the bottom, the imprecise
336:25 - ones on the top, the accurate ones on the
right, the inaccurate ones on the left. And
336:31 - so that's a way of comparing it. But, no matter
what you do, you have to interpret confidence
336:39 - interval. Now, the statistically accurate
way that has very little interpretation is
336:43 - this: you would say the 95% confidence interval
for the mean is 5.8 to 7.2. Okay, so that's
336:48 - just kind of taking the output from your computer
and sticking it to sentence form. The Colloquial
336:55 - Interpretation of this goes like this: there
is a 95% chance that the population mean is
337:03 - between 5.8 and 7.2. Well, in most statistical
procedures, specifically frequentist as opposed
337:06 - to bayesian you can't do that. That implies
the population mean shifts, that's not usually
337:10 - how people see it. Instead, a better interpretation
is this; 95% of confidence intervals for randomly
337:16 - selected samples will contain the population
mean. Now, I can show you this really easily,
337:21 - with a little demonstration. This is where
I randomly generated data from a population
337:26 - with a mean of 55 and I got 20 different samples.
And I got the Confidence Interval from each
337:33 - sample and I charted the high and the low.
And the question is, did it include the true
337:39 - population value. And you can see of these
20, 19 included it, some of them barely made
337:46 - it. If you look at sample #1 on the far left;
barely made it. Sample #8, it doesn't look
337:52 - like it made it, sample 20 on the far right,
barely made it on the other end. Only one
337:59 - missed it completely, that sample #2, which
is shown in red on the left. Now, it's not
338:04 - always just one out of twenty, I actually
had to run this simulation about 8 times,
338:10 - because it gave me either zero or 3, or 1
or two, and I had to run it until I got exactly
338:17 - what I was looking for here,. But this is
what you would expect on average. So, let's
338:27 - say a few things about this. There are somethings
that affect the width of a Confidence Interval.
338:31 - The first is the confidence level, or CL.
Higher confidence levels create wider intervals.
338:36 - The more certain you have to be, you're going
to give a bigger range to cover your basis.
338:43 - Second, the Standard Deviation or larger standard
deviations create wider intervals. If the
338:49 - thing that you are studying is inherently
really variable, then of course you're estimate
338:55 - of the range is going to be more variable
as well. And then finally there is the n or
339:03 - the sample size. This one goes the other way.
Larger sample sizes create narrower intervals.
339:09 - The more observations you have, the more precise
and the more reliable things tend to be. I
339:14 - can show you each of these things graphically.
Here we have a bunch of Confidence Intervals,
339:18 - where I am simply changing the confidence
level from .50 at the low left side to .999
339:23 - and as you can see, it gets much bigger as
we increase. Next one is Standard Deviation.
339:29 - As the sample standard deviation increases
from 1 to 16, you can see that the interval
339:34 - gets a lot bigger. And then we have sample
size going from just 2 up to 512; I'm doubling
339:39 - it at each point. And you can see how the
interval gets more and more and more precise
339:49 - as we go through. And so, let's say this to
sum up our discussion of estimation. Confidence
339:53 - Intervals which are the most common version
of Estimation focus on the population parameter.
340:00 - And the variation in the data is explicitly
included in that Estimation. Also, you can
340:09 - argue that they are more informative, because
not only do they tell you whether the population
340:15 - value is likely, but they give you a sense
of the variability of the data itself, and
340:22 - that's one reason why people will argue that
confidence levels should always be included
340:27 - in any statistical analysis. As we continue
our discussion on "Statistics and Data Science",
340:32 - we need to talk about some of the choices
you have to make, some of the tradeoffs and
340:36 - some of the effects that these things have.
We'll begin by talking about Estimators, that
340:41 - is different methods for estimating parameters.
I like to think of it as this, "What kind
340:46 - of measuring stick or standard are you going
to be using?" Now, we'll begin with the most
340:52 - common. This is called OLS, which is actually
short for Ordinary Least Squares. This is
340:57 - a very common approach, it's used in a lot
of statistics and is based on what is called
341:02 - the sum of squared errors, and it's characterized
by an acronym called BLUE, which stands for
341:11 - Best Linear Unbiased Estimator. Let me show
you how that works. Let's take a scatterplot
341:15 - here of an association between two variables.
This is actually the speed of a car and the
341:21 - distance to stop from about the ‘20's I
think. We have a scatterplot and we can draw
341:24 - a straight regression line right through it.
Now, the line I've used is in fact the Best
341:32 - Linear Unbiased Estimate, but the way that
you can tell that is by getting what are called
341:41 - the Residuals. If you take each data point
and draw a perfectly vertical line up or down
341:47 - to the regression line, because the regression
line predicts what the value would be for
341:51 - that value on the X axis. Those are the residuals.
Each of those individual, vertical lines is
341:57 - Residual. You square those and you add them
up and this regression line, the gray angled
342:03 - line here will have the smallest sum of the
squared residuals of any possible straight
342:09 - line you can run through it. Now, another
approach is ML, which stands for Maximum Likelihood.
342:15 - And this is when you choose parameters that
make the observed data most likely. It sounds
342:22 - kind of weird, but I can demonstrate it, and
it's based on a kind of local search. It doesn't
342:29 - always find the best, I like to think of it
here like the person here with a pair of binoculars,
342:35 - looking around them, trying hard to find something,
but you could theoretically miss something.
342:40 - Let me give a very simple example of how this
works. Let's assume that we're trying to find
342:46 - parameters that maximize the likelihood of
this dotted vertical line here at 55, and
342:50 - I've got three possibilities. I've got my
red distribution which is off to the left,
342:54 - blue which is a little more centered and green
which is far to the right. And these are all
343:01 - identical, except they have different means,
and by changing the means, you see there the
343:05 - one that is highest where the dotted line
is the blue one. And so, if the only thing
343:13 - we are doing is changing the mean, and we
are looking at these three distributions,
343:19 - then the blue one is the one that has the
maximum likelihood for this particular parameter.
343:24 - On the other hand, we could give them all
the same meaning right around 50, and vary
343:30 - their standard deviations instead and so they
spread out different amounts. In this case,
343:36 - the red distribution is highest at the dotted
vertical line and so it has the maximum value.
343:41 - Or if you want to, you can vary both the mean
and the standard deviations simultaneously.
343:45 - And here green gets the slight advantage.
Now this is really a caricature of the process
343:53 - because obviously you would just want to center
it on the 55 and be done with it. The question
344:04 - is when you have many variables in your dataset.
Then it's a very complex process of choosing
344:09 - values that can maximize the association between
all of them. But you get a feel for how it
344:17 - works with this. The third approach which
is pretty common is MAP or map for Maximum
344:24 - A Posteriori. This is a Bayesian approach
to parameter estimation, and what it does
344:33 - it adds the prior distribution and then it
goes through sort of an anchoring and adjusting
344:37 - process. What happens, by the way is stronger
prior estimates exert more influence on the
344:43 - estimate and that might mean for example larger
sample or more extreme values. And those have
344:49 - a greater influence on the posterior estimate
of the parameters. Now, what's interesting
344:56 - is that all three of these methods all connect
with each other. Let me show you exactly how
345:02 - they connect. The ordinary least squares,
OLS, this is equivalent to maximum likelihood,
345:04 - when it has normally distributed error terms.
And maximum likelihood, ML is equivalent to
345:09 - Maximum A Posteriori or MAP, with a uniform
prior distribution. You want to put it another
345:15 - way, ordinary least squares or OLS is a special
case of Maximum Likelihood. And then maximum
345:22 - likelihood or ML, is a special case of Maximum
A Posteriori, and just in case you like it,
345:28 - we can put it into set notation. OLS is a
subset of ML is a subset of MAP, and so there
345:34 - are connections between these three methods
of estimating population parameters. Let me
345:38 - just sum it up briefly this way. The standards
that you use OLS, ML, MAP they affect your
345:44 - choices and they determine which parameters
best estimate what's happening in your data.
345:49 - Several methods exist and there's obviously
more than what I showed you right here, but
345:54 - many are closely related and under certain
circumstances they're all identical. And so
345:59 - it comes down to exactly what are your purposes
and what do you think is going to work best
346:01 - with the data that you have to give you the
insight that you need in your own project.
346:08 - The next step we want to consider in our "Statistics
and Data Science", are choices that we have
346:14 - to make. Has to do with Measures of fit or
the correspondence between the data that we
346:20 - have and the model that you create. Now, turns
out there are a lot of different ways to measure
346:24 - this and one big question is how close is
close enough or how can you see the difference
346:30 - between the model and reality. Well, there's
a few really common approaches to this. The
346:32 - first one has what's called R2. That's kind
of the longer name, that's the coefficient
346:36 - of determination. There's a variation; adjusted
R2, which takes into consideration the number
346:40 - of variables. Then there's minus 2LL, which
is based on the likelihood ratio and a couple
346:45 - of variations. The Akaike Information Criterion
or AIC and the Bayesian Information Criterion
346:50 - or BIC. Then there's also Chi-Squared, it's
actually a Greek c, it looks like a x, but
346:56 - it's actually c and it's chi-squared. And
so let's talk about each of these in turn.
347:01 - First off is R2, this is the squared multiple
correlation or the coefficient of determination.
347:11 - And what it does is it compares the variance
of Y, so if you have an outcome variable,
347:16 - it looks like the total variance of that and
compares it to the residuals on Y after you've
347:23 - made your prediction. The scores on squared
range from 0 to 1 and higher is better. The
347:30 - next is -2 Log-likelihood that's the likelihood
ratio or like I just said the -2 log likelihood.
347:36 - And what this does is compares the fit of
nested models, we have a subset then a larger
347:44 - set, than the larger set overall. This approach
is used a lot in logistic regression when
347:51 - you have a binary outcome. And in general,
smaller values are considered better fit.
347:55 - Now, as I mentioned there are some variations
of this. I like to think of variations of
348:01 - chocolate. The -2 log likelihood there's the
Akaike Information Criterion (AIC) and the
348:03 - Bayesian Information Criterion (BIC) and what
both of these do, they adjust for the number
348:08 - of predictors. Because obviously you're going
to have a huge number of predictors, you're
348:11 - going to get a really good fit. But you're
probably going to have what is called overfitting,
348:17 - where your model is tailored to specifically
to the data you currently have and that doesn't
348:20 - generalize well. These both attempt to reduce
the effect of overfitting. Then there's chi-squared
348:21 - again. It's actually a lower case Greek c,
looks like an x and chi-squared is used for
348:22 - examining the deviations between two datasets.
Specifically between the observed dataset
348:23 - and the expected values or the model you create,
we expect this many frequencies in each category.
348:24 - Now, I'll just mention when I go into the
store there's a lot of other choices, but
348:25 - these are some of the most common standards,
particularly the R2. And I just want to say,
348:26 - in sum, there are many different ways to assess
the fit that corresponds between a model and
348:28 - your data. And the choices effect the model,
you know especially are you getting penalized
348:29 - for throwing in too many variables relative
to your number of cases? Are you dealing with
348:31 - a quantitative or binary outcome? Those things
all matter, and so the most important thing
348:32 - as always, my standing advice is keep your
goals in mind and choose a method that seems
348:35 - to fit best with your analytical strategy
and the insight you're trying to get from
348:38 - your data. The "Statistics and Data Science"
offers a lot of different choices. One of
348:39 - the most important is going to be feature
selection, or the choice of variables to include
348:40 - in your model. It's sort of like confronting
this enormous range of information and trying
348:43 - to choose what matters most. Trying to get
the needle out of the haystack. The goal of
348:48 - feature selection is to select the best features
or variables and get rid of uninformative/noisy
348:53 - variables and simplify the statistical model
that you are creating because that helps avoid
348:59 - overfitting or getting a model that works
too well with the current data and works less
349:06 - well with other data. The major problem here
is Multicollinearity, a very long word. That
349:14 - has to do with the relationship between the
predictors and the model. I'm going to show
349:19 - it to you graphically here. Imagine here for
instance, we've got a big circle here to represent
349:24 - the variability in our outcome variable; we're
trying to predict it. And we've got a few
349:28 - predictors. So we've got Predictor # 1 over
here and you see it's got a lot of overlap,
349:34 - that's nice. Then we've got predictor #2 here,
it also has some overlap with the outcome,
349:38 - but it's also overlaps with Predictor 1. And
then finally down here, we've got Predictor
349:44 - 3, which overlaps with both of them. And the
problem rises the overlap between the predictors
349:50 - and the outcome variable. Now, there's a few
ways of dealing with this, some of these are
349:56 - pretty common. So for instance, there's the
practice of looking at probability values
350:01 - and regression equations, there's standardized
coefficients and there's variations on sequential
350:04 - regression. There are also, there's newer
procedures for dealing with the disentanglement
350:07 - of the association between the predictors.
There's something called Commonality analysis,
350:10 - there's Dominance Analysis, and there are
Relative Importance Weights. Of course there
350:11 - are many other choices in both the common
and the newer, but these are just a few that
350:12 - are worth taking a special look at. First,
is P values or probability values. This is
350:15 - the simplest method, because most statistical
packages will calculate probability values
350:19 - for each predictor and they will put little
asterisks next to it. And so what you're doing
350:27 - is you're looking at the p-values; the probabilities
for each predictor or more often the asterisks
350:30 - next to it, which sometimes give it the name
of Star Search. You're just kind of cruising
350:32 - through a large output of data, just looking
for the stars or asterisks. This is fundamentally
350:34 - a problematic approach for a lot of reasons.
The problem here, is your looking individually
350:37 - and it inflates false positives. Say you have
20 variables. Each is entered and tested with
350:40 - an alpha or a false positive of 5%. You end
up with nearly a 65% chance of a least one
350:42 - false positive in there. That's distorted
by sample size, because with a large enough
350:43 - sample anything can become statistically significant.
And so, relying on p-values can be a seriously
350:44 - problematic approach. Slightly better approach
is to use Betas or Standardized regression
350:47 - coefficients and this is where you put all
the variables on the same scale. So, usually
350:48 - standardized from zero and then to either
minus 1/plus 1 or with a standardized deviation
350:49 - of 1. The trick is though, they're still in
the context of each other and you can't really
350:51 - separate them because those coefficients are
only valid when you take that group of predictors
350:54 - as a whole. So, one way to try and get around
that is to do what they call stepwise procedures.
350:55 - Where you look at the variables in sequence,
there's several versions of sequential regression
350:56 - that'll allow you to do that. You can put
the variables into groups or blocks and enter
350:57 - them in blocks and look at how the equation
changes overall. You can examine the change
350:58 - in fit in each step. The problem with a stepwise
procedure like this, is it dramatically increases
350:59 - the risk of overfitting which again is a bad
thing if you want to generalize your data.
351:02 - And so, to deal with this, there is a whole
collection of newer methods, a few of them
351:03 - include commonality analysis, which provides
separate estimates for the unique and shared
351:04 - contributions of each variable. Well, that's
a neat statistical trick but the problem is,
351:05 - it just moves the problem of disentanglement
to the analyst, so you're really not better
351:06 - off then you were as far as I can tell. There's
dominance analysis, which compares every possible
351:08 - subset of Predictors. Again, sounds really
good, but you have the problem known as the
351:09 - combinatorial explosion. If you have 50 variables
that you could use, and there are some that
351:10 - have millions of variables, with 50 variables,
you have over 1 quadrillion possible combinations,
351:11 - you're not going to finish that in your lifetime.
And it's also really hard to get things like
351:12 - standard errors and perform inferential statistics
with this kind of model. Then there's also
351:13 - something that's even more recent than these
others and that's called relative importance
351:14 - weights. And what that does is creates a set
of orthogonal predictors or uncorrelated with
351:15 - each other, basing them off of the originals
and then it predicts the scores and then it
351:16 - can predict the outcome without the multicollinear
because these new predictors are uncorrelated.
351:17 - It then rescales the coefficients back to
the original variables, that's the back-transform.
351:18 - Then from that it assigns relative importance
or a percentage of explanatory power to each
351:19 - predictor variable. Now, despite this very
different approach, it tends to have results
351:20 - that resemble dominance analysis. It's actually
really easy to do with a website, you just
351:21 - plug in your information and it does it for
you. And so that is yet another way of dealing
351:22 - with a problem multicollinearity and trying
to disentangle the contribution of different
351:23 - variables. In sum, let's say this. What you're
trying to do here, is trying to choose the
351:24 - most useful variables to include into your
model. Make it simpler, be parsimonious. Also,
351:25 - reduce the noise and distractions in your
data. And in doing so, you're always going
351:26 - to have to confront the ever present problem
of multicollinearity, or the association between
351:27 - the predictors in your model with several
different ways of dealing with that. The next
351:29 - step in our discussion of "Statistics and
the Choices you have to Make", concerns common
351:30 - problems in modeling. And I like to think
of this is the situation where you're up against
351:32 - the rock and the hard place and this is where
the going gets very hard. Common problems
351:35 - include things like Non-Normality, Non-Linearity,
Multicollinearity and Missing Data. And I'll
351:36 - talk about each of these. Let's begin with
Non-Normality. Most statistical procedures
351:37 - like to deal with nice symmetrical, unimodal
bell curves, they make life really easy. But
351:38 - sometimes you get really skewed distribution
or you get outliers. Skews and outliers, while
351:39 - they happen pretty often, they're a problem
because they distort measures like the mean
351:40 - gets thrown off tremendously when they have
outliers. And they throw off models because
351:41 - they assume the symmetry and the unimodal
nature of a normal distribution. Now, one
351:42 - way of dealing with this as I've mentioned
before is to try transforming the data, taking
351:43 - the logarithm, try something else. But another
problem may be that you have mixed distributions,
351:44 - if you have a bimodal distribution, maybe
what you really have here is two distributions
351:45 - that got mixed together and you may need to
disentangle them through exploring your data
351:48 - a little bit more. Next is Non-Linearity.
The gray line here is the regression line,
351:49 - we like to put straight lines through things
because it makes the description a lot easier.
351:50 - But sometimes the data is curved and this
is you have a perfect curved relationship
351:51 - here, but a straight line doesn't work with
that. Linearity is a very common assumption
351:52 - of many procedures especially regression.
To deal with this, you can try transforming
351:53 - one or both of the variables in the equation
and sometimes that manages to straighten out
351:54 - the relationship between the two of them.
Also, using Polynomials. Things that specifically
351:55 - include curvature like squares and cubed values,
that can help as well. Then there's the issues
351:56 - of multicollinearity, which I've mentioned
previously. This is when you have correlated
351:57 - predictors, or rather the predictors themselves
are associated to each other. The problem
351:58 - is, this can distort the coefficients you
get in the overall model. Some procedures,
351:59 - it turns out are less affected by this than
others, but one overall way of using this
352:00 - might be to simply try and use fewer variables.
If they're really correlated maybe you don't
352:01 - need all of them. And there are empirical
ways to deal with this, but truthfully, it's
352:02 - perfectly legitimate to use your own domain
expertise and your own insight to the problem.
352:03 - To use your theory to choose among the variables
that would be the most informative. Part of
352:04 - the problem we have here, is something called
the Combinatorial Explosion. This is where
352:05 - combinations of variables or categories grow
too fast for analysis. Now, I've mentioned
352:06 - something about this before. If you have 4
variables and each variable has two categories,
352:07 - then you have 16 combinations, fine you can
try things 16 different ways. That's perfectly
352:08 - doable. If you have 20 variables with five
categories; again that's not to unlikely,
352:09 - you have 95 trillion combinations, that's
a whole other ball game, even with your fast
352:10 - computer. A couple of ways of dealing with
this, #1 is with theory. Use your theory and
352:11 - your own understanding of the domain to choose
the variables or categories with the greatest
352:12 - potential to inform. You know what you're
dealing with, rely on that information. Second
352:13 - is, there are data driven approaches. You
can use something called a Markov chain Monte
352:14 - Carlo model to explore the range of possibilities
without having to explore the range of possibilities
352:15 - of each and every single one of your 95 trillion
combinations. Closely related to the combinatorial
352:16 - explosion is the curse of dimensionality.
This is when you have phenomena, you're got
352:17 - things that may only occur in higher dimensions
or variable sets. Things that don't show up
352:18 - until you have these unusual combinations.
That may be true of a lot of how reality works,
352:19 - but the project of analysis is simplification.
And so you've got to try to do one or two
352:20 - different things. You can try to reduce. Mostly
that means reducing the dimensionality of
352:21 - your data. Reduce the number of dimensions
or variables before you analyze. You're actually
352:22 - trying to project the data onto a lower dimensional
space, the same way you try to get a shadow
352:23 - of a 3D object. There's a lot of different
ways to do that. There's also data driven
352:24 - methods. And the same method here, a Markov
chain Monte Carlo model, can be used to explore
352:25 - a wide range of possibilities. Finally, there
is the problem of Missing Data and this is
352:26 - a big problem. Missing data tends to distort
analysis and creates bias if it's a particular
352:27 - group that's missing. And so when you're dealing
with this, what you have to do is actually
352:28 - check for patterns and missingness, you create
new variables that indicates whether or not
352:29 - a variable is missing and then you see if
that is associated with any of your other
352:30 - variables. If there's not strong patterns,
then you can impute missing values. You can
352:31 - put in the mean or the median, you can do
Regression Imputation, something called Multiple
352:32 - Imputation, a lot of different choices. And
those are all technical topics, which we will
352:33 - have to talk about in a more technically oriented
series. But for right now, in terms of the
352:34 - problems that can come up during modeling,
I can summarize it this way. #1, check your
352:35 - assumptions at every step. Make sure that
the data have the distribution that you need,
352:36 - check for the effects of outliers, check for
ambiguity and bias. See if you can interpret
352:37 - what you have and use your analysis, use data
driven methods but also your knowledge of
352:38 - the theory and the meaning of things in your
domain to inform your analysis and find ways
352:39 - of dealing with these problems. As we continue
our discussion of "Statistics and the Choices
352:40 - that are Made", one important consideration
is Model Validation. And the idea here is
352:41 - that as you are doing your analysis, are you
on target? More specifically, the model that
352:42 - you create through regression or whatever
you do, your model fits the sample beautifully,
352:43 - you've optimized it there. But, will it work
well with other data? Fundamentally, this
352:44 - is the question of Generalizability, also
sometimes called Scalability. Because you
352:45 - are trying to apply in other situations, and
you don't want to get too specific or it won't
352:46 - work in other situations. Now, there are a
few general ways of dealing with this and
352:47 - trying to get some sort of generalizability.
#1 is Bayes; a Bayesian approach. Then there's
352:48 - Replication. Then there's something called
Holdout Validation, then there is Cross-Validation.
352:49 - I'll discuss each one of these very briefly
in conceptual terms. The first one is Bayes
352:50 - and the idea here is you want to get what
are called Posterior Probabilities. Most analyses
352:51 - give you the probability value for the data
given; the hypothesis, so you have to start
352:52 - with an assumption about the hypothesis. But
instead, it's possible to flip that around
352:53 - by combining it with special kind of data
to get the probability of the hypothesis given
352:54 - the data. And that is the purpose of Bayes
theorem; which I've talked about elsewhere.
352:55 - Another way of finding out how well things
are going to work is through Replication.
352:56 - That is, do the study again. It's considered
the gold standard in many different fields.
352:57 - The question is whether you need an exact
replication or if a conceptual one that is
352:58 - similar in certain respects. You can argue
for both ways, but one thing you do want to
352:59 - do is when you do a replication then you actually
want to combine the results. And what's interesting
353:00 - is the first study can serve as the Bayesian
prior probability for the second study. So
353:01 - you can actually use meta-analysis or Bayesian
methods for combining the data from the two
353:02 - of them. Then there's hold out validation.
This is where you build your statistical model
353:03 - on one part of the data and you test it on
the other. I like to think of it as the eggs
353:04 - in separate baskets. The trick is that you
need a large sample in order to have enough
353:05 - to do these two steps separately. On the other
hand, it's also used very often in data science
353:06 - competitions, as a way of having a sort of
gold standard for assessing the validity of
353:07 - a model. Finally, I'll mention just one more
and that's Cross-Validation. Where you use
353:08 - the same data for training and for testing
or validating. There's several different versions
353:09 - of it, and the idea is that you're not using
all the data at once, but you're kind of cycling
353:10 - through and weaving the results together.
There's Leave-one-out, where you leave out
353:11 - one case at a time, also called LOO. There's
Leave-p-out, where you leave out a certain
353:12 - number at each point. There's k-fold where
you split the data into say for instance 10
353:13 - groups and you leave out one and you develop
it on the other nine, then you cycle through.
353:14 - And there's repeated random subsampling, where
you use a random process at each point. Any
353:15 - of those can be used to develop the model
on one part of the data and tested on another
353:16 - and then cycle through to see how well it
holds up on different circumstances. And so
353:17 - in sum, I can say this about validation. You
want to make your analysis count by testing
353:18 - how well your model holds up from the data
you developed it on, to other situations.
353:19 - Because that is what you are really trying
to accomplish. This allows you to check the
353:20 - validity of your analysis and your reasoning
and it allows you to build confidence in the
353:21 - utility of your results. To finish up our
discussion of "Statistics and Data Science"
353:22 - and the choices that are involved, I want
to mention something that really isn't a choice,
353:23 - but more an attitude. And that's DIY, that's
Do it yourself. The idea here is, you know
353:24 - really you just need to get started. Remember
data is democratic. It's there for everyone,
353:25 - everybody has data. Everybody works with data
either explicitly or implicitly. Data is democratic,
353:26 - so is Data Science. And really, my overall
message is You can do it! You know, a lot
353:27 - of people think you have to be this cutting
edge, virtual reality sort of thing. And it's
353:28 - true, there's a lot of active development
going on in data science, there's always new
353:29 - stuff. The trick however is, the software
you can use to implement those things often
353:30 - lags. It'll show up first in programs like
R and Python, but as far as it showing up
353:31 - in a point click program that could be years.
What's funny though, is often these cutting
353:32 - edge developments don't really make much of
a difference in the results of the interpretation.
353:33 - They may in certain edge cases, but usually
not a huge difference. So I'm just going to
353:34 - say analyst beware. You don't have to necessarily
do it, it's pretty easy to do them wrong and
353:35 - so you don't have to wait for the cutting
edge. Now, that being said, I do want you
353:36 - to pay attention to what you are doing. A
couple of things I have said repeatedly is
353:37 - "Know your goal". Why are you doing this study?
Why are you analyzing the data, what are you
353:38 - hoping to get out of it? Try to match your
methods to your goal, be goal directed. Focus
353:39 - on the usability; will you get something out
of this that people can actually do something
353:40 - with. Then, as I've mentioned with that Bayesian
thing, don't get confused with probabilities.
353:41 - Remember that priors and posteriors are different
things just so you can interpret things accurately.
353:42 - Now, I want to mention something that's really
important to me personally. And that is, beware
353:43 - the trolls. You will encounter critics, people
who are very vocal and who can be harsh and
353:44 - grumpy and really just intimidating. And they
can really make you feel like you shouldn't
353:45 - do stuff because you're going to do it wrong.
But the important thing to remember is that
353:46 - the critics can be wrong. Yes, you'll make
mistakes, everybody does. You know, I can't
353:47 - tell you how many times I have to write my
code more than once to get it to do what I
353:48 - want it to do. But in analysis, nothing is
completely wasted if you pay close attention.
353:49 - I've mentioned this before, everything signifies.
Or in other words, everything has meaning.
353:50 - The trick is that meaning might not be what
you expected it to be. So you're going to
353:51 - have to listen carefully and I just want to
reemphasize, all data has value. So make sure
353:52 - your listening carefully. In sum, let's say
this: no analysis is perfect. The real questions
353:53 - is not is your analysis perfect, but can you
add value? And I'm sure that you can. And
353:54 - fundamentally, data is democratic. So, I'm
going to finish with one more picture here
353:55 - and that is just jump write in and get started.
You'll be glad you did. To wrap up our course
353:56 - "Statistics and Data Science", I want to give
you a short conclusion and some next steps.
353:57 - Mostly I want to give a little piece of advice
I learned from a professional saxophonist,
353:58 - Kirk Whalum. And he says there's "There's
Always Something To Work On", there's always
353:59 - something you can do to try things differently
to get better. It works when practicing music,
354:00 - it also works when you're dealing with data.
Now, there are additional courses, here at
354:01 - datalabb.cc that you might want to look at.
They are conceptual courses, additional high-level
354:02 - overviews on things like machine learning,
data visualization and other topics. And I
354:03 - encourage you to take a look at those as well,
to round out your general understanding of
354:04 - the field. There are also however, many practical
courses. These are hands on tutorials on these
354:05 - statistical procedures I've covered and you
learn how to do them in R, Python and SPSS
354:06 - and other programs. But whatever you're doing,
keep this other little piece of advice from
354:07 - writers in mind, and that is "Write what you
know". And I'm going to say it this way. Explore
354:08 - and analyze and delve into what you know.
Remember when we talked about data science
354:09 - and the Venn Diagram, we've talked about the
coding and the stats. But don't forget this
354:10 - part on the bottom. Domain expertise is just
as important to good data science as the ability
354:11 - to work with computer coding and the ability
to work with the numbers and quantitative
354:12 - skills. But also, remember this. You don't
have to know everything, your work doesn't
354:13 - have to be perfect. The most important thing
is just get started, you'll be glad you did.
354:14 - Thanks for joining me and good luck!