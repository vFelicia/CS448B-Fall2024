00:03 - hey welcome to neural nets in javascript
00:06 - with brain.js i am super excited to
00:08 - teach you this course the goal is to
00:10 - give you a practical introduction to
00:12 - problem solving with neural networks
00:15 - what you're going to be learning in this
00:16 - course propagation both forward and
00:18 - backward layers neurons training error
00:20 - what feed forward neural networks are
00:22 - what recurrent neural networks are and a
00:23 - whole lot more we're going to build azor
00:25 - gate a counter a basic math network an
00:28 - image recognizer a sentiment analyzer
00:30 - and a children's book creator
00:33 - and how we're going to do it is with 17
00:35 - lectures where we're going to focus on
00:37 - practice over theory what that means is
00:38 - you are going to get your hands dirty
00:40 - but more than that you're going to walk
00:41 - away knowing the ideas behind neural
00:44 - networks there's as well a bunch of
00:45 - interactive challenges along the way
00:47 - and that brings me to
00:49 - our use of scrimba scrumba is a
00:51 - fantastic platform for learning and at
00:54 - any point during the entire lecture you
00:56 - can stop me it won't hurt my feelings
00:58 - you can just
00:59 - introduce brand new script and you can
01:01 - press
01:03 - command plus s if you're on a mac or
01:06 - control plus s if you're on linux or
01:09 - windows and it will execute exactly your
01:12 - code that is super important throughout
01:14 - this course as well i'm going to make
01:15 - regular reference to the console which
01:17 - is directly below here
01:22 - so if you see some numbers go down there
01:24 - like i'll go ahead and test that right
01:25 - now
01:28 - 0.05 just appeared that's super
01:31 - important anytime i start talking about
01:32 - if the neural net was good because it
01:34 - had a low error rate or if the neural
01:36 - net was bad because it had a higher rate
01:38 - just look down there and that will give
01:40 - you a little bit of reference as to what
01:42 - we're doing so let's get started this is
01:44 - going to be awesome
01:49 - this is our very first neural net this
01:51 - is going to be awesome
01:53 - so the first problem that we're going to
01:54 - tackle is called exclusive or and you
01:56 - can do some research on it if you like
01:58 - but more or less this is what happens
02:00 - you have inputs that are the same they
02:02 - result in a zero output when they differ
02:05 - it results in a one
02:07 - there's always two inputs there's always
02:09 - one output
02:10 - so let's take this very simple comment
02:13 - and let's translate it into something
02:15 - that the neural net or rather the
02:17 - javascript can understand let's have a
02:20 - variable we're going to call it
02:23 - training data
02:28 - and there's our very simple
02:30 - variable that represents all of our
02:31 - training data
02:33 - and let's go ahead and import
02:35 - brainjs
02:37 - brain.js is i'm just going to grab a
02:39 - link that
02:40 - imports it from a cdn
02:43 - content delivery network
02:49 - got that
02:50 - and
02:52 - next we want to instantiate a new
02:54 - instance of brain
02:57 - and we do that const
02:59 - i'm going to say net equals new
03:02 - brain dot neural network
03:10 - and down here
03:13 - we're going to say net
03:15 - dot
03:16 - train and we're going to give it our
03:19 - training data
03:20 - and now at line 16 by the time we get
03:23 - there
03:24 - the the net will have understood
03:26 - what our inputs and outputs are
03:29 - and so we can hear console log out
03:33 - net dot run
03:36 - one of our inputs so let's choose the
03:38 - first one
03:45 - got to
03:46 - give it our layers hidden layers three
03:49 - we'll get more into hidden layers later
03:52 - and now we're going to go ahead and hit
03:53 - run
03:55 - and now we have an output
03:58 - that's so awesome now the reason that
04:00 - this number here is not zero
04:03 - is because we're using
04:05 - uh a neural net and it's very hard for
04:07 - them to speak specifically zero and one
04:10 - they can speak close to that so
04:13 - that's exactly what we want is is a
04:15 - number close to zero which is 0.05
04:19 - now here's a challenge for you
04:21 - go ahead and get the next
04:23 - outputs
04:24 - console logged out
04:26 - and just kind of play around with their
04:27 - values and and see how the net operates
04:34 - in our last tutorial we talked about how
04:36 - to build a neural net a very simple one
04:38 - to solve exclusive or and in this one
04:40 - we're going to discuss how it did it so
04:42 - the neural net has different stages
04:44 - you'll notice i use two different
04:45 - methods here the first one is train and
04:47 - the other one's run now in train we do
04:50 - something called forward propagation and
04:51 - back propagation those terms may seem
04:54 - scary at first
04:55 - but they're actually quite simple in
04:57 - fact we're going to reduce their
04:58 - complexity down to something that even a
05:00 - child can understand you'll take a look
05:02 - at my slides here
05:04 - forward propagation and back propagation
05:06 - we have a ball
05:08 - we're going to take a ball and we're
05:09 - going to throw it at a goal now when we
05:11 - do that we're going to make a prediction
05:13 - as to how far
05:15 - the ball needs to go
05:16 - uh how much energy to put behind it the
05:18 - pathway of the ball etc i want you to go
05:21 - ahead and pause the video here
05:23 - and to think about the different steps
05:25 - that happen when you throw a ball at a
05:27 - goal
05:29 - the first step is prediction
05:31 - now in prediction we're going to think
05:33 - about how we're going to throw the ball
05:35 - where it needs to land how much power we
05:36 - need to put behind it
05:38 - that first step
05:40 - and with my ascii art showing off that
05:41 - we did not go far enough with the ball
05:44 - this step
05:45 - is for propagation
05:47 - we ourselves are making a prediction
05:50 - from that prediction we can see how far
05:52 - we were off from the actual goal
05:56 - we can measure that
05:58 - and that that step of measuring
06:00 - that is back propagation
06:03 - now the next thing that we want to do
06:07 - is make a determination as to what we're
06:08 - going to do next
06:11 - that is our second step of back
06:13 - propagation that is our learning step
06:17 - and you know how the story goes we throw
06:19 - the ball again it goes too far we
06:21 - measure that we make another prediction
06:22 - we throw the ball again
06:24 - third time's a charm that illustrates
06:28 - this very first method and everything
06:30 - that goes on inside the net the next
06:32 - stage is running our net
06:34 - now in running our net we no longer have
06:36 - to
06:37 - measure how far we are from the goal we
06:40 - already know
06:42 - and because of that there is no need to
06:44 - back propagate so that step goes away
06:47 - now throughout this entire training of
06:49 - the neural net the net is measuring and
06:52 - that measurement is referred to as error
06:57 - check this out if we go to our net
07:00 - during its training we can actually
07:02 - enable
07:03 - something really interesting
07:05 - we're going to give it a log function
07:10 - we're going to console log
07:13 - error
07:16 - and we're going to have our log period
07:20 - set to 100 intervals
07:23 - check this out
07:26 - now we can actually see the error how
07:28 - far off the net was and you can see for
07:30 - a time some of these errors may go down
07:33 - or may go up but eventually the net
07:35 - catches on and it starts to accelerate
07:39 - its ability to learn
07:41 - until
07:42 - the error rate starts to drop
07:44 - to a ridiculously low number
07:46 - not zero though
07:48 - until training is completed
07:50 - and once training is completed there's
07:52 - no need to continue training as we
07:54 - discussed we can then just forward
07:56 - propagate
08:01 - in our last tutorial we talked about how
08:03 - neural networks learn using forward
08:05 - propagation and back propagation in this
08:07 - tutorial we're going to understand more
08:09 - the structure of the programmatic neural
08:11 - net neural nets are actually quite
08:13 - simple they are composed of a function
08:15 - that receive inputs as an argument and
08:18 - produce outputs if we think of our
08:20 - neural net in this very simplistic way
08:23 - then really we can reduce the complexity
08:25 - of it down to one of the simplest
08:27 - functions that you can write pause the
08:30 - video here and just look at the
08:32 - structure
08:35 - now we're going to talk about
08:36 - how the network
08:38 - initiates if you think about when you
08:40 - were first born
08:42 - likely you didn't know very much over
08:45 - time though you begin to know more and
08:47 - more the neural net begins with a bunch
08:49 - of random values so everything that
08:51 - affects the outputs is just random at
08:53 - first you may ask yourself why the
08:55 - reason is because mathematically we've
08:58 - proven that is an effective way to start
09:01 - off with knowledge the knowledge is very
09:03 - random at first we don't know the idea
09:06 - it's not zero and it's not one it's
09:08 - somewhere in between over time we can
09:10 - shape that random data so that it
09:12 - finally becomes where we store what's
09:15 - going on inside of the neural net
09:17 - each neuron is quite literally
09:20 - math.random
09:21 - go ahead and pause the video here and
09:23 - get comfortable with the idea that the
09:25 - net starts out with random data
09:28 - next i want to talk about activation a
09:30 - really popular and effective activation
09:32 - function that's used nowadays is called
09:34 - relu
09:35 - relu looks kind of like this
09:37 - if we were to put it inside of a
09:39 - function
09:40 - the function would quite literally look
09:42 - like this
09:51 - that is our activation function called
09:53 - relu now activation functions are
09:56 - measured in back propagation
09:58 - using what is called their derivative
10:00 - i'll go ahead and put a link here in our
10:02 - bonus material two i'll go ahead and
10:04 - post some links that take you to where
10:06 - relu and its derivative are used in
10:08 - brain
10:13 - our last tutorial we talked about the
10:15 - structure of a neural net and in this
10:16 - one we're going to be talking about
10:18 - layers take a look at my illustration
10:20 - this is a neural net each circle
10:22 - represents a neuron the arrows represent
10:25 - a bit of math
10:28 - stacked circles
10:30 - are layers
10:32 - and so here we have what are called
10:34 - input layers
10:35 - that's this first one
10:37 - this next layer would be a hidden layer
10:41 - it's composed of two neurons
10:43 - the next is another hidden layer
10:45 - composed of two neurons
10:47 - and the last is called an output layer
10:49 - now in brain
10:51 - the input layers and output layers are
10:54 - configured for you kind of automatically
10:57 - however our hidden layers
10:59 - can be configured by us our first neural
11:02 - net was composed of two input neurons
11:06 - one hidden layer
11:07 - that had three neurons and an output
11:10 - layer that had one neuron just as our
11:13 - illustration has
11:14 - two
11:16 - neurons for the input layer
11:18 - two hidden layers the first having two
11:21 - neurons the second having two neurons
11:24 - and the last one having two neurons
11:26 - what's interesting about hidden layers
11:28 - is that's really where the majority of
11:31 - their storage is if you likened it to a
11:34 - human the hidden layers are where the
11:35 - ideas are you may run into a scenario
11:38 - where your neural net isn't learning
11:39 - i'll go ahead and recreate that here i'm
11:41 - going to change the hidden layers from a
11:43 - single hidden layer with three neurons
11:46 - to that of one and i'm going to log out
11:49 - training data
12:02 - watch what happens
12:05 - we hit 20 000 iterations without fully
12:08 - understanding the problem and how to
12:10 - solve it
12:11 - now we can easily fix that by changing
12:14 - our hidden layers to a single hidden
12:15 - layer
12:16 - of three neurons
12:19 - you can see we're able to train in a
12:20 - short amount of time 4 800 iterations we
12:23 - can as well have more than one hidden
12:25 - layer our illustration
12:27 - has two hidden layers we could mimic
12:29 - this exact configuration two hidden
12:31 - layers followed by two hidden layers and
12:34 - this way
12:37 - note of caution though the more hidden
12:39 - layers that you add the longer it takes
12:41 - for the neural net to train
12:43 - let's try it
12:44 - see we hit 20 000 iterations without
12:46 - actually fully training there is no hard
12:49 - and fast rules when it comes to hidden
12:51 - layers this would be an invitation to
12:53 - experiment something i have seen though
12:55 - is to treat the hidden layers sort of
12:57 - like a funnel so if you had for example
12:59 - 20 inputs you could have 15 hidden
13:02 - layers followed by 10 hidden layers
13:04 - followed by two output layers that's
13:05 - just an example though and many problems
13:08 - take on different configurations
13:10 - switching gears for a moment take a look
13:12 - back at our illustration you remember
13:13 - that we have these arrows and i said
13:15 - these arrows represent a bit of math and
13:17 - a feed forward neural net that math can
13:20 - be described as this
13:23 - we have our input weights times our
13:26 - inputs
13:27 - plus biases activated now this is simple
13:30 - math but the implications of it are huge
13:32 - pause here and just think about how
13:34 - simple that is and let it sink into your
13:36 - brain
13:39 - this tutorial series is really about the
13:40 - practical application of neural nets but
13:43 - if you're curious like me
13:46 - you can take a look here
13:48 - to see
13:50 - how brain uses this math as another
13:52 - bonus take a look at the additional
13:54 - options that are available for brain
13:56 - neural nets can be widely configured to
13:58 - solve many different problems all it
14:00 - takes is experimentation
14:02 - time and enthusiasm
14:08 - up till now we've concentrated on the
14:10 - basics of how a neural net works and
14:12 - we've used arrays for our training data
14:15 - but in this tutorial we're going to be
14:17 - talking about sending different shaped
14:19 - data into a neural net
14:21 - now to illustrate what i mean by that
14:23 - let's take a look at my slides
14:27 - here we have
14:28 - an array now it's a very simple array
14:31 - but it's an array and what makes arrays
14:34 - incredibly useful is we can reference
14:36 - the values
14:38 - by index
14:39 - for example the arrays index of 0 gives
14:42 - us 0.3 the array index of 1 gives us the
14:45 - value 0.1 arrays are useful in neural
14:48 - nets because they represent a collection
14:51 - generally of the same type of values and
14:53 - we can use the index to look up those
14:56 - values
14:57 - if we look at an array beside a neural
15:00 - net
15:03 - we can see each neuron
15:06 - associates meaning with each of the
15:09 - arrays
15:10 - indexes now when dealing with a
15:11 - collection of the same type of value
15:13 - this is perfect
15:15 - but what about when our data isn't in
15:18 - the form of an array
15:20 - what about for example
15:23 - objects it just so happens that brain.js
15:25 - was built for this type of practical use
15:28 - and we are going to build a neural net
15:30 - that uses objects
15:32 - for training to get us started i've
15:34 - included the browser version of brain.js
15:36 - here in the index.html file next let's
15:40 - define what our training data will look
15:42 - like
15:44 - our input
15:46 - will be an object that is going to have
15:49 - the properties red
15:51 - green
15:52 - and blue
15:55 - and our
15:57 - output
16:00 - will have properties
16:03 - light
16:05 - neutral
16:06 - and dark it just so happens i have some
16:08 - training data
16:09 - i'll go ahead and paste it in
16:12 - and you can see our colors
16:15 - and brightnesses
16:16 - what's really useful with brain.js is
16:18 - you don't have to define all the
16:19 - properties you can but you don't have to
16:22 - when they're missing it simply uses a
16:24 - zero in their place so in this first
16:28 - object we see that red is missing well
16:31 - red here will just simply be zero
16:33 - and you can see a similar practice on
16:35 - the brightnesses let's go ahead and turn
16:38 - these two different arrays of objects
16:40 - into our training data
16:42 - so we're going to define const
16:45 - training data equals an array now we're
16:48 - going to iterate over the brightnesses
16:49 - and colors and build up our training
16:51 - data
16:52 - so four
16:53 - we'll say let i equal zero i is less
16:57 - than
16:58 - colors.length
17:00 - i plus plus
17:02 - or you could use a for each or even a
17:04 - map
17:05 - training data
17:06 - dot push
17:08 - we're going to give it an object each
17:10 - one of our training sets is an object
17:12 - and that training set will have an input
17:14 - and that input will be of colors
17:18 - and our output
17:20 - will be from the brightnesses
17:22 - and those indexes are equivalent
17:27 - next we'll define our neural net
17:30 - so const
17:31 - net equals
17:33 - new brain dot neural net work
17:37 - and we're going to give this
17:40 - hidden layers
17:44 - i'll have a single layer with three
17:46 - neurons
17:50 - and next we can basically just train our
17:53 - neural net so net.train
17:57 - and
17:58 - we're going to give it the training data
18:03 - rather than logging out what is
18:05 - happening inside the neural net let's
18:06 - just
18:07 - get our stats what happens at the very
18:10 - end
18:11 - and we'll go ahead and log those
18:15 - to the console now let's see what we get
18:19 - cool 1200 iterations it learned it
18:21 - fairly quickly and let's see actually
18:23 - what the neural net outputs
18:25 - so net dot run
18:28 - and a value of
18:31 - red
18:32 - 0.9 of red and we'll
18:35 - log those values out
18:38 - let's see what we get
18:41 - very cool
18:42 - so you can see in the training set
18:45 - we did not include dark neutral and
18:48 - light in every single one
18:50 - of the brightnesses however brain is
18:53 - smart enough to combine those together
18:55 - and it gives us with red being 0.9
18:59 - that red is dark now
19:02 - as a bonus
19:05 - i could spell bonus correctly
19:08 - there we go what if we had to invert the
19:11 - problem
19:13 - what i mean by that
19:14 - is what if we are for example asking our
19:17 - neural net for colors rather than
19:20 - classifying
19:21 - their brightness in this scenario pause
19:24 - it here and think about how you would
19:25 - accomplish this
19:30 - so by inverting the problem
19:32 - our inputs
19:35 - would then be
19:38 - light
19:40 - neutral and dark
19:43 - and our output would be a color red
19:46 - green
19:48 - or blue for us to flip the values
19:51 - let's define our training data again
19:53 - this will be
19:54 - const inverted
19:57 - training data
20:00 - and we are going to
20:03 - for
20:06 - i equal zero
20:08 - i equals colors
20:11 - dot length
20:15 - we're going to take
20:18 - the inverted training data and push
20:22 - objects to it that are just like prior
20:27 - our input and output
20:30 - but we are going to change
20:33 - the input to accept brightnesses
20:36 - and our output to accept colors
20:39 - so that's our training data
20:41 - next let's define a
20:43 - inverted
20:45 - net
20:46 - those new brain dot neural
20:50 - neural network
20:52 - the same hidden layers
20:59 - and let's train it
21:03 - inverted
21:07 - stats
21:08 - equal
21:10 - inverted net
21:11 - dot train
21:14 - inverted
21:16 - training data
21:19 - we'll go ahead and train on this
21:22 - cool let's log out the stats
21:33 - there's our stats
21:36 - and we can see it actually didn't do a
21:38 - great job at learning the problem but
21:40 - that isn't the point of this exercise
21:42 - it's really just to understand the
21:44 - neural net from a different vantage
21:46 - point when you flip a neural net like
21:48 - this you're kind of generating values
21:50 - that can be really useful in predictions
21:57 - in this tutorial we're going to get a
21:58 - bit more adventurous and push the
22:00 - boundaries of what you can do with a
22:02 - neural net but doing so in a safe and
22:04 - easy manner
22:06 - but to help us understand take a look
22:09 - at my slides in our previous tutorials
22:12 - we used numbers directly in the neural
22:15 - net and numbers are just one of the
22:18 - types that exist in javascript and in
22:20 - other languages now javascript is fairly
22:22 - simple in its types in fact we have only
22:25 - boolean numbers objects strings null and
22:29 - undefined now it would be nice if we
22:31 - could feed
22:32 - these other types of values into the
22:35 - neural net so that it can understand
22:37 - context better and solve different and
22:39 - seemingly more complex problems
22:42 - so
22:44 - are we
22:45 - doomed then
22:47 - do they just speak numbers previously we
22:51 - used an object with a neural net and we
22:53 - did so using its properties defined with
22:56 - numbers but the principle of assigning a
22:58 - value to a neuron will provide us the
23:02 - answer for our neural net to speak more
23:05 - than just numbers
23:06 - now the question in your mind is
23:08 - probably how let me illustrate in a way
23:11 - that a child would understand this is a
23:14 - light switch it is off
23:16 - now if you were to ask a child to turn
23:18 - the light switch on
23:20 - they of course
23:23 - would
23:24 - but it wouldn't just happen like that
23:26 - it would happen more like my son does
23:29 - he sees that it is off
23:32 - and he becomes excited and he's taking
23:35 - gymnastics and so he will use that to
23:37 - his advantage he'll perform some amazing
23:39 - gymnastic maneuver over to the light
23:41 - switch
23:42 - turning it on
23:43 - and seeing that it's on that he just did
23:45 - something very useful he'll let out a
23:47 - cheer and do some gymnastic move off and
23:50 - away
23:52 - celebrating that he was a useful kid and
23:55 - that he has the ability to flip switches
23:59 - this ability to understand
24:01 - both
24:02 - off
24:03 - and on has huge implications now our
24:06 - computers they speak binary
24:08 - that's just ones and zeros so it's very
24:10 - very similar language to the neural net
24:13 - we could use this same practice we could
24:16 - say that zero is off and one is on now
24:20 - we sort of did this previously with
24:23 - objects via their property name but we
24:25 - fed the inputs directly into the neural
24:28 - net in this case we assign a neuron to a
24:32 - specific value that we're training
24:34 - towards either the input or the output
24:37 - and when that input or output neuron
24:39 - fires
24:41 - we basically just assign that value
24:44 - as one otherwise it's zero
24:47 - so just
24:49 - like our
24:50 - on and off
24:52 - we're taking
24:53 - these values like a boolean or null
24:56 - value or a string
24:59 - etc
25:00 - and we're simply assigning it to the
25:03 - input
25:04 - so here my potentially null value
25:07 - is being fired upon so it's
25:10 - a value of 1. my string 1 is as well the
25:13 - other values are not and as for output
25:15 - the net is going to try to learn those
25:17 - values just as before and so really the
25:19 - implications here are that we can send
25:20 - just about any value into a neural net
25:23 - so long as that value is represented by
25:26 - a neuron
25:27 - okay let's go ahead and get coding and
25:28 - then this will all make sense
25:31 - first off in our index.html file i have
25:33 - included the browser version of brain.js
25:36 - next we are going to get some data that
25:39 - we're eventually going to use to train
25:40 - on
25:41 - this initial data is an object with
25:43 - property names of restaurants whose
25:46 - values are what day we can eat free with
25:48 - kids
25:50 - and our objective is to find a way to
25:51 - get these
25:52 - string values
25:54 - represented as ones and zeros into the
25:56 - neural net and what we're going to do is
25:57 - give the neural net a day of the week
26:00 - and it's going to tell us where to go on
26:02 - that day of the week so that we can eat
26:04 - free with our kids
26:06 - now pause it here for a moment and think
26:09 - how you would accomplish this using that
26:12 - light switch analogy
26:17 - next let's go ahead and plan how we're
26:19 - going to input our training data into
26:21 - the neural net so if we are going to use
26:25 - the day of the week
26:26 - as the question we're going to ask our
26:29 - neural net that will be our input so our
26:31 - input is going to be a day of the week
26:33 - so
26:34 - monday
26:35 - tuesday
26:37 - wednesday
26:40 - etc
26:41 - our output
26:43 - is going to be
26:46 - the restaurant name
26:48 - so restaurant
27:00 - so that is our input and output for our
27:03 - neural net now next we're going to go
27:05 - ahead and build up our training data
27:08 - so let's go ahead and build
27:10 - that using a const
27:12 - that's going to be training data that's
27:14 - going to be an array
27:16 - and to
27:18 - put data into the training data from our
27:21 - restaurant so we'll need to iterate over
27:22 - our restaurants so for
27:24 - let
27:25 - grant
27:27 - name
27:28 - in
27:29 - restaurants
27:32 - and the day of the week is the value
27:36 - therein so cost
27:39 - day of week equals
27:42 - restaurants
27:44 - restaurant name
27:46 - all right so we've got our day of the
27:47 - week and now our restaurant
27:50 - and our training data
27:53 - we're going to push a value to it that
27:56 - has our inputs and outputs now our input
27:59 - is going to be
28:01 - this is where the rubber meets the road
28:02 - so to speak
28:04 - it's going to be our day
28:06 - of the week
28:07 - and we're going to assign that a value
28:10 - of 1. now just think about how simple
28:12 - that is if you need to pause it please
28:14 - do so but think about how simple
28:16 - what we just did is
28:18 - we are giving
28:20 - an input
28:22 - to the neural net of a day of the week
28:24 - assigned by
28:25 - its value of one
28:27 - now all the other days of the week
28:30 - because of the way that brain.js is
28:32 - built are going to initially be zero
28:34 - so only this day of the week will be one
28:36 - next let's go ahead and assign our
28:38 - output
28:41 - that's going to be it's restaurant name
28:46 - and we have just built
28:48 - our training data if you need a moment
28:50 - to pause and think about what we've just
28:52 - done
28:52 - please do so but it's a very simple
28:55 - principle
28:56 - the principle of represented values
28:59 - next let's go ahead and define our
29:01 - neural net
29:02 - that'll be const net new
29:05 - brain dot neural network
29:09 - and we're going to give it the same
29:10 - hidden layers as before
29:13 - single hidden layer with three neurons
29:17 - and all that is left to do is train on
29:20 - our training data so we'll do const
29:22 - stats
29:23 - equals net dot net.train
29:27 - training data
29:30 - and then we'll console log our stats out
29:34 - all right are you ready
29:36 - here we go
29:41 - look at that
29:42 - under 2000 iterations the neural net has
29:45 - deciphered
29:46 - what day of the week to eat for free
29:48 - with our kids that's nice but let's look
29:50 - at what comes out of the neural net
29:53 - console.log
29:54 - net.run
29:59 - we're going to say
30:01 - monday
30:06 - see what it says
30:08 - now this is where it gets kind of
30:09 - interesting
30:11 - all the restaurants are included with
30:14 - our result we just have a likelihood
30:16 - associated to each one of those
30:18 - restaurants but really what we want is
30:21 - to put a string in and to get a string
30:24 - from our neural net
30:26 - we're going to do that next
30:28 - if you can pause it just think about how
30:30 - you might be able to do that
30:36 - okay so we're going to create a function
30:39 - and its name is restaurant
30:47 - for day
30:49 - it's gonna
30:50 - get a day of
30:53 - week
30:57 - we're going to use that value with our
31:00 - neural net so net dot run
31:03 - day
31:04 - of the week
31:12 - this is going to be our result
31:20 - and from that result we'll have this
31:22 - collection
31:23 - of restaurants with the likelihood that
31:26 - we should eat there and the highest
31:28 - likelihood will be the correct
31:30 - prediction for the given day
31:32 - so what we're going to do is start out
31:34 - with
31:35 - a
31:37 - highest value
31:41 - and as well the highest
31:51 - and we're going to iterate over our
31:54 - results so for let
31:58 - rest
32:04 - name
32:05 - in
32:06 - result
32:08 - and then we're going to say
32:11 - if
32:12 - the result
32:14 - restaurant
32:16 - name
32:19 - is higher
32:21 - than our highest value
32:25 - we're going to set our highest value
32:27 - to that value
32:31 - name
32:32 - or is we're going to save the highest
32:35 - restaurant name
32:40 - close restaurant
32:44 - and from here we'll just return
32:47 - the highest restaurant name
32:49 - and so from this we're going to accept a
32:51 - day of the week a string we're going to
32:53 - put that into the neural net the neural
32:55 - net's going to give us its predictions
32:57 - those predictions are a list of
32:59 - restaurants
33:00 - we're going to iterate over those
33:02 - restaurants
33:03 - and then we're going to
33:05 - save the highest one
33:07 - and we're going to return the highest
33:09 - one
33:11 - we'll go ahead and log the results out
33:13 - so this is restaurant for day
33:16 - we're going to say monday
33:18 - and we'll test it all out
33:22 - there it is brilliant yellow coral
33:24 - perfect
33:25 - now let's
33:26 - let's add the rest of the days of the
33:28 - week so monday tuesday wednesday
33:30 - thursday friday saturday sunday
33:35 - tuesday
33:37 - [Music]
33:40 - thursday
33:44 - friday
33:52 - there we go
33:56 - we've got
33:58 - all the restaurants for the given days
34:00 - of the week so now we've got string in
34:03 - and string out for our neural net
34:06 - next as a bonus
34:09 - try and flip this logic the other way
34:12 - so that you are inputting a restaurant
34:14 - name and you are getting out a day of
34:17 - the week
34:19 - i'll leave you to it
34:26 - in this tutorial we're going to learn
34:27 - how to count
34:28 - and although that sounds kind of like an
34:30 - easy task at first it actually is a
34:32 - little bit difficult but when we use the
34:35 - right tools it becomes easy
34:38 - take a look at my slides and this will
34:40 - give us a background on where to get
34:42 - started this is exclusive or our first
34:45 - problem that we solved
34:47 - each of the empty squares is a zero and
34:50 - each of the black squares is a one
34:54 - now let's take a look at a different
34:55 - input one that may be a little bit more
34:57 - tricky
34:59 - it's hard to kind of see what this
35:01 - particular input means so let's
35:03 - rearrange it so that it's easier for us
35:06 - humans we can see this is a 4. now both
35:09 - of these inputs illustrate something
35:11 - that may have occurred to you already
35:14 - and that is
35:15 - that they have
35:17 - width and height
35:19 - or
35:20 - length now width and height is just sort
35:22 - of another way of looking at length
35:25 - width and height don't really change
35:27 - neural nets
35:28 - they are
35:29 - constant
35:30 - but in computers there are some rules
35:33 - that can be bent and others that can be
35:36 - broken
35:37 - and we'll start illustrating that now by
35:39 - going to the movies on your trip to the
35:42 - movies you're going to bring your best
35:43 - friend along and they're of course
35:45 - thrilled at going to the movies with you
35:48 - because it's the latest and greatest
35:49 - movie that you've been looking forward
35:51 - to
35:52 - and everything is going fantastic and in
35:54 - fact it's the cliffhanger scene right
35:57 - there at the middle but all of a sudden
35:59 - the screen goes black
36:05 - why why did this happen this the screen
36:08 - went black it was right as i was
36:10 - expecting something to happen next
36:13 - why were they expecting something to
36:15 - happen next
36:17 - well it's because they they built up a
36:19 - memory of what was happening up to that
36:22 - point that's important think about that
36:24 - just for a moment we'll come back to it
36:27 - back to our non-playing movie your best
36:29 - friend looks at you and they are of
36:31 - course thrilled that they're at the
36:32 - movies but are saddened that the movie
36:33 - will not continue to play
36:35 - and then all of a sudden it starts to
36:36 - play again and they're of course as
36:38 - thrilled as can be and the movie ends
36:40 - just as you would hoped
36:42 - so that is our illustration of the
36:44 - movies
36:45 - now if you think back about what we
36:47 - paused in the movie
36:49 - every frame of the movie
36:52 - was the same
36:53 - height and width
36:55 - every frame
36:56 - no frame was different in size if you
36:58 - think about that over time though a
37:01 - frame being one part of a movie one
37:05 - frozen image of the movie
37:08 - each frame
37:10 - has that constant size but there are
37:13 - hardly any movies that are the same
37:14 - duration they all have
37:16 - different times that they play out
37:19 - that
37:21 - duration
37:22 - that's depth that's our frames plural
37:26 - that's mini frames that is really
37:28 - important with neural nets
37:31 - the depth
37:33 - the frames
37:35 - how long the movie is and what happens
37:37 - on each frame and what leads to the next
37:39 - one that gives us a context as to what
37:42 - is happening in the movie
37:45 - it's the same with neural nets
37:48 - this context and a neural net recurs
37:53 - it's something that happens over and
37:54 - over again something that has to in a
37:56 - sense repeat
37:58 - this terminology in neural nets is
38:00 - called recurrent now that sounds like a
38:03 - very complex word recurrent oh no what
38:05 - are we going to do next
38:07 - it's actually quite simple and to
38:08 - illustrate that
38:10 - let's
38:11 - go simple let's let's go to something
38:13 - that even a child can understand
38:16 - one
38:18 - now at its very simplest if i go to a
38:20 - child and i say one likely the child
38:23 - will not understand what i'm talking
38:25 - about unless we've trained on that in
38:27 - previous sessions
38:29 - so one to a
38:31 - to a child is is foreign and it's the
38:33 - same with a neural net i don't know what
38:35 - you mean by one that's essentially what
38:37 - the neural net will say however as soon
38:39 - as we start giving it context it being a
38:42 - neural net or it being a child
38:44 - they can start to decipher what we are
38:46 - trying to ask from them
38:49 - i'll continue
38:50 - we talk to our child we say one
38:54 - two
38:54 - [Music]
38:55 - three
38:57 - four and we
38:58 - pause what do you think the child will
39:01 - reply with
39:04 - likely a response would be
39:06 - five
39:09 - now this is
39:10 - where recurrent happens with neural nets
39:13 - recurrence is like taking each of these
39:15 - states one two three and four and sort
39:18 - of putting them together in a sense
39:21 - adding them together into a kind of a
39:23 - pool
39:24 - and that poll says you know the most
39:26 - likely thing they're looking for is
39:28 - probably and then out comes a five that
39:31 - is depth it's the same as our movie it's
39:34 - the same as being able to see each frame
39:37 - and that depth happens over time
39:40 - if the movie played out of sequence if
39:42 - the frames were shuffled in a sense
39:45 - likely would get very little out of the
39:46 - movie now this context is sort of a
39:49 - observer
39:50 - that that looks at each of these steps
39:53 - and can guess what comes next
39:57 - that context that depth that time that
40:01 - recurrence
40:02 - all refer to a very similar concept
40:07 - that depth
40:09 - that time that context it's all dynamic
40:12 - they're all the same width and height or
40:14 - even the same length
40:16 - but they're not of the same depth
40:18 - because we can feed in more than one
40:21 - and the context is dynamic two in the
40:23 - sense that we can say one two and ask
40:26 - for what's next and it'll say three or
40:28 - we can say three four what's next and it
40:31 - will give us a five or we can even
40:33 - reverse it and say five four three two
40:36 - what's next and it'll give us a one
40:39 - that's how dynamic that recurrent
40:42 - concept is in our neural net the ability
40:45 - to sort of take in those multiple frames
40:48 - that's called a recurrent neural net and
40:52 - in this simplest form the feeding of for
40:55 - example numbers is like stepping through
40:58 - time or a time
41:00 - step and as i said in this tutorial
41:02 - we're going to learn how to
41:04 - count okay now to get started let's go
41:07 - ahead and include the browser version
41:11 - of
41:12 - brain.js
41:14 - so we'll add that here
41:19 - our training data it's going to have two
41:20 - different outcomes one is going to count
41:22 - from one to five and the next one is
41:24 - going to count from five
41:26 - to one
41:28 - we'll define our training data manually
41:31 - that'll be a const
41:33 - called training
41:34 - data
41:36 - and it is going to be an array
41:38 - and in that array we'll have two arrays
41:43 - the first one will be
41:45 - one two three four and five and the next
41:48 - one will be five four three two
41:50 - and one
41:53 - that's it that's our entire training
41:55 - data
41:57 - next we'll define our neural net
41:59 - const net
42:01 - equals
42:02 - now this is a new namespace in brain new
42:06 - brain dot recurrent
42:09 - dot long short term memory or lstm
42:14 - time
42:15 - step
42:18 - now to train the neural net we'll give
42:20 - it our training data
42:22 - using the train method
42:24 - net dot train
42:27 - training data
42:30 - and let's see what actually comes out of
42:33 - the neural net while we're training it
42:36 - by logging
42:38 - we're going to give it a log function
42:48 - let's go ahead and see what happens
42:52 - all right it trained really fast
42:54 - very cool
42:55 - but let's
42:56 - remove the logging now that we know that
42:58 - it can train and let's see actually what
43:00 - comes out of the neural net
43:03 - so
43:03 - console.log
43:05 - net dot run
43:07 - and we're going to give it part of one
43:09 - of the arrays that we defined in the
43:11 - beginning so one two three four
43:13 - and that's it for the first one
43:16 - and we'll do the same for the second one
43:18 - right because we wanna as well count
43:20 - down from five net dot run
43:23 - and we'll do five four three and two
43:26 - all right let's see what comes out
43:30 - there we go awesome we got exactly what
43:32 - we wanted and that is how you count
43:35 - using an element in our first run we
43:37 - gave it an array of one two three and
43:41 - four expecting a five and that's what we
43:44 - got four point nine eight
43:46 - and in the second one we sent in a five
43:49 - four three two
43:51 - and we're expecting a one just like we
43:54 - have up here in our training data and we
43:56 - got a 1.005
43:59 - so that's really exactly what we wanted
44:01 - now as a bonus
44:02 - try adding another training array
44:05 - that counts from 10 to 5.
44:08 - or even from 5 to 10.
44:16 - in our last tutorial we used a long
44:19 - short term memory time step neural
44:22 - network to count that's a mouthful and
44:25 - in this tutorial we're going to use that
44:27 - same sort of net and we're going to work
44:30 - up to predicting stock market data so
44:32 - let's take a look at how our data is
44:34 - shaped first
44:35 - our raw data is going to be where all of
44:38 - our values live now this isn't yet
44:40 - training data we're going to turn it
44:42 - into that our values are going to be
44:44 - from an object and that object is going
44:46 - to have properties open high
44:49 - low and close each one of those are
44:52 - numbers now let's take a look at our raw
44:54 - data for a moment now if you look for
44:56 - example at the open property you'll see
44:58 - it's quite a bit larger than numbers
44:59 - that we've used previously which were
45:01 - from 0 to 1 or
45:03 - 0 to 10. the values repeat and you'll
45:06 - see that open high low and close follow
45:09 - a similar pattern what we want to do
45:11 - though because the neural net was
45:13 - instantiated with values between 0 and 1
45:16 - and that is sort of the language that
45:17 - the neural net speaks if we just send
45:19 - these values into the neural net it's
45:21 - going to take it quite a bit of time to
45:23 - sort of ramp up its understanding of
45:26 - these larger numbers if you can imagine
45:28 - it's like walking up to somebody who has
45:30 - only ever heard whispers and then just
45:33 - yelling at them it would be first off
45:35 - rude
45:36 - and secondly it would just be loud it
45:38 - wouldn't be
45:40 - what they were used to what we want to
45:41 - do is make it easy for the neural net to
45:43 - interpret this data and this is a very
45:45 - common practice so let's start out by
45:47 - writing a function and this function is
45:49 - going to normalize our data but since
45:51 - normalize is the most friendly term
45:53 - let's just call it scale
45:55 - down
45:58 - that's our function name and scale down
46:01 - is going to accept a single object and
46:04 - we're going to call that a step
46:07 - so that's a step in time now the same
46:09 - object that we have coming in we just
46:11 - want to turn down those values
46:13 - and so we're going to return an object
46:16 - and that object will have
46:19 - open and for the time being we'll go
46:21 - ahead and return step.open or define it
46:23 - rather with step.open
46:25 - and the same for
46:27 - et cetera
46:38 - and that is our scale down function
46:40 - however we're not normalizing anything
46:41 - yet
46:46 - and since if we go over to our training
46:49 - data once more and we look at these
46:51 - values one of the lowest ones that we
46:52 - can come to is this value 138
46:55 - or so so what we're going to do to get
46:57 - these values easily into the neural net
47:00 - is simply divide by that value so 138
47:09 - there and that's our normalize function
47:13 - quite simple and to prove that it works
47:15 - let's go ahead and console log it out
47:18 - console.log
47:20 - scale down
47:21 - we'll just
47:23 - grab the first item
47:25 - in our raw data
47:29 - there and we can see the values are
47:31 - fairly close to between one and zero now
47:33 - if you'll pause here just for a moment
47:35 - think about how we go the other way
47:37 - scaling up or as they say denormalizing
47:45 - okay so now what we're going to do
47:47 - before we do any machine learning we're
47:49 - actually going to write the inverse of
47:50 - this function which would be the
47:52 - denormalize or in practical terms the
47:55 - scale up function
47:57 - so
47:58 - function scale up and it's going to take
48:01 - a
48:02 - step and we're going to return that same
48:04 - sort of signature that same sort of data
48:09 - but rather than
48:10 - dividing
48:12 - by 138 the exact inverse of dividing is
48:18 - multiply
48:19 - so we will multiply by 138
48:23 - and now we have our scale up function
48:26 - referred to normally as d normalize
48:30 - so normalize brings it down denormalize
48:33 - brings it up
48:35 - and to test them both
48:37 - side by side
48:38 - we'll go ahead and console.log
48:41 - scale
48:42 - up
48:43 - it'll look kind of funny scale down
48:47 - in our we'll have our raw data and we'll
48:50 - send that in
48:51 - and
48:52 - what do we get
48:54 - there we go
48:55 - our scaled up and scale down
48:58 - functions
49:00 - we're going to use these in our next
49:02 - neural net but this is a very common
49:04 - practice across neural nets in general
49:06 - not just for current neural nets
49:08 - it is normalizing our values a more
49:11 - common approach to normalizing your data
49:14 - would be to subtract the lowest value
49:17 - from all the other values that you're
49:19 - sending into the neural net and then to
49:21 - divide
49:22 - by the highest value minus the lowest
49:25 - value that sounds kind of confusing at
49:27 - first if we were to take one of these
49:28 - lines for example
49:30 - uh this first one had scaled down and we
49:33 - used it right here and the net result of
49:36 - this
49:37 - is let's say the step dot open
49:40 - was 140.
49:42 - we're going to subtract uh if 138 was
49:46 - the lowest
49:49 - and then we'll divide let's say the the
49:50 - highest value is 147
49:53 - minus 138
49:56 - we would get
50:04 - and we'll subtract
50:06 - 138
50:08 - from 147 and that equals nine
50:14 - and so the net result is two divided by
50:16 - nine
50:17 - and that equals zero point two two two
50:19 - two two to two and what's important
50:21 - about this is it's the end value is
50:23 - between zero and one try and rewrite the
50:26 - scale down and scale up functions to
50:28 - incorporate
50:29 - the more generalized approach which
50:31 - would be to subtract the lowest value
50:34 - divided by the highest minus the lowest
50:36 - value
50:41 - in our last tutorial we talked about
50:43 - normalizing data and in this tutorial
50:46 - we're going to write the neural net and
50:47 - we're going to put that normalized data
50:48 - into it
50:49 - so first let's scale all of our raw data
50:52 - we're going to call this scaled
50:55 - data it's not yet training data but it's
50:57 - close
50:58 - we're going to say raw data
51:01 - dot map and we're going to map over all
51:04 - those values using our scale down
51:06 - function
51:08 - and so now our scaled data will have
51:11 - all of those new values that are
51:13 - normalized next what we want to do is
51:16 - rather than feed in one long array of
51:19 - all these objects these properties so
51:21 - that the neural net memorizes this one
51:24 - long pattern we want the neural net to
51:26 - understand smaller patterns and to
51:29 - predict off of those and so how we're
51:32 - going to do that is we're going to
51:34 - create finally our training data
51:38 - and our training data is going to be an
51:40 - array of arrays
51:42 - we're going to take our scaled data
51:45 - and we're going to slice it into chunks
51:48 - of five
51:50 - starting at the first
51:52 - index
51:54 - and we're going to
51:57 - progress
51:59 - by
52:00 - five indexes each time
52:06 - and so that is our training data and we
52:09 - can console log it out
52:13 - make sure it looks right
52:16 - very nice
52:17 - okay so it's an array of arrays
52:21 - that's an important concept
52:23 - okay so now that we have our data
52:26 - chunked and normalized and ready to go
52:29 - into the neural net we're going to write
52:30 - our neural net so const net equals new
52:34 - brain dot recurrent that long short term
52:38 - memory time step
52:41 - that's our neural net and we're going to
52:43 - define it with a few options now this is
52:46 - important our scaled down
52:49 - data
52:50 - has four different properties here open
52:52 - high low and close
52:54 - that represents one point in time or one
52:57 - step through time so our neural net is
53:00 - going to have an input size of those
53:03 - properties
53:04 - being them four properties our input
53:07 - size will be
53:09 - four
53:10 - and it's very rare to deviate from that
53:12 - with output size
53:14 - so we'll go ahead and put an output size
53:15 - of 4 as well
53:18 - and so now we want to define our hidden
53:20 - layers
53:23 - and our hidden layers are simply going
53:25 - to be
53:26 - eight neurons and eight neurons so input
53:29 - size of four hidden layers of eight and
53:32 - eight and an output size of four
53:35 - and now we can learn our stock market
53:38 - data net dot
53:40 - train training data
53:44 - out
53:47 - now we're going to tweak our training
53:49 - options just a little bit here
53:52 - we're going to put our learning rate
53:56 - at
53:57 - 0.005 and the reason we're going to do
53:59 - that is so it doesn't sort of shoot past
54:02 - the values that we're looking for we
54:04 - want very small increments uh toward our
54:07 - goal
54:08 - and the next is our error threshold
54:11 - now the longer and the more data that
54:13 - you end up using with your neural net
54:16 - potentially longer it's going to take to
54:18 - train
54:18 - and so for this being just in the web
54:21 - browser and we want to train to a
54:23 - sufficiently good error i'm going to
54:24 - turn it down to 0.02
54:28 - and as well let's go ahead and log out
54:30 - our values
54:39 - let's clean this up
54:41 - all right and now let's learn let's see
54:43 - what comes out very cool
54:46 - all right so it trained let's log out
54:48 - what really matters so that's
54:50 - console.log and that'll be net.run
54:54 - and let's just give it uh the first
54:56 - item in our training data
54:58 - so our training data
55:01 - and let's see what comes out
55:03 - very cool so our net actually learns
55:06 - something and returns something now
55:07 - there's a problem here if we look at our
55:10 - original values
55:12 - they're all
55:14 - in the mid
55:15 - 140s or so
55:18 - but our values here are not
55:20 - they're close to zero and one so what do
55:23 - we want to do there well i'll give you a
55:24 - moment to think about it
55:29 - okay this is where we finally use
55:32 - scale up this is where we denormalize
55:35 - our values here we go i went ahead and
55:38 - added that as a wrapper around
55:41 - net run and we'll run it again and there
55:44 - we go the values that look very familiar
55:46 - our 140s or so one thing that would be
55:49 - really useful is to not just look one
55:51 - step into the future which is what we're
55:53 - doing right here that's what net.run
55:55 - would be it's going to take all of our
55:57 - existing steps and say hey this is the
55:59 - potential next step what we want to do
56:01 - is
56:02 - actually look and see
56:04 - what the next two or three steps may be
56:12 - in our last tutorial we predicted the
56:14 - next step for stock market data and in
56:18 - this tutorial we're going to predict the
56:20 - next three steps that's really
56:22 - interesting and cool and it's actually
56:23 - quite simple
56:24 - so we're going to
56:26 - actually just change one little method
56:29 - so rather than use net.run
56:32 - i'm going to comment this out
56:33 - and i'm going to say console.log
56:36 - we're going to do net dot forecast and
56:40 - let's say we just have
56:43 - a couple numbers to go on we're going to
56:46 - send in our training data
56:49 - but we're going to only send in
56:52 - a couple of steps of that data
56:57 - and then
56:59 - we're going to say with forecast that
57:01 - we'd like the next three steps
57:05 - and it doesn't stop there it's actually
57:06 - going to return an array and before we
57:09 - had wrapped
57:11 - run in scale up whereas here in
57:14 - net.forecast because we're getting an
57:16 - array rather than an object which would
57:18 - be the next step the array would be the
57:20 - next steps
57:21 - and so we can take this and map to scale
57:24 - up
57:25 - all right here we go
57:27 - and there's our data
57:29 - that's what the net has learned and it's
57:31 - its next prediction so we've got
57:34 - the beginning of one
57:36 - there's our our second
57:38 - and three i think the console doesn't
57:40 - log up very well with the cursor but the
57:43 - idea is there and so is the data
57:50 - in this tutorial we are going to take a
57:53 - recurrent neural network and we're going
57:55 - to learn math and we're going to do it
57:57 - character by character
57:59 - and at first that sounds kind of
58:02 - weird
58:03 - but you'll notice something interesting
58:05 - at first with our training data here
58:07 - there are no numbers not directly these
58:10 - are strings even though there are
58:11 - numbers inside of them and we are going
58:13 - to feed these directly into the neural
58:15 - net
58:16 - this is where it gets interesting
58:17 - especially with recurrent neural nets so
58:19 - the way that a recurrent neural net
58:21 - works
58:22 - is it has an input map
58:25 - and that map is kind of like an array
58:28 - and that array
58:31 - maps a
58:32 - value that is coming into the neural net
58:34 - to a neuron
58:36 - and it does so by index
58:38 - and this is how it works
58:40 - by first sitting in our training data
58:42 - the training data is analyzed and we
58:46 - pull out unique values like zero
58:50 - the next value being plus
58:52 - and the next value being equals
58:55 - because 0 is repeating up here and we've
58:58 - already established it
58:59 - equals would be the next one 0
59:02 - is repeating again and zero and plus
59:05 - repeats again and the next unique value
59:07 - we have is one in the next
59:10 - item in the array
59:12 - and that happens uh throughout our
59:13 - entire training set so one two three
59:17 - four
59:19 - five
59:20 - six
59:21 - seven
59:23 - eight
59:24 - and nine
59:25 - and so we we build sort of an input map
59:28 - and the reason why that's important is
59:31 - the input map
59:33 - lines up with neurons so our input map
59:35 - is more or less the same size
59:42 - as our input size
59:45 - and that's calculated internally inside
59:47 - of the neural net so that's uh it's
59:50 - important but we don't have to think
59:51 - about it so much
59:53 - so input map
59:55 - is the same size as input size and what
59:58 - that means is for each one of these
60:00 - characters they get their own neuron
60:04 - and so if we step through a problem for
60:06 - example like zero plus zero equals zero
60:10 - to the net we feed in a brand new array
60:14 - each time with that one value activated
60:18 - and so our input
60:21 - internally
60:26 - may look like this so we've got a
60:30 - a bunch of zeros
60:34 - and so
60:35 - the the inputs uh literally look like
60:41 - okay so input length equals input size
60:44 - so each one of these values
60:47 - is sort of attached to an input neuron
60:51 - and this is why that matters
60:54 - each time we feed in one of these
60:56 - characters it goes all the way through
60:58 - the neural net and then we can do that
61:00 - over and over and over again
61:02 - so
61:02 - the math problem zero plus zero equals
61:06 - one literally looks like this to the
61:08 - neural net it looks like
61:11 - zero
61:15 - plus
61:17 - zero
61:20 - and that's the third one
61:22 - equals
61:24 - zero
61:26 - so that is our first math problem sort
61:28 - of internally to the neural net but it
61:30 - it does everything
61:32 - sort of automatically for us
61:34 - and this is a good kind of magic it's
61:36 - not the bad kind it's the kind that is
61:38 - predictable so we'll go ahead and
61:39 - comment that out
61:40 - and this out too all right so we're
61:42 - ready to go ahead and predict our
61:44 - numbers so this is what happens next
61:45 - we're going to define our net const net
61:47 - equals new brain dot recurrent dot along
61:52 - short term memory
61:56 - and we're going to have hidden layers
61:59 - of
62:01 - 20
62:04 - and we're going to net net.train on our
62:06 - training data
62:10 - and we are going to set our
62:13 - error threshold
62:15 - to 0.025
62:20 - five
62:22 - and we're gonna go ahead and log out our
62:24 - value
62:26 - all right
62:27 - so we'll go ahead and learn it and we'll
62:28 - see what happens very cool now a word of
62:31 - caution here it did learn it but if you
62:34 - crank down your error threshold
62:37 - that may be able to learn it but in the
62:39 - browser it can take a long time and a
62:41 - long short term memory neural net is
62:43 - quite complex in contrast with our
62:45 - previous neural nets there's a lot more
62:47 - going on in the long short term memory
62:49 - than the others
62:51 - so our net was able to learn it but of
62:53 - course that's not the fun part the fun
62:55 - part is actually getting to see what
62:56 - comes out of the net so let's do that
62:59 - net dot run and we're just going to give
63:01 - it some straight up old strings
63:04 - 0 plus 1
63:06 - equals and we're going to see what comes
63:08 - out there uh but let's not stop there
63:10 - let's just think of a few
63:12 - let's see uh
63:14 - 4.1
63:16 - uh and depending on the error rate too
63:18 - some of these values may be a little bit
63:20 - incorrect
63:21 - uh but they should be pretty close
63:23 - it's uh quite entertaining what a neural
63:25 - net can
63:28 - can actually give us okay so we're going
63:29 - to run these three values and see what
63:32 - happens
63:33 - all right zero plus one
63:35 - is one four plus one is five and two
63:39 - plus one is three
63:42 - we nailed it awesome so that is
63:45 - recurrent neural networks learning math
63:47 - now if you want to do some
63:50 - bonus material
63:52 - light
63:53 - it up with some math problems
64:03 - in this tutorial we're going to read
64:04 - numbers like you would from an image and
64:07 - for visual simplicity our raw data is
64:09 - going to be of strings like you see here
64:12 - spaces are what we don't want the neural
64:14 - net to see and this asterisk is what we
64:17 - do want the neural net to see now we're
64:19 - going to use this simple string and a
64:21 - simple normalization function to convert
64:23 - these inputs right here two zeros and
64:25 - ones so let's go ahead and write that
64:27 - first
64:28 - so we're gonna create a function
64:31 - and we're gonna call it two number and
64:34 - it's gonna take a character as an input
64:36 - what we're gonna return from this
64:38 - function is if the character
64:40 - equals
64:42 - an asterisk
64:43 - we're going to return a 1.
64:45 - otherwise we're going to return a 0.
64:47 - that's our character by character
64:49 - normalization function now we also need
64:51 - to create a function that normalizes for
64:54 - the entire input like the entire array
64:57 - that function's going to be called to
64:58 - array and it's going to bring in a
65:00 - string now the string needs to be a
65:03 - specific size
65:04 - and if that string dot length isn't
65:09 - seven by seven
65:10 - and we're going to throw
65:12 - a new error string in wrong
65:15 - size
65:16 - otherwise we're going to take the string
65:19 - and we're going to split it
65:21 - and then we're going to map it
65:23 - to number
65:25 - and that'll be our return value that is
65:27 - everything that we need to normalize our
65:29 - input value cool so next what we want to
65:32 - do is i'm going to paste in some number
65:35 - data but you'll notice the number of
65:37 - data isn't just zero to nine it's
65:38 - actually a string and you can see that
65:40 - these functions start to make a little
65:42 - bit more sense now two array is going to
65:44 - take this string in
65:46 - and it's gonna split up this long set of
65:50 - characters and it's going to convert
65:52 - them to ones and zeros so that the net
65:55 - can sort of understand what is is being
65:58 - said here now we have zero through nine
66:01 - and each of these you can see
66:03 - looks exactly like the character it's
66:04 - representing let's test at least one of
66:06 - these values so console.log
66:08 - we're going to put nine
66:11 - very good and we can see that represents
66:14 - what the neural net is going to speak so
66:16 - let's build our neural net so const net
66:19 - equals
66:20 - new brain dot
66:22 - neural network
66:25 - we don't have to mess with size in this
66:26 - because it's a feed forward neural net
66:28 - so let's build our training data const
66:30 - training data equals that's going to be
66:32 - an array of objects that have an input
66:36 - and each of those inputs are going to be
66:37 - one of our values above so the first one
66:39 - let's just say will be zero and the
66:41 - output is going to be an object
66:45 - and that object is going to be of the
66:48 - property that we're classifying towards
66:50 - with a value of one very good so let's
66:52 - go ahead and add the additional
66:54 - numbers here you see we've got one two
66:57 - three all the way up to nine
66:59 - and so now all we need to do is train
67:01 - net.train
67:03 - training data
67:04 - and we're going to log our results out
67:07 - all right let's see what happens very
67:08 - cool 190 iterations
67:11 - fairly low error rate very cool but we
67:14 - still haven't seen anything interesting
67:16 - yet
67:17 - let's go ahead and actually see what the
67:19 - net is outputting so net dot run
67:22 - to array
67:23 - and we're going to give this
67:25 - the number eight and now let's see what
67:28 - the neural net outputs we'll say const
67:30 - result equals
67:32 - and we'll turn off our logging up here
67:34 - because we know the neuron that's
67:35 - training just fine all right here it
67:36 - goes
67:37 - very cool now what you'll notice here is
67:39 - that the output is all of the properties
67:42 - that we used in the training data
67:44 - they're compiled together and the one
67:46 - that is most likely has the highest
67:48 - value so here 8 is a likelihood of 0.7
67:53 - and that's really nice we don't have it
67:54 - though in a simplified format there is
67:56 - however a utility built into brain
67:59 - that is called
68:01 - likely that we can use here const or
68:03 - result equals brain dot
68:07 - likely and we're going to use our two
68:09 - array and we're going to send our net
68:12 - to it
68:13 - and here we're going to take
68:17 - this two array we're going to give it a
68:19 - string
68:20 - very good so that's our 8 and let's
68:23 - console log out the results all right
68:25 - here it goes
68:26 - very cool we got an 8 out of our neural
68:28 - net all right so that was a really
68:30 - simple example and it took a lot to work
68:32 - up to it because we've got a little bit
68:34 - of training data there some conversion
68:35 - etc but play around with this and see if
68:37 - you can get some different characters to
68:39 - be recognized by the neural net now a
68:40 - word of caution here this method will
68:43 - likely be a bit fragile in the real
68:44 - world without something like
68:46 - convolutions and those are coming to
68:47 - brain.js in version two but the thing to
68:50 - note here is that the principles still
68:51 - applies now there is a question to be
68:54 - begged here how resilient is a neural
68:56 - net why wouldn't we just use a bunch of
68:59 - if statements and a bunch of loops to
69:01 - build this example of an eight if i were
69:03 - to remove for example something that the
69:06 - neural net sees like one of these
69:07 - asterisks
69:09 - would the neural net still recognize
69:11 - this as an 8
69:12 - or if i did the same with a 2 or a 6 etc
69:16 - well let's find out
69:18 - see it still found the eight this
69:20 - demonstrates one of the coolest parts
69:21 - about neural nets and that they are
69:22 - dynamic and they're resilient and that's
69:24 - why they're so popular and so amazing
69:26 - and for your bonus
69:28 - write
69:29 - three
69:30 - examples
69:33 - that still match like this eight
69:36 - but are not like your training data
69:39 - and that'll demonstrate the resilience
69:41 - of the neural net
69:43 - have fun
69:48 - in this tutorial we're going to write a
69:50 - children's book using our current neural
69:52 - network
69:53 - and we'll start with our training data
69:55 - here and if you'll go ahead and think
69:57 - about pausing it here what type of
70:00 - neural net that would use based off our
70:02 - previous tutorials
70:06 - okay
70:06 - likely you arrived at
70:09 - a long short term memory recurrent
70:11 - neural network we're going to go ahead
70:13 - and train on our data
70:15 - and we're going to
70:16 - give it a few options our iterations
70:19 - we're going to stop at 1500 our error
70:22 - threshold
70:24 - we're going to stop at 0.011
70:26 - and we're going to log our details
70:28 - all right let's see if it rains
70:30 - very cool and now let's get it to output
70:33 - something interesting our first line
70:35 - from the training data
70:38 - net dot run
70:40 - is
70:41 - jane we're going to stop there with it
70:44 - and jumping to the more complex line
70:46 - i'm going to say net dot run
70:49 - it
70:50 - was just based off of these two lines
70:52 - let's see what the network outputs
70:54 - get rid of the log
70:56 - very cool so it starts out jane
70:59 - saw doug
71:01 - and jane looking at each other
71:02 - oh wow
71:04 - and then
71:05 - net.run it was love at first sight and
71:08 - spot had a front row seat it was a very
71:10 - special moment for all you see the net
71:13 - already altered a little bit of what was
71:14 - going on
71:16 - what you'll find is that depending on
71:18 - how you set up your neural net it can
71:19 - give you some really interesting outputs
71:21 - and the more data that you give it the
71:23 - more interesting it's going to get so
71:24 - your bonus here is to experiment
71:27 - with book writing
71:35 - in this tutorial we are going to learn
71:36 - how to detect sentiment from text
71:39 - directly and you'll notice our training
71:41 - data is a bit different than prior here
71:43 - we have an array
71:44 - of objects
71:46 - whose properties are
71:48 - strings
71:50 - rather than
71:51 - arrays of numbers and brain js is set up
71:54 - to go ahead and accept this type of
71:56 - input it's not a problem so let's go
71:57 - ahead and get started we'll build our
71:59 - neural net const net equals
72:02 - new brain dot recurrent dot long short
72:05 - term memory neural net and we'll go
72:08 - ahead and train net.train
72:11 - on training data
72:12 - now we're going to alter our training
72:15 - options
72:16 - we're going to stop at 100 iterations
72:18 - our error threshold
72:21 - is going to be 0.011
72:23 - and we're going to log
72:25 - our stats
72:27 - we're going to cut the chase and go
72:28 - ahead and output something interesting
72:30 - we are going to use an input that's not
72:33 - from our list above we're just going to
72:35 - use something that's similar to it so
72:37 - net dot run i am
72:39 - unhappy
72:41 - go ahead and log that out let's see what
72:43 - we get
72:44 - very cool
72:45 - we got sad
72:47 - let's remove our
72:48 - log all right now let's add an
72:50 - additional item
72:52 - net dot run
72:54 - i am
72:55 - happy
72:56 - very cool now i want to point out that
72:58 - we're not training this for very long
73:00 - and that the error threshold if you were
73:02 - to bring that a bit down and as well
73:04 - iterations it's going to be quite a bit
73:05 - more stable but what you may have not
73:07 - noticed is that we did not send into the
73:09 - neural net
73:11 - of values that we trained it with and
73:13 - yet it was still able to classify them
73:15 - as happy and sad so again we're kind of
73:17 - highlighting the dynamic ability of the
73:20 - neural net
73:21 - and for your bonus
73:25 - tweak the iterations and threshold be
73:28 - careful because in the browser you can
73:30 - cause it to train indefinitely or for a
73:32 - very long period of time
73:34 - but add five new examples in the
73:38 - training
73:39 - data
73:40 - that would be up here
73:43 - and then
73:45 - log out of five examples that aren't in
73:49 - the training data all right have fun
73:51 - yelling at the computer
73:57 - sentiment detection recurrent neural
73:59 - network
74:00 - and in this tutorial we're going to
74:01 - explain kind of how we did it
74:04 - now if you'll recall
74:05 - we use both inputs and outputs for the
74:08 - sentiment detection recurrent neural
74:10 - network
74:11 - that is kind of a new concept when
74:13 - talking about recurrent neural networks
74:15 - because we explain them kind of like
74:17 - going to the movies now there are no
74:19 - inputs and outputs with movies we're
74:21 - just an observer sort of watching what
74:23 - happens next however we can bend those
74:26 - rules in order to fit the paradigm of an
74:29 - input and an output and i'll explain how
74:32 - now previously we mentioned that a
74:33 - recurrent neural network builds an input
74:35 - map
74:36 - and that is kind of like an array and
74:38 - each value in that input map corresponds
74:41 - to characters
74:43 - that are used in the training data but
74:45 - those rules apply here as well nothing
74:47 - has changed the input character one
74:51 - gets its own special neuron
74:54 - and its own special map its own special
74:57 - lookup and as well the same is with the
75:00 - output of character two however our
75:03 - input map is not complete what is
75:05 - different here with the training data is
75:07 - that there is an unseen character a
75:09 - character that tells the neural net to
75:11 - transition between an input and an
75:14 - output
75:16 - we'll call that character the new
75:19 - idea character
75:21 - all right
75:22 - and so if we look at our training data
75:26 - we have an input one
75:28 - and then we have our new idea
75:32 - and then we have our output of two
75:36 - and so if we were to look at a set of
75:38 - arrays that consist of what is being fed
75:42 - into the neural net they look much like
75:43 - this so our input map we have three
75:46 - indexes three neurons okay
75:49 - and the first one is going to have a
75:51 - value of one
75:53 - that first value is activated that input
75:56 - character one
75:58 - that is activated here and our value
75:59 - that's being sent through the neural net
76:01 - next we transition to the new idea
76:03 - character and so there new idea gets a
76:07 - one the next character
76:10 - is followed by two
76:12 - so there we have zero
76:15 - zero
76:16 - one
76:17 - so each one of these items in the array
76:20 - correspond to a neuron that correspond
76:23 - to our input map that correspond to our
76:25 - training data
76:26 - it sounds kind of confusing at first
76:28 - but what you'll get to notice with
76:30 - recurrent neural networks and neural
76:32 - networks in general is that they're
76:34 - actually quite simple and even the math
76:36 - that represents them is quite simple
76:38 - there's just a lot of it by taking
76:40 - advantage of those rules and that math
76:42 - we can bend those rules to our will
76:45 - now for our bonus here
76:47 - what would the training data
76:50 - look like if we start with 2
76:54 - and end with
76:56 - 1. so 2 would be our input
76:59 - and our output
77:00 - would be
77:01 - one
77:02 - and then as well
77:04 - what would this set of arrays look like
77:07 - i'll go ahead and pause it here go ahead
77:09 - and write your answer and we'll compare
77:11 - notes
77:17 - all right and so now if we're to compare
77:20 - our answers
77:22 - they actually should look flipped and so
77:25 - our
77:26 - training data that has an input of
77:30 - 2 and an output
77:33 - of 1
77:36 - will internally to brain.js look like
77:38 - this it'll start with a 0
77:41 - 0
77:42 - 1
77:43 - zero one
77:44 - zero
77:46 - and then and finally with a one zero
77:48 - zero now this is all internal to brain
77:51 - js this isn't stuff that you have to
77:53 - learn it's just understanding the
77:54 - principles behind the neural net because
77:57 - again a neural net is simple there's
77:59 - just a lot of it
78:04 - in this tutorial we are going to use
78:06 - reinforcement learning this is a really
78:08 - exciting frontier of machine learning
78:10 - and we're going to use it right here
78:12 - you'll notice our training data is
78:13 - exclusive or right where we started
78:15 - originally and half of our training data
78:18 - has been commented out this is important
78:20 - what we're going to do is learn these
78:22 - first two items in our training data
78:24 - we're going to test against these first
78:26 - two items we're going to look at this
78:28 - next item and see that the net really
78:30 - doesn't understand it and then we're
78:31 - going to learn it so let's start by
78:33 - creating our neural net constnet equals
78:36 - new brain dot
78:39 - neural
78:40 - network
78:42 - and we had hidden layers
78:45 - of three
78:47 - and we'll do net.train
78:50 - that's training data we're gonna log out
78:52 - our results and let's see what we get
78:54 - very cool so we've learned these very
78:56 - first two items let's go ahead and
78:58 - remove our logging
78:59 - and then let's take a look at what the
79:01 - net actually outputs so net
79:03 - dot run and we're going to give it an
79:05 - input 0.0
79:08 - we're going to console log that and i'm
79:10 - going to do array dot from
79:17 - all right let's see what we get
79:21 - very cool and that's exactly what we
79:23 - were expecting however if we add one of
79:25 - the examples
79:28 - of like
79:29 - one zero
79:32 - let's see what we get
79:34 - all right very cool and that's what we
79:35 - were expecting the net has not been yet
79:37 - trained on this data and so this
79:38 - illustrates why reinforcement learning
79:40 - is so important and necessary our
79:42 - training data may not be available all
79:44 - at once it may come over time and so we
79:47 - want to make sure that our neural net
79:48 - can adjust for that so let's go ahead
79:50 - and adjust our training data adding one
79:52 - brand new example the one zero
79:55 - outputting of one and you'll get to see
79:57 - how we use reinforcement learning in
79:58 - this feed forward neural net alright so
80:01 - training data dot
80:04 - push
80:05 - and we're going to give it an object
80:07 - that's an input
80:10 - array
80:11 - and 1 0 is our input and then output is
80:14 - going to be of one array
80:17 - all right and then really all we have to
80:19 - do is take net.train
80:21 - put it down here and we can give it some
80:23 - titles to be fancy console.log
80:25 - before reinforcement
80:30 - well we'll just copy all of this
80:32 - and then here we'll do
80:35 - we'll say after
80:37 - reinforcement all right let's see what
80:39 - happens
80:42 - all right before
80:43 - we have two very similar values
80:46 - and after illustrates that our net was
80:49 - able to reinforce
80:51 - now as a bonus
80:56 - we go ahead and add the missing item
80:58 - from our training data
81:06 - all right have fun
81:11 - in this tutorial we're going to build a
81:12 - recommendation engine we're going to do
81:13 - so using the simplest sort of
81:15 - methodologies this is one of my favorite
81:17 - examples because it seems so hard when i
81:20 - first started in neural nets but in fact
81:21 - it's actually quite simple we're going
81:23 - to start with our training data which is
81:25 - color preference
81:27 - and imagine that this list was built you
81:29 - just gave somebody
81:31 - these list of colors blue red black
81:34 - green brown
81:35 - and they basically wrote a number by
81:38 - them how much they liked that color so
81:41 - they like blue a lot they like red a lot
81:43 - however they don't really like black
81:46 - green or brown and that's how we end up
81:47 - with these numbers on both the inputs
81:49 - and outputs so that's our training data
81:51 - and we can learn that really quick using
81:53 - a feed forward neural net so const net
81:56 - equals
81:57 - new brain dot
82:00 - neural network
82:04 - net dot train
82:06 - and we're going to have our training
82:07 - data
82:08 - we'll go ahead and log out
82:10 - our stats
82:11 - see what happens
82:13 - cool our net was able to train really
82:15 - quick on that data let's turn off
82:16 - logging and see what we get
82:19 - console.log
82:20 - array.from
82:26 - net dot run
82:30 - and we'll say blue
82:33 - one
82:34 - now we're going to duplicate this line
82:37 - we're going to put brown
82:40 - all right here we go
82:43 - all right so the user has a very strong
82:45 - preference towards blue and not so much
82:48 - towards brown we can see that here from
82:50 - our
82:51 - output so here's the scenario over time
82:53 - you go back to that original user you
82:55 - show them this list and based off of
82:57 - their experience things they like things
83:00 - they dislike they now have changed what
83:02 - they prefer and now brown is one of
83:05 - their favorite colors so how do we get
83:07 - that into a neural net why don't you go
83:08 - ahead and think about that just for a
83:10 - moment and pause it here
83:16 - okay for us to get brown into that
83:18 - training data we have to put it there so
83:20 - training data dot and we're going to say
83:22 - push
83:24 - let's say brown
83:26 - one
83:27 - and output
83:29 - we're gonna put as a one
83:33 - all right and so now we have our
83:34 - training data in there so now what do we
83:36 - need to do to get the net to understand
83:39 - what this new training data means i'll
83:41 - pause it here and let you figure it
83:46 - out okay
83:48 - next what you want to add is net.train
83:52 - and we're going to give it training data
83:55 - and we're going to log out the results
83:56 - again
83:58 - stats
84:01 - and console.log
84:03 - stats
84:04 - all right
84:05 - and now let's see what happens and
84:07 - actually let's go ahead and console log
84:10 - out our values but after we train we'll
84:13 - give us a fancy title as well so before
84:17 - and you'll notice that we are actually
84:20 - using reinforcement learning a bit here
84:24 - just very simple it's not very
84:25 - complicated at all so before preference
84:27 - change after preference change
84:30 - let's see what happens
84:32 - very cool
84:33 - so we scroll through our training
84:35 - actually let's go ahead and remove the
84:37 - logging there we know that it logged
84:40 - now let's see what happens
84:42 - very cool so now before our preference
84:43 - changed you can see that brown was rated
84:46 - fairly low
84:47 - but after our preference change it was
84:49 - rated at a mediocre point five now why
84:52 - could that be we'll pause it here and
84:53 - let you think about it
84:59 - okay the reason is this
85:01 - our training data
85:04 - still has reference to when their
85:08 - preference
85:09 - was
85:10 - towards zero rather than towards one we
85:12 - want to ensure that we only have their
85:14 - current preference so
85:16 - training data dot pop
85:19 - that's going to remove the last item
85:20 - from our array
85:22 - so we'll pretend this is some sort of
85:23 - user
85:24 - interaction or server interaction
85:26 - something that happened to adjust their
85:29 - preferences that gave us brand new
85:31 - training data and now we're going to
85:33 - train on it again and then after that
85:35 - preference change we're going to have
85:36 - our brand new updated values here we go
85:39 - all right
85:40 - before our preference change brown was
85:42 - rated fairly low at 0.05 and after it
85:45 - came out of a 0.89 so this illustrates
85:47 - both reinforcement learning and also how
85:49 - to build a very simple recommendation
85:51 - engine now your bonus here
85:56 - build your own recommendation engine
85:59 - what's your favorite food where do you
86:00 - like to go does that change over time
86:02 - has it changed since you were a little
86:04 - let the net know
86:05 - have fun
86:10 - awesome job you got all the way through
86:12 - the course i am super proud and i'm very
86:14 - excited that you were able to get
86:15 - through listening to me but also
86:17 - building and applying and seeing these
86:20 - new concepts and how they can actually
86:21 - be quite simple but as we're progressing
86:23 - i'd like to give you some takeaways for
86:25 - this course to be able to remember and
86:28 - to share with others and that is this
86:30 - machine learning is moving really fast
86:32 - and that is super exciting that means
86:34 - it's changing and it's being adopted
86:36 - we're solving problems that we've never
86:38 - been able to solve before and you have a
86:40 - voice in that machine learning can
86:41 - change directions and a lot of that has
86:43 - to do with our understanding of how to
86:45 - use it we're also arriving at simple
86:47 - solutions are are so much more dynamic
86:50 - and powerful than complex ones when
86:52 - applying it so if there is a practice
86:54 - that you want to see more of or if there
86:56 - is something you like to change about
86:58 - machine learning try and introduce it
87:00 - study it prove it test it to see if it
87:02 - is worthwhile and not only will you
87:04 - benefit machine learning for you you're
87:06 - going to help all of us to have those
87:08 - new tools and capabilities so machine
87:10 - learning is moving fast and you have a
87:13 - voice and where it goes from this course
87:15 - to i wanted really to highlight that
87:18 - machine learning is actually quite
87:19 - simple the terminologies behind it we
87:21 - can get very lost in them but when we
87:23 - arrive at the logical foundation of
87:26 - machine learning it is simple there's
87:27 - just a lot of it a lot of numbers a lot
87:29 - of logic and i want to highlight where
87:30 - you go next with this simplicity with
87:33 - three illustrations
87:36 - and they are equals mc squared and ac
87:39 - motor and washing your hands these may
87:41 - seem comical at first but follow me the
87:44 - first is equals mc squared albert
87:46 - einstein discovered equals mc squared
87:47 - because he reasoned on that electrons
87:49 - moving around an atom would be affected
87:51 - by gravity kind of like when you throw a
87:53 - ball into the wind like a beach ball or
87:55 - something like that a child can tell you
87:57 - if you throw a beach ball into the wind
87:58 - that is going to be affected by it and
88:00 - that's how he reasoned on the matter so
88:02 - the point is that albert einstein used
88:05 - intuition
88:06 - use your intuition when you're solving
88:09 - problems with machine learning and solve
88:11 - for simplicity like e equals m c squared
88:14 - the next illustration was that of an ac
88:16 - motor now a lot of people know that
88:18 - nikola tesla helped to build and refine
88:21 - the ac motor but if you go back further
88:23 - in his career you'll arrive at that when
88:26 - he was in school he was shown this
88:28 - dynamo type dc motor that had these
88:31 - brushes and that arced it sparked
88:34 - everywhere and was really crude and
88:35 - noisy and he asked his teacher why it
88:38 - needed to have
88:39 - the brushes why it needed to spark
88:41 - everywhere and his teacher was insulted
88:43 - by him asking the question and made him
88:45 - write an essay and it's a really
88:47 - interesting story but the point is this
88:48 - he asked why it was even there and from
88:51 - then on he started to model his career
88:53 - to build a motor that didn't have those
88:55 - brushes and he arrived at the ac
88:57 - induction motor and refined it and he
88:59 - eventually used that same idea to help
89:02 - power both chicago and new york
89:05 - simultaneously from niagara falls it was
89:07 - a simple question a simple idea that got
89:10 - him started nikola tesla used intuition
89:13 - something wasn't right about using all
89:15 - that complexity solve for simplicity use
89:18 - your intuition this last illustration is
89:20 - one of my favorites and it's a washing
89:22 - your hands and the reason is because
89:23 - it's such a simple practice most people
89:25 - know washing your hands it's common
89:27 - knowledge it helps you to be clean but
89:29 - prior to the mid-1800s it was not in
89:31 - fact in hospitals doctors would do all
89:33 - kinds of weird things like perform
89:35 - surgeries and then go help
89:37 - with births and they were wondering why
89:39 - their patients are dying left and right
89:41 - but it turns out that egonos symbolwise
89:43 - theorized not only that we were being
89:45 - infected by something but that we should
89:47 - wash our hands to prevent it and at
89:48 - first it was rejected the idea was
89:51 - absurd why why would you ever want to
89:52 - wash your hands which to us sounds
89:54 - absurd but eventually it was accepted
89:56 - the idea though is that ignosingwise did
89:58 - not have access to microscopes and it
90:00 - was not yet discovered what a germ was
90:02 - ignos symbolwise used intuition and it
90:05 - was a very simple problem and he used a
90:08 - very simple solution
90:10 - use your intuition find those simple
90:14 - solutions machine learning is being
90:16 - applied everywhere and that is super
90:18 - exciting because there is not an
90:20 - industry that is not directly or
90:22 - indirectly affected by it we are solving
90:24 - problems that we have never been able to
90:26 - as a race that is awesome but when
90:29 - you're applying machine learning use
90:32 - your intuition think problems through to
90:34 - the end and think about the simplicity
90:36 - and arrive at using that over complexity
90:39 - now this entire tutorial has been
90:42 - with brain.js and javascript and for
90:45 - that reason javascript thanks you and
90:47 - what i mean by that is until recently
90:49 - javascript really wasn't a very serious
90:50 - language with machine learning but that
90:52 - idea is changing and you are in part to
90:55 - be thanked for that in fact anybody who
90:58 - is looking at javascript as a potential
91:00 - solution for machine learning that is
91:02 - awesome and we should spread it like
91:04 - wildfire not just in javascript but in
91:06 - every language every computer language
91:08 - that is out there should have the
91:09 - capability of using machine learning
91:12 - because it is so amazing now i want to
91:14 - take this moment to invite you over to
91:17 - the machine learning movement in
91:18 - javascript which is the bri.im
91:20 - website and as well their slack channel
91:22 - there you're going to find like-minded
91:24 - individuals who are not only interested
91:26 - in javascript as a means of using
91:29 - machine learning but also challenging
91:31 - the ideas that are out there now and
91:33 - introducing new ones i look forward to
91:34 - talking with you there and there are
91:36 - many others that do as well thank you as
91:38 - well for letting me teach this course
91:40 - teaching using the scrimma platform has
91:41 - been a real honor and i really too thank
91:43 - the encouragement of the scrimba crew
91:46 - you know who you are for prodding me
91:47 - along to make this course and i want to
91:49 - thank you for your time and letting me
91:51 - talk about these ideas and try to arrive
91:53 - at some sort of simplicity with them if
91:55 - you're following the entire course all
91:57 - the way through to the end i look
91:59 - forward to seeing what you can build and
92:00 - now comes the fun part we get to use
92:02 - machine learning to solve real world
92:04 - problems
92:06 - i'll leave you to it