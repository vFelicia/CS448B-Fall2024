00:00 - in this applied data science crash
00:02 - course you learn all about AB testing
00:05 - from the concepts to the Practical
00:07 - details they can apply in business AB
00:09 - testing is commonly used in data science
00:12 - it's an experiment on two variants to
00:15 - see which performs better based on a
00:17 - given metric this course merges in-depth
00:20 - statistical analysis with the kind of
00:22 - data science theories big Tech firms
00:24 - rely on T from L Tech developed this
00:27 - course she is a very experienced a
00:30 - scientist and teacher welcome to the
00:32 - handsone ab Testing crash course where
00:35 - we will do some refreshment when it
00:37 - comes to AB testing if you're looking
00:39 - for that one course where you can learn
00:41 - and quickly refresh your memory for AB
00:44 - testing and how to actually do an AB
00:46 - testing case study hands on in Python
00:49 - then you are in the right place in this
00:52 - crash course we are going to refresh our
00:54 - memory for the a test design including
00:57 - the power analysis and defining those
00:59 - different PR such as minimum detectable
01:01 - effect statistical significance level
01:04 - and also the uh type two probability so
01:06 - the power of the test and then we are
01:08 - going to do Hands-On case study project
01:11 - where we will be conducting an AB
01:13 - testing results analysis in Python at
01:16 - the end of this course you can expect to
01:18 - know everything about designing an AB
01:20 - test what it means as to design a proper
01:23 - AB test and how to do a Ab test results
01:26 - analysis in Python in a proper way I'm
01:29 - dat Vas and co-founder at Lun Tech and I
01:32 - have been in data science for the last 5
01:34 - years I have learned AB testing end to
01:37 - end after following numerous blogs and
01:39 - numerous research papers and courses and
01:42 - I've noticed that there is not a one
01:44 - place one course that will cover all the
01:47 - fundamentals and necessary stuff both
01:50 - the theory and implementation in Python
01:52 - in one place and that's about to change
01:55 - as we have this crash course that will
01:57 - help you to do exactly that to learn how
02:00 - to design an AB test in a proper way as
02:03 - a good and solidated scientist and to
02:05 - Showcase your skills by doing python AB
02:09 - testing results and asset don't forget
02:11 - to subscribe like and comment to help
02:14 - the algorithm to make this content more
02:16 - accessible to everyone across the world
02:19 - and if you want to get free resources
02:22 - make sure to check the free resources
02:24 - section at lunch. and if you want to
02:28 - become a job ready data scientist and
02:30 - you are looking for this accessible boot
02:32 - camp that will help you to make your job
02:35 - ready data scientist consider enrolling
02:38 - to the data science boot camp so whether
02:40 - you are a product scientist whether you
02:42 - are a data analyst data scientist or a
02:46 - product manager who wants to learn about
02:47 - AB testing at high level and how it can
02:50 - be done in Python then you are in the
02:53 - right place because in this crash course
02:55 - we're going to refresh our memory what
02:57 - it means to properly design an a test
02:59 - test which means doing power analysis
03:02 - and also calculating the sample size by
03:05 - hand by following the statistical
03:07 - guidelines and ensuring that everything
03:10 - is done properly and then as the second
03:13 - part of this Crush course we are also
03:15 - going to do an handson case study in
03:18 - Python when it comes to performing AB
03:20 - testing results analysis so we are going
03:23 - to cover all these important Concepts
03:25 - such as P values sample size and also uh
03:29 - interpreting the ab test results using
03:31 - standard error calculating those uh
03:33 - estimates pulled variance and then
03:36 - evaluating the ab test results including
03:38 - confidence interal generalizability of
03:41 - the results reproducibility of the
03:43 - results so without further Ado let's get
03:49 - started AB testing is an important topic
03:52 - for data scientists to know because it's
03:54 - a powerful method for evaluating changes
03:57 - or improvements to the products or
03:59 - services
04:00 - it allows us to make data driven
04:02 - Decisions by comparing the performance
04:04 - of two different versions of a product
04:06 - or a service usually referred as
04:08 - treatment or control for example a
04:11 - testing allows data scientists to
04:13 - measure the effectiveness of changes to
04:15 - your product or a service which is
04:17 - important as it enables data scientists
04:19 - to make data driven decisions rather
04:21 - they're relying on Intuition or
04:24 - assumptions secondly AB testing helps
04:26 - data Sciences to identify the most
04:29 - effective change changes to a product or
04:31 - a service which is really important
04:33 - because it allows us to optimize the
04:35 - performance of a product or a service
04:37 - which can then lead to increased
04:39 - customer satisfaction and
04:42 - sales AB testing helps us also to
04:45 - validate certain hypothesis about what
04:47 - changes will improve a product or
04:49 - service this is important because it
04:52 - helps us to build a deeper understanding
04:54 - of the customers and the factors that
04:56 - influence customers
04:57 - Behavior finally AB testing is a common
05:01 - practice in many Industries such as
05:02 - e-commerce digital marketing website
05:05 - optimization and many others so data
05:08 - scientists who have knowledge and
05:09 - experience in a testing will be more
05:12 - valuable to these
05:14 - companies no matter in which industry
05:16 - you want to enter as a data scientist
05:18 - and what kind of job you will be
05:20 - interviewed for and even if you believe
05:22 - more technical data scien is your cup of
05:24 - tea be prepared to know at least higher
05:26 - level understanding and the details
05:28 - behind this method will definitely help
05:30 - you to know about this topic when you
05:31 - are speaking with product owners
05:33 - stakeholders product scientists and
05:35 - other people involved in the
05:38 - business let's briefly discuss the
05:40 - perfect audience for the section of the
05:42 - course and prerequisites there are no
05:44 - prerequisites of the section in terms of
05:46 - AB testing Concepts that you should know
05:48 - already but knowing the basics and
05:51 - statistics which you can find in the
05:53 - fundamentals to statistics section is
05:55 - highly recommended this section will be
05:58 - great if you have no priority AB testing
06:00 - knowledge and you want to identify and
06:02 - learn the essential AB testing Concepts
06:04 - from scratch so this will help you to
06:06 - prepare for your job interviews it will
06:09 - also be a good refresher for anyone who
06:11 - does have AB testing knowledge but who
06:13 - wants to refresh their memory or want to
06:15 - fill in the gaps in their knowledge in
06:18 - this lecture we will start off the topic
06:20 - about AB testing where we will formally
06:22 - Define what AB testing is and we will
06:25 - look at the high level overview of AB
06:27 - testing process step by step
06:32 - by definition AB testing or split
06:34 - testing is originated from the
06:36 - statistical randomized control trials
06:38 - and is one of the most popular ways for
06:41 - businesses to test new ux features new
06:43 - versions of a product or an algorithm to
06:46 - decide whether your business should
06:47 - launch that new ux feature or should
06:49 - productional IE that new recommender
06:51 - system create that new product that new
06:54 - button or that new
06:56 - algorithm the idea behind a testing is
06:59 - that you should show the variated or the
07:01 - new version of the product to sample of
07:03 - customers often referred as experimental
07:05 - group and the existing version of the
07:07 - product to another sample of customers
07:09 - referred as control group then the
07:11 - difference in the product performance in
07:13 - experimental versus control group is
07:15 - tracked to identify the effect of these
07:18 - new versions of the product on the
07:20 - performance of the product so the goal
07:22 - is then to track the metric during the
07:24 - test period and find out whe there is a
07:27 - difference in the performance of the
07:28 - product and and what type of difference
07:30 - is it the motivation behind this test is
07:33 - to test new product variants that will
07:35 - improve the performance of the existing
07:37 - product and will make this product more
07:39 - successful and optimal showing a
07:41 - positive treatment effect what makes
07:44 - this testing great is that businesses
07:46 - are getting direct feedback from their
07:48 - actual users by presenting them the
07:50 - existing versus the variated product
07:52 - version and in this way they can quickly
07:54 - Test new ideas in case of ab Test shows
07:58 - that the variated version is not
08:00 - effective at least businesses can learn
08:02 - from this and can decide whether they
08:03 - need to improve it or need to look for
08:05 - other ideas let us go through the steps
08:08 - included in the AB testing process which
08:11 - will give you a higher level overview
08:13 - into the
08:14 - process the first step in conducting AB
08:16 - testing is stating the hypothesis of the
08:18 - ab test this is a process that includes
08:21 - coming up with business and statistical
08:23 - hypothesis that you would like to test
08:25 - with this test including how you
08:27 - measured the success which will
08:29 - primary
08:31 - metric next step in AB testing is to
08:34 - perform what we call power analysis and
08:37 - design the entire test which includes
08:39 - making assumptions about the most
08:40 - important parameters of the test and
08:42 - calculate the minimum sample size
08:44 - required to claim statistical
08:47 - significance the third step in AB
08:49 - testing is to run the actual AB test
08:51 - which in practical sense for the data
08:53 - scientist means making sure that the
08:55 - test runs smoothly and correctly
08:57 - collaborate with engineers and product
08:59 - managers to ensure that all the
09:01 - requirements are satisfied this also
09:04 - includes collecting the data of control
09:06 - and experimental groups which will be
09:07 - used in The Next
09:10 - Step next step in AB testing is choosing
09:13 - the right statistical test whether it is
09:15 - z test T Test Ki Square test Etc to test
09:19 - the hypothesis from the step one by
09:21 - using the data collected from the
09:23 - previous step and to determine whether
09:25 - there is a statistically significant
09:27 - difference between the control versus
09:29 - experimental
09:32 - group The Fifth and the final step in AB
09:34 - testing is continuing to analyze the
09:36 - results and find out whether besides
09:38 - statistical significance there is also
09:40 - practical significance in this step we
09:43 - use the second step's power analysis so
09:45 - the assumptions that we made about model
09:47 - parameters and the simple siiz and the
09:50 - four steps results to determine whether
09:52 - there is a practical significance beside
09:54 - of the statistical significance this
09:57 - summarizes the AB testing process at a
09:59 - high level in next couple of lectures
10:01 - we'll go through the steps one at a time
10:04 - so buckle up and let's learn about AB
10:07 - testing in this lecture lecture number
10:10 - two we will discuss the first step in a
10:12 - testing process so let's bring our
10:15 - diagram back as you can recall from the
10:18 - previous lecture when we were discussing
10:20 - the entire process of AB testing at a
10:22 - high level we saw that in the first step
10:24 - in conducting AB testing is stating the
10:26 - hypothesis of ab test this process
10:29 - includes coming up with a business and
10:31 - statistical hypothesis that you would
10:32 - like to test with this test including
10:35 - how you measured the success which we
10:37 - call a primary metric so what is the
10:39 - metric that we can use to say that that
10:42 - the product that we are testing performs
10:45 - well first we need to State the business
10:47 - hypothesis for our AB test from a
10:50 - business perspective so formally
10:52 - business hypothesis describes what the
10:54 - two products are that being compared and
10:56 - what is the desired impact or the
10:58 - difference for the businesses so how to
11:00 - fix a potential issue in the product
11:02 - where a solution of these two problems
11:04 - will influence what we call a key
11:06 - performance indicator or the kpi of the
11:09 - interest business hypothesis is usually
11:12 - set as a result of brainstorming and
11:14 - collaboration of relevant people on the
11:16 - product team and data science team the
11:19 - idea behind this hypothesis is to decide
11:21 - how to fix a potential issue in the
11:23 - product where a solution of these
11:25 - problems will improve the target kpi one
11:28 - example of business hypothesis is that
11:30 - changing the color of learn more button
11:33 - for instance to Green will increase the
11:36 - engagement of the web
11:41 - page next we need to select what we call
11:43 - primary metric for our av testing there
11:46 - should be only one primary metric in
11:48 - your ab test choosing this metric is one
11:51 - of the most important parts of ab test
11:53 - since this metric will be used to
11:55 - measure the performance of the product
11:57 - or feature for the experiment Al and
11:59 - control groups and they will be used to
12:02 - identify whether there is a difference
12:03 - or what we call statistically
12:05 - significant difference between these two
12:06 - groups by definition primary metric is a
12:09 - way to measure the performance of the
12:11 - product being tested in the ab test for
12:14 - the experimental and control groups it
12:16 - will be used to identify whether there
12:18 - is a statistically significant
12:19 - difference between these two groups the
12:21 - choice of the success metric depends on
12:23 - the underlying hypothesis that is being
12:25 - tested with this AB test this is if not
12:28 - the most one of the most important parts
12:30 - of the ab test because it determines how
12:32 - the test will be designed and also how
12:34 - will the proposed ideas perform choosing
12:37 - poor metrics might disqualify a large
12:39 - amount of work or might result in wrong
12:41 - conclusions for instance the revenue is
12:44 - not always the end goal therefore in AB
12:47 - testing we need to tie up the primary
12:49 - metric to the direct and the higher
12:51 - level goals of the
12:55 - product the expectation is that if the
12:57 - product makes more money then this
12:59 - suggests the content is great but in
13:01 - achieving that goal instead of improving
13:03 - the overall content of the material and
13:06 - writing one can just optimize the
13:08 - conversion funless one way to test the
13:10 - accuracy of the metric you have chosen
13:12 - as your primary metric for your ab test
13:14 - could be to go back to the exact problem
13:17 - you want to solve you can ask yourself
13:19 - the following question what I tend to
13:21 - call the metric validity
13:23 - question so if the Chen metric were to
13:26 - increase significantly while everything
13:28 - else T constant would we achieve our
13:31 - goal and would we address our business
13:32 - problem is it higher revenue is it
13:35 - higher customer engagement or is it high
13:37 - views that we are chasing in the
13:39 - business so the choice of the metric
13:42 - will then answer this question though
13:44 - you need to have a single primary metric
13:46 - for your ab test you still need to keep
13:48 - an eye on the remaining metrics to make
13:50 - sure that all the metrics are showing a
13:52 - change and not only the target one
13:55 - having multiple metrics in your ab test
13:57 - will lead to false positives since you
13:59 - will identify many significant
14:01 - differences well there is no effect
14:03 - which is something you want to avoid so
14:05 - it's always a good idea to pick just a
14:07 - single primary metric but to keep an eye
14:10 - and monitor all the remaining
14:15 - metrics so if the answer to the metric
14:18 - validity question is higher Revenue
14:20 - which means that you are saying that the
14:22 - higher revenue is what you are chasing
14:25 - and better performance means higher
14:26 - revenue for your product then you can
14:28 - use your primary metric what we call a
14:30 - conversion rate conversion rate is a
14:33 - metric that is used to measure the
14:34 - effectiveness of a website a product or
14:37 - a marketing campaign it is typically
14:39 - used to determine the percentage of
14:40 - visitors or customers who take a desired
14:43 - action such as making a purchase filling
14:45 - out a form or signing up for a service
14:48 - the formula for conversion rate is
14:51 - conversion rate is equal to number of
14:53 - conversions divided to number of total
14:55 - visitors multiplied by 100% for example
14:59 - if a website has thousand visitors and
15:01 - 50 of them make a purchase the
15:04 - conversion rate would be equal to 50
15:06 - divide 2,000 multiply by 100% which
15:09 - gives us 5% this means that our
15:11 - conversion rate in this case is equal to
15:14 - 5% conversion rate is an important
15:17 - metric because it allows us and
15:19 - businesses to measure the effectiveness
15:21 - of their website a product or a
15:23 - marketing campaign it can help
15:25 - businesses to identify areas for
15:27 - improvement such as increasing the
15:28 - number of conversions or improving the
15:30 - user experience conversion rate can be
15:33 - used for different purposes for example
15:35 - if a company wants to measure the
15:37 - effectiveness of an online store the
15:39 - conversion rate would be the percentage
15:40 - of visitors who make a purchase and on
15:43 - the other hand if a company wants to
15:45 - measure the effectiveness of landing
15:47 - page the conversion rate would be the
15:49 - percentage of visitors who fill out a
15:51 - form or sign up for a service so if the
15:53 - answer to the metric validity question
15:55 - is higher engagement then you can use
15:57 - the clickr rate or CTR as your primary
16:01 - metric this is by the way a common
16:03 - metric used in a testing whenever we are
16:05 - dealing with e-commerce product search
16:07 - engine recommander system clickr rate or
16:11 - CTR is a metric that measures the
16:13 - effectiveness of a digital marketing
16:15 - campaign or the user engagement or some
16:17 - feature on your web page or your website
16:20 - and it's typically used to determine the
16:21 - percentage of users who click on a
16:24 - specific link or button or call to
16:26 - action CTA out of the total to number of
16:29 - users who view it the formula for the
16:31 - clickr rate can be represented as
16:33 - follows so the CTR is equal to number of
16:36 - clicks divided to number of Impressions
16:38 - multiply by 100% not to be confused with
16:41 - click through probability because there
16:43 - is a difference between the click
16:44 - through rate and click through
16:46 - probability for example if an online
16:48 - advertisement receives thousand of
16:50 - Impressions which means that we are
16:51 - showing it to the customers for a
16:53 - thousand times and there were 25 clicks
16:56 - which means 25 out of all this
16:58 - impression resulted in clicks this means
17:00 - that the clickr rate for this specific
17:02 - example would be equal to 25 divide
17:05 - 2,000 multiply by 100% which gives us
17:09 - 2.5% this means that for this particular
17:11 - example our clickr rate is equal to
17:14 - 2.5% cure rate is an important metric
17:17 - because it allows businesses to measure
17:19 - the effectiveness of their digital
17:20 - marketing campaigns and the user
17:22 - engagement with their website or web
17:24 - pages High click through rate indicates
17:27 - that a campaign or the web page or
17:29 - feature is relevant and appealing to the
17:31 - target audience because they are
17:33 - clicking on it while low clickthrough
17:35 - rate indicates that a campaign or the
17:37 - web page needs an improvement click
17:39 - through rate can be used to measure the
17:41 - performance of different digital
17:42 - marketing channels such as PID search
17:45 - display advertising email marketing and
17:48 - social media it can also be used to
17:50 - measure the performance of different ad
17:52 - formats such as text advertisements
17:54 - Banner advertisement video
17:56 - advertisements Etc
18:00 - next and the final task in this first
18:01 - step in the process of AP testing is to
18:04 - State the statistical hypothesis based
18:06 - on business hypothesis and the chosen
18:08 - primary
18:09 - metric next and in the final task in
18:12 - this first step of the AB testing
18:13 - process we need to State the statistical
18:15 - hypothesis based on the business
18:17 - hypothesis we stated and the chosen
18:19 - primary metric in the section of
18:22 - fundamentals through statistics of this
18:23 - course in lecture number seven we went
18:26 - into details about statistical
18:27 - hypothesis testing included what n
18:29 - hypothesis is and what alternative
18:32 - hypothesis is so do have a look to get
18:34 - all the insight about this topic AB
18:37 - testing should always be based on a
18:39 - hypothesis that needs to be tested this
18:42 - hypothesis is usually set as a result of
18:44 - brainstorming and collaboration of
18:46 - relevant people on the product team and
18:49 - data science team the idea behind this
18:51 - hypothesis is to decide how to fix a
18:54 - potential issue in a product where a
18:56 - solution of these problems will
18:58 - influence the key performance indicators
19:00 - or the kpi of interest it's also highly
19:03 - important to make prioritization out of
19:06 - a range of product problems and ideas to
19:08 - test while you want to P that fixing
19:10 - this problem would result in the biggest
19:13 - impact for the
19:15 - product we can put the hypothesis that
19:18 - is subject to rejection so that we want
19:19 - to reject in the ideal World Under The N
19:22 - hypothesis what we Define by AG zero
19:25 - well we can put the hypothesis subject
19:27 - to acceptance so the desire hypothesis
19:29 - that we would like to have as a result
19:31 - of AB testing under the alternative
19:33 - hypothesis defined by
19:35 - H1 for example if the kpi of the product
19:38 - is to increase the customer engagement
19:40 - by changing the color of the read more
19:42 - button from blue to green then under the
19:45 - N hypothesis we can state that clickr
19:48 - rate of learn more button with blue
19:50 - color is equal to the click through rate
19:51 - of green button under the alternative we
19:54 - can then state that the click true rate
19:56 - of the learn more button with green
19:57 - color is Lar larger than the click
19:59 - through of the blue
20:03 - button so ideally want to reject this no
20:06 - hypothesis and we want to accept the
20:08 - alternative hypothesis which will mean
20:10 - that we can improve the clickr rate so
20:13 - the engagement of our product by simply
20:15 - changing the color of the button from
20:17 - blue to green once we have set up the
20:20 - business hypothesis selected the primary
20:22 - metrics and stated the statistical
20:24 - hypothesis we are ready to proceed to
20:26 - the next stage in the ab testing
20:30 - process in this lecture we will discuss
20:33 - the next Second Step In AB testing
20:35 - process which is designing the ab test
20:37 - including the power analysis and
20:40 - calculating the minimum sample sizes for
20:42 - the control and experimental groups stay
20:45 - tuned as this is a very important part
20:47 - of AB testing process commonly appearing
20:49 - during the data science interviews some
20:52 - argue that AB testing is an art and
20:55 - others say that it's a business adjusted
20:57 - common statistical test but the
20:59 - borderline is that to properly Design
21:01 - This experiment you need to be
21:03 - disciplined and intentional while
21:04 - keeping in mind that it's not really
21:06 - about testing but it's about learning
21:09 - following AR steps you need to take to
21:11 - have a solid design for your ab test so
21:14 - let's bring the diagram back so in this
21:16 - step we need to perform the power
21:18 - analysis for our AB test and calculate
21:20 - the minimum sample size in order to
21:22 - design our AB
21:24 - test AB test design includes three steps
21:27 - the first step is power analysis which
21:30 - includes making assumptions about model
21:32 - parameters including the power of the
21:34 - test the significance level Etc the
21:37 - second step is to use these parameters
21:39 - from Power analysis to calculate the
21:41 - minimum sample size for the control and
21:43 - experimental groups and then the final
21:46 - third step is to decide on the test
21:48 - duration depending on several factors so
21:51 - let's discuss each of these topics one
21:53 - by
21:54 - one power analysis for AB testing
21:57 - includes this tree specific specific
21:58 - steps the first one is determining the
22:01 - power of the test this is our first
22:03 - parameter the power of the statistical
22:05 - test is a probability of correctly
22:07 - rejecting the N hypothesis power is the
22:10 - probability of making a correct decision
22:12 - so to reject the N hypothesis when the N
22:15 - hypothesis is false if you're wondering
22:18 - what is the power of the test what is
22:20 - this different concepts that we just
22:21 - talk about what is this null hypothesis
22:23 - and what does it mean to reject the null
22:25 - hypothesis then head towards the
22:27 - fundamental statistic section of this
22:28 - course as we discuss this topic in
22:31 - detail as part of that
22:35 - section the power is often defined by 1
22:38 - minus beta which is equal to the
22:40 - probability of not making a type two
22:42 - error where type two error is a
22:44 - probability of not rejecting the null
22:46 - hypothesis while the null is actually
22:48 - false it's common practice to pick 80%
22:51 - as the power of the ab test which means
22:53 - that we allow 20% of type to error and
22:56 - this means that we are fine with not
22:58 - detecting so failing to reject n
23:00 - hypothesis 20% of the time which means
23:03 - that we are fine with not detecting a
23:05 - true treatment effect while there is an
23:07 - effect which means that we are failing
23:09 - to reject the N however the choice of
23:12 - value of this parameter depends on
23:14 - nature of the test and the business
23:17 - constraints secondly we need to
23:19 - determine a significance level for our
23:21 - AB test the significance level which is
23:24 - also the probability of type one error
23:26 - is the likelihood of rejecting the no
23:28 - hence detecting a treatment effect while
23:30 - the know is actually true and there is
23:32 - no statistically significant impact this
23:35 - value often defined by a Greek letter
23:36 - Alpha is a probability of making a false
23:39 - Discovery often referred to as a false
23:41 - positive rate generally we use the
23:43 - significance level of 5% which indicates
23:46 - that we have 5% risk of concluding that
23:48 - there exists a statistically significant
23:50 - difference between the experimental and
23:52 - control variant performances when there
23:54 - is no actual difference so we are fine
23:56 - by having five out of 100 cas Cas is
23:58 - detecting a treatment effect well there
24:00 - is no effect it also means that you have
24:02 - a significant result difference between
24:04 - the control and the experimental groups
24:06 - within 95% confidence like in the case
24:10 - of the power of the test the choice of
24:12 - the alpha is dependent on the nature of
24:14 - the test and the business constraints
24:15 - that you have for instance if running
24:18 - this a test is related to high
24:20 - engineering course then the business
24:22 - might decide to pick a high offer such
24:24 - that it would be easier to detect a
24:26 - treatment effect on the other hand the
24:28 - implementation costs of the proposed
24:30 - version in production are high you can
24:33 - then pick a lower significance level
24:35 - since this proposed feature should
24:37 - really have a big impact to justify the
24:39 - high implementation cost so it should be
24:41 - harder to reject n
24:44 - hypothesis finally as the last tyep of
24:47 - power analysis we need to determine a
24:49 - minimum detectable effect for the
24:51 - test last parameter as part of the power
24:54 - analysis we need to make assumptions
24:55 - about is what we call minimum detectable
24:58 - effect or Delta from the business point
25:00 - of view so what is the substantive to
25:02 - the statistical significance that the
25:04 - business wants to see as a minimum
25:06 - impact of the new version to find this
25:08 - variant investment
25:11 - worthy the answer to this question is
25:13 - what is the amount of change we aim to
25:15 - observe in a new versions metric
25:17 - compared to the existing one to make
25:19 - recommendations to the business that
25:21 - this feature should be launched in the
25:23 - production that it's investment worthy
25:26 - an estimate of this parameter is what is
25:27 - known as as a minimum detectable effect
25:30 - often defined by a Greek letter Delta
25:32 - which is also related to the Practical
25:34 - significance of the test so this mde or
25:36 - the minimum detectable effect is a proxy
25:39 - that relates to the smallest effect that
25:40 - would matter in practice for the
25:42 - business and it's usually set by
25:44 - stakeholders as this parameter is highly
25:46 - dependent on the business there is no
25:48 - common level of it instead so this
25:51 - minimum detectable effect is basically
25:53 - the translation from statistical
25:55 - significance to the Practical
25:56 - significance and here we want to see and
25:59 - we want to answer the question what is
26:00 - this percentage increase in the
26:02 - performance of the product that we want
26:04 - to experiment with that will tell to the
26:06 - business that this is good enough to
26:08 - invest in this new feature or in this
26:10 - new product and this can be for instance
26:12 - 1% for one product it can be 5% for
26:15 - another one and it really depends on the
26:17 - business and what is the underlying
26:22 - kpi a popular reference to the
26:24 - parameters involved in the power
26:25 - analysis for AB testing is like this so
26:28 - 1 minus beta for the power of the test
26:31 - Alpha for the significance level Delta
26:33 - for the minimum detectable effect to
26:36 - make sure that our results are
26:37 - repeatable robust and can be generalized
26:39 - to the entire population we need to
26:41 - avoid P hacking to ensure real
26:44 - statistical significance and to avoid
26:46 - biased results so we want to make sure
26:48 - that we collect enough amount of
26:50 - observations and we run the test for a
26:52 - minimum predetermined amount of time
26:55 - therefore before running the test we
26:57 - need to determine the samp size of the
26:58 - control and experimental groups as well
27:00 - as later on in this lecture we will see
27:02 - also how long we need to run the test so
27:05 - this is another important part of AB
27:07 - testing which needs to be done using the
27:10 - defined power of the test which was the
27:12 - one minus beta the significance level
27:14 - and a minimum detectable effect so all
27:16 - the parameters that we decided upon when
27:19 - conducting the power
27:21 - analysis calculation of the sample size
27:24 - depends on the underlying primary metric
27:26 - as well that you have chosen for
27:28 - tracking the progress of the control and
27:30 - experimental versions of the product so
27:32 - we need to distinguish here two
27:34 - cases so when discussing the primary
27:36 - metric we saw that there are different
27:38 - ways that we can measure the performance
27:40 - of different type of products if we are
27:42 - interested in engagement then we are
27:44 - looking at a metric such as click
27:46 - through rate which is in the form of
27:48 - averages so the case one will be where
27:50 - the primary metric of AB testing is in
27:52 - the form of a binary variable it can be
27:55 - for instance conversion or no conversion
27:58 - click or no click and in case two where
28:01 - the primary metric of the test is in the
28:03 - form of proportions or averages which
28:05 - means mean order amount or mean click
28:08 - through
28:09 - rate for today we will be covering only
28:12 - one of these cases but you can find more
28:14 - details on the second case in my blog
28:17 - which I posted also as part of the
28:18 - resources section this blog post
28:21 - contains all the details that you need
28:23 - to know about AB testing including the
28:25 - statistical test and their corresponding
28:27 - hypothesis the descriptions of different
28:29 - primary metrics that go beyond what we
28:31 - have covered as part of this section as
28:33 - well as many more details that you need
28:35 - to know about a
28:38 - testing so let's look at a case two
28:40 - where the primary metric of the test is
28:42 - in the form of proportions or averages
28:44 - so let's say we want to test whether the
28:46 - average click to rate of control is
28:48 - equal to the average click to rate of
28:49 - experimental group and under HD we have
28:53 - that the m control is equal to M
28:54 - experimental and under H1 we have that
28:57 - the m control is not to Mu experimental
28:59 - so here the MU control and mu
29:01 - experimental are simply the average of
29:03 - the primary metric for control group and
29:05 - for the experimental group respectively
29:08 - so this the formal hypothesis we want to
29:10 - test with our AB test and we can assume
29:13 - that this new control is for instance
29:15 - the clickr rate of the control group and
29:17 - the MU experimental is the clickr rate
29:19 - of the experimental
29:21 - group so this is the formal statistical
29:24 - hypothesis we want to test with our AB
29:26 - test if you haven't done so I would
29:28 - highly suggest you to head towards the
29:30 - fundamental statistic section of this
29:32 - course where in lecture number seven and
29:34 - eight of the statistical part of this
29:36 - course I go in detail about statistical
29:38 - hypothesis testing the means averages
29:41 - significance level Etc this also holds
29:44 - for the theorem that the some prise
29:46 - calculation is based upon called Central
29:48 - limit theorem so check out the last
29:51 - lecture about inferential statistics
29:53 - where I covered the central limit
29:55 - theorem which we will also use in this
29:57 - section and finally also check the
30:00 - lecture number five in that section
30:02 - where we cover the normal distribution
30:04 - another thing that we will use as part
30:07 - of this section so the central limit
30:09 - theorem states that given a sufficiently
30:11 - large sample size from an arbitrary
30:13 - distribution the sample mean will be
30:15 - approximately normally distributed
30:17 - regardless of the shape of the original
30:18 - population distribution this means that
30:20 - the distribution of the sample means
30:22 - will be approximately normal if we take
30:24 - a large enough sample even if the
30:26 - distribution of the orig sample is not
30:29 - normal so when we are dealing with a
30:31 - primary performance tracking metric that
30:33 - is in the form of average such as this
30:35 - one that we are covering today which is
30:36 - a clickr rate we intend to compare the
30:39 - means of the control and experimental
30:40 - groups then we can use the central limit
30:43 - theorem as state that the mean sampling
30:45 - distribution of both controlling
30:46 - experimental groups follow normal
30:49 - distribution consequently the sampling
30:51 - distribution of the difference of the
30:52 - means of these two groups also will be
30:55 - normally distributed
30:59 - so this can be expressed like this where
31:01 - we see that the mean of the control
31:02 - group and mean of the experimental group
31:04 - follows normal distribution with mean mu
31:07 - control and mu experimental respectively
31:09 - and then with the variance of Sigma
31:11 - control squared and sigma experimental
31:13 - squared respectively though derivation
31:16 - of this Pro is out of the scope of this
31:17 - course we can state that the difference
31:19 - between the means of the true group so
31:21 - xar control minus xar experimental also
31:25 - follows normal distribution with a mean
31:26 - new control minus new experimental and
31:29 - with a variance of Sigma control squ /
31:31 - to n Control Plus Sigma experimental
31:33 - Square / to n experimental so the sample
31:36 - size of the experimental group and the
31:38 - sample size of the control group hence
31:41 - the sample size needed to compare the me
31:43 - of the two normally distributed samples
31:45 - using a two-sided test which prespecify
31:47 - significance of alpha power level and
31:50 - minimum detectable effect can be
31:52 - calculated as
31:54 - follows so here you can see the
31:56 - mathematical representation of the
31:58 - minimum sample size so the N which
32:00 - stands for the minimum sample size is
32:02 - equal to and in denominator we have Sig
32:04 - control S Plus Sigma experimental squar
32:07 - multip by z 1us alpha / to 2 + z 1us
32:11 - beta squ / to the Delta squ and here the
32:14 - Alpha and the beta and the Delta we have
32:17 - made assumptions about as part of the
32:18 - power analysis and the sigma control
32:21 - squar and a sigma experimental squared
32:23 - are the uh estimates of the variance
32:25 - that we can come up with using the
32:27 - So-Cal A8 testing I would say you do not
32:30 - necessarily need to know this derivation
32:32 - as there are many online calculators
32:34 - that will ask you for the alpha the beta
32:36 - and the Delta values as well as the
32:38 - sample estimates for the sigma squ
32:40 - control and experimental and then these
32:42 - calculators will automatically calculate
32:44 - the minimum S size for you if you're
32:46 - wondering what this AA testing is and
32:48 - how we can come up with the sigma
32:49 - control squared and sigma experimenting
32:51 - squared as well as all the other values
32:53 - then make sure to to check out the blog
32:55 - that I posted before and that I
32:57 - mentioned before as I explained in
32:59 - detail all these values as well as check
33:01 - out the resource section where I've
33:02 - included many resources regarding this
33:05 - but for now just keep in mind that the
33:07 - Z1 minus Alpha / to two and Z1 minus
33:10 - beta are just two constants and come
33:12 - from the normal distributed and standard
33:14 - normal distributed tables I would say
33:16 - you do not necessarily need to know this
33:18 - derivation as there are many online
33:20 - calculators that will ask you for this
33:22 - Alpha Beta And Delta values as well as
33:24 - the sule estimates for the sigma squ
33:26 - controling Sigma experimental control
33:29 - and then we'll calculate automatically
33:30 - the sample size for you for the control
33:32 - and experimental group
33:36 - effectively one example of such
33:38 - calculator is this AB testy online
33:40 - calculator but if you Google it you will
33:42 - find many others that will ask you for
33:44 - the minimum detectable effect for the
33:46 - statistical significance or the
33:48 - statistical power and then it will
33:50 - automatically calculate for you the
33:51 - minimum sample size that you should have
33:53 - in order to have a statistical
33:55 - significance and in order to have a
33:56 - valid AB test
33:58 - one thing to keep in mind is that you
33:59 - will notice that the statistical
34:01 - significance level is set to 95% in here
34:04 - which is not what we have seen when we
34:06 - were discussing the alpha significance
34:08 - level so sometimes these online
34:10 - calculators will confuse or will
34:12 - interchangeably use the significance
34:14 - level versus the confidence level which
34:16 - are the opposite so the significance
34:18 - level is usually at the level of 5% or
34:21 - 1% confidence level is around 95% so
34:24 - which is basically 100% minus the alpha
34:27 - therefore whenever whenever you see this
34:28 - 95% know that this means that your Alpha
34:31 - should be 5% so it's really important to
34:33 - understand how to use this calculator
34:35 - not to end up with the wrong minimum
34:36 - sample size conduct an entire AB test
34:39 - and then at the end realize that you
34:41 - have used the wrong uh significance
34:46 - level the final step is to calculate the
34:48 - test duration this question needs to be
34:50 - answered before you run your experiment
34:53 - and not during the experiment sometimes
34:55 - people stop the test when they detect
34:57 - statistical significance which is what
34:58 - we call P hacking and that's absolutely
35:01 - not what you want to do to to determine
35:03 - the Baseline of the duration time a
35:05 - common approach is to use this formula
35:07 - as you can see duration is equal to n
35:10 - ided to the number of visitors per day
35:12 - where n is your minimum sample size that
35:14 - we just calculated in the previous step
35:16 - and the number of visitors per day is
35:18 - the average number of visitors that you
35:20 - expect to see as part of your
35:23 - experiment for instance if this formula
35:26 - results in 14 days or 14 this suggest
35:28 - that running the test for two weeks is a
35:30 - good idea however it's highly important
35:33 - to take many business specific aspect
35:35 - into account when choosing the time to
35:38 - run the test and for how long you need
35:40 - to run it and simply using this formula
35:42 - is not enough for example if you want to
35:45 - run an experiment at the end of the
35:46 - month December with Christmas breaks
35:49 - when higher than expected or lower than
35:50 - expected number of people are usually
35:52 - checking your web page then this
35:54 - external and uncertain event had an
35:56 - impact on the page page to search for
35:58 - some businesses this
36:02 - means for example if you want to run an
36:05 - experiment at the end of the month of
36:06 - December with Christmas breaks when
36:08 - higher than expected or in some cases
36:11 - lower than expected number of people are
36:13 - usually checking the web page so
36:15 - depending on the nature of your business
36:16 - or the product then this external and
36:19 - uncertain event can have an impact on
36:21 - the page usage for some businesses which
36:24 - means that for some businesses a high
36:26 - increasing the page usage can be the
36:28 - result and for some a huge decrease in
36:30 - usability in this case running AB test
36:33 - without taking into account this
36:35 - external Factor would result in
36:37 - inaccurate results since the activity
36:39 - period would not be true representation
36:41 - of a common page usage and we no longer
36:44 - have this Randomness which is a crucial
36:46 - part of AB
36:47 - testing beside this When selecting a
36:50 - specific test duration there are few
36:52 - other things to be aware of firstly two
36:54 - small test duration might result in what
36:56 - we call novelty effects users tend to
36:59 - react quickly and positively to all
37:01 - types of changes independent of their
37:03 - nature so it's referred as a novelty
37:06 - effect and it vares of in time and it is
37:08 - considered illusionary so it would be
37:11 - wrong to describe this effect to the
37:12 - experimental version itself and to
37:14 - expect that it will continue to persist
37:16 - after the noble T effect wears off hence
37:19 - when picking a test duration we need to
37:21 - make sure that we do not run the test
37:23 - for too short amount of time period
37:25 - otherwise we can have a novelty effect
37:27 - novelty effect can be a major threat to
37:29 - the external validity of an AV test so
37:32 - it's important to avoid it as much as
37:35 - possible secondly if the test duration
37:38 - is too large then we can have what we
37:40 - call maturation effects when planning an
37:42 - AB test it's usually useful to consider
37:44 - a longer test duration for allowing
37:46 - users to get used to a new feature or
37:49 - product in this way one will be able to
37:52 - absorve the real treatment effect by
37:54 - giving more time to returning users to
37:56 - cool down from an initial positive
37:58 - reaction or a spike of Interest due to a
38:01 - change that was introduced as part of a
38:03 - treatment this should help to avoid
38:05 - novelty effect and is better predictive
38:07 - value for the test outcome however the
38:10 - longer the test period the larger is the
38:13 - likelihood of external effect impacting
38:15 - the reaction of the users and possibly
38:18 - contaminating the test results this is
38:21 - what we call maturation effect and
38:23 - therefore running the AP test for too
38:25 - short amount of time or too long amount
38:27 - of time is not recommended as it's a
38:30 - very involved topic we can talk for
38:32 - hours about this part of the ab test and
38:35 - also a topic that is asked a lot during
38:37 - the data science and product scientist
38:39 - interviews therefore I highly suggest
38:41 - you to check out this book about AB
38:43 - testing which is a Hands-On tutorial
38:45 - about everything you need to know about
38:47 - AB testing as well as check out the
38:49 - interview preparation guide in this
38:51 - section that contains 30 most popular AB
38:54 - testing related questions you can expect
38:56 - during your data science interviews so
38:58 - stay tuned and in the next couple of
39:00 - lectures we will cover the next stages
39:02 - of AB testing process if you are looking
39:05 - for one place to learn everything about
39:08 - AB testing without unnecessary
39:10 - difficulties but also with a good
39:13 - statistical and da Science Background
39:16 - then make sure to check out the AB
39:18 - testing course at lunch. a so if you
39:21 - want to learn all this background
39:23 - information including what is
39:24 - statistical significance what is AB
39:26 - testing how can AB testing be done and
39:29 - you want to have this endtoend AB
39:31 - testing course then make sure to check
39:34 - the AB testing for data science course
39:37 - at
39:38 - l. that's the only course that is
39:40 - available at the moment on the internet
39:43 - that covers the most fundamental concept
39:46 - of a testing including the theory and
39:48 - the implementation in Python without
39:51 - know the extra details and right going
39:54 - straight to the point in order to help
39:56 - you to Kickstart your Journey with AB
39:58 - [Music]
40:00 - Tes the resource that I would suggest
40:02 - you to keep by the hand is the blog
40:05 - called complete guide toab testing
40:06 - design implementation and pitfalls which
40:09 - is part of the Hands-On tutorials of the
40:11 - towards data science so in here and
40:13 - specifically this part where we are
40:15 - discussing the two sample that test I
40:17 - would suggest you to go through it as we
40:19 - are going to conduct this two sample Z
40:22 - test as part of our Python and we are
40:24 - going to learn how to implement this in
40:26 - Python in this book you can learn
40:28 - everything out there that you need to
40:30 - know about AV testing including
40:32 - different uh pits include of Av testing
40:35 - the process behind it how you can
40:37 - conduct the ab test end to end how you
40:39 - can calculate a sample size how you can
40:41 - choose a test the primary metric
40:43 - definitions different statistical test
40:46 - that you can use including the Ki Square
40:47 - test the two sample Z test and two
40:49 - sample T Test so given that as part of
40:52 - the lectures of the um AB testing and
40:55 - specifically lecture number five we have
40:57 - already discussed the two sample T Test
41:00 - and how to implement it I thought it
41:02 - would be more useful for you to know how
41:03 - to implement the two sample Z test such
41:05 - that you know both of them and you know
41:07 - their theory behind it and also how to
41:10 - implement them and finally if you are
41:12 - wondering how you can Implement them in
41:14 - Python then head towards my uh blog uh
41:17 - in the medium as well as my GitHub
41:19 - repository that I will post in the
41:20 - resource section where you can find all
41:22 - the different statistical tests you can
41:24 - use for analyzing your ab test results
41:27 - including the two sample T Test two
41:29 - sample Z test K Square test and much
41:31 - more so without further Ado let's get
41:33 - started with our
41:35 - demo so uh as you can see here I'm
41:38 - generating the data myself assuming that
41:41 - uh the uh primary metric follows bomal
41:43 - distribution so the output is in the
41:46 - form of zeros and ones because we are
41:48 - looking into the click event and click
41:50 - can be either zero or one and then I'm
41:53 - using here the binomial distribution to
41:55 - randomly sample from it and in case of
41:57 - the experimental version I'm using a
41:59 - probability of success equal to 0.4 and
42:02 - in case of the control version I'm using
42:03 - a probability of success equal to 0.2
42:06 - because I want to have a quiet
42:08 - difference between the two groups and
42:10 - then later on we can also adjust this
42:12 - and we can change the difference to see
42:14 - how our it has behaves so um I'll assume
42:18 - that um the uh at the end of the uh data
42:21 - generation process we have a data that
42:24 - is similar to the form that you will get
42:26 - from the uh engine engers once they uh
42:28 - finish up collecting all the data from
42:30 - your customers and I will also assume
42:32 - that the Integrity of theab test is held
42:36 - which means that the observations who
42:38 - were in the control group they only saw
42:40 - the control version of the product and
42:42 - observations who were in the
42:43 - experimental group they only saw the
42:44 - experimental version of the product and
42:47 - let's actually go ahead and see how the
42:49 - data looks
42:51 - like so so as you can see here we are
42:53 - generating our data so the data is in
42:56 - this format so you can see that we have
42:57 - an observation in total we have 20K
43:00 - observations because we have two
43:01 - different groups each with 10K
43:03 - observations and then the first col
43:05 - describes the click event so we will
43:07 - either have a click or we will have no
43:09 - click and the primary metric is in the
43:11 - form of a click so we are measuring the
43:13 - performance of the product both control
43:15 - and the experimental with the same
43:17 - metric which is whether there is a click
43:18 - event or no click event and the primary
43:20 - metric is in the form of a binary
43:22 - variable so we have either zeros or we
43:24 - have ones Whenever there is a click then
43:27 - the corresponding value is one whenever
43:28 - there is no click then the corresponding
43:30 - value is zero and then we have the
43:32 - corresponding Group which helps us to
43:34 - understand whether the observation
43:36 - belongs to the experimental group so X
43:38 - or the control group which is a uh Co so
43:42 - uh this is how the data looks like and
43:43 - this also what you can uh expect from uh
43:46 - data Engineers uh once the uh AB test is
43:49 - conducted so you have run your ab test
43:51 - and Engineers have collected data
43:53 - assuming that the data Integrity has
43:55 - been kept and also that there was no
43:57 - systematic error when collecting and
43:59 - measuring the performance of the uh
44:01 - control and the experimental versions of
44:03 - the product first thing that we are
44:05 - going to do is to estimate the P hat
44:08 - control and a p hat experimental and for
44:11 - that what we need to do first is to
44:13 - count the number of clicks per group so
44:16 - we saw earlier that we have this data
44:18 - that we generate ourselves consisting of
44:20 - 20 K rows where 10 belongs to the uh
44:24 - control group and the 10K belongs to the
44:26 - experimental group and each consists of
44:28 - this click variable and the group The
44:30 - Click variable is an indicator uh that
44:33 - says that the observation clicked on the
44:36 - uh page versus uh not clicked on the
44:38 - page so whenever there was a click we
44:40 - have here one whenever there was no
44:42 - click we have here zero and then we have
44:44 - the corresponding uh group such that we
44:46 - can use to group this data based on the
44:48 - control versus experimental group and
44:51 - that's exactly what we are going to do
44:53 - as the first step in our process so we
44:55 - are going to calculate the number of
44:58 - total clicks for control group and for
45:00 - the experimental
45:02 - group so here we are making use of the
45:04 - function Group by in order to group this
45:07 - data frame so this data frame based on
45:09 - the group and then we want to click uh
45:12 - the we want to get the uh click variable
45:15 - and we want to sum this variable because
45:18 - the variable is of a binary nature so we
45:20 - have ones and zeros if we do the sum we
45:23 - are basically counting the number of
45:25 - times we have the uh observation click
45:29 - equal to one so by summing a binary
45:32 - variable we are simply getting the
45:34 - number of ones in that variable and
45:36 - that's exactly what we are doing in this
45:38 - part and then what is remaining is to
45:41 - get the uh number of clicks from control
45:44 - group and number of clicks from the
45:46 - experimental group by using this
45:47 - function code look so we saw earlier
45:50 - when we were discussing the um accessing
45:52 - of observations in a pendis data frame
45:55 - that there is a difference between iog
45:57 - and loog and the reason why we are using
45:59 - here the loog is because the uh group uh
46:02 - data that we are getting in here it will
46:05 - provide us an output where the index is
46:08 - in the format of a string so let's
46:10 - actually go ahead and print that part
46:12 - because I think it's an important part
46:14 - to see how the data looks
46:16 - like and it also will make sense why I'm
46:19 - using here the look function to access
46:21 - the uh control groups number of clicks
46:23 - and the experimental groups number of
46:25 - clicks
46:32 - so this is the uh group data frame that
46:35 - we are getting as you can see we are
46:36 - getting here the group and here we are
46:38 - getting for the control uh index the
46:41 - number of clicks is equal
46:44 - 2,924 and for the experimental group
46:46 - it's equal to
46:49 - 5,7 so then the next thing what we need
46:52 - to do is actually access this value and
46:54 - for that we need to specify that we want
46:56 - to access the value corresponding to the
46:59 - index equal to control and this can be
47:02 - done by using this log function so you
47:05 - cannot use ilog or any other way of
47:07 - accessing this because the index is of
47:09 - string type and therefore we are using
47:11 - the log so let's actually also add some
47:14 - print statements to make our code more
47:18 - readable so this will then print the
47:20 - number of clicks per control group and
47:23 - per experimental group
47:27 - here we go so as you can see we are
47:29 - nicely accessing the correct values then
47:33 - the next step is to calculate the P had
47:36 - control and the P experimental so
47:39 - basically the estimate of the click
47:40 - probabilities of the control group and
47:42 - the experimental group respectively and
47:45 - for that we just need to take the uh
47:47 - number of clicks and we need to divide
47:49 - it to the number of observations for
47:51 - that
47:53 - group so it is this part let's go ahead
47:57 - and calculate those values so as you can
47:59 - see I'm taking the number of clicks that
48:01 - we just obtained and I'm dividing it to
48:03 - the number of observations that we have
48:05 - defined in the very
48:13 - beginning here we go so as you can see
48:16 - for the control group the uh click
48:18 - probability is equal to
48:20 - 020 and in case of the experimental
48:23 - group is equal to 0.5 so we see that
48:26 - there is a large difference between the
48:28 - quick probability for these two groups
48:31 - which is um a reflection of what we saw
48:34 - here because we generated the data such
48:37 - that the uh success probability for the
48:40 - experimental group is equal to
48:42 - 0.5 and the um for the control group is
48:45 - equal to 0.2 so we see these numbers
48:47 - reflecting also in here and the reason
48:50 - for that is because we have sampled our
48:52 - data large enough and we see that the um
48:55 - probability so the the mean of our
48:58 - sample um converges in a probability to
49:02 - the mean that we use and this is also
49:05 - the idea behind the low of large num
49:07 - something that we have also discussed as
49:09 - part of the fundamental to statistic
49:11 - section of this
49:12 - course so the next thing what we need to
49:15 - do is to compute the P poed hat or the
49:19 - uh estimate of the pulled success
49:22 - probability and we saw uh when we were
49:25 - discussing the theory behind it that
49:26 - it's equal to the sum of the clicks for
49:29 - both control and experimental group
49:31 - divided to the total number of
49:32 - observations in both control and the
49:34 - experimental group so
49:37 - basically the P pulled head is equal to
49:42 - xcore control plus xcore experimental
49:46 - ided to the ncore Control Plus ncore
49:54 - experimental then the next thing we need
49:56 - to do is to compute a pulled variance
49:59 - and we just so that the pulled variance
50:01 - can be calculated by taking the pulled
50:04 - uh estimate for the click probability so
50:06 - this p p and then multiply by one minus
50:10 - p p head and then multiply by the
50:12 - inverses of the uh observ number of
50:15 - observations in each of the groups and
50:17 - there sum so 1 / 2 N Control Plus 1/ to
50:20 - an experimental so it can be calculated
50:24 - as follows
50:27 - so pulled variance then is equal to P
50:29 - ped head multip by 1 minus P ped head
50:33 - multip by 1 / to n control + 1 / to n
50:38 - experimental let's also add some print
50:46 - statements here we
50:49 - go and then the next step is to
50:51 - calculate the standard error so the
50:53 - standard error is the square root of the
50:55 - pulled variance so quite straightforward
50:58 - and here we are going to make use of the
51:00 - npy function so the SE is equal to npy
51:04 - Dot and a square roof is simply uh
51:07 - calculated by using the function sqrt
51:10 - which stands for square roof and then
51:12 - here we need to mention the pulled
51:14 - variance let's also add the print
51:17 - statement explaining the uh the code and
51:21 - this really can help your reviewer the
51:22 - code reviewer to understand what you are
51:24 - doing
51:27 - okay so now we have also the standard
51:28 - eror and now we are ready to calculate
51:31 - our test statistics so we saw that the
51:33 - test statistics is equal to the P
51:36 - control head minus P experimental head
51:38 - divided to the standard error and that's
51:41 - exactly what we are going to implement
51:43 - in
51:44 - here so as you can see the test
51:47 - statistics is equal to P control head
51:49 - minus P experimental head divided 2D SC
51:52 - so standard
51:53 - error and then finally what we need to
51:56 - do is to compute the Z critical value
51:58 - the P value and the confidence interval
52:01 - but for doing that we need to assume the
52:04 - significance level so usually this is
52:06 - done before conducting the test but here
52:08 - I'm assuming that before conducting the
52:10 - test there was a power analysis and as
52:12 - part of that we have decided that the
52:14 - statistical significance level is equal
52:16 - to
52:17 - 5% so let's add that here so Alpha is
52:19 - equal to 0.05 therefore we are going to
52:22 - use this specific Alpha so 5% in order
52:25 - to calculate our critical value coming
52:27 - from the normal table and to do this
52:29 - there are uh various options so one way
52:32 - of doing that is to hard code the value
52:34 - which I would not recommend but it is
52:35 - definitely uh an easy way to go if you
52:38 - um haven't used uh the python libraries
52:41 - to automize this process but here I will
52:44 - provide you the code and I will also
52:45 - tell you how you can use the scipi norm
52:49 - um function in order to calculate the
52:51 - critical volume and I think keeping the
52:53 - code as general as possible will help
52:55 - you in the term to because it can be
52:58 - that this time you're calculating the
52:59 - critical value corresponding to Alpha is
53:01 - equal to 0.05 but maybe next time you
53:03 - want to calculate the critical value
53:06 - when your Alpha is equal to 1% so you're
53:09 - interested in the uh case when your type
53:11 - one probability is equal to 1% so for
53:14 - those cases uh you want to keep your
53:17 - code as general as possible such that by
53:19 - changing your uh variable let's say
53:21 - Alpha you don't need to go each time and
53:24 - then in the chat GPT look for the a
53:26 - corresponding uh value coming from the
53:28 - standard normal table so for this what
53:32 - we are going to use is the norm function
53:35 - so the norm function come from the CPI
53:37 - stats library for that we need to import
53:39 - from ci. stats the norm function which
53:42 - stand for the normal distribution so in
53:44 - here what we need to use is the function
53:47 - called ppf which is the uh percentage
53:49 - Point
53:52 - function so the norm done ppf function
53:54 - stands for the present Point function
53:56 - and it's usually known as the inverse
53:58 - cumulative distribution function or the
54:00 - CDF of the standard normal distribution
54:03 - and it takes as an input the probability
54:05 - value and it Returns the corresponding
54:07 - value on the xaxis of the CDF once you
54:10 - provide a p so here we are providing the
54:11 - P which is equal to 1 minus Alpha / to 2
54:14 - then this function calculates the X so
54:16 - the xaxis such that the probability of
54:18 - observing a volue less than or equal to
54:21 - two or 2 x in a standard normal
54:23 - distribution is equal to P so we have
54:26 - this inverse CDF and we have the xaxis
54:28 - and we have the y-axis on the y- axis we
54:30 - have the probabilities and on the x-axis
54:32 - we have the X values so here what we are
54:34 - basically doing is that we are providing
54:36 - the probability that we have which is
54:38 - equal to 1 minus Alpha / to 2 and we
54:40 - want to know the corresponding X valume
54:43 - therefore it's also called inverse
54:44 - humity distribution function and in this
54:47 - way we can calculate Z critical value
54:49 - which can help us to identify the place
54:51 - where we need to have our rejection
54:53 - region and so here is the uh rejection
54:56 - region of this test and as you can see
54:58 - we have two-sided test therefore we have
55:01 - also a two regions and whenever the um
55:04 - test statistics is larger than the
55:06 - critical value in the right hand side
55:08 - and it is smaller than the critical
55:09 - value from the left hand side then we
55:11 - are saying that we can reject the N
55:14 - hypothesis therefore it's also called
55:15 - the rejection
55:17 - region so uh once we calculate this set
55:20 - critical value we are ready to go to the
55:22 - next step but before that let's also add
55:24 - some statement print statement for
55:25 - readability
55:30 - here so the next step is to calculate a
55:33 - p volume and a p volue can be calculated
55:35 - by using the norm. SF function so the
55:38 - norm function comes once again from the
55:40 - scipi library and the SF stands for
55:43 - survival
55:47 - function the norm. SF function stands
55:50 - for a survival function and it stands
55:52 - for the complement of the CDF function
55:55 - so the cumulative distribution fun
55:56 - function of the standard normal
55:57 - distribution it calculates the
55:59 - probability of observing a value greater
56:01 - than a given
56:02 - threshold so in this case we want to
56:05 - calculate the uh probability that our
56:08 - test statistics will be smaller than
56:09 - equal the critical volume and as we saw
56:12 - that the standard normal distribution
56:13 - was symmetric here we are multiplying
56:16 - just one side of that probability by two
56:18 - in order to obtain our final value so
56:21 - here once we run this test we will
56:24 - finally get our P value and as you can
56:27 - see here the P value of the two sample
56:29 - that test that we got is equal to zero
56:32 - well now once we have the P value and
56:34 - also we know what is our Alpha we are
56:36 - ready to test for the statistical
56:37 - significance of our results so given
56:40 - that our P value is equal to zero and
56:42 - it's smaller than 0.05 so our Alpha we
56:45 - can state that the null hypothesis can
56:47 - be rejected and we can state that there
56:49 - is a statistically significant
56:51 - difference between our experimental
56:53 - version of the product and the control
56:55 - version of the product
56:57 - so this will help us to test for the
57:00 - statistical significance of our AB test
57:04 - however if you were for instance to have
57:06 - a different samples so let's say we
57:08 - would compute uh we would randomly
57:10 - sample from the binomial
57:14 - distribution so as you can see once we
57:16 - are getting the uh probability of the
57:18 - success the same for the two groups then
57:21 - the P value becomes large at least much
57:23 - larger than the alpha which means that
57:25 - we can no longer reject the ne
57:27 - hypothesis and we can no longer State
57:29 - there is a statistical evidence at the
57:30 - 5% statistical level that the control
57:33 - version is statistically significantly
57:35 - different from the experimental version
57:37 - and this uh verifies that everything
57:39 - that we have done here is correct so the
57:42 - ab test results analysis is accurate now
57:45 - the question is whether we um also have
57:48 - a practical significance once we pass
57:50 - the statistical significance test so
57:52 - let's move this back to what we had
57:54 - before so this is your
57:57 - .5 and once again the P value is just
58:00 - zero and let's go ahead and calculate
58:03 - our confidence interval such that we can
58:05 - test for the Practical significance and
58:07 - we can comment on the accuracy of the
58:09 - test and the general ability of our AB
58:12 - test so we saw that the confidence
58:14 - interval can be calculated as
58:16 - follows so we have the difference
58:19 - between the P had experimental and the P
58:22 - had control and then for the lower bound
58:24 - we need to uh subtract from this
58:26 - standard or multiply by Z critical value
58:29 - and then for the upper bound we need to
58:30 - do the same only with summing the
58:32 - standard multiply by Z critical volume
58:35 - so the difference here you might notice
58:37 - is this round function and the reason
58:39 - why I'm adding this is because I want to
58:41 - have nice numbers that will be rounded
58:43 - uh just three numbers after to decimal
58:45 - instead of having long uh floating
58:48 - numbers so once we go ahead and print
58:51 - this confidence interval we can also see
58:54 - the lower bound and upper bound in
58:57 - numbers here we go so as you can see we
59:00 - are getting a confidence info which is
59:02 - quite narrow so this is a suggestion
59:05 - that our AB test results are most likely
59:07 - accurate and that the Precision of our
59:10 - AB test is high and this is a good sign
59:13 - because then we can say that the ab test
59:15 - we have conducted in here is most likely
59:17 - generalizable to the entire
59:19 - population then the next question is
59:22 - okay do we have a practical significance
59:24 - or not and for that we do need the final
59:27 - assumption regarding the minimum
59:29 - detectable effect so let's say during
59:32 - the power analysis before conducting our
59:34 - AB test we got an mde which or let's
59:37 - actually call it Delta let's keep the
59:39 - Greek letters uh and the Delta let's say
59:41 - is equal to 3% so
59:45 - 0.03 well in this case we can notice
59:48 - that the Delta 0 03 so 3% is much lower
59:53 - than the lower bound of our confidence
59:55 - interval which is equal to 30% so
59:58 - 29.7% this means that in that case we
60:01 - would have said that there is a
60:02 - practical significance also but if the
60:05 - uh Delta would have been for instance
60:07 - the uh 0.31 so we have a 31% Delta then
60:12 - in that case the Delta is no longer
60:14 - smaller than the lower bound of our
60:16 - confidence interval and in that case we
60:18 - cannot say that our results are also
60:20 - practically significant so depending on
60:22 - the business and depending on the
60:24 - Assumption regarding the Del or the
60:26 - minum detectable effect we can then
60:28 - compare this to the lower bound of the
60:30 - confidence interval and we can State
60:32 - whether there is a practical
60:33 - significance or not in case there is a
60:35 - practical significance then we are good
60:38 - to go so we can say that we have a
60:39 - statistical significance we have a
60:41 - practical significance and we also have
60:43 - a narrow confidence interval which is a
60:45 - suggestion that our results are also
60:47 - generalizable and accurate so uh this
60:51 - completes our uh AB test results
60:53 - analysis and this is all that you need
60:55 - to do in order to have a valid and uh
60:58 - good quality AB test looking to elevate
61:01 - your data science or data analytics
61:03 - portfolio then you are in the right
61:05 - place with this AB testing and Trend
61:08 - case study you can showcase your AB
61:10 - testing and coding skills in one place
61:13 - I'm T Vasan data scientist and AI
61:16 - professional and I'm the co-founder of
61:18 - lunar Tech where we are making data
61:20 - science and AI accessible to everyone
61:23 - individuals businesses and
61:27 - institutions in this case study we are
61:29 - going to complete an endtoend case study
61:32 - with AB testing where we are going to
61:34 - test in a datadriven way whether it's
61:36 - worth to change one of our features in
61:39 - our ux design in the lunar text landing
61:41 - page this is a real life data science
61:45 - case study that you can conduct and you
61:47 - can put it on your resume in order to
61:50 - Showcase your experience in datadriven
61:52 - decision making where you will showcase
61:55 - your statistic skills experimentation
61:57 - skills with AB testing and your coding
62:00 - skills in Python using Library such as T
62:03 - models but also the pendas npy also metp
62:07 - lip and caburn we are going to start
62:10 - with the business objective of this case
62:13 - study then we are going to translate the
62:16 - business objective into a data science
62:18 - problem then we are going to start with
62:21 - the actual coding we are going to load
62:23 - libraries we are going to look into Data
62:25 - visual the data The Click data we are
62:28 - going to look into the motivation behind
62:30 - choosing that specific primary metric
62:32 - which is the clickr rate then we are
62:34 - going to talk about the statistical
62:36 - hypothesis for our AB testing I will
62:40 - also teach you step by step all the
62:42 - calculation starting from the
62:43 - calculation of the pulled estimate from
62:45 - the clickr rate and then a computation
62:48 - of the uh pulled variance the standard
62:51 - error but also the motivation behind
62:54 - choosing the searches statistical test
62:56 - that I will be using such as the two
62:58 - sample Z test and then how you can
63:01 - calculate the test statistics how you
63:04 - can calculate the P value of the test
63:06 - statistics and then use that with the
63:08 - statistical significance to test the uh
63:11 - statistical significance of your ab test
63:14 - after this we will also then compute the
63:16 - confidence interval comment on the
63:18 - general ability of the ab test and then
63:22 - at the end we will also test for the
63:24 - Practical significance of the ab test
63:28 - then we will conclude and we will wrap
63:29 - up and we will make a decision based on
63:32 - our data driven approach using the ab
63:35 - test to check whether it's worth it to
63:37 - change a feature in our ux design in the
63:40 - lunar text landing page so without
63:43 - further Ado let's get started so let's
63:46 - now start our case study in here I have
63:49 - in the left hand side this uh version of
63:52 - our landing page so which is our control
63:55 - vers version so to say the existing
63:57 - version where you can see that here we
63:59 - have start freet trial and here we got
64:02 - us our button secure free trial in the
64:06 - right hand side we got this new
64:07 - experimental version that we would like
64:09 - to have which is the Andro Now button so
64:12 - as we saw in the introduction what we
64:14 - are trying to understand is that whether
64:17 - our customers click more on the new
64:20 - version the experimental version versus
64:22 - the existing version the control version
64:25 - so uh um as of the day of uh loading
64:28 - this and uh conducting this case study
64:30 - our landing page uh has a secure free
64:33 - trial but what we wanted to test with
64:35 - our data is whether the uh enroll now is
64:40 - more engaging such that we can go from
64:42 - the secure free trial version to the
64:44 - enroll now version and uh here um for
64:49 - this specific case not only but also in
64:52 - general as we know from a testing is
64:55 - that when ever we got an existing
64:58 - algorithm or existing feature existing
65:01 - button then we are referring this group
65:04 - that we will um where we will expose
65:06 - this existing version of the product we
65:09 - are referring this as a control group so
65:11 - all the users to whom we will show the
65:14 - existing version of our landing page we
65:17 - will refer them as the uh control group
65:20 - participants and then we have the the
65:22 - right hand side our experimental version
65:25 - and our experimental users so the users
65:28 - our existing customers that are selected
65:30 - to be taken part um in our experimental
65:33 - group and in our experiment they will be
65:36 - then uh exposed to this new version of
65:39 - our landing page which contains this
65:41 - androll now button so our end goal in
65:44 - terms of the business as we saw in the
65:47 - introduction is to understand whether we
65:50 - should release the new button which will
65:53 - end up being high High engaging which
65:57 - means that we will have higher CTR or
65:59 - higher uh more uh clicks that will come
66:03 - from our user site which uh
66:05 - automatically means better business
66:07 - because we want to have highly engaging
66:10 - users if they are clicking on this
66:12 - button it means that it interests them
66:15 - more compared to the control version and
66:18 - uh if something on our landing page in
66:20 - this case our call to action is more
66:22 - interesting and highly engaging it means
66:25 - means that we are doing something right
66:27 - and our users might uh either make use
66:31 - of our free products or uh purchase our
66:33 - products or um just stay engag with us
66:37 - to keep real Tech in mind and whenever
66:40 - there is someone who uh is interested in
66:43 - data science or AI um Solutions or
66:46 - products then they can at least refer
66:48 - their friends if they are just clicking
66:50 - to understand and to learn more about
66:52 - our products that's also possibility so
66:56 - from a business perspective we therefore
66:59 - are using here as our primary
67:03 - metric uh our click through rate the CTR
67:06 - of this specific button which in our
67:08 - control version is the secure free trial
67:11 - and in our experimental version is the
67:13 - enroll now and what we want to
67:15 - understand is that whether this new
67:17 - button will end up having higher CTR or
67:20 - not because higher CTR from the
67:23 - technical perspective will translate to
67:26 - higher engagement from the business
67:28 - perspective so here we are making this
67:30 - translation from business versus
67:33 - technical um when it comes to AB testing
67:36 - we can have different sorts of primary
67:37 - metrics we can have a clickr rate as a
67:40 - primary metric we can have a conversion
67:42 - rate as a primary metric or any other
67:46 - primary metric what we want to have as
67:49 - our metric that will work as the single
67:53 - measure that will will compare our dra
67:55 - an experimental group to understand
67:57 - which version performs better is first
68:00 - to understand what this definition of
68:01 - better is and how that translates back
68:03 - to the business because if the
68:06 - engagement is what we are referring as
68:09 - Better Business for some reason and I
68:13 - will explain you in a bit why we think
68:14 - the engagement in this case is what we
68:16 - what matter for us at ler Tech then it
68:19 - means that clickr rate can be used as a
68:21 - primary metric this is just a universal
68:24 - metric that has been used across um
68:27 - different web applications search
68:29 - engines recommender systems and many
68:31 - other digital products to understand
68:33 - whether the engagement of that specific
68:35 - algorithm feature web design whether
68:38 - that is better or not and in this case
68:41 - in this specific case study we are also
68:43 - going to use the CTR because we are
68:45 - interested in the engagement so at luner
68:48 - Tech we really care about the engagement
68:51 - um with our users and we want our users
68:56 - to make use of our products but uh
68:58 - ultimately to engage with us because if
69:00 - they engage with us it means that our
69:03 - products are being seen our uh landing
69:06 - page is being visited and the user is
69:08 - actually interested to click on that
69:10 - button and then the action point and
69:13 - then to start either free trial or to
69:16 - enroll to see what is going on because
69:18 - all these are signs of Interest coming
69:20 - from the user side and in the control
69:24 - version as our click to action is to
69:27 - secure a free trial which directly uh
69:30 - lends the user to our free trial to our
69:34 - ultimate data science boot camp but
69:36 - given that we are expanding which means
69:38 - that we are now offering more courses we
69:40 - are offering free products and also we
69:42 - have uh Enterprise clients uh we have
69:45 - businesses as clients who want data
69:47 - science and AI Solutions and who want
69:49 - corporate training therefore we want to
69:51 - go from this Niche uh version of a
69:54 - landing page so secure free trial to
69:56 - enroll now because we already have a lot
69:59 - of Engagement in terms of the free trial
70:01 - we want to make it more General so
70:03 - that's the business perspective and on
70:05 - the other hand we also want to change
70:09 - beside of changing this um main um call
70:13 - Action we want to make it generalized
70:16 - and at the same time we want to see
70:18 - whether this generalized version will
70:20 - end up leading us um a higher engagement
70:24 - not only in terms of of the other
70:25 - products but also for the tree trial
70:27 - free trial itself because we always are
70:30 - looking for educating people and
70:33 - providing this free trial such that they
70:34 - can make use of our Flagship product
70:37 - which is the the ultimate data science
70:38 - boot camp so now when we understand why
70:41 - we care about the engagement here at ler
70:43 - Tech and we understand why we want to
70:46 - check whether this new button in our ux
70:48 - design will end up increasing the
70:51 - engagement or not we can now make this
70:54 - translation back to the data science
70:56 - terms because we know now from the
70:58 - business perspective All We Care is to
71:00 - understand whether this experimental
71:02 - version of the product is performing
71:04 - better or not but then this means that
71:07 - we need to conduct an AV test and we
71:09 - need to understand whether the ideas
71:12 - that we got and the speculation that the
71:15 - enroll now more General Button as so
71:17 - call you action will be better than the
71:19 - secure free trial version whether this
71:22 - is actually true or not from the uh
71:24 - customer perspective because if we want
71:26 - to call us a datadriven company we
71:29 - cannot just base our conclusions and our
71:32 - decisions for our products or for just
71:34 - in general for our product road map
71:36 - based on Intuition or logic we want this
71:39 - to be data driven which means that the
71:41 - customers are at the first place we are
71:44 - customer driven and our customers need
71:46 - to tell us whether the new um button is
71:50 - better or not and here we have conducted
71:53 - conducted an Navy test and um here I
71:57 - won't be using the real data I will be
71:59 - using the uh proxy data or simulated
72:02 - data that I generated myself and um this
72:06 - one contains the similar structure and
72:09 - this uh the same um idea of the data
72:12 - that we got when we were conducting our
72:15 - IB test and collecting this data and
72:17 - what is our business uh hypothesis in
72:20 - our business hypothesis we can say that
72:23 - we have at least temp % increase in our
72:26 - click through rate so 10% higher
72:28 - engagement when we have our enroll Now
72:31 - versus the secure free trial version of
72:34 - the product so this is our business
72:36 - hypothesis which means that our enroll
72:39 - now CTR so click through rate of the
72:41 - enroll Now button will result in at
72:43 - least 10% higher
72:45 - CTR than the secure free trial so there
72:48 - exists uh 10% at least 10% difference in
72:52 - terms of the engagement when we compare
72:53 - this new version of the product versus
72:56 - the old version of this new uh
72:59 - button and when we translate this back
73:01 - to statistical hypothesis we can say
73:03 - that under the N hypothesis we are
73:05 - saying that there is no statistically
73:07 - significant difference between the um
73:10 - control p and then P experimental which
73:13 - means the um um probability clickr
73:17 - probability clickr rate for control
73:20 - group versus experimental group so under
73:22 - AG n the null hypothesis we are stating
73:25 - what we ideally want to reject we are
73:27 - saying there is no difference between
73:29 - the experimental and control group CTR
73:32 - and under the alternative hypothesis so
73:34 - the H1 we are saying no uh we do have a
73:38 - difference which means that the uh
73:40 - control
73:42 - groups CTR is different from the control
73:45 - experimentals group CTR and one key part
73:50 - here is to mention that they are not
73:52 - just different but they are
73:54 - statistically significantly different so
73:57 - uh when it comes to starting the case
73:59 - study first things first is to load the
74:02 - libraries in this case study we are
74:04 - going to use a numpy we are going to use
74:06 - a pendas as usual for any sort of data
74:09 - analytics data size um case studies you
74:12 - always need those two usually pendis
74:15 - will be needed for our data wrangling to
74:17 - load data process the data visualize it
74:21 - nonp will be used to uh work with
74:24 - different arrays and part of the data
74:26 - then we are going to use the ci. stat uh
74:29 - model and from that we will import the
74:31 - norm function later on um we will see
74:34 - that we are using this in order to
74:37 - visualize this um uh rejection region
74:40 - that we get from for our test to
74:42 - understand whether we need to reject our
74:44 - null hypothesis or not then in this case
74:47 - study we also want to visualize our
74:49 - results and visualize our data for which
74:52 - we are going to need our visualization
74:54 - Library from python which are the curn
74:56 - and the med plot L let's look into our
74:59 - data so what we have in our data we have
75:02 - four different columns and of course
75:03 - this is a filter data that contains the
75:07 - information that we need but in general
75:10 - you can have a larger database you can
75:12 - have more sorts of um um matric matrices
75:16 - and uh different other Matrix but for
75:19 - conducting your ab test the pure AB test
75:22 - you actually need only the following
75:24 - information
75:25 - so you need your user ID to understand
75:29 - uh what are the user you are dealing
75:31 - with so it's the user one user two user
75:34 - 10 it can be that you have other way of
75:36 - referring to your users and uh those can
75:39 - be for instance this long strings that
75:41 - we use to refer to our user but given
75:44 - that our case is a simple one our case
75:48 - study we have just a user ID and this
75:51 - user ID is just a integers that go from
75:55 - one till uh until the end of our uh data
75:59 - and here we got in total 20,000 users
76:03 - therefore this number user ID goes to um
76:07 - 20,000 and those 20,000 um are all part
76:12 - of the user group which means that they
76:14 - are all users and they contain both the
76:17 - experimental and control users then we
76:21 - have our uh click variable and this
76:24 - click variable it's a binary variable
76:27 - which can be uh either one or zero where
76:30 - one refers that the user has clicked on
76:34 - the button and zero means the user
76:38 - didn't click on the button this is our
76:40 - primary metric for our AB test then we
76:44 - have the group reference which is this
76:46 - um string variable and this string
76:49 - variable helps us to understand whether
76:51 - the user comes from the experimental
76:53 - group or from the control group so this
76:55 - can contain only two different values
76:57 - two strings and it is X referring to the
77:01 - experimental and control referring to
77:03 - the uh control group if you can see here
77:06 - we got just this three letters X
77:08 - referring to the experimental group and
77:11 - then if we go in here because we have
77:13 - first the experimental and then the
77:15 - control ones you can see that here we
77:17 - got the uh control group now we have
77:20 - also some time stamp which is not
77:22 - something relevant so we'll be skipping
77:24 - that for now um given that this uh data
77:28 - that we have here it's not the actual
77:31 - data our data but it's a synthetic one
77:34 - but similar in terms of its structures
77:36 - in terms of the uh nature of variables
77:40 - and you can Implement exactly the same
77:42 - steps when you have your data and you
77:44 - are getting it from your ab test and
77:46 - then you are conducting your ab test uh
77:49 - case study so in here what we are going
77:52 - to make use of the most is our click
77:54 - click variable and the group variable
77:56 - because we want to find out per group
77:59 - what are the users that have clicked on
78:02 - the uh button and to be more specific we
78:05 - are looking for these averages so we are
78:08 - not so much interested that that
78:10 - specific user from that specific group
78:12 - has clicked on the product or not that's
78:14 - something that we can explore later but
78:17 - for now we are interested on the more
78:19 - high level so what is this uh
78:21 - percentages what is the click
78:23 - probability or click the true rate perir
78:25 - group and here we got groups of
78:27 - experimental and control as it should be
78:30 - in any source of ab test so once we have
78:34 - conducted our AB test then I will also
78:36 - provide you more insights on what you
78:38 - can do with your data especially with
78:40 - this user ID to learn more about uh the
78:44 - idea behind these different decisions or
78:48 - whether your ab test is different per
78:51 - group but the idea is that this AB test
78:54 - that we are conducting by following all
78:57 - the steps and by ensuring that the uh
79:00 - pitfalls are avoided that we are making
79:02 - a decision that um represents the entire
79:06 - population so we are using a sample that
79:09 - is large enough for us to make a
79:11 - decision for our product and for our
79:14 - business that will be generalized and
79:16 - will be a representation and
79:18 - representative when we apply this
79:20 - decision on our
79:22 - population so let me close this part
79:26 - because we no longer need this and let's
79:29 - go ahead and load this data so here I'm
79:31 - using the pendus library and the common
79:35 - uh approvation of PD and I'm saying pd.
79:39 - read CSV and then I'm here referring to
79:42 - the name of the data that contains my
79:46 - click data and here you can see that dat
79:48 - that that data is here so abore testore
79:53 - click uncore data that's here PV and I
79:56 - will be providing you this data because
79:58 - you won't have this in your own Google
80:00 - collab you will have the link to this
80:02 - Google clab and I'll provide you also
80:04 - the data such that you can put that data
80:07 - you can download it first from my source
80:09 - and then load it in here by using this
80:14 - specific button in here and by doing
80:17 - that you can then go to that specific
80:19 - folder where you downloaded the data and
80:21 - then you will have also this uh
80:23 - corresponding CSV file in your folders
80:27 - so once you have that then you will uh
80:29 - smoothly run this code and uh here I'm
80:34 - loading that data and putting under the
80:36 - name of DF abore test so basically the
80:39 - data frame containing my ab test click
80:41 - data what I want you to do is to
80:44 - Showcase you how the data looks like so
80:47 - here you will see the header given that
80:49 - here I haven't provided any argument it
80:51 - just looks at the top five elements so
80:54 - the top five rows and here I got only
80:58 - the first five users from the
81:00 - experimental group I see that some of
81:02 - them have clicked some of them didn't
81:03 - click and the corresponding user ID and
81:06 - the Tim stamp uh that they um done the
81:09 - click
81:11 - action then um when we look at
81:16 - the describe function you can see here
81:20 - that this gives us more general idea uh
81:23 - of uh what the contains not so much what
81:26 - the top five rows just look like which
81:28 - is great in terms of to understand what
81:30 - kind of data you are dealing with with
81:32 - what kind of variable you have now you
81:35 - can see more the uh total uh picture so
81:38 - high level picture what kind of um data
81:40 - what amount of data you got so the
81:42 - descriptive statistics so here we can
81:45 - see that in total we got 20,000 of users
81:48 - included in this data so 20,000
81:51 - observations 20K rows and we have the
81:55 - mean for the user ID of course it it's
81:58 - not relevant the mean is
82:00 - 10,000 and um this is an interesting
82:03 - number so we see that um average click
82:06 - uh when we look at both user and control
82:09 - the experimental and control groups it
82:11 - is 40% so
82:14 - 0.40 uh 52 so 40.
82:18 - 52% however this is not what we are too
82:21 - much interested so this is not to be
82:25 - confused with the click through rate
82:27 - perir group what we are interested is
82:30 - the click through rate or the mean
82:32 - clickr um when it comes to the
82:35 - experimental group and the control group
82:39 - so then we have our standard deviation
82:42 - we see a high standard deviation which
82:45 - is understandable given that we have
82:47 - this uh large variation in our data we
82:51 - got a control group and experimental
82:53 - group and this Vari shows that we have a
82:55 - huge difference in the these different
82:58 - values uh when it comes to the click
83:00 - event and then we have the mean and the
83:03 - maxim which doesn't give us too much
83:04 - information because the click event so
83:07 - the click variable is a binary variable
83:09 - it contains the zeros and one so
83:11 - naturally the minimum will be the value
83:14 - zero because the click can take value
83:16 - zero and one and the largest one is of
83:18 - course one which means the maximum will
83:19 - be one and then for the rest the 25% so
83:23 - the first quantile the second quantile
83:24 - the 15% which SS the median or the third
83:27 - quantile the 75th percentile is not that
83:30 - much relevant so when it comes to the
83:32 - descriptive statistics for this kind of
83:34 - data especially if it's filtered is not
83:36 - super relevant but if you would have a
83:38 - larger data more matrices beside of
83:41 - Click which is your primary matric but
83:43 - you all have also measured some other
83:45 - Matrix which is
83:46 - recommendable then you would see more um
83:49 - values which would be interesting to
83:51 - look at so not only to look at the click
83:54 - rate but also to look at for instance
83:56 - the mean or maybe the median of
83:58 - conversion rate or the uh mean uh amount
84:02 - of time the average amount of time the
84:04 - user has spent on your landing page or
84:06 - how much time did that user end up
84:08 - spending before making that decision of
84:10 - a click those can be all very
84:12 - interesting Matrix to look into from the
84:14 - product uh data science perspective to
84:17 - understand the decision process and the
84:19 - channel and The Funnel of these clicks
84:22 - but for now for our study what we are
84:25 - purely interested is in our primary
84:28 - metric which is the click event so what
84:32 - we can also see in here is that we got
84:35 - um uh in our group um when it comes to
84:38 - the control group we got uh
84:42 - 1,989 users out of all uh control users
84:47 - that end up clicking versus the
84:49 - experimental group where we have
84:52 - 6,6 users who did click so do not
84:56 - confuse this with the total amount of
84:59 - users per group this amount is the um
85:04 - grouping of the uh data so using the
85:08 - group by and then group so we are
85:10 - grouping that data per group and we want
85:12 - to see per group what is the sum of this
85:15 - variable sum of the clicks and given
85:17 - that the click is a biner variable we
85:19 - know from basics of python that we are
85:22 - basically accounting the number of click
85:24 - events because if you got a binary
85:26 - variable containing zeros and ones if
85:29 - you do the sum of the clicks adding the
85:32 - zeros do not doesn't have any impact
85:34 - which means that um you end up just
85:37 - summing up all the ones to each other
85:40 - and then you end up getting the number
85:43 - of or the total amount of uh cases when
85:46 - this click variable is equal to one so
85:48 - in this case when there is a click event
85:50 - therefore we can see that per
85:52 - experimental group we got
85:56 - 6,16 uh users out of all the
85:58 - experimental users that end up clicking
86:01 - and then out of control group this
86:02 - amount is much lower so we end up having
86:05 - uh only
86:07 - 989 users clicking so let's now go ahead
86:10 - and visualize this data I want to
86:12 - showcase in a bar chart using this
86:15 - clicks what is the total number of
86:16 - clicks so I want to show the
86:18 - distribution of the clicks when it comes
86:21 - to um the uh click event pair group and
86:24 - here I want to uh see next to each other
86:30 - the experimental group and control group
86:33 - and as you can see here here we are
86:36 - getting our bar charts and the yellow
86:40 - corresponds to the no which means that
86:43 - there was no click versus the uh black
86:47 - corresponds to the yes which means there
86:48 - was a click so whenever you see this
86:51 - amount it means that that amount
86:53 - corresponds to no click no engagement
86:56 - from the user side and this is per group
86:59 - so this is what we are referring as a
87:01 - click distribution in our uh data in our
87:04 - experimental uh and control groups and
87:07 - the way that I generated this bar chart
87:10 - is by first creating this um uh list
87:13 - that will contain the colors that I want
87:16 - to assign to each of my groups and I'm
87:18 - saying zero corresponds to the yellow
87:20 - and one corresponds to Black which means
87:22 - that if my variable contains amount of
87:24 - zero in this case my click is equal to
87:26 - zero it means that I don't have a click
87:30 - so it's a no and this I want to
87:32 - visualize by yellow otherwise I have a
87:35 - black which means that um the um one
87:39 - corresponds to the case when we have um
87:42 - click and in this case we will get a
87:44 - black as you can see here the uh yes
87:48 - which means a click is um visualized by
87:51 - this black color and then what I'm doing
87:53 - is that I'm initializing this uh figure
87:56 - size by saying that I I want to have a
87:59 - figure size of 10 and six you you can
88:01 - also skip it but I I think it's always
88:03 - great to put the size of a figure to
88:05 - ensure that you are getting the size
88:07 - like you want it to be such lat you can
88:09 - also download or take a screenshot then
88:12 - we have this uh here I'm using as you
88:15 - can see a combination of the Met plot
88:17 - leap. pip plot Library as well as the uh
88:20 - cabbo because CBO has much nicer colors
88:23 - and here here I'm saying uh we are going
88:26 - to uh make use of the curn to um create
88:30 - um count plot because we are going to
88:34 - count and we are going to showcase the
88:36 - counts per group uh what is the number
88:40 - or the count of the clicks versus no
88:43 - clicks for a group called experimental
88:47 - and what is the number of um or the
88:49 - percentage of clicks versus no clicks
88:51 - when it comes to the group control and
88:54 - then here I'm specifying that the Hue
88:56 - should be on the click which means that
88:59 - we are looking at the click variable and
89:01 - we are going to use the data dfab test
89:04 - which means that we are going to look in
89:05 - this data from here we are going to
89:07 - select this specific variable called
89:09 - click and we are going to use this in
89:11 - order to group our data based on this
89:14 - group so you can see that we are doing
89:16 - the grouping on the variable called
89:18 - group so the argument is called x x is
89:21 - equal to group we're grouping our dfab
89:24 - has data on this group and we are going
89:25 - to do the count in our count plot based
89:28 - on this variable click basically what
89:30 - I'm saying here is that go and group our
89:34 - data dfap test based on Group which
89:37 - means that we will group based on
89:39 - experimental versus control and then I'm
89:41 - saying go and count the click events
89:45 - count pair group so pair experimental
89:48 - perir control group what is the number
89:50 - of times when we have a no so we have a
89:53 - zero and and what is the number of times
89:55 - when we have a yes or we have a one as a
89:58 - value for click variable and then as a
90:00 - pet I'm using my custom pet that I just
90:02 - created which should be in the form of a
90:04 - list as you can see in here if I would
90:07 - have here also my third group or fourth
90:09 - group then I of course need to extend
90:11 - this color palette because I need to
90:14 - have the same amount of colors as the
90:16 - number of groups pair my target variable
90:19 - in this case The Click has only two
90:21 - possible values 0 and one which means
90:23 - that I'm only or only specifying the two
90:25 - colors in my list so then we have the
90:29 - title of our plot always nice to add by
90:32 - and then we have our labels which means
90:35 - that I want to emphasize uh as my X
90:38 - label so here I want to have my group
90:40 - you can see here is my group because I
90:42 - will either have group experiment or
90:44 - control that's my variable on my xaxis
90:48 - and on my y AIS of course I have the the
90:51 - count so I'm counting the number of
90:54 - times I got uh the uh no click versus
90:58 - click event so here note that the um y
91:02 - AIS is in terms of this count so here
91:04 - you can see it's uh 8,000 here sa 7,000
91:09 - or 6,000 5,000 which means that we are
91:12 - talking about the numbers and the counts
91:15 - rather than
91:16 - percentages and this is important
91:18 - because um another thing that I'm also
91:21 - doing is that I'm going the extra Mile
91:24 - and I'm also adding beside of this
91:27 - counts on the top of each bar I I want
91:30 - to visualize and clarify what are the
91:32 - corresponding percentages it's always
91:35 - great to enhance your data visualization
91:39 - with some percentages percentages is
91:42 - easier for the uh person who follows
91:44 - your presentation to understand for
91:47 - instance if you got an experimental
91:48 - group and the the users is here 6,000
91:52 - and um 4,000 they might not quickly
91:55 - understand that you got for instance in
91:57 - total 10,000 of users and then 6,000 has
92:01 - then uh clicked and then 4,000 didn't
92:04 - click so um then the idea is that by
92:09 - adding these percentages we can then see
92:12 - that
92:14 - 61.2% has clicked in this experimental
92:18 - group and
92:20 - 38.8% has not clicked of course this a
92:23 - simulated data I specifically pick the
92:25 - extreme in such way that we can clearly
92:28 - see this difference in the clickr rates
92:31 - but um in the reality you can have a
92:33 - click-through rate of 10% up to 14%
92:35 - which is usually a good number if you
92:37 - have a clickthrough rate of 40% is great
92:39 - but it's really depend on underlying
92:41 - user base what kind of product you got
92:43 - how large is your user base because if
92:45 - you have very large user base then 10%
92:48 - can be a good clickr rate versus if you
92:51 - have a very small user base Maybe 61% is
92:55 - considered uh good or
92:58 - average so uh in here we have just a
93:02 - simulated data of course and I've have
93:04 - added these percentages uh by using the
93:08 - following code so I won't go too much
93:11 - into detail in here um uh feel free to
93:14 - check and see uh and if something
93:16 - doesn't make sense go back to our python
93:19 - for data science course that contains
93:21 - lot of information on the basics in
93:23 - Python
93:24 - but here just quickly what I'm doing is
93:26 - that I am uh calculating the percentages
93:29 - and I'm annotating the bars so I want to
93:32 - know what are these percentages which
93:33 - means that per group I want to take the
93:36 - total amount of clicks I want to
93:38 - understand what is number of Click event
93:40 - when the click variable is equal to one
93:42 - and what are the number of cases when
93:44 - there was no click from the user s which
93:46 - is what are the number of cases when the
93:47 - click variable is equal to zero and then
93:50 - I'm counting those amounts and then
93:52 - using the total amount to calculate the
93:54 - percentage for instance in this specific
93:57 - case I'm filtering the data for
93:58 - experimental group I'm looking at the
94:00 - total number of users for this group
94:02 - which is 10K and then I'm counting the
94:05 - number of times when out of this 10,000
94:07 - users the amount of users that end up
94:09 - clicking on that button which is the
94:12 - click is equal to one case and then I'm
94:15 - taking that number dividing it to the
94:17 - total number of users for this
94:19 - experimental group multiplying by 100 in
94:22 - order to get that in percentage
94:24 - and this is the calculation that you can
94:26 - see in here one thing that is important
94:29 - here is that here I'm using this um uh
94:34 - percentage um so for the current bar I'm
94:37 - saying U as a way to identify whether we
94:40 - are dealing with experimental or control
94:42 - group is by getting by looking into this
94:45 - uh p and uh this p in here is the
94:49 - basically the patches so in this case
94:52 - I'm basically saying if if I'm dealing
94:54 - with experimental group then go ahead
94:56 - and calculate what is this uh total
94:59 - amount of observations and then take
95:01 - what is the uh number of clicks and then
95:04 - divide the two numbers uh C multiply
95:07 - this with 100 and this will then give us
95:10 - the percentage and then I'm doing this
95:12 - for each of those groups so I'm doing it
95:15 - for this group I'm doing for this group
95:17 - and for this one and for this one so I
95:18 - got two groups but then within each
95:20 - group I got clicks and no clicks and I'm
95:23 - calculating these four different
95:24 - percentages and then I'm adding these
95:27 - percentages on the top of those bars so
95:29 - I not only want to have numbers repr
95:32 - presented in my visualizations but I
95:33 - also want to add this corresponding
95:36 - percentages at the top just for
95:38 - visualization purposes I wanted to put
95:40 - this out there because this can help
95:43 - your uh data visualization toolkit and
95:46 - it also will um make your audience from
95:49 - your presentations be more thankful to
95:51 - you when you are telling the story of
95:54 - your
95:55 - data so uh this is about the data that
95:58 - we have we see that uh 38.8% of our
96:01 - experimental group users have not
96:03 - clicked on the button versus the
96:06 - 61.2% have clicked on the button based
96:09 - on the simulated data and then uh in the
96:12 - control group we have a quite the
96:14 - opposite situation we got the majority
96:17 - of the users
96:19 - 80.1% not clicking on the button versus
96:22 - the remaining 19 .9% have actually
96:25 - clicked on that button so we got a huge
96:28 - difference a dis anounce when it comes
96:30 - to the experimental group and uh control
96:32 - group this kind of gives us an
96:34 - indication hey something is going on
96:37 - here we kind of uh have already um
96:41 - higher level intuition what the
96:44 - remaining analysis will look like um
96:47 - which is that there most likely will be
96:50 - a difference in their ctrs when it comes
96:53 - to the uh the um uh control versus
96:57 - experimental group and the corresponding
97:01 - buttons but uh hey let's continue that's
97:04 - the entire goal behind a testing is to
97:07 - ensure that our intuition our
97:09 - conclusions are all based on the data
97:12 - rather than on our intuition so what are
97:15 - the parameters that I'm using here for
97:18 - conducting our AB test when I was
97:21 - designing this AB test uh the first step
97:23 - step was to of course do all these
97:25 - different translations that we learn as
97:26 - part of our AB test course um conducting
97:29 - it properly which means coming up with
97:32 - this three different parameters when
97:35 - doing our power analysis and usually
97:38 - this should be done when you are
97:40 - collaborating also with your colleagues
97:42 - and uh with your product managers or
97:44 - your product people domain experts
97:47 - because they have um a lot of
97:49 - information on what it means to have um
97:53 - threshold that you need to pass in order
97:56 - to say that for instance this new
97:59 - version of your feature is different and
98:02 - is uh considerably uh different from the
98:05 - existing one and here um in order to for
98:09 - us to understand this uh and make these
98:11 - conclusions we need to come up with the
98:14 - three different parameters that can help
98:16 - us to properly conduct an AB test as we
98:18 - learned when we were looking into
98:20 - designing a proper AB test so so first
98:23 - we have our significance level the
98:26 - significance level or the alpha the
98:28 - Greek letter that we are using to refer
98:30 - to the significance level which is also
98:32 - the probability of the type one error
98:35 - and that amount we have chosen following
98:38 - the industry standard which is 5% given
98:41 - that we didn't have any uh previous
98:44 - information or specific reason to choose
98:46 - a different significance level so lower
98:49 - or higher we decided to go with the
98:51 - industry standard which is the 5%
98:54 - this means that we want to have um we
98:57 - want to compare our P value of our uh
99:01 - statistical test to this 5% and then say
99:04 - whether we have a statistically
99:06 - significant difference between the
99:08 - control and experimental group based on
99:10 - this 5% significance level and let's
99:12 - refresh our memory on this Alpha this
99:15 - Alpha uh or significance level is also
99:18 - the probability of type one error so
99:20 - this is the amount of error that we are
99:22 - comfortable making
99:23 - when we um reject the N hypothesis well
99:27 - the null hypothesis is actually uh true
99:32 - which means that we are detecting a
99:34 - difference between the experimental and
99:36 - control version while there is no
99:38 - difference and we are making that
99:40 - mistake and here we are saying that we
99:42 - are fine and we are comfortable with
99:45 - making this mistake at a maximum of 5%
99:48 - but higher than that it's not allowed we
99:50 - are not comfortable making uh error um
99:54 - higher than 5% then the next variable uh
99:57 - in this case the B uh or beta the
99:59 - probability of type two error which is
100:01 - the opposite of the type 1 error which
100:03 - is a false negative rate or the amount
100:05 - of time the um um proportion of time
100:10 - when we end up failing to reject the
100:14 - null hypothesis while null hypothesis is
100:16 - false and it should have been rejected
100:19 - then the 1 minus beta is actually power
100:21 - of the test so what is the amount of
100:23 - time we are correctly rejecting our null
100:26 - hypothesis and correctly stating that
100:28 - there is indeed a statistically
100:30 - significant difference between our
100:32 - experimental group and our control group
100:35 - so we have chosen for this the uh
100:37 - industry standard as well which is the
100:39 - 80% but given that for your results
100:42 - analysis in this case for conducting
100:44 - this case study that part of the power
100:46 - analysis is not relevant we use that
100:49 - when calculating our minimum sample size
100:51 - but we don't need that when conduct our
100:53 - results analysis therefore I'm not
100:56 - initializing that as part of this code
100:59 - so here I'm only providing to my program
101:02 - the values for my significance level
101:04 - which is 0.05 or this is the same as
101:08 - 5% and then the Delta which is the third
101:12 - parameter and this Delta is our minimum
101:15 - detectable effect so this a Greek letter
101:18 - Delta which is the minimum detectable
101:20 - effect helps us to understand whether
101:24 - beside of having this statistically
101:26 - significant difference whether this
101:28 - difference is large enough for us to say
101:31 - that we are comfortable making that
101:33 - business decision to launch this new
101:36 - button so it can be that when we are
101:39 - conducting an AB test we are finding out
101:42 - that the experimental group has indeed
101:45 - higher engagement than the uh control
101:48 - group and we are uh getting a small P or
101:52 - at least is smaller than the alpha and
101:54 - we are seeing that P is small than the
101:56 - alpha level which means that we can
101:57 - reject the null hypothesis and we can
101:59 - say that the CTR or the clickr rate of
102:03 - the experimental group is statistically
102:05 - significantly different from the control
102:07 - group at 5% significance level but we
102:09 - know from the theory of ab test that
102:12 - only that is not enough only statistical
102:15 - significance is not enough for the
102:16 - business to make that important decision
102:19 - to launch an algorithm or to launch a
102:21 - feature in this case to change change
102:23 - our landing page the button from the
102:26 - start free trial to the enroll now which
102:30 - means that we want to have enough users
102:33 - and we want to have enough difference
102:35 - large difference in our click through
102:37 - rate or enough users saying that we are
102:39 - more happy with this uh new version of
102:42 - the landing page for us to go and change
102:45 - our feature and what is the definition
102:48 - of enough what is the difference in the
102:50 - click through rate that we need to
102:52 - detect
102:54 - after we have detected the statistical
102:55 - significance in order for us to say that
102:58 - we also have a practical significant so
103:01 - practically we are also comfortable
103:04 - making that business decision and then
103:07 - launching this new feature and changing
103:09 - our landing page button and that is
103:11 - exactly what we have under our Delta
103:13 - this minimum detectable effect in this
103:16 - case we have chosen for Delta of
103:20 - 10% so you can see here 0.5
103:23 - this is 10% this means that our Delta or
103:26 - the MD the Min detectable effect is 10%
103:30 - this means that we are saying not only
103:33 - we should have a statistically
103:34 - significant difference between the
103:35 - experimental group and control group but
103:38 - also we need to have this difference to
103:41 - be at least 10% which means that we need
103:45 - to
103:46 - have detected that the experimental
103:50 - version of the landing page results in a
103:53 - at least 10% higher quick rate compared
103:56 - to the control version for us to go
103:59 - ahead and to launch this new version and
104:03 - deploy this new uh ux uh feature so this
104:08 - is really important because many people
104:11 - go and check for statistical
104:12 - significance so they do their Alpha and
104:15 - then check uh whether the P value is
104:17 - more the alpha and then say hey we have
104:19 - a statistically significant difference
104:21 - and then they are done with that but
104:23 - that's not correct after you have
104:26 - conducted your uh statistical
104:28 - significant analysis and you have
104:30 - detected that your uh experimental
104:32 - version has a statistically significant
104:35 - different um CTR than the control
104:39 - version at your Alpha significance level
104:43 - the next thing you need to do is to
104:45 - ensure that you also have a practical
104:47 - significance beside of the statistical
104:49 - significance and this practical
104:51 - significance you can detect
104:53 - and you can check when you use your mde
104:56 - or your Delta and you compare it to your
104:59 - confidence interval that you have
105:01 - calculated something that we have also
105:03 - learned as part of the theory of
105:05 - conducting a proper AB test but once we
105:08 - come to that point so after we check for
105:10 - our statistical significance I will also
105:12 - explain how exactly uh we will need to
105:15 - do this check and at the same time we
105:17 - will also be refreshing our theory on
105:19 - the Practical significance so let's now
105:22 - goad head and calculate the total number
105:25 - of clicks per group by summing up these
105:27 - clicks and I also want to calculate and
105:31 - group by this amounts just to Showcase
105:34 - how you can do that on your own so here
105:36 - what I'm doing is that I'm taking my ab
105:38 - test data I'm grouping by by group group
105:43 - is the uh variable that contains the
105:45 - reference where we are dealing with
105:47 - experimental group or control group and
105:49 - as you know from our uh python series
105:52 - and demos python for data science course
105:55 - that uh whenever we want to group that
105:57 - data a pend this data frame first we
105:59 - need to say pendas data frame name.
106:02 - Group by Within parenthesis the variable
106:04 - that we are using to do the grouping
106:06 - which is in this case group and then
106:08 - within Square braces I want to emphasize
106:11 - and put the name of a variable that I
106:13 - want to um apply operations on so I want
106:17 - to group my data on the group variable
106:20 - and I want to count the number of times
106:24 - I have a click in my control group and
106:26 - in my experimental group this will be my
106:29 - X control and X experimental variables
106:32 - so X control will then compute contain
106:36 - information about number of Clicks in my
106:38 - control group and then each experimental
106:40 - will contain the number of Clicks in my
106:42 - experimental group and given that um I
106:45 - want to refer to the name of that uh
106:50 - Group after I did my
106:53 - grouping so I am getting this kind of
106:57 - this shape of data frame of course I
107:00 - then need to uh use my do log function
107:04 - in order to properly call that amount so
107:07 - to understand what is this amount
107:09 - corresponding to this index and what is
107:10 - this amount corresponding to this index
107:12 - and given that my index is in strings
107:14 - I'm then using here mylog function
107:17 - something that we also learned as part
107:18 - of our python for data science course so
107:22 - here is basically the printing just
107:25 - writing nicely what are the results
107:28 - which means that we are counting that
107:30 - the let me count again that the uh
107:33 - number of uh clicks for my control group
107:37 - is
107:41 - 1,989 so you can see that it
107:45 - is want to double check and see what we
107:48 - got yes so we got same number so we are
107:50 - dealing with the same data set just to
107:52 - to make sure and here the number of
107:54 - clicks for experimental group is equal
107:56 - to 6K and 116 so
108:00 - 6,116
108:01 - clicks so then we are calculating the uh
108:04 - pulled estimates for the clicks per
108:07 - group let me quickly fix this typo so
108:11 - calculating the uh pulled estimate for
108:14 - the clicks perir Group which means the P
108:17 - estimate for the experimental group and
108:20 - for the uh control group so let me
108:22 - quickly at here how I can calculate the
108:26 - uh total cases when we got uh
108:30 - experimental group users so what is the
108:32 - number of users in the experimental
108:33 - group and what is the number of users in
108:35 - the uh control group so here what I want
108:38 - to do is that I want to say that the
108:41 - group The DF test group should be equal
108:45 - to
108:47 - expermental and this of
108:50 - course should be my filter and I want to
108:54 - count
108:55 - this and let me quickly copy this I saw
108:59 - that is already under the control so
109:01 - here I'm changing to the control and
109:04 - this will need to give me the number of
109:09 - users in each of these groups too so
109:11 - number of users in control and number of
109:15 - users in Click and here I will simply
109:18 - check
109:19 - this so I will print then the number of
109:22 - user users per group and at the same
109:25 - time I will also click the number of
109:26 - clicks per
109:28 - group there we go so now when we have
109:32 - done this what we are ready to do is to
109:35 - go ahead and calculate the F estimate
109:38 - for clicks perir Group which means pair
109:41 - control group and perir experimental
109:43 - group for that what we need to do is to
109:46 - take the number of clicks of the control
109:49 - group divide to the number of all users
109:51 - for control group as you can see in here
109:53 - x control / to n control and we are
109:56 - referring to this variable as P control
109:59 - head because we know that the estimate
110:02 - of this click probability um is always
110:05 - with a hat it's just the way that we
110:07 - reference it in um statistics and in ay
110:11 - testing so this is the estimate
110:13 - something that we are estimating
110:15 - therefore we are saying hat and then we
110:18 - have the same for experimental group
110:20 - which means that the estimate of the
110:21 - experimental groups uh click probability
110:24 - is equal to X exp and then divide it to
110:26 - n x then um in order to calculate the uh
110:31 - pulled estimate or uh pulled click
110:34 - probability which means the value that
110:37 - will describe of the uh control group
110:39 - and experimental group we need to follow
110:42 - this formula which means that we are
110:44 - taking the ex control we are adding to
110:47 - that X control the X experimental this
110:49 - is our nominator of our
110:53 - uh value and then we are dividing this
110:56 - to the uh sum of the sizes of each of
111:00 - those groups which is n control and N
111:04 - experimental so this is the common
111:06 - formula of the pulled estimate uh when
111:10 - it comes to this type of experimentation
111:12 - when you are dealing with um primary uh
111:15 - metric that is in the form of zeros and
111:17 - ones and if you want to refresh your
111:20 - memory on this type of formulas then
111:22 - make sure to also check our AB testing
111:25 - course because in there we go in detail
111:28 - in this uh specific lesson of the uh AB
111:32 - test result results analysis we are
111:34 - looking into this uh all this formulas
111:37 - on how we can calculate the pulled
111:39 - estimate of this uh click
111:42 - probability so click probability but
111:46 - then we are calling
111:47 - it
111:51 - P click for
111:53 - probability and then what we
111:57 - got is this
112:00 - volum so that amount is then
112:05 - 040 this number should look familiar
112:07 - because this is then the mean that we
112:09 - saw when we were looking at the um uh
112:13 - descriptive statistics table if you can
112:16 - recall this
112:19 - table let me see this number
112:23 - so now basically we are then calculating
112:25 - this manually because we need a variable
112:28 - that will hold this uh value so it is
112:32 - simply summing up all the
112:34 - clicks for control group and
112:36 - experimental group to get the total
112:38 - number of clicks and we're dividing it
112:40 - to the total number of users so n
112:42 - Control Plus n experiment so now when we
112:46 - have this we are ready to also calculate
112:48 - what we are referring as a pulled
112:49 - variance also something that we have
112:51 - learned as part of this theory for AB
112:53 - testing so the pulled variance is equal
112:55 - to the pulled estimate of the clicks so
112:59 - P had something that we just
113:01 - calculated multiplied by one minus P
113:05 - head so the uh click event the estimate
113:08 - of the click probability multiplied by
113:10 - the estimate of no click and we know
113:13 - already this IDE of berola distribution
113:16 - that the variable that uh describes this
113:18 - process of clicks and no clicks follows
113:21 - kind of this idea of B distribution when
113:23 - we have a click and no click so we have
113:26 - probability of click and then we have
113:28 - probability of no click which is the one
113:30 - minus that click probability so that's
113:32 - the idea or the part of the formula that
113:35 - we are following as kind of an intuition
113:38 - and then this multiplied by 1 / to n
113:42 - control+ 1 / to n experimental so here
113:46 - I'm purely following the formula for the
113:48 - pulled variance if you want more details
113:50 - and explanations and sure to check the
113:52 - corresponding Theory lecture because we
113:55 - are going into details of each of those
113:58 - formulas and understanding why we
114:00 - calculate this um P variance and P
114:03 - estimates uh in this specific way and
114:05 - using these specific formulas so here by
114:09 - just follow following the uh formula I'm
114:12 - getting that the uh pull uh variance is
114:16 - this
114:17 - amount so this is in nutshell how I
114:20 - calculated my uh pull click probability
114:23 - and a pulled variance of that click
114:26 - event and we are going to need that in
114:29 - the next very important step which is
114:32 - calculating the standard error and
114:35 - calculating the test statistics because
114:38 - in this case what we are doing is that
114:41 - we are dealing with a case when the
114:44 - primary metric is in the form of zeros
114:47 - and one so we let's Now quickly talk
114:49 - about the uh choice of a statistical
114:53 - test be uh before conducting the actual
114:56 - calculation of standard error in the
114:58 - test statistics so here I went for the
115:00 - two samples at test and let me explain
115:03 - you why and what is the motivation
115:05 - because as we learned as part of the
115:07 - theory um whenever we have a primary
115:10 - metric that is in the form of an
115:12 - averages like we have now because we are
115:15 - using the P control head and P
115:17 - experimental head head so we have a
115:20 - primary metric that is the uh click
115:22 - through rate which is the average clicks
115:26 - per group so we have calculated the
115:28 - average click per experimental group and
115:31 - control group then the primary metric
115:35 - the form of it already dictates given
115:37 - that it's in averages that we need to
115:40 - look at uh either parametric test
115:43 - corresponding to this averages or
115:45 - non-parametric test corresponding to the
115:47 - um averages in this case I went for the
115:50 - parametri case because uh it has Better
115:52 - Properties if I have this information
115:55 - about the distribution of my data and
115:59 - why do I have this information and then
116:01 - this also dictates the uh choice of my
116:05 - um statistical test well I have a size
116:09 - of my sample which is over 100 and
116:12 - actually over 30 that's the threshold
116:14 - that we tend to use in statistics and in
116:16 - a testing in order to say whether we
116:19 - have a large size or large data or not
116:22 - if our sample is not large so it
116:25 - contains less than 30 users per group
116:28 - which happens as well then we say that
116:30 - we need to go for um statistical
116:34 - test uh that will be specific for this
116:38 - kind of cases because we can no longer
116:40 - make use of the uh statistical theorems
116:43 - like the central limit theorem which
116:45 - helps us to um uh to take the uh to the
116:49 - inference so to make use of the
116:50 - inferential statistics and make
116:52 - conclusions regarding the distribution
116:54 - of our population just having the sample
116:57 - and what do I mean by that so if my
116:59 - sample is larger than 30 like in this
117:02 - specific case I got 10,000 users per
117:04 - group so it is definitely larger than 30
117:08 - uh
117:09 - users then in that case I can say that
117:12 - by making use of the central limit
117:14 - theorem I can say that my sampling
117:18 - distribution is normally distributed and
117:21 - this is simply making use of the central
117:24 - limit theorem something that we have
117:27 - also learned when we were looking into
117:28 - this concept of inferential statistics
117:31 - as part of the fundamental statistics
117:33 - course uh course um in lunar Tech so
117:36 - this is a powerful theorem that we use
117:39 - in AB testing in order to make our life
117:42 - easier because when we have a sample
117:45 - that is larger than
117:48 - 30 for each of these groups then we can
117:51 - say that even if we don't know the
117:54 - actual distribution or the name of the
117:56 - distribution that our uh sample follows
118:00 - where it comes to the click um event so
118:03 - the random variable that describes this
118:05 - number of clicks or the average click
118:08 - true rate what is that um distribution
118:11 - exactly but given that we have that this
118:14 - uh size is large enough it's large than
118:16 - 30 users we can say that by making use
118:20 - of the central limit theorem we can say
118:22 - that the uh the uh sample distribution
118:27 - follows a normal distribution if given
118:30 - that the sample size is large enough and
118:34 - this helps us to say that well in that
118:36 - case it doesn't matter whether we make
118:38 - use of the two sample Z test or two
118:40 - sample T Test we can make use of either
118:44 - of these test in order to conduct our
118:47 - analysis and we had this specific
118:50 - template to make this choice easier uh
118:53 - in our AB test course at the loer tech
118:57 - where we were making all this decisions
119:00 - and saying if the SLE size is this we
119:02 - need to do this if the SLE size is this
119:04 - we need to do this and in this specific
119:06 - case following that exact structured and
119:08 - organized approach I ended up seeing
119:11 - that my sample size is large so it's
119:14 - larger than 30 so I can then make use of
119:17 - the central limit theorem I then know
119:19 - what is the random um what my random
119:21 - variable describing this click through
119:23 - rate um follows the kind of distribution
119:27 - in this case a normal distribution and
119:29 - then this means that whether I use a t
119:31 - test or Z test doesn't really matter I'm
119:33 - going to end up with the same
119:35 - conclusions therefore I will just go
119:37 - with the two sample set test simply
119:39 - because um it is just easier for me to
119:43 - do for
119:44 - example you can also go with the two
119:46 - sample T Test and you can even change
119:49 - this case study and tweak it and then
119:51 - make make it your own and put it on your
119:53 - resume in that way by making it more
119:55 - unique and that will be totally fine
119:57 - because you will see that you are going
119:59 - to end up with exactly the same
120:01 - conclusions as we do in this specific
120:03 - case study because if you have a large
120:05 - enough sample it won't matter whether
120:07 - you have a two sample Z test as your
120:09 - parametric test or the two sample T Test
120:13 - and um if you want to know why why this
120:16 - matters and all the different details
120:18 - statistical insights make sure to check
120:20 - the actual uh course dedicated to AB
120:23 - testing because there will be cover this
120:26 - all and you will then become a master in
120:28 - the field of AB testing now we know this
120:32 - uh decisions and the motivation behind
120:34 - choosing the uh two samples that test
120:37 - let's now go ahead and do the actual
120:39 - calculations so here we have a standard
120:41 - error which we calculate by taking the
120:44 - pulled variance and taking the square
120:46 - root of it and this is again using the
120:49 - idea of this formulas that we learned as
120:51 - part of the ab test so we are using this
120:54 - P variance taking the square root of
120:56 - this which gives us the standard error
120:59 - and the standard error as you can see in
121:01 - here is then equal to
121:05 - 0.69
121:07 - 29499 this
121:09 - amount there we calculate our test
121:11 - statistic for our two sample at test so
121:14 - the test statistic is equal to P control
121:17 - head minus P experimental heads divided
121:19 - to standard error so here uh you can now
121:22 - see the motivation behind not only
121:25 - Computing the P pulled head but really
121:27 - also the p uh control head and P
121:30 - experimental head and then I take the P
121:33 - control head and subtract the P
121:35 - experimental head and I divide it to the
121:37 - standard error to compute my test
121:40 - statistics once I did this as you can
121:43 - see this is this amount so test
121:45 - statistics for our two sample that test
121:48 - is this amount minus 5956
121:52 - around it then um we can also compute
121:56 - the critical value of our Z test which
121:59 - is uh by using this Norm function that
122:02 - we uh loaded in here from the
122:05 - S and this will help us to understand
122:08 - what is this value from our normal
122:11 - distribution table the standard normal
122:13 - distribution table uh where by making
122:16 - use of this table we
122:19 - identify what is this critical value
122:22 - that we need to have to uh create our
122:25 - rejection regions and to say whether we
122:27 - can uh reject our n hypothesis or not so
122:31 - to conduct our test we need to have a
122:34 - critical
122:35 - value for uh to which we will compare
122:38 - our test statistics and this critical
122:41 - value will be based simply on the
122:43 - standard normal distribution so this is
122:45 - this norm.
122:46 - ppf and then uh probability um uh
122:51 - function base basically uh the the
122:53 - probability function that comes from the
122:56 - normal distribution standard normal
122:57 - distribution and as you can see it is
122:59 - correspond specifically to this percent
123:02 - Point function which is the inverse of
123:04 - the cumulative distribution function so
123:07 - this based on the alpha / 2 so 1us Alpha
123:12 - / 2 is the argument that we need to put
123:14 - for our percent Point uh probability
123:17 - function and why divide it to two
123:21 - because we have a two sample test so
123:25 - because we have a two-sided two sample
123:27 - test sorry so if you want to understand
123:30 - this difference between uh two sample um
123:33 - test two-sided Test please check out the
123:36 - uh fundamentals to statistics course at
123:39 - ler Tech because we cover this uh Topic
123:42 - in detail and it's a very involved topic
123:44 - it contains many complex stst U from
123:47 - statistical point of view so I won't be
123:49 - spending in this case study too much
123:51 - time on that here I'm assuming that you
123:53 - know this formula already but if you
123:56 - don't and if you quickly need to do your
123:58 - case study May testing feel free just
124:00 - just to copy this line which basically
124:03 - is a value that we need based on the
124:06 - corresponding chosen statistical
124:08 - significance level that we need to
124:11 - compute to compare our test statistics
124:14 - so our test statistics is this value and
124:16 - the value that we need to compare it to
124:19 - is the Z critical volue so we can see
124:22 - that this critical value is then equal
124:25 - to
124:27 - 1.96 this is actually a very common
124:30 - value that we know even without looking
124:32 - at a standard normal table when you make
124:35 - use of this test enough often then you
124:38 - know that the uh critical value
124:41 - corresponding to a two-sided test when
124:43 - it comes to normal table is equal to uh
124:46 - 1.96 this is just the value that we know
124:49 - and in here by even without calculating
124:53 - the next step which is a P value we can
124:55 - even say already what is the decision we
124:58 - need to make in terms of statistical
124:59 - significance because we know that one
125:02 - way we can test our hypothesis
125:04 - statistical hypothesis is by Computing
125:06 - the test statistics and checking with
125:08 - the test statistics the absolute value
125:11 - of it is larger than the critical value
125:14 - and we see that the test statistics is
125:16 - equal to minus
125:18 - 5956 the absolute value of that is 59
125:21 - .56 and that value is much larger than
125:24 - our critical value which is equal to
125:28 - 1.96 this already gives us an idea that
125:30 - we can reject our null hypothesis at 5%
125:34 - statistical significance level but I
125:37 - want to uh go on to the next step
125:39 - actually because that's um more
125:42 - structured more organized way to doing
125:45 - and conducting experimentations as in
125:47 - the industry we tend to make use of the
125:51 - p values instead of making use of this
125:53 - econometrical approach and statistical
125:56 - approach of um testing the statistical
125:59 - test so once we have calculated our test
126:02 - statistics the next thing we need to do
126:04 - is to calculate our P value and then use
126:09 - that P value compare to the significance
126:11 - level Alpha and then make a decision
126:14 - whether we need to reject our n
126:16 - hypothesis and say that we have a
126:18 - statistical significance or we cannot
126:20 - reject our n hyp hypthesis and then we
126:22 - need to say that we don't have a
126:25 - statistical significance so we don't
126:26 - have enough evidence to reject anal
126:28 - hypothesis so the idea here is that we
126:33 - need to make use of our uh normal
126:36 - function and specifically the norm. SF
126:39 - so making use of exactly the same
126:41 - Library the norm from CI do DOS and then
126:45 - this time we're using the survival
126:47 - function which is the one minus the
126:49 - cumulative distribution function of
126:51 - normal distribtion this comes again from
126:54 - statistics and then using the absolute
126:57 - value of our test statistics multiplying
126:59 - it by two given that we have a two-sided
127:01 - test I'm calculating my P value this is
127:04 - simply by making use of the same formula
127:08 - that we saw when we were uh studying
127:10 - theab test from a technical point of
127:13 - view because we learned that the P value
127:15 - is then the probability that Z will be
127:18 - smaller than equal the minus test
127:20 - statistics or that the test statistic is
127:22 - smaller than equal to Z so uh we
127:25 - basically want to calculate what is this
127:28 - probability the P value which is equal
127:31 - to the probability that our test
127:33 - statistics will be smaller than the
127:35 - critical value or
127:37 - our negative of the test statistics will
127:40 - be larger than equal of the critical
127:43 - value and we want to know this
127:45 - probability because what this
127:47 - probability represents is that what is
127:50 - the chance that we will get a large test
127:53 - statistics well this is due to a random
127:56 - chance and not because we have a uh
127:59 - actual statistical difference between
128:02 - the clickr rate of the experimental
128:03 - group versus control group so this is
128:06 - the idea behind P value so what is this
128:09 - chance that we are uh mistaking this
128:12 - random
128:13 - mistake this random observation that we
128:17 - got a large test statistic and saying
128:20 - that there is a statis significance well
128:22 - there is no such thing and we are purely
128:26 - getting this large test statistics um
128:29 - because of the random chance if the
128:32 - probability of getting a large test
128:34 - statistics by random chance is small so
128:36 - if this P value is small then we can say
128:39 - that we have a statistical significance
128:41 - that's the idea behind it and this P
128:43 - value when we calculate uh we are
128:46 - storing it in this variable called pcore
128:49 - volume and then the next thing what I'm
128:51 - doing is that I'm writing this function
128:53 - quote is statistically significant which
128:55 - takes argument as P value in Alpha so I
128:59 - just need the P value that I just
129:01 - calculated for my test Set uh test uh
129:03 - statistics and then I want the
129:06 - statistical significance level that I
129:07 - want to use for my test and then this is
129:11 - the value that comes from my power
129:13 - analysis as I mentioned before that's
129:15 - the 5% this P value I'm calculating for
129:19 - my test statistics so in here and then
129:22 - I'm taking the two and I want to compare
129:25 - them so I want to assess whether I have
129:28 - a statistical significance by comparing
129:30 - my P value to my statistical
129:32 - significance level Alpha and what is
129:35 - this comparison well we know uh from the
129:38 - theory that um if we have a low P value
129:41 - and specifically in the P that we are
129:44 - getting the P value is small than equal
129:46 - the 5% or 0.05 which is the significance
129:50 - level then this indicates that we have a
129:53 - strong statistical uh evidence that uh
129:56 - the N hypothesis is false and we need to
129:58 - reject it so we have a strong evidence
130:01 - against the null
130:02 - hypothesis and otherwise if the P value
130:07 - is larger than
130:08 - 0.05 so it's larger than 5% that we have
130:12 - chosen as the maximum threshold of that
130:15 - mistake so the significance level is uh
130:18 - uh no longer the largest element but the
130:21 - P value is larger than your significance
130:23 - level then this indicates that you don't
130:25 - have enough
130:27 - evidence against the N hypothesis so
130:29 - your evidence is weak this means that
130:32 - you fail to reject the N
130:35 - hypothesis so this is what I'm doing in
130:38 - here with this code so I'm saying print
130:41 - the P value first and we are rounding it
130:44 - up with this round function I'm rounding
130:46 - it to the three decimal and then I want
130:50 - to check and Det determine whether I
130:52 - have a statistically significant or not
130:54 - and the way that I'm doing that is I'm
130:56 - saying if my P value is more than my
130:58 - alpha or actually let at smaller than
131:01 - equal than Alpha then we can print that
131:03 - there is a statistical significance
131:05 - which indicates that the observed
131:07 - differences between the experimental and
131:10 - control groups are un unlikely to occur
131:13 - due to random
131:14 - chance which means that this is not
131:17 - random chance and uh we have a strong
131:20 - evidence that there is a statistical
131:22 - significance and this suggests that this
131:25 - new feature that we got this new version
131:28 - of our landing page with this um uh call
131:33 - to action um as the and now is better
131:37 - and result in higher statistically
131:40 - significantly higher clickr rate than
131:42 - the existing version of the control uh
131:45 - group so there is a real effect then
131:48 - otherwise if this is not the case which
131:50 - means that my P value is larger than my
131:52 - Alpha then I'm saying print that there
131:55 - is no seral significance and that the
131:57 - observed difference that we see in the
132:00 - clickr rate is not because uh of the
132:03 - real difference in the performance but
132:05 - TR truly this is just the random chance
132:08 - so here we can see that once we run our
132:11 - we call the function in here which is
132:14 - simply the function name and the
132:15 - argument so P value and alpha alpha
132:18 - comes from the initialized value that we
132:20 - had from our power analis so
132:24 - from here we initialize this value
132:28 - 0.05 and then here we got the P value
132:32 - that we just calculated then what we are
132:35 - getting in here is that our P value is
132:37 - actually so small that it's um rounded
132:41 - to the zero so what this means is that
132:44 - that there is evidence that suggest that
132:46 - at 5% statistical level significance
132:49 - level that the uh clickr rate of the
132:52 - experimental group is different from the
132:54 - clickr rate of the control group note
132:57 - that I'm not saying higher or lower
132:59 - because our statistical test was
133:02 - two-sided so under new hypothesis we had
133:05 - that the uh P control so in here as you
133:09 - can see our P control was equal to P
133:12 - experimental and under the alternative
133:14 - we had that the P control is not equal
133:16 - to P experimental this means that we um
133:20 - have now how rejected the null
133:22 - hypothesis we have found evidence that
133:25 - suggests that the null hypothesis can be
133:28 - rejected since our P value is zero and
133:31 - it's smaller than the statistical
133:33 - significance level
133:35 - 5% and this means that we can reject the
133:37 - hle and we can say that uh there is
133:41 - enough evidence to say that P control is
133:43 - not equal to P
133:45 - experiment and given that that we saw
133:49 - from the uh visualization and from our
133:51 - calculations that the um clickr rate for
133:55 - our experimental group is much
133:58 - higher then the click rate of the uh
134:02 - control group we can also say that we
134:04 - have found evidence that at 5%
134:07 - significance level we have found out
134:11 - that there is a statistically
134:12 - significant difference between the
134:15 - experimental and control groups clickr
134:17 - rate and that the experimental groups
134:19 - clickr rate is actually higher so
134:21 - statistically significantly higher than
134:24 - the control versions clickr rate so this
134:28 - is really important because this
134:30 - suggests that this difference in their
134:32 - click to rate is not due to random
134:34 - chance Alan but truly that there is
134:36 - evidence statistical evidence that can
134:38 - support this hypothesis that there is a
134:41 - true difference between the performance
134:44 - of the experimental version of the
134:46 - product so in this case in our case the
134:49 - landing page that has enrolled now
134:51 - button versus the control version of the
134:54 - product which had the uh start free
134:57 - trial version of the landing page the
134:59 - existing version so beside of
135:01 - calculating this P value it's always a
135:03 - great practice to also visualize your
135:06 - results and this is great for your
135:08 - audience who are technically sound and
135:11 - who know uh these different concepts and
135:14 - you want to visualize uh the results
135:16 - that you got not only by showing some
135:18 - number that is the P value and say hey I
135:21 - have a statistical significance but you
135:23 - also want to showcase the actual picture
135:26 - of what you got what is your test
135:28 - statistic what is the significance level
135:30 - that you use to kind of tell a story
135:32 - around your numbers and that's the uh
135:35 - art behind the data science I would say
135:38 - so let's go ahead and do some art so
135:40 - what I'm doing here is that I am making
135:43 - use of my standard normal distribution
135:45 - or the gaan distribution the way that we
135:48 - are referring to the standard normal
135:49 - distribution in statistics I'm saying
135:51 - that my mean or the MU is equal to zero
135:54 - my Sigma is equal to one which is my
135:55 - standard deviation and I'm saying that
135:58 - my uh I want to now plot my uh standard
136:01 - normal distribution by getting my uh X
136:05 - values which are the uh number of uh X
136:09 - elements that I want to have my xaxis
136:11 - and then taking the PDF or the
136:14 - probability distribution function for
136:16 - the normal distribution by using the CP
136:18 - Library I'm then providing my my X
136:22 - values for which I want to get
136:25 - my uh
136:27 - corresponding uh values of Y so
136:29 - basically here are all the values
136:31 - between let's say minus something minus
136:34 - three and then so between - 3 and three
136:38 - and I want to find all the Y's
136:40 - corresponding to this which basically
136:42 - plus the probability distribution
136:44 - function of the goian distribution or
136:45 - the standard normal distribution and
136:47 - then I want to add to this graph all
136:50 - also the uh corresponding rejection
136:53 - region and as you can see it is here so
136:57 - then what I'm adding here by using this
137:00 - part of the plot is that I want to fill
137:03 - in the rejection regions so I'm saying
137:06 - for all the values in this figure
137:09 - whenever the uh value is lower than that
137:13 - threshold in this case the threshold is
137:15 - z critical
137:17 - 1.96 so whenever my threshold is smaller
137:21 - than minus this uh
137:24 - 1.96 and larger than this
137:27 - 1.96 then we are in the rejection region
137:31 - we are saying then if my test statistics
137:34 - is falling in the rejection region in
137:36 - this case you can see that we are in the
137:38 - far left so the test statistic is minus
137:43 - 59.4 and it's much lower than this
137:46 - threshold as you can see in here this is
137:49 - this left Blue Line in here
137:51 - then in this case it falls in this
137:53 - rejection region so actually this entire
137:56 - thing is the rejection region it starts
137:59 - from here and it goes all the way to
138:01 - here anything anything in this region
138:04 - means that we need to we have a test
138:07 - statistic fully in the rejection region
138:10 - which means that we can reject to n
138:12 - hypothesis if we were to get a test
138:15 - stais that is very large and very
138:17 - positive it means we would be in this
138:19 - part of the figure and again in the
138:21 - rejection region anything above this
138:24 - line is then uh going under this
138:26 - category of rejection region and also
138:29 - anything in here so for anything in here
138:33 - we are in the rejection region being in
138:36 - the rejection region it means that we
138:38 - can reject the hypothesis and we can say
138:41 - that we have a statistically significant
138:44 - results so now when we have our
138:46 - statistical significance it's always a
138:49 - great idea to go on to the next step and
138:51 - it's actually mandatory to do this
138:54 - because not only a statistical
138:56 - significance is important but also the
138:58 - Practical significance as I mentioned in
139:00 - the beginning of this case study so for
139:03 - that what we are going to do is first we
139:05 - are going to calculate a confidence
139:06 - interval of the test and this confidence
139:09 - interval will help us to first of all
139:12 - make um comments regarding the quality
139:15 - of our test and its
139:17 - generalizability uh at our entire
139:20 - population
139:21 - and the accuracy of our results and then
139:24 - we will use this confidence interval to
139:26 - make a comments and to test for the
139:29 - Practical significance in our AB test so
139:33 - let's go ahead and calculate the
139:35 - confidence interval so as we learn as
139:38 - part of our lectures the confidence
139:39 - interval can be calculated by first
139:42 - taking the uh P experimental head and P
139:45 - control head and the standard error and
139:47 - the Z critical so here we need the two
139:49 - different estimat of the experimental
139:51 - groups click through rate and the
139:53 - control groups click through rate we
139:55 - also need the standard error of our two
139:57 - sample Z test as well as the critical
140:00 - value and then we need to First
140:02 - calculate the lower bound of our
140:04 - confidence interval and then we need to
140:06 - calculate the upper bound of our
140:07 - confidence interval and in this case uh
140:10 - given that the um statistical
140:13 - significance level we are using is
140:15 - Alpha uh the uh Z critical is based on
140:19 - that therefore for we are also saying
140:21 - that we are calculating the 95%
140:24 - confidence interval so in here the way
140:28 - we will calculate the lower bound is by
140:30 - taking the P experimental head
140:32 - subtracting from that the P control head
140:34 - and then once we have done that we then
140:36 - substract from that the standard error
140:38 - multiply by Z critical volume and we are
140:41 - just rounding this up up to the three
140:43 - decimal behind the zero then we are
140:46 - doing the same thing only with a plus
140:48 - sign in here for the opp upper bound
140:51 - calculation of the confidence interval
140:53 - so this is just pure following the
140:55 - formula of the confidence interval that
140:57 - I will set you
140:58 - here and let's go ahead and print this
141:03 - value which is this interval so what we
141:06 - are seeing here is that we have a
141:08 - confidence interval that is from
141:10 - 0 399 so
141:13 - 0.4 to 0 uh
141:16 - 43 so quite a narrow confidence interval
141:19 - I would say which is actually a good
141:21 - sign because this confidence interal
141:24 - that provides this range of values
141:26 - within which the true difference between
141:28 - this control and experimental groups
141:30 - proportions or the clickr rate is likely
141:33 - to lie within a certain level of
141:35 - confidence in this case 95% confidence
141:39 - this is very narrow and if it's a narrow
141:41 - confidence interval it means that the uh
141:45 - accuracy of our results is higher and it
141:48 - means that the results we are getting
141:50 - BAS B on our smaller sample it will most
141:53 - likely generalize well when we apply
141:55 - these changes and deploy these changes
141:58 - and we put this new product in front of
142:00 - the entire population of users because
142:02 - now we are doing all this experiment for
142:04 - a small group for the sample and this
142:08 - confidence info that is narrow it's not
142:10 - wide it's narrow it means that the
142:12 - results that we are getting is are
142:15 - accurate more or less accurate and this
142:17 - means that we the results that we are
142:20 - getting based on a sample are most
142:22 - likely a true representation of the
142:25 - entire population that we got this is
142:27 - the idea behind the width of the
142:29 - confidence interval the narrower it is
142:32 - the higher uh is the quality of your
142:34 - results which means that the uh more
142:37 - generalizable are your results so let's
142:41 - now go on to the final stage of our case
142:44 - study which is to test the Practical
142:46 - significance of our results so now when
142:49 - we know that the statistical
142:51 - significance is there the experimental
142:54 - version of our feature is statistically
142:56 - significantly different from the control
142:58 - version in terms of the clickr rate and
143:01 - we have seen that the competence
143:02 - interval is narrow which means that our
143:04 - results are accurate quite uh with quite
143:08 - high accuracy then we can now comment on
143:11 - the Practical significance of our
143:13 - results this means we want to see
143:15 - whether the significant difference that
143:18 - we obtained whether this difference is
143:20 - actually large enough from the business
143:23 - perspective to say that it's worth to
143:25 - put our engineering resources and our
143:28 - money and our uh uh product into uh to
143:34 - put through through this change and to
143:36 - uh say that it's wor from the business
143:39 - perspective to change this button and to
143:42 - put this into um the production and in
143:45 - front of our users and of course here we
143:48 - are not only talking about the engineer
143:50 - ing resources that it will take from us
143:52 - to change this and the deployment and
143:55 - the monitoring but also in terms of the
143:58 - quality of the product we are providing
144:00 - to our users because whenever we are
144:01 - making a change to our product it is a
144:04 - risk because we are changing what our
144:06 - user is used to see and this can always
144:09 - be scary uh when it comes uh to the
144:13 - business because we don't want to uh
144:15 - make our customers scared so therefore
144:19 - we need to also check for this practical
144:21 - significance so for that what I'm doing
144:23 - is that I'm creating this python
144:25 - function that will take two arguments so
144:28 - two values that is the minimum
144:29 - detectable effect and then the 95%
144:32 - confidence interval that I just
144:34 - calculated those will be the two
144:36 - arguments for my function and I'm
144:37 - calling this function is practically
144:40 - significant and this function will go
144:43 - and check whether the uh practical
144:46 - significance is there or not and it will
144:48 - then return through true or false and
144:51 - then it will also print whether we have
144:53 - a practical significance or not and we
144:56 - learned from the theory and we know from
144:58 - this AB testing concept that whenever
145:01 - the uh MD or the Delta that we got the
145:04 - minimum detectable effect is larger than
145:07 - the lower bound of our confidence
145:09 - interval it means that the lowest
145:12 - possible value that we can get based on
145:15 - the results that we obtain in our
145:17 - sample that that amount is smaller than
145:20 - and the minimum detectable effect that
145:23 - we assumed before even conducting our AB
145:26 - test this suggests that we have a
145:29 - practical significance and the
145:30 - difference the minimum difference that
145:32 - we will obtain is large enough for us to
145:35 - have a motivation to make this change in
145:38 - our product for that what I'm doing is
145:42 - that first I'm taking my 95% confidence
145:44 - interval and I'm taking the first
145:47 - element because we know that a
145:48 - confidence interval is actually ranged
145:50 - so two pull of two numbers the lower
145:52 - bound and upper bound I need the lower
145:54 - bound because all I care for this
145:56 - practical significance is to compare the
145:58 - lower bound of the 95% confidence
146:01 - interval to this minimum detectable
146:04 - effect which is my Delta so therefore
146:06 - I'm taking this lower bound of
146:08 - confidence interval putting that into a
146:11 - variable and then I'm using this
146:14 - variable this lower uncore bound uncore
146:17 - CI confidence interval and I'm comparing
146:20 - this to my Delta I'm saying if my lower
146:23 - Bond of the confidence
146:25 - interval actually I'm noticing that here
146:28 - I got a mistake it should be the other
146:29 - way around we need to say that if our
146:33 - Delta is larger or
146:36 - equal the uh lower bound of the
146:39 - confidence interval which is the same as
146:42 - if
146:42 - our lower bound of the confidence
146:46 - interval is smaller than equal our Delta
146:50 - so if our we can also write this the
146:52 - other way around so if our
146:55 - Delta
146:56 - is larger than equal than our lower
147:00 - underscore bounde uncore CI then we can
147:05 - say that we have a practical
147:06 - significance so we the MDA of in this
147:10 - case so I want to use my initial Delta
147:13 - therefore I won't be initializing this
147:16 - so you might recall here a Delta of 10%
147:19 - I want to still make use of that Delta
147:22 - so therefore I will just go ahead and
147:25 - then in here what I want to do is to
147:27 - call this function by using that
147:30 - specific Delta so I want to have a 10%
147:33 - as my MD and whenever this Delta will be
147:36 - larger than the lower bound of my
147:38 - confidence interval that I just obtained
147:41 - I will then say that we have a practical
147:45 - significance and with an MDA of 10% the
147:49 - differ between control and experimental
147:52 - group is also practically significant so
147:55 - you can see that the lower bound is 0.04
147:58 - something that we obtain in here and
148:00 - that amount is then compared to this
148:04 - Delta and here you can see that we have
148:08 - concluded that we also have a practical
148:12 - significance so amazing we have come to
148:15 - the end of this case study and in this
148:17 - involved case study we have conducted an
148:19 - entire app AB has results in ases so
148:22 - this case study a has and to end going
148:25 - from the point of loading the data and
148:28 - then understanding this business concept
148:30 - or business objective of ab test where
148:32 - we were testing whether the um enroll
148:36 - Now button which is the new version the
148:38 - experimental version should replace the
148:40 - existing button which is the secure
148:43 - vraal and based on this case study what
148:45 - we found out is that we have a
148:47 - statistical significance at 5%
148:50 - significance level suggesting that we
148:51 - can reject the N hypothesis and we can
148:53 - say that indeed there exists a
148:56 - statistical significant difference
148:58 - between the click through rate in the
149:00 - experimental group versus control group
149:03 - uh and specifically that the enroll now
149:05 - experimental button results in
149:07 - statistically significantly higher click
149:09 - through rate than the uh secure free
149:12 - trial button and beside this we also
149:15 - checked the um accuracy of our results
149:18 - by looking at a confidence interval and
149:20 - saw that the confidence interval was
149:21 - quite narrow suggesting that the results
149:24 - we obtained were quite uh accurate and
149:27 - this means that the results that we got
149:29 - for the sample will generalize to our
149:31 - population of users and finally we have
149:34 - also checked the Practical significance
149:36 - of our results by using the 95%
149:39 - confidence interval and comparing the
149:41 - lower bound of that interval with our
149:44 - minimum detectable effect Delta and we
149:47 - saw that we will have at least 10%
149:50 - significant difference between the
149:52 - control groups CTR and the control uh
149:55 - the experimental group CTR and the
149:57 - experimental group CTR will be at least
149:59 - 10% higher than the uh control groups
150:03 - and this suggests that uh from the
150:05 - business perspective we also have a
150:08 - motivation uh beside of this statistical
150:11 - significance we also have practical
150:12 - significance suggesting that we also
150:14 - have enough motivation and reason from
150:17 - the business perspective to put this new
150:20 - button into production so we can
150:22 - conclude that uh based on this
150:24 - datadriven approach and conducting an AB
150:27 - testing we uh can see a clear motivation
150:31 - of deploying this new button and draw
150:34 - now and replace the existing one secure
150:38 - free trial version and we will then
150:40 - expect to see more users clicking on
150:43 - this and engaging with our product and
150:46 - for now this will be all for this case
150:48 - study if you want to learn more about AB
150:50 - testing make sure to check our AB
150:53 - testing course as well as the ultimate
150:55 - data science boot camp don't forget to
150:57 - try our free trial this time using our
151:00 - enroll Now button and if you want to see
151:03 - more case studies like this make sure to
151:06 - check our tic case studies we have many
151:08 - case studies also included as part of
151:10 - our ultimate data science boot camp
151:12 - where we go in detail of these different
151:15 - steps and we conduct different sorts of
151:17 - case studies to put our data science
151:19 - theory in to practice including from the
151:21 - field of NLP machine learning
151:23 - recommended systems Advanced analytics
151:26 - and also AB testing and soon also from
151:29 - AI so for now thank you for staying with
151:32 - me and conducting this case study happy
151:38 - learning thank you for watching this
151:40 - video If you like this content make sure
151:42 - to check all the other videos available
151:44 - on this channel and don't forget to
151:46 - subscribe like and comment to help the
151:49 - algorithm to make this content more
151:51 - accessible to everyone across the world
151:54 - and if you want to get free resources
151:57 - make sure to check the free resources
151:59 - section at lunch. and if you want to
152:02 - become a job rate data scientist and you
152:05 - are looking for this accessible boot
152:07 - camp that will help you to make a job
152:09 - ready data scientist consider enrolling
152:12 - to the data science boot camp the
152:14 - ultimate data science boot camp at
152:17 - l. you will learn all the things the
152:20 - fundamentals to become a jbre data
152:22 - scientist he will also implement the
152:25 - learn theory into a real world multiple
152:29 - data science projects beside this after
152:32 - learning the theory and practicing it
152:34 - with the real world case studies you
152:36 - will also prepare for your data science
152:38 - interviews and if you want to stay up to
152:41 - date with the recent developments in
152:42 - Tech what are the headlines that you
152:44 - have missed in the last week what are
152:47 - the open positions currently in the
152:49 - market across the globe and what are the
152:51 - tech startups that are making waves in
152:53 - the tech and sure to subscribe to the
152:56 - data science Nai newsletter from lunar
152:58 - Tech
153:03 - [Music]