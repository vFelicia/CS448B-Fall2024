00:00 - ready to learn mathematics behind
00:01 - machine learning generative AI or
00:04 - Cutting Edge teag then you are in the
00:06 - right place today we are going to learn
00:08 - linear algebra like never before in this
00:11 - Crush course in 6 hours we are going to
00:14 - learn the basics and the fundamental
00:17 - concepts in linear algebra a fundamental
00:19 - part in mathematics when it comes to
00:21 - many applied sciences our Crush course
00:24 - in fundamentals to linear algebra will
00:26 - be a great starting point for anyone who
00:28 - wants to refresh their your algebra and
00:30 - knowledge or who wants to start with the
00:33 - fundamentals of mathematics for AI data
00:36 - science machine learning or generative
00:38 - AI so if you are looking for that one
00:41 - course to refresh your memory in those
00:43 - fundamental concepts and be able to
00:45 - create your own algorithms and create
00:48 - your own AI dream products then you are
00:50 - in the right place and if you're really
00:52 - serious about your education and career
00:55 - and backup with your mathematical
00:57 - foundations then you can also check our
00:59 - 20 5 plus our comprehensive fundamental
01:02 - to linear alup course which is also part
01:05 - of our mathematics boot camp so check at
01:08 - lunch. and you can get this
01:10 - comprehensive 25 plus hour course along
01:12 - with the certification in linear algebra
01:15 - so here's what we are going to cover as
01:17 - part of our fundamentals to linear
01:19 - algebra crash course so we are going to
01:22 - start with the introduction to linear
01:23 - algebra we're going to talk about the
01:25 - prerequisites what is exactly that you
01:27 - need in order to get into linear algebra
01:29 - whether as a student as part of your
01:31 - undergrad or Graduate Studies or as a
01:33 - working professional who wants to learn
01:35 - the mathematics in a good way in order
01:37 - to become a well-rounded professional
01:39 - whether it is for your future in quantum
01:42 - physics in a future of data science
01:44 - machine learning or artificial
01:46 - intelligence so we are going to learn
01:48 - the basics of the vectors this is the
01:50 - first section that we are going to cover
01:51 - here we are going to learn the vectors
01:53 - their basic definitions the theory but
01:56 - also the examples so in this course we
01:58 - are not just going to scratch the
01:59 - surface we are going to dive deep and
02:01 - the idea is that for even for those
02:04 - basic concepts we are going to learn not
02:05 - just the theory the basic definitions
02:08 - but we are also going to implement them
02:10 - in different examples we are going to
02:12 - manually solve different examples by
02:14 - hand in order to truly master and
02:17 - practice this topic and beside this
02:19 - we're also going to talk about the
02:20 - applications of them across different
02:22 - domains including in data science and AI
02:25 - the idea is that those basic concepts
02:28 - are your starting point in the future of
02:30 - linear algebra because to truly
02:33 - understand this topic and to truly
02:35 - understand this entire linear algebra
02:36 - field at least the course the academic
02:39 - level course that you are used to seeing
02:41 - as part of the different Advanced
02:43 - mathematical studies you will need to
02:45 - dedicate more than just 6 hours so if
02:47 - you are serious about your education and
02:49 - career and you want to learn mathematics
02:52 - from ground up then check also our 25
02:55 - plush hour complete fundamental to
02:57 - linear AR records to truly Master the
03:00 - entire linear algebra in a very economic
03:03 - and efficient way which contains an
03:05 - entire semester equivalent material
03:09 - Theory practice and the applications so
03:12 - here's what we are going to cover as
03:14 - part of our crash course so we will
03:16 - start with the introduction to linear
03:18 - algebra we will talk about this concept
03:19 - of linear algebra we will see what are
03:21 - the prerequisites in order to even
03:24 - understand and start with this course
03:26 - we're going to refresh our memory with
03:28 - the different fundamental concepts
03:30 - here in this introduction to linear
03:32 - algebra section we are going to quickly
03:34 - cover the prerequisite that you must
03:36 - know in order to get started with linear
03:38 - algebra here I'm also going to refresh
03:41 - our memory on the fundamental concepts
03:43 - like real numbers and Vector spaces the
03:46 - norm of a vector the cian coordinate
03:48 - system and also the angles triogen
03:51 - ometry the norm versus equid in distance
03:53 - pedian theorem Etc so uh after this we
03:57 - are going to finally Kickstart dealing
03:59 - your algebra Concepts so first up we are
04:01 - going to talk about the basic Vector
04:04 - spaces here we are going to cover the
04:06 - definition of the vectors what are
04:08 - vectors the basic representation of it
04:11 - but also the indexing of it and how we
04:14 - can apply different simple operations
04:16 - like vector addition Vector subtraction
04:19 - and also scalar multiplication after
04:21 - this in the next section we are going to
04:23 - talk about the matrices and the basic
04:25 - operations we are going to truly
04:27 - understand the matrices the basic
04:29 - definition of them we are going to see
04:30 - many examples of matrices and also in
04:33 - practice what are the basic operations
04:35 - that we can apply to matrices including
04:37 - the additions the scale and
04:39 - multiplication and
04:40 - subtractions after this we are going to
04:43 - look into the dot product and the vector
04:45 - length in this section we are going to
04:46 - talk about the idea of dot product what
04:49 - is the difference between dot product
04:51 - and inner product understand the
04:53 - fundamentals of that product and how
04:54 - they are calculated we're going to
04:56 - complete many examples as this a very
04:59 - important topic especially for data
05:01 - science machine learning in
05:03 - Ai and once we are done with this we
05:05 - will also look into this concept of the
05:07 - length of a vector and how this is
05:10 - related to for instance cosine rule and
05:12 - these different inequalities when it
05:14 - comes to the vector spaces after to this
05:17 - we are going to look into the basic
05:18 - linear systems how we can solve a linear
05:21 - system by using the infamous gaussian
05:24 - elimination here we are going to talk
05:26 - about this basic concept of solving
05:28 - linear equations with a many unknowns
05:30 - using the concept of matrices we're are
05:33 - going to see detailed examples where
05:36 - manually step by step by hand we are
05:38 - going to derve the um entire process
05:40 - using gin elimination how we can solve
05:43 - the system of linear equations first
05:45 - getting the arent Matrix and then this
05:48 - concept of reduce row form so RF and
05:51 - then R RF so row H form and reduce Ro
05:54 - form and uh we are going to look into
05:57 - two different scenarios to ensure that
05:59 - we
06:00 - understand this intuition behind solving
06:01 - this linear systems in various um type
06:05 - of cases now we are going to uh finish
06:08 - off uh the crash course with the preview
06:11 - of the advanced concepts we are going to
06:13 - see the special matrices we are going to
06:15 - see symmetric matrices diagonal matrices
06:19 - and then uh the idea of uh n matrices W
06:22 - matrices how they are used uh and why
06:25 - they are used commonly in programming
06:27 - and then we are going to see a glimpse
06:29 - of how linear algebra can be applied
06:32 - across real world scenarios so in this
06:36 - crash course we are not just going to
06:37 - cover the uh quick definitions but we
06:40 - are going to dive deep we are going to
06:41 - truly try to understand those concept
06:44 - those uh geometric intuition and
06:46 - interpretation behind those Concepts and
06:48 - how they are related uh towards many
06:50 - applied sciences so to truly understand
06:54 - how each of those concepts are actually
06:56 - implemented in the real world like in
06:59 - the fields of data science statistics
07:01 - and AI this course is perfectly tailored
07:04 - for working professionals who need to
07:06 - sharpen their math skills who want to uh
07:09 - quickly and efficiently refresh their
07:11 - memory whether is before their
07:12 - interviews or who want to learn machine
07:15 - learning and statistics and AI but they
07:17 - don't have the mathematical background
07:20 - and instead of just going and studying
07:22 - at University like I did and spending
07:24 - many years to do an undergraduate uh
07:27 - studies in mathematics or statistics
07:30 - instead you can just follow this course
07:32 - to quickly refresh your memory but if
07:34 - you truly want to understand all the
07:36 - concepts which is equivalent to one
07:38 - semester or linear algebra course then
07:41 - definitely check out our 25 plus hour
07:43 - fundamentals to linear algebra course as
07:46 - part of our mathematics boot camp at
07:49 - l. because you can't master linear
07:51 - algebra in just 6 hours so if you are a
07:54 - student this course will also be a good
07:57 - way for you to refresh your memory if
07:59 - you you are at the end of your deadline
08:01 - and you are about to complete your
08:02 - linear track exam and if you want to
08:05 - learn everything from scratch and you
08:07 - have the time then you can also check
08:09 - our 25 plus hour course to learn the
08:11 - linear algebra fully and to Ace your
08:14 - exams this crash course is just the
08:17 - beginning for you in this way you are
08:19 - laying a solid foundation to become a
08:21 - well-rounded professional and it truly
08:24 - understand this topic of linear algebra
08:26 - so if you're ready I'm really excited so
08:29 - let's let's get started welcome to the
08:31 - mathematics for data science and AI road
08:33 - map in 2024 today we are going to
08:36 - discuss the road map behind a topic that
08:39 - powers many algorithms the linear
08:41 - algebra we are going to talk about all
08:44 - the concepts and the topics that you
08:46 - must know in order to master the linear
08:48 - algebra which is the backbone of data
08:51 - science machine learning generative Ai
08:53 - and many other topics and Cutting Edge
08:56 - Tech whether you are aspiring to become
08:58 - a data scientist or AI professional this
09:01 - is your starting point to learn the
09:03 - mathematics of data the fundamentals or
09:05 - linear algebra if you want to become a
09:08 - good professional a well-rounded data
09:10 - scientist and master the art of machine
09:13 - learning generative Ai and many other
09:15 - Cutting Edge Tech domains then your
09:18 - starting point should be to learn the
09:19 - mathematics to truly understand this
09:22 - different mathematical Concepts that
09:24 - power the algorithms behind the scenes
09:26 - including many Infamous algorithms like
09:29 - the GPT series or the bird or
09:31 - Transformers linear algebra is the
09:33 - mathematics of data it's about the
09:35 - vectors matrices projections and how to
09:38 - solve a system of linear equations and
09:41 - how to decompose your Matrix into many
09:44 - parts to simplify the equations and
09:46 - solving of the linear systems think
09:48 - about metric factorization that is used
09:50 - as part of the recommended systems
09:52 - including the ones in the Netflix that
09:54 - uses the streaming company to recommend
09:57 - you movies when you go to the net
10:00 - so linear albra is one of the most
10:03 - fundamental and must to know topics when
10:06 - it comes to mathematics for your data
10:08 - science AI Journey so let's now dive
10:11 - into the road map behind linear algebra
10:14 - and how you can learn linear algebra in
10:17 - 2024 I'm going to make use of the
10:19 - curriculum and the road map that we have
10:21 - carefully drafted as part of our 25 plus
10:24 - hour fundamental SAR algebra course
10:27 - which is also part of our mathematics
10:29 - boot camp at
10:31 - lunch. so I'm going to make use of that
10:34 - road map and I'm going to also make use
10:36 - of the applications of lunch. a in order
10:39 - to Showcase you all these different
10:41 - topics as part of linear algebra road
10:43 - map of 2024 so let's
10:46 - di first we need to start with the
10:48 - linear algebra introduction you will
10:50 - need to understand this Concepts behind
10:52 - linear algebra where it is positioned in
10:55 - this entire field of mathematics and why
10:57 - you exactly needing your algebra to
10:59 - study so um first you need to learn the
11:03 - prerequisites and all this must you know
11:05 - topics that you need in order to be able
11:07 - to understand linear algebra think about
11:10 - the real numbers the vector space this
11:12 - concept of the norm of a vector the
11:15 - length of a vector the cation coordinate
11:17 - system which is usually covered as part
11:19 - of the pre-algebra or algebra courses
11:22 - and also the Angles and tri gometry
11:24 - think about the cosine the sign cosine
11:27 - rule the sinus rule the Tang
11:29 - function how it looks like when you
11:31 - visualize it the geometric
11:33 - interpretation behind it and also this
11:35 - concept of Al and distance and how Elan
11:38 - distance is different from the norm and
11:41 - finally you also need to understand this
11:43 - concept of Pythagorean theorem and the
11:46 - orthogonality so when it comes to the
11:47 - linear algebra one thing to keep in mind
11:50 - is that linear algebra is usually a
11:52 - course that is part of the undergrad
11:55 - studies like computer science
11:56 - econometrics advanced mathematics
11:58 - advanced statistics and usually the
12:00 - students of the studies they in the
12:03 - second or third year of their bachelor
12:05 - they learn linear algebra and this road
12:08 - map that I'm providing to you is based
12:10 - on academic level plus the industry
12:12 - Insight curriculum which means that uh
12:15 - the um uh concept is quite technical so
12:19 - if you want to truly understand the
12:21 - linear algebra and you are serious about
12:23 - your career and your education then this
12:26 - road map on one hand might seem bit
12:29 - Technical and intimidating but on the
12:31 - other hand you need to understand that
12:33 - students spend about semester on this
12:36 - entire road map but if you understand
12:39 - these Concepts this will open many doors
12:41 - for you and instead of just using the
12:44 - libraries of people who have created for
12:46 - example large language models like the
12:48 - gpts or the bird or the uh mixtur uh AI
12:53 - models instead of just using those
12:55 - libraries and pre-trained model you can
12:57 - create your own models and I think
12:59 - that's a good enough motivation for us
13:01 - to truly understand and invest the time
13:04 - and motivation to master the art of
13:06 - linear algebra so once you have these
13:09 - prerequisites that usually come from
13:11 - prealgebra or courses like the Calculus
13:13 - 1 and calculus 2 to uh learn these
13:16 - different concepts you are ready to go
13:18 - onto the first journey and first section
13:22 - behind the linear algebra which is to
13:24 - learn the vectors and operations so be
13:27 - prepared to learn the foundations of the
13:28 - vector s the special vectors and
13:31 - operations and let me actually go ahead
13:33 - and get more uh in detail curriculum for
13:35 - you so um start to understand uh the
13:40 - vectors and operations what are the
13:41 - foundations of vectors the fundamentals
13:43 - of linear algebra the scalers and the
13:45 - vectors this representation of the
13:48 - vectors in terms of the magnitude and
13:49 - the direction and also understand this
13:52 - uh Vector notation the indexing of the
13:55 - vectors first upop you can uh start with
13:57 - learning vectors and operations so here
13:59 - you need to understand this uh scalers
14:02 - the vectors the representation of the
14:03 - vectors like the magnitude and Direction
14:06 - what is a common notation of the vectors
14:08 - in the industry as well as this indexing
14:11 - in vectors so understand what the
14:13 - indexing behind vectors are because then
14:15 - you need to implement this in
14:18 - programming languages like
14:20 - python then uh once you are clear on
14:23 - those topics then you can get into the
14:25 - special vectors and operations so
14:27 - understand what are the zero vectors and
14:29 - the unit vectors the sparsity vectors
14:32 - the vectors in higher Dimensions the
14:34 - vector addition and subtraction as well
14:36 - as the uh Vector uh multiplication so
14:39 - scalar multiplication how you can
14:41 - calculate the multiplication behind the
14:43 - scalar and a vector also learn to um the
14:47 - different properties of the vectors and
14:49 - the operations uh of these vectors so
14:53 - once you are done with that get into the
14:55 - advanced Vector Concepts so learn the
14:57 - linear combination concept and try to
15:01 - understand this concept of unit vectors
15:03 - the span of a vector what is this
15:05 - definition and the application of it uh
15:08 - the concept of linear Independence is
15:11 - really
15:12 - important understand this concept and
15:14 - also uh be able to uh analyze and see
15:18 - from uh multiple vectors whether they
15:20 - are linearly independent or not look
15:22 - into this concept of scalar
15:24 - multiplication as well as the
15:25 - application of scalar Vector
15:27 - multiplication and look into this
15:30 - concept of a length of a vector and a
15:32 - DOT product as well as what is the
15:34 - difference between the dot product and
15:36 - the inner
15:37 - product after this get into the topic of
15:40 - dot product the coui squares and its
15:42 - application so understand the concept of
15:44 - dot product the inner product what are
15:47 - the differences between the two what is
15:48 - the properties of dot product calculate
15:50 - dot product by hand on many examples and
15:53 - also learn this concept of COI squs CI
15:56 - squs is an Infamous inequality theorem
15:59 - comes from linear algebra and
16:02 - understanding its uh Theory but also
16:04 - intuition maybe you can even derive the
16:06 - proof of it using the cosine law and how
16:09 - that relates to this concept of the norm
16:11 - of a vector and when it is that the norm
16:14 - of a vector is equal to zero so this
16:19 - completes then the vector operation
16:21 - section uh in here this is the high
16:24 - level overview of the topics that you
16:25 - must know in order to learn the vectors
16:28 - and the operations and once you are done
16:30 - with this then you need to get into the
16:33 - next section and the uh domain in linear
16:36 - algebra which is the matrices and
16:37 - solving linear systems here I'm talking
16:40 - about foundations of linear systems and
16:42 - matrices the introduction to matrices
16:44 - the theory and the intuition behind it
16:46 - core Matrix operations gaan reduction n
16:49 - space Comm space Rank and the full rank
16:53 - so here think about the uh General
16:56 - linear systems like uh the co efficient
16:59 - labeling homogeneous versus
17:01 - non-homogeneous systems and what is this
17:04 - uh definition of matrices the uh
17:06 - notation of it as well as this this
17:09 - concept of the rows the columns the
17:11 - dimensions of matrices the identity
17:14 - Matrix diagonal matrix other other
17:16 - special type of matrices like uh one's
17:19 - Matrix the zero Matrix but also learn
17:22 - the core Matrix operations and practice
17:24 - with them like Matrix operations of
17:26 - addition subtraction scalar
17:28 - multiplication
17:29 - and also the multiplication behind
17:31 - multiple matrices once you are done with
17:34 - this um in terms of the solving of
17:36 - linear systems think about Matrix
17:38 - representation of linear systems the
17:39 - solving of linear systems using the
17:42 - infamous gausian elimination like
17:45 - understanding this argumented
17:46 - coefficient Matrix the row H form the
17:48 - reduced row H form the identity Matrix
17:51 - and the reduced row H form look into the
17:54 - detailed examples implement this in uh
17:57 - practice with multiple examp step by
17:59 - step by um manually deriving the
18:02 - solution to the system as well as how
18:04 - you can calculate a new space a com
18:06 - space and the basis of these different
18:08 - uh spaces on an actual example once you
18:12 - are done with this learn this concept of
18:14 - the rank and in what case you can say
18:16 - that the Matrix has a full rank next up
18:19 - is the section of linear transformation
18:21 - and matrices so learn the algebraic laws
18:24 - of matrices they approves the
18:25 - determinant the concept of the transpose
18:28 - and the inverses of matrices and here
18:31 - this is exactly what I mean in more
18:33 - detail so a way comes to the algebraic
18:36 - Row for matrices learn about the
18:38 - cumulative low for matrices associative
18:40 - low distribution low the scaler
18:42 - multiplication low for matrices and look
18:45 - into the proof of this lws as well as
18:47 - the examples after this get into the
18:50 - determinants and their properties look
18:53 - into the uh theory behind determinants
18:56 - why we use determinant how we can
18:58 - calculate related determinant on an
19:00 - example as as what are the properties of
19:03 - determinants and the geometric intuition
19:05 - behind it so when it comes to the Matrix
19:08 - inverses and identity Matrix look into
19:11 - the definition the theory behind it also
19:13 - this non- Singularity of matrices and
19:16 - what is this definition of and the
19:20 - formula when it comes to the inverse of
19:22 - a 2x2 matrix and how you can go from
19:24 - there to a larger matrices let's say 3x3
19:27 - matrices looking to the detailed
19:29 - examples calculate them inverses of the
19:33 - 2x2 matrices detailed uh in detailed
19:36 - examples and also do the same for the
19:39 - 3X3 matrices so how you can use this
19:41 - idea of conjugates and by changing these
19:44 - different signs to calculate the inverse
19:46 - of a 3X3 Matrix after this get into the
19:49 - uh applications of uh inverses of a
19:52 - matrixes why we need them what is the
19:54 - intuition behind it and where do we use
19:56 - it even when we are dealing with the
19:58 - very basic uh machine learning algorithm
20:01 - like um linear regression we use
20:04 - actually the transpose of matrices the
20:06 - inverse of matrices the multiplication
20:08 - behind matrices to find the uh solution
20:12 - to the uh linear system behind the
20:15 - linear regression so uh this is a very
20:18 - important concept that I recommend you
20:20 - to learn before getting into machine
20:23 - learning next up is the transpose of
20:25 - matrices learn the um properties of the
20:28 - transpose of matrices the U theory
20:31 - behind it but also the implementation
20:33 - and the relationship behind the dot
20:35 - product so once you are done with this
20:38 - the next St is to uh get into more
20:42 - advanced section which is the final
20:44 - section way it comes to mastering
20:45 - fundamentals of linear algebra which is
20:48 - this um different Advanced linear
20:50 - algebra topics think about Vector spaces
20:52 - the projections of vectors the grumme
20:55 - process the metrix factorization and
20:57 - look into couple of examples of Matrix
21:00 - factorization techniques that are widely
21:02 - used across the industry including the
21:05 - QR de composition the igen de
21:07 - composition which includes the
21:08 - calculation of igon values and igon
21:10 - vectors very popular topic as part of
21:12 - data science and statistical modeling
21:14 - also uh Ai and get into the singular
21:18 - value that composition which is the SVD
21:21 - so let's dive into the detailed uh
21:24 - details of this uh section when it comes
21:27 - to the vector spaces projections and the
21:29 - gred process look into the definition of
21:32 - it the step-by-step process behind it
21:34 - the uh projection formula and the
21:37 - step-by-step example behind the uh uh
21:40 - projection in order to truly understand
21:42 - how you can calculate the projection of
21:44 - a vector and also look into the
21:47 - geometric interpretation and the
21:49 - intuition behind vector projections uh
21:52 - learn the theory but also the
21:53 - implementation of the dotproduct of
21:55 - vectors and the dotproduct of a vector
21:57 - with itself
21:59 - look into the uh relationship between
22:01 - the dot product the vector of ve the
22:04 - projection of vectors and then look into
22:07 - the concept of orthonormal basis
22:10 - understand the orthogonality
22:12 - normalization and look into the infamous
22:15 - grme process understand how you can use
22:17 - the IDE of that products but also the
22:19 - projections in order to conduct this
22:21 - grme process step by step once you are
22:24 - done with this then uh look into the
22:26 - application of grme process and the
22:29 - auton normal bases so uh as part of
22:32 - these special matrices and their
22:33 - properties I would recommend you to
22:35 - learn the special matrices like the
22:37 - diagonal matrix the symmetric Matrix The
22:40 - orthogonal Matrix and the different uh
22:43 - applications and examples of them step
22:45 - by step once you are done with this
22:48 - learn this concept of Matrix
22:50 - factorization Matrix factorization is a
22:52 - very important topic when it comes to
22:54 - data science machine learning but also
22:56 - AI uh the metrix factorization different
23:00 - examples and techniques that um are
23:02 - falling under this umbrella of metrix
23:04 - factorization techniques that are used
23:06 - in both in machine learning but also in
23:09 - deep learning and in generative AI so if
23:12 - you want to become a true uh
23:15 - well-rounded professional in this Fields
23:17 - then definitely be prepared to learn
23:19 - metric authorization techniques dive
23:21 - into its applications understand the
23:24 - different examples of what kind of
23:26 - metric alization techniques are used in
23:28 - what uh algorithms and this will help
23:31 - you to get more excited and warm up for
23:34 - the next sections which are detailed
23:36 - examples and you truly understand at
23:39 - least couple of metrix factorization
23:41 - techniques so first up uh learn for
23:44 - example the QR de composition which is a
23:46 - matrix factorization technique learn the
23:49 - theory behind it the step-by-step
23:50 - process behind QR de composition how you
23:53 - can calculate the um q's and then the
23:57 - vectors of r so construct The Matrix of
24:00 - Q and then Matrix R and then uh
24:03 - decompose your Matrix a into uh q and
24:06 - then R matrices in order to decompose
24:08 - your entire Matrix look into detailed
24:11 - example the calculation of it uh with a
24:14 - stepbystep manual process and look into
24:16 - its practical consideration and
24:18 - application in solving linear systems
24:21 - after you are done with this look into
24:23 - the concept of IG value the composition
24:25 - how you can calculate the IG values and
24:28 - I vectors what is the theory but also
24:29 - the Practical implications of them where
24:32 - are ion values and igon vectors used for
24:35 - example in the infamous principal
24:37 - component analysis or PCA then look into
24:40 - this concept of ion de composition look
24:43 - into detailed examples and manually
24:45 - calculate the ion values and ion vectors
24:48 - as well as the igon value that
24:50 - composition so once you are done with
24:53 - this and you have looked into the
24:54 - applications of igon bi and ion vectors
24:56 - the calculation of it as well as the
24:58 - Theory getting into the final topic
25:00 - which is another Matrix factorization
25:02 - technique an Infamous one which is the
25:05 - singular value de composition the SVD
25:07 - look into the theory behind the SVD the
25:10 - left singular matrices the right
25:12 - singular matrices this uh theory behind
25:14 - them but also the Practical example
25:17 - behind them uh what is this idea behind
25:20 - um singular value de composition how you
25:23 - can construct The Matrix of s The Matrix
25:25 - of v and the Matrix of D and how you can
25:28 - decompose a matrix into the three
25:30 - different matrices by constructing them
25:33 - separately once you are done with this
25:35 - look into detailed example this might
25:37 - take a bit let's say an half hour or 1
25:40 - hour in the first go to calculate this
25:43 - different matrices by hand but trust me
25:46 - it is worth it because you will then
25:47 - truly understand those Concepts and you
25:50 - will understand how and why those uh
25:54 - Matrix factorization techniques are used
25:56 - and applied in the dat science and
25:59 - artificial intelligence welcome to the
26:02 - course on the fundamentals of linear
26:04 - algebra my name is D Vasan and today we
26:07 - are going to start with some basic
26:09 - concepts that are important for
26:11 - understanding linear algebra linear
26:13 - algebra is one of the most applicable
26:15 - areas of mathematics it is used by pure
26:18 - mathematicians that you will see in a
26:20 - universities doing research publishing
26:23 - research papers but also by the
26:25 - mathematically trained scientists of all
26:27 - disciplines
26:28 - this is really one of those areas in
26:30 - mathematics that you will see time and
26:32 - time again appearing in your
26:35 - professional life if you want to become
26:37 - a job ready uh data scientist or you
26:40 - want to do some handson machine learning
26:43 - deep learning Andi stuff but also linear
26:46 - algebra is used in cryptology it is used
26:48 - in cyber security and in many other
26:51 - areas of computer science and artificial
26:54 - intelligence so if you want to become
26:57 - this well-rounded Prof professional you
26:58 - want to go beyond using libraries and
27:01 - you want to truly understand the uh
27:04 - mathematics and the technical side of
27:07 - these different machine learning
27:08 - algorithms from very basic ones like
27:10 - linear regression to most complex ones
27:13 - coming from Deep learning like
27:14 - architectures in neural network how the
27:16 - optimization algorithms work how the
27:19 - gradient descent works and all these
27:21 - other uh different methods and models
27:23 - then you are in the right place because
27:26 - you must know linear algebra such that
27:28 - you will understand these different
27:30 - concepts from very basic ones to most
27:33 - advanced ones in data science machine
27:35 - learning deep learning artificial
27:38 - intelligence data analytics but also in
27:41 - many other applied science
27:44 - disciplines so before starting this
27:46 - comprehensive course that will give you
27:48 - everything that you need to know about
27:50 - linear algebra first I'm going to tell
27:52 - you what we assume that you already know
27:55 - because linear algebra it comes from
27:57 - about third uh year of Bachelors um of
28:01 - different uh highly technical studies
28:04 - and um here um we are assuming that you
28:08 - already know certain Concepts so uh to
28:11 - ensure that this course stays really on
28:13 - the topic of linear algebra and that you
28:17 - uh understand all these Concepts really
28:19 - well for that we need to uh be able to
28:23 - know different topics so before we dive
28:26 - into these Concepts uh let's familiarize
28:29 - ourselves with the basic prerequisites
28:31 - and notations used throughout this
28:33 - course and you will really need to know
28:36 - this in order to understand this
28:38 - Concepts really well such that instead
28:40 - of memorizing you will actually just
28:42 - hear me once or maybe twice and then
28:45 - every time you hear later on or you see
28:48 - it in the papers or in some algorithms
28:50 - you will recognize um this is something
28:52 - that we already
28:54 - learned so uh some key prerequisites
28:57 - overview is here
28:58 - um first of all to fully grasp the
29:01 - upcoming material you should be familiar
29:03 - with some basic concept like real
29:05 - numbers Vector spaces so you don't need
29:08 - to know this idea of vectors though you
29:11 - uh already most likely are familiar with
29:14 - this given that you know how to plot
29:17 - different uh lines you know the idea of
29:20 - x's and y's and how to plot these
29:22 - different graphs but um here we are
29:25 - going to touch base on this every time
29:27 - when we come close to this Concepts I
29:29 - will refresh you uh your memory and we
29:32 - will go through this numbers the idea of
29:35 - norms and distance measures because when
29:38 - it comes to the vectors when it comes to
29:40 - the magnitude and all these different uh
29:42 - topics that we are going to discussed as
29:44 - part of linear algebra knowing the what
29:47 - Norm is and um what is the definition of
29:50 - distance what is the length between uh
29:54 - two points when we plot it in the two
29:56 - dimensional space or three dimension
29:58 - space those are all very basic concept
30:00 - that usually you see as part of a basic
30:03 - pre-algebra or just a common algebra
30:06 - courses and um lessons in order to truly
30:11 - understand what the new algebra is about
30:13 - to understand this direction of vectors
30:15 - the angle and then um the uh
30:19 - dimensionality reduction how linear
30:20 - algebra is applied for instance in
30:22 - different algorithms in machine learning
30:24 - deep learning data science statistics
30:27 - you really need to understand this
30:28 - Cartesian coordinate system so uh this
30:31 - is not only important for linear algebra
30:34 - but I assume you already know it given
30:36 - that you have passed those um uh other
30:39 - courses like uh calculus or usually they
30:41 - are covered as part of prealgebra or
30:44 - algebra so the cartisian coordinate
30:47 - system I mean here understanding uh what
30:49 - is for instance the the common um a
30:52 - description of them for instance when
30:54 - you when we write like X and then y on
30:57 - the vertic axis and then we can uh we
30:59 - have here zero and um then uh we can
31:03 - always plot this different plots you
31:05 - know we we have a clear understanding
31:07 - what is this um Y is equal to X line we
31:11 - understand how by knowing certain points
31:13 - we can plot different plots for instance
31:16 - that this is the Y is equal to X line
31:18 - that here it means that if we have here
31:21 - one then this is just one two this is
31:23 - two so we understand when we have the
31:26 - function of the line and we have a
31:28 - certain value that is our y coordinate
31:31 - or x coordinate then the corresponding
31:33 - uh coordinates can be found then um you
31:37 - also need to know um some basic things
31:40 - that I just didn't mention uh right now
31:43 - so for instance that the numbers here
31:44 - can be like 1 2 three up to Infinity so
31:48 - you understand this concepts of infinity
31:50 - and then here the same uh story then
31:53 - here we have minus one you know Min - 2
31:57 - uh and then this is then used later on
32:02 - and we will be uh touch basing this one
32:04 - we will be describing our vectors and
32:06 - how uh we can visualize our vectors in a
32:10 - two dimensional space like we have here
32:12 - because this is two dimensional so we
32:13 - have X and Y but we can also of course
32:16 - visualize it in three-dimensional
32:19 - Etc so this idea of basic coordinate
32:22 - system is really important um usually
32:25 - covered as part of algebra if not pre
32:28 - algebra then we have basic triog gometry
32:31 - which means that you need to have a
32:33 - clear understanding what sinus is what
32:35 - cosine is what tangent is and their
32:37 - reciprocals and here I mean uh that you
32:40 - know for instance um what is cosine
32:43 - function what is sign function um you
32:46 - know that you have an
32:49 - understanding for instance that um uh
32:52 - what is this line you know um whether
32:55 - it's a sinus line or cosine line you
32:58 - have also an understanding what this Pi
33:00 - is um one thing that I didn't mention
33:03 - but it it just goes um around all these
33:07 - topics some basic things that you
33:09 - understand what is X what is why why we
33:12 - uh use them and this idea of uh
33:16 - variables uh and also uh you need to
33:19 - understand this idea of square uh or you
33:23 - know a
33:25 - 90° uh angle and then
33:28 - uh pagas theorem here we have the same
33:31 - so what is this relationship between
33:32 - different sides of a triangle that is a
33:35 - very unique triangle and that has one of
33:38 - the uh angles as 90° um and uh this idea
33:43 - of um you know the sides how this
33:46 - relates to the sinus cosinus tangent
33:49 - cotangent um and also um how the
33:52 - Pythagorean um Pythagorean theorem
33:54 - applies when we have uh uh triangular
33:58 - but it is no longer with a angle that is
34:02 - 90° what is the sum of all the angles of
34:06 - triangle so those are basic stuff that
34:08 - are com commonly covered as part of uh
34:11 - trigonometric uh lessons or part of
34:14 - General geometry so when it comes to
34:17 - this R so as part of real numbers and
34:20 - Vector spaces R represents the set of
34:23 - all real numbers so you can be dealing
34:26 - with for instance an integers like 1 2 3
34:29 - this can Al this will also cover all the
34:31 - negative numbers like minus one - 2 - 3
34:34 - but also the floting numbers like 1. 223
34:40 - and all the other numbers that you can
34:42 - think of those are the set of all real
34:46 - numbers so this is in onedimensional
34:48 - space right so you can see that I'm
34:50 - writing just one number you know two
34:53 - three and other numeric numbers then we
34:56 - have the idea of R2 R3 up to RN where
35:00 - all these numers they represent in this
35:02 - case the N it represents the N
35:05 - dimensional aidian space so when it
35:09 - comes to this idea of n dimensional
35:12 - numbers so for instance
35:15 - R2 here we just mean 2D plane so I'm
35:19 - pretty sure you are familiar with this
35:20 - idea of for instance
35:22 - x-axis and y axis here we are dealing
35:25 - with two dimensional plane so for every
35:29 - point that we can find here we can
35:31 - describe them by assigning them a value
35:34 - X so coordinate X and a coordinate y
35:39 - that's exactly what we mean by saying
35:42 - that the number can be represented in a
35:44 - 2d plane so here we are dealing with
35:46 - this two dimensional space this is our
35:49 - two-dimensional Elan space and every
35:52 - number in
35:53 - here that is part of this R2 can can be
35:58 - pictured here can be represented in this
36:00 - visualization so for instance if I have
36:03 - this number and let's assume that the
36:05 - value on the x- axis is two and we can
36:09 - see here that the corresponding Y is
36:11 - zero I can describe this number which I
36:13 - will call a I can describe this by
36:16 - writing down first the x coordinate
36:18 - which is two and then the y-coordinate
36:21 - which is zero so I'm then saying that a
36:24 - which is a point with x coordinate 2 and
36:27 - y coordinate zero it is part of my
36:31 - R2 and it's part of my two dimensional
36:34 - alian space when it comes to R3 similar
36:37 - thing we can do with that only in that
36:39 - case we need not just x axis and y AIS
36:43 - but we need to add our third dimension
36:46 - so here for instance when it comes to
36:49 - the r
36:51 - Tre then we need to do y AIS we need to
36:56 - have X axis but also we need to have
36:59 - some Z axis so such that every time
37:04 - every point in the space we can then
37:07 - describe by x y and Zed
37:12 - coordinates so if we write it in terms
37:14 - of the vector something that we will see
37:17 - very soon as part of our first unit of
37:20 - this course we will then need to
37:22 - represent every number in this
37:24 - three-dimensional Alan Space by writing
37:26 - down first the x coordinate let's say
37:28 - one and then y coordinate let's say
37:31 - another one and then Zed coordinate
37:33 - which is one or even better even easier
37:36 - let's use 0 0 0 which means that we are
37:38 - dealing with this initial number which
37:40 - is the center of this three-dimensional
37:43 - Elan space when it comes to the nend
37:46 - dimensional or higher dimensional spaces
37:49 - it's much harder to visualize therefore
37:53 - usually when it comes to
37:55 - visualizations we do usually we us
37:57 - usually only visualize the
37:59 - onedimensional two-dimensional and
38:01 - threedimensional spaces above then it
38:03 - just no longer does make sense to
38:05 - visualize it but we definitely deal with
38:08 - them and they are part of Applied linear
38:13 - algebra so understanding the spaces is
38:16 - very important for analyzing vectors for
38:19 - their interactions and this holds not
38:22 - just for this two-dimensional and
38:23 - three-dimensional but really for
38:26 - multi-dimensional space
38:31 - let's Now quickly Define this idea of
38:33 - Norm so the norm of a vector denoted by
38:37 - this uh uh V which you can see kind of
38:40 - like similar to the absolute value from
38:43 - prealgebra you can see here that we have
38:46 - this double straight lines like from
38:49 - absolute value then we have the name of
38:52 - the vector or the variable name that we
38:54 - are assigning to our vector and then you
38:56 - might notice is here on the top of this
38:59 - this Arrow this basically says that we
39:03 - are dealing not with just a variable but
39:05 - really we are dealing with a vector this
39:08 - is really important because you can see
39:10 - that there makes a huge difference if we
39:13 - have for instance just V or V1 I have to
39:16 - say or just V those are really important
39:19 - and things that you need to keep in mind
39:21 - when it comes to linear algebra and
39:24 - trying to differentiate vectors from a
39:26 - point
39:27 - you will notice that when it comes to
39:29 - Norm we can uh represented it either by
39:33 - this notation or this usually it's a
39:36 - common um notation uh in machine
39:39 - learning or in data science um with this
39:42 - uh two bars and um when we do this we
39:46 - automatically also know L2 norm and this
39:50 - is something very common and uh usually
39:53 - used as part of U
39:55 - retrogression which is an application of
39:59 - um linear algebra uh and it's used in uh
40:02 - regularization so we are regularizing
40:04 - our machine learning algorithms so when
40:07 - you get into machine learning you will
40:08 - see time and time again this um notation
40:11 - so uh next time when you see this then
40:13 - you know automatically that you are
40:15 - dealing with L2 norm and L2 Norm which
40:18 - is also used a lot in machine learning
40:21 - it is referring to the usage of L2 Norm
40:24 - to uh in the uh read regression and
40:27 - regression or L2 regularization is a
40:31 - very popular regularization techniques
40:33 - as part of machine learning so right now
40:36 - even you can see this uh intersection of
40:39 - linear algebra or um this uh idea of
40:42 - norms in machine learning the norm of
40:45 - this vector v is equal to square roof
40:47 - and then V1 squ plus v2^ S Plus and all
40:51 - this in between numbers plus VN s so
40:55 - here basically it means take square root
40:58 - of V1 2 then V2 S Plus V3 squ blah blah
41:04 - blah plus VN squ so basically take all
41:09 - the units that form this vector and then
41:12 - so are on this vector and use them
41:16 - Square them and then add them and then
41:18 - take the square root of that that's the
41:21 - distance or I have to say the norm of
41:24 - this Vector we saw already the norm here
41:27 - is just a another example what Norm is
41:30 - and um on a specific two dimensional
41:33 - Vector when we have for instance that a
41:35 - vector is equal to three and four which
41:37 - means for the First Dimension let's say
41:39 - on x-axis we have three and then on Y
41:41 - axis is equal to four then the norm so
41:44 - this is equal to we take the x value so
41:48 - three and then we Square it so V you can
41:51 - see here this is the case when n is
41:53 - equal to 2 this is simply equal to
41:56 - square root for v1^2 + v2^ 2 and as V1
42:01 - is equal to 3 so this is our maybe I can
42:05 - make this just V1 and this is my V2 then
42:09 - the norm or the equ in distance for this
42:12 - Vector so this thing is equal
42:15 - to V1 2 + V2 2 which is equal to 3^ 2 +
42:21 - 4 squ and this value is square root of
42:24 - 25 and it's equal to 5 let's now see the
42:28 - difference between aladine distance and
42:30 - the norm so you could see here the norm
42:33 - here we have just one vector like here
42:38 - and this Norm it has just two
42:41 - corresponding values into two
42:42 - dimensional space you see here we have
42:45 - just three and then four so this is V1
42:47 - and V2 when it comes to the Alan
42:50 - distance this is kind of the
42:51 - generalization of this idea of Norm so
42:54 - the aladine distance between two points
42:57 - A and B in r n so in the N dimensional
43:00 - space is the norm of the vector
43:03 - connecting a to B so we see that the
43:06 - norm and the Alan distance are highly
43:09 - related to each other only we are
43:12 - talking about the norm when it comes to
43:14 - one vector but when we have this Vector
43:18 - a and the vector
43:21 - B this is simply the Alan distance so
43:27 - for the Aline distance we know already
43:30 - this idea of distance how we can measure
43:32 - it and you can see that this comes very
43:35 - similar to what we see here notation and
43:38 - here we are saying well we have this
43:41 - vector and then it has this two
43:43 - coordinates in N is equal to 2 in two
43:45 - dimensional space when it comes to the
43:47 - Aline distance Alan distance helps you
43:50 - understand what is this distance between
43:53 - two points in an N dimensional space so
43:57 - the aladan distance between two points
43:59 - let's say A and B in N dimensional space
44:03 - is the norm of the vector connecting a
44:07 - to B so for instance if we have a point
44:09 - a and we have a point B we are
44:13 - connecting this and this is the vector
44:15 - connecting these two points then the
44:17 - aladan distance is simply the norm of
44:21 - this Vector so this is the aladine
44:24 - distance so we can see that norm and the
44:28 - distance they are highly related to each
44:30 - other in the alian distance we are using
44:32 - this idea of norm and specifically the
44:35 - norm two as I mentioned before so here
44:39 - you can see that the definition of Aline
44:41 - distance so the distance between A and B
44:44 - the two point is equal to square root of
44:47 - A1 - B1 2 + a and then here we have
44:50 - basically A2 - b 2^ 2 and then plus A3 -
44:57 - B3 squ those are things that we cover as
44:59 - part of this dot dot dot and then plus
45:01 - after the last point when we have a n
45:03 - minus BN 2 so here what we mean
45:06 - basically is that if we
45:10 - have two points here is a and here's B
45:13 - and this our vector and we know all
45:16 - these different points so A1 B1 A2 B2 A3
45:22 - B3 blah blah blah and then here a n BN
45:26 - we know all this points ly in here in
45:28 - this distance then we are taking them
45:31 - and using them to calculate the L in
45:33 - distance so here for instance if we have
45:37 - um point A and B so in this example
45:42 - let's do a quick one specific example
45:45 - when we have a point a which has
45:47 - coordinates one and two so this is
45:48 - basically A1 A2 and then point B with uh
45:53 - points in it like B1 B2 you you can
45:57 - notice that the da AB so the distance or
45:59 - the Eid distance of these two points
46:02 - which is equal to the norm of this um
46:09 - Vector but here this is a and this is B
46:12 - and this is this Vector this is equal to
46:15 - square root of 4 min-1 so it takes the
46:17 - B1 so this is B1 and this is
46:22 - A1 takes the square and then says Plus
46:27 - B2 - A 2^ 2 takes the square root of
46:31 - that and says this equal to 5 now you
46:34 - might be wondering but hey why do we do
46:37 - then instead of 1 - B1 2 we do B1 - A1 2
46:41 - and the answer to this question lies in
46:43 - the um uh properties that we learn as
46:46 - part of prealgebra because it doesn't
46:49 - matter when we take uh A1 - B1 squ or B1
46:52 - - A1 squ because this squared ensures
46:55 - that it doesn't matter which one we take
46:57 - first and subtract the other now the
46:59 - proof of that is outside of the scope of
47:02 - this um course is this is part of
47:04 - prealgebra but I just wanted to put this
47:06 - out there to ensure that uh you are uh
47:09 - seeing what we are seeing here because
47:12 - here it says A1 minus B1 but in this
47:14 - example we are taking instead depth uh
47:16 - B1 and we are subtracting A1 this is a
47:19 - common thing that we do in um
47:22 - pre-algebra and just in general uh in
47:24 - different um Alin dist or distance
47:27 - related cases so I just wanted to put
47:29 - this here to ensure that uh later on
47:32 - this is something that can be clear um
47:34 - from the first view why this is
47:36 - important this idea of norms and Equity
47:38 - IND distance beside of being used in
47:40 - machine learning and why is it used so
47:43 - Norms they provide a way to measure the
47:45 - size or the length of a vector in Vector
47:47 - spaces which means that when we want to
47:50 - measure a distance a similarity a
47:54 - relationship between for instance
47:56 - vectors then it becomes much easier to
47:58 - use this idea and El IND distance is not
48:02 - only used in regularization techniques
48:05 - like L2 regularization or retrogression
48:08 - but it's also used in other machine
48:11 - learning or deep learning algorithms as
48:13 - a way to measure the distance or the
48:15 - relationship or the similarity between
48:18 - two different entities those can be
48:20 - variables those can be two people that
48:22 - we want to compare in our algorithm or
48:25 - two entities uh um for instance the um
48:29 - Norms or the distance they are also used
48:32 - as part of K me algorithm something that
48:34 - you might have heard and if you follow
48:36 - later on the machine learning and the
48:38 - clustering section of machine learning
48:40 - you will see that aladan distance is
48:42 - used as part of K's algorithm that aims
48:44 - to Cluster observations into different
48:47 - groups so this also yet another highly
48:50 - applicable uh topic that you must know
48:52 - in order to understand different linear
48:54 - ALB top topics but also machine learning
48:59 - topics why this prerequisites matter and
49:02 - why I mentioned those understanding this
49:04 - concept is very crucial they underpin
49:07 - this geometric interpretation linear
49:09 - algebra they will help you to better
49:11 - understand these Concepts and not just
49:12 - to memorize them but really understand
49:15 - and later on when you go into your
49:17 - machine learning and AI journey and your
49:20 - data science Journey seeing these
49:22 - Concepts will help you to better
49:25 - understand those different all
49:26 - algorithms these optimization techniques
49:29 - what we mean when we say we want our
49:31 - optimization algorithm to move towards
49:33 - local minimum Global minimum this idea
49:36 - of movement this idea of vectors later
49:39 - on WE you will also understand this
49:41 - different concepts in deep learning how
49:43 - these models work how the neural
49:45 - networks work and those are essential
49:48 - Concepts that you need for solving
49:51 - different systems of linear equation a
49:53 - core part of this course they also help
49:56 - you in visualizing vectors spaces which
49:59 - are critical to understand this concept
50:02 - of linear algebra the applications of
50:04 - linear algebra when it comes to the real
50:07 - world applications so those are things
50:10 - that you can definitely Master by
50:12 - following some of our other courses but
50:15 - for this course I assume that you are
50:17 - already familiar with these Concepts
50:19 - right so now we are ready to actually
50:22 - begin and with this prerequisites in
50:24 - mind you are prepared to part your
50:27 - linear algebra Journey we are going to
50:29 - learn everything in the most efficient
50:32 - way in such a way that you will learn
50:33 - the theory you are going to see many
50:36 - examples we are going to learn
50:37 - everything in detail but at the same
50:39 - time you're going to learn the must know
50:41 - Concepts and I'm not going to overwhelm
50:43 - you with this most difficult concept
50:46 - that you will not be seeing in your
50:48 - career I'm going to give you this bare
50:50 - minimum when it comes to really knowing
50:53 - and the must know for linear algebra
50:56 - such that that you will be ready to
50:58 - apply linear algebra in your
51:00 - professional Journey whether you want to
51:02 - get into machine learning deep learning
51:04 - artificial intelligence data science
51:07 - knowing these different concepts in
51:08 - linear algebra you will be a pro in your
51:12 - field going to give you everything that
51:14 - you need the theory examples
51:16 - implementations everything in detail but
51:19 - at the same time you will be doing that
51:22 - in the most efficient and time-saving
51:24 - way so without further ado
51:27 - let's get started hi there so let's get
51:29 - started with our first module which is
51:31 - foundations of vectors in this module we
51:33 - are going to talk about fundamentals of
51:35 - linear algebra vectors we are going to
51:38 - make a differentiation with between
51:39 - scalars and vectors we are going to
51:41 - Define them so first we will learn the
51:44 - theory then we will Implement them into
51:46 - practice by plotting them by looking
51:47 - into different examples then we will
51:50 - look into this representation of vectors
51:53 - by looking into the magnitude and the
51:55 - direction of it and the representation
51:57 - of them just in general we are going to
52:00 - plot them in our coordinate system then
52:03 - we are going to see the common notation
52:04 - of vectors and indexing of them vectors
52:08 - are super important when it comes to
52:10 - linear algebra and application of it and
52:13 - uh they matter not only in mathematics
52:16 - but beyond so uh vectors help us in many
52:20 - ways from figuring out how objects move
52:22 - to solving math problems in science and
52:25 - just in general in technology including
52:27 - in data science machine learning
52:29 - artificial intelligence Etc they are
52:32 - super useful tool so uh let's start our
52:37 - journey with looking into scalers so
52:39 - scalers they are displ numbers and by
52:42 - definition a scaler is a single numeric
52:45 - volume often representing magnitude or
52:49 - quantity for example uh scalers can be
52:53 - describing um the temperature out side
52:57 - for instance the temperature of um a
53:01 - 22° uh can be represented by a scaler or
53:04 - a height of a person can be represented
53:07 - it's a scaler so let's assume we have a
53:10 - scaler that we will Define by a letter s
53:13 - it's just a variable this scaler is then
53:16 - equal to 22 for instance and we are
53:18 - measuring it in degrees so it means that
53:22 - uh if this s measures a room temperature
53:26 - then and the scaler s which is equal to
53:29 - 22° it represents the room temperature
53:32 - it can be for instance 18° or 9° if it's
53:36 - very cold uh it just measures a single
53:39 - volume it represents just a single
53:42 - number or it can be for instance 17 100
53:47 - 2.22 so all this they are just scalers
53:51 - they represent a single numeric volume
53:54 - they often represent the magnet itude or
53:57 - a quantity very soon we will see that
53:59 - scalers they are a value that represent
54:02 - the magnitude of a
54:04 - vector so uh now when we are clear on
54:07 - this very basic concept of scalers let's
54:10 - actually move to this idea of vectors so
54:14 - by definition a vector is an ordered
54:16 - array of numbers which can represent
54:18 - both magnitude and direction in space so
54:23 - uh vectors they are bit more they
54:25 - represent bit more than tailers there
54:26 - are numbers that also show direction
54:30 - like a car spitting down the highway or
54:33 - a bow uh being
54:34 - TR for instance uh when it comes to our
54:37 - previous example where we were using
54:39 - this uh uh room temperature as a way to
54:44 - uh think about the scaler a scaler for
54:47 - instance scaler that we just saw was
54:50 - this room temperature room temperature
54:54 - which was 22°
54:56 - when it comes to the vector vector is
54:59 - different for Vector for instance we can
55:02 - have an example when a bird for instance
55:06 - bird it
55:08 - flies
55:10 - flies at 10
55:15 - kilomet per
55:17 - hour and I also add here another
55:20 - information which will make this as a
55:22 - vector which is that it flies s
55:26 - sou so here as you can see what I'm
55:29 - doing is that I'm not just oh let me
55:34 - actually remove this part to make it
55:35 - easier to
55:38 - understand okay so uh in this example
55:42 - let me write it down that the
55:45 - example
55:47 - bird
55:50 - flies
55:53 - South at 10
55:56 - kilometer per hour so you can see that
56:00 - I'm not just adding the scaler which is
56:03 - in this case the
56:06 - magnitude we will see very soon the
56:08 - formal definition of it so I'm writing
56:11 - down the speed I'm defining the speed
56:13 - but also the direction so I'm
56:16 - saying I know that the bird is flying
56:19 - south that's the direction and I know
56:22 - also the speed of it which is the
56:24 - magnitude so thank you
56:26 - hour so here in the vector I have much
56:29 - more information than in the scaler
56:32 - because in the scaler I just got
56:33 - temperature room temperature single
56:35 - value but in case of a vector I not only
56:39 - have a magnitude or speed like 10 kilm
56:42 - per hour but I have extra information
56:45 - which is the direction of it for
56:47 - instance flying to the South so let's
56:50 - now look into some real examples and
56:52 - plotting them to make more sense out of
56:55 - this Ideo vector and what is this
56:57 - magnitude what is the
56:59 - direction so let's assume we have a 2d
57:03 - plane so we have xaxis we have y AIS
57:07 - here like usual we have our z0 Center
57:10 - and we want to plot a simple Vector so
57:15 - uh usually the way we represent Vector
57:18 - in tutorials or just writing down is by
57:20 - writing the name of the vector this can
57:23 - be just a a random name let's assume
57:26 - that it's a v letter V and then on the
57:29 - top we are always adding this Arrow so
57:33 - this Arrow it says and it tells the
57:36 - person who is reading that we are
57:37 - dealing with the vector arrow on the top
57:40 - is that reference so let's assume this
57:44 - uh vector v it starts from the center of
57:47 - our coordinate system and it goes to
57:51 - this point so let's say in here this is
57:54 - our vector
57:57 - V so let's assume that this point in
58:01 - here is equal to 4 which means that the
58:05 - x coordinate is four and the y
58:07 - coordinate is zero as the um uh Arrow it
58:11 - just as the point in here it has a a y
58:14 - value of zero so you can see that it
58:16 - goes straight from zero to this one to
58:20 - this
58:20 - point okay so what tells this Vector uh
58:24 - to us is that we have a value that
58:28 - describes the length of the vector so it
58:30 - goes from 0 to four which means that the
58:34 - length is equal to unit 4 so it's equal
58:38 - to
58:39 - four um
58:41 - and we have just learned and we were
58:44 - just talking about that the magnitude is
58:47 - the length in this case so the length
58:49 - describes the magnitude in this
58:54 - case so this means that the magnitude of
58:58 - this Vector is equal to four and then um
59:02 - what else we can see here we can see the
59:05 - direction of the vector which means that
59:07 - the direction is also something that we
59:10 - can see here this is the direction of
59:12 - the vector so this going straight from
59:16 - this point to this point in a horizontal
59:19 - way
59:21 - so independent whether I plot this
59:25 - Vector from 0 to for in here or in here
59:28 - or in here or in here or in here in all
59:33 - cases as long as the length is this I'm
59:35 - dealing with the same Vector because I
59:38 - am basically in this entire
59:42 - R2 space I have exactly the same Vector
59:46 - all I care is about the magnitude and
59:49 - the Direction Where will this Vector
59:52 - start and where will it
59:54 - end I am not interested I'm interested
59:57 - that the that the magnitude in this case
59:59 - the length is equal to the direction of
60:02 - the vector so let's now look into
60:04 - another example where we go a bit more
60:07 - difficult on our coordinates and on our
60:10 - Vector we already saw that we had this
60:12 - Vector where we went let me change the
60:15 - color so this was our vector
60:18 - v and it went from 0er till 4 so this
60:22 - point to be more specific is so this
60:25 - Vector it goes
60:26 - the vector B it goes from 0 0
60:31 - to 40 so the coordinate X was four and
60:36 - the Y was Zero now let's plot another
60:40 - one um where the direction is no longer
60:43 - horizontal for this Vector let's call it
60:45 - Vector
60:46 - W and for this Vector w we will again
60:49 - start with z0 so we will start again in
60:52 - here but this time we will go like this
60:57 - so let's say we go all the way to this
61:00 - point so this
61:04 - point has a value for an X axis of three
61:10 - and for y axis it has a value of four
61:13 - which means it goes from this point to
61:15 - this point and this is the direction of
61:19 - our vector v so it goes 2 3 4 because
61:24 - this point is
61:26 - 3
61:27 - 0 and this point is 04 so xaxis is 0 x
61:33 - coordinate and y coordinate is four so
61:36 - now you can see that the direction of
61:40 - this Vector is like
61:43 - this while the direction of the vector B
61:46 - was like
61:47 - this and like in case of vector v i
61:52 - again no longer care about where exactly
61:55 - my vector dou use TS and ends but all I
61:58 - care is about its magnitude so the
62:00 - length and the direction so for
62:03 - instance I can have the same Vector in
62:06 - here the same Vector in
62:10 - here as long as the length the magnitude
62:13 - is the same and the direction I am
62:16 - dealing with the same Vector that's all
62:18 - I care so the magnitude and the
62:20 - direction is all that you care about all
62:24 - right so now about the length um that's
62:27 - uh something that you can see very
62:29 - easily from this specific example
62:31 - because by using the Pythagoras Theorem
62:33 - or P Pythagorean theorem we can see very
62:37 - quickly that as the length of this side
62:40 - of our right angle
62:43 - 30° so right triangle we can see that
62:46 - this side is three this side is
62:49 - four which means that this side is five
62:52 - because 4^ 2 + 3^ 2 then we take the
62:56 - square root of that a square root of 25
62:59 - and it's equal to 5 so the length or the
63:02 - magnitude of this vector v is simply
63:04 - equal to
63:06 - five all right this was about this uh
63:09 - specific vectors let's now look into the
63:12 - uh common representation of the vectors
63:15 - so we always use the magnitude as well
63:17 - as the direction you know to represent
63:19 - the vectors and they commonly are
63:21 - represented by two different uh ways
63:24 - let's now look into the first way that
63:25 - the vectors can be represented and then
63:27 - we will move on to the next one so when
63:30 - it comes to the vector v so we saw that
63:33 - vector v was moving from 0 0 till uh to
63:37 - the point of 40 so we can represent the
63:41 - vector B by 4 and
63:45 - Zer when it comes to the vector w we can
63:49 - represent that uh Vector so Vector w we
63:52 - can again do the parenthesis and we can
63:55 - say that is equal to 34 so by using the
63:59 - coordinates from the coordinate system
64:01 - we can then represent our uh vectors so
64:06 - this is just one way of representing a
64:08 - vector another way of representing these
64:10 - vectors is by using this Square braces
64:13 - given that we are in a two dimensional
64:15 - space first we will mention here the
64:17 - four then we will mention the zero in
64:21 - here two so we can say three and four
64:25 - this is yet another way of representing
64:28 - the vectors in a two dimensional
64:31 - space so if we were to have a
64:34 - threedimensional space so let me
64:36 - actually show it on a new page so if we
64:38 - were to um if we were um to have vectors
64:43 - in three dimensional space so we are
64:45 - dealing with
64:46 - R3 so we have points that can be
64:50 - described by X Y and Z
64:54 - so coordinates space like this so X and
64:57 - then Y and then Z then every point so
65:01 - let's say we have this Vector then we
65:04 - had to represent it by a value let's say
65:07 - x uh X1 y1 and Z1 or um better let me
65:13 - actually use a different
65:15 - letters a b and c and this would be my
65:20 - vector v and I could also represent this
65:24 - vector v
65:26 - is the same so vector v can be
65:29 - represented as a b and c so what thing
65:33 - that you can notice is that unlike the
65:35 - R2 now I have three different entries
65:40 - what we are also referring as rows and
65:43 - we just got one column so um we can uh
65:47 - often represent and usually that's a
65:49 - common way of representing vectors by
65:51 - using this um columns so columns help us
65:55 - to to represent our vectors and you can
65:59 - see very clearly then when it comes to
66:01 - the two-dimensional space so when we
66:04 - have
66:06 - R2 so then our vectors have just two
66:10 - rows so three and four four Z like in
66:13 - here when it comes to a three
66:15 - dimensional space we have three entries
66:18 - and so on so the same holds of course
66:19 - also for for instance R5 then for R5 U
66:23 - our vectors so coordinate space can be
66:25 - for instance X Y Zed and then gamma and
66:29 - then let's say
66:31 - Delta and then the coordinates uh of a
66:34 - vector in that space can be V and then
66:37 - arrow is equals sh and then we would
66:39 - have uh let's say A B C D E you get the
66:45 - idea so depending on the space the
66:47 - coordinate space and the dimension of
66:49 - that space then the corresponding
66:51 - vectors can be represented accordingly
66:58 - so the vectors are quantities that have
67:00 - both magnitude and direction as we just
67:02 - so distinguishing them from scalers
67:05 - which only have magnitude so we saw that
67:08 - the scalers got only magnitude while in
67:10 - case of vectors we saw both for the
67:12 - vector v and for the vector w we didn't
67:15 - we didn't only have the magnitude so the
67:17 - length of the vector but also the
67:20 - corresponding
67:22 - Direction so uh when it comes to the um
67:26 - vectors so this is exactly what we just
67:28 - saw in our example a vector in a
67:31 - two-dimensional space so in r
67:34 - two uh can be represented by using this
67:38 - Square braces and the corresponding
67:40 - entries for exam Y where X is basically
67:43 - the x coordinate in our coordinate
67:45 - system so in our X and Y system whenever
67:50 - you have this x and y
67:52 - coordinate then uh this X coordinate
67:56 - Will then describe your magnitude and
67:59 - the y coordinate Will then describe your
68:01 - second entry that you need to put when
68:03 - representing your
68:06 - vectors so here the X and Y indicate the
68:09 - movement in the horizontal and in the
68:11 - vertical Dimensions respectively so for
68:14 - X's it's always the x coordinate so how
68:17 - far you move towards the horizontal
68:20 - Direction in here in here or independent
68:22 - in here so always take the x coordinate
68:25 - that is the value that you need to put
68:27 - first and then the Y need to be put it
68:30 - in here so indexing in vectors when it
68:33 - comes to the um indexing the standard
68:36 - mathematical notation uh indices in the
68:38 - N vectors goes from I is equal to 1 2 I
68:42 - is equal to n so the um notation here
68:46 - can be a bit ambiguous so AI uh could
68:49 - mean the E element of AI uh the a vector
68:53 - or the E Vector in a collection so let's
68:55 - start with a simple one and then move on
68:57 - to this next part so what this means and
69:01 - what this means we will look into now so
69:05 - um usually uh when we have a um n
69:09 - dimensional space we are having hard
69:12 - time visualizing it therefore we use
69:14 - this two dimensional space or maximum
69:16 - threedimensional space in order to get
69:18 - an understanding of what these vectors
69:20 - are so we just saw examples of them uh
69:23 - when uh creating our vectors in um V and
69:27 - V uh and W in uh R2 and also in
69:32 - R3 but we can have similar vectors also
69:35 - in R4 in
69:38 - R5 or all the way down to RN where n can
69:42 - be 100 200 500 any number as large as
69:46 - you want the thing is is that
69:50 - visualizing R4 R5 RN is very hard but we
69:54 - can still benefit from great properties
69:56 - of the vectors matrices and in general
69:59 - linear algebra in order to describe
70:01 - different things that have more than
70:03 - three dimensions therefore we have this
70:06 - a bit more ambiguous notation where we
70:09 - use r n and this n can be any real
70:13 - number and it can be all the way to
70:16 - Infinity so very large
70:18 - number and uh let's say we have a vector
70:21 - in this RN then this Vector is usually
70:24 - described Red by using similar Square uh
70:28 - brackets like before only with uh more
70:32 - entries so like before we got just one
70:35 - column so that's something that we
70:37 - didn't uh change but here we have
70:39 - instead of just two entries or three
70:41 - entries like in the two dimensional or
70:43 - threedimensional spaces now we have A1
70:46 - A2 A3 all the way down 2 a n minus one
70:51 - and a n so we got in total n elements in
70:56 - our column and this describes our single
71:00 - Vector so this Vector in an N
71:03 - dimensional space this we can call also
71:06 - a so one thing that we just so is that
71:09 - it was saying in our definition and
71:12 - notation that uh we might also be
71:14 - dealing with the E Vector in a
71:16 - collection which means that sometimes
71:19 - you will
71:20 - see this while here the A1 A2 they are
71:24 - vector
71:25 - themselves so here we saw that these are
71:28 - just entries so A1 is a number A2 is a
71:32 - number A3 is a number a n is just a
71:34 - number but it's also possible uh when
71:38 - you have a much more difficult and
71:39 - complicated case that you got an
71:43 - A let's write it down with a capital
71:46 - letter A which is equal to
71:51 - A1 or let's actually remove this
71:55 - so we
71:57 - got let's say
71:59 - A1
72:01 - A2 A3 all the way down to a n minus one
72:07 - and a
72:10 - n where you can already see what is
72:12 - going on so instead of having just a
72:14 - number as an entries instead we have
72:17 - vectors in here so our first element is
72:21 - actually Vector our second element is
72:23 - actually Vector so A2 Arrow A3 Arrow all
72:26 - the way down to a n arrow so while here
72:30 - this can be for instance some numbers
72:32 - let's say one one one all the way down
72:34 - to one
72:36 - one here we have a vector vector another
72:41 - vector and all the way down here yet
72:43 - another Vector where for instance let me
72:46 - remove this
72:49 - part where for
72:52 - instance A1 error is actually equal
72:58 - to A1 1 A1 2 a13 all the way down to A1
73:06 - n one thing that you will notice here is
73:10 - that unlike in
73:12 - here here I got double indices so I got
73:18 - here a one1 and then A1 2 and then a13
73:22 - all the way to a 1 n so the first index
73:26 - it doesn't change as I have here a one
73:30 - so I'm writing down the index
73:32 - corresponding to this Vector but the
73:36 - second index it changes per entry
73:39 - indicating which element specifically in
73:42 - the vector I'm talking about so from the
73:44 - first index you can identify the vector
73:48 - that I'm referring to which is A1 and
73:50 - from the second index you can see the
73:53 - corresponding
73:55 - um entry or the value that that um
73:59 - element is positioned in this Vector so
74:02 - you can see that this Valu is for
74:04 - instance in the um Vector one so A1 to
74:08 - be more specific but then it is in the
74:11 - first position this is in the second
74:13 - position in the third position all the
74:15 - way down to the end position so this is
74:19 - something that is really important to
74:21 - understand well because this notation
74:24 - it's going to appear time and time again
74:27 - across various applications of matrices
74:30 - and vectors so it is really important to
74:32 - understand well therefore I want to go
74:34 - one more time through this to make sure
74:37 - that we are clear on what this indices
74:39 - represent so whenever we have an index
74:42 - uh an a vector that we want to uh
74:46 - represent and it's um it has just um it
74:49 - is just a vector which means that it's
74:52 - not a nested vector vector in vector
74:55 - then um we can Define it by let's say a
74:58 - and then on the top an array and it's
75:00 - equal to and here we can have A1 A2 all
75:05 - the way down to a n so you can see what
75:10 - we are also referring as dimension of
75:12 - this Vector is equal to n by 1 so I got
75:17 - n entries and just one column so n by
75:22 - one which means that this already gives
75:25 - me indication that most likely this A1
75:28 - is a number this A2 is a number this a n
75:30 - is a number so let's say this equal to
75:34 - one two uh three blah blah blah and then
75:37 - here I have let's say
75:41 - 100 but if I'm dealing with a nested
75:45 - Vector later we will see that this can
75:47 - be represented by a matrix then um I can
75:51 - also Define this
75:55 - by capital letter A which is a common
75:58 - way to refer to either matrices or
76:01 - nested vectors and then this is equal to
76:05 - A1 Arrow A2 Arrow A3 Arrow this already
76:11 - sends a message to the reader that we
76:13 - are dealing with no longer um constants
76:17 - within uh Vector but rather vectors in a
76:21 - vector and uh what can we see here is
76:24 - that the dimension of this nested vector
76:28 - or which we can also refer to as a
76:31 - matrix here the number of rows so the
76:34 - number of entries this elements we can
76:36 - see it's equal to n but then this time
76:40 - the number of values that form these
76:43 - vectors is no longer one because we're
76:45 - are not dealing with just a constant
76:47 - this is not some constant but rather
76:50 - this is yet not Vector so let's assume
76:53 - this Vector has a length or M so let's
76:56 - say this has a length of M then the
77:00 - dimension of this Matrix a is equal to M
77:04 - so something that we will see also when
77:06 - talking about
77:07 - matrices so let me actually clarify this
77:11 - bit more for better understanding let's
77:13 - say we look into one of those um one uh
77:17 - one other example of an entry so let's
77:20 - say we look into this specific Vector
77:23 - which is in the U the Third
77:26 - uh Vector within this Vector capital A
77:29 - so this
77:31 - A3
77:35 - Vector so one thing to see here already
77:39 - is that I assumed that these vectors
77:43 - they got M elements and keep in mind
77:45 - that all these vectors they should be of
77:47 - the same size so it means that I already
77:49 - know that this specific Vector A3 has M
77:54 - elements elements so M elements so I'm
77:58 - representing this uh H Vector from here
78:03 - I'm taking this out from this entire uh
78:05 - nested a vector and I just want to
78:08 - represent this and now unlike this
78:12 - elements that got an arrow on the top
78:14 - this time I will have uh constants
78:18 - forming the A3 Vector so I no longer
78:21 - have vectors but I have elements in it
78:24 - so
78:25 - in here I will have a a let me actually
78:29 - write down all the A's but to refer and
78:33 - to make sure that I recognize that I'm
78:35 - dealing with the thir a vector so A3
78:38 - Arrow here I will put three Tre all the
78:41 - way here three so they all come from the
78:43 - same third a Tre Vector but then their
78:47 - positions is different because this is
78:49 - let's say uh one two and then all the
78:53 - way down to
78:55 - Ed
78:57 - position so this indices help us to keep
79:00 - track what are the um position that this
79:06 - values are taking part in the vector A3
79:10 - Arrow this might seem bit complicated at
79:13 - the moment but once we move on onto bit
79:15 - more complex material like uh matrices
79:18 - it will make much more sense this is bit
79:21 - of an extra I just wanted to Showcase
79:23 - this but this is what uh is at its core
79:26 - and what you need to uh understand at
79:29 - the moment to understand this concept of
79:31 - vectors so you need to know that vectors
79:34 - can be represented by this arrow on the
79:36 - top so let's say Vector a and it has
79:38 - let's say n elements then you can write
79:40 - the square brackets and then you will
79:43 - need to mention A1 A2 all the way to a n
79:46 - which means that you have n different
79:48 - entries describing your vector so you
79:50 - have A1 which is the first element in
79:52 - your vector A2 the second element all
79:54 - the way to a n which is the end element
79:57 - but here you can see for instance so if
80:00 - I had here a Tre that uh A1 is simply
80:03 - equal to 1 A2 is equal to 2 A3 is equal
80:07 - to 3 all the way to a n is equal to 100
80:11 - so these numbers I'm basically taking
80:14 - and I'm representing them I'm putting
80:17 - them in here within Square braces in
80:19 - order to get a representation of my
80:21 - Vector so my Vector a has all these
80:24 - different entries and different entries
80:26 - and it starts with one and it ends with
80:28 - 100 this is a vector and then when it
80:31 - comes to the vectors within vectors here
80:33 - we need to be bit more
80:35 - careful cuz here we not just have uh
80:39 - constant values forming a vector but we
80:41 - have vectors that form yet not vectors
80:44 - so our Vector a our nested Vector a
80:48 - which we uh later will refer as Matrix a
80:53 - has actually enter that also are vectors
80:56 - so we have A1 Vector A2 Vector A3 Vector
80:59 - they are not just constants but on their
81:01 - own they are vectors so here for
81:04 - instance we have defined also an example
81:06 - of it we have said let's look into this
81:08 - third specific Vector that is part of a
81:11 - which is a Tre uh vector and uh that one
81:15 - has M different
81:18 - elements here we have then the index
81:21 - referring to the which Vector from the
81:24 - ne Vector a it is which is the third one
81:27 - because we have taken it from here but
81:29 - then on its own this Vector has
81:31 - different members and different members
81:33 - to be more specific therefore we have
81:35 - also an index to keep track of the
81:37 - position of this value one to up to M
81:40 - and this can be yet another uh this time
81:44 - it can contain some elements an example
81:46 - of which is for instance zero one 2 all
81:50 - the way to let's say 500 and this can be
81:53 - different numbers it doesn't need to be
81:55 - ordered it doesn't need to have a
81:57 - specific pattern they can be just random
82:00 - numbers describing this A3 Vector so
82:04 - hopefully this makes sense if it doesn't
82:06 - don't worry because we are going to see
82:08 - this time and time again I just wanted
82:10 - to give you brief of an intro such that
82:13 - you can uh remember this when we come uh
82:17 - back to bit more uh complex topics like
82:20 - uh indexing in matrices so now let's
82:24 - talk about about special vectors and
82:26 - operations here we are going to talk
82:28 - about zero vectors unit vectors the
82:30 - concept of sparcity in vectors as well
82:33 - as vectors in higher Dimensions like we
82:36 - just saw about this n dimensional space
82:38 - we will also talk about different
82:39 - operations we can apply when it comes to
82:42 - vectors like uh addition subtraction and
82:45 - then later on in the next module we will
82:48 - also talk about multiplication we will
82:50 - also be looking into the properties of
82:52 - vector addition after we looked into
82:55 - some detailed examples when it comes to
82:57 - operations on
82:59 - vectors all right so let's start with
83:01 - the zero vectors and unit vectors when
83:04 - it comes to the zero vectors you can see
83:06 - here already that um the zero and arrow
83:10 - on the top it basically refers to the
83:12 - vector like we saw before only with the
83:15 - difference that all its members are zero
83:19 - so you can see here that we have zero
83:22 - and then an arrow and then underneath
83:24 - here we have some number tree and then
83:27 - this is described by this common
83:29 - representation with the square braces
83:31 - and then three different members Z 0 0
83:35 - so all zero and then it says in R
83:39 - Tre okay so why are we doing this well
83:44 - uh when it comes to uh different linear
83:46 - Lal operation sometimes we just need to
83:49 - add zero uh vectors or we just want to
83:52 - create zero vectors it's just easier
83:54 - here to work with we want to uh just
83:57 - create an empty uh Vector we want we
84:00 - know the length but we want to keep it
84:03 - empty such later on we can add something
84:05 - on the top or knowing that when we add a
84:08 - zero on a number the number stays the
84:11 - same we can make use of this property to
84:14 - uh do different um uh tricks when it
84:17 - comes to programming in Python in color
84:20 - or in C++ Etc so therefore this idea of
84:23 - zero vector can become very handy now
84:26 - one thing that you need to notice here
84:28 - is that we are not just writing down
84:30 - this zero to emphasize we are dealing
84:33 - with a vector but like uh before we have
84:36 - this eror on the top emphasizing that we
84:38 - are dealing with a vector then what we
84:42 - are doing is that we are also adding the
84:45 - dimension of this Vector so in what
84:48 - dimension in what space are we um uh
84:51 - creating this zero Vector that this
84:53 - vector is located is it in R2 in RN in
84:58 - R3 in this specific case you can see
85:00 - that in this example the uh index that
85:03 - we got here is three which basically
85:05 - indicates we are dealing with a zero
85:07 - Vector in threedimensional
85:09 - space so in the
85:12 - R3 uh in general we would just note this
85:15 - by n keeping the uh notation general
85:19 - which means that we are dealing with 0 0
85:23 - all the way down to 0 so it has n by one
85:27 - dimension in r
85:31 - n all right so this is about zero
85:34 - vectors it is just a way to uh make our
85:37 - programming life easier also to use it
85:39 - in different uh algorithms when it comes
85:41 - to bit more advanced
85:43 - algebra uh the next type of special
85:46 - vectors that we will look into is this
85:48 - unit vectors so vectors with a single
85:51 - element equal to one and all the other
85:54 - zero denoted as EI for the E unit Vector
85:59 - in N dimensions are referred by unit
86:02 - vectors
86:04 - so uh what we mean here when it comes to
86:08 - the unit vectors uh if we have for
86:11 - instance E1 it means that we have a
86:14 - vector where the is in this case the
86:17 - first element is equal to 1 so you can
86:20 - see that E1 is equal to 1 0 0 so in the
86:25 - first element we got one and the
86:27 - remaining is zero and this is really
86:29 - important that we are dealing with
86:31 - vectors that contain only elements of
86:34 - zeros and ones and the only member that
86:38 - is equal to the only element in that
86:40 - Vector that is equal to one is the E
86:42 - element in the entire Vector all the
86:44 - remaining ones are zero and you can see
86:47 - here that the dimension is no longer
86:49 - specified but just the um index of the
86:53 - entry where the
86:54 - um uh the uh one is
86:58 - located so let's look at another example
87:01 - in here for instance when it comes to
87:03 - the um uh unit Vector yet another unit
87:07 - Vector is E2 which basically means that
87:10 - in the second element so in the second
87:13 - place uh the uh Vector contains one and
87:18 - all the other members are zero so here
87:20 - you can see Zero here you can see Zero
87:22 - only in the second element we have one
87:24 - and then in the E3 what we have here is
87:28 - that the third element is one and all
87:30 - the other ones are zero so let's
87:33 - actually look into uh one um bigger
87:37 - Vector uh in higher Dimension to make it
87:40 - even more sense so first I will Define
87:43 - and assume that we are dealing with a
87:45 - vector in RN so in an N dimensional
87:49 - space this gives me an idea that we are
87:50 - dealing with um so we are not dealing
87:53 - with nested Vector we are dealing with a
87:55 - simple n dimensional Vector so it has n
87:58 - rows and one column so using the square
88:01 - braces I'm going to represent my Vector
88:04 - so I have all these different members n
88:07 - members C so e let's say it is
88:13 - E5 so what does this mean it means that
88:16 - I is equal to 5 and this e element so
88:20 - the fifth element is equal to one and
88:22 - all the other entries the elements in
88:24 - this Vector are zeros so let's look into
88:27 - this is 0 0 0 0 I'm approaching the
88:33 - fifth element in my Vector so it's this
88:37 - one this is one and the remaining all
88:41 - zeros so this is a unit Vector in an N
88:45 - dimensional space and I'm defining it by
88:48 - E5 because my fifth element is equal to
88:52 - one now those are very handy when it
88:55 - comes to some other uh techniques in
88:58 - linear algebra and just in general think
89:01 - about techniques like um uh row ulum
89:05 - form solving linear equation something
89:07 - that we will see as part of the next
89:09 - unit so many things um we can do by
89:13 - using unit vectors unit vectors are
89:15 - super important so you need to
89:18 - understand this concept uh very well
89:20 - such that later on you will understand
89:23 - uh more advanced concept CS in linear
89:25 - algebra now look into the topic of
89:28 - sparsity in vectors so by definition
89:31 - sparse Vector is characterized by having
89:34 - many of its entries as zero so its
89:37 - parcity pattern indicates the position
89:39 - of a non zero
89:41 - entries so uh what we are basically
89:44 - saying is that if we are dealing with a
89:46 - vector that contains too many zeros we
89:49 - are dealing with a sparse Vector so uh
89:53 - this sparsity pattern indicates uh also
89:56 - the positions of a nonzero elements so
90:01 - um if we have um unit Vector it means
90:04 - that we are already dealing with a spse
90:08 - uh Vector this is a concept that is
90:11 - super important when it comes to linear
90:12 - algebra but also in general data science
90:15 - machine learning and AI because having
90:18 - the spity your vector it means that you
90:20 - don't have much of an information
90:22 - usually a value is zero it means you
90:24 - don't know much about that specific
90:27 - value and if you got just too many of
90:30 - zeros and too few numbers which do um
90:34 - provide information it means that you
90:36 - are dealing with a vector that doesn't
90:38 - provide you much information and there's
90:40 - always a problem when it comes to data
90:42 - science machine learning and AI so
90:44 - sparcity is something that you need to
90:47 - be aware of you need to know how to
90:49 - recognize it and you also need to know
90:51 - whe there's a problem in your specific
90:53 - case or not so let's look into an
90:56 - example let's say we are dealing with
90:58 - this Vector X that has five different
91:01 - elements so X is a vector coming from um
91:05 - five dimensional space so we have for
91:09 - instance an element of three in the
91:11 - first entry then we have z0 in the
91:13 - second and third uh entries then we have
91:16 - an entry um four which coincidentally
91:19 - also contains value four and then the
91:21 - last element in our five dimensional
91:23 - that Vector X is equal to zero now what
91:27 - do we see here we see that the majority
91:29 - of elements of a vector X is equal to
91:33 - zero because we got in total five
91:35 - elements and then we got three of it
91:39 - actually uh being equal to zero and only
91:42 - two of them containing information like
91:45 - equal to three and four so only two
91:47 - elements that are not zero so nonzero
91:50 - elements it means that three / to 4
91:53 - which is is basically 60% 60% of all the
91:58 - entries in the vector X are equal to
92:01 - zero so the 60% it means that is above
92:06 - half so above 50% 60% of all the
92:10 - information in this Vector um the
92:12 - majority is simply equal to zero this
92:16 - type of vectors we are uh calling sparse
92:18 - vectors and sparsity is really important
92:21 - concept uh that we need to keep in mind
92:24 - later
92:25 - on so uh while we can visualize vectors
92:29 - in two and three dimensions in linear
92:31 - algebra like we just saw in case of this
92:33 - n dimensional vectors
92:36 - visualizing uh the this type of higher
92:38 - dimensional vectors becomes very
92:40 - difficult so uh this mathematical
92:44 - flexibility uh to work with uh this type
92:47 - of uh information so when we can
92:50 - represent uh information many with many
92:53 - entries
92:54 - we can represent it by vectors which we
92:56 - can actually not visualize becomes very
92:59 - handy for complex data structures for
93:02 - different simulations in physics and
93:04 - much more so uh we just saw in couple of
93:07 - examples uh how we can represent vectors
93:10 - in a high dimensional space using this
93:13 - Square braces and this common Vector
93:15 - notation representation we saw that in
93:18 - an N dimensional space we could uh very
93:21 - easily represent this uh very large
93:24 - Matrix or vectors uh by just um using
93:29 - this Vector representation for instance
93:31 - if we got a vector that had n different
93:34 - entries where n is for instance thousand
93:36 - so let's say we have thousand then uh we
93:40 - can represent uh this uh vector or this
93:44 - information by using common Vector
93:46 - notation so A1 A2 all the way to a th000
93:52 - so of course we cannot visual ize this
93:54 - it just doesn't make sense we can
93:56 - visualize two dimensional vectors we can
93:58 - visualize three dimensional vectors but
94:00 - we cannot uh visualize thousand
94:04 - dimensional vectors so Vector that comes
94:06 - from uh
94:07 - r Thousand but what we can do is still
94:12 - make use of this very useful
94:14 - information in order to uh do different
94:18 - operations when and later on we will see
94:21 - that uh this property as specifically
94:24 - this part of linear algebra it helps us
94:26 - to work with vectors in any number of
94:29 - Dimensions whether thousands million
94:31 - billions this mathematical flexibility
94:34 - is super important for more complex data
94:37 - structures uh for metrix multiplications
94:40 - when for instance we are doing different
94:42 - uh algorithms including how we can
94:45 - represent a very large matrices very
94:48 - large feature spaces all this different
94:51 - information we can represent just by
94:54 - making use of
94:56 - vectors coming from this specific uh
94:59 - part of linear
95:02 - algebra let's now finish off this module
95:04 - by looking into some applications of
95:07 - vectors so one common application of
95:09 - making use of vectors is uh when we are
95:13 - performing different operations while
95:15 - having words and we want to count those
95:17 - words so this is a super common
95:19 - application of vectors and we can
95:22 - account this words and can even plot a
95:24 - histogram of how often each of these
95:27 - words appear in a document so a vector
95:30 - of a length n for instance can represent
95:33 - the number of times each of these words
95:36 - in a dictionary of n words appears in a
95:39 - document so uh just for the sake of
95:42 - Simplicity let's assume that we got um
95:44 - dictionary that contains only three
95:47 - words of course in the reality the um
95:51 - dictionary what we also refer often as
95:53 - Cor purpose it contains much many uh
95:55 - much more many words but for the
95:58 - Simplicity we will assume that we just
95:59 - got three different words in our
96:01 - dictionary so that's a total now let's
96:04 - assume that we got a document uh with
96:06 - these different words and we want to
96:08 - count how many times each of those words
96:11 - that we got in dictionary actually
96:13 - appear in our document so um if our
96:17 - document is described by this Vector so
96:20 - it contains an entry of 25 2 and zero it
96:24 - means that in our document we got
96:30 - 25 word one in our from our dictionary
96:34 - so in the position one two * word two
96:39 - and zero times
96:42 - word three so basically we have a
96:46 - predetermined set of words in our
96:49 - dictionary in this case three words word
96:51 - one word two and word three
96:54 - and they have a specific index specific
96:56 - position in our vector and when we are
96:59 - putting these values in here then the
97:02 - machine or the uh computer the program
97:06 - will understand that if we have 25 in
97:09 - the first position then the word one in
97:12 - the dictionary appeared 25 times in our
97:15 - document whereas the second word
97:18 - appeared only two times and the last
97:20 - word word three didn't appear at all so
97:23 - zero times times in the entire
97:25 - document so let's look into a practical
97:28 - example actually to make even more
97:32 - sense so um this is by the way a common
97:36 - practice to count different variations
97:38 - of a word they are common application in
97:41 - engrams large language models
97:43 - Transformers they are just a Cornerstone
97:46 - of many language models when we want to
97:49 - count the words in the document to
97:52 - understand how often the word appears
97:54 - because this gives us a idea what this
97:56 - document is about knowing how many time
97:58 - the same word appears in that uh
98:01 - document it gives us an indication of
98:03 - the topic of uh the do document also we
98:07 - can make use of it to do sentiment now
98:09 - to understand what this document is
98:11 - about not only in terms of the topic but
98:13 - also is it a positive is it a natural or
98:17 - a negative uh document so to say so uh
98:22 - for instance if we got uh the following
98:26 - words uh that correspond to our
98:28 - dictionary and in our dictionary we got
98:30 - just um let's say six different words
98:34 - then what we can do is that we can say 3
98:38 - 2 one let's say 0 4
98:42 - 2 and the corresponding words are
98:46 - word
98:48 - row
98:51 - number horse
98:55 - e and then
98:58 - document what this means is that we have
99:01 - a text what we refer as a document that
99:04 - contains three times the word
99:07 - word that contains two time the word row
99:10 - contains one time the word number zero
99:13 - times the word horse and four times the
99:16 - word eel and two times the word
99:20 - document so uh this is basically a
99:24 - common way representing the uh frequency
99:27 - of the words in the
99:29 - document let me actually give you uh
99:31 - another example
99:35 - and in here I want to emphasize another
99:38 - thing the concept of stop words so uh
99:43 - let's say I make
99:45 - this 10 and then
99:50 - here I say there is a three times the
99:53 - word
99:55 - I two times the
99:58 - word uh
100:02 - reading two times the word
100:06 - library four times the word
100:09 - book zero times the word
100:13 - shower and 10 times the word
100:18 - uh so uh you can see a that in here
100:24 - we are dealing with a document that
100:26 - contains 10 times the word uh which is
100:29 - what something that we refer as a stop
100:31 - word so those are things that actually
100:34 - don't give us too much information about
100:36 - what the document is about because uh
100:38 - it's just used everywhere but it is
100:41 - appearing too often so you can see 10
100:43 - times the most frequently appearing word
100:46 - this is what we refer as a toop word and
100:49 - then another thing that we can observe
100:51 - the second thing we can observe is that
100:54 - we are dealing most likely with a
100:55 - document that describes library reading
100:59 - uh because you see the words like
101:00 - reading you see the word like book
101:02 - library but another word shower that is
101:05 - totally unrelated to reading book or
101:08 - library is appearing zero times so even
101:11 - by looking at discounts we can already
101:13 - get an idea what a topic of this
101:15 - document is about so uh you can see
101:19 - already now from this very basic example
101:21 - where I made too many assumptions
101:23 - regarding how small the the uh
101:25 - dictionary should be uh you can even see
101:28 - now how we can use this counts in our
101:31 - dictionary from our text in order to get
101:34 - idea about the topic of the document or
101:37 - topic of the conversation it can be a
101:39 - topic of the uh tweets if you have a
101:42 - tweet data it can be topic uh regarding
101:45 - book if you have many book um uh book
101:49 - text it can be for instance the topic of
101:53 - of the review if you got uh product
101:55 - reviews from uh Amazon for instance
101:58 - using this count can help you to get a
102:01 - topic regarding topic from that text
102:06 - then you can also use it to remove the
102:08 - stop words because usually the stop
102:10 - words are the most frequently appearing
102:11 - words it can also give you an idea about
102:14 - the sentiment for instance here we are
102:16 - dealing with natural sentiment it's not
102:18 - positive it's not negative it's just
102:20 - reading a book in library that kind of
102:22 - topic so all this can be super helpful
102:25 - when it comes to natural language
102:27 - processing that's a field where this uh
102:29 - text processing text cing and then using
102:32 - that for modeling purposes is what uh
102:35 - what plays a central role it also plays
102:38 - a super important Ro in the large
102:40 - language models in the Transformer
102:41 - models and uh in the simple matters like
102:45 - uh back of words or uh in the uh tfidf
102:50 - all this they are based on this idea of
102:52 - counting words
102:53 - and how we can use this information and
102:56 - you can see how vectors come into play
102:59 - in this different applications of linear
103:02 - algebra in data science natural language
103:05 - processing in artificial intelligence in
103:07 - machine learning so they are super
103:13 - important another application of vectors
103:15 - can be representing customer purchases
103:19 - for example an N Vector p so let's say p
103:25 - can record a customer purchases over
103:27 - time with pi being the quantity or
103:29 - dollar value of an item I now what does
103:33 - this mean so let's say we have Vector P
103:37 - that represents the customer purchases
103:39 - and we are dealing with a single
103:41 - customer and we are just saving over
103:43 - time that information how many time this
103:46 - customer has made purchases over
103:50 - time so the quantity is in the
103:53 - um dollars so the dollar value of item I
103:56 - purchase so we are basically keeping
103:59 - track of what is the value of the item I
104:04 - that the customer has
104:06 - purchased so what we can
104:09 - do is we can assume that in here
104:14 - actually it already Mak that assumption
104:16 - it says n Vector which means that the
104:18 - number of rows or number of um items
104:22 - that the C Custer purchases
104:25 - is
104:27 - n now what the um the problem says that
104:32 - it represents is that in each
104:37 - entry and here we have in total n
104:39 - entries we got a dollar value of item I
104:43 - which means that here if I have P1 P2
104:47 - all the way to PN and here somewhere in
104:50 - the middle I got Pi in the east position
104:54 - it means Pi represents the
104:59 - value of
105:01 - item I so for example if I'm dealing
105:06 - with a
105:08 - CER that buys um let's say uh
105:13 - courses and uh the first item that the
105:17 - customer is buying is a mathematics
105:20 - course so I'm writing mathematics
105:24 - course and this is the first course that
105:26 - it buys e is by the way just a um way to
105:32 - refer to the E purchase so let's say um
105:36 - here somewhere in the middle the um
105:40 - customer decides to buy a deep learning
105:42 - course deep
105:46 - learning
105:48 - learning
105:50 - course and then it continues by buying
105:53 - uh the customer continues buying courses
105:55 - and the last course that the customer
105:57 - buys is let's say um career
106:02 - coaching
106:05 - course
106:06 - now let's say the mathematics course
106:09 - cost uh around
106:13 - $1,000 let's say the uh deep learning
106:17 - course costs
106:19 - $3,000 and then let's say the career
106:22 - coaching service which is usually one of
106:24 - the most applied and personalized one
106:26 - can cost all the way to
106:30 - $5,000 now we see that in the East
106:33 - position this is the East position let
106:36 - me change the color by the
106:38 - way so let's say this is the East
106:40 - position this is the first position and
106:43 - this is the last position so those are
106:45 - just
106:46 - indices we can see that in the East
106:48 - position we got the 3,000 which means
106:52 - that the p e is equal to
106:57 - $3,000 so this indicates that in the
107:01 - East
107:02 - purchase the customer purchased deep
107:04 - learning course and the value of that
107:07 - item was equal to
107:13 - $3,000 all right so now we are ready to
107:16 - go on to next major topic which is about
107:19 - vector addition and subtraction so we
107:21 - are going to do some operations and
107:23 - apply this operations to vectors so
107:26 - let's first formally Define this ideal
107:28 - of vector addition so uh two vectors of
107:32 - the same size are added by adding their
107:35 - corresponding elements the result is a
107:38 - vector of the same size so uh let's
107:41 - unpack this it says two vectors of the
107:45 - same size are added by their
107:47 - corresponding elements so here it refers
107:51 - to two different vectors let's say
107:53 - vector v and Vector W and it says let's
107:57 - add them what we refer as vector
108:00 - addition and says for that what we need
108:03 - to do is to take all the elements of v
108:07 - and then all the elements of w and using
108:10 - their corresponding elements so
108:14 - indices that helps us to understand
108:16 - where those elements are located we are
108:18 - using in order to add each
108:21 - element in the vector v to the element
108:25 - of the vector W in the same position and
108:28 - do note that in the second part it says
108:31 - the result is a vector of the same size
108:35 - because we are adding two different
108:36 - vectors of the same size mentioning here
108:41 - it means if we add two different vectors
108:43 - to the same uh that have the same size
108:46 - we are going to end up with a vector
108:49 - that has the same
108:50 - size now once I go into to the examples
108:53 - it will make much more sense let's
108:56 - quickly also look into this concept of
108:58 - subtraction so on its own a substraction
109:02 - is very similar to this idea of addition
109:04 - so if we have a substraction let's say
109:06 - we have vector v We substract Vector W
109:10 - then we are doing basically uh what we
109:12 - just did to the addition only instead of
109:15 - uh doing add we are doing substract so
109:19 - again we are just uh we are just
109:21 - substracting from vector v v Vector W
109:24 - they have the same size so we end up
109:26 - having the result which is a vector of
109:29 - the same size only one thing that you
109:32 - can see is that this can be also re
109:35 - written as V Vector plus and then minus
109:39 - W so we basically can represent
109:43 - substraction um on its own as a way of
109:46 - adding only we take the negative so the
109:50 - um opposite directed back Vector so this
109:53 - will make even much more sense uh once
109:56 - we go on to the examples so let's look
109:58 - into our first operation example where
110:01 - we are adding two different vectors this
110:03 - a basic example we got just two
110:05 - dimensional two vectors we got Vector a
110:09 - that has entries 2 three and Vector B
110:12 - that has entries 1 4 and what we are
110:15 - doing is that we are adding Vector a to
110:17 - Vector B we just learned that a we need
110:21 - to have the same size of vectors so you
110:23 - can see that Vector a has a dimension 2x
110:26 - one vector B has a dimension of 2x one
110:29 - so their sizes is the same both they got
110:32 - two entries only two
110:35 - elements and at the same time we just
110:38 - learned that what we need to do is to
110:40 - take their corresponding elements and
110:42 - add them to each other now what does
110:44 - this mean it means that we take from a
110:50 - the first element to
110:53 - and then we take the first element of
110:56 - the second Vector which is the B so we
110:58 - take the two from here and one from here
111:02 - the first element of a and the first
111:04 - element of B and then we are adding them
111:07 - to each other 2 + one is equal to three
111:10 - and then the same holds for the second
111:11 - and Tre so three which is the second
111:14 - element of vector a and then 4 which is
111:17 - the second element of vector B we are
111:19 - saying 3 + 4 is = to 7 so let me write
111:22 - it down even in a simpler manner such
111:25 - that it will make much more sense so
111:28 - Vector a has elements to three in the
111:31 - first element we got two in the second
111:33 - element we got three so a then we want
111:37 - to add B which has in the first element
111:40 - element equal to 1 and the second
111:43 - element is equal to four this means that
111:45 - if we want to add this vectors 2 3 + 1 4
111:51 - this is equal to we need to take two we
111:54 - need to add one so this element and this
111:57 - element and then we need to take three
112:00 - we need to add to four so this one and
112:02 - this one which is equal to 2 + 1 is
112:05 - equal to 3 3 + 4 is equal to 7 so we got
112:09 - a vector
112:11 - 37 do note that this Vector the result
112:15 - Vector contains again two elements and
112:17 - just one column so 2 by 1 so you notice
112:21 - that the sign
112:23 - that the size is the same of this result
112:26 - Vector now let's actually generalize
112:29 - this concept before moving on to the
112:31 - next example so if we got let's say
112:36 - Vector
112:38 - a that contains n elements A1 A2 all the
112:44 - way down to a
112:46 - n and it is
112:50 - from n dimensional space
112:53 - and we got Vector
112:58 - B that also has an element so remember
113:01 - that they both need to have the same
113:03 - size so B1 B2 all the way to b
113:08 - n so they come also so B comes also from
113:12 - n dimensional
113:17 - space so then when we add a to B
113:23 - this is equal to
113:29 - A1 A2 all the way to a
113:34 - n
113:35 - plus B1
113:38 - B2 all the way
113:41 - to
113:43 - BN so n by 1 n by one the sizes this is
113:48 - equal
113:49 - to let me actually use this color to
113:53 - make it even more visible so I for the
113:55 - first entry for my result Vector I will
114:00 - get A1 + B1 then A2 + B2 so all the way
114:05 - down onto the end element which is a n
114:13 - plus let me use a different color A1
114:17 - B1
114:19 - B2 b n
114:23 - so you can notice now in general terms
114:27 - what we are doing here so we are taking
114:29 - the A1 coming from the vector a we are
114:34 - adding in the same uh position the value
114:39 - that comes from Vector B which is B1 we
114:42 - are saying take the a1+ B1 this is the
114:46 - uh first element so the position stays
114:49 - the same and then in the result Vector
114:52 - so we take all the corresponding values
114:55 - that are have the same position in the
114:57 - corresponding Vector first from Vector a
115:00 - and then Vector B we are adding them and
115:02 - this forms our new vector and this new
115:05 - Vector will again have a size n by one
115:08 - so you can see that a the sizes of the
115:11 - two vectors are the same both have n
115:14 - elements and then we are using their
115:16 - corresponding elements to add them to
115:18 - each other element wise and then we are
115:21 - getting the result that has the same
115:23 - size so n by 1 so this is a more General
115:27 - description of how you can add two
115:31 - vectors let's now look into this
115:33 - specific example so we have a vector
115:36 - with the entry 073 so this comes from R3
115:40 - you can see so three dimensional vectors
115:43 - the second Vector is 1 2 0 and then the
115:47 - final result is 1 193 so how we got this
115:51 - we took 0 we added one 7 we added two
115:54 - and then three we added zero so you can
115:56 - see all these
115:58 - elements element Y and then this is
116:01 - equal to 0 + 1 is 1 7 + 2 is 9 and then
116:05 - 3 + 0 is 3 exactly what we got here so
116:09 - again the same sizes and the result is
116:11 - from the same
116:13 - size so quite
116:15 - straightforward now when it comes to the
116:17 - vector substruction what are we doing
116:20 - that um so what are we doing here so we
116:23 - are doing kind of very similar thing we
116:27 - are taking this element one we are
116:29 - subtracting the other one in this first
116:32 - element then we are taking the nine in
116:35 - the second position and subtracting this
116:38 - again from the second position of the
116:40 - second vector and we are putting in here
116:42 - 1 and then 1 - 1 is equal to 0 9 y - 1
116:46 - is equal to 8 so we get result Factor 08
116:50 - like in here and you can you can see
116:52 - that the sizes stay the same so also in
116:55 - this case let's write more General um
116:58 - this ideal of subtraction if we got a
117:01 - vector
117:02 - a
117:04 - from RN so n dimensional space and it
117:08 - can be represented by A1 A2 all the way
117:12 - down to a n so it has n elements n by
117:16 - one and then we got
117:20 - B also for from RN so coming from the uh
117:24 - n dimensional space which means that it
117:26 - got n elements so B1 B2 all the way down
117:31 - to BN again with the same size n by one
117:36 - then a minus B is simply equal
117:41 - to a A1 let me actually use the same
117:47 - colors to make it easier to follow
117:52 - so let me first draw my Square
117:57 - races and then here I will use blue for
118:01 - a and then red for the uh color for
118:06 - second Vector which is
118:08 - B here I will use black
118:18 - minus then given that the same size
118:22 - should be for the result Vector I
118:24 - already know that I expect n different
118:26 - elements for this and then
118:29 - here I'm taking
118:34 - this first element that comes from
118:37 - Vector
118:38 - a i subtracting from this the first
118:43 - element that comes from Vector B so
118:45 - element y subtraction B1 and I'm already
118:49 - getting the result for the first element
118:52 - in my result Vector so you can see A1
118:57 - minus B1 I'm taking this element and
119:00 - this element and subtracting them from
119:02 - each other to get A1 minus B1 and then
119:05 - the same holds for all the other
119:07 - values only coming from different
119:10 - elements from Vector
119:13 - a subtracting from this the
119:17 - corresponding values element Wise from
119:20 - the vector B so B2 B3 all the way to a n
119:26 - so you can see that in my result
119:28 - Vector a vector minus B Vector in the
119:31 - first element I get A1 - B1 then A2 - B2
119:36 - then A3 - B3 in the third element all
119:38 - the way down to the end element which is
119:41 - equal to oh this should be b a n minus
119:46 - BN so um this already should makes uh
119:50 - much more sense so every time we take
119:52 - the element in the same position from
119:54 - one vector than the other we subtract
119:56 - from each other in order to get the
119:58 - corresponding element in the final
120:03 - Vector all right so let's now uh before
120:06 - moving on to the
120:07 - properties um I wanted to show you um
120:13 - this only in a coordinate space so what
120:17 - this means in terms of visualization in
120:19 - a coordinate space
120:22 - so uh let's say we have a coordinate
120:32 - space this is my Y
120:39 - axis this is my x axis
120:43 - so this is X and the Y and this is my
120:47 - Center so 0
120:50 - 0 and what I'm doing
120:54 - here is
120:56 - simply I want to have Vector a let's say
121:01 - this is just um Vector a simple one with
121:04 - the coordinates um let's say four and
121:09 - minus
121:10 - 2 and I got Vector
121:14 - B let me use a different
121:18 - color Vector
121:20 - B that
121:22 - has
121:24 - coordinates let's
121:26 - say
121:29 - minus four and
121:33 - four so let's actually visualize them
121:37 - let's first start with the vector a uh
121:41 - which has a x value of
121:44 - four three and
121:46 - 4 1 2 3 and
121:49 - 4 and the Y value - 2 so this
121:59 - is my Vector
122:04 - a and let's now visualize the vector B
122:08 - so minus 4 and
122:10 - 4 which means
122:16 - that let me
122:18 - actually extend this
122:23 - this is min -4 so the x coordinate is
122:25 - min -4 so it should be here and then the
122:29 - y coordinate is four so 1 2 3 and 4 it's
122:35 - this one which means that my Vector B is
122:41 - this
122:47 - one all
122:49 - right so you can can see now that the
122:53 - vector a is in here and the vector B is
122:55 - in
122:56 - here now what I want to do is to add
123:00 - these two vectors to each other so what
123:03 - I want to do is to take this Vector
123:07 - a and add to
123:10 - this the vector B which
123:15 - is is equal to 4 - 4 was 0 and then - 2
123:21 - + 4 is equal to 2 so 0er and then two it
123:30 - is zero and
123:36 - two
123:45 - two so this is my result Vector so now
123:51 - when we are clear on how we can add
123:53 - vectors how we can perform these
123:55 - different operations and what it means
123:58 - in practice when it comes to looking at
124:00 - the vectors in a cordant space and
124:02 - adding them or subtracting them we are
124:04 - ready to look into the properties of
124:07 - vector additions so this is something
124:10 - that will definitely seem familiar to
124:12 - you uh from
124:14 - pre-algebra where we are basically using
124:16 - all these properties that we already
124:18 - know that holds for uh numeric values
124:21 - for the scalers that being transferred
124:24 - to this Vector space so we are going to
124:27 - talk about this four different
124:29 - properties that the vectors have the
124:32 - first one is the cumulative property
124:34 - which says that if we add a vector a to
124:37 - Vector B then this is the same as adding
124:41 - a vector B to Vector a so basically the
124:45 - order of the vectors doesn't really
124:46 - matter when it comes down to adding them
124:49 - so formally a plus b is equal to B + a
124:53 - for any vectors A and B of the same size
124:57 - then we have associative property which
124:59 - says A + B + C is equal to a + b + C we
125:06 - can write both us A + B + C now what
125:10 - does this mean we know from pre-algebra
125:13 - that this parenthesis means first do
125:15 - this addition and then do the rest of
125:18 - operations in here it basically says if
125:21 - you add add a to the B first and then
125:24 - you add the C is the same as first you
125:28 - add B to the C and then on the top of
125:30 - that you add the vector a so then the
125:35 - third property is addition of zero
125:37 - vectors which says if we add a zero
125:39 - Vector to Vector a then this is equal to
125:43 - adding a vector zero to a and this is
125:46 - equal to Vector a so adding the zero
125:49 - Vector has basically no impact act on
125:52 - the vector
125:54 - whatsoever then the final property is
125:56 - subtracting a vector from itself which
125:58 - means if we take the vector we subtract
126:00 - the same Vector from itself so a minus a
126:03 - and we get a zero Vector so a minus a is
126:07 - equal to zero vector and this heals the
126:09 - zero Vector now let's look into each of
126:12 - those properties one by one and let's uh
126:15 - look into specific examples uh in some
126:18 - cases we will prove this on the example
126:21 - that we have to make this Concepts much
126:23 - more clear so let's start with this
126:26 - cumulative property of vector additions
126:29 - so we want to see whether a plus b is
126:31 - equal to B + a so let's say we have a
126:36 - vector
126:37 - a that has coordinates or magnitude and
126:42 - direction that is equal to one and two
126:45 - then we have a vector uh let's say B
126:52 - that has a magnitude and direction of
126:56 - Min - 2 and
126:59 - three so the first thing that we want to
127:01 - check is indeed whether the A + B is
127:05 - equal to B + a so therefore let's first
127:10 - calculate this part and then we will
127:12 - calculate this part that I will Define
127:14 - by one and two and we will see whether
127:16 - we are indeed having the same value the
127:19 - same vector or not so let's see so we
127:21 - have here a so A + B which is the first
127:28 - value that we want to calculate a + b is
127:31 - equal to 1 2 + -
127:36 - 23 and we learned before that this is
127:40 - simply equal to take this value and then
127:43 - add this one so 1 + - 2 and then 2 +
127:49 - 3 so this gives us a Vector 1 - 2 is = -
127:54 - 1 and 2 + 3 is = 5 so we get that A + B
127:59 - is equal to -5 this Vector now let's
128:03 - look at the second quantity so B Vector
128:07 - B plus Vector a this is equal to - 2 3 +
128:13 - 1 2 and this is equal 2 - 2 + 1 and then
128:18 - 3 + 2 this gives us - 2 + 1 is = to -1
128:23 - and 3 + 2 is = to 5 so we can already
128:27 - see from here that the quantity one is
128:31 - indeed equal to quantity 2 which proves
128:35 - that indeed the A + B is equal to B + a
128:40 - what this basically means is that adding
128:43 - two different vectors the direction or
128:45 - the order is not important whether you
128:48 - add a on the top of the b or B to a it
128:51 - doesn't M at the end is the same and
128:54 - actually you can also see it if you uh
128:56 - combine this or if you do this in a more
128:59 - general terms so let's say if we
129:03 - have a vector a which is equal to in an
129:06 - N dimensional space A1 A2 up to a n so
129:13 - it has n by one dimension and you have a
129:16 - vector B with the same size from the
129:19 - same RN dimension
129:22 - and it has elements B1 B2 up to BN and
129:27 - the dimension is equal to n by one then
129:30 - if we calculate first
129:33 - A+
129:35 - B and this is equal to Simply A1 + B1 A2
129:41 - + B2 up to a n +
129:45 - BN and if you calculate the second uh
129:49 - amount which is B plus a and this is
129:53 - equal to B1 + A1 B2 + A2 up to bn+ a n
130:04 - you can see that A1 + B1 is equal to B1
130:09 - + A1 simply from prealgebra you know
130:13 - that if those are all constants for
130:15 - inance 2 + 3 is = to 3 + 2 in the same
130:19 - way a 2 + B2 is equal to B2 + A2 and
130:23 - then here up to a n + BN is equal to BN
130:28 - + a n what this means is that all these
130:31 - elements they are basically the same
130:35 - which means that we already have a proof
130:38 - so we get this proof and we can see that
130:41 - even for the general term independent
130:43 - what this Vector a is what this Vector B
130:46 - is that a + b is equal to B plus
130:53 - a this is exactly what we saw before in
130:56 - the first property which is called
130:58 - commutative property of the vectors that
131:00 - A+ B is equal to B + a now let's move on
131:04 - to the other property which is called
131:06 - associative property of the vectors now
131:08 - what this property does and says is that
131:11 - a plus b so first we do this plus C is
131:15 - equal to a + b + C and this is then
131:19 - equal to a + b plus C now let's then see
131:24 - um this specific property on an actual
131:27 - example so what is basically says is
131:30 - that if we have this example where a is
131:33 - equal to actually I had this before let
131:36 - me simply just remove this part let's
131:41 - then add our third Vector which is C and
131:45 - let's call it let's say it has a
131:47 - representation of four and five
131:51 - then the idea behind this property is
131:55 - that what we need to prove here that a
131:59 - plus b within the parentheses plus C is
132:03 - equal to
132:08 - a
132:17 - plus B+
132:19 - C and then this is equal to a + b plus C
132:27 - so let's see actually whether this is
132:29 - indeed true for this specific case now
132:32 - this should come very intuitive so I'm
132:34 - going to do it very
132:36 - quickly so first we have this quantity
132:39 - this one then we have this one and the
132:41 - third one let's do it very quickly so A
132:45 - +
132:46 - B plus
132:48 - C is equal to
132:54 - one2
132:57 - plus and then we at C so it is
133:03 - simply 4
133:06 - five and then this is equal to we saw
133:09 - before when doing this that we were
133:12 - getting
133:13 - 1 - 2 2 + three and then we add this
133:18 - four
133:19 - five this is simply equal 2 1 - 2 is
133:26 - -1 and 2 + 3 is
133:29 - 5 + 4
133:33 - 5 now given that it doesn't really
133:36 - matter no longer that do we have uh here
133:39 - parenthesis or not this basically means
133:43 - that this volue is simply equal
133:47 - to -1 + 4 so here - 1 + 4 here 5 +
133:54 - 5 so this is then equal to 3 and then
134:00 - 10 all
134:02 - right let's then now quickly do the
134:06 - second amount which says first add the
134:10 - vector B to Vector
134:12 - C and only then add the vector a on the
134:16 - top what this means is that we need to
134:18 - take one two this is Vector a and and we
134:21 - will only add this once we have added
134:23 - the minus 23 the vector B plus to the
134:27 - vector 4
134:29 - 5 okay
134:32 - so we can see that we are just leaving
134:34 - this in here let's first add this two -
134:40 - 2 + 4 3 +
134:46 - 5 so this gives us one two plus - 2 + 4
134:51 - is uh 2 and then 3 + 5 is 8 so this
134:56 - gives
134:57 - us let me remove this
135:03 - calculations so this gives us 1 + 2 is =
135:08 - to three and then 2 + 8 is = to 10 okay
135:13 - great so now we got already the quantity
135:17 - one being equal to quantity two let's
135:20 - check whether this is all equal to this
135:23 - one it should already be um something
135:26 - that you see now given that um we know
135:29 - just from mathematics that parenthesis
135:31 - doesn't really matter when it comes to
135:33 - the scalers and adding two vectors is
135:35 - basically very close to this idea of
135:38 - edited property um of the edited
135:41 - property of the scalers but just let's
135:43 - quickly do it to be 100% sure so when we
135:46 - take this Vector a to the B and to the C
135:50 - we had all this this is equal to one two
135:54 - added to minus 2 three and then added
135:57 - this to 4 and five now what this is
136:01 - equal
136:02 - to let me actually write this in bit
136:06 - shorter way such that it can be all fit
136:08 - in in the small place so 1 two + - 2 3
136:15 - plus
136:17 - 45 this is equal to basically 1 - 2 + 4
136:24 - and then 2 + 3 +
136:26 - 5 now what is this
136:32 - number 1 - 2 + 4 is simply equal to 1 -
136:37 - 2 is = to -1 and then + 4 is = to 3 so
136:41 - first element is 3 2 + 3 + 5 is = to 5 +
136:45 - 5 which is equal to 10 perfect so now we
136:48 - get the confirmation thatting the A + B
136:52 - + C is = to A + B + C is = to A + B +
137:00 - C so let's quickly also look into this
137:03 - addition of zero vector and the
137:04 - substracting a vector from itself
137:06 - properties and uh the detailed
137:08 - explanation of this or example of this I
137:10 - will leave it to you so when it comes to
137:13 - this a plus um 0 is equal to 0 + a is
137:16 - equal to a so this property let's say if
137:20 - a is equal to this 23 and then we are
137:24 - adding on this a plus some zero Vector
137:28 - which basically means take two tree and
137:31 - then added the same size of zero Vector
137:34 - you can see that this is the same as
137:37 - adding this zeros on these values now
137:41 - what do we get we get that this is equal
137:45 - to 2 + 0 is 2 and then 3 + 0 is
137:48 - 3 there we go so we already see very
137:52 - quickly that it doesn't really matter
137:54 - whether we add a zero Vector to this
137:56 - original a vector or not we in all cases
138:00 - it just adding a zero Vector has no
138:03 - effect and seeing from the commutative
138:05 - property that a plus b is equal to B+ a
138:08 - we already know that if um a + 0 is
138:12 - equal to uh a and is equal to this then
138:15 - also 0 + a will be the same and we can
138:20 - see indeed that we just saw that a + 0
138:23 - is simply equal to a so we basically
138:25 - have quickly proven all
138:28 - this now when it comes to the
138:30 - subtracting Vector from itself I think
138:32 - this is a very nice one just to see how
138:34 - we um uh take the same vector and
138:38 - subtract from that value and we get zero
138:40 - and this is very similar to working with
138:42 - just real numbers in the same way as 3 -
138:45 - 3 is equal to Z also when we have a
138:48 - vector consisting of the scalers
138:51 - like a is equal 2
138:54 - 23 in the same manner if we take this a
138:58 - and we subtract it from itself so a
139:00 - minus a then what we will get is 23
139:04 - minus 23 and this will give us 2 - 2 is
139:08 - z and then 3 - 3 is z so we get a vector
139:12 - zero so zero
139:14 - Vector so now when we are clear on how
139:16 - we can perform different operations on
139:18 - our vectors and also we know uh what are
139:21 - the properties of uh adding and
139:24 - subtracting uh different vectors we are
139:26 - ready to move on to a bit more advanced
139:29 - topics so uh in this module we are going
139:31 - to discuss this idea of scalar
139:34 - multiplication we're going to look into
139:36 - the example how what happens and how we
139:39 - can do the uh Vector multiplication with
139:41 - the scaler then we are going to uh look
139:44 - into the Spen of vectors what it means
139:47 - to have a Spen of vectors uh what is the
139:50 - linear combination and the relationship
139:53 - between the span and linear combination
139:55 - and the unit vectors then we are going
139:58 - to look into the application of scalar
140:00 - Vector multiplication in audio scaling
140:03 - uh example and then finally we are going
140:05 - to finish off this module by looking
140:07 - into the length of a vector and a DOT
140:10 - product and we are going to uh go back
140:12 - to this idea of distance understanding
140:15 - vector magnitude and understanding
140:17 - Vector
140:18 - length so let's get started it now
140:21 - before we look into this idea of span
140:24 - and linear combination I quickly wanted
140:26 - to look into this idea of scalar
140:28 - multiplication and the um specific
140:31 - definition of it so formally the scalar
140:34 - multiplication involves multiplying each
140:36 - component of a vector by scalar value
140:40 - effectively scaling the vector's
140:41 - magnitude so what do I mean here let's
140:45 - say we have a vector and I will write it
140:48 - in the general terms to keep everything
140:50 - General so let's say we have a vector a
140:53 - let me pick up my pen a and this Vector
140:57 - a is from n dimensional space so it is
141:01 - from
141:02 - RN and it can be represented by A1 A2 up
141:07 - to a
141:09 - n and I have this magnitude um of a
141:13 - vector and now I want to scale this uh
141:18 - Vector for which I know the magnitude
141:20 - and the dire diretion I want to scale it
141:22 - with a scaler and we learned before that
141:24 - the scaler is just a number so um scaler
141:27 - in this case I will be uh referring it
141:30 - to uh by C so c will be my scaler and uh
141:35 - this comes from R which means that it's
141:37 - a real
141:39 - number let me actually use a different
141:42 - color to make it easier to
141:46 - follow okay so my scaler will be with
141:49 - the color uh red so C and C comes from
141:56 - R
141:58 - so what do I mean by scalar
142:00 - multiplication I mean that I want to
142:02 - find what is
142:04 - this c
142:08 - times
142:10 - a this is what we mean by scalar
142:13 - multiplying with Vector now what does
142:17 - this definition say it says when we are
142:19 - multiplying in scalar with Vector so the
142:22 - scalar multiplication meaning
142:24 - multiplying Vector with the scalar it
142:27 - involves multiplying each component of a
142:30 - vector by a scalar value so if we
142:33 - translate it to this specific example it
142:36 - means
142:37 - that this
142:39 - amount so this amount is equal
142:48 - to taking C
142:51 - and multiplying it with each
142:54 - element of this Vector so each component
142:58 - of a vector and what are the components
143:00 - of my Vector the A1 A2 a up to the point
143:04 - of a n so all these
143:06 - components so that means that the first
143:08 - element of this new Vector the scalar
143:12 - multiplication result will be C *
143:17 - A1 C * h
143:21 - two dot dot dot so all this middle
143:23 - elements and at the end again c times
143:27 - and then a n and then in both cases of
143:32 - course the number of elements doesn't
143:35 - change so the so the number of rows of
143:38 - my Vector doesn't change it's n so here
143:40 - also n and then number of columns is the
143:44 - same so it's just a column Vector so one
143:47 - column so what we see here is the that
143:50 - we go from A1 to C * A1 we go from A2 to
143:55 - C C * A2 up to the N transforms into C *
144:01 - a n so we see very easily that I keep
144:05 - all the elements from this Vector I take
144:08 - them in here and instead what I'm doing
144:10 - is that I'm multiplying every element
144:13 - from this vector by the scaler
144:16 - C so this is exactly what this
144:19 - definition says
144:21 - and let's actually go ahead and do a
144:25 - Hands-On example with some real numbers
144:28 - to have this um method and to have this
144:32 - uh definition very clear in our mind
144:34 - because we are going to make use of this
144:36 - fundamental operation scalar
144:38 - multiplication on and on in the upcoming
144:41 - lectures and just in general in your
144:43 - journey in any applied
144:45 - sciences so this is an example of scalar
144:49 - multiplication uh here here what we are
144:51 - doing is that we want to multiply this
144:53 - Vector C so in this case the vector is
144:57 - defined by a letter C and then on the
145:00 - top we can see the arrow indicating that
145:02 - this is the vector now and here we refer
145:05 - the scalar by a letter K we are saying
145:08 - we want to perform scalar multiplication
145:11 - which means that we want to
145:13 - multiply the uh Vector C by the scaler K
145:19 - so how we can do that
145:20 - so what we want is to multiply K by C
145:25 - and we just learned that for that what
145:27 - we need to do let me write this
145:30 - over so this equal to minus 2
145:37 - multiplied by 4 - 3 this is my Vector so
145:41 - this is the K and this is the C this is
145:44 - equal 2 so I take my
145:47 - scalar and I multiply it with the each
145:51 - of the element of the C so - 2 * 4 and
145:54 - then -2 *
145:58 - -3 so - 2 * 4 is = to - 8 and then - 2 *
146:04 - - 3 so- - it goes away it becomes a plus
146:08 - and the 2 * 3 is 6 so my end result the
146:11 - K * C is equal to minus 8 6 this is my
146:16 - final
146:18 - result so let's quickly also do yet
146:22 - another example and this one is a unique
146:25 - one because it's relating to this idea
146:27 - of U multiplying something with a zero
146:32 - uh which is something that we also uh
146:33 - know from our high school that when we
146:35 - multiply number let's say seven by zero
146:39 - they're getting zero and here in this
146:42 - example the uh problem is describe the
146:46 - effect of a scalar multiplication by
146:48 - zero on any vector VOR which means what
146:52 - we are doing is that in this example is
146:54 - we want to know what is this result
146:58 - of any Vector let's say Vector uh C so
147:02 - we will use the same example C only this
147:05 - time instead of multiplying it with the
147:07 - scalar k equal to minus 2 our scalar
147:09 - will be zero which means that c is equal
147:12 - to 4 - 3 and then K is now equal to zero
147:16 - and we want to find out what is this K *
147:18 - C
147:21 - let me actually write down the K with a
147:23 - different
147:31 - color k is equal to zero so what we want
147:35 - to find out is K * and then
147:38 - C and this is that equal
147:44 - to zero so I'm taking the
147:47 - K 0 times
147:50 - then I'm taking each of the elements of
147:52 - C which is 4 and then minus 3 and I know
147:56 - that when multiplying the number with a
147:59 - zero it gives me zero which means that I
148:02 - end up with 0 here 0 * 4 is 0 0 * - 3 is
148:06 - also zero so I end up with a zero Vector
148:11 - now this gives me an idea already that I
148:14 - can make a general conclusion that
148:18 - independent of the type of Vector that I
148:20 - have independent what are these values
148:22 - in my C uh if I have any
148:26 - Vector C and I'm multiplying it with
148:31 - zero then this will always give me a
148:34 - vector of zero because all the members
148:38 - of this final Vector will be just zeros
148:42 - so if for instance the C comes from uh
148:47 - let's say r n so it has n different
148:51 - elements it comes from uh n dimensional
148:53 - space then my final result
148:57 - of0 * C so this zero Vector this one so
149:03 - zero that this one will come also from
149:07 - our end so you will be having a vector
149:11 - so 0
149:13 - * c will then be equal to 0 0 blah blah
149:18 - blah blah zero so n time
149:21 - zeros so this is then the idea of
149:24 - multiplying so scaling a vector with
149:28 - zero and this is our example to all
149:31 - right so let's now move on on to our
149:36 - application of scalar vual
149:37 - multiplication and then after this we
149:39 - will go back to this idea of linear
149:41 - combinations and
149:42 - dispense so in this specific application
149:46 - we have a scalar Vector multiplication
149:48 - and we are looking into application of
149:50 - audio scaling so the scalar Vector
149:53 - multiplication audio processing this can
149:55 - change the volume for instance of an
149:57 - audio signal without altering its
150:00 - content so um you might have noticed
150:03 - that um when uh when you are listening
150:07 - to video you can simply increase the
150:10 - volume of that video or decrease it but
150:12 - you will notice that the content doesn't
150:14 - change you are just increasing the
150:15 - volume or decreasing it even on the TV
150:18 - when you are watching a show you are
150:19 - increasing ining The Voice or decreasing
150:22 - now what you're basically doing behind
150:24 - and this is super interesting is that
150:26 - behind the scenes what is happening is
150:28 - that there is simply um audio that um
150:33 - contains that show and audio of that
150:36 - show is being multiplied with a scaler
150:39 - and that scale is simply the volume
150:41 - scale if you scale it in such way that
150:45 - you want to decrease the volume so the
150:47 - audio will then have uh lower volume
150:51 - then you are simply multiplying it uh
150:54 - your vector containing the audio
150:56 - information in such way that those newer
150:59 - volume indications they will be they
151:02 - will be containing lower
151:04 - numbers hope this makes sense let's look
151:06 - into the example this make uh this will
151:08 - definitely clear this out so um let's
151:12 - assume we have an a vector a that
151:15 - represents the audio signal and we want
151:19 - to m multiply Vector a by scalar B to
151:23 - adjust the volume so B is some sort of
151:26 - number it can be so B comes from our so
151:31 - is a real number while
151:34 - a is simply a vector given that it
151:37 - doesn't mention it here I'm assuming
151:39 - that a comes from RN so it comes from r
151:43 - n dimensional space so imagine of a as
151:47 - this Vector A1 HQ blah blah blah blah up
151:51 - to a n and each of these values it
151:54 - basically describes uh an the audio
151:58 - signal so it represents um uh an amount
152:01 - so it contains an amount that represents
152:03 - the audio signal of your uh video or uh
152:08 - your uh
152:09 - show and then the b in this case for
152:13 - instance in this example you can see
152:15 - that the B is then uh equal to for
152:18 - instance 1.2 1 2 or B is equal to - 1 /
152:23 - 2 so you can see that b is = to 1 / 2
152:27 - which basically is offensive saying that
152:30 - b is equal to 0.5 or B can be equal to
152:35 - min-1 / 2 which is -
152:39 - 0.5 now then it says then the B * a
152:43 - which basically means multiplying our um
152:47 - scalar beta
152:50 - by the vector containing the audio
152:53 - signal a so this B * a is perceived as
152:57 - the same audio signal but at the lower
153:00 - volume now why lower because you can see
153:03 - that b is equal to 0.5 or Min - 0.5 it
153:07 - means that once you take all these
153:10 - elements of your a and you multiply it
153:13 - with the number that is smaller than one
153:15 - in this case 0.5 then all these numbers
153:18 - will decrease which means that also your
153:21 - audio volume will
153:24 - decrease so let me actually uh show you
153:28 - an
153:29 - example so let's say our talk show is
153:33 - very short and you know the audio
153:35 - variation is very low you have a vector
153:39 - a that is quite small it comes from a
153:43 - three diim menure space so R Tre and it
153:46 - has numbers like three uh six and then
153:49 - five
153:50 - so 3x1 vector and then we have our audio
153:55 - adjustment scalar beta which is equal to
153:59 - 0.5 now when we take the beta we're
154:02 - multiply it by our audio
154:04 - signal then what we
154:06 - do
154:09 - times this clear so times what we are
154:13 - doing is that we are simply taking all
154:16 - the elements of our a so three 6 and 5
154:21 - and what we are doing is that we are
154:23 - multiplying it by
154:25 - 0.5 0.5 and 0.5 or you can also say 1 /
154:31 - 2 so what this is equal is that 3 * 0.5
154:35 - is
154:36 - 1.5 6 * 0.5 is 3 and then 5 * 0.5 is
154:42 - 2.5 and you can see that all these
154:45 - numbers 1.5 3 and 2.5 they are smaller
154:49 - and specifically two times less than all
154:52 - the original values in the um original
154:55 - audio so original audio is a which was 3
155:01 - 6 and 5 and the new audio the the scaled
155:06 - one is so audio scaled so B * a is equal
155:12 - to 1.5 3 and
155:16 - 2.5 so you can clearly see this
155:19 - transform
155:20 - where uh this element three is larger
155:22 - than 1.5 6 is larger than three and then
155:25 - the last element five is larger than 2.5
155:28 - which means that this
155:30 - audio
155:32 - audio is much at a higher volume so the
155:38 - volume two times
155:41 - higher than this audio
155:50 - so this is basically the idea of uh
155:53 - applying scalar multiplication to our
155:55 - audio pre-processing I will leave the
155:57 - other example to you that will show that
156:00 - when your scaler is equal to minus 0.5
156:04 - you again will end up with a lower
156:06 - volume only that time the volume will be
156:09 - much much lower than the original one so
156:12 - now that we know how we can perform scal
156:14 - multiplication in theory as well as we
156:17 - have looked into an example how we can
156:19 - do it in terms of the numbers and
156:20 - multiplying them and we have also seen
156:23 - uh applying still multiplication in
156:26 - practice uh so we have seen in this
156:29 - audio processing stage the uh
156:32 - multiplication process we are ready to
156:35 - look into the visualization of it this
156:37 - will help us to get a better
156:39 - understanding on uh what exactly happens
156:43 - when we are scaling different vectors
156:46 - let's look actually in the following
156:47 - example so let's assume we have a vector
156:51 - oh let me remove
156:55 - that so let's assum we have a vector and
157:00 - that Vector is let me get a color this
157:04 - one for instance a vector a and this
157:07 - Vector a consists of elements one and
157:11 - two so where does this Vector lie the
157:16 - vector is with
157:18 - um one so here in our coordinate system
157:21 - this is our xaxis this is our y AIS and
157:24 - here we got uh let me actually pick
157:27 - another color let's say black
157:31 - one and then we got one and then two
157:36 - right this is two this is one so it is
157:39 - this one so the line that we get here it
157:43 - is this one so this is our Vector
157:46 - a now let's assume I want to
157:50 - multiply my Vector a so I want to scale
157:53 - my Vector a by a constant Tree by scaler
157:57 - tree so I have a scaler let's say I call
158:00 - K and this
158:02 - k a different number let's say k is
158:06 - equal to
158:08 - three so what I wanted to do is to
158:10 - perform a scal multiplication so I want
158:13 - to obtain K multiplied by a and we
158:17 - learned that this is simply =
158:21 - 2 three
158:24 - times and
158:26 - then one
158:28 - two and then this is equal
158:34 - to 3 * 1 3 * 2 which is equal to 3 and
158:41 - then
158:42 - six so let's also visualize the scaled
158:46 - uh
158:47 - Vector so let me pick
158:49 - this yellow color this will be our
158:52 - scaled Vector so we have that scale
158:55 - multiplication and we are going to
158:56 - visualize that so we have three and six
159:00 - so this is three 1 2 three and this is
159:05 - six so we have this
159:07 - point so you should already see what is
159:10 - going on
159:11 - here
159:13 - so we
159:15 - got 3 a here
159:20 - so you can see that this part is our
159:23 - Vector a and this longer one is 3 a and
159:27 - even visually you can see that this
159:30 - longer Vector is simply the three times
159:33 - of the shorter Vector so we got this and
159:37 - then if you add on the top of this the
159:40 - same three times you will then end up
159:44 - with
159:46 - the
159:47 - original so sced version of
159:54 - that so basically this is a this is a
159:57 - this is a we combine three different so
160:00 - we scale a three times and we simply get
160:04 - a three times longer version with the
160:07 - same direction so you can see that when
160:10 - we are scaling even visually it makes
160:12 - sense so we are scaling our Vector a
160:15 - three times and we are just getting that
160:18 - Vector so we are
160:20 - transforming let me remove
160:28 - this so
160:31 - basically we are taking this vector and
160:34 - we are scaling it up to this
160:39 - point if I would do it only two times
160:43 - then it would be something like
160:46 - this or one and half times
160:49 - it would be something like this so only
160:54 - half of
160:57 - it so now this should make much more
161:00 - sense let us actually do yet another
161:02 - example to uh make sure that we are
161:05 - clear on these visualizations because we
161:07 - are going to make use of it when uh
161:09 - looking into this idea of linear
161:11 - combination in a Spen so let's say we
161:15 - have a vector B and this Vector B has
161:18 - elements Z and three so let's visualize
161:23 - and uh plot this Vector so it contains
161:26 - elements Z and 0er and three so0 and
161:30 - three so this is the X element and the Y
161:33 - element on the Y AIS we can see this is
161:36 - three which means that our Vector B is
161:40 - this
161:40 - Vector all right perfect so this is our
161:44 - B let's now multiply so scale our Vector
161:48 - B by scalar 2 so let's say we want to
161:52 - get 2
161:55 - times
161:57 - B so what is this amount this is equal
162:00 - to 2 * 2 * and I'm simply taking each of
162:04 - those elements 0o and then
162:06 - three
162:09 - so this is then equal to 2 * 0 is 0 and
162:13 - then 2 * 3 is equal to
162:16 - 6 so this is my new SK fed Vector 2 * B
162:21 - Vector this one so let's visualize this
162:24 - the x-axis value is zero so we are still
162:27 - here and then the Y AIS value is sixth
162:32 - so what is sixth this thing all right so
162:36 - you already should see that this is very
162:39 - similar what we had before so this is 2
162:43 - B all right so this all uh should make
162:47 - sense uh also we learned as part of the
162:51 - um High School when visualizing
162:53 - different plots so this is quite similar
162:55 - to this idea of having y isal to X and
162:58 - then scaling it getting like Y is equal
163:00 - to 2x so in this case only we know
163:03 - exactly where the vector starts and ends
163:07 - uh so we have a much more specific
163:11 - definition instead of having all this
163:13 - infinite number of points on the line
163:15 - but the idea stays the same so we are
163:18 - taking this vector and we are then
163:21 - scaling it two times so we get 2 B
163:24 - vector and I could do the same only
163:28 - instead what I could also do is I could
163:30 - do like uh 0.5 or 1 / 2 * B so I take
163:35 - the half of it which means I would get
163:37 - this
163:38 - vector or I could multiply it with minus
163:41 - one so
163:45 - minus - 1 * B so I was skill with minus
163:49 - one and then I will simply get the
163:52 - negative
163:54 - version of
163:56 - my original Vector so this thing this
164:00 - would be minus b or Min -1 * B so this
164:05 - is basically the idea of uh scal
164:07 - multiplication when visualizing it in
164:10 - our coordinate system cian cordinate
164:13 - system and now when we know all this we
164:16 - are ready to move on on this idea of
164:19 - linear combination so let's now talk
164:22 - about another super important topic
164:25 - which is the dot product and it's
164:27 - applications so we are going to talk
164:29 - about the dot product and we are going
164:31 - to uh make this relation between the dot
164:34 - product and the distance something that
164:36 - we have learned as part of the high
164:39 - school so the dot product we are going
164:41 - to understand it we are going to plotted
164:43 - the vectors we are going to perform this
164:45 - dot product in examples as well as we
164:48 - are going to to look into the properties
164:50 - of the dot product and the inner product
164:53 - concept we are going to see the
164:55 - relationship between dot product and
164:56 - inner product we're going to talk about
164:59 - the cou squares inequality and the
165:02 - geometric interpretation of it so those
165:05 - are really important Concepts um the CI
165:09 - SARS inequality um the intuition of it
165:12 - as well as the implication of it um
165:15 - specifically in machine learning and
165:17 - then we are going to talk about the
165:19 - vector triangle inequality and uh in
165:22 - what cases also the norm is equal to
165:25 - zero and finally we're going to define
165:28 - the angle between vectors so when it
165:32 - comes to the dot product uh the dot
165:34 - product is also known as the scalar
165:36 - product it's a fundamental operation in
165:39 - linear algebra it combines the two
165:42 - vectors to produce a scalar it's a way
165:45 - to measure how much one vector extends
165:48 - in the direction of the one so providing
165:52 - insights into the geometric and the
165:55 - algebraic properties of the vectors so
165:59 - when we are using this operation of the
166:01 - dot product you can see it as an
166:03 - operation that we apply to vectors and
166:06 - in this way once we look into the
166:08 - geometric uh side of it so we will
166:11 - visualize the vectors and we will see
166:13 - the geometric insights behind it it will
166:17 - all make much more sense because this
166:19 - will relate back to our geometric
166:21 - interpretation this idea of Pythagorean
166:23 - theorem because they are highly related
166:25 - this idea of distance and how by using
166:29 - the dot product we can relate all these
166:32 - different concepts so let's first
166:35 - formally um Define the idea of uh dot
166:39 - product so the uh dot product of a
166:43 - vector v with itself gives the square of
166:47 - the length of V so earlier when we were
166:51 - looking into the prerequisites of the
166:52 - course I did briefly mention that we
166:55 - need to know this ideal length of a
166:57 - vector or length of a line we said that
167:00 - we denoted this by this concept or by
167:03 - this notation of two straight lines and
167:07 - then the name of the vector and then
167:09 - another two straight lines and this is
167:12 - what we refer as the length of vector so
167:15 - the distance between two points or the
167:17 - starting point and the end point point
167:19 - where our Vector is described so if we
167:22 - have for instance this Vector then if we
167:24 - take this point and we take this point
167:27 - then we can use that in order to
167:29 - calculate the distance of this vector
167:32 - and the distance of this Vector is
167:33 - defined by this
167:36 - notation so that's exactly what we have
167:39 - here too so you can see that we are
167:41 - saying that the dot product is actually
167:44 - this V so we take the vector we multiply
167:48 - with it itself we are seeing this is the
167:50 - dotproduct of a vector and it is equal
167:54 - to the square of the distance or the
167:58 - length of the vector so the uh in the
168:03 - left hand side we have this notation of
168:06 - the dot product we just use this dot as
168:08 - usual with any multiplication or any
168:11 - product and we are saying this is equal
168:13 - to and we take the length of a vector v
168:19 - and we Square it and this gives us our
168:22 - DOT product very soon we will see when
168:26 - uh applying it on a uh more general
168:28 - terms what this IDE of dot product is
168:31 - when we apply it to vectors just in
168:34 - general so in a two dimensional space
168:37 - when we have each Vector represented by
168:39 - two numbers let's say vector v which is
168:41 - equal to X and Y so we have on the first
168:43 - element the uh x coordinate of our
168:47 - Vector in our c coordinate system and
168:50 - the second element so this one is the Y
168:53 - so from y AIS on our Cartesian
168:56 - coordinate system then the dot product
168:58 - and the length are highly related as
169:01 - usual so we have here V the vector and
169:05 - then again so dotproduct of vector v is
169:08 - equal to x² + y s so this is the length
169:13 - of this vector and it is equal to then
169:17 - the square
169:19 - of this length so uh let's actually plot
169:23 - this to see it in our coordinate system
169:25 - such that we can relate this back to the
169:27 - Pythagorean theorem and we can see how
169:30 - it is possible that the dot product the
169:33 - V * V which is equal to the square of
169:36 - the distance is actually equal to x² +
169:40 - Y2 so let's assume we have this vector v
169:44 - and this vector v is represented by this
169:46 - x and the Y so we are in the
169:52 - R2 so this is our xaxis this is our y
169:56 - AIS and in
170:00 - here our vector v can be
170:05 - represented and I'm just taking random
170:08 - some random point and I'm saying this is
170:10 - my X and this is my
170:15 - y then this is simply
170:19 - the vector v so this X and Y this can be
170:23 - random numbers I just want to keep it
170:25 - General and to make it very similar to
170:27 - the definition what we just saw so we
170:30 - are going to relate this back to the
170:33 - Pythagorean theorem and we are going to
170:36 - prove how Pythagorean theorem is related
170:39 - to this vector and that the dotproduct
170:41 - of this Vector so V * V is actually
170:47 - equal 2 to the square of the distance of
170:52 - this vector
170:54 - v and then on its own this is then equal
170:58 - 2 x² + Y 2 all right so the first thing
171:03 - that we can do is to check and use a
171:06 - Pythagorean theorem in order to prove
171:09 - that the length of this
171:12 - vector and uh so the distance from this
171:15 - point to this point is actually equal to
171:18 - x² + Y
171:21 - 2 and to be more specific square root of
171:24 - x² + y
171:26 - s
171:29 - so when we look in here one thing that
171:32 - we can see is that this
171:35 - side which is the side that we can have
171:39 - here in order to form our triangle so I
171:41 - want to take the beginning uh where my
171:44 - Vector starts and I want to see what is
171:46 - that that we have in here so this side
171:49 - given that this is zero and this is X
171:52 - this means that this entire distance is
171:54 - just
171:55 - X and of course the other way around
171:58 - holdes as well when we look at the
172:00 - horizontal side this
172:03 - length so this is the y coordinate of
172:06 - this point which is y so we see here
172:10 - that it is equal to this part so we see
172:13 - that we are
172:14 - dealing with um triangle
172:22 - where so this is our vector
172:25 - v and we can see that this this side is
172:30 - X and then this side is
172:32 - y now we also know that this angle is
172:43 - 90° using the Pythagorean theorem which
172:46 - states that in this type of triangles
172:50 - when it is a right
172:52 - triangle and we have X and Y as the
172:56 - sides we can compute the opposite side
172:58 - so the distance or the length of this
173:03 - side which is let's call it so in this
173:07 - case we have already the vector so we
173:09 - can say that the length of this
173:14 - Vector this is how we were defining the
173:17 - length of the vector this is then the
173:21 - square of it according to the
173:23 - Pythagorean theorem is equal
173:27 - to x² + y
173:32 - s because the pagoria theorem says if
173:36 - this is a 90° angle here we have a here
173:39 - we have B then this side which is
173:41 - defined by C so c² is equal to a² + b²
173:47 - and here the C is basically our vector
173:49 - and we are seeing the length of this
173:51 - Vector so the length of this Vector is
173:55 - basically equal to C from here to make
173:59 - it easier to uh to compare the
174:03 - two so this means that using the
174:06 - Pythagorean theorem we can already find
174:08 - that the length of this Vector is equal
174:11 - to the square roof of the x² + y²
174:17 - knowing that the vector has those
174:18 - coordinates X and
174:20 - Y okay so now when we are clear on uh
174:23 - what the norm is let's
174:27 - actually look into what we have
174:30 - here we are seeing that the dotproduct
174:34 - v v is equal to d square of the length
174:41 - of it now we just show that the length
174:45 - of the V Vector is equal to the square
174:47 - root of x square + y sare this means
174:50 - that this amount if we remove this what
174:53 - we were wanted what we wanted to
174:56 - prove so this is equal to square root x²
175:01 - + y s and the square of it so we just
175:04 - want to prove by making use of all the
175:07 - geometric interpretation and the
175:09 - phenomenon that we already know
175:11 - including the Pagan theorem so this is
175:14 - then equal to square root with the
175:17 - square here it cancels out so this two
175:21 - and we simply end
175:23 - up with x² + y s so we already proved
175:32 - that the dot product that we defined
175:34 - like this and we are saying according to
175:36 - the definition of the dot product V Dot
175:39 - product of the V so the vector v is
175:41 - equal to the square of the distance or
175:46 - the length of the vector
175:49 - that this in this specific case if we
175:52 - are in the two dimensional space and our
175:54 - Vector is defined by this x and the Y
175:57 - then this dot product is simply equal to
176:00 - the x² + y^ 2 that's something that we
176:05 - just
176:06 - proved all right so let's now apply to
176:10 - an actual example so here we are seeing
176:13 - that the length of a vector is deeply
176:14 - related to dot products that's something
176:16 - that we just saw also when plotting it
176:19 - in our cartan coordinate system so let's
176:21 - now look into this specific example so V
176:24 - is equal to 34 so this is the xaxis this
176:26 - is the Y
176:28 - AIS and here the proof is that the
176:32 - dotproduct VV is equal to 3^ 2 + 4 2 is
176:37 - equal to 9 + 16 is = to 25 which means
176:41 - that the length is equal to square root
176:44 - of this dotproduct and it's equal to
176:46 - five now let's actually prove this this
176:48 - just to ensure that we are at the same
176:51 - page so we can actually go ahead and
176:55 - plot that in here so we have that the
176:58 - vector v is equal
177:03 - to three and
177:06 - four so a few things just to get
177:08 - straight before plotting
177:13 - this so we know that the dot product can
177:16 - be defined like this and we said that
177:18 - this is equal to the square of the
177:22 - length of this vector and we also prove
177:26 - that this is also equal to the x² + y s
177:30 - if we are in the
177:33 - two-dimensional cartisian coordinate
177:35 - system so here this is our X and this
177:39 - our y
177:41 - so our X is equal to 3 and our Y is
177:44 - equal to 4 let's actually solve this
177:47 - problem and find it also geometrically
177:49 - to prove what is the dotproduct and what
177:51 - is the length of this Vector so the
177:55 - vector v is 34 which means that it is 1
177:59 - 2 three so this is three and this is
178:01 - four so we are
178:04 - here and this is our vector
178:07 - v so this also means that one of the
178:12 - sides of our
178:14 - triangle with 90° angle
178:20 - so our
178:24 - X so this our X and then X is equal to
178:31 - Tri so X is equal to
178:35 - Tri and our
178:39 - Y is equal to 4 so it is this one this
178:43 - is our
178:45 - y so now we want to
178:49 - find what is this length so we want to
178:54 - find what is the length of this vector
178:59 - v so the length of vector
179:02 - v is then
179:06 - equal square root
179:09 - of so it is equal to x² + y s according
179:15 - to Pythagorean theorem
179:19 - and then this is equal to just filling
179:21 - in the
179:28 - numbers 4 S is
179:31 - 16 and then this is 9 + 16 is simply
179:37 - equal to 25 so this equal to
179:41 - 5 so in this way we can see that a we
179:46 - are proving that the the length of this
179:49 - vector v is equal to 5 if the sides are
179:53 - three and four which means that the
179:55 - vector has this coordinates 34 and at
179:58 - the same time so in this way we already
180:00 - see a few things so we see
180:04 - that the length of the vector v is equal
180:09 - to 5 that's something that we already
180:13 - proved and we also saw that the dot
180:17 - product
180:20 - is equal to the square of the
180:23 - length which means that it's equal to 5^
180:26 - S which means that it's equal to
180:31 - 25 so and that's exactly what we see
180:33 - here it says that the do product of V is
180:37 - equal to 25 and the length of that
180:40 - Vector is equal to 5 so the dot product
180:44 - of the two n vectors A and B is defined
180:47 - as
180:49 - ATB which is equal to A1 B1 plus A2 B2
180:52 - up to a n BN so this is basically the
180:56 - generalization of the definition that we
180:59 - just saw before so we go from the um
181:03 - from the description of working with the
181:06 - vector v and the distance to the more
181:09 - General one when we are now dealing with
181:12 - n dimensional vectors so now we are no
181:16 - longer in a basic two dimensional space
181:18 - but we are in an IND dimensional space
181:21 - and we have these two different vectors
181:23 - A and B think of a as this Vector A1 A2
181:29 - up to a n and then think of this Vector
181:33 - b as B is equal to B1 B2 up to
181:40 - BN and the dotproduct of this two we are
181:44 - writing as
181:46 - a and then
181:48 - this small letter T which means
181:52 - transpose and we will see something very
181:54 - soon and we will also be discussing the
181:56 - topic of transpose later in the next
181:59 - module two multiplied by
182:04 - B so this ATB or a transpose B is um bit
182:11 - more advanced way of writing down this
182:14 - idea of dot
182:15 - product and the dot product of these two
182:18 - n vectors A and B is defined as this
182:23 - formula so we are basically cross
182:26 - multiplying the two we are taking this
182:29 - element from a and we are multiplying it
182:32 - with this one then we are taking this
182:34 - element of a and multiplying with this
182:36 - element of B so we are basically taking
182:39 - all the corresponding elements from
182:42 - these two different vectors and we are
182:45 - multiplying them together and creating
182:47 - this uh combination and adding them up
182:50 - to create this single value to represent
182:53 - these two vectors with the single value
182:56 - which we call Dot product so this
182:58 - notation
183:00 - ATB um which is a common way of noting
183:05 - and denoting this dot product this is
183:08 - just one way of describing this dot
183:10 - product there are also people who will I
183:12 - write down this so you see that this uh
183:15 - triangle type of braces and then the
183:17 - first vector and then the second one and
183:19 - then this one so you will notice one
183:21 - thing that will be uh quite different in
183:25 - here versus in this
183:27 - definition because in here we were
183:30 - discussing the uh dotproduct of a vector
183:33 - so we were talking about exactly the
183:35 - same Vector so here we have V and V and
183:39 - we were just saying let's take the dot
183:41 - product of a vector with
183:43 - itself and this was related to this idea
183:45 - of length when we go from one vector to
183:49 - this idea of multiple vectors so now we
183:51 - are creating no longer the do product of
183:53 - single Vector but we are defining the
183:55 - dot product of two vectors A and B the
183:59 - definition changes as you will notice
184:01 - but that's important to differentiate
184:03 - the two and to understand that one is
184:06 - actually U not the other one and to um
184:09 - to be able to see the difference between
184:12 - the two and not to confuse them so uh in
184:15 - the upcoming presentations we will use
184:17 - this notation of ATB much more often
184:19 - than this uh this second
184:23 - notation but if you see this this uh you
184:26 - can just know that we are talking about
184:27 - the same thing which is that the dot
184:29 - product so when the N is equal to one
184:32 - then the then this inner product or the
184:34 - dot product simplifies to the
184:36 - multiplication of just two numbers
184:38 - because we no longer have vectors but we
184:40 - just got a single value for a and a
184:43 - single value of B so let's assume that
184:45 - the N is equal to 1 and then a is equal
184:48 - to 2 and then B is equal to 3 and then
184:52 - the dot product is simply the a TB uh
184:56 - which is equal to 2 * 3 which is just 6
185:00 - so uh when it comes to this uh idea of
185:03 - transpose I will explain this in a bit
185:06 - and we are going to uh describe this in
185:09 - detail as part of the next unit so I
185:11 - won't go too much into it but it's
185:14 - always a great idea to to know um also
185:18 - for this module um at high level this
185:21 - idea of transpose this T is basically
185:24 - referring to this so
185:33 - transpose so um to understand this idea
185:36 - of dot product uh the dot product is an
185:39 - operation that takes these two vectors
185:41 - vectors A and vectors B and returns a
185:45 - single number a scaler
185:48 - and the single number is uh computed by
185:51 - calculating the sum of all these
185:54 - individual values so it takes all these
185:57 - elements from one vector then the other
185:59 - one so A1 and B1 A2 B2 a and BN takes
186:04 - the value from one vector with the
186:07 - corresponding one from the other Vector
186:09 - multiplies them to each other and then
186:11 - add them all up and then this single
186:13 - value is created which will refer as the
186:15 - dot product
186:18 - so geometrically it represents this
186:20 - product of the vectors magnitudes and
186:23 - this cosine of the angle between them so
186:28 - um the we already know from high school
186:31 - and also from Tri gometry and geometry
186:33 - this idea of angles and what is the
186:35 - cosine of an angle so I won't go too
186:39 - much into into the detail of it I will
186:41 - assume that you know what a cosine is so
186:44 - the dot product is used to determine the
186:46 - angle between the two vectors and also
186:48 - to check whether they are orthogonal of
186:50 - not because uh we have a property that
186:54 - relates to dot product directly to the
186:57 - orthogonality of the two vectors and
187:01 - knowing how to calculate the dotproduct
187:03 - of the two vectors will help us to check
187:05 - whether the two vectors are orthogonal
187:08 - or not and just in general understanding
187:10 - this concept of dot product is super
187:12 - important because we use dot product in
187:14 - machine learning we use do product in uh
187:17 - generally in AI you will see that
187:19 - product appearing also when you when we
187:21 - are talking about attention mechanisms
187:23 - or calculating this uh attention scores
187:26 - as part of multi-head attention in
187:28 - Transformers We also use this idea of
187:31 - dot product scale do product so it's
187:34 - really fundamental and essential to
187:36 - understand this concept of dot product
187:38 - for metrix multiplication but also in
187:41 - general to apply linear algebra in
187:43 - different science related domains
187:48 - so let's now look into a specific
187:49 - example to um understand this concept
187:53 - the dotproduct of these two uh vectors
187:56 - in this case we have three dimensional
187:58 - vectors A and B the dot product can be
188:01 - calculated as follows so you can see
188:03 - that we are taking the one this is the
188:06 - A1 and we are multiplying it with
188:11 - B1 so it is the this one so B1 B1 is
188:15 - equal to 4 A1 is equal to 1 so it is
188:19 - this
188:19 - one we multiply the two and then we go
188:23 - to the next sum so here I will write it
188:25 - down actually underneath to make it
188:28 - easier so this is A1 this is A2 this is
188:32 - A3 as you can see and this is B1 B2 and
188:37 - B3 so taking
188:41 - A1 so taking A1 B1 adding A2 B2 two and
188:48 - adding A3
188:51 - B3 we are getting the dot product which
188:56 - is equal to 4 - 6 + 5 which is equal to
189:00 - 3 so if you do the calculation or you
189:02 - use a calculator you will find out that
189:04 - the dot product that can be described by
189:07 - this formula which is very similar to
189:09 - what we have seen in here only n is
189:12 - equal to
189:13 - 3 we are getting that the dot product of
189:16 - the vectors A and B in R3 is equal to 3
189:21 - so let's now look into another example
189:23 - where we will do all the calculations
189:25 - manually step by step so we have these
189:28 - two different vectors we have Vector a
189:30 - and Vector B let me also hear the
189:33 - arrows
189:35 - and I want to calculate the um dot
189:39 - product
189:40 - a
189:44 - b so the ATB
189:48 - the dotproduct
189:52 - ATB is equal to and we know that if we
189:56 - are in the N three dimensional space so
189:59 - we are in the R3 we expect that we will
190:02 - need to just use the cross product three
190:04 - times and we need to add them up so we
190:06 - know that it's equal to
190:08 - A1 let me use the black color so A1 B1 +
190:14 - A2 B2 + A3 B
190:17 - B3 now what is A1 what is A2 and what is
190:21 - A3 and what is B1 B2 B3 so A1 is equal
190:25 - to -1 B1 is equal to
190:29 - 0 A2 is = to 2 B2 is equal to
190:35 - 1 and A3 is equal to
190:39 - 2 this is three and then B3 is equal to
190:43 - - 3 now what is this amount -1 * is 0 2
190:48 - * 1 is 2 and then 2 * - 3 is - 6 and
190:54 - what is 0 + 2 - 6 is
190:58 - -4 so let's Now understand what is this
191:00 - application of without product and how
191:02 - it can be used so the dot product
191:04 - between these two vectors gives us an
191:07 - idea uh a way to measure their
191:10 - similarity so it combines the magnitude
191:13 - the length of the vectors and the cosine
191:16 - of the angle between between them in the
191:18 - simple terms it tells us how much one
191:21 - vector extends in the direction of the
191:23 - other one now we know how we can
191:25 - calculate it once we have the vectors we
191:28 - we understand the notation we also know
191:31 - this relationship between the um vector
191:35 - and the uh distance what we mean when we
191:38 - have um Norm when we have a distance we
191:42 - also know the the length of a vector I
191:46 - want to combine all this and uh combine
191:49 - this with this idea of cosine which is
191:52 - something that we learn in our high
191:53 - school as part of our CH genetry and
191:55 - geometry I want to relate this all to
191:58 - make a sense how geometrically we can
192:02 - interpret deety of dot product and it
192:04 - will all uh make much more sense so the
192:08 - dot product between the two vectors
192:11 - gives us a measure of their similarity
192:14 - this is something that we are also using
192:15 - a lot in the field of artificial
192:17 - intelligence when we want to compare
192:20 - compare two different uh for instance
192:22 - users so we have one user and we want to
192:24 - compare the uh this one user to the
192:26 - other user then we are using this cosine
192:29 - Ru we are using what we call cosign
192:31 - similarity as way to measure their
192:34 - similarity so how similar those two
192:37 - customers are how similar those two
192:39 - users are how similar those two
192:42 - companies are and we are doing that by
192:44 - using this simple cosine idea which also
192:48 - is related to this idea of norms this
192:51 - idea of distance so or the length of a
192:54 - vector and the relationship between two
192:56 - vectors so let's actually wrap this up
192:59 - together and um clearly uh indicate what
193:03 - is this relationship between the dot
193:06 - product the length of vector cosine rule
193:09 - and this cosine
193:11 - similarity so in a simpler terms this uh
193:15 - dotproduct um it tells us how much one
193:19 - vector it extends in the direction of
193:21 - the other one so uh when vectors point
193:24 - in the same direction then we know that
193:26 - the dot product is positive and largest
193:28 - it means that we are dealing with two
193:30 - vectors that are very similar but when
193:33 - the vectors are perpendicular then the
193:35 - dot product is zero and we say that
193:38 - those two vectors do not share any
193:40 - Direction they are not similar so when
193:42 - the vectors point in the opposite
193:44 - direction then the dot product is
193:45 - negative we are saying those two users
193:47 - are really um negatively correlated so
193:51 - one is the opposite of the other one so
193:54 - to say so let's actually plot this and
193:58 - see it um and bring them all together so
194:02 - uh let's say we have a vector s and we
194:05 - have a vector
194:08 - R and we want to see how similar the two
194:13 - are so we have here our Vector s
194:19 - and here we have our Vector
194:21 - R and those two vectors come together
194:25 - and they um create this angle and we are
194:29 - referring this angle by angle Theta this
194:31 - is just a way to refer to our angle
194:33 - those are all things that we learned as
194:35 - part of high school through a gometry so
194:37 - they should all seem very familiar so uh
194:41 - from the geometry we know that if we
194:44 - have this triangle
194:47 - and here we got this
194:52 - um angle which is Theta and here we got
194:56 - uh let's say side C and then here a and
194:59 - then the B we know that the c^ 2 is
195:02 - equal to a 2 + b 2 - 2 * a * B * cine of
195:10 - the opposite angle of the C which is
195:13 - Theta now let's actually go ahead and
195:16 - apply is to our case when we instead of
195:19 - having just a triangle we got a vectors
195:22 - so we got a set of three vectors that
195:25 - form this triangle and we already have
195:27 - seen in many examples previously one way
195:31 - of doing that and that's actually go
195:33 - ahead and transform this Vector uh this
195:36 - uh into vectors and we can do that if we
195:40 - plot in here this side then this so we
195:43 - basically recreate the same triangle
195:45 - only this time the side are just vectors
195:48 - so this is let's say um our Vector R
195:52 - this is our Vector s and then this is
195:57 - obviously the r minus
196:00 - s and the opposite side is Alpha so the
196:04 - vector R and Vector s they form the
196:07 - angle Alpha so when we apply to the
196:11 - cosine rule to this of course we instead
196:14 - of using Vector notation we need to use
196:16 - their distances because the cosine rule
196:19 - is based on the sides so the length of
196:21 - the sides so the length of the uh of
196:24 - this
196:25 - side this one we denote by our common
196:29 - notation of the norm which is simply
196:33 - this
196:37 - thing and then the same holds for this
196:39 - side which is
196:43 - simply the norm or vector s
196:47 - and then for the third
196:50 - side for this one we then
196:54 - have Norm of R so if we apply this
196:58 - cosine low to our specific example we
197:01 - can see that in our case our C square is
197:04 - simply our the length of our Vector r- s
197:09 - only squared which means
197:12 - that so let let me actually write it
197:15 - down for Simplicity that part so a is
197:19 - then in this case my Vector
197:22 - R and then the distance of it obviously
197:25 - so the length of the uh Vector R and
197:28 - then B is the length of vector s and
197:32 - then the C is simply equal to the length
197:37 - of r- s then here I will just change
197:43 - my angle and I will call it t
197:47 - done and let's now apply the cosine law
197:52 - to our actual example which means that
197:54 - we got r - s and then squ is equal to
198:01 - and then we got R
198:04 - squ and then
198:08 - plus s squ so the length of the vector S
198:12 - and S S minus 2 * R * X s time cosine of
198:21 - theta so one thing that we can recognize
198:23 - here very quickly is that here we are
198:25 - dealing with the S squ of a length of a
198:29 - vector Rus s and we have learned from
198:32 - the previous definition of a dotproduct
198:35 - of a vector with itself so the
198:37 - dotproduct of vector v is equal to the
198:40 - square of a distance so this means that
198:43 - here we already got the squar of a
198:46 - distance of the vector v so this is
198:48 - something that we have because we have R
198:52 - minus s and then squ this is exactly the
198:56 - same as the V vector v and a squ with
199:00 - its length so the length of vector v squ
199:03 - is the same as the length of the vector
199:05 - R minus S 2 because in this case the V
199:07 - is equal to so the vector v is equal to
199:10 - Vector r- s and uh from this we can see
199:14 - that this means that here we are dealing
199:18 - with the dotproduct of R minus s so let
199:22 - me actually write this down in here in
199:25 - our example so this is basically the
199:29 - right hand side of the formula that we
199:30 - saw before where it was saying that
199:33 - vector v do product of vector v so this
199:36 - is equal 2 and then vector v the
199:39 - distance of or the length of it and then
199:42 - squared so in this case the vector v is
199:46 - = to r - s this means knowing this we
199:53 - can actually make use all that formula
199:55 - and say that here we are dealing with
199:58 - the dotproduct of Rus s which means that
200:02 - we can write
200:04 - that this can be re Rewritten as R minus
200:09 - s and then times r- s so in the left
200:15 - hand side we will be stting working with
200:17 - the dot product with the vectors while
200:19 - in the right hand side we will still
200:20 - keep our
200:22 - lengths so here we have the length of
200:25 - the r Vector so R 2 + S
200:30 - 2 - 2 * R and then s and then times
200:36 - cosine of
200:37 - theta now let's actually open those
200:41 - parentheses so this gives us R and then
200:44 - R basically the same do product of
200:47 - vector R and then minus
200:52 - Sr so dotproduct of vector S and R and
200:56 - then minus
200:59 - s actually plus s and then s and then
201:06 - minus s * R so I'm basically opening the
201:11 - parenthesis of the uh above part this
201:14 - left hand side to simp ified and then in
201:18 - the right hand side I will just copy
201:19 - paste this so r n 2 + S and then s and
201:26 - then Min - 2 * R forgot here
201:32 - arrows and then s and then cosine of
201:38 - theta let me actually go ahead and
201:41 - remove all these parts to open up some
201:44 - space for us
201:48 - and there we
201:54 - go right so let's actually go ahead and
201:57 - simplify what we got here now what do we
202:00 - see here we see
202:02 - here that here we got one dotproduct and
202:05 - we know that the dotproduct of a vector
202:07 - r r is simply the distance of that
202:10 - vector and then squared minus here we
202:14 - got so we will just keep it like that we
202:17 - are taking the dotproduct of vector s
202:20 - with Vector R we are adding here here we
202:24 - again got a DOT product of a vector with
202:27 - itself which is equal to the the the
202:30 - length of that Vector s only the square
202:33 - of it using exactly the same lad that we
202:35 - just saw the dot product of a vector and
202:38 - then here minus and I will keep the dot
202:40 - product of these two vectors S and
202:43 - R
202:44 - together so
202:47 - this is then equal to and I we just take
202:49 - over this part exactly as it was so R
202:52 - and then squ plus s and then squ and
202:56 - then minus 2 * R and then s and then
203:02 - cosine of
203:09 - theta now uh we already see couple of
203:12 - things that we can cancel out which is
203:14 - great so we see here that we are dealing
203:16 - with um the length of a vector R and A
203:19 - squar something that we see here too so
203:22 - we can safely cancel those out another
203:25 - thing that we can see that we can cancel
203:27 - is this so again the length of the
203:30 - vector SN squ so this also go away and
203:34 - we end up with this much more simpler
203:36 - for formula so we got minus s and then R
203:41 - and then minus s n r
203:47 - and then this is equal to minus 2 and
203:52 - then
203:52 - R and then
203:56 - s then cine of
203:59 - theta
204:01 - now we already see that this we can
204:05 - combine we can say this equal to minus 2
204:09 - times and then the order doesn't matter
204:11 - of course in the dotproduct so R and
204:15 - then s
204:18 - and then this is equal to minus 2 * and
204:21 - then the
204:22 - distance or the length of the vector R
204:26 - and then the length of a vector s and
204:28 - then times cosine
204:31 - Theta okay perfect now we can get rid of
204:33 - minus 2 to so let me clear some space up
204:37 - to see what we end up
204:42 - with so we are left with the for
204:45 - following formula
204:46 - so we see that R and then s so the dot
204:50 - product of these two vectors is equal to
204:55 - R so the length of the vector R
204:58 - multiplied with the length of vector s
205:01 - multiplied by
205:04 - cosine and then
205:07 - Theta so this is basically what we end
205:09 - up with and exactly what we wanted to
205:12 - prove we wanted to see that the dot
205:14 - product so this is the DOT
205:17 - product dot
205:21 - product of vectors R and S is equal to
205:25 - the length of the vector R multiplied
205:28 - with the length of a vector s multiplied
205:31 - by the cosine of this Vector that they
205:34 - are forming so cosine of
205:40 - theta now why is this important this
205:43 - means that we can also take this cosine
205:45 - bring it to the other side and we can
205:47 - use that to describe the cosine with
205:51 - this uh dot product as well as the
205:54 - length of these two um uh vectors so
205:58 - from here we can say that the cosine of
206:02 - theta is equal to the dotproduct of the
206:05 - vector R and
206:07 - S divided to the length of vector R
206:12 - multiplied with the length of vector s
206:17 - so using this we can then calculate the
206:22 - cosine similarity between two different
206:25 - vectors and this then helps us to
206:28 - mathematically compute what is this
206:30 - measure of similarity between two
206:33 - different entities whether do we want to
206:35 - compute similarity between two customers
206:37 - whether do we want to compute the
206:38 - similarity between two users similarity
206:41 - between two uh profiles of uh clients so
206:46 - so this can be applied really to do very
206:49 - large range of applications but the idea
206:53 - is that in this way we have proven that
206:55 - we can use this algebraic concept the
206:59 - linear algebra concept of the dot
207:01 - product in order to compute a similarity
207:04 - measure that will help you to compute
207:06 - the similarity between two different
207:08 - entities and this is a very common
207:11 - application in machine learning in
207:12 - artificial intelligence uh the coite
207:15 - similarities one of the most popular uh
207:18 - similarity measures that is used in data
207:20 - science in machine learning in
207:22 - artificial intelligence whether it's in
207:24 - the clustering algorithms like K means
207:26 - whether it's in different sorts of
207:27 - machine learning algorithms or just in
207:30 - general to compute the similarity
207:32 - between uh two uh reviews that customer
207:35 - leave similarities between two profiles
207:37 - of customer similarity of two different
207:40 - um uh users for instance two users that
207:43 - are watching movie and you are building
207:45 - a movie Commander system you can use COI
207:48 - similarity to compare the two users
207:51 - behavior in order to improve the quality
207:54 - of the movie
207:56 - recommender and the applications are so
207:59 - R so versatile that um my takeway uh
208:05 - would be to just uh keep this cosign
208:07 - similarity in mind the formula of it and
208:10 - how it is related to the dot product
208:12 - because uh this is something that will
208:14 - be very useful uh for for your uh
208:16 - journey in applied sciences let's now
208:20 - talk about the idea of in product and
208:22 - how it relates to the idea of dot
208:24 - product so by definition the inner
208:27 - product generalizes the dot product to
208:30 - more abstract Vector spaces which can
208:33 - include spaces with complex numbers so
208:36 - it retains these Key Properties like
208:38 - linearity you know linear Independence
208:41 - and notion of angle and the length that
208:43 - we saw before the norm and how we
208:45 - related back to cosign similarity for
208:48 - much more complex vectors the inner
208:51 - product involves uh basically
208:53 - conjugating one of the vectors and in
208:56 - real Vector spaces the inner product is
208:59 - simply the dot product so if we have
209:02 - vectors that are from uh real spaces so
209:05 - they are from R2 R3 then in this case
209:08 - the inner product is exactly the same as
209:11 - the dot
209:14 - product so let's now move move on
209:16 - towards uh one of our final uh topics as
209:20 - part of this unit and this is a very
209:23 - important topic um very often referred
209:26 - as couchi SS
209:28 - inequality so this inequality and the
209:31 - law states that for all vectors X and Y
209:34 - in an inner product space or dot product
209:37 - space the absolute value of their inner
209:41 - product is less than or equal to the
209:43 - product of their Norms mathematic Al we
209:46 - can express this as the following
209:49 - expression so where this part it simply
209:52 - represents this dot product of X and Y
209:57 - and here as we already saw before the X
210:01 - and with this specific notation it
210:03 - refers to the length or the norm of our
210:07 - Vector X and then this one the norm of a
210:11 - vector
210:12 - y so this definition should and this um
210:17 - and this way of referencing the norm and
210:19 - the length as well as the dot product
210:21 - should seem very familiar to you so what
210:24 - we're saying here is basically if we
210:27 - have a vector X and the Y and we have a
210:31 - DOT product of X and
210:33 - Y if we take the uh
210:38 - magnitude then this will be smaller than
210:41 - equal the length of the vector X and the
210:45 - length of vector
210:47 - y so the intuition behind this low is
210:52 - that it tells us that the absolute value
210:54 - of this dot product between two vectors
210:57 - cannot exceed the product of their
210:59 - lengths so basically if we have a vector
211:05 - a and Vector
211:07 - B and we are Computing the dot product
211:10 - between them so
211:12 - ATB then this amount
211:16 - will always be smaller than equal than
211:18 - the length of vector a multiplied by the
211:22 - length of vector B this is all what this
211:25 - law is about oh this inequality it tells
211:28 - us that the absolute value of the dot
211:31 - product between two vectors cannot
211:33 - exceed the product of their lengths so
211:38 - let's actually look into and understand
211:40 - why does this even make sense so this
211:43 - idea this inequality C squ inequality it
211:47 - is about the limits of similarity so
211:49 - what is this level the maximum level
211:52 - that we can have when it comes to the
211:53 - similarity of two entities for instance
211:56 - if we got two um users and we want to
212:00 - understand how similar those users are
212:03 - this inequality helps us to understand
212:05 - what is this maximum level that we can
212:07 - have when it comes to the similarity of
212:09 - these two people what is this maximum
212:11 - similarity measure that we can have
212:13 - based on the characteristics of these
212:15 - users
212:16 - so no matter how similar the two vectors
212:19 - are there is an upper bound to this
212:22 - measure based on what kind of vectors we
212:23 - are talking about and this uh maximum
212:26 - amount this maximum measure is defined
212:29 - by their lengths so it confirms that
212:32 - this uh cosine of an angle that we just
212:35 - saw because we saw that the cosine Theta
212:40 - is equal
212:41 - 2 is exactly what we got in here
212:47 - so we saw that cosine of theta is equal
212:50 - to the dotproduct of vector R and S
212:53 - divided to the length of
212:56 - them and that's exactly what this cou
212:59 - squares is basically saying it's saying
213:02 - that it confirms that this cosine of the
213:04 - angle between them between these two
213:06 - vectors part of this dot product
213:08 - calculation it can not exceed one which
213:11 - feeds to our understanding of the cosine
213:14 - because we know that cosine is a
213:18 - volume that should be between minus one
213:21 - and one to be more specific minus one
213:23 - and one
213:24 - included and knowing this this helps us
213:28 - to get a limit on this idea so how can
213:32 - we prove the cou sares INE equality this
213:34 - comes down to this formula that we have
213:36 - already proven so let us actually go
213:39 - ahead and prove this couch squares
213:41 - inequality quite straightforward so we
213:43 - have just learned and we have actually
213:46 - proven that cosine Theta is equal to R
213:50 - and then s if those are our two vectors
213:53 - and then here we got the
213:56 - length of the vector R and then the
214:00 - length of a vector
214:01 - s now we know that a cosine is a measure
214:05 - that will always be between Min -1 and
214:10 - 1 so this means that we can use in order
214:14 - to prove
214:23 - this so knowing that this volume will
214:27 - always be smaller than equal to one It
214:30 - means that we can just take this and we
214:32 - can bring it here we know that the
214:35 - lengths are both positive amounts so it
214:38 - means that I can just multiply both
214:40 - sides by this
214:42 - amount that means that I will be left
214:45 - with R and then s so the dot product
214:48 - between the two vectors that will be
214:50 - smaller than equal and then
214:53 - R the length of this Vector multiply by
214:56 - the length of the vector
214:59 - s now as you can
215:03 - see this is exactly the C
215:07 - squares inequality that it says because
215:11 - it says that the absolute value
215:17 - of the inner product is less than or
215:20 - equal to the product of their
215:27 - Norms so the cou SES inequality is a
215:31 - fundamental principle in mathematics but
215:33 - also just in general it's used in so
215:35 - many applied sciences and it's applied
215:38 - across various domains including linear
215:40 - algebra physics but also machine
215:42 - learning and in finance so this helps us
215:45 - to establish a limit on the correlation
215:49 - or this idea of similarity that can
215:51 - exist between two vectors in any inner
215:55 - product space so if we got two different
215:57 - companies and we want to understand how
215:59 - similar they are performing we by
216:02 - knowing their performances and their
216:04 - measures we can uh then have a limit on
216:06 - how similar the two companies can be or
216:09 - how similar the two users can be based
216:11 - on their previous history and this all
216:14 - is being done by using this idea of C
216:16 - squares
216:18 - inequality now why does this matter so
216:21 - understanding this inequality gives us
216:23 - deeper insights into the vector spaces
216:26 - especially when working with this high
216:28 - dimension data which is quite uh often
216:31 - the case when we are dealing with
216:32 - Transformers with large language models
216:34 - with uh deep neural networks or um we
216:38 - are working in finance so this is a
216:41 - super uh important concept when it comes
216:43 - to applied sciences so it ensures that
216:47 - this um validity of operations when it
216:50 - comes to mathematical side and the
216:52 - Transformations we perform on our data
216:55 - on the Practical side that those two are
216:58 - coherent and we ensure that we are safe
217:00 - in making different assumptions and we
217:02 - provide in this way the safety net for
217:05 - um our design of the algorithm we ensure
217:08 - that our algorithm is performing well so
217:11 - every time when we are uh creating a
217:13 - program or an algorithm for a specific
217:15 - case then using this inequality we can
217:18 - basically add a use cases we can create
217:21 - a test cases and we can add these
217:23 - constraints to our algorithm and we can
217:26 - design our algorithm in more intelligent
217:28 - way and we can ensure that we won't have
217:31 - cases when the uh limit goes above uh
217:35 - this threshold that we already got based
217:37 - on this inequality and this is a very
217:39 - important application in a practical
217:42 - sense when it comes to applying layer
217:44 - algebra to applied
217:46 - sciences now uh the final question that
217:49 - I will leave you with is the case when
217:52 - the norm is equal to zero so we briefly
217:56 - tached upon this point and we spoke
217:59 - about this idea of
218:00 - perpendicularity and in a one specific
218:03 - case when the norm of a vector v equals
218:06 - zero so the theorem
218:09 - says that the norm of a vector b equals
218:12 - z if and only if B is the zero Vector so
218:17 - mathematically this means that the
218:19 - distance of the vector v is equal to
218:22 - zero so basically we are dealing with a
218:25 - zero Vector so V is equal to Zer this
218:28 - means that the only Vector with no
218:29 - magnitude pointing in no direction is a
218:32 - zero Vector only in that case we will
218:35 - get a norm equal to Z this concept is
218:38 - super important to understand this
218:40 - uniqueness of a vector and this
218:42 - different foundational principles and
218:44 - properties that will also see appearing
218:46 - in the upcoming lectures and in the
218:48 - upcoming lessons this is all for this
218:51 - module and for this unit uh we have
218:53 - learned ton of uh important stuff when
218:55 - it comes to the vectors so we have
218:57 - looked into the uh from very scratch the
219:00 - idea of vectors the Norms the distances
219:04 - the inner product the dot product we
219:06 - have looked into the geometric
219:08 - interpretation of it we have visualized
219:10 - it we have looked into different
219:11 - properties of vectors the spaces we have
219:14 - also looked into the properties of the
219:16 - dot product we have proven them we have
219:18 - looked into ton of examples we have
219:21 - understood how we can plot vectors how
219:23 - we can understand and relate the IDE of
219:26 - vectors to this dot product and the dot
219:28 - product to the cosine of two vectors and
219:31 - the cosine of an angle and the
219:33 - relationship between two vectors and the
219:35 - similarity between these vectors how we
219:38 - can use and apply that in machine
219:40 - learning in artificial intelligence we
219:43 - have kind of made those relationships
219:44 - there
219:46 - and um we have also looked into
219:49 - fundamental concepts including the um uh
219:52 - scalar Vector multiplications operations
219:55 - with vectors adding them subtracting
219:57 - them and um we have also looked into
220:01 - this idea of linear combination dispan
220:03 - of vectors as well as linear
220:05 - Independence we have defined linear
220:07 - Independence and we have also spoken
220:10 - about uh and provided examples for
220:13 - vectors that can be uh marked and
220:15 - defined as linearly independent and also
220:19 - examples when our vectors were linearly
220:22 - dependent thanks for uh staying with me
220:26 - uh so far and I will see you in the next
220:29 - unit welcome to the module one of this
220:31 - new unit when we are going to talk about
220:34 - about matrices as well as linear systems
220:37 - so those are all fundamental concepts
220:39 - that you will see time and time again
220:40 - when applying linear algebra not only in
220:43 - mathematics but also in applies Sciences
220:45 - like data science artificial
220:48 - intelligence when training different
220:50 - machine learning models and trying to
220:51 - see what is this mathematics behind
220:53 - machine learning models different
220:55 - optimization techniques when you want to
220:57 - solve different problems using linear
221:01 - algebra so in this first module as part
221:04 - of foundations of linear systems and
221:06 - matrices we're going to introduce this
221:08 - concept of linear systems and then we
221:11 - are going to talk about the general
221:12 - linear systems we are going to uh see
221:15 - this common labeling all the
221:17 - coefficients this idea of indices that
221:19 - refer to the rows and the columns we're
221:21 - are going to see what is this
221:23 - differentiation between homogeneous and
221:25 - nonhomogeneous systems so without
221:28 - further Ado let's get started so U the
221:32 - linear systems form the uh bedro of
221:36 - linear algebra modeling this areay of
221:39 - problems thanks to this advancements in
221:42 - these linear systems and solving it in
221:44 - Computing we can now solve a large
221:47 - amount of problems in a very efficient
221:49 - and a fast
221:52 - way so uh the general linear systems can
221:55 - be represented by this uh set of M
221:58 - equations with n
222:01 - unknowns in the previous unit when we
222:03 - were looking into this uh linear
222:06 - combination of vectors we saw this
222:09 - notation which was A1 and then we had
222:15 - C1 multiplied or rather let me keep me
222:18 - uh let me keep the same notation so we
222:21 - had this linear combination of vectors
222:23 - so we had beta 1 and then we had A1 Plus
222:29 - beta 2 and then A2 and those are all
222:33 - vectors plus A3 so bet 3 * A3 dot dot
222:38 - dot and then beta
222:42 - M time a M this is the notation that we
222:47 - saw before and we said we want to come
222:50 - up we wanted to come up with the linear
222:52 - combination of these different vectors
222:54 - A1 A2 A3 up to a and then we use that in
222:58 - order to get a sense of whether we are
223:00 - dealing with linearly independent
223:02 - variables vectors or linearly dependent
223:05 - vectors and then we also commented on
223:07 - this pen that this vectors
223:10 - take now when it comes to um the uh
223:14 - vectors and just in general linear
223:16 - systems we can represent what we had
223:19 - before now in terms of with uh bigger
223:22 - system so in terms of M equations and
223:26 - with n unknowns so here what you can see
223:29 - here is that we have M different
223:33 - equations so we have beta B1 B2 up to
223:39 - BM so you can see it in here and then
223:43 - each of this equation it contains n
223:46 - unknowns so you can see that the
223:48 - unknowns it stays the same so the
223:52 - unknowns are those X1 X2 up to xn so X1
223:57 - X2 up to xn are the set of
224:02 - all n
224:07 - unknowns and then M equations that you
224:10 - can see in here are all these equations
224:13 - so a11 X1 plus a12 X2 dot dot dot and
224:17 - then a1n and then xn is equal to B1 and
224:22 - here one thing that is really important
224:24 - to keep in mind is that the indexing is
224:29 - what we need to focus on so we need to
224:33 - keep this one in mind this a i
224:36 - j and this
224:38 - XI so this is something that we also
224:42 - spoke about when uh discussing the lar
224:45 - combination of vectors we slightly uh
224:49 - touched upon on this topic so let's now
224:52 - dive into this this indexing and how do
224:54 - we indexes a i j what are these A's what
224:59 - are these JS and here you can see that
225:02 - we have a11 and then a12 and then up to
225:07 - the a1n and this is in our equation one
225:12 - and then we have in our equation two
225:16 - A1
225:17 - two I may actually write this with a
225:19 - different color so in our equation two
225:22 - we got a 21 a23 up to
225:27 - a2n
225:28 - and this a that you see here those are
225:32 - just real numbers so a11 can be 1 A1 2
225:36 - can be three A1 n can be 100 and then
225:40 - the same also holds for this B1 for this
225:43 - B2 and for this BN
225:45 - and all these values A's and B's they
225:48 - are just real numbers the only unknowns
225:51 - that we got here are those so the X1 X2
225:55 - up to
226:05 - xn all right so what about the indexing
226:08 - now so we got a i j and as you can see
226:15 - in this
226:17 - case the first thing that we can see
226:20 - here it stays everywhere the same which
226:21 - is the one so we got here one we got
226:25 - here one and up to the point we got here
226:27 - one whereas the second
226:29 - Index this one it does change it grows
226:33 - gradually with
226:36 - one and it becomes it goes from one to
226:39 - two and up to n so you can see here that
226:44 - the first
226:50 - index first
226:53 - index or index
226:57 - I it
227:00 - goes from one it doesn't change it's
227:03 - just one so it is one one and one so
227:07 - here in all cases for this equation I is
227:10 - equal to 1 but another thing that you
227:13 - can notice here is is that the index 2
227:17 - unlike index I so the second index which
227:20 - is the J so you see here that the second
227:23 - index is referred as J this is a general
227:26 - way of defining the indexes so here J is
227:31 - equal to one 2 dot dot dot and then n so
227:39 - basically the I doesn't change in the
227:43 - same row but the G
227:46 - changes and then of course we have
227:49 - slightly different in terms of I but
227:51 - then the same for J for our second
227:53 - equation so here I is equal to 2 and
227:56 - then J is again equal to 1 and then two
228:01 - dot dot dot and then
228:03 - n and then here up
228:06 - to for the last equation our I is equal
228:10 - to M and then our J
228:15 - is again equal to one till 2 dot dot
228:19 - dot so you might notice that I was
228:22 - looking at this from the row perspective
228:25 - so I was saying pair equation or pair
228:30 - Row the I doesn't change but then the J
228:36 - stays the same and then it is I one two
228:39 - up to n but the set is the same so it is
228:42 - it contains all these different elements
228:44 - here so one one two and then n but it
228:48 - contains all these different real
228:50 - numbers going from one till n because we
228:52 - are combining and we are creating this
228:55 - combination the sum of all these values
228:58 - a11 and then X1 a12 X2 A1 n
229:03 - xn and another thing that you can also
229:06 - notice here is
229:08 - that here with the
229:12 - second index so with this J J is equal
229:17 - to one then here the X's corresponding
229:20 - index is also one when the J is equal to
229:24 - 2 then the ex's corresponding index is
229:27 - also two and then here the same story
229:30 - and you will notice that while the
229:32 - coefficient contains two indices 1 one
229:36 - one two or 1 n which are the two indices
229:39 - for the coefficients for the unknowns we
229:42 - got just single index which goes from
229:46 - one till n so basically for A's for the
229:51 - coefficients so
229:54 - I let me write with the right
229:57 - color so I can be one 2 all the way to
230:04 - M whereas in case of
230:10 - J it can be one two all the way to
230:15 - n
230:17 - and
230:20 - the indices are basically used to help
230:23 - us to keep track of in which row we are
230:27 - and what is the um
230:32 - variable that the coefficient belongs to
230:35 - because knowing this second Index this
230:40 - helps us to understand that we are
230:41 - dealing with a coefficient that
230:43 - corresponds to this first
230:46 - unknown the first variable X1 and then
230:49 - the same holds in here as you can see in
230:53 - here and in here we are dealing with the
230:55 - same variable X1 therefore the second
230:59 - index the index J is then the same both
231:04 - in the first equation and in the second
231:05 - one in both cases it's equal to
231:09 - one okay so now when we are clear on
231:12 - that let's also
231:16 - understand this high level concept
231:18 - because you will see this system of
231:21 - linear systems this m equations and N
231:23 - unknowns appearing a lot not only in
231:26 - terms of calculating and finding the
231:28 - solution to this linear system but this
231:31 - actually has a very common application
231:34 - when it comes to um running regression
231:37 - linear regression
231:39 - specifically and one thing that you can
231:42 - notice here is that
231:45 - here we got also this B1 B2 up to BM and
231:49 - you will notice that here the index also
231:53 - uh goes from one but then this time to M
231:56 - so when it comes to the
231:59 - rows we have M rows or M
232:04 - equations therefore we also expect when
232:07 - it comes to Counting from the top that
232:09 - at the bottom we will see an M whereas
232:12 - if we count from this side
232:15 - so kind of like imagine it like a column
232:18 - then we see that it goes from one till
232:22 - n so those are common obervations and
232:25 - reference to um number of observations
232:29 - and number of uh features that you will
232:32 - see in your data when dealing with data
232:35 - analysis or modeling data so just this
232:38 - uh just keep those things in mind this
232:41 - uh abbrevation of M and then n m
232:43 - equation and unknowns because this will
232:46 - become very handy and the same also
232:48 - holds for this indexing just to keep in
232:50 - mind that this I and this J what those
232:54 - indices are and how for instance the
232:57 - first you know the I the first index
233:01 - changes when we go from up to the bottom
233:04 - and how the second index J goes and it
233:06 - changes when we go from left to the
233:09 - right when we go through the
233:10 - columns but we are going to see this
233:13 - also in the upcoming slides so uh we can
233:16 - we will have time to practice
233:19 - it so um this is what we are calling a
233:22 - coefficient labeling the coefficient uh
233:25 - a i j so this thing in a linear system
233:30 - they are labeled where the first index
233:32 - represents the row and the second index
233:36 - denotes the column so when we see a i j
233:41 - we know that this refers to the row and
233:46 - the J refers to the
233:49 - column so this is something that we use
233:53 - in order to understand where exactly in
233:55 - our Matrix something that we can we will
233:58 - see very soon where exactly our unit or
234:03 - our uh member that is part of our Matrix
234:07 - where exactly is that located in which
234:10 - row and in which
234:12 - column the systematic labeling is super
234:15 - important because this helps us to keep
234:17 - the structure and this helps us to
234:19 - understand uh what does this uh
234:22 - coefficients are present what what is
234:24 - this row that it belongs and what is the
234:27 - column it belongs so for which equation
234:30 - and for which unknown we have already
234:31 - solved the problem such that we can know
234:34 - what this uh coefficient
234:38 - represents so before moving on onto the
234:41 - actual linear systems and this
234:42 - definition of metrices let's quickly
234:45 - understand this distinction between
234:46 - homogeneous and non-homogeneous because
234:48 - this will help us to also get an
234:51 - understanding how we can solve a system
234:52 - of linear systems so a system is
234:56 - homogeneous if all the constant terms b
235:00 - i are zero otherwise it's non
235:03 - homogeneous so identifying this helps us
235:06 - to really understand nature of the
235:08 - solution set that we need to get and to
235:12 - understand what kind of strategy we need
235:14 - to to use in order to solve this problem
235:17 - now what do I mean by
235:19 - bi we just saw that we had this system
235:22 - of M equations with n unknowns and we
235:26 - saw that that we have in the right hand
235:28 - side this B1 B2 up to BM which means
235:32 - that we had this m different equations
235:35 - with n different unknowns and to find a
235:39 - solution to the system it means finding
235:42 - this values
235:46 - corresponding
235:51 - to X1 X1 here X2 X2 xn so basically
235:56 - finding the set of X1 X2 up to xn that
236:00 - solves this problem and for us to know
236:03 - how to solve this problem we need to
236:04 - know whether this B1 is equal to zero or
236:08 - not this B2 is equal to zero or not and
236:13 - then this BM is equal to zero or
236:16 - not this is very similar to this idea of
236:19 - solving any sorts of um problems that
236:22 - contain unknowns for instance if we have
236:26 - three
236:27 - x is equal
236:29 - to let's say five solving this is
236:34 - entirely different than if we know that
236:36 - the Tre X is equal to
236:39 - zero
236:41 - so this is a super simplified version of
236:44 - course but the idea is the same knowing
236:46 - that this B1 B2 up to BM this R zero
236:51 - this gives us an idea how we can solve
236:54 - this problem and later on we will see
236:56 - this distinction between non-homogeneous
236:57 - and homogeneous system and whenever this
237:01 - BS so whenever this B1 B2 up to BM
237:04 - whenever these BS are zero then we are
237:07 - saying that the system is homogeneous
237:10 - and we need to solve a homogeneous
237:11 - system otherwise we are dealing with non
237:13 - homog system so this means that the bis
237:17 - are not all zero let's now move on to
237:20 - the second module which is about the
237:22 - matrices so we are going to define the
237:24 - Matrix we are going to see the
237:26 - definition of it as well as the notation
237:29 - the IDE of rows columns
237:31 - Dimensions uh some of which we have
237:33 - already touched upon but we are going to
237:36 - uh go into the depth of it we're going
237:38 - to learn properly as well as we are
237:40 - going to see many examples then we are
237:43 - going to talk about Matrix types so here
237:45 - we will talk about identity Matrix
237:47 - diagonal matrices and also special type
237:50 - of matrices like matrices containing
237:52 - only zeros and only
237:55 - ones so by definition and a matrix is a
237:59 - rectangular array of real numbers that
238:02 - are arranged in rows and in columns for
238:07 - example an M by n Matrix a can be
238:11 - represented as follows so so let's look
238:15 - into this definition and this reference
238:18 - to Matrix we call this Matrix or
238:23 - Matrix
238:25 - a and every Matrix it can be described
238:29 - by this rows and columns where we always
238:34 - have this uh way of describing this
238:38 - Matrix always should be
238:40 - defined by the number of rows and number
238:47 - of
238:48 - columns so this is super
238:50 - important and let's look into this
238:53 - specific Matrix so we have a matrix a
238:56 - and all these values they are members of
238:59 - this Matrix they form the
239:02 - Matrix and we already saw this labeling
239:05 - of a i j where we said that I is
239:10 - referred to the row so you might recall
239:13 - that those all these equations that we
239:15 - got so this horizontal lines where I was
239:21 - equal to one I it was equal to two I was
239:24 - equal to three up to the point of I was
239:26 - equal to M and then we had this J so
239:32 - this thing and then J was referred to
239:37 - the
239:38 - columns and we had
239:42 - J was here here one and then two and
239:45 - then three up to the point of n so 1 2 3
239:51 - and
239:53 - N this is exactly what you can see here
239:56 - so in this Matrix we got all these
239:58 - elements a11 is a number a12 is a number
240:01 - up to the a1n is a number those are all
240:04 - real numbers and one thing that you can
240:07 - notice here is that here we got a11 so
240:11 - this is our first row and First Column
240:14 - here we got A1 two this is our first row
240:19 - and second column and then we got up to
240:23 - the point of a1n actually let me just
240:26 - write this down even at a bigger scale
240:29 - such that I can make more
240:34 - nose so let's assume we have this Matrix
240:40 - a and this Matrix a
240:48 - if I'm
240:50 - bigger and we got all these different
240:53 - elements so we start with our first row
240:57 - and here we have a 1 one so
241:02 - here the row that I will write with
241:07 - let's say
241:09 - we blue the row is equal to one and then
241:15 - the column is one so this is Row one
241:21 - this is Row one row one and this is
241:25 - column
241:27 - one let me write it wait right this is
241:32 - column
241:33 - one this is column
241:35 - two this is column three dot dot dot and
241:40 - this is column
241:42 - n and this this is row two this is Row
241:47 - three dot dot dot and this is row M so
241:51 - in total I got M rows and N columns I
241:58 - will come to this notation that I'm
242:01 - putting here later for now let's keep
242:03 - track of the rows and the columns to get
242:06 - a good understanding what this indices
242:08 - were about that we just
242:10 - learned so every time I will also
242:12 - mention this reference to a i j to keep
242:16 - track of this and also let me write it
242:19 - with the right colors so a i this is the
242:25 - row and
242:27 - J which is the
242:30 - column so all the elements I'm just
242:34 - defining by this a because it's just a
242:36 - way to reference a part that comes from
242:39 - a matrix it's a just common way to write
242:42 - the entire matrix by capital letter A
242:45 - whereas its members we will write with
242:47 - the um with the lower case
242:52 - a so this is
242:55 - Matrix
242:57 - Matrix
242:59 - a all right so here in the second row
243:04 - But First Column we got
243:07 - a two and then one because it is still
243:12 - in the first column and then when it
243:14 - comes to this
243:17 - element we have here
243:20 - a the row is the first one because we
243:23 - are in the first
243:24 - row but then we are in the second column
243:27 - so this one should be
243:30 - two then we go on to the next element in
243:33 - our first row so a
243:37 - one and then
243:40 - three and then dot dot dot
243:46 - the last element is an a as we are still
243:49 - in the first row it will be one the I
243:52 - but then given we are in the last column
243:55 - the column index of the J will be equal
243:58 - to
243:59 - n because we got in total in
244:04 - columns so we are now ready to go into
244:06 - the second row so here given that we
244:10 - already have our first element
244:14 - a21 this is in our second row and the
244:17 - First Column so the I is equal to here
244:20 - two and G is equal to 1 let's now write
244:23 - down the element in the second draw a
244:26 - second column as you might have already
244:28 - guessed I is equal to here one I equal
244:30 - to here two sorry and then uh the J is
244:34 - equal to
244:36 - 2 and then we go on to the next element
244:39 - which is in the second row and the third
244:41 - column so it's a the
244:45 - row index is two so I is equal to 2 and
244:50 - then the column index is three dot dot
244:53 - dot and then we
244:57 - got a as we are in the second row it is
245:02 - the I is equal to 2 and as we are in the
245:05 - last column the J is equal to n now you
245:09 - might have already guessed when I was
245:10 - writing this down that whenever you are
245:13 - in row and you move on to all the
245:15 - elements in the same
245:17 - Row the I so the row index it stays the
245:21 - same only you need to uh update the
245:25 - column index so here for instance you
245:27 - got one one one here also one so all the
245:30 - way down in the same row or one which
245:33 - logically makes sense because we are in
245:35 - the same row so the row index should not
245:38 - change but instead you should change the
245:40 - column index like here column one column
245:42 - two column Tre all the way to column n
245:45 - so those
245:46 - are our
245:50 - columns dot dotp so let me make this
245:56 - distinction and
245:59 - those are our
246:02 - row as you can
246:06 - see so this kind of mentally helps us to
246:09 - understand why we are writing all the
246:12 - indices over time once you practice more
246:14 - with this this will become more
246:22 - natural very quickly remove
246:25 - this so now I will write the rest very
246:28 - quickly so as you might have already
246:30 - guessed we are in the third row so we
246:33 - have a tree so everywhere I would just
246:36 - write down
246:39 - the ace so first I'll write down the ace
246:44 - and then the rows the row index will
246:47 - stay the same as I'm in the same row but
246:49 - then I will increase the columns
246:51 - gradually so we are in the column one
246:53 - and the column two column three up to
246:55 - the column n so
246:58 - now the remaining stuff you can actually
247:01 - write down yourself to just
247:04 - practice let's now move on onto the last
247:06 - row and last column so in the last row
247:10 - we got a a a up to here
247:14 - here and in the last row the uh row
247:18 - index is M which means that here I need
247:21 - to have M M M everywhere I need to have
247:25 - M and then the column index is 1 2 three
247:31 - all the way to
247:34 - n so this last column is very
247:37 - interesting too you can see here that we
247:40 - have the opposite of what we have here
247:44 - because in the last column we see that
247:48 - the uh column index is the same so it is
247:53 - everywhere n Only the first index the
247:55 - index of the row it changes it goes from
247:58 - 1 2 3 up to M which is of course logical
248:01 - because we said that in the last column
248:03 - if we are looking it from the
248:05 - perspective of column so all this values
248:08 - this A's so the all the ends they are
248:11 - logical because they we are in the last
248:13 - column we are in the same column but
248:15 - then the row changes here we are in the
248:17 - row one here we are in the row two Row
248:19 - three after to row M therefore we have
248:21 - also at the end a
248:24 - MN now let's talk about this idea of
248:29 - MN we said that our Matrix
248:32 - a
248:35 - has
248:37 - M as a number of
248:41 - rows and n as a number of
248:46 - columns which you can see by the way
248:48 - also
248:50 - here so we always refer the dimension of
248:55 - a
248:56 - matrix so the
249:00 - dimension dimension of Matrix a by these
249:05 - two
249:08 - numbers so first we always write down
249:11 - the number of row in this case
249:16 - M then as the second element we are
249:20 - writing the number of columns in this
249:23 - case n we are always putting this small
249:26 - X in between to kind of emphasize M by n
249:31 - Matrix and we most of the time use the
249:34 - square braces to Showcase that we are
249:37 - dealing with Dimension and in this case
249:39 - we are saying the dimension of Matrix a
249:42 - is equal to M byn so we are dealing with
249:46 - M byn Matrix this is a common convention
249:50 - used in linear algebra in mathematics
249:52 - General but also used in data science uh
249:55 - in machine learning artificial
249:57 - intelligence so whenever you are dealing
249:59 - with matrices a it is a common
250:02 - convention to talk about this idea of
250:04 - dimensions and the idea of Dimensions is
250:07 - super important when it comes to the
250:09 - idea of multiplication multiplying
250:12 - Vector with Matrix Matrix with Matrix so
250:15 - this dot product Dimensions play a
250:18 - central role in here so keep this one in
250:21 - mind once we uh get to the point of that
250:24 - products this one will become very handy
250:26 - so let's now look into a specific
250:28 - example where we see simple Matrix a so
250:31 - in this case you can see that we are
250:33 - dealing with a matrix that has a 2x3
250:35 - Dimensions so like we just learned
250:40 - 2x3 means that we got
250:47 - two
250:49 - rows and three
250:53 - columns that's something that you can
250:55 - also see here very quickly so you have a
250:58 - small Matrix on the small matrix it's
251:00 - really easy to actually count so you can
251:03 - see that we got Row one and row two and
251:06 - we got column one column two and column
251:09 - three so this basically confirms these
251:11 - Dimensions therefore we are also saying
251:14 - that we have a 2
251:17 - by three Matrix and like usual we first
251:22 - write down the number of rows and then
251:24 - the number of columns you can see here
251:27 - that here we have this elements for our
251:30 - Matrix so a is equal to 1 2 3 for the
251:33 - first row and then our 4 5 6 for the
251:36 - second row so from this actually I think
251:39 - it's a good exercise to just uh verify
251:41 - our understanding of IND
251:44 - and from this um we can write down that
251:46 - for instance all these different
251:49 - elements uh like a 1 one is equal to 1
251:53 - A1 2 which means that we are in the
251:56 - first row and in the second column so we
251:59 - have this element is equal
252:03 - to two and then we got a and then one
252:08 - three so we are in the third column so
252:11 - this one is equal
252:14 - 2 3 and then a 21 is equal to 4 a 22 is
252:22 - equal to 5 and then a 23 is equal to 6
252:28 - so this is actually a good way to
252:30 - practice our understanding of indices
252:32 - our understanding of this Matrix
252:34 - structure and the understanding of
252:36 - dimension of the Matrix which in this
252:38 - case is 2x3 so this is yet another
252:41 - different definition of a matrix
252:43 - structure when it comes to the rows
252:45 - columns and dimensions so this is
252:47 - exactly what we just spoke about on our
252:50 - example and let's just quickly look at
252:52 - the formal definition so the rows of a
252:55 - matrix are the horizontal lines of the
252:58 - of the entries while the comms are the
253:01 - vertical lines so basically it's saying
253:05 - those are let me remove
253:08 - this so the rows are the horizontal line
253:14 - and the columns are those vertical lines
253:17 - those are the columns this helps us to
253:19 - form these columns so col one colum two
253:22 - and colum three whereas this horizontal
253:24 - lines it helps us to create the rows so
253:26 - Row one and row two which is a core
253:29 - Matrix operations so when it comes to
253:32 - matrices we often perform Matrix
253:36 - additions Matrix substraction but also
253:39 - Matrix um a scalar multiplication of
253:41 - this Matrix so multiplying Matrix MX
253:43 - with a scalar and then Matrix um
253:46 - multiplication just in general so taking
253:49 - two matrices and multiplying them we are
253:52 - going to look into this concept in
253:54 - detail we are going to see many examples
253:56 - like before we are going to dive deeper
253:58 - into this such that we lay the ground on
254:01 - uh to the next module which is solving a
254:04 - system of M equations with an unknown so
254:08 - solving this General U linear
254:11 - system so for the beginning uh we will
254:14 - be looking into this Matrix operations
254:17 - where we are adding or subtracting
254:18 - matrices so by definition the sum of two
254:21 - matrices A and B of the same dimensions
254:25 - is obtained by adding their
254:26 - corresponding elements so by taking the
254:30 - element i j from both matrices and
254:34 - adding them to each other so in this
254:36 - case you can see that Matrix A and B are
254:39 - here and uh the uh definition says we
254:44 - just simply need to take the
254:45 - corresponding elements corresponding
254:47 - elements from the row I and the column J
254:51 - take them add them and this will become
254:54 - an element in our final um Matrix
254:59 - because when we are adding two matrices
255:02 - of the same size the result is yet
255:04 - another Matrix so we will use the Matrix
255:07 - a to add to Matrix B and this will give
255:11 - us a matrix a plus b and this i j simply
255:17 - refers to the indices corresponding to
255:19 - the row and the
255:21 - column we will look into an example in a
255:23 - bit and this will make much more sense
255:26 - and the same holds also for the
255:28 - difference so by definition the
255:29 - difference of the two matrices A and B
255:32 - of the same dimensions is obtained by
255:34 - subtracting their corresponding Elements
255:37 - which means that in order to obtain this
255:40 - Matrix a minus B this is a new Matrix we
255:44 - simply need to look for each element so
255:48 - we are going to index them for a row I
255:51 - and J we are going to do this pairwise
255:54 - element wise subtractions we are going
255:57 - to see what is that element
255:58 - corresponding to the row I and column G
256:01 - in The Matrix a which we say is a i j we
256:05 - are going to subtract from this the
256:08 - element in the row
256:10 - I and column G that comes from Matrix B
256:14 - and this will give us our new Matrix
256:16 - which is a minus
256:19 - B so let's now look into an example in
256:22 - this Matrix Matrix um uh a and Matrix B
256:26 - are used and Matrix a is of the size 3x3
256:29 - Matrix 3 Matrix B is of the
256:32 - size 3 by 3 in order to obtain a plus b
256:38 - what we are doing is that we are
256:39 - performing element wise additions now
256:43 - let's verify
256:45 - this so what we are doing here is that
256:48 - we are saying a plus
256:51 - b let me actually get a larger area
256:56 - here so let's say we have the two
256:58 - matrixes I want to add the two in such
257:01 - way that we do everything one by one
257:03 - such that the idea of a plus b and
257:06 - addition of the matrices will make
257:08 - sense so we want to find out a plus b
257:12 - for that what we are going to do is that
257:15 - we are going to make use of this
257:17 - definition that A + B and then I J is
257:21 - equal to a i j+ b i j which is a fancy
257:28 - way or mathematical way or describing
257:30 - that for each element we need to go a
257:32 - look for the row I and column J and take
257:35 - that element from the um column from
257:39 - that uh Matrix a and from The Matrix B
257:43 - so this means
257:45 - that for a + b this is going to be a
257:50 - matrix that will have the same number of
257:53 - rows and the same number of columns as
257:55 - two matrices because both A and B are
257:58 - 3x3 which means also their sum is going
258:01 - to be 3x3 so this going to be 3x3 and
258:05 - here we are going to do so we are going
258:07 - to take for the first row and the First
258:11 - Column so for
258:15 - a +
258:17 - b 1 one so first row and First Column we
258:21 - need to go to the first row and First
258:23 - Column of Matrix a and the first row and
258:26 - First Column of Matrix B and we need to
258:28 - add this two elements so we need to do 1
258:32 - + 1 and then we need to go on to the
258:35 - second column so the first row and the
258:38 - second column which means that we need
258:40 - to be here
258:42 - in both
258:44 - matrices so here we have 0 + 2 and then
258:49 - we got 2 + 3 and then we got 0 + 0 so
258:53 - you can see it in
258:54 - here and then we have 1 + 0 and then we
258:58 - have 3 + 1 0 + 1 and then 0 + 2 and then
259:04 - 1 + 3 which gives
259:11 - us so so 1 + 1 is = 2 0 + 2 is = 2 and
259:17 - then 2 + 3 is = 5 0 + 0 is = to 0 0 + 1
259:23 - is = 1 1 + 0 is = to 1 0 + 2 is = 2 and
259:28 - then 3 + 1 is equal to 4 1 + 3 is equal
259:31 - to 4 which means that our A + B is equal
259:35 - to this Matrix that we got in here so
259:39 - you can see that we are getting exactly
259:42 - what we uh what we have here only we
259:45 - have done it manually one by one so the
259:48 - same idea holds exactly when we have a
259:51 - minus B only instead of adding you will
259:54 - have to do here minuses so minus minus
259:57 - so everywhere minus so 1 - 1 0 - 2 2 - 3
260:05 - Etc so let's look into another addition
260:08 - so in this case by definition it is
260:11 - defined as this element wise uh of the
260:14 - adding of these two matrices here the
260:17 - only difference in this definition is
260:19 - that it's saying it's calling this a
260:21 - plus b as C so this new Matrix that we
260:25 - are getting as a result of adding a to B
260:28 - it's calling C so basically it's the
260:31 - same as calling this Matrix as C you
260:34 - will see also this type of definitions
260:36 - so in this case The Matrix C is equal to
260:39 - a plus b which basically means that for
260:41 - each row row with index I and with each
260:45 - column with index J go and look for row
260:49 - I and index J take the corresponding
260:51 - elements from Matrix a and Matrix B add
260:54 - them in order to get that corresponding
260:56 - element in our new Matrix C and you can
261:00 - see that in this example that's exactly
261:02 - what we are doing we have a we have B we
261:04 - are taking this element and this one so
261:07 - 1 + 1 we are getting here two and then 0
261:10 - + 2 we are getting two here 2 + 3 is 5
261:14 - and then 0 + 0 is = to 0 1 + 0 is = to 1
261:18 - and then 3 + 1 is = to
261:21 - 4 so now we already go to the next topic
261:24 - which is about scalar multiplication of
261:26 - a matrix so by definition scalar
261:29 - multiplication of a matrix a by scalar
261:32 - Alpha results in new Matrix where each
261:35 - entry of a is multiply by Alpha the idea
261:39 - of scalar multiplication matrices is
261:41 - actually quite similar to this idea of
261:44 - scaled multiplication in vectors so uh
261:47 - we have already seen in the lecture of
261:50 - the vector multiplication that when we
261:52 - were having this scaler C and we had
261:56 - this Vector
261:58 - a then uh when we are multiplying C
262:03 - which is a real number with Vector a
262:05 - then we simply need to take all the
262:08 - elements of vector a so A1 A2 or all the
262:12 - way down to a n and we need to multiply
262:15 - them by this same scaler so
262:19 - C this is what we were doing with
262:21 - vectors and that's exactly the idea
262:24 - behind matrices and when uh during the
262:27 - scalar multiplication of matrices only
262:30 - instead of multiplying only just one
262:33 - vector with this scaler C now we need to
262:37 - apply this to all the rows and all the
262:39 - columns so here we got this one column
262:41 - and Matrix is simply a combination of
262:44 - multiple vectors which means that we
262:46 - need to multiply all these elements of
262:49 - all the vectors of all the columns in
262:52 - this Matrix so let's actually look into
262:55 - a specific
262:57 - example so in this case we have a matrix
263:00 - a and this Matrix a is this thing and we
263:04 - have a scaler which is three so in here
263:07 - our Alpha is equal to three or you can
263:09 - call it C or anything so
263:13 - you can see that when we are scaling The
263:17 - Matrix with a scaler in this case Tre
263:20 - with this Matrix what we are doing is
263:23 - that we are simply taking each of these
263:24 - elements and multiplying it with the
263:26 - scaler so 1 by 3 is 3 2x 3 is 6 3x 3 is
263:31 - 9 and 4x 3 is
263:35 - 12 this is the idea behind this entire
263:39 - scalar multiplication of a matrix in
263:42 - more general terms if we for instance
263:45 - have a matrix a so let's actually look
263:48 - into a high level General example when
263:51 - we have the a matrix M by n so we got M
263:56 - rows and N columns and we want to get a
263:59 - scal multiplication of this Matrix and
264:03 - um scaler that we have here as in our
264:06 - definition it is defined by this alpha
264:09 - alpha is just a number you can go C you
264:11 - can quote B anything so in this case our
264:15 - scalar
264:17 - alpha alpha is coming from R so it's a
264:21 - real number so Alpha time a is then
264:27 - simply equal to to this new
264:30 - Matrix where all of these elements are
264:36 - simply multiplied by this scale so I
264:39 - would just take over all these values H1
264:43 - up to a M1 and then A1 2 a22 all the way
264:49 - down to a M2 and then let me also add
264:54 - the last column just for fun here a 2 N
264:59 - and then here a
265:05 - MN so here this new scaled mul
265:09 - multiplies so scaled uh Matrix a so
265:14 - Alpha * a is simply equal to Alpha time
265:17 - all these elements are simply multiplied
265:20 - by this
265:21 - scale it is as simple as
265:30 - that so that's the simple idea behind uh
265:34 - metrix uh scaling so when you are doing
265:37 - scalar multiplication of this Matrix you
265:40 - simply take all the values
265:42 - and you multiply them element by element
265:45 - per row and per column by that single
265:49 - scaler Alpha do note that you are
265:51 - multiplying them all without exclusion
265:54 - with exactly the same number which is
265:56 - that
265:59 - Alpha so let's now look into the
266:02 - definition of matrix
266:04 - multiplication so here we are no longer
266:06 - multiplying a matrix with a scaler but
266:09 - we are multiplying Matrix with a matrix
266:11 - so the product of an M by n Matrix a and
266:15 - an N by P Matrix B results in an M by P
266:19 - Matrix C where each entry cig is
266:22 - computed as the dotproduct of the each
266:24 - Road of a and the J column of B now what
266:29 - does this mean firstly let's look and
266:32 - unpack this part of the definition so we
266:37 - got Matrix
266:39 - a that is M by by n and then we got
266:44 - Matrix B which is n by P what this means
266:49 - is that in this case Matrix a has M
266:54 - rows and N
266:57 - columns and Matrix B has n
267:01 - rows and P
267:05 - cups so this is then simply the
267:09 - dimension dimensions of the two matrices
267:15 - so then it's saying that by definition
267:18 - the product of these two matrices so the
267:21 - product of A and B the product of the
267:32 - two
267:37 - me is equal to to Matrix C
267:43 - and each entry cig
267:46 - J so C J is computed as the dotproduct
267:53 - of the E row of a and the J column of B
267:59 - now this part might seem bit difficult
268:03 - but once we look into the actual example
268:05 - and we illustrate this on our common
268:08 - high level General expressions of Matrix
268:10 - A and B and multiplic ation this will
268:13 - make much more sense for now before
268:16 - coming to this one I just wanted to
268:18 - refresh our memory on one thing I said
268:21 - before when discussing also this idea of
268:24 - improving uh this uh different
268:26 - properties of vectors that when we want
268:29 - to
268:30 - multiply a vector with a matrix or
268:33 - Matrix with Matrix or vector with a
268:35 - vector we need to ensure that from the
268:39 - first element the number of columns is
268:41 - equal to the number number of rows of
268:42 - the second element this is also very
268:45 - important for this specific case and
268:47 - just in general for matrix
268:49 - multiplication so you can notice here
268:52 - that the number of comms here is equal
268:55 - to the number of rows in here and the
268:58 - order is very important so in case of
269:02 - matrix multiplication the order is
269:04 - really important which means that if you
269:07 - have a matrix a and you want to multiply
269:10 - it with a matrix B
269:12 - then
269:13 - the number of
269:18 - columns of
269:21 - a should be equal to the number of
269:27 - rows of
269:29 - B otherwise you cannot multiply those
269:33 - two matrices with each other so in case
269:36 - you got a matrix a that doesn't have the
269:38 - same number of columns as the rows of
269:41 - number of the Matrix B then there are
269:44 - some alternative things that you can do
269:45 - including this idea of the transpose
269:47 - that we saw also doing when Computing
269:50 - the dot product between this Vector a
269:52 - and Vector B that's something that we
269:54 - also do in programming when we are
269:56 - dealing with this Matrix and we want to
269:58 - compute this relationship between two
270:01 - matrices but the number of columns of
270:04 - one of the first one is not equal to the
270:06 - number of rows of the second one we are
270:08 - simply uh manipulating these matrices or
270:11 - moving some data if that's not hurting
270:14 - our problem maybe uh flipping so
270:17 - transposing our Matrix or applying any
270:20 - other source of operation to it to
270:22 - ensure that the two matrices that we are
270:25 - multiplying with each other the first
270:27 - one's number of columns is equal to the
270:30 - second one's number of rows so that's
270:33 - just the low and that's something that
270:35 - you should follow if you want to
270:36 - multiply these two
270:38 - matrices all right so now let's move on
270:41 - onto the idea of multiplying and Dot
270:43 - product let's look into a specific
270:46 - example and this will um help us to
270:49 - understand this process
270:51 - better so before doing that I just want
270:54 - to quickly show you this general idea so
270:58 - if we have a matrix a that is M by n
271:03 - which means that it looks something like
271:05 - this like
271:06 - A1 1 a 2 1 up to the point of a m one
271:13 - and then here we got let's say a one 2 a
271:18 - 22 up to the point of a M2 and then at
271:23 - the end we got a MN and here we got a
271:28 - one
271:32 - n so let me also add this one 2 N and we
271:38 - got a matrix B this Matrix B is n by P
271:43 - so it has n rows and P columns so we are
271:48 - fine in terms of Dimension
271:50 - here and we got here
271:53 - b11 B21 up to the point of b m sorry BN
271:59 - in this case let's not confuse the
272:03 - letters so b n 1 B one 2 B 22 up to the
272:10 - point of B n two because n now is the
272:15 - number of rows for Matrix B unlike for
272:18 - the Matrix a up to b 1 p and here b 2 p
272:26 - and here up to the point of B and then n
272:29 - p this is the last element in order to
272:32 - perform um multiplication between these
272:35 - two matrices so to obtain a matrix C
272:39 - which is equal to a *
272:42 - B what we need to do is we simply need
272:47 - to take pair case so pair Row for the
272:50 - row I for
272:53 - instance we need to take this element so
272:56 - this row and we need to multiply it with
272:59 - this so we need to find a DOT product
273:01 - between this row and this column then we
273:05 - need to move on onto the next one and
273:08 - then for the second element we will then
273:11 - take
273:12 - this row and we will multiply it with
273:16 - this
273:18 - one so this is then something that we
273:21 - need to do in order to obtain these
273:24 - elements and you might have already
273:25 - noticed that we got this m by n and n by
273:29 - P so you might have already guessed what
273:32 - will be the dimension of the C if we got
273:35 - that the dimension of a is equal to M by
273:40 - n and the D dimension of B is equal
273:45 - 2 N by P then the
273:49 - results Matrix after M multiplying the
273:52 - two of Matrix c will be will be having a
273:57 - number of rows equal to this and the
274:00 - number of coms equal to this so This
274:02 - middle part basically disappears and the
274:05 - number of rows of the first Matrix will
274:08 - be then the number of rows of this
274:09 - result Matrix C and the number of
274:12 - columns or the second Matrix so Matrix B
274:14 - will then be our final number of columns
274:17 - so we will then have a matrix C that
274:19 - will have a
274:21 - dimension so
274:25 - Dimension so dimension of C will then be
274:29 - equal
274:31 - to M
274:35 - by P so we will have M rows and P
274:41 - columns
274:43 - so how we are going to compute
274:46 - this so for c i j which means row
274:53 - I and column J let's look into the
274:57 - definition of it it's saying CI J is
275:01 - computed as a dotproduct of the E row
275:05 - and the J column so each roow from a and
275:08 - Jade column of B what is the each Road
275:11 - of a the each Road of a is somewhere
275:14 - here so each Road of
275:17 - a it is uh d a and then
275:25 - I
275:28 - one then
275:31 - a and then I2 and then a and then I3 dot
275:36 - dot dot and then
275:38 - a i and then we got in total n color
275:42 - columns and and we always do the
275:46 - transpose right when Computing this um
275:48 - dot product so we then take the
275:51 - transpose so we take this row row I and
275:57 - we multiply it so we do the dot product
275:59 - between this one this is the a
276:03 - i and the
276:06 - B
276:08 - J this is column J it is somewhere
276:14 - here so it is B and then we got the
276:19 - first element which is one and then J
276:23 - and then b 2 J
276:26 - B 3j dot dot dot up to B and then in
276:31 - total we got n rows in B so n and then
276:37 - the J is the column so it's Tak is the
276:43 - same so this is then the dot product
276:47 - between
276:49 - e row that comes from Matrix a and the J
276:54 - column that comes from Matrix B so it's
276:56 - always like that actually so we always
276:59 - take row by row so we
277:04 - take this different so every time we
277:07 - take just a row and we multiply with the
277:12 - corresponding column and then we get the
277:15 - dot product between this row that comes
277:17 - from the first Matrix and then the
277:18 - column that comes from the second Matrix
277:20 - in that specific order in order to get
277:23 - our DOT product and that specific volume
277:25 - and what is this amount actually
277:29 - so when we calculate this dot product
277:32 - you can quickly see that we have a
277:37 - i1 multiplied by B1 J plus a I2
277:44 - multiplied by b 2 J and then dot dot dot
277:49 - a i n multiplied by b n
277:54 - g and this new Matrix c will then have
277:59 - all these elements so C11 c21 and then
278:04 - c31 dot dot dot and then
278:07 - see the
278:09 - last row as the number of rows of C is m
278:14 - c m see here M so C and then here it
278:19 - will be 1 2 C 22 C3 and then two up to
278:26 - the point of
278:28 - cm and then two and then here the last
278:31 - col will be c one and then p is the
278:35 - number of coms in C so C1 p and then C2
278:39 - p and then C3 P that do dot dot and then
278:45 - c m and then
278:50 - P
278:52 - okay so this is what we get this is our
278:55 - final Matrix C when multiplying Matrix a
279:00 - and Matrix B so let me clean this
279:07 - up C is to now if you want to find out
279:12 - what is C11 you can easily feel in this
279:15 - general formula that uh that we just
279:17 - calculated the I is equal to 1 and then
279:19 - J is equal to 1 and this will give you
279:22 - C11 by using this formula if you want to
279:25 - get the CM P then just fill in the I is
279:28 - equal to M and then G is equal to P in
279:31 - order to get this value CN P so you can
279:34 - already see the amount of calculations
279:36 - you need to do in order to get all these
279:38 - elements from these large matrices A and
279:41 - B
279:42 - let's actually look into a simple
279:43 - example to clarify this so we have a
279:46 - matrix a here and Matrix B here and we
279:49 - want to do a multiplication of the two
279:51 - and we have just learned how to do it
279:52 - let's actually do it one by one so we
279:54 - got a matrix a which is equal
279:59 - to 1 2 3 4 with Dimensions 2 by 2 then
280:05 - we got a matrix
280:07 - B which has values 2 Z and then then 1 2
280:14 - so it is 2x two and I want to find what
280:17 - is c that is equal to a * B and I know
280:21 - already by looking at these Dimensions
280:23 - that c is going to be equal to 2 by 2 so
280:28 - you might recall that I said that when
280:30 - looking at this final result the number
280:32 - of rows or the final um Matrix will be
280:36 - this so the number of rows of the
280:39 - initial Matrix a and then the number of
280:41 - columns of this final Vector c will be
280:43 - the number of vectors number of columns
280:47 - of this second Matrix B so two therefore
280:50 - I know already before even doing
280:53 - calculations that the uh product Matrix
280:56 - c equal to a * B is going to have a
280:58 - dimension 2x two let's actually do a
281:00 - calculation to check this so C is then
281:04 - equal to a * B and it's equal
281:06 - to 1 2 3 4 multiply by 21
281:14 - 2 okay
281:22 - so I expect to have four different
281:24 - elements here here here and here so to
281:29 - obtain the C11 so it is C11 in here what
281:34 - I need to do is that I need to look at
281:37 - the first row and the first column in
281:41 - here here so first row from a and the
281:43 - First Column of B and I'm doing the
281:46 - dotproduct which means 1 * 2 + 2 * 1 1 *
281:51 - 2 is 2 2 * 1 is 1 so here I'm getting 1
281:56 - * 2 + 2 * 1 which basically gives me 2 +
282:02 - 2 and that's equal to 4
282:12 - so here I'm just writing
282:18 - down 1 * 2 + 2 * 1 now when I want to
282:25 - get this value which is C1 two this
282:28 - means that I want to get the first row
282:30 - and the second column and that's exactly
282:33 - what I'm doing so I'm going back and I'm
282:36 - saying let's look at the first row but
282:38 - this time we will look at the second col
282:41 - from the from the Matrix B so 1 *
282:47 - 0 * 0 plus 2 * 2 and then I do the same
282:52 - only this time for the second row which
282:54 - means I'm picking this row and then this
282:58 - column
283:00 - so it
283:01 - is 3 * 2 + 4 * 1 and for the final
283:08 - element c22
283:13 - I'm taking the second row and the second
283:20 - column which gives me 3 *
283:24 - 0+ 4 *
283:27 - 2 now what does this giv
283:33 - me this gives me this 4x4 Matrix where 1
283:37 - * 2 + 2 * 1 is 4 1 * 0 + 2 * 2 is 4 3 *
283:44 - 2 + 4 is = to 6 + 4 which is
283:49 - 10 and then 3 * 0 + 4 * 2 is =
283:53 - 8 so let's check 4 4108 that's is
283:58 - exactly what we have here so as you
284:01 - could see here the idea is that every
284:03 - time to follow what element I'm looking
284:06 - for for the c i j and then I just go to
284:09 - the each rows from the first Matrix and
284:13 - the J column from the second Matrix and
284:16 - I do the dot product of the A and then
284:21 - I and then K let's say so I'm going to
284:26 - the E Row from the first Matrix and I'm
284:29 - taking all the elements which means I
284:32 - don't even need to mention this index it
284:35 - just means the entire each row coming
284:38 - from the Matrix a and then I'm doing the
284:40 - DOT product between this row and the
284:45 - column that comes from the Matrix B
284:49 - which means B and then
284:52 - J which then will give me the c i so I'm
284:57 - looking at this and taking this
284:58 - multiplying this dot product and this
285:00 - gives me the first element then the
285:02 - first row and then the second column
285:04 - which gives me the uh second element in
285:06 - the first row in my Matrix so this one
285:09 - and so on so
285:11 - hope this makes sense uh if it doesn't
285:14 - make sure to reach out because it's a
285:16 - very important concept and uh let's also
285:19 - look into another example to make sure
285:21 - that we got this right so in this case
285:23 - as you can see we have another matrices
285:26 - so set of A and B matrices again 2x two
285:29 - a simple
285:30 - one and we want to know what is a so
285:34 - let's say we call the C we already know
285:36 - C should be 2 by two and what we are
285:39 - doing is basically for
285:42 - C11 we are saying let's look at the
285:46 - first row so first row and the First
285:50 - Column coming from the second Matrix B
285:54 - and let's do the dot product so 2 * 1 2
285:58 - * 1 2 * 1 4 * 5 4 * 5 we get this and
286:04 - then when we want to find what is C oh
286:09 - what is C and then one two so in the
286:12 - first row but in the second element in
286:14 - our final Matrix so I is equal to 1 and
286:17 - J is equal to 2 it means we need to look
286:20 - at the first row from The Matrix a but
286:25 - this time
286:26 - the second column from The Matrix B so
286:31 - it is 2 by 3 2 by 3 4 * 7 4 * 7 and this
286:38 - gives us a number 34 even if you
286:40 - calculate you can see that 2 * 1 is
286:42 - equal to 2 4 * 5 is 5 so 2 4 * 5 is 20
286:46 - so 2 + 20 is 22 in here and then you can
286:50 - do the rest of calculations and this
286:52 - will be a good practice to see how we
286:55 - can do a basic matrix multiplication the
286:58 - idea is actually quite straightforward
287:00 - when it comes to multiplying it it just
287:02 - it comes with a practice when we see all
287:05 - this uh much bigger
287:08 - matrices so um this is another example I
287:12 - will leave this one to you to complete
287:14 - it just uh to keep in mind we always do
287:19 - uh so we always look at the dimension
287:21 - first and here 2 * 2 and 2 * 2 which
287:24 - gives me an impression already what I
287:25 - can expect the result will be 2x two and
287:29 - when it comes to the uh cross elements
287:32 - just ensure to always look to the E row
287:36 - and the J
287:39 - column this comes Matrix a and this
287:41 - control Matrix B take them compute the
287:44 - dot product and then you will find your
287:47 - C uh your final result let's call it
287:52 - um k i j because in this case we have a
287:55 - matrix C already welcome to the module 4
287:58 - of this course when we are talking about
288:00 - matrices and linear systems so in this
288:03 - module we are going to dive deeper into
288:05 - this uh idea of linear systems with
288:07 - matrices and solving linear systems
288:10 - using different techniques and
288:12 - specifically we are going to learn the
288:15 - uh concept behind solving linear systems
288:17 - using matrices named gaussian
288:19 - elimination and gausian reduction
288:21 - welcome to the module 4 of this course
288:24 - when we are talking about matrices and
288:26 - linear systems so in this module we are
288:29 - going to dive deeper into this uh idea
288:31 - of linear systems with matrices and
288:33 - solving linear systems using different
288:35 - techniques and specifically we are going
288:38 - to learn the uh concept behind solving
288:41 - linear systems using matrices named
288:43 - gausian elimination and gausian
288:46 - reduction so uh we are going to solve
288:49 - linear systems um with M equations and N
288:53 - unknowns using this idea of our
288:55 - commented coefficient Matrix and then
288:57 - reduce row hon forms and we are going to
289:00 - see many examples we are going to solve
289:03 - these problems and we are going to do it
289:05 - step by step such that this very
289:07 - important concept from linear algebra
289:10 - will be uh entirely demystified for us
289:14 - so we are going to learn each of these
289:16 - Concepts one by one and we are going to
289:19 - uh solve different problems like before
289:22 - with examples such that at the end of
289:25 - this module we will be clear on how we
289:28 - can use this idea of gaussian
289:30 - elimination and gaan reduction in order
289:32 - to solve a linear system with
289:35 - matrices so uh solving linear systems is
289:39 - crucial in your algebra for many reasons
289:42 - it helps to um provide this fundamental
289:46 - method for understanding and working
289:48 - with linear equations which are the uh
289:52 - fundamental and backbone of various
289:55 - mathematical but also um the applied
289:59 - sciences like uh statistics data science
290:02 - machine learning deep learning and
290:04 - engineer artificial intelligence so uh
290:08 - here is a high level overview of its
290:10 - importance and applications so uh it's a
290:13 - basic but essential part of linear
290:15 - algebra it introduces key Concepts such
290:18 - as matrices vectors determinant so we
290:20 - need to know um and we need to
290:22 - incorporate all these different concepts
290:24 - that we already learned as part of
290:26 - previous modules in here because we need
290:28 - to know um the idea of matrices the
290:31 - dimension of the matrices uh this idea
290:34 - of coefficients versus unknown so
290:37 - variables in our moral homogeneous
290:39 - versus non homog continues how we do the
290:42 - indexing so coefficient labeling in our
290:44 - matrices that we learned before we also
290:47 - need to know this uh multiplication
290:50 - between a matrix and a vector
290:52 - multiplication between Matrix and Matrix
290:54 - the idea of transpose and all these
290:57 - different topics that we saw before uh
291:00 - coming to this specific module so the uh
291:04 - solving linear systems has a wide range
291:06 - of applications when it comes to the um
291:09 - usage of it Beyond linear algebra
291:12 - because it's being used also as part of
291:14 - many other linear algebra Concepts
291:16 - including the um dimensionality
291:19 - reduction techniques uh the composition
291:22 - techniques like um igon values ion
291:25 - vectors uh all these different
291:27 - mathematical and linear algebra concept
291:29 - itself they rely on this idea of solving
291:33 - linear systems with matrices but we also
291:35 - go beyond the application mathematics
291:38 - when it comes to solving linear systems
291:40 - it's used in physics it's used in
291:42 - engineering um and just in general also
291:46 - even in finance
291:47 - economics uh in different uh investment
291:50 - strategies when we are talking about
291:52 - quantitative finance and using data s
291:54 - linear algebra in order to understand
291:56 - what kind of investment decisions we can
291:58 - make so in order to set some uh
292:01 - constraints or objectives and in the
292:04 - essence solving linear systems is kind
292:06 - of this getaway to both understanding
292:09 - deeper mathematical theories but also
292:11 - applying these Concepts to solve real
292:13 - world problem across various disciplines
292:16 - and domains including data analytics
292:19 - data science machine learning deep
292:22 - learning artificial intelligence and
292:24 - much more so without further Ado let's
292:28 - get
292:28 - started so we will start with the
292:31 - refreshment of this General linear
292:33 - system that we saw when we were
292:34 - beginning this uh unit so we saw that we
292:37 - could represent a linear system with m
292:41 - equations and N unknowns with this
292:44 - format where we said that we had this um
292:48 - M equations so you can see we got in
292:51 - total M rows and we got n unknowns which
292:54 - were referring to the
292:56 - columns so then we also saw this idea of
293:00 - indexing a i j where I was the index
293:03 - corresponding to the row and then J was
293:06 - the index corresponding to the column
293:08 - and we said that these coefficient a11
293:11 - a12 up to A1 n and then the same holds
293:13 - for all the other equations or the rows
293:16 - this were ways for us to understand
293:18 - where exactly we are in our system what
293:21 - is that the variable corresponding to
293:23 - which we are dealing that Co coefficient
293:25 - and what that coefficient will basically
293:27 - tell us in which equation we are and
293:30 - corresponding to which variable or
293:32 - unknown we um have the coefficient
293:35 - because this this a11 holds entirely
293:39 - different information that is a23 and
293:42 - then this amn this amn tells us in the M
293:46 - equation what kind of information we
293:49 - know and the relationship between this
293:52 - when it comes to the end variable or end
293:55 - unknown so this a11 a12 all these AIG Js
293:59 - those are known because we are saying we
294:02 - can estimate them those are constants
294:04 - they come from
294:05 - R whereas the X1 X2 up to xn those are
294:09 - variables those are things that we
294:11 - assume that either we don't know or in
294:13 - the future we will see that those are
294:15 - basically the features that we got in
294:18 - our data and then this B1 and B2 up to
294:22 - BM so far we have been kind of ignoring
294:24 - them we already touched upon this when
294:26 - we were differentiating this idea of
294:28 - non-homogeneity and homogeneity of our
294:30 - system but in this specific module we
294:33 - are going to finally make this B in use
294:37 - we are going to make use of them and we
294:39 - are going to see how we can transform
294:42 - this General linear system into uh
294:46 - Concepts like matrices and vectors that
294:49 - we already have seen at the end of the
294:51 - previous module so we are going to bring
294:54 - all these different topics together to
294:56 - solve this General linear system and
294:59 - very uh soon it will also become much
295:01 - more clear why we want to do that
295:03 - because solving linear system it means
295:06 - that we are um getting an understanding
295:10 - uh what what is the exact relationship
295:12 - between these coefficients and the
295:15 - unknowns now keep this notation in mind
295:18 - where we have all these different
295:20 - equations you notice here the pluses so
295:23 - we have the common notation of equations
295:26 - that we know from high school and
295:28 - prealgebra because we are going to
295:30 - transform these linear systems into
295:34 - matrices and vectors and the products of
295:36 - them so we are going to go from this uh
295:39 - simpler notations to bit more fancy
295:42 - notation and we are going to bring our
295:44 - linear algebra into use in order to
295:47 - solve a system of M equations with n
295:51 - unknowns so let's go to that bit more
295:54 - advanced notation so here we go so this
295:58 - is the Matrix representation of linear
296:00 - systems and a linear system of equations
296:03 - can be represented compactly using these
296:06 - matrices so we already have seen this
296:09 - Matrix this is very familiar to us we
296:11 - have been using this a
296:13 - lot and this is the m by n Matrix a so
296:18 - this is the Matrix a you can note here
296:20 - we have a11 a12 up to a1n and then we uh
296:25 - we have all these different rows and the
296:27 - columns so another thing that you can
296:29 - quickly notice is that we are dealing
296:31 - with a similar dimension for Matrix a so
296:35 - we have M by n you can see we have M
296:38 - rows and we have n columns something
296:41 - that you can note when you are looking
296:43 - at the last column and the last row and
296:46 - then uh keeping in mind that we have
296:49 - always AI J where the a is the row index
296:52 - and then J is the column index so that
296:55 - gives us understanding what is the
296:57 - dimension of a which is M by n and then
297:01 - we have now a new component here which
297:04 - is something new here which is this
297:06 - Vector so we got a column Vector here X1
297:10 - X2 up to
297:11 - xn so we already see here from the last
297:14 - element that most likely we are dealing
297:18 - with an N by one vector and how I can
297:22 - know this and how I can be assured by
297:24 - this is because first of all the index
297:28 - is n and secondly we already have
297:31 - learned for us to be able to multiply a
297:34 - matrix with another Matrix or Matrix
297:36 - with another Vector the middle elements
297:39 - of the two so those things they need to
297:42 - be the same so we have learned that the
297:45 - number of columns of the
297:48 - first multiplier in this case The Matrix
297:51 - a needs to be equal to the number of
297:55 - rows of our second multiplier in this
297:57 - case the number of rows of a vector X so
298:01 - you can see it in here so here we got n
298:05 - columns as part of Matrix a here we got
298:08 - n rows as part of vector X which does
298:11 - make sense which means that if we are
298:13 - writing like this and we are multiplying
298:15 - the two then we are indeed dealing with
298:17 - the case when those two should be the
298:20 - same so we are definitely safe to assume
298:23 - that our Vector X it needs to have n
298:27 - rows and only one column so of course
298:32 - the one could have been different but we
298:34 - have already seen another thing before
298:37 - so why do we know that this is right we
298:40 - are indeed dealing with a single column
298:42 - Vector well if we go back in here we are
298:45 - simply taking this system and we are
298:47 - trying to rewrite it in this form and
298:49 - what have we seen here that we only got
298:52 - n different unknowns something that was
298:55 - also given which were X1 X2 up to xn
299:00 - which we can also rewrite in this format
299:03 - X1 X2 up to xn so basically a column
299:06 - Vector that's exactly what we have done
299:09 - here so we have taken all the x's and we
299:11 - have put it in the vector and we are
299:14 - saying let's multiply this Matrix with
299:17 - this com vector and we will get
299:19 - this and how we can know this well let's
299:24 - first look into this first
299:26 - part and then we will understand why we
299:29 - have here this um Vector of
299:34 - P let me clean this
299:37 - up well what we have done simply here is
299:42 - that we have Rewritten this
299:46 - part this
299:49 - part as a
299:51 - multiplication of a matrix and a vector
299:56 - so we have said let's take this we have
299:59 - already learned how we can perform the
300:01 - multiplication between a vector and a
300:03 - matrix we have seen that and how we can
300:05 - do that now we are basically doing the
300:08 - opposite so we have already the final
300:11 - version we have all these equations you
300:13 - can see that we have taken a11 we have
300:16 - multiplied with X1 A1 2 we have
300:18 - multiplied with X2 and A1 n we have
300:20 - multiplied with xn so basically what we
300:23 - have done here is that we have taken the
300:27 - a11 a12 and then a13 dot dot dot and
300:32 - then A1 n we said let's take this and
300:36 - then multiply with X1 X2 dot dot dot xn
300:40 - which is basically the dot product of
300:43 - this row Vector with this column Vector
300:47 - so only the first equation can be
300:50 - Rewritten as a DOT product between this
300:53 - Vector that we see in here and this
300:56 - Vector all right perfect but you can say
300:59 - well we just got uh we not only got one
301:02 - equation but we got M equation this
301:05 - already gives us an idea what we have
301:07 - done in the left hand side what I just
301:10 - did for one equation we can also of
301:13 - course do for multiple equations so
301:17 - basically what we have done was that we
301:20 - have set for equation
301:22 - one or I can also say for I is equal to
301:26 - one we have taken a 1 one
301:31 - a12 dot dot dot a one n because we got n
301:37 - columns or n unknowns
301:40 - and we multiply this with X1 X2 dot dot
301:44 - dot
301:47 - xn and this dot product this is
301:49 - basically
301:51 - A1 vector and then T and then X if we
301:56 - call this
301:58 - X and then this if you open this up and
302:02 - if you calculate this you will see very
302:03 - quickly that you get a11 X1 plus a12 X2
302:09 - do do do A1 n xn this is simply the the
302:14 - proof how we can go from
302:17 - here to
302:20 - here now if we do if we show the same in
302:24 - the same way you can actually go ahead
302:27 - and prove for yourself
302:30 - that I is equal to 1 we simply
302:35 - have this Vector a11 a12 dot dot dot A1
302:41 - n
302:43 - multiplied by X1 X2 do do
302:52 - xn but we also have the same for I is
302:55 - equal 22 only with a
302:57 - different row Vector which is of course
303:01 - a21 a22 dot dot dot a 2 N and then
303:06 - dotproduct with X1 X2 do xent so
303:10 - basically the column Vector X doesn't
303:13 - change but we need to change the
303:16 - coefficient that comes because uh in
303:19 - each equation we have different set of
303:21 - coefficients corresponding to this X and
303:23 - not you can see that here coefficients
303:25 - are different their I or the row index
303:29 - is changing because here we have I is
303:31 - equal to 1 then I is equal to two dot
303:33 - dot do I is equal to 1 I is equal to m
303:36 - in the same way here we are also doing
303:38 - the same so here you can see we have
303:41 - exactly the same story only now a is
303:44 - equal to M for our last row which is a
303:47 - M1 and then a M2 dot dot dot and then a
303:53 - MN and then multiply so the do product
303:56 - with X Vector X2 dot dot dot
304:01 - xn given that for all these cases we got
304:07 - the same column Vector you can easily
304:09 - see see how we instead of writing down
304:14 - this equations as separate row vectors
304:19 - we have seen also the same doing
304:20 - previously and instead we will just
304:23 - create one coefficient Matrix so we can
304:26 - say instead of just doing this
304:28 - separately we're also safe to do it um
304:31 - in a combined way so therefore we are
304:35 - rewriting this all and we are saying we
304:37 - can rewrite all these equations now
304:41 - as
304:43 - a11 a12 dot dot dot
304:46 - a1n so as I had before so I'm taking
304:50 - just this
304:55 - part and I'm adding here my Comm mat com
304:59 - Vector that I had to multiply this um
305:03 - A1
305:05 - T X2 dot dot dot xn
305:10 - so basically I'm just taking this that
305:13 - we
305:14 - had and I'm writing over in here so this
305:18 - is my first
305:20 - row but then of course I can also add
305:23 - there is nothing that I'm changing if
305:25 - I'm adding similar story only this time
305:29 - I'm using this second row Vector a21 a22
305:34 - up to a2n because at the end of the day
305:37 - I'm doing similar multiplication with x
305:39 - in here too so a21 and then a22 dot dot
305:43 - dot a 2
305:47 - N and then of course the same holds for
305:50 - a M1 a M2 for my last row and then up to
305:55 - a
305:57 - MN so if you open this up you will
306:00 - quickly see that what you will get is
306:04 - that for the first equation I will get
306:06 - entirely
306:08 - separate amount so a11 X1 plus a12 X2
306:14 - dot dot dot plus A1 n xn so I'm Simply
306:18 - Computing the dot product between this
306:21 - row vector and this com Vector we have
306:23 - already seen how we can do a calculation
306:26 - like this this should seem very
306:29 - familiar now I'm only doing it at a
306:32 - scale so I'm doing I'm adding here also
306:34 - the second row so a21 a22 dot dot dot
306:38 - and then a
306:40 - 2
306:41 - N and then here of course I forgot to
306:44 - add the exists so a21 * X1 plus a22 * X2
306:51 - plus dot dot dot and then A2 n and then
306:55 - xn and then I'm doing this all the way
306:57 - down to the last row a M1 X1 plus a M2
307:03 - X2 plus dot dot dot and then a m n xn
307:09 - and then what I'm doing is that I'm
307:13 - saying this is equal to this you are
307:18 - free to check it yourself um you can see
307:21 - that once you write down even all the
307:23 - smaller elements in here you will see
307:25 - that you are getting exactly the same
307:27 - this is simply uh another way of writing
307:30 - that you will take all these different
307:34 - um vectors A1 t a 2 t so transpose dot
307:41 - dot dot and then you also represent your
307:43 - last row by a
307:47 - vector and then you take this which is
307:50 - basically M by n and then you do dot
307:54 - product with a vector X1 and X2 dot dot
307:58 - dot xn and why am I allowed to do so
308:01 - because I have M different equations so
308:04 - equation one equation two dot dot dot
308:06 - equation three those are the
308:07 - coefficients of those corresponding
308:09 - equations and those are the unknowns and
308:12 - given that the unknowns are exactly the
308:14 - same across all the equations so as you
308:16 - can see here the unknowns they don't
308:20 - change so this means that every time I
308:23 - need to multiply comput the dot product
308:27 - between a different coefficient so a11
308:30 - a12 and then it changes to a21 a23 dot
308:32 - dot a2n but the unknowns they do not
308:35 - change so the vector the unknown Vector
308:38 - that I need to to multiply them it
308:40 - doesn't change every time it's this so
308:43 - in here I have the same in here I have
308:45 - the same and in here I have the same con
308:48 - Vector this means that I can simply
308:51 - rewrite the set of dot products into a
308:55 - larger uh system where I have this
308:57 - coefficient Matrix which is a so and we
309:00 - are calling it coefficient Matrix we saw
309:02 - already before y because those are the
309:04 - coefficients corresponding to the
309:06 - unknowns we are distorting them in one
309:08 - place and then we are storing the
309:10 - unknowns X1 to xn in a con Vector
309:13 - because we got n
309:15 - unknowns and then the final piece of
309:18 - this puzzle is what we see in here in
309:21 - the right hand side after the equation
309:23 - sign of course I'm talking about the B
309:27 - so we got here B1 B2 up to BN you might
309:30 - have already guessed that we are uh
309:32 - dealing with um Vector not a matrix
309:35 - because we got M equations and M
309:39 - B which means that we can represent this
309:43 - as B1 B2 dot dot dot BM as a column
309:48 - Vector with M rows because we got here
309:52 - one row two row dot dot dot here M row
309:55 - and then by given that it's just a one
309:57 - column here one
310:00 - so basically we are saying for equation
310:05 - one a11 A2 a12 dot dot dot A1 and
310:09 - multiply it so dotproduct with X so X1
310:12 - X2 up 2 xn this should be equal to B1
310:16 - this is exactly what we
310:23 - have let me clean this up this is
310:26 - exactly what we have here we say a11 X1
310:29 - plus a12 X2 dot dot dot A1 n xn is equal
310:33 - to B1 that's what we have
310:36 - here because we are saying the dot
310:38 - product
310:39 - of this row Vector that we have just
310:42 - used to rewrite this
310:45 - thing that product of this uh
310:48 - coefficient with the corresponding
310:50 - unnown is equal to B1 and we have
310:52 - already seen that it is simply this
310:55 - thing that we are rewriting this first
310:58 - equation we have is equal to B1 as you
311:01 - can see in
311:03 - here therefore when we do exactly the
311:06 - same it's simply equal to here B2 for
311:09 - the second equation dot dot dot and then
311:11 - here is equal to b m so given that from
311:16 - the left hand side here we already have
311:18 - seen that this is simply equal to
311:20 - coefficient Matrix a and then dot
311:23 - product with X where X is this
311:27 - Vector it means that we can also take
311:30 - over this equal sign from here and the
311:33 - only thing that is left from us is to
311:37 - represent this column
311:40 - Vector in here which we can call a
311:44 - vector
311:51 - B and that's exactly what is in here so
311:55 - we soon we'll also see the clear
311:58 - definition of it but for now let's see
312:01 - how we uh we have used this linear
312:03 - system in order to transform it in the
312:06 - notation with matrices so we have taken
312:08 - this C eents and we have seen that in
312:12 - here we can represent these coefficients
312:14 - as a vector a row Vector so a11 a12 a1n
312:19 - multiplied with a common Vector X1 X2 up
312:22 - to xn so this is exactly the first
312:25 - equation the left hand side so we say
312:28 - that this is the first dot product this
312:30 - is the second dot product after to the
312:32 - last row the last dot product so we have
312:35 - already represented this dot products as
312:38 - um
312:39 - multiplication so product of a
312:42 - coefficient Matrix
312:45 - a
312:47 - by column Vector containing all these
312:50 - unknowns X1 X2 up to xn
312:53 - X and we said let's take over this equal
312:57 - sign and given that the same logic
313:01 - rowwise holds also in here so we say
313:05 - this is Row one this is row two dot dot
313:09 - dot
313:12 - row
313:14 - M and everything that holds here for the
313:17 - rows holds here as well we need to take
313:19 - over everything in a consistent way
313:22 - therefore we are saying let's also
313:24 - represent this in terms of the vector if
313:27 - we are doing that in
313:29 - here therefore we are calling this a
313:34 - b and this PS are just real numbers B1
313:37 - B2 up to BM
313:40 - and this is how we can transform and we
313:43 - can rewrite this system with M equations
313:47 - and N unnown into a
313:50 - product in terms of matrices or
313:53 - coefficient Matrix a a column Vector
313:56 - containing the n
313:58 - unowns and then a vector a column Vector
314:02 - B that contains all these different
314:04 - values B1 B2 up to
314:07 - BM so this basically rewriting what we
314:10 - have in
314:16 - here hope this makes sense but if it
314:18 - doesn't we are going to see this again
314:20 - in the definition I just wanted to warm
314:22 - up with this before we move on onto the
314:25 - uh official
314:29 - definition so a linear system of
314:32 - equations can be represented compactly
314:34 - using matrices and we we just saw that
314:37 - an M byn Matrix a multiplied by an N
314:41 - Vector X results in an M Vector B so you
314:44 - can see it in
314:46 - here so this is an M byn Matrix
314:50 - a this is
314:52 - the n by 1 column Vector
314:57 - X and this is the m by1 con Vector
315:05 - B and one thing that we can see very
315:07 - quickly is that what we are doing here
315:09 - is that we are transforming that uh
315:12 - equations into a matrix representation
315:14 - but nothing changes with the equations
315:16 - we can still write them down like that
315:19 - it's just by using matrices and vectors
315:21 - we will soon see how we can use this in
315:23 - order to solve this problem so why are
315:27 - we doing this uh when we have just two
315:31 - equations with two unknowns we know from
315:33 - high school that we can quickly solve
315:35 - that we can just write down for instance
315:37 - X1 + 2 X2 is equal to 3 and then 3 X1 +
315:42 - 4 X2 is 6 we can quickly solve this
315:46 - problem by uh you know writing down the
315:49 - X1 in terms of 3 and 2x2 filling that in
315:53 - in here in the X1 and then solving the
315:55 - problem because we got two equations
315:57 - with two unknowns the problem become
315:59 - much more complicated when we got three
316:02 - equations with three unknowns five
316:04 - equations with five unknowns 100
316:07 - equations with 100 unknowns
316:09 - therefore to do this uh solving of the
316:12 - system of equations at the scale we need
316:15 - this Matrix notation and that's exactly
316:18 - what we are doing in here so uh let's
316:22 - also quickly check the dimensions such
316:24 - that we confirm that we are uh on the
316:26 - right path before moving on to the uh
316:29 - practical material so here we got Matrix
316:32 - a containing the coefficients uh from
316:35 - our linear system and it is
316:40 - let me remove
316:42 - this it is M
316:46 - by
316:47 - n then we got our
316:51 - X so this thing is a convector
316:54 - containing the uh unknowns in our system
316:58 - and it contains n rows so the X has a
317:03 - dimension of n by 1 is a single col
317:09 - vector and we are saying this is equal
317:11 - to and that's just given that's how we
317:14 - transform our linear system into this
317:16 - notation this is equal to a Comm Vector
317:19 - B and the dimension of it is M by 1 now
317:25 - let's check whether this makes sense we
317:27 - know that already and we just said that
317:29 - this two needs to be equal it's fine so
317:31 - we have the uh this uh criteria
317:34 - satisfied so the number of columns of
317:37 - Matrix a coefficient
317:39 - Matrix a is equal to n and then the
317:41 - number of rows of con Vector x with
317:43 - unnown is equal to n so we can indeed do
317:46 - the multiplication so we are correct in
317:48 - that
317:49 - notation but uh is it true to claim that
317:53 - indeed here we got M different rows well
317:57 - we have learned that if we take M byn
318:01 - Matrix and multiply it by n by one
318:04 - vector or Matrix then what we are
318:06 - getting is that we first take over over
318:09 - this so the number of rows this will
318:13 - become the new number of rows of our
318:15 - final
318:16 - Matrix and then
318:18 - M by and then this last element so the
318:22 - number of columns of our last item so in
318:26 - this case DX so it's equal to 1 this is
318:30 - our final number of columns so the new
318:33 - dimension then becomes M by 1 and what
318:36 - do we see here that inde the the result
318:39 - that we say we claim that this amount is
318:42 - equal to is M by 1 because if we are
318:46 - saying that the two amounts are equal to
318:48 - each other then uh naturally they should
318:50 - have the same Dimension so a * X will
318:54 - have a dimension M by 1 which is
318:57 - coherent to what we are claiming that is
319:00 - equal to which is M by one uh column
319:03 - Vector B all right so now when we have
319:07 - um refreshed our memory on that
319:09 - Dimensions we are ready to move on onto
319:12 - the next slide so now we need to solve
319:16 - this linear system so we need to find
319:18 - this set of unknowns that will help us
319:20 - to identify and clarify all the unknowns
319:22 - in our system so one way and the INF
319:25 - famous way of solving this linear system
319:27 - is what we are referring as gausian
319:30 - elimination gausian elimination is the
319:32 - systematic method for solving linear
319:34 - systems it basically transforms this
319:37 - system into what are referring us row
319:40 - Ulin form from which the solutions can
319:43 - be more easily
319:45 - identified so in the next couple of
319:47 - slides and uh lessons we are going to
319:50 - learn this idea of elimination we will
319:53 - understand why is it actually called
319:55 - elimination which is quite intuitive on
319:57 - once we see the
319:59 - examples and also we are going to learn
320:01 - this idea of row H form reduce row form
320:05 - because those are very important
320:07 - Concepts to ENT this entire idea of
320:10 - solving linear systems using linear
320:13 - algebra so before moving on onto that we
320:16 - need to know the basics one of which is
320:19 - the idea of argumented coefficient
320:21 - Matrix so by definition the argumented
320:24 - coefficient Matrix of linear systems a
320:26 - XB uh combines the coefficient Matrix a
320:30 - and the vector B into one
320:32 - Matrix so you can see here the Matrix a
320:36 - and then we have here the straight line
320:38 - and then B this is basically this entire
320:42 - argumented coefficient matrix it's a
320:44 - fancy way of saying let's take all the
320:47 - unknowns from our system which are the
320:49 - coefficient in our system and then the
320:52 - constant uh in the B so I have to say
320:55 - not nouns but rather uh the uh values so
320:59 - the scalers coefficients and let's take
321:02 - the coefficients um in our system which
321:05 - are represented in our coefficient
321:07 - Matrix and let's combine this with the
321:11 - information that we got in our system
321:13 - which is the B1 B2 up to BM and let's
321:17 - combine the two and we call it argument
321:19 - coefficient Matrix so we are simply
321:22 - taking the coefficient Matrix a you can
321:25 - see a11 a12 up to a1n and then also in
321:28 - the last row am1 am2 amn this should
321:31 - look very familiar because it's just
321:33 - simply this Matrix Matrix a
321:45 - we are taking it over in here and then
321:48 - we are saying let's draw a straight
321:50 - line and then add here the uh column
321:54 - Vector our result Vector which is B so
321:58 - we saw in here that is this
322:00 - thing so let's take this part in this
322:03 - part put a straight line in between and
322:06 - this will become our new Matrix matx so
322:11 - as you might have already guessed in the
322:14 - a in The Matrix a in the coefficient we
322:16 - got M
322:20 - rows and N
322:23 - columns which means that our new Matrix
322:26 - will have at least M rows and at least n
322:30 - columns now given that we are adding yet
322:33 - another column at the end it means that
322:36 - the number of columns will increase
322:39 - but one thing that is important to note
322:41 - is that the number of rows will not
322:43 - increase because both the coefficient
322:45 - Matrix and the coln vector B they have
322:48 - the same amount of rows they both got uh
322:51 - just m rows so let me actually rewrite
322:54 - this so we know that a is M by n so it
323:02 - has M rows and N
323:07 - columns and we know that b is M by 1
323:13 - which means that it got M rows and M uh
323:18 - N1
323:22 - call this means that if we
323:25 - combine horizontally so if we put them
323:28 - together next to each other the
323:31 - coefficient Matrix A and
323:34 - B this means that we will end up with
323:37 - the same rows so we still have M rows
323:41 - but now we will have n + one so here are
323:47 - the N columns from n from uh our
323:50 - coefficient Matrix a and here yet
323:53 - another extra one column that comes from
323:57 - the uh Vector B
324:00 - therefore we get the argument
324:03 - coefficient Matrix that has a dimension
324:05 - M by n +
324:13 - 1 so this Matrix is key when we are
324:17 - applying these different raw operations
324:20 - to solve
324:21 - our system of M equations and N
324:26 - unknowns so we are basically bringing
324:29 - all these values in order to solve our
324:32 - problem and find the
324:34 - unknowns now before moving on onto
324:37 - actual uh definition of row H form RF I
324:42 - wanted to provide here an example so how
324:45 - we go from uh from this system of
324:48 - equations into this uh new form a uh * X
324:53 - is equal to B and then how we can
324:56 - represent our Matrix into this idea of
324:59 - argumented coefficient Matrix so let's
325:02 - assume we have the following system of
325:04 - equations so we got X1 + 2 x 2 +
325:11 - X3 +
325:14 - X4 is equal to 7 and then we got X1 + 2
325:19 - X2 + 2 x3 - X4 is = to 12 and then 2 X1
325:28 - + 4 X2 and then + 6 x 4 is = to
325:36 - 4 let's now first just write this
325:39 - equations
325:42 - with three
325:47 - equations and then four
325:51 - unknowns into this system where we
325:54 - transform this into this matrices and
325:56 - the vectors so let's write it in this
325:59 - format of a xal to B that we saw before
326:04 - and then we will also write this in the
326:07 - um argument coefficient
326:12 - Matrix so given that I know that we have
326:15 - three equations and four unknowns the
326:18 - first thing that I will do is that I
326:20 - will get
326:22 - my coefficient Matrix
326:25 - a so for a what I need to do is that
326:30 - first I need to understand what is the
326:31 - dimension of it and how can I know the
326:34 - dimension I know that it should be M byn
326:37 - from the Theory where m is the number of
326:41 - equations the number of rows and N is
326:43 - the number of unknowns I got three
326:46 - equations for unknowns so this is 1 2 3
326:51 - and four X1 X2 X3 X4 those are my
326:54 - unknowns this means that I already know
326:57 - exactly what the dimension of my
326:58 - coefficient Matrix a should be which
327:01 - should be three by 4 let's now go in and
327:04 - search those values and filling in here
327:06 - to get our coefficient Matrix
327:08 - so here I see that X1 has a coefficient
327:11 - of one so whenever you don't see here a
327:13 - value it means we have one times that
327:15 - variable because one * a number is
327:17 - simply that number so here I got one and
327:22 - then uh for 2 X2 I got two for X3 I got
327:26 - one and then for x four I also got one
327:30 - and do keep in mind that in the columns
327:33 - I have to have the coefficients
327:35 - corresponding to this unknowned X1 X to
327:38 - ext4 this will be kind of a guide
327:42 - mentally to check every time whether we
327:44 - are dealing with the right
327:47 - coefficients so now we go on to our
327:50 - second equation or our second draw which
327:53 - means I got one in
327:56 - here see here one and then two two and
328:01 - then minus one and then here for my
328:05 - third dra third equation I got two for
328:08 - X1 I got four for my X2 I got nothing
328:13 - for my X3 which means that it is 0er
328:16 - because 0 * a variable is equal to zero
328:19 - and then finally for my X4 I got six so
328:23 - now I have my coefficient Matrix we can
328:27 - rewrite very quickly
328:29 - the common Vector containing the
328:32 - unknowns we already know what the
328:33 - dimension of it should be because we
328:35 - have four
328:38 - uh unknowns so it should be 4 by one by
328:42 - one and it is X1 X2 X3 and
328:49 - X4 and this is my X and
328:54 - finally I see what my B is
328:57 - here so we got three equations so we
329:00 - remember that the dimension should be M
329:02 - by 1 which is corresponding to what we
329:06 - also see here it's coherent we got three
329:08 - Bs for three equations so therefore I'm
329:12 - saying my
329:14 - B is equal to it is 3x one con
329:19 - vector and is equal to 7 12 and
329:24 - 4 now the final result is simply let me
329:28 - rewrite this in the form that we just
329:29 - saw so I can write this
329:33 - equations the system of three equations
329:37 - with four announce
329:39 - as 1 2 1 1 1 2 2 - 1 2 4 0
329:48 - 6 multiplied by X1 X2 X3 and
329:55 - X4 equal to 7 12 and
330:00 - 4 this is in the a x is equal to B
330:05 - Matrix
330:06 - notation so this is the first part now
330:10 - we know how we can rewrite the system of
330:13 - linear equations with uh M equations and
330:16 - N unknowns into this form of a x is
330:20 - equal to B let's now try the next step
330:23 - which is writing this um system uh
330:27 - rewriting it into this argumented
330:30 - coefficient Matrix so I want to get this
330:32 - a b this one is actually quite
330:35 - straightforward now when we have our a
330:38 - and the B because what this argumented
330:41 - coefficient Matrix represents it's
330:43 - simply taking the Matrix a and then
330:47 - adding the vector B next to it in order
330:50 - to get our
330:52 - final uh a plus b Matrix so not a plus b
330:57 - but basically a and then next to it the
330:59 - B so what is my a it is this thing so it
331:05 - is 1 one
331:08 - 2 and
331:10 - then 2 2 4 I'm just taking over
331:15 - this 1 2 0 1 - one and then 6 so now
331:23 - this is the
331:25 - a with three rows and then four columns
331:30 - and then here I'm simply putting this
331:32 - straight line like in here and then
331:36 - putting the b in there
331:38 - and the B was this so 7 12 and
331:42 - 4 so 7 12 and 4 this is the B which
331:51 - was 3x one 3 by 1 so one column and then
331:56 - three rows which means that this final
331:59 - Matrix The argumented Matrix a b it
332:03 - should have three rows
332:08 - and five
332:13 - columns so it's really important to um
332:16 - distinguish the multiplication and this
332:18 - unique form which is called uh the
332:21 - argumented coefficient Matrix so here we
332:24 - are not multiplying or adding we are
332:26 - simply taking the coefficient Matrix
332:27 - that we just found a next to the VOR B
332:33 - and given that they both have the same
332:35 - rows that's very straightforward and
332:37 - easy
332:40 - so this is about the argumented
332:42 - coefficient Matrix let's now move on to
332:45 - the next concept which is the row etum
332:48 - form or RF in short so Matrix a is in a
332:52 - row etum form if all nonzero rows are
332:56 - above any rows of all zeros so all
333:01 - nonzero rows are above any rows of all
333:05 - zeros each leading and entry of a row is
333:09 - to the right of the leading entry of the
333:11 - row above it and all entries in a column
333:15 - below a leading entry are zeros the
333:19 - leading entry in each row is known as
333:21 - the pivot or corner corner entry now
333:25 - this is a whole mouthful and um the
333:28 - concept itself is not straightforward
333:30 - when it comes to this definition but
333:32 - once we do the actual examples all this
333:34 - concept will be clear and when we solve
333:36 - these multiple problems and we are going
333:38 - to solve like three till five problems
333:41 - it will become easier to understand this
333:43 - concept of row H form what we mean by
333:47 - pivot or Corner entry or pivot uh uh
333:51 - Vector what we mean by uh having all
333:54 - these entries in a column below a
333:57 - leading entry zeros or what we mean with
334:00 - this concept of all nonzero rows being
334:03 - above any rows of all zeros so in terms
334:08 - of the words uh this might seem bit
334:10 - complicated and it is but when we look
334:12 - into the example this concept will be
334:16 - demystified so um once we have the RF
334:21 - which will be uh will uh come back in a
334:24 - bit and we will look into the example to
334:26 - clarify this definition let's quickly
334:28 - look at the next step that we need to
334:30 - perform which is the reduced row Edon
334:33 - form so here you can see that this
334:35 - reduced is what is basically added in
334:37 - front of the row H form so this was the
334:40 - row H form that we will now uh be
334:43 - discussing and then once the row H form
334:46 - is done we basically do a reduced row H
334:50 - form so that is the next step and by
334:52 - definition a matrix is in reduced row H
334:55 - form if in addition to the row H form so
334:59 - RAF we also have every leading entry in
335:03 - a row that is uh equal to one also we
335:06 - are calling that we have leading one and
335:09 - each leading one is the only nonzero
335:12 - entry in its column so r r EF or the
335:16 - reduced R HL form it simplifies the
335:19 - system for easier solution derivation so
335:22 - in terms of explaining or in terms of
335:25 - formulating the uh RF doesn't really uh
335:28 - sound that convincing because we need to
335:31 - demystify all these different concepts
335:33 - so my suggestion would be let's move
335:35 - actual to the actual practical problem
335:37 - problem and once we solve this step by
335:40 - step with all these details all these
335:43 - different criteria this one 2 and three
335:45 - and then the four and the fifth criteria
335:48 - they will be domestify and will be very
335:51 - clear to you let's now solve another
335:53 - problem a similar one to what we just
335:55 - solved only uh this time we will go a
335:58 - bit faster through all these steps just
336:00 - to ensure that our understanding of
336:02 - solving a linear system with this
336:05 - multiple equations and multiple unknowns
336:07 - using this idea of gaan elimination
336:09 - reduction and all these different steps
336:11 - up to the point of the redu through form
336:15 - we can go through quickly and we will
336:17 - ensure that uh we end up uh with the
336:21 - solution uh to the system using those
336:23 - techniques that we have just learned so
336:26 - here is my system of three equations and
336:28 - three unknowns this time and like we
336:31 - learned before the first step is to
336:34 - transform this into this argumented
336:37 - Matrix but first of course I need to
336:40 - know what is my a and what is my B
336:41 - because uh transforming this and writing
336:44 - this in the um argumented Matrix form a
336:48 - and then B it means I need to know what
336:51 - is my a and what is my B so let's write
336:53 - the first that down so my
336:55 - step one in this case is to write it in
336:59 - the argumented so
337:03 - argumented
337:05 - Matrix and for that I need to know what
337:08 - is my
337:09 - a so my a is equal
337:14 - to 1 1 1 and then 2 3 1 and then 1 - one
337:21 - and 2 this is my a which I take from all
337:26 - these coefficients that I see in here
337:29 - and then my B I can see in here from all
337:33 - this values so it is 6 14 and 8 this
337:37 - means that my argumented Matrix is a and
337:43 - then B and it's equal 2 1 1
337:47 - 1 and then 2 3 1 and then 1 - 1 and then
337:53 - two then straight line and then 6 14 and
337:58 - 8 this is my augumented Matrix this is
338:02 - my first
338:03 - step so let me go ahead and remove this
338:09 - details because then I can take this
338:12 - over and I can then simply
338:15 - continue so my argumented Matrix is then
338:19 - 1 1 1 and then 2 3 and then 1
338:27 - 1 Min - one and then
338:31 - 2 and then I have the values for my B
338:35 - which is 64 14 and
338:39 - 8 all right so first step is
338:42 - done now we are ready to go on to the
338:45 - next step step
338:48 - two which is to apply all sorts of
338:51 - operations and ideally by every time
338:54 - normalizing each row so every time
338:57 - ensuring that here the leading entries
338:59 - are once we call it
339:01 - normalization and then by elimination so
339:05 - we saw that every time we were
339:07 - eliminating some of the variables to
339:09 - ensure that we will end up with the
339:11 - zeros in the uh lower diagonal of our
339:15 - argument Matrix and we will have only
339:18 - the leading ones in our um only a single
339:24 - one and a leading pivot value so the
339:27 - pivot entry in our argument Matrix per
339:31 - column it will be the only element that
339:34 - will be uh non zero and the r should be
339:37 - all Z
339:37 - so here I'm talking about performing all
339:39 - these different operations to ensure
339:41 - that we uh create and we come to this
339:44 - point of the row eelin form and then
339:47 - from there we will go to the reduceed
339:49 - row eelin form so the r RF that we saw
339:52 - before so let's do that so the step two
339:54 - will be to uh
339:57 - get the
340:00 - RF of
340:02 - a or rather I have to say R RF maybe we
340:06 - can combine the two uh and do it at the
340:08 - same time
340:11 - so for that what I will do is that like
340:16 - before I would just draw this
340:18 - line because this will help me to keep
340:21 - track of all the operations that I'm
340:23 - performing later on to uh to check one
340:26 - thing that we can notice here that the
340:28 - first dra is already normalized which
340:30 - means that I have a one in here so I'm
340:33 - happy with that I won't do anything to
340:35 - it so uh what I need to do is to move on
340:39 - on t uh on the elimination of this um X1
340:45 - from the row two and Row three so I need
340:47 - to find a way to ensure that this
340:49 - entries become zero because I want to
340:52 - get here one0 0 so I want to have a
340:55 - single pivot entry in here and the
341:00 - remaining values in this column under
341:04 - this one should become zero so how can I
341:07 - do that
341:08 - the first thing that I can do to
341:10 - eliminate this X1 from row two is
341:14 - subtract from this two times the row one
341:19 - so you can see that if I take this row
341:21 - two so this is my row two this is my Row
341:24 - one and
341:27 - oh this is my row two this is my Row one
341:30 - and this is my Row three if I take row
341:33 - two this is my step
341:39 - 1 row two is equal
341:44 - 2 and then I take the row two and I
341:47 - subtract from this 2 * Row
341:54 - one let me remove this to open up some
341:59 - space so if I take this this row and
342:03 - subtract two times this one then what I
342:06 - will get
342:12 - is 2 - 2 * 1 which is
342:16 - 0o 3 - 2 * 1 so 3 - 2 is 1 and then 1 -
342:23 - 2 * 1 which is
342:26 - -1 and then when it comes to the value
342:28 - of b 14 - 2 * 6 so 14 - 12 is
342:33 - 2 so this is what I'm getting and then
342:36 - given that I have already a normalized
342:38 - first row so I have already one in here
342:41 - I will just take this over for
342:45 - now and then for the last column for the
342:48 - R3 in order to normalize that one and in
342:51 - order to get R of
342:53 - this one from here so first thing I want
342:56 - to do is to eliminate the X1 from here
343:00 - which means I need to get rid of this
343:02 - one and how can I do that I can simply
343:05 - take the Row three so
343:09 - R3 and I can subtract from the R3 the R1
343:13 - because both have one in here and this
343:15 - one I can simply take this and then
343:17 - remove and then subtract from this the
343:19 - first row I can uh then end up with
343:23 - basically a
343:28 - zero so here if I
343:31 - take this one I got this
343:34 - one and then I subtract from this 1 * 1
343:40 - so 1 - 1 is 0 - 1 - 1 is - 2 2 - 1 is
343:48 - 1 and then 8 - 6 is
343:57 - two this is what I get after performing
344:00 - this eliminations hence also the name um
344:03 - gausian elimination so using this
344:06 - elimination s we are then getting rid of
344:09 - certain elements and certain
344:12 - variables so this is what we get after
344:15 - performing these row operations and now
344:17 - we are ready to go on to the next
344:20 - stage so we see now that the um in this
344:25 - specific case the row two is already
344:28 - normalized which means that here we
344:30 - already have a one so we are happy with
344:32 - that we want it to be like
344:34 - that so I already see
344:38 - another coefficient that I want to get
344:41 - rid of that I want to eliminate from uh
344:44 - X2 which is this one so I want to get
344:48 - rid of this minus 2 and I want it to
344:50 - become a zero and one way that I can do
344:55 - that is to take this Row three and then
344:58 - add two * row two to this Row
345:03 - three so why am I not taking Row one
345:06 - because row one has a one in here and if
345:09 - I take that I will then um basically um
345:14 - bring me back to the point where I have
345:16 - I'm having here an element because zero
345:19 - and then uh adding something that uh is
345:22 - related to this first row or subtracting
345:25 - from it a multiple of this first row it
345:27 - will always lead me here this zero
345:30 - becoming a nonzero value that's
345:31 - something that I want to avoid therefore
345:34 - I want to perform some sort of oper
345:37 - to this Row three by using row two
345:40 - because that's my only way to eliminate
345:43 - this minus 2 from here without hurting
345:46 - my zero here and therefore I'm looking
345:50 - at this row that already contains the
345:52 - zero here so that's great and I'm going
345:55 - to use row two to eliminate this minus
345:58 - two and the way that I can do it given
346:00 - that here I have a minus so I have minus
346:02 - 2 I need to add to this 2 * Row 2
346:07 - so in the step number two I will then
346:11 - say that my R3 is equal to R3 and then
346:15 - plus 2 *
346:20 - R2 so let's go ahead and see what that
346:23 - gives us
346:27 - so 0 0 + 2 * 0 is
346:31 - 0 and then - 2 + 2 * 1 is - 2 + 2 which
346:37 - is
346:38 - zero so I'm already getting rid of the
346:41 - minus 2 which is great that's exactly
346:43 - what I
346:45 - wanted and then 1 here I have a one so 1
346:51 - + 2 * - 1 so 1 + 2 * -1 that gives me 2
346:58 - - 2 or 1 - 2 which is minus1 so I'm
347:02 - getting here a
347:05 - minus1 and then
347:07 - here when it comes down to the B element
347:11 - so I'm talking about this specific
347:13 - element so 2 and then plus 2 * 2 2 so 2
347:19 - + 4 is equal to 6 so here I'm getting a
347:26 - six okay perfect so let me take over the
347:29 - row number two it's already normalized
347:31 - so I already have a one in here so 0 1
347:35 - and then minus 1 and here I got a two so
347:40 - I just simply take over I'm not doing
347:42 - anything at this stage to that row
347:44 - number
347:48 - two
347:49 - so
347:54 - oh now the question is whether there is
347:57 - something that I need to do to my Row
347:59 - one well the question is yes because in
348:01 - our definition we saw that we wanted to
348:04 - have um this
348:09 - element ideally being zero because we
348:13 - need to have here a zero for this column
348:16 - to have a
348:18 - single element of one in the pivot entry
348:22 - which is this one which means that I
348:24 - need to make
348:27 - somehow this element one that we see in
348:30 - the row one being equal to zero at this
348:34 - stage let's see how we can achieve that
348:39 - so we got a one in
348:44 - here
348:45 - and we
348:51 - got a one in
348:54 - here so one thing that we can do is to
348:58 - take Row one and subtract from that row
349:01 - to in that case I won't do anything to
349:05 - my first entry my pivot my Pivot entry
349:08 - for my first row I won't do anything to
349:11 - it because 1 minus 0 is 1 but then I
349:14 - will get rid of this one because 1 - 1
349:18 - is equal to Z so let's go ahead and do
349:20 - that
349:22 - so R1 is = to R1 -
349:27 - R2 so in that way if I do 1 - 0 I'm
349:31 - ending up with 1 1 - 1 is = to 0 so I'm
349:37 - getting what I just wanted and then 1
349:40 - minus -1 it is 2 because it becomes +
349:45 - one + 1 and then 6 - 2 becomes 4 so this
349:51 - one becomes four this is the end result
349:54 - after performing these operations in
349:56 - Step number
349:57 - two perfect so now uh let's see what
350:02 - else we got in here that we want to
350:05 - change
350:07 - let me make this line
350:10 - longer all right so there is something
350:13 - that I still want to do in order to
350:16 - normalize the last row because here
350:19 - instead of having one I have minus one
350:21 - so firstly I need to multiply this row
350:25 - by one simply by a scaler in order to
350:28 - ensure that I can have one in here
350:31 - instead of just minus one so
350:35 - in Step number
350:37 - three I will do R3 is equal to
350:42 - -1 *
350:45 - R3 so I will
350:50 - get 0 0 1 given and then 6 * -1 is -
351:01 - 6 now what else we need to do uh in
351:05 - terms of the rest of the
351:07 - values in our uh argumented Matrix so we
351:11 - got
351:13 - one let me actually write it over such
351:15 - that we see what is going on so I'm
351:17 - taking over this and then minus one and
351:21 - then here I got two then here we got 1 Z
351:28 - 2 and then
351:33 - 4 all right so this is our argumented
351:35 - Matrix
351:37 - and there are few elements that we want
351:39 - to get rid of it is this element and
351:42 - this one because we want them to be
351:44 - equal to zero and how we can do that so
351:47 - we got two things to eliminate we want
351:50 - to eliminate from the row one this two
351:53 - and we want to eliminate from this row
351:55 - uh to this minus
351:57 - one how can we do that well for this Row
352:02 - one we can already see that we got here
352:05 - this one from row three so it's super
352:07 - helpful in that aspect so we can use
352:10 - that in order to get rid of that
352:14 - R2 so let me actually write it down in
352:19 - the
352:20 - uh step
352:23 - four so step number
352:25 - four what we can do is to take R1 and it
352:29 - will be equal to R1 minus 2 *
352:35 - R3 because here here I got one and I can
352:38 - use this one in order to get rid of this
352:40 - two so 2 - 2 * 1 is equal to uh 2 - 2 *
352:45 - 1 is equal to 0 so I can get a zero in
352:47 - here and that's exactly what we want to
352:50 - do we want to eliminate this
352:53 - x3e so we want to eliminate this from
352:56 - here this two from this first row that's
352:59 - something that we can achiev by this and
353:02 - another thing that I can also see let me
353:05 - free up some space in
353:10 - here
353:11 - to write down these
353:17 - equations so remove this given that the
353:21 - steps are already written you can always
353:23 - replicate that so then the
353:26 - new
353:29 - augmented Matrix a will become so here 0
353:32 - 0 and then one I'm living the last R it
353:36 - is already in a uh format that is
353:39 - desired so I will leave that there and
353:42 - then I'm going to update my first row so
353:45 - first row R1 is equal to R1 - 2 * R3
353:48 - which means it's equal to 1 - 2 * 0 is 1
353:52 - 0 - 2 * 0 is 0 2 - 2 * 1 is equal to
353:58 - 0 and then 4 - 2 * - 6 so 4 - 2 2 * - 6
354:08 - is equal to 4 and then + 12 which is 16
354:15 - so I'm getting a 16
354:20 - here and then for the second row I also
354:25 - need to do elimination to get R this
354:29 - time me remove this this time I want to
354:33 - get rid of this minus one because then
354:36 - I will have um criteria satisfied that
354:41 - this should also be zero because this
354:43 - should be zero for this pivot to be the
354:45 - only nonzero element in this column tree
354:48 - only then I can say that I'm dealing
354:50 - with r RF so that I have my Matrix in
354:55 - the reduced row Edon
354:57 - form so for that what I'm going to do is
355:02 - I'm going to make use of my third row
355:07 - again so I'm going to take row two and
355:14 - I'm going subtract from row two I'm
355:17 - actually going to add Row three to it
355:19 - because here I have minus one and if I
355:21 - have - one to 1 I will just get a zero
355:25 - so for that I'm going to write that uh
355:29 - for the elimination so for eliminating
355:32 - each tree from the second row I'm going
355:35 - to write that R R2 is equal to R2 +
355:44 - R3 so then I'm going to get here 0 + 0
355:49 - is 0 1 + 0 is 1 and then -1 + 1 is zero
355:56 - now why I have used actually the third
355:58 - drawing not the first one
356:01 - because I want to ensure that I want do
356:04 - anything to my zero in here
356:06 - if I do something with this first row
356:09 - then I cannot do that I cannot reach
356:11 - that it's just counterintuitive because
356:13 - I want to keep my work intact I have
356:15 - worked uh in such a way that I can
356:17 - eliminate this X1 from the second dra
356:20 - and that's something that I don't want
356:21 - to go back to instead I want to use
356:25 - something that is in the column three to
356:28 - get rid of this volum and that's
356:30 - something that we could do by using this
356:33 - element so we always look at the
356:36 - shortest and easiest path to get rid of
356:38 - a coefficient in our Matrix in our
356:41 - argumented Matrix now let's see what is
356:44 - left so R2 is equal to R2 + R3 which
356:48 - means that we need to add this two to
356:51 - this minus 6 and 2 + - 6 is equal to
356:57 - -4 -
357:00 - 4 there we go so after performing all
357:03 - the steps we end up with the following
357:06 - argumented Matrix and if you can notice
357:09 - here we not just achieved the row HED
357:12 - love form but we have actually achieved
357:15 - the reduced row hen form of our a this
357:22 - is that form because we have on the
357:25 - entry so our pivots are the ones in here
357:28 - and we can see that all the criteria are
357:31 - satisfied so here on the lower diagonal
357:34 - so the lower part of our Matrix are
357:36 - zeros and on the top of that also all
357:39 - the elements in this columns they are
357:41 - all zero except of the pivot values and
357:45 - they are all equal to one and that's
357:47 - exactly what we wanted to have in order
357:50 - to say that we are dealing with the
357:52 - reduced row Eon
357:54 - form perfect so now we are ready to move
357:59 - on to the last final step to get this
358:02 - solution and unlike the previous system
358:05 - which was bit more complicated and it
358:07 - had multiple infinite solutions so you
358:10 - might recall this example where we had
358:14 - to describe the final solution using
358:17 - this X2 and X3 because depending on
358:20 - different values of X2 and X3 which
358:22 - could take infinite values we then could
358:24 - get a different um solution for our x uh
358:28 - Vector so we we saw that when for
358:31 - instance the X1 is equal to 1 and X4 is
358:34 - equal to 3 then our solution was this
358:37 - but then of course if the uh x 2 and
358:42 - X4 were different we would have gotten
358:45 - another solution and the result of the
358:47 - linear system had infinite number of
358:49 - solution unlike that one this one this
358:53 - example is much more convenient because
358:55 - even from the RF of this so row reduced
358:58 - row form of the uh Matrix a we can
359:02 - already see what is the solution of this
359:04 - problem of this linear system we had
359:06 - three equations and three unknowns so we
359:10 - actually got to single Solution that's
359:12 - something that I can already see from
359:14 - here and let's uh break it down to see
359:18 - how um I see that solution so we
359:21 - transformed this to a system of
359:28 - equations so we are basically doing it
359:30 - backwards and now after getting this RF
359:33 - of Matrix a we are going going back and
359:36 - we are going to rewrite the system with
359:38 - the unknowns X1 X2 and X3 so here I see
359:43 - one which means that the coefficient of
359:45 - X1 is 1 so 1 X1 which I will just write
359:52 - X1
360:00 - X1 so I get X1
360:06 - X1 + 0 * X2 + 0 * X3 is equal to
360:15 - 16 and
360:17 - then 0 * X1 + 1 * X2 + 0 * X3 I'm
360:26 - writing down this zero such that it will
360:27 - make sense what we are doing and in case
360:30 - next time you have a different system it
360:32 - will be easier to follow so 0 * X1 + 0 *
360:37 - X2 plus Z
360:40 - plus 1 * X3 is = to
360:44 - -6 now what is
360:46 - this we get rid of all these zeros so
360:50 - you can already see what is going on we
360:52 - are ending up with X1 is equal to 16 X2
360:56 - is = to
360:57 - -4 and X3 is equal to -
361:02 - 6 there we go so we got our solution for
361:08 - X which is X1 X2 X3 is equal
361:15 - to
361:16 - 16 - 4 - 6 this is the unique solution
361:22 - to our problem and if we clean this up
361:25 - and we write down in a nicer term so I'm
361:28 - going to remove
361:32 - this then we can say that the solution
361:37 - for our problem so this problem that we
361:40 - saw in
361:42 - here so the X which is equal to X1 X2
361:50 - X3 it's equal to 16 - 4 - 6 which you
361:57 - can see it also here so one trick is
362:00 - always to look into in here if you have
362:03 - the identity Matrix
362:06 - part of your argument Matrix then you
362:08 - have a unique solution to your problem
362:11 - so you got three rows and you got three
362:12 - column and you have all these identity
362:15 - so these unit vectors then this the
362:17 - indication that you got a unique
362:19 - solution to your problem and that
362:22 - solution is simply this column as you
362:26 - can see in here so next time when you
362:28 - have something like this you don't even
362:30 - need to rewrite the transformation back
362:32 - to the equations you can directly say
362:34 - that the solution to your system with
362:36 - these three equations and three unknowns
362:39 - is this one so this column Vector equal
362:43 - with this entry 16 - 4 and - 6 and
362:46 - that's the end of this problem so in
362:49 - this way we have learned how we can
362:51 - quickly solve this type of problems by
362:54 - just following this uh common procedure
362:57 - and this ended up with a unique solution
363:00 - so you can have cases when you will not
363:02 - have any solution and that will be the
363:04 - case when we saw before two so in case
363:07 - um there is an equation says that um in
363:11 - the left hand side you have zeros but
363:12 - then in the right hand side you have a
363:14 - number so you end up with 0 is equal to
363:16 - two number let's say eight this this
363:19 - cannot happen which means that you are
363:21 - saying your system doesn't have any
363:23 - solutions so let's let me actually write
363:25 - it down so you can have three possible
363:27 - cases case
363:30 - one when you end up with one of your um
363:34 - equations and here you got numbers let's
363:37 - say 3 4
363:39 - 5
363:40 - and here this is zero so basically you
363:43 - end up saying 0 is equal to 5 which is
363:46 - not something that is possible so you
363:48 - say the
363:50 - solution
363:53 - solution is this so it doesn't exist it
363:57 - is also possible that you have case
364:01 - two when um like in the previous case
364:05 - that we saw before in this example you
364:09 - don't have um the um you don't have all
364:13 - the uh unknowns expressed in terms of
364:16 - the other ones which means that you
364:18 - don't have a unique solution to your
364:20 - system so you can even see that from
364:23 - here therefore you have your X2 and X4
364:26 - so you describe your final solution as a
364:29 - linear combination of some of the uh
364:32 - variables that can take infinite amount
364:34 - of uh vol so X2 can be anything and X4
364:38 - can be anything which means that your
364:41 - Solutions will also be infinite so this
364:44 - case two will have infinite amount of
364:47 - solutions and the case three is the case
364:50 - that we saw before right now when we end
364:54 - up with this identity Matrix in here and
364:56 - this corresponding uh amounts in here so
365:00 - this gives us indication that we got
365:02 - just single solution so single
365:07 - solution a solution to your general
365:10 - linear systems