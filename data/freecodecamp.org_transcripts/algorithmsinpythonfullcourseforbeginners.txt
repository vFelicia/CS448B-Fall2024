00:00 - in this python algorithms course you
00:02 - will learn to work with recursion data
00:04 - structures greedy algorithms dynamic
00:07 - programming and more
00:09 - you don't need any previous experience
00:11 - and algorithms to follow along
00:14 - algorithms help us solve problems
00:15 - efficiently and employers pay good money
00:18 - for good problem solvers
00:20 - your instructor for this course is joy
00:22 - brock
00:23 - she has an amazing talent for breaking
00:26 - down complex topics using humor and
00:28 - visualizations
00:32 - [Music]
00:55 - hi
00:56 - can i ask you a question raise your hand
00:58 - if you love being bossed around
01:02 - i'm not seeing any hands
01:04 - i know that i don't like to be bossed
01:06 - around
01:07 - now you might be wondering why i would
01:08 - ask a question like that and what does
01:11 - that have to do with computer algorithms
01:13 - but that's just it
01:15 - computers love to be lost around in fact
01:18 - a computer can't do anything on its own
01:21 - without being told what to do and that
01:23 - my friends is what an algorithm is
01:26 - an algorithm is a specific set of
01:28 - instructions that we write to the
01:30 - computer to tell it what to do after all
01:33 - that's why you're here right to learn
01:35 - how to tell that old ball and chain
01:37 - who's boss you know the one that keeps
01:40 - us perpetually locked in a prison of
01:42 - screen time so without further ado
01:45 - welcome to the introduction to
01:47 - algorithms course i am so glad you're
01:49 - here we are going to have an insane
01:52 - amount of fun so grab your beverage of
01:55 - choice grab a notepad for notes and be
01:58 - prepared to learn the five basic
02:00 - algorithms that every programmer should
02:02 - know
02:05 - by official definition an algorithm can
02:07 - be defined as being the current term of
02:09 - choice for a problem solving procedure
02:12 - it is commonly used nowadays for the set
02:14 - of rules a machine especially a computer
02:16 - follows to achieve a particular goal it
02:19 - does not always apply to computer
02:21 - mediated activity however the term may
02:24 - is accurately be used for the steps
02:26 - followed in making a pizza or solving a
02:28 - rubik's cube as for computer powered
02:30 - data analysis
02:32 - algorithms are often paired with words
02:34 - specifying the activity for which a set
02:37 - of rules have been designed
02:39 - a search algorithm for example is a
02:41 - procedure that determines what kind of
02:43 - information is retrieved from a large
02:45 - mass of data an encryption algorithm is
02:48 - a set of rules by which information or
02:50 - messages are encoded so that
02:52 - unauthorized persons cannot read them
02:54 - this comes from the merriam-webster
02:56 - dictionary what does algorithm mean
03:02 - mathematically speaking algorithms have
03:04 - been around for nearly 900 years pretty
03:07 - amazing isn't it in fact algorithms are
03:10 - all around us every day and have been
03:13 - our whole lives in this course we're
03:15 - going to dive into algorithms that are
03:17 - finite or determine sequences of
03:19 - explicit instructions that are used to
03:22 - solve problems and or perform
03:24 - computations these computations are
03:26 - generally to perform calculations or to
03:29 - manipulate data using natural language
03:32 - processing natural language processing
03:34 - or nlp for short is what tells the
03:37 - computer how to comprehend and interpret
03:40 - written text
03:42 - it's important to distinguish the
03:44 - difference between using well-defined
03:46 - formal languages to calculate functions
03:48 - versus algorithms used in artificial
03:50 - intelligence or machine learning
03:53 - those algorithms are where automated
03:55 - deductions are made using mathematical
03:57 - and logical tests
03:59 - or where predictions are made based on
04:01 - patterns learned from experience for
04:04 - example the other day when i was looking
04:05 - at the front page of google on their
04:08 - news i thought you know this algorithm
04:11 - better be so good that i can read about
04:13 - the news before it even happens
04:16 - those types of algorithms are for
04:18 - another video on another day
04:22 - the objective here in this course is to
04:24 - give you the tools you'll need to
04:26 - undertake the journey of using a
04:28 - well-defined formal language in this
04:30 - case python to calculate functions
04:33 - within a defined amount of space and
04:35 - time that may be asked of you for
04:37 - interviews or tests
04:39 - in other words the goal is for you to be
04:42 - able to
04:43 - find differences in algorithmic
04:45 - approaches based on computational
04:47 - efficiency
04:48 - this means that how we write our program
04:51 - is very important in regards to how much
04:54 - space it can use up on our computer or
04:57 - how quickly it can run
04:59 - and it all begins with the foundational
05:01 - principles that every algorithm has
05:04 - in input
05:06 - well-defined instructions
05:08 - execution of those instructions step by
05:10 - step leading to an output that ends in
05:14 - termination of the program
05:16 - now there are many different algorithms
05:19 - that accomplish
05:20 - these basics when it comes to
05:22 - programming and as much as i love
05:25 - spending time with you all
05:27 - it would be so difficult to cover every
05:29 - single one so for the sake of brevity
05:32 - we'll be covering the following
05:35 - simple recursive algorithms
05:37 - algorithms within data structures
05:40 - divide and conquer
05:42 - greedy algorithms
05:44 - and last but certainly not least dynamic
05:47 - programming
05:49 - for this course you'll need some
05:51 - understanding of the python language
05:53 - like writing functions variables
05:55 - conditionals for loops and basic syntax
06:02 - an ide or integrated development
06:04 - environment an ide is the virtual place
06:08 - to write our programs
06:09 - now if you don't have a downloaded
06:11 - platform like pycharm jupiter notebook
06:13 - or visual studio code you can just use
06:16 - any online python ide through a google
06:19 - search
06:21 - if you would like to download your own
06:22 - personal ide such as pycharm vsc or
06:26 - jupiter you can definitely do that as
06:29 - each one of those platforms are free to
06:31 - install
06:33 - go ahead and pause the video now if you
06:35 - need to get situated otherwise let's get
06:38 - this party started enroll into our very
06:40 - first lesson on simple recursive
06:42 - algorithms
06:43 - in our first lesson we're going to learn
06:45 - what is recursion and we're going to
06:47 - examine three different problems ranging
06:50 - from easy to mind-breaking
06:54 - we're going to look at the differences
06:55 - in computational efficiency and see
06:58 - which approach is best
07:00 - is implementing recursion the best way
07:03 - let's find out
07:07 - [Music]
07:12 - if the easiest way to define recursion
07:14 - is to say it's where a function calls
07:15 - itself then what's the hard way to
07:18 - define it in all truth it's easy to
07:20 - throw a statement out hand on hip that
07:21 - goes oh it's a function that calls
07:23 - itself
07:24 - sure okay good for you but what does
07:27 - that really mean i think oftentimes when
07:30 - it comes to programming it's easy to
07:32 - take for granted this idea that we
07:33 - should automatically know what it all
07:35 - means
07:36 - i don't know about you but i wasn't born
07:38 - with the ability to comprehend various
07:40 - methods to solve problems let alone be
07:43 - able to decipher which which method is
07:45 - better in the first place you know
07:47 - we all need to learn these concepts
07:49 - layer upon layer step by step so let's
07:52 - start at the very first layer and build
07:54 - up from there now when we write our
07:56 - program recursively it's important to
07:58 - understand that this is in contrast to
08:00 - writing it iteratively or through
08:02 - iteration when we do that we're
08:04 - generally using for loops to iterate
08:06 - over an object instead of making a
08:08 - function call back within the program
08:10 - you'll see what i mean when we get into
08:12 - our ides and and start to write the code
08:15 - recursion
08:16 - is it originates from a latin verb that
08:19 - means to run back and this is what the
08:22 - program we're going to write together is
08:24 - going to do it will run back to itself
08:26 - over and over as many times as it needs
08:28 - until the program terminates so does
08:31 - this mean that we can't use for loops
08:33 - with recursion no of course you can use
08:35 - them in fact they will be needed and
08:38 - you'll see how that all works in just a
08:40 - moment in order to get started with our
08:42 - very first lesson involving factorials
08:44 - and the algorithmic method that we are
08:47 - going to use for better efficiency we
08:49 - will calculate the factorial of a number
08:50 - using the leaf method or last in first
08:53 - out and it all begins with a mental
08:55 - picture of a bookshelf that doesn't have
08:57 - any books on it yet to perform a lethal
08:59 - operation on your hypothetical bookshelf
09:02 - you will grab some books and begin
09:03 - placing them on the shelf let's say you
09:05 - ordered some britannica encyclopedias
09:08 - wait why would you do that
09:10 - why did any of us do that oh right it
09:12 - was our grandparents that saw to it to
09:14 - make sure that we had our own modern day
09:16 - google machine thanks grandma your
09:18 - legacy lives on at the donation center
09:21 - okay that was a wonderful trip down
09:23 - memory lane wasn't it okay so back to
09:26 - lefo or lifo if that's what you call it
09:28 - so you put your encyclopedias on the
09:30 - shelf a to z in a last in first out
09:32 - scenario what was the last book you you
09:34 - sat on the shelf z what's the first book
09:37 - that's going to come off z
09:39 - so last in first out and this would be
09:42 - important to remember in our number
09:43 - oriented recursive function coming up
09:45 - that the last number that goes in will
09:47 - be the first number that gets called out
09:49 - or computed
09:51 - now if you're a little bit confused it's
09:53 - okay you won't be for long we're going
09:55 - to see this in action right now and talk
09:57 - about the computationally efficient way
09:59 - to solve as we write our first recursive
10:02 - program to calculate the factorial of a
10:04 - number
10:09 - for a quick recap on factorials say you
10:12 - take the number 5 and multiply it by
10:14 - itself minus 1 all the way down until
10:17 - you hit one
10:18 - your answer will equal 120. remember
10:21 - factorials only involve positive
10:23 - integers so once you hit one the program
10:25 - terminates
10:27 - now let's get to work
10:30 - we're going to look at two ways that we
10:33 - can write a program that will calculate
10:35 - the factorial of a number this is going
10:37 - to be a wonderful foray into differences
10:40 - regarding complexity and program
10:42 - efficiency in our first method we're
10:45 - going to tackle this iteratively or like
10:48 - i mentioned before through iteration
10:50 - this means that our program will cycle
10:52 - through or iterate each and every step
10:55 - we tell it to
10:58 - first we define our function with one
11:00 - parameter the number we want to
11:02 - calculate in this case we'll just call
11:04 - it n
11:05 - then we're going to create a variable
11:07 - and start it at one positive numbers
11:10 - remember no zeros here next we construct
11:13 - a for loop to iterate over that variable
11:16 - or object if you like
11:18 - and do something to it in this case
11:20 - we're going to multiply it by every
11:22 - number in a range starting at 2 and
11:25 - adding in our ending at our number plus
11:27 - one this gives us a range of four
11:30 - numbers two three four five now why not
11:34 - six since n or five plus one equals six
11:37 - remember we're using python so if we
11:39 - loop to a range of two
11:41 - comma 6 in parenthesis our indices will
11:44 - always be minus 1 because python starts
11:46 - at 0. so an index position of 6 will
11:49 - give us our last number for our
11:51 - calculation 5. then we take our object
11:54 - called fact and multiply it by every
11:56 - number in our range because we can use
11:58 - the asterisk and equal sign to condense
12:01 - our code a little fact equals fact time
12:04 - asterisk i that assignment operator is
12:07 - reassigning our variable each time it
12:09 - cycles through our loop
12:11 - when the cycling or iteration is
12:13 - finished we simply return our finished
12:15 - fact variable if you want to see the
12:18 - results of each
12:20 - iteration you can just place a print
12:21 - statement within the for loop and run
12:23 - the program
12:24 - for a recursive method it's going to be
12:27 - already typed out while i talk
12:29 - this is where our last in first out
12:31 - comes into play remember when we talked
12:33 - about that this is in contrast to our
12:35 - iterative method that starts at the top
12:37 - are fact variables starting at 1 and
12:39 - then iterates through our range going up
12:41 - until our number is reached in the
12:43 - algorithmic method of recursion
12:46 - when we go to call the function if our
12:48 - number isn't 1 it moves to our else
12:50 - statement where in our else statement we
12:52 - have a temp variable that runs back to
12:54 - our function call and creates a pocket
12:56 - of space if you will for each number
12:59 - minus one
13:00 - it will run back all the way down until
13:02 - it hits one our last n if you will once
13:05 - that happens the return statement and
13:07 - our else statement starts working its
13:09 - magic and then we start heading back up
13:11 - our last in which was one becomes our
13:14 - first out
13:15 - our first number to get computed
13:18 - ta-da
13:19 - [Applause]
13:22 - now you might be tempted to think that
13:24 - the recursive method is top dog here but
13:27 - alas you'd be wrong our iterative method
13:30 - is twice as fast
13:32 - for what we're trying to accomplish
13:33 - which is
13:34 - just a basic calculation of a factorial
13:36 - what's important what's important to be
13:38 - able to understand is a how recursion
13:41 - works by using a simple program such as
13:43 - momentous did and b how how and why one
13:46 - approach can be better than another
13:48 - in this case recursion is taking up more
13:51 - space and time by running back to the
13:53 - function call to create those pockets
13:55 - for our numbers
13:56 - versus iteration where it's simply
13:58 - multiplying each number in place and not
14:00 - creating anything new
14:03 - it's simply iterating over each number
14:05 - reassigning our variable each time and
14:07 - producing a final output
14:09 - the objective for our basic exercise is
14:11 - to just understand and see recursive
14:13 - algorithms in action so that as you gain
14:16 - experience you'll be able to exercise
14:18 - different modes of expression or
14:20 - expressivity in your writing
14:22 - and in some cases use your readability
14:25 - check out this example
14:34 - on the upside being able to write
14:36 - code in three lines instead of seven or
14:38 - eight can be a huge deal depending on
14:40 - the circumstance however on the flip
14:42 - side may decrease the performance of the
14:45 - code let's like this example that we
14:48 - just coded together
14:50 - it's just not as efficient as iteration
14:52 - in this case so if you're ready let's
14:54 - move up to a more advanced recursive
14:56 - algorithm involving permutations
15:02 - a permutation is when you take a set of
15:04 - elements and find all the different
15:06 - variations that can be made with it for
15:08 - instance if we take a b and c we can get
15:11 - six variations out of these three
15:14 - letters mathematically speaking we can
15:16 - find the number of variations by
15:17 - calculating the factorial of the
15:19 - elements just like we did a few minutes
15:21 - ago
15:23 - for a hands-on approach to understanding
15:25 - a level up on recursion let's take a
15:27 - peek under the hood and see how this
15:29 - will work in a program shelby
15:32 - [Music]
15:34 - the underlying basis of how most
15:36 - functions for permutations work is to
15:39 - take off the first element and then
15:41 - create two subsets of numbers that get
15:43 - swapped like this
15:46 - then the original first element gets put
15:49 - back onto the front of each subset then
15:51 - we do this all over again this time
15:53 - instead of popping out the first element
15:55 - we pop out the second one and create two
15:57 - new subsets and so on and so forth
16:01 - the algorithm behind permutations can
16:03 - really make your head spin so take a
16:05 - deep breath
16:06 - find your center and jump in
16:09 - to
16:10 - a puddle
16:11 - let's not jump into the ocean just yet
16:14 - it's good to start small and by small
16:16 - let's recursively swap just two letters
16:19 - of a string this is a little bit harder
16:21 - to do than a list because strings are
16:23 - immutable but
16:25 - this is a good launch point for us to
16:27 - build up from here also we'll start with
16:29 - recursion this time and then afterwards
16:32 - we'll look at a different method for
16:33 - comparison
16:37 - to write a recursive function that will
16:38 - swap just two letters we will need to
16:40 - create a pocket for where our strings
16:43 - need to go remember again strings are
16:45 - immutable and if this were a list we
16:47 - could simply slice it and create a
16:49 - variable swap but that's a no go for
16:52 - strings we're moving into a little bit
16:54 - steeper territory here but stay with me
16:57 - okay you can do this
16:59 - let's create our function and call it
17:01 - permute with two parameters one called
17:04 - string and for the string of two letters
17:06 - that we want to swap and an empty pocket
17:08 - to put our new string in this is our
17:10 - parameter default defaulted to an empty
17:13 - string then we create an if statement
17:15 - for an edge case in the event that the
17:17 - string is empty and if it's empty we
17:19 - just return our empty pocket now we'll
17:22 - start with looping through the range of
17:25 - the length of our string and within our
17:27 - loop we'll start with creating three
17:29 - variables that will contain one each
17:32 - letter on our string two we'll need to
17:34 - take off the front of the string that
17:35 - we're going to call front and then three
17:37 - a variable called back that will slice
17:39 - off the back of our string if you study
17:42 - this algorithm you'll see some
17:43 - programmers call it head and tail
17:45 - current and the rust first next or front
17:49 - and back i like the terms front and back
17:51 - as it gives me a better mental picture
17:53 - but you can use whatever terms you're
17:54 - more comfortable with
17:57 - now those first three variables are
17:59 - fairly obvious but this is where it
18:01 - might get a little hazy we need to
18:03 - create a fourth variable that will put
18:05 - our front and back together and we'll
18:08 - need this variable of combined elements
18:10 - to plug into our function call
18:13 - this is what creates spaces to in our
18:16 - pocket to be able to move our letters
18:17 - around i know this may sound a little
18:19 - complicated at the moment but you'll see
18:22 - shortly that we're going to need each
18:23 - and every letter of our string even
18:25 - however many that will eventually come
18:28 - to be to have the space it needs to go
18:30 - into the right place in our pocket now
18:33 - whether that's going to lead up to
18:34 - eventually being three letters or four
18:37 - keep in mind our strengths can't be too
18:39 - long because of two things the rate of
18:41 - exponential growth and the fact that
18:43 - recursion can get exhaustive very fast
18:46 - the more letters you have recursively
18:48 - that's a lot of running back okay we
18:51 - have all the variables we need right now
18:53 - our letter a front which is just an
18:56 - empty string right now our back which is
18:58 - the last set of our string b
19:00 - and are together that combines our front
19:02 - and back now for our algorithm this is
19:04 - where the magic happens this is the true
19:08 - ham in our sandwich we call our function
19:11 - premium and then incorporate two
19:12 - parameters are together
19:14 - and our letter added to our pocket hence
19:17 - the addition operator
19:19 - this is the heartbeat of our algorithm
19:21 - it needs to run back and put our letters
19:24 - into our pocket once in reverse and then
19:26 - the second time in a copy of its
19:28 - original form because it's still
19:29 - considered a permutation watch how this
19:32 - works in the visualizer
19:43 - i think that if you get really get a
19:44 - really good handle on this concept and
19:46 - you have a solid foundation
19:48 - it becomes so much easier to see how
19:50 - this works with additional letters if
19:52 - you can see it mentally this makes all
19:54 - the difference in the world
19:58 - it's a neat little program that only has
20:00 - 11 lines of code which is really nice
20:03 - however this program isn't as efficient
20:07 - as a program that uses iteration and
20:09 - swapping again
20:12 - that program has nearly double the lines
20:14 - of code i'll spare you the direction of
20:17 - watching me type it all out and just
20:19 - show you here
20:25 - this would be the code that executes
20:28 - just like the beginning
20:29 - [Music]
20:31 - that you saw on the earlier slides where
20:33 - it pops off the first letter and then
20:35 - swaps the it pops out the first letter
20:37 - swaps the last two and then
20:39 - pops the first letter back on then it
20:41 - rotates to the second letter
20:44 - swaps those two
20:45 - and then puts it back on
20:48 - check the output here and you'll see
20:50 - what i mean now if you needed to know
20:52 - all the permutations of a string and
20:54 - don't really have time to write the code
20:56 - or the patience you could use the inner
20:59 - tools permutations module it's really
21:02 - easy
21:03 - but
21:03 - that would be primarily for you
21:06 - just to know for yourself or for a quick
21:08 - look up when it comes to interviews or
21:10 - testing companies would generally prefer
21:14 - that you know the mechanics behind the
21:16 - method for solving whether that's using
21:18 - recursion or swapping
21:21 - as with any program the caveat must
21:23 - always be made can this program be
21:25 - written differently
21:26 - as always yes however you will see very
21:30 - little difference when it comes to
21:31 - recursion of most programs like this
21:34 - very generally speaking they will call
21:37 - the function in a very similar way as it
21:40 - is written here by taking the front and
21:42 - back together and incorporating it in
21:45 - some way
21:46 - within the function call
21:48 - along with the letter
21:50 - hopefully you were able to distinctly
21:52 - see how recursion works in this program
21:55 - even just for two letters
21:57 - it can work great on small strings but
22:00 - it will get computationally expensive
22:03 - the longer the string now if you're not
22:05 - too overwhelmed hopefully not we've only
22:07 - done two
22:08 - we're going to move into the mind
22:11 - breaker section like it should be the
22:13 - mind blowing section
22:16 - the end cleans
22:18 - problem
22:23 - this computational problem is about
22:25 - placing eight chess queens on an eight
22:27 - by eight board so that no two queens are
22:30 - able to attack each other according to
22:33 - wikipedia the problem finding all
22:35 - solutions to the eight queens problem
22:37 - can be quite computationally expensive
22:40 - there are 4 billion possible
22:42 - arrangements of the 8 queens on an 8x8
22:45 - board but only 92 solutions wikipedia
22:48 - also goes on to say that the use of
22:49 - shortcuts and various rules of thumb
22:51 - that avoid brute force techniques can
22:53 - greatly reduce the number of possible
22:55 - combinations which i'm sure is always a
22:58 - good thing but what is brute force
23:00 - though
23:01 - well that's would be brute force would
23:04 - be looking at every single possible
23:06 - arrangement 4 billion of them to find
23:09 - the 92 solutions how long do you think
23:11 - that would take
23:13 - probably take decades for our computers
23:14 - to run a program like that but guess
23:17 - what the use of permutations can reduce
23:19 - the possibilities from 4 billion down to
23:21 - just 40 000. that's a big reduction
23:25 - now we're not are we going to use
23:27 - permutations on the end queen's problem
23:28 - today no way jose
23:31 - but it's good to know that this problem
23:33 - is often used in examples for various
23:35 - programming techniques such as solving
23:37 - it by using recursive algorithms it's
23:40 - also used
23:41 - as an example in constraint programming
23:42 - which is a paradigm for solving
23:44 - combinatorial problems that draws on a
23:47 - wide range of techniques from artificial
23:49 - intelligence computer science and
23:51 - operations research
23:53 - to get a better idea of what that would
23:55 - be we can use a real world example like
23:58 - employee scheduling say for instance a
24:00 - company operates continuously like a
24:02 - factory they need to create weekly
24:04 - schedules for their employees and if
24:07 - that company runs three eight hour
24:09 - shifts per day in a science three of its
24:11 - four employees different shifts each day
24:13 - while giving a fourth one's a day off
24:16 - even in such a small case like that the
24:19 - number of possible schedules is huge
24:22 - each day there are factorially 24
24:24 - possible employee assignments for four
24:26 - people
24:27 - so the number of possible weekly
24:28 - schedules is 247 which calculates over
24:32 - 4.5 billion
24:34 - constraint programming methods keep
24:36 - track of which solutions remain feasible
24:38 - and makes it a powerful tool for solving
24:40 - real world
24:42 - scheduling problems who knew queens on a
24:44 - chessboard combined with computer
24:46 - programming could help solve real world
24:48 - problems like that
24:54 - so what did we learn in our very first
24:57 - lesson in our algorithms course
24:59 - we learned simple recursion and saw it
25:01 - in action that when a function calls
25:04 - itself it creates new spaces to store
25:06 - information and sometimes this may not
25:08 - be an efficient solution to a problem
25:11 - it's definitely more elegant with fewer
25:13 - lines of code
25:15 - that can come in handy depending on the
25:17 - problem but it's not always cost
25:19 - effective
25:20 - but it is important to see and
25:22 - understand the difference
25:24 - factorials and permutations are used in
25:27 - a lot of computational mathematical and
25:29 - real world applications
25:31 - learning how to write programs rather
25:33 - recursively or iteratively to find the
25:35 - best method
25:36 - in this area can go a long way and may
25:39 - lead to unknown opportunities for you
25:40 - down the road as a programmer
25:43 - we learned about constraint programming
25:45 - where recursion plays a role for solving
25:47 - mind-bending problems
25:49 - not only within board games such as
25:51 - chess but real life issues that
25:53 - companies can face
25:55 - as a programmer that works to gain the
25:57 - tools needed to understand these types
26:00 - of methods to solve problems can make
26:02 - them a valuable asset to any company
26:07 - [Music]
26:17 - welcome to level two
26:20 - working with data
26:22 - that is structured i like to think of
26:24 - data structures like something that
26:26 - holds money say for instance you have on
26:28 - your person a big wad of cash consisting
26:31 - of a lot of different bills what could
26:33 - you do with it well you could store it
26:36 - sort it search it add it spend it
26:39 - delete it
26:40 - count it so with data structures they're
26:42 - simply containers or collections of data
26:45 - they provide a means of managing small
26:47 - or large amounts of values for uses and
26:49 - such things like
26:50 - databases web traffic monitoring user
26:53 - registrations and or indexing within a
26:56 - contiguous memory location like an array
26:59 - they can range from as simple as lists
27:02 - to linked lists stacks cues hash tables
27:05 - trees and heaps
27:07 - these are all examples of data
27:09 - structures and since algorithms are
27:10 - steps to solving problems what kind of
27:13 - problem-solving techniques are common
27:15 - with data structures well just like
27:17 - watts of cache data often needs storage
27:20 - sorting counting retrieving searching
27:23 - spending all those fun things now arrays
27:26 - can be a really basic container for
27:28 - information but don't let that fool you
27:30 - many different algorithms can be used on
27:32 - one-dimensional arrays but what about
27:34 - two-dimensional arrays what about 32
27:37 - dimensions since that's the maximum
27:39 - amount can you imagine
27:41 - arrays are among the oldest and most
27:43 - important data structures and are used
27:45 - for almost every program for a basic
27:48 - level of algorithms with data structures
27:50 - we'll go easy and stick to a linear or
27:53 - one-dimensional array
27:55 - a one-dimensional array is just an array
27:57 - stored in an unbroken block of memory to
28:00 - access the next value in an array we
28:02 - just move sequentially to the next
28:04 - memory address it's best to see this
28:06 - visually so check these slides out for a
28:08 - better idea
28:17 - in our basic section we're going to
28:19 - cover
28:20 - linear search binary search
28:23 - bubble sort
28:24 - and insertion sort
28:26 - we'll be covering a little bit more for
28:29 - the basic level on data structures
28:31 - because it's a really big topic actually
28:34 - entire books are written about them let
28:36 - alone design techniques and algorithms
28:39 - i'll do my best not to put you to sleep
28:42 - hopefully it'll be enough to watch your
28:43 - appetite where array is our concern as
28:45 - this is a huge topic as well and as much
28:48 - as python is a wonderful program with
28:50 - tons of features like built-ins and
28:52 - libraries and modules that can help
28:54 - programmers code faster it's still
28:56 - really important to understand how these
28:58 - features work algorithmically and the
29:00 - concepts underneath it all
29:02 - let's get started
29:10 - a linear search could be equated to
29:13 - pulling that a lot of cash out of your
29:15 - wallet
29:16 - wait does anyone carry cash anymore no
29:19 - okay well let's pretend that you have a
29:22 - wallet full of really big bills i mean
29:24 - go big or go home right and you need a
29:26 - 500 bill from your stack
29:29 - why well you're at the mall of course
29:31 - and you want to buy that really cool
29:33 - pair of socks that you saw in the window
29:35 - and you might be thinking 500 for a pair
29:37 - of socks and i would say yup that's
29:40 - inflation for you but this is a fantasy
29:42 - remember besides inflation slomation you
29:45 - got a wallet full of big bills and in
29:47 - this fantasy let's go for a big ticket
29:50 - item
29:51 - well maybe not sucks but maybe a t-shirt
29:54 - in a linear or sequential search we
29:56 - could technically transpose transposo
29:59 - those terms you would have to go through
30:01 - each bill to find that 500 same with an
30:03 - array of say six random integers
30:07 - if we were wanting to find the eight we
30:09 - we would have to manually check each
30:12 - element in order to return the position
30:14 - of the eight
30:15 - algorithmically it would look like this
30:18 - and can you imagine if we had an array
30:20 - containing a large amount of numbers it
30:22 - would be very very time consuming and
30:25 - inefficient to use a linear search
30:27 - because it would be comparing each
30:29 - integer one by one
30:31 - now can it be used for searching sure
30:34 - but it's only when the numbers are
30:36 - relatively few and relatively sorted but
30:39 - hey let's code this out anyway we'll
30:42 - define our function with two parameters
30:44 - our array and the element that we're
30:46 - searching for then we'll write a for
30:48 - loop to iterate over the length of our
30:49 - array and if there is an element there
30:51 - that's the same as our element then just
30:54 - return where it is in the array when we
30:56 - print it out we get index position 4 as
30:59 - that is where our 10 is
31:05 - a more efficient way to search is to
31:07 - move up to a binary search and the clue
31:09 - is in the name binary as in by meaning
31:12 - something that has two parts
31:14 - in this search algorithm we would create
31:16 - two pointers and repeatedly divide the
31:18 - search interval by half naturally this
31:21 - would reduce our search space so that
31:23 - we're not searching the whole array but
31:25 - only the divided parts
31:28 - the catches at the binary search
31:30 - is that it is usually mostly
31:33 - generally
31:34 - done on sorted arrays that looks like
31:37 - this
31:38 - and this way the algorithm checks
31:39 - whether the middle element is equal to
31:41 - the target that we're searching for in
31:43 - every iteration if the target value
31:45 - matches the element then its position in
31:48 - the array is returned if the if the
31:50 - target value is less than the element
31:52 - the search continues in the lower half
31:54 - of the array the target value is greater
31:57 - than it searches in the upper half of
31:59 - the array ultimately the algorithm
32:01 - doesn't need to search the half our
32:03 - target isn't in with each iteration
32:06 - now let's look at some code
32:09 - because our program is only searching
32:11 - half the array it makes it a much better
32:14 - search
32:15 - let's look at the iterative method first
32:17 - in our function we can create four
32:19 - parameters our array a start point an
32:23 - endpoint
32:24 - and the target that we're searching for
32:26 - we'll create a while loop to iterate
32:28 - over our array from start to end and
32:31 - then create our midpoint our variable
32:33 - called mid is the magic in the sauce we
32:36 - only want to traverse the half that
32:38 - contains our element feel free to print
32:40 - out the mid when you run the whole
32:42 - program for yourself to get a good idea
32:44 - of this
32:45 - next we can implement our if statements
32:48 - we'll only need three
32:50 - check if our target is in the upper half
32:52 - then reassign our start to begin moving
32:55 - to the right of the array
32:57 - if our target is lower than our mid we
33:00 - reassign our end to begin moving to the
33:02 - left of the array if our target matches
33:05 - our mid then just return it easy peasy
33:09 - then we just return our start or we can
33:11 - return something like negative one to
33:13 - print out some strings to make it a
33:15 - little fancy
33:16 - but either way you prefer to have your
33:18 - results printed we have to be sure to
33:20 - declare our start and end in the
33:22 - function call our start is zero from the
33:24 - beginning of the array and our end is
33:26 - the last element
33:28 - our length of array minus one
33:35 - for our recursive function i won't spend
33:38 - a lot of time here because the
33:39 - difference between our iterative program
33:41 - and the recursive program is very
33:43 - minimal runtime and memory space is very
33:46 - very close meaning they're both really
33:48 - efficient
33:49 - either way it's good to see both
33:51 - programs
33:52 - basically all we're doing is just
33:54 - calling back to the original function
33:57 - within our else statements
33:58 - feel free to pause the video for a
34:00 - moment and take a closer look
34:06 - okay now that we have some super basic
34:10 - searches under our belt let's move into
34:12 - a deeper section of sorting because well
34:16 - sorting is fun
34:18 - who doesn't like to sort money piles of
34:20 - 50s here piles of honey is there
34:24 - holy crap i sound like mr krabs
34:26 - all right all right let's mix the money
34:28 - references let's get to sorting our
34:30 - boring old elements
34:33 - as for sorting there are a vast array of
34:36 - techniques
34:38 - that are available and here's a little
34:40 - quick list
34:43 - actually
34:44 - it's not that quick as there are many to
34:46 - choose from
34:53 - bubble sort is the simplest sorting
34:55 - algorithm that works by repeatedly
34:57 - swapping the adjacent elements if they
34:59 - are in the wrong order for instance the
35:02 - largest integer bubbles up to the top or
35:05 - iterates its way to the end of the array
35:08 - through a comparison of each element
35:10 - although this isn't a very efficient
35:12 - algorithm perhaps the worst it's still
35:14 - an important concept to understand the
35:17 - only real perk the bubble sort has
35:19 - compared to other algorithms is its
35:21 - built-in ability to detect whether or
35:23 - not an array is sorted in the first
35:25 - place in a best case scenario the list
35:28 - is already sorted or nearly sorted and
35:31 - this is in contrast to other algorithms
35:33 - even those with better than average
35:35 - performance whose entire sorting process
35:37 - caused more causes more inversions
35:39 - reversals reversals or swaps to happen
35:42 - an optimized algorithm for the bubble
35:44 - sort is to find the nth largest element
35:46 - and put it at the end of the array there
35:49 - is an unoptimized version that you can
35:51 - check out for comparison purposes if you
35:53 - want to look that up on your own you're
35:54 - more than welcome to do that but the
35:56 - optimized version is done so that the
35:58 - inner loop can avoid having to look all
36:00 - the way at the end when it's running the
36:02 - last iteration it's just saving time and
36:04 - energy by not having to do that although
36:07 - bubble sort is one of the simplest
36:09 - sorting algorithms to understand and
36:11 - implement its performance means that
36:14 - it's that its efficiency decreases
36:16 - dramatically on arrays of larger amounts
36:19 - of elements and it's even been argued
36:21 - that it shouldn't even be taught anymore
36:23 - as it's outdated and is even hazardous
36:27 - to some cpus
36:29 - yikes i bet you didn't realize that
36:31 - you'd be entering dangerous territory
36:33 - here i mean who knew conan could be
36:35 - fraught with such tenuous escapades such
36:38 - as this
36:39 - how sexy
36:40 - let's code it
36:46 - our function will take just one
36:47 - parameter our array
36:49 - and then i implemented an iteration
36:51 - count just for informational purposes
36:54 - next we create a for loop to iterate
36:57 - over our indices in the array going to
36:59 - the right combined with a nested loop
37:02 - inside that iterates over our indices
37:04 - going left
37:06 - or reverse that we're going to call j
37:10 - i strongly encourage you to write this
37:12 - program for yourself and within our
37:14 - nested loop print out j just to see it
37:17 - in action
37:19 - we have our iteration counter when this
37:22 - prints you'll see why i added this now
37:25 - we just tell it to swap elements if they
37:27 - are lower than the one to the left in
37:29 - our if statement if our element to the
37:31 - left is greater then swap it
37:34 - then we just return our newly sorted
37:36 - array so let's print this so you can see
37:39 - our iteration count
37:40 - with 36 iterations that's pretty
37:43 - efficient
37:44 - check this out now i know i said that it
37:47 - wasn't going to look at the unoptimized
37:49 - but guys check this out double the
37:52 - iterations for two functions and it's
37:55 - double the code
37:57 - i liked our first program
37:58 - simple shorter faster
38:02 - unstable
38:07 - oh
38:08 - that was dangerous i kind of like it but
38:11 - hey if dangerous not your thing you
38:14 - thrive in a more stable and secure
38:15 - environment that's fine too we can
38:17 - totally move on to the insertion sort
38:20 - algorithm insertion sort is another
38:22 - simple sorting algorithm that builds the
38:24 - final array one item at a time
38:26 - it's still inefficient on large lists
38:29 - like the bubble sore but it can have
38:31 - some advantages such as simple
38:33 - implementation meaning a reduced amount
38:36 - of code and performs well in small data
38:38 - sets and relatively sorted arrays and it
38:41 - doesn't require much space meaning that
38:43 - it can be done in place therefore not
38:45 - using much computer memory in order to
38:48 - perform well the concept of the
38:50 - insertion sort is to split the array
38:52 - into two sections in order to compare
38:54 - its elements we can consider one side
38:57 - sorted and the other side unsorted our
39:00 - algorithm takes one element at a time
39:03 - from the unsorted side and compares it
39:05 - to our elements on the sorted side
39:07 - if our element is larger or smaller it
39:10 - places it where it needs to be on our
39:12 - sorted side each generation takes an
39:15 - element from the unsorted side and
39:17 - places it into our sorted side all the
39:19 - way till there are no more elements to
39:21 - compare to
39:22 - and in the end we are left with a sorted
39:24 - array
39:27 - for our program our parameter in our
39:30 - function call will be our array starting
39:32 - with a for loop that will iterate
39:34 - starting at our second element we don't
39:37 - need to start at our first integer
39:40 - because that's our sorted side
39:43 - we leave that alone for now we create
39:45 - two variables our key which is our
39:48 - current element and a pointer to the
39:50 - left of our key
39:52 - we'll implement a while loop to iterate
39:54 - as long as our index is greater or equal
39:57 - to zero and our index is greater than
40:00 - our key
40:02 - we then program it to reassign the
40:04 - larger one to the right position and
40:06 - then move down and put our small element
40:09 - to the left
40:10 - we do this all in place without creating
40:13 - extra space
40:15 - it's fairly simple and straight to the
40:17 - point
40:20 - working with arrays can last for days
40:26 - you like that no but seriously what
40:28 - we've touched on is only the tip of the
40:30 - iceberg as a personal challenge to
40:32 - yourself try searching and sorting a
40:34 - two-dimensional array are you up to the
40:37 - challenge
40:45 - for our advanced level algorithm section
40:48 - we're going to dive into linked lists a
40:51 - linked list is a linear data structure
40:53 - in which the elements are are not stored
40:55 - at contiguous memory locations
40:58 - remember this picture
41:00 - also remember that arrays are stored at
41:02 - contiguous memory locations
41:05 - a wikipedia definition states a linked
41:08 - list is a linear collection of data
41:11 - elements whose order is not given by
41:12 - their physical placement in memory
41:15 - instead each element points to the next
41:18 - it's a data structure consisting of a
41:21 - collection of nodes which together
41:23 - represent a sequence in its most basic
41:26 - form each node contains data and a
41:28 - reference in other words a link to the
41:30 - next node in the sequence
41:33 - this structure allows for efficient
41:35 - insertion or removal of elements from
41:38 - any position in the sequence during
41:40 - iteration
41:41 - more complex variants and additional
41:44 - links allowing more efficient insertion
41:46 - or removal of nodes at arbitrary
41:48 - positions a drawback of linked lists is
41:51 - that access time is linear and difficult
41:53 - to pipeline
41:55 - some say that linked lists are among the
41:58 - simplest and most common data structures
42:02 - are they though
42:03 - as an example
42:05 - think of a photo storage website if you
42:07 - click on the website landing page this
42:10 - would be considered your header
42:12 - then
42:13 - when you click on the first image this
42:15 - would become a node
42:17 - the arrow next to it that you can click
42:20 - to see the next picture would be your
42:22 - pointer
42:23 - now if you get to the third or fourth
42:25 - picture and want to go back to the first
42:28 - picture
42:29 - and look at that again
42:30 - those back arrows would mean that you're
42:32 - on a doubly linked list
42:34 - a doubly linked list means that you can
42:36 - traverse the list forward and backward
42:40 - however for this lesson we're only going
42:42 - to cover a list that we can go forward
42:44 - on the singly linked list
42:47 - now the upside of a linked list over a
42:49 - conventional array is that the list
42:51 - elements can be easily inserted or
42:53 - removed without re allocation or
42:56 - reorganization of the entire structure
42:58 - because the data items do not need to be
43:01 - stored contiguously
43:02 - linked lists allow insertion and removal
43:05 - of nodes at any point in the list and
43:07 - they keep up with a constant number of
43:09 - operations by storing the link previous
43:12 - to any link being added or removed
43:14 - during a list traversal so let's talk
43:17 - about our four basic algorithms or
43:20 - common problem solving operations for
43:22 - singly linked lists
43:24 - every programmer needs to be able to do
43:26 - the following traverse a linked list or
43:29 - be able to travel along the nodes and
43:31 - pointers
43:32 - search a list for the header a node or
43:35 - tail a tail is the last and final node
43:38 - insert a new header note or tail
43:40 - and to delete a note or tail a link to
43:43 - list generally always needs a header
43:46 - let's create a simple linked list
43:48 - representing a small family member group
43:51 - we want to be able to do all the
43:53 - operations mentioned above this will
43:55 - require a little bit of object oriented
43:57 - programming so hopefully you are
43:59 - somewhat familiar with that but if not
44:01 - that's okay we'll stay simple and make
44:03 - it easy peasy
44:05 - we need our pretty standard class node
44:08 - and class linked list to start our nodes
44:11 - will be our family members and our
44:13 - linked list will represent our whole
44:15 - family this can be considered a little
44:17 - backwards but i like to instantiate our
44:19 - objects first so it's easier to see what
44:20 - we're dealing with being able to
44:22 - visualize what we're doing is an
44:24 - important skill in programming and
44:26 - design because it helps us think about
44:28 - the big picture throughout the solution
44:31 - let's picture our knowns as a family of
44:33 - mom dad and two kids we'll create our
44:36 - family head as bob his wife amy our
44:39 - header will point to amy as the next
44:41 - node and then our two little nodes max
44:44 - and jenny they all need to point to each
44:46 - other as they will be linked together in
44:48 - our list so we will make sure our
44:50 - pointers are created which is
44:52 - object.next
44:54 - everything looks good and we have all
44:56 - our members individually as nodes but to
44:59 - really see our algorithm in action we
45:00 - need to link all our members together in
45:02 - our linked list where we can operate on
45:05 - that list and design them to be really
45:07 - complex or really simple
45:09 - linked list problems are often used as
45:11 - interview and exam questions so knowing
45:13 - how to create linked lists how to
45:15 - traverse them how to add and take away
45:17 - elements is very important once you get
45:20 - really familiar with those concepts you
45:22 - can move into learn learning how to
45:24 - traverse how to reverse a list swap
45:27 - elements maybe even go on to doubly
45:29 - linked lists or circular
45:31 - it's a really big topic to explore and
45:33 - fun too and if you're not having fun
45:36 - why are why are you here like why
45:41 - so back to our family
45:43 - let's get them created and then set it
45:45 - up to where we can traverse this family
45:48 - list
45:49 - once we have established our class we
45:51 - just need to implement our functions
45:53 - that tell our linked list class what to
45:55 - do
45:56 - this is where we implement all the
45:58 - things we need done with our nodes and
46:00 - the first thing we're going to do is
46:02 - make our nodes by creating the data and
46:04 - a pointer
46:05 - let's create our objects
46:09 - then create pointers that go to each
46:11 - family member
46:13 - now that we have our nodes we can now
46:15 - link them in a list by creating our
46:17 - linked list class this is where we
46:20 - create a function to traverse our nodes
46:23 - in a list and be able to print them all
46:24 - out
46:27 - we only need one argument self because
46:30 - it doesn't need any other value or input
46:32 - to be to be able to do what we need
46:35 - which is starting at our header and
46:36 - printing them all out next by next by
46:39 - nuts
46:40 - then we just call family.traversal
46:44 - and out pops our family members now if
46:47 - you want to shoot these out in a
46:48 - particular way such as in a list or
46:50 - fancy strings that would have to be done
46:52 - within another function in the linked
46:54 - list class which unfortunately is not
46:58 - within the scope of this course that's
47:00 - more in tune with object oriented
47:02 - programming a whole other separate topic
47:05 - the goal here is to focus on being able
47:07 - to operate or solve problems to creating
47:10 - simple function calls on linked lists
47:12 - now our family member list is looking
47:14 - pretty good but word on the street is
47:17 - that bob isn't treating amy too well and
47:20 - he's been running around on her
47:22 - from the looks of it bob might be
47:24 - packing his bags and leaving the family
47:26 - but it's not all bad because dave has
47:28 - been hanging around a bit and although
47:30 - he's bob's brother he's a really good
47:33 - guy
47:34 - i know i know it sounds really back once
47:38 - but seriously dave is going to step up
47:40 - to the plate and he's looking forward to
47:42 - being a dunkle to the kids
47:44 - you know you know a dunkle like when
47:46 - your uncle becomes your dad and it's not
47:48 - weird at all
47:49 - so for our list we need to put dave as
47:52 - the new header position and because this
47:54 - is a computer and it can't do anything
47:56 - that we don't tell it to do we need to
47:59 - implement some functions
48:01 - let's call it insert new header
48:04 - and this is going to fall under our
48:06 - insertion algorithm
48:08 - we need to know how to insert elements
48:10 - into the list and not only that we have
48:12 - to tell it where to insert for davis the
48:14 - new head we are going to put him at the
48:16 - front of the list
48:18 - so for our function apart from self we
48:21 - need new data or information to be added
48:23 - to the parameter since we are adding a
48:25 - new name
48:26 - our new name is our new node so we will
48:29 - need our node class
48:31 - remember the nodes are our family
48:33 - members then all we need to do is assign
48:35 - him as the next new node to the
48:37 - self.head and then assign yourself.head
48:40 - to the new node let's traverse the list
48:43 - now
48:51 - oh boy bob is still in the picture and
48:54 - you know what amy can't have her new
48:56 - hubby under the same roof as her ex that
48:59 - is just not gonna work this is a little
49:01 - two backwoods so bob needs to
49:04 - go go
49:05 - creating our new header simply pushed
49:08 - all our nodes down so we need to get bob
49:10 - out of there let's create a function
49:12 - that deletes a node however we can't
49:15 - just delete some rando node we have to
49:18 - tell our computer where bob the node is
49:21 - in the first place
49:22 - this means we have to search our family
49:25 - list so let's create a search search
49:27 - function with one parameter x
49:31 - see what i did there pretty clever we
49:33 - need to know if bob is even in there to
49:34 - delete so in order to search we need to
49:36 - start at the beginning of the list
49:39 - let's create a temp variable assigned to
49:41 - the self.head and make a while loop that
49:44 - will traverse as long as that list or
49:46 - header isn't empty if any node or temp
49:49 - node isn't equal to x or whatever it is
49:52 - we're searching for we want to return
49:54 - true or else return false
49:56 - then we need to assign our temp to the
49:58 - next node because in a while loop it
50:01 - will need to continue until the node is
50:03 - not is none then it will return our pool
50:07 - great let's find out if bob is in there
50:09 - by printing our search function uh oh
50:12 - it's true bob's still there and he's
50:15 - giving amy a hard time about leaving
50:17 - let's help her out and create a delete
50:19 - function
50:21 - this is going to be similar to our other
50:23 - functions where we need to put our data
50:25 - in the parameter and begin at the head
50:27 - of our list
50:28 - we need our if else statements and we
50:31 - will
50:32 - and we will say bob isn't there then
50:34 - tell us and if he is let's kick him out
50:37 - we do this by changing the pointers from
50:39 - the previous node and pointing it to the
50:40 - node after bob so in essence we're
50:43 - skipping over bob but watch what happens
50:46 - when we try to try to delete amy
50:49 - bob will come back
50:51 - why does this happen
50:53 - because our list has four nodes
50:56 - as created here
50:58 - and those nodes will always be there
51:00 - unless we go in and manually delete the
51:03 - object itself
51:05 - so far we've traversed our list searched
51:07 - our list inserted a new header deleted a
51:10 - node and last but not least we're going
51:12 - to delete our tail
51:14 - yep that would be poor jenny
51:16 - she's had enough of these adult
51:18 - shenanigans and she's out of here
51:20 - heading to new york to become the
51:22 - greatest fashion editor that vogue has
51:24 - ever known she's ambitious okay
51:26 - we started the head and as long as the
51:29 - temp dot next next is not none our
51:32 - pointer is two nodes ahead we make our
51:34 - temp the next node and set it to none
51:37 - voila
51:38 - best of luck jenny
51:44 - so there you have it some very basic
51:46 - foundational algorithms for linked lists
51:49 - which in itself is a complicated subject
51:52 - but if you get these classical concepts
51:54 - down solid it becomes easier and easier
51:56 - to build up from on your own once you're
51:59 - more comfortable with singly linked
52:01 - lists try to challenge yourself with
52:04 - doubly linked lists with pointers that
52:06 - point back who knows maybe bob will
52:08 - shape up and amy will go
52:11 - or point get it
52:14 - back to bob
52:21 - for our mind breakers section in data
52:24 - structures we're going to be looking at
52:27 - hash tables right off the rib i'm going
52:29 - to tell you the key phrases or terms you
52:31 - need to be familiar with when it comes
52:33 - to hash tables associative arrays
52:36 - hash functions
52:38 - key value pairs collision
52:41 - and chaining
52:43 - a hash table consists of an array of
52:45 - pockets or buckets where each stores a
52:48 - key value pair in order to locate the
52:50 - pocket where a key value pair should be
52:52 - stored the key is passed through a
52:54 - hashing function
52:56 - this function returns an integer which
52:58 - is used as the pairs index in the array
53:01 - of buckets when we want to retrieve a
53:03 - key value pair we supply the key to the
53:06 - same hashing function receive its index
53:08 - and use the index to find it in the
53:10 - array
53:11 - associative arrays can be implemented
53:14 - with many different underlying data
53:16 - structures and it refers to an array
53:18 - with strings as an index rather than
53:20 - storing element values in a strict
53:22 - linear index order this stores them in
53:25 - combination with key values multiple
53:28 - indices are used to access values in a
53:31 - multi-dimensional array which contains
53:33 - one or more arrays a non-performant one
53:37 - can be implemented by simply storing
53:39 - items in an array and iterating through
53:41 - the array when searching remember the
53:43 - wallet with lots of cash
53:46 - associative arrays and hash tables are
53:48 - often confused because associative
53:50 - arrays are so often implemented as hash
53:52 - tables according to wikipedia
53:55 - a hash table uses a hash function to
53:58 - compute an index also called a hash code
54:01 - into arrays of buckets or slots from
54:03 - which the desired value can be found
54:06 - during lookup the key is hashed and the
54:09 - resulting hash indicates where the
54:11 - corresponding value is stored
54:13 - in many situations hash tables turn out
54:16 - to be on average more efficient than
54:18 - search trees or any other table lookup
54:20 - structure for this reason they are
54:22 - widely used in many kinds of computer
54:25 - software particularly for associative
54:27 - arrays database indexes caches and sets
54:31 - to summarize we can use hash tables to
54:34 - store retrieve and delete data uniquely
54:37 - based on their unique key
54:39 - the most valuable aspect of a hash table
54:42 - over other abstract data structures is
54:44 - its speed to perform insertion deletion
54:47 - and search operations
54:49 - hash tables can do
54:51 - all of them in constant time
54:53 - for those unfamiliar with time
54:54 - complexity big o notation constant time
54:58 - is the fastest possible time complexity
55:01 - hash tables can perform nearly all
55:03 - methods except list very fast in o b o
55:07 - one time
55:08 - hash tables have to support three
55:11 - functions
55:12 - insert for key value
55:14 - get
55:15 - key
55:16 - or delete a key
55:18 - collision happens when there could
55:21 - possibly be two or more pieces of data
55:23 - in
55:24 - data set x that will hash to the same
55:26 - integer and data set y
55:28 - this means that whenever two keys have
55:30 - the same hash value it's considered a
55:33 - collision
55:34 - what should our hash table do if it just
55:36 - wrote the data into the location anyway
55:38 - we would be losing the object that is
55:41 - already stored under a different key
55:43 - ideally we want our hash values to be
55:45 - evenly distributed among our buckets as
55:48 - possible to take full advantage of each
55:50 - bucket and avoid collisions
55:53 - but generally speaking this is unlikely
55:55 - so this is where two common methods to
55:58 - solve are employed separate chaining
56:00 - using linked lists or binary search
56:02 - trees for example and local addressing
56:05 - using linear probing
56:08 - with some basic concepts of hash tables
56:10 - under your belt a fun exercise or
56:13 - challenge that you can try would be
56:15 - pattern matching on strings using the
56:17 - robin carp algorithm
56:19 - yes it's fairly advanced and yes this is
56:22 - our mind breaker problem but shoot for
56:25 - the stars people who knows you might
56:27 - just surprise yourself
56:31 - what have we learned what have we
56:33 - learned from lesson two
56:35 - okay
56:36 - we touched on a one-dimensional raise as
56:39 - our data structure of choice at our
56:41 - basic level
56:42 - we looked at contiguous versus
56:45 - non-contiguous different ways to search
56:48 - linear
56:49 - binary
56:50 - and we sort bubble insert and even
56:53 - talked about an array with more than one
56:55 - dimension we learned about dunkles
56:58 - no no no no we learned about link list
57:01 - by creating a family of sorts to
57:03 - demonstrate how to traverse search add
57:06 - and delete headers
57:08 - nodes tails on linked lists some of the
57:10 - most common operations used that every
57:12 - programmer should be familiar with
57:15 - we broke our heads on hash tables and
57:18 - hashing functions by going over what
57:20 - they are and what can happen when
57:21 - problems arise from colliding data
57:24 - within a bucket ultimately hash tables
57:27 - are for storing retrieving and deleting
57:30 - data based on key value pairs and on
57:32 - average are more efficient than search
57:34 - trees or any other table lookup
57:36 - structure
57:40 - [Music]
57:50 - i learned about the divide and conquer
57:52 - concept from a very early age
57:55 - it was traumatizing actually you see my
57:57 - brother and i used to fight all the time
57:59 - when we'd swim in the pool and finally
58:01 - one day our mom came and said to us just
58:04 - divide it in half and you stand once and
58:06 - you stay on another side
58:08 - my brother picked the top half
58:10 - so yeah divide and conquer
58:12 - i drowned but only momentarily
58:15 - in programming when we cross paths with
58:18 - programs that are too complicated to be
58:19 - solved all at once we can implement
58:22 - methods to break them down into smaller
58:24 - pieces and if we solve all the smaller
58:26 - pieces we are in essence dividing out
58:28 - the big
58:29 - solving the small and in the end this
58:31 - makes our lives so much easier a divide
58:34 - and conquer algorithm paradigm can solve
58:36 - a large problem by recursively breaking
58:39 - it down into smaller sub-problems until
58:42 - they become simple enough to be solved
58:43 - directly it could be compared to the
58:45 - age-old problem of swallowing an
58:47 - elephant
58:48 - can't do that so we devour it one bite
58:51 - at a time
58:52 - actually it's more of a solution of
58:54 - three bytes dividing solving and then
58:56 - merging it all back together
58:58 - unfortunately since divide and conquer
59:00 - algorithms are usually implemented
59:02 - recursively this means they require
59:04 - additional memory allocation and without
59:07 - sufficient memory a stack overflow error
59:10 - can or will occur
59:12 - this is due to the overhead of the
59:14 - repeated subroutine calls along with
59:17 - storing the call stack the state of each
59:19 - point in the recursion and that can
59:21 - outweigh advantages of any approach
59:24 - on the flip side if the dnc solutions
59:26 - are implemented by a non-recursive
59:28 - algorithm that stores the partial sub
59:30 - problems in some explicit data structure
59:32 - like a stack queue or priority queue it
59:35 - can allow more freedom in the choice of
59:37 - the sub-problem that is to be solved
59:39 - next a feature that is important in some
59:41 - applications including breath first
59:44 - recursion and the branch and brown
59:46 - branch and bound method
59:48 - for function optimization
59:51 - second even if a solution to the full
59:53 - problem is already known like sorting a
59:56 - brute force comparison of all elements
59:58 - with one another divide and conquer can
60:00 - substantially reduce computational cost
60:03 - lower cost means higher efficiency
60:06 - third explicitly recursive subdivision
60:09 - can have benefits to the temporary
60:11 - memory of the computer not the main hard
60:13 - drive
60:14 - which is called a cache
60:16 - using divide and conquer one can even
60:18 - design an algorithm to use all levels of
60:20 - the cache hierarchy in such a way as to
60:23 - have an algorithm that is optimally
60:25 - cached oblivious which means operating
60:28 - independently from its temporary memory
60:30 - storage
60:31 - this approach has been successfully
60:33 - applied to many problems including
60:36 - matrix multiplication which we're going
60:38 - to get into in just a few minutes
60:40 - so let's get into it and start at the
60:43 - base level algorithm for divide and
60:44 - conquer the merge sort
60:51 - conceptually merge sort works as follows
60:55 - divide the unsorted list into two sub
60:58 - lists about half the size
61:00 - sort each of the two sub lists
61:03 - merge the two sorted sublists back into
61:06 - one sorted list
61:08 - invented by john von neumann in 1945 the
61:11 - merger sort is an efficient general
61:13 - all-purpose and comparison-based sorting
61:15 - algorithm an example of merge sort would
61:18 - be to first divide the list into the
61:19 - smallest unit one element this becomes a
61:23 - list itself
61:24 - then compare each element which is a
61:26 - list itself remember with the adjacent
61:29 - list to sort and merge them done
61:31 - repetitively the end results in the
61:33 - elements sorted and merged
61:36 - i understand that can be a little bit
61:37 - confusing but let's think of it in this
61:40 - way
61:41 - say we have two different lists of
61:43 - numbers list a and list b and then we'll
61:46 - have an empty list we'll call c
61:49 - lists a and b are ordered from least to
61:52 - greatest and what we would do is
61:53 - repeatedly compare least value min of a
61:57 - to the least value min of b and remove
62:01 - the lesser value and then append that
62:03 - value into c
62:04 - when one list is exhausted we append the
62:07 - the remaining items into c they're
62:10 - already started so we're good there
62:12 - then we just return list c which is
62:14 - sorted we're systematically merging a
62:16 - and b together and appending it into c
62:20 - but for this lesson and although you can
62:22 - append or extend on an empty list to see
62:26 - we're going to swap our elements i just
62:28 - wanted to share a very simplistic
62:29 - explanation for that hoping that would
62:32 - give you an idea of the concept
62:34 - typically creating more space for an
62:36 - empty list or temporary list for storage
62:39 - isn't always the best approach we want
62:41 - we want to do our best to optimize our
62:43 - program if we can
62:46 - let's head into our ides and write our
62:48 - programs
62:53 - the best way to see the different
62:55 - methods of the merge store and their
62:57 - efficiencies is to compare four programs
62:59 - that i have here
63:01 - some things i wrote myself
63:03 - some things i borrowed
63:05 - we're going to look at a single array of
63:07 - 10 integers and just sort it by brute
63:09 - force this means that the program takes
63:12 - the first number in index position 0 and
63:15 - compares it to each and every number in
63:17 - the array
63:18 - if it's the lowest it gets reassigned at
63:21 - the first position
63:23 - then it moves to the next number and
63:25 - does the same thing it gets compared to
63:28 - each and every integer and the lowest
63:30 - number gets reassigned at index position
63:33 - one and so on and so forth
63:35 - for this little iterative program it's
63:37 - 168 iterations or cycles to get an
63:40 - output and that's just for 10 elements
63:43 - keep that in mind as we look at the
63:45 - differences within the merge sort
63:47 - our first real program for the merge
63:49 - sort is about as simple as we can get it
63:52 - we're starting with an array that is
63:54 - already split in half and sorted the
63:56 - program we're looking at right now won't
63:58 - work properly if our two halves are not
64:01 - already sorted
64:02 - why
64:03 - because ultimately this is our divide
64:05 - and conquer algorithm we are dividing
64:08 - our array into two parts and conquering
64:11 - both halves separately by sorting them
64:14 - and then merging them back together
64:16 - remember just a few minutes ago when we
64:18 - talked about list a list b and empty
64:21 - list c
64:22 - this program is comparing each number at
64:24 - each index position determining which is
64:27 - lower and then popping it out and then
64:29 - appending it into our empty list c
64:32 - and
64:33 - then just in case either list is longer
64:36 - than the other that leftover portion
64:38 - just gets added to c and since it's
64:41 - already sorted we'll have no problems
64:43 - there
64:44 - this is a neat little non-recursive
64:46 - merge sort for an array that has a small
64:48 - amount of elements and is already
64:50 - divided in half however since this isn't
64:53 - a perfect world and you'll be more than
64:55 - likely asked to write a program for an
64:57 - array that isn't divided and sorted
64:59 - beforehand
65:00 - we're going to have to rewrite this
65:03 - so here's our big dilemma do we sort our
65:06 - halves recursively or do we sort them
65:08 - iteratively
65:09 - or do we sort our halves using python's
65:12 - built-in feature called sort or sorted
65:17 - let's shoot for sorting our halves
65:18 - recursively this is what is called the
65:21 - top-down method
65:23 - what it does is repeatedly produce
65:25 - sub-lists half by half by half until
65:27 - there's only one sub-list or one element
65:30 - remaining which becomes a list all in
65:33 - itself a list of one element how many of
65:36 - you are willing to but it's the optimal
65:38 - or optimized method
65:41 - well it's not
65:44 - once we have our have sorted we continue
65:46 - on very much like before we compare each
65:49 - element and append it into our empty
65:51 - list
65:52 - now if recursion isn't the optimal
65:55 - method then it must be iteration right
65:58 - well yes
66:00 - mostly this is the bottom up method
66:03 - think of the top down just in reverse it
66:05 - starts at one element then two elements
66:08 - then four remember that very first brute
66:11 - force method of sorting a list we can do
66:14 - that after we divide the array into two
66:16 - parts and then merge it all back
66:18 - together hmm
66:20 - let's see what happens when we run it in
66:22 - something like leeco that'll really put
66:24 - it to the test
66:27 - [Music]
66:51 - okay well that didn't go as planned but
66:54 - it does work it's just bad on really big
66:57 - arrays
66:58 - so what is the bust
67:00 - using python's awesome sort or sorted
67:03 - feature of course
67:05 - well splitter array have python sorted
67:07 - for us and the rest is good to go with
67:10 - our 10 element array it's only 34 steps
67:13 - to completion compare that to our brute
67:15 - force program no dividing no conquering
67:18 - with almost 170 iterations our bottom up
67:21 - iterative method is smoking
67:24 - check it in leak code
67:26 - talk about optimization
67:28 - now let's move out into deeper waters
67:31 - with some matrix multiplication
67:41 - matrix multiplication is wild y'all
67:45 - but it is a fundamental concept to
67:47 - understanding computing among the most
67:49 - common tools in electrical engineering
67:51 - and computer science are rectangular
67:53 - grids of numbers known as matrices the
67:56 - numbers in a matrix can represent data
67:58 - and they can also represent mathematical
68:00 - equations
68:01 - in many time sensitive engineering
68:03 - applications multiplying matrices can
68:06 - give a quick but good approximations of
68:08 - much more complicated calculations did
68:11 - you know that one of the areas of
68:12 - computer science in which matrix
68:14 - multiplication is particularly useful is
68:17 - graphics it's crazy to think that a
68:19 - digital image is basically a matrix with
68:22 - rows and columns of another matrix
68:25 - corresponding to rows and columns of
68:27 - pixels and the numerical entries
68:29 - correspond to a pixel color value
68:32 - decoding digital video for instance
68:35 - requires matrix multiplication and did
68:37 - you know that mit researchers were able
68:40 - to build one of the first tips to
68:42 - implement a high efficiency video coding
68:44 - standard for ultra high definition tvs
68:47 - partly because of patterns they
68:48 - discerned in the matrices
68:50 - pretty cool huh
68:52 - so the dimensions are listed as rows and
68:55 - columns or in many equations n times m
68:58 - where n represents the number of rows
69:00 - and n represents the number of columns
69:03 - be careful that you that you know the
69:05 - difference between rows and columns and
69:07 - don't mix them up rows go across columns
69:10 - go up and down to see how matrices are
69:12 - multiplied check out this graphic that
69:15 - shows how it all works we multiply each
69:18 - element in the row by each element in
69:20 - the column and then we add them together
69:22 - in order to code this we'll start with
69:24 - the naive method it's not too bad to
69:26 - learn however just know that it can get
69:28 - very expensive as we increase the size
69:31 - of our matrices for larger matrix
69:33 - operations
69:34 - we wouldn't want to use too many nested
69:36 - loops because it could really start
69:38 - bogging down
69:39 - ideally we would want to use optimized
69:41 - software packages like numpy
69:44 - numpy should be the go-to which is
69:46 - insanely faster and easy too but before
69:49 - we start cheating we really need to
69:52 - examine what's going on underneath the
69:54 - hood like how a program is created that
69:56 - multiplies matrices in python
69:59 - because if we can get a really good
70:01 - handle on this
70:02 - we'll get a better handle on the mind
70:05 - breaker
70:06 - [Music]
70:12 - okay so what we're doing is basically
70:14 - creating a calculator in python that
70:16 - multiplies matrices and you might be
70:19 - wondering hey what's the deal here how
70:21 - is this part of dividing and conquering
70:24 - algorithms we're definitely going to get
70:26 - into that but this is all preparation
70:28 - for the mind breaker lesson you'll need
70:31 - this under your belt first plus it's a
70:33 - really good foray into matrices which is
70:36 - one of the most well studied problems in
70:38 - numerical computing today so yeah let's
70:41 - dig in
70:42 - we'll create two matrices x and y and to
70:46 - store the results of our products we'll
70:48 - create an empty matrix we'll call result
70:51 - the most popular method in my humble
70:54 - opinion is to use nested loops to
70:56 - iterate over the rows and columns and
70:58 - implement the results into our matrix of
71:01 - zeros using an addition operator that
71:04 - adds our product of the rows
71:06 - and the columns of x and y
71:08 - we could do this recursively sure but it
71:11 - wouldn't really be advantageous the goal
71:14 - is to help you understand the logic
71:16 - behind matrix multiplication so we can
71:18 - move on to our mind breaker lesson
71:20 - strassen's algorithm for matrix
71:22 - multiplication
71:28 - volker strassen is a german
71:31 - mathematician who made important
71:32 - contributions to the analysis of
71:34 - algorithms and has received many awards
71:37 - over his lifetime he's still alive by
71:39 - the way
71:40 - he began his research as a probabilist
71:43 - his 1964 paper an invariance principle
71:47 - for the law of the iterated logarithm
71:49 - defined a functional form of the law of
71:51 - the iterated logarithm showing a form of
71:53 - scale scale and variance in random walks
71:57 - this result now known as strassen's
71:59 - environments principle or strassen's law
72:01 - of the iterated logarithm has been
72:03 - highly cited and led to the 1966
72:05 - presentation at the international
72:07 - congress of mathematicians
72:10 - now if you know what an iterative
72:12 - logarithm is that shows a form of scale
72:15 - and variance in random walks or in
72:17 - variance principles in general well then
72:20 - please by all means invite us to your
72:22 - latest book site
72:24 - but if you're a mere mortal like the
72:26 - rest of us
72:28 - hold on to your hats because that was
72:30 - only the beginning for strassen
72:32 - three years later strassen shifted his
72:34 - research efforts toward the analysis of
72:37 - algorithms with a paper on the gaussian
72:39 - elimination come on everybody knows what
72:42 - the gaussian elimination is
72:45 - and there in that paper he introduced
72:48 - the first algorithm for performing
72:50 - matrix matrix multiplication faster than
72:53 - the o and three time bound that would
72:55 - result from the naive method
72:58 - this means that his algorithm is like
73:01 - the best of the best when it comes to
73:03 - computing the product of two two by two
73:05 - matrices there is no formula that uses
73:08 - fewer than seven multiplications
73:11 - how he did it he doesn't say maybe
73:13 - somebody flew into german can call him
73:15 - up and ask him like what kind of person
73:18 - do you have to be to discover some great
73:20 - mathematical algorithm like what are
73:22 - your hobbies can we talk what do you do
73:25 - on the weekends i mean at this point at
73:27 - the time of recording this he's 85 so i
73:30 - don't know he probably just relaxes his
73:32 - brain muscles for the most part i mean i
73:34 - know i would they're probably still
73:36 - smoking anyway not to knock on this
73:39 - awesome person and as great as the
73:41 - algorithm is and the contribution it's
73:43 - made to matrix multiplication
73:45 - unfortunately it's too numerically
73:48 - unstable for some applications
73:51 - in practice fast matrix multiplication
73:54 - implementation for dense matrices use
73:56 - strawson's algorithm for matrix sizes
73:59 - above a breaking point then they switch
74:01 - to a simpler method once the subproblem
74:03 - size reduces to below the breakover see
74:06 - divide and conquer in any event the
74:08 - publication of its algorithm did result
74:10 - in much more research about multi matrix
74:13 - multiplication that led to faster
74:15 - approaches it opened the door to bigger
74:17 - and greater things which is pretty neat
74:20 - okay now that we've got that all under
74:22 - our belt let's take a deep dive into
74:24 - what the algorithm really does just a
74:27 - few minutes ago we saw the naive method
74:29 - so you should be somewhat familiar with
74:31 - it
74:32 - in two two by two matrices we have eight
74:35 - multiplications with stress since we can
74:37 - get this down to seven with basic
74:40 - mathematical formulas
74:41 - and i've gotten it all written out so
74:44 - you can see it all work in a program
74:46 - let's take a peek
74:50 - we're still going to keep our 2x2
74:52 - matrices for simplicity yay
74:55 - but we're going to look at our famous or
74:58 - infamous methods of optimization
75:01 - iteration and recursion for strassen's
75:03 - algorithm we're dividing our matrices
75:05 - not by halves but by quarters and then
75:08 - we'll solve or conquer
75:10 - each sub-matrix to solve the entire
75:12 - problem
75:14 - in our iterative method we're breaking
75:15 - down the two matrices all the way down
75:17 - to each individual square or quadrant
75:20 - we'll name our passes p and simply plug
75:23 - in our mathematical formula used to
75:25 - compute our multiplication and addition
75:27 - on the quadrants
75:29 - now although we called numpy cheating
75:32 - we're going to implement it here just
75:34 - for a little bit of ease by instead
75:36 - creating a result matrix of zeros we're
75:39 - going to make a new matrix called c and
75:42 - number each quadrant of c as c1 c2 c3
75:46 - and c4 for each quadrant we plug in our
75:49 - equations for our passes these equations
75:52 - are our conquered solutions to the
75:55 - quadrants
75:56 - lastly we use numpy to implement a new
75:59 - matrix by using the np dot array feature
76:03 - and input our four c quadrants
76:06 - pretty cool
76:12 - the recursive solution is very similar
76:14 - only we're recursively splitting the
76:16 - matrices into our quadrants and
76:18 - recursively computing our seven passes
76:33 - what did we learn what did we learn we
76:36 - learned that the merge store is an
76:38 - efficient general purpose sort that can
76:41 - be implemented in several ways including
76:43 - the top-down and bottom-up approach
76:46 - we learned that matrix multiplication is
76:49 - used in simple graphics all the way up
76:51 - to hd tvs and we wrote our own python
76:55 - program to calculate two matrices
76:58 - we looked at strassen's algorithm for
77:00 - matrix multiplication
77:02 - and learned that he discovered how to
77:04 - get the lowest amount of calculations
77:06 - possible for multiplying two matrices
77:09 - to this day computational efficiency of
77:12 - matrix multiplication is still a hot
77:15 - topic in theoretical computer science
77:21 - [Music]
77:31 - apparently if you google greediest
77:34 - person a real live person does pop up
77:37 - can you imagine like googling the worst
77:40 - person in the world and your face gets
77:42 - blasted on the screen
77:44 - now i'm not going to show who this
77:46 - person is because i don't know who he is
77:48 - and it's not really important just kind
77:50 - of funny but it does tie in with our
77:52 - algorithm
77:53 - what is greed to you like what is a
77:56 - greedy approach
77:58 - say your beautiful old granny baked a
78:01 - pie for your family and uncle bob shows
78:03 - up you know amy's ex-husband
78:06 - and he takes the biggest piece of pie
78:08 - for himself that'd be pretty greedy
78:10 - wouldn't it by him eating the largest
78:12 - portion that sure would make the pie
78:15 - easier to finish wouldn't it it wouldn't
78:17 - be great for everyone else or their
78:19 - rumbling tummies but by taking the
78:21 - biggest portion and leaving just little
78:24 - portions to take care of
78:26 - that's
78:27 - sort of the concept behind the algorithm
78:30 - the paradigm behind the green concept is
78:32 - that it builds up a solution piece by
78:35 - piece always choosing the next piece
78:37 - that offers the mo the obvious and most
78:40 - immediate benefit
78:41 - by using several iterations and by
78:43 - obtaining the best result at a certain
78:46 - iteration the results should be computed
78:48 - in other words it follows the
78:50 - problem-solving method of making the
78:52 - locally optimum choice at each stage
78:55 - with the hope of finding the global
78:57 - optimum something to keep in mind though
78:59 - is that they do not consistently find
79:01 - the globally optimum solution because
79:04 - generally speaking they don't operate
79:06 - exhaustively on all the data
79:08 - nevertheless they are useful because
79:11 - they are quick and often give good
79:13 - approximations to the optimum if a
79:16 - greedy algorithm can be proven to yield
79:18 - the global optimum for a given problem
79:20 - class it typically becomes the method of
79:23 - choice
79:30 - a really fun problem that we can lead
79:33 - off with is called assign mice to holes
79:36 - what we need to do is put every mouse to
79:38 - its nearest hole to minimize the time
79:41 - how can we do that and what's the
79:43 - optimal approach
79:45 - what would you think about sorting the
79:47 - positions of the mice and holes first
79:50 - that would be pretty greedy of us
79:52 - wouldn't it
79:53 - this allows us to put a particular mouse
79:56 - closest to the corresponding hole in the
79:58 - holes list we can then find the maximum
80:01 - difference between the mice and the
80:03 - corresponding hole position to determine
80:06 - how much time it will take to get there
80:09 - now does being greedy always equate to
80:12 - maximum of something regarding problem
80:14 - solutions not necessarily don't let that
80:17 - word fool you
80:18 - for this problem we're finding the
80:20 - maximum difference hint subtraction to
80:23 - find the minimum amount of time it takes
80:25 - for our mice to travel
80:27 - as a quick side note another problem
80:29 - that i would encourage you to learn on
80:31 - your own a triple dog dare you is the
80:33 - kids with greatest candies problem using
80:36 - the greedy approach
80:38 - this is considered an easy problem so
80:40 - don't worry too much the problem is
80:42 - similar to this one by asking for the
80:44 - distribution of the minimum number of
80:46 - candies that can be given to a kid
80:49 - and then seeing which kid will contain
80:51 - the maximum number of candies
80:54 - so again the greedy approach can be
80:57 - tricky we have to carefully decipher
80:59 - word usages to make sure we're solving
81:01 - the problem accurately
81:03 - but with that being said the assigned
81:05 - mice to holes problem will help set you
81:07 - on some solid ground and make it a bit
81:10 - easier to work on other challenges now
81:13 - if you do decide to tackle the candy
81:15 - problem and i do highly encourage you to
81:17 - do that i'll give you a little hint
81:19 - the brute force method would be a one by
81:22 - one distribution of candies to each
81:24 - child until the condition satisfies
81:27 - and yet the greedy way would be to
81:29 - traverse the array just twice from
81:31 - beginning to end and back again by
81:34 - greedily determining the minimum number
81:36 - of candies required by each child
81:38 - as a personal challenge to yourself work
81:41 - out how that could be written it'll be
81:43 - fun let's head over to our ides and slam
81:48 - dunk some mice into holes
81:51 - okay we need to get our mice into holes
81:54 - and we have to do it quickly but first
81:56 - things first we have to make sure that
81:58 - there are an equal amount of holes to
82:00 - our mice this is our base
82:03 - now
82:04 - we initialize our greedy approach by
82:06 - sorting the mice first then we create a
82:09 - variable to store our maximum difference
82:12 - and loop through the length of our mice
82:16 - in our if statement we create the
82:18 - condition that if the position of the
82:20 - mouse subtracted from the position of
82:23 - the hole is greater than the maximum
82:25 - difference
82:26 - our max diff variable gets reassigned to
82:29 - hold that difference
82:31 - lastly we just return the largest
82:33 - difference our last mouse gets to the
82:36 - hole in x amount of time
82:44 - by sorting our two lists first this puts
82:48 - us at an advantage to finding the
82:50 - minimum time faster
82:52 - our code is concise short and along with
82:55 - python's sort feature we are really
82:58 - rocking it out
83:00 - next up is kind of a doozy the
83:02 - fractional knapsack a pretty well known
83:05 - problem for our greedy algorithm
83:07 - if you were a little bored with our mice
83:09 - and no holes you won't be with the
83:12 - knapsack
83:18 - for a step up on the greedy method let's
83:21 - take a peek into what's called the
83:23 - fractional knapsack now this is intense
83:27 - fractional knapsack freshness people
83:30 - okay so this problem on the surface
83:32 - seems really complicated but it's only
83:34 - as complicated as you want to make it
83:37 - you know what i mean so let's not make
83:39 - it complicated okay so just so what you
83:42 - can do is just envision me it's the
83:44 - apocalypse it's the end of the world
83:47 - wait maybe it's just a monday food is
83:49 - scarce people are bartering water is
83:52 - practically non-existent so i need to
83:54 - conserve every bit of energy i have to
83:56 - walk to the market and try to trade what
83:58 - i have to stay alive okay now i have
84:01 - five objects i can take our knapsack or
84:04 - bag is our container for holding our
84:06 - objects think of our array as a storage
84:09 - device
84:10 - and each object is worth something some
84:13 - are worth more than others and each
84:15 - object weighs something some objects way
84:19 - more than others the key to this problem
84:22 - is to make the right decision on which
84:24 - objects are going to bring me the most
84:26 - profit i mean i gotta know i have
84:29 - animals to feed
84:31 - that haven't become somebody else's
84:32 - dinner you know while at the same time
84:35 - they can't be too heavy that i can't
84:37 - carry in my container because this is
84:40 - crucial i might have to make a decision
84:42 - based on its object the weight of the
84:45 - object not necessarily its profit if i
84:48 - want to make it back alive from zombies
84:50 - r us
84:51 - now the real kick in the head on solving
84:54 - this
84:55 - lies in that little teeny tiny word
84:57 - fractional this means that my objects
85:00 - can be divided if they are divisible
85:03 - say one of my objects contains three
85:05 - chapsticks i can divide that object and
85:08 - bring only two chapsticks if it will
85:10 - make a difference who knows cat fat
85:13 - might be a real hot commodity remember
85:15 - in that book of eli movie where he
85:16 - killed that cat
85:18 - well it was for food but he also made
85:20 - chapstick out of it
85:22 - not saying my chapstick would be that
85:25 - it'd probably be made from beeswax or
85:28 - something
85:29 - don't forget i'm risking my brains to go
85:31 - to the market to feed my animals in the
85:33 - first place so no cancel be harmed here
85:36 - no siree bob okay wow that was a really
85:39 - long
85:40 - way off the beaten path if let's get
85:42 - back to the problem so the weight is
85:44 - divisible and it would help to best fit
85:47 - how many profitable objects in my
85:49 - backpack then we should do that why we
85:52 - need to fill to the brim in our fixed
85:54 - size knapsack the most valuable items
85:57 - that we are able to carry
85:59 - for the steps of our algorithm keep in
86:01 - mind we are given weights and values of
86:04 - n items and we need to put these items
86:06 - in a knapsack of capacity w to get the
86:10 - maximum total value in the knapsack by
86:13 - the way in the o1 knapsack problem a
86:16 - whole different problem is one where
86:18 - we're not allowed to break items down or
86:20 - divide them using fractions
86:22 - we either take the whole item or don't
86:24 - take it but in this problem fractions
86:27 - can be used
86:28 - a really efficient solution to this
86:30 - problem is to use the greedy approach as
86:33 - opposed to brute force remember brute
86:36 - force is calculating every possible
86:39 - solution and is typically
86:40 - computationally expensive meaning it
86:43 - takes a lot of runtime and memory space
86:46 - the basic idea is to calculate the ratio
86:49 - of value slash weight for each item and
86:52 - sort the item on the basis of this ratio
86:55 - then take the item with the highest
86:57 - ratio greedy remember and add them until
87:00 - we can't add the next item as a whole
87:03 - and at the end add the next item as much
87:05 - as we can
87:06 - this will generally always be the
87:08 - optimal solution to this problem
87:11 - you'll see what i mean when we get into
87:13 - our ides and start coding it so let's
87:16 - get to work
87:19 - okay
87:20 - we'll define our functions with three
87:23 - parameters
87:24 - we need our values our weights and the
87:26 - capacity of the bag
87:29 - now we have our created variables here
87:32 - but i implemented print calls to show
87:34 - you what's going on
87:36 - we have our items a range of zero to
87:39 - four equaling five items
87:41 - the ratios are values divided by weight
87:44 - using floor division to eliminate floats
87:48 - list comprehension is good to use when
87:50 - you can
87:51 - then we'll sort our ratios in decreasing
87:54 - order
87:55 - lastly the ratios index positions are
87:58 - printed here to show the to show you the
88:01 - order that it will be computed in
88:05 - i'm going to
88:06 - take out the third variable here it as
88:09 - it won't be needed
88:11 - i just wanted to show you how it matches
88:14 - up with its index position
88:16 - our ratios being sorted is taken care of
88:19 - in the items.sort line
88:22 - we need a counter to keep a running
88:25 - tally of our max value
88:27 - we will create a list that is the length
88:29 - of our values we'll call fractions that
88:32 - will contain all zeros
88:34 - i'll tell you why we're doing this at
88:36 - the end of the program now it's time to
88:39 - run our for loop we'll iterate over our
88:42 - items and if the weight of our item is
88:44 - less than capacity it will add a one to
88:47 - our fractions list of zeros at its index
88:50 - position then we'll add that value to
88:52 - our max value and subtract the weight
88:55 - from our capacity now when all those
88:57 - conditions satisfy and we hit our else
89:01 - we take the fractional difference of our
89:03 - last item and add it to our max value
89:06 - then we return our maximum value which
89:09 - is 500 now here's the key you might be
89:13 - wondering this
89:14 - we have an item that weighs
89:17 - a 10 and it's worth 90.
89:20 - at a capacity of 150 we could put 15
89:24 - of these items into our bag for a value
89:27 - of 15 times 90 equaling 1350.
89:31 - why can't we just do that
89:33 - we can't do that because what we're
89:36 - looking at today is what is called the
89:38 - bounded knapsack problem or bkp for
89:41 - short
89:42 - the scenario i just painted a 15
89:45 - 15 of the same items would be allowing
89:47 - for copies of that item called the
89:50 - called the ukp type short for unbounded
89:53 - knapsack problem and that's a whole
89:56 - other ball of wax kids
89:59 - this is why we set up the fractions list
90:02 - containing nothing but zeros
90:04 - this is important because it's putting
90:06 - in our highest values first
90:09 - and then taking a fraction of the last
90:12 - item from maximum capacity in our bag
90:15 - each item counts as one in our fractions
90:18 - list until every zero is turned into a
90:21 - one but our very last item to change
90:23 - from zero to one is that special one
90:26 - that gets the remaining capacity divided
90:28 - by the weight
90:30 - that figure gets multiplied to our value
90:33 - and then added to our max value in the
90:35 - case of our program that ends up being
90:38 - 28 and a half percent and what's 28 and
90:41 - a half percent of 140
90:43 - is the value of 40 that puts us up to
90:46 - 500. sorting our ratios in decreasing
90:49 - order gives us the optimal or best
90:52 - approach this is considered greedy as
90:54 - you are placing items with the largest
90:57 - ratios into our bag first
91:00 - now you can sort by price and you can
91:03 - sort by weight as those are considered
91:06 - methods and they will get you close to
91:08 - 500
91:10 - but it's just not 500. and we should
91:12 - always strive for the best method
91:14 - algorithmically
91:17 - hopefully you now understand a pretty
91:19 - well-known problem not only in logistics
91:22 - but other fields as well like
91:24 - manufacturing and finance it's all about
91:27 - optimization
91:29 - what is the optimal way to get the items
91:31 - of highest value into a bag that you can
91:34 - reasonably carry
91:36 - and this greedy concept can be carried
91:39 - out in other reasonable scenarios as
91:41 - well
91:42 - with the goal of calculating solutions
91:44 - whether that be approximations or actual
91:47 - figures
91:48 - let's move on to our last section with
91:52 - good old fibonacci
91:58 - in our mind breaker episode we're going
92:01 - to talk about another well-known
92:03 - mathematician leonardo alpisa aka
92:07 - fibonacci considered to be the most
92:09 - talented western mathematician of the
92:11 - middle ages
92:13 - according to wikipedia he wrote the
92:15 - libre abachi the book of calculation and
92:18 - is it is a historic 13th century latin
92:20 - manuscript on arithmetic which includes
92:23 - the posing and solving of a problem
92:25 - involving the population growth of
92:27 - rabbits based on idealized assumptions
92:30 - the solution
92:31 - generate generation by generation of
92:33 - rabbit offspring was a sequence of
92:36 - numbers later known as the famous
92:38 - fibonacci numbers or fibonacci sequence
92:41 - in a little side compartment of
92:43 - mathematics however what we are going to
92:45 - talk about today is the greedy algorithm
92:48 - for egyptian fractions first described
92:51 - by fibonacci for transforming rational
92:54 - numbers into egyptian fractions
92:56 - an egyptian fraction is a representation
92:59 - of an irreducible fraction as a sum of
93:02 - distinct unit fractions and this means
93:04 - that every positive fraction can be
93:07 - represented as a sum of unique unit
93:10 - fractions a fraction is a unit fraction
93:12 - if the numerator is 1 and the
93:14 - denominator is a positive integer for
93:16 - example 1 3
93:18 - is a unit fraction to take a closer look
93:21 - we'll reduce a fraction of 7 14 in a
93:24 - pizza example the egyptian way
93:30 - [Applause]
93:35 - [Applause]
93:35 - [Music]
93:40 - in order to generate egyptian fractions
93:42 - we will utilize the greedy algorithm
93:44 - because although possible to use brute
93:46 - force search algorithms to find it it
93:49 - would be extremely inefficient
93:51 - remember 3d algorithms are for
93:53 - optimizing our problems we always want
93:56 - to know and understand the optimal
93:58 - solution as a side note although
94:00 - fibonacci's greedy algorithm isn't the
94:02 - only way to efficiently solve our
94:04 - problem he's our focus for today
94:07 - let's look at a program
94:10 - so yeah i got this program here
94:13 - we're importing some tools
94:16 - scroll down some
94:18 - we got three functions going strong a
94:21 - main a validate a converting function we
94:25 - got some wrappers nestled functions we
94:28 - got some yields which means it's a
94:30 - generator not even gonna touch that
94:34 - this is a lot of code
94:36 - heading into nearly 50 lines
94:39 - is it good
94:40 - yes
94:41 - hard to write
94:43 - depends but if you're looking for
94:45 - kindergarten style programming you come
94:47 - to the right place
94:49 - i totally dumbed it down to show you the
94:52 - mechanics not that you're dumb i love
94:55 - simple make it plain am i right
94:58 - so we have this pie remember that bob
95:01 - took a big old chunk of let's change
95:04 - that to a pizza since it kind of goes
95:07 - with our theme of leonardo of pizza
95:10 - or fibonacci
95:12 - so to get this real simple we'll write a
95:15 - function that takes our numerator and
95:17 - our denominator
95:20 - we'll implement an empty list for our
95:22 - denominators we won't need one for our
95:25 - numerators because we already know that
95:27 - they're going to be ones
95:29 - then we write our while loop that as
95:31 - long as our numerator isn't zero we can
95:34 - create a variable that stores our
95:36 - quotient using the math dot seal that
95:39 - will round it up
95:41 - this is important for when we have a
95:43 - number that's a float lower than one
95:47 - next we reassign our numerator to be the
95:50 - product of the quotient and the
95:51 - difference of our denominator subtracted
95:54 - from our numerator then we reassign our
95:57 - denominator to be multiplied by our
95:59 - quotient or x
96:02 - for our fraction of 7 12 this is going
96:05 - to put a 2 and a 12 into our egypt list
96:09 - and those two numbers are our lowest
96:11 - common denominators or lcd for short
96:15 - all that's left to do now is put a 1
96:18 - over those lcds as those are our
96:21 - egyptian fractions and that can be
96:23 - accomplished a few ways but basically i
96:26 - just implemented some good old-fashioned
96:28 - string formatting by iterating over our
96:31 - egypt list of denominators and adding
96:34 - them to an empty string
96:36 - once we do that we just return it the
96:39 - caveat to this really simple program is
96:41 - that it probably doesn't work for really
96:44 - complex fractions or when the numerator
96:46 - is larger than the denominator
96:49 - but this is a good exercise in
96:52 - understanding how to break down a
96:54 - fraction the egyptian way ultimately all
96:57 - of this runs more along the line of
96:59 - recreational mathematics however there
97:01 - are still open problems involving
97:03 - egyptian fractions that have yet to be
97:05 - solved or proved
97:06 - who knows maybe you can be the next
97:08 - mathematical genius to solve one of them
97:13 - so what did we learn about greedy
97:15 - algorithms we learned optimization of
97:18 - solutions to problems using the greedy
97:20 - algorithm provided in our example with
97:22 - the mice in the holes it's like a
97:24 - delicious big pie where you take the
97:26 - biggest slice out first
97:28 - this makes the pie easier to finish we
97:31 - learned that chapstick made from cats is
97:34 - a hot commodity in the apocalypse
97:36 - kidding
97:37 - we walk through the fractional nap stack
97:40 - problem where we find the largest value
97:42 - of an item in a way to maximize the
97:44 - total value within the knapsack we want
97:47 - to avoid brute force here because
97:49 - finding all the possible subsets would
97:51 - take too much time we learned that
97:53 - fibonacci was the first to describe an
97:55 - algorithm for transforming rational
97:57 - numbers into egyptian fractions and it
98:00 - is where we really take the largest unit
98:03 - fraction we can and then and then repeat
98:06 - that on the remainder although this is
98:08 - purely mathematics it's still a good
98:10 - representation of the greedy algorithm
98:15 - [Music]
98:22 - [Music]
98:26 - what do you think when you hear the word
98:29 - dynamic
98:30 - well if you're a programmer or an
98:32 - aspiring programmer
98:34 - you may not know what to think
98:37 - what does being dynamic really mean
98:40 - for once in my life it would be awesome
98:43 - to be described as dynamic because when
98:46 - we talk about a dynamic person
98:49 - it means energetic
98:52 - spirited
98:53 - active
98:54 - lively
98:56 - zestful vital
98:59 - vigorous
99:01 - maybe this is why when it comes to
99:03 - computer science students can become
99:06 - intimidated with the subject of dynamic
99:08 - programming
99:09 - ultimately the word dynamic means a
99:12 - process or system characterized by
99:15 - constant change activity or progress
99:19 - with its roots in greek to the word
99:21 - dynamous meaning power
99:25 - hint dynamite
99:27 - so it's no wonder we subconsciously shy
99:30 - away from it because it just sounds like
99:33 - a handful it dangerous pretty wild how
99:36 - words and their meanings really deep
99:39 - down can make us feel a certain way you
99:42 - know
99:42 - but i'm here to help you out and put you
99:46 - at ease and tell you a little secret
99:49 - they introduce dynamic programming to
99:51 - children
99:53 - okay maybe not children children but
99:56 - like upper elementary middle schoolers
100:00 - kids like that so if they can learn
100:03 - about it so can you
100:05 - but first we have to unplug our
100:07 - preconditioned mentality if you will to
100:10 - think and believe that anything in
100:11 - computer science is too hard
100:14 - it's not it's all just one big treasure
100:17 - hunt to find information and knowledge
100:20 - that best suits your intellectual filter
100:23 - not all people teach all things the same
100:25 - way
100:26 - not all people learn information or
100:29 - absorb knowledge the same way so let's
100:32 - take a deep breath and step back to look
100:35 - at the different dynamics that are out
100:38 - there
100:39 - that's a long list
100:41 - for us there should only be two that
100:44 - should really mean something
100:46 - that's dynamic programming languages and
100:48 - dynamic programming as a methodology you
100:51 - can have programming languages that are
100:53 - either dynamic or static python is
100:55 - dynamic by the way and then you have
100:57 - programming methodologies that are
100:59 - concerned with analyzing a problem by
101:01 - developing algorithms based on modern
101:03 - programming techniques
101:05 - in the world of dynamic programming
101:07 - dp is used when the solution to a
101:10 - problem can be viewed as a result of a
101:12 - sequence of decisions with the intended
101:14 - outcome of reducing run time and any
101:17 - implementation that can make a solution
101:18 - more efficient is considered optimizing
101:21 - it
101:21 - in an early lesson we saw several
101:24 - problems that can be viewed in this way
101:26 - i.e the knapsack problem
101:28 - essentially the difference between the
101:30 - greedy method and dp is that in the
101:33 - greedy method only one decision sequence
101:36 - is ever generated and in dp many
101:39 - decision sequences may be generated
101:41 - however sequences containing sub-optimal
101:44 - sub-sequences cannot be optimal and as
101:47 - far as possible will not be generated
101:49 - and this is the key that helps make a
101:51 - reduced run time optimal substructure
101:54 - means that optimal solutions of
101:56 - subproblems can be used to find the
101:58 - optimal solutions of the overall problem
102:01 - in dp this is called the principle of
102:04 - optimality the principle of optimality
102:06 - as relayed by richard bellman who
102:08 - introduced dynamic programming in 1953
102:11 - states that an optimal policy has the
102:13 - property that whatever the initial state
102:16 - and initial decision are the remaining
102:18 - decisions must constitute an optimal
102:20 - policy with regard to the state
102:22 - resulting from the first decision
102:25 - so what does that mean
102:26 - in general we can solve a problem with
102:29 - optimal substructure by doing three
102:31 - things
102:32 - breaking the problem down into smaller
102:34 - sub problems
102:37 - solving the sub problems recursively
102:40 - and then using the optimal solutions to
102:42 - construct an optimal solution for the
102:45 - original problem
102:46 - but there's an issue with this that we
102:48 - need to be aware of and that is
102:50 - sometimes when we do this we can get
102:53 - overlapping subproblems we want to avoid
102:56 - that
102:57 - for instance in the fibonacci sequence
103:00 - f3 equals f1 plus f2 and f4 equals f2
103:04 - plus f3 computing each number involves
103:07 - computing f2 because both f3 and f4 are
103:11 - needed to compute f5 a naive approach to
103:14 - computing f5 may end up computing f2
103:17 - twice or more
103:19 - in order to avoid this we instead save
103:22 - or store the solutions to problems we
103:24 - have already solved in dynamic
103:27 - programming there are three basic
103:29 - methods
103:30 - we have recursion
103:32 - top down with memoization and bottom up
103:35 - with tabulation
103:38 - and although we could have chosen the
103:40 - well-worn path of the fibonacci sequence
103:43 - to do this
103:45 - oh no
103:47 - i'm encouraging you to step out and
103:49 - think bigger have confidence
103:52 - you are smart and you can do big things
103:55 - so
103:57 - let's get ugly with it in our first
104:00 - lesson
104:05 - what are ugly numbers and why do we call
104:08 - them ugly and what do we do with them
104:12 - to answer the first question i saw this
104:16 - and i think it will give you a big hint
104:23 - okay
104:24 - that's funny and i wish i could have
104:26 - been the one to think it up but alas i
104:29 - was too slow to the game
104:32 - darn
104:33 - i stumbled upon a pretty funny thread
104:36 - about ugly numbers and i stole that so
104:38 - shh don't tell on me
104:40 - but the long and short of it is this
104:43 - to check if a number is ugly in the
104:46 - first place
104:47 - we divide the number by the greatest
104:50 - divisible powers of two three and five
104:53 - and if the number becomes one
104:55 - then it's an ugly number otherwise it's
104:58 - not
104:58 - we're employing our power of prime
105:01 - factorization
105:03 - we learned that way back in the fifth
105:05 - grade so that tells me that you probably
105:07 - don't remember
105:08 - but it'll all come back to you today
105:12 - the sequence of one to six eight nine 10
105:15 - 12 15 shows the first 11 ugly numbers by
105:19 - convention one is included
105:22 - all right now that you got that under
105:24 - your belt let's tackle the problem of
105:26 - finding the nth ugly number
105:29 - can we find the hundredth ugly number
105:32 - would you believe that if we crafted a
105:34 - program using brute force it would be
105:37 - over a thousand iterations
105:40 - for the hundredth number
105:42 - i think i broke my visualizer trying to
105:44 - run it
105:45 - and we don't want to break things or
105:47 - make our computers smoke out the back or
105:50 - anything like that so it's time to
105:52 - incorporate our spirited energized
105:55 - vigorous programming that's like
105:57 - dinomite but not a lot of dynamite
106:00 - we'll start small and just find the 15th
106:03 - ugly number how does that sound
106:06 - since we're going big and not going home
106:09 - we're gonna shoot for the bottom up
106:11 - method for a dynamic approach
106:14 - if we use a pretty simple iterative with
106:17 - some recursion solution we're still
106:19 - looking at nearly 600 steps or
106:22 - iterations in the program
106:24 - to get our results as compared to just a
106:26 - little over a hundred steps for the
106:29 - bottom-up dynamic programming method
106:31 - talk about optimization
106:34 - if we use simple iteration and
106:36 - continuously loop through all the
106:39 - numbers to find whether each number is
106:41 - ugly or not it can really bog down when
106:44 - we get to larger numbers
106:46 - with dynamic programming instead of
106:48 - starting from the head we start from the
106:50 - bottom we're also using a table to help
106:53 - us store the values for easy access if
106:56 - we look at the ugly number sequence 1 to
106:59 - 6 8 9 10 12 15 we can see that it can be
107:02 - further divided into sub sequences of 2
107:06 - 3 and 5. and these subsequences
107:08 - themselves are ugly numbers therefore we
107:11 - take every ugly number from the above
107:14 - sequences and choose the smallest ugly
107:17 - number in every step to make sure that
107:19 - we don't miss any number in between
107:22 - let's head into our ides
107:26 - okay so if we're looking for the nth
107:29 - ugly number in a sequence we have to
107:31 - write a function that will tell us if
107:33 - our numbers
107:34 - can be evenly divided in the first place
107:36 - meaning no remainder
107:38 - x modulo y equaling 0 gives us what we
107:41 - need to prove that if there is a
107:43 - remainder we'll just write it to return
107:46 - our original number for our second step
107:48 - before we can find the nth ugly number
107:50 - position we have to check if it's an
107:53 - ugly number we can do this recursively
107:55 - by going back to our first function the
107:58 - successive division
108:00 - what this does is plug in the number
108:02 - we're checking and successively or
108:05 - successfully if you want to think of it
108:07 - that way dividing it by two three and
108:09 - five if our number is successfully
108:12 - divided by two three and five with no
108:14 - remainder we have an ugly number on our
108:16 - hands let's check a six
108:20 - it's true six is ugly
108:22 - okay so we have what we need to find our
108:24 - nth number six is fairly easy since
108:28 - every number before it is ugly so the
108:30 - sixth ugly number is
108:32 - six but what's the 15th number well
108:36 - let's write a program to find out
108:38 - essentially what we're doing is merely
108:40 - creating a counter coupled with
108:42 - recursively calling back our ugly check
108:44 - function we'll create two variables one
108:47 - for i to iterate up our sequence and one
108:50 - for our counter and then we just
108:52 - incorporate a while loop to say as long
108:54 - as our number is greater than our
108:56 - counter we'll add one to i heading up
108:59 - our number sequence and if that number
109:01 - within our while loop is ugly add a 1 to
109:03 - our counter
109:05 - then we just return i
109:07 - so what's our 15th ugly number
109:11 - it's 24.
109:13 - now if we run this program we have a lot
109:15 - of running back in our recursive calls
109:17 - for every number in our sequence
109:20 - and that's a lot of running back with
109:22 - 600 steps or iterations that's a lot
109:25 - considering with dp we can write a much
109:28 - more efficient program that cuts all
109:30 - those iterations by nearly 80 percent
109:33 - let's check out the difference the first
109:35 - major thing we can do is eliminate those
109:38 - three functions within our original
109:40 - program we won't need those remember
109:42 - when i said we were going to implement a
109:45 - table to store information
109:47 - well that's what we're doing then we'll
109:49 - start our first position in that table
109:52 - or in other words a dynamic programming
109:54 - array to store the ugly numbers next set
109:58 - pointers for our multiples of 2 3 and 5
110:01 - and we can do this all in one line which
110:04 - is pretty cool you'll see how this works
110:07 - as we get deeper into the program here's
110:09 - our multiples of two three and five in
110:12 - their own variables after that we loop
110:15 - from one to n at each step and we find
110:18 - the minimum ugly number out of three
110:21 - multiples and store it in our dp array
110:24 - we're also increasing the pointer for
110:26 - the number whose multiple has been added
110:29 - and replace it by its next multiple so
110:32 - suppose a multiple of two has been added
110:34 - to the array we increase twos pointer by
110:37 - one and multiply the previous multiple
110:39 - to two to get the next multiple and so
110:42 - on and so forth for other numbers as
110:44 - well and this way starting from the base
110:47 - case we generate all the ugly numbers
110:49 - and then apply dynamic programming to
110:51 - store the numbers in the array until we
110:53 - get the nth ugly number
110:59 - this approach is way better than the
111:01 - previous one because instead of checking
111:04 - all the numbers for being ugly we just
111:06 - used the previous ugly numbers to
111:08 - generate new ones
111:10 - watch how this runs through the program
111:27 - this really is the better way
111:30 - very efficient now let's put this
111:32 - knowledge to good use on a very famous
111:35 - problem called the traveling salesman
111:38 - problem
111:40 - if you haven't heard of this yet you
111:42 - will
111:43 - best to get it in your brain bucket now
111:46 - rather than later
111:52 - the traveling salesman problem is one of
111:54 - the most intensely studied and famous
111:57 - problems in computational mathematics
112:00 - with its main main really main focus on
112:03 - optimization
112:05 - so much research has gone into this
112:08 - challenge of finding the cheapest
112:10 - shortest fastest route in the visitation
112:12 - of locations by this dashingly handsome
112:16 - or beautiful salesperson
112:18 - this person starts in their home city
112:21 - travels around and returns to their
112:23 - starting point most easily expressed in
112:26 - a graph
112:27 - the general form of the tsp traveling
112:30 - salesperson appears to have been first
112:33 - studied by mathematicians during the
112:34 - 1930s in vienna and at harvard notably
112:39 - by karl menger
112:41 - menger defines the problem considers the
112:44 - brute force algorithm and observes the
112:46 - non-optimality of the nearest neighbor
112:49 - heuristic using the mailman as an
112:52 - example now the order in which the
112:54 - person travels isn't cared about as long
112:57 - as he visits each one
112:59 - once during his trip
113:01 - and finishes where he was at the first
113:03 - in the most cost effective way but
113:06 - there's a catch
113:07 - maybe the salesman needs to take a plane
113:10 - to one of the cities and this will cost
113:12 - him a pretty penny
113:14 - maybe he can walk to one of his cities
113:17 - therefore making it a free trip the cost
113:20 - associated with the trip describes how
113:23 - difficult it is to to traverse the
113:25 - journey or graph for us computer
113:28 - scientists whether that's money or time
113:31 - or distance required to complete the
113:33 - traversal the salesman ideally wants to
113:36 - keep every cost as low as possible he
113:39 - wants the cheapest fastest shortest
113:41 - route wouldn't you the easiest and most
113:44 - expensive solution
113:46 - is to simply try all the permutations
113:49 - and then see which one is cheapest using
113:51 - brute force the problem with this though
113:54 - is that for n cities you have n minus 1
113:58 - factorial possibilities this means that
114:00 - just for 10 cities there are over 180
114:04 - 000 combinations to try since the start
114:08 - city is defined there could be
114:09 - permutations on the remaining nine
114:12 - and at 20 cities this just becomes
114:15 - simply infeasible
114:17 - this is why four is often used in the
114:19 - majority of problems as you'll quite
114:22 - often see in the 4x4 matrices keeping it
114:25 - small makes it much more manageable and
114:27 - understandable which is good
114:30 - the awesome news for us is that the
114:32 - problem is solvable but keep in mind
114:35 - that there can also be solutions to the
114:37 - problems that are approximations or
114:39 - commonly called heuristic algorithms if
114:42 - that's what is being asked for according
114:45 - to wikipedia even though the problem is
114:47 - computationally difficult many
114:50 - heuristics and exact algorithms are
114:52 - known so that some instances with tens
114:56 - of thousands of cities can be solved and
114:59 - even problems with millions of cities
115:01 - can be approximated within a fraction of
115:04 - one percent
115:06 - wow
115:07 - so let's get to the nitty gritty and
115:10 - look at our steps number one consider
115:13 - city one as the starting and ending
115:15 - point
115:16 - number two generate all n
115:19 - one factorial permutations of cities
115:23 - three calculate cost of every
115:25 - permutation and keep track of minimum
115:27 - cost permutation four return the
115:30 - permutation with minimum cost
115:32 - it's not hard to see how this could get
115:34 - computationally expensive in a very
115:36 - short amount of time so we will move
115:39 - into the optimize method right away for
115:42 - our savings
115:43 - and our sanity
115:45 - the goal is to break this problem down
115:47 - into the smallest pieces possible and
115:49 - then solve solve solve
115:52 - let's get into it
115:57 - because permutations are a big part of
116:00 - the tsp problem we're going to import
116:02 - intertools permutations to make our
116:05 - lives a whole lot easier
116:07 - it's totally okay for this one
116:10 - we'll implement a variable that we'll
116:12 - call v
116:13 - short for vertices and vertices are the
116:16 - dots on our graph think of our sales
116:19 - person traveling to four cities on our
116:21 - graph a single dot is called a vertex
116:24 - on our graph and they link or connect to
116:27 - one another now we can move on to
116:29 - implementing our function call here with
116:31 - two parameters our graph that will be a
116:34 - four by four matrix and s is going to be
116:37 - our counter that will need to calculate
116:39 - the sum of our path i'll explain this a
116:41 - little bit deeper at the end of the
116:43 - program
116:44 - we'll create an empty list for our
116:45 - vertices to be stored in and we'll call
116:48 - this vertex just to make it plain this
116:51 - is going to tell us the best route for
116:54 - our rows in the graph again this is
116:56 - going to be come real clear at the end
116:59 - of the program so hold on to yourselves
117:02 - then we iterate over our range of v and
117:05 - if the index does not equal us which is
117:08 - zero we append it to our empty vertex
117:10 - list this is going to show us where our
117:13 - salesperson is traveling within the
117:15 - graph but here's a little catch that we
117:18 - need to be aware of check out our graph
117:21 - see those trails of zeros
117:23 - we have no need to calculate those so
117:26 - when we append our indices to our vertex
117:28 - list we're only producing three integers
117:31 - one two and three
117:33 - next we create another empty list that
117:36 - we'll call min path
117:38 - we need to know the best route so this
117:40 - will store our lowest combination of
117:42 - vertices within the graph
117:45 - then we create a variable that we'll
117:46 - call next permutation and we'll
117:49 - implement the inter tools permutations
117:51 - module here to give us all the
117:53 - combinations of the vertices for one two
117:55 - and three
117:56 - since we have three numbers we know that
117:59 - the amount of permutations will be six
118:02 - then we iterate over all those
118:04 - combinations and create a counter that
118:06 - we'll call current pathway we'll
118:08 - reassign our s to a variable we'll call
118:11 - k and k will represent the row in our
118:14 - graph
118:15 - and as we iterate over our indices
118:17 - within our permutations by using j
118:20 - this becomes our columns i know this
118:23 - sounds i know this all sounds really
118:25 - confusing but stay with me when we add
118:28 - the k and j of our graph to our current
118:30 - pathway look what prints out
118:35 - this is calculating all the paths within
118:37 - our graph and adding them together
118:40 - pretty cool isn't it
118:41 - next we reassign k to j and then use an
118:45 - addition operator to add the sums to our
118:48 - graph to the current pathway append them
118:50 - to our min pass hold them in a variable
118:53 - called x that's sorted
118:56 - and simply return the very first number
118:59 - in our sorted list which is our lowest
119:01 - number
119:02 - our shortest route on the graph to see
119:05 - how this travels
119:07 - on the graph watch this
119:17 - this is how it is adding up within our
119:20 - program
119:21 - hopefully you can see and understand how
119:24 - this is all working it sounds so much
119:27 - more complicated than it really is but
119:30 - it's only math
119:32 - we're simply calculating the paths of a
119:35 - matrix by pulling out all the
119:37 - combinations or permutations of the
119:39 - indices and returning the lowest number
119:42 - or lowest sum of the path it's not too
119:45 - bad my recommendation to you would be to
119:49 - create your own graph and then run it in
119:51 - the program once you do that
119:53 - map that course on paper or even
119:56 - whiteboard it to see it visually
120:03 - for our very last lesson today and not
120:06 - only for dp but
120:09 - all together
120:10 - i'm so sad to leave you
120:14 - so let's make this session a great one
120:17 - what we're doing for our mindbreaker
120:19 - sounds really intimidating but i can
120:21 - assure you that once we break this down
120:24 - you're gonna say wow that wasn't even
120:26 - that bad and then you can brag to all
120:28 - your friends and then they'll shun you
120:31 - for your genius
120:33 - but it'll be totally worth it i i mean
120:36 - because who needs friends am i right
120:41 - now your brain won't break too much if
120:44 - we take each word out and examine it
120:46 - first we have palindromic which is
120:48 - religious palindrome matrix we'll keep
120:51 - it low and slow with a 3x3
120:54 - and then just paths which we're only
120:56 - going to go right and down only
120:59 - since we worked on a matrix in our last
121:01 - problem this should be old hat hopefully
121:04 - by now you have a decent handle on
121:06 - breaking down the problem into smaller
121:08 - ones called sub problems and solving
121:10 - them little by little in order to solve
121:12 - the whole
121:14 - tips for getting started on the
121:16 - palindromic matrix path problem would be
121:18 - to first know how to check whether a
121:21 - string is a palindrome or not in the
121:23 - first place
121:24 - you can use python's python has a little
121:27 - pip install program called
121:29 - palindrome that
121:31 - checks whether a string is pendulum or
121:33 - not but i do highly recommend you learn
121:35 - how to do this on your own
121:37 - because it will help with this problem
121:40 - the second tip would be knowing how to
121:42 - traverse a matrix going right and down
121:45 - only
121:46 - we're not going to get too crazy for our
121:48 - final lesson and just have some fun with
121:51 - it by printing our palindrome strings
121:54 - we're not going to return counts or find
121:57 - the longest because that might make some
121:59 - of us cry
122:01 - and there will be no crying today any
122:04 - tears that gets shed must be happy tears
122:07 - only whether that's joy at finishing
122:09 - this course or not seeing my face
122:13 - anymore
122:14 - tears of accomplishment are fine but no
122:18 - tears of sadness okay to keep it simple
122:21 - we'll start at ground zero and start
122:23 - with an empty three by three matrix and
122:25 - find all the unique paths
122:28 - what's really cool about this is that
122:30 - it's actually just a math problem
122:33 - what we'll do is create in our three by
122:35 - three by starting with a row of ones
122:38 - that we will build up from
122:41 - our bottom row and far right column will
122:43 - be ones and when we add their adjacent
122:46 - numbers together and build up we get a
122:48 - row of ones one one one at the bottom
122:51 - and then three two one in the middle and
122:53 - then six three one at the top
122:56 - if we return row zero
123:00 - that will give us six and that tells us
123:02 - that we have six unique paths in our
123:04 - matrix
123:05 - this will come in handy later when we
123:07 - implement this principle into our matrix
123:09 - of strings
123:11 - we can know that we will have six
123:12 - strings returned to us and all we need
123:15 - to do is check whether they're
123:17 - palindromes and print those out
123:20 - so we're not going to talk much if at
123:23 - all about the naive approach or brute
123:26 - force or whatever the difference is in
123:27 - optimization no
123:29 - let's part ways on a good note and have
123:32 - some fun with this
123:34 - i mean we walked through 14 other
123:37 - problems with multiple methods in a
123:40 - relatively short amount of time
123:42 - and here we are breaking our minds on
123:45 - palindromic matrix paths so yeah let's
123:49 - get straight to the optimum let's code
123:52 - the paths problem first shall we we'll
123:54 - start by creating our function with two
123:56 - parameters our row and our column we
123:59 - already know it's going to be a three by
124:01 - three then we create our dynamic
124:03 - programming array of all ones which you
124:06 - can envision as our bottom row of ones
124:09 - next we iterate over our rows in a for
124:12 - loop and then create another dp array
124:14 - which can be envisioned as an
124:16 - accumulative next row up in our matrix
124:19 - our original for loop is going to
124:21 - iterate three times 0 1 2
124:24 - for each row in our matrix in a second
124:27 - for loop we now want to iterate from the
124:29 - bottom up and reassign our new row
124:31 - elements by adding one element to the
124:34 - one next to it going left
124:36 - we do this twice
124:38 - while in our final iteration it has
124:40 - accumulated each row to end with six
124:43 - three one we then reassign our original
124:46 - row as the final row and return the very
124:49 - first element which will be six
124:51 - our answer to the number of paths now
124:53 - this isn't super necessary but it will
124:56 - give you an idea of the path traversal
124:58 - within a three by three matrix and will
125:00 - give us a good launch point to moving up
125:02 - to a matrix consisting of letters we'll
125:05 - keep it in the realm of just two letters
125:08 - a
125:08 - and b and we'll write a very quick
125:10 - little program to see how many
125:12 - palindromes of a and b there are
125:15 - let's import intertools to help us and
125:18 - then create our rows and columns of
125:20 - letters
125:21 - a neat little feature of inter tools is
125:23 - product with an asterisk on our matrix
125:26 - this will print out every combination of
125:29 - letters in our matrix totaling 27. yes
125:33 - there will be duplicates here but it's
125:35 - nothing that python's set can't handle
125:39 - let's create an empty list called p1 and
125:41 - then iterate over our list of 27 tuples
125:45 - join them together to make it easy on
125:47 - the eyes and append them to our empty
125:49 - list
125:50 - then all that's left to do is check
125:52 - which ones are palindromes by making a
125:54 - new function plugging in our p1 create
125:57 - another empty list p2 and then iterating
126:01 - over a non-duplicate set of p1
126:04 - incorporate an if statement is it a
126:07 - palindrome yes put it in our empty list
126:10 - then return it
126:11 - okay so far we've traversed a matrix in
126:14 - order to count the number of paths in it
126:16 - we created a three by three matrix of
126:18 - just two letters found every possible
126:21 - combination and created a function to
126:24 - pull the palindromes from it
126:26 - we are doing fabulously if you know how
126:29 - to count the paths of a matrix and find
126:31 - the palindromes in a matrix you're
126:34 - totally equipped to take on the
126:36 - palindromic paths problem
126:39 - let's get cracking
126:41 - so
126:42 - before we start i have to tell you
126:45 - there's going to be a little bit of a
126:47 - difference here for this program
126:49 - it's the order of traversal we're going
126:52 - to go from the top and right to the
126:55 - bottom not the bottom up and left to the
126:58 - top
126:59 - so keep that in mind as we march forward
127:02 - we're going to throw in our little
127:05 - palindrome checker up here first before
127:08 - we print them out at the end of our
127:10 - function hopefully by now you're
127:12 - familiar with that
127:14 - next we're going to create our main
127:16 - function with several parameters
127:18 - a string that will be empty but we're
127:21 - just going to call it string here
127:24 - a for our array
127:27 - our pointers that we'll call i and j
127:32 - our row and column that we'll call m and
127:35 - n
127:36 - now our conditional if statements are
127:39 - going to traverse the paths of our array
127:42 - we minus one from m and n because as you
127:45 - know this is python and we only need to
127:47 - iterate from indices starting at zero
127:50 - then one then two for our total size of
127:53 - rows and columns three by three see the
127:56 - i plus one and the j plus one within our
127:59 - recursive callbacks look at these slides
128:01 - to see the paths
128:09 - you could also print i and j within the
128:12 - conditional statements on your own to
128:14 - see it as well but when we add one to
128:17 - our pointers we're moving to the next
128:19 - element then
128:21 - once the traversal is completed it hits
128:24 - our else statement and that hits up our
128:26 - palindrome checker if it's true it'll
128:29 - print then it moves on to do the next
128:32 - reversal over and over until it's done
128:35 - and there you have it
128:37 - four palindrome strings in our matrix
128:40 - yay
128:43 - [Applause]
128:46 - so what did we learn on our final lesson
128:50 - we learned about dynamic programming
128:53 - which is finding the optimal solution to
128:55 - sub problems that can be used to find
128:57 - the optimal solutions to the overall
129:00 - problem
129:01 - we saw this in our lesson with ugly
129:03 - numbers the traveling salesman problem
129:06 - and palindromic matrix paths
129:09 - in the easy lesson we learned about ugly
129:11 - numbers what they are and how to write a
129:13 - program to find the nth ugly number in
129:16 - the sequence avoiding brute force and
129:20 - recursion we were able to implement
129:22 - dynamic programming to optimize the
129:24 - solution from over one thousand
129:27 - iterations down to just a little over
129:30 - one
129:31 - we looked at the very famous traveling
129:34 - salesman problem with its main focus in
129:37 - optimizing a solution to finding the
129:39 - most efficient route a solution used
129:43 - across the board for so many companies
129:46 - in order to keep operational costs low
129:50 - and lastly we broke down the mind
129:53 - breaker lesson by studying unique paths
129:55 - in a matrix
129:57 - string palindromes within a matrix and
129:59 - combining those two problems together to
130:02 - find all the palindromic paths within a
130:05 - matrix
130:09 - are you excited i know that i am we
130:12 - covered so much ground in this course
130:14 - it's crazy
130:16 - hopefully you took lots of notes so you
130:18 - can practice later you should be coding
130:21 - something every day even if it's just
130:24 - for a couple minutes 10 to 15. since
130:28 - every little bit helps now listen to
130:30 - your mother thank you so much for
130:32 - joining me today and if you made it all
130:34 - the way here kudos to you i wish you all
130:37 - the best in your programming journey bye