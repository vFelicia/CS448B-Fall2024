00:00 - Master technical interviews this course
00:02 - breaks down complex topics like data
00:05 - structures algorithms and interview
00:07 - strategies into easily digestible
00:10 - content par teaches this course he is an
00:13 - experienced engineer and he will teach
00:15 - you both the fundamentals and help you
00:18 - understand Advanced coding patterns and
00:20 - common pitfalls to avoid Hello friends
00:23 - hope you are having a fantastic day
00:24 - today I want to welcome you to a journey
00:27 - that will transform the way you approach
00:29 - technical interviews and my aim is to
00:31 - help you become significantly better at
00:33 - technical interviews I'm here to be your
00:36 - guide as someone who has walked through
00:38 - this path and knows the twist and turns
00:40 - every step of the way together we will
00:42 - unlock the secrets of technical
00:44 - interviews from the basics of bigo
00:46 - notation to the integrities of data
00:48 - structure and algorithms so before we
00:50 - move forward let me introduce myself my
00:52 - name is par and I'm based in Canada I
00:55 - have been in the tech industry for close
00:56 - to 10 years and I have worked at
00:58 - companies like Microsoft NOK Kia and
01:00 - Royal Bank of Canada I'm a big believer
01:02 - in technology and love Building
01:04 - Solutions and softwares last year I
01:07 - became dad of twin daughters and on the
01:09 - side I also run my own YouTube channel
01:11 - called destination Fang where I teach
01:13 - about data structure and algorithm
01:15 - related problems so now let's just get
01:17 - started with this
01:20 - course so first let's try to understand
01:23 - that what is going to be the format at
01:25 - any typical interview for any tech
01:27 - company usually during the interview
01:29 - process you can expect somewhere between
01:31 - 3 to six rounds of interviews and they
01:33 - are usually broken down into three main
01:35 - categories first category is the
01:37 - behavioral interview and the purpose of
01:39 - the behavioral interview is to check
01:42 - what kind of person you are are you a
01:44 - good culture fit do you work well with
01:46 - different people do you work well under
01:48 - pressure can you raise your concerns if
01:50 - you have any conflict between two people
01:52 - how can you handle that and these are
01:55 - the things companies wants to know
01:56 - before they hire you second type of
01:58 - interviews are system design design
02:00 - interviews and they are usually more
02:02 - emphasized for more senior level folks
02:05 - and if you are someone with like 10
02:06 - years or 20 years of experience that
02:08 - would be one of the most significant
02:10 - chunk of importance for any job that you
02:12 - are applying to but that doesn't mean
02:15 - that uh they are not going to be there
02:16 - for junior level folks there would still
02:19 - be some emphasis but not as great and
02:22 - usually those interviews tend to be
02:24 - open-ended where they ask you questions
02:26 - like if you have to build your own
02:27 - Twitter or your own Google how would you
02:29 - do that what are the things you would
02:31 - consider and then the conversation Dives
02:33 - deep into the vast sea of software
02:36 - Solutions and the third category is
02:39 - technical interviews which is going to
02:40 - be our main point of focus for now from
02:43 - now on T typically a technical interview
02:46 - lasts for somewhere between 45 to 60
02:48 - Minutes during this time interviewer
02:51 - usually ask one to two questions and
02:53 - once you have the solution in place they
02:55 - would judge the solution if solution
02:57 - seems to be good enough or fine they
03:00 - will ask you to write a preliminary code
03:02 - for that in any language of your choice
03:05 - but the thing is it's not quite as
03:07 - simple as it sounds there are lot of
03:10 - moving Parts in any technical interview
03:12 - usually a technical interview has a
03:15 - problem statement and that problem
03:16 - statement usually refers to some real
03:19 - life scenario and that problem statement
03:22 - needs to be solved using a computer
03:25 - program that we we are trying to build
03:27 - for that you would need two things for
03:29 - sure number one is data structures and
03:31 - data structure is nothing but a way to
03:33 - store the data inside the memory of the
03:35 - computer there are lot of different
03:37 - methods available uh so you will need to
03:40 - take care of data structure second thing
03:42 - is algorithm and algorithm is nothing
03:45 - but a set of instructions that a
03:47 - computer needs to follow in order to
03:49 - come up with a solution trust me there
03:51 - are lot of data structures and lot of
03:52 - algorithms and lot of different
03:54 - techniques that you have to understand
03:55 - but when you combine these two you get
03:58 - somewhat a solution but how would you
04:00 - know that whether your solution is a
04:02 - good solution or a bad one for that we
04:04 - actually need some way to measure that
04:07 - how effective our solution is and in
04:09 - order to do that in computer system we
04:11 - use something called Big O So Big O is a
04:14 - notion where we compute what is going to
04:17 - be the worst case scenario time and
04:19 - space complexity and depending on those
04:23 - results we make the decision that
04:25 - whether the solution or the option of
04:27 - solutions we have which one is the best
04:30 - and most optimal one and which one are
04:32 - suboptimal options so before we start
04:34 - going deeper into the realm of data
04:36 - structure and algorithm let's first
04:38 - understand that what are the measurement
04:40 - units how do we actually understand that
04:43 - this is a good
04:47 - solution okay so let's talk about big O
04:50 - big O is typically defined by a big O
04:53 - and in the bracket we are going to Mark
04:56 - the complexity of Any Given algorithm
04:58 - now usually when when we talk about Big
05:00 - O we are usually talking about two items
05:02 - and those two items are time complexity
05:05 - and space complexity and remember the
05:08 - purpose of Big O is to identify the
05:11 - efficiency of Any Given algorithm now uh
05:14 - there are couple of ways to measure the
05:17 - efficiency of Any Given algorithm but
05:19 - the most important consideration that we
05:21 - are always going to have is that we are
05:23 - going to consider the worst case
05:25 - scenario that in the worst case how
05:28 - efficiently our algorithm is going to
05:30 - perform on top of that we are also going
05:33 - to check that for any given algorithm
05:35 - let's say that currently we are dealing
05:37 - with five number of inputs uh so in with
05:40 - five number of inputs what is the
05:42 - efficiency now let's say that the number
05:44 - of inputs grow to 5 million in this case
05:48 - what is going to be the efficiency and
05:50 - that becomes the true measure of Any
05:52 - Given algorithm and there is also one
05:54 - more condition consideration while
05:56 - calculating big hope is that we don't
05:58 - care about Hardware or software
06:01 - abilities for any given particular
06:03 - solution so we are assuming that our
06:05 - algorithm is currently running on the
06:07 - best hardware or software available so
06:09 - that's why we eliminate those outside of
06:12 - our control entities and only we focus
06:14 - on the efficiency part of our algorithm
06:17 - so first uh and foremost time or space
06:21 - complexity is Big O of one now currently
06:24 - I'm only going to talk about time
06:26 - complexity because that is sort of like
06:28 - more important of course space
06:29 - complexity is also important but time
06:31 - complexity is usually where you will
06:33 - have to improve whenever you are
06:34 - building your solution now in terms of
06:37 - that uh I'm going to currently show you
06:39 - five different examples of different Big
06:41 - O's uh for all I'm explaining it using
06:44 - the time complexity but throughout this
06:47 - course we are going to be solving bunch
06:49 - of different technical interview
06:50 - problems and where I'm also going to
06:53 - explain you the what space and time
06:54 - complexity we are using and how do we
06:56 - calculate that so don't worry about
06:58 - anything we are we are going to cover
07:00 - all the topics uh for that is for sure
07:02 - now first and foremost the Big O
07:05 - notation is a constant time Big O
07:07 - notation that is usually defined by like
07:10 - like this where in the middle there is a
07:11 - big one present this simply means that
07:14 - the current operation does not depend on
07:17 - the given input size and uh one clear
07:20 - example if you want to put it in the
07:22 - real world is that let's say that you
07:24 - are currently shopping in some uh Plaza
07:27 - some shopping cart or some somewhere
07:29 - like that
07:29 - and you can see in front of you that uh
07:32 - you have the option to pick any item
07:34 - from the five available items so since
07:37 - you have the option to see these items
07:40 - and you are being asked that hey uh in
07:42 - the bracket number three there is a
07:43 - banana that you need to pick you can
07:45 - just directly walk in and pick that
07:47 - banana up and in this case you don't
07:49 - have to think anything you just B there
07:52 - fetch that data and you completed your
07:54 - operation so same way instead of five
07:57 - items let's say if there are 500 items
07:59 - and we are assuming that your eye vision
08:01 - is the best you are able to see
08:02 - everything so in that case uh even if
08:05 - the value is located at 499 place and I
08:07 - tell you that go and grab that you can
08:09 - still be able to go and grab that within
08:11 - the constant time so timing would not be
08:14 - impacted based on the growth of number
08:16 - of input size so in this kind of
08:18 - scenario we can consider that operation
08:21 - to have a constant time now next
08:23 - operation is bigo of end time now what
08:26 - does bigo of end time means is that the
08:29 - number of inputs as the number of input
08:31 - size grows the more time it takes for us
08:34 - to complete that task and the
08:36 - progression is essentially linear so
08:39 - let's say that uh this is the graph
08:41 - where we can see that the number of
08:43 - input size and the time it takes to
08:44 - complete the line is going to be a
08:47 - straight line that they are in
08:49 - proportion to each other and if we have
08:51 - to consider an example for uh this kind
08:54 - of scenario let's assume that currently
08:56 - uh you have 10 items that that are
08:59 - already placed in 10 different pots and
09:03 - now these pots they are covered with
09:04 - different lids and you have no way for
09:07 - know that what is currently present in
09:09 - each of the pot so if I ask you that uh
09:12 - from these 10 pots I want you to find a
09:15 - banana so in this case what would be
09:17 - your approach well you will definitely
09:19 - go to the first place and in the first
09:21 - place you will try to check that uh if
09:23 - the banana is present uh currently
09:25 - banana is not present here so you will
09:27 - check the next spot you will check the
09:29 - the next part again you will check the
09:30 - next Sport and eventually you would find
09:32 - a place where has a banana and you would
09:34 - return that in that scenario let's say
09:37 - that instead of 10 values if I put like
09:40 - 1 million pots then essentially you will
09:43 - have to open 1 million pots in order to
09:45 - find the banana so there is a direct
09:47 - correlation between the given input and
09:50 - the size so that's why this would be a
09:53 - good example for bigo of n time
09:56 - complexity where our solution is direct
09:59 - ly in correlation with the given number
10:01 - of input size next one is actually big
10:03 - of log n and this is a logarithmic time
10:06 - operation this is quite un interesting
10:10 - to understand what it happens in this
10:13 - type of operation is that with every
10:16 - step you make you essentially eliminate
10:19 - 50% of given input and same thing
10:22 - happens when you make the next step and
10:24 - same thing happens when you make the ne
10:26 - Next Step so say initially uh uh this is
10:30 - your entire input size when you make the
10:32 - first change the change is going to
10:34 - enable you to essentially get rid of
10:37 - this entire part so now you are only
10:39 - dealing with these number of input sizes
10:42 - with the next step you took again you
10:44 - eliminated half of them so you get rid
10:46 - of this part and then now you're only
10:48 - dealing with this much portion of the
10:50 - given input and same way if you keep on
10:52 - going and going and going eventually you
10:54 - would be able to find the solution you
10:56 - are looking for if we have to consider a
10:58 - practical example for this type of
11:00 - logarithmic time I told you that hey
11:04 - this is the phone book now in this phone
11:06 - book I want you to find the number of
11:08 - Patrick the plumber now for this in
11:11 - order to find the number of Patrick the
11:12 - plumber what will you do is you will
11:14 - open the book somewhere from the middle
11:17 - and let's say that you open uh at where
11:20 - character or alphabet m in the middle
11:22 - portion so essentially you can eliminate
11:25 - that from a to M this Patrick the
11:27 - plumber is not going to be present so
11:29 - you will essentially get rid of all of
11:31 - these pages so let's say that uh it's a
11:33 - 500 page phone book you essentially
11:35 - eliminated 250 pages in just one go and
11:38 - in the next iteration you only have to
11:40 - check for n to Z and again you try to
11:43 - open some middle page and then you would
11:45 - be end up with this a smaller subset
11:47 - size so with every increment of step you
11:51 - take you get almost 50% closer to your
11:55 - uh given answer and you are eliminating
11:57 - 50% of input sizes in that so that is
12:00 - defined as bigo of log n or logarithmic
12:03 - n time complexity well usually the
12:06 - constant time complexity is the fastest
12:08 - and second fastest is the logarithmic
12:10 - time complexity now the next sequence is
12:13 - going to be the combination of two items
12:16 - and that is going to be bigo of n log n
12:18 - so bigo of n log n is an operation where
12:23 - essentially let's say that this is the
12:24 - current input you are given initially
12:27 - first of all you are you will have to
12:28 - itate over all of these places in the
12:31 - input but once you do that then for the
12:34 - next set of instructions you only need
12:36 - to do it in the logarithmic time so for
12:38 - the next operation you are essentially
12:40 - again eliminating half of the given
12:42 - inputs with every single change you are
12:44 - trying to make and this is quite an
12:47 - interesting approach one of the good
12:49 - examples for this one could be that
12:50 - let's say that you are playing a card
12:52 - game with your friends now initially in
12:55 - your card game in your hand you receive
12:57 - five random cards but you are are a
12:59 - person who wants who likes to do things
13:01 - in more of like a sorted array or sorted
13:04 - way so your approach is that hey uh
13:07 - since the these are random values let me
13:09 - put them in a correct sequence so for
13:11 - that what would be your approach your
13:13 - approach would be that first you are
13:15 - going to put down the value number two
13:17 - and you will treat that two is the
13:18 - minimum number and this is the new sort
13:20 - of sequence that you are trying to make
13:22 - now in this case currently since this is
13:24 - empty you can put two anywhere so you
13:26 - will put two in the first place now you
13:28 - need to put value number 7 now again you
13:30 - know that 7 is greater than two so you
13:33 - put 7 on the right side of two now you
13:36 - need to put value number three you
13:38 - already know that all the values that
13:39 - are currently present in this array they
13:41 - are already going to be sorted because
13:43 - one by one you are sorting them now in
13:45 - order to put this value number three you
13:47 - will have to find the location where it
13:49 - should be the ideal location and ideal
13:51 - location would be between two and 7 the
13:54 - three should fall between two and 7 so
13:56 - what would you do you would first try to
13:58 - find the middle Point Middle Point is
13:59 - currently let's say uh 2 and 7 so you
14:03 - pick seven so which means you know that
14:04 - three needs to be on the left side of
14:06 - seven so again there is only one element
14:07 - so you know that three needs to be in
14:09 - the middle so once again you adjust the
14:12 - values and you are going to put three
14:14 - over here and then seven over here again
14:16 - you have value number one so once again
14:18 - you will have to make adjustments or you
14:19 - will have to identify that one needs to
14:21 - go in the beginning and again you are
14:23 - going to use the binary search approach
14:25 - the same approach we were using in the
14:27 - previous uh
14:29 - application or previous phone book
14:31 - example that we saw so let me just
14:33 - redraw all of this and this would be the
14:36 - sequence and now you have value number
14:37 - five you need to put so once again you
14:39 - will try to find the middle value and
14:41 - you find that the correct location for
14:42 - five is going to be this place and you
14:44 - will again going to put it over here so
14:47 - now after this after completing this
14:50 - example if you see the solution you
14:53 - currently have is actually now sorted so
14:56 - let's calculate the the complex for this
14:59 - well you had to go through each and
15:01 - every input in order to receive or
15:04 - complete the steps so definitely you did
15:06 - n work but even for every single element
15:10 - once you did or process that you also
15:13 - had to do some work in order to put it
15:14 - in the sorted place so for that you are
15:17 - not doing the N work you are actually
15:19 - doing the log n work so in this case the
15:22 - Big O is actually going to be Big O of n
15:25 - log n so these are typically the four
15:28 - four most popular Big O's um in terms of
15:31 - like any architect any technical
15:33 - interviews that you have to consider but
15:35 - apart from that we also have to worry
15:38 - about bigo of N squ and bigo of n cub
15:42 - and bigo of uh 2 to the power n and then
15:46 - there is the worst one and that is bigo
15:48 - of n factorial so let me just quickly
15:51 - explain these four of four of you for
15:53 - this so suppose in this case uh big of n
15:56 - Square you already you can already guess
15:57 - that for every every single element we
15:59 - will have to work with all the other
16:01 - elements so let's say that currently I
16:03 - have this value number X now I want to
16:06 - check that in the remaining portion does
16:08 - X is present or not so one by one I will
16:11 - have to check all of these values and
16:13 - since it is not present so then I can
16:16 - say that okay for in order to process
16:19 - one element I had to process all the
16:21 - other elements so essentially I did B of
16:23 - n Square work so or B of n cross n work
16:27 - and same would apply for all the
16:29 - subsequent examples as well and uh that
16:32 - is how we reach big of n Square time
16:34 - complexity this is also uh referred to
16:37 - as quadratic time complexity then for
16:39 - big of n Cube again you are adding one
16:42 - more n over here so let's say that you
16:45 - this step needs to be done as part of
16:48 - one of your algorithm and then once you
16:50 - get the process of this you again have
16:52 - to do n work or compare with all of
16:55 - these values and once again repeat the
16:57 - same exercise in this case case uh the
16:59 - time complexity is going to be big of n
17:01 - Cub now this one big of 2 to the^ N is a
17:05 - Time complexity where at every single
17:07 - location you have the option to process
17:09 - two values and among these two options
17:12 - depending on which option you took your
17:15 - complete computation path would change
17:17 - so in these kinds of scenario the
17:20 - typical uh calculation is going to be
17:22 - big of 2 to the power n uh and the last
17:26 - one is the factorial time complexity so
17:28 - factorial time complexity is where the
17:31 - input sizes grow exponentially with the
17:35 - increase in the number of input size and
17:37 - this is actually a huge restriction
17:38 - because these two time complexities are
17:41 - so bad that they can't even function and
17:44 - one of the very good example for this
17:46 - big of 2 to the power nend time
17:47 - complexity is actually traveling
17:49 - salesman problem um if you want go ahead
17:52 - and search about that it's a quite an
17:53 - interesting problem to study that gives
17:55 - you quite lot of idea on how
17:58 - computational how there are limitations
18:00 - to computations as well uh for the N
18:03 - factorial one of the very good example
18:04 - is to find permutations and combination
18:07 - for all the given input sizes that we
18:09 - are trying to deal with so this is what
18:12 - you need to understand regarding the
18:15 - time and space complexity and uh now
18:17 - let's try to start understanding that
18:20 - what are the data
18:23 - structures now we are going to talk
18:25 - about data structures the most important
18:28 - topic that you have to understand for
18:30 - any technical interview or computer
18:31 - science in general now when we talk
18:33 - about data structures data structure is
18:35 - nothing but a way for us to store the
18:37 - data inside the computer's memory so
18:40 - let's assume that these are all the
18:41 - memory blocks of Any Given computer so
18:44 - we have the option to store the data in
18:46 - like an entirely common box like this or
18:50 - we have the option to save the data
18:52 - where one node is present over here the
18:54 - connecting node is present somewhere
18:56 - over here its next connecting node is
18:57 - present some somewhere over here and so
18:59 - on and so forth we also have the option
19:02 - where we know the option of that this is
19:04 - one node and then this node can only be
19:06 - accessed through these two nodes or
19:08 - something like that and then there are
19:10 - also quite connected nodes as well plus
19:13 - there are some structures data
19:14 - structures like q and stack so data
19:17 - structures in itself is a huge and
19:19 - complex topic now let's try to break
19:22 - this down into categories so it helps us
19:25 - understand what type of data structures
19:27 - are and how they operate so if we have
19:30 - to create the categorization data
19:32 - structures are typically two types of
19:34 - data structures mainly first one is a
19:36 - primitive data structure and second one
19:38 - is a non-primitive data structure now
19:41 - these primitive data structures are the
19:43 - smallest unit of data that we can store
19:45 - inside the computer and this is your
19:48 - integer value or your character value or
19:51 - Boolean value typically all the values
19:53 - that we usually use in any compiler
19:56 - language so to store store the raw bits
19:59 - of data in the computer and lucky for us
20:03 - or that this is too preliminary that it
20:05 - is not considered for any technical
20:07 - interview but you must have encountered
20:10 - this if you are a programming person so
20:13 - now the next category is the
20:15 - nonprimitive types of data structure and
20:17 - that two has further
20:19 - subcategorization so in the
20:21 - non-primitive type of data structure the
20:23 - first of characterization is the linear
20:25 - data structure and second one is the
20:28 - nonlinear data structure now for the
20:30 - linear as the name suggest linear data
20:32 - structure defines as the data that are
20:35 - typically stored in a sequence or in a
20:38 - linear fashion so this includes
20:41 - array then uh link list
20:45 - stack and Q so these are the major
20:49 - categorization of the linear data
20:52 - structure now when we talk about
20:53 - nonlinear data structure this is
20:56 - essentially the data structure that that
20:58 - is in a weird shape uh where the
21:01 - connection or the hierarchy between any
21:04 - two nodes is quite deep and quite
21:07 - significant and this has some of the
21:10 - most interesting properties actually so
21:12 - that is uh trees and graphs now for the
21:16 - apart from trees and graphs uh there are
21:19 - many subtypes in each one of them and
21:22 - when we get to those topics we will
21:24 - discuss it in the in more detail but for
21:26 - now understand that these are the to
21:28 - generic subcategories now the there is
21:30 - also third unofficial category which is
21:33 - really important for us to learn and
21:34 - that is Hash based data structure so
21:37 - hash based data structure typically
21:39 - includes uh hash map and hash set and
21:42 - these will these are very powerful tools
21:45 - at our disposal that we can actually
21:47 - achieve quite an interesting uh output
21:50 - with these data structure so without any
21:53 - further Ado let's just get started with
21:55 - the linear data structure
22:00 - now arrays are a not only they are a
22:03 - linear data structure but arrays tend to
22:06 - be fixed in the size and there are some
22:08 - properties associated with arrays so
22:10 - first let's understand that typically uh
22:14 - let's assume that if this is the whole
22:16 - memory of the computer then if whenever
22:19 - we Define an array we have to give it a
22:21 - sum size uh and the size of an array
22:25 - usually tends to stay within a fixed
22:27 - Contin ous memory space inside any given
22:30 - computer as well so let's say that we
22:32 - create an array with the size five then
22:36 - there would be a memory space like this
22:38 - with five consecutive memory spaces are
22:41 - going to be located next to each other
22:44 - and this is how it would be represented
22:47 - in the memory now since we already know
22:50 - that there are going to be five
22:51 - continuous bits of memory uh one good
22:54 - thing about array is that if you want to
22:56 - access any element even from anywhere in
22:59 - the middle you can access it quite
23:01 - quickly and very fast so uh let me give
23:04 - you an example for that suppose uh this
23:07 - uh this is currently the whole chunk of
23:09 - memory for the given array now how would
23:12 - the fast access work is that we already
23:14 - know that what would be the initial
23:16 - location of this position in the array
23:18 - inside our memory I'm giving you like a
23:20 - detailed explanation to help you
23:23 - understand how does arrays work so let's
23:25 - say that the computer memory address
23:27 - like this portion is referred to as 100
23:30 - and for the Simplicity we are expecting
23:32 - that all of these nodes they are 100 in
23:35 - the size which means that this if this
23:38 - memory location is 100 then this
23:40 - location is at 200 then this location is
23:42 - at 300 and so on and so forth so let's
23:45 - say that uh I currently wanted to access
23:48 - the fourth element that is this element
23:50 - present inside the given array so do I
23:53 - have to do any additional research or
23:55 - any additional extra effort no why why
23:58 - because I can quickly come over here I
24:00 - can see that okay this starts at Value
24:01 - number 100 I want to access the fourth
24:04 - element which means fourth element has
24:06 - to be present at Lo value memory value
24:09 - number 400 and that we can access quite
24:12 - quickly so just because we learn about
24:15 - the bigo time complexity the retrival
24:18 - bigo time complexity for array is
24:19 - actually going to be bigo of one or
24:21 - constant and how typically we access the
24:24 - elements inside the array uh we use
24:27 - something called indexing so arrays
24:29 - let's say that this is an array of size
24:31 - five because we can store five elements
24:33 - in this usually it is going to be
24:35 - indexed starting with value number 0o so
24:37 - 0 1 2 3 and 4 this is going to this is
24:41 - how array is going to be represented and
24:43 - you can store any item in the array you
24:46 - can store either character or you can
24:48 - store integer you can store Boolean you
24:50 - can store even some some of your
24:52 - separate classes you created all the
24:54 - things you can store inside the array
24:56 - but uh in order order to access that you
24:58 - will have to use these index values so
25:01 - let's say that I put down the values as
25:03 - uh 5 15 3 1 and two now if I want to
25:08 - access that what is located on index
25:10 - number two or position number three why
25:13 - position number three because this is
25:14 - the first element this is second and
25:16 - this is third element and we have an
25:18 - index starting from zero so why am I
25:20 - pointing you out this because this tends
25:23 - to be a tricky thing and many times you
25:25 - would encounter in technical interviews
25:27 - that they would say that hey I have an
25:28 - array that starts with index one so that
25:31 - this is like a common trap that you
25:33 - sometimes can find yourself into so
25:35 - that's why make sure that what type of
25:37 - index you are using 99% of cases you are
25:40 - going to be using an array starting with
25:41 - index number
25:43 - zero now let's just go back uh again so
25:46 - in this case uh let's say that the name
25:49 - of this array is a so if I want to
25:51 - access this third element all I need to
25:53 - do is try to fetch the value that is
25:56 - located and on the second index position
25:58 - and this would give us the answer as
26:00 - value number three so retrival inside
26:03 - the array is quite quickly now let's see
26:05 - that what are the different operations
26:07 - we we have the ability to perform inside
26:09 - an array let's say that currently we
26:11 - have a blank array present and what are
26:13 - the things we can do so let's mark down
26:16 - the index values now first operation we
26:18 - can do is we have the ability to insert
26:20 - element inside inside any given array
26:23 - now inserting an element is actually
26:25 - quite quick uh because if you know know
26:28 - which index location you are inserting
26:30 - element into you can actually do it in
26:31 - within constant time so B go of one time
26:34 - you can insert element you can insert
26:36 - element to the end of the array you can
26:37 - insert element to the beginning of the
26:39 - array all you need to know is the index
26:41 - value and that's it now second thing we
26:43 - can do is we can retrive the G the
26:46 - inserted array as well and retrival is
26:48 - also quite quick we we just saw so that
26:50 - is also a constant time next operation
26:53 - we can do is we can find element inside
26:56 - the given array so let's say that how
26:58 - does finding inside the given array
27:00 - would actually work and uh this is a
27:03 - question for you guys that while I
27:05 - explain try to think that what should be
27:07 - the time complexity for this one uh
27:09 - ready so let's say that currently the
27:11 - values are 5 15 3 and 1 and I ask you
27:15 - that inside this given array I want to
27:17 - find that uh value number three where it
27:19 - is located so how would you do it
27:21 - approach is going to be quite simple the
27:23 - logical explanation is going to be that
27:25 - you go to the first element if it is
27:27 - three that's it if it is not three go to
27:29 - the next element again not three go to
27:31 - the next element okay this is three then
27:33 - retri the index position and that's how
27:36 - you find it now in this case what was
27:38 - the time complexity well the answer is
27:40 - quite obvious because we had to jump
27:42 - through all the values in order to find
27:43 - the value we are looking for it was big
27:45 - of n so logically why it was big of n
27:50 - because this is unsorted array so in the
27:53 - unsorted array always if you have to
27:55 - find any element it's going going to
27:57 - take big off end time but let me show
28:00 - you a quick uh interesting thing over
28:03 - here let's assume that I currently have
28:06 - a sorted array so 1 2 3 4 5 these are
28:09 - the elements I have and now I want to
28:11 - find that whether four is present or not
28:14 - so in this case what what is going to be
28:15 - the approach since I already know that
28:18 - this is a sorted array I'm going to
28:20 - first check for the middle element and
28:22 - again I can check what element is there
28:25 - pretty quickly because retrival is Big
28:27 - go of one so that is an important
28:29 - property so in this case I can see that
28:31 - this value is three the moment I realize
28:33 - that this value is three I can say for
28:35 - certainty that four is not going to be
28:38 - anywhere left of three because all the
28:40 - valents all the elements are going to be
28:42 - less than three and we have we are
28:44 - trying to find for the value number four
28:46 - that is actually greater than three so
28:48 - we can eliminate these two or these
28:50 - three elements with a single check again
28:54 - we will try to check for the middle
28:56 - element and we find Value number four
28:57 - and we can retrive that so if the array
29:00 - is unsorted the finding is going to take
29:02 - big off end time but if the array is
29:05 - sorted then in that case the finding is
29:07 - going to take because of log log and
29:09 - time and why because the method we just
29:12 - use is actually called binary search
29:15 - where we are dividing the all the
29:17 - elements in one half or the other half
29:20 - and then we are essentially making
29:22 - judgment on which direction to choose
29:24 - and find the answer in the most optimal
29:26 - manner so finding uh takes B of end time
29:30 - next question is deletion so deletion is
29:34 - also if you know what place you wants to
29:36 - delete it only takes big off one time
29:38 - because all you need to do is just go to
29:40 - that index and get rid of that value so
29:42 - deletion is also very quick inside the
29:44 - array now we all find quite interesting
29:48 - items about array let's see that what
29:50 - are some of the drawbacks of an array
29:52 - well the number one drawback of an array
29:54 - is that it is actually a continuous
29:56 - memory size
29:57 - and it is also a fixed memory size so
30:00 - what do I mean by fixed memory size
30:02 - because initially when we are defining
30:04 - an array we actually have to Define that
30:06 - what is going to be the size of an array
30:08 - and let's say if we exceed the size of
30:10 - that array then the operation becomes
30:13 - quite expensive uh let me give you an
30:15 - example so in this case suppose the
30:17 - array is 1 2 3 4 5 these are the five
30:19 - values now we realize that in the same
30:21 - array we need to add more elements so
30:24 - what behind the scenes procedure is
30:26 - going to be that
30:27 - there will be a new array created of
30:30 - size 10 and now in this new size 10
30:33 - array first it is going to shift all of
30:35 - these elements one by one and after that
30:38 - once the shifting is done then the next
30:41 - element that you wanted to add over here
30:43 - value number six would be added over
30:45 - here so previously when we have to
30:48 - insert any element inside the given
30:50 - array that was a bigo of one operation
30:53 - the moment we exceeded the size the
30:55 - operation became bigo of n operation uh
30:59 - because we had to repeat this whole
31:01 - process and that too when we did that
31:04 - what was also another fault with that is
31:07 - that currently all of this space is
31:10 - actually unused so we we are already
31:14 - running on a very important resource
31:16 - that is our computer's memory and we are
31:18 - leaving some values unused because they
31:20 - have not been filled yet so this is one
31:23 - of the biggest issue with the arrays now
31:26 - second issue with the array is that uh
31:29 - actually if you want to find any element
31:31 - then it takes bigo of end time because
31:33 - that you have no way for to understand
31:36 - that where any particular element is
31:37 - present so this is also another issue
31:40 - with an array but apart from that if we
31:42 - have to talk about benefits of an array
31:44 - well uh there are quite a few benefits
31:47 - actually if you believe it or not number
31:49 - one it's the simplest algorithm to
31:51 - understand or simplest data structure
31:53 - number two it is actually quite uh
31:56 - useful and and quite fast uh quite fast
31:59 - means that whenever you have to retrive
32:01 - any element from any particular index
32:03 - you can actually do it quite quite
32:05 - quickly uh allocation memory allocation
32:07 - is not an issue this works really well
32:11 - with other data structures when you are
32:13 - trying to solve some complex problems so
32:16 - when we get to the points of dynamic
32:18 - programming and Matrix manipulation we
32:21 - are going to learn quite a lot about
32:22 - arrays and we are going to use lot of
32:24 - arrays in many different scenarios now
32:27 - let's try to understand that how does a
32:29 - typical array problem work with an
32:31 - example and the example we are doing is
32:33 - actually the number one problem on the
32:35 - lead code and one of the most popular uh
32:37 - interview question that is two some
32:39 - problem so what problem statement
32:41 - suggest is that suppose we are given an
32:43 - unsorted array and uh in this case we
32:46 - have five elements so let's give some
32:48 - arbitrary values of these elements and
32:51 - in this case uh we are trying to
32:53 - identify that uh the we are also given a
32:57 - sum value so let's say that sum property
32:59 - is currently given as value number 17
33:02 - now we are trying to find that find a
33:05 - pair that actually has the answer as
33:08 - value number 17 and then return its
33:11 - index position so we are know that the
33:13 - answer is actually going to be in this
33:15 - case 2 and three because these are the
33:17 - 10 + 7 but let's see that what is going
33:20 - to be the approach that we need to take
33:22 - so this will give us a very good Insight
33:25 - on how does algorithms also work and how
33:27 - does time and space complexity works so
33:30 - let's see the first approach first
33:32 - approach is going to be our Brute Force
33:34 - approach or the most preliminary basic
33:36 - approach the idea we are going to use is
33:39 - that we will take any element we will
33:42 - minus it with 17 so whatever the
33:44 - remaining element is left we will try to
33:47 - see that in the remaining array does
33:48 - that remaining element is present or not
33:51 - uh so if we are considering five so
33:54 - let's assume that five is currently part
33:56 - of this two some so which means if five
33:59 - is part of this 17 that makes 17 then
34:02 - there has to be a remaining value 12
34:04 - present inside the remaining array and
34:07 - we all know if we have to find any value
34:09 - inside the array we can do it in B go of
34:11 - end time because this is an unsorted
34:13 - array so we are going to jump through
34:15 - all of these values to find Value number
34:17 - 12 but currently 12 is not present so
34:20 - because 12 is not present we can
34:22 - consider five not to be part of the
34:24 - answer then we will go back to Value
34:26 - number three once again 17 - 3 value is
34:29 - going to be 14 so we will try to see
34:31 - that whether 14 is present again 14 is
34:33 - also not present so we will get rate of
34:35 - three again for seven So currently we
34:38 - are considering value number seven which
34:40 - means for the sum we need to have value
34:42 - number 10 present somewhere and yes 10
34:45 - is present over here so in this case we
34:47 - did find the answer as 2 and three
34:49 - because we need to return the index
34:50 - value and we can return it quite quickly
34:53 - but if I have to ask you that what is
34:55 - going to be the time time complexity or
34:58 - the Big O for this given input what
35:00 - would be the answer it's quite obvious
35:03 - for any single element we have to search
35:06 - all the remaining elements and if we do
35:09 - or consider that in the worst case
35:11 - scenario essentially the time complexity
35:13 - is going to be biger of n Square so that
35:17 - let's see that can there be a better
35:19 - approach using the array to solve this
35:21 - problem and the answer is actually yes
35:24 - there is a much better approach and
35:26 - first let me give you a slightly better
35:28 - approach right so a better approach in
35:31 - this case would be that what if for the
35:34 - given input array we decide to sort that
35:37 - and if we sort this given input array we
35:40 - will find the values to be 1 3 5 7 and
35:44 - 10 now once we have these values stored
35:47 - all we need to do is uh we need to check
35:50 - that uh whether for Value number one if
35:53 - the subsequent pair the value number 10
35:56 - n is present or not and in this case
35:59 - since originally we need to find the
36:01 - index value when we create the Sorting
36:03 - we have to create actually a new array
36:05 - so this is an important piece of
36:08 - information so just Im Just a thing so
36:10 - now if we have value number one which
36:12 - means we are trying to find the sum to
36:14 - be 17 so then uh 17 minus 1 is going to
36:19 - be 16 so 16 needs to be present so since
36:23 - this is a sorted array we can actually
36:25 - find in log n time
36:27 - that whether 16 is present or not and
36:29 - all we need to do is we need to apply
36:31 - the binary search where we first check
36:33 - the middle value So currently this value
36:35 - number five so we make sure that 16 has
36:38 - to be somewhere on the right side but
36:39 - since it is not present over here we are
36:41 - not going to bother checking same thing
36:43 - we are going to check with for the value
36:45 - number three that again for Value number
36:48 - three if that is has to be part of the
36:49 - answer we can actually try to find the
36:52 - answer in log and time so to check
36:54 - whether value number 14 is present or
36:56 - not again that is also not present for
36:58 - five value number 12 is also not present
37:01 - but when we get to Value number seven
37:03 - actually the value number 10 is present
37:05 - and we can find this in log n time so
37:08 - what do what did we did we found out
37:11 - that the correct values we need are 7
37:13 - and 10 so once we have the solution we
37:16 - again need to go back to our original
37:18 - array because we need to return the
37:20 - index values so now once again we would
37:22 - search all of these values one more time
37:25 - to see where what are the index location
37:28 - and then return the values now let's try
37:30 - to calculate the time and space
37:32 - complexity in this case so for the time
37:34 - complexity what are the operations we
37:36 - did number one operation is that for the
37:38 - given input array we actually sorted
37:40 - this so once we sorted this given input
37:43 - array uh it took us bigo of n log n time
37:47 - to sort because based on the example I
37:48 - showed you earlier plus after sorting it
37:52 - we had to iterate over all of these
37:54 - elements to find that what is is the
37:56 - subsequent or remaining element that is
37:59 - present or not and this so iterating
38:02 - over all of these elements took us B go
38:04 - of end time and uh in order to find that
38:08 - for each one of them the subsequent
38:10 - element is present or not again it took
38:12 - us beo of log n time so again we did B
38:15 - of n log n so in total we did two times
38:18 - B go of 2 * n log n but the interesting
38:23 - thing about Big O is that all the
38:25 - constant values are not considered so we
38:28 - are not going to consider big two in
38:30 - this case and we will only consider this
38:32 - to be big off and login on top of it
38:35 - once we find the correct pair we again
38:37 - had to go back to our original array to
38:40 - find the uh appropriate index values so
38:43 - for that we also did Big O of n work but
38:47 - when we are generalizing the Big O we
38:50 - only care so whenever there is a plus
38:52 - operation or the additional task
38:55 - essentially our main main component is
38:57 - still heavily going to be dependent on
38:59 - big of n login because that is greater
39:02 - than big of n so we can also ignore this
39:05 - one and the time complexity is going to
39:07 - be big go of n log n in order to
39:09 - generate this now in this case space
39:12 - complexity is also an issue why because
39:15 - we had to create an additional new array
39:17 - for the sorted property and because the
39:21 - new arrays size would be dependent on
39:24 - the original size and the original size
39:27 - we are considering the value to be n
39:29 - because of that in this case the space
39:32 - complexity is also going to be bigger of
39:33 - n so I wanted to show you this approach
39:37 - to show you that how to calculate time
39:40 - and space complexity and what are the
39:42 - considerations we need to make so I hope
39:44 - you find that uh very useful and you
39:46 - find the idea now of course this
39:49 - solution is much better than our
39:50 - previous Pro Force solution which gave
39:52 - us the result of big of n square but
39:55 - still this is not the most optimal
39:57 - approach there are still optimal
39:59 - approaches available and the optimal
40:01 - approach uh let's see that how we are
40:03 - going to deal with it well actually I
40:06 - should not be showing it to you but I'll
40:08 - just show it to you for the optimal
40:10 - approach we are actually going to use
40:11 - another data structure called hashmap
40:13 - and when we get to that point we will
40:15 - I'll explain you in much higher detail
40:18 - that what an hashmap is but for now
40:21 - consider that hashmap is a key value
40:23 - pair uh data structure that looks like
40:26 - like this where you have a key and you
40:29 - have a value and if you know the key you
40:31 - can find the value associated in
40:34 - constant time we go off one time so it's
40:36 - very quick now let's just draw back our
40:39 - uh original array so I think the values
40:42 - were 5 1 7 10 and 3 and we are trying to
40:45 - find the sum for Value number 17 now
40:48 - again the approach is going to be that
40:50 - if let's assume that five needs to be
40:53 - the part of the answer so if five has to
40:55 - be part of the answer sir we need to
40:57 - make sure that value number 12 is
40:58 - present somewhere in the remaining array
41:01 - now currently we don't want to check the
41:03 - whole remaining array because if 12 is
41:05 - present eventually we would also
41:07 - encounter 12 somewhere down the road as
41:09 - well by the time we would have already
41:11 - processed value number five so what we
41:13 - are going to do is we are first check
41:15 - that whether 12 is present inside the
41:17 - hashmap or not currently it's not
41:19 - present in the hashmap if it is not
41:21 - present in the hashmap we are going to
41:23 - add an entry of key value pair over here
41:25 - where we we will add value number five
41:27 - as the key so this value and its
41:30 - subsequent index value as its value
41:32 - because we need to return this in the
41:34 - answer now again going back currently
41:37 - value number one if one has to be part
41:39 - of the answer then value number 16 has
41:41 - to be present but currently in the
41:42 - hashmap we don't have value number 16 so
41:45 - again we are going to add one more entry
41:47 - that is value number one and oh sorry
41:49 - for this five the initial entry is going
41:51 - to be zero because its index location is
41:53 - zero so for this one the index location
41:55 - is one 1 once again for this 7 we need
41:59 - to find the value number 10 but 10 is
42:01 - not currently present inside the hashmap
42:03 - and we are able to check all of these in
42:05 - big off one time so that's a great
42:07 - benefit now we are not able to check
42:09 - this one so what we will do is we will
42:11 - actually add entry number seven over
42:13 - here with the index value of two now
42:16 - let's add one more room and now we are
42:18 - at Value number 10 so if 10 has to be
42:20 - part of the answer then 7 needs to be
42:22 - present somewhere in the array and 7 is
42:25 - already present in the Dash map because
42:26 - we have already processed that value the
42:29 - moment we realize that all we need to do
42:31 - is grab the index element of 10 that is
42:34 - value number three and grab the index
42:36 - element of value number seven that we
42:38 - can find it from the hashmap over here
42:40 - and we can return return uh 3 and two as
42:43 - the answer that this is the index pair
42:46 - that makes us the two sum possible
42:48 - inside the given array and now let's
42:51 - break down the time and space complexity
42:53 - for this solution so of course time comp
42:56 - complexity actually is very fast in this
42:58 - case because we only have to iterate
43:00 - over the given input array just once and
43:03 - we would be able to find the array so
43:05 - the time complexity is actually going to
43:06 - be dependent on big of n the given
43:09 - number of input sizes which is much
43:11 - better compared to both our Brute Force
43:13 - approach and our sorting approach now in
43:16 - terms of space
43:18 - complexity uh we actually have to create
43:20 - an additional hash map and the size of
43:22 - the hash map is actually going to be the
43:24 - size of the array maximum in the worst
43:26 - case scenario so again the space
43:28 - complexity is also going to be bigger of
43:29 - N and this is how you actually solve
43:32 - this problem so now let's quickly try to
43:35 - understand the problem statement for two
43:37 - some problem that is I just explained it
43:39 - to you and the coding solution so we
43:42 - have a Java code over here and first of
43:44 - all we have the two some method where we
43:46 - need to return the index values for both
43:48 - the positions and in the input we are
43:49 - given the num nums array and the target
43:52 - value or our two sum now first of all we
43:55 - are initializing our hash map where we
43:57 - are going to store the values of the
44:00 - subsequent values we processed inside
44:02 - the given array and also their indices
44:05 - then we are simply going to run a for
44:07 - Loop to iterate over our given array
44:09 - first of all we will try to see that
44:11 - what is the complement value that we
44:13 - need to check and then we will we can do
44:15 - that by subtracting the current array
44:17 - position minus the target value once
44:19 - that happens we check that if the
44:22 - hashmap already contains that complement
44:24 - value or not if it it contains the
44:26 - compliment value we can return the index
44:28 - positions quickly if it does not contain
44:31 - then we are going to add that value in
44:33 - our hashmap and in the end I'm just
44:35 - adding uh an illegal argument exception
44:38 - that there is no toome solution but this
44:40 - is never going to be the case because we
44:41 - are explicitly told that there is always
44:43 - going to be exactly one solution so
44:46 - let's try to run this code and this code
44:49 - runs pretty quickly let's submit this
44:51 - code and our code runs quite efficiently
44:54 - compared to a lot of other Solutions
44:59 - now let's learn about the next data
45:01 - structure that is link list Now link
45:03 - list is quite an interesting data
45:05 - structure because it's not what array
45:09 - was at all uh in the array we actually
45:13 - had all the memories being stored in a
45:15 - continuous block and the size used to be
45:18 - fixed link list is actually complete
45:20 - opposite of this let's say that this is
45:22 - currently the memory then we would have
45:24 - a link list that would look look
45:26 - somewhere something like this it would
45:27 - be sprad out all across computer and the
45:30 - nodes would be connected with each other
45:33 - where one node would know the address of
45:35 - the other node and that is how we
45:37 - actually Traverse over any given link
45:39 - list so typically link list is a dynamic
45:42 - structure well of course it is a linear
45:44 - data structure but it is also dynamic in
45:47 - nature and uh typically a link list
45:50 - looks like this where we only have the
45:51 - information of the first element that is
45:53 - present inside the link list nothing
45:55 - more than that that but if we have to go
45:58 - and reach to any particular element we
46:01 - can actually do it by going sequentially
46:04 - to from that original first element to
46:06 - the last element and we would be able to
46:08 - find that value somewhere in between so
46:10 - typically a link list would look
46:12 - something like this where you have these
46:15 - memory blocks or these nodes and the
46:18 - first node is referred as the root of
46:20 - the link list and in every single node
46:23 - there is going to be a partition and
46:25 - what this partition defines is that this
46:28 - would have the memory location of the
46:31 - next element inside the link list so
46:34 - let's assume that currently we have this
46:36 - value number five and this is located at
46:38 - memory location 100 now this particular
46:41 - block is going to have the memory
46:43 - location of this block so let's say that
46:45 - this block is currently located
46:47 - somewhere randomly at Value number 600
46:49 - so that 600 would be stored over here
46:53 - once again let's give some arbitrary
46:55 - value over here so let's say that value
46:56 - number three is the value for this node
46:59 - next element it has is it could be
47:01 - anything it could be 300 and now at 300
47:03 - memory location we have another node
47:06 - that is present with value number seven
47:09 - um and same way it has the address for
47:12 - this next element so basically the
47:15 - structure of a link list looks something
47:18 - like this where we have the first
47:20 - element that points to the next that
47:23 - points to the next and that points to
47:24 - the next and the last element actually
47:27 - points to the null value and when we see
47:29 - an element pointing to the null value
47:32 - that defines that whether the given link
47:35 - list uh where it ended basically now for
47:39 - the link list it's actually uh there are
47:42 - three different types of Link list now
47:44 - what I just showed you this is called a
47:46 - singly link list and why singly link
47:48 - list because one element knows that what
47:51 - is going to be the element after that
47:53 - but any particular element has no idea
47:56 - that what is the element before me
47:58 - because it does not have that address so
48:00 - that is why this is referred as a singly
48:02 - link list where you have the idea of
48:04 - what the single next node is going to be
48:07 - in the iteration but there is also a dou
48:10 - link list and W link list as the name
48:13 - suggest it has the idea that who is the
48:15 - previous caller and who is going to be
48:16 - the next caller so the structure is
48:19 - usually like this where you have
48:21 - additional memory slots uh where we
48:24 - store the information so something that
48:26 - looks like this and uh once again so
48:30 - let's assume that this is currently the
48:32 - root node so even for the root node now
48:34 - in this case since root node is not
48:37 - being pointed by any other node this
48:40 - address is going to be null value and
48:42 - then we will have some value being
48:44 - associated with this so let's say 10 so
48:47 - 10 11 3 1 these are the values
48:50 - associated with the link list but now
48:53 - this 10 this portion is actually going
48:55 - to to contain the address of this
48:58 - element so it has the address 300 let's
49:01 - assume and we can also assume that this
49:04 - address is 50 so this value number 50
49:07 - would actually be stored over here that
49:10 - this second node would know that who is
49:12 - the first node that is calling them so
49:16 - when this node needs to go back or
49:18 - Traverse in the reverse order it can
49:20 - also do that so when traversing in the
49:22 - reverse order uh it will look something
49:25 - like this that this would actually point
49:26 - to this address and uh same way this has
49:29 - the address over here this has this
49:31 - address and this has this address and
49:33 - this is this address and in the end we
49:35 - have this last element that actually
49:37 - points to a null value and when we see
49:39 - last element pointing to the null value
49:41 - which means that the link list has ended
49:44 - or W link list has ended and when we see
49:46 - first element pointing to the null value
49:48 - which means that the W link list
49:50 - actually started there is one more link
49:53 - list that is called a circular link list
49:55 - and in the circular link list as the
49:57 - name suggest essentially there is a
49:59 - cycle that keeps on happening and uh how
50:03 - it operates is that usually if we
50:06 - consider it's an additional extended
50:07 - version of The W link list so let's say
50:10 - that currently we have a w link list
50:11 - where uh in the normal case scenario
50:14 - this W link list would be pointing to
50:16 - the null value in the first place but
50:18 - let's assume that that is not the case
50:19 - and currently let's just give some
50:20 - arbitary values 1 2 4 7 these are the
50:23 - values right now we know that that one
50:26 - is pointing towards two and two is
50:27 - pointing towards one same way this is
50:30 - relation is already stable now
50:33 - previously in the normal link list the
50:35 - last element its next node would be a
50:37 - null value but in this case the next
50:40 - node is actually going to point back to
50:42 - the very first value and same way this
50:44 - very first node is actually going to
50:46 - point back to the last element so that
50:48 - is why there is a circle circular nature
50:51 - happening inside the link list now
50:53 - understanding this let's see that what
50:54 - are the different operations we can
50:56 - perform on any given link list and for
50:58 - reference I'm just drawing a simple
51:00 - singly link list but same would be true
51:02 - for all the W link list and circular
51:05 - link list as well and uh let's define
51:08 - some arbitary values and currently this
51:11 - one is pointing towards null and uh
51:13 - currently this is going to be the root
51:15 - of our link list so we have the option
51:18 - to add values to the link list now if we
51:21 - have to add values to the link list the
51:24 - time complexity depends on where you are
51:26 - trying to add so let's say that if you
51:28 - are trying to add the link list to the
51:30 - root element and sometimes this is also
51:32 - referred as the head of the link list as
51:33 - well so let's say that you are trying to
51:35 - add elements to the root or the head
51:37 - element of the link list it's actually
51:39 - quite simple all you need to do is that
51:42 - you will first try to go to the root
51:43 - element you would see that what is the
51:46 - current memory address for this root
51:48 - element and then you would actually
51:50 - create another node and let's mark the
51:53 - value as nine you would point point this
51:55 - nine back to this root Two element and
51:58 - that's it now you have essentially
52:00 - created a new head element or the new
52:02 - first element where this is this becomes
52:05 - the root and this is being pointed by
52:08 - this value number nine or root so in
52:11 - this case uh you only have access to the
52:14 - next element so you give the next
52:17 - element of the previous head to the new
52:19 - head and this can be done in big of one
52:21 - time constant time operation same goes
52:24 - that if if you want to add an element
52:27 - somewhere in the middle well this the
52:29 - whole procedure is actually quite simple
52:32 - let me first show you the procedure and
52:33 - then we will talk about the time
52:34 - complexity well the procedure would be
52:37 - that let's say that I want to add a node
52:39 - number seven over here so what I would
52:41 - do is that currently this four is
52:44 - pointing to the one rather than doing
52:46 - that I'm going to Mark 4 do next or the
52:49 - next element that four is pointing to to
52:51 - 7 and then 7 do next to the previous
52:55 - element that four was pointing to so the
52:57 - value number one so we would have a
52:59 - temporary variable in between but
53:00 - essentially you get the idea what I'm
53:02 - trying to say and now this process only
53:05 - took big off one time but now the tricky
53:08 - part in this case is that in order to
53:10 - reach to the four we actually have to
53:12 - first go to Value number nine and then
53:14 - through nine we would do sequential
53:16 - jumps until we reach to Value number
53:18 - four and then we would be able to make
53:20 - this changes so if we have to make the
53:23 - change somewhere in the middle uh the
53:26 - time complexity is actually big of N and
53:28 - same goes for the end as well because if
53:31 - you have to add it into the end you will
53:33 - also have to update and go to the end
53:35 - and then that would also yield the time
53:37 - complexity of big of n but now there is
53:40 - there are some slight variation
53:42 - difference where if you have a w link
53:44 - list and you wanted to add to the last
53:46 - element maybe you can do it faster if
53:47 - you already have that information stored
53:49 - or you can go in the reverse order or in
53:51 - the circular link list you can do it
53:53 - quickly in the end but that these are
53:56 - rare anomalies that you are rarely going
53:58 - to encounter now let's see that if you
54:01 - have to delete an element how does it
54:03 - would work once again deletion process
54:06 - is quite simple that all you need to do
54:08 - is that let's say I want to delete this
54:10 - value number seven so what I would do is
54:12 - currently the next node that seven is
54:14 - pointing to I would ask four to point to
54:16 - that node and that's it seven has been
54:18 - deleted because now there is no way for
54:20 - us to reach to seven and so this the
54:23 - process in itself is quite easy but
54:26 - again the same question remains that in
54:29 - order to get to the value number four we
54:31 - will have to do all of these Hops and
54:33 - get there so if we have to do some
54:36 - deletion in the start it takes big off
54:38 - one time but in the middle it takes big
54:40 - off end time now let's see that what is
54:43 - the next operation we can do and that is
54:45 - finding an element inside the link list
54:47 - once again you get the idea you can find
54:49 - the element but once again in order to
54:51 - find the element you will have to
54:52 - Traverse through all of those these
54:54 - Hoopes in order ultimately till you
54:56 - reach that element so that two is going
54:58 - to be big go of end time and once again
55:01 - uh the last one is that you can also
55:03 - modify any existing element but in order
55:06 - to do that that also takes big off end
55:08 - time so essentially all the operations
55:11 - in the link list if they are done
55:12 - somewhere in the middle or the end of
55:14 - the link list it takes big off end time
55:16 - so now the question comes that why do we
55:19 - even care about link list why should we
55:21 - use it well one of the most important
55:23 - benefit is that it it's dynamic in
55:25 - nature so it keeps on growing and
55:27 - growing you don't have to worry about
55:29 - memory allocation size allocation
55:31 - because it can find sizes anywhere so
55:34 - anywhere there is an empty block of
55:35 - memory present link list can acquire
55:38 - that put it in the memory and do it on
55:41 - top of it swapping any two elements so
55:44 - if this pointing over here we can
55:46 - actually start pointing or add couple of
55:48 - new link list and then bring them back
55:50 - and then get rid of this uh list this is
55:53 - actually quite simple and quite easy so
55:55 - in many of the branching strategies this
55:58 - is actually quite helpful that uh you
56:00 - can actually create all sorts of like
56:02 - get a branch and repos and stuff like
56:04 - that using link list on top of it uh
56:06 - many times we are going to use just like
56:09 - arrays link list in tandem with other
56:12 - data structures so we will see that when
56:14 - we get to that point now let's talk
56:17 - about uh some of the examples of how
56:19 - typically we solve any problem inside a
56:22 - link list so one of the popular very
56:24 - popular problem is that we need to find
56:27 - the middle of the link list or middle
56:28 - element of the link list so how do we
56:31 - actually do that so let's assume that we
56:33 - are currently given a singly link list
56:35 - and we don't know that what is the size
56:38 - of this link list so how do we encounter
56:40 - that what would be the middle point for
56:42 - this one so one approach is that uh it's
56:46 - the simplest approach we actually start
56:48 - at the root value or at the head of the
56:50 - link list we keep on going to the next
56:52 - element until we reach to a point where
56:54 - the next element points to a null and
56:56 - the moment we reach that we would know
56:58 - the size of the link list so in this
57:00 - case we found out that the size of Link
57:02 - list is five so middle element is going
57:04 - to be the third element then once again
57:06 - we start once again from the root and
57:08 - let me use another color and then you we
57:10 - hop back and we reach to the middle
57:12 - element and then we say that okay this
57:14 - is the middle pointer of the link list
57:16 - so this this can be one of the examples
57:18 - so what is going to be the time
57:19 - complexity well time complexity is going
57:22 - to be big of n because we have to go
57:24 - through essenti all the inputs but there
57:26 - is also another better approach where
57:29 - once again time complexity is going to
57:31 - be biger of n but in practice it is
57:33 - going to be faster and that is that
57:35 - rather than making one hop you actually
57:38 - make two hops and how to do that well
57:41 - you can actually have two pointers so
57:44 - one pointer is a fast pointer and the
57:46 - second one is a slow pointer both start
57:48 - at the same time but slow pointer makes
57:50 - one jump and fast pointer makes two
57:52 - jumps so the moment Fast Point pointer
57:55 - reaches to the end of the link list the
57:57 - middle pointer or the slow pointer has
57:59 - to be in the middle and that you can
58:00 - return as the answer so in this case the
58:02 - time complexity would be big of n
58:04 - divided by 2 so overall since 2 is the
58:07 - constant value we would still write it
58:09 - as big of n but in practice this is
58:11 - going to be much faster compared to the
58:13 - other approaches so now let's try to
58:16 - understand the middle of the linkless
58:17 - problem and as the problem statement
58:19 - states that we are given the head of a
58:21 - singly link list and we need to return
58:23 - the middle node of the link list now
58:25 - this is the Java code for that and
58:27 - typically for the link list we are
58:29 - initializing two pointers for that first
58:32 - one is known as slow and second one is
58:34 - known as fast and currently both are
58:36 - located at the very first be uh
58:38 - beginning position of the link list now
58:39 - we run a v Loop that while fast is not
58:42 - equal to null and fast. next is not
58:44 - equal to null we are going to keep on
58:46 - moving the slow pointer to one jump so
58:48 - slow will go to the slow. next or the
58:51 - next element inside the link list and
58:53 - fast would go to fast do next do next so
58:56 - it would make two jumps at a time and in
58:59 - the end the moment this Loop ends we
59:01 - would be at a position where fast is at
59:03 - the last position or the next element to
59:05 - the fast is actually the null value in
59:07 - either case so slow has to be the middle
59:10 - pointer of the link list and we can
59:12 - actually return that so let's try to run
59:14 - this code and our solution works
59:16 - perfectly fine so let's submit this and
59:19 - our code runs with 100% efficiency so
59:22 - this is pretty good and uh all the code
59:24 - that I'm showing you would be posted in
59:26 - a gith up public repo so the link is
59:28 - going to be in the description so you
59:29 - would be able to check it out from
59:33 - there now let's see another data
59:35 - structure that is a
59:37 - stack now stack is once again very
59:41 - similar to an array or link list it's a
59:44 - linear data structure but uh the stack
59:47 - has a very specific property and that
59:50 - specific property is actually that stack
59:52 - is something like this uh you can
59:54 - consider it as like a test tube um in
59:57 - any lab or any sort of thing or a stack
60:00 - of plates this would be like a good
60:02 - example to understand what stack is
60:05 - where the value where you first store
60:08 - sto gets stored at the very bottom once
60:11 - that value is filled then the next value
60:13 - when you have to put in that would
60:15 - become at like the bottom to the uh
60:18 - second place and it will keep on going
60:21 - now when you have to take things out you
60:24 - will always take things out that are
60:26 - currently at the very top because you
60:29 - cannot reach to the bottom element
60:30 - because they are already Behind These
60:33 - memory lines so in order to reach to the
60:35 - bottom elements you will actually have
60:37 - to go through one by one for all the
60:39 - values so let's see this an example
60:41 - let's say that currently our stack is
60:42 - empty we are trying to fill in some
60:44 - values so let's say that we put down
60:46 - value number one first so one has to go
60:48 - at the very bottom now the we have we
60:51 - are putting value number two then three
60:52 - then four then five and then six So
60:54 - currently we have six elements inside
60:56 - our stack now when we start taking the
60:59 - values out actually the only value we
61:03 - can take out at this moment is value
61:05 - number six because that is the only
61:07 - value we have access to because this is
61:09 - like a test tube or a a stack of plates
61:12 - kind of a structure so that is why when
61:15 - we decide to take things out we are
61:17 - going to take value number six out first
61:20 - after taking value number six out then
61:22 - we we can start taking value number five
61:24 - out after five we will get to the value
61:27 - number four and so on and so forth so
61:29 - essentially the last element that was
61:32 - encountered or entered inside the stack
61:35 - would be the first element that is going
61:37 - to come out so this is this works on the
61:39 - Leo principle last in first out okay so
61:44 - and that's it this is what the whole
61:47 - stack is uh now if you have to see what
61:50 - are the operations you can do inside a
61:52 - stack uh you actually have
61:55 - some limited operations but they have
61:57 - very specific use cases so let's say
61:59 - that currently we have a stack like this
62:01 - 321 there are three elements so number
62:03 - first thing we can do is we can push
62:05 - value inside the stack because we
62:08 - already pushed these three values uh
62:10 - inside the stack so pushing element
62:12 - inside the stack is actually quite
62:14 - simple all you need to do is just push
62:16 - it and whichever is like sequentially
62:19 - available memory block so let's say that
62:21 - this is a stack like this so this would
62:23 - be the element that gets filled so this
62:26 - takes big off one time now you also have
62:29 - the option to pop a element out so when
62:31 - you pop an element out you only have
62:33 - access to just pop out the element that
62:36 - is currently located at the first pace
62:38 - so in this case you can only pop out
62:40 - this element and after that is done you
62:41 - will pop out this element so this also
62:44 - takes big off one time now the next
62:46 - option we have is that many times rather
62:49 - than popping one element out we can just
62:52 - check that what is the current element
62:54 - at the very top and that for that we
62:57 - usually do like a peak operation and
63:00 - this also takes big off one time so you
63:02 - would you must be thinking that hey this
63:04 - is super fast uh data structure so let's
63:08 - use this up to the fullest extent but uh
63:11 - this is very limited in memory size and
63:14 - also like the way data is being stored
63:17 - so if we when we start talking about
63:19 - additional operations if we have to find
63:22 - an element inside a stack that takes big
63:24 - off end time for obvious reasons because
63:26 - at one point we are only accessing one
63:28 - element out and let's say that the
63:30 - element we are trying to get out is
63:31 - actually located at the very bottom we
63:33 - won't be able to reach it until we reach
63:35 - all the elements if we have to once
63:37 - again delete any element once again it
63:39 - will take big of end time because we
63:41 - will have to first find that element so
63:43 - that also adds an additional layer of
63:46 - complexity and same goes for modify as
63:48 - well that that this also takes B go off
63:50 - end time so all of these things they are
63:53 - closed L correlated with each other now
63:56 - let's see that what are the operations
63:59 - we can do or what are the use cases of a
64:01 - stack so number one use case of a stack
64:04 - is that wherever you need to maintain a
64:06 - sequence in one order and the reverse
64:10 - order sequence has to be match in that
64:13 - case uh stack would be a great example
64:16 - and this is one of the very popular
64:18 - scenario for that where we are we want
64:21 - to check that the number of parentheses
64:23 - we have are they in the correct order or
64:25 - not and that can very easily be
64:27 - determined using a stack where all the
64:30 - parentheses that are coming in they also
64:33 - get out in the same manner and U if you
64:36 - want we can also try to do a quick
64:38 - example so let's say that currently we
64:40 - are trying to build a compiler and in
64:43 - our compiler we wants to check that all
64:45 - the parentheses that were opened they
64:48 - are closed in the same sequence so what
64:50 - we can do is okay the moment we open a
64:52 - parenthesis we are going to push an
64:55 - value inside the stack So currently
64:57 - stack is empty and let's say that these
64:59 - are the sequence of operation we are
65:00 - going to do right that first we are
65:02 - going to open a curly bracket then we
65:04 - are going to open a square bracket then
65:07 - a circular bracket and then we are going
65:09 - to close in the correct manner as well
65:12 - so the idea would be that first we open
65:14 - the bracket so we are going to add a
65:16 - value over here then once again open so
65:18 - add open so add now when we are closing
65:22 - the bracket we will remove the value and
65:26 - try to see that whether the bracket is
65:28 - of same kind or not so the first value
65:30 - we removed is a curly is a circular
65:33 - bracket and the value we are trying to
65:35 - close is also a circular bracket which
65:37 - means awesome so this sequencing was
65:40 - correct now uh the very first element
65:44 - currently is the square bracket open
65:47 - square bracket and now the bracket we
65:49 - are trying to close is also an open type
65:51 - of bracket so once again this is also a
65:53 - match so we can consider this and the
65:56 - last element is a curly bracket and a
65:58 - curly bracket so once again this is also
65:59 - a match so the whole solution work and
66:02 - now we are at the end of our parenthesis
66:05 - at that time we also check inside our
66:08 - stack and currently our stack is also
66:10 - empty which means all the brackets that
66:13 - were opened they got closed in the same
66:15 - manner let's assume that uh there is one
66:18 - more additional bracket over here in the
66:20 - beginning that this was the case so then
66:23 - after these three has been done we would
66:25 - still have one entry over here which
66:27 - would dictate that the parentheses were
66:29 - not opened and closed in the same manner
66:31 - and uh that would throw an error so this
66:34 - is actually the use case being used in
66:36 - many of the compilers as well so that's
66:38 - actually quite cool to understand that a
66:41 - simple data structure can give birth to
66:43 - such a wide variety or such a huge
66:46 - functionality now here is the problem
66:49 - statement for the valid parenthesis
66:50 - problem where we are given the string of
66:53 - opening and and closing brackets and we
66:55 - need to determine that whether it is a
66:57 - valid sequence or not so for that we are
67:00 - actually going to use a hashmap in this
67:02 - case where we are going to create the
67:03 - mapping of opening and closing brackets
67:06 - where all the closing brackets are going
67:08 - to be placed as the keys in the hashmap
67:11 - and their subsequent values would be
67:12 - their counterpart now let's move on to
67:15 - our is valid method where as an input we
67:17 - are given a string so first of all we
67:19 - will initialize our stack uh we are
67:22 - going to run a for Loop across the given
67:24 - string and we will see that whether this
67:26 - is a closing bracket or not if that is
67:28 - the closing bracket we would get the top
67:31 - element of the stack and if the stack is
67:34 - empty we can return false if it is not
67:36 - empty we will pop the element and we
67:38 - will compare it with the closing element
67:40 - so if they match that's fine if they
67:42 - don't match we can return false and if
67:44 - that is not the case if it is an opening
67:46 - bracket we will simply push the value
67:48 - inside the stack now in the end after
67:51 - this whole Loop has ended essentially a
67:53 - our stack should be empty if the stack
67:56 - is empty we can return true that the
67:57 - sequence is valid if it is not empty we
68:00 - can return false so let's try to run
68:02 - this
68:03 - code and seems like our solution is
68:06 - working as expected let's submit this
68:08 - code and our code runs pretty
68:10 - efficiently and you can imagine that how
68:13 - Stacks are actually serving a very
68:15 - critical functionality now let's try to
68:17 - think about another example of what a
68:20 - stack can do and that is a redo
68:23 - functionality so we all know that what a
68:25 - redo or undo functionality is basically
68:28 - if you are going through your Google web
68:31 - browser whenever you need to go back to
68:33 - the previous website you were at uh
68:36 - essentially a stack is being used and
68:38 - how it does is that let's say that
68:40 - initially you opened your Google So
68:42 - currently your stack is empty now you
68:45 - first go to the Facebook page so it will
68:47 - Mark the value as Facebook from Facebook
68:49 - you decided to go to the Instagram page
68:51 - from Instagram you go to the YouTube
68:53 - page and and from YouTube you go back to
68:55 - the Twitter page now at the Twitter you
68:57 - decide to hit that uh Arrow back button
69:00 - or the undo button the moment you hit
69:02 - back it is actually going to pop the
69:04 - value out so currently okay uh you are
69:07 - at the Twitter so previous element like
69:10 - the YouTube would be there okay now
69:12 - currently you are the Twitter you hit
69:14 - the element back so it is going to pop
69:15 - the first value out so first value
69:17 - popped out would be YouTube so it will
69:19 - take you to the YouTube page now from
69:21 - the YouTube you once again hit the arrow
69:23 - back so once again the Instagram page is
69:25 - going to come out now currently you are
69:28 - at the Instagram page and you have this
69:31 - value now from the Instagram page you
69:33 - decide to go to the let's say Uber page
69:36 - so now because you went to a new page
69:38 - there will be an entry for Uber being
69:40 - added over here now once again when you
69:42 - hit the undo button actually from The
69:44 - Uber page you would actually go back to
69:46 - the Facebook page so that's how these
69:49 - things would work and uh this is
69:52 - actually quite useful data structure in
69:54 - my
69:56 - opinion the last linear data structure
69:59 - that is a Q and Q is also very popular
70:03 - as the name suggest Q is actually quite
70:05 - simple where unlike stack we actually
70:10 - have both ends open but all the values
70:13 - are coming in from one end and going out
70:15 - from the other end so let's say that
70:18 - currently you have the cube and you have
70:20 - value number five come in so five would
70:22 - be the first candidate to get out then
70:25 - we have value number six coming so six
70:27 - would be the second candidate to get out
70:30 - same way all the subsequent elements
70:32 - that keeps on coming in they all would
70:34 - be stored in the same sequence of the
70:36 - sequence they came in so we have a value
70:39 - where we can NQ or where we can add
70:42 - values to the cube and where we have a
70:45 - position where we can DQ where the
70:47 - values go out of the cube and uh we all
70:51 - know the functionality of cube uh there
70:53 - are lot of real life systems where we
70:55 - need to have cues uh if we want to
70:58 - generate like some Network packet
71:00 - manufacturing or uh Network packet
71:03 - submission type of system definitely we
71:05 - are going to use because we want uh our
71:08 - let's say that we have a system a and we
71:10 - have a system B and we are sending some
71:12 - Network packets so we want Network B to
71:15 - receive the network packets in the same
71:16 - manner they were sent from Network a so
71:18 - for that on the B side we are actually
71:20 - going to create a cube and this cube is
71:22 - going to have the sequence IAL flow of
71:24 - the networks so now let's see that for
71:27 - the Q we already know the use case but
71:30 - first understand that what are the types
71:32 - of cues available so the number one type
71:34 - of que is actually the just a regular
71:36 - quebe then there is also priority Cube
71:40 - now what priority Q means is that this
71:43 - represents some real life scenario so
71:45 - let's assume that you are currently
71:48 - waiting at the DMV Center to make your
71:51 - driving license and currently the
71:53 - officer sitting there is asking for
71:55 - everyone in a sequential manner so
71:57 - currently all the people are standing in
71:59 - the line and of course the person who
72:01 - came the first would be the person
72:03 - reaching out to the DMV officer in the
72:05 - first manner but during this time DMV
72:08 - officer identify a pregnant lady or some
72:11 - person who had some disability so in
72:14 - that case this person would be the first
72:16 - person to go and meet to the officer and
72:19 - complete the task and this is the real
72:21 - example of how a priority looks like
72:24 - where it mostly operates as a normal
72:26 - queue unless you have some event that
72:29 - you can Define that would dictate that
72:32 - it has a higher priority so process that
72:34 - first and this is quite useful in many
72:37 - real life scenarios as well where let's
72:39 - say that in your phone you wants most of
72:41 - the stuff to happening in the sequential
72:43 - manner but what if there is something
72:45 - related to like payment fraud happening
72:47 - so you want to put first first priority
72:50 - for that and for that usually a priority
72:52 - queue gets implemented now let's see
72:55 - that what are the different operations
72:56 - you can do or you can perform on a given
72:59 - Cube so I'm drawing a preliminary basic
73:02 - q and uh we are going to have the values
73:05 - coming in from one end and going out of
73:06 - the other end so essentially the first
73:09 - value would be five that came in so the
73:12 - first value would be five that would be
73:14 - going out so Q operates on fif principle
73:18 - first in first out unlike stack that was
73:21 - working on Leo principle last in first
73:24 - out so that is that is a difference
73:26 - between them and you have to understand
73:28 - now uh in the Q we first have the option
73:31 - for NQ which means we are adding an
73:33 - value to the back and this process takes
73:36 - big off one time because it's a constant
73:38 - time operation we already know where we
73:39 - are heading same way for the DQ uh it
73:42 - also takes big off one time because
73:45 - again once again the operation is quite
73:46 - similar now on top of that in the Q we
73:49 - have the option to Peak that who is
73:51 - going to be the first value coming out
73:54 - so that does not actually comes out but
73:56 - we would know that this is the value
73:59 - then there is also another option to
74:01 - search inside the queue but searching
74:04 - like other data structure takes big off
74:06 - end time and by the way for peing would
74:07 - take big off one time now on top of that
74:10 - we can actually delete an element from
74:13 - the queue but we would rarely do that
74:15 - because we want to process those
74:16 - elements this also takes meig go off end
74:18 - time and that essentially that's it now
74:22 - I I think I already give you sufficient
74:24 - examples of how does a queue operate or
74:26 - how it looks like so you can actually
74:28 - imagine that uh how things work with the
74:31 - cube and one of the most popular data
74:33 - structures out there is actually trees
74:36 - so trees is not your everyday looking
74:39 - array or link list it's actually
74:41 - completely different uh first thing it
74:43 - is a hierarchical data structure second
74:45 - thing for the trees you actually have to
74:47 - consider some terminologies so typically
74:50 - a tree looks like this where it has a
74:52 - node in this node you can store some
74:55 - information so that information can be
74:57 - anything you can store an integer value
74:59 - you can store uh a character value or if
75:02 - you want you can store some class values
75:04 - as well so let's say that you created a
75:06 - class called person and in that class
75:08 - you have that first name last name and
75:10 - date of birth you can fill out all of
75:12 - the those information in this node but
75:14 - apart from that on top of storing all of
75:17 - this information let's say that you
75:18 - store that information in the majority
75:20 - part of the given tree node in the rest
75:23 - of the part you actually create the
75:26 - connections that it is connected to with
75:28 - other nodes and this is really important
75:31 - so one single tree you only have access
75:34 - to the root node but from this root node
75:37 - you can actually connect to multiple
75:39 - places inside the given tree so you can
75:41 - actually be connected with two more
75:43 - nodes that are below that under one of
75:46 - nodes you can be connected with three
75:48 - more or four more or any more number of
75:50 - nodes this node maybe only have one node
75:53 - then and this node has another node and
75:55 - this is how typically a tree progresses
75:58 - so when we are talking about trees we
76:00 - will have to first Define few important
76:02 - things so I already told you the number
76:04 - one thing that what a node is that node
76:07 - is basically the portion of tree that
76:09 - contains data that contains important
76:12 - information now next important
76:14 - information for that a tree needs is an
76:16 - edge and what does an edge represents so
76:19 - if we see or take a look at this picture
76:21 - Edge is essentially the o that connects
76:24 - any two node and there needs to be some
76:27 - relationship between them so typically
76:29 - in a tree you would always see a
76:31 - relationship like this where a tree is
76:34 - always under some other parent tree so
76:37 - in this case this root node is actually
76:40 - a parent node for these two subsequent
76:43 - nodes but also at the same time let's
76:45 - say that you have four more nodes over
76:46 - here then in this case for these nodes
76:49 - this becomes a grandparent and uh this
76:53 - becomes uh these two nodes becomes
76:55 - parents for these subsequent nodes so
76:57 - there is always a parent child
76:59 - relationship and that relationship is
77:01 - typically defined through an edge next
77:04 - we need to discuss that what a root
77:06 - means so root means the node that is
77:10 - that supersedes all the nodes so
77:12 - essentially the great great great great
77:14 - greatest grandparent essentially the
77:16 - very first node that becomes your entry
77:19 - point into the tree and this is defined
77:21 - as the root node now let's talk about
77:24 - that what does a leaf node means so Leaf
77:26 - node means that the node that does not
77:29 - have any children so let's say we have a
77:31 - tree that looks like this in this case
77:33 - we currently have a node this node is by
77:36 - default the root node and this tree is
77:38 - connected with these two edges to these
77:40 - two children but these two children does
77:42 - not have any subsequent children of
77:44 - their own so in this case uh these two
77:47 - nodes actually becomes the leaf node
77:49 - that we are trying to explain now let's
77:52 - talk about the concept of depth now what
77:55 - does a depth means inside a tree
77:57 - essentially for any given tree uh it is
78:01 - always placed in the layers that one
78:04 - root node has some children then these
78:06 - children have subsequent children of
78:09 - themselves as well there can be n number
78:11 - of Childrens and for the Simplicity sake
78:13 - I'm drawing two children for every
78:15 - single parent but there can be n number
78:17 - of children and in this case we can see
78:20 - that currently this root node is
78:23 - actually located at depth one or you can
78:26 - also consider it depth zero depending on
78:28 - what type of situation you're tackling
78:30 - they are just numbers but they would be
78:32 - the initial depth and then with every
78:34 - level you go down the number of depth
78:37 - increases by one so in this case this is
78:39 - located at depth one this is located at
78:41 - depth two so essentially depth can also
78:44 - be considered as level of parents or
78:48 - children uh that each one of them has
78:51 - and very close and very similar similar
78:53 - concept is of height so height basically
78:56 - defines that how deep a tree goes so in
79:00 - this case from the root node we actually
79:02 - go two levels down so we can consider
79:04 - the height of this tree to be two uh
79:07 - same way uh from any single separate
79:09 - node you can also calculate the height
79:11 - from that position as well and in this
79:13 - case the height would be one if we
79:15 - consider this smaller sub tree of a tree
79:18 - so these are the different terminology
79:20 - that you have to understand before we
79:21 - proceed with uh what a tree is now let's
79:25 - try to see that what are the different
79:27 - types of trees that are available and
79:29 - trust me there are lot of types of trees
79:32 - so first let's just start with the
79:34 - simplest trees and that is binary tree
79:36 - now what does binary tree defines well
79:39 - we all know the meaning of binary binary
79:41 - means either zero or one which means it
79:43 - means two so binary tree means that
79:46 - every single root node has exactly two
79:49 - children so in this case this would have
79:52 - two children same way these subsequent
79:54 - nodes would have two children of their
79:55 - each and so on and so forth and until we
79:58 - run out of the level so in this case
80:00 - let's say that this has two children and
80:03 - that's it so this is a binary tree why
80:06 - because by definition every single
80:08 - parent has exactly two children or no
80:11 - children at all so and that property is
80:13 - followed across all the nodes uh so
80:17 - binary tree is what you're are going to
80:19 - encounter mostly in your typical
80:22 - interviews and very closely and a a
80:25 - subset of this binary tree is actually
80:28 - binary search tree so what does binary
80:30 - search tree means that it follows all
80:33 - the properties that a binary tree has
80:35 - that every single node has exactly two
80:37 - children or no children at all but on
80:40 - top of that the binary search tree has
80:42 - an additional property where the values
80:46 - that are being represented in that those
80:49 - binary trees they are actually sorted
80:51 - with the condition that everything on
80:54 - the left is actually less than
80:57 - everything on the right uh and this
80:59 - property is followed throughout the
81:02 - entire tree so let me give you an
81:03 - example let's say that we currently have
81:06 - a tree that looks like this and
81:07 - currently the value of this root node is
81:09 - s then it it can only have children
81:13 - where the scenario is that everything on
81:15 - this left sub tree has to be less than
81:18 - seven if it is everything on this left
81:21 - sub tree is less than seven and
81:22 - everything on the right sub tree is
81:24 - greater than seven in that case we can
81:27 - Define this as a binary tree or sorry
81:29 - binary sear tree so let's say that we
81:31 - add two more nodes over here and these
81:33 - two nodes Define the values as that this
81:36 - one is value number five and this one is
81:38 - value number eight so far this follows
81:41 - the property of a binary search tree
81:43 - that is good for us now let's just take
81:45 - it one step forward as well so we have
81:47 - two more child over here in this case so
81:50 - in this case let's say that the value of
81:52 - this child is three and again value of
81:54 - this child is six is it still a binary
81:56 - tree yes why because let's follow the
81:58 - same property currently if we consider
82:01 - this parent node then this becomes the
82:03 - left sub Tree in left subtree all the
82:05 - values are less than value s so that is
82:08 - good now let's consider this node so
82:11 - this node is five its left is three and
82:13 - Its Right is six and that is also
82:15 - followed because left sub tree is less
82:17 - than five and right sub tree is greater
82:19 - than five so both properties match same
82:21 - way the right sub tree is currently
82:23 - greater than seven so this property also
82:25 - matches so in this case this would be a
82:28 - binary search tree and binary search
82:30 - tree has lot of potential and lot of
82:33 - application you can actually use it for
82:35 - sorting or you can use it for to store
82:38 - data or to go hierarchically down so we
82:41 - we will talk more about this when we go
82:43 - when we start talking about uh various
82:46 - uh scenarios where you can actually use
82:48 - the uh tree data structure now there is
82:50 - one more data structure that is called a
82:52 - AVL trees so what AVL trees is that AVL
82:56 - tree is actually a special kind of tree
82:58 - where not only it is a binary tree but
83:02 - apart from that it is also binary search
83:04 - tree and on top of that it is a balance
83:07 - tree so what does the balance tree means
83:10 - so let me give you first an example of a
83:12 - balance tree balance tree is any tree
83:15 - where all the nodes on the left side of
83:17 - sub tree and all the nodes on the right
83:19 - side of subtree are even so in this case
83:21 - let's say if I have a tree like this
83:23 - this is a balance tree if I have a tree
83:26 - like this where left node has two sub
83:28 - nodes and right node also has two sub
83:30 - nodes this is also balance tree so any
83:33 - symmetric tree is basically balance tree
83:35 - and AVL trees contains all of these
83:38 - three properties yeah so that is about
83:40 - AVL tree then there are all more
83:42 - specialized trees that is red black tree
83:45 - and there is also B trees but these are
83:47 - just too higher of the concepts but I'm
83:50 - just going to give you an example that
83:52 - red red black tree is actually a tree
83:55 - that has separate colors so some nodes
83:58 - are defined in a certain color and the
84:00 - other nodes are defined in the certain
84:02 - colors and depending on their
84:04 - positioning the colors are maintained so
84:07 - whenever you try to add new value uh it
84:10 - automatically maintains the colors and
84:12 - it automatically generates that what
84:15 - should be the next subsequent value
84:16 - based on those coloring groupings so the
84:19 - red black trees are great at sorting
84:21 - various items and various operations
84:23 - depending on the color property and B
84:25 - trees are multi-level balance and sorted
84:30 - uh tree data structure that can have
84:32 - more than two uh more than two children
84:36 - so it is actually much more complex
84:38 - topic and in its own uh it has actually
84:41 - lot of uh things that we can think about
84:44 - but we are not going to go deeper
84:45 - because that would be much that would be
84:47 - a topic for much higher level I just
84:49 - wanted to give you a brief overview for
84:51 - that now let's let's see that what are
84:53 - the operations we can do on a tree so
84:56 - the operations are quite simple we can
84:59 - do like searching we can also do sorting
85:02 - we can also do uh deletion and in
85:06 - insertion all of those things right but
85:09 - in order to do that uh we actually have
85:12 - to Traverse over the tree all the time
85:15 - and the most important operation that
85:17 - you will have to learn about is that how
85:20 - how the traversal Works inside a given
85:22 - tree so actually for trees there are
85:25 - typically three different ways you can
85:27 - Traverse over inside the given tree and
85:29 - that that are that in order traversal
85:32 - pre-order traversal and postorder
85:34 - traversal so how each each and every one
85:37 - of them is going to work well I actually
85:38 - have a separate video on that but if you
85:40 - want you can just check what I'm trying
85:42 - to explain right now what in order
85:45 - traversal means is that first we are
85:47 - going to visit the node then we are
85:49 - going to visit the left child and then
85:50 - we are going to visit the right right
85:52 - child child pre-order means that first
85:54 - we are going to visit the left child
85:55 - then we are going to visit the node and
85:57 - then we are going to visit the right
85:58 - child and post order means that first we
86:00 - visit left then we visit right and then
86:02 - we visit node so let's see this in
86:04 - action suppose we uh we are given a
86:07 - simple tree so I'm currently drawing a
86:09 - simple most basic tree and we will go
86:12 - through its values right so let's say
86:14 - the values are one then on the left is
86:16 - two and then Its Right is three and also
86:20 - on the right is four and on the left is
86:22 - five and on the right is six let's say
86:24 - that this is the type of tree that we
86:26 - are trying to generate now let's see
86:29 - that what would be the in order
86:30 - traversal would be for this part this
86:32 - type of tree well of course first we are
86:35 - going to visit the node so node in this
86:37 - case is going to be the root node and
86:39 - that is going to be one so the very
86:40 - first value we are going to go through
86:42 - is going to be value number one then we
86:44 - are going to see all the left sub tree
86:47 - So currently for the left subtree it
86:49 - only has node number two but again at
86:52 - node number two we are also going to
86:53 - visit this and then after visiting now
86:56 - currently for this node number two it
86:58 - does not have any left sub tree that we
86:59 - can visit so we are going to go into the
87:01 - right side and on the right sub tree we
87:03 - have value number three that we haven't
87:04 - visited so we are going to visit node
87:06 - number three after that again we are
87:08 - going to repeat the same process now in
87:09 - this case we already took care of this
87:11 - value this value and this value so now
87:13 - even for node number one we are going to
87:15 - go to the right sub Tre but for right
87:17 - sub tree again we are going to apply the
87:18 - same logic of node left and right so
87:21 - once again we we are first of all going
87:23 - to visit node number four and after that
87:25 - we are going to visit node number five
87:27 - and then we are going to visit node
87:28 - number six so we took care of all the
87:30 - nodes so this is how uh the traversal
87:33 - works for in order traversal now let's
87:35 - do the pre-order traversal and inside
87:38 - the pre-order traversal it's clearly see
87:41 - that we need to go to the left as much
87:43 - possible as we can so from the initial
87:45 - note do we have a left sub tree yes uh
87:47 - in the left sub tree we are going to go
87:49 - to this node now does this node has any
87:51 - left sub tree that we haven't checked no
87:53 - because this does not have a left sub
87:55 - tree then we can visit the node so while
87:57 - visiting the node the first node we are
87:59 - going to visit for the pre-order
88:01 - traversal let me just write it over here
88:03 - so the first node would be node number
88:04 - two now the next node we are going to
88:06 - visit is going to be the right subt tree
88:08 - of this node now for this three it does
88:10 - not have any children which means we
88:11 - will have to visit this node so we are
88:13 - going to visit node number three first
88:15 - after visiting these two now for this
88:17 - node number one we took care of the
88:19 - entire left sub tree so we can actually
88:21 - visit node number one now after visiting
88:23 - node number one we need to take care of
88:25 - the entire right subtree but again with
88:27 - the same logic of left node and right so
88:30 - again we are at this position now this
88:32 - does have a left node so we are going to
88:34 - visit five first then we are going to
88:36 - visit the node and then we are going to
88:38 - visit node number six so this would be
88:40 - the pre-order traversal for the given
88:42 - tree now let's see that what would be
88:44 - the postorder traversal for the given
88:46 - tree so in terms of post order traversal
88:49 - it is going to be actually quite simple
88:52 - uh again using the same logic we need to
88:55 - First go through every single left node
88:57 - then we need to go through every single
88:58 - right node and then only we will visit
89:00 - the existing node So currently for this
89:02 - one it does have a left node for this
89:04 - two it does not have a left node but it
89:05 - does have a right node so we are going
89:07 - to visit node number three first so in
89:09 - the post order traversal the first node
89:11 - we are going to visit is going to be
89:12 - node number three then the next node we
89:14 - are going to visit is going to be node
89:16 - number two and then we are going to
89:17 - visit node number uh one no we are not
89:19 - going to visit node number one right now
89:21 - because because node needs to be visited
89:23 - in the end and it still has right subt
89:25 - tree that we haven't checked so now we
89:27 - are going to visit the right subtree and
89:29 - this this portion now becomes the node
89:32 - and for this it also has a left node so
89:34 - now we are going to visit five then we
89:35 - are going to visit six then we are going
89:37 - to visit node number four and after
89:39 - completing both of these portion we are
89:41 - going to visit node number one in the
89:43 - end so this would be the full traversal
89:46 - for the given tree uh the in order
89:49 - pre-order and post order traval uh this
89:52 - is the most trickiest thing to
89:53 - understand for any single tree and I
89:56 - hope you find you find it useful now
89:58 - let's see that what is going to be the
90:00 - searching sorting deletion and insertion
90:02 - for the given tree so let's say that
90:04 - searching if we are given a normal tree
90:07 - then the searching is for sure going to
90:10 - take big of end time but if we are given
90:12 - a binary search tree in that case
90:14 - searching would be bigo of log n only
90:17 - because every single iteration we would
90:20 - be uh moving half of the element on one
90:24 - side and we we can focus steadily on the
90:28 - target so that is why binary search
90:29 - trees are so popular now let's talk
90:32 - about sorting uh so for sorting actually
90:35 - if we are given a binary search tree
90:37 - then sorting is actually constant uh
90:39 - sorry because of log and time because
90:41 - it's quite easy to do uh we only need to
90:44 - Traverse over over the given tree and uh
90:46 - many for many sorting algorithm binary
90:48 - search trees are being used deletion if
90:51 - if we know which node we have to delete
90:53 - then it's a constant time deletion
90:55 - operation but if we don't know the node
90:57 - and we have to search first then it
90:58 - becomes big go of n or log n depending
91:02 - on what type of tree we are given and
91:04 - same goes with the insertion that if we
91:06 - know that we we can insert any randomly
91:08 - then it's B go of one but if we cannot
91:12 - do it and we have to insert it at
91:13 - specific location then it's B of n or
91:16 - big of log n depending on where we are
91:19 - trying to insert and what type of tree
91:20 - we are given so this is everything you
91:23 - need to understand about trees but now
91:25 - let's see that what are the uses on how
91:29 - on and where we can actually use trees
91:31 - so number one use case for a tree is
91:33 - typically a database management system
91:36 - uh why database management system
91:38 - because number one binary search trees
91:41 - then AVL trees then red black trees then
91:44 - B trees all of these are huge and very
91:47 - powerful in separating all the elements
91:51 - so that's why they are quite popular
91:53 - whenever you are trying to build a data
91:55 - stru database on top of that they are
91:57 - really po really powerful when you are
91:59 - trying to index all the values and
92:02 - indexing and enabling fast data retrival
92:05 - can only be possible using the trees if
92:08 - you have to use uh anything for file
92:10 - systems or file management uh so because
92:13 - tree they are already in hierarchical
92:16 - nature and whenever you see your windows
92:18 - path or your something something you
92:20 - will always see something like C drive
92:22 - it has a file called programs it has a
92:24 - file called uh Java it has a file called
92:27 - bin something like that so this is a
92:30 - parent child relation where every single
92:32 - node is connected with the other node
92:34 - and uh you can actually go over that
92:36 - plus if you want to create like a syntax
92:39 - trees you can also do that using the
92:42 - trees why syntax trees because let's say
92:45 - that if you're trying to build a
92:47 - scenario or a compile or something where
92:50 - you want to show that what class is
92:52 - connected with what other class and you
92:54 - are trying to avoid any infinite Loop
92:56 - scenarios so in that case tree data
92:58 - structure would be a huge help and on
93:01 - top of that you can actually use trees
93:03 - to build priority cues as well so this
93:06 - is also a very good benefit to use you
93:09 - can actually do that using Heap and uh I
93:11 - won't be explaining the whole concept
93:14 - but just know that this can be done okay
93:17 - so now let's try to see uh some examples
93:20 - for the tree based questions so one of
93:22 - the very common example is that
93:24 - typically we are being asked to validate
93:25 - a binary search tree now we all know by
93:28 - definition that what a binary search
93:30 - tree is that essentially every single
93:32 - node that is on the left side of the
93:34 - tree is actually less than uh the node
93:37 - value and every single node on the right
93:39 - side of the sub tree is actually greater
93:41 - than the node value and this property
93:43 - has to be followed throughout the entir
93:45 - of tree so let me give you a couple of
93:48 - examples where we will try to see that
93:50 - which are some of the valid trees which
93:52 - are not valid trees and how we can solve
93:54 - this problem in an actual interview or
93:56 - for our practice so assume we are given
93:59 - a tree like this where the values are 1
94:01 - 2 3 4 and 5 and uh these are the
94:05 - connection of the nodes so in this case
94:07 - we can clearly see that this is a valid
94:10 - binary search tree why because this is
94:13 - the middle node node number three now if
94:15 - we see the left sub tree for this node
94:17 - is actually the values are 2 and 1 and
94:20 - both are small than value number three
94:22 - same way in the right side of the sub
94:24 - tree the values are four and five and
94:26 - again both are greater than that
94:28 - particular value now we have couple of
94:30 - more sub trees as well that we need to
94:32 - check and that is this one so first sub
94:34 - tree is that this value of two and the
94:37 - value of one so again since the one is
94:39 - less than two and it is on the left side
94:41 - of the two uh it is a valid path same
94:44 - goes for this four and five as well and
94:46 - in this case we can Define this to be a
94:48 - valid binary search tree but now let's
94:51 - consider a scenario that if we are given
94:53 - a tree like this suppose the values are
94:55 - 5 4 3 and then this one is 6 and then
94:58 - this one is 2 and uh these are the nodes
95:02 - that are currently connected now let's
95:04 - break them down sube by sub tree so
95:07 - let's consider first this value number
95:08 - three and value number two they are just
95:10 - uh simple or Leaf nodes so they don't
95:13 - mean anything now let's consider this
95:15 - sub tree is this a valid sub tree yes
95:17 - why because three is less than four and
95:19 - it is currently on the left child of
95:21 - four so that is true now let's consider
95:24 - this entire sub tree so this is also
95:26 - valid because both four and five three
95:28 - are smaller than value number five now
95:30 - let's consider this sub tree this is
95:32 - also valid scenario where six is
95:34 - actually uh higher and in the greater
95:37 - value so 2 is smaller than six so that
95:39 - is why it is on the left side but when
95:41 - we consider this whole portion then it
95:44 - fails and why it fails because the value
95:48 - number for Value number five the
95:49 - expectation is that everything that is
95:52 - on the right side of value number five
95:53 - which means this portion has to be
95:56 - greater than value number five but since
95:58 - this two is actually less than five so
96:00 - because of this we can Define this to be
96:03 - a wrong wrongly placed uh value so this
96:07 - is not a valid binary search Tre now the
96:10 - question is how can we actually find the
96:12 - solution for this type of problem uh for
96:15 - trees so one simple approach is that if
96:20 - we do an in order traversal for any
96:23 - binary search tree if the tree is valid
96:27 - we should get a sorted array or sorted
96:29 - values in return and let me give you an
96:32 - example for this let me for a moment
96:35 - make this a valid binary search
96:37 - Tre so the method for in order traversal
96:40 - is first we visit the left sub child
96:43 - then we visit the node we want to check
96:45 - and then we visit Its Right sub child or
96:47 - right child and we keep on repeating for
96:49 - all the sub trees so so essentially
96:52 - first of all we will start our journey
96:54 - at this root position so at the root
96:56 - position first we need to check that
96:58 - whether a left child exist or not and
97:00 - yes because left sh exist once again
97:02 - this becomes our current root node or
97:04 - current node we are working with again
97:06 - this also has a left sh so three would
97:07 - be the first value we would visit so
97:09 - let's mark three would be the first
97:11 - value we visited then we would go back
97:13 - to the root now this node has no other
97:16 - left Sub sub trial uh that has not been
97:19 - visited so we would visit value number
97:20 - four and then it does not have a right
97:22 - child so we will again go back and now
97:24 - for this five we took care of the entire
97:26 - left sub tree so we can mark five as
97:29 - visited as well and then we will go to
97:31 - the right side of the sub tree now in
97:32 - the right side of the sub tree for seven
97:34 - we still have a left node available
97:36 - which means we would first have to
97:38 - Traverse that so we would Traverse value
97:40 - number six and then we would go back and
97:42 - since this seven we took care of the
97:44 - left sub tree so now we will take care
97:46 - of value number seven and this is where
97:48 - we would end and now if you see this
97:50 - sequence this actually came out as a
97:52 - sorted uh values so this is all we need
97:56 - to do that whenever we need to uh run
97:59 - for a tree we simply need to do an in
98:01 - order traversal if all the values are in
98:04 - correct sorted manner then we can Define
98:06 - this to be a valid binary search tree if
98:09 - that is not the case we can Define it
98:10 - invalid and let's try to understand this
98:12 - with a very small example suppose we are
98:14 - given the values as uh 5 3 and 2 suppose
98:19 - this is the sequence of values now if we
98:20 - do order traversal in this one first for
98:23 - five we are going to visit the left
98:25 - child so we are going to visit value
98:26 - number three that is good then we would
98:28 - visit value number five and in the end
98:30 - we would visit the right side right sub
98:32 - tree that is value number two now up
98:34 - until this portion this was a valid
98:37 - binary search Tre but the moment two
98:39 - entered over here this this is no longer
98:41 - a sorted uh sequence so that's why we
98:44 - can Define that this is not a valid
98:46 - binary search tree and let's see the
98:48 - code for this one right now
98:52 - so this is the validate binary search
98:53 - tree problem and now let's see the Java
98:55 - solution for this approach we already
98:57 - know that we need to do an in order
98:59 - traversal in order to validate that
99:01 - whether given tree is valid or not and
99:03 - for that we are creating a helper method
99:05 - where first of all we are checking that
99:07 - if the given root is equal to null then
99:09 - we can return true if that is not the
99:11 - case we need to do the in order for the
99:13 - left sub tree of the given root and the
99:15 - right sub Tre of the given roote so we
99:17 - do this that we call our uh recursive
99:20 - method once again where we call the left
99:22 - subtree uh as an input and we try to see
99:25 - that what is going to be its answer so
99:27 - let's say that if this yields the answer
99:29 - as false uh then we can simply return
99:32 - false if that is not the case then we
99:34 - can move forward and we will check that
99:36 - if whether the previous element was null
99:39 - not equal to null and if the value of
99:41 - the root is less than the previous
99:43 - element then also we can return false
99:45 - which means in this scenario we
99:47 - identified an anomaly where there has
99:50 - been a mismatch between any two values
99:52 - and they are not currently sorted if
99:54 - that is not the case then we are going
99:56 - to Mark the previous node as the root of
99:59 - the value of the root we identified and
100:01 - then we are going to call the in order
100:03 - function once again on the right side of
100:05 - the sub tree so this is a very simple
100:07 - piece of code but it's actually very
100:09 - powerful and help us Traverse the tree
100:12 - in the correct manner recursively plus
100:14 - we are learning that uh what type of uh
100:17 - tree it is now let's submit this code
100:20 - and our code runs pretty fast compared
100:23 - to a lot of other Solutions so that's
100:25 - pretty
100:27 - good where graph is a very similar data
100:30 - structure to a tree uh and if we just
100:32 - talk about the terminology that we are
100:34 - going to use in the graph even in the
100:37 - graph we are going to have a node but in
100:39 - the graph we can also Define node as a
100:41 - vertex and that essentially represents
100:44 - the same circular dot that I just
100:46 - mentioned where you can store all sorts
100:48 - of information like integer or character
100:50 - Boolean or class value or whatever he
100:52 - wants to store you can store that inside
100:55 - but on top of that apart from that nodes
100:57 - node having that important data
100:59 - information it also has the information
101:01 - of other nodes that is that it is
101:04 - connected to in the current system and
101:07 - those other nodes can be n number of
101:09 - nodes on top of that those other nodes
101:12 - can also be connected inter internally
101:15 - as well and there are some scenarios
101:18 - where one node is connected with another
101:19 - node but another node is not connected
101:22 - with that node and that node might be
101:24 - connected with some other node and that
101:25 - some other node might be connected with
101:27 - that node as well so there are lot of
101:29 - different uh ways uh graphs can work now
101:33 - one key difference between a tree and a
101:36 - graph is that graphs are graphs can be
101:39 - cyclic in nature and what do I mean by
101:42 - cyclic that let's say that you have a
101:44 - node that is connected with another node
101:46 - it could be possible that is also
101:47 - connected with another node and that is
101:49 - also connected with the previous note
101:51 - and this type of configuration would not
101:53 - be present inside a tree this this can
101:56 - only be present or prevalent inside the
101:58 - graph and that is the the biggest
102:00 - difference between trees and graphs and
102:03 - that's why they have their own entire
102:05 - set of different con considerations and
102:07 - connections so now let's just go back to
102:10 - the basic terminology that we need to
102:12 - understand first we understood that what
102:13 - a node or Vex is or vertices is next
102:17 - thing uh that we have to consider is The
102:19 - Edge and Edge is the it means the same
102:22 - thing that that is the connection point
102:25 - between any two values uh that are next
102:27 - to each that are connected with each
102:29 - other so Edge means the connecting
102:31 - points now there is also the concept of
102:34 - adjacent uh adjacency or adjacent
102:37 - vertices what do I mean by adjacent
102:39 - vertices let's say I have a graph that
102:41 - looks like this and in this case uh
102:44 - currently let me Mark the values as a b
102:47 - c and d so in this case if I consider
102:49 - this node number c I can say that node D
102:53 - node B and let's mark this node as node
102:56 - e so B D and E are actually adjacent
103:00 - nodes to this node C because they're
103:02 - directly connected with that with an
103:04 - edge but if the this node is not
103:07 - actually connected with vertices then in
103:10 - this case this node C and A are they are
103:13 - not adjacent nodes to each other so this
103:16 - is what what do we mean by adjacent
103:18 - nodes now there is also a concept ccept
103:21 - of degree inside for any single vertex
103:24 - and what does a degree means that degree
103:27 - means that how many number of vertexes
103:29 - uh or connections that every single node
103:31 - has so let's say if I have a graph that
103:34 - looks like this in this case I have node
103:37 - a b and c so I can Define that this node
103:39 - only has one Edge so I can consider the
103:42 - degree of this node to be one in this
103:44 - case for this B it has the degree of two
103:46 - and in this case this C it has degree of
103:49 - one now there is also another concept of
103:51 - the total degree depending on the
103:54 - incoming and out
103:55 - outcoming edges as well and where does
103:58 - incoming and outcoming edges comes to
104:00 - place well we we will talk more about
104:03 - that when we discuss the type of graphs
104:05 - but remember that graphs can have a
104:07 - functionality where two one graph is
104:10 - connected with another graph but another
104:12 - graph is not connected so in this case
104:14 - if we Mark if we have a graph like this
104:16 - where a has an edge leading to the B but
104:19 - B does not have an edge leading to an a
104:22 - which means from a we can actually go to
104:25 - B but from B we cannot go back to a so
104:28 - in this case a would have an degree of
104:30 - one but B would have degree of zero in
104:33 - this case so that is that is the
104:35 - difference between these two uh this
104:38 - concept of degree for the graphs now
104:40 - let's see that what are the types of
104:42 - graphs that we we can deal with so
104:45 - number one type of graph is a directed
104:47 - graph and what does a directed graph
104:49 - means that I just showed it to you you
104:51 - that this is actually a directed graph
104:53 - where one node is actually connected
104:55 - with another node but it is not the vice
104:58 - versa is not true and the directed
105:00 - graphs can get really complicated as
105:02 - well it is not always going to be this
105:04 - simple or this similar and it could also
105:06 - be possible that there can be hundreds
105:08 - or even millions of nodes that are
105:10 - connected with each other in this
105:11 - fashion as well uh where we would have
105:13 - various degrees and various directions
105:16 - happening amongst different vertices
105:18 - there could be undirected graphs as as
105:20 - well and undirected graphs are actually
105:23 - a graph where two nodes share a common
105:26 - Edge so in this case rather than
105:28 - treating is as a single edge typically
105:30 - this is used to be like this where node
105:33 - a has an edge leading to node B and B
105:37 - has an edge leading to node a which
105:39 - means from B you can also come back to a
105:41 - and from a you can also go back to B but
105:43 - in this case typically we only show them
105:45 - using a single edge then whenever there
105:48 - is an edge without an arrow it defines
105:50 - that both nodes are connected with each
105:52 - other in the simplest Manner and there
105:54 - are there is a bidirected connection or
105:57 - an undirected connection now there is
105:59 - also consider uh also a concept of
106:02 - weighted graph and what does a weighted
106:05 - graph means that let's say that you have
106:08 - different nodes and different nodes had
106:10 - different connections and different
106:12 - connections have edges so edges carry
106:15 - weight as well let's say if I'm trying
106:17 - to plot a graph of a city now in this
106:20 - city uh this node represents the Young
106:24 - Street and this node represents the main
106:27 - street now these two streets are being
106:30 - represented and there is let's say there
106:32 - is also third street called Queen Street
106:35 - now there is a graph that looks like
106:37 - this but in this case it could be
106:39 - possible that this is this graph I'm
106:43 - defining this is defined as a road so it
106:45 - could be possible that from Young to
106:47 - main though there is a road there is
106:48 - some construction going on so because of
106:51 - that this Edge is actually weights five
106:53 - which means it takes 5 minutes from
106:55 - Young to go to main but it could be
106:57 - possible that from Young if we need to
106:59 - go to Queen it only takes 1 minute
107:01 - because there is no construction and
107:02 - from Queen there is only uh there is an
107:05 - edge going to main that also only takes
107:07 - 1 minute because there's no uh traffic
107:09 - so in this case because this Edge
107:12 - contains higher weight if we have to
107:14 - choose the shortest path between young
107:16 - and Main Street it it has to go through
107:19 - the queen Street and uh this would be
107:22 - the concept of a weighted graph this is
107:25 - really powerful especially for GPS
107:27 - system and the shortest path that we
107:29 - have to create uh between any two edges
107:31 - like Google Maps use this like crazy uh
107:34 - now there is also another concept of
107:36 - unweighted uh Edge and unweighted Edge
107:39 - is basically an edge that does not have
107:41 - any parameter uh so let's say that I'm
107:44 - creating a graph of just friends so
107:47 - currently a is friends with B and B is
107:49 - friends with c and C is also friends
107:51 - with a so in this casee I don't need to
107:53 - have any weight on the edge um they are
107:55 - all just friends and none of them are
107:57 - best friends so that's why it's a common
107:59 - relationship and U we can do whatever he
108:01 - wants to do then there is also concept
108:04 - of Click graphs and a cyclic graphs so
108:07 - cyclic and ayylic both means the same
108:10 - thing essentially this is a cyclic graph
108:13 - and let's say that if I did not had this
108:15 - Edge in this case then this would have
108:17 - been an ayylic graph where cylic graph
108:20 - has means that there exist a cycle
108:22 - between the nodes and a CLI graph means
108:24 - that there are no no Cycles happening so
108:27 - usually in ayli graphs there is a very
108:30 - popular concept of dag which is which
108:33 - means directed ayylic graph and directed
108:35 - asly graph means that every single node
108:38 - has an directed Edge so let's say that
108:40 - there this is node a node a is connected
108:42 - with node B and node B is connected with
108:44 - node C and C is connected with node D so
108:48 - this is an example of directed A
108:50 - basically graph where we see bunch of
108:52 - different edges but there are there are
108:54 - no issues with the graph now because
108:59 - graphs are a little bit trickier uh we
109:02 - have to take care of two things we have
109:04 - to take care of vertices and we also
109:06 - have to take care of edges to represent
109:08 - that what graph is connected with which
109:11 - so that is why uh even in order to
109:14 - represent a graph we need to have some
109:16 - different set of data structure so one
109:19 - possible data structure is an adjacency
109:21 - Matrix and another data possible data
109:24 - structure is an adjacency list so I'll
109:27 - I'll give you give you an explanation of
109:29 - both of them let's just have a demo
109:31 - graph uh that is a simple enough so
109:34 - let's just say we have a graph that a is
109:37 - connected with B and B is connected with
109:40 - C and C is connected with d and this is
109:44 - a graph right uh that we are trying to
109:46 - represent so if we have to create an
109:48 - adjacency Matrix what adjacency Matrix
109:51 - means is that we are actually going to
109:53 - have a 2X two or sorry M cross n Matrix
109:57 - where for every single vertices and
109:59 - edges we are going to have uh a matrix
110:02 - looking like this so we are going to
110:04 - have rows a b c d same way we are going
110:07 - to have columns marked as a b c d now
110:11 - currently uh all of the these four
110:14 - values will always be zero because node
110:17 - cannot be connected or cannot be its own
110:18 - neighbor now now let's say that
110:21 - currently a has a connection with B so
110:24 - we are talking about node a and its
110:27 - neighboring Edge to be B so in this case
110:29 - we would mark this value as one now B
110:31 - has a connection with C so node B has a
110:34 - connection with C so we would also Mark
110:35 - this value as one C has a connection
110:37 - with d so C to D we would mark it as one
110:40 - and D to uh and D does not have any
110:43 - connection which means this is going to
110:45 - be zero and all the other values they
110:47 - are going to be marked as zeros because
110:50 - they don't represent anything which
110:52 - means currently we have a graph that
110:54 - contains four nodes a b c d amongst
110:57 - these four nodes we only have connection
111:00 - between from A to B so that is why this
111:02 - is one but we do not have a connection
111:04 - from B to a so that is why this is
111:06 - defined as zero let's say that rather
111:08 - than this being a directed graph If This
111:10 - Were to be an undirected graph and we
111:13 - only have edges like these in this case
111:16 - in our adjacency Matrix we are going to
111:18 - represent values differently
111:20 - where uh even from B to a we are also
111:23 - going to Mark as an edge same way from C
111:26 - to B we are also going to mark an edge
111:29 - and same way uh D to C we are also going
111:32 - to mark an edge over here so all of
111:35 - these values would also be one in this
111:38 - case so this is the way on how we can
111:41 - represent graph in The adjacency Matrix
111:43 - Now adjacency list is a little bit
111:45 - different where adjacency list we
111:47 - actually have a hash map and I I know I
111:50 - haven't talk talk about hashing yet but
111:53 - inside a typical hash we have two values
111:55 - we have a key and we have some value
111:58 - associated with those that key so as a
112:00 - key we are actually going to have the
112:02 - four vertices that we are given so in
112:04 - this case it's a b c and d now for this
112:08 - we are going to mark that what are all
112:09 - the neighbors that a is connected with
112:12 - in this case currently a is only
112:13 - connected with B so we are going to have
112:16 - a value like B Associated for a same way
112:19 - currently B is actually connected with A
112:21 - and C both so for B we will have a value
112:24 - of a and C both and this is actually
112:27 - going to be a link list that where which
112:29 - represents that there can be n number of
112:32 - children associated with a sing or n
112:34 - number of neighbors associated with a
112:36 - single node same goes for C that from C
112:39 - it is connected with b and d and d is
112:42 - only connected with c and this is how it
112:44 - would be represented in adjacency list
112:47 - now the question is which one is better
112:50 - and which one is worse well in my
112:52 - opinion both does the job well but if
112:55 - you're dealing with a low number of
112:57 - edges then in that case it makes sense
113:00 - to use an adjacency list because it SP
113:03 - it uses less space uh meanwhile over
113:06 - here you see that there are more zeros
113:08 - than the number of ones but let's say
113:10 - that we have bunch of different edges
113:11 - that we are trying to work with
113:12 - something like this in this case it
113:14 - would make more sense to use an
113:16 - adjacency Matrix rather than using
113:18 - adjacency list so so these are the two
113:21 - ways on which we can actually use uh to
113:24 - represent graphs now what are the
113:26 - operations we can do on the graphs uh
113:29 - speaking of operations well the common
113:32 - operations are always going to be there
113:34 - that in terms of operations we would be
113:36 - able to insert we would be able to
113:37 - delete we would be able to modify we
113:40 - would be able to search there is no
113:43 - there is very little concept of sorting
113:45 - in this case because it is not very
113:47 - optimal for sorting but apart from that
113:49 - these these are all the things that we
113:51 - can do but in order to do that we can
113:53 - only do it by traversing over the given
113:56 - graph because we would not know that
113:58 - where we are traversing so we must have
114:01 - to Traverse and for Traverse we actually
114:03 - have two options we have the breath
114:04 - first search and we have the depth first
114:07 - search so I'm going to talk about both
114:09 - of them in a simplest manner uh if you
114:11 - want to know more about this uh I have
114:14 - created an entire separate videos on
114:17 - these two topics so that's why let me
114:19 - give you the basic idea that BFS
114:22 - represents breath first search what does
114:24 - breath first search means that we are
114:27 - actually going to search our neighbors
114:30 - first and before going to their
114:32 - neighbors and DFS means that depth first
114:34 - search what does depth for search means
114:37 - that we will pick a neighbor then we
114:40 - will pick one more neighbor for that
114:41 - neighbor then we will pick one more
114:42 - neighbor for that neighbor and we would
114:44 - keep on moving forward in that direction
114:47 - so let's try to see that in
114:50 - let's say that we have a graph that
114:51 - looks like this where we have bunch of
114:54 - different nodes that are closely
114:57 - connected with each other and we are
114:59 - trying to find some particular value
115:01 - right uh let me draw out bunch of
115:03 - different edges and with every single
115:06 - edge uh we would be able to reach to a
115:09 - certain conclusion about the given graph
115:12 - now let's say that this is the graph
115:14 - currently we have and we also have a
115:15 - cycle in in it now we are currently
115:18 - located at this node number a and we are
115:20 - trying to find this node number B and
115:23 - all of these nodes they can have their
115:24 - own subsequent separate values so if we
115:26 - are going in the BFS manner breath for
115:29 - search manner what we what we will start
115:31 - to do is let's say that this a is also
115:33 - the root node because even just like
115:36 - trees graphs also have the concept of
115:38 - root node so let's say that this a is
115:40 - the root node so from this a we know
115:43 - that what are the neighbors of a through
115:46 - either adjacency list or adjacency
115:47 - Matrix so we are going to use that in
115:49 - information and we will start traversing
115:52 - to all the neighbors so first we will go
115:53 - to this neighbor okay this is not value
115:55 - number B then we will go to this
115:57 - neighbor this is not value number B then
115:59 - we will go to this neighbor this is not
116:00 - value number B then we will go to this
116:02 - neighbor then we will go to this number
116:04 - we ex ex uh excluded all the
116:07 - possibilities for all the neighbors none
116:10 - of these neighbors were actually value
116:12 - number B so then we will pick one
116:15 - neighbor at R random and we would keep
116:17 - on moving forward so let's say we pick
116:19 - pick this neighbor again we go to its
116:22 - neighbor and again we go to its neighbor
116:24 - none of these yielded any good any
116:26 - particular good result so what we do we
116:29 - backtrack in that case and again from
116:31 - this a we would pick another neighbor so
116:33 - let's say we pick this neighbor this
116:35 - time and through this neighbor we would
116:36 - go to this neighbor and we would also go
116:38 - to this neighbor and we found out value
116:40 - number B so we can say that okay this is
116:42 - the connection between a to B and let's
116:44 - just say for this example the value of
116:47 - this neighbor is node number c so we can
116:49 - say that there exist a path from a to c
116:51 - and from C to B where it is connected
116:54 - now this would be the strategy for
116:57 - breath first search uh and I hope that
117:00 - my explanation was clear enough so that
117:03 - you get the idea now let's see that what
117:05 - would happen in the depth for search
117:07 - scenario so in the depth for search
117:09 - scenario we will pick one neighbor and
117:11 - we will keep going deeper and deeper
117:13 - into that neighbor so let's try to
117:16 - understand this with an example suppose
117:18 - we are again located at this position
117:20 - number a so from this position number a
117:22 - first we will pick a neighbor so let's
117:24 - say we pick again this neighbor now
117:26 - again for this neighbor we would pick
117:28 - one more neighbor so one more neighbor
117:30 - we ex now we uh concluded all the
117:33 - possibility that there this neighbor
117:35 - does not have any neighbor that we
117:37 - haven't visited so what we would do is
117:39 - we would backtrack to this position
117:41 - again see that are there any neighbors
117:42 - that we haven't visited so we actually
117:45 - check this neighbor okay this one we
117:46 - haven't visited so we visit that and
117:49 - that also did not yield any result so
117:51 - again we backtrack we come back to our
117:53 - main a then we pick another neighbor so
117:55 - let's say in this case we pick another
117:57 - neighbor called C from the C we again go
118:00 - to deep okay so this one did not yield
118:03 - it result so in this case again we check
118:05 - for this one and this one yielded the
118:07 - correct result and we again created a
118:10 - path from A to B to C and again we got
118:13 - the correct answer now you must know
118:16 - that under which situation you need to
118:18 - use breath for search and under with
118:20 - situation you need to use depth for
118:22 - search let's say that you have a graph
118:25 - and you expect B to be somewhat closely
118:30 - to your current a if you expect that in
118:32 - that case it would make more sense to
118:34 - use breath for search because you are
118:36 - more likely to find a result within some
118:38 - of the neighboring graphs but let's say
118:41 - that you have a graph that is much more
118:42 - complicated and B can be located
118:45 - somewhere down the road uh somewhere
118:48 - very far far far away in that case it
118:51 - would make sense to use a depth for
118:53 - search rather than breadth for search in
118:56 - either case the time and space
118:57 - complexity for both breadth for search
118:59 - and depth for search is actually going
119:01 - to be bigo of M cross n where M can be
119:04 - the number of vertices and N cross be
119:06 - the number of edges so it could it would
119:07 - be B of Vertes multiply by edges and all
119:11 - the operations we defined for the arrays
119:13 - they are also going to have or they are
119:15 - also going to follow the same time
119:17 - complexity that whenever we need to to
119:19 - find any any two values it is going to
119:21 - be the same now let's see that what are
119:23 - the use cases when you have to use
119:26 - graphs number one use case is actually
119:28 - social network so in Social Network we
119:31 - know that let's say that my name is
119:34 - person a so person a can be friends with
119:38 - many other person so I can be friends
119:40 - with B I can be friends with C I can be
119:42 - friends with d i can be friends with e
119:44 - something something something like that
119:46 - right and E can have their own separate
119:49 - friends so e can have friends F and uh
119:52 - Zed and all all all of that now this is
119:55 - how a typical social network looks like
119:58 - and that is why it is really common to
120:00 - use that let's say if I'm using a social
120:03 - network like Facebook where if I am I
120:05 - can only be friends with someone if that
120:07 - person is friends with me in that case
120:09 - there would be an undirected
120:11 - relationship but let's say if I'm using
120:13 - something like Instagram where some X
120:15 - person can follow me but I cannot follow
120:18 - that X person so in that case let's say
120:21 - that I have a relationship like this
120:23 - where X is following me but I'm not
120:25 - following back X so in this case we have
120:27 - a directed graph or a directed
120:29 - relationship but either cases it works
120:32 - perfectly fine next there would be a
120:34 - great use case for uh different internet
120:38 - and because in the internet we have a
120:41 - many different web pages that contains
120:43 - lot of information and we have the HTTP
120:47 - uh URLs that are are connected with each
120:49 - other on top of that from every single
120:52 - uh web page we can also have many
120:55 - different HTTP web pages that connects
120:57 - to some other web page and from that it
120:59 - can have many different web pages that
121:01 - connects to some other web page so in
121:03 - this case graph can be a good way to
121:05 - store all of that information plus I
121:08 - already mentioned that if you want to
121:09 - create a GPS system or any navigation
121:12 - system graphs are a wonderful approach
121:14 - and you cannot build a navigation system
121:16 - without using graphs where cities or
121:18 - places are treated as vertices and the
121:20 - roads are treated as edges and uh that
121:23 - is how you move move forward on top of
121:25 - that wherever you have to resolve any
121:28 - dependencies you can actually use graphs
121:31 - to resolve that dep dependency so let me
121:33 - give you a quick example let's say that
121:35 - you are studying in a university and in
121:37 - the University we know that we can only
121:39 - take courses that are like 204 or 205 if
121:43 - we already completed its prerequisite of
121:45 - 105 something like this so if that
121:48 - relationship is done done if unless I
121:50 - have completed this I cannot move
121:52 - forward to this particular course so in
121:54 - that scenarios treating different
121:56 - courses as vertices and connections with
121:58 - them between them or dependencies
122:00 - between them as edges would yield to
122:03 - Greater success in terms of all the
122:05 - results so these are all the operations
122:08 - and all the things where you must use
122:10 - graphs so first of all we are going to
122:13 - understand a graph with an example so
122:16 - this is a very simple problem and I
122:18 - might use some terms or some Concepts
122:21 - that you might not be familiar with but
122:23 - don't worry about it I would be guiding
122:25 - you throughout the whole journey and
122:27 - it's actually a quite simple way to
122:29 - understand what does a typical graph
122:30 - problem looks like and how does it work
122:33 - so number one thing in the graph is that
122:36 - for this problem we are actually given
122:37 - an N cross n Matrix and in this n cross
122:40 - n Matrix we are being told that this n
122:44 - represents the basically cities and if
122:47 - there is a path from one city that
122:49 - connects to another city then there
122:51 - would be a link in this n cross n m
122:54 - Matrix so it would Mark as one for that
122:57 - particular cell and if there is not a
122:59 - path then there won't be U any
123:01 - connection and it would be defined as
123:03 - value zero so let's try to see that what
123:06 - we need to check we essentially needs to
123:08 - calculate that how many number of
123:10 - provinces are there so what provinces
123:12 - are being defined as that let's say that
123:15 - if there is there are cities connected
123:17 - with each other then they would be part
123:20 - of a single Province so this would be a
123:22 - single Province P1 and even if there is
123:25 - a unique city that is not connected with
123:27 - each other still it is a province on its
123:29 - own so we would consider this as a
123:31 - province as well and uh let's try to see
123:34 - some examples of what different
123:36 - questions or what different things that
123:38 - we can refer so number one example is
123:40 - actually quite simple let's assume that
123:42 - we are given the CI a b and c in this
123:44 - case and we can see that City a and City
123:47 - B is connected with each other City C is
123:49 - not connected with each other so in this
123:51 - case this is a province and this is in
123:54 - itself another Province so we can return
123:56 - that in this case there are two
123:57 - provinces present and we can return two
123:59 - as the answer let's consider one more
124:01 - scenario suppose we are given uh four
124:04 - cities in this case and let's say that
124:06 - for the four cities all four cities are
124:09 - connected like this and you see that
124:11 - there is actually no connection over
124:13 - here but still we can conclude these
124:16 - cities to be connected how because let's
124:19 - say that this is a so a is connected
124:22 - with B and B is connected with C and C
124:24 - is connected with d so in this case you
124:25 - can still reach to D so all of this
124:28 - entire cluster would be considered as a
124:30 - single Province only let's take just one
124:32 - more example to make things more clear
124:35 - let's suppose we are given four
124:36 - different cities and all four different
124:39 - cities are not connected with each other
124:41 - in this case we can Define that four
124:43 - there are in total four separate
124:45 - provinces available in this case so
124:47 - let's see that what is going to be the
124:49 - approach to solve this
124:51 - problem okay so let's assume that this
124:53 - is the example we are given and this
124:55 - would be its Matrix that is going to
124:57 - represents the connection connection
124:59 - between any two cities now in this
125:01 - example we can clearly see that there
125:02 - are actually going to be three number of
125:04 - provinces uh this is the first Province
125:06 - this is the second Province and this is
125:08 - the third Province but let's see that
125:10 - how we are going to compute these
125:11 - results and for that we are going to uh
125:14 - first Mark or populate these Matrix
125:17 - basically this Matrix is already given
125:18 - to you in the example but I'm just
125:20 - showing you that what is the approach we
125:22 - are going to take in order to solve this
125:24 - problem so first we are going to see
125:26 - that uh since there is a single ended or
125:29 - one unidirectional connection between a
125:30 - to B which means we can also Define that
125:32 - there is a connection from B to a as
125:34 - well and this is what we are going to
125:36 - represent in this n cross n Matrix so
125:39 - first let's see okay so we have u a
125:41 - connection between a to B so we are
125:43 - going to mark this as value number one
125:45 - same way we have a connection between B
125:47 - to a so we are going to mark this as is
125:48 - one as well and B to C there is a
125:50 - connection so from B to C there is a
125:53 - connection same way from C to B there is
125:54 - a connection up again from D to e there
125:58 - is a connection so let's mark that from
126:00 - D to e there is a connection over here
126:04 - and same way from E there is a
126:06 - connection to d as well so let's quickly
126:10 - Mark that and uh that is over here okay
126:13 - and then F there is no connection so
126:15 - basically these are all the connections
126:17 - we have apart from that all the values
126:20 - are going to be filled out with values
126:22 - of zero we can actually consider this as
126:24 - a uh adjy Matrix for our our given graph
126:30 - and we can treat each of the Cities as
126:32 - the node of a graph and the connection
126:36 - or the edges between these cities are
126:38 - actually the edges between the these
126:41 - nodes so now all we need to do is we
126:43 - need to find the number of connected
126:45 - components and that's it so that is
126:48 - actually quite simple to do but how to
126:49 - do it I'll just show it to you quickly
126:51 - now I already mentioned to you that in
126:53 - the graph we have to keep keep track of
126:56 - the notes that we have visited already
126:58 - so that we don't encounter any issues
127:00 - with the visited uh we don't have any
127:02 - issues in the future so we are going to
127:05 - have uh basically a hash map that where
127:08 - we are going to store the values of all
127:10 - the visited values and plus we are also
127:13 - going to keep track of the neighbors or
127:15 - the connected cities for that particular
127:18 - node as so we can actually skip that in
127:20 - the future and the approach we are going
127:22 - to take is we are going to do a breath
127:24 - for search starting from any node and we
127:27 - would keep on doing it until we exhaust
127:30 - or we take care of all of these six
127:32 - cities so let's start our approach and
127:35 - the idea is that we are going to start
127:37 - working from the node a so let's mark
127:40 - that currently we are located at
127:42 - position number a so we are located at
127:44 - this position now from a we are going to
127:46 - see that what are the places it it is
127:48 - marked as one or it is connected to
127:50 - those cities and we are going to Mark
127:52 - those cities as visited so let's just
127:55 - say since because we started a new
127:57 - province with this new traversal a we
127:59 - are going to increase the number of
128:01 - provinces so this probe is defined as
128:04 - the variable that keeps track of how
128:06 - many number of provinces we have been
128:08 - able to find so initial value was zero
128:10 - and now we have value as one now from a
128:13 - we see what who is the neighbor of a so
128:15 - the neighbor of a is actually value
128:17 - number B which means means in our
128:19 - hashmap we already had entry a then we
128:21 - are also going to add entry B and this
128:23 - can actually be a hash set rather than
128:25 - hashmap what is the difference you would
128:27 - soon find out uh when we talk about
128:30 - hashing but right now just consider that
128:31 - we are marking these values so this
128:33 - would allow us to let us know that these
128:35 - are the cities we have already visited
128:37 - so we don't visit them again okay now we
128:41 - are at position number B from B which
128:44 - are the cities B is connected with so
128:46 - number one city B is connected with is
128:47 - City number a but a we have already
128:50 - visited so we are going to skip this for
128:52 - now then we are going to visit City
128:54 - number C and C we haven't visited so we
128:57 - will mark C over here okay so now we are
129:00 - at City number c what are the cities C
129:03 - has that we have we haven't visited so C
129:05 - is only connected with b and we have
129:08 - already visited B so which means we can
129:10 - conclude that in this case since we
129:12 - visited C does not have any more
129:14 - neighbors that we need to visit so we
129:16 - will do a backt trck we will go to B B
129:18 - again does not have any more neighbor
129:20 - that we have to take keep track of so we
129:22 - and once again we will backt track to a
129:24 - and a we already visited all of its
129:26 - neighbors so we can conclude this a b
129:29 - and c to be completed over here and so
129:32 - far we have been able to find one
129:34 - Province but now in this case we will
129:37 - jump or have to go to the next city d by
129:40 - taking a jump because we are not going
129:43 - through a connection from any particular
129:45 - node so that is why we are adding one
129:47 - more Province or we are exploring one
129:49 - more Province so we will increase the
129:50 - number of Province to two and in this
129:53 - case uh at City number D we are going to
129:56 - visit all the neighbors of D in breath
129:58 - for search manner so since D only has
130:01 - one neighbor that is City number e so we
130:03 - are going to go to City number e and we
130:05 - are going to Mark uh both d and e as
130:08 - visited in our hashmap on top of it with
130:11 - e e only has one city as its neighbor
130:14 - and that is City number D so in this
130:16 - case we have already visited D which
130:17 - means we can Al conclude both d and e to
130:19 - be visited as well and now we will have
130:22 - to still go to a city that we haven't
130:25 - visited so far that is City number have
130:27 - the moment we go to a new city we are
130:28 - going to increase the number of
130:29 - provinces so now number of provinces is
130:32 - going to be three and we are going to
130:33 - say that we are currently visiting City
130:35 - number F now F does not have any more
130:38 - Neighbors which means we can conclude
130:40 - that from F we cannot go anywhere else
130:42 - and now we have visited all the cities
130:45 - that were present because we have that
130:47 - value in our hashmap and we so far we
130:49 - have been able to find three different
130:51 - provinces connected using this manner so
130:54 - this is how typically a graph problem
130:56 - operates where in most of the cases you
130:59 - are going to have vertices you are going
131:01 - to have edges you are somehow going to
131:04 - iterate over all the vertices through
131:06 - edges and you are going to use an
131:09 - adjacency Matrix Or adjacency List uh uh
131:13 - as the way that stores the information
131:15 - of what the graph is and most like you
131:18 - are going to use either hashmap or hash
131:21 - set to keep track of the Cities you have
131:23 - already visited so you don't end up in a
131:26 - a continuous loop so let's see the Java
131:28 - solution for number of provinces problem
131:30 - basically uh we are we are given the 2x2
131:34 - or n cross n Matrix that defines that
131:37 - what are the components that are
131:38 - connected with each other so first of
131:40 - all we are going to define a number n
131:42 - that is be that is going to be the
131:43 - cities that we have and we are going to
131:45 - create a Boolean visited array to keep
131:47 - track of all the cities that we have
131:49 - visited so far initially the number of
131:51 - provinces are going to be zero and then
131:53 - we are going to run a for Loop to
131:55 - iterate over all the given number of
131:57 - provinces and initially we are going to
132:00 - check that whether that particular city
132:02 - has been visited or not if it is not
132:04 - being visited then we we will go to our
132:07 - BFS call or breath for search call and
132:10 - uh recursively try to find the solution
132:12 - and every time uh the Call Comes Back we
132:14 - are going to update the number of
132:16 - provinces now in the end we are simply
132:18 - going to return the number of provinces
132:19 - now let's see that how does our BFS
132:21 - function works now since we are doing
132:23 - breath for search breath for search can
132:26 - be done using a cube so we are going to
132:29 - first of all initialize a cube and uh we
132:31 - are going to have a link list for the
132:33 - cube you can have lot of data structures
132:35 - s Cube so first we are going to start
132:38 - with the start value then we are going
132:40 - to mark that start value as V visited
132:42 - and then through that start value we
132:45 - will keep on iterating to its neor
132:48 - neighboring cities until we have visited
132:50 - all the values inside the current q and
132:53 - the moment as Q is empty we are going to
132:55 - jump out and say that okay now we have
132:57 - visited all the connected cities and
133:00 - throughout our connection we are marking
133:02 - those cities as visited so that's how we
133:05 - are we have been able to calculate uh
133:07 - all the connected cities as visited and
133:09 - only increasing the number of provinces
133:11 - for the new cities now let's try to run
133:13 - this
133:14 - code okay seems like our solution is
133:17 - working let's submit this code code and
133:19 - our codee runs pretty efficiently
133:21 - compared to lot of other Solutions so
133:23 - you can imagine that how a combination
133:25 - of data structures are being used to
133:27 - solve a simple problem and uh KN knowing
133:32 - all of these data structures would help
133:34 - you quite significantly in order to
133:36 - break down and solve a problem and
133:38 - generate the algorithm for that
133:42 - problem okay now we will learn about
133:44 - hash based data structures now hash
133:47 - based data structure are actually quite
133:49 - unique in nature plus they have their
133:51 - own set of uh very important use cases
133:55 - that you are going to see out throughout
133:56 - your data structure and algorithm
133:58 - problems so first let's understand that
134:00 - what does a hash based data structure is
134:02 - typically when we talk about hash based
134:04 - data structure we are usually talking
134:06 - about a hash map and a hash set now both
134:11 - are quite similar in nature and both
134:13 - have very similar properties but the
134:15 - only difference is that hashmap is
134:17 - typically used used for to store a key
134:19 - value based uh mechanism where we have
134:22 - some key that we use to search through
134:25 - the given uh data structure so it will
134:28 - give us an idea that what are the things
134:30 - currently present inside our given data
134:32 - structure and what are the values
134:34 - associated with that hash set is uh
134:36 - where we only save hash based keys so
134:40 - nothing more than that so now first
134:42 - let's understand that what does a hash
134:44 - map is how to use it we will see an
134:47 - example of how it is being used uh in an
134:49 - actual interview and then we will learn
134:51 - about hash set so hashmap as I already
134:54 - mentioned that there are two properties
134:56 - associated with that first one is a key
134:59 - and second one is a value now let's try
135:01 - to understand hashmap from a real life
135:04 - point of view if you have ever seen any
135:07 - kind of dictionary well typically the
135:09 - dictionary is a very close example of a
135:12 - hashmap where in the dictionary
135:14 - typically on the first page we usually
135:16 - have an index index page and in that
135:18 - index page we are given the information
135:21 - that uh a starts from page number one uh
135:24 - something like B starts from page number
135:26 - 15 and so on and so forth and this is
135:29 - usually sorted so if we have to find any
135:31 - particular value all we need to do is go
135:34 - to the index page in the index page we
135:36 - can find the information that okay uh
135:38 - letters starting from um M are actually
135:41 - stored at page number 53 then all we
135:44 - need to do is that in our dictionary or
135:45 - in our book just go to page number 53
135:48 - and we would be able to find the meaning
135:50 - of word man or map or anything that it
135:53 - would be present over there so this is
135:55 - the actual idea being used in the inside
135:58 - the hashmap as well where we are given
136:01 - two items or we are storing two items
136:04 - first item is a key and second item is a
136:07 - value now key is going to be the hash
136:10 - function that is a some computational
136:13 - method so this is usually the hash
136:17 - function we Define where the values are
136:20 - being stored as keys and whenever we
136:22 - need to search inside our hashmap that
136:25 - whether this particular key is present
136:27 - or not we can do that in B of one time
136:29 - in constant time so let me give you an
136:31 - example of how does this actually work
136:34 - let's try to understand that we
136:36 - currently have five different values
136:39 - that we are trying to store and these
136:41 - five values are 11 12 13 14 and 15 now
136:46 - for these values I want to create a
136:49 - hashmap function where I want to retrive
136:52 - these data in the quickest manner
136:54 - possible which means I already know that
136:56 - these five are going to be my values but
136:59 - I need to create keys for them and the
137:01 - key I'm creating is that I create a
137:04 - method where I say that any particular
137:06 - value that comes in I'm just creating a
137:09 - basic hash function so you would get an
137:11 - idea that I create a method where any
137:14 - particular value that comes in I'm going
137:16 - to divide that value by five and
137:19 - whatever the remainder is so the
137:21 - remainder I'm going to treat it as a key
137:25 - and now let's see that how would this
137:27 - work so first let me draw my hashmap and
137:30 - inside the hashmap I'm going to have two
137:32 - values key and value so first value is
137:34 - 11 that I'm need I'm trying to input so
137:36 - I'm going to divide 11 by 5 so 11 by 5
137:40 - is going to yield me the remainder as
137:42 - one I'm only concerned about the
137:45 - remainder I'm not concerned that what
137:46 - does the division value comes in so
137:49 - remainder is going to be one so I'm
137:50 - going to create a value over here where
137:52 - the key is one and its Associated value
137:56 - is going to be 11 same way for Value
137:59 - number 12 the remainder is going to be
138:01 - two so once again I have an entry two
138:03 - and the value is 12 same way there is uh
138:06 - for 13 we have value number three and
138:09 - the value is three for 14 we have value
138:12 - number four and value number uh uh 14
138:15 - and the last one is the value number 15
138:19 - so what should be the key for Value
138:21 - number 15 well if you guessed five
138:24 - that's wrong actually the key should be
138:25 - zero because remainder in this case is
138:27 - actually zero when you divide uh 15
138:30 - divided 5 you actually get the remainder
138:32 - as zero so 0o is associated with value
138:35 - number 15 now I have these currently
138:39 - these values being stored now let's say
138:41 - in my program for some purpose I want to
138:44 - check that whether in our hash map do we
138:47 - all already have value number 13 or not
138:49 - so what I'm going to do in this scenario
138:51 - is that for Value number 13 I'm going to
138:54 - uh basically divide this 13 by 5 so I'm
138:57 - going to check that remainder is equal
138:58 - to 3 now this remainder is actually my
139:01 - key so then I would go to the hashmap
139:03 - and say that hey hashmap do you have any
139:06 - value where key is equal to three so
139:08 - hashmap would say that yes I have key is
139:10 - equal to three present but I don't know
139:12 - what value is so you would say okay
139:14 - bring me this value so you you fetch
139:17 - this value and this value it turns out
139:19 - to be 13 and then you would be able to
139:21 - say that okay in my hashmap I have a
139:24 - value 13 that is stored currently
139:26 - present and then I can do some
139:27 - competition with this now can you
139:31 - identify some issues with the hash
139:33 - function I have created uh let me just
139:35 - draw the hash function again and
139:38 - currently these are the values I have
139:41 - and the associated values are 11 12 13
139:44 - 14 and 15 now this is currently my my
139:47 - hash function okay so far it looks good
139:50 - but now let's say that I'm trying to see
139:53 - that does value 61 exist in in my hash
139:56 - map or not so what would my Approach is
139:58 - going to be once again I'm going to do
139:59 - 61 divid 5 and remainder is going to be
140:02 - 1 so once again as a remainder I'm going
140:04 - to go to the hashmap and see that okay
140:06 - hey for this key number one what is the
140:09 - value associated with this the value
140:11 - Associated right now is only 11 so I can
140:14 - say that 61 is currently not present in
140:16 - the hashmap once again I got this result
140:18 - in big off one time but now what if it
140:20 - happens that I want to add 61 to my hash
140:23 - map as well well in this scenario I
140:26 - since the key can only remain constant
140:29 - so which means for this key 1 I have one
140:32 - more entry that is 61 that I need to add
140:35 - over here so in the place of value I
140:37 - might I might need to create something
140:39 - like a link list because there is a
140:41 - collision over here of the keys because
140:43 - the number of values are more and keys
140:46 - are less yes so in this kind of scenario
140:49 - collisions are bound to happen and
140:51 - because there is a collision now I have
140:53 - value number 61 let's say the value is
140:55 - 71 I'm trying to add so once again I
140:57 - will add one more node in the link list
140:59 - and I would add value number 71 over
141:01 - here so so far let's assume that
141:03 - initially when we only had five values
141:06 - we actually had five keys and five
141:09 - values so the equation was almost one to
141:12 - one relation and we were able to fetch
141:14 - any particular data in big of one time
141:17 - but now let's assume that we still keep
141:19 - only five keys and now we have uh for an
141:22 - example one 1 million entries so in this
141:25 - scenario or let's say 5 million entries
141:28 - and five keys so mathematically every
141:32 - single key would have 1 million entries
141:35 - ass associated with that particular key
141:37 - and in that case all of these 1 million
141:40 - values would be stored in a link list uh
141:43 - in the value section connect where one
141:45 - node is connected with the another node
141:47 - so so once again in this kind of
141:49 - scenario if I have to fetch any value
141:52 - now now it it's going to take big of end
141:54 - time where n is going to be 1 million in
141:57 - this case so this is actually really bad
141:59 - so in the hash whenever you are building
142:01 - your own hash map you need to make sure
142:03 - that the hash function you are you are
142:05 - using to build evenly distributes the
142:08 - values which means that for any single
142:10 - value typically the hash function tends
142:12 - to be a unique set of key and that is
142:15 - one of the biggest consideration now now
142:17 - lucky for us most of the programming
142:19 - languages like Java python or whatever
142:22 - you can think of they already have this
142:24 - functionality of implementing a very
142:26 - good hash function that usually tends to
142:28 - be unique and implements the keys in the
142:31 - unique manner so we don't have to worry
142:33 - about it all we need to do is just
142:35 - declare the hashmap and uh we need to
142:37 - declare that what type of key and value
142:39 - pairs are we are going to store so we
142:41 - can store like integer as the key and
142:44 - string as the value and then we can name
142:46 - our hash map as something like my
142:48 - hashmap and then we simply need to
142:50 - initialize it and that's it so this way
142:53 - it's it becomes really powerful so
142:55 - always make sure that even during the
142:57 - interview interviewers love asking this
142:59 - question that uh how do you handle
143:01 - Collision in your hashmap so typically
143:03 - you would assign a link list for to
143:06 - store values when you predict a scenario
143:08 - where there is going to be a collision
143:10 - in terms of values now let's see that
143:12 - what are the different operations we can
143:14 - do for any given hashmap and I'm going
143:16 - to store some values so let's say one is
143:19 - associated with 11 and two is associated
143:21 - with 12 so this is our current hash map
143:23 - right now hashmap tends to be dynamic in
143:25 - nature so which means that uh all the
143:28 - values you want want to assign if you
143:30 - want to keep on adding more and more
143:32 - values in this scenario essentially you
143:34 - don't have to do anything it will
143:36 - usually takes care of it and most of the
143:37 - languages are sophisticated enough to be
143:39 - smart in that regard so that is really
143:41 - good for us now first operation we can
143:44 - do in hashmap is we can add a value so
143:46 - typically this is done by using
143:48 - something called a put variable uh and
143:51 - we can put a new value inside the
143:53 - hashmap and this is usually done in big
143:55 - off one time where we need to provide
143:57 - the value of a key and Associated value
143:59 - with that remember key cannot be a null
144:03 - value you can have a key uh that does
144:06 - not have any value associated with that
144:08 - that's fine but still key cannot be a
144:10 - null value you cannot have a value that
144:12 - does not have a key associated with that
144:14 - in the hashmap second operation we you
144:16 - can do is you can delete any value from
144:18 - the hashmap and this is also done in big
144:20 - off one time so that's great next
144:23 - operation you can do inside the hash map
144:25 - is that you can search for any element
144:27 - and searching also takes big off one
144:29 - time because we are using the property
144:31 - of key value pair and we are searching
144:33 - using the keys we are not searching
144:35 - using the values so that is an important
144:38 - distinction you have to understand
144:40 - whenever you are trying to think that
144:42 - how you're going to use hashmap to
144:44 - achieve your goal next uh operation we
144:47 - can do is we can actually sort all the
144:50 - values but that we need to do it
144:52 - sequentially so there is no inherent
144:55 - method to do that so essentially this is
144:57 - still big of n log n uh time operation
145:00 - where you need to uh call some other
145:02 - data structure and then you need to
145:04 - populate all the values uh let's see
145:06 - what are other operations we can do you
145:08 - can of course you can modify all the
145:10 - values that are currently present and
145:11 - that can be done in B of one time and
145:14 - that's it so now you can see that uh
145:17 - hashmap on paper looks pretty good
145:18 - because all the operations that you can
145:20 - think of insertion deletion addition
145:22 - modification uh searching sorting
145:24 - they're all pretty fast and pretty
145:26 - efficient uh there is one big issue with
145:29 - the hashmap and that big issue is that
145:32 - you cannot have duplicate keys so uh the
145:36 - key value has to be unique so no
145:39 - duplicate keys and also the duplicated
145:42 - values would be stored under the same
145:45 - key because key these are typically
145:47 - generated based on based on their values
145:50 - so always remember that duplicate values
145:52 - cannot reside inside the hashmap and now
145:56 - let's see that how does a hash set work
145:59 - and then we will see an example of a
146:01 - typical hashmap problem so hash set is
146:04 - very similar to hashmap but the only
146:06 - difference is that inside the hash set
146:08 - there is no concept of key value pair it
146:10 - is only just the values that are being
146:12 - stored but these values are typically
146:14 - stored using the hash function so uh
146:17 - let's say that I currently have values 1
146:19 - 5 15 uh 31 and 21 uh that I need to
146:23 - store so so far I have stored these four
146:25 - values inside my hash uh set so I can of
146:29 - course enter these values 11 15 uh 31
146:33 - and five now I want to check that
146:34 - whether 21 value is present inside the
146:36 - hash set or not so I can check this in
146:39 - big of one time why because just like a
146:42 - hash map hash set also uses the hashing
146:44 - function it just doesn't store the key
146:47 - but still it calculates based on the
146:49 - value that you are trying to input so
146:51 - there is going to be a mathematical
146:53 - function that operates and that would be
146:55 - able to immediately identify that
146:56 - whether this value has been processed
146:58 - and stored inside the headset or not and
147:00 - uh it's not stored so I we can add this
147:02 - entry now again just like hash map hash
147:06 - set also does not allow duplicates so
147:09 - there is there are no duplicated entries
147:10 - over here and uh one good property that
147:14 - since there are no duplicated entries
147:16 - allowed whenever you need to check that
147:18 - in any other data structure like a link
147:20 - list or um an array if any duplicated
147:24 - entries exist you can actually use
147:25 - hashset to solve that kind of problem
147:27 - quite easily and there are many use
147:29 - cases for that as well uh once again if
147:32 - we look at the operation operations are
147:34 - pretty much the same so uh everything
147:36 - like uh adding value takes big off one
147:39 - time once again deleting also takes big
147:41 - of one time uh searching takes big of
147:43 - one time sorting takes big of uh and log
147:47 - and time so this you can usually
147:49 - consider very similar to hashmap the
147:52 - only difference is that in the hashmap
147:54 - you can actually store the correlation
147:56 - between key and value in the hash set
147:58 - you only store the
148:00 - value now let's try to understand couple
148:02 - of examples through which we will
148:04 - understand both hash map and hash set so
148:08 - let's say for an example we are given
148:10 - suppose an array a where we are given
148:13 - five different elements and now our job
148:15 - is to check that whether
148:17 - uh the array contains any duplicate
148:19 - element or not and if it does what is
148:21 - the duplicate element so let me give you
148:24 - an example suppose the values are 5 3 1
148:26 - 2 and 3 so we can see that three is the
148:29 - duplicated element and the purpose of
148:32 - our program is to check or find that
148:34 - whether there is a duplicate element or
148:36 - not and if it if yes then which is the
148:39 - duplicated element so first let's
148:41 - understand that what would be the Brute
148:42 - Force approach in this scenario well it
148:44 - is quite simple we just take take any
148:47 - element so let's say we take element
148:49 - number five in this case and then
148:50 - iterate over the rest of the array to
148:52 - see that whether Val value five is
148:54 - present or not if it it's present we can
148:56 - return true if it's not present we can
148:58 - return false and go to the next element
149:00 - so for three we will repeat the same
149:01 - process and we would find that three is
149:03 - also present in this case so we will
149:05 - return return three as a duplicated
149:06 - element but this approach yields the big
149:08 - of n Square time complexity because for
149:11 - every n element we are essentially doing
149:13 - n minus one work so that is a
149:14 - multiplication factor and uh which is
149:17 - not too good so let's see what would be
149:19 - the better solution so better solution
149:21 - would be that uh if we try to sort this
149:24 - given array then things would become
149:26 - easier so if we sort this input we will
149:28 - get an array that looks like this that 1
149:30 - 3 uh sorry 1 2 3 3 and 5 so in this case
149:35 - uh then we only need to check the
149:37 - adjacent values and here we would be
149:39 - able to see that these two adjacent
149:40 - values are same so we would return
149:42 - return three as the answer now this
149:44 - better solution where we are sorting the
149:45 - array that is also bigo of n log n time
149:49 - solution so could there be a better
149:52 - approach and the answer is yes and that
149:54 - is by using a hash set in this scenario
149:57 - so what we would do is we would
149:58 - initialize a hash set our algorithm
150:00 - would be that we would first check that
150:02 - whether any element if that is present
150:05 - inside the hash set or not if it is
150:07 - present already and it is also in the
150:09 - array then we can consider that to be a
150:11 - duplicated element if it is not present
150:14 - then we would simply add that element to
150:15 - our ha set so let's try to run this
150:18 - method so first we would add value
150:19 - number five over here because it is not
150:21 - present already uh so then after adding
150:24 - it we would move to the next element now
150:26 - value number three is not currently
150:28 - present inside the haset so we would
150:30 - again repeat the same process then again
150:32 - we would repeat the same process for
150:33 - Value number one and again we would
150:35 - repeat the same process for Value number
150:37 - two and all of this insertion would take
150:39 - big go of one time Plus checking that
150:41 - whether the element is present or not
150:43 - that would also take big off one time so
150:45 - this is not any uh additional overhead
150:48 - now when we are at position number three
150:50 - we check that whether three is already
150:52 - present inside the hash set or not
150:54 - within big go of one time we would be
150:56 - able to find out that three is already
150:58 - present which means we would know that
151:00 - three is the duplicated element and we
151:02 - can return true in this case that yes
151:04 - this array does contain a duplicate
151:05 - element and if we are being asked to
151:07 - pass on that which value is repeated we
151:09 - can return three in the answer so either
151:12 - case this hash set works perfectly fine
151:15 - now what would be the solution in this
151:16 - case or the time complexity well time
151:19 - complexity is going to be biger of n
151:21 - because we are going to only iterate
151:23 - over the element or this array ones to
151:25 - solve this problem but in this case
151:27 - space complexity is also going to be
151:29 - bigger of one uh bigger of n because we
151:32 - need to create an additional hash Set uh
151:34 - and its size is dependent on the number
151:37 - on the given input now let's see that uh
151:40 - this would be an example where we would
151:42 - use hash set to solve the answer but now
151:44 - consider a scenario that in the same
151:46 - example uh we are being asked that
151:49 - rather than just giving out the value of
151:51 - the uh duplicated element give us the
151:54 - index value so again let's see that how
151:56 - would that scenario work suppose this is
151:58 - the uh given new array and we need to
152:01 - return the index elements of the
152:03 - duplicated element how what should be
152:05 - our approach once again we already know
152:07 - that brute force and uh sorting approach
152:09 - already works the way it does in this
152:12 - scenario as well but in this case since
152:13 - we need to return the index values hash
152:17 - set would not be the optimal choice and
152:19 - it would make sense for us to use a hash
152:21 - map where inside the hash map we would
152:24 - actually create a key value pair based
152:26 - database where in the key we are going
152:29 - to Mark these values and as their
152:32 - subsequent values we are going to Mark
152:34 - the index positions so when we need to
152:36 - fetch that which index positions are
152:38 - duplicated we can return that quite
152:40 - easily so now let's see the solution in
152:43 - this case so for first of all we would
152:45 - check whether five is is present or not
152:47 - five is currently not present inside our
152:48 - hash map so we would add an entry called
152:50 - five and we would add its index location
152:53 - zero now again one is also not present
152:55 - so we would add entry one and its index
152:57 - location is one three is also not
152:59 - present so three and index location two
153:01 - and four is not present so four and
153:03 - index location three now again this is
153:06 - key value pair so you understand what
153:07 - I'm doing now when we are at this
153:09 - position number five or position number
153:11 - four uh where the value is one we try to
153:15 - add one but we already see that one is
153:17 - already present because it is marked as
153:18 - a key and this operation can be done in
153:20 - big off one time because it's a hashmap
153:23 - so since we already found that we can
153:26 - actually return one uh is the duplicated
153:29 - entry and if we want to return the index
153:31 - positions we can return the index
153:32 - position from here that would be four
153:34 - and the value of this key one so that
153:37 - would be one and this would be the
153:39 - answer we need to return in this case so
153:41 - you can understand that how quickly we
153:43 - are able to solve problems using hash
153:46 - map or hashset and uh we are going to
153:48 - use them throughout this course for
153:51 - bunch of different problems so you
153:52 - should be able to understand them quite
153:57 - easily now after learning most of the
153:59 - basic data structures we are going to
154:01 - learn couple of advanced data structures
154:03 - and these Advanced data structures are
154:05 - actually quite popular in technical
154:07 - interviews and also a lot of
154:08 - computational problems as well so there
154:11 - are quite a lot of advanced data
154:13 - structures but we are going to shift our
154:15 - focus on two two main ones first one is
154:18 - called Heap and second one is called try
154:21 - now both are combination of various data
154:23 - structures but they have a very specific
154:25 - use cases where it is really popular and
154:28 - really powerful so first let's
154:30 - understand that what a heap is Heap is
154:33 - typically an extraction of a binary uh
154:36 - tree tree based data structure where it
154:39 - it maintains the property of a binary
154:41 - tree which means any single node has at
154:43 - most two children and it tries to
154:45 - balance out this property as quickly as
154:48 - possible and even if there is an anomaly
154:50 - it would only be at one of the positions
154:52 - where the number of children would be
154:54 - one but nothing more than that so you
154:56 - would never encounter a scenario where
154:58 - any single node has three children or
154:59 - something like that so after that there
155:02 - is also one more important property for
155:04 - a heap and that is that it is a common
155:07 - implementation for a typical priority
155:09 - Cube and we all know that in a priority
155:11 - Cube the most important thing is higher
155:13 - or lower priority depending on that so
155:16 - in the Heap usually we use it to keep
155:19 - track of uh the maximum value out of any
155:22 - given incoming data stream or the
155:24 - minimum value of uh any incoming data
155:27 - stream and both have very significant
155:30 - purposes and very significant importance
155:32 - in many of the questions so first let's
155:34 - try to understand that how does a
155:36 - typical Heap looks like how does it work
155:39 - and then we will see some examples so
155:41 - first we will try to implement a minimum
155:43 - Heap now the property of a minimum he
155:46 - Heap is that number one it is going to
155:48 - maintain the binary tree nature so one
155:50 - node is going to have at most two
155:52 - children number two is that every single
155:56 - parent is going to have less value than
155:59 - its child and number three property is
156:02 - that all the nodes that we would try to
156:04 - enter first we would enter in the root
156:06 - position if the root is already filled
156:08 - we would try to enter that node into the
156:10 - leftmost position if that is also filled
156:12 - then we would try to enter that node as
156:14 - a right child for any particular empty
156:17 - location so we would try to find the
156:18 - location first once that is done we
156:21 - would sort of heapify the given input or
156:24 - the our given Heap to maintain this
156:26 - property so let's see this in action
156:30 - suppose uh currently we have an empty
156:32 - Heap so let me draw an empty circle and
156:35 - now let's assume that the first value we
156:37 - are trying to enter is going to be value
156:39 - number five so initially we are entering
156:41 - value number five over here now the next
156:43 - value we try to enter let's assume that
156:45 - that is value number 10 so remember the
156:49 - scenario is first we will try to add it
156:51 - to the root value root we already have
156:53 - added which means we will try to add in
156:55 - the left children so let's add the
156:58 - number 10 in the left child and after
157:00 - adding that we need to check that
157:02 - whether this property has been
157:03 - maintained or not that whether the
157:05 - parents value is less than the children
157:07 - value or not and in this case since five
157:10 - is the parent it is less than 10 so this
157:13 - is currently correct so we do not need
157:15 - to do he operation for this step now
157:17 - let's see one more element so suppose we
157:20 - try to enter value number 15 now again
157:23 - 15 needs to go to uh either root place
157:26 - but root is already filled so it needs
157:28 - to go to left that is also filled so it
157:30 - will go to the right place and right
157:32 - place after adding 15 we would again
157:34 - check that whether the he5 property is
157:36 - maintained so again parents is still
157:38 - smaller than children so that is good
157:40 - now let's try to enter the value number
157:43 - 25 so once again 25 will go go to the
157:46 - left position and so far everything
157:48 - seems to be working okay no issues with
157:50 - this now let's try to enter the value
157:52 - number seven now again by this logic
157:56 - first of all seven has to enter to the
157:58 - right available position so this is not
158:01 - available this is not available but this
158:03 - position is available so let me enter
158:05 - seven over here but does there an issue
158:09 - with parents being lesser than child yes
158:12 - there is in this case seven is smaller
158:14 - than uh value number 10 so we would have
158:16 - to do the he5 operation so we would move
158:18 - seven from here and we would make it as
158:21 - parent and 10 would come to this place
158:24 - okay now everything is good so far now
158:27 - let's say that we try to enter value
158:28 - number 31 once again 31 would be entered
158:32 - over here and the property is still
158:34 - maintained now let's try to enter value
158:36 - number two so by default value number
158:38 - two is going to come over here but since
158:40 - two is smaller we will have to make a
158:42 - shift over here so let me quickly shift
158:45 - the values so now 2 is going to come and
158:49 - 15 is going to come over here once again
158:51 - two is still smaller so once again we
158:53 - will have to do a shifting operation so
158:56 - in the end two is going to come over
158:58 - here and five is going to come over here
159:00 - and now the Heap has been maintained so
159:02 - this is how typical Heap operates now in
159:05 - this case if you see always the parent
159:08 - is going to be smaller than its children
159:10 - so currently for the entire Heap we have
159:14 - the smallest value POS positioned at the
159:16 - root position so this is whenever if we
159:19 - have to identify that hey we have an
159:21 - incoming constant data stream that is
159:23 - coming in and we want to find that what
159:25 - is the minimum value at all times we can
159:27 - actually use Heap because it's a self
159:30 - adjusting uh algorithm or data structure
159:33 - that maintains its property of whether a
159:35 - Min Heap or Max Heap so in this case we
159:38 - can identify that okay the minimum value
159:39 - is going to be the value at our very
159:42 - first node and maximum value is going to
159:44 - be some value amongst the these Leaf
159:46 - node so in this case maximum is going to
159:48 - be 31 but we are not bothered about it
159:50 - right now now once again if we take any
159:53 - subtree so let's say in this case once
159:54 - again parent is still going to be
159:56 - smaller than child once again parent is
159:59 - still going to be smaller than children
160:01 - so Heap is a great algorithm whenever
160:04 - you need to keep track of that hey I
160:06 - have an continuous incoming stream of
160:09 - data coming in and I want to check the
160:11 - fifth smallest value from the beginning
160:15 - so in that that case Heap would be an
160:16 - ideal choice where you keep track of all
160:19 - the values in a Min Heap and then you
160:21 - essentially needs to go and select the
160:23 - fifth smallest value which can be done
160:25 - in log and time so that's great you will
160:28 - get the answers quite quickly now let's
160:30 - see an example for a Max Heap and Max
160:32 - Heap also works in the similar fashion
160:34 - the only difference is that now the
160:37 - value of parent has to be greater than
160:39 - the value of child so let's see that
160:41 - first we are try we have the element and
160:44 - we are trying to add value number two
160:45 - over here now the next element we are
160:47 - trying to add is value number three so
160:48 - in this case we will have to do an
160:50 - adjustment so first we will add three
160:51 - over here and then we will do our HEI so
160:54 - we would adjust the values so now three
160:56 - would be the maximum value and two would
160:58 - be the smallest value now we are trying
160:59 - to add value number 15 once again first
161:02 - we will add 15 over here but then again
161:04 - we will do the hepy principle and we
161:06 - will flip the values so 15 will become
161:08 - over here and three will come over here
161:10 - now we want to add value number five so
161:12 - by default five would be added over here
161:14 - but once again we are going to do over
161:15 - ep5 operation so according to that logic
161:18 - five will come over here and two will
161:20 - come over here and same way we can
161:22 - maintain a Max Heap as well so in this
161:25 - case whenever you would see the root
161:28 - node is going to be the maximum element
161:30 - at any given location for any particular
161:33 - subtree the parent is always going to be
161:35 - greater than child and Heap is going to
161:38 - Bubble Up and maintain its Max Heap
161:40 - property so this is how typically we use
161:43 - Heap in order to keep track of min
161:45 - minimum value or maximum value at any
161:47 - given moment on top of that we also keep
161:50 - track to for any questions like find the
161:54 - K largest value or k smallest value from
161:57 - the top so in either case the Min Heap
162:00 - and Max Heap or are going to be the
162:04 - ideal choice to solve these kind of
162:06 - problems and we would see bunch of
162:08 - operations later on in the course so now
162:11 - let's try to see that what are going to
162:13 - be the time and space complexity so
162:15 - ially the time complexity in order to
162:17 - insert any element inside the Heap is
162:19 - actually big go of logarithmic n why log
162:22 - n because it is already a sorted tree
162:25 - structure whenever we try to enter any
162:28 - value in any particular location we
162:30 - simply need to find that what would be
162:32 - the right place for it to be so at most
162:36 - all we need to do is make change to
162:38 - couple of places and then we can
162:39 - actually find its correct location so
162:42 - insert insertion is login same way
162:44 - deletion is also login same way finding
162:47 - anything is also login so this is a very
162:49 - powerful and very fast data structure uh
162:52 - from many point of you and that is why
162:54 - it has lot of use cases in the real
162:56 - world scenarios so this is the example
163:00 - of Heap now let's try to understand that
163:02 - what does a TR data structure looks like
163:05 - now try is also uh an additional
163:08 - abbreviation of a tree based data
163:10 - structure now inside the try uh we
163:14 - actually use try on a daily basis so
163:17 - whenever you see any system that uses
163:20 - autocorrect functionality so that is
163:22 - actually using a try so this covers your
163:25 - iPhone Android phone your messaging app
163:27 - anything plus wherever you see
163:28 - predictive searches so what do I mean by
163:30 - predictive search uh you can imagine
163:33 - that whenever you try to search anything
163:34 - on the Google or Amazon it automatically
163:37 - completes the statement for you and for
163:39 - that actually behind the scenes it is
163:41 - using try as a data structure so try is
163:44 - a very powerful tool since you can see
163:46 - that it has lot of practical
163:47 - applications and also remains favorite
163:49 - amongst technical interviews as well so
163:52 - that is why we are going to understand
163:54 - what a try is basically try typically
163:57 - operates on a string based characters so
164:00 - wherever you have bunch of different
164:02 - characters that you that you working
164:04 - with and you want to store them in a
164:07 - sequential manner where depending on the
164:11 - values you enter in the character You
164:13 - can predict the next element you can
164:15 - actually use a try so how does a try
164:17 - typically work is typically it has a
164:19 - root node and this root node tends to be
164:21 - an empty node but in its children we are
164:24 - going to store all the uh alphabets that
164:27 - are possible so typically in a real life
164:30 - try there would be 26 children one for
164:33 - each character from a to zed but in this
164:35 - case let's just let's just consider our
164:37 - example small and concise so we would
164:40 - try to add five different words to our
164:42 - try and we would see how they they would
164:45 - look look like so first word we are
164:46 - going to add is going to be the word rat
164:49 - second word is going to be R third word
164:51 - is going to be uh
164:54 - men and fourth word we would be uh let's
164:58 - say tree and fifth word would be uh try
165:04 - so these are the words we are trying to
165:05 - add first so let's just start with first
165:09 - one so first we will try to add R so we
165:11 - are going to create a child over here
165:13 - called r then the next word is a so
165:16 - again R is going to have a sub child
165:19 - called a and next word is t so T is
165:22 - going to going to be the child of a and
165:25 - now since this word has ended so now
165:27 - this is going to be an end node so we
165:29 - can determine this as a star or
165:31 - something okay that defines that this
165:33 - word currently ended over here now next
165:35 - word we are trying to add is R so once
165:37 - again from our root node we are not
165:39 - creating we are not going to create a
165:41 - separate child for R we are actually
165:43 - going to create a r as it as it is and
165:46 - then we are going to create a sub Branch
165:49 - so this R has already been taken care of
165:51 - now there is the word a so once again
165:54 - for a we are not going to create a new
165:56 - Branch but we are going to treat this R
165:58 - A to be of the same group so now you can
166:01 - see that how different words can be
166:03 - stored within a tree like data structure
166:06 - by their character properties because
166:08 - they are all connected with each other
166:10 - very deeply and next one is p but over
166:13 - here the next word is actually T so in
166:16 - this case we are going to create a new
166:17 - Branch over here and this new branch is
166:20 - going to have value number P and since
166:22 - the word ends over here we are going to
166:25 - have an ending node so that is denoted
166:27 - by star so now whenever I start typing r
166:31 - a uh the system would come over here and
166:34 - then system would predict that either
166:36 - I'm typing r a rep or r a rat and then
166:40 - that is how it is able to do a
166:42 - predictive search let's say that I print
166:44 - uh I type something like r a r rare but
166:48 - the system knows that rare is not a
166:50 - valid word which means system would
166:52 - predict that either the valid word has
166:54 - to be r or R so depending on the con uh
166:57 - context of the entire statement you are
166:59 - trying to make it will automatically
167:02 - adjust the word that you are writing and
167:04 - over here it would from this it might
167:07 - autoc correct it to the word rat and uh
167:09 - things would work fine so this is how
167:12 - typically autocorrect functionality
167:13 - works now let's try to add the word man
167:15 - over here now for M there is no child so
167:18 - we are going to create a new child and
167:20 - once again we are going to create a new
167:22 - child and we are going to create a new
167:23 - child and then we are going to create an
167:25 - end node so this is how men would look
167:28 - like now let's try to create the word
167:30 - tree so once again for tree uh we do not
167:33 - have any child so let's create a new
167:35 - branch and we are going to start with
167:38 - word t now next one is r and next one is
167:42 - e and next one is going to be e and then
167:45 - there is an end node but now let's try
167:49 - to add one more word try t r i e so we
167:52 - can already see for T and R it is going
167:55 - to have the same path or same Branch but
167:58 - now it is going to have a separate
167:59 - branch called I and then it is going to
168:02 - have a separate branch called e and then
168:05 - it would end the word essentially so
168:07 - this is how using just these three
168:10 - branches we can actually navigate all of
168:13 - these five uh words
168:15 - so this is the true power of the TR data
168:18 - structure and uh try is pretty popular
168:22 - in lot of technical interviews and
168:24 - wherever you see the example where you
168:26 - are given bunch of different words and
168:28 - you are trying to find some meaning and
168:30 - you are trying to find the best way on
168:32 - how to process them and how to store
168:34 - them try to consider or thinking of
168:36 - using the the data structure try and
168:39 - once again for try later in the course
168:41 - we are going to see an example so things
168:43 - would become much more more clear so I
168:46 - hope that you got the idea about
168:48 - Advanced data structures of Max Heap Min
168:51 - Heap and tribe and now let's move
168:53 - forward so first of all I would like to
168:55 - congratulate all of you for making up
168:58 - until this far uh we have cross the
169:01 - biggest hurdle in our technical
169:02 - interview preparation journey and that
169:04 - is learning about all the popular data
169:06 - structures I hope that you must have
169:08 - good enough understanding of each one of
169:10 - them and now we are going to shift our
169:12 - Focus towards algorithms which is not so
169:15 - complicated as data structures were but
169:17 - needless to say they are quite important
169:20 - so I hope you give your utmost attention
169:22 - to this portion because once you
169:24 - understand algorithms things will become
169:26 - much more easier for you to proceed for
169:28 - the subsequent
169:32 - parts now one of the most important
169:34 - thing that you have to master in order
169:36 - to master any technical interview is to
169:39 - understand that what are the different
169:40 - algorithms available to you and what are
169:42 - typical coding patterns that you need to
169:44 - to recognize but before we start diving
169:47 - deep into it first let's understand that
169:49 - what does algorithm mean algorithm is
169:51 - nothing but a simple set of procedure a
169:54 - computer or a program needs to follow in
169:56 - order to generate some solution so let's
169:59 - say that this is the problem statement
170:01 - given you need to follow a certain steps
170:03 - in a certain sequence in order to
170:05 - generate the results and there are many
170:07 - different techniques to that that we
170:08 - will tackle in this uh portion so
170:11 - following that would generate the
170:14 - unnecessary output that you looking for
170:16 - and understanding that what are the
170:18 - options available to you is going to be
170:20 - a significant key for your Tech
170:22 - preparation Journey now what is coding
170:25 - patterns let me give you an example
170:27 - suppose if I write down uh few words
170:30 - over here let's say 2 4 6 can you
170:33 - predict what is going to be the next
170:34 - alphabet or next character I'm going to
170:36 - write over here of course you can
170:37 - predict that I'm going to write eight
170:39 - why because by looking at this you
170:42 - understood that this makes the next
170:44 - logic sequential sound move and this is
170:47 - what we need to do in terms of technical
170:49 - interviews as well that depending on the
170:52 - problem statement we should get some
170:54 - idea that hey in this type of scenario
170:56 - it would make sense to use a data
170:58 - structure like an array and in the array
171:01 - we are going to apply maybe sorting and
171:04 - then with the Sorting we are maybe going
171:06 - to apply uh binary search and this would
171:08 - yield us the correct result but we need
171:11 - to understand this type of pattern and
171:13 - this can only be done if we examine many
171:17 - questions of a similar type and then
171:19 - understand or refer meaning to that so
171:22 - combination of algorithms and coding
171:24 - patterns are closely related with each
171:26 - other and you do that in tandem with
171:28 - data structures and that's it these are
171:31 - the three building blocks for any
171:33 - efficient data structure algorithm
171:35 - technical interview and uh we are going
171:37 - to take care of it so these are all the
171:40 - different techniques that we are going
171:41 - to cover so first of all we are going to
171:44 - cover the searching algorithm and
171:45 - sorting algorithm then one of my
171:48 - favorite topics that is dynamic
171:50 - programming and recursion so after that
171:53 - we are going to see bunch of different
171:54 - tree and graph algorithms and these are
171:57 - really popular because there are some
171:59 - very practical real life applications
172:02 - completely built upon the concepts of
172:04 - tree and graph algorithms then we we
172:07 - will see some greedy algorithms and
172:09 - backtracking techniques and in the end
172:11 - we would start talking about the divide
172:13 - and conquer sliding window two pointers
172:16 - and interval coding patterns so this
172:18 - would cover most of the topics that you
172:21 - can think of for any technical interview
172:24 - and once you understand this you would
172:27 - essentially able to master all the
172:28 - problems that you are facing so without
172:31 - any further Ado let's just get started
172:34 - and uh for each and every concept I will
172:37 - explain the concept first then I'll show
172:39 - you either one or two examples depending
172:42 - on how complex the problem is and we
172:44 - will solve that problem before moving
172:46 - forward to the next one so completing
172:48 - this would be one of the most important
172:51 - key factors for this uh
172:55 - course so first let's try to understand
172:57 - the searching algorithm Now searching
172:59 - algorithms are pretty common across data
173:02 - structure and algorithm related problems
173:03 - because at any given moment we need to
173:06 - find some elements from a variety of
173:09 - data structures and these data
173:10 - structures can be an array or something
173:13 - like a St or a CU and it on top of it
173:17 - there can be some other more complicated
173:19 - data structures uh maybe something like
173:21 - a hash or a tree or a graph so
173:24 - essentially for any given data structure
173:26 - you might be asked to search some data
173:29 - from that and typically we follow two
173:33 - main techniques so number one technique
173:35 - is a linear search and linear search is
173:37 - uh as the name suggest it's quite simple
173:40 - essentially you jump from one node to
173:42 - another node to another node until you
173:44 - find the element that you are looking
173:46 - for and the moment you find the element
173:48 - you simply return that search element so
173:51 - if we have to understand it with an
173:52 - example suppose we are given an array
173:55 - and inside our array the values are
173:57 - randomly jumbled up something like 4 3 1
174:00 - 7 and 8 now I ask you that hey go ahead
174:03 - and search for Value number eight the
174:05 - approach is going to be that we are
174:06 - going to jump through all of these
174:08 - values until we reach to Value number
174:10 - eight and the moment we reach we can say
174:12 - that okay this eight is located at index
174:14 - number number four or on the fifth
174:15 - position or we can simply say that yes
174:18 - eight is present currently in this array
174:20 - in either case linear search would work
174:23 - just fine and if we see time complexity
174:25 - for a linear search it's typically going
174:27 - to be biger of n usually whenever you
174:30 - are dealing with data structures like
174:32 - link list or even Q or stack typically
174:35 - you are going to deal on the linear
174:37 - search basis because that is the only
174:39 - way you can access data uh and if there
174:42 - are some additional versions of maybe Q
174:45 - something like a Min Heap or Max Heap in
174:48 - that case you can actually find the
174:50 - value you are looking for in little bit
174:52 - simpler manner but still essentially
174:54 - linear search you are always going to
174:56 - encounter uh in whatever you are trying
174:58 - to do next example uh for a searching is
175:01 - actually a binary search and binary
175:04 - search is quite powerful but it has some
175:06 - predefined condition associated with
175:08 - that and that condition is that whenever
175:11 - you are trying to use a binary search
175:13 - the condition is that the data set has
175:15 - to be sorted so it needs to be either a
175:18 - sorted array or let's say a binary
175:22 - search tree something like that where
175:24 - you can actually apply the concept or
175:26 - the principle of binary search and
175:28 - principle is actually quite simple say
175:30 - for an example we are given bunch of
175:32 - different elements over here uh and they
175:34 - are all sorted in a particular fashion
175:36 - so 1 3 11 15 19 and 27 these are the
175:42 - values we are currently present let me
175:44 - just add one more element somewhere so
175:46 - value number four okay so currently we
175:48 - are given Seven Elements inside this
175:50 - array now I ask you that for this array
175:53 - can you go ahead and find me value
175:55 - number 15 what would be your approach
175:57 - well of course you always have the
175:59 - option to go linearly and search for
176:01 - each and every value individually till
176:03 - you find the value you are looking for
176:05 - but since this is already a sorted
176:07 - property it makes sense to use the
176:09 - binary search approach where first we
176:12 - are going to find the middle element the
176:14 - middle element in this case is going to
176:15 - be value number 11 now we know that
176:18 - since this array is sorted we are
176:20 - guaranteed that if 15 exist it has to
176:23 - exist within this portion and there it
176:26 - it is not possible that it lies within
176:29 - these values so we can essentially get
176:31 - rid of all of these inputs and only
176:34 - focus our effort for this remaining
176:36 - portion now once again for this
176:37 - remaining portion again we will try to
176:39 - find the middle pointer and middle
176:41 - pointer in this case is going to be 19
176:43 - which means once again again we can
176:44 - eliminate these two values because all
176:47 - the values are going to be either 19 or
176:49 - greater than 19 and in either case it is
176:52 - not going to be 15 so we are only left
176:54 - with one element in this case and that
176:56 - is value number 15 we which we can
176:58 - return so only in three itations we were
177:00 - able to find the our answer we are
177:03 - looking for in an array of uh seven
177:06 - total elements basically the binary
177:08 - search actually operates in bigo of log
177:11 - n time and this logarithmic time
177:14 - complexity is what makes it viable so
177:18 - this is what how you can understand
177:20 - searching algorithm and you are going to
177:22 - see them quite a lot in lot of different
177:25 - problems you are trying to
177:28 - solve now before we start understanding
177:31 - the example for searching algorithm
177:33 - let's try to understand sorting first
177:36 - and then combination of searching and
177:38 - sorting would would be what we would use
177:40 - to understand one of the actual data
177:42 - structure and algorithm Rel related
177:44 - examples so let's try to understand
177:46 - sorting first now sorting as the name
177:49 - suggest uh it is the procedure to sort
177:52 - all the given data in some particular
177:54 - fashion it could be whether all the data
177:57 - you are trying to sort in an increasing
177:58 - order or reverse could be true that you
178:01 - are trying to sort everything in the
178:02 - decreasing order or you are trying to
178:05 - sort everything based on the colors that
178:07 - you are provided or the names or
178:08 - alphabets in either case you are trying
178:11 - to bring an ordered list and sorting can
178:15 - apply to most of the data uh most of the
178:18 - data structures like arrays and uh link
178:20 - list and trees and graphs but mostly you
178:22 - are going to typically use it on the
178:25 - arrays that's what I have observed by
178:27 - solving bunch of different problems so
178:29 - even to understand the Sorting let's try
178:32 - to see some examples from array suppose
178:34 - we are given an unsorted array and the
178:37 - values currently are 2 3 1 7 and 8 okay
178:42 - now in this unsorted array if we trying
178:44 - to sort it how can we do it well
178:47 - actually sort there are multiple
178:49 - techniques we can use in order to sort
178:51 - the data and one of the in inefficient
178:54 - approach is actually something called
178:56 - bubble sort now bubble sort is typically
178:59 - where you compare any two elements and
179:02 - try to see that whether those two
179:04 - elements are in correct order or not and
179:06 - if they're not you swap those two
179:08 - elements and then go to the next element
179:10 - and you keep on repeating this process
179:11 - until all the elements are sorted so so
179:14 - if we have to see that in example we can
179:16 - see something like this where we okay we
179:18 - check two and three they are already in
179:19 - the correct order so we will leave them
179:21 - for now then we will compare 3 to 1 So
179:24 - currently 3: 1 is not in correct order
179:26 - so we will bring one over here and three
179:28 - over here and leave 7 and 8 as it is now
179:31 - once again we would compare these two
179:33 - elements So currently one and two are
179:35 - not in correct order so we will add one
179:37 - over here two over here three over here
179:39 - and then seven and 8 once again we would
179:41 - repeat the same procedure so one and two
179:42 - are correct order two and three are
179:44 - correct order and three and seven are
179:45 - Cor in correct order and same same goes
179:48 - for seven and 8 so in this case all the
179:50 - elements are now in correct order which
179:52 - means we can return this as sorted array
179:55 - and uh we use the bubble sort method but
179:58 - the issue with the bubble sort method is
180:00 - that it takes big of n Square time
180:02 - because it could be possible that for
180:04 - every single element you need to keep on
180:06 - uh Shifting the places and that could be
180:09 - pretty bad so one better approach is
180:12 - actually something called uh M sort or
180:15 - there is also a quick sort so let me
180:17 - give you the better faster sorting
180:19 - approach once again let's assume that we
180:21 - are given some random array and we are
180:24 - trying to sort this now we only have
180:26 - four elements so a better approach is
180:29 - that we actually create an entirely new
180:31 - array and currently this new array is
180:34 - blank so what we would do is we would
180:36 - add all the values in the sorted Manner
180:39 - and whenever we have to identify that
180:41 - where any new value should go we would
180:43 - use binary search in order to find the
180:46 - correct location for that place to be
180:48 - entered and we all know that binary
180:50 - search operates in log and time so
180:52 - essentially our code should run in N log
180:55 - and time why because we will have to do
180:58 - the log and operation for every single
181:01 - element that is currently present inside
181:03 - our given input array So currently let's
181:05 - try to see for Value number one so
181:06 - currently this is empty so we will add
181:08 - one just anywhere now five for five we
181:12 - can we need to check that where five
181:13 - needs to added and five is greater than
181:15 - one so it has to be added on the right
181:17 - side of one so we will add five over
181:20 - here once again for Value number two uh
181:23 - let's see where it needs to be added now
181:24 - since 2 is greater than 1 it needs to be
181:27 - added on the right side of one but it
181:28 - needs to be added on the left side of
181:30 - five and this we can calculate quite
181:33 - easily using binary search that which
181:35 - should be the correct position for two
181:36 - to be entered so in this case we can
181:38 - enter two over here and then five we can
181:40 - shift on one position to the right and
181:43 - this whole operation would take
181:44 - logarithmic of end time once again for
181:47 - Value number eight we will repeat the
181:48 - same procedure and we can find that uh
181:51 - the ideal place for eight is going to be
181:53 - here on the very end and in the end we
181:55 - would get our sorted array now this
181:57 - sorted portion looks quite good and we
182:01 - were able to achieve this whole thing in
182:03 - N log n time which is quite nice so
182:07 - whenever you are trying to deal with uh
182:09 - any operations try to see that can you
182:12 - use searching and sorting to solve this
182:15 - problem and combination of these two
182:17 - would ultimately yield you better
182:19 - results because if you can do any
182:22 - operation on an unsorted array for bigo
182:25 - of n Square time then same operation can
182:28 - be done using n log and time uh if you
182:31 - just simply sort this that given input
182:33 - array so that is a huge Advantage now
182:36 - let's try to see one real example from a
182:39 - problems uh with from the lead
182:42 - code
182:44 - okay so now let's try to understand this
182:46 - problem that is how many numbers are
182:48 - smaller than the current number the
182:50 - problem is really easy to understand uh
182:52 - we are given an input array and we need
182:54 - to create another array where for every
182:57 - single position inside the original
182:59 - array we need to determine that how many
183:01 - number of elements are smaller than that
183:04 - particular value so in this case for
183:06 - four we can see that there are actually
183:08 - two elements that are smaller than four
183:10 - so in the answer we are going to mark
183:12 - two same way for three there is only one
183:15 - element that is smaller than three so we
183:16 - are going to mark one for five there are
183:18 - actually three elements smaller than
183:20 - five so we are going to mark three as
183:22 - the answer and zero is the smallest
183:24 - element so in this case we can Mark
183:26 - there are zero elements that are smaller
183:28 - than zero and this would be the answer
183:30 - we need to return now this is actually
183:33 - quite simple to understand let's see
183:34 - that what would be the Brute Force
183:36 - approach to solve this problem well in
183:38 - The Brute Force approach the most
183:40 - simplest thing we can do is that rather
183:42 - than uh doing anything else we can
183:44 - simply start checking one element one by
183:47 - one so we first make a pair from four to
183:49 - three and we check okay that this is
183:51 - smaller so we increment the value by one
183:54 - once again we make a pair of four to
183:56 - five We compare these two value since
183:57 - five is bigger we don't do anything then
183:59 - we check one more time and then since 0
184:01 - is less than four so once again we add
184:04 - one more increment and the answer we
184:06 - store in the answer that for zero there
184:08 - are two elements that are smaller than
184:10 - that and we keep on repeating the same
184:12 - process so this Pro Force approach would
184:14 - yield as the solution in bigo of n
184:16 - Square time because for every single n
184:18 - element we are doing n minus one work so
184:22 - this is not really good so let's try to
184:24 - see that what should be the better
184:26 - approach and one of a very easy
184:29 - approaches that actually if we take the
184:32 - same input as it is but we if we sort
184:35 - that input then the answer is going to
184:37 - become quite easy so the answer is 4 3 5
184:40 - and 0 now in this case we need to return
184:42 - that how many many number are smaller
184:44 - than that so first let's see that what
184:47 - does a sorted array looks like and in
184:50 - this case it's going to be0 3 4 and 5
184:53 - now at any given position in the sorted
184:56 - array defines that how many values are
184:59 - actually smaller than that so in the
185:01 - case of zero there are actually zero
185:02 - values smaller than that same way for
185:05 - the case of three there is one value
185:07 - smaller than that and that value is zero
185:09 - for four there are two values smaller
185:11 - than that and that is two and for five
185:12 - all three values are zero so we can use
185:16 - this property to calculate that uh or to
185:19 - generate this array but how we need to
185:22 - do that uh we actually need to know the
185:24 - index location for all of these and then
185:27 - Mark appropriate value so for four we
185:30 - need to enter answer two but we there is
185:34 - one simple way to do it that is that uh
185:36 - for four we first iterate over this
185:38 - given sorted are and find the value and
185:40 - then again repeat the same work but that
185:42 - would be an additional overhead a better
185:44 - approach is to use a hashmap here so
185:47 - after we generate the sorted array we
185:49 - can put all of its values as keys and
185:53 - all the index positions as values inside
185:56 - the hashmap and then uh we can use that
186:00 - property to to populate our answer quite
186:03 - easily so in this case 0 is going to
186:05 - have zero value three as key is going to
186:07 - have value number one four S key is
186:10 - going to have value number two and five
186:12 - s key is going to value number three now
186:15 - all we need to do is from our example
186:17 - array let me get rid of this for now and
186:20 - now all we need to do is from our
186:22 - example array we will first iterate over
186:24 - this value number four try to see that
186:27 - which is the value associated with this
186:29 - value number four create an answer array
186:31 - and in the answer array just Mark that
186:33 - value same way for Value number three
186:36 - again check in the hash map and the
186:37 - associated value is value number one
186:40 - again for five Associated value is value
186:42 - number three and for 0 Associated value
186:44 - is zero and this would give us the
186:46 - correct answer we are looking for in the
186:48 - simplest manner so now let's see that
186:50 - what is going to be the time and space
186:52 - complexity in this case the time
186:54 - complexity is going to be big of n log n
186:57 - in order to generate the sorted
186:59 - algorithm or sorted array plus we have
187:01 - to do big of n work in order to generate
187:04 - this answer from this example by
187:05 - comparing this hashmap so that is pretty
187:08 - good so overall time complexity is going
187:10 - to be big of n log n now let's see that
187:13 - what is going to be the space complexity
187:15 - well of course since we are using an
187:17 - additional hashmap the space complexity
187:19 - is going to be bigger of n but that is
187:21 - still reasonable because we are saving
187:23 - uh quite some computation in the time
187:26 - complexity so now you must understand
187:28 - that how we are actually using sorting
187:31 - and then we are using searching plus we
187:34 - are using the capabilities of hashmap
187:36 - and everything is connected with arrays
187:39 - in order to generate one answer and this
187:42 - is is how typically data structure and
187:45 - algorithm related videos or uh problems
187:48 - are done so now let's see the coding
187:51 - solution okay so let's see that what is
187:53 - going to be the Java solution for this
187:55 - problem basically we are given an
187:57 - integer array called nums so first thing
188:00 - we are going to do is we are going to
188:01 - create another array and we are going to
188:03 - uh sort it so this is the sorted array
188:07 - and now we are going to create a hash
188:09 - map uh where inside this hash map we are
188:13 - going to Mark the indexes values for all
188:16 - the elements that we have in the sorted
188:18 - array so it becomes really easy for us
188:20 - to navigate and generate the results for
188:23 - our original array then all we need to
188:25 - do is initialize another array where we
188:27 - are going to store the results plus we
188:29 - are going to run a for loop on the
188:31 - original input array and we are going to
188:34 - uh check that what is the result from
188:37 - the hashmap for that particular uh key
188:39 - value and uh we populate the values
188:42 - inside our result array and we get the
188:44 - answer let's try to run this
188:46 - code and our code runs pretty
188:49 - efficiently so let's submit this
188:51 - code and our code runs actually very
188:54 - fast compared to lot of other Solutions
188:56 - so this how you can understand what are
188:59 - the powers of sorting storing and
189:01 - hashing and combine combining lot of
189:03 - data structures and to generate a
189:10 - solution now we will try to understand a
189:12 - very important topic that is called
189:14 - recursion recursion is highly used in
189:17 - many of the data structure and algorithm
189:19 - related problems plus there are many
189:22 - algorithms and many programming
189:23 - techniques that are heavily dependent on
189:26 - recursion to solve the problem so
189:29 - recursion is actually a method that we
189:31 - can typically use uh in order to
189:33 - complete any set of task where we
189:36 - usually see some sort of repeative work
189:39 - that needs to be done for a various set
189:42 - of uh different inputs and essentially
189:44 - we need to collect the results of the
189:46 - them to generate an answer now let me
189:49 - explain you recursion by a real life
189:51 - scenario first and then we will talk
189:53 - about what it is it and how do we
189:55 - actually use it in our typical technical
189:57 - interviews say for an example you are
189:59 - currently standing on a line and you
190:02 - don't know that how many number of
190:03 - people are currently in front of you so
190:06 - all you can see is that there are bunch
190:07 - of different people ahead of you but you
190:09 - have no idea and currently this is the
190:12 - person who is the very first person in
190:14 - the line now you decide that you want to
190:16 - check that what is your position inside
190:19 - the current line so the one approach is
190:22 - that you get out of the line and then
190:25 - you start counting all the people that
190:27 - are in between and then you try to come
190:29 - back to the line but there are more
190:31 - people behind you so if you get out you
190:33 - will essentially lose your place so
190:35 - there is one more way when you can
190:37 - identify that what is your current
190:39 - position that is that first you ask the
190:42 - person ahead of you that hey what is
190:45 - your position now again the person ahead
190:47 - of you also has no idea so he repeats
190:50 - the same position that hey what is your
190:52 - position and this question keeps going
190:55 - on and on and on until we reach to the
190:57 - very first person and because this very
191:00 - first person already knows what is his
191:03 - or her position is this person would say
191:06 - that hey my position is one which means
191:09 - this person would understand that his
191:11 - position has to be second second
191:13 - position so then he would respond back
191:15 - to the person behind him and this person
191:18 - would understand that his position is
191:19 - now the third position same with this
191:21 - person will conclude that his position
191:23 - must be fourth position because this
191:25 - person is at third position and same
191:27 - with this person is at fifth position
191:29 - and he tells you that hey I am on the
191:32 - fifth position which means you can
191:34 - conclude that your position has to be
191:36 - the sixth position in the current line
191:38 - now what you essentially did is you made
191:41 - a recursive call and you get the desired
191:44 - answer you were looking for and you ask
191:47 - the same question to a different set of
191:49 - input results until you f reached to a
191:52 - place who knew the answer and depending
191:54 - on that answer you started building all
191:57 - the other answers and that's it that's
192:00 - the recursion now I know at first it
192:03 - seems complicated to comprehend but if
192:05 - you try to understand with real life
192:07 - examples things would become much more
192:10 - easier let me give you one analogy
192:13 - that how it typically appears have you
192:15 - ever seen those Russian dolls that uh
192:19 - whenever you open one doll and then uh
192:22 - inside that doll there is another doll
192:24 - hanging and inside that there is another
192:26 - doll and it keeps on happening over and
192:28 - over for all the different shapes of
192:30 - dolls so the same concept applies for
192:33 - the recursion as well that behind any
192:36 - big problem or behind solution of any
192:39 - big problem lies the solution of a
192:41 - smaller problem plus plus some
192:43 - computation same way for even for that
192:45 - smaller problem there lies some other
192:47 - computation and same goes on and on
192:50 - until we reach to a base case where in
192:53 - the base case we directly know that what
192:55 - is going to be the solution for that
192:56 - particular case and apart from that from
192:59 - that we would build this solution and
193:01 - then we would build the other solution
193:03 - and we would keep on repeating the
193:04 - process again and again and again let's
193:07 - try to see that what are the things or
193:09 - areas of concern for us in any
193:11 - particular recurs
193:13 - so every single recursion has two items
193:16 - first item is that uh recursive function
193:19 - and this recursive the purpose of this
193:21 - recursive function is to call the next
193:24 - sequence or next input in line in order
193:27 - to get the answer so first one is a
193:30 - recursive function and second one is a
193:33 - base case so base case is a scenario
193:36 - where we know that this has to be the
193:38 - answer or this has to be the minimum
193:40 - answer now uh again going back to to our
193:43 - uh people standing in a q position the
193:46 - person standing very at the very first
193:48 - po position in the line knew that he was
193:52 - the first person in the line and this
193:54 - was basically our base case now our what
193:58 - was our recursive function in this case
193:59 - in our recursive function the case was
194:02 - to ask the next person that hey what is
194:05 - your position and keep on asking that to
194:08 - the next person and the moment we get an
194:10 - answer from the base case we need to
194:12 - return to the previous person that hey
194:15 - whatever the answer we get from the next
194:17 - element or the base case we need to add
194:20 - one to that so here base case would say
194:23 - that hey I'm first person so this person
194:25 - would say back to the back to his
194:27 - previous person so person who called
194:29 - originally this person he would respond
194:31 - back saying that hey I second I'm in
194:33 - second position same way this person
194:35 - would say I'm third position and so on
194:38 - and so forth the recursive call would
194:40 - keep on coming back until we reached to
194:42 - a point who originally called or
194:45 - initiated that recursive call recursions
194:47 - are very powerful and they are used in
194:49 - bunch of different data structures in
194:51 - many programming techniques data
194:53 - structures such as uh typical tree or
194:57 - graphs plus programming techniques like
194:59 - dynamic programming and also many of the
195:02 - Matrix manipulation all of this are
195:05 - heavily using recursion to solve their
195:08 - uh problem because typically in all of
195:10 - these problems we need to do the same
195:13 - computation again and again for
195:16 - different set of inputs and whenever we
195:18 - see things like that we have to try to
195:20 - see that can we solve this problem using
195:22 - recursion now there is also one more
195:24 - thing that you need to understand that
195:26 - any problem you can solve recursively
195:29 - you can also say solve the same problem
195:31 - iterative iteratively but the question
195:33 - is recursive solution is going to be
195:35 - typically very simple to implement so
195:38 - you can think of it like a simple manner
195:40 - just like asking a question again going
195:42 - back to our people standing in the line
195:44 - scenario you could have gone out of the
195:46 - line and calculated all of these values
195:49 - one by one by yourself but why did you
195:52 - decide it not to do and you just decided
195:53 - to ask to the next person because
195:55 - repetitively you can ask this question
195:57 - again and again and you can expect to
195:59 - get an answer coming back in return so
196:03 - that is how if you would have gone out
196:05 - and calculated all of this by yourself
196:07 - it would have been an iterative approach
196:09 - and if you would have stood in the line
196:10 - and waited for the result come back to
196:12 - you it would be recursive approach but
196:14 - there is a key difference between
196:16 - iterative and recursive that in the
196:18 - iterative approach memory is not being
196:21 - used that much why because in this case
196:23 - you are the only person calculating that
196:25 - how many number of people are there
196:27 - currently present in the line and then
196:29 - they can you can infer the result that
196:31 - you're looking for but in our recursive
196:33 - approach we are dependent on every
196:35 - single person to know that what is the
196:38 - position of other person and then adding
196:41 - one line to it and sending back the
196:43 - results which means we are actually
196:45 - creating a memory stack where we are
196:47 - keeping track of all the memories that
196:50 - actually called us back so all the
196:53 - places where we called we are placing
196:56 - their uh line in the stack and when we
196:59 - get the result we fetch these values out
197:01 - one by one because remember the property
197:03 - of a stack is last and first out so we
197:05 - get these values out and then we in the
197:08 - end conclude the answer so whenever
197:10 - you're dealing with recursion make sure
197:12 - that uh memory usage is not so
197:15 - significant that you actually end up in
197:18 - uh stack Overflow issue now let's try to
197:22 - understand a program programming uh
197:25 - example for recursion and that is
197:27 - something called a Fibonacci series now
197:29 - we all know what a Fibonacci series is
197:32 - Fibonacci series is basically the sum of
197:34 - two numbers now let me give an example
197:37 - initially so let's say that first value
197:40 - of Fibonacci series is Fibonacci of zero
197:42 - so that is going to be the value number
197:44 - zero second is Fibonacci of 1 Fibonacci
197:47 - of one is going to be value number one
197:49 - now if we have to calculate Fibonacci of
197:51 - 2 we will actually have to do the sum of
197:53 - these two values and that will give us
197:55 - the answer of fibon 2 so Fibonacci of 2
197:58 - is actually Fibonacci of 0 plus
198:00 - Fibonacci of 1 so the answer is going to
198:02 - be 1 once again Fibonacci of 3 is
198:05 - actually the sum of previous two
198:07 - elements so Fibonacci of 2 plus fibon
198:09 - Fibonacci of 1 so the answer answer is
198:12 - once again going to be value number two
198:14 - same way Fibonacci of 4 is sum of
198:16 - previous two elements so Fibonacci of 3
198:18 - plus Fibonacci of 2 and the answer is
198:21 - going to be value number three and this
198:23 - keeps on going on and on until whatever
198:25 - the value we are trying to find we get
198:27 - the answer so now in this case this is a
198:30 - very good candidate if you want to
198:32 - implement the solution recursively and
198:34 - how would the recursive function would
198:36 - work well we know that for recursion we
198:39 - need two things we need our base case
198:41 - and base case is lying right in front of
198:43 - our eyes that is that uh the Fibonacci
198:47 - of 0o is zero and Fibonacci of 1 is 1
198:50 - now we need a recursive function and
198:52 - recursive function is also in place that
198:55 - fibon of any number is actually
198:58 - Fibonacci of number minus one plus
199:01 - Fibonacci of number minus 2 and you can
199:03 - apply it to anywhere so Fibonacci of 2
199:06 - is going to be Fibonacci of Z or
199:07 - Fibonacci of 1 same way Fibonacci of 3
199:10 - is going to be sum of Fibonacci of 2 and
199:12 - ion of 1 so this function applies which
199:14 - means we got our recursive function and
199:17 - we also got our base case so now to
199:19 - calculate any result is going to be very
199:21 - easy for us okay so let's say that I'm
199:23 - trying to find the Fibonacci numbers and
199:25 - I try to find the Fibonacci number of
199:28 - Fibonacci value number four so first
199:30 - let's see that what is going to be the
199:31 - recursive approach well first of all in
199:33 - the recursive approach we know that what
199:34 - is the recursive function for Fibonacci
199:36 - of 4 recursive function is Fibonacci of
199:39 - 3 plus Fibonacci of 2 now we all know
199:43 - that what is the answer for Fibonacci of
199:46 - 3 that is Fibonacci of 2 plus Fibonacci
199:50 - of 1 now we know the value of Fibonacci
199:52 - of 1 but we do not know the value of
199:54 - Fibonacci of 2 so once again even in
199:57 - order to get this value we are going to
199:58 - do Fibonacci of 1 plus Fibonacci of 0
200:02 - and we know these two values so sum of
200:05 - these two values is going to be 1 + 0 so
200:08 - the value is going to be 1 so this is
200:10 - going to return us the answer
200:12 - as one so we get the value of one so in
200:16 - this case currently the Fibonacci of 2
200:18 - is going to be of value number one and
200:20 - Fibonacci of 1 is already value number
200:22 - one so 1 + 1 is going to yield us the
200:24 - result of three now we got the answer
200:27 - three in this case now for Fibonacci of
200:29 - two we again need to do the same
200:31 - computation and that is Fibonacci of 1
200:34 - plus Fibonacci of 0o now we don't know
200:37 - we already know the values of fibon 1
200:39 - and Fibonacci of zero and that is going
200:41 - to be some is equal to 1 so this is also
200:43 - going to be one so in this case
200:45 - currently the value of Fibonacci of
200:47 - three is going to be okay this is not
200:49 - going to be three this is going to be
200:50 - two I think I Mis wrote this this is
200:53 - going to be two so Fibonacci of three is
200:55 - going to be two and Fibonacci of 2 is
200:56 - going to be 1 so the answer for
200:58 - Fibonacci of 4 is going to be three okay
201:02 - so we got the recursive answer now you
201:04 - see what you did what we did in this
201:06 - case we took the larger case and then we
201:09 - started breaking down into the smaller
201:10 - pieces and even for those smaller pieces
201:13 - we started breaking down into sub
201:15 - smaller pieces until we reach to a point
201:18 - where we are at the base case and
201:20 - through the base case we found the
201:21 - solution we return those solution to our
201:24 - previous calls and then we kept on
201:26 - repeating the process until we find to
201:28 - the main value we were looking for now
201:30 - what are the things we had to take care
201:31 - of we had to take care of base case we
201:34 - had to take care of the recursive
201:35 - function and we had to take care of the
201:37 - memory call back because this function
201:40 - is calling back to Fibonacci of 2 this
201:42 - function is calling back to Fibonacci of
201:43 - 3 same way this function is calling back
201:45 - to Fibonacci of 2 and we are getting the
201:47 - answer but at least as long as we are
201:49 - getting the answer now for the same
201:51 - problem let's try to do it iteratively
201:53 - so in the iteratively we are trying to
201:55 - find Fibonacci of 4 so obviously The
201:57 - Logical approaches that we are going to
201:59 - start with Fibonacci of 0 and Fibonacci
202:01 - of 0 is already Zer Fibonacci of 1 is
202:03 - already 1 now we are going to calculate
202:05 - Fibonacci of 2 that is going to be sum
202:07 - of 0 + 1 so that is going to be 1 okay
202:09 - so this Fibonacci of 3 is going to be
202:11 - value two and then we can calculate
202:14 - Fibonacci of four that is going to be
202:16 - value three and we will also get the
202:17 - same answer now in this case in the
202:19 - iterative approach we started from the
202:21 - bottom and reach all the way to the
202:23 - value we were looking for in the
202:25 - recursive approach we started from the
202:26 - value we were looking for started asking
202:29 - and went to the base case and then came
202:31 - back again for the previous case so this
202:33 - is one common way to implement recursion
202:36 - now you are going to see quite a lot of
202:38 - examples whenever you are trying to
202:40 - prepare for any technical interviews
202:42 - because recursion are one of the most
202:44 - commonly widely used programming
202:49 - techniques now we are going to learn the
202:51 - most important topic inside the Entire
202:54 - Computer Science in my opinion and
202:55 - specifically for the technical
202:57 - interviews because if you are applying
202:59 - for any Tech interview for sure you are
203:02 - going to be asked some questions related
203:04 - to dynamic programming so let's first
203:07 - understand that what dynamic programming
203:08 - is what are the different variations
203:11 - associated with that and then we are
203:13 - going to explore bunch of different
203:14 - examples so you can get the complete
203:16 - idea on how dynamic programming Works
203:19 - basically dynamic programming is nothing
203:22 - but the art of saving the data that we
203:25 - have already computed and then reuse
203:28 - that data for some future computation
203:30 - that's it uh now let's try to understand
203:33 - this with an example suppose we have a
203:36 - map like this where we are given bunch
203:39 - of different places or points and we we
203:41 - we are trying to go from one city to
203:43 - another city now in this case currently
203:46 - we have all of these variations and we
203:49 - are trying to get to a city number F now
203:53 - uh each of this path we can assume that
203:56 - they are let's say 1 km long on top of
203:59 - this from a we have some path directly
204:01 - leading to seat that is also 1 km long
204:04 - and from C we have one path that that is
204:06 - directly leading to e and that is also 1
204:08 - km long and now the aim is to get to
204:10 - City number f
204:12 - Now The Logical conclusion is that most
204:15 - common bro Force approach would be that
204:17 - we start from City number a and we
204:19 - explore every single path and for each
204:21 - of the path we calculated that how much
204:24 - kilometers or how many kilometers did it
204:26 - took so we would first calculate a
204:28 - result for a path like this then we
204:30 - would calculate a result for a path like
204:32 - this this this and this and then in the
204:34 - end we would calculate the result for a
204:36 - path like this and we would find the
204:38 - answer but the question is uh is this
204:40 - the most approp rate approach to reach
204:43 - over here because we are repeating so
204:45 - many competitions here we already
204:47 - calculated for this portion and this
204:49 - portion and we are actually calculating
204:50 - it two times uh in order to generate the
204:52 - result one better approach is that we
204:55 - actually use dynamic programming to
204:57 - store the already calculated path and
205:00 - how we are going to do it typically in
205:02 - dynamic programming there are two
205:04 - approaches first approach is a bottom up
205:06 - approach where you start from the base
205:09 - case or the very first case and try to
205:11 - build a solution that would eventually
205:13 - lead us to the final result we are
205:15 - trying to get to so that is one way
205:17 - second approach is a top down approach
205:19 - where in the top down approach you start
205:22 - at the very end or at the very top and
205:25 - then you build smaller and smaller
205:28 - Solutions and try to come up or try to
205:30 - reach to the first point and then try to
205:32 - build a path in this manner now of
205:34 - course both of this are going to look
205:36 - quite similar uh compared in regards to
205:40 - the previous example we C for recursion
205:43 - because the top down approach is
205:46 - actually used by done by doing recurs
205:49 - recursive calculation and bottom up
205:51 - approach is typically done by using an
205:54 - iterative solution where we are storing
205:56 - the data of all the things calculated
205:58 - and that is a common factor we are going
206:00 - to store the result of already
206:03 - calculating calculated the data and we
206:05 - are going to use it for our next
206:07 - computation so let's try to see an
206:09 - example in this case the approach I am
206:12 - suggesting is that we start from F in
206:15 - this case and we try to go in the
206:18 - reverse order to find the best path
206:21 - suited path so the question is in order
206:24 - to get to the F what is the nearest
206:26 - point that leads us to Value number F
206:29 - and that is going to be value number e
206:31 - that say somehow if we get to Value
206:34 - number e we can reach to F in just 1 km
206:38 - so we can say that distance from E to F
206:40 - is going to be 1 one okay that's good
206:43 - now uh in order to get to e what are
206:46 - different options we have so first let's
206:47 - explore all of them so let's say that we
206:49 - are currently at position number D if we
206:51 - are at D we if we need to get to F we
206:55 - have this path where we are going to go
206:58 - from D to e and then e to F so in this
207:01 - case if you are if we are at D do we
207:04 - need to calculate this whole path from
207:06 - going to e and then again going from E
207:08 - to F no why because we already know that
207:11 - that once we get to e we can directly go
207:14 - to F with this effort so we are going to
207:17 - save our computation and we are only
207:19 - going to calculate that okay from D if I
207:22 - get to e using one e using one step then
207:26 - from E to F distance is already one so I
207:29 - can say that from d i if I take two
207:32 - jumps or if I go travel 2 kilm I can go
207:35 - to the value F that's awesome now once
207:38 - again let's repeat the same computation
207:40 - now we are at position position number c
207:42 - so from C how many options we have one
207:45 - option we have is that from C if we go
207:48 - to D we already know that from D to F is
207:51 - already 2 km so and from C to D it takes
207:54 - us 1 km so if we were to take this path
207:57 - from C to D then we don't care how many
207:59 - number of hops it has to take all we
208:01 - consider is that from C to D if we go
208:03 - there then within 3 km we would be able
208:06 - to reach to our final destination F so
208:08 - in this scenario we are not even going
208:10 - to bother checking for this path from E
208:12 - to F we are only consider the path
208:14 - between C to D and which is the true
208:17 - power of dynamic programming so now in
208:19 - this case we are actually we can say
208:22 - that from C if we take 3 km we would be
208:25 - able to get to D but at C we have one
208:28 - more option and that option is that if
208:30 - somehow we can go from C to e then at
208:33 - location e we can only travel 1 km and
208:36 - get to F so in this case now at C we
208:40 - have a path from C to D that takes us uh
208:43 - 3 km time and we have a path from C to e
208:47 - that takes us 2 km time and we are
208:49 - trying to find the minimum uh time it
208:51 - takes or minimum travel it takes to get
208:53 - to e so we can see that at C if we
208:57 - somehow end up at position number c then
209:00 - we can actually reach to F within 2 km
209:03 - so we are going to Mark the value of C
209:05 - S2 and then we are going to repeat the
209:07 - same process so now from B if we somehow
209:10 - end up at C
209:11 - we would be able to get to F within 2 km
209:13 - and from B to C it takes 1 km to get
209:16 - there so we can see that at B it takes 3
209:18 - km to get to F same way from a if we go
209:22 - to B it takes us 3 km plus 1 km so 4 km
209:25 - to get there but if we from a go to C
209:30 - then it only takes 3 km so we can say
209:33 - that the minimum amount of time it takes
209:35 - from us to go from a to F would be 3 km
209:39 - and this is how we build the solution by
209:42 - already calculating the previously
209:45 - computed results and in this case we
209:47 - actually use a top down approach where
209:50 - we started from the top and we kept
209:52 - going in the reverse order and we can
209:54 - actually have a recursive function call
209:56 - to find the solution in the recursive
209:58 - function the base case is going to be
210:00 - that if we are located at position
210:02 - number e it only takes us 1 km to get
210:05 - there and then uh the solution would be
210:07 - that we need to check that how can we
210:09 - get to that next location in the minimum
210:11 - amount of hop so that's pretty awesome
210:14 - actually now let's see some other
210:16 - examples on how would a dynamic
210:19 - programming would work now previous
210:21 - example we saw that how does a recursive
210:23 - function work for a Fibonacci numbers
210:26 - now once again we are going to use the
210:28 - same Fibonacci numbers but this time we
210:30 - are going to use dynamic programming to
210:33 - solve the problem so let's see that in
210:35 - action in the previously recursive
210:37 - operation we had the example that if we
210:40 - wanted to calculate the the result of
210:41 - Fibonacci number five then what we would
210:43 - do first we would calculate the number
210:45 - of four plus uh Fibonacci number three
210:49 - same way for four we would calculate the
210:51 - number phys Fibonacci of 3 plus
210:54 - Fibonacci of 2 and plus over here we
210:56 - would do Fibonacci of 2 plus Fibonacci
210:59 - of 1 same way over here we would do
211:01 - Fibonacci of 1 plus Fibonacci of 2 and
211:06 - here we would do Fibonacci of 1 plus
211:10 - Fibonacci of 0 in order to calculate
211:12 - this value and in this case we will do
211:16 - Fibonacci of 1 plus Fibonacci of 0 in
211:20 - order to calculate this Fibonacci of 2
211:22 - and Fibonacci of 1 we already know the
211:24 - value as 1 because this is a base case
211:27 - now see how many times we are repeating
211:30 - the same computation over here we are
211:33 - calculating the value of Fibonacci of 1
211:35 - plus fibon of 2 in order to generate the
211:37 - answer same computation we are doing
211:39 - here as well Plus here we are
211:41 - calculating Fibonacci of 1 plus
211:43 - Fibonacci of 0 same way we are doing it
211:45 - for Fibonacci of 1 and Fibonacci of 0 so
211:48 - in both the cases rather than doing all
211:50 - of this computation what we can do is we
211:52 - can actually create an array and inside
211:55 - the array we are going to store the
211:57 - results that we have already calculated
211:59 - now we know that for our base case it's
212:01 - fibon of Z so let me mark down the value
212:03 - 0 1 2 3 and four for all of this and let
212:07 - me make one node for five as well okay
212:10 - now we all know that the values for 0 is
212:12 - actually going to be 0er and 1 is going
212:14 - to be one these two values are already
212:16 - calculated now let's start Cal start
212:19 - let's start our recursive function so
212:21 - first we start with four four and three
212:24 - now in order to do that we need to
212:25 - calculate these two values now we need
212:28 - to calculate this two value so this
212:29 - would be the first case we would be able
212:31 - to calculate where we need to calculate
212:33 - the value for Fibonacci of 2 so we
212:35 - already know what is Fibonacci of 1 and
212:37 - Fibonacci of 0o and this is going to be
212:39 - value number one so we can do that that
212:42 - we can put Fibonacci of 2 as value
212:43 - number one okay now for Fibonacci of 2
212:46 - we need to do Fibonacci of 2 plus
212:47 - Fibonacci of 1 that is going to give us
212:50 - the result for Fibonacci of 3 and that
212:52 - would be value number two okay now we
212:54 - are done with this portion and this
212:56 - whole computation now let's do
212:58 - computation for Fibonacci of 4 so in
213:01 - order to calculate Fibonacci of 4 we
213:03 - actually had to do all of these three
213:05 - calculations but in this case we don't
213:07 - have to do it why because at Fibonacci
213:10 - of 4 we need to get the answer of
213:11 - Fibonacci of 3 which we have already
213:13 - computed and Fibonacci of 2 which we
213:16 - have already computed as well so we can
213:18 - just say that this value is going to be
213:19 - three and based on that the value of
213:21 - Fibonacci of five is going to be some of
213:23 - these two values and we can just mark it
213:25 - as five so see how storing all of these
213:29 - values actually saved us lot of time and
213:32 - imagine that if we were not using
213:34 - dynamic programming so if we are not
213:37 - using dynamic programming then the time
213:38 - complexity would be big go of 2 to the
213:41 - power n because at every single position
213:44 - we are doing two more computation and
213:46 - same way for every single computation we
213:48 - are doing two more another computation
213:51 - so that is a very bad time complexity
213:53 - meanwhile in this approach we can
213:55 - actually complete this whole transaction
213:57 - and B go of uh and time only that we are
214:01 - only traversing and we are only doing
214:03 - the all the entire transformation just
214:05 - once and generating the result so that
214:07 - is pretty quick and pretty efficient
214:10 - approach and this I showed you the
214:12 - example of top down approach now let me
214:14 - show you the same for the same example
214:16 - bottom up approach now in the bottom up
214:18 - approach once again let me just draw an
214:21 - array with all the values I have
214:22 - currently calculated so 0 1 2 3 and 4
214:26 - okay and uh the values are going to be 0
214:29 - and 1 these are my base cases now I'm
214:31 - trying to calculate the value for
214:32 - Fibonacci of 2 so Fibonacci of 2 I can
214:35 - easy easily calculate using these two
214:36 - values as value number one and in the
214:39 - bottom up we are actually using the
214:40 - iterative solution so we are doing it
214:42 - iteratively okay now same way for the
214:45 - third value we can do some of these two
214:47 - value get the answer fourth value once
214:49 - again do the sum of these two values get
214:51 - the answer and same way for Fibonacci of
214:53 - five do the sum of these two values and
214:55 - get the answer as five and return this
214:57 - in the answer once again pretty
214:59 - awesomely and pretty simply we were able
215:02 - to solve the entire computation now you
215:05 - must be thinking that this is a very
215:06 - simple scenario for dynamic programming
215:08 - and yes this is a very simple scenario
215:10 - so let me give you a more complex task
215:14 - and that is actually uh that we need to
215:16 - calculate the number of coins problem
215:19 - okay so what does number of coins
215:21 - problem suggest is that we are given a
215:23 - value and let's say that this value is
215:26 - uh currently value number eight okay and
215:28 - we are given some coins so in this case
215:31 - let's assume that the coins we are
215:33 - currently given are 1 and two now we are
215:36 - told that we can use as many number of
215:38 - coins as we want in order to reach or
215:41 - build value number eight but the catch
215:44 - is that we need to use the minimum
215:46 - number of coins so in this case the
215:48 - answer is going to be if we use 4 * 2
215:50 - Coins we would get the value eight so
215:53 - the minimum number of coins we need to
215:54 - use in this case is going to be four but
215:56 - how do we actually calculate this so for
215:58 - this let me show you a bottom up
216:00 - approach and same would be true for the
216:03 - top down approach as well so I'm just
216:04 - showing it to you from the bottom of
216:06 - approach let me just quickly draw an
216:09 - array where we are going going to store
216:11 - the value for all the items we are
216:13 - calculating and initially our case is
216:15 - going to be zero that if we want to
216:17 - generate zero value how do we do that so
216:20 - let's say and for at each position we
216:24 - are going to calculate the possibility
216:25 - that what if we use coin with value one
216:29 - uh what how many number of coins would
216:30 - we use if we have to use coin number two
216:32 - how many number of coins would we use
216:34 - okay so both scenarios we are using and
216:37 - uh let's mark all of these values so 0 1
216:40 - 2 2 3 4 5 6 uh 7 and 8 okay now for zero
216:47 - we know that we need to use zero coins
216:49 - so let's just mark this as zero now if
216:51 - we have to generate value number one uh
216:53 - how many number of coins do we need
216:55 - currently previous results we can't get
216:57 - much information from because it is uh
216:59 - for the zero value now we have two coins
217:02 - available to us the coin if we use coin
217:05 - number one we can create one within just
217:07 - one coin okay so that is one option and
217:10 - and second option is if we use coin
217:12 - number two but that we cannot use to
217:13 - generate value one so we have to use one
217:16 - coin in order to generate value one and
217:18 - that one coin is going to be the simple
217:21 - coin one that we use I'm just marking
217:23 - white color as the the coin we use now
217:26 - same way in order to generate two what
217:28 - are the options we have we already know
217:30 - that currently let's say this coin is
217:32 - one coin number one so now our aim is to
217:35 - generate value number two but if we use
217:37 - one coin number one then we can already
217:41 - know that if we do 2 - 1 so whatever
217:45 - value we have stored on top of that if
217:47 - we just add one more coin over here that
217:50 - is this coin number one we would be able
217:52 - to generate this value too you will
217:55 - understand what I'm trying to say but
217:56 - let me just write it down for now so in
217:58 - this case if I use two coins with value
218:01 - number one then I can generate the value
218:04 - two so two would be minimum number of
218:06 - coins we have calculated so far but we
218:09 - also have the option for coin number two
218:11 - so if we take for check for coin number
218:14 - two two is actually the exact value so
218:16 - if we just use one single coin of value
218:20 - two we can generate the amount two so in
218:23 - this case the minimum number of coins we
218:25 - need to generate is going to be value
218:26 - number one and the coin we are using is
218:28 - going to be value number two okay now
218:31 - let's say that we need to generate value
218:32 - number three now once again the number
218:34 - of option we have for the coins is going
218:36 - to be 1 and two so let's redo the same
218:39 - calculation again now if we use coin
218:43 - number one which means if we are let's
218:46 - say somehow we are at this previous
218:48 - position two and then we use coin number
218:50 - one we would be able to generate value
218:52 - number three quite easily that's a given
218:54 - fact and we already know that in order
218:56 - to generate value number two we only
218:58 - need to use one coin and that is the
219:01 - true power of dynamic programming that
219:03 - we are using the value we have already
219:05 - calculated over here so in this case if
219:07 - we use coin number one
219:10 - plus the dynamic programming value of
219:13 - coin number two and how did we arrive at
219:16 - this coin number two we did a
219:17 - subtraction from the current coin we are
219:20 - trying to calculate that is this coin
219:22 - number three minus the coin value we are
219:25 - currently using that is value number one
219:27 - so we can make a dynamic programming
219:29 - function that would be that dynamic
219:31 - programming of let's say this one is
219:33 - currently value number three is equal to
219:36 - going to be the so first condition we
219:39 - have is that we can actually use dynamic
219:42 - programming of three minus coin that
219:45 - currently we are using and in this case
219:47 - coin is 1 so we can say dynamic
219:49 - programming of 3 minus 1 and if we use
219:52 - that then we will also have to use the
219:54 - current coin we have and that coin only
219:57 - takes one value one coin only adds one
219:59 - coin to the previous coins so this would
220:02 - be one consideration and second
220:04 - consideration would be the next option
220:07 - with value number two but currently in
220:09 - both the cases the number of coins are
220:11 - going to be two so I'm just marking
220:13 - coins to be two and you have two options
220:15 - over here you can either pick coin as
220:17 - one and two or you can pick coins as two
220:19 - and one in either case you will be
220:21 - picking two coins in order to generate
220:23 - value number three and we don't care
220:25 - about what coins we pick we only care
220:27 - that what are the minimum number of
220:29 - coins we need to generate that value now
220:31 - we are at position number four okay now
220:34 - let's again apply the same logic to our
220:36 - dynamic programming function and let's
220:38 - see that what are the options we have so
220:40 - So currently we are considering value to
220:42 - be one so coin we are choosing is one
220:44 - okay if we and we are trying to
220:46 - calculate for dynamic programming for
220:47 - Value number 4 so first option is that
220:50 - we do 4 Min - 1 so 4 - 1 is going to be
220:53 - dynamic programming of three dynamic
220:55 - programming of three is already know
220:56 - that the minimum value is 2 so that is
220:58 - going to be 2 + 1 so we have the option
221:02 - of choosing three coins in order to
221:05 - generate value number four but we have
221:06 - another option where we can use coin
221:08 - number two as well so let's try to do
221:10 - that so again the second option is that
221:13 - we do dynamic programming of 4 - 2 + 1
221:17 - because we are using the the coin with
221:19 - value two so in this case uh we know
221:22 - that we need to generate dynamic
221:24 - programming of two so dynamic
221:26 - programming of two's result we have
221:28 - already stored and that is one so second
221:30 - comparison is that 2 + 1 so these are
221:34 - the two comparisons we have and we need
221:36 - to find the minimum value amongst both
221:37 - of this so this would lead us the value
221:39 - of d dynamic programming of four or the
221:43 - number of coins needed to generate value
221:45 - four so first option is three and second
221:48 - option is actually uh three as
221:52 - well sorry second option is actually two
221:55 - because the value of dynamic programming
221:56 - of two is actually one so in this case
222:00 - we are going to mark this as two so
222:02 - among these two of course two is smaller
222:04 - so we are going to mark two over here
222:07 - and the coins we took to generate is
222:08 - going to be 2 and two same way for for
222:11 - Value number five what are the options
222:13 - we have we can if we subtract one value
222:16 - so then uh if we use coin number one
222:18 - then we can generate this in three coins
222:20 - if we subtract or if we use coin number
222:22 - two then 5 - 3 is going to be 2 so 2 + 1
222:25 - is also going to be three so we this
222:27 - three is going to remain constant here
222:29 - same way in order to generate value
222:31 - number six if we use coin number one
222:34 - then we can use 3 + 1 so one option we
222:36 - have is four second option is that we
222:39 - can come up up to Value number four and
222:41 - then use one coin to reach to six and
222:43 - the value over here is two so we have
222:45 - two options to choose from that is four
222:47 - and three so we can actually choose the
222:48 - value number uh three in this case okay
222:51 - cool now after calculating this at 7 we
222:55 - again have two options we can either use
222:57 - coin number one or coin number two in
222:59 - either case the answer is going to be
223:01 - four so we can mark four over here now
223:03 - at this position number eight we
223:05 - actually we can actually check that if
223:07 - we use one coin like coin with value 1
223:10 - then we need to do dynamic programming
223:12 - of 7 + 1 so that gives us the result of
223:15 - five second option is if we use coin
223:18 - number two then we need to do dynamic
223:19 - programming of 2 + 1 so uh sorry not
223:23 - dynamic programming of two dynamic
223:24 - programming of 6+ 1 so in this case that
223:28 - is going to give us the result of four
223:29 - since we we are picking the minimum
223:31 - value we only need to pick the value S4
223:35 - and that's it we got the answer that it
223:39 - takes 4 minutes minimum coins to
223:41 - generate value number eight if the given
223:43 - coins are uh 1 and two so you see how
223:47 - beautifully we we were able to solve
223:49 - this problem in just one single go
223:52 - because we were calculating all the
223:53 - results we have calculated so far and
223:56 - that is the true power of dynamic
223:57 - programming in my
223:59 - opinion so now let's try to do a really
224:02 - popular lead code problem that has been
224:04 - asked in tons tons of interviews many
224:06 - times and you can see how popular It Is
224:08 - by number of people who have liked this
224:09 - video
224:10 - so basically uh you are you are being
224:13 - asked to climb a staircase now you it
224:16 - takes end steps to reach to the top you
224:18 - are given the value of n now we have the
224:20 - option of either climbing one step at a
224:22 - time or climbing two steps at a time now
224:25 - in either case we need to calculate that
224:27 - how many distinct ways can we climb to
224:29 - the top now for this problem I'm going
224:32 - to show you both top down approach and
224:34 - also bottom of approach and in the code
224:36 - we will see the solution for the bottom
224:38 - of approach okay that Mak makes sense so
224:40 - first let's try to understand the
224:42 - problem with a few simple statements so
224:44 - let's say that if you are currently
224:45 - standing at stair number zero how many
224:48 - steps does it takes of course zero
224:50 - that's a given fact let's say now you
224:52 - currently have one steps available and
224:55 - you are currently standing at position
224:57 - zero and you need to get to step number
224:58 - one how many distinct ways can you get
225:00 - there of course one distinct way because
225:02 - you can only take one step let's take
225:05 - one more example uh currently the number
225:08 - of steps given are two and you are
225:10 - standing at step number zero now how
225:12 - many distinct ways can you get to to
225:14 - Value number two once again you have the
225:16 - option to taking one step over here and
225:18 - then one step over here so that is one
225:20 - way and second way is you can take two
225:22 - steps directly to get to Value number
225:23 - two so there are two distinct ways you
225:25 - can get to Top Just One Last example
225:29 - that is for Value number three okay that
225:32 - let's say that you want to get to the
225:34 - step number three you are at zero and
225:36 - you have the option of 1 2 and three so
225:38 - now how many distinct ways can you go
225:40 - first distinct way is just taking one
225:42 - step each time so this would be way
225:44 - number one second distinct way is that
225:47 - we take two steps directly and then take
225:49 - one step so this would be second step
225:51 - and third step is that we take one step
225:54 - and then we take two steps directly so
225:57 - there are three distinct ways to climb
225:58 - to the top uh for Value number three but
226:01 - now if you notice some interesting
226:04 - property the interesting property is we
226:06 - already know that if we have just one
226:08 - step we need to climb there are only one
226:10 - ways we can do it if we want to climb
226:12 - two steps there are at Max Two Steps we
226:14 - can do it the moment we wanted to
226:17 - calculate that how many times what are
226:19 - the distinct ways we can climb to the
226:21 - top we don't even have to calculate all
226:24 - the values why because in order to reach
226:26 - to three there are only two
226:28 - possibilities first possibility is that
226:30 - we start from value number two and then
226:32 - take one step so that would be one way
226:35 - and second POS possibility would be that
226:37 - we start from value number one and then
226:40 - take two steps basically this would be
226:42 - second possibility now you must be
226:44 - asking that hey what if we take one step
226:46 - to get here still it is still going to
226:48 - be counted towards this distinct step so
226:51 - all we care about is that how many
226:54 - different ways can we get to this value
226:56 - and how many different ways can we get
226:58 - to this value some of these two would
227:00 - allow us to find the next value in our
227:03 - path and this is the important property
227:06 - for dynamic programming that we need so
227:08 - let's try to understand this with an
227:10 - example that in this case for in order
227:12 - to climb to step number three we have
227:14 - two options that we can climb from one
227:16 - is from Step number two and one is from
227:18 - Step number one so if we do sum of these
227:20 - two we can get the value 1 + 2 is equal
227:22 - to 3 now let's try to understand the
227:24 - same logic for five steps okay and now
227:28 - for five steps I'm actually drawing it
227:30 - on an array uh and starting from value
227:33 - zero we will go to all the way to five
227:35 - and see that how each iteration how many
227:37 - changes it takes for us to to get to the
227:40 - value number five okay now in this case
227:43 - uh currently we are using the bottom up
227:46 - approach we are starting from the bottom
227:47 - and then we are going up so initially
227:50 - this takes zero steps this takes one
227:52 - step and this takes two step that's a
227:53 - that's a given fact now in order to
227:55 - climb to step number three we can either
227:57 - take one jump from here or we can take
228:00 - one jump directly from here so we are
228:02 - going to do some of these two so the
228:03 - answer is going to be three once again
228:06 - from 3 to 4 it only has two
228:07 - possibilities one step from here or two
228:10 - steps directly from here so once again
228:12 - we are going to do some of these two the
228:14 - answer is going to be five now in order
228:16 - to reach to step number five we only
228:18 - need to do the sum of these two values
228:20 - and that is eight and eight are the
228:22 - distinct ways we can reach to Value
228:24 - number five see how easy it became in if
228:28 - we if you were to do this in the Brute
228:30 - Force manner you would actually have to
228:32 - first create all the different decision
228:34 - trees you can make starting from
228:36 - position number zero so zero now you
228:38 - have uh two options you can either take
228:40 - one jump and go to position number one
228:42 - or you can take two jumps and go to
228:44 - position number two once again you have
228:45 - two more possibilities you can go to
228:48 - position number two or position number
228:49 - three you can once again have two more
228:52 - possibilities you can go to position
228:54 - number three or position number four and
228:55 - once again you have two more
228:57 - possibilities you can go to position
228:58 - number five or position number four or
229:01 - five and here position number five and
229:03 - the moment you reach to the five you can
229:05 - consider that to be an end of the path
229:07 - and just over here you see that these
229:11 - are this is just one path you can take
229:14 - in order to reach to Val number five
229:15 - then this would be a second path this
229:17 - would be third path and so on and so
229:19 - forth if you keep on going on you would
229:20 - be able to find eight distinct paths and
229:23 - if you were to do it in the Brute Force
229:25 - manner without using the dynamic
229:27 - programming the time complexity would
229:29 - have been 2 to the power of n because at
229:31 - every single position we have two
229:33 - different options to choose from and
229:36 - this is a disastrous result so that is
229:38 - why you using dynamic programming was
229:40 - great now you understood that how we use
229:43 - bottom of approach let me show you the
229:44 - result for the top down approach as well
229:47 - where we are going to start from the
229:48 - very top element and we are going to
229:50 - come our way down to the very last
229:52 - element okay so currently the values are
229:54 - 5 4 3 2 1 and zero now we know that if
229:58 - we are at step number five how many
230:00 - distinct ways to get there of course
230:01 - zero because we are already at set step
230:04 - number five we don't have to calculate
230:05 - anything if we are at step number four
230:07 - there is only one distinct way to get to
230:09 - step number five so we are going to mark
230:11 - this as one if we are at step number
230:13 - three there are actually two ways to get
230:15 - to step number five we can take one step
230:18 - over here and then one step over here or
230:20 - we can take one step directly over here
230:22 - so in this case there are two options
230:24 - from now on we can apply the same logic
230:26 - in the reverse order because from Step
230:29 - number two we only have two options that
230:31 - we can choose from we can either go
230:33 - directly to step number three that would
230:35 - be one distinct way and second distinct
230:37 - way would be we can directly go to step
230:38 - number four from there on we already
230:40 - know that how many different ways it
230:42 - takes to get to step number five so we
230:44 - can just use that property so basically
230:46 - from Step number two we can go to three
230:49 - distinct ways and that came from some of
230:51 - these two values same way from value
230:54 - number one we can actually go to five
230:56 - distin distinct Ways by taking the sum
230:59 - of these two values because these are
231:01 - the only two possible jumps we can make
231:03 - and from value number zero we again have
231:06 - eight different ways that we can make
231:07 - this jump and this is our top down
231:10 - approach so see in both the sessions we
231:13 - got the correct answer we used dynamic
231:16 - programming to calculate the already
231:18 - calculated result and then we were able
231:20 - to come up with the appropriate and
231:22 - successful result in the end so now
231:24 - let's see the code for this and I hope
231:27 - you understood what dynamic Pro dynamic
231:29 - programming is and what are its key
231:31 - fundamental reasons so this is the
231:34 - problem statement and here is the Java
231:36 - code for that uh basically we are using
231:38 - bottom up op approach so first we are
231:40 - checking for the edge case that if the
231:41 - given n is equal to 1 we can simply
231:43 - return one if that is not the case we
231:45 - are going to initialize our dynamic
231:47 - programming array and we are going to
231:49 - check out or put down the base case
231:52 - values for uh number of stairs as one
231:54 - and number of stairs as two after that
231:56 - we are going to run a for Loop starting
231:58 - from the step number three all the way
232:00 - to the end and we are going to apply our
232:02 - dynamic programming logic which is quite
232:04 - simple to understand in the end we are
232:06 - simply going to return the dynamic
232:08 - programming value be found at the very
232:10 - end value now let's try to run this code
232:12 - okay seems like our solution is working
232:14 - let's submit this code and our code runs
232:17 - 100% faster than all the other Solutions
232:19 - which is pretty good to see and uh now
232:22 - you can understand that how simple it is
232:24 - to code the dynamic programming
232:28 - solution okay so now let's learn the
232:31 - sliding window technique this is a
232:33 - really popular coding technique and
232:35 - typically works really well with many of
232:37 - the array and string kind of problems
232:40 - now first let's try to understand that
232:42 - what does a sliding window technique
232:43 - means let me give you an example suppose
232:46 - I tell you that you have this rope and
232:49 - currently this rope the entire rope is
232:51 - actually in the color blue now I ask you
232:55 - that I want to change this color from
232:57 - Blue to Pink so how would you do that
232:59 - well you actually have a couple of
233:01 - options to do it number one and most
233:04 - naive approaches that you actually put
233:06 - down a table now on top of this table
233:09 - table you lay down your blue colored uh
233:12 - uh rope and then you start pouring on
233:16 - the pink color all across the table and
233:18 - then eventually entire table is now
233:20 - dripping up with p pink color and some
233:23 - of that paint has already stuck onto the
233:25 - Rope as well and now our rope is
233:27 - actually pink colored so this approach
233:29 - will give you the correct rope color
233:31 - that you want but in this process you
233:33 - would have wasted lot of paint as well
233:35 - so that is not a very good approach a
233:38 - better solution might be that suppose
233:40 - you are given this rope uh I ask you
233:43 - that hey paint this uh in the pink color
233:45 - you take a paint brush and now in this
233:48 - paint brush you apply the color pink for
233:51 - this portion and after applying whatever
233:53 - the current width of the paint brushes
233:56 - you take that width and try to paint
233:59 - that much area pink for now and once
234:02 - that is done you again take your
234:04 - paintbrush replenish it with the pink
234:06 - color and again repeat the same process
234:08 - for for the next set of uh remaining uh
234:12 - rope as well and eventually you would
234:14 - cover all the ground and eventually the
234:16 - whole rope would have been painted blue
234:18 - or sorry pink and that is the most
234:21 - simple example of what a sliding window
234:24 - is where in the longest input we had
234:28 - which was our rope we actually created a
234:31 - small subset of window that we applied
234:36 - on one particular portion of given in
234:38 - input once it achieved our con
234:41 - conclusion or the thing we wanted to
234:43 - compute then we took the same thing to
234:46 - the next area and then we took the same
234:48 - thing to the next area until we reach to
234:50 - the very end and we find the appropriate
234:53 - answer that we were looking for so this
234:56 - can be a very powerful tool in many
234:58 - examples and let's try to understand
235:01 - that with a simple example suppose we
235:02 - are given an input array a where we are
235:05 - given some arbitrary random values okay
235:07 - let's just Mark these values is 1 3 7 uh
235:11 - 2 5 8 and 9 okay these are the values we
235:14 - are currently given now I ask you that
235:17 - find me the maximum sum of three
235:21 - consecutive elements so what you what
235:24 - would be your approach in this case well
235:27 - one approach is that you actually start
235:30 - doing or making every single pair and
235:33 - try to come up with the solution so in
235:35 - that case you first take these three
235:37 - values do it sum then you take these
235:39 - three values do it sum then you take
235:41 - these three values do it sum and so on
235:43 - and so forth and eventually you will
235:44 - find the result or the second option is
235:47 - that you actually create a window of
235:50 - three characters and then keep on
235:52 - incrementing the sum uh and try to find
235:55 - that which has the maximum sum so let's
235:57 - try to use the sliding window technique
236:00 - in this one so suppose I create an
236:03 - element called Max sum that is to keep
236:05 - track of the maximum sum I'm also
236:07 - creating another variable called current
236:09 - sum and this is going to be our window
236:12 - of three elements that we are dealing
236:13 - with so first let's say that we take
236:15 - these three elements so now the sum is
236:18 - current sum is 11 and so far the maximum
236:20 - sum we have been able to find is also 11
236:23 - okay now we need to check or move our
236:25 - window on the on the side but the window
236:29 - size will remain same so once again we
236:32 - are creating a new window and that new
236:35 - window is going to be starting with
236:38 - value value three okay so let's create
236:41 - another window now the moment we we
236:44 - created new window do we need to
236:46 - calculate the sum of all of these three
236:49 - values well actually no we have a better
236:52 - approach and that better approach is
236:54 - that we can actually take the previous
236:56 - sum we already had that had current
236:59 - value what we did essentially in this
237:01 - one we subtracted one from this window
237:05 - that we got rid of this value and we
237:07 - added two in this case so we can simply
237:10 - do this computation now you must be
237:12 - wondering that this is only three
237:14 - elements so why are we even bothering
237:16 - doing this imagine if this was like 500
237:19 - elements and this was a very large area
237:21 - of million characters in that case this
237:24 - computation would save you so much time
237:26 - because every time you only have to
237:28 - subtract one element and add another
237:30 - element and that's it you will get the
237:33 - sum of all the consecutive values of
237:36 - those 500 values you don't have to to do
237:39 - much work and iterate over all 500
237:41 - values to find the result so in this
237:42 - case now this sum is going to be uh 12
237:45 - so 12 is definitely greater than 11 so
237:47 - we would update our Max variable that
237:50 - currently the maximum sum we have found
237:52 - is going to be 12 okay and we are also
237:54 - going to update our current sum as well
237:56 - to Value number 12 once again we got rid
237:58 - of three and we add value number five in
238:00 - this case so - 3 + 5 the answer is going
238:04 - to be 14 once again we update our
238:06 - maximum variable to Value number 14 and
238:09 - same way we update our current variable
238:11 - as well so 14 and 14 and currently we
238:13 - were looking at this uh pair now let's
238:16 - subtract seven and add value number
238:18 - eight so in this case the current sum is
238:21 - going to be 15 which is also the maximum
238:23 - sum we have found so far so we would
238:24 - update that once again let's get rid of
238:27 - value number two and add value number N9
238:29 - so in this case the sum is going to be
238:31 - 22 and that is the maximum sum and we
238:33 - can update that see how easy it became
238:36 - once we created a simple window and then
238:39 - we iterated or moved all the way over
238:41 - there so this is everything you need to
238:44 - understand about sliding window
238:46 - technique well sliding window is not
238:48 - very TP very difficult to comprehend and
238:50 - let me give you another example okay now
238:54 - this is one of the most popular problem
238:56 - on lead code and has been asked in tons
238:58 - of companies uh basically we are given a
239:01 - simple string s and we need to find the
239:03 - longest substring that does not have any
239:06 - repeating characters so if we see an
239:08 - example example over here in this case
239:10 - we have we are given a bunch of long
239:12 - string but we can see that first a b c
239:15 - is the longest string that does not have
239:16 - any repeating characters and its length
239:19 - is three so we are going to return three
239:20 - as the answer because apart from that
239:22 - all the other places actually has
239:24 - repeating character so the window is not
239:26 - large enough okay so now let's try to
239:28 - see that how would we solve this problem
239:30 - suppose we are given a problem like this
239:32 - that uh a b a c d a something like this
239:38 - this okay we are given this as the input
239:40 - and we are trying to find the longest
239:41 - substring without repeating characters
239:43 - so of course we are going to use sliding
239:45 - window in this on top of that we will
239:47 - also have to use some additional data
239:49 - structure to keep track of all the
239:51 - values we currently have in our existing
239:53 - window so for that uh hash map or hash
239:56 - set is actually great so we will use
239:58 - hash set because we don't need to have
240:00 - indexing in this place but we only need
240:02 - to know that what are the elements
240:04 - currently present inside our existing
240:06 - window okay so let me just quickly draw
240:09 - our hash set and now the approach is
240:11 - that we are going to have two pointers
240:13 - so first pointer is going to be left
240:15 - pointer second pointer is going to be
240:16 - right pointer initially we are going to
240:19 - keep moving right pointer until the
240:21 - point where we do not encounter any uh
240:25 - repeated characters and we would Mark
240:28 - its length uh as the maximum length we
240:31 - have been able to find so far the moment
240:33 - we encounter a repeated character we are
240:35 - going to take left pointer to move one
240:37 - step to the right right and then keep on
240:39 - repeating the same process and meanwhile
240:41 - we would only move right if the value is
240:43 - not currently present inside the hash
240:45 - set if it is present then we would move
240:47 - the left pointer and uh let's try to see
240:51 - the solution in action okay so first we
240:53 - are both are at the same position and we
240:56 - have our maximum uh substring length
240:59 - that is currently zero and hash set is
241:01 - empty okay so now let's see currently a
241:05 - is not in the hash set so we will add
241:06 - entry a over here and then move right
241:08 - pointer to the next element uh now after
241:12 - doing that now let's see a b are still
241:15 - distinct B is not added so we would add
241:17 - B over here as well and move on to the
241:19 - next element so next element currently
241:21 - is uh a again so because this is a once
241:25 - again okay what is the maximum window we
241:26 - have been able to find so far that was
241:28 - only two characters so we will Mark two
241:30 - two over here as the answer now because
241:33 - we find an identical value we are going
241:36 - to move our left pointer to the right
241:38 - the moment we move left pointer to the
241:40 - right whatever the value we had in the
241:42 - left pointer we are going to delete this
241:44 - so first let me get rid of this value
241:47 - okay now after getting rid of this value
241:50 - uh we we have our R located over here so
241:52 - we will have to add an entry a over here
241:55 - and now currently the window size is
241:57 - still two now let this R go to next
242:00 - element so it will go to next element c
242:02 - c is also unique now let this R go to
242:05 - next element d d is also unique so it
242:07 - can still still stay over here now the
242:10 - next element is once again a a we
242:13 - already have inside the existing hash
242:15 - set which means okay so far the maximum
242:17 - window we have been able to find is of
242:19 - size four so let's mark four over here
242:22 - and once again let's try to update the
242:24 - value so now we are going to move move
242:26 - our left pointer one step to the right
242:28 - okay after moving it one step to the
242:30 - right again remember we haven't added
242:32 - this a yet because it was a duplicated
242:34 - entry okay so we added we removed B from
242:37 - here so we got rid of B from our hash
242:40 - set once again this value is a and once
242:43 - again our right pointer is still stuck
242:44 - over here now this left pointer we got
242:46 - rid of a okay after getting rid of a now
242:51 - we can check that whether right pointer
242:52 - can be added over here or not yes now it
242:54 - can so we will add a over here and now
242:58 - uh we will try to move our right pointer
243:00 - but now it is the end of our list so
243:02 - this is only of size three this uh
243:06 - substring without repeated characters
243:08 - and maximum we have been able to find is
243:10 - four so four is going to be our answer
243:11 - and this is what we are going to return
243:12 - return so now you imagine how we had
243:15 - like a dynamically changing sliding
243:17 - window in this example to solve this
243:20 - problem and we actually use hash set and
243:22 - tandem to solve this problem now U this
243:25 - is quite a simple problem so I'm not
243:28 - going to show the coded solution but I
243:30 - do have the coded solution available on
243:32 - the GitHub link and I'm going to post it
243:35 - in the description so you would be able
243:36 - to check it out from there if you are
243:37 - curious ious now let's move on to the
243:39 - next topic that is called two
243:44 - pointers okay now let's try to
243:46 - understand one of the interesting uh
243:48 - coding pattern that is very similar to
243:50 - sliding window and that is called two
243:52 - pointers now this two pointers is
243:54 - actually really popular with data
243:56 - structures like array and strings and
243:59 - the idea is that just like a sliding
244:01 - window we are going to have two pointers
244:04 - but these two pointers are typically
244:06 - going to be located at the edges of Any
244:09 - Given array and depending on certain
244:11 - scenarios the pointers would come
244:13 - towards each other based on the various
244:16 - scenarios that we have available okay so
244:18 - this is the whole premise now if you
244:20 - want to understand this uh with a real
244:23 - life example you can think about that
244:25 - let's say that you are currently located
244:27 - on a Long Beach and inside this Beach uh
244:31 - you maybe lost something or uh some some
244:35 - of your maybe cousin or child or someone
244:38 - so the idea you are going to use is that
244:41 - you and your wife you both are going to
244:43 - go to different ends of the beach and
244:46 - you would try to come towards each other
244:48 - trying to find the child that whether
244:50 - that child is maybe located somewhere
244:52 - and try to consider that beach is just
244:54 - like a simple line so you are not moving
244:57 - in any other direction So eventually you
244:59 - would be able to find the child who is
245:01 - going to be located somewhere but
245:03 - essentially you and your wife you are
245:05 - going to be coming towards each other
245:07 - and the this is the whole concept of
245:09 - two- pointer solution so let's try to
245:11 - understand this with an example okay so
245:14 - let's try to solve this problem squares
245:16 - of a sorted array now this is a very
245:18 - popular problem and also a very simple
245:21 - explanation yet it explains that how we
245:24 - can use two pointers to our maximum
245:26 - Advantage uh basically we are given a
245:28 - nums array and we are told that this
245:29 - array is already sorted in an increasing
245:32 - order now we need to return a new array
245:36 - that contains square of every single
245:38 - element but we need to make sure that
245:40 - this is also in non decreasing or
245:42 - increasing order so assume that suppose
245:46 - we are given an input array like this 1
245:48 - 2 and four this array is currently in
245:50 - increasing order so in the answer we
245:52 - need to return the square that is also
245:54 - going to be 1 4 and 16 and this is the
245:57 - answer we need to return now looking at
245:59 - this example you must be thinking that
246:01 - hey this is quite simple all we need to
246:03 - do is just square of two elements and
246:05 - then we can simply return it as it is
246:08 - but it's not as as simple as it is
246:10 - because we could have an input that
246:12 - looks like this where the values could
246:14 - be Min - 8 - 1 0 and then 1 and then two
246:18 - suppose if this is the array given in
246:21 - this case the answer has to look
246:23 - something like this where the first
246:25 - value is going to be 0o because 0 square
246:27 - is 0 then 1 square is going to be 1 then
246:31 - once again - 1 square is also going to
246:33 - be 1 then 2 square is going to be 4 and
246:35 - then - 8 square is going to be 16 64 so
246:38 - see this value came over here this value
246:41 - came over here this value came over here
246:43 - so everything got jumbled up because we
246:46 - had some negative values and yet this
246:48 - satisfied the property of being in
246:50 - increasing order and this also became an
246:53 - increasing order so now how do we deal
246:55 - with negative numbers in this scenario
246:58 - so one root force or naive approaches
247:01 - that whatever input we are given suppose
247:03 - the values are -4 -2 1 and 3 suppose
247:07 - this is the input value we are given uh
247:09 - one simple approach is that we simply do
247:12 - a square as it is so values like 16 4 1
247:16 - and 9 and then we do the sort operation
247:19 - on this array so if we do that we will
247:21 - again get the correct answer that is 1 4
247:24 - 9 and 16 but this is going to be done in
247:27 - N log n time so we will try to see that
247:31 - can we do it faster in just log and uh B
247:35 - of end time rather than using log andun
247:37 - function and yes the answer is we can do
247:40 - it plus you already know because we are
247:42 - explaining two pointers for sure we are
247:44 - going to do it using two pointers the
247:46 - idea is we will have our left pointer
247:48 - located on the last location we will
247:50 - have a right pointer located on the
247:52 - rightmost position and then we are going
247:54 - to have our answer Square okay now what
247:58 - we are going to do is it is only
248:01 - possible that if this value is too
248:03 - negative it could either end up
248:06 - somewhere over here here or it could e
248:09 - if it is positive value then it has to
248:11 - end up over here there are only two
248:13 - possibilities so what we are going to do
248:15 - is at every single location we are going
248:17 - to compare the value of this with its
248:20 - right counterart and whichever value is
248:23 - going to be higher we are going to put
248:24 - that in the end and whichever value we
248:27 - put that pointer we would move to the
248:29 - next element and again repeat the same
248:31 - procedure so let's see the solution I'm
248:33 - proposing in ation action so first we'll
248:36 - compare this left with right and we are
248:38 - going to do square of both of them so
248:40 - square of left is going to be 16 and
248:43 - right square is going to be 9 so again
248:45 - left is greater because left is greater
248:48 - first we are going to add value number
248:49 - 16 over here and then we will move our
248:51 - left pointer and once again repeat the
248:53 - same exercise so now this time uh left
248:57 - pointer is located over here okay so now
248:59 - square of two is going to be four and
249:01 - square of three is going to be 9 so
249:03 - because 9 is greater now we are going to
249:05 - put 9 over here once again we are going
249:07 - to move left pointer in this case
249:09 - because we added nine here now this is 1
249:11 - and this is min -2 so square is going to
249:14 - be four and this is going to be 1 so
249:15 - once again next value we have to enter
249:17 - is going to be four and then we are
249:18 - going to move our left Point pointer on
249:20 - the right and then since because left
249:22 - and right now both are at the same
249:24 - position we are going to add the one as
249:26 - the last element over here and return
249:28 - this as the answer so see how we were
249:31 - able to generate the entire solution in
249:33 - just one single pass and this is the
249:36 - beauty of two pointer approach that
249:38 - whenever you see solution where you need
249:41 - to compare both sides of given input
249:43 - array or string try to think that can
249:46 - you solve this problem using two
249:47 - pointers and most likely this could be
249:50 - one of the possible solution so now let
249:53 - me give you a quick trick after we have
249:55 - understood lot of things about array
249:57 - first thing is whenever you are given an
249:59 - array try to think that can you sort
250:02 - this given array to generate some result
250:04 - second try to think can you use some
250:06 - some of the hashing function like hash
250:08 - map or hash set then if not try to S
250:12 - think can you use a sliding window
250:13 - technique otherwise try to think can you
250:16 - use two pointer technique typically
250:18 - amongst all of these most likely you
250:21 - would be able to find the answer you are
250:23 - looking for and this is the key trick
250:26 - for to solve any array based question in
250:29 - any of the technical interviews okay so
250:31 - now we are going to see the solution for
250:33 - squares of sorted array uh so let's see
250:36 - first of all we are going to create a
250:38 - variable called n and that is going to
250:40 - be the length of given input array then
250:42 - we are going to create a new array to
250:44 - store the result we are also going to
250:46 - initialize two pointers first is going
250:47 - to be the left pointer second is going
250:49 - to be the right pointer left pointer is
250:51 - located on the leftmost position
250:53 - rightmost right pointer is located on
250:55 - the rightmost position then we are
250:58 - running a for Loop in the reverse order
251:00 - and we are we are going to have a
251:02 - function that basically takes care of
251:05 - the Square value and then we are going
251:07 - to compare the square of the left
251:10 - element with the right element so
251:12 - basically what we are doing is uh we are
251:14 - simply comparing the absolute values of
251:18 - left element with the absolute values of
251:20 - right element so we are ignoring the
251:22 - minus sign and whichever value is
251:24 - greater we are putting that in the
251:26 - square and in the end in the result of
251:29 - that particular I value we are just
251:32 - storing whatever the square variable we
251:34 - were able to find either left or right
251:36 - and then uh in the end we simply return
251:38 - the result array that we have created
251:41 - now let's try to run this code okay
251:43 - seems like our solution is working let's
251:45 - submit this code and our code runs 100%
251:48 - faster than all the other Solutions
251:50 - which is quite awesome so now you got
251:52 - the idea of how two pointer Solutions
251:57 - work okay so first let's try to
252:00 - understand a really interesting coding
252:02 - pattern that is called fast and slow
252:04 - pointer now this is most prevalent and
252:06 - very heavily used in link list kind of
252:09 - problem so you might think whenever you
252:12 - are trying to solve some linkless
252:13 - problem try to think that can you use
252:15 - this technique in this scenario and
252:17 - first let's understand that what does a
252:19 - fast and slow pointer technique is it is
252:21 - very similar to a two-p pointer
252:23 - technique but uh in the two pointers
252:25 - where we have two pointers left and
252:27 - right coming towards each other in this
252:29 - scenario we actually have a fast pointer
252:32 - and a slow pointer both typically starts
252:34 - from the same position but fast pointer
252:37 - would would make double jumps so fast
252:39 - pointer would go to the next to the next
252:42 - value meanwhile slow pointer would only
252:44 - make a single jump like a normal uh
252:47 - typical traversal we do and doing this
252:50 - would yield us uh some good results in
252:52 - couple of specific positions so first
252:55 - let's try to see that how does a typical
252:57 - concept looks like let me draw a random
253:00 - uh link list now we are assuming that
253:02 - this one is a single link list but same
253:04 - thing would apply for a dou link list as
253:06 - well and this is the head of the link
253:08 - list and this is the last element so
253:10 - this value points to the null value now
253:13 - in this case the overall idea for the
253:16 - pattern is that we are going to have two
253:18 - variables so first one is a fast and
253:20 - second one is a slow variable okay so
253:22 - currently let's imagine that both fast
253:25 - and slow pointers are based at the head
253:27 - position okay now fast pointer is move
253:30 - in the Direction Where We are going to
253:31 - choose the next element for fast is
253:33 - going to be node do next do next so next
253:39 - to the next element for any given node
253:41 - would be the next node for the fast
253:43 - pointer meanwhile for slow pointer it is
253:45 - only going to be node do next that is
253:47 - the regular traversal so during the
253:50 - first iteration fast node is going to do
253:52 - node do next do next so fast pointer
253:54 - would end up over here meanwhile slow
253:57 - pointer would have end up over here in
253:58 - the second iteration fast pointer would
254:01 - end up over here meanwhile slow pointer
254:03 - would have ended up over here and doing
254:05 - this now fast pointer is is at the very
254:07 - last element inside the link list so now
254:10 - we can use some properties so the
254:12 - question comes that where does this
254:14 - technique is actually used so I can give
254:16 - you two examples right now first example
254:18 - is that I already showed you previously
254:20 - in the course where you need to find the
254:22 - middle of the link list in that case uh
254:25 - fast and slow pointer is very good use
254:28 - case to follow that if we want to find
254:30 - the middle of the link list typically we
254:32 - can do that and try to consider the same
254:35 - scenario in this example we are seeing
254:37 - that right now the fast pointer actually
254:40 - reached to the end position because fast
254:42 - do next is equal to null which means
254:44 - this is the last position around that
254:46 - time slow pointer is exactly located at
254:48 - the middle element and which we can see
254:51 - right over here so whenever we are being
254:52 - asked that hey go ahead and find the
254:54 - middle of the link list always try to
254:56 - use fast and slow pointer because that
254:58 - would be the quickest method to find the
255:01 - solution another one solution is that
255:04 - when you need to detect a cycle inside a
255:06 - link list and now the question comes
255:08 - that how can we actually detect a cycle
255:10 - inside the link list using the fast and
255:12 - slow pointer so let me give you an
255:13 - example for that as well now suppose uh
255:16 - let's assume that we currently have a
255:18 - bunch of different nodes lying around
255:21 - and we will try to see that how would a
255:24 - cycle inside the link list would look
255:26 - like okay now let's assume that all of
255:29 - these nodes are connected and in this
255:32 - case there is a cycle present like this
255:35 - okay now we don't know initially because
255:37 - this is a singly link list and we we
255:39 - have no idea that what are the next
255:41 - elements so let's assume that if you had
255:43 - to detect that find that if there is a
255:46 - cycle in this case or not how would you
255:47 - detect it uh one basic and naive
255:50 - approaches that let's say that these are
255:52 - all the values for this given uh link
255:55 - list what we can do is we can actually
255:57 - start iterating the given link list and
255:59 - we can have another data structure like
256:01 - a hash set and we can keep checking that
256:04 - whether this node has been previously
256:06 - visited or not and if we reach to the
256:08 - null value we can say that there are no
256:10 - Cycles in the link list like the next
256:12 - node is the null node then there are no
256:14 - Cycles in the link list if the next node
256:16 - is not null and we encounter some node
256:18 - that is already present in the hashset
256:20 - we can say that this node has been
256:21 - repeated so let's try to run this
256:23 - example first we will add all of these
256:25 - values so 1 2 3 4 5 6 7 and 8 all the
256:29 - values we would iterate but before that
256:31 - we would check whether it is present
256:33 - inside the hash set or not because it's
256:35 - not present we would add all of this 1
256:37 - to eight values over here now the moment
256:39 - we would try to iterate to this value we
256:41 - would realize that this three has
256:43 - already been added to to the hash set
256:45 - which means we encountered a value that
256:47 - has been added so that's why we can say
256:49 - that yes there exist a cycle in this
256:51 - case but now issue with this approach is
256:53 - that we actually have to use an
256:55 - additional has set so the space
256:57 - complexity in this case would be big go
256:59 - of n which we don't want we want to do
257:02 - this problem without using the space
257:04 - complexity so the another approach is to
257:06 - use fast and slow pointer and let me
257:09 - show you that how would that look like
257:11 - the whole solution okay so let's get rid
257:13 - of this hash set and let's have our fast
257:16 - and slow pointer ready so initially our
257:18 - fast pointer would be located over here
257:20 - and slow pointer would be located over
257:22 - here now let's make two jumps for the
257:24 - fast pointer and one jump for the slow
257:26 - pointer so after which fast pointer is
257:28 - going to end it up end it up over here
257:31 - and same way slow pointer is going to
257:33 - end up over here okay now once again
257:35 - fast pointer is going to end up over
257:36 - over here and slow pointer in this
257:39 - position has ended up over here okay now
257:43 - let's repeat the same process now fast
257:45 - pointer would come over here same way
257:48 - slow pointer would come over here and
257:50 - let's get rid of the previous two
257:52 - elements once again fast pointer is
257:55 - going to make two jumps so fast pointer
257:56 - would end up over here and in this time
258:00 - slow pointer would have made one jump so
258:02 - slow pointer would be here now when fast
258:04 - pointer makes two jumps it is guarantee
258:07 - that it is going to encounter slow
258:08 - pointer in this scenario and the moment
258:11 - we find out that fast pointer actually
258:12 - came back to the slow pointer or the
258:15 - position where slow pointer is we can
258:17 - say that there is a cycle in this
258:18 - scenario and we can return true that yes
258:21 - there exists a cycle so basically this
258:24 - is the whole concept of fast and slow
258:26 - pointers so whenever you are trying to
258:27 - solve some problem related to link list
258:30 - try to think that hey can I use two
258:32 - pointers where one pointer is faster
258:34 - than the other and try to come up with
258:36 - the solution more than likely you will
258:39 - be able to find some good answers in
258:44 - that now let's learn an awesome
258:46 - technique called backtracking
258:48 - backtracking is highly popular in lot of
258:50 - different scenarios and you can
258:52 - typically use backtracking to solve
258:54 - problems like dynamic programming
258:57 - recursion uh tree problems and graph
259:00 - traversal problems so for all of these
259:03 - things backtracking is heavily useful so
259:05 - you can imagine that how popular it is
259:07 - going to be if you ever wants to
259:10 - implement these data structures and try
259:12 - to come up with a solution so first
259:14 - let's do this we will try to understand
259:16 - that what backtracking is then we will
259:19 - try to see that what is some real life
259:21 - example so I'll probably give you a
259:23 - couple of real life examples and then we
259:25 - will try to see how we can navigate
259:28 - using backtracking in the depth for
259:31 - search manner so that is how and we are
259:34 - we will try to navigate it within trees
259:36 - and graph and on top of that we will see
259:38 - one of the lead code examples as well
259:40 - okay so first let's understand that what
259:42 - does a backtracking is backtracking is
259:45 - nothing but an ability to go back to the
259:49 - previous place we were at essentially
259:51 - let's say that we are following some
259:53 - certain path and at any given position
259:57 - we have the option to choose uh some
259:59 - different paths as well depending on the
260:01 - CH choices we make so let's assume that
260:03 - in this scenario we decide to take down
260:06 - uh and go we initially started with this
260:09 - and we go down this path this path and
260:11 - from this moment we decide to go to next
260:14 - PATH over here and from this we decide
260:16 - to go to the next PATH over here but we
260:19 - did not find the answer we were looking
260:20 - for so what we would do we would come
260:23 - back again to this node and try to check
260:26 - a different path once again since we did
260:28 - not get the answer we would once again
260:30 - try to get back to this path where we
260:32 - originally branched out come back to the
260:34 - same PL same path and then choose a
260:37 - different option and keep a track that
260:40 - which path that we have already taken so
260:43 - that would help us determine or backrack
260:46 - our actions that's it this is the whole
260:49 - concept of backtracking if you have to
260:51 - understand this let's say that you are
260:53 - currently located inside a garden and
260:56 - this is like a maze Garden so you don't
260:59 - know where you are going okay so
261:01 - typically all the roads are crossed in
261:05 - weird manner
261:06 - where some roads don't lead to any other
261:09 - place and then there are some roads that
261:11 - leads to different positions in the
261:13 - garden so what would be the approach you
261:15 - would take let's say that you typically
261:16 - started from here what is going to be
261:18 - the approach you are going to take well
261:20 - essentially the idea is let's say from
261:22 - this place you started traversing this
261:25 - path you would keep on going and realize
261:27 - that hey there is no path in this case
261:28 - so of course you are going to backtrack
261:30 - you are going to come back to the
261:32 - position where you originally started
261:33 - once again try to take a different path
261:36 - and again make some different decisions
261:38 - so if you go down this path once again
261:40 - it does not work so you will backtrack
261:41 - to a previous position where you made
261:43 - the choice and once again take this New
261:46 - Path and eventually this New Path might
261:48 - end up might lead you to the Final
261:50 - Destination you are trying to get so
261:52 - this is one example of backtracking so
261:54 - now let's try to see that how does
261:55 - backtracking would typically work in a
261:57 - tree like scenario okay so currently let
262:00 - me just draw a very simple binary tree
262:03 - where I have bunch of different uh nodes
262:05 - and each node had has some children
262:08 - associated with them and for each one of
262:11 - them I'm trying to see that what is
262:13 - going to be the most optimal path or
262:16 - let's say I'm trying to find this value
262:18 - x uh this node X so first I go down this
262:22 - path then once again I go down this path
262:24 - and by the way I'm using DFS in this
262:26 - case depth for search which means I'm
262:28 - going in the depth rather than going in
262:30 - the breath so I go over here I don't
262:32 - find anything so I will backtrack once
262:35 - again from this I know that there is one
262:37 - option left that I haven't checked once
262:39 - again in this case I haven't checked the
262:42 - I didn't find anything so I'll backtrack
262:45 - once I know that I have exhausted all of
262:46 - this possibilities I can also go back to
262:49 - the original root node and once again
262:51 - repeat the same process and in this case
262:53 - since I don't find anything I will again
262:54 - backtrack and okay here I find the X I
262:57 - was looking for and I can say that yeah
262:59 - this is the correct path and I can
263:00 - return that as the answer so this this
263:02 - would be an example of how uh
263:05 - backtracking would work in a tree like
263:07 - data structure and same concept applies
263:09 - to the graph as well because trees and
263:11 - graphs are very similar the only
263:13 - difference is graphs can have Cycles
263:14 - trees typically don't have cycle so even
263:17 - in terms of backtracking the same logic
263:19 - would apply that let's say that you
263:20 - decide to go to this path and then this
263:22 - path first but this part does not yield
263:23 - the result you are looking for once
263:25 - again you would backtrack and you would
263:27 - go to a different path and through which
263:29 - you find the result you are looking for
263:30 - and you get the correct answer so
263:33 - backtracking is nothing but the ability
263:35 - to go back now let's try to understand
263:37 - backtracking with one of the lead code
263:41 - examples okay now let's assume that this
263:44 - is the problem we are trying to solve
263:45 - binary tree paths now we can see that
263:48 - this has been relatively popular problem
263:50 - and the problem statement is actually
263:52 - quite simple to understand we are simply
263:53 - given the root of a binary tree and we
263:56 - need to return all root to leaf paths in
263:59 - on any order so let's try to understand
264:01 - this with an example suppose we
264:03 - currently have a binary tree that looks
264:06 - like this now in this case let's give
264:09 - some arbitary values as well uh so the
264:11 - values are going to be 1 2 3 4 five and
264:15 - six okay these are the values for each
264:16 - node so in this case how many different
264:19 - paths we have from root to node so first
264:21 - path is that we can go down this this
264:23 - scenario and this scenario so 1 to 2 to
264:27 - 3 this is a path to leave okay once we
264:29 - identify and let's create our answer
264:31 - list where we are going to store all the
264:33 - results that we are we have been able to
264:35 - find so far Okay so we found one path
264:37 - after finding this we will backtrack to
264:39 - the previous element once again repeat
264:41 - the same procedure to the remaining path
264:44 - okay we completed the remaining path and
264:46 - now this path is 1 2 and four okay once
264:49 - again back track okay now we exhausted
264:51 - all the possibilities over here so once
264:53 - again we are going to backtrack to the
264:55 - previous element from one we still have
264:57 - unexplored paths so we will start
265:00 - exploring and we will keep on going
265:02 - until we find the leaf node that we are
265:04 - looking for so in this case okay we find
265:06 - the leaf node as six and now we will add
265:08 - one more path that is 1 5 and six and
265:11 - now we explored all the paths so this
265:13 - answer list we can return as the answer
265:16 - and basically that's it okay so this is
265:19 - the problem statement binary tree paths
265:21 - and here is the solution basically the
265:24 - first thing we are going to do is uh we
265:25 - are going to check for the edge cases
265:27 - that if the given root element is equal
265:29 - to null we can simply return the paths
265:31 - we have if not we have already created a
265:33 - new link list that contains the list of
265:35 - all the parts we are going to create now
265:38 - in order to implement the backtracking
265:40 - we will have to use stacks and in the
265:42 - stack we are going to add the current
265:45 - root node and then we will keep on going
265:47 - it to iterate all the possibilities
265:50 - until we find our stack is empty or not
265:52 - and when we learn more about DFS you
265:54 - would be able to understand this
265:56 - technique that how this technique
265:58 - actually really works and how it is
265:59 - pretty efficient way to implement the uh
266:02 - backtracking method for the depth first
266:04 - search scenario so so now we have our
266:07 - link lists initialized now first we are
266:09 - going to add the root element to our
266:11 - link list and then we are also going to
266:13 - add a new path to store the integer
266:16 - value and then we are going to run a
266:18 - while loop that while the given stack is
266:20 - not empty we will keep on repeating the
266:23 - same process and basically this piece of
266:25 - code does nothing but it only Travers
266:28 - through the left side of the node or the
266:31 - right side of the node until we reach to
266:33 - a point where there no longer exist any
266:36 - more children for any particular value
266:38 - and once we get that we would add those
266:40 - values to the subsequent paths and in
266:43 - the end we are simply returning the path
266:45 - so if you didn't understood what I
266:47 - mentioned I'm going to post the solution
266:49 - in the GitHub link as well so you can
266:50 - check it out from there and you should
266:52 - be able to understand let's try to run
266:54 - this
266:55 - code and seems like the solution is
266:57 - working as expected let's submit this
266:59 - code and our code runs pretty
267:02 - efficiently compared to a lot of other
267:03 - Solutions Plus this is this gives you an
267:06 - idea that how does a typical uh depth
267:09 - for search or backtracking works for any
267:13 - tree or graph related
267:18 - problems okay now we are going to
267:20 - understand another interesting pattern
267:22 - that is called intervals intervals are
267:25 - nothing but the spaces of time or
267:28 - sequence in between and we need to do
267:31 - some work with that or some computation
267:32 - with that typically in the intervals
267:35 - type of problem you would always be
267:37 - given some specific set of time based
267:41 - connections or intervals like something
267:43 - like a starting time and ending time
267:45 - starting time and ending time and then
267:47 - through this you would try to make some
267:49 - meaningful information so let's say that
267:51 - this is the starting and ending time
267:53 - chunks of the entire day uh between 9:
267:57 - to 5: and let's say that these are the
267:58 - meetings that this meeting starts at 10:
268:00 - and ends at 11: this one starts at 1 and
268:03 - ends at 2 and this one starts at 4 and
268:05 - ends at 5 so if this is the information
268:08 - given if I'm trying to create or
268:11 - schedule a meeting how would I be able
268:13 - to do it so what are the empty time
268:15 - slots available to me so this is an
268:17 - empty time slot available this is an
268:18 - empty time slot available so if I want
268:20 - to schedule a new meeting I can actually
268:22 - schedule in that and that is the whole
268:24 - point of intervals typically most of the
268:27 - times intervals are actually being used
268:29 - for various set of uh time management or
268:33 - calendar management type of activities
268:35 - and trust me this is important because I
268:38 - personally got asked interval questions
268:40 - in two of the very big company
268:43 - interviews I I I actually give so that
268:46 - is why interviews are intervals are
268:48 - really simple to understand on top of it
268:51 - once you know the technique it's
268:52 - actually quite easy to take care of it
268:54 - so let's see that what are the different
268:56 - possible scenarios we can have inside
268:58 - the interval number one possible
269:00 - scenario is that we are given the time
269:02 - schedule for A and B to two different
269:05 - people now we are being told that uh
269:07 - this is the time sequence now these are
269:10 - the times that a currently has a meeting
269:13 - and for B we are being told that these
269:16 - are the times B currently has a meeting
269:19 - so now if I if we want to arrange a
269:21 - common meeting between them to take
269:23 - place how can we do that and the idea in
269:27 - this case would be to create or merge a
269:31 - new interval common interval this is
269:33 - just for the example sake I'm telling
269:35 - you and in this case what we would do is
269:37 - we would take whichever the starting
269:39 - point is smaller at first and whichever
269:43 - the ending point is larger at second so
269:45 - if there is a conflict between any two
269:47 - entities that can be resolved quite
269:49 - easily and we would start creating new
269:52 - intervals that are combination of both
269:54 - of them so that would give us the idea
269:56 - that which are the empty times available
269:59 - so another interval like is like this
270:02 - and this one ends with the schedule so
270:06 - now this is the
270:08 - common time schedule for A and B which
270:12 - means we can say that there is only one
270:14 - empty space available where we can
270:16 - schedule a new meeting if you want to so
270:19 - this this would be one of the use cases
270:21 - second use case would be that let's say
270:23 - that for a we are actually being told
270:27 - that a is overbooked with multiple
270:29 - meetings on many places so which means a
270:32 - has overlapping meetings throughout his
270:34 - day and we wants to simplify this
270:36 - process so what we want to do is uh
270:39 - whichever meeting is smaller in the
270:42 - value we want to get rid of it so we
270:45 - want to get rid of the interval that is
270:47 - actually causing a conflict between the
270:49 - existing meetings so in this scenario we
270:52 - can check that okay currently this
270:53 - meeting is 2 hour long let's I'm giving
270:56 - you for an example and this meeting is
270:58 - only 1 hour long but we do see that
271:00 - there is a conflict in this case so
271:02 - because there is a conflict if we want
271:04 - we can get rid of this meeting entirely
271:06 - and say that uh now a does not have any
271:09 - conflicts same way in this case there is
271:11 - a conflict between this time and of
271:13 - course this meeting is smaller so we can
271:14 - also get rid of this one so this would
271:16 - be a remov of removal of interval kind
271:19 - of scenario another way to treat this is
271:22 - that we can actually move this meeting
271:24 - to the side so let's say if meeting is
271:26 - like this rather than canceling the
271:28 - whole meeting what we can do is we can
271:30 - just get rid of the conflicted part and
271:32 - we can create another new meeting that
271:34 - looks like this that okay now a has two
271:37 - separate meetings even though they are
271:39 - back to back with each other still they
271:41 - are not conflicting so this would be
271:43 - another type of scenario for inter
271:44 - interval questions and the third type of
271:47 - scenario would be where we are actually
271:49 - being told to merge
271:52 - intervals okay now let's assume that
271:54 - this is the problem statement given to
271:55 - us that we need to merge the intervals
271:57 - and you can see that this is a very well
271:59 - like problem on lead code basically we
272:01 - are given an array of intervals where at
272:04 - any particular position position defines
272:06 - the starting and ending point for that
272:08 - particular interval now we need to merge
272:10 - all the overlapping intervals and return
272:12 - return an array of nonoverlapping
272:14 - intervals that cover all the intervals
272:17 - so here we are given an example if you
272:19 - want you can take a look at this example
272:20 - but I actually plotted this example on a
272:23 - very nice like graph so you you should
272:26 - be able to see the idea now in this case
272:28 - we can clearly see that there are four
272:31 - different intervals given to us but
272:33 - among these four intervals we see that
272:35 - these two intervals are non overlapping
272:37 - they are just as it is now how do we
272:40 - know that on our side we can just see
272:42 - them and realize that they are not
272:43 - overlapping but if you wanted to check
272:46 - that if any two o intervals overlap or
272:48 - not all you need to check is the ending
272:51 - point of the previous element and the
272:53 - starting point of the next element and
272:56 - if there is a difference between them
272:58 - let's say that because this ending point
273:00 - is smaller than the starting point of
273:02 - the next one which means we can Define
273:04 - that there is no overlap between these
273:06 - two so because there is no overlap we
273:07 - can keep them as it is we don't need to
273:10 - do anything now in this case if we see
273:11 - this is the starting point this is the
273:13 - ending point same way this is the
273:14 - starting point and this is the ending
273:15 - point so starting point of this is one
273:18 - and ending point of this is three same
273:21 - way starting point of this new element
273:23 - is actually two and this uh this
273:26 - interval is actually six so in this case
273:29 - we can see that for this interval we do
273:33 - have another interval Who start starting
273:35 - point comes before the starting before
273:38 - the ending point of this previous uh
273:40 - interval so that is why we detect that
273:42 - there is an overlap because there is an
273:44 - overlap what we can simply do in this
273:46 - scenario is that we would take the
273:49 - starting point whichever is smaller so
273:52 - in this case the smaller starting point
273:54 - is one and then we would take the ending
273:56 - point whichever is greater so ending
273:59 - point greater is six so we would create
274:01 - a new interval called 1 to 6 and then we
274:03 - would take keep this 8 to 9 interval as
274:05 - it is and same with this 15 to 18
274:07 - interval as it is so the new answer
274:10 - array is going to be this 8 to 10 and
274:12 - then 15 to 18 and this is the answer we
274:15 - need to return so basically this becomes
274:18 - a very simple problem to understand I
274:20 - don't know why lead code mentions this
274:22 - as a medium problem in my opinion this
274:23 - should have been easy all you are doing
274:25 - is comparing the values between starting
274:27 - and ending point between any two
274:28 - subsequent variables and remember in
274:31 - many scenario to trick you the intervals
274:35 - that we are originally given in the
274:37 - original input might not be sorted so if
274:40 - they are not sorted then in this case
274:42 - first thing you will have to do is sort
274:44 - them in the basis of starting time and
274:47 - once you have the sorted values then
274:50 - things becomes quite easy and then you
274:51 - should be able to solve any of the
274:53 - interval problem so let's quickly see
274:55 - the Java code for this problem so here
274:57 - is the problem statement merge intervals
274:59 - and let's see the Java code for that so
275:01 - first thing we are doing is we are
275:03 - actually sorting the given input based
275:06 - on the starting times and once we have
275:08 - that things becomes quite easy now we
275:10 - are just initializing a new link list uh
275:12 - named answer and then we are iterating
275:14 - over the original intervals array that
275:17 - we are given where we are given two
275:18 - values one for starting point and one is
275:20 - for ending point where we have all of
275:23 - these conditions that if the answer is
275:26 - empty or if it is the last element or
275:28 - whatnot we are going to add element to
275:30 - the interval if that is not the case we
275:33 - will have to do the merge operation
275:35 - where we are simply going to add the
275:37 - value of the last element or the maximum
275:40 - value of the last element compared to
275:42 - both the intervals and minimum value of
275:44 - the first interval that comes in the
275:45 - normal sequence and let's try to run
275:48 - this
275:49 - code okay seems like our solution is
275:51 - working as expected let's submit this
275:54 - code and our code runs decently
275:56 - efficiently so this is a very simple
275:58 - problem to solve once you know that you
276:00 - have to sort the given intervals based
276:02 - on the starting
276:04 - time
276:06 - now we are going to start with a very
276:07 - important topic called BFS now if you
276:10 - don't know what BFS means uh it is short
276:13 - for breath for search and it is a very
276:16 - popular tree and graph traversing
276:19 - traversal
276:20 - service
276:22 - so typically whenever we are going to
276:24 - see any data structure such as tree or a
276:28 - graph and we need to iterate over either
276:30 - tree or graph for traversal one of the
276:33 - very popular technique is is a breath
276:35 - for search technique and we spoke about
276:38 - this little bit uh in the previous
276:39 - sections but now we are going to go into
276:42 - much more deeper on how what are the
276:45 - considerations how it works and how to
276:47 - implement this plus we are going to see
276:49 - an example of uh an actual live lead
276:52 - code problem for this problem as well so
276:55 - we all know that in the breath for
276:57 - search essenti essentially you are
277:00 - traversing outwards towards your
277:02 - subsequent neighbors first before going
277:04 - out out to their neighbors so at any
277:07 - given position first you will encounter
277:09 - all the neighbors of that value then you
277:12 - will start encountering its neighbors
277:15 - and until you have done for all the
277:17 - cases you will not Branch out to the
277:20 - deeper levels so essentially you are
277:23 - traversing level by level for all of the
277:26 - nodes that are currently present inside
277:28 - your graph so if this is let's say that
277:30 - this is a graph that you are given now
277:32 - in this graph first of all you are going
277:34 - to start at the root position and this
277:36 - is the root position so first you are
277:38 - going to visit all four neighbors once
277:41 - visiting all four neighbors then for
277:44 - each neighbor you would start visiting
277:46 - their subsequent neighbors and one only
277:49 - after you have visited all the
277:50 - subsequent neighbors then only you would
277:52 - go to further higher or deeper levels so
277:55 - let's see some examples of this in both
277:58 - tree and graph like structures suppose
278:00 - this is the tree we are given I'm just
278:02 - drawing a very simple binary tree and
278:05 - and I'm going to associate some values
278:07 - to it so let's say that the values are 1
278:09 - 2 3 and 4 5 6 7 these are the values so
278:13 - if we if we have to apply breath first
278:15 - search in this scenario basically we are
278:18 - going to first visit for any root node
278:21 - we are going to first visit its
278:22 - neighbors so first of all we are going
278:24 - to visit node number two and node number
278:26 - three uh and then we are going to visit
278:29 - its subsequent nodes that are 4 5 6 and
278:31 - 7 so where this type of solution can be
278:35 - useful well let's say that at any given
278:38 - position you want to print out that what
278:39 - are all the numbers available at any
278:41 - given level of a tree then you can use
278:43 - this uh BFS approach or if you are
278:46 - trying to find some elements and you
278:48 - expect that element would generally be
278:50 - closer to the root element also in that
278:53 - case it would make more sense to use the
278:55 - BFS approach plus now let's see that how
278:57 - this would work in a graph like data
278:59 - structure typically in a graph we have
279:01 - bunch of different various data
279:03 - connected with each other
279:05 - and there are there can be some or many
279:07 - cycles associated with each one of them
279:10 - now assume that in this graph this is
279:12 - currently the root node so in this case
279:15 - for the BFS basically we will visit all
279:19 - the neighbors so this is the neighbor
279:20 - this is the neighbor and this is the
279:22 - neighbor and also this is the neighbor
279:24 - and only after visiting all these four
279:26 - neighbors we would start going to the
279:28 - deeper levels and that would be visiting
279:31 - neighbors of these neighbors uh so where
279:34 - is typically BFS useful for the graph
279:37 - likee structure is that let's say that
279:39 - for any particular node you want to know
279:41 - that how distant it is connected with
279:44 - some other node so in this scenario we
279:46 - can consider these two nodes to be
279:48 - Distance by two because this is directly
279:52 - connected with this so there is a
279:53 - distance of one and this is the distance
279:55 - of two this can easily calculated using
279:57 - BFS and there are some very real life
280:00 - practical use cases like uh websites
280:02 - like LinkedIn or even Facebook you can
280:06 - actually see that how what is your
280:08 - common connection between any two people
280:10 - and for that uh it's quite easy to
280:13 - implement the graph based bread for
280:15 - search method to calculate those values
280:17 - so you can calculate the degree of
280:19 - distance between each each other
280:21 - neighbor on top of that even for graph
280:24 - basically uh BFS is used to search any
280:28 - element plus Traverse over the entire
280:31 - graph and if you want to find any
280:34 - connection between between two end
280:35 - points you can do that if you have to
280:36 - visit all the neighbors of any
280:38 - particular element for any of your
280:40 - problem solving requirement you can also
280:42 - use BFS in that scenario and again
280:44 - remember any problem that you can solve
280:47 - with BFS you can also solve that with
280:50 - DFS as well the only difference is
280:52 - depending on the problem statement
280:54 - sometimes it would make sense to use the
280:56 - BFS and many times it would make sense
280:58 - to use DFS so always make sure that you
281:00 - are aware that which data structure
281:03 - approach you are going to choose now
281:05 - let's see that how BFS gets typically
281:07 - implemented and usually in order to
281:10 - implement a BFS we usually use a cube
281:13 - where we put one value inside the cube
281:16 - and then we add its children to that
281:18 - Cube and then we keep on repeating the
281:20 - process so the output of that CU would
281:23 - be the breath for search manner
281:25 - traversal for any given tree or graph
281:27 - and let me show you an example of a tree
281:29 - so suppose I'm given a simple tree and
281:32 - I'm just drawing uh five noes over here
281:35 - and let me Mark the values as 1 2 3 and
281:37 - four and 5 so initially we are at this
281:39 - first position now we are trying to do
281:41 - the BFS so let me also initialize my Q
281:44 - as well and in my Q uh we all know the
281:47 - principle first in first out okay so
281:49 - first the value I have currently is
281:51 - value number one so I'm adding one over
281:53 - here now before processing one I'm going
281:57 - to add all the children of one I repeat
282:00 - before processing one I'm going to add
282:02 - all the children of one so there are two
282:04 - two childrens's of 1 2 and three so I'm
282:06 - going to Mark values 2 and three over
282:08 - here after that and only after adding
282:11 - all the children I'm going to process
282:13 - one so let me print out the value one
282:15 - over here that this node has already
282:16 - been processed and also let me get rid
282:19 - of this now the immediate value or the
282:22 - very first element inside the Q is value
282:23 - number two so once again before
282:25 - processing two we are going to add all
282:27 - of its children so the values are four
282:30 - and five and then after that we can
282:32 - process value number two so once again
282:34 - let me get two out and uh second element
282:37 - over here would be two now in this case
282:40 - the next element I have is three now
282:42 - since we already process all the
282:43 - children of three or three does not have
282:45 - any children so we can take three out
282:48 - then same way after taking three out
282:50 - four and five also does not have any
282:52 - children so we will take four out and
282:53 - then we will take five out and in the
282:55 - end our que would be entirely empty
282:58 - because we process all the nodes so we
283:00 - are going to keep on processing until Q
283:02 - is not empty and using this approach we
283:04 - can actually solve the BFS for any given
283:07 - tree problem same logic applies for the
283:10 - graph problem as well so you would be
283:11 - able to understand what I mean now let's
283:14 - try to consider one example uh and then
283:17 - we will see the code for that the
283:19 - example is that we want to find the
283:21 - average at the uh level for any given
283:25 - binary tree so the problem statement is
283:28 - quite simple to understand we are simply
283:29 - given a binary tree and all the values
283:32 - we need to find the average at the its
283:36 - own level now let's give some arbitrary
283:40 - values so 1 2 3 and 4 5 6 7 and then 8
283:45 - and 9 so these are the values now what
283:47 - is the answer going to be currently on
283:50 - the very first level there is only one
283:52 - element and its value is one so the
283:54 - average for this is also going to be one
283:57 - now for the second element we have two
284:00 - values 2 + 3 so the average is going to
284:02 - be uh 5 / 2 so 2.5 so this answer is
284:06 - going to be 2.5 next over here the
284:08 - average is going to be uh 4 + 5 is 9 and
284:13 - 9 + 13 so 22 22 divided 4 so I think
284:17 - it's something like 7 uh sorry
284:20 - 5.5 yeah I think it should be 5.5 so
284:24 - this average is also going to be
284:27 - 5.5 and now for this one the average is
284:29 - going to be 8.5 and this is what we need
284:32 - to calculate so you must must have
284:34 - understood by the logic of it that we
284:36 - are going to do the traversal based on
284:39 - the levels that we are
284:41 - performing and we are going to have our
284:43 - Cube and in the cube we are going to
284:45 - first insert the value number one we are
284:47 - also going to have a method to calculate
284:49 - the average and in order to do that we
284:51 - will need to know that how many children
284:53 - are currently Pro or how many elements
284:55 - are there because this is a root element
284:57 - currently we only have one element so
284:59 - I'm going to encounter a value called
285:01 - count and initially the count is only
285:03 - one for this one I'm going to add its
285:05 - children so I added children 2 and three
285:08 - over here because I added two values I
285:11 - know that for next time when I need to
285:13 - calculate the average the count is going
285:15 - to be two okay so now this one this time
285:18 - using this I can calculate the average
285:20 - one and I can print one over here same
285:22 - way uh before processing two and three
285:24 - I'm going to process its children and
285:26 - even for the children I'm going to keep
285:28 - track that how many what was the count
285:30 - and eventually I should be able to make
285:32 - all of this continuous a average
285:34 - calculation because it's a very simple
285:36 - problem and this is how we can actually
285:38 - Traverse in the level order for the
285:40 - given tree and solve this problem quite
285:42 - easily so now let's see the lead code
285:44 - solution for this as well this is the
285:46 - problem we are trying to solve average
285:48 - levels in the binary tree and here is
285:50 - the Java code for that first of all we
285:52 - are given the definition of the tree and
285:54 - then in the main solution first of all
285:56 - we create a new list where we are going
285:58 - to store the result values plus we are
286:01 - also going to initialize a q where we
286:03 - are going to store our tree nodes and on
286:05 - top of that we first add the root
286:07 - element to our Q now we run our while
286:10 - loop that while Q is not empty we are
286:12 - going to keep track of what has been the
286:13 - long sum plus what has been the count
286:16 - for every single children we added we
286:18 - are going to add a temporary node to for
286:20 - our Cube and we once again for that we
286:23 - are going to keep track of or add all
286:25 - the children of it and increase the
286:27 - value of the count and we are going to
286:29 - keep on incrementing until the left node
286:31 - is not equal to null or right node is
286:33 - not equal to null and uh after
286:35 - calculating that we are simply going to
286:38 - run the average function that is Su
286:41 - multiply by one divided by count and um
286:44 - uh that's it so we will add those
286:47 - results into the result list we created
286:49 - earlier and this is the whole solution
286:51 - now let's try to run the
286:53 - code okay seems like our solution is
286:55 - working as expected let's submit this
286:56 - code and our code runs pretty fast
286:59 - pretty efficiently which is awesome so I
287:02 - hope you understood uh that how things
287:05 - get easier whenever you need to Traverse
287:08 - in level order for any graph or any uh
287:11 - tree you can use BFS quite
287:15 - easily okay so just like BFS now we are
287:18 - going to shift our focus on the DFS and
287:21 - DFS is also a graph and tree traversal
287:24 - method and in this scenario we are
287:26 - actually going down into the depth
287:29 - before going into the breadth so we are
287:31 - going to pick a path keep on going on
287:34 - and on in the depth until we reach to
287:36 - the very last Leaf node on that path and
287:39 - then only we are going to backtrack our
287:41 - way to a different possibility and then
287:43 - keep on repeating the same process so
287:45 - let's try to see an example suppose this
287:48 - is a tree that we are given now in the
287:50 - same example rather than traversing in
287:53 - into the sequence of breath we are
287:55 - actually going to go down into the deep
287:57 - so first we are going to Traverse down
287:59 - this path then after traversing this we
288:02 - still have one node that we haven't
288:04 - process in the reverse order so we will
288:05 - go and do a backtrack after backtracking
288:08 - go back over here once again solving
288:10 - this we would again backtrack and
288:12 - through here we would complete the
288:13 - remaining uh path that we haven't taken
288:16 - and in the end we will return this
288:18 - solution of all the paths that has been
288:20 - traveled so this was an a depth for
288:22 - search for a tree same logic will apply
288:25 - for a graph based depth for search as
288:27 - well that let's assume that we have some
288:30 - complicated graph based algorithm or
288:33 - many different nodes that are connected
288:35 - with each other in all sorts of manner
288:37 - and once again we would try to generate
288:40 - some graph based solution for this one
288:42 - as well so let's assume that if this is
288:44 - our root node initially so we decided to
288:47 - go in in the depth for each one of them
288:50 - so first let's say that we pick this
288:52 - node now this node also has other
288:54 - neighboring connection node so we are
288:56 - going to go to its neighbors again this
288:59 - also has neighbor so again we are going
289:00 - to go to its neighbors and again this
289:02 - also has neighbors so we we are going to
289:04 - go to its neighbors after completing all
289:06 - of this we are going to do a back trck
289:07 - to see if we missed any other branches
289:09 - in the depth this one has no branches
289:12 - this one also has no branches this one
289:14 - also has no branches so we come back to
289:16 - our root node but root node we still
289:17 - have other nodes that we haven't
289:18 - traveled so we will go to that node
289:20 - first before traveling to the other
289:22 - nodes and every time we are going to do
289:24 - the backtrack function now we know that
289:27 - in the breath first search we were using
289:28 - Cube but actually in the depth for
289:30 - search we are going to use a stack T to
289:33 - keep track of of all the nodes that
289:35 - would help us with the backtrack
289:37 - function so how would this work I will
289:40 - just give you an example but first let
289:41 - me talk about that if you have to do a
289:44 - tree traversal using DFS you actually
289:47 - have three different ways to do it now I
289:49 - already showed you what those ways are
289:51 - but I'm just giving you the name that is
289:52 - in order
289:54 - traversal pre-order traversal and post
289:57 - order traversal and all of these would
290:00 - be part of or would be considered a
290:02 - depth for search traversing
290:04 - methodology now the question is that
290:07 - what are some of the benefits of using
290:09 - the depth first search so first number
290:11 - one use is that if you are trying to
290:13 - find the full path between any two
290:15 - entities so in that case DFS tend to be
290:18 - more useful second thing is if you're
290:20 - trying to find some element so again in
290:23 - that regard as well that you are able to
290:25 - generate it very easily using DFS uh
290:29 - same same way let's say that if you are
290:31 - trying to make some dependency graph and
290:33 - and this is heavily useful in lot of
290:35 - scenarios let's say if you are trying to
290:37 - build a compiler or if you're trying to
290:40 - build like a schedule prerequisite
290:43 - dependency course kind of a module for
290:45 - our University so in all of these
290:47 - scenarios graph based DFS is going to be
290:50 - very useful so these are some of the
290:52 - most critical scenarios wherever you can
290:54 - think of that from one node in the graph
290:56 - you need to go to far away to some other
290:59 - node try try of thinking to use
291:02 - DFS okay now let's assume that we would
291:05 - try to do a DFS stack run example for
291:08 - any given note now initially we are
291:10 - going to have an empty stack and inside
291:12 - our empty stack just like BFS method in
291:15 - the DFS we are going to add the root
291:17 - node now remember stack has a different
291:20 - property that is last in first out okay
291:23 - and we are going to follow some
291:24 - principles so first okay let me add one
291:26 - value number one over here now the
291:28 - principle we are going to follow is the
291:30 - moment we take one value out from the
291:32 - stack we would see that how many
291:34 - children that value has and both of
291:36 - these children we are going to add to
291:37 - the stack after that and we are going to
291:39 - keep on repeating the same process okay
291:41 - so now currently we have element number
291:43 - one so we are going to take one out so
291:46 - if we take one out let me just node
291:49 - create a method where we are going to
291:51 - keep track of process node So currently
291:53 - we have process node number one okay now
291:56 - we are going to add its children 2 and
291:58 - three into the stack as well so let me
292:00 - add this value now once again just like
292:02 - the same logic we are going to pop value
292:04 - number two first so let me pop value
292:06 - number two so we process node number two
292:09 - but because we process node number two
292:10 - we are going to add its children into
292:12 - the node as well so let me add values
292:14 - five and four over here as well now once
292:17 - again same logic we are going to pop
292:19 - value number four first so after popping
292:21 - value number four we are also going to
292:23 - add children of four as well that is
292:26 - value 8 and N so once again even for 8
292:29 - and 9 we are going to pop them now since
292:31 - 8 and N does not have a children of
292:33 - Their Own so even if we pop value number
292:35 - eight and value number nine they are
292:36 - going to remain as it is because they
292:38 - don't have any children that we can add
292:40 - so now we can get rid of 8 and N from
292:42 - here now next value we have is element
292:44 - number five again element number five
292:46 - also does not have any children so we
292:48 - are going to mark value number five over
292:50 - here and then we can remove that okay
292:53 - after that we are only left with value
292:55 - number three so we will try to pop value
292:57 - number three out the moment we pop value
292:59 - number three in this case we will have
293:01 - to add its children over here so first
293:04 - let me delete this okay so now currently
293:07 - we are going to add value number six and
293:09 - seven here and then we are going to mark
293:11 - three as presented and then we are going
293:14 - to pop value number seven out and then
293:16 - we are going to pop value number six out
293:18 - so this is going to be the whole flow of
293:20 - the sequence in which we Traverse
293:23 - through all the nodes inside the given
293:25 - tree using the depth first search method
293:28 - and uh we can see that all the
293:30 - iterations leads us in the in the graph
293:32 - sequence so here here first we visit
293:34 - node number one then we visit node
293:36 - number two then we visit node number
293:38 - four then we visit node number 8 so we
293:40 - go down in depth after visiting eight we
293:44 - do a back track and we go to the four
293:46 - and visit the remaining child that is
293:48 - nine so like this and after visiting
293:50 - nine we do another backtrack and two we
293:53 - visit value number five so this is how
293:55 - the sequencing of death depth for search
293:58 - is being followed and you can find
294:00 - plenty of examples of depth for search
294:03 - in all across places so I hope uh the
294:05 - concept of BFS and DFS is quite clear to
294:08 - you now so we can move on to our next
294:14 - topic okay now we are going to see a
294:17 - really important algorithm that is
294:18 - called greedy approach now as the name
294:21 - suggest in the greedy approach we try to
294:24 - be greedy in order to generate the
294:26 - answer and we try to find the solution
294:28 - on every substep as well in the given uh
294:32 - algorithm or given problem now before we
294:35 - start understanding the technical
294:36 - details of greedy approach let's first
294:38 - try to understand the local detail of
294:40 - the how greedy approach typically Works
294:43 - let's say that you currently have an
294:45 - empty truck and you are trying to fill
294:48 - this empty truck with bunch of different
294:49 - boxes uh that you currently have now in
294:52 - terms of boxes you have three different
294:55 - types of boxes first boxes that uh and
294:59 - remember all three box boxes are
295:00 - identical in size but they contents are
295:03 - different so let's assume that the first
295:05 - box we have this contains all the ion
295:09 - now second box we have this contains all
295:11 - the aluminum and the third box we have
295:14 - this contains all the
295:16 - feathers now in this case what should be
295:19 - our approach to fill in the all of these
295:22 - boxes into the trucks so that we can get
295:25 - the maximum return out of weight we are
295:27 - trying to put in of course the approach
295:30 - is going to be quite simple we are at
295:32 - every given position or every particular
295:35 - item we are trying to fill we will try
295:37 - to maximize the value of these pink
295:40 - boxes and we will try to see as many
295:42 - number of uh these boxes we would try to
295:44 - fit inside the given uh truck before we
295:48 - run out of them and once let's assume
295:51 - that we fit all of the iron boxes what
295:53 - would be our strategy in next case next
295:56 - case strategy is to fill out the
295:58 - remaining of remaining portion of the
296:00 - truck with all the aluminum boxes uh so
296:03 - doing this method would grant us the
296:06 - best weight ratio amongst uh inside our
296:09 - truck and in the end if we have space
296:12 - then we will try to put the boxes with
296:14 - feathers in them if not then we would
296:17 - essentially fill out our entire uh truck
296:19 - with these two type of boxes so the load
296:22 - we would be handling the heaviest load
296:25 - so what we did in this scenario that
296:27 - let's assume that we consider that
296:29 - filling one box inside the truck as one
296:32 - sub problem for the given algorithm or
296:35 - given step so in every sub problem
296:38 - amongst all the options we always choose
296:41 - the best fit option in order to solve
296:44 - our problem that we are trying to solve
296:46 - and this is the classical example of a
296:48 - greedy approach where even during the
296:51 - each and every suboptimal level we try
296:53 - to choose the most optimized and best
296:57 - available option in order to go to the
296:59 - next next step so now let's see that
297:01 - what are some of the important
297:02 - characteristics of a greedy algorithm so
297:05 - greedy algorithm as I mentioned it is
297:07 - built on piece by piece by piece and
297:10 - every single time we are always choosing
297:12 - the most available and best uh
297:16 - suited particular option for our graph
297:20 - in order to build the solution so no
297:22 - matter if there are how many number of
297:24 - choices available we are always going to
297:26 - pick the choice that is the most suited
297:28 - in order to F fulfill our need now
297:32 - greedy algorithm also suggest that the
297:36 - global Optimum or the best results we
297:39 - can achieve by every single time at
297:41 - every single sub problem selecting the
297:43 - best available option so that is one of
297:46 - the characteristics of the greedy
297:47 - approach and uh there is also one more
297:51 - choice or one more uh thing that we have
297:53 - to understand that is that in order to
297:56 - optimize or find the optimal solution
297:58 - for all we need to consider that what
298:01 - are the overall constraints available to
298:03 - us because many times the greedy
298:06 - approach might look okay at the
298:08 - beginning but it might not be the
298:11 - correct Choice uh because let me give
298:13 - you an example for that do you remember
298:15 - the scenario we were trying to solve
298:17 - where we had bunch of different coins
298:19 - and we were trying to make uh any
298:21 - particular number so let's just say that
298:23 - we were trying to make value number 11
298:25 - uh to see that which are the smallest
298:27 - number of coins we can make or we can
298:29 - use to to build this property and let's
298:32 - assume that the available coins to us
298:34 - are going to be value number one uh then
298:37 - value number five and then value number
298:39 - s and we are trying to see that how many
298:41 - number of coins would it take for us to
298:42 - generate this value number 11 if we were
298:45 - to use greedy approach our approach is
298:48 - going to be that for first sub problem
298:50 - we are going to choose the coin with the
298:51 - maximum value So currently the maximum
298:53 - value is seven okay so after choosing
298:56 - the 7 once again now the remaining value
298:59 - we have to create is going to be value
299:02 - number five because sorry value number
299:04 - four because 17 minus uh sorry 11 - 77
299:08 - 11 - 7 so 11 - 7 is give us the result
299:12 - four so now we will have to create value
299:14 - number four for that we still have three
299:17 - options but we cannot choose value
299:18 - number seven because it is too high we
299:21 - cannot also choose value number five
299:23 - because that is also too high so the
299:24 - next option is to choose four different
299:27 - $1 coin so in this case we are going to
299:29 - choose 1 + 1 + 1 + 1 so in this case if
299:33 - you see in total it took us five coins
299:36 - to build this value number 11 uh and we
299:40 - use the greedy approach but do you think
299:42 - this was the most optimal way to solve
299:44 - this problem no why because we already
299:46 - have a sub option where we could have
299:48 - choose coins like 5 + 5 + 1 and this
299:51 - would have also given us the value
299:52 - number 11 and even though at every
299:55 - suboptimal level we did not choose the
299:58 - most appropriate or most greedy approach
300:00 - we still found the optimal result in
300:02 - terms or three coins so you always have
300:05 - to consider that whenever you are trying
300:06 - to solve a problem think about it that
300:09 - will greedy would be the best solution
300:12 - or could there be a scenario where
300:14 - rather than using a greedy approach you
300:16 - can try to think of using a dynamic
300:18 - programming approach which is what we
300:19 - did in this scenario so greedy problem
300:22 - and dynamic programs they go hand in
300:24 - hand they're very closely correlated
300:27 - with each other but you will have to
300:28 - make that decision and you can only
300:30 - understand that decision after correct
300:32 - directly figuring out the given input so
300:35 - this is really important now let's move
300:38 - on to the next property and let's try to
300:40 - understand this with an example now the
300:42 - example is that suppose uh we are given
300:46 - an we are given a text okay and this
300:50 - text contains lot of different
300:51 - characters now we are trying to build an
300:54 - editor where inside the editor we are
300:57 - trying to put the encryption of some of
301:00 - these values so now we need to identify
301:03 - that which character should be encrypt
301:06 - so that the that those encrypted values
301:09 - would be easy to travel over the given
301:12 - Network and that would consume less
301:13 - space so let's say that if there is a
301:15 - character like um
301:20 - chloroform and there is another
301:22 - character called the so which character
301:24 - should I pick in order to uh compress or
301:28 - in order to encrypt of course at first
301:31 - glance we would think that chloroform is
301:32 - is a bigger word so we should put the
301:34 - chloroform in the encryption but the
301:36 - idea the better choice would be the why
301:38 - because the is more likely to appear at
301:41 - multiple places throughout the document
301:43 - so if we choose to encrypt this
301:45 - character it would give us more value so
301:48 - what should be the greedy approach in
301:50 - this case to decide that which
301:51 - characters should be good candidates in
301:53 - order to uh have
301:55 - them have them available for the
301:57 - encryption and one of the best
301:59 - approaches that what we we can still use
302:02 - the greedy approach in this case we can
302:04 - actually create a heap for all the
302:07 - characters by their
302:09 - frequencies and frequ frequencies means
302:12 - I can see that how many times they
302:14 - appear now we can have the condition
302:16 - that if any two characters A and B if
302:19 - they have the same amount of frequency
302:21 - then we would choose the character with
302:25 - bigger length so this is our greedy
302:28 - approach that if the if there are like
302:31 - let's say that there are 10 10 times
302:33 - character a appears and 10 times
302:35 - character the appears in the same
302:37 - document so the would be our first
302:39 - choice rather than the character a
302:41 - because the contains three different
302:43 - characters so if we encrypt that there
302:45 - is more value behind it and in the end
302:47 - we would be able to written uh using the
302:50 - Heap whatever the top five answers are
302:53 - those five would be the characters that
302:55 - are most repeated bigger in size and
302:58 - also give us the best value for encrypt
303:00 - encrypting our result so this would be
303:04 - one of the good example for greedy
303:05 - approach and basically this sums up most
303:09 - of the questions we had regarding our
303:12 - different
303:15 - algorithms so first of all I would like
303:17 - to congratulate all of you for making up
303:19 - until this far I know it was lot of
303:21 - information to take in but you took it
303:23 - like like a champ so congratulations on
303:26 - that now let me give you some of the
303:27 - popular rule of thumbs and some tips and
303:30 - tricks that will help you
303:33 - come to the conclusion faster in an
303:35 - actual technical interview manner so
303:37 - first one is whenever you identify any
303:40 - question that deals with uh hey give me
303:43 - top closest minimum maximum of K numbers
303:47 - out of the given total n numbers in that
303:49 - case of scenario always try to think
303:51 - that hey can I use a heap in this
303:53 - scenario because Heap will allow you to
303:55 - store the values based on their
303:57 - properties either like maximum values or
303:59 - minimum values and let's say that I I
304:02 - ask you that hey out of the given input
304:04 - data stream give me the fourth maximum
304:06 - or fourth largest number the approach
304:08 - would be to generate a heap to store all
304:11 - of those values and then start popping
304:13 - out values one by one by one until you
304:15 - reach to the fourth character and that
304:17 - would be the answer you are looking for
304:18 - so always think about that can I use
304:20 - Heap in these kinds of scenarios and
304:22 - most likely the answer is going to be
304:24 - correct with that approach now second
304:26 - thing is whenever you are given a binary
304:29 - a sorted array always try to do a b
304:32 - binary search in that that type of input
304:34 - because binary search is going to save
304:36 - you lot of time because binary search
304:39 - operates on log and time meanwhile the
304:41 - regular search operates in bigo of end
304:43 - time so it makes huge difference in
304:44 - terms of performance now next step is
304:47 - let's say that whenever you are given or
304:49 - you are being asked to compute the all
304:51 - the combinations and permutations of
304:53 - given various path choices always think
304:56 - that can I use backtracking or breath
304:58 - first search in such scenarios uh why
305:01 - I'm telling you this because
305:03 - most of the time there would be
305:04 - possibilities where you need you are
305:06 - given bunch of different input paths and
305:08 - you need to pick one correct path and
305:11 - many times you need to try out different
305:13 - paths and then come back to some
305:14 - previous point and then again try
305:16 - another path so for such kind of
305:19 - problems backtracking and breath for
305:20 - search are perfect uh also one more
305:23 - thing whenever you see any question
305:25 - related to trees or graphs try to think
305:28 - about solving them using either breadth
305:29 - first search or depth first search we
305:31 - already talked talked in quite detail
305:33 - that what are the difference between
305:35 - each one of them and what are under what
305:37 - circumstances which one to choose but
305:39 - either ways you would be able to come up
305:41 - with the solution by using either breath
305:43 - for search or depth for search because
305:44 - most likely you are going to Traverse
305:46 - over the entire tree or entire graph to
305:48 - find the optimal solution you are
305:50 - looking for so always trees and graphs
305:52 - equals to BFS and DFS that's a golden
305:55 - rule next thing is whatever solution you
305:58 - try to make if you make a recursive
306:00 - solution you can all also make the same
306:03 - solution using iterative approach using
306:05 - Stacks so many times it would happen
306:08 - that during an interview you are
306:09 - discussing you are uh brainstorming you
306:11 - are coming up with the solution that is
306:13 - a Rec recursive approach your
306:15 - interviewer is for sure going to ask you
306:16 - that hey instead of recursive approach
306:18 - can I do something else and you you can
306:21 - say that yeah instead of recursion if
306:23 - you use uh an iterative approach you can
306:26 - come up to the solution using Stacks so
306:29 - that is a very powerful tool and
306:30 - basically they both achieve same kinds
306:32 - of uh results on top of it many times if
306:36 - the recursion is long enough there you
306:38 - could encounter issues such as like
306:40 - memory overflow or stack Overflow and
306:42 - things like that so that can be avoided
306:44 - if you are going to use the iterative
306:46 - approach next thing is if any problem
306:50 - related to array that you can solve
306:52 - using bigo of n Square time I repeat if
306:56 - there is any problem using an array that
306:58 - you can solve in bigo of n Square time
307:01 - you can solve the same problem using n
307:04 - log end time if you decide to go with
307:07 - the Sorting approach and you can solve
307:09 - the same problem in big off end time if
307:11 - you decide to go with either hash map or
307:13 - hash set approach and we already saw
307:16 - examples of that using two some problem
307:19 - um and many other array problems so
307:21 - always arrays and be go of n sare time
307:24 - complexity try thinking about sorting
307:26 - the array or try thinking about using a
307:28 - hash map or hash set in such kind of
307:30 - scenarios next one is is whenever you
307:33 - see that you are being asked to optimize
307:36 - or do the find the minimum or maximum
307:40 - value amongst all of the given path and
307:42 - you need to do some sort of optimization
307:44 - most of the cases you would be able to
307:46 - come up with a dynamic programming
307:48 - approach that would be able to do things
307:50 - in much faster Manner and we saw that
307:53 - using coin change example where we were
307:55 - given bunch of different coins and we
307:57 - were trying to make a particular value
307:58 - so in that case dynamic programming
308:00 - allowed us to come up with a much better
308:03 - approach and much faster approach now
308:06 - next uh Golden Rule has to deal with uh
308:09 - searching or manipulating bunch of
308:11 - different strings now we all know that
308:13 - try is a data structure very closely
308:16 - associated with strings and whenever you
308:18 - are given this kind of scenario try to
308:20 - think that if there are multiple strings
308:22 - and I wanted to do many manipulations
308:24 - can I use a try and more than likely
308:27 - that would be the correct solution uh so
308:30 - these are the nine golden rules let me
308:32 - me give you the 10th one and the 10th
308:34 - one is actually that whenever you are
308:36 - given a link list and you are explicitly
308:38 - told that you should not use any
308:41 - additional space so IE you should not
308:43 - use any additional hashmap or hash set
308:45 - or something like that to store any
308:47 - extra computation in such scenario try
308:50 - to think that in the existing link list
308:52 - can I use a fast and slow pointer method
308:55 - because that would yield us the correct
308:57 - result and we I already showed you two
308:59 - examples of that first example is
309:01 - finding the middle of the link list
309:02 - second example was that finding that
309:04 - whether a link list has a cycle or not
309:06 - and both are pretty popular and widely
309:08 - known problems amongst tech tech
309:10 - interviews so these are the most common
309:15 - approaches and rule of thumb that should
309:17 - be in the behind the back of your mind
309:19 - so always make sure that you are
309:20 - following upon
309:24 - them now let's talk about what are the
309:27 - common pitfalls to avoid because many
309:29 - times you can do everything right and
309:31 - still get it wrong during the actual
309:33 - technical interview and for that it's
309:35 - not only about your technical knowledge
309:38 - but it is you need to have a combination
309:40 - of smart communication skills uh
309:42 - optimized thinking and also being able
309:46 - to come up with the solution in the
309:48 - given limited time management so there
309:50 - are lot of things so let me tell you
309:52 - that what are some of the common
309:54 - pitfalls that you should also avoid
309:56 - during any of your interviews so number
309:58 - one is always make sure that you are
310:00 - well prepared if you're not well
310:02 - prepared you cannot blame anything on
310:04 - anyone else because if you are not
310:07 - prepared and you go to a battle you are
310:09 - for sure going to lose and missing out
310:12 - any opportunity for a technical
310:14 - interview could be a career changing
310:16 - opport opportunity that you might be
310:17 - missing out you might be uh saying
310:20 - goodbye to your favorite job or your
310:22 - favorite company so please don't do that
310:24 - always make sure that even if you are an
310:26 - experienced individual you have like 10
310:28 - years of experience still go through all
310:31 - the data structures once go through all
310:32 - the coding patterns once maybe try to
310:34 - solve two or three questions amongst
310:37 - each one of them on the lead lead code
310:39 - or hacker rank or there are a lot of
310:41 - resources available because preparation
310:43 - is half half the battle one so that is
310:45 - the number one thing I would advise
310:47 - number two
310:48 - is always try to listen to your
310:51 - interviewer first because many times
310:53 - interviewers are intentionally giving
310:55 - you a very small and very a complete
310:59 - subset of the original larger problem St
311:02 - because they wanted to check that can
311:04 - you come up with the thinking that hey
311:06 - what should we do in this kind of
311:07 - scenario or what should we do in this
311:08 - other kind of scenario are you asking
311:10 - the clarifying questions so always try
311:13 - to understand the problem fully first
311:15 - before diving deep into start going on
311:16 - and solving the problem uh because I
311:19 - have seen and I have personally
311:20 - experienced this many times that
311:22 - whenever I hear a question I would get
311:24 - too excited if I know the answer and I
311:25 - would try to start building and start
311:27 - coding and after 5 minutes the
311:29 - interviewer would be like hey did you
311:31 - think about this scenario and in those
311:33 - cases interviewers were actually
311:34 - expecting me to ask that clarifying
311:36 - question so always make sure that you do
311:39 - that number three suggestion would be
311:42 - always manage your time accordingly
311:45 - because you are only going to be given
311:47 - 45 to 60 Minutes to complete an
311:49 - interview and during this 45 to 60
311:51 - Minutes you need to maybe uh introduce
311:54 - yourself so 5 minutes go there there
311:56 - would be 5 minutes in the end uh to ask
311:59 - any questions you have so you are
312:00 - essentially your interview time breaks
312:03 - down to only 50 minutes amongst these 50
312:05 - minutes you need to understand two
312:07 - problems you need to come up with the
312:08 - solution you need to explain the
312:10 - solution you need to walk through all
312:11 - the edge cases you need to code the
312:13 - solution check whether your code has any
312:15 - errors or not run through run through it
312:17 - and uh explain the time and space
312:19 - complexity maybe explain different
312:21 - approaches and also say that why did you
312:24 - choose this or why did you choose that
312:25 - data structure answer those kinds of
312:27 - questions so there are lot of things
312:29 - that needs to happen amongst those 50
312:31 - minutes so make sure that you are not
312:33 - you are managing your time correctly and
312:35 - you know that what are the cases you
312:37 - need to do and you are you need to
312:39 - prepare for on top of that that brings
312:41 - us to our next point do not Overlook the
312:44 - edge cases try to think that what if the
312:46 - given input does not have any value what
312:48 - would be the result then what if there
312:50 - happens to be like a million values for
312:52 - this particular uh use case or this
312:54 - particular input how would your solution
312:56 - approach that is your solution scalable
312:58 - enough or not are you considering all
313:00 - the edge cases so so think about those
313:02 - things and always make sure that you are
313:04 - taking care of all the happy path plus
313:07 - edge cases and my next step is do not
313:10 - write any messy code because many times
313:13 - you would be solving these problems
313:14 - either on a page like Google Docs or
313:17 - maybe if you are in person you would be
313:19 - solving it on a on an actual physical
313:21 - whiteboard where your all your thoughts
313:24 - and your code is all over the place and
313:28 - you get lost of track and you don't
313:31 - realize that where where you have
313:32 - written what and which method or which
313:34 - class is pointing back towards which
313:36 - instance or Which object and then it
313:38 - gets confusing so you that would cost
313:40 - you your time and also it doesn't look
313:42 - good on your uh on your personality as a
313:44 - coder as
313:45 - well next step is always say what you
313:49 - are thinking because the interviewer
313:51 - doesn't like awkward silences it's okay
313:53 - that you you first ask the permission
313:55 - that hey uh can I take maybe 30 seconds
313:57 - or 1 minute to think over this problem
313:59 - that is uh fine that is acceptable but
314:03 - if you are just sitting quiet for long
314:05 - periods of quiet time then those
314:07 - uncomfortable silences doesn't look good
314:10 - on your interview and also it sometimes
314:12 - the interviewer gets the impression that
314:14 - maybe you don't know the answer or you
314:15 - are getting confused always say what you
314:18 - are thinking on top of it always reach
314:20 - out to interviewer if you are stuck at
314:22 - anywhere I have seen at lot of places uh
314:25 - where people don't raise their concerns
314:27 - and if even if they are stuck they don't
314:30 - voice that uh where they are stuck and
314:32 - what are they thinking most of the cases
314:34 - it is acceptable uh and interviewer also
314:36 - predicts that that you might get stuck
314:38 - at some place and they are there to help
314:40 - you they are it's in their interest that
314:42 - you get a job because an interviewer has
314:45 - so many things to do throughout the week
314:47 - and if they are spending 5 hours just
314:50 - speaking with five different candidates
314:51 - it is a major wastage of their time so
314:54 - they want uh they want to have a good
314:56 - candidate who have solid understanding
314:58 - and they would be more than happy to
314:59 - help you out to reach to that end goal
315:01 - but in order to do that the precursor is
315:04 - that you should be able to speak out
315:06 - loud and always mention that what is
315:08 - your thought process how are you
315:11 - processing the problem and what are you
315:12 - currently thinking what are the
315:13 - different options are coming that are
315:15 - coming to your mind so always make sure
315:17 - that you are voicing your thoughts next
315:20 - thing is at least be familiar with one
315:23 - of the programming language it could be
315:24 - Java python go JavaScript whichever you
315:27 - choose make sure that you at least have
315:29 - good command over at least one program
315:31 - programming language because you at the
315:33 - end of the day you would still have to
315:35 - write the code in any language so make
315:37 - sure that you have good understanding of
315:40 - different programming languages also
315:43 - don't become flustered or don't become
315:46 - frustrated if you make any mistakes
315:49 - let's say that I ask you a question and
315:51 - then you started solving and then you
315:53 - went on the wrong path and you went keep
315:55 - on going for five minutes before
315:56 - interviewer corrected you and then now
315:59 - you just you just feel burdened that I
316:01 - wasted 5 minutes on top of it I was not
316:05 - able to come up with the solution and
316:06 - interviewer had to point me back don't
316:08 - think about these thoughts have these
316:10 - thoughts after you done with the
316:11 - interview during the interview just stay
316:13 - focused that okay now at least you have
316:15 - the correct course now let's uh think
316:17 - about moving forward and reaching to the
316:19 - end line and finding that correct
316:22 - optimal solution so that would be the
316:24 - number one thing that your that should
316:26 - be your focus and coming up with that is
316:30 - don't give up even if you don't know the
316:32 - solution at least try to come up with a
316:34 - Brute Force approach see that what would
316:36 - be the most trivial most preliminary
316:39 - approach you can take and once you have
316:41 - the Brute Force approach then you can
316:42 - start building on top of that for that
316:45 - next block or try to see that why brute
316:48 - force is not optimal what are the things
316:51 - it is lacking are we doing a lot of
316:52 - repeated work can we use something else
316:55 - um maybe it's an array and we have to
316:56 - search it all the time in order to find
316:58 - a value can we use a hash map so things
317:01 - like this will pop up if you at least
317:02 - have the basic Brute Force solution so
317:06 - if you can't find any of the Optimal
317:08 - Solutions at least start with the brute
317:10 - force and interviewer would help you
317:12 - navigate through your thought process as
317:14 - well so that would look at at least in
317:17 - better condition that even if you did
317:19 - not knew the answer you still attempted
317:22 - and at least went as far as you got so
317:25 - that that will that can also work in
317:26 - your
317:27 - favor next thing is that do not fail to
317:32 - optimize your solution because many
317:34 - times you would think you would be
317:35 - thinking that hey my solution is the
317:36 - optimal solution and this is the best I
317:38 - can do but try to think about scenarios
317:41 - that okay can can I do better in terms
317:43 - of time complexity U maybe I'm using an
317:45 - additional space can I reduce that
317:47 - additional space and improve upon the
317:48 - space complexity ask these questions to
317:51 - yourself and also voice their answers in
317:54 - the interview towards the interviewer
317:56 - because many times let's say that you
317:58 - are building a great solution or you are
318:00 - using Dynamic program pramming and in
318:02 - order to do that you are actually using
318:04 - an additional hashmap to store all the
318:06 - results but as it turns out that you
318:08 - don't need to store all the results
318:10 - maybe you just need to store couple of
318:11 - results so rather than spending
318:14 - resources on creating an entirely new
318:16 - hashmap you can actually just have two
318:18 - variables and those variables can go
318:20 - through and solve the problem that you
318:22 - that was needed so these are the
318:24 - optimizations is what interviewer is
318:26 - looking for and if you can do that you
318:28 - would be a great candidate in everyone's
318:31 - eyes
318:31 - and the last note I would like to give
318:33 - you is after our interview is done make
318:36 - sure that within 24 to 48 hours at least
318:39 - you are sending a thank you note and
318:41 - after 3 to 5 days you are at least
318:44 - asking that hey what is the status on my
318:46 - application uh maybe they are
318:48 - interviewing other candidates and they
318:49 - haven't made the decision but still you
318:51 - do not want yourself to be uh hiding
318:54 - behind the scenes and not appear at
318:56 - least uh connect with them write a nice
318:59 - thank you email saying that hey thanks
319:00 - for the giving me the time I really like
319:03 - the interview process I learned a couple
319:04 - of new things and whatnot and it it's
319:06 - always going to look good imagine if you
319:08 - are an interviewer how would you feel if
319:10 - the candidate actually sends you a
319:11 - message and saying that thank you for
319:13 - giving me your time I find it productive
319:15 - of course it's going to bring a smile to
319:16 - your face so think about these things
319:20 - and uh I wish you best of luck in your
319:23 - Tech preparation
319:27 - Journey so now we are at the last piece
319:30 - of our our entire course and it has been
319:32 - an incredible journey for me I hope it
319:34 - has been productive for you as well now
319:36 - let me give you some of the important
319:38 - resources that would become quite handy
319:41 - whenever you are trying to prepare for
319:42 - your interviews so first resources at
319:46 - least get a lead code account I'm not
319:48 - saying that get a lead code premium
319:49 - account if you have the financial means
319:51 - for sure if you cannot still at least
319:54 - get your lead code account in line uh
319:56 - and there are some other websites like
319:58 - code Chef hacker rank geek for geeks
320:01 - they are also quite good so if you want
320:02 - you can check those out as well but in
320:04 - either case build the practice of
320:06 - solving the questions and the tech
320:08 - interview questions now first confusion
320:11 - comes in whenever you are you start
320:13 - grinding lead code is that lead code
320:15 - actually has more than 2,000 questions
320:17 - and you are not going to do all of them
320:19 - so the question comes that where can I
320:21 - find the curated list of important
320:23 - questions so I have actually created one
320:26 - list where I have listed down 125 most
320:29 - Tas most uh popular and most like
320:33 - problems from the lead code on top of
320:36 - that I have also curated the data
320:39 - associated with that which means that
320:41 - for any particular question how many
320:43 - companies have asked this that question
320:45 - and how popular that is uh also what is
320:48 - the difficulty level so I'm going to
320:50 - link that uh Google doc in the
320:53 - description as well and that is open to
320:55 - public so anyone can use it that can be
320:58 - a good starting point if you want there
321:00 - are other list available as well like
321:02 - blind 75 or need code 150 they are also
321:05 - pretty good so if you can you you can
321:08 - use that resource second resource I
321:10 - would recommend if you like to read
321:12 - stuff this is a great book that cracking
321:15 - the coding interview this has been the
321:16 - Bible for all the tech interview
321:18 - preparation so if you can buy the book
321:21 - if you cannot there are online copies
321:23 - available as well so you can read those
321:25 - uh and one last thing is don't stop
321:28 - practicing and if you can find a
321:31 - mock interview buddy or group of buddies
321:34 - or some friends or anyone there are some
321:36 - online forums available who also
321:38 - conducts mock interviews because by
321:40 - doing a mock interview number one you
321:43 - would alleviate the pressure of actually
321:44 - being in the interview this would be
321:46 - like a net practice for your actual
321:48 - technical interview preparation Journey
321:50 - uh and if you are playing the role of an
321:53 - interviewer during the mock interview
321:55 - with any of your friend you can also
321:56 - imagine that when your friend responds
321:59 - how as an interviewer you you are also
322:01 - considering or you are also thinking
322:03 - that this is what he's doing right and
322:05 - this is what he's doing wrong and you
322:07 - can build a sort of an expectation that
322:09 - how should I approach any question from
322:11 - interviewer's point of view and that
322:13 - would become that would become greatly
322:15 - advantageous um whenever you are
322:17 - actually appearing for your interviews
322:19 - also if you cannot find Solutions there
322:22 - are very good YouTube channels available
322:24 - so if you want you can go to my channel
322:26 - destination Fang or otherwise if you
322:28 - want you can also go to a channel called
322:30 - need code he is pretty good uh and there
322:32 - are channels by hacker rank that is also
322:35 - pretty good so there are lot of
322:37 - resources available there is no shortage
322:39 - of it uh the only thing is you need to
322:41 - keep on grinding and keep on preparing
322:44 - yourself now at the very last moment I
322:46 - just want to take this time and say
322:48 - thank you thank you to all of you
322:50 - because of your constant motivation I
322:53 - was able to build and make this entire
322:55 - course this is something I never thought
322:57 - I would be able to do but it has been an
322:59 - incredible experience so good luck with
323:02 - your journey and take care