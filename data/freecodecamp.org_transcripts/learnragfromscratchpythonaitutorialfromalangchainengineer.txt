00:00 - in this course Lance Martin will teach
00:02 - you how to implement rag from scratch
00:06 - Lance is a software engineer at Lang
00:08 - chain and Lang chain is one of the most
00:10 - common ways to implement rag Lance will
00:14 - help you understand how to use rag to
00:16 - combine custom data with llms hi this is
00:19 - Lance Martin I'm a software engineer at
00:21 - Lang chain I'm going to be giving a
00:23 - short course focused on rag or retrieval
00:26 - augmented generation which is one of the
00:28 - most popular kind of ideas and
00:30 - in llms
00:32 - today so really the motivation for this
00:34 - is that most of the world's data is
00:37 - private um whereas llms are trained on
00:41 - publicly available data so you can kind
00:43 - of see on the bottom on the x-axis the
00:45 - number of tokens using pre-training
00:47 - various llms so it kind of varies from
00:50 - say 1.5 trillion tokens in the case of
00:52 - smaller models like
00:54 - 52 out to some very large number that we
00:56 - actually don't know for proprietary
00:58 - models like GPT 4 CLA
01:00 - three but what's really interesting is
01:03 - that the context window or the ability
01:06 - to feed external information into these
01:08 - LMS is actually getting larger so about
01:11 - a year ago context windows were between
01:13 - 4 and 8,000 tokens you know that's like
01:16 - maybe a dozen pages of text we've
01:18 - recently seen models all the way out to
01:20 - a million tokens which is thousands of
01:22 - pages of text so while these llms are
01:24 - trained on large scale public data it's
01:28 - increasingly feasible to feed them
01:30 - this huge mass of private data that
01:33 - they've never seen that private data can
01:35 - be your kind of personal data it can be
01:37 - corporate data or you know other
01:40 - information that you want to pass to an
01:42 - LM that's not natively in his training
01:45 - set and so this is kind of the main
01:48 - motivation for rag it's really the idea
01:52 - that llms one are kind of the the center
01:54 - of a new kind of operating
01:56 - system and two it's increasingly
01:59 - critical to be able to feed information
02:01 - from external sources such as private
02:04 - data into llms for processing so that's
02:07 - kind of the overarching motivation for
02:10 - Rag and now rag refers to retrieval
02:13 - augmented generation and you can think
02:14 - of it in three very general steps
02:17 - there's a process of indexing of
02:19 - external data so you can think about
02:22 - this as you know building a database for
02:24 - example um many companies already have
02:27 - large scale databases in different forms
02:29 - they could be SQL DBS relational DBS um
02:32 - they could be Vector Stores um or
02:35 - otherwise but the point is that
02:38 - documents are indexed such that they can
02:40 - be retrieved based upon some heuristics
02:43 - relative to an input like a question and
02:45 - those relevant documents can be passed
02:47 - to an llm and the llm can produce
02:50 - answers that are grounded in that
02:52 - retrieved information so that's kind of
02:54 - the centerpiece or central idea behind
02:56 - Rag and why it's really powerful
02:58 - technology because it's really uniting
03:01 - the the knowledge and processing
03:03 - capacity of llms with large scale
03:06 - private external data source for which
03:09 - most of the important data in the world
03:11 - still
03:13 - lives and in the following short videos
03:16 - we're going to kind of build up a
03:17 - complete understanding of the rag
03:20 - landscape and we're going to be covering
03:22 - a bunch of interesting papers and
03:24 - techniques that explain kind of how to
03:25 - do rag and I've really broken it down
03:28 - into a few different sections
03:30 - so starting with a question on the left
03:32 - the first kind of section is what I call
03:35 - query trans translation so this captures
03:37 - a bunch of different methods to take a
03:39 - question from a user and modify it in
03:42 - some way to make it better suited for
03:44 - retrieval from you know one of these
03:45 - indexes we've talked
03:47 - about that can use methods like query
03:50 - writing it can be decomposing the query
03:52 - into you know constituent sub
03:55 - questions then there's a question of
03:57 - routing so taking that decomposed a
04:00 - Rewritten question and routing it to the
04:02 - right place you might have multiple
04:03 - Vector stores a relational DB graph DB
04:06 - and a vector store so it's the challenge
04:08 - of getting a question to the right
04:10 - Source then there's a there's kind of
04:12 - the challenge of query construction
04:15 - which is basically taking natural
04:16 - language and converting it into the DSL
04:19 - necessary for whatever data source you
04:21 - want to work with a classic example here
04:23 - is text a SQL which is kind of a very
04:25 - kind of well studied process but text a
04:28 - cipher for graph DV is very interesting
04:31 - text to metadata filters for Vector DBS
04:34 - is also a very big area of
04:36 - study um then there's indexing so that's
04:40 - the process of taking your documents and
04:42 - processing them in some way so they can
04:44 - be easily retrieved and there's a bunch
04:46 - of techniques for that we'll talk
04:47 - through we'll talk through different
04:48 - embedding methods we'll talk about
04:50 - different indexing
04:51 - strategies after
04:53 - retrieval there are different techniques
04:56 - to rerank or filter retrieve documents
04:59 - um and then finally we'll talk about
05:00 - generation and kind of an interesting
05:02 - new set of methods to do what we might
05:05 - call as active rag so in that retrieval
05:08 - or generation stage grade documents
05:12 - grade answers um grade for relevance to
05:15 - the question grade for faithfulness to
05:18 - the documents I.E check for
05:19 - hallucinations and if either fail
05:22 - feedback uh re- retrieve or rewrite the
05:25 - question uh regenerate the qu regenerate
05:28 - the answer and so forth so there's a
05:30 - really interesting set of methods we're
05:31 - going to talk through that cover that
05:33 - like retrieval and generation with
05:37 - feedback and you know in terms of
05:39 - General outline we'll cover the basics
05:41 - first it'll go through indexing
05:43 - retrieval and generation kind of in the
05:44 - Bare Bones and then we'll talk through
05:47 - more advanced techniques that we just
05:48 - saw on the prior slide career
05:50 - Transformations routing uh construction
05:52 - and so forth hi this is Lance from Lang
05:55 - chain this the second video in our
05:57 - series rack from scratch focused on
05:59 - indexing
06:01 - so in the past video you saw the main
06:04 - kind of overall components of rag
06:06 - pipelines indexing retrieval and
06:09 - generation and here we're going to kind
06:10 - of Deep dive on indexing and give like
06:13 - just a quick overview of it so the first
06:16 - aspect of indexing is we have some
06:18 - external documents that we actually want
06:20 - to load and put into what we're trying
06:22 - to call Retriever and the goal of this
06:25 - retriever is simply given an input
06:27 - question I want to fish out doents that
06:30 - are related to my question in some
06:32 - way now the way to establish that
06:35 - relationship or relevance or similarity
06:37 - is typically done using some kind of
06:39 - numerical representation of documents
06:42 - and the reason is that it's very easy to
06:44 - compare vectors for example of numbers
06:47 - uh relative to you know just free form
06:50 - text and so a lot of approaches have
06:53 - been a developed over the years to take
06:56 - text documents and compress them down
06:58 - into a numerical rep presentation that
07:01 - then can be very easily
07:03 - searched now there's a few ways to do
07:06 - that so Google and others came up with
07:08 - many interesting statistical methods
07:11 - where you take a document you look at
07:13 - the frequency of words and you build
07:15 - what they call sparse vectors such that
07:18 - the vector locations are you know a
07:20 - large vocabulary of possible words each
07:23 - value represents the number of
07:24 - occurrences of that particular word and
07:27 - it's sparse because there's of course
07:28 - many zeros it's a very large vocabulary
07:31 - relative to what's present in the
07:32 - document and there's very good search
07:34 - methods over this this type of numerical
07:37 - representation now a bit more recently
07:40 - uh embedding methods that are machine
07:41 - learned so you take a document and you
07:43 - build a compressed fixed length
07:45 - representation of that
07:47 - document um have been developed with
07:51 - correspondingly very strong search
07:53 - methods over
07:54 - embeddings um so the intuition here is
07:59 - that we take documents and we typically
08:01 - split them because embedding models
08:04 - actually have limited context windows so
08:07 - you know on the order of maybe 512
08:09 - tokens up to 8,000 tokens or Beyond but
08:12 - they're not infinitely large so
08:14 - documents are split and each document is
08:16 - compressed into a vector and that Vector
08:20 - captures a semantic meaning of the
08:22 - document itself the vectors are indexed
08:25 - questions can be embedded in the exactly
08:27 - same way and then numerical kind of
08:30 - comparison in some form you know using
08:33 - very different types of methods can be
08:34 - performed on these vectors to fish out
08:37 - relevant documents relative to my
08:41 - question um and let's just do a quick
08:43 - code walk through on some of these
08:46 - points so I have my notebook here I've
08:51 - installed here um now I've set a few API
08:55 - keys for lsmith which are very useful
08:58 - for tracing which we'll see
09:00 - shortly um previously I walked through
09:03 - this this kind of quick start that just
09:04 - showed overall how to lay out these rag
09:07 - pipelines and here what I'll do is I'll
09:09 - Deep dive a little bit more on indexing
09:11 - and I'm going to take a question and a
09:13 - document and first I'm just going to
09:16 - compute the number of tokens in for
09:18 - example the question and this is
09:19 - interesting because embedding models in
09:22 - llms more generally operate on tokens
09:24 - and so it's kind of nice to understand
09:27 - how large the documents are that I'm
09:28 - trying to feed in in this case it's
09:30 - obviously a very small in this case
09:33 - question now I'm going to specify open
09:35 - eye embeddings I specify an embedding
09:38 - model here and I just say embed embed
09:40 - query I can pass my question my document
09:43 - and what you can see here is that runs
09:47 - and this is mapped to now a vector of
09:49 - length 1536 and that fixed length Vector
09:53 - representation will be computed for both
09:56 - documents and really for any document so
09:58 - you're always is kind of computing this
10:00 - fix length Vector that encodes the
10:02 - semantics of the text that you've passed
10:05 - now I can do things like cosine
10:06 - similarity to compare
10:08 - them and as we'll see here I can load
10:13 - some documents this is just like we saw
10:16 - previously I can split
10:18 - them and I can index them here just like
10:22 - we did before but we can see under the
10:23 - hood really what we're doing is we're
10:25 - taking each split we're embedding it
10:27 - using open eye embeddings into this this
10:29 - kind of this Vector representation and
10:31 - that's stored with a link to the rod
10:33 - document itself in our Vector store and
10:36 - next we'll see how to actually do
10:38 - retrieval using this Vector store hi
10:41 - this is Lance from Lang chain and this
10:43 - is the third video in our series rag
10:45 - from scratch building up a lot of the
10:47 - motivations for rag uh from the very
10:50 - basic
10:51 - components um so we're going to be
10:54 - talking about retrieval today in the
10:56 - last two uh short videos I outlined
10:59 - indexing and gave kind of an overview of
11:01 - this flow which starts with indexing of
11:04 - our documents retrieval of documents
11:06 - relevant to our question and then
11:07 - generation of answers based on the
11:09 - retriev
11:11 - documents and so we saw that the
11:13 - indexing process basically makes
11:15 - documents easy to retrieve and it goes
11:18 - through a flow that basically looks like
11:20 - you take our documents you split them in
11:22 - some way into these smaller chunks that
11:25 - can be easily embedded um those
11:28 - embeddings are then numerical
11:30 - representations of those documents that
11:31 - are easily
11:32 - searchable and they're stored in an
11:35 - index when given a question that's also
11:38 - embedded the index performs a similarity
11:41 - search and returns splits that are
11:43 - relevant to the
11:45 - question now if we dig a little bit more
11:47 - under the hood we can think about it
11:49 - like this if we take a document and
11:52 - embed it let's imagine that embedding
11:54 - just had three dimensions so you know
11:56 - each document is projected into some
11:58 - point in this 3D
12:00 - space now the point is that the location
12:03 - in space is determined by the semantic
12:06 - meaning or content in that document so
12:09 - to follow that then documents in similar
12:12 - locations in space contain similar
12:15 - semantic information and this very
12:17 - simple idea is really the Cornerstone
12:19 - for a lot of search and retrieval
12:21 - methods that you'll see with modern
12:22 - Vector stores so in particular we take
12:25 - our documents we embed them into this in
12:27 - this case a toy 3D space
12:30 - we take our question do the
12:32 - same we can then do a search like a
12:35 - local neighborhood search you can think
12:37 - about in this 3D space around our
12:39 - question to say hey what documents are
12:42 - nearby and these nearby neighbors are
12:45 - then retrieved because they can they
12:47 - have similar semantics relative to our
12:50 - question and that's really what's going
12:53 - on here so again we took our documents
12:56 - we split them we embed them and now they
12:58 - exist in this high dimensional space
13:00 - we've taken our question embedded it
13:02 - projected in that same space and we just
13:04 - do a search around the question from
13:07 - nearby documents and grab ones that are
13:09 - close and we can pick some number we can
13:12 - say we want one or two or three or n
13:14 - documents close to my question in this
13:17 - embedding space and there's a lot of
13:19 - really interesting methods that
13:20 - implement this very effectively I I link
13:22 - one
13:24 - here um and we have a lot of really nice
13:28 - uh Integrations to play with this
13:30 - general idea so many different embedding
13:32 - models many different indexes lots of
13:35 - document loaders um and lots of
13:37 - Splitters that can be kind of recombined
13:39 - to test different ways of doing this
13:41 - kind of indexing or
13:43 - retrieval um so now I'll show a bit of a
13:45 - code walkth through so here we defined
13:50 - um we kind of had walked through this
13:52 - previously this is our notebook we've
13:54 - installed a few packages we've set a few
13:57 - environment variables using lsmith
14:00 - and we showed this previously this is
14:02 - just an overview showing how to run rag
14:04 - like kind of end to end in the last uh
14:07 - short talk we went through
14:09 - indexing um and what I'm going to do
14:11 - very simply is I'm just going to reload
14:14 - our
14:15 - documents so now I have our documents
14:19 - I'm going to resplit
14:21 - them and we saw before how we can build
14:23 - our
14:24 - index now here let's actually do the
14:27 - same thing but in the slide we actually
14:29 - showed kind of that notion of search in
14:31 - that 3D
14:33 - space and a nice parameter to think
14:35 - about in building your your retriever is
14:38 - K so K tells you the number of nearby
14:41 - neighbors to fetch when you do that
14:42 - retrieval process and we talked about
14:44 - you know in that 3D space do I want one
14:47 - nearby neighbor or two or three so here
14:49 - we can specify k equals 1 for example
14:53 - now we're building our index so we're
14:54 - taking every split embedding it storing
14:57 - it now what's nice is I asked a a
14:59 - question what is Task decomposition this
15:01 - is related to the blog post and I'm
15:03 - going to run get relevant documents so I
15:05 - run that and now how many documents do I
15:08 - get back I get one as expected based
15:10 - upon k equals 1 so this retrieve
15:12 - document should be related to my
15:14 - question now I can go to lsmith and we
15:17 - can open it up and we can look at our
15:19 - Retriever and we can see here was our
15:21 - question here's the one document we got
15:23 - back and okay so that makes sense this
15:27 - document pertains to task ke
15:29 - decomposition in particular and it kind
15:31 - of lays out a number of different
15:33 - approaches that can be used to do that
15:35 - this all kind of makes sense and this
15:37 - shows kind of in practice how you can
15:38 - implement this this NE this kind of KNN
15:42 - or k nearest neighbor search uh really
15:45 - easily uh just using a few lines of code
15:48 - and next we're going to talk about
15:50 - generation
15:52 - thanks hey this is Lance from Lang chain
15:55 - this is the fourth uh short video in our
15:57 - rack from scratch series
15:59 - that's going to be focused on
16:01 - generation now in the past few videos we
16:04 - walked through the general flow uh for
16:07 - kind of basic rag starting with indexing
16:10 - Fall by
16:11 - retrieval then
16:13 - generation of an answer based upon the
16:15 - documents that we retrieved that are
16:17 - relevant to our question this is kind of
16:19 - the the very basic
16:21 - flow now an important consideration in
16:25 - generation is really what's happening is
16:28 - we're taking the documents you retrieve
16:30 - and we're stuffing them into the llm
16:32 - context window so if we kind of walk
16:34 - back through the process we take
16:37 - documents we split them for convenience
16:39 - or embedding we then embed each split
16:43 - and we store that in a vector store as
16:45 - this kind of easily searchable numerical
16:47 - representation or vector and we take a
16:50 - question embed it to produce a similar
16:52 - kind of numerical representation we can
16:55 - then search for example using something
16:57 - like KN andn in this kind of dimensional
17:00 - space for documents that are similar to
17:02 - our question based on their proximity or
17:05 - location in this space in this case you
17:07 - can see 3D is a toy kind of toy
17:10 - example now we've recovered relevant
17:12 - splits to our question we pack those
17:15 - into the context window and we produce
17:17 - our
17:19 - answer now this introduces the notion of
17:22 - a prompt so the prompt is kind of a you
17:25 - can think have a placeholder that has
17:27 - for example you know in our case B keys
17:30 - so those keys can be like context and
17:33 - question so they basically are like
17:35 - buckets that we're going to take those
17:37 - retrieve documents and Slot them in
17:40 - we're going to take our question and
17:41 - also slot it in and if you kind of walk
17:44 - through this flow you can kind of see
17:45 - that we can build like a dictionary from
17:48 - our retrieve documents and from our
17:50 - question and then we can basically
17:52 - populate our prompt template with the
17:54 - values from the dict and then becomes a
17:57 - prompt value which can be passed to llm
17:59 - like a chat model resulting in chat
18:01 - messages which we then parse into a
18:03 - string and get our answer so that's like
18:06 - the basic workflow that we're going to
18:07 - see and let's just walk through that in
18:09 - code very quickly to kind of give you
18:11 - like a Hands-On intuition so we had our
18:14 - notebook we walk through previously
18:16 - install a few packages I'm setting a few
18:19 - lsmith environment variables we'll see
18:21 - it's it's nice for uh kind of observing
18:23 - and debugging our
18:25 - traces um previously we did this quick
18:27 - start we're going to skip that over
18:30 - um and what I will do is I'm going to
18:33 - build our retriever so again I'm going
18:36 - to take documents and load them uh and
18:38 - then I'm going to split them here we've
18:40 - kind of done this previously so I'll go
18:42 - through this kind of quickly and then
18:43 - we're going to embed them and store them
18:45 - in our index so now we have this
18:47 - retriever object here now I'm going to
18:50 - jump down here now here's where it's
18:51 - kind of fun this is the generation bit
18:54 - and you can see here I'm defining
18:55 - something new this is a prompt template
18:58 - and what my prompt template is something
18:59 - really simple it's just going to say
19:00 - answer the following question based on
19:02 - this context it's going to have this
19:04 - context variable and a question so now
19:06 - I'm building my prompt so great now I
19:08 - have this prompt let's define an llm
19:11 - I'll choose
19:12 - 35 now this introdu the notion of a
19:15 - chain so in Lang chain we have an
19:17 - expression language called L Cel Lang
19:20 - chain expression language which lets you
19:22 - really easily compose things like
19:24 - prompts LMS parsers retrievers and other
19:27 - things but the very simple kind of you
19:30 - know example here is just let's just
19:32 - take our prompt which you defined right
19:33 - here and connect it to an LM which you
19:35 - defined right here into this chain so
19:37 - there's our chain now all we're doing is
19:39 - we're invoking that chain so every L
19:42 - expression language chain has a few
19:44 - common methods like invoke bat stream in
19:47 - this case we just invoke it with a dict
19:50 - so context and question that maps to the
19:54 - expected Keys here in our template
19:58 - and so if we run invoke what we see is
20:01 - it's just going to execute that chain
20:02 - and we get our answer now if we zoom
20:05 - over to Langs Smith we should see that
20:07 - it's been populated so yeah we see a
20:09 - very simple runable
20:11 - sequence here was our
20:14 - document um and here's our output and
20:18 - here is our prompt answer the following
20:21 - question based on the context here's the
20:24 - document we passed in here is the
20:26 - question and then we get our answer so
20:28 - that's pretty nice um now there's a lot
20:32 - of other options for rag prompts I'll
20:34 - pull one in from our prompt tub this
20:36 - one's like kind of a popular prompt so
20:39 - it just like has a little bit more
20:41 - detail but you know it's the main the
20:43 - main intuition is the same um you're
20:47 - passing in documents you're asking them
20:48 - to reason about the documents given a
20:50 - question produce an answer and now here
20:53 - I'm going to find a rag chain which will
20:55 - automatically do the retrieval for us
20:57 - and all I have to do is specify here's
20:59 - my retriever which we defined
21:01 - before here's our question we which we
21:03 - invoke with the question gets passed
21:06 - through to the key question in our dict
21:10 - and it automatically will trigger the
21:12 - retriever which will return documents
21:14 - which get passed into our context so
21:16 - it's exactly what we did up here except
21:18 - before we did this
21:20 - manually and
21:22 - now um this is all kind of automated for
21:25 - us we pass that dick which is autop
21:28 - populated
21:29 - into our prompt llm out to parser now
21:31 - let invoke it and that should all just
21:34 - run and great we get an answer and we
21:37 - can look at the
21:39 - trace and we can see everything that
21:41 - happened so we can see our retriever was
21:44 - run these documents were
21:46 - retrieved they get passed into our
21:49 - LM and we get our final answer so this
21:53 - kind of the end of our overview um where
21:56 - we talked about I'll go back to the
21:58 - slide here quickly we talked about
22:00 - indexing retrieval and now
22:02 - generation and follow-up short videos
22:04 - we'll kind of dig into some of the more
22:06 - com complex or detailed themes that
22:09 - address some limitations that can arise
22:11 - in this very simple pipeline
22:14 - thanks hi my from Lang chain over the
22:17 - next few videos we're going to be
22:18 - talking about career
22:20 - translation um and in this first video
22:23 - we're going to cover the topic of
22:24 - multi-query
22:26 - so query translation sits kind of at the
22:29 - first stage of an advanced rag Pipeline
22:34 - and the goal of career translation is
22:36 - really to take an input user question
22:39 - and to translate in some way in order to
22:42 - improve
22:44 - retrieval so the problem statement is
22:47 - pretty intuitive user queries um can be
22:51 - ambiguous and if the query is poorly
22:53 - written because we're typically doing
22:56 - some kind of semantic similarity search
22:57 - between the query and our documents if
23:01 - the query is poorly written or ill
23:02 - opposed we won't retrieve the proper
23:05 - documents from our
23:07 - index so there's a few approaches to
23:10 - attack this problem and you can kind of
23:13 - group them in a few different ways so
23:15 - here's one way I like to think about it
23:17 - a few approaches has involveed query
23:19 - rewriting so taking a query and
23:22 - reframing it like writing from a
23:23 - different perspective um and that's what
23:26 - we're going to talk about a little bit
23:27 - here in depth using approaches like
23:29 - multi-query or rag Fusion which we'll
23:32 - talk about in the next video you can
23:34 - also do things like take a question and
23:36 - break it down to make it less abstract
23:38 - like into sub questions and there's a
23:40 - bunch of interesting papers focused on
23:42 - that like least to most from
23:44 - Google you can also take the opposite
23:46 - approach of take a question to make it
23:48 - more abstract uh and there's actually
23:50 - approach we're going to talk about later
23:51 - in a future video called stepback
23:53 - prompting that focuses on like kind of
23:56 - higher a higher level question from the
23:59 - input so the intuition though for this
24:03 - multier approach is we're taking a
24:05 - question and we're going to break it
24:06 - down into a few differently worded
24:08 - questions uh from different
24:10 - perspectives and the intuition here is
24:13 - simply that um it is possible that the
24:17 - way a question is initially worded once
24:21 - embedded it is not well aligned or in
24:24 - close proximity in this High dimensional
24:26 - embedding space to a document that we
24:27 - want to R that's actually related so the
24:30 - thinking is that by kind of rewriting it
24:32 - in a few different ways you actually
24:34 - increase the likel of actually
24:36 - retrieving the document that you really
24:37 - want to um because of nuances in the way
24:41 - that documents and questions are
24:43 - embedded this kind of more shotgun
24:46 - approach of taking a question Fanning it
24:47 - out into a few different perspectives
24:50 - May improve and increase the reliability
24:51 - of retrieval that's like the intuition
24:54 - really um and of course we can com
24:56 - combine this with retrieval so we can
24:59 - take our our kind of fan out questions
25:01 - do retrieval on each one and combine
25:03 - them in some way and perform rag so
25:06 - that's kind of the overview and now
25:07 - let's what let's go over to um our code
25:11 - so this is a notebook and we're going to
25:13 - share all
25:14 - this um we're just installing a few
25:18 - packages we're setting a lsmith API Keys
25:21 - which we'll see why that's quite useful
25:22 - here shortly there's our diagram now
25:25 - first I'm going to Index this blog post
25:27 - on agents I'm going to split it um well
25:31 - I'm going to load it I'm going to split
25:32 - it and then I'm going to index it in
25:34 - chroma locally so this is a vector store
25:37 - we've done this previously so now I have
25:38 - my index defined so here is where I'm
25:41 - defining my prompt for multiquery which
25:44 - is your your assistant your task is to
25:47 - basically reframe this question into a
25:48 - few different sub
25:50 - questions um so there's our
25:53 - prompt um right here we'll pass that to
25:56 - an llm part it um into a string and then
26:01 - split the string by new lines and so
26:02 - we'll get a list of questions out of
26:04 - this chain that's really all we're doing
26:06 - here now all we're doing is here's a
26:09 - sample input question there's our
26:11 - generate queries chain which we defined
26:13 - we're going to take that list and then
26:15 - simply apply each question to retriever
26:19 - so we'll do retrieval per question and
26:21 - this little function here is just going
26:22 - to take the unique Union of documents uh
26:25 - across all those retrievals so let's run
26:27 - this and see what happens so we're going
26:29 - to run this and we're going to get some
26:32 - set of questions uh or documents back so
26:36 - let's go to Langs Smith now we can
26:37 - actually see what happened under the
26:39 - hood so here's the key
26:42 - point we ran our initial chain to
26:44 - generate a set of of reframed questions
26:48 - from our input and here was that prompt
26:51 - and here is that set of questions that
26:52 - we generated now what happened is for
26:55 - every one of those questions we did an
26:57 - independent retrieval that's what we're
26:58 - showing here so that's kind of the first
27:00 - step which is great now I can go back to
27:03 - the notebook and we can show this
27:05 - working end to end so now we're going to
27:07 - take that retrieval chain we'll pass it
27:09 - into context of our final rag prompt
27:12 - we'll also pass through the question
27:14 - we'll pass that to our rag prompt here
27:16 - pass it to an LM and then Pary output
27:19 - now let's let's kind of see how that
27:21 - works so again that's okay there it is
27:24 - so let's actually go into langth and see
27:26 - what happened under the hood so this was
27:28 - our final chain so this is great we took
27:31 - our input question we broke it out to
27:33 - these like five rephrase questions for
27:36 - every one of those we did a retrieval
27:38 - that's all great we then took the unique
27:41 - Union of documents and you can see in
27:42 - our final llm prompt answer the
27:45 - following cont following question based
27:46 - on the context this is the final set of
27:50 - unique documents that we retrieved from
27:52 - all of our sub
27:54 - questions um here's our initial question
27:57 - there's our answer so that kind of shows
27:59 - you how you can set this up really
28:00 - easily how you can use l Smith to kind
28:02 - of investigate what's going on and in
28:04 - particular use l Smith to investigate
28:06 - those intermediate questions that you
28:08 - generate in that like kind of question
28:11 - generation phase and in a future talks
28:13 - we're going to go through um some of
28:15 - these other methods that we kind of
28:17 - introduced at the start of this one
28:18 - thank
28:20 - you last L chain this is the second
28:23 - video of our Deep dive on query
28:25 - translation in our rag from scratch
28:27 - series focused on a method called rag
28:30 - Fusion so as we kind of showed before
28:33 - career translation you can think of as
28:35 - the first stage in an advanced rag
28:38 - pipeline we're taking an input user
28:40 - question and We're translating it some
28:42 - way in order to improve
28:44 - retrievable now we showed this General
28:48 - mapping of approaches previously so
28:50 - again you have kind of like rewriting so
28:52 - you can take a question and like kind of
28:54 - break it down into uh differently worded
28:58 - are different different perspectives of
29:00 - the same question so that's kind of
29:02 - rewriting there's sub questions where
29:04 - you take a question break it down into
29:06 - smaller problems solve each one
29:08 - independently and then there step back
29:10 - where you take a question and kind of go
29:11 - more abstract where you kind of ask a
29:14 - higher level question as a precondition
29:16 - to answer the user question so those are
29:18 - the approaches and we're going to dig
29:20 - into one of the particular approaches
29:22 - for rewriting called rat Fusion now this
29:25 - is really similar to what we just saw
29:27 - with multiquery
29:28 - the difference being we actually apply a
29:31 - a kind of a clever rank ranking step of
29:33 - our retriev documents um which you call
29:36 - reciprocal rank Fusion that's really the
29:38 - only difference the the input stage of
29:41 - taking a question breaking it out into a
29:44 - few kind of differently worded questions
29:47 - retrieval on each one is all the same
29:50 - and we're going to see that in the code
29:51 - here shortly so let's just hop over
29:54 - there and then look at this so again
29:56 - here is a notebook that we introduced
29:59 - previously here's the packages we've
30:01 - installed we've set a few API keys for
30:04 - lsmith which we see why is quite
30:07 - useful um and you can kind of go down
30:11 - here to a rag Fusion
30:13 - section and the first thing you'll note
30:16 - is what our prompt is so it looks really
30:18 - similar to The Prompt we just saw with
30:19 - multiquery and simply your helpful
30:22 - assistant that generates multiple search
30:23 - queries based upon user input and here's
30:26 - the question output for queries so let's
30:30 - define our prompt and here was our query
30:33 - Generation chain again this looks a lot
30:34 - like we just saw we take our prompt Plum
30:37 - that into an llm and then basically
30:39 - parse by new lines and that'll basically
30:42 - split out these questions into a list
30:46 - that's all it's going to happen here so
30:47 - that's cool now here's where the novelty
30:51 - comes
30:52 - in each time we do retrieval from one of
30:56 - those questions we're going to get back
30:58 - a list of documents from our Retriever
31:01 - and so we do it over that we generate
31:03 - four questions here based on our prompt
31:06 - we do the over four questions well like
31:07 - a list of lists
31:09 - basically now reciprocal rank Fusion is
31:12 - really well suited for this exact
31:13 - problem we want to take this list to
31:15 - list and build a single Consolidated
31:17 - list and really all that's going on is
31:20 - it's looking at the documents in each
31:22 - list and kind of aggregating them into a
31:24 - final output ranking um and that's
31:28 - really the intuition around what's
31:29 - happening
31:33 - here um so let's go ahead
31:39 - and so
31:46 - let's so let's go ahead and look at that
31:49 - in some detail so we can see we
31:53 - run
31:55 - retrieval that's great now let's go over
31:57 - to Lang Smith and have a look at what's
32:00 - going on here so we can see that here
32:05 - was our prompt to your helpful assistant
32:06 - that generates multiple search queries
32:08 - based on a single input and here is our
32:10 - search queries and then here are our
32:14 - four retrievals so that's that's really
32:16 - good so we know that all is
32:19 - working um and then those retrievals
32:23 - simply went into this rank
32:25 - function and our correspondingly ranked
32:29 - to a final list of six unique rank
32:31 - documents that's really all we
32:34 - did so let's actually put that all
32:37 - together into an a full rag chain that's
32:41 - going to run
32:43 - retrieval return that final list of rank
32:47 - documents and pass it to our context
32:50 - pass through our question send that to a
32:53 - rag prompt pass it to an LM parse it to
32:56 - an output and let's run all that
32:58 - together and see that
33:01 - working cool so there's our final
33:07 - answer now let's have a look in lsmith
33:10 - we can see here was our four questions
33:13 - here's our retrievals and then our final
33:15 - rag prompt plumed through the final list
33:18 - of ranked six questions which we can see
33:22 - laid out here and our final answer so
33:25 - this can be really convenient
33:26 - particularly if we're operating across
33:29 - like maybe different Vector stores uh or
33:32 - we want to do like retrieval across a
33:34 - large number of of kind of differently
33:36 - worded questions this reciprocal rank
33:38 - Fusion step is really nice um for
33:41 - example if we wanted to only take the
33:43 - top three documents or something um it
33:46 - can be really nice to build that
33:47 - Consolidated ranking across all these
33:49 - independent retrievals then pass that to
33:52 - for the final generation so that's
33:54 - really the intuition about what's
33:55 - happening here thanks
33:58 - hi this is Lance from Lang chain this is
34:00 - our third video focused on query
34:02 - translation in the rag from scratch
34:04 - series and we're going to be talking
34:06 - about
34:07 - decomposition so query translation in
34:09 - general is a set of approaches that sits
34:11 - kind of towards the front of this
34:13 - overall rag Pipeline and the objective
34:16 - is to modify or rewrite or otherwise
34:18 - decompose an input question from a user
34:21 - in order improve
34:24 - retrieval so we can talk through some of
34:26 - these approaches previously in
34:28 - particular various ways to do query
34:30 - writing like rag fusion and multiquery
34:32 - there's a separate set of techniques
34:34 - that become pretty popular and are
34:35 - really interesting for certain problems
34:37 - which we might call like kind of
34:39 - breaking down or decomposing an input
34:41 - question into a set of sub
34:43 - questions um so some of the papers here
34:46 - that are are pretty cool are for example
34:49 - this work from
34:50 - Google um and the objective really is
34:54 - first to take an input question and
34:57 - decompose it into a set of sub problems
35:00 - so this particular example from the
35:02 - paper was the problem of um last letter
35:07 - concatenation and so it took the inut
35:10 - question of three words think machine
35:13 - learning and broke it down into three
35:15 - sub problems think think machine think
35:17 - machine learning as the third sub
35:19 - problem and then you can see in this
35:20 - bottom panel it solves each one
35:23 - individually so it shows for example in
35:25 - green solving the problem think machine
35:28 - where you can catenate the last letter
35:30 - of k with the last letter of machine or
35:32 - last letter think K less machine e can
35:36 - concatenate those to K and then for the
35:39 - overall problem taking that solution and
35:43 - then and basically building on it to get
35:45 - the overall solution of keg so that's
35:47 - kind of one concept of decomposing into
35:50 - sub problems solving them
35:52 - sequentially now a related work called
35:55 - IRC or in leap retrieval combines
35:58 - retrieval with Chain of Thought
36:01 - reasoning and so you can kind of put
36:03 - these together into one approach which
36:05 - you can think of as kind of dynamically
36:08 - retrieval um to solve a set of sub
36:11 - problems kind of that retrieval kind of
36:13 - interleaving with Chain of Thought as
36:15 - noted in the second paper and a set of
36:19 - decomposed questions based on your
36:21 - initial question from the first work
36:23 - from Google so really the idea here is
36:25 - we're taking one sub question we're
36:28 - answering it we're taking that answer
36:29 - and using it to help answer the second
36:31 - sub question and so forth so let's
36:34 - actually just walk through this in code
36:35 - to show how this might
36:37 - work so this is The Notebook we've been
36:40 - working with from some of the other uh
36:42 - videos you can see we already have a
36:43 - retriever to find uh up here at the top
36:47 - and what we're going to do
36:49 - is we're first going to find a prompt
36:52 - that's basically going to say given an
36:54 - input question let's break it down to
36:57 - set of sub problems or sub question
36:59 - which can be solved individually so we
37:01 - can do that and this blog post is
37:03 - focused on agents so let's ask a
37:05 - question about what are the main
37:06 - components of an LM powerered autonomous
37:08 - agent system so let's run this and
37:13 - see what the decomposed questions are so
37:16 - you can see the decomposed questions are
37:18 - what is LM technology how does it work
37:21 - um what are components and then how the
37:23 - components interact so it's kind of a
37:25 - sane way to kind of break down this
37:27 - problem into a few sub problems which
37:28 - you might attack individually now here's
37:32 - where um we Define a prompt that very
37:35 - simply is going to take our question
37:38 - we'll take any prior questions we've
37:40 - answered and we'll take our retrieval
37:42 - and basically just combine them and we
37:45 - can Define this very simple
37:47 - chain um actually let's go back and make
37:49 - sure retriever is defined up at the
37:52 - top so now we are building our
37:56 - retriever good we have that now so we
37:59 - can go back down here and let's run this
38:02 - so
38:03 - now we are
38:06 - running and what's happening is we're
38:10 - trying to solve each of these questions
38:12 - individually using retrieval and using
38:15 - any prior question answers so okay very
38:18 - good looks like that's been done and we
38:20 - can see here's our answer now let's go
38:23 - over to langth and actually see what
38:25 - happened under the hood so here's what's
38:27 - kind of of interesting and helpful to
38:28 - see for the first question so here's our
38:31 - first one it looks like it just does
38:33 - retrieval which is we expect and then it
38:36 - uses that to answer this initial
38:37 - question now for the second question
38:40 - should be a little bit more interesting
38:42 - because if you look at our prompt here's
38:44 - our question now here is our background
38:47 - available question answer pair so this
38:49 - was the answer question answer pair from
38:51 - the first question which we add to our
38:52 - prompt and then here's the retrieval for
38:55 - this particular question so we're kind
38:56 - of building up up the solution because
38:58 - we're pending the question answer pair
39:00 - from question one and then likewise with
39:03 - question three it should combine all of
39:05 - that so we can look at here here's our
39:08 - question here's question one here's
39:10 - question two great now here's additional
39:14 - retrieval related to this particular
39:16 - question and we get our final answer so
39:18 - that's like a really nice way you can
39:20 - kind of build up Solutions um using this
39:23 - kind of
39:24 - interleaved uh retrieval and
39:26 - concatenating question answer pairs I do
39:29 - want to mention very briefly that we can
39:31 - also take a different approach where we
39:33 - can just answer these all individually
39:36 - and then just concatenate all those
39:37 - answers to produce a final answer and
39:39 - I'll show that really quickly here um
39:43 - it's like a little bit less interesting
39:44 - maybe because you're not using answers
39:46 - from each uh question to inform the next
39:50 - one you're just answering them all in
39:51 - parallel this might be better for cases
39:53 - where it's not really like a sub
39:55 - question decomposition but maybe it's
39:57 - like like a set of set of several in
40:00 - independent questions whose answers
40:02 - don't depend on each other that might be
40:04 - relevant for some
40:05 - problems um and we can go ahead and run
40:08 - okay so this ran as well we can look at
40:11 - our
40:12 - trace and in this
40:14 - case um yeah we can see that this
40:16 - actually just kind of concatenates all
40:19 - of our QA pairs to produce the final
40:22 - answer so this gives you a sense for how
40:24 - you can use quer decomposition employ
40:27 - IDE IDE from uh from two different
40:28 - papers that are pretty cool thanks hi
40:32 - this is Lance from Lang chain this is
40:34 - the fourth video uh in our Deep dive on
40:37 - queer translation in the rag from
40:39 - scratch series and we're going to be
40:40 - focused on step back
40:42 - prompting so queer translation as we
40:45 - said in some of the prior videos kind of
40:48 - sits at the the kind of first stage of
40:51 - kind of a a a rag pipeline or flow and
40:56 - the main aim is to take an question and
40:58 - to translate it or modify in such a way
41:00 - that it improves
41:02 - retrieval now we talked through a few
41:04 - different ways to approach this problem
41:07 - so one General approach involves
41:08 - rewriting a question and we talk about
41:10 - two ways to do that rag fusion
41:12 - multiquery and again this is this is
41:14 - really about taking a question and
41:16 - modifying it to capture a few different
41:19 - perspectives um which may improve the
41:21 - retrieval
41:22 - process now another approach is to take
41:25 - a question and kind of make it less
41:26 - abstract like break it down into sub
41:29 - questions um and then solve each of
41:31 - those independently so that's what we
41:32 - saw with like least to most prompting um
41:35 - and a bunch of other variants kind of in
41:37 - that in that vein of sub problem solving
41:42 - and then consolidating those Solutions
41:44 - into a final
41:45 - answer now a different approach
41:48 - presented um by again Google as well is
41:52 - stepback prompting so stepback prompting
41:55 - kind of takes the the the opposite
41:57 - approach where it tries to ask a more
42:00 - abstract question so the paper talks a
42:04 - lot
42:05 - about um using F shot
42:09 - prompting to produce what they call the
42:12 - stepback or more abstract questions and
42:15 - the way it does it is it provides a
42:17 - number of
42:19 - examples of stepb back questions given
42:22 - your original question so like this is
42:24 - like this is for example they like for
42:26 - prompt temp
42:27 - you're an expert World Knowledge I asked
42:29 - you a question your response should be
42:30 - comprehensive not contradict with the
42:32 - following um and this is kind of where
42:35 - you provide your like original and then
42:38 - step back so here's like some example um
42:43 - questions so like um
42:46 - like uh at year saw the creation of the
42:50 - region where the country is located
42:53 - which region of the
42:54 - country um is the county of of herir
42:58 - related um Janell was born in what
43:01 - country what is janell's personal
43:04 - history so that that's maybe a more
43:05 - intuitive example so it's like you ask a
43:07 - very specific question about like the
43:09 - country someone's born the more abstract
43:11 - question is like just give me the
43:12 - general history of this individual
43:14 - without worrying about that particular
43:16 - um more specific question um so let's
43:21 - actually just walk through how this can
43:22 - be done in practice um so again here's
43:25 - kind of like a a diagram of uh the
43:28 - various approaches um from less
43:31 - abstraction to more
43:32 - abstraction now here is where we're
43:35 - formulating our prompt using a few of
43:38 - the few shot examples from the
43:40 - paper um so again like input um yeah
43:45 - something about like the police perform
43:46 - wful arrests and what what camp members
43:48 - of the police do so like it it basically
43:51 - gives the model a few examples um we
43:54 - basically formulate this into a prompt
43:56 - that's really all going on here again we
43:58 - we repeat um this overall prompt which
44:02 - we saw from the paper your expert World
44:04 - Knowledge your test is to step back and
44:06 - paraphrase a question generate more a
44:08 - generic step back question which is
44:10 - easier to answer here are some examples
44:12 - so it's like a very intuitive prompt so
44:16 - okay let's start with the question what
44:18 - is Task composition for llm agents and
44:21 - we're going to say generate stack
44:23 - question okay so this is pretty
44:24 - intuitive right what is a process of
44:26 - task compos I so like not worrying as
44:28 - much about agents but what is that
44:30 - process of task composition in general
44:33 - and then hopefully that can be
44:36 - independently um retrieved we we can
44:39 - independently retrieve documents related
44:41 - to the stepb back question and in
44:43 - addition retrieve documents related to
44:45 - the the actual question and combine
44:47 - those to produce kind of final answer so
44:49 - that's really all that's going on um and
44:52 - here's the response template where we're
44:53 - Plumbing in the stepback context and our
44:57 - question context and so what we're going
45:00 - to do here is we're going to take our
45:01 - input question and perform retrieval on
45:04 - that we're also going to generate our
45:06 - stepb back question and perform
45:07 - retrieval on that we're going to plumb
45:10 - those into the prompt as here's our very
45:13 - here's our basically uh our prompt Keys
45:15 - normal question step back question um
45:19 - and our overall question again we
45:21 - formulate those as a dict we Plum those
45:23 - into our response prompt um and then we
45:29 - go ahead and attempt to answer our
45:31 - overall question so we're going to run
45:33 - that that's
45:35 - running and okay we have our answer now
45:38 - I want to hop over to Langs Smith and
45:41 - attempt to show you um kind of what that
45:45 - looked like under the hood so let's see
45:48 - let's like go into each of these steps
45:51 - so here was our prompt right you're an
45:53 - expert World Knowledge your test to to
45:55 - step back and paraph as a question
45:58 - um so um here were our few shot prompts
46:03 - and this was our this was our uh stepb
46:06 - question so what is the process of task
46:08 - composition um good from the input what
46:12 - is Tas composition for LM agents we
46:14 - perform retrieval on both what is
46:16 - process
46:17 - composition uh and what is for LM agents
46:20 - we perform both retrievals we then
46:23 - populate our prompt with both
46:27 - uh original question answer and then
46:29 - here's the context retrieve from both
46:31 - the question and the stepb back question
46:33 - here was our final answer so again this
46:35 - is kind of a nice technique um probably
46:38 - depends on a lot of the types of like
46:41 - the type of domain you want to perform
46:43 - retrieval on um but in some domains
46:47 - where for example there's a lot of kind
46:48 - of conceptual knowledge that underpins
46:52 - questions you expect users to ask this
46:54 - stepback approach could be really
46:55 - convenient to automatically formulate a
46:59 - higher level question um to for example
47:02 - try to improve retrieval I can imagine
47:04 - if you're working with like kind of
47:05 - textbooks or like technical
47:07 - documentation where you make independent
47:10 - chapters focused on more highlevel kind
47:12 - of like Concepts and then other chapters
47:15 - on like more detailed uh like
47:18 - implementations this kind of like stepb
47:19 - back approach and independent retrieval
47:22 - could be really helpful thanks hi this
47:25 - is Lance from Lang chain
47:27 - this is the fifth video focused on queer
47:29 - translation in our rack from scratch
47:31 - series we're going to be talking about a
47:33 - technique called
47:34 - hide so again queer translation sits
47:37 - kind of at the front of the overall rag
47:39 - flow um and the objective is to take an
47:41 - input question and translate it in some
47:43 - way that improves
47:46 - retrieval now hide is an interesting
47:49 - approach that takes advantage of a very
47:51 - simple idea the basic rag flow takes a
47:55 - question and embeds it takes a document
47:57 - and embeds it and looks for similarity
48:00 - between an embedded document and
48:02 - embedded question but questions and
48:04 - documents are very different text
48:06 - objects so documents can be like very
48:08 - large chunks taken from dense um
48:11 - Publications or other sources whereas
48:14 - questions are short kind of tur
48:16 - potentially ill worded from users and
48:19 - the intuition behind hide is take
48:22 - questions and map them into document
48:25 - space using a hypothetical document or
48:28 - by generating a hypothetical document um
48:31 - that's the basic intuition and the idea
48:34 - kind of shown here visually is that in
48:36 - principle for certain cases a
48:38 - hypothetical document is closer to a
48:41 - desired document you actually want to
48:42 - retrieve in this you know High
48:45 - dimensional embedding space than the
48:47 - sparse raw input question itself so
48:50 - again it's just kind of means of trans
48:52 - translating raw questions into these
48:54 - hypothetical documents that are better
48:57 - suited for
48:58 - retrieval so let's actually do a Code
49:00 - walkthrough to see how this works and
49:02 - it's actually pretty easy to implement
49:04 - which is really nice so first we're just
49:06 - starting with a prompt and we're using
49:08 - the same notebook that we've used for
49:09 - prior videos we have a blog post on
49:12 - agents r index um so what we're going to
49:15 - do is Define a prompt to generate a
49:17 - hypothetical documents in this case
49:19 - we'll say write a write a paper passage
49:22 - uh to answer a given question so let's
49:25 - just run this and see what happens again
49:26 - we're taking our prompt piping it to to
49:29 - open Ai chck gpte and then using string
49:32 - Opa parer and so here's a hypothetical
49:34 - document section related to our question
49:38 - okay and this is derived of course lm's
49:40 - kind of embedded uh kind of World
49:43 - Knowledge which is you know a sane place
49:45 - to generate hypothetical documents now
49:48 - let's now take that hypothetical
49:50 - document and basically we're going to
49:52 - pipe that into a retriever so this means
49:54 - we're going to fetch documents from our
49:56 - index related to this hypothetical
49:59 - document that's been embedded and you
50:01 - can see we get a few qu a few retrieved
50:04 - uh chunks that are related to uh this
50:08 - hypothetical document that's all we've
50:11 - done um and then let's take the final
50:15 - step where we take those retrieve
50:17 - documents here which we
50:19 - defined and our question we're going to
50:22 - pipe that into this rag prompt and then
50:25 - we're going to run our kind of rag chain
50:27 - right here which you've seen before and
50:29 - we get our answer so that's really it we
50:32 - can go to lsmith and we can actually
50:33 - look at what happened um so here for
50:37 - example this was our final um rag prompt
50:43 - answer the following question based on
50:44 - this context and here is the retrieve
50:47 - documents that we passed in so that
50:48 - part's kind of straightforward we can
50:50 - also look at um okay this is our
50:55 - retrieval okay now this is this is
50:57 - actually what we we generated a
51:01 - hypothetical document here um okay so
51:05 - this is our hypothetical document so
51:08 - we've run chat open AI we generated this
51:10 - passage with our hypothetical document
51:12 - and then we've run
51:13 - retrieval here so this is basically
51:15 - showing hypothetical document generation
51:17 - followed by retrieval um so again here
51:21 - was our passage which we passed in and
51:24 - then here's our retrieve documents from
51:25 - the retriever which are related to the
51:27 - passage content so again in this
51:30 - particular index case it's possible that
51:32 - the input question was sufficient to
51:34 - retrieve these documents in fact given
51:36 - prior examples uh I know that some of
51:38 - these same documents are indeed
51:39 - retrieved just from the raw question but
51:42 - in other context it may not be the case
51:44 - so folks have reported nice performance
51:46 - using Hyde uh for certain domains and
51:49 - the Really convenient thing is that you
51:52 - can take this this document generation
51:55 - prompt you can tune this arbitrarily for
51:57 - your domain of Interest so it's
51:59 - absolutely worth experimenting with it's
52:01 - a it's a need approach uh that can
52:03 - overcome some of the challenges with
52:04 - retrieval uh thanks very much hi this is
52:08 - Lance from Lang chain this is the 10th
52:10 - video in our rack from scratch series
52:12 - focused on
52:13 - routing so we talk through query
52:16 - translation which is the process of
52:17 - taking a question and translating in
52:19 - some way it could be decomposing it
52:21 - using stepback prompting or otherwise
52:24 - but the idea here was take our question
52:26 - change it into a form that's better
52:27 - suited for retrieval now routing is the
52:30 - next step which is basically routing
52:32 - that potentially decomposed question to
52:35 - the right source and in many cases that
52:37 - could be a different database so let's
52:38 - say in this toy example we have a vector
52:40 - store a relational DB and a graph DB the
52:43 - what we redo with routing is we simply
52:45 - route the question based upon the cont
52:47 - of the question to the relevant data
52:50 - source so there's a few different ways
52:52 - to do that one is what we call logical
52:54 - routing in this case we basically give
52:56 - an llm knowledge of the various data
52:59 - sources that we have at our disposal and
53:02 - we let the llm kind of Reason about
53:04 - which one to apply the question to so
53:07 - it's kind of like the the LM is applying
53:09 - some logic to determine you which which
53:11 - data sour for example to to use
53:13 - alternatively you can use semantic
53:15 - routing which is where we take a
53:17 - question we embed it and for example we
53:19 - embed prompts we then compute the
53:22 - similarity between our question and
53:24 - those prompts and then we choose a
53:27 - prompt based upon the similarity so the
53:29 - general idea is in our diagram we talk
53:31 - about routing to for example a different
53:33 - database but it can be very general can
53:35 - be routing to different prompt it can be
53:37 - you know really arbitrarily taking this
53:40 - question and sending it at different
53:41 - places be at different prompts be at
53:43 - different Vector
53:44 - stores so let's walk through the code a
53:46 - little bit so you can see just like
53:48 - before we've done a few pip installs we
53:51 - set up lsmith and let's talk through uh
53:54 - logical routing first so so in this toy
53:57 - example let's say we had for example uh
54:00 - three different docs like we had python
54:02 - docs we had JS docs we had goang docs
54:05 - what we want to do is take a question
54:07 - route it to one of those three so what
54:10 - we're actually doing is we're setting up
54:11 - a data model which is basically going to
54:15 - U be bound to our llm and allow the llm
54:19 - to Output one of these three options as
54:23 - a structured object so you really think
54:26 - about this as like
54:27 - classification classification plus
54:29 - function calling to produce a structured
54:31 - output which is constrained to these
54:33 - three
54:34 - possibilities so the way we do that is
54:37 - let's just zoom in here a little bit we
54:39 - can Define like a structured object that
54:41 - we want to get out from our llm like in
54:44 - this case we want for example you know
54:46 - one of these three data sources to be
54:49 - output we can take this and we can
54:52 - actually convert it into open like open
54:54 - for example function schema
54:57 - and then we actually pass that in and
54:58 - bind it to our llm so what happens is we
55:01 - ask a question our llm invokes this
55:04 - function on the output to produce an
55:07 - output that adheres to the schema that
55:09 - we specify so in this case for example
55:12 - um we output like you know in this toy
55:15 - example let's say we wanted like you
55:17 - know an output to be data source Vector
55:19 - store or SQL database the output will
55:22 - contain a data source object and it'll
55:23 - be you know one of the options we
55:25 - specify as a Json string we also
55:28 - instantiate a parser from this object to
55:32 - parse that Json string to an output like
55:35 - a pantic object for example so that's
55:37 - just one toy example and let's show one
55:39 - up here so in this case again we had our
55:41 - three doc sources um we bind that to our
55:46 - llm so you can see we do with structured
55:49 - output basically under the hood that's
55:51 - taking that object definition turning
55:54 - into function schema and binding that
55:55 - function schema to our llm and we call
55:58 - our prompt you're an expert at routing a
56:00 - user question based on you know
56:03 - programming language um that user
56:05 - referring to so let's define our router
56:08 - here now what we're going to do is we'll
56:10 - ask a question that is python code so
56:14 - we'll call that and now it's done and
56:16 - you see the object we get out is indeed
56:18 - it's a route query object so it's
56:20 - exactly it aderes to this data model
56:22 - we've set up and in this case it's it's
56:26 - it's correct so it's calling this python
56:27 - doc so you can we can extract that right
56:29 - here as a string now once we have this
56:33 - you can really easily set up like a
56:35 - route so this could be like our full
56:37 - chain where we take this router we
56:39 - should defined here and then this choose
56:41 - route function can basically take that
56:44 - output and do something with it so for
56:46 - example if python docs this could then
56:49 - apply the question to like a retriever
56:51 - full of python information uh or JS same
56:55 - thing so this is where you would hook
56:57 - basically that question up to different
56:59 - chains that are like you know retriever
57:02 - chain one for python retriever chain two
57:04 - for JS and so forth so this is kind of
57:06 - like the routing mechanism but this is
57:08 - really doing the heavy lifting of taking
57:10 - an input question and turning into a
57:12 - structured object that restricts the
57:14 - output to one of a few output types that
57:18 - we care about in our like routing
57:20 - problem so that's really kind of the way
57:22 - this all hooks
57:23 - together now semantic outing is actually
57:26 - maybe even a little bit more
57:27 - straightforward based on what we've seen
57:29 - previously so in that case let's say we
57:32 - have two prompts we have a physics
57:33 - prompt we have a math
57:35 - prompt we can embed those prompts no
57:38 - problem we do that here now let's say we
57:41 - have an input question from a user like
57:43 - in this case what is a black hole we
57:45 - pass that through we then apply this
57:47 - runnable Lambda function which is
57:49 - defined right here what we're doing here
57:51 - is we're embedding the question we're
57:53 - Computing similarity between the
57:54 - question and the prompts uh we're taking
57:57 - the most similar and then we're
58:00 - basically choosing the prompt based on
58:02 - that similarity and you can see let's
58:03 - run that and try it
58:05 - out and we're using the physics prompt
58:07 - and there we go black holes region and
58:09 - space so that just shows you kind of how
58:11 - you can use semantic routing uh to
58:15 - basically embed a question embed for
58:17 - example various prompts pick the prompt
58:19 - based on sematic similarity so that
58:22 - really gives you just two ways to do
58:23 - routing one is logical routing with
58:25 - function in uh can be used very
58:27 - generally in this case we applied it to
58:29 - like different coding languages but
58:31 - imagine these could be swapped out for
58:33 - like you know my python uh my like
58:36 - vector store versus My Graph DB versus
58:38 - my relational DB and you could just very
58:42 - simply have some description of what
58:43 - each is and you know then not only will
58:46 - the llm do reasoning but it'll also
58:49 - return an object uh that can be parsed
58:52 - very cleanly to produce like one of a
58:54 - few very specific types which then you
58:56 - can reason over like we did here in your
58:59 - routing function so that kind of gives
59:01 - you the general idea and these are
59:02 - really very useful tools and I encourage
59:05 - you to experiment with them
59:07 - thanks hi this is Lance from Lang chain
59:10 - this is the 11th part of our rag from
59:12 - scratch video series focused on query
59:15 - construction so we previously talked
59:17 - through uh query translation which is
59:19 - the process of taking a question and
59:21 - converting it or translating it into a
59:24 - question that's better optimized for
59:25 - retrieval then we talked about routing
59:27 - which is the process of going taking
59:29 - that question routing it to the right
59:30 - Source be it a given Vector store graph
59:33 - DB um or SQL DB for example now we're
59:37 - going to talk about the process of query
59:38 - construction which is basically taking
59:40 - natural language and converting it into
59:42 - particular domain specific language uh
59:45 - for one of these sources now we're going
59:47 - to talk specifically about the process
59:49 - of going from natural language to uh
59:52 - meditated filters for Vector
59:54 - Stores um the problem statement is
59:56 - basically this let's imagine we had an
59:58 - index of Lang Chain video transcripts um
60:01 - you might want to ask a question give me
60:04 - you know or find find me videos on chat
60:06 - Lang chain published after 2024 for
60:09 - example um the the process of query
60:13 - structuring basically converts this
60:15 - natural language question into a
60:17 - structured query that can be applied to
60:19 - the metadata uh filters on your vector
60:22 - store so most Vector stores will have
60:24 - some kind of meditative filters that can
60:26 - do kind of structur querying on top of
60:29 - uh the chunks that are indexed um so for
60:32 - example this type of query will retrieve
60:34 - all chunks uh that talk about the topic
60:36 - of chat Lang chain uh published after
60:39 - the date 2024 that's kind of the problem
60:41 - statement and to do this we're going to
60:43 - use function calling um in this case you
60:46 - can use for example open AI or other
60:48 - providers to do that and we're going to
60:50 - do is at a high level take the metadata
60:53 - fields that are present in our Vector
60:55 - store and divide them to the model as
60:57 - kind of information and the model then
61:00 - can take those and produce queries that
61:03 - adhere to the schema provided um and
61:06 - then we can parse those out to a
61:07 - structured object like a identic object
61:10 - which again which can then be used in
61:12 - search so that's kind of the problem
61:14 - statement and let's actually walk
61:15 - through
61:17 - code um so here's our notebook which
61:20 - we've kind of gone through previously
61:22 - and I'll just show you as an example
61:24 - let's take a example YouTube video and
61:26 - let's look at the metadata that you get
61:28 - with the transcript so you can see you
61:30 - get stuff like description uh URL um
61:34 - yeah publish date length things like
61:36 - that now let's say we had an index that
61:39 - had um basically a that had a number of
61:42 - different metadata fields and filters uh
61:46 - that allowed us to do range filtering on
61:47 - like view count publication date the
61:49 - video length um or unstructured search
61:52 - on contents and title so those are kind
61:54 - of like the imagine we had an index that
61:56 - had uh those kind of filters available
62:00 - to us what we can do is capture that
62:03 - information about the available filters
62:05 - in an object so we're calling that this
62:06 - tutorial search object kind of
62:08 - encapsulates that information about the
62:10 - available searches that we can do and so
62:12 - we basically enumerate it here content
62:14 - search and title search or semantic
62:15 - searches that can be done over those
62:19 - fields um and then these filters then
62:22 - are various types of structure searches
62:24 - we can do on like the length um The View
62:28 - count and so forth and so we can just
62:30 - kind of build that object now we can set
62:33 - this up really easily with a basic
62:35 - simple prompt that says you know you're
62:36 - an expert can bring natural language
62:38 - into database queries you have access to
62:40 - the database tutorial videos um given a
62:43 - question return a database query
62:44 - optimize retrieval so that's kind of it
62:47 - now here's the key point though when you
62:49 - call this LM with structured output
62:51 - you're binding this pantic object which
62:53 - contains all the information about our
62:55 - index to the llm which is exactly what
62:58 - we talked about previously it's really
63:00 - this process right here you're taking
63:02 - this object you're converting it to a
63:04 - function schema for example open AI
63:05 - you're binding that to your model and
63:07 - then you're going to be able to get um
63:10 - structured object out versus a Json
63:13 - string from a natural language question
63:15 - which can then be parsed into a pantic
63:18 - object which you get out so that's
63:19 - really the flow and it's taking
63:21 - advantage of function calling as we said
63:23 - so if we go back down we set up our
63:26 - query analyzer chain right here now
63:28 - let's try to run that just on a on a
63:30 - purely semantic input so rag from
63:32 - scratch let's run that and you can see
63:35 - this just does like a Content search and
63:36 - a title search that's exactly what you
63:38 - would expect now if we pass a question
63:41 - that includes like a date filter let's
63:43 - just see if that would work
63:45 - and there we go so you kind of still get
63:48 - that semantic search um but you also get
63:52 - um search over for example publish date
63:54 - earliest and latest publish date kind of
63:56 - as as you would expect let's try another
63:58 - one here so videos focus on the topic of
64:01 - chat Lang chain they're published before
64:02 - 2024 this is just kind of a rewrite of
64:04 - this question in slightly different way
64:06 - using a different date filter and then
64:08 - you can see we can get we get content
64:10 - search title search and then we can get
64:12 - kind of a date search so this is a very
64:14 - general strategy that can be applied
64:16 - kind of broadly to um different kinds of
64:19 - querying you want to do it's really the
64:21 - process of going from an unstructured
64:23 - input to a structured query object out
64:26 - following an arbitrary schema that you
64:29 - provide and so as noted really this
64:32 - whole thing we created here this
64:33 - tutorial search is based upon the
64:35 - specifics of our Vector store of
64:37 - interest and if you want to learn more
64:39 - about this I link to some documentation
64:41 - here that talks a lot about different uh
64:44 - types of of Integrations we have with
64:46 - different Vector store providers to do
64:47 - exactly this so it's a very useful trick
64:50 - um it allows you to do kind of query uh
64:54 - uh say metadata filter filtering on the
64:56 - fly from a natural language question
64:58 - it's a very convenient trick uh that
65:01 - works with many different Vector DBS so
65:03 - encourage you to play with it
65:06 - thanks this is Lance from Lang chain I'm
65:09 - going to talk about indexing uh and
65:11 - mulation indexing in particular for the
65:14 - 12th part of our rag from scratch series
65:17 - here so we previously talked about a few
65:20 - different major areas we talk about
65:22 - query translation which takes a question
65:24 - and translates it in some way to
65:26 - optimize for retrieval we talk about
65:28 - routing which is the process of taking a
65:30 - question routing it to the right data
65:32 - source be it a vector store graph DB uh
65:35 - SQL DB we talked about queer
65:37 - construction we dug into uh basically
65:39 - queer construction for Vector stores but
65:41 - of course there's also text SQL text to
65:44 - Cipher um so now we're going to talk
65:46 - about indexing a bit in particular we're
65:48 - going to talk about indexing indexing
65:50 - techniques for Vector Stores um and I
65:53 - want to highlight one particular method
65:54 - today called multi-representation
65:57 - indexing so the high LEL idea here is
66:01 - derived a bit from a paper called
66:03 - proposition indexing which kind of makes
66:05 - a simple
66:06 - observation you can think about
66:10 - decoupling raw documents and the unit
66:13 - you use for
66:15 - retrieval so in the typical case you
66:18 - take a document you split it up in some
66:21 - way to index it and then you embed the
66:24 - split directly
66:26 - um this paper talks about actually
66:29 - taking a document splitting it in some
66:32 - way but then using an llm to produce
66:35 - what they call a proposition which you
66:37 - can think of as like kind of a
66:38 - distillation of that split so it's kind
66:41 - of like using an llm to modify that
66:43 - split in some way to distill it or make
66:45 - it like a crisper uh like summary so to
66:49 - speak that's better optimized for
66:51 - retrieval so that's kind of one
66:52 - highlight one piece of intuition so we
66:55 - actually taken that idea and we've kind
66:57 - of built on it a bit in kind of a really
66:59 - nice way that I think is very well
67:01 - suited actually for long context llms so
67:04 - the idea is pretty simple you take a
67:07 - document and you you actually distill it
67:10 - or create a proposition like they show
67:11 - in the prior paper I kind of typically
67:14 - think of this as just produce a summary
67:16 - of the document and you embed that
67:18 - summary so that summary is meant to be
67:21 - optimized for retrieval so might contain
67:23 - a bunch of keywords from the document or
67:25 - like the big
67:26 - ideas such that when you embed the
67:29 - summary you embed a question you do
67:31 - search you basically can find that
67:34 - document based upon this highly
67:35 - optimized summary for retrieval so
67:38 - that's kind of represented here in your
67:39 - vector store but here's the catch you
67:42 - independently store the raw document in
67:44 - a dock store and when you when you
67:47 - basically retrieve the summary in the
67:50 - vector store you return the full
67:52 - document for the llm to perform
67:54 - generation and this is a nice trick
67:56 - because at generation time now with long
67:59 - condex LMS for example the LM can handle
68:02 - that entire document you don't need to
68:03 - worry about splitting it or anything you
68:05 - just simply use the summary to prod like
68:08 - to create a really nice representation
68:10 - for fishing out that full dock use that
68:13 - full dock in generation there might be a
68:14 - lot of reasons you want to do that you
68:16 - want to make sure the LM has the full
68:17 - context to actually answer the question
68:19 - so that's the big idea it's a nice trick
68:22 - and let's walk through some code
68:23 - here we have a notebook all set up uh
68:26 - just like before we done some pip
68:28 - installs um set to maybe I Keys here for
68:32 - lsmith um kind of here's a diagram now
68:35 - let me show an example let's just load
68:37 - two different uh blog posts uh one is
68:39 - about agents one is about uh you know
68:41 - human data quality um and what we're
68:45 - going to do is let's create a summary of
68:46 - each of those so this is kind of the
68:48 - first step of that process where we're
68:50 - going from like the raw documents to
68:51 - summaries let's just have a look and
68:53 - make sure those ran So Okay cool so the
68:57 - first DOC discusses you know building
68:58 - autonomous agents the second doc
69:00 - contains the importance of high quality
69:01 - human data and training okay so that's
69:03 - pretty nice we have our summaries now
69:06 - we're going to go through a process
69:07 - that's pretty
69:08 - simple first we Define a vector store
69:10 - that's going to index those
69:12 - summaries now we're going to Define what
69:14 - we call like our our document storage is
69:16 - going to store the full documents okay
69:19 - so this multiv Vector retriever kind of
69:21 - just pulls those two things together we
69:23 - basically add our Dock Store we had this
69:25 - bite store is basically the the the full
69:28 - document store uh the vector store is
69:30 - our Vector store um and now this ID is
69:32 - what we're going to use to reference
69:34 - between the chunks or the summaries and
69:37 - the full documents that's really it so
69:40 - now for every document we'll Define a
69:42 - new Doc ID um and then we're basically
69:44 - going to like take our summary documents
69:47 - um and we're going to extract um for
69:51 - each of our summaries we're going to get
69:53 - the associated doc ID so we go um so
69:58 - let's go ahead and do that so we have
70:01 - our summary docs which we add to the
70:03 - vector store we have our full documents
70:06 - uh our doc IDs and the full raw
70:08 - documents which are added to our doc
70:10 - store and then let's just do a query
70:12 - Vector store like a similarity search on
70:14 - our Vector store so memory and agents
70:16 - and we can see okay so we can extract
70:19 - you know from the summaries we can get
70:22 - for example the summary that pertains to
70:24 - um a agents so that's a good thing now
70:27 - let's go ahead and run a query get
70:30 - relevant documents on our retriever
70:32 - which basically combines the summaries
70:35 - uh which we use for retrieval then the
70:38 - doc store which we use to get the full
70:40 - doc back so we're going to apply our
70:42 - query we're going to basically run this
70:45 - and here's the key Point we've gotten
70:48 - back the entire
70:49 - article um and we can actually if you
70:53 - want to look at the whole thing we we
70:55 - can just go ahead and do this here we go
70:58 - so this is the entire article that we
70:59 - get back from that search so it's a
71:02 - pretty nice trick again we query with
71:04 - just memory and agents um and we can
71:07 - kind of go back to our diagram here we
71:09 - quered for memory and agents it started
71:11 - our summaries it found the summary
71:13 - related to memory and agents it uses
71:15 - that doc ID to reference between the
71:17 - vector store and the doc store it fishes
71:19 - out the right full doc returns us the
71:21 - full document in this case the full web
71:23 - page that's really it simple idea nice
71:27 - way to go from basically like nice
71:30 - simple proposition style or summary
71:32 - style indexing to full document
71:34 - retrieval which is very useful
71:36 - especially with long contact LMS thank
71:40 - you hi this is Lance from Lang chain
71:43 - this is the 13th part of our rag from
71:45 - scratch series focused on a technique
71:47 - called
71:48 - Raptor so Raptor sits within kind of an
71:51 - array of different indexing techniques
71:54 - that can be applied on Vector Stores um
71:57 - we just talked about
71:58 - multi-representation indexing um we I
72:01 - priv a link to a video that's very good
72:04 - talking about the different means of
72:05 - chunking so I encourage you to look at
72:06 - that and we're going to talk today about
72:09 - a technique called Raptor which you can
72:10 - kind of think of it as a technique for
72:12 - hierarchical
72:14 - indexing so the highle intuition is
72:17 - this some questions require very
72:20 - detailed information from a corpus to
72:23 - answer like pertain to a single document
72:25 - or single chunk so like we can call
72:28 - those low-level
72:29 - questions some questions require
72:31 - consolidation across kind broad swast of
72:34 - a document so across like many documents
72:37 - or many chunks within a document and you
72:39 - can call those like higher level
72:41 - questions and so there's kind of this
72:44 - challenge in retrieval and that
72:46 - typically we do like K nearest neighbors
72:48 - retrieval like we've been talking about
72:50 - you're fishing out some number of chunks
72:53 - but what if you have a question that
72:55 - requires information across like five
72:57 - six you know or a number of different
72:59 - chunks which may exceed you know the K
73:02 - parameter in your retrieval so again
73:04 - when you typically do retrieval you
73:06 - might set a k parameter of three which
73:08 - means you're retrieving three chunks
73:10 - from your vector store um and maybe you
73:12 - have a high very high level question
73:14 - that could benefit from infation across
73:16 - more than three so this technique called
73:18 - raptor is basically a way to build a
73:21 - hierarchical index of document summaries
73:25 - and the intuition is this you start with
73:27 - a set of documents as your Leafs here on
73:30 - the left you cluster them and then you
73:33 - Summarize each cluster so each cluster
73:36 - of similar documents um will consult
73:39 - information from across your context
73:42 - which is you know your context could be
73:44 - a bunch of different splits or could
73:45 - even be across a bunch of different
73:47 - documents you're basically capturing
73:49 - similar ones and you're consolidating
73:51 - the information across them in a summary
73:53 - and here's the interesting thing you do
73:54 - that
73:55 - recursively until either you hit like a
73:57 - limit or you end up with one single
73:59 - cluster that's a kind of very high level
74:01 - summary of all of your
74:03 - documents and what the paper shows is
74:05 - that if you basically just collapse all
74:08 - these and index them together as a big
74:10 - pool you end up with a really nice array
74:12 - of chunks that span the abstraction
74:15 - hierarchy like you have a bunch of
74:17 - chunks from Individual documents that
74:19 - are just like more detailed chunks
74:21 - pertaining to that you know single
74:23 - document but you also have chunks from
74:25 - these summaries or I would say like you
74:27 - know maybe not chunks but in this case
74:29 - the summary is like a distillation so
74:31 - you know raw chunks on the left that
74:33 - represent your leavs are kind of like
74:35 - the rawest form of information either
74:37 - raw chunks or raw documents and then you
74:40 - have these higher level summaries which
74:42 - are all indexed together so if you have
74:44 - higher level questions they should
74:46 - basically be more similar uh in sematic
74:49 - search for example to these higher level
74:50 - summary chunks if you have lower level
74:53 - questions then they'll retrieve these
74:55 - more lower level chunks and so you have
74:56 - better semantic coverage across like the
74:59 - abstraction hierarchy of question types
75:01 - that's the intuition they do a bunch of
75:02 - nice studies to show that this works
75:04 - pretty well um I actually did a deep
75:07 - dive video just on this which I link
75:09 - below um I did want to cover it briefly
75:12 - just at a very high level um so let's
75:15 - actually just do kind of a code walkr
75:17 - and I've added it to this rack from
75:19 - scratch course notebook but I link over
75:21 - to my deep dive video as well as the
75:23 - paper and the the full code notebook
75:27 - which is already checked in is discussed
75:28 - at more length in the Deep dive the
75:31 - technique is a little bit detailed so I
75:33 - only want to give you very high levels
75:35 - kind of overview here and you can look
75:37 - at the Deep dive video if you want to go
75:39 - in more depth again we talked through
75:41 - this abstraction
75:43 - hierarchy um I applied this to a large
75:46 - set of Lang chain documents um so this
75:49 - is me loading basically all of our Lang
75:51 - chain expression language docs so this
75:53 - is on the order of 30 documents you can
75:55 - see I do a histogram here of the token
75:57 - counts per document some are pretty big
76:00 - most are fairly small less than you know
76:02 - 4,000 tokens um and what I did is I
76:05 - indexed all of them um individually so
76:09 - the all those raw documents you can kind
76:11 - of Imagine are here on the left and then
76:14 - I do um I do embedding I do clustering
76:18 - summarization and I do that recursively
76:21 - um until I end up with in this case I
76:24 - believe I only set like three levels of
76:27 - recursion and then I save them all my
76:29 - Vector store so that's like the highle
76:31 - idea I'm applying this Raptor technique
76:33 - to a whole bunch of Lang chain documents
76:36 - um that have fairly large number of
76:38 - tokens um so I do that um and yeah I use
76:45 - actually use both CLA as well as open AI
76:48 - here um this talks through the
76:50 - clustering method which they that they
76:52 - use which is pretty interesting you can
76:54 - kind of dig into that on your own if if
76:55 - you're really um interested this is a
76:58 - lot of their code um which I cite
77:00 - accordingly um this is basically
77:02 - implementing the clustering method that
77:03 - they use um and this is just simply the
77:08 - document embedding stage um this is like
77:12 - basically embedding uh and clustering
77:15 - that's really it uh some text formatting
77:19 - um summarizing of the clusters right
77:22 - here um and then this is just running
77:24 - that whole process recursively that's
77:26 - really it um this is tree building so
77:30 - basically I have the RO the rod docs
77:33 - let's just go back and look at Doc texts
77:35 - so this should be all my raw documents
77:37 - uh so that's right you can see it here
77:39 - doc text is basically just the text in
77:41 - all those Lang chain documents that I
77:43 - pulled
77:45 - um and so I run this process on them
77:49 - right
77:50 - here uh so this is that recursive
77:52 - embedding cluster basically runs and
77:54 - produces is that tree here's the results
77:57 - um this is me just going through the
77:59 - results and basically adding the result
78:02 - text to this list of uh texts um oh okay
78:07 - so here's what I do this Leaf text is
78:09 - all the raw documents and I'm appending
78:12 - to that all the summaries that's all
78:14 - it's going on and then I'm indexing them
78:16 - all together that's the key Point rag
78:19 - chain and there you have it that's
78:21 - really all you do um so anyway I
78:23 - encourage you to look at this in depth
78:24 - it's a pretty interesting technique it
78:26 - works well long with long contexts so
78:29 - for example one of the arguments I made
78:30 - is that it's kind of a nice approach to
78:32 - consult information across like a span
78:35 - of large
78:36 - documents like in this particular case
78:38 - my individual documents were lch
78:40 - expression language docs uh each each
78:42 - being somewhere in the order of you know
78:44 - in this case like you know most of them
78:46 - are less than 4,000 tokens some pretty
78:48 - big but I index them all I cluster them
78:51 - without any splits uh embed them cluster
78:54 - them build this tree um and go from
78:56 - there and it all works because we now
78:58 - have llms that can go out to you know
79:01 - 100 or 200,000 up to million tokens and
79:03 - Contex so you can actually just do this
79:05 - process for big swats of documents in
79:08 - place without any without any splitting
79:10 - uh it's a pretty nice approach so I
79:12 - encourage you to think about it look at
79:13 - it watch the deep that video If you
79:14 - really want to go deeper on this um
79:20 - thanks hi this is Lance from Lang chain
79:22 - this is the 14th part of our rag from
79:24 - scratch series we're going to I'm going
79:26 - to be talking about an approach called
79:28 - cold
79:29 - bear um so we've talked about a few
79:34 - different approaches for indexing and
79:37 - just as kind of a refresher indexing
79:38 - Falls uh kind of right down here in our
79:41 - flow we started initially with career
79:43 - translation taking a question
79:45 - translating it in some way to optimize
79:47 - retrieval we talked about routing it to
79:49 - a particular database we then talked
79:51 - about query construction so going from
79:53 - natural language to the DSL or domain
79:56 - specific language for E any of the
79:59 - databases that you want to work with
80:01 - those are you know metadata filters for
80:02 - Vector stores or Cipher
80:05 - for graph DB or SQL for relational DB so
80:09 - that's kind of the flow we talked about
80:10 - today we talked about some indexing
80:12 - approaches like multi-representation
80:14 - indexing we gave a small shout out to
80:16 - greet camer in the series on chunking uh
80:19 - we talked about hierarchical indexing
80:22 - and I want to include one Advanced kind
80:24 - embedding approach so we talked a lot
80:26 - about embeddings are obviously very
80:28 - Central to semantic similarity search um
80:31 - and
80:32 - retrieval so one of the interesting
80:35 - points that's been brought up is that
80:38 - embedding models of course take a
80:39 - document you can see here on the top and
80:42 - embed it basically compress it to a
80:45 - vector so it's kind of a compression
80:48 - process you representing all the
80:49 - semantics of that document in a single
80:51 - Vector you're doing the same to your
80:53 - question you're doing similarity search
80:55 - between the question embedding and the
80:57 - document embedding um in order to
80:59 - perform retrieval you're typically
81:01 - taking the you know K most similar um
81:05 - document abetting is given a question
81:07 - and that's really how you're doing it
81:10 - now a lot of people said well hey the
81:12 - compressing a full document with all
81:13 - this Nuance to single Vector seems a
81:15 - little bit um overly restrictive right
81:18 - and this is a fair question to ask um
81:21 - there's been some interesting approaches
81:22 - to try to address that and one is this
81:24 - this this approach method called Co bear
81:28 - so the intuition is actually pretty
81:30 - straightforward there's a bunch of good
81:31 - articles I link down here this is my
81:33 - little cartoon to explain it which I
81:35 - think is hopefully kind of helpful but
81:37 - here's the main idea instead of just
81:39 - taking a document and compressing it
81:41 - down to a single Vector basically single
81:44 - uh what we might call embedding Vector
81:46 - we take the document we break it up into
81:49 - tokens so tokens are just like you know
81:51 - units of of content it depends on the
81:54 - token areas you use we talked about this
81:56 - earlier so you basically tokenize it and
81:59 - you produce basically an embedding or
82:01 - vector for every token and there's some
82:03 - kind of positional uh waiting that
82:05 - occurs when you do this process so you
82:08 - obviously you look to look at the
82:09 - implementation understand the details
82:10 - but the intuition is that you're
82:12 - producing some kind of representation
82:14 - for every token okay and you're doing
82:17 - the same thing for your question so
82:19 - you're taking your question you're
82:20 - breaking into a tokens and you have some
82:22 - representation or vector per token
82:25 - and then what you're doing is for every
82:27 - token in the question you're Computing
82:29 - the similarity across all the tokens in
82:33 - the document and you're finding the max
82:36 - you're taking the max you're storing
82:38 - that and you're doing that process for
82:42 - all the tokens in the question so again
82:45 - token two you compare it to every token
82:48 - in the in the document compute the
82:51 - Max and then the final score is in this
82:54 - case the sum of the max similarities uh
82:57 - between every question token and any
83:00 - document token so it's an interesting
83:03 - approach uh it reports very strong
83:06 - performance latency is definitely a
83:08 - question um so kind of production
83:11 - Readiness is something you should look
83:12 - into but it's a it's an approach that's
83:14 - worth mentioning here uh because it's
83:16 - pretty
83:18 - interesting um and let's walk through
83:20 - the
83:21 - code
83:22 - so there's actually nice Library called
83:25 - rouille which makes it very easy to play
83:27 - with Co bear um she's pip install it
83:30 - here I've already done that and we can
83:33 - use one of their pre-train models to
83:35 - mediate this process so I'm basically
83:36 - following their documentation this is
83:38 - kind of what they recommended um so I'm
83:40 - running this
83:41 - now hopefully this runs somewhat quickly
83:43 - I'm not sure I I previously have loaded
83:45 - this model so hopefully it won't take
83:47 - too long and yeah you can see it's
83:48 - pretty quick uh I'm on a Mac M2 with 32
83:51 - gigs um so just as like a context in
83:54 - terms of my my system um this is from
83:56 - their documentation we're just grabbing
83:57 - a Wikipedia page this is getting a full
83:59 - document on Miyazaki so that's cool
84:03 - we're going to grab that now this is
84:05 - just from their docs this is basically
84:06 - how we create an index so we provide the
84:09 - you know some index name the collection
84:11 - um the max document length and yeah you
84:13 - should look at their documentation for
84:15 - these flags these are just the defaults
84:16 - so I'm going to create my index um so I
84:19 - get some logging here so it it's working
84:21 - under the hood um and by the way I
84:24 - actually have their documentation open
84:26 - so you can kind of follow along um
84:30 - so
84:31 - um let's see yeah right about here so
84:35 - you can kind of follow this indexing
84:36 - process to create an index you need to
84:37 - load a train uh a trained model this can
84:41 - be either your own pre-train model or
84:42 - one of ours from The Hub um and this is
84:44 - kind of the process we're doing right
84:45 - now create index is just a few lines of
84:48 - code and this is exactly what we're
84:49 - doing um so this is the you know my
84:52 - documents and this is the indexing step
84:54 - that we just we just kind of walk
84:56 - through and it looks like it's done um
84:58 - so you get a bunch of logging here
85:00 - that's fine um now let's actually see if
85:02 - this works so we're going to run drag
85:04 - search what an emotion Studio did Miaki
85:06 - found set our K parameter and we get
85:09 - some results okay so it's running and
85:13 - cool we get some documents out so you
85:15 - know it seems to work now what's nice is
85:17 - you can run this within lighting chain
85:18 - as a liting chain retriever so that
85:20 - basically wraps this as a lighting chain
85:22 - Retriever and then you can use it freely
85:24 - as a retriever within Lang chain it
85:25 - works with all the other different LMS
85:27 - and all the other components like
85:28 - rankers and so forth that we talk
85:30 - through so you can use this directly as
85:32 - a retriever let's try this out and boom
85:35 - nice and fast um and we get our
85:38 - documents again this is a super simple
85:40 - test example you should run this maybe
85:42 - on more complex cases but it's pretty
85:44 - pretty easy spin up it's a really
85:45 - interesting alternative indexing
85:47 - approach um using again like we talked
85:49 - through um a very different algorithm
85:53 - for computing do similarity that may
85:55 - work better I think an interesting
85:57 - regime to consider this would be longer
85:59 - documents so if you want like longer um
86:02 - yeah if if you basically want kind of
86:04 - long context embedding I think you
86:06 - should look into for example the uh Max
86:09 - token limits for this approach because
86:11 - it partitions the document into into
86:13 - each token um I would be curious to dig
86:16 - into kind of what the overall context
86:18 - limits are for this approach of coar but
86:21 - it's really interesting to consider and
86:22 - it reports very strong performance so
86:24 - again I encourage you to play with it
86:26 - and this is just kind of an intro to how
86:27 - to get set up and to start experimenting
86:29 - with it really quickly
86:32 - thanks hi this is Lance from Lang chain
86:35 - I'm going to be talking about using
86:36 - langra to build a diverse and
86:39 - sophisticated rag
86:40 - flows so just to set the stage the basic
86:44 - rag flow you can see here starts with a
86:47 - question retrieval of relevant documents
86:49 - from an index which are passed into the
86:52 - context window of an llm for generation
86:54 - of an answer ground in your documents
86:57 - that's kind of the basic
86:59 - outline and we can see it's like a very
87:01 - linear
87:02 - path um in practice though you often
87:06 - encounter a few different types of
87:08 - questions like when do we actually want
87:10 - to
87:11 - retrieve based upon the context of the
87:13 - question um are the retrieve documents
87:16 - actually good or not and if they're not
87:18 - good should we discard them and then how
87:21 - do we loot back and retry retrieval with
87:23 - for example and improved
87:25 - question so these types of questions
87:29 - motivate an idea of active rag which is
87:32 - a process where an llm actually decides
87:34 - when and where to retrieve based upon
87:36 - like existing
87:37 - retrievals or existing
87:40 - Generations now when you think about
87:43 - this there's a few different levels of
87:45 - control that you have over an llm in a
87:48 - rag
87:49 - application the base case like we saw
87:52 - with our chain is just use an llm to
87:55 - choose a single steps output so for
87:58 - example in traditional rag you feed it
88:00 - documents and it decides to generation
88:04 - so it's just kind of one step now a lot
88:07 - of rag workflows will use the idea of
88:09 - routing so like given a question should
88:12 - I route it to a vector store or a graph
88:16 - DB um and we have seen this quite a
88:20 - bit now this newer idea that I want to
88:23 - introduce
88:24 - is how do we build more sophisticated
88:28 - logical
88:29 - flows um in a rag
88:32 - pipeline um that you let the llm choose
88:36 - between different steps but specify all
88:41 - the transitions that are
88:43 - available and this is known as we call a
88:45 - state
88:46 - machine now there's a few different
88:50 - architectures that have emerged uh to
88:53 - build different types of rag
88:54 - chains and of course chains are
88:58 - traditionally used just for like very
88:59 - basic rag but this notion of State
89:02 - machine is a bit newer and Lang graph
89:06 - which we recently released provides a
89:07 - really nice way to build State machines
89:10 - for Rag and for other
89:13 - things and the general idea here is that
89:16 - you can lay out more diverse and
89:18 - complicated rag
89:20 - flows and then Implement them as graphs
89:24 - and it kind of motivates this more broad
89:26 - idea of of like flow engineering and
89:28 - thinking through the actual like
89:30 - workflow that you want and then
89:32 - implementing it um and we're gonna
89:35 - actually do that right now so I'm GNA Pi
89:39 - a recent paper called CAG corrective rag
89:42 - which is really a nice method um for
89:46 - active rag that incorporates a few
89:48 - different
89:50 - ideas um so first you retrieve documents
89:54 - and then you grade
89:55 - them now if at least one document
89:59 - exceeds the threshold for
90:01 - relevance you go to generation you
90:04 - generate your
90:06 - answer um and it does this knowledge
90:10 - refinement stage after that but let's
90:13 - not worry about that for right now it's
90:15 - kind of not essential for understanding
90:16 - the basic flow here so again you do a
90:20 - grade for relevance for every document
90:23 - if any is relevant you
90:25 - generate now if they're all ambiguous or
90:29 - incorrect based upon your
90:31 - grader you retrieve from an external
90:34 - Source they use web
90:36 - search and then they pass that as their
90:39 - context for answer
90:42 - generation so it's a really neat
90:44 - workflow where you're doing retrieval
90:46 - just like with basic rag but then you're
90:48 - reasoning about the documents if they're
90:50 - relevant go ahead and at least one is
90:53 - relevant go ahead and generate if
90:54 - they're not retrieve from alternative
90:57 - source and then pack that into the
90:59 - context and generate your
91:02 - answer so let's see how we would
91:04 - implement this as a estate machine using
91:09 - Lang
91:10 - graph um we'll make a few
91:13 - simplifications
91:15 - um we're going to first decide if any
91:19 - documents are relevant we'll go ahead
91:22 - and do the the web search
91:25 - um to supplement the output so that's
91:28 - just like kind of one minor
91:29 - modification um we'll use tab search for
91:32 - web search um we use Query writing to
91:35 - optimize the search for uh to optimize
91:38 - the web search but it follows a lot of
91:40 - the the intuitions of the main paper uh
91:44 - small note here we set the Tav API key
91:48 - and another small mode I've already set
91:50 - my lsmith API key um with which we'll
91:54 - see is useful a bit later for observing
91:57 - the resulting
91:59 - traces now I'm going to index three blog
92:02 - posts that I
92:03 - like um I'm going to use chroma DB I'm G
92:06 - use open ey embeddings I'm going to run
92:09 - this right now this will create a vector
92:11 - store for me from these three blog
92:15 - posts and then what I'm going to do is
92:19 - Define
92:20 - State now this is kind of the core
92:23 - object that going to be passed around my
92:25 - graph that I'm going to
92:27 - modify and right here is where I Define
92:29 - it and the key point to note right now
92:32 - is it's just a dictionary and it can
92:35 - contain things that are relevant for rag
92:37 - like question documents generation and
92:40 - we'll see how we update that in in in a
92:42 - little bit but the first thing to note
92:44 - is we Define our state and this is
92:47 - what's going to be modified in every Noe
92:49 - of our
92:50 - graph now here's really the Crux of it
92:53 - and this is the thing I want to zoom in
92:54 - on a little bit um
92:58 - so when you kind of move from just
93:00 - thinking about promps to thinking about
93:02 - overall flows it it's like kind of a fun
93:05 - and interesting exercise I kind of think
93:07 - about this as it's been mentioned on
93:09 - Twitter a little bit more like flow
93:12 - engineering so let's think through what
93:15 - was actually done in the paper and what
93:19 - modifications to our state are going to
93:21 - happen in each stage so we start with a
93:24 - question you can see that on the far
93:25 - left and this kind of state is represent
93:28 - as a dictionary like we have we start
93:29 - with a question we perform retrieval
93:31 - from our Vector store which we just
93:33 - created that's going to give us
93:35 - documents so that's one node we made an
93:38 - an adjustment to our state by adding
93:41 - documents that's step
93:43 - one now we have a second node where
93:45 - we're going to grade the documents and
93:48 - in this node we might filter some out so
93:50 - we are making a modification to state
93:52 - which is why it's a node so we're going
93:54 - to have a
93:55 - greater then we're going to have what
93:58 - we're going to call a conditional Edge
94:00 - so we saw we went from question to
94:02 - retrieval retrieval always goes to
94:05 - grading and now we have a
94:07 - decision if any document is
94:10 - irrelevant we're going to go ahead and
94:14 - do web search to
94:15 - supplement and if they're all relevant
94:17 - will go to generation it's a minor kind
94:20 - of a minor kind of logical uh decision
94:23 - ision that we're going to
94:25 - make um if any are not relevant we'll
94:28 - transform the query and we'll do web
94:31 - search and we'll use that for Generation
94:33 - so that's really it and that's how we
94:35 - can kind of think about our flow and how
94:37 - our States can be modified throughout
94:39 - this
94:40 - flow now all we then need to do and I I
94:44 - kind of found
94:46 - spending 10 minutes thinking carefully
94:48 - through your flow
94:50 - engineering is really valuable because
94:52 - from here it's really implementation
94:55 - details um and it's pretty easy as
94:58 - you'll see so basically I'm going to run
95:02 - this code block but then we can like
95:03 - walk through some of it I won't show you
95:04 - everything so it'll get a little bit
95:06 - boring but really all we're doing is
95:09 - we're finding functions for every node
95:12 - that take in the state and modify in
95:15 - some way that's all it's going on so
95:17 - think about retrieval we run retrieval
95:19 - we take in state remember it's a dict we
95:22 - get our state dick like this
95:24 - we extract one keyy question from our
95:26 - dick we pass that to a retriever we get
95:29 - documents and we write back out State
95:32 - now with documents key added that's
95:36 - all generate going to be similar we take
95:39 - in state now we have our question and
95:41 - documents we pull in a prompt we Define
95:44 - an llm we do minor post processing on
95:47 - documents we set up a chain for
95:49 - retrieval uh or sorry for Generation
95:51 - which is just going to be take our
95:52 - prompt pump Plum that to an llm
95:55 - partially output a string and we run it
95:57 - right here invoking our documents in our
96:01 - question to get our answer we write that
96:04 - back to State that's
96:06 - it and you can kind of follow here for
96:09 - every node we just Define a function
96:12 - that performs the state modification
96:13 - that we want to do on that
96:15 - node grading documents is going to be
96:17 - the same um in this case I do a little
96:21 - thing extra here because I actually
96:23 - Define a identic data model for my
96:25 - grader so that the output of that
96:28 - particular grading chain is a binary yes
96:31 - or no you can look at the code make sure
96:33 - it's all shared um and that just makes
96:36 - sure that our output is is very
96:38 - deterministic so that we then can down
96:41 - here perform logical filtering so what
96:45 - you can see here is um we Define this
96:49 - search value no and we iterate through
96:53 - our documents we grade them if any
96:56 - document uh is graded as not relevant we
97:00 - flag this search thing to yes that means
97:04 - we're going to perform web search we
97:06 - then add that to our state dict at the
97:08 - end so run web search now that value is
97:10 - true that's
97:12 - it and you can kind of see we go through
97:14 - some other nodes here there's web search
97:16 - node um now here is where our one
97:20 - conditional Edge we Define right here
97:23 - this is where where we decide to
97:24 - generate or not based on that search key
97:27 - so we again get our state let's extract
97:30 - the various values so we have this
97:32 - search value now if search is yes we
97:37 - return the next no that we want to go to
97:40 - so in this case it'll be transform query
97:42 - which will then go to web search else we
97:46 - go to
97:47 - generate so what we can see is we laid
97:51 - out our graph which you can kind of see
97:53 - up
97:55 - here and now we Define functions for all
97:57 - those nodes as well as the conditional
98:01 - Edge and now we scroll down all we have
98:05 - to do is just lay that out here again as
98:08 - our flow and this is kind of what you
98:10 - might think of as like kind of flow
98:11 - engineering where you're just laying out
98:13 - the graph as you drew it where we have
98:17 - set our entry point as retrieve we're
98:19 - adding an edge between retrieve and
98:21 - grade documents so we went retrieval
98:23 - grade documents we add our conditional
98:25 - Edge depending on the grade either
98:28 - transform the query go to web search or
98:31 - just go to generate we create an edge
98:34 - between transform the query and web
98:35 - search then web search to generate and
98:38 - then we also have an edge generate to
98:40 - end and that's our whole graph that's it
98:42 - so we can just run
98:44 - this and now I'm going to ask a question
98:47 - so let's just say um how does agent
98:51 - memory work for example let's just try
98:53 - that and what this is going to do is
98:55 - going to print out what's going on as we
98:58 - run through this graph so um first we
99:01 - going to see output from
99:03 - retrieve this is going to be all of our
99:05 - documents that we retrieved so that's
99:07 - that's fine this just from our our
99:08 - retriever then you can see that we're
99:11 - doing a relevance check across our
99:13 - documents and this is kind of
99:15 - interesting right you can see we grading
99:17 - them here one is grade as not
99:20 - relevant um and okay you can see the
99:23 - documents are now filtered because we
99:24 - removed the one that's not relevant and
99:26 - because one is not relevant we decide
99:29 - okay we're going to just transform the
99:31 - query and run web
99:33 - search and um you can see after query
99:37 - transformation we rewrite the question
99:39 - slightly we then run web
99:42 - search um and you can see from web
99:44 - search it searched from some additional
99:47 - sources um which you can actually see
99:50 - here it's
99:51 - appended as a so here it is so here it's
99:55 - a new document appended from web search
99:57 - which is from memory knowledge
99:59 - requirements so it it basically looked
100:01 - up some AI architecture related to
100:03 - memory uh web results so that's fine
100:06 - that's exactly what we want to
100:08 - do and then um we generate a
100:11 - response so that's great and this is
100:14 - just showing you everything in kind of
100:15 - gory detail but I'm going to show you
100:18 - one other thing that's that's really
100:19 - nice about this if I go to lsmith
100:24 - I have my AP I ke set so all my
100:26 - Generations are just logged to to lsmith
100:29 - and I can see my Lang graph run here now
100:33 - what's really cool is this shows me all
100:36 - of my nodes so remember we had retrieve
100:42 - grade we evaluated the grade because one
100:45 - was irrelevant we then went ahead and
100:47 - transformed the query we did a web
100:50 - search we pended that to our context you
100:52 - can see all those steps are laid out
100:54 - here in fact you can even look at every
100:56 - single uh grader and its output I will
100:59 - move this up
101:01 - slightly um so you can see the the
101:04 - different scores for grades okay so this
101:06 - particular retrieval was graded as as
101:09 - not relevant so that's fine that that
101:11 - can happen in some cases and because of
101:15 - that um we did a query transformation so
101:18 - we modified the question slightly how
101:21 - does memory how does the memory system
101:23 - an artificial agents function so it's
101:25 - just a minor rephrasing of the question
101:27 - we did this Tav web search this is where
101:30 - it queried from this particular blog
101:33 - post from medium so it's like a sing web
101:35 - query we can like sanity check it and
101:37 - then what's need is we can go to our
101:39 - generate step look at open Ai and here's
101:41 - our full prompt how does the memory
101:43 - system in our official agents function
101:46 - and then here's all of our documents so
101:49 - this is the this is the web search as
101:51 - well as we still have the Rel chunks
101:54 - that were retrieved from our blog posts
101:57 - um and then here's our answer so that's
102:01 - really it you can see how um really
102:05 - moving from the notion of just like I'll
102:09 - actually go back to the original um
102:12 - moving
102:14 - from uh I will try to open this up a
102:17 - little
102:18 - bit um
102:24 - yeah I can see my face
102:26 - still um the transition from laying out
102:33 - simple
102:35 - chains to
102:38 - flows is a really interesting and
102:40 - helpful way of thinking about why graphs
102:42 - are really interesting because you can
102:45 - encode more sophisticated logical
102:48 - reasoning
102:49 - workflows but in a
102:51 - very like clean and well-engineered way
102:56 - where you can specify all the
102:58 - transitions that you actually want to
103:00 - have
103:01 - executed um and I actually find this way
103:05 - of thinking and building kind of logical
103:08 - uh like workflows really
103:10 - intuitive um we have a blog post coming
103:13 - out uh tomorrow that discusses both
103:17 - implementing self rag as well as C rag
103:20 - for two different active rag approaches
103:22 - using using uh this idea of of State
103:25 - machines and Lang graph um so I
103:28 - encourage you to play with it uh I found
103:31 - it really uh intuitive to work with um I
103:35 - also found uh inspection of traces to be
103:38 - quite intuitive using Lang graph because
103:43 - every node is enumerated pretty clearly
103:46 - for you which is not always the case
103:48 - when you're using other types of of more
103:50 - complex reasoning approaches for example
103:52 - like agents so in any case um I hope
103:56 - this was helpful and I definitely
103:58 - encourage you to check out um kind of
104:00 - this notion of like flow engineering
104:02 - using Lang graph and in the context of
104:04 - rag it can be really powerful hopefully
104:06 - as you've seen here thank
104:09 - you hey this is Lance from Lang chain I
104:12 - want to talk to a recent paper that I
104:14 - saw called adaptive rag which brings
104:16 - together some interesting ideas that
104:17 - have kind of been covered in other
104:18 - videos but this actually ties them all
104:21 - together in kind of a fun way so the the
104:23 - two big ideas to talk about here are one
104:27 - of query analysis so we've actually done
104:30 - kind of a whole rag from scratch series
104:31 - that walks through each of these things
104:32 - in detail but this is a very nice
104:35 - example of how this comes together um
104:37 - with some other ideas we've been talking
104:39 - about so query analysis is typically the
104:41 - process of taking an input question and
104:44 - modifying in some way uh to better
104:46 - optimize retrieval there's a bunch of
104:48 - different methods for this it could be
104:50 - decomposing it into sub questions it
104:52 - could be using some clever techniques
104:54 - like stepb back prompting um but that's
104:57 - kind of like the first stage of query
104:59 - analysis then typically you can do
105:01 - routing so you route a question to one
105:03 - of multiple potential sources it could
105:05 - be one or two different Vector stores it
105:08 - could be relational DB versus Vector
105:10 - store it could be web search it could
105:12 - just be like an llm fallback right so
105:15 - this is like one kind of big idea query
105:17 - analysis right it's kind of like the
105:18 - front end of your rag pipeline it's
105:20 - taking your question it's modifying it
105:22 - in some way it's sending it to the right
105:24 - place be it a web search be it a vector
105:26 - store be it a relational DB so that's
105:29 - kind of topic one now topic two is
105:32 - something that's been brought up in a
105:34 - few other videos um of what I kind of
105:36 - call Flow engineering or adaptive rag
105:40 - which is the idea of doing tests in your
105:43 - rag pipeline or in your rag inference
105:45 - flow uh to do things like check
105:47 - relevance documents um check whether or
105:51 - not the answer contains hallucinations
105:53 - so this recent blog post from Hamil
105:55 - Hussein actually covers evaluation in in
105:58 - some really nice detail and one of the
106:00 - things he highlighted
106:01 - explicitly is actually this topic so he
106:04 - talks about unit tests and in particular
106:06 - he says something really interesting
106:08 - here he says you know unlike typical
106:09 - unit tests you want to organize these
106:11 - assertions in places Beyond typical unit
106:14 - testing such as data cleaning and here's
106:17 - the key Point automatic retries during
106:19 - model inference that's the key thing I
106:21 - want to like draw your attention to to
106:23 - it's a really nice approach we've talked
106:25 - about some other papers that do that
106:26 - like corrective rag self rag but it's
106:28 - also cool to see it here and kind of
106:30 - encapsulated in this way the main idea
106:33 - is that you're using kind of unit tests
106:34 - in your flow to make Corrections like if
106:38 - your retrieval is bad you can correct
106:39 - from that if your generation has
106:41 - hallucinations you can correct from that
106:43 - so I'm going to kind of draw out like a
106:45 - cartoon diagram of what we're going to
106:47 - do here and you can kind of see it here
106:50 - we're starting with a question we talked
106:52 - about query analysis we're going to take
106:53 - our question and we're going to decide
106:55 - where it needs to go and for this
106:57 - particular toy example I'm going to say
106:59 - either send it to a vector store send it
107:01 - to web search or just have the llm
107:03 - answer it right so that's like kind of
107:05 - my fallback
107:06 - Behavior then we're going to bring in
107:08 - that idea of kind of online flow
107:10 - engineering or unit testing where I'm
107:13 - going to have my retrieval either from
107:15 - the VOR store or web search I'm then
107:17 - going to ask is this actually relevant
107:19 - to the question if it isn't I'm actually
107:21 - going to kick back to web sech so this
107:23 - is a little bit more relevant in the
107:24 - case if I've routed to to the vector
107:27 - store done retrieval documents aren't
107:29 - relevant I'll have a fallback
107:32 - mechanism um then I'm going to generate
107:34 - I check for hallucinations in my
107:36 - generation and then I check for um for
107:39 - whether or not the the generation
107:41 - actually answers the question then I
107:42 - return my answer so again we're tying
107:44 - together two ideas one is query analysis
107:47 - like basically taking a question routing
107:49 - it to the right place modifying it as
107:51 - needed and then kind of online unit
107:54 - testing and iterative flow
107:56 - feedback so to do this I've actually
108:00 - heard a lot of people talk online about
108:02 - command r a new model release from gooh
108:04 - here it has some pretty nice properties
108:07 - that I was kind of reading about
108:08 - recently so one it has nice support for
108:11 - Tool use and it does support query
108:14 - writing in the context of tool use uh so
108:18 - this all rolls up in really nice
108:20 - capabilities for routing it's kind of
108:22 - one now two it's small it's 35 billion
108:26 - parameter uh it's actually open weight
108:28 - so you can actually run this locally and
108:30 - I've tried that we can we can talk about
108:31 - that later uh so and it's also fast
108:34 - served via the API so it's kind of a
108:36 - small model and it's well tuned for rag
108:39 - so I heard a lot of people talking about
108:40 - using coher for Rag and it has a large
108:43 - context 120,000 tokens so this like a
108:45 - nice combination of properties it
108:47 - supports to and routing it's small and
108:49 - fast so it's like quick for grading and
108:51 - it's well tuned for rag so it's actually
108:54 - a really nice fit for this particular
108:55 - workflow where I want to do query
108:57 - analysis and routing and I want to do
108:59 - kind of online checking uh and rag so
109:02 - kind of there you go now let's just get
109:05 - to the coding bit so I have a notebook
109:07 - kind of like usual I've done a few pip
109:09 - installs you can see it's nothing exotic
109:11 - I'm bringing Lang chain coh here I set
109:13 - my coher API key now I'm just going to
109:16 - set this Lang chain project within
109:18 - lsmith so all my traces for this go to
109:20 - that project and I have enabled tracing
109:23 - so I'm using Langs Smith here so we're
109:25 - going to walk through this flow and
109:27 - let's do the first thing let's just
109:28 - build a vector store so I'm going to
109:30 - build a vector store using coherent
109:32 - beddings with chroma open source Vector
109:34 - DB runs locally from three different web
109:37 - pages on blog post that I like so it
109:39 - pertains to agents prompt engineering
109:41 - and adversarial attacks so now I have a
109:43 - retriever I can run retriever invoke and
109:46 - I can ask a question about you know
109:48 - agent
109:50 - memory agent
109:53 - memory and there we go so we get
109:56 - documents back so there we go we have a
109:58 - retriever now now here's where I'm going
110:01 - to bring in coh here I also want a
110:03 - router so you look at our flow the first
110:06 - step is this routing stage right so what
110:08 - I'm going to do is I'm guess we going to
110:10 - find two tools a web search tool and a
110:14 - vector store tool okay in my Preamble is
110:17 - just going to say you're an expert
110:18 - routing user questions to Vector store
110:20 - web search now here's the key I tell it
110:23 - what the vector store has so again my
110:25 - index my Vector store has agents prompt
110:28 - engineering adial tax I just repeat that
110:30 - here agents prompt adversarial tax so it
110:32 - knows what's in the vector store um so
110:35 - use it for questions on these topics
110:36 - otherwise use web search so that's it I
110:40 - use command R here now I'm going to bind
110:42 - these tools to the model and attach the
110:44 - Preamble and I have a structured LM
110:47 - router so let's give it a let's give
110:49 - this a few tests just to like kind of
110:51 - sandbox this a little bit
110:53 - so I can inval here's my chain I have a
110:55 - router prompt I pass that to the
110:56 - structured LM router which I defined
110:58 - right here and um let's ask a few
111:01 - different questions like who will the
111:02 - Bears draft in the NFL draft with types
111:05 - of agent memory and Hi how are you so
111:08 - I'm going to kick that off and you can
111:10 - see you know it does web search it does
111:13 - it goes to Vector store and then
111:14 - actually returns this false so that's
111:16 - kind of interesting um this is actually
111:19 - just
111:20 - saying if it does not use either tool so
111:23 - for that particular query web search or
111:26 - the vector store was inappropriate it'll
111:28 - just say hey I didn't call one of those
111:30 - tools so that's interesting we'll use
111:31 - that later so that's my router tool now
111:35 - the second thing is my grader and here's
111:38 - where I want to show you something
111:40 - really nice that is generally useful uh
111:43 - for many different problems you might
111:45 - encounter so here's what I'm doing I'm
111:48 - defining a data model uh for My Grade so
111:51 - basically grade documents it's going to
111:54 - have this is a pantic object it is just
111:57 - basically a binary score here um field
112:00 - specified here uh documents are relevant
112:03 - to the question yes no I have a preamble
112:05 - your grer assessing relevance of
112:06 - retrieve documents to a user question um
112:09 - blah blah blah so you know and then
112:11 - basically give it a b score yes no I'm
112:13 - using command R but here's the catch I'm
112:16 - using this wi structured outputs thing
112:18 - and I'm passing my grade documents uh
112:21 - data model to that that so this is the
112:23 - key thing we can test this right now as
112:26 - well it's going to return an object
112:29 - based on the schema I give it which is
112:31 - extremely useful for all sorts of use
112:33 - cases and let's actually Zoom back up so
112:36 - we're actually right here so this
112:39 - greater stage right I want to constrain
112:41 - the output to yes no I don't want any
112:43 - preambles I want anything because the
112:45 - logic I'm going to build in this graph
112:48 - is going to require a yes no binary
112:50 - response from this particular Edge in
112:52 - our graph
112:53 - so that's why this greater tool is
112:55 - really
112:56 - useful and I'm asking like a mock
112:59 - question types of agent memory I do a
113:01 - retriever I do a retrieval from our
113:03 - Vector store I get the tuck and I test
113:05 - it um I invoke our greater retrieval
113:08 - grater chain with the question the doc
113:11 - text and it's relevant as we would
113:13 - expect so that's good but again let's
113:16 - just kind of look at that a little bit
113:17 - more closely what's actually happening
113:19 - under the hood here here's the pantic
113:21 - object we passed
113:23 - here's the document in question I'm
113:24 - providing basically it's converting this
113:26 - object into coher function schema it's
113:29 - binding that to the
113:31 - llm we pass in the document question it
113:34 - returns an object basic a Json string
113:37 - per our pantic schema that's it and then
113:41 - it's just going to like parse that into
113:42 - a pantic object which we get at the end
113:44 - of the day so that's what's happening
113:45 - under the hood with this with structured
113:47 - output thing but it's extremely useful
113:49 - and you'll see we're going to use that a
113:50 - few different places um um because we
113:53 - want to ensure that in our in our flow
113:57 - here we have three different grading
113:58 - steps and each time we want to constrain
114:00 - the output to yes no we're going to use
114:02 - that structured output more than
114:04 - once um this is just my generation so
114:07 - this is good Old Rag let's just make
114:09 - sure that works um I'm using rag chain
114:12 - typical rag prompt again I'm using
114:15 - cohere for rag pretty easy and yeah so
114:19 - the rag piece works that's totally fine
114:21 - nothing to it crazy there um I'm going
114:24 - to find this llm fallback so this is
114:27 - basically if you saw a router chain if
114:31 - it doesn't use a tool I want to fall
114:33 - back and just fall back to the llm so
114:35 - I'm going to kind of build that as a
114:37 - little chain here so okay this is just a
114:39 - fallback I have my Preamble just you're
114:42 - you're an assistant answer the question
114:43 - based upon your internal knowledge so
114:46 - again that fallback behavior is what we
114:48 - have here so what we've done already is
114:51 - we defined our router piece we've
114:53 - defined our our basic retrieval our
114:56 - Vector store we already have here um
114:58 - we've defined our first logic or like
115:01 - grade check and we defined our fallback
115:04 - and we're just kind of roll through the
115:05 - parts of our graph and Define each piece
115:08 - um so I'm going to have two other
115:10 - graders and they're going to use the
115:11 - same thing we just talked about slightly
115:14 - different data model I mean same output
115:16 - but actually just slightly different uh
115:19 - prompt um and you know descript destion
115:22 - this in this case is the aners grounded
115:24 - the facts yes no this is my
115:25 - hallucination
115:26 - grater uh and then I have an answer
115:28 - grader as well and I've also run a test
115:31 - on each one and you can see I'm getting
115:32 - binary this this these objects out have
115:35 - a binary score so this a pantic object
115:38 - with a binary score uh and that's
115:39 - exactly what we want cool
115:43 - and I have a Search tool so that's
115:46 - really nice we've actually gone through
115:48 - and we've kind of laid out I have like a
115:50 - router I've tested it we have a vector
115:51 - story tested we've tested each of our
115:54 - graders here we've also tested
115:55 - generation of just doing rag so we have
115:58 - a bunch of pieces built here we have a
116:00 - fallback piece we have web search now
116:02 - the question is how do I Stitch these
116:04 - together into this kind of flow and for
116:06 - that I I like to use Lang graph we'll
116:09 - talk a little about Lang graph versus
116:10 - agents a bit later but I want to show
116:12 - you why this is really easy to do using
116:14 - Lang graph so what's kind of nice is
116:17 - I've kind of laid out all my logic here
116:19 - we've tested individually and now all
116:21 - I'm going to do
116:22 - is I'm going to first lay out uh the
116:25 - parts of my graph so what you're going
116:27 - to notice here is first there's a graph
116:30 - state so this state represents kind of
116:32 - the key parts of the graph or the key
116:35 - kind of objects within the graph that
116:36 - we're going to be modifying so this is
116:38 - basically a graph centered around rag
116:40 - we're going to have question generation
116:42 - and documents that's really kind of the
116:44 - main things we're going to be working
116:45 - with in our
116:46 - graph so then you're going to see
116:49 - something that's pretty intuitive I
116:51 - think what you're going to see is we're
116:53 - going to basically walk through this
116:55 - flow and for each of these little
116:57 - circles we're just going to find a
116:58 - function and these uh little squares or
117:02 - these these you can think about every
117:04 - Circle as a node and every kind of
117:06 - diamond here as as an edge or
117:08 - conditional Edge so that's actually what
117:10 - we're going to do right now we're going
117:11 - to lay out all of our nodes and edges
117:14 - and each one of them are just going to
117:15 - be a function and you're going to see
117:16 - how we do that right now so I'm going to
117:20 - go down here I def find my graph state
117:22 - so this is what's going to be kind of
117:23 - modified and propagated throughout my
117:24 - graph now all I'm going to do is I'm
117:27 - just going to find a function uh for
117:29 - each of those nodes so let me kind of go
117:32 - side by side and show you the diagram
117:33 - and then like kind of show the nodes
117:35 - next to it so here's the
117:38 - diagram so we have uh a retrieve node so
117:42 - that kind of represents our Vector store
117:43 - we have a fallback node that's this
117:46 - piece we have a generate node so that's
117:49 - basically going to do our rag you can
117:51 - see there we have a grade documents node
117:54 - kind of right
117:55 - here um and we have a web search node so
117:58 - that's right here cool now here's where
118:02 - we're actually to find the edges so you
118:04 - can see our edges are the pieces of the
118:05 - graph that are kind of making different
118:06 - decisions so this route question Edge
118:09 - basic conditional Edge is basically
118:12 - going to take an input question and
118:14 - decide where it needs to go and that's
118:15 - all we're doing down here it kind of
118:18 - follows what we did up at the top where
118:20 - we tested this individually so recall we
118:23 - basically just invoke that question
118:24 - router returns our source now remember
118:27 - if tool calls were not in the source we
118:30 - do our fall back so we show actually
118:32 - showed that all the way up here remember
118:34 - this if tool calls is not in the
118:36 - response this thing will just be false
118:38 - so that means we didn't either we didn't
118:39 - call web search and we didn't call uh
118:43 - our retriever tool so then we're just
118:45 - going to fall
118:46 - back
118:48 - um yep right here and this is just like
118:53 - uh you know a catch just in case a tool
118:54 - could make a decision but most
118:57 - interestingly here's where we choose a
118:59 - data source basically so um this is the
119:02 - output of our tool call we're just going
119:05 - to fish out the name of the tool so
119:08 - that's data source and then here we go
119:10 - if the data source is web search I'm
119:12 - returning web search as basically the
119:14 - next node to go to um otherwise if it's
119:18 - Vector store we return Vector store as
119:19 - the next node to go to so what's this
119:22 - search thing well remember we right up
119:24 - here Define this node web search that's
119:27 - it we're just going to go to that node
119:30 - um what's this Vector store um you'll
119:32 - see below how we can kind of tie these
119:35 - strings that we returned from the
119:36 - conditional Edge to the node we want to
119:38 - go to that's really it um same kind of
119:42 - thing here decide to generate that's
119:44 - going to roll in these two conditional
119:46 - edges into one um and basically it's
119:49 - going to do if there's no documents so
119:51 - basic basically if we filtered out all
119:54 - of our documents from this first test
119:56 - here um then what we're going to do is
120:01 - we've decided all documents are not
120:02 - relevant to the question and we're going
120:04 - to kick back to web search exactly as we
120:06 - show here so that's this piece um
120:09 - otherwise we're going to go to generate
120:11 - so that's this piece so again in these
120:14 - conditional edges you're basically
120:15 - implementing the logic that you see in
120:17 - our diagram right here that's all that's
120:19 - going on um and again this is just
120:23 - implementing the final two checks uh for
120:25 - hallucinations and and answer
120:27 - relevance um
120:30 - and um yep so here's our hallucination
120:34 - grader we then extract the grade if the
120:38 - if basically there are
120:39 - hallucinations um oh sorry in this case
120:42 - the grade actually yes means that the
120:45 - answer is grounded so we say answer is
120:48 - actually grounded and then we go to the
120:50 - next step we go to the next test that's
120:52 - all this is doing it's just basically
120:53 - wrapping this logic that we're
120:55 - implementing here in our graph so that's
120:57 - all that's going on and let's go ahead
120:59 - and Define all those things so nice we
121:02 - have all that um now we can actually go
121:05 - down a little bit and we can pull
121:09 - um this is actually where we stitch
121:12 - together everything so all it's
121:13 - happening here is you see we defined all
121:16 - these functions up here we just add them
121:18 - as nodes in our graph here and then we
121:21 - build our graph here basically by by
121:24 - basically laying out the flow or the
121:26 - connectivity between our nodes and edges
121:29 - so you know you can look at this
121:30 - notebook to kind of study in a bit of
121:31 - detail what's going on but frankly what
121:33 - I like to do here typically just draw
121:36 - out a graph kind of like we did up
121:38 - here and then Implement uh the Lo
121:42 - logical flow here in your graph as nodes
121:45 - and edges just like we're doing here
121:47 - that's all that's happening uh so again
121:49 - we have like our entry point is the
121:51 - router
121:52 - um this is like the output is this is
121:55 - basically directing like here's what the
121:57 - router is outputting and here's the next
122:00 - node to go to so that's it um and then
122:03 - for each node we're kind of applying
122:05 - like we're saying like what's what's the
122:06 - flow so web search goes to generate
122:09 - after um and retrieve goes to grade
122:12 - documents grade documents um kind of is
122:16 - is like is a conditional Edge um
122:18 - depending on the results we either do
122:19 - web search or generate and then our
122:21 - second one we go from generate to uh
122:25 - basically this grade uh generation
122:27 - versus documents in question based on
122:29 - the output of that we either have
122:31 - hallucinations we regenerate uh we found
122:34 - that the answer is not useful we kick
122:36 - back to web search or we end um finally
122:39 - we have that llm fallback and that's
122:41 - also if we go to the fallback we end so
122:44 - what you're seeing here is actually the
122:46 - the logic flow we're laying out in this
122:49 - graph matches the diagram
122:52 - that we laid out up top I'm just going
122:54 - to copy these over and I'll actually go
122:56 - then back to the diagram and and kind of
122:57 - underscore that a little bit more so
123:01 - here is the flow we've laid out again
123:03 - here is our diagram and you can kind of
123:05 - look at them side by side and see how
123:07 - they basically match up so here's kind
123:10 - of our flow diagram going from basically
123:12 - query analysis that's this thing this
123:15 - route question and you can see web
123:18 - search Vector store LM fallback LM
123:20 - fallback web search vector store so
123:22 - those are like the three options that
123:23 - can come out of this conditional Edge
123:26 - and then here's where we connect so if
123:27 - we go to web search then basically we
123:30 - next go to
123:31 - generate so that's kind of this whole
123:34 - flow um now if we go to
123:38 - retrieve um then we're going to grade so
123:41 - that's
123:42 - it um and you know it follows kind of as
123:46 - you can see here that's really it uh so
123:49 - it's just nice to draw the these
123:51 - diagrams out first and then it's pretty
123:54 - quick to implement each node and each
123:56 - Edge just as a function and then stitch
123:59 - them together in a graph just like I
124:00 - show here and of course we'll make sure
124:01 - this code's publ so you can use it as a
124:03 - reference um so there we go now let's
124:07 - try a few a few different test questions
124:10 - so like what player the Bears to draft
124:12 - and NFL draft right let's have a look at
124:14 - that and they should print everything
124:16 - it's doing as we go so okay this is
124:19 - important route question it just decides
124:21 - to route to web search that's good it
124:23 - doesn't go to our Vector store this is a
124:24 - current event not related to our Vector
124:26 - store at all it goes to web search um
124:29 - and then it goes to generate so that's
124:30 - what we'd expect so basically web search
124:32 - goes through to generate
124:35 - um and we check hallucinations
124:38 - Generations ground the documents we
124:40 - check generation versus question the
124:42 - generation addresses the question the
124:43 - Chicago Bears expected to draft Caleb
124:45 - Williams that's right that's that's the
124:48 - consensus so cool that works now let's
124:50 - ask a question related to our Vector
124:52 - store what are the types of agent memory
124:54 - we'll kick this off so we're routing
124:56 - okay we're routing to rag now look how
124:58 - fast this is that's really fast so we
125:01 - basically whip through that document
125:03 - grading determine they're all relevant
125:06 - uh we go to decision to
125:07 - generate um we check hallucinations we
125:10 - check answer versus question and there
125:13 - are several types of memory stored in
125:15 - the human brain memory can also be
125:16 - stored in G of Agents you have LM agents
125:19 - memory stream retrieval model and and
125:20 - reflection mechanism so it's
125:22 - representing what's captured on the blog
125:23 - post pretty reasonably now let me show
125:25 - you something else is kind of nice I can
125:27 - go to Langs Smith and I can go to my
125:29 - projects we create this new project
125:31 - coher adaptive rag at the start and
125:33 - everything is actually logged there
125:35 - everything we just did so I can open
125:37 - this up and I can actually just kind of
125:40 - look through all the stages of my Lang
125:42 - graph to here's my retrieval stage um
125:45 - here's my grade document stage and we
125:47 - can kind of audit the grading itself we
125:49 - kind of looked at this one by one
125:51 - previously but it's actually pretty nice
125:53 - we can actually audit every single
125:54 - individual document grade to see what's
125:56 - happening um we can basically go through
126:00 - um to this is going to be one of the
126:02 - other graders here
126:05 - um yep so this is actually going to be
126:09 - the hallucination grading right here uh
126:12 - and then this is going to be the answer
126:13 - grading right here so that's really it
126:15 - you can kind of walk through the entire
126:16 - graph you can you can kind of study
126:18 - what's going on um which is actually
126:20 - very useful so it looks like this worked
126:23 - pretty well um and finally let's just
126:25 - ask a question that should go to that
126:27 - fallback uh path down at the bottom like
126:30 - not related at all to our Vector store
126:32 - current events and yeah hello I'm doing
126:34 - well so it's pretty neat we've seen in
126:36 - maybe 15 minutes we've from scratch
126:38 - built basically a semi- sophisticated
126:40 - rag system that has agentic properties
126:42 - we've done in Lang graph we've done with
126:44 - coher uh command R you can see it's
126:46 - pretty darn fast in fact we can go to
126:48 - Langs Smith and look at so this whole
126:50 - thing took 7 seconds uh that is not bad
126:54 - let's look at the most recent one so
126:56 - this takes one second so the fallback
126:58 - mechanism to the LM is like 1 second um
127:01 - the let's just look here so 6 seconds
127:04 - for the initial uh land graph so this is
127:08 - not bad at all it's quite fast it done
127:10 - it does quite a few different checks we
127:12 - do routing uh and then we have kind of a
127:15 - bunch of nice fallback behavior and
127:17 - inline checking uh for both relevance
127:20 - hallucinations and and answer uh kind of
127:23 - groundedness or answer usefulness so you
127:27 - know this is pretty nice I definitely
127:28 - encourage you to play with a notebook
127:30 - command R is a really nice option for
127:31 - this due to the fact that is tool use
127:33 - routing uh small and quite fast and it's
127:36 - really good for Rags it's a very nice
127:38 - kind of uh a very nice option for
127:41 - workflows like this and I think you're
127:43 - going to see more and more of this kind
127:44 - of like uh adaptive or self-reflective
127:47 - rag um just because this is something
127:50 - that a lot systems can benefit from like
127:53 - a a lot of production rack systems kind
127:55 - of don't necessarily have
127:57 - fallbacks uh depending on for example
128:00 - like um you know if the documents
128:03 - retrieved are not relevant uh if the
128:05 - answer contains hallucinations and so
128:07 - forth so this opportunity to apply
128:09 - inline checking along with rag is like a
128:11 - really nice theme I think we're going to
128:13 - see more and more of especially as model
128:15 - inference gets faster and faster and
128:17 - these checks get cheaper and cheaper to
128:18 - do kind of in the inference Loop
128:22 - now as a final thing I do want to bring
128:23 - up the a point about you know we've
128:25 - shown this Lang graph stuff what about
128:27 - agents you know how do you think about
128:28 - agents versus Lang graph right and and I
128:30 - think the way I like to frame this is
128:33 - that um Lang graph is really good
128:37 - for um flows that you have kind of very
128:41 - clearly defined that don't have like
128:43 - kind of open-endedness but like in this
128:45 - case we know the steps we want to take
128:47 - every time we want to do um basically
128:49 - query analysis routing and then we want
128:51 - to do a three grading steps and that's
128:53 - it um Lang graph is really good for
128:56 - building very reliable flows uh it's
128:58 - kind of like putting an agent on guard
129:00 - rails and it's really nice uh it's less
129:04 - flexible but highly reliable and so you
129:07 - can actually use smaller faster models
129:08 - with langra so that's the thing I like
129:10 - about we saw here command R 35 billion
129:12 - parameter model works really well with
129:14 - langra quite quick we' were able to
129:16 - implement a pretty sophisticated rag
129:18 - flow really quickly 15 minutes um in
129:21 - time is on the order of like less than
129:23 - you know around 5 to 6 seconds so so
129:25 - pretty good right now what about agents
129:28 - right so I think Agents come into play
129:30 - when you want more flexible workflows
129:32 - you don't want to necessarily follow a
129:34 - defined pattern a priori you want an
129:36 - agent to be able to kind of reason and
129:38 - make of open-end decisions which is
129:40 - interesting for certain like long
129:41 - Horizon planning problems you know
129:43 - agents are really
129:44 - interesting the catch is that
129:46 - reliability is a bit worse with agents
129:48 - and so you know that's a big question a
129:50 - lot of people bring up and that's kind
129:52 - of where larger LMS kind of come into
129:53 - play with a you know there's been a lot
129:55 - of questions about using small LMS even
129:57 - open source models with agents and
129:59 - reliabilities kind of continuously being
130:01 - an issue whereas I've been able to run
130:03 - these types of land graphs with um with
130:06 - uh like mraw or you know command R
130:09 - actually is open weights you can run it
130:10 - locally um I've been able to run them
130:12 - very reproducibly with open source
130:14 - models on my laptop um so you know I
130:17 - think there's a tradeoff and Comm
130:20 - actually there's a new coher model
130:21 - coming out uh believe command R plus
130:25 - which uh is a larger model so it's
130:27 - probably more suitable for kind of more
130:29 - open-ended agentic use cases and there's
130:32 - actually a new integration with Lang
130:33 - chain that support uh coher agents um
130:36 - which is quite nice so I think it's it's
130:38 - worth experimenting for certain problems
130:40 - in workflows you may need more
130:42 - open-ended reasoning in which case use
130:43 - an agent with a larger model otherwise
130:46 - you can use like Lang graph for more uh
130:49 - a more reliable potential
130:51 - but con strain flow and it can also use
130:53 - smaller models faster LMS so those are
130:56 - some of the trade-offs to keep in mind
130:57 - but anyway encourage you play with a
130:59 - notebook explore for yourself I think
131:01 - command R is a really nice model um I've
131:03 - also been experimenting with running it
131:04 - locally with AMA uh currently the
131:07 - quantise model is like uh two bit
131:09 - quantise is like 13 billion uh or so uh
131:14 - yeah 13 gigs it's it's a little bit too
131:17 - large to run quickly locally for me
131:22 - um inference for things like rag we're
131:25 - on the order of 30 seconds so again it's
131:27 - not great for a live demo but it does
131:29 - work it is available on a llama so I
131:30 - encourage you to play with that I have a
131:32 - Mac M2 32 gig um so you know if I if
131:35 - you're a larger machine then it
131:36 - absolutely could be worth working with
131:38 - locally so encourage you to play with
131:40 - that anyway hopefully this was useful
131:41 - and interesting I think this is a cool
131:43 - paper cool flow um coher command R is a
131:46 - nice option for these types of like
131:48 - routing uh it's quick good with Lang
131:52 - graph good for rag good for Tool use so
131:55 - you know have a have a look and uh you
131:57 - know reply anything uh any feedback in
132:00 - the comments
132:06 - thanks hi this is Lance from Lang chain
132:09 - this is a talk I gave at two recent
132:10 - meetups in San Francisco called is rag
132:13 - really dead um and I figured since you
132:16 - know a lot of people actually weren't
132:17 - able to make those meetups uh I just
132:19 - record this and put this on YouTube and
132:21 - see if this is of interest to folks um
132:24 - so we all kind of recognize that Contex
132:26 - windows are getting larger for llms so
132:29 - on the x-axis you can see the tokens
132:31 - used in pre-training that's of course
132:33 - you know getting larger as well um
132:35 - proprietary models are somewhere over
132:37 - the two trillion token regime we don't
132:39 - quite know where they sit uh and we've
132:41 - all the way down to smaller models like
132:43 - 52 trained on far fewer tokens um but
132:46 - what's really notable is on the y axis
132:49 - you can see about a year ago da the art
132:52 - models were on the order of 4,000 to
132:54 - 8,000 tokens and that's you know dozens
132:56 - of pages um we saw Claude 2 come out
132:59 - with the 200,000 token model earlier I
133:02 - think it was last year um gbd4 128,000
133:06 - tokens now that's hundreds of pages and
133:09 - now we're seeing Claud 3 and Gemini come
133:11 - out with million token models so this is
133:13 - hundreds to thousands of pages so
133:16 - because of this phenomenon people have
133:17 - been kind of wondering is rag dead if
133:19 - you can stuff you know many thousands of
133:22 - pages into the context window llm why do
133:24 - you need a reteval system um it's a good
133:27 - question spoke sparked a lot of
133:29 - interesting debate on Twitter um and
133:32 - it's maybe first just kind of grounding
133:33 - on what is rag so rag is really the
133:35 - process of reasoning and retrieval over
133:37 - chunks of of information that have been
133:40 - retrieved um it's starting with you know
133:42 - documents that are indexed um they're
133:45 - retrievable through some mechanism
133:47 - typically some kind of semantic
133:48 - similarity search or keyword search
133:50 - other mechanisms
133:51 - retriev docs should then pass to an llm
133:54 - and the llm reasons about them to ground
133:56 - response to the question in the retrieve
133:58 - document so that's kind of the overall
134:00 - flow but the important point to make is
134:02 - that typically it's multiple documents
134:04 - and involve some form of
134:06 - reasoning so one of the questions I
134:08 - asked recently is you know if long
134:10 - condex llms can replace rag it should be
134:12 - able to perform you know multia
134:15 - retrieval and reasoning from its own
134:16 - context really effectively so I teamed
134:19 - up with Greg Cameron uh to kind of
134:21 - pressure test this and he had done some
134:23 - really nice needle the Haack analyses
134:25 - already focused on kind of single facts
134:28 - called needles placed in a Hy stack of
134:31 - Paul Graham essays um so I kind of
134:33 - extended that to kind of mirror the rag
134:36 - use case or kind of the rag context uh
134:39 - where I took multiple facts so I call it
134:41 - multi needle um I buil on a funny needle
134:45 - in the HTO challenge published by
134:46 - anthropic where they add they basically
134:48 - placed Pizza ingredients in the context
134:51 - uh and asked the LM to retrieve this
134:53 - combination of pizza ingredients I did I
134:56 - kind of Rift on that and I basically
134:58 - split the pizza ingredients up into
134:59 - three different needles and place those
135:01 - three ingredients in different places in
135:03 - the context and then ask the um to
135:06 - recover those three ingredients um from
135:09 - the context so again the setup is the
135:12 - question is what the secret ingredients
135:13 - need to build a perfect Pizza the
135:15 - needles are the ingredients figs Pudo
135:17 - goat cheese um I place them in the
135:21 - context at some specified intervals the
135:23 - way this test works is you can basically
135:25 - set the percent of context you want to
135:28 - place the first needle and the remaining
135:30 - two are placed at roughly equal
135:31 - intervals in the remaining context after
135:33 - the first so that's kind of the way the
135:35 - test is set up now it's all open source
135:36 - by the way the link is below so needs
135:39 - are placed um you ask a question you
135:42 - promp L them with with kind of um with
135:45 - this context and the question and then
135:47 - produces the answer and now the the
135:48 - framework will grade the response
135:51 - both one are you know all are all the
135:55 - the specified ingredients present in the
135:56 - answer and two if not which ones are
136:00 - missing so I ran a bunch of analysis on
136:03 - this with GPD 4 and came kind of came up
136:05 - with some with some fun results um so
136:07 - you can see on the left here what this
136:09 - is looking at is different numbers of
136:11 - needles placed in 120,000 token context
136:14 - window for
136:15 - gbd4 and I'm asking um gbd4 to retrieve
136:20 - either one three or 10 needles now I'm
136:24 - also asking it to do reasoning on those
136:26 - needles that's what you can see in those
136:28 - red bars so green is just retrieve the
136:30 - ingredients red is reasoning and the
136:32 - reasoning challenge here is just return
136:34 - the first letter of each ingredient so
136:37 - we find is basically two things the
136:39 - performance or the percentage of needles
136:42 - retrieved drops with respect to the
136:44 - number of needles that's kind of
136:45 - intuitive you place more facts
136:48 - performance gets worse but also it gets
136:50 - worse if you ask it to reason so if you
136:53 - say um just return the needles it does a
136:56 - little bit better than if you say return
136:58 - the needles and tell me the first letter
137:00 - so you overlay reasoning so this is the
137:02 - first observation more facts is harder
137:06 - uh and reasoning is harder uh than just
137:09 - retrieval now the second question we ask
137:11 - is where are these needles actually
137:12 - present in the context that we're
137:14 - missing right so we know for example um
137:18 - retrieval of um 10 needles is around 60%
137:23 - so where are the missing needles in the
137:26 - context so on the right you can see
137:28 - results telling us actually which
137:30 - specific needles uh are are the model
137:33 - fails to retrieve so what we can see is
137:36 - as you go from a th000 tokens up to
137:39 - 120,000 tokens on the X here and you
137:42 - look at needle one place at the start of
137:44 - the document to needle 10 placed at the
137:46 - end at a th000 token context link you
137:49 - can retrieve them all so again kind of
137:52 - match what we see over here small well
137:54 - actually sorry over here everything I'm
137:56 - looking at is 120,000 tokens so that's
137:59 - really not the point uh the point is
138:01 - actually smaller context uh better
138:04 - retrieval so that's kind of point one um
138:08 - as I increase the context window I
138:10 - actually see that uh there is increased
138:14 - failure to retrieve needles which you
138:15 - see can see in red here towards the
138:18 - start of the
138:19 - document um and so this is an
138:21 - interesting result um and it actually
138:23 - matches what Greg saw with single needle
138:25 - case as well so the way to think about
138:27 - it is it appears that um you know if you
138:31 - for example read a book and I asked you
138:33 - a question about the first chapter you
138:34 - might have forgotten it same kind of
138:36 - phenomenon appears to happen here with
138:38 - retrieval where needles towards the
138:40 - start of the context are are kind of
138:42 - Forgotten or are not well retrieved
138:45 - relative to those of the end so this is
138:47 - an effect we see with gbd4 it's been
138:49 - reproduced quite a bit so ran nine
138:51 - different trials here Greg's also seen
138:53 - this repeatedly with single needle so it
138:55 - seems like a pretty consistent
138:57 - result and there's an interesting point
138:59 - I put this on Twitter and a number of
139:00 - folks um you know replied and someone
139:03 - sent me this paper which is pretty
139:04 - interesting and it mentions recency bias
139:07 - is one possible reason so the most
139:09 - informative tokens for predicting the
139:10 - next token uh you know are are are
139:14 - present close to or recent to kind of
139:17 - where you're doing your generation and
139:18 - so there's a bias to attend to recent
139:20 - tokens which is obviously not great for
139:23 - the retrieval problem as we saw here so
139:26 - again the results show us that um
139:30 - reasoning is a bit harder than retrieval
139:32 - more needles is more difficult and
139:34 - needles towards the start of your
139:36 - context are harder to retrieve than
139:38 - towards the end those are three main
139:40 - observations from this and it maybe
139:42 - indeed due to this recency bias so
139:45 - overall what this kind of tells you is
139:47 - be wary of just context stuffing in
139:49 - large long context
139:51 - there are no retrieval
139:52 - guarantees and also there's some recent
139:55 - results that came out actually just
139:56 - today suggesting that single needle may
139:58 - be misleadingly easy um you know there's
140:02 - no reasoning it's retrieving a single
140:04 - needle um and also these guys I'm I
140:08 - showed this tweet here show that um the
140:12 - in a lot of these needle and Haack
140:13 - challenges including mine the facts that
140:16 - we look for are very different than um
140:20 - the background kind of Hy stack of Paul
140:21 - Graham essays and so that may be kind of
140:23 - an interesting artifact they note that
140:25 - indeed if the needle is more subtle
140:28 - retrievals is worse so I think basically
140:31 - when you see these really strong
140:33 - performing needle and hyack analyses put
140:35 - up by model providers you should be
140:37 - skeptical um you shouldn't necessarily
140:39 - assume that you're going to get high
140:40 - quality retrieval from these long
140:41 - contact LMS uh for numerous reasons you
140:45 - need to think about retrieval of
140:46 - multiple facts um you need to think
140:48 - about reasoning on top of retrieval you
140:50 - need need to think about the subtlety of
140:52 - the retrieval relative to the background
140:54 - context because for many of these needle
140:56 - and the Haack challenges it's a single
140:57 - needle no reasoning and the needle
141:00 - itself is very different from the
141:01 - background so anyway those may all make
141:03 - the challenge a bit easier than a real
141:05 - world scenario of fact retrieval so I
141:07 - just want to like kind of lay out that
141:09 - those cautionary notes but you know I
141:13 - think it is fair to say this will
141:14 - certainly get better and I think it's
141:17 - also fair to say that rag will change
141:19 - and this is just like a nearly not a
141:21 - great joke but Frank zap a musician made
141:24 - the point Jazz isn't dead it just smells
141:26 - funny you know I think same for rag rag
141:28 - is not dead but it will change I think
141:30 - that's like kind of the key Point here
141:32 - um so just as a followup on that rag
141:35 - today's focus on precise retrieval of
141:37 - relevant doc chunks so it's very focused
141:39 - on typically taking documents chunking
141:42 - them in some particular way often using
141:44 - very OS syncratic chunking methods
141:46 - things like chunk size are kind of
141:48 - picked almost arbitrarily embeding them
141:50 - storing them in an index taking a
141:52 - question embedding it doing K&N uh
141:55 - similarity search to retrieve relevant
141:57 - chunks you're often setting a k
141:59 - parameter which is the number of chunks
142:00 - you retrieve you often will do some kind
142:02 - of filtering or Pro processing on the
142:04 - retrieve chunks and then ground your
142:06 - answer in those retrieved chunks so it's
142:08 - very focused on precise retrieval of
142:10 - just the right chunks now in a world
142:14 - where you have very long context models
142:16 - I think there's a fair question to ask
142:18 - is is this really kind of the most most
142:20 - reasonable approach so kind of on the
142:22 - left here you can kind of see this
142:25 - notion closer to today of I need the
142:27 - exact relevant chunk you can risk over
142:29 - engineering you can have you know higher
142:31 - complexity sensitivity to these odd
142:33 - parameters like chunk size k um and you
142:36 - can indeed suffer lower recall because
142:38 - you're really only picking very precise
142:40 - chunks you're beholden to very
142:41 - particular embedding models so you know
142:44 - I think going forward as long context
142:46 - models get better and better there are
142:48 - definitely question you should certainly
142:50 - question the current kind of very
142:52 - precise chunking rag Paradigm but on the
142:54 - flip side I think just throwing all your
142:56 - docs into context probably will also not
142:59 - be the preferred approach you'll suffer
143:01 - higher latency higher token usage I
143:03 - should note that today 100,000 token GPD
143:06 - 4 is like $1 per generation I spent a
143:09 - lot of money on Lang Chain's account uh
143:11 - on that multile analysis I don't want to
143:13 - tell Harrison how much I spent uh so
143:16 - it's it's you know it's not good right
143:18 - um You Can't audit retrieve
143:21 - um and security and and authentication
143:23 - are issues if for example you need
143:25 - different users different different
143:26 - access to certain kind of retriev
143:28 - documents or chunks in the Contex
143:30 - stuffing case you you kind of can't
143:32 - manage security as easily so there's
143:33 - probably some predo optimal regime kind
143:35 - of here in the middle and um you know I
143:39 - I put this out on Twitter I think
143:40 - there's some reasonable points raised I
143:42 - think you know this inclusion at the
143:44 - document level is probably pretty sane
143:46 - documents are self-contained chunks of
143:48 - context um so you know what about
143:51 - document Centric rag so no chunking uh
143:54 - but just like operate on the context of
143:56 - full documents so you know if you think
143:59 - forward to the rag Paradigm that's
144:01 - document Centric you still have the
144:03 - problem of taking an input question
144:05 - routing it to the right document um this
144:07 - doesn't change so I think a lot of
144:09 - methods that we think about for kind of
144:11 - query analysis um taking an input
144:14 - question rewriting it in a certain way
144:16 - to optimize retrieval things like
144:18 - routing taking a question routing to the
144:20 - right database be it a relational
144:22 - database graph database Vector store um
144:25 - and quer construction methods so for
144:27 - example text to SQL text to Cipher for
144:30 - graphs um or text to even like metadata
144:32 - filters for for Vector stores those are
144:35 - all still relevant in the world that you
144:37 - have long Contex llms um you're probably
144:40 - not going to dump your entire SQL DB and
144:42 - feed that to the llm you're still going
144:43 - to have SQL queries you're still going
144:45 - to have graph queries um you may be more
144:48 - permissive with what you extract but it
144:50 - still is very reasonable to store the
144:52 - majority of your structured data in
144:53 - these in these forms likewise with
144:55 - unstructured data like documents like we
144:58 - said before it still probably makes
145:00 - sense to ENC to you know store documents
145:02 - independently but just simply aim to
145:04 - retrieve full documents rather than
145:05 - worrying about these idiosyncratic
145:07 - parameters like like chunk size um and
145:10 - along those lines there's a lot of
145:12 - methods out there we've we've done a few
145:14 - of these that are kind of well optimized
145:16 - for document retrieval so one I want a
145:18 - flag is what we call multi repesent
145:20 - presentation indexing and there's
145:21 - actually a really nice paper on this
145:23 - called dense X retriever or proposition
145:25 - indexing but the main point is simply
145:27 - this would you do is you take your OD
145:29 - document you produce a representation
145:31 - like a summary of that document you
145:33 - index that summary right and then um at
145:37 - retrieval time you ask your question you
145:39 - embed your question and you simply use a
145:41 - highle summary to just retrieve the
145:43 - right document you pass the full
145:45 - document to the LM for a kind of final
145:48 - generation so it's kind of a trick where
145:51 - you don't have to worry about embedding
145:52 - full documents in this particular case
145:54 - you can use kind of very nice
145:56 - descriptive summarization prompts to
145:58 - build descriptive summaries and the
146:00 - problem you're solving here is just get
146:02 - me the right document it's an easier
146:04 - problem than get me the right chunk so
146:06 - this is kind of a nice approach it
146:08 - there's also different variants of it
146:10 - which I share below one is called parent
146:11 - document retriever where you could use
146:13 - in principle if you wanted smaller
146:15 - chunks but then just return full
146:17 - documents but anyway the point is
146:19 - preserving full documents for Generation
146:21 - but using representations like summaries
146:23 - or chunks for retrieval so that's kind
146:25 - of like approach one that I think is
146:27 - really interesting approach two is this
146:30 - idea of raptor is a cool paper came out
146:32 - of Stamper somewhat recently and this
146:34 - solves the problem of what if for
146:36 - certain questions I need to integrate
146:38 - information across many documents so
146:41 - what this approach does is it takes
146:42 - documents and it it embeds them and
146:45 - clusters them and then it summarizes
146:47 - each cluster um and it does this
146:49 - recursively in up with only one very
146:51 - high level summary for the entire Corpus
146:53 - of documents and what they do is they
146:55 - take this kind of this abstraction
146:57 - hierarchy so to speak of different
146:59 - document summarizations and they just
147:01 - index all of it and they use this in
147:03 - retrieval and so basically if you have a
147:05 - question that draws an information
147:07 - across numerous documents you probably
147:09 - have a summary present and and indexed
147:12 - that kind of has that answer captured so
147:15 - it's a nice trick to consolidate
147:16 - information across documents um they
147:19 - they paper actually reports you know
147:22 - these documents in their case or the
147:23 - leavs are actually document chunks or
147:25 - slices but I actually showed I have a
147:28 - video on it and a notebook that this
147:29 - works across full documents as well um
147:33 - and this and I segue into to do this you
147:36 - do need to think about long context
147:37 - embedding models because you're
147:38 - embedding full documents and that's a
147:40 - really interesting thing to track um the
147:43 - you know hazy research uh put out a
147:45 - really nice um uh blog post on this
147:48 - using uh what the Monch mixer so it's
147:50 - kind of a new architecture that tends to
147:53 - longer context they have a 32,000 token
147:56 - embedding model that's pres that's
147:58 - available on together AI absolutely
147:59 - worth experimenting with I think this is
148:01 - really interesting Trend so long long
148:03 - Contex and beddings kind of play really
148:05 - well with this kind of idea you take
148:07 - full documents embed them using for
148:09 - example long Contex embedding models and
148:11 - you can kind of build these document
148:12 - summarization trees um really
148:14 - effectively so I think this another nice
148:16 - trick for working with full documents in
148:19 - the long context kind of llm regime um
148:23 - one other thing I'll note I think
148:25 - there's also going to Mo be move away
148:27 - from kind of single shot rag well
148:29 - today's rag we typically you know we
148:31 - chunk documents uh uh embed them store
148:34 - them in an index you know do retrieval
148:36 - and then do generation but there's no
148:38 - reason why you shouldn't kind of do
148:40 - reasoning on top of the generation or
148:42 - reasoning on top of the retrieval and
148:44 - feedback if there are errors so there's
148:46 - a really nice paper called selfrag um
148:49 - that kind of reports this we implemented
148:50 - this using Lang graph works really well
148:53 - and the simp the idea is simply to you
148:55 - know grade the relevance of your
148:57 - documents relative to your question
148:59 - first if they're not relevant you
149:01 - rewrite the question you can do you can
149:02 - do many things in this case we do
149:04 - question rewriting and try again um we
149:06 - also grade for hallucinations we grade
149:09 - for answer relevance but anyway it kind
149:11 - of moves rag from like a single shot
149:12 - Paradigm to a kind of a cyclic flow uh
149:16 - in which you actually do various
149:17 - gradings Downstream and this is all
149:19 - relev in the long context llm regime as
149:22 - well in fact you know it you you
149:24 - absolutely should take advantage of of
149:27 - for example increasingly fast and
149:30 - Performing LMS to do this grading um
149:33 - Frameworks like langra allow you to
149:35 - build these kind of these flows which
149:37 - build which allows you to kind of have a
149:38 - more performant uh kind of kind of
149:42 - self-reflective rag pipeline now I did
149:44 - get a lot of questions about latency
149:46 - here and I completely agree there's a
149:47 - trade-off between kind of performance
149:49 - accuracy and latency that's present here
149:51 - I think the real answer is you can opt
149:54 - to use very fast uh for example models
149:57 - like grock where seeing um you know gp35
150:00 - turbos very fast these are fairly easy
150:03 - grading challenges so you can use very
150:05 - very fast LMS to do the grading and for
150:07 - example um you you can also restrict
150:11 - this to only do one turn of of kind of
150:13 - cyclic iteration so you can kind of
150:14 - restrict the latency in that way as well
150:16 - so anyway I think it's a really cool
150:18 - approach still relevant in the world as
150:20 - we move towards longer context so it's
150:22 - kind of like building reasoning on top
150:23 - of rag um in the uh generation and
150:28 - retrieval stages and a related point one
150:30 - of the challenges with rag is that your
150:33 - index for example you you may have a
150:36 - question that is that asks something
150:38 - that's outside the scope of your index
150:39 - and this is kind of always a problem so
150:41 - a really cool paper called c c rag or
150:44 - corrective rag came out you know a
150:45 - couple months ago that basically does
150:47 - grading just like we talked about before
150:50 - and then if the documents are not
150:51 - relevant you kick off and do a web
150:53 - search and basically return the search
150:55 - results to the LM for final generation
150:57 - so it's a nice fallback in cases where
151:00 - um your you the questions out of the
151:02 - domain of your retriever so you know
151:04 - again nice trick overlaying reasoning on
151:07 - top of rag I think this trend you know
151:09 - continues um because you know it it just
151:12 - it makes rag systems you know more
151:14 - performant uh and less brittle to
151:18 - questions that are out of domain so you
151:19 - know you know that's another kind of
151:21 - nice idea this particular approach also
151:23 - we showed works really well with with uh
151:25 - with open source models so I ran this
151:27 - with mraw 7B it can run locally on my
151:29 - laptop using a llama so again really
151:32 - nice approach I encourage you to look
151:33 - into this um and this is all kind of
151:35 - independent of the llm kind of context
151:37 - length these are reasoning you can add
151:40 - on top of the retrieval stage that that
151:42 - can kind of improve overall performance
151:45 - and so the overall picture kind of looks
151:47 - like this where you know I think that
151:49 - the the the the problem of routing your
151:52 - question to the right database Andor to
151:54 - the right document kind of remains in
151:56 - place query analysis is still quite
151:58 - relevant routing is still relevant query
152:00 - construction is still relevant um in the
152:02 - long Contex regime I think there is less
152:04 - of an emphasis on document chunking
152:07 - working with full documents is probably
152:08 - kind of more parto optimal so to speak
152:11 - um there's some some clever tricks for
152:13 - IND indexing of documents like the
152:15 - multi-representation indexing we talked
152:16 - about the hierarchical indexing using
152:19 - Raptor that we talked about as well are
152:20 - two interesting ideas for document
152:22 - Centric indexing um and then kind of
152:25 - reasoning in generation post retrieval
152:28 - on retrieval itself tog grade on the
152:30 - generations themselves checking for
152:32 - hallucinations those are all kind of
152:34 - interesting and relevant parts of a rag
152:36 - system that I think we'll probably will
152:38 - see more and more of as we move more
152:40 - away from like a more naive prompt
152:42 - response Paradigm more to like a flow
152:44 - Paradigm we're seeing that actually
152:45 - already in codenation it's probably
152:47 - going to carry over to rag as well where
152:49 - we kind of build rag systems that have
152:51 - kind of a cyclic flow to them operate on
152:53 - documents use longc Comics llms um and
152:55 - still use kind of routing and query
152:57 - analysis so reasoning pre- retrieval
152:59 - reasoning post- retrieval so anyway that
153:01 - was kind of my talk um and yeah feel
153:04 - free to leave any comments on the video
153:05 - and I'll try to answer any questions but
153:07 - um yeah that's that's probably about it
153:09 - thank you