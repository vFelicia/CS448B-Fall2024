00:01 - my talk is called theory of neuro
00:03 - networks and the alternate title would
00:05 - be deep learning without frameworks
00:08 - because i'm going to be um talking
00:11 - i'm going to be talking about the basics
00:13 - of deep learning and neural networks but
00:16 - i'm not going to be using any framework
00:18 - so i'm not going to be using
00:21 - keras or tensorflow
00:23 - and the whole point of this talk is to
00:25 - hopefully help you guys understand
00:28 - how deep learning and neural networks
00:30 - work kind of under the hood because
00:33 - usually when you're working with these
00:35 - things you're going to use a framework
00:36 - that kind of hides a lot of the
00:38 - implementation so hopefully after this
00:41 - talk
00:42 - even if you're you're completely new to
00:44 - deep learning and neural networks you'll
00:45 - have a
00:46 - better understanding of how how those
00:48 - things work
00:50 - and so the first part of my talk i'm
00:52 - going to be
00:53 - going over some of the basic concepts
00:56 - and showing how things work
00:59 - and then i have a second part after
01:01 - lunch where i'm going to bring it all
01:03 - together and do some live coding and
01:06 - show you how to solve an actual problem
01:09 - using just just straight python
01:12 - and
01:14 - showing you how to
01:16 - create a neural network from scratch
01:18 - so
01:20 - first a little bit about me
01:23 - let me get my
01:25 - there we go
01:26 - so my name is beau carnes like like you
01:29 - heard and i am from the united states
01:32 - i'm from the state of michigan and i
01:34 - currently work at freecodecamp.org
01:37 - it's a non-profit with the goal of
01:39 - providing a free curriculum for people
01:42 - learning software development
01:44 - and so anybody can go on and learn for
01:47 - free so i like a new thing that we're
01:50 - doing over at free code camp is we've
01:53 - translated the curriculum into five
01:55 - different world languages and we just
01:58 - use like this google automatic translate
02:01 - google automatic translator but we are
02:04 - an open source community so we're trying
02:06 - to get people to go in their native
02:09 - speakers and fix the automatic
02:11 - translations and russian is one of the
02:13 - languages so if anybody's interested in
02:15 - trying to help out this open source
02:17 - community and you speak russian you can
02:19 - go in and help with the translation
02:22 - besides that i also have made some
02:24 - courses for this book publishing company
02:26 - called manning publications
02:28 - i saw on the the booth out there i saw
02:31 - some manning publications books
02:33 - and i've created some video courses so i
02:37 - have one called algorithms in motion and
02:39 - then grocking deep learning in motion so
02:42 - the the second one is kind of the
02:44 - grocking deep learning emotion that's
02:45 - what this talk is based off of and the
02:48 - whole course um i mean the whole video
02:51 - course is actually based on this great
02:53 - book
02:54 - by another author named andrew trask so
02:57 - so this i would highly recommend this
02:59 - book for anybody trying to learn deep
03:02 - learning so my my whole my video course
03:04 - is based on the book and this talk is
03:06 - based on the course
03:08 - so let's kind of get into it here
03:11 - um
03:12 - oh first of all i wanted to tell you
03:14 - that i do so this is my personal website
03:16 - karns.cc and then i made a
03:19 - site a page called moscow
03:21 - so all my my slides are on there and all
03:24 - the code that i'm going to be going over
03:27 - during this part of the talk and the
03:28 - second part of the talk
03:30 - so you don't necessarily need to look at
03:31 - it now but if you ever want to refer to
03:33 - that you can
03:34 - and
03:36 - so i'm going to be going into more
03:38 - detail into code than some of the
03:39 - previous speakers
03:41 - so sometimes it's good to see the actual
03:43 - code
03:44 - so
03:45 - deep learning verse versus machine
03:47 - learning so machine deep learning is
03:50 - actually a subset of machine learning so
03:53 - if we go here um
03:55 - you can see that we have machine
03:56 - learning and the sub says deep learning
03:58 - and then the subset of that is
03:59 - artificial intelligence
04:01 - so um machine learning is kind of like
04:05 - what it sounds like machines or
04:06 - computers are are trying to learn
04:09 - something that they were not explicitly
04:11 - programmed for
04:12 - so machines observe a pattern and
04:14 - attempt to imitate it in some way
04:17 - so machine learning is often attempting
04:19 - to take an input data set and transform
04:22 - it into an output data set
04:24 - so let me show you some examples with an
04:27 - input dataset and an output data set so
04:29 - these are all examples that you would
04:31 - use machine learning or even deep
04:33 - learning to try to figure out so if you
04:35 - have pictures of a cat you have the
04:38 - input is pixels and then the output
04:40 - would be a presence or absence of a cat
04:43 - so you input the pixels and the output
04:45 - of the algorithm would be whether or not
04:47 - there's a cat there
04:49 - and the next one would be
04:51 - you could input movies you liked into
04:53 - the algorithm and then on output movies
04:55 - you may like so these are just some
04:58 - different possible use cases you would
05:00 - input words and then would output
05:02 - whether those words indicated that the
05:04 - person was happy or sad
05:06 - and then you would input weather sensor
05:08 - data and the output would be the
05:10 - probability of rain
05:13 - so
05:14 - these are all supervised machine
05:16 - learning tasks
05:18 - and the machine learning algorithm is
05:21 - attempting to imitate the pattern
05:23 - between two data sets in such a way that
05:26 - it can take use one data set to predict
05:28 - the other data set
05:30 - so i'm going to give you a summary of
05:32 - how machine learnings would do that and
05:35 - how a deep learning algorithm would do
05:37 - that let's say you have this data set of
05:40 - pictures and a lot of the pictures are
05:42 - pictures of cats but then some of the
05:44 - pictures are not cats
05:47 - so how would work is you would input all
05:49 - these pictures into into your algorithm
05:53 - and you would also tell it which
05:55 - pictures were cats
05:57 - and which pictures are not cats
06:00 - so um the supervised learning algorithm
06:02 - is going to extract patterns from that
06:04 - data set after it's learned the patterns
06:08 - we can show it a picture and algorithm
06:10 - will hopefully tell us whether the
06:11 - picture is of a cat or not so it takes
06:14 - all the information you give it and then
06:16 - hopefully
06:17 - later you can get a picture of a cat
06:19 - that it hasn't seen before and will be
06:21 - able to figure out whether it's a cat or
06:23 - not
06:24 - so deep learning is a subset of methods
06:28 - in the machine learning
06:29 - toolbox that uses artificial neural
06:32 - networks so let's get into more details
06:35 - about deep learning oh here would be an
06:37 - example is this a cat
06:39 - yes it's a cat
06:41 - so
06:44 - all
06:45 - deep learning and machine algorithm
06:47 - learning algorithms are going to be
06:49 - classified
06:50 - as either supervised or unsupervised
06:53 - learning
06:55 - uh so
06:57 - we take what we know and we transform
07:00 - into what we want to know so that would
07:01 - be a supervised learning we have
07:03 - something that we know to be true
07:05 - and we transform it into what we want to
07:08 - know
07:09 - so like an example would be
07:12 - we know
07:13 - that
07:15 - we know that
07:16 - this picture we know that this is this
07:19 - is a picture of a cat or maybe not a cat
07:22 - and we put into the algorithm to find
07:24 - out what we want to know like whether
07:26 - really it is a picture of a cat
07:28 - and so the um the goal of a supervised
07:31 - learning
07:33 - is is to learn a function that given a
07:35 - sample of data and the desired outputs
07:38 - best approximates the relationship
07:40 - between the input and the output in the
07:42 - data
07:44 - supervised learning is done using prior
07:46 - knowledge of what the output values for
07:48 - a sample should be
07:49 - like in the cat example
07:52 - we know that the
07:53 - that the output is either going to be
07:55 - cat or not cat or maybe you're trying to
07:58 - identify handwriting and you know it's
08:01 - going to be
08:02 - one of the letters of the alphabet you
08:04 - know you know the alphabet you know the
08:06 - the you already know what the output
08:08 - could be for supervised learning
08:10 - so for unsupervised machine learning
08:14 - we don't know what the output could be
08:17 - we're just import and putting a list of
08:20 - data points
08:21 - and we put into the algorithm and then
08:23 - out comes a list of cluster labels so
08:27 - we don't know for sure so the algorithm
08:30 - is trying to
08:32 - figure out and sort the data but it
08:35 - doesn't know what the categories that
08:37 - it's trying to sort into
08:39 - so unsupervised learning groups your
08:42 - data the goal is to infer a natural
08:44 - structure present within a set of data
08:46 - points
08:47 - and we put in a we put in a list of data
08:50 - points and we get the cluster labels out
08:52 - so
08:53 - basically an unsupervised learning
08:55 - algorithm says find patterns in this
08:58 - data and tell me about them so it's just
09:00 - trying to figure out some patterns
09:04 - let's see
09:05 - that was slightly out of order we'll get
09:07 - back to that slide so in this so this
09:09 - would be kind of an example of
09:11 - unsupervised data where you have
09:13 - all this all these data points and your
09:15 - algorithm groups them into these
09:17 - different
09:18 - groups
09:19 - and it basically so the algorithm
09:21 - basically says hey
09:22 - hey data scientist i found some
09:25 - structure in your data it looks like
09:27 - here are some groups in your data here
09:29 - are the groups
09:30 - and then it's up to the data scientist
09:32 - or the programmer to figure out what the
09:35 - groups are it doesn't tell you what the
09:36 - groups are you just kind of have to
09:38 - figure that out after you see the groups
09:41 - so
09:42 - the next thing i want to talk about is
09:43 - parametric versus non-parametric
09:46 - learning
09:47 - and
09:47 - you know pretty soon we'll get to some
09:49 - actual code examples so this will start
09:51 - to make more sense
09:53 - but for
09:54 - for this
09:56 - um you can just kind of imagine a cloud
09:59 - so with uh so this is our machine
10:01 - learning cloud there's two knobs we have
10:03 - supervised an unsupervised knob and a
10:05 - parametric and non-parametric so there's
10:08 - you can kind of switch your algorithm to
10:10 - be either supervised or unsupervised
10:12 - parametric or non-parametric supervision
10:15 - is about the type of pattern being
10:17 - learned and parametricism is the way the
10:20 - learning is stored
10:22 - so the two types parametric and
10:24 - non-parametric parametric
10:27 - is with a fixed number of parameters and
10:30 - non-parametric is possibly an infinite
10:33 - number of parameters so here's kind of
10:36 - an oversimplification a parametric is
10:38 - trial and error
10:40 - and non-parametric is about counting and
10:42 - probability
10:44 - so
10:45 - you could have
10:47 - you're just kind of counting and you
10:48 - don't know how many parameters there's
10:50 - going to be a non-parametric
10:51 - so let's do an example i'm going to go
10:54 - back to
10:56 - this slide so let's say you're trying to
10:58 - figure out you have this square and
11:00 - you're trying to figure out where it
11:02 - should go in this little toy here now
11:05 - again this is a pretty simple example
11:07 - but a some people like a baby may may
11:10 - just jam it into every hole until they
11:12 - find out where it fits so that's kind of
11:14 - like parametric learning
11:16 - now non-parametric learning would be
11:18 - you're counting the sides of the square
11:20 - and counting the sides of the the hole
11:22 - and then you kind of figure out where
11:24 - it'll go on that one so a parametric
11:27 - models tend to use trial and error like
11:29 - just trying everything a non-parametric
11:31 - tends to count to figure out where it
11:33 - should go
11:34 - but most algorithms
11:37 - are supervised parametric learning so
11:41 - from the rest of this talk and the next
11:42 - talk i'm gonna be talking about
11:44 - supervised parametric learning and
11:46 - giving you some examples to go along
11:48 - with that
11:49 - so
11:50 - this is basically trial and error using
11:53 - knobs
11:54 - and so there are three steps uh step one
11:57 - will be predict so
11:59 - um the three steps to supervise
12:01 - parametric learning is the first step is
12:03 - predict so for the predict step you take
12:05 - all your data in this case we're trying
12:08 - to predict if a sports team is going to
12:10 - win a game or not so we input this data
12:13 - like number of toes on the team number
12:15 - of players number of fans we put it into
12:19 - our algorithm which is our machine
12:21 - and then out comes a prediction so in
12:24 - this case
12:25 - our prediction is saying that there's a
12:27 - 98 chance given this data that our team
12:30 - is going to win
12:32 - so step two
12:34 - is going to be
12:36 - compared to the truth pattern so we have
12:39 - our prediction which is 98 chance that
12:41 - the team's gonna win and our truth
12:43 - pattern is we found out that in real
12:45 - life they lost the game so zero percent
12:47 - chance of that they're gonna win so we
12:49 - thought we our original prediction was
12:51 - 98 chance they're gonna win but actually
12:54 - they lost uh so we after and so then
12:57 - after we compare to the truth pattern
12:59 - this is where we are going to
13:02 - learn the pattern
13:04 - so
13:04 - learning um
13:05 - [Music]
13:08 - when we're going to just
13:10 - um take the information about how much
13:12 - we were wrong
13:14 - so we look at how much we missed by
13:17 - which is 98
13:19 - and we also look at the input data at
13:21 - the time of the prediction
13:22 - and then we're going to turn the knobs
13:24 - so these knobs would be weights so
13:27 - with the knob of the wind loss the home
13:29 - away number of toes number of fans so
13:32 - based on all the information we're going
13:34 - to adjust the weights
13:36 - so hopefully the next time the next
13:38 - iteration of the algorithm
13:40 - we can
13:41 - uh be a little closer instead of 98
13:43 - percent chance we're going to win that's
13:45 - going to that number is going to be
13:46 - lower because we know that in reality
13:48 - they didn't win with that information
13:51 - now
13:52 - um
13:53 - so now that you kind of know the basic
13:55 - steps i know it's kind of hard to
13:57 - understand all that how all that fits
13:59 - together but now i'm going to get into
14:01 - some actual code so hopefully it starts
14:04 - to make sense what i was talking about
14:06 - with the predict compare learn
14:08 - so this is a simple
14:10 - neural network
14:12 - and a neural network is one or more
14:14 - weights which we can multiply by our
14:17 - input data to make a prediction
14:19 - so it's all about trying to make a
14:21 - prediction on on the correct answer
14:23 - based on those weights so in the example
14:26 - that i gave the prediction will be
14:27 - whether or not the team is going to win
14:29 - a game based on the the input debt and
14:32 - the weights
14:33 - so here are our first few lines of code
14:36 - here
14:37 - we i start by just hard coding a weight
14:40 - number now generally your weights are
14:42 - going to start as random numbers which
14:44 - we'll see in the live coding section
14:46 - later
14:46 - but um here it's just a hard-coded so
14:49 - you can kind of see how it works better
14:51 - and we have our neural network function
14:54 - so
14:56 - we can see that our neural network
14:57 - function we just
14:59 - pass in the input and the weights and
15:01 - then it's going to set the prediction to
15:03 - input times weights and it returns the
15:05 - prediction
15:06 - so it's all about just multiplying
15:08 - inputs times weights so
15:10 - this diagram would kind of show to you
15:12 - how it works you input the information
15:14 - here multiplies by the weight of 0.1 and
15:17 - the prediction is the percent chance
15:19 - that the team is going to win
15:22 - so
15:23 - this next section of code we just talked
15:25 - about this part right here but here is
15:28 - our data so we have this
15:30 - array of number of toes this would be
15:33 - like average number of toes for each of
15:35 - the first four games in in the season so
15:38 - it's kind of a silly example but just
15:40 - kind of bear with me here
15:43 - our input is just going to be the first
15:44 - number because we can only run on one
15:46 - number at a time
15:48 - so we're inputting just as 8.5
15:50 - and then our prediction
15:52 - we just call the neural network function
15:54 - which multiplies the input times the
15:56 - weight and then we get out the
15:57 - predictions so it inputs so we have the
16:00 - 8.5 times the weight so our prediction
16:03 - is that it's going there's an 85 chance
16:06 - that our team is going to win
16:09 - so we can see that the prediction is the
16:11 - input times the weight here
16:14 - so let's talk more about what the input
16:16 - data is so an input data is a number
16:19 - recorded in the real world somewhere so
16:22 - it's usually something that's easily
16:23 - knowable like in the example average
16:25 - number of toes it could be today's
16:27 - temperature it could be yesterday's
16:29 - stock price
16:30 - um so it's just some we're going to
16:33 - input some data that we can see in the
16:35 - real world and then we are going to make
16:38 - a
16:39 - prediction so a prediction is what the
16:43 - neural network tells us given our input
16:45 - data so like given today's temperature
16:49 - we i think the
16:50 - network thinks there's a zero percent
16:52 - chance that people are going to wear
16:53 - swimsuits today or maybe given
16:56 - yesterday's stock price
16:57 - there's uh today's stock pro stock price
17:00 - is going to be 101
17:02 - 0.3 or something like that or given this
17:05 - many number of toes this team is going
17:08 - to
17:08 - win the game so that would be the the
17:11 - prediction
17:12 - and then
17:13 - [Music]
17:16 - the network learns through trial and
17:18 - error so in that example it was just it
17:21 - only went through at one time
17:23 - so in a real neural network which we
17:26 - will get to and i'll show you a network
17:28 - with a lot of iterations
17:30 - every time it runs through the code
17:32 - it updates the weight and updates the
17:35 - error and it keeps running through the
17:36 - code over and over
17:38 - to to learn
17:40 - so it first it's going to try to make a
17:43 - prediction then it sees whether it was
17:45 - too high or too low
17:47 - and then finally it changes the weight
17:49 - up or down to predict more accurately
17:51 - the next time it sees the same input
17:53 - so
17:54 - so far we've seen
17:56 - um
17:58 - we've seen the network making a
17:59 - prediction so remember the three steps
18:01 - were prediction and then comparing and
18:04 - then learning
18:06 - so we still need to i still need to show
18:08 - you how you would compare it to the
18:10 - truth pattern and learn and i'll get to
18:12 - that in a minute but i'm going to show
18:14 - you uh one another example before i get
18:17 - to the compare and learn
18:20 - so here i'm going to show you a neural
18:23 - network with multiple inputs and outputs
18:26 - so the first example there was just one
18:28 - input and one output
18:30 - in this example we're going to have
18:32 - three inputs so be like the number of
18:34 - toes per player the win loss record
18:37 - number of fans would be the inputs we're
18:38 - inputting all those things into the
18:40 - network and we're outputting
18:42 - the percentage of hurt players whether
18:44 - or not the team is going to win and
18:47 - whether or not the team is sad
18:49 - so this is just kind of to show that
18:52 - i started with showing a very simple
18:53 - network but they can get more complex
18:55 - you can have
18:56 - any number of inputs you want and you
18:58 - can be trying to predict multiple things
19:00 - you don't just have to predict one thing
19:02 - for the inputs you can predict multiple
19:04 - things
19:06 - so
19:07 - here's all the code now i'm going to
19:10 - kind of zoom in so you can see the
19:11 - different sections so first we're going
19:13 - to talk about the weights so before we
19:16 - just inputted one number for the weight
19:18 - but now that we have multiple inputs and
19:20 - multiple predictions we have to have a
19:22 - matrix so a matrix is just an array of
19:25 - arrays basically so you can see the
19:27 - first line
19:28 - is the no is the
19:31 - the weights for number of people hurt
19:33 - second line number of people that if we
19:35 - won or lost and then percentage of sad
19:38 - and then for kind of each column
19:41 - we have the weights for the number of
19:42 - toes
19:43 - what the win ratio is and the number of
19:45 - fans
19:47 - so then our neural network looks almost
19:50 - exactly the same as before remember
19:52 - before we just multiply the input times
19:54 - the weights
19:55 - and that's what we're doing now except
19:58 - we have this fancy function of vector
20:00 - matrix multiplication
20:02 - before we are just multiplying one
20:04 - number by another number now we're
20:06 - multiplying a a vector which is an array
20:10 - by a matrix which is an array of arrays
20:12 - and so we need an extra some extra code
20:15 - to make it so you can multiply a vector
20:17 - by a
20:18 - matrix and we'll see that in a little
20:20 - bit here so this is our information
20:24 - we're passing our this is our input data
20:26 - for the first four games but for our
20:28 - input we're just going to take the first
20:30 - number of each array so we're just going
20:32 - to do kind of one game at a time
20:34 - and
20:36 - so here's an example of what it would
20:39 - look like
20:41 - for we're passing in our input data
20:44 - and then we have all these predictions
20:46 - that kind of come out of the network
20:49 - so i talked to talk about the vector
20:51 - matrix multiplication function i'm not
20:53 - going to go into this and great detail
20:55 - it's just a way to
20:57 - multiply a vector by a matrix but in
21:00 - real and this function kind of uses this
21:02 - weighted sum function but in real life
21:05 - you're probably never going to write
21:06 - these functions yourself
21:08 - because most people are going to use
21:10 - numpy so numpy gives you
21:13 - just extra
21:15 - extra functions to do math like
21:17 - multiplying matrices and
21:20 - func matrices and
21:22 - vectors so if you this line right here
21:25 - from our neural network and numpy is
21:28 - going to be like this and when i do my
21:30 - live coding i'm actually going to use
21:31 - numpy so you'll see how that works so
21:34 - this dot dot here that's a numpy
21:37 - function and this just means multiply
21:39 - the input
21:40 - by weights
21:41 - and then we get the answer into our
21:43 - prediction here
21:46 - okay so at the end of our network we
21:47 - just set our prediction to equal the
21:50 - the return value of our neural network
21:52 - and we can print our predictions so you
21:55 - can see here here are predictions that
21:58 - we printed out based on this network
22:01 - here
22:02 - so if we go over here you can kind of
22:04 - see everything
22:05 - on the screen at once and then down here
22:08 - what you can see the prediction so that
22:09 - first and you can see how the math is
22:11 - calculated
22:12 - and so our her prediction was 0.55 so we
22:15 - think
22:16 - there's a
22:18 - probably 55 percent of players may be
22:20 - hurt a 98 chance of winning and a 97
22:24 - chance that people are sad on the team
22:27 - so we can predict three things at once
22:30 - so
22:31 - the next thing i want to talk about is
22:33 - error
22:34 - and gradient descent
22:37 - so
22:38 - this green descent is used while
22:40 - training a machine learning model and
22:43 - it's an optimization algorithm that
22:45 - tweaks its parameters iteratively to
22:48 - minimize a given function to its local
22:50 - minimum so
22:51 - in other words it allows us to calculate
22:53 - both the direction and the amount that
22:56 - we should change our weights so we
22:58 - reduce our error
22:59 - so the whole point of our neural network
23:01 - is trying to get our error to zero
23:04 - so our gradient descent which we'll see
23:06 - the code later it's pretty simple
23:08 - it helps us figure out
23:11 - which direction we should move our
23:12 - weights and how much we should move our
23:13 - weight so when we calculate the error it
23:16 - goes down it's just a way to every time
23:18 - we run the the code
23:20 - every iteration we're going to use our
23:22 - great descent function
23:24 - to
23:25 - get our weights to the right spot so our
23:27 - error goes to zero
23:33 - so
23:34 - we talked about supervised parametric
23:36 - learning in the three steps we talked
23:38 - about
23:39 - so but so far you've only seen predict
23:42 - but now i'm going to show you how to do
23:44 - the second two steps the compare to
23:46 - truth pattern and learning the pattern
23:50 - so
23:50 - for
23:51 - this step two is compared to the truth
23:54 - pattern
23:56 - to measure the error we determine how
23:58 - far our prediction is from our goal
24:00 - prediction
24:02 - and then step three learn the pattern it
24:05 - takes our air and tells each weight how
24:07 - how it can change to reduce it so the
24:11 - whole point is trying to figure out
24:13 - which weights are impacting our air so
24:16 - we can reduce our error
24:18 - to zero and by time when our air is zero
24:22 - that shows us that our weights are
24:23 - accurate because there's no error
24:26 - so now i'm going to show you
24:33 - pressing this button here hopefully i'll
24:35 - change slides just a second
24:38 - oh wait
24:44 - okay so learning is adjusting our weight
24:46 - to reduce the error to zero so that's
24:49 - what we're trying to do to actually
24:50 - that's what our learning is actually
24:52 - doing so here is our one iteration of
24:55 - gradient descent
24:57 - now in a real example you could have
24:59 - like hundreds of iterations or thousands
25:01 - of iterations and when i get to the live
25:03 - coding i'll show you one with a lot of
25:04 - iterations but let's just take it one
25:06 - iteration at a time so first we get our
25:08 - weight
25:09 - and our alpha
25:11 - and
25:12 - our alpha is the simplest way to prevent
25:15 - over correcting our weight updates
25:18 - so sometimes the goal is to get our air
25:20 - to zero but sometimes instead of our air
25:22 - going down it will start going up so
25:25 - alpha is something that's going to help
25:27 - it to always go down and go in the right
25:29 - direction and you'll see later how that
25:31 - fits into our code
25:32 - it's kind of trial and error for what
25:34 - alpha you're going to use but it's
25:36 - usually a multiple of ten so you would
25:38 - start with maybe point zero one then you
25:40 - could try point one and then maybe try
25:43 - one and just try to see which one is
25:45 - gonna make your air go down instead of
25:47 - up
25:48 - so
25:50 - uh let's see and then our neural network
25:52 - is the same as before which is input
25:54 - times weight and here's the diagram so
25:57 - our input the data goes here goes
25:59 - through a weight and then we outcomes
26:02 - our prediction of our wind prediction
26:05 - so
26:07 - now
26:08 - we are going to
26:10 - um
26:12 - take
26:13 - we we're going to do our prediction step
26:15 - so we pass in our information you can
26:17 - see there's only a few things different
26:19 - here which are goal prediction and error
26:22 - so we didn't talk about air in our last
26:24 - code but that's very important because
26:25 - we're trying to get our air to zero so
26:27 - right here we see that we have our
26:30 - number of toes is 8.5 just like before
26:32 - our input
26:34 - is our number of toes now our goal
26:35 - prediction is our winner loss binary
26:37 - which is a one
26:39 - means that the number one means that
26:41 - they won the game so our goal prediction
26:44 - is one so we think that with an input of
26:46 - 8.5 the goal prediction is one that we
26:50 - they won the game so we do our
26:53 - prediction just like before with our
26:54 - neural network we multiply the input
26:56 - times the weight
26:57 - and then our error is going to equal
27:00 - prediction minus goal prediction square
27:05 - so this is
27:07 - prediction minus gold prediction is the
27:09 - raw error it's it's how much we
27:11 - predicted was going to happen
27:12 - minus
27:14 - what we thought was going to happen and
27:16 - then the reason why we square it it
27:18 - forces the raw error to be positive by
27:21 - multiplying by itself so a a negative
27:24 - prediction a negative error when it
27:26 - makes sense so when we square it's
27:28 - always positive and also has added
27:30 - benefit of making
27:32 - large areas larger and small errors
27:35 - smaller which actually helps our neural
27:37 - network to go quicker to the correct
27:40 - uh
27:41 - to the correct thing by squaring it like
27:42 - that
27:44 - so
27:45 - if we
27:47 - run our neural network we do the 8.5
27:50 - times 0.1 we get the 0.85 but then our
27:52 - error ends up being 0.023
28:02 - okay
28:04 - so then we have our delta so this is the
28:07 - compare step we're going to calculate
28:09 - our node delta and the delta is how much
28:12 - this node missed so that's the raw error
28:14 - prediction minus goal prediction so so
28:17 - we need to know how much the node missed
28:19 - so we can
28:21 - calculate we can kind of put it into our
28:24 - output
28:25 - so
28:26 - we know uh we know we missed
28:30 - by negative 0.15
28:33 - because our prediction was
28:37 - 8.85
28:39 - and our goal prediction was 1. so 0.85
28:42 - minus 1 is negative 0.15
28:45 - so
28:46 - weight delta this is kind of where this
28:48 - is where the gradient descent happens so
28:49 - you can see it's actually a pretty
28:51 - simple line of code it's how much this
28:53 - weight caused the network to miss
28:56 - so we want to find out we know that
28:59 - are we missed but how much do we miss we
29:01 - just use the weight delta which is this
29:03 - the input times delta
29:06 - gives us the weight delta
29:08 - and we're trying to figure out how much
29:10 - to update the weight because we're
29:12 - trying to change the weight on every
29:14 - iteration now this actually does three
29:17 - good things for us it helps us with
29:18 - stopping
29:19 - negative reversal
29:21 - and scaling
29:23 - so stopping
29:25 - it's the first effect on our pure error
29:26 - caused by multiplying it by our input so
29:29 - just imagine you're listening to music
29:31 - or you're trying you're playing music on
29:33 - your computer
29:34 - or you're trying to play music and you
29:36 - have speakers so you turn up the
29:37 - speakers all the way but music still
29:39 - isn't coming out because you forgot to
29:41 - hit play on your computer so this would
29:44 - kind of be an example of stopping where
29:46 - it's kind of addresses in our neural
29:48 - network so if our input is zero like
29:50 - you've got to explain your computer
29:52 - it will force the weight delta to also
29:54 - be zero
29:55 - so we don't learn when our input is zero
29:58 - because there's nothing there's nothing
29:59 - to learn
30:01 - and so moving it makes no difference
30:03 - then negative reversal
30:05 - um
30:06 - when our input is positive moving our
30:09 - weight upward makes the prediction move
30:11 - upward but if our input is negative then
30:14 - all of a sudden our weight changes
30:15 - directions now we only want our weight
30:18 - to go in one direction so when our input
30:20 - is negative then moving our way up makes
30:22 - the prediction go goes down we don't
30:24 - want that so multiplying our delta by
30:28 - our input
30:29 - will reverse the sign of our weight
30:31 - delta in the event that our input is
30:33 - negative
30:34 - so this is the negative reversal
30:37 - this ensures that our weight moves in
30:38 - the correct direction
30:40 - um scaling is just when we mol anytime
30:43 - you multiply things together
30:45 - it's just it's either going to get a lot
30:47 - bigger or a lot smaller if the number is
30:49 - less than zero
30:50 - so this is this is good because we want
30:53 - our we want our weight delta to we want
30:56 - big errors
30:57 - to be really big and small to be really
31:00 - small
31:02 - and
31:02 - alpha is going to help it so it doesn't
31:05 - go out of control so we don't want it to
31:06 - get too big that's why we have the alpha
31:08 - which we'll just about to talk about
31:10 - here
31:10 - but first we can see how the weight
31:13 - delta it kind of gets applied to the
31:15 - weight here of negative 1.25
31:18 - okay so now we're getting toward the end
31:20 - of this we are going to update the
31:23 - weight so when it says weight minus
31:25 - equals that just means weight equals
31:27 - weight minus weight delta times alpha
31:31 - and so this this allows
31:34 - we're going to multiply the weight delta
31:36 - times the weight or we're going to do
31:38 - weight minus the weight delta
31:40 - and we multiply it by the alpha to
31:42 - control how fast the network learns
31:45 - because it can like i said can it can
31:47 - update weights too aggressively but the
31:50 - alpha is going to remember the alpha in
31:52 - this case was .01 so it makes it so the
31:55 - weight doesn't get updated as quick
31:57 - quickly and so it doesn't get too out of
31:59 - control so in this case the new weight
32:02 - is 0.11275
32:06 - so
32:07 - we're actually getting toward the end of
32:09 - our the first part of the talk i know
32:11 - this is kind of a lot of information all
32:14 - at once especially if you're kind of new
32:15 - to deep learning um so my hope is that
32:19 - in this far part of your the talk i gave
32:21 - you some like good
32:23 - foundations and background knowledge so
32:26 - when we get to the second part of the
32:28 - talk um it will you'll be able to
32:30 - understand how it all goes together so
32:33 - in the second part of the talk i'm
32:34 - basically we're going to use everything
32:36 - we've learned but we're going to use a a
32:38 - real problem and i'm going to live code
32:41 - um a full neural network complete with
32:44 - iterations so you'll be able to see how
32:45 - the weight updates you'll be able to see
32:48 - how the air updates you'll you'll be
32:50 - able to see in the output of the the
32:52 - neural network how the air is going to
32:53 - zero
32:54 - and
32:55 - hopefully all these things we've kind of
32:57 - talked about that you've seen for the
32:59 - first time will start to make more sense
33:01 - when you see how it all works in an
33:03 - actual neural network so we'll also in
33:06 - the second part of the talk talk more
33:08 - about the deep learning part of this so
33:11 - so far i haven't even talked about why
33:13 - it's called deep learning but that's
33:15 - something else we'll be talking about
33:17 - i'm glad to see so many people decide to
33:19 - come back to part two of my talk so this
33:21 - is where i'll be pulling it all together
33:23 - what we were talking about in the first
33:24 - talk and hopefully a lot of things will
33:26 - start to make a lot of sense once you
33:28 - see a full neural network
33:30 - based on trying to solve a problem let's
33:32 - um let's get into it first of all i'm
33:34 - going to explain
33:36 - the problem so this is a problem we're
33:38 - going to try to solve with a neural
33:39 - network
33:41 - and it's kind of um
33:43 - it's it's a real world problem but it's
33:46 - a simplified real world problem so we
33:48 - have enough time to get through
33:49 - everything and so you can understand all
33:51 - the components of it
33:52 - so the problem so we you have to imagine
33:55 - the scenario so imagine the situation
33:58 - where you go to a country you've never
34:00 - been to before
34:01 - and you see a stoplight
34:04 - and you don't know there's three lights
34:06 - but you don't know which configurative
34:08 - configuration of lights means to walk
34:11 - and which configuration of lights mean
34:13 - to stop
34:14 - so um since you're since you're a
34:17 - programmer and a data scientist you
34:18 - decide you want to collect some data
34:20 - so you just sit there and watch and you
34:22 - observe whether people stop or walk so
34:25 - after a little while you you collect
34:28 - this information so you can see this is
34:30 - the first stop light in that
34:32 - configuration of lights with the two on
34:34 - the side on and the middle one off
34:37 - people stopped and then you see this
34:40 - people walked in the the second
34:42 - configuration of lights and then the
34:44 - third was just this one on a person was
34:46 - stopping so you're trying to figure out
34:48 - which configuration means walk and stop
34:50 - but this isn't quite enough information
34:52 - so you keep looking and you collect all
34:54 - this information here
34:56 - and so we're trying to um
35:02 - we collected the information of what we
35:04 - know
35:04 - and what we want to know so what we know
35:07 - is the configuration of the stoplights
35:10 - what we want to know is whether they
35:12 - mean to walk or stop
35:15 - so we're going to use this data and see
35:18 - if our neural network that we're going
35:20 - to develop can figure out
35:22 - which configuration of lights means to
35:25 - walk or stop now this is a simple
35:27 - example so hopefully you can kind of
35:29 - already see a pattern that the the
35:31 - middle light is perfectly correlated to
35:34 - whether you should walk or stop and
35:36 - actually the left and right light don't
35:38 - have anything to do with whether you can
35:40 - walk or stop
35:41 - so when we develop our neural network
35:43 - we're not going to tell it that
35:45 - information we're going to see if our
35:46 - neural network can learn on its own
35:49 - that the middle light is perfectly
35:51 - correlated to the walk and stop
35:54 - data so
35:56 - to make a neural network we cannot just
35:59 - put in these notes or this picture here
36:01 - so we have to convert it to numbers so
36:04 - this is how we're going to convert our
36:06 - street light data to numbers we're going
36:09 - to make if the light is on it's a one if
36:12 - the light is on
36:13 - or if it's off it's a zero so you can
36:15 - kind of see how these
36:17 - correspond to each other
36:19 - so we've got this new data set and we're
36:22 - going to use it to create a neural
36:24 - network to solve it
36:26 - so i'm going to go over
36:28 - to my code editor here now this is
36:32 - something called google colab
36:35 - and google colab a lot it's basically
36:37 - like an online code editor for jupyter
36:40 - notebook so i if you got i don't know if
36:42 - you're familiar with jupiter notebook
36:44 - but it's used a lot for deep learning
36:46 - and machine learning and this is an
36:48 - online version so you don't have to have
36:50 - anything installed on your computer
36:52 - and so it's make things makes things a
36:55 - lot simpler when you're testing things
36:56 - out here so
36:58 - i'll zoom in a little bit more
37:01 - so this is where i'm going to start
37:02 - typing code if you want to follow along
37:04 - you can or you can just look up at the
37:06 - screen but the first thing we're going
37:07 - to do is import numpy
37:11 - as np now let me build
37:15 - okay let's can you guys can see that
37:18 - okay so
37:20 - before we didn't use numpy but now we're
37:22 - going to use numpy and that's going to
37:24 - help with our multiplication remember
37:26 - before i showed you that vector matrix
37:28 - multiplication function we're not going
37:30 - to need that because numpy has that
37:32 - built in
37:34 - and that's the only thing we're going to
37:35 - import the rest of it's just going to be
37:36 - straight python so now we're going to
37:38 - have our weights array
37:41 - and i'm going to do mp dot array
37:45 - and the mp.array means it's a it's a
37:48 - numpy array and this allows us to do um
37:52 - special operations with it like the
37:54 - vector matrix multiplication and stuff
37:56 - like that so
37:57 - a lot of neural networks their weights
38:00 - are going to start off randomly
38:03 - and so i thought this would be a cool
38:04 - chance for some audience participation
38:06 - and i'm gonna see if you guys can give
38:08 - me some numbers they have to be between
38:09 - negative one and one so give me a
38:11 - decimal point between negative one and
38:13 - one so does anybody want to call out a
38:15 - number for our first weight
38:20 - zero seven okay how about 0.7
38:24 - and then we need another it has to be
38:26 - between negative 1 and 1. anybody else
38:27 - have another number
38:29 - 0.2 okay
38:31 - and then our final number
38:38 - okay so this is just a fun way to get
38:40 - random numbers so you can see later that
38:42 - it kind of doesn't matter what you start
38:44 - your weights out as your network's going
38:47 - to be able to learn and get to the
38:48 - correct weight with just random weights
38:51 - so the next line
38:53 - is the alpha
38:56 - and we talked about that a little bit
38:57 - before
38:58 - the alpha helps make sure your network
39:00 - doesn't get out of control and the whole
39:02 - point of the network is to get our air
39:04 - to zero and but sometimes the air can
39:07 - start going up instead of down to zero
39:10 - and the alpha will make sure it doesn't
39:12 - go go out of control like that
39:14 - so now we're going to
39:17 - import our
39:18 - data that we collected
39:21 - so it's going to do mp.array
39:24 - now
39:25 - i'm going to be just
39:27 - this is going to be a vector or a matrix
39:29 - so that's an array of arrays
39:32 - and
39:32 - i'm just going to be typing in this
39:34 - information i kind of have some notes
39:36 - here but this is directly from the other
39:38 - slide
39:39 - of what this is going to look like
39:42 - [Applause]
39:44 - so this is kind of the
39:46 - sometimes the tedious part of machine
39:49 - learning and data science is like
39:51 - dealing with the data and inputting data
39:53 - but it's just a part of it is actually
39:55 - just trying to put everything into the
39:56 - network
39:59 - and usually you're not going to be
40:02 - having to type in the data you're
40:04 - usually going to get it from some other
40:06 - source
40:07 - so it may not be you typing in every
40:09 - single number in the data set
40:12 - let's see
40:14 - one
40:23 - and you at some point you may see me
40:25 - type something in wrong that lets it's
40:27 - obvious that's wrong so feel free to let
40:29 - me know like if i spell like a variable
40:30 - wrong or something like that
40:37 - okay so if we go back to our data this
40:40 - should
40:41 - look just like
40:42 - what we typed in right here
40:45 - and now our next piece of information is
40:48 - our walk
40:49 - verse stop array so this is again
40:52 - information we collected in the real
40:54 - world in this example
40:56 - and we're going to do another mp.array
40:59 - and then pass in the array
41:02 - which is going to be 0
41:04 - 1 zero
41:08 - one
41:09 - one
41:10 - zero so again that's the the walk versus
41:13 - if we actually have to go back a slide
41:16 - so this is anytime it's zero or stop is
41:18 - zero one is walk
41:22 - okay
41:23 - there we go
41:25 - and then we can start with our iteration
41:29 - so in the last example
41:31 - before the break i showed you a single
41:34 - iteration but in this example we're
41:37 - going to do an iteration
41:39 - of 40.
41:40 - so i'm going to do
41:42 - four iteration and range
41:44 - 40. now
41:45 - like i had said previously and sometimes
41:49 - you could have hundreds or thousands of
41:50 - iterations and the whole point is that
41:53 - every time we go through an iteration
41:55 - the error gets closer and closer to zero
41:58 - and the way you figure out how many
42:00 - iterations to do
42:02 - is just through trial and error
42:04 - so from like from
42:06 - preparing for the talk and other
42:08 - experiences kind of trying out different
42:10 - things i found out that in 40 iterations
42:13 - this network should be able to get the
42:14 - air to zero but there's no there's not
42:17 - really any magic number you just have to
42:18 - try different things and see the results
42:20 - and see how long it takes to get to zero
42:23 - so
42:25 - now
42:26 - we are going to
42:27 - have a variable called error for
42:30 - all
42:31 - lights and it's going to be set to zero
42:34 - so
42:35 - we we need to figure out the air because
42:37 - the whole point is to get the air to
42:38 - zero so we have to something to collect
42:40 - the air so we're going to be adding to
42:42 - that air later and now
42:45 - i'm going to have another iteration for
42:47 - row index
42:49 - and range
42:50 - and then it's going to just be the
42:52 - length of the stop
42:54 - verse
42:56 - walk array
42:57 - so we're going to do something for
43:00 - every
43:01 - element in this walk versus stop array
43:03 - and the number of elements in the walk
43:05 - for stop array is also the number of
43:07 - arrays or vectors in the streetlights
43:09 - array so we're going to do something for
43:11 - each one that you'll see in a second
43:12 - here
43:13 - so in our last example just like in our
43:15 - exam last example we need an input and a
43:18 - goal prediction
43:19 - so this is going to be pretty similar
43:23 - i'm going to the input is going to be
43:24 - from the streetlights array
43:26 - and i'm going to put
43:29 - the the row index
43:32 - and then the goal prediction
43:37 - is going to be from the walk verstappen
43:46 - and that's going to be the row index
43:49 - so you'll see we're going to go through
43:51 - we're going to iterate through each one
43:52 - because this array corresponds to this
43:55 - number the second array
43:57 - corresponds to the second number the
43:58 - third rate corresponds to the third
44:00 - number
44:05 - okay what part
44:14 - the space after
44:20 - what oh
44:23 - stop verse walk i should definitely
44:26 - oh like is this what we're talking about
44:32 - oh
44:33 - yeah okay
44:34 - thank you i don't know i was looking at
44:36 - it for some reason i was thinking
44:37 - there's a spelling here so it's like
44:38 - what's spelled wrong so yeah thanks for
44:40 - pointing that out so this is a this is
44:42 - an example of what i was talking about
44:44 - i'm gonna make some mistakes but luckily
44:45 - i have the audience here too to help me
44:47 - fix these mistakes here
44:49 - okay so
44:51 - now we have our input and our goal
44:54 - prediction
44:55 - we can
44:56 - make our prediction
44:59 - so let me see if i can
45:01 - can you guys still see down at the
45:03 - bottom of the screen there
45:05 - okay so
45:06 - our prediction
45:08 - now if you remember before when we had
45:11 - our prediction in the first part of the
45:13 - talk i showed that we called our neural
45:15 - network function and our neural network
45:17 - function just multiplied the input times
45:20 - the weight
45:21 - so instead of creating a neural network
45:23 - function we're just going to do the
45:24 - input times the weight
45:26 - but we're going to use
45:28 - numpy
45:32 - so
45:32 - this is this dot function
45:35 - just multiplies input times weights and
45:38 - it can multiply a vector times a matrix
45:41 - so we're getting the the prediction the
45:43 - same ways from before and now we have to
45:47 - calculate the error
45:49 - let's see
45:51 - okay there we can see a little better
45:54 - the error is going to equal
45:57 - prediction
45:59 - minus
46:00 - goal prediction
46:02 - and that's the same as before so that's
46:05 - how we figure how far away our
46:07 - prediction was from what we thought it
46:09 - was going to be is the error
46:11 - and then also oh yeah one last thing we
46:13 - have to
46:14 - take that to the power of two so
46:16 - remember the point of that is to make
46:17 - sure the error is positive if you square
46:20 - it it's going to be a positive number
46:21 - and then also it makes big errors even
46:25 - bigger and smaller areas even smaller
46:27 - and that's actually something we want
46:28 - because it can it will get to the
46:31 - correct answer even faster
46:33 - so
46:35 - now we are just going to add to our
46:37 - error for all lights
46:40 - let's see error
46:42 - for all
46:44 - lights
46:45 - and i'm just going to plus equals error
46:49 - so we're just
46:50 - we're just collecting all the errors
46:52 - into the air for all lights so we're
46:54 - getting an air for each one of the walk
46:56 - for stop array and we're putting them
46:58 - all together because we want the
46:59 - collective error to get as close to zero
47:02 - as possible
47:04 - and then we're going to calculate the
47:05 - delta
47:07 - and that's going to be prediction
47:09 - minus goal prediction
47:12 - this is just how far away our prediction
47:14 - is from our goal prediction and it's
47:16 - considered the raw error this is the
47:18 - same as before
47:19 - and now we are going to calculate the
47:22 - weights
47:24 - now in our previous example the weights
47:26 - equaled um weights minus weight delta
47:29 - times alpha and we're going to basically
47:31 - have the same
47:33 - same thing here but we're going to
47:34 - instead of creating the weight delta
47:36 - variable if you remember from before
47:38 - we're going to kind of put it in line
47:40 - and it's going to be
47:42 - weights
47:44 - minus
47:47 - alpha
47:49 - oh and we need a parenthesis here
47:56 - times
47:58 - input so before the weight delta was
48:01 - input times delta
48:03 - and instead of making that weight delta
48:05 - variable
48:08 - we're going to put it right in line so
48:09 - weights equals weights minus alpha times
48:11 - input times delta so that's the same as
48:13 - before
48:14 - and now
48:18 - we're going to just print out some data
48:19 - so this is actually the whole code and
48:22 - so i'm going to print out some data here
48:24 - and then we'll see what happens so we're
48:26 - going to do
48:29 - we're going to print the prediction
48:35 - so we want to see the prediction
48:37 - variable and i'll just do the string of
48:39 - prediction
48:42 - now i'm going to use some copying and
48:43 - pasting here
48:45 - the prediction will do for each of that
48:47 - iteration but the next two will put in
48:50 - to this next
48:52 - loop let's see
48:55 - if that lines up right
49:02 - oh right here okay thanks i appreciate
49:05 - that
49:05 - okay so now we have um
49:09 - weights
49:11 - and weights
49:14 - and then
49:17 - we'll print out the error
49:22 - and the error
49:25 - okay
49:26 - so we're about to run this code and see
49:27 - what happens but before we run the code
49:29 - i'm going to go back to
49:32 - the slides
49:34 - okay so this diagram
49:36 - demonstrates what our network looks like
49:39 - so we have three inputs
49:41 - and then we pass them through these
49:42 - weights
49:43 - and then we're going to get an output
49:45 - which is the walk versus opt
49:47 - whether the the output is going to show
49:50 - whether those inputs mean to walk or
49:53 - stop so that's the whole point of our
49:55 - network we're trying to learn from do
49:57 - these inputs of the light mean to walk
50:00 - or stop so that's what we're going to
50:03 - do here
50:04 - we go i'll just run this code
50:08 - and it's going to take a little bit but
50:10 - we should see
50:11 - if i didn't make any spelling mistakes
50:13 - we should see some stuff appearing at
50:14 - the bottom here which is the data
50:19 - okay so you'll see at the fir first the
50:23 - the prediction doesn't really correspond
50:25 - to anything at first and the weights are
50:27 - going to be pretty similar to the
50:28 - weights we passed in at the very
50:29 - beginning
50:31 - so
50:31 - you can see here here are the weights so
50:33 - we have 0.6 0.4 negative 0.2 that's
50:36 - pretty close to the weights we started
50:38 - with right here but if we keep going
50:41 - down remember we did 40 iterations so
50:43 - every time it gets the air we're to the
50:46 - next iteration so the air starts at
50:48 - point three we're trying to get as close
50:50 - as possible to zero
50:51 - so if we just keep going down we're not
50:54 - going to look at every single iteration
50:56 - we're going to just kind of go down to
50:57 - the bottom and see
51:00 - what we've gotten to
51:02 - okay so here's the the last
51:05 - the last thing that we want to look at
51:07 - and if you look at the prediction um
51:10 - this number is very close to zero then
51:12 - the second one is close to one then we
51:14 - have one close to zero one one zero so
51:17 - if you remember one
51:19 - this actually is the same as
51:24 - our stop first walk so we have the stop
51:26 - is zero one zero one one zero and that's
51:29 - what the prediction ended up being close
51:31 - to zero one zero one one zero
51:34 - um now that's not necessarily super
51:36 - exciting because we actually gave gave
51:38 - the network that information so of
51:40 - course it's going to be able to figure
51:41 - that out that information because we put
51:43 - it right in the code uh another thing i
51:46 - just kind of showed you first was the
51:48 - error
51:49 - this is a number very close to zero it
51:51 - starts with a three point but then when
51:52 - you see
51:53 - we this is just like a
51:56 - an exponent that makes it really it's a
51:58 - really small number very close to zero
52:00 - so and that's the whole point of our
52:02 - our algorithm is trying to get the air
52:04 - to zero so what's really important to
52:07 - look at is the weights
52:09 - so
52:10 - if you see the weights these weights
52:12 - correspond to each light so the first
52:14 - light the first weight is close to zero
52:17 - the second light is close to one and the
52:19 - third white is close to zero so one
52:22 - means it's perfectly correlated and zero
52:26 - means it's not correlated at all at all
52:28 - and if you remember from our diagram
52:31 - the middle light was perfectly
52:33 - correlated on whether you should walk or
52:35 - stop and the the right and left lights
52:37 - were not correlated at all so there was
52:40 - no point in the code that we gave the
52:43 - program that information
52:45 - but somehow it was able to learn that
52:48 - the middle light was perfectly
52:50 - correlated and the outer lights were not
52:52 - correlated at all now in this example it
52:55 - was something that you can easily figure
52:57 - out just by looking at the data which
52:59 - one's correlated but you can just kind
53:01 - of imagine in a more complex data set
53:05 - you may not be able to figure out
53:07 - how the data is correlated just by
53:09 - looking at it so you use the exact same
53:14 - type of ideas to make an even more
53:17 - complex neural network and so you can
53:19 - actually learn things that that's not
53:22 - that aren't as easy to just learn just
53:24 - by looking at the data
53:26 - so
53:28 - um
53:29 - [Music]
53:30 - our network like i said our network
53:32 - correctly identified the middle light by
53:35 - analyzing the final weight positions of
53:38 - the network so
53:40 - to like think about how it identified
53:42 - the correlation it was in the process of
53:45 - gradient descent
53:46 - each training example either is going to
53:48 - assert upward pressure or downward
53:51 - pressure on the weights so on average
53:53 - there was more upward pressure for the
53:55 - middle weight and more more downward
53:56 - pressure on the outer weights so they
53:59 - got to the correct numbers
54:01 - so
54:02 - this is pretty cool and now i'm going to
54:05 - show you something even a little a
54:07 - little more complex
54:09 - so the next part of the talk
54:12 - instead of live coding i'm going to show
54:13 - you some code that's already written
54:15 - that's pretty similar to the code we
54:17 - already had but i'm going to introduce a
54:19 - few more concepts
54:21 - first of all i'm going to tell you that
54:22 - some of the concepts could even be a
54:24 - whole talk to themselves so i'm going to
54:26 - do my best to like quickly overview the
54:28 - concepts but some of the things you'll
54:30 - just kind of get a taste of so you can
54:32 - learn more about on your own later
54:35 - but the reason why i'm going to show you
54:37 - one more program is to talk about the
54:40 - deep part of deep learning so nothing
54:43 - i've showed you so far
54:45 - is actually deep so let me deep refers
54:50 - to one or more hidden layers between the
54:53 - input and the output layers of a neural
54:55 - network so if i go to
54:59 - my code here so our input layer is right
55:02 - here our output layer is right there
55:03 - there's nothing in between
55:05 - but if i look we look at this example
55:07 - this is a true deep neural network
55:10 - because we have our input layer that
55:12 - looks the same there's three on the
55:13 - input and one on the output for layer
55:16 - two but now there's this middle layer
55:18 - which is a hidden layer and since
55:21 - there's one or more hidden layers this
55:23 - is a deep neural network
55:25 - so you may be wondering why would you
55:27 - even need a hidden layer if our other
55:29 - network was able to correctly predict
55:31 - the correlation between the lights
55:33 - because we're going to use the same
55:34 - example from before
55:35 - so if you remember the data the middle
55:38 - light was perfectly correlated with
55:40 - whether you should walk or stop
55:42 - but if the middle light wasn't so
55:44 - perfectly correlated our network would
55:46 - have had a much harder time figuring out
55:49 - how the how the input was related to the
55:52 - output so like for instance if our
55:54 - training data looked like this
55:56 - our other network wouldn't have worked
55:58 - so well if you look at this so the the
56:01 - numbers in red
56:02 - are the the light information whether
56:04 - the lights are on or off and then the
56:06 - numbers in black are the stop or walk
56:09 - information so you can see this first
56:12 - the first three up here means to walk
56:14 - and we have walk stop stop and one thing
56:17 - you'll notice is that the output data is
56:19 - not perfectly correlated with any of the
56:22 - rows
56:23 - so
56:24 - this is where you need to have a
56:27 - hidden layer
56:28 - um what is because since our input data
56:31 - set does not correlate with our
56:33 - correlate with our output data set we're
56:35 - going to use our input data set to
56:37 - create an intermediate data set that
56:40 - does have correlation with our output
56:42 - and this intermediate data set is our
56:45 - hidden layer it's almost going to be
56:47 - like two neural networks it's going to
56:49 - be
56:50 - from layer 0 to layer 1 is going to be
56:52 - one network that runs just like before
56:55 - and then the so
56:57 - the output to the layer 0 to layer 1
57:00 - network will become the input to the
57:03 - layer 1 to layer 2 network
57:05 - so let me
57:07 - show you
57:08 - okay
57:10 - so can you guys see this all right
57:12 - even in the back okay so
57:14 - a lot of this is going to look pretty
57:15 - similar here we have we're importing
57:18 - numpy as mp
57:20 - and then this line right here
57:22 - just make sure we're going to be using
57:24 - random numbers in this code and that
57:27 - line just makes sure the rand.seed makes
57:29 - sure every time we get random numbers
57:31 - it's the same random numbers so a lot of
57:34 - times when you're dealing with neural
57:36 - networks you're using random numbers for
57:37 - the weights but to make sure the output
57:40 - is always the same
57:42 - even though you're using random numbers
57:43 - you can make sure that every time the
57:45 - code runs the random numbers were the
57:47 - same random numbers that you used last
57:49 - time you ran the code and it helps for
57:51 - like comparing uh to make sure the code
57:54 - the results are exactly the same
57:58 - so this is called a this is an
58:01 - activation function it's the relu
58:02 - function
58:04 - we're going to pass our data into it
58:06 - it's going to return x or return it's
58:08 - going to return what the data we passed
58:11 - in if x is more than zero
58:13 - or return zero otherwise
58:16 - now
58:16 - this is a it's called an activation
58:19 - function because it activates some input
58:22 - by returning the input and deactivates
58:25 - other input by returning zero
58:28 - this code is going to show a three layer
58:30 - neural network
58:31 - and by turning any middle network
58:34 - off whenever it would be negative we
58:36 - allow the network to sometimes
58:39 - have the correlation from various inputs
58:41 - and sometimes not have the correlation
58:44 - so it's impossible for a two layer
58:45 - network to sometimes allow this input to
58:48 - correlate and sometimes not allow it to
58:50 - imp correlate so it adds more power to
58:53 - three layer networks so this sometimes
58:56 - correlation is important for things to
58:58 - work correctly and this is one of the
59:00 - areas that like i was saying could
59:02 - get a whole talk to itself as activation
59:04 - functions and how they work and why
59:06 - but i'm just going to kind of leave it
59:08 - at that for now and
59:10 - that the activation is functions
59:13 - important to make it sometimes correlate
59:14 - and sometimes not correlate the the relu
59:17 - two derive function is next
59:20 - and
59:21 - uh this returns one when the output is
59:24 - more than zero or zero otherwise and
59:26 - that's the slope of a reload function or
59:29 - its derivative and you'll see later how
59:31 - that's important
59:34 - so the next two this is just inputting
59:36 - the data just like we did before
59:38 - we do have the dot t that's a numpy
59:41 - thing it's the transpose operator and it
59:44 - just um changes so you have a matrix
59:47 - which is going to have like uh almost
59:49 - like an x and y coordinates like the so
59:52 - it's a if it's a three by four matrix
59:54 - the transpose operator would make it a
59:55 - four by three or if it's a one by four
59:57 - it'll become a
59:58 - four by one and when it's when you're
60:00 - multiplying a matrix and a vector it
60:03 - matters
60:04 - um which kind of direction the the
60:07 - matrix is if that makes sense
60:10 - and so
60:12 - the alpha is same before again sometimes
60:16 - you have to kind of try out different
60:17 - things to figure out what it's going to
60:18 - work
60:19 - and the hidden size is four so if we go
60:22 - back to our diagram you can see that our
60:24 - middle layer has four nodes so that's
60:27 - what this hidden size equals four is
60:29 - there's four nodes
60:30 - and then here is our weights so
60:36 - okay let me
60:39 - get back to that code here i
60:40 - accidentally did the
60:43 - keyboard the mouse shortcut for going
60:45 - back so if we go to our weights here
60:48 - before this is where we had the audience
60:50 - participation and i had people
60:52 - shout out different weights but now
60:54 - we're going to get truly random weights
60:56 - because we have a lot more weights this
60:59 - is going to be a matrix of weights and
61:02 - the size of the matrix like the matrix
61:04 - is similar to this this is a matrix
61:06 - right here up top a 3 by 4 matrix and
61:09 - this is going to be a three by four
61:11 - matrix and this is going to be a four by
61:13 - one matrix and each number in that
61:16 - matrix is going to be between negative
61:18 - one and zero
61:19 - and the reason why we have three by four
61:21 - is because you can see there's three
61:23 - nodes by four nodes and then we have
61:24 - four nodes by one node so the size of
61:27 - the weights has to correspond to the
61:30 - input and output layers because you can
61:32 - kind of you can see that this has
61:34 - there's a weight between layer one and
61:36 - layer zero and there's weights between
61:37 - layer one and layer two and the size of
61:39 - the weights have to correspond to those
61:41 - layers
61:42 - so
61:44 - now
61:45 - we're gonna you can see we're gonna
61:46 - iterate 60 times
61:49 - and then we're gonna keep track of the
61:50 - air just like before we were keeping
61:52 - track of the error
61:53 - it's just called something different so
61:56 - in this code there's gonna be a lot of
61:58 - things that are pretty much just like
61:59 - the code that i just did for the live
62:01 - coding but the names are slightly
62:03 - different
62:05 - so
62:06 - layer 0 here
62:09 - this is the same as the input from
62:11 - before so we're just getting the first
62:14 - street light which would be
62:16 - this matrix right here this vector right
62:18 - here
62:19 - and
62:20 - this is just
62:22 - a kind of fancy way of saying that we're
62:23 - going to get a
62:25 - nested mate a nested array so instead of
62:27 - an
62:28 - instead of an array of one zero one it
62:31 - will be an array of an array of one zero
62:33 - one so they'll just be like and if you
62:34 - output it there'd be an extra array on
62:36 - the sides
62:37 - um
62:38 - there you go so
62:42 - the re so that that just makes the kind
62:44 - of math work to have an array of arrays
62:46 - but it's just getting the first array
62:48 - from the streetlights
62:50 - and then layer one here
62:52 - is going to be the same as the input and
62:55 - the same as the prediction from before
62:57 - remember the other network we had one
62:59 - input and that input had one prediction
63:03 - but in this neural network it's
63:05 - basically two neural networks uh there's
63:08 - two two networks there's layer one to
63:09 - layer there's layer zero to layer one
63:11 - and there's layer one to layer two so
63:14 - the
63:14 - prediction or output
63:16 - from layer 0 to layer 1 becomes the
63:19 - input from layer 1 to layer 2.
63:22 - so
63:23 - this layer 1 is the input and
63:26 - the prediction so
63:28 - just like before we're always just mult
63:31 - to get a prediction you always multiply
63:34 - the input times weights so that's what
63:37 - this is it's another numpy function to
63:39 - just multiply the things that you pass
63:41 - into that function layer zero and weight
63:43 - zero one that's what you're always doing
63:45 - for your prediction multiplying inputs
63:47 - times weights
63:48 - then we run it by the relu function
63:50 - which will selectively activate like we
63:53 - talked about before layer two is our
63:55 - output layer
63:57 - and
63:58 - which is the same as our prediction from
64:00 - before and again we're just multiplying
64:02 - the input times the weight so the input
64:04 - is layer one
64:06 - and the weight is weights one two so
64:09 - this is the same thing we've been doing
64:10 - all along to get the prediction from the
64:12 - first stock from the very first code
64:14 - example
64:15 - this was the neural network we had a
64:17 - neural network function that just
64:18 - multiplied weights times input and
64:20 - that's exactly what we're doing here
64:23 - so now we have to get our error
64:28 - so our error is going to we have our
64:30 - layer 2 error here and we're going to
64:32 - calculate that pretty much just like
64:34 - before but we have different names so
64:38 - before it was the air was prediction
64:40 - minus goal prediction squared uh but now
64:44 - the prediction is that layer two
64:45 - variable the goal prediction is the
64:48 - getting something from the walk versus
64:49 - stop array and then we square it so air
64:52 - is always prediction my school
64:54 - prediction squared and then
64:57 - we're going to do a plus equal so we're
64:59 - going to add up all the errors together
65:02 - into our layer 2
65:05 - error variable
65:07 - okay so the next thing is our delta this
65:11 - is basically exactly like the the other
65:13 - example
65:14 - um so
65:16 - this
65:17 - we have the layer two
65:19 - so this is just
65:20 - input or wait a second yeah input minus
65:24 - goal prediction prediction minus goal
65:26 - prediction so layer two minus walk for
65:28 - stop is the same as prediction mice goal
65:30 - prediction
65:31 - and remember the goal here is um air
65:35 - attribution it's all about figuring out
65:37 - how much each weight contributed to the
65:39 - final error
65:40 - so in our first two layer neural network
65:42 - we calculated a a delta value a delta
65:45 - variable which told us how much higher
65:48 - or lower the output prediction is
65:50 - supposed to be so we're calculating that
65:52 - the same way
65:53 - so now we have how much that we that
65:57 - though we want the final prediction to
65:58 - move up or down that's the delta and now
66:01 - we need to figure out how much we want
66:02 - each middle layer layer one node to move
66:05 - up or down so
66:07 - one thing that's interesting about this
66:10 - is if we go to our diagram we can see
66:12 - there's an order layer 0 layer 1 layer
66:15 - 2.
66:16 - but in our code
66:18 - we're gonna calculate layer two first
66:21 - and then
66:22 - layer one so we're not calculating it in
66:25 - the same order as you would normally
66:28 - think going through the going through
66:30 - the layers
66:31 - this is another like really big concept
66:33 - called back propagation let me it's a
66:38 - let me zoom out so you can see all the
66:40 - notes i have here so
66:42 - in back propagation the layer 2 delta is
66:45 - back propagated to the layer one delta
66:48 - and this is going to give us a weighting
66:50 - of how much each weight contributed to
66:53 - that
66:54 - error
66:57 - that's supposed to be in the word error
66:59 - oh it is it's just off the side of the
67:01 - screen okay so
67:04 - we need some way for our layer two
67:08 - weights to impact layer one because we
67:10 - need to update everything so that's what
67:12 - back propagation is
67:14 - and so this is another pretty big
67:16 - concept i mean back propagation
67:19 - could be a whole talk of its own but
67:21 - it's all about taking uh information
67:24 - from later on in the the network and
67:27 - passing it back
67:28 - to uh to the earlier in the network so
67:30 - it can use that information when it's
67:32 - making the calculations
67:34 - and
67:36 - let's see
67:37 - if
67:38 - there's one more thing we need to talk
67:39 - about so this is for the layer one delta
67:43 - here we're just using this dot
67:46 - function to multiply layer two delta by
67:49 - the weights and we have the transpose
67:51 - operator
67:52 - but then the final thing
67:54 - is this relu to derive function
67:56 - so if the relu set the output to layer
67:59 - one to the layer one node to be zero
68:02 - then it didn't contribute to the air at
68:03 - all so when this was true we should also
68:07 - set the delta of that node to be zero
68:09 - and multiplying each layer one node by
68:13 - the relu two derived function
68:14 - accomplishes this the relu two derived
68:17 - is either a one or a zero depending on
68:20 - whether the layer one value was
68:23 - more than zero or not
68:26 - so now we're going to update the weights
68:30 - and we update the weights
68:31 - just like we did before
68:34 - so
68:35 - for each weight we are going to multiply
68:37 - its input value by its output delta
68:42 - and so it's just weights one two equals
68:45 - weights one two
68:46 - uh
68:48 - minus
68:49 - wait i'm thinking yeah yeah weights one
68:51 - two equals weights one two minus alpha
68:54 - times and then so this is just how we
68:57 - calculate the weight before where you
68:59 - multiply
69:00 - the
69:01 - input by the delta
69:03 - and then you multiply that by the alpha
69:06 - and then that's that's just how you
69:07 - update the weights
69:09 - so
69:11 - again this code is all all online so you
69:14 - can kind of review it later like some of
69:16 - this stuff it kind of takes going
69:17 - through a few times to really see how it
69:20 - all goes together but you'll see that
69:22 - we're updating the weights in the exact
69:23 - same way as we were in the other live
69:26 - coding sessions that we did
69:28 - so
69:30 - down here
69:34 - this is going to just make sure that
69:36 - we're only going to
69:38 - um print
69:39 - this information uh just every once in a
69:42 - while so every few iterations so when we
69:45 - run when we run this it's only going to
69:47 - give the information
69:48 - every few iterations so i'm just going
69:50 - to run this right now
69:52 - and
69:53 - i think it has that from last time i ran
69:55 - it
69:56 - but if the main thing to
69:59 - look at here
70:01 - is that
70:02 - if we go to the bottom
70:03 - [Music]
70:06 - our error
70:08 - gets very close to zero so that's the
70:11 - whole point of learning the learning is
70:13 - trying to get our air as close as
70:14 - possible to zero
70:16 - now if you remember before that that
70:18 - meant that our weights were correctly
70:20 - we're making a correct prediction in
70:22 - this case it's it's the same
70:24 - now
70:25 - i'm kind of getting close to to the
70:28 - the conclusion of kind of why all this
70:30 - matters so what i mean there's probably
70:33 - we could probably talk about this code
70:35 - for a few more hours to explain it every
70:37 - little part like the back propagation
70:39 - and and the weights but i kind of want
70:41 - to do kind of step back a little bit and
70:43 - give kind of an overview of the point of
70:47 - why we create intermediate data sets
70:49 - that have correlation
70:51 - so i go back over here
70:54 - okay so consider this image of this
70:57 - handwritten four
70:59 - that's kind of blown up it's supposed to
71:01 - be supposed to look like the number four
71:03 - and uh if you if we had a data set of a
71:06 - bunch of images of handwritten digits
71:09 - and they were all labeled like zero to
71:11 - nine um if we wanted to train a neural
71:14 - network to take the pixel values from
71:18 - this image and predict that this was a
71:21 - four
71:22 - our two layer neural network would have
71:24 - never been able to do it so they're just
71:26 - there's no
71:28 - individual pixel in this
71:30 - and this here so there's no individual
71:32 - pixel that perfectly correlates with
71:35 - whether it's a four or not
71:37 - just like in our second example that i
71:40 - just went over there was no
71:42 - light combination that perfectly
71:44 - correlated with walk or stop
71:46 - so there's
71:47 - there's only different configurations of
71:49 - pixels that correlate with whether or
71:50 - not this is a four so this is the
71:53 - essence of deep learning if i go to the
71:55 - next slide you'll see how this would
71:57 - work with kind of the multiple layers
71:59 - where deep learning is all about
72:01 - creating intermediate data sets or
72:03 - layers where each node in an
72:05 - intermediate layer
72:06 - represents the presence or absence of a
72:09 - different configuration of inputs
72:12 - so
72:13 - in this way no pixel has to perfectly
72:15 - correlate with whether or not it's a
72:18 - four or not instead we have these middle
72:20 - layers that
72:23 - they attempt to identify different
72:25 - configurations of pixels that may or may
72:27 - not correlate with the four so we go
72:29 - through a lot of different layers and
72:30 - finally
72:32 - it finds which layer correlates if it
72:34 - finds a configuration of pixels that
72:36 - correlates with whether it's a four not
72:38 - and then it outputs that this is a four
72:42 - so the presence of many four like
72:45 - configurations
72:46 - would then give the final layer the
72:48 - information or correlation it needs to
72:51 - correctly predict whether the digit is a
72:54 - four
72:55 - so we can take our three layer network
72:58 - and continue to stack
73:00 - many many more layers
73:02 - some networks even have hundreds of
73:04 - layers
73:05 - and each neuron plays its part in
73:07 - detecting different configurations of
73:09 - input data so you could start with like
73:12 - we started with these uh
73:14 - input of three layers but the hidden
73:15 - layers could get bigger and bigger and
73:17 - bigger
73:18 - until it finally has what it needs to
73:20 - correctly predict the output layer
73:23 - so i'm hoping that just kind of seeing
73:25 - this for example helps you kind of see
73:29 - how how adding additional layers can
73:32 - help you
73:34 - get the final prediction where you don't
73:35 - need just like our stop light example
73:38 - you don't need the perfect correlation
73:41 - to figure out the final answer you can
73:43 - make other layers and other layers after
73:45 - that that finally get the correlation to
73:47 - get the final
73:49 - answer so i'm getting kind of i'm cl to
73:51 - the end of the talk here so i'm hoping
73:54 - you just by this talk you've got a good
73:57 - introduction to deep learning and neural
73:58 - networks and you saw by the examples how
74:01 - we were able to write a neural network
74:03 - they actually learned what we're trying
74:05 - to learn
74:06 - and then i also hope you unders you
74:08 - start to understand how hidden layers
74:11 - can can give your neural network even
74:14 - extra power to figure out things it
74:15 - would not have normally
74:17 - figured out
74:18 - so yeah thank you for coming to the talk
74:21 - and i'm open for questions right now
74:24 - [Applause]