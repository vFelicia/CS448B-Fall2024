00:00 - opencv is a popular python library for
00:02 - real-time computer vision
00:04 - this course comes directly from the
00:06 - creators of opencv and is the perfect
00:09 - course for beginners at the end of the
00:10 - course there's an interview with the ceo
00:12 - of opencv dr malik and he talks to you
00:16 - about how to get a job in computer
00:17 - vision and ai
00:19 - in 2005 for the first time in human
00:21 - history an autonomous vehicle traveled
00:24 - 132 miles through the mojave desert to
00:28 - win the 2 million dollar darpa grand
00:30 - challenge
00:31 - the name of the car was stanley and it
00:33 - used a computer vision library called
00:36 - opencv
00:37 - hello everybody i'm satya malik and i'm
00:39 - thrilled to help you get started with
00:41 - opencv
00:42 - built over 20 years opencv is the most
00:45 - extensive computer vision library in the
00:47 - world it is downloaded between one to
00:50 - two million times a week and it contains
00:52 - over 2500 optimized algorithms
00:56 - to build computer vision and ai
00:58 - applications opencv is the first library
01:01 - you need to learn welcome to this free
01:03 - getting started series we designed it
01:05 - for absolute beginners all you need is
01:08 - an intermediate level of programming
01:09 - knowledge and python opencv is vast it
01:13 - is not possible to cover all aspects of
01:15 - the library in the short period of time
01:18 - this series is your first step it will
01:20 - help you get started the material in the
01:23 - series will be covered using jupyter
01:26 - notebooks
01:27 - we will go over every notebook in a
01:29 - video to help you understand the code
01:33 - the first few notebooks are all about
01:35 - basics
01:37 - what are images and videos how do we
01:39 - represent them inside opencv what opencv
01:42 - functions do we use to read write and
01:45 - manipulate photos and videos
01:47 - next we will go over image enhancement
01:50 - and filtering
01:52 - our objective in this series is also to
01:54 - give you a glimpse into applications you
01:57 - can build using opencv functions we will
01:59 - go over different image transformations
02:02 - and show you how to align two images
02:05 - the same idea can be modified slightly
02:07 - to create beautiful panoramas
02:10 - next we will dip our toes into
02:12 - computational photography and create
02:14 - high dynamic range images
02:16 - by combining photos taken using
02:18 - different exposures into one single
02:21 - beautifully lit photo
02:23 - opencv also implements many classical
02:26 - machine learning algorithms and has an
02:29 - entire module dedicated to deep learning
02:31 - inference we will learn how to implement
02:34 - face detection and object tracking
02:37 - finally we will wrap up the series by
02:39 - learning how to use deep learning module
02:42 - for object detection and pose estimation
02:45 - it's going to be very interesting
02:48 - that's all we will cover in this getting
02:50 - started series and after completing this
02:52 - series i encourage you to go and
02:55 - take a look at the free content at
02:57 - opencv.org
02:58 - and learn opencv.com
03:01 - and when you're ready for structured
03:03 - learning and you're seeking mastery in
03:06 - computer vision and ai check out our
03:08 - courses at opencv.org
03:11 - courses
03:12 - i wish you all the best in your learning
03:14 - path but today let's get started with
03:17 - opencv
03:20 - hi everyone in this introductory video
03:22 - we're going to be covering several basic
03:24 - image processing concepts related to
03:26 - working with images
03:28 - all this material is very essential and
03:30 - we'll be using python and specifically
03:32 - opencv to demonstrate for example how to
03:34 - read an image and display it
03:36 - understanding how images are represented
03:37 - by data
03:38 - of the differences between grayscale
03:40 - images and color images and specifically
03:42 - what it means to have multiple channels
03:44 - in color images and then finally how to
03:46 - save images
03:48 - so just a couple of things i wanted to
03:49 - mention before we get started is that
03:51 - we'll be using jupiter notebooks for
03:53 - most of these demonstrations mainly
03:55 - because it's a very convenient way to
03:56 - display intermediate results and has
03:59 - some very nice documentation features
04:01 - that make it easy to present code and
04:03 - supporting material
04:04 - so in some cases we might actually use
04:06 - python scripts for specific applications
04:09 - but mainly we'll be using jupyter
04:10 - notebooks in this series
04:13 - so to get started i just wanted to talk
04:14 - a little bit about this first section
04:16 - here where we're importing some required
04:17 - libraries
04:18 - one thing i wanted to mention is that
04:20 - when you use jupyter notebooks
04:22 - you want to use this map plot lib inline
04:25 - specification here so that we can
04:27 - display images directly in the notebook
04:29 - and then one other thing that we'll be
04:31 - using in this notebook is this ipython
04:33 - function image
04:35 - which will allow us to display and
04:37 - render images directly in the notebook
04:40 - so in this first example here below
04:42 - we're going to actually use that
04:43 - function to display two checkerboard
04:46 - images here they're both black and white
04:47 - checkerboard images
04:49 - and the first one here is 18 by 18
04:51 - pixels and this next one here is 84
04:53 - pixels by 84 pixels and you can see that
04:56 - if we
04:57 - read in those files and display them
04:59 - directly in the notebook
05:01 - that their
05:02 - actual size is uh
05:04 - faithfully rendered so the
05:06 - 18 by 18 pixel image is quite small here
05:10 - and you can see the difference between
05:11 - the two images
05:13 - and one of the reasons we wanted to
05:15 - start with that is because generally
05:17 - we'll be using opencv to actually
05:19 - read in
05:20 - an image and store that data in memory
05:23 - as a numpy array and then
05:25 - working with that numpy array in terms
05:28 - of manipulating the image or saving the
05:31 - image or displaying the image and in
05:33 - those cases we're really displaying a
05:34 - mathematical representation of the image
05:36 - and not necessarily
05:38 - a faithful representation of the image
05:40 - within the browser itself so that'll
05:42 - become more clear as we um
05:44 - proceed further through this notebook
05:45 - but we just wanted to draw that
05:46 - distinction
05:48 - so let's scroll down here and take a
05:49 - look at the first function that we're
05:51 - going to be using from opencv
05:53 - before we get started though i just
05:54 - wanted to mention that
05:56 - when we introduced new functions we
05:58 - decided to include this documentation
06:01 - section here in the notebooks that
06:03 - summarize the functional syntax i
06:05 - describe some information about the
06:07 - arguments required maybe some optional
06:09 - arguments and then the um
06:11 - we've also provided the opencv
06:13 - documentation links
06:15 - so we're not going to go through these
06:17 - sections in great detail but we will
06:19 - refer to them when we describe some of
06:20 - the code in these notebooks
06:23 - so let's just take a look at how we use
06:25 - i am read and in this first example here
06:27 - we're reading in the smaller
06:28 - checkerboard image notice that the first
06:30 - argument is just the file name for the
06:33 - for the image itself which can be either
06:35 - a relative or an absolute path name
06:37 - and notice also there's actually an
06:39 - optional second argument here and here
06:41 - we're indicating zero
06:43 - and if you look back up here in the
06:44 - documentation section you see that zero
06:46 - corresponds to a flag that specifies
06:49 - that we want to read this image in as a
06:50 - grayscale image
06:52 - and we'll talk more about this further
06:54 - below but i just wanted to point out
06:55 - that there is an optional second flag
06:57 - here that we'll make use of quite a bit
07:00 - and then the return from imread is a
07:03 - numpy 2d array representing the image
07:06 - and i can print that information here
07:07 - using the print command and you'll see
07:09 - uh down here there's the data that
07:11 - represents that image so it's 18 rows
07:14 - and 18 columns and each of the values
07:16 - represents the pixel intensities
07:19 - for each of those pixels
07:21 - and notice that they're in the range of
07:22 - 0 to 255 because this image is being
07:25 - represented by an unsigned 8-bit integer
07:28 - so let's scroll down here a bit further
07:29 - and print some information associated uh
07:32 - with the image
07:33 - so drawing your attention up here uh to
07:36 - this code block
07:37 - we're using the shape and d type methods
07:40 - uh of that
07:41 - numpy array object to print out both the
07:44 - image size and the data type so
07:47 - here you can see it's 18 by 18 and
07:49 - unsigned in as we had pointed out
07:51 - and so at this point let's talk about
07:53 - how to display the image so here in this
07:55 - code block we're using the matplotlib
07:58 - function i am show
08:00 - to display a representation of that
08:02 - image and so here we're just passing it
08:04 - the numpy array that represents that
08:05 - image
08:06 - and you'll see this plot below so notice
08:09 - that this is actually a plot or
08:10 - mathematical representation of that
08:12 - image but it's not 18 pixels
08:15 - a wide on my screen it's it's just a
08:17 - plot representing 18 pixels and so you
08:19 - can see the axes here correspond to 18
08:22 - by 18.
08:24 - also notice that it's not black and
08:25 - white as we expected but looks like
08:28 - yellow and dark navy blue
08:30 - and
08:31 - the reason for that is that matplotlib
08:33 - uses color maps to represent image data
08:37 - and in this particular case it's using
08:39 - some other color map rather than a
08:42 - grayscale color map so
08:44 - if we want to display this as an actual
08:45 - grayscale image we need to
08:48 - actually set the color map so we're
08:50 - going to do that here in this in this
08:52 - section where we call i am show with an
08:54 - optional
08:55 - argument
08:56 - uh color map equals gray and if we set
08:58 - the color map equal to gray then then we
09:00 - get what we expected a black and white
09:02 - uh image
09:04 - representation
09:05 - so let's take a look at another example
09:10 - here we're reading in another
09:12 - checkerboard image of the same size
09:15 - but this has
09:16 - pixel intensities that range from 0 to
09:18 - 255 with lots of gray values in between
09:21 - so you can see that reflected here in
09:23 - the matrix itself
09:25 - and then when we plot the image you can
09:27 - see the grayscale representation of
09:29 - those middle tone values
09:31 - so it's not a very interesting image but
09:33 - it demonstrates just the idea that a
09:35 - grayscale image can have values between
09:37 - 0 and 255 and that those are represented
09:40 - on a continuum
09:42 - from
09:44 - pure black to pure white
09:46 - so now we're ready to talk about color
09:49 - images so let's scroll down to this next
09:50 - section here where we're going to read
09:52 - in a high-resolution image of the
09:54 - coca-cola logo
09:56 - so here we're using the ipython image
09:59 - function to do that and we're rendering
10:00 - that image in the browser
10:02 - and um let's scroll down to this next
10:05 - section now and actually use opencv
10:07 - imread to read that in and store that
10:10 - data in a in a matrix
10:12 - so here we're storing that information
10:14 - in this coke underscore image object
10:17 - notice that when we read it in we
10:18 - specified an optional second argument of
10:21 - one here so it's going to read in this
10:23 - image as a
10:24 - color image
10:25 - and it turns out this image is in a rgb
10:28 - format so there's three channels one for
10:30 - red green and blue and when we print out
10:32 - the size and the data type of this
10:34 - matrix you can see it's 700 by 700 by 3
10:37 - where 3 is the number of channels
10:40 - but notice that when we display the
10:41 - image down here using matplotlib i am
10:44 - show
10:45 - comes up blue which is unexpected
10:47 - and the reason for that is because
10:49 - opencv actually uses a um
10:51 - a different format for storing the
10:53 - channel information than most other
10:55 - applications
10:56 - so for rgb images opencv uses a
10:59 - channel order of bgr rather than rgb
11:03 - so matplotlib is expecting this to be
11:06 - rgb
11:08 - but the way that opencv read this into
11:10 - the matrix it stores it in bgr so it
11:14 - ends up looking blue unless we swap the
11:16 - order of the channels so that's what
11:18 - this next bit of code down here does
11:21 - in this block here we're
11:23 - taking this coke image array
11:25 - and swapping the order of the channels
11:26 - here that's what this syntax does here
11:28 - it reverses the order of that last
11:30 - member of the array
11:31 - and now we're going to display that and
11:33 - it comes up
11:34 - red and white as we expected
11:36 - so that's just something to be aware of
11:38 - whenever you're working with opencv you
11:40 - need to be aware of the channel order
11:42 - convention and we'll see that come up
11:44 - again and again in these notebooks
11:46 - so now we're going to take a look at
11:48 - splitting and merging color channels in
11:50 - this next section here
11:55 - splitting and merging um are pretty
11:57 - straightforward in opencv
11:59 - and they can refer to this documentation
12:01 - link for more details but uh here's the
12:03 - example so let's just go over that
12:06 - here on this first line we're using i am
12:08 - read to read in a color image and notice
12:10 - here for the optional argument we're
12:12 - actually specifying the flag as opposed
12:14 - to one
12:15 - for specifying that we want to read this
12:17 - in as a color image and we're going to
12:19 - store that result in image underscore nz
12:22 - underscore bgr and i specifically used
12:25 - bgr in the name of the variable to
12:27 - remind myself that
12:28 - that's what this represents since we're
12:30 - reading it in using opencv
12:33 - and now on this next line i'm going to
12:35 - call the opencv split function to take
12:38 - that multi-channel image and split it
12:40 - into its components b g and r and so
12:43 - each of these uh variables represent a
12:45 - 2d numpy array that contain the pixel
12:48 - intensities for those color channels
12:51 - so in this next section of code here
12:53 - we're simply going to use um
12:55 - i am show to display
12:57 - each of those representations as a
13:00 - grayscale map
13:01 - and then this last bit of code here
13:03 - takes those individual channels and uses
13:05 - the merge function to merge them back
13:07 - into
13:08 - what should be the original image and
13:10 - we'll call that image merged here and
13:12 - we'll show that as well
13:14 - so now taking a look at the images below
13:16 - we've got the red green and blue
13:19 - grayscale representation of each of the
13:21 - channels and then the merged output over
13:24 - here to the right
13:25 - and it's worth a little bit of a mention
13:26 - here but you can get some intuition by
13:29 - just taking a look at the original image
13:31 - so for example this lake is a kind of a
13:34 - turquoise blue if you will it's got some
13:37 - green and blue in it for sure and
13:38 - probably very little red
13:40 - so if you now go back to these channels
13:42 - you can see that the red channel for the
13:44 - portion of the lake is is low meaning
13:46 - there's not much red component in that
13:48 - color so that's why it's darker it's
13:50 - closer to zero
13:51 - and notice that the green and the blue
13:53 - channels are fairly high intensity for
13:54 - their respective colors
13:57 - so that's indicating that the color of
13:59 - that water there has a very little red
14:01 - in it but quite a bit of green and
14:03 - definitely quite a bit of blue so that's
14:05 - kind of the interpretation
14:08 - so next let's scroll down here and talk
14:10 - about another function in opencv
14:12 - called cvt color this allows you to
14:16 - essentially convert between color spaces
14:19 - so
14:20 - the syntax here is you supply a source
14:22 - image
14:23 - and a code indicating the type of
14:26 - conversion you want and the the result
14:28 - will be
14:29 - a different color space
14:31 - so this is easiest to talk about just
14:33 - with an example so we have a very simple
14:35 - example here converting from bgr to rgb
14:39 - so we're calling cvt color and we're
14:41 - passing it the bgr representation of
14:44 - that image that we read in above
14:46 - and we're specifying a code of
14:49 - bgr to rgb so this is simply a flag
14:52 - indicating to opencv that we're
14:54 - supplying it a bgr image and we want to
14:56 - convert it to rgb
14:58 - and so i'm storing that here in this new
15:01 - variable and then i'm going to use
15:02 - i am show to display that and this is
15:05 - what we expect we're just simply
15:06 - displaying the original image but if you
15:08 - look at the documentation for cbt color
15:11 - you'll see that there's all kinds of
15:13 - color codes that allow you to convert
15:14 - between color spaces and so that's the
15:16 - subject of this next section of the
15:18 - notebook here
15:20 - where we are
15:21 - going to convert
15:23 - that image to a different color space so
15:25 - in this first line up here we're going
15:27 - to convert the bgr representation of
15:29 - that image
15:30 - to an hsv representation so hsv stands
15:34 - for hue saturation and value and that's
15:36 - another color space
15:38 - that's often used in image processing
15:40 - and computer vision
15:41 - and so we're going to store that result
15:43 - in a in a variable named image
15:45 - underscore hsv
15:47 - so now i can split those channels just
15:49 - like we did above and get the h s and v
15:52 - components
15:53 - and just like the example above i'm
15:55 - going to plot all four images here the
15:57 - three channels and then the original
15:59 - image
16:00 - for example uh hue
16:02 - represents
16:03 - the color of the image saturation
16:06 - represents the intensity of the color
16:08 - and v represents the value so you can
16:11 - think of saturation as being
16:14 - a pure red versus a dull red
16:17 - and you can think of value as being how
16:19 - light or dark the color is irrespective
16:21 - of the color itself
16:23 - and then hue is more like the um
16:26 - representation of the actual color
16:28 - so
16:30 - just as an example in this next section
16:31 - here we're going to actually modify one
16:33 - of the channels
16:34 - so if you look at this first line of
16:36 - code i'm going to take the um
16:38 - the current q value and add 10 to it so
16:40 - we're just shifting the where we are on
16:42 - the color spectrum and then i'm going to
16:44 - merge
16:45 - that new channel with the original s and
16:47 - v
16:48 - channels and get a merged image and then
16:52 - i'm going to use a cvt color to convert
16:54 - that from hsv to rgb
16:57 - so now i've modified one of the channels
16:59 - i've merged it and now i've converted it
17:01 - and then i'm going to
17:03 - use i am show below
17:05 - to display each of the color channels
17:07 - and then the modified
17:09 - merged image
17:12 - so you can see here that the modified
17:13 - image because we've changed the hue
17:16 - looks different from the original image
17:17 - up here
17:18 - so that's a brief introduction to color
17:21 - channels and color spaces and now we're
17:23 - ready to move on with final section of
17:25 - this notebook
17:26 - having to do with saving images and
17:28 - writing them to disk
17:30 - so there's a function in opencv similar
17:33 - to imread it's called i am right and
17:35 - it's very simple to use you simply pass
17:37 - in the file name that you want to save
17:40 - the file as and then the the image
17:41 - itself
17:43 - and so there's an example here
17:45 - we're going to use the i am write
17:46 - function and we're going to give it a
17:48 - file name new zealand lake underscore
17:50 - saved to indicate that we're actually
17:52 - saving this from our notebook
17:54 - and we're passing it this image that
17:56 - we've been working with here in bgr
17:58 - format
17:59 - and then
18:00 - on the very next line here we're going
18:01 - to use that ipython image function to
18:04 - read that file back in and render it
18:06 - right here in the browser
18:08 - and then the last thing we wanted to
18:09 - conclude with is just taking this image
18:12 - and using i am read to read it in
18:15 - uh both as a color image here and as a
18:18 - grayscaled image here and then print out
18:20 - both of those arrays
18:22 - and notice that the first one read in as
18:24 - a color image has got three channels and
18:26 - the one that's read in his gray scale
18:28 - has got just the single channel
18:30 - so we did cover quite a bit of material
18:31 - in this notebook and we hope it's been a
18:33 - good reference for you there is one
18:35 - other thing that we'd like to discuss
18:36 - but not from within the notebook so
18:38 - we're going to move over to a
18:39 - development environment and spend just a
18:40 - few minutes talking about the
18:42 - differences between matplotlib i am show
18:44 - and opencv item show so we'll continue
18:47 - on there in just a second okay so here
18:49 - we are in the development environment
18:50 - and uh we put this script together uh so
18:53 - that we could demonstrate some of the
18:54 - differences between
18:56 - the matplotlib version of i am show and
18:58 - the opencv version of i am show
19:01 - and we start by just reading in two
19:03 - colored images this first one here is a
19:05 - color checkerboard image and then the
19:07 - second one here is the coca-cola logo
19:10 - and then this very first block of code
19:12 - here is using the matplotlib version
19:14 - of i am show to display the checkerboard
19:17 - image and we've seen examples of that
19:19 - before in the notebook
19:21 - the rest of this code is all centered
19:22 - around using the opencv version of i am
19:25 - show
19:26 - and there's uh some extra code here
19:27 - that's required in order to use this
19:29 - properly so the first thing we do is uh
19:31 - create a named window here
19:34 - and then we call the opencv version of i
19:36 - am show passing at the named window and
19:38 - then the image we want to display in the
19:39 - window
19:41 - and notice right after this we call a
19:42 - wait key function
19:44 - and the argument to that function is the
19:46 - number of milliseconds that this window
19:48 - will be displayed
19:50 - so eight seconds in this case
19:52 - uh if we didn't uh call this weight key
19:54 - function then the im show
19:57 - function would display the window
19:58 - indefinitely and there'd be no way to
20:00 - actually exit the window
20:02 - so the weight key function is meant to
20:03 - be used in conjunction with the i am
20:05 - show function
20:07 - so then we're going to continue on and
20:09 - and perform the same set of actions
20:11 - except with a different image the coke
20:13 - image in this case just so you can see
20:14 - the dynamic behavior
20:16 - and then finally down here in this third
20:18 - section
20:19 - we're going to do the same thing
20:21 - displaying the checkerboard image again
20:22 - but this time we're going to pass
20:25 - a 0 to the weight key function so rather
20:27 - than displaying
20:29 - the image for 8 seconds
20:31 - 0 means it'll be displayed indefinitely
20:34 - unless any key on the keyboard is struck
20:37 - so this allows you to have user input
20:39 - rather than waiting for a specific time
20:41 - to pass
20:43 - and then
20:44 - there's one other option down here that
20:46 - you could use in conjunction with a
20:48 - while loop
20:49 - you could
20:50 - call the im show function inside of a
20:52 - while loop and then monitor the keyboard
20:54 - to only exit the loop if the user types
20:57 - a specific key like for example the
20:59 - lowercase q to quit
21:01 - so let's take a look at how this behaves
21:03 - when we run the demonstration
21:07 - so here we're displaying the image using
21:10 - matplotlib
21:11 - and i can go ahead and exit that window
21:14 - and now this checkerboard image is going
21:16 - to be displayed using the opencv version
21:18 - of i am show but only for eight seconds
21:23 - and then the same with the coca-cola
21:25 - logo and notice that i i'm not able to
21:27 - actually exit the window i have to wait
21:29 - the eight seconds
21:31 - and now we're back to the checkerboard
21:32 - image but in this case
21:34 - we passed a zero to the wait key
21:36 - function so this is just going to be
21:37 - displayed indefinitely until i hit
21:39 - until i hit a key on the keyboard so
21:41 - i'll go ahead and hit the space bar and
21:42 - this will disappear
21:45 - and now we're in the while loop and the
21:47 - coca-cola image is being displayed
21:50 - and if i hit uh the spacebar nothing
21:52 - happens because
21:54 - the weight key function is being
21:56 - monitored
21:58 - for the user to type a lowercase q so
22:00 - lowercase q is going to be the only key
22:02 - that will allow this to actually exit so
22:04 - i'll go ahead and hit the lowercase q
22:06 - and so that's another
22:08 - way for you to use the opencv version of
22:10 - im show
22:11 - so we hope that gives you a better feel
22:13 - for some of the subtleties associated
22:15 - with using the opencv version of i am
22:17 - show
22:18 - and that's all we wanted to cover in
22:19 - this section
22:23 - in this video we're going to be talking
22:24 - about basic techniques for manipulating
22:26 - images which include changing the values
22:28 - of individual pixels within an image as
22:30 - well as some other useful
22:31 - transformations like cropping resizing
22:34 - and flipping images these are all very
22:36 - standard transformations and
22:38 - very easy with the help of opencv
22:40 - functions so let's get started and take
22:42 - a look at the first example below here
22:44 - where we're going to manipulate
22:46 - individual pixels
22:48 - this is the image that we worked with in
22:49 - the first video it's the
22:52 - black and white checkerboard that's 18
22:53 - pixels wide and 18 pixels tall
22:56 - so let's scroll down here a bit further
22:58 - and talk about how to access uh
23:00 - individual pixels within this uh image
23:03 - so let's suppose we wanted to
23:05 - access this very first pixel in the
23:08 - first black box
23:09 - and then this very first white pixel in
23:11 - that box that would correspond to this
23:14 - element up here zero and this first
23:16 - element here 255.
23:18 - so the first thing we need to mention uh
23:20 - with regard to accessing these elements
23:22 - is that numpy's are razor zero based and
23:25 - so if we just draw your attention down
23:26 - here to this code we're going to print
23:28 - out the cell associated with the first
23:30 - column in the first row
23:32 - so that would be the 0 0
23:34 - pixel and that would be this 0 right up
23:37 - here
23:38 - and then this next print statement is
23:40 - going to
23:41 - print out the pixel associated with the
23:44 - first row
23:45 - and the 7th column
23:47 - so that's a 1 two
23:50 - three four five six seven
23:53 - but we give it an index of six because
23:55 - it's zero based so if you uh take a look
23:57 - at the printout you get the zero and
23:59 - 255. so that's printing out the value of
24:01 - this pixel here
24:03 - and this pixel here
24:06 - so now let's scroll down a little bit
24:07 - further and see how we actually modify
24:10 - uh pixels
24:12 - so
24:13 - in this next code block here we're going
24:15 - to make a copy of the image just so we
24:17 - can modify it and still retain the
24:19 - original image for reference
24:22 - so here we're going to modify the value
24:23 - of four specific pixels and indicated by
24:26 - these uh four assignments here
24:28 - from the 2 2
24:30 - entry to the 3-3 entry
24:32 - so that would actually correspond to the
24:34 - um
24:35 - third row and third column
24:38 - and
24:39 - fourth row and fourth column those four
24:41 - pixels right there so we're going to set
24:43 - those to 200. uh this could also be
24:45 - accomplished by um
24:47 - numpy slicing
24:49 - notation so that would just be
24:51 - two to three comma two to three as
24:53 - opposed to setting all four of these
24:55 - assignments but in either case uh if we
24:57 - display the image you can see the
24:59 - modified pixels here
25:01 - uh in the in the matrix and then of
25:03 - course um if you display the image then
25:05 - you can see that these have been set now
25:07 - to a
25:08 - a light gray tone here in the center so
25:11 - it's very simple to modify individual
25:12 - pixels and we just want to give you a
25:15 - brief demonstration of that
25:17 - the next topic that we're going to cover
25:19 - here is cropping
25:21 - so cropping is similar in some sense to
25:24 - what we described above because it
25:25 - involves
25:27 - array indexing so let's take a look at
25:29 - this example here
25:31 - uh we're reading in a color image
25:33 - of this boat in the water and we're
25:35 - specifying of course that this is uh to
25:37 - be read in as a color image here
25:39 - and we're storing that as a bgr format
25:43 - and then the first thing we're going to
25:44 - do is a swap the color channels on that
25:46 - as we've uh indicated before and then
25:49 - display the image using matplotlib i am
25:52 - show so there's the image and now
25:54 - suppose we're interested in cropping out
25:56 - uh the small area around this boat
25:59 - so let's say from rows
26:00 - 200 to 400 and columns 300 to 600 well
26:04 - the way that we would do that is simply
26:06 - uh index into the
26:09 - original array here
26:11 - rows 200 to 400 and columns 300 to 600
26:15 - and then reassign uh those values to
26:17 - this new variable called cropped region
26:20 - and if we plot that cropped region you
26:23 - can see that's what we get
26:24 - so cropping is a very simple and
26:26 - straightforward it's simply indexing
26:28 - into an existing image and extracting
26:32 - uh the region that you're interested in
26:34 - so now let's move on to the next section
26:36 - which is resizing
26:38 - and for this purpose we're going to use
26:40 - the opencv function resize
26:43 - and it takes several arguments so
26:44 - there's two required arguments the first
26:46 - one is the source image and the second
26:48 - required argument
26:50 - is the um desired output size of the
26:53 - image and then there's several optional
26:56 - arguments here and specifically fx and
26:58 - fy which are scale factors which we're
27:00 - going to demonstrate below and then
27:03 - there's this uh
27:04 - interpolation method
27:06 - and we're not going to go into those
27:07 - details but just know that there's
27:09 - several interpolation methods that you
27:11 - can select from
27:13 - for example when you're resizing an
27:15 - image up
27:16 - uh you're having to invent new pixels
27:19 - and therefore there's an interpolation
27:20 - that's required to do that
27:22 - so let's take a look at an example down
27:25 - here in this first method we're going to
27:27 - use the
27:29 - scaling factors fx and fy
27:32 - and so uh here's the call to the resize
27:35 - function so we're passing it the cropped
27:37 - image
27:38 - and then for the second argument since
27:39 - that's required we have to specify it
27:41 - but it's okay if you specify it as none
27:44 - and now that allows us to uh instead use
27:47 - the scaling factors f x and f y
27:50 - in this example we're just going to set
27:52 - those to two so we're going to double
27:54 - the size
27:55 - of this cropped region and you can see
27:58 - that when we display the result that's
28:00 - that's exactly what we get this is now
28:02 - 400 pixels high and 600 pixels wide
28:07 - so now let's talk about another method
28:08 - for resizing images
28:10 - and this method we're going to set a
28:13 - specific width and height for the image
28:15 - so in this case 100 and 200
28:17 - and we're going to create this two
28:19 - dimensional vector here indicating both
28:21 - of those dimensions
28:22 - and pass that as a second argument to
28:24 - the resize function
28:26 - and display the resize cropped region
28:30 - and in this case we get exactly what we
28:32 - asked for 100 wide and 200 high
28:35 - but of course the image has been
28:36 - distorted now because we didn't maintain
28:38 - the original aspect ratio of the image
28:41 - so that leads us to the
28:43 - last method here which is
28:46 - still using this
28:48 - dimension vector here
28:51 - but we're going to start by specifying a
28:52 - width of 100
28:54 - and then calculate the associated
28:56 - desired height while maintaining the
28:58 - aspect ratio so here we're
29:01 - creating this ratio of the desired width
29:03 - to the original width of the image and
29:06 - then using that factor to derive the
29:08 - desired height here
29:10 - and so now when we pass that revised
29:13 - dimension to the resize function you see
29:15 - that we get an image here that 100
29:17 - pixels wide
29:19 - and the appropriate amount high to
29:21 - maintain the the proper relationship
29:23 - which turns out to be about 67 pixels
29:26 - and then further here below we're going
29:28 - to go ahead and write these images to
29:30 - disk and then read them in
29:32 - so this is the cropped region that's
29:34 - been resized by a factor of two
29:36 - and we're going to swap the channels on
29:38 - that here and then write that to disk
29:41 - and save it as this file name here
29:44 - and then read it back in and display it
29:46 - directly in the browser so you can see
29:47 - how large it is here and then we'll also
29:49 - perform the same operation on the
29:51 - cropped region prior to resizing and
29:54 - display that and you can see that it's
29:56 - half the size
29:57 - as the um as the one that we resized
30:00 - so uh that's all there is to resizing
30:03 - it's fairly straightforward and now
30:05 - we're going to move on to image flipping
30:09 - and the function we'll be using is flip
30:11 - and it simply takes the image itself as
30:13 - the first argument and then a flip code
30:15 - which specifies how we want to flip the
30:17 - image and there's three options for that
30:19 - you can flip it horizontally vertically
30:22 - or in both directions
30:24 - and those are simply specified by 1 0
30:27 - and -1
30:28 - and so let's take a look at an example
30:30 - here
30:32 - here we're passing in the original image
30:34 - and we're making three function calls
30:36 - uh corresponding to the three options
30:38 - here for flipping and we're simply
30:39 - displaying them down here below so this
30:41 - first one here has been flipped
30:42 - horizontally this one's been flipped
30:45 - vertically and this one's been flipped
30:47 - around uh both axes
30:49 - and then finally this is the original
30:50 - here
30:51 - so that's all we wanted to cover in this
30:53 - section and
30:54 - thank you and we'll see you next time
30:58 - in this video we're going to describe
30:59 - for you how to annotate images
31:02 - with lines circles rectangles and text
31:05 - uh keep in mind that this also applies
31:07 - to video frames and so that can be very
31:08 - helpful as well
31:10 - i sort of get started we're going to uh
31:12 - read in an image here and we've got it
31:14 - displayed here in the browser and we'll
31:16 - be working with this image for the rest
31:18 - of the notebook but one thing i wanted
31:20 - to point out is that notice that even
31:21 - though this may have been a grayscale
31:23 - image
31:24 - we're actually reading it in as color
31:26 - and we're doing that because we want to
31:28 - demonstrate
31:29 - annotations in color
31:31 - so we'll need to
31:32 - have a color image to work with or a
31:34 - color representation at least
31:37 - and so let's proceed down to the first
31:39 - section here we're going to learn about
31:40 - how to draw a line on an image
31:43 - the opencv
31:44 - functions that allow you to annotate
31:46 - images are all very straightforward
31:48 - and in this case the first argument here
31:50 - is the image itself
31:52 - and then the next two arguments are the
31:54 - first point and last point of the line
31:56 - and then the color so those are the four
31:58 - required arguments and then there's some
32:00 - optional arguments that we'll
32:02 - specifically take a look at as well
32:03 - including thickness and line type
32:06 - so let's scroll down here to the first
32:08 - example so here we're
32:11 - making a copy of the image simply so we
32:13 - can preserve the original image and then
32:15 - annotate a copy so this one's called
32:17 - image line
32:18 - and we're going to draw a line on this
32:20 - image and you can already see it's a
32:22 - yellow line and we're going to draw it
32:24 - from 0.1 to 0.2
32:26 - so that's 200 along the x-axis
32:29 - and 100 along the y-axis that's this
32:32 - point here and then 400 along the x-axis
32:35 - and 100 along the y-axis so that's this
32:38 - point here
32:39 - and then we're going to specify yellow
32:42 - and recall that this has to be in bgr
32:45 - not rgb and that's the reason that the
32:47 - last two channels here are 255 to
32:49 - produce yellow from red and green
32:52 - and then a line thickness of five and
32:54 - then for the line type we're using uh
32:56 - this line underscore aaa which stands
32:58 - for anti-aliased
33:00 - and uh that's usually a good choice it
33:02 - uses semi-transparent pixels and often
33:04 - produces a very smooth and nice looking
33:06 - result
33:08 - so that's the first example the
33:10 - remaining examples would be very similar
33:12 - to this with just some minor variations
33:14 - so let's take a look at how to render a
33:16 - circle on an image
33:18 - in this case we have to specify the
33:20 - center and radius of the circle here
33:22 - but everything else is the same in the
33:24 - argument list
33:25 - so scrolling down to this uh
33:28 - example you see we're going to render a
33:30 - circle here at the coordinate 900 500
33:34 - so that's 900 along the x-axis and 500
33:38 - down and then with a radius of 100
33:41 - so moving on to
33:43 - rectangles
33:46 - again there's four required arguments
33:48 - but in this case uh 0.1 and 0.2
33:51 - refer to the top left corner of the
33:53 - rectangle in the bottom right corner of
33:55 - the rectangle
33:57 - but again all the other arguments are
33:58 - the same
33:59 - and the
34:00 - specifications are all the same so in
34:02 - this case
34:04 - we're going to uh draw a rectangle
34:07 - around this launch tower here
34:09 - the upper left corner of the rectangle
34:10 - is at 500 100 which you can see here and
34:14 - then the lower right is 700 600 which is
34:17 - right here
34:18 - and we're going to specify a different
34:20 - color for that
34:22 - and then finally moving on to text
34:24 - text is a little bit different there's
34:26 - obviously some additional arguments the
34:28 - first argument here is the text string
34:31 - and the next argument is the origin of
34:33 - the text and that refers to the bottom
34:36 - left corner of the text string where
34:38 - that's going to be placed on the image
34:40 - and then the font face you might also
34:43 - think of that as the font style
34:45 - and then font scale is is a floating
34:48 - point number that scales the font size
34:50 - and then again we have some optional
34:51 - arguments here
34:53 - so
34:54 - um
34:55 - just taking a look at this example here
34:58 - we're setting several of the arguments
35:00 - right here so the text string is here
35:02 - we're setting a font scale of 2.3
35:05 - the font face
35:07 - is uh font hersheyplane if you go to the
35:09 - documentation link here you can find out
35:12 - what font faces are available and i just
35:14 - selected this one and then the font
35:16 - color is going to be bright green and
35:18 - the font thickness of two
35:20 - and then i've got the um origin right
35:23 - here for the texturing so that's 200 700
35:25 - so that's just right here at the lower
35:27 - left-hand base of that text string
35:29 - so that's really it annotations using
35:32 - opencv are very straightforward and
35:34 - simple and that's all we wanted to cover
35:36 - in this section and we'll see you next
35:37 - time
35:39 - in this video we're going to be covering
35:41 - several different basic image processing
35:43 - techniques used for both image
35:44 - enhancement as well as upstream
35:46 - pre-processing functions that are often
35:48 - used for many different applications
35:50 - we'll be covering quite a few topics
35:51 - including arithmetic operations
35:53 - thresholding and masking and also
35:55 - bit-wise operations all of these are
35:57 - very fundamental to many computer vision
36:00 - processing pipelines and we've also
36:02 - included a couple of different
36:04 - application examples so you can get a
36:05 - better feel for just how these
36:07 - techniques can be used in practice
36:09 - so with that let's scroll down here and
36:11 - take a look at our first example
36:14 - here we're reading in a color image and
36:16 - displaying it right here within the
36:17 - browser
36:20 - and what we'd like to do is adjust the
36:22 - brightness of this image so let's
36:24 - take a look at how we can do that
36:29 - so in this section here we're displaying
36:31 - the original image here in the center
36:33 - and then we've adjusted the brightness
36:35 - of the image both darker to the left and
36:38 - brighter to the right
36:39 - and so let's take a look at the code
36:40 - that accomplishes that so we start here
36:43 - in the first line by creating this
36:44 - matrix and we use the numpy ones method
36:47 - to do that and we're going to pass in
36:49 - the shape of the original image here and
36:51 - then also the data type which is
36:52 - unsigned 8-bit
36:54 - and we're going to multiply that by 50.
36:56 - so now the result of that is a matrix
36:59 - that's the same size as the original
37:01 - image and it's got pixel intensities of
37:04 - 50 everywhere in the image and so now
37:06 - we're simply going to use the opencv add
37:08 - and subtract functions to uh add and
37:11 - subtract that matrix
37:13 - from the original image and then we're
37:15 - simply going to display those so that's
37:16 - all that's required to generate an image
37:20 - that's darker than the original and an
37:21 - image that's lighter than the original
37:23 - so now let's talk about how to change
37:25 - the contrast in an image and that's a
37:28 - little bit different because contrast is
37:30 - defined as the difference in intensity
37:32 - values of the pixels within an image and
37:35 - so that's going to require a
37:36 - multiplication operation
37:38 - so we start here by creating two
37:39 - matrices and each of these is going to
37:41 - use the numpy ones function and create a
37:44 - matrix the same size as the original
37:46 - image and in both cases we're going to
37:48 - multiply those matrices by a factor so
37:51 - in this first case it's a factor of 0.8
37:54 - and in the next case it's a factor of
37:55 - 1.2
37:57 - so now these matrices contain floating
37:59 - point values that have been scaled by
38:01 - these two factors here
38:02 - and on these next two lines we're going
38:04 - to multiply those matrices by the
38:06 - original image but note that there's a
38:08 - there's a nesting here that's required
38:10 - so the two matrices we defined above
38:12 - contain floating point values so in
38:14 - order to multiply those by the original
38:16 - image which was unsigned int we're going
38:18 - to first convert those to float and then
38:20 - do the multiplication here using the
38:23 - opencv multiply function
38:25 - and then after that we're going to
38:26 - convert those back to unsigned in 8-bit
38:29 - and so now we have those results here in
38:31 - these two variables and we're simply
38:34 - going to display those and you can see
38:35 - the results below
38:37 - the original image is in the center here
38:39 - the lower contrast image to the left and
38:41 - the higher contrast image to the right
38:44 - so you'll notice here that in the uh on
38:46 - the right hand image there is this um
38:49 - odd color coding in here something's
38:50 - gone wrong
38:51 - and the reason for that is because when
38:53 - we multiply the original image by this
38:55 - matrix it has a factor of 1.2 in it
38:58 - we potentially get values that are
39:00 - greater than 255. so if you look at the
39:02 - original image here these clouds here
39:04 - were probably um close to 255 some of
39:07 - them at least and when we multiplied by
39:09 - 1.2 we exceeded 255
39:12 - so then when we attempt to convert those
39:15 - values to an unsigned 8-bit number
39:18 - uh rather than exceeding 255 they just
39:20 - roll over to some small number and
39:23 - that's why these intensity values are
39:25 - now close to zero and so that's the
39:27 - reason for the issue here so let's take
39:28 - a look at how do we remedy that
39:30 - so if we scroll down to this next
39:32 - section
39:33 - and take a look at that line of code
39:34 - here what we can do is use the numpy
39:36 - clip function
39:38 - to first uh
39:39 - clip those values to the range 0 to 255
39:42 - before converting them to unsigned 8-bit
39:45 - and now when you look at the right-hand
39:46 - image
39:48 - it looks fine and in fact this portion
39:51 - of the image here
39:52 - has been completely saturated so some of
39:55 - these values here are right at 255 so
39:57 - they have really no information they're
39:59 - the extreme highlights within the image
40:01 - so that's a summary of brightness
40:02 - adjustment and also contrast adjustment
40:05 - so let's continue on to the next section
40:07 - of the notebook which covers image
40:09 - thresholding
40:10 - thresholding is a very important
40:11 - technique that is often used to create
40:13 - binary images that allow you to
40:15 - selectively modify portions of an image
40:17 - while leaving other portions intact and
40:20 - we have a couple of examples below to
40:22 - demonstrate this but first i just wanted
40:24 - to point out a few notes here in the
40:25 - documentation i noticed that we're
40:27 - specifying two different functions here
40:30 - threshold and adaptive threshold so just
40:32 - taking a look at the threshold function
40:34 - here to see how this works
40:36 - it takes as input a source image and
40:38 - then a threshold value between 0 and 255
40:41 - and then a max value for the binary map
40:44 - and then a type of thresholding that
40:46 - we're going to perform
40:48 - and in all of our examples below we're
40:50 - going to be using a binary threshold
40:52 - so the idea here is that whatever
40:54 - threshold you specify
40:56 - pixels in the original image that are
40:58 - below this threshold will be set to zero
41:00 - and pixels that are above that threshold
41:02 - will be set to 255.
41:04 - and so the result will be a binary map
41:06 - that contains either zeros or intensity
41:08 - values of 255 or whatever you had set
41:11 - the max value here to which is typically
41:13 - 255.
41:15 - so let's take a look at the adaptive
41:16 - threshold function it also takes a
41:18 - source image a maximum value for the
41:21 - binary map and then a method type to
41:24 - perform the adaptive thresholding
41:26 - and also a threshold type which is the
41:28 - same type of input as we had up here in
41:31 - the first function
41:32 - and then also a block size and a
41:34 - constant value here both of these are
41:36 - used by the adaptive method algorithm
41:40 - basically the block size is an
41:42 - indication of the pixel area that's
41:43 - considered when computing the adaptive
41:46 - threshold spatially across the image
41:48 - so there's a lot of detail in here and
41:50 - we simply wanted to include this for
41:52 - reference
41:53 - and then to show you some examples below
41:55 - so let's take a look at the first
41:57 - example here
41:59 - so here we're reading in an image which
42:01 - is a photograph of a building with lots
42:03 - of windows and a geometric structure
42:06 - and we're going to call the threshold
42:08 - function and pass it to that image and
42:11 - then give it a threshold of 100 with a
42:13 - max value of 255 and then specify
42:17 - this flag here to indicate that we want
42:18 - a binary map and what's returned from
42:21 - this function
42:22 - is the binary image this return value
42:25 - here is not important at this point so
42:28 - it's this argument here that contains
42:30 - the actual binary
42:32 - map and so we're simply going to display
42:34 - that below
42:35 - both the original and the thresholded
42:38 - image
42:38 - and you can see that there's an
42:39 - opportunity here for you to
42:42 - use this map as a way to selectively
42:45 - process certain parts of the image
42:48 - so let's take a look at a more concrete
42:49 - example down here below
42:54 - suppose you were interested in building
42:55 - an application that could
42:57 - read and decode sheet music
43:00 - which is very similar to optical
43:01 - character recognition where the goal is
43:03 - to recognize characters in text
43:06 - documents in this case you'd be trying
43:08 - to recognize musical notes for the
43:10 - purpose of digitizing that information
43:13 - so in the example below it's easiest to
43:15 - actually start off with talking about
43:17 - these two images here and then we'll
43:18 - come back up and and talk about the code
43:21 - so over here on the left is the original
43:23 - uh photograph of some sheet music and
43:25 - you can see that it's kind of dark here
43:27 - in the lower right hand corner
43:28 - clearly not a white background
43:31 - but the notes are fairly well defined
43:32 - they're all very dark black which looks
43:34 - good
43:35 - and the idea here is that we'd like to
43:37 - perform thresholding on this image
43:39 - to achieve a binary map
43:42 - similar to the one shown here to the
43:43 - right
43:44 - so just taking a look at the dark values
43:47 - of all these notes in the musical
43:49 - notation
43:50 - it looks like perhaps the intensity
43:52 - values of all these black areas might be
43:54 - below 50 for example
43:56 - they all look pretty black
43:58 - even some of these up in here look
44:00 - fairly black so if we
44:02 - create a binary map with a threshold of
44:04 - 50
44:05 - we're hoping that we would be able to
44:07 - isolate all this important information
44:10 - and the image to the right here was
44:12 - actually produced with a threshold of 50
44:14 - and the result is rather surprising
44:16 - because notice that there's no
44:18 - information up here in the top portion
44:20 - of the image
44:21 - that would mean that the intensity
44:22 - values of all these black notes are
44:24 - actually above 50
44:26 - which isn't very intuitive because they
44:28 - look rather dark
44:30 - so that's just one example so let's go
44:32 - back up here and take a look at the code
44:33 - that produces these plots so here we're
44:36 - just reading in the um the original
44:38 - image here and then we're going to call
44:40 - the threshold function passing at the
44:42 - original image a threshold of 50 a max
44:45 - value of 255 and then a flag here
44:48 - indicating that we want to produce a
44:49 - binary map
44:50 - and so what we get back here is the
44:52 - thresholded image and this is what is
44:54 - actually displayed down here to the
44:56 - right
44:57 - but there's some additional code here
44:59 - so there's another thresholding call
45:01 - here and this time we're going to pass
45:02 - it a higher threshold of 130
45:05 - with the hopes that we can
45:07 - extract more of this information up here
45:09 - in the top of the image and then finally
45:11 - we're going to call the adaptive
45:12 - thresholding function
45:14 - and
45:15 - specify the type of thresholding
45:17 - algorithm and the fact that we want to
45:19 - create a binary map and then some
45:21 - settings here for the algorithm
45:23 - and and then we're going to display all
45:25 - four of these uh below so you can see
45:27 - the first two uh here that we've already
45:29 - talked about but let's scroll down here
45:30 - and see the next two
45:34 - so you can see that the one at the lower
45:35 - left here that was produced with the
45:38 - global threshold of 130
45:40 - did a better job of isolating
45:42 - the musical notes here in the top
45:44 - portion of the page but
45:46 - that threshold was far too high in order
45:49 - to accommodate the lower portion of the
45:51 - page so what's going on there is that
45:53 - these dark values on the page this
45:55 - shadow essentially is actually
45:58 - lower than 130. so as a result this this
46:00 - whole portion of the page is just
46:02 - blacked out so neither of these global
46:04 - thresholds 50 or 130
46:06 - do a very good job and you could
46:08 - actually experiment with other
46:09 - thresholds and find out that there's not
46:11 - going to be a single global threshold
46:14 - that's going to do
46:15 - very well in this situation so notice
46:17 - that the plot at the lower right here
46:19 - that was produced using adaptive
46:21 - thresholding is much better
46:24 - this is a very good example of how you
46:26 - can take an image that's
46:28 - pretty challenging and has several dark
46:30 - areas here and actually isolate
46:33 - just about everything you want to in the
46:35 - image
46:36 - so we simply wanted to point out the
46:37 - importance of thresholding and in
46:39 - particular adaptive thresholding and so
46:42 - now let's move on to the next section of
46:43 - the notebook
46:45 - which covers bitwise operations
46:48 - so here in the documentation section you
46:49 - can see that we have four different
46:51 - functions bitwise and or xor and not and
46:56 - here we're showing an example of the
46:57 - bitwise and
46:59 - it takes two input images these can
47:01 - actually be the same image
47:04 - but they don't have to be and then it
47:06 - takes an optional mask
47:08 - and the mass specifies which portion of
47:10 - these two images the logical operation
47:12 - applies to
47:14 - so let's take a look at this uh example
47:16 - down here we're reading in uh two
47:18 - different images these are both
47:20 - grayscale
47:21 - binary images that we're going to
47:23 - perform these operations on
47:25 - and
47:26 - let's just see how that works so we'll
47:28 - start with the
47:30 - bitwise and operator here
47:33 - so in this case we're
47:35 - passing in the image of the rectangle
47:37 - and the image of the circle
47:39 - and then we're indicating the mask is
47:40 - none so we're simply going to do a bit
47:42 - wise and comparison between these two
47:45 - images
47:46 - and the value returned from that
47:47 - comparison
47:49 - will be
47:50 - 255 or white if the corresponding pixels
47:53 - in both images are white
47:55 - so in this case the result will be just
47:57 - this left side of this half circle since
47:59 - that's the only region in both images
48:01 - where the pixels are white
48:03 - so now let's take a look at the bitwise
48:05 - or um
48:07 - operation in this case the return value
48:09 - from the operation will be white if the
48:12 - corresponding pixel from either image is
48:14 - white and so in that case we get the
48:16 - entire
48:17 - left side of the rectangle which is
48:18 - white and then the
48:20 - right hand side of the circle
48:23 - so now let's take a look at the xor
48:25 - operator
48:26 - and in this case we're passing at the
48:28 - same set of arguments the operation is
48:31 - simply different
48:32 - and the exclusive or works as follows
48:34 - it'll only return a value of white
48:38 - if uh either corresponding pixel is
48:41 - white but not both
48:43 - so this is the result that you get here
48:46 - so that's a summary of those three
48:48 - functions so now let's take a look at an
48:50 - application
48:51 - using um
48:54 - bitwise operations and
48:56 - binary maps
48:58 - so in this example here we're interested
49:00 - in manipulating this coca-cola logo
49:03 - and we're going to start with the logo
49:05 - and also
49:06 - what we call a background image here
49:08 - this colorful checkerboard
49:10 - and we'd like to
49:11 - achieve this result to the right so
49:13 - essentially being able to display a
49:15 - background image behind the white
49:18 - lettering and have it show through
49:20 - so that's the goal and we're going to go
49:21 - ahead and proceed through this notebook
49:24 - to see how that's done
49:25 - so first down here we're going to read
49:27 - in what we call the foreground image
49:28 - which is the coca-cola logo itself
49:31 - and we're going to display that in the
49:33 - browser here
49:34 - and then
49:35 - further down here we're going to do the
49:37 - same thing with the background image
49:39 - in this case there's a little bit of
49:41 - extra code up here in order to make sure
49:43 - that that image is the exact same size
49:46 - as the coca-cola logo so as we've seen
49:49 - before in a previous video
49:51 - we're making use of the opencv resize
49:53 - function here
49:55 - to accomplish that
49:56 - so now we're going to go ahead and
49:59 - create a couple of masks
50:01 - from the coca-cola
50:03 - logo so in this top portion here we're
50:05 - going to
50:06 - pass in the logo here to cvt color
50:08 - convert it to gray
50:10 - and then
50:11 - use the opencv threshold function
50:14 - to create a binary mask
50:16 - from this grayscale image and we're
50:18 - going to call that image underscore mask
50:20 - and we're displaying that down here so
50:22 - this is only going to contain values of
50:23 - 0 and 255.
50:26 - and then we're going to perform a
50:27 - similar operation down here
50:30 - but not using the threshold function
50:32 - although we could have we could have
50:33 - used the threshold function down here
50:35 - and specified a threshold binary inverse
50:39 - mask
50:40 - but
50:41 - instead we can just simply call the
50:43 - bitwise not function on the image mask
50:46 - to return the inverse mask and so you
50:49 - see both of these masks displayed here
50:50 - in the browser now we're going to make
50:52 - use of those
50:53 - down here below
50:56 - so now in this section
50:59 - we're going to do a bit wise and
51:01 - on the background image with itself but
51:03 - using the image mask
51:05 - so this bitwise and operation is going
51:08 - to perform a bit wise in between the
51:10 - corresponding pixels between these two
51:12 - images which is the same image but it's
51:14 - only going to apply it
51:17 - to the mask
51:19 - which is the white lettering in this
51:21 - case and so that's the result we get
51:23 - everything else is going to be zero
51:25 - and we're going to get just the colors
51:27 - showing through in the logo
51:30 - and then we need to do a similar
51:31 - operation on on the um
51:34 - on the logo itself which is image
51:36 - underscore rgb
51:38 - and we're going to do a bit wise and
51:39 - operation on
51:41 - on that and pass it the inverse mask
51:44 - and that's going to allow us to only
51:46 - show the
51:47 - the red foreground
51:49 - and everything else is going to be
51:51 - black
51:52 - and so now you can see that if you just
51:54 - added these two images together the
51:56 - blacks sum to zero and what you get is
52:01 - the following result
52:03 - so we thought that was an interesting
52:05 - way to demonstrate how you could use
52:08 - binary maps
52:09 - and thresholding and logical operations
52:12 - to accomplish something like this
52:15 - so we covered a lot of material in this
52:16 - video and just keep in mind that all of
52:18 - this is very fundamental to many
52:20 - different image processing and computer
52:21 - vision pipelines and we'll continue on
52:24 - in this course
52:25 - with a little more focus on actual
52:27 - applications uh now that we have some of
52:29 - this basic material under our belts so
52:32 - thank you and we'll see you next time
52:37 - in this video we're going to be
52:38 - describing for you how to access the
52:40 - camera attached to your computer system
52:42 - and send that streaming video to an
52:44 - output window on your display
52:46 - so we have a short script here that
52:48 - accomplishes that and we'll walk through
52:50 - this and then we'll go ahead and execute
52:52 - it so you can see it in action
52:54 - so starting on lines 35 and 36 we're
52:57 - importing opencv and the systems module
53:00 - both of which are required below
53:02 - and then on line 38 we're specifying a
53:04 - default camera device index of zero
53:08 - on line 39 and 40 we're simply checking
53:10 - to see if there was a command line
53:12 - specification to override that default
53:15 - value
53:16 - but in this case we're just going to use
53:17 - zero
53:19 - and then on line 42 we're going to call
53:21 - the video capture class to create a
53:23 - video capture object and we pass in
53:26 - device index into that class
53:28 - so device index of zero will
53:31 - access the default camera on your system
53:34 - if you had more than one camera attached
53:35 - to your system then you would need to
53:37 - indicate a device index
53:39 - that points to the correct one so zero
53:41 - would be the default one would be the
53:43 - second camera two would be the third
53:44 - camera and so forth
53:46 - on line 44 and 45 we're creating a named
53:50 - window which we're going to eventually
53:51 - send the streamed output to
53:53 - and then finally on line
53:55 - going to create a while loop
53:57 - and this while loop is going to allow us
53:59 - to continuously stream video from the
54:01 - camera and send it to the output
54:03 - unless the user hits the escape key so
54:06 - that's what this weight key function
54:07 - does it continuously checks whether or
54:09 - not the users hit the escape key
54:12 - so the first line in this loop line 48
54:15 - uses that video capture object source to
54:18 - call the read method in that class and
54:21 - that read method will return a single
54:23 - frame from the video stream as well as a
54:26 - logical variable uh has underscore frame
54:29 - so if there's any kind of a problem with
54:31 - reading the video stream or accessing
54:33 - the camera then has frame would be
54:36 - false
54:37 - and
54:38 - we would break from the loop otherwise
54:40 - we'd continue on
54:42 - and call the im show function in opencv
54:45 - to actually send the video frame to the
54:47 - output window so that's all there is to
54:50 - it there's not much code we just wanted
54:52 - to walk through that and give you an
54:53 - example of how to do this so let's go
54:55 - ahead and execute it
54:59 - and there it is here's the window
55:01 - streaming video from my web camera right
55:03 - to the display as soon as i hit the
55:05 - escape key this is going to uh exit
55:08 - and uh that's all we really wanted to
55:09 - cover in this video in the next video
55:12 - we're going to uh build on this little
55:14 - bit and do some processing
55:16 - of the video frame from the camera and
55:19 - then send the post-processed output to
55:21 - the display so that's going to be a
55:22 - little bit more interesting to take a
55:24 - look at so that's it for now and we'll
55:26 - see you next time
55:29 - in this section we're going to describe
55:31 - for you how you can save videos to disc
55:34 - in a previous video we described how you
55:36 - can send the streaming output from your
55:37 - webcam to the output display but here we
55:40 - wanted to cover how you can actually
55:41 - write that to disk
55:43 - so we're going to start by specifying a
55:46 - source here this could have been the
55:47 - webcam
55:48 - but here in this example we're going to
55:50 - specify a file from disk and read it in
55:53 - by creating a video capture object here
55:56 - and so here we're specifying uh as the
55:58 - input argument to this the source for
56:00 - the video file so this is our video
56:02 - capture object here
56:04 - and in this next section we're just
56:06 - going to simply check that that was
56:07 - successful
56:08 - and then scrolling down here a little
56:10 - bit we're going to use the read method
56:13 - from that class to retrieve the first
56:16 - frame of the video
56:17 - and then use uh
56:19 - i am show to display that in the browser
56:21 - so that's the first frame of the video
56:23 - and we can actually load the video here
56:25 - in the browser as well
56:27 - with this command here and we've already
56:28 - executed that so i'll go ahead and play
56:30 - it just so you can see it it's just a
56:31 - very short clip of a race car
56:39 - okay so then down here we're going to
56:41 - talk a little bit about
56:42 - the video writer
56:44 - function in opencv
56:46 - so this um this function allows you to
56:50 - create a video file on disk and it takes
56:52 - his argument the file name
56:54 - and then this 4cc argument and that
56:57 - stands for four character code
56:59 - that describes the codec that's used to
57:01 - compress the video frames
57:03 - and then uh there's also the frames per
57:05 - second and then the frame size
57:07 - now the frame size is important because
57:09 - um that needs to be the dimensions of
57:11 - the frames that you have in memory that
57:13 - you want to write to disk
57:15 - so we'll see an example of that below
57:18 - so let's take a look at this next code
57:20 - section here the very first thing we're
57:21 - going to do is use the video capture
57:23 - object
57:25 - to call this get method which is going
57:26 - to retrieve for us the dimensions of the
57:30 - video frame that we have in memory
57:32 - and then now we're going to create two
57:34 - video writer objects one for an avi
57:37 - format and one for an mp4 format
57:41 - so you can see here in both cases we're
57:42 - specifying the file name
57:44 - and then the 4cc codec and to do that
57:47 - we're going to use this video writer
57:49 - underscore 4cc function here
57:52 - and for an avi file you need to specify
57:54 - these specific arguments and for an mp4
57:57 - file you need to specify it just like
57:59 - this
58:00 - so you can take a look at the
58:01 - documentation and read up on this but
58:04 - this is sort of the tricky thing
58:06 - associated with writing video files
58:09 - getting this codec right
58:11 - and then also making sure that the frame
58:12 - dimensions
58:14 - match the frame size that you're trying
58:16 - to write to disk
58:19 - so let's now take a look at how we
58:20 - actually do this
58:22 - we're going to create a while loop here
58:24 - and
58:25 - we're going to use the read method from
58:27 - this video capture object to read every
58:29 - frame from the video file
58:31 - and then we're simply going to pass that
58:32 - frame back out to those video writer
58:35 - objects that we just created up above
58:36 - one for the avi format and one for the
58:39 - mp4 format
58:41 - and then when that's done processing
58:43 - we're simply going to release the
58:44 - resources down here
58:47 - so that's all there is to it but we did
58:49 - want to point out that
58:51 - getting these codecs correct and making
58:53 - sure that the
58:55 - frame dimensions match the frame
58:57 - dimensions of the video frames that you
58:58 - have in memory are the two key things
59:00 - that you need to watch out for so that's
59:03 - all we wanted to cover in this section
59:05 - and uh thanks so much and we'll see you
59:06 - next time
59:09 - in this video we'll be demonstrating
59:10 - some of the more common image processing
59:12 - techniques that are often used in
59:14 - computer vision pipelines and to do this
59:16 - we're going to build on the camera
59:18 - demonstration from the last video where
59:19 - we sent streaming video from the camera
59:21 - to an output window in the display
59:24 - however this time we'll be doing some
59:25 - image processing on the video frames
59:27 - first and then send those results to the
59:29 - output display so we hope you find this
59:31 - informative and to get started we'll
59:33 - first take a look at the code and then
59:35 - execute it so we can talk more in depth
59:36 - about the various processing techniques
59:39 - and their associated parameter settings
59:41 - so starting on line 39 we're defining
59:44 - the four different run modes for the
59:46 - script which include a preview mode a
59:48 - blurring filter a corner feature
59:50 - detector and a canny edge detector
59:53 - then on line 44 we're defining a small
59:55 - dictionary of parameter settings for the
59:58 - corner feature detector and those
59:59 - include
60:00 - the maximum number of corners that the
60:03 - algorithm will return
60:05 - quality level is a parameter for
60:07 - characterizing the minimum acceptable
60:08 - quality of image corners
60:11 - and the way that works is that the
60:12 - corner feature with the highest value in
60:14 - the entire image is multiplied by this
60:16 - parameter and then that value is used as
60:18 - a minimum threshold for filtering corner
60:20 - features from the final list that's
60:22 - returned by the algorithm
60:24 - so for example if you had several
60:26 - features that were detected and the
60:28 - maximum value of those features was 100
60:31 - then we'd multiply 100 by 0.2 which
60:33 - would be 20 and then 20 would be the
60:35 - threshold for determining whether or not
60:38 - a a feature corner was detected
60:41 - and then the the next parameter here the
60:43 - minimum distance this is the minimum
60:45 - distance between adjacent uh feature
60:47 - corners and this is measured in pixel
60:49 - space so it's the euclidean distance in
60:50 - pixel space
60:52 - which describes how close two corner
60:54 - features can be in the list that's
60:57 - returned from the algorithm and then
60:59 - finally block size is the size of the
61:01 - pixel neighborhood that is used in the
61:03 - algorithm for computing uh the feature
61:05 - corners
61:06 - so this next block of code started on
61:07 - line 48 is very similar to the code in
61:10 - the previous video where we're setting
61:12 - the device index for the camera
61:14 - creating an output window for the uh
61:17 - streamed results and then creating a
61:19 - video capture object so that we can
61:21 - process the video stream in the loop
61:23 - below so here on line 61 we enter a
61:27 - while loop and uh first line in that
61:29 - loop is to
61:31 - read a frame from the video stream
61:33 - and then on line 66 i'm going to
61:37 - flip that frame horizontally mainly as a
61:40 - convenience for myself so that it's
61:41 - easier for me to point things out in the
61:44 - video stream
61:45 - and then here on line 68 depending on
61:48 - the run configuration for the script
61:51 - we'll be executing uh one of these
61:53 - functions in opencv
61:55 - of course if we're in preview mode we're
61:57 - simply going to take the frame and set
61:58 - that to the result and then display that
62:00 - directly
62:01 - to the output window using im show
62:04 - but for these other run modes we're
62:06 - going to do some processing first and
62:07 - then send the processed results to the
62:09 - output window starting on line 71 uh
62:12 - here we're calling the canny edge
62:13 - detection function in opencv
62:16 - and the first argument there is the
62:18 - image frame and then there's two
62:19 - additional arguments a lower threshold
62:21 - and an upper threshold the upper
62:23 - threshold is used for deciding whether
62:25 - or not a series of pixels should be
62:26 - considered as an edge so if the
62:28 - intensity gradient of those pixels
62:30 - exceeds the upper threshold then we'll
62:31 - declare those pixels as constituting a
62:34 - sure edge and likewise for pixels whose
62:37 - intensity gradients are below the lower
62:38 - threshold and those segments will be
62:40 - completely discarded
62:42 - however for the pixels whose gradients
62:44 - fall in between these two thresholds
62:45 - we'll consider those as candidate edges
62:47 - if they can be associated with a nearby
62:49 - segment that has already been declared
62:52 - as an edge so in other words we're
62:53 - allowing for weaker edges to be
62:55 - connected to stronger ones if the weaker
62:57 - edges are likely to be along the same
62:59 - true edge and we'll see an example of
63:01 - that when we run the demo for edge
63:02 - detection uh in just a little bit
63:05 - then the next function here is a blur
63:08 - function in opencv and this blur
63:11 - function uses a box filter to blur the
63:14 - image so the first input to this
63:16 - function is the image frame itself and
63:18 - then this second input are the
63:20 - dimensions for the box kernel so this
63:22 - would be a 13 by 13 box kernel that
63:25 - would be convolved with the image to
63:27 - result in a
63:28 - blurred image so if the size of the
63:30 - kernel is smaller than the blurring is
63:32 - less and if the size of the kernel is
63:33 - larger then you get more substantial
63:35 - blurring
63:37 - and then finally for the corner feature
63:39 - detector converting the frame uh the
63:41 - image frame to a grayscale image
63:44 - and then on line 77 we're going to call
63:47 - the function good features to track
63:49 - and although it isn't indicated in the
63:51 - name of this function what this function
63:53 - does is compute corner features so the
63:56 - first argument is a grayscale image of
63:58 - the
63:58 - video frame and then the second argument
64:01 - is that dictionary of feature parameters
64:03 - that we described up above
64:06 - and so what that returns is a list of
64:09 - corners that were detected in the image
64:11 - and if we have
64:13 - one or more corners detected then we're
64:14 - going to simply annotate
64:16 - the result with small green circles to
64:19 - indicate the locations of those features
64:22 - and then after we're done with all this
64:24 - whatever result we have under whatever
64:26 - run mode we've been working with we're
64:27 - going to send that to the output stream
64:30 - so this next block of code here is
64:32 - simply monitoring the keyboard for user
64:34 - input
64:35 - the script was written so that the run
64:37 - modes could be toggled interactively so
64:39 - for example if you were running in
64:41 - preview mode and you wanted to
64:43 - transition to candy detection mode you
64:45 - would simply type a c on the keyboard so
64:47 - that's all there is there really isn't
64:48 - very much code required to implement
64:50 - this and at this point we're ready to go
64:52 - ahead and execute the script and we'll
64:54 - cycle through the different run options
64:57 - and talk a little bit about the
64:59 - results that we see so this is the
65:01 - preview mode and here we're simply
65:03 - sending the video stream from the camera
65:05 - to the output window and the display so
65:08 - what i'd like to do next is toggle
65:10 - through the other filters that we
65:11 - implemented and we'll start with the
65:12 - blurring filter so i'm going to type a b
65:14 - on the keyboard
65:15 - and you can see that the image has been
65:17 - slightly blurred
65:18 - there's a few reasons you might want to
65:20 - do this for example if you had a noisy
65:22 - image you could apply a small amount of
65:24 - blurring and still obtain an
65:25 - aesthetically pleasing result
65:27 - but more importantly in computer vision
65:29 - and image processing we often use
65:31 - blurring as a pre-processing step to
65:34 - performing feature extraction and the
65:36 - reason for that is that most feature
65:38 - extraction algorithms
65:40 - i use some kind of numerical gradient
65:42 - computation and performing numerical
65:45 - gradients on raw pixel data can be a
65:47 - rather noisy and
65:49 - not well behaved process
65:51 - so
65:52 - smoothing the image prior to performing
65:54 - gradients uh turns out to be much more
65:57 - robust and well-behaved
65:59 - and so that's uh one of the primary
66:00 - reasons we use blurring in computer
66:03 - vision so now let's take a look at the
66:06 - next option which is the corner feature
66:08 - detector so now i've turned that mode on
66:12 - and you can see a small amount of corner
66:15 - features in the image there's some here
66:17 - on the microphone
66:18 - there's a few around my face here and in
66:20 - particular there's uh several there in
66:22 - the painting of the horses behind me
66:26 - and we're going to talk a little bit
66:27 - about
66:28 - how
66:29 - these features are generated based on
66:32 - the input arguments
66:34 - that we selected and there were two
66:35 - input arguments that we talked about in
66:36 - particular one was a minimum
66:39 - distance between features which is
66:41 - fairly straightforward and the other one
66:43 - was a
66:44 - quality level of the features so first
66:47 - we'll talk about
66:48 - the uh minimum distance so here i've got
66:50 - a textbook with uh some where very well
66:53 - defined characters on the front that
66:54 - have nice sharp edges and we're
66:55 - detecting all kinds of corners in those
66:57 - characters
66:58 - you can see that um
67:00 - each of those letters probably has two
67:02 - maybe three
67:04 - uh corner features for each character
67:06 - but when i move the book much closer to
67:08 - the camera you'll see now that um there
67:11 - are several more corner features
67:12 - associated with each character and the
67:14 - reason for that is that those letters
67:16 - are much larger in pixel space
67:18 - so now i'm not constrained by the
67:19 - minimum distance between
67:22 - between the features because i've
67:24 - made the letters so much larger in pixel
67:26 - space and then one other thing i'd like
67:29 - to talk about is
67:32 - i've drawn your attention to this
67:33 - section of the book here with this
67:34 - graphic image on it if i put this very
67:37 - close to the camera
67:39 - we're going to see that
67:40 - we detect all kinds of corners
67:44 - especially with the dots and that
67:46 - pattern of the book there so the reason
67:48 - those are jumping around so much is that
67:49 - i'm having a hard time holding the book
67:51 - really still but the main point is that
67:52 - i i'm getting all these detections here
67:54 - now watch what happens when i lower the
67:56 - book
67:57 - and expose
67:59 - the text from the title of the book as
68:02 - soon as i expose the text from the title
68:04 - all the features associated with the
68:06 - graphic image below have been filtered
68:07 - out so let's take a look at that again i
68:09 - raised the book
68:10 - and i get all these features here and
68:13 - now when i lower the book and expose
68:16 - the title of the book
68:18 - all those have been filtered out and the
68:19 - reason for that is that that
68:21 - quality level threshold we talked about
68:24 - is based on the
68:26 - highest score for a corner feature in
68:28 - the entire image and because
68:30 - the corner features associated with
68:32 - these characters in the title of the
68:33 - book
68:34 - are so much stronger their feature score
68:36 - is higher and therefore we're
68:37 - effectively raising
68:39 - the detection threshold for corner
68:42 - features
68:43 - so i just thought that was an
68:44 - interesting
68:45 - example of
68:47 - how that
68:48 - parameter is actually used and how it
68:50 - can affect um
68:53 - uh the algorithm uh in your particular
68:55 - application
68:56 - so uh uh finally let's uh go ahead and
68:59 - cycle to the um
69:01 - canny edge detection so
69:04 - uh toggle to that mode and so now you
69:06 - can see the results of edge detection
69:08 - here you can see the microphone is very
69:10 - well defined
69:12 - the corner of my shoulder against the
69:13 - light background of the wall is very
69:15 - well defined
69:16 - and then
69:17 - up here uh
69:19 - behind my shoulder you see a painting of
69:21 - some horses and
69:23 - the subject matter in that painting is
69:25 - is a partially defined but there's a lot
69:27 - of broken edges
69:29 - in that painting and so i thought it'd
69:30 - be interesting to
69:32 - talk about the threshold inputs for the
69:34 - canny edge detector and see if we can
69:36 - improve what that looks like
69:38 - so before we do that i'm going to make a
69:40 - screen snap of this
69:42 - video feed just so we can have something
69:44 - to compare to so i'll put this aside and
69:46 - now i'm going to
69:49 - edit the threshold for the canny edge
69:52 - detector so
69:54 - previously the lower threshold was very
69:55 - close to the upper threshold so there
69:57 - wasn't much opportunity for us to
69:59 - find some weaker edges and connect them
70:01 - to stronger edges but if i lower this to
70:03 - something like 80
70:05 - we now have an opportunity to
70:08 - consider weaker edges that might be
70:10 - associated with the stronger edges so
70:12 - i'm going to go ahead and run this
70:14 - and now we'll put these side by side
70:17 - for a comparison
70:20 - so if you take a look at the uh
70:23 - image down below you can see that
70:26 - again the outline of the horses was
70:28 - rather broken in some places and now if
70:31 - you compare that to the video stream up
70:33 - above you can see that there's been some
70:35 - improvement we're effectively
70:38 - extending
70:39 - the definition of these edges because
70:41 - we're allowing those edges to be
70:43 - connected to weaker edges
70:45 - that were in between those two
70:47 - thresholds rather than discarding those
70:48 - edges altogether so i thought that was
70:51 - an interesting way to demonstrate
70:54 - how those inputs affect the results
70:57 - obviously
70:58 - all these algorithms require
71:01 - some experimenting and
71:03 - tuning and things that will depend on
71:05 - your particular application but
71:08 - we hope this was a nice introduction for
71:10 - you and
71:11 - definitely encourage you to take a look
71:13 - at the opencv documentation on these
71:15 - functions and other functions
71:17 - and write similar scripts like this one
71:19 - here and do some experimentation so
71:20 - that's all we wanted to cover in this
71:22 - section and we'll see you next time
71:26 - in this video we're going to be talking
71:27 - about image alignment which is also
71:29 - referred to as image registration
71:32 - image alignment is used in many
71:33 - applications such as building panoramic
71:36 - images from multiple photos or
71:37 - constructing hdr photos from multiple
71:39 - images taken at different exposures it's
71:42 - also used in the medical field for
71:43 - comparing digital scans to highlight
71:45 - small changes between images
71:47 - so in this particular example we're
71:49 - going to introduce the topic by showing
71:50 - how we can perform document alignment
71:53 - the image on the left here
71:55 - is an image of a form that's been
71:57 - printed out and filled out by hand and
71:59 - placed on a table and the goal here is
72:01 - to
72:02 - transform that image to an image on the
72:05 - right that would align with the original
72:06 - template of the form and therefore make
72:09 - optical character recognition
72:11 - of this form a much easier task so
72:14 - that's a preview of what we're going to
72:15 - talk about so first let's scroll down
72:16 - here a little bit and talk about some of
72:19 - the theory of transformations
72:21 - so on the left here we have an original
72:23 - image in the shape of a square
72:25 - and one of the simplest transformations
72:27 - we can make is a translation which is
72:28 - simply a shifting of the pixel
72:30 - coordinates in the original image
72:33 - to the translated image as shown here
72:35 - and then beyond that we have euclidean
72:37 - transformations which now include
72:38 - rotation
72:40 - but notice that the size and shape of
72:42 - the original image has been preserved
72:44 - and then further to the right we have a
72:46 - fine transformations which encompass
72:48 - euclidean transformations but also
72:50 - include shear and scale changes
72:52 - so now we have some distortion but
72:54 - notice that parallel lines remain
72:56 - parallel
72:57 - and finally we have the homography which
72:59 - is the most general transformation for
73:01 - 2d images which allows us to transform
73:03 - the original square into an arbitrary
73:06 - quadrilateral
73:07 - and the reason this is useful and
73:08 - interesting is that it allows us to warp
73:10 - an image to effectively change its
73:12 - perspective
73:13 - so just as a more concrete example let's
73:15 - scroll down to this next section where
73:17 - we showed two images of the same book
73:19 - taken from different perspectives here
73:21 - we're interested in talking about the
73:22 - homography between these two images and
73:24 - specifically for the 2d plane and each
73:27 - image is represented by the front cover
73:29 - of the book
73:30 - so if we can identify at least four
73:32 - points in both images that correspond to
73:33 - the same physical location on the front
73:35 - cover of the book then we can compute
73:37 - the homography that relates these two
73:39 - images
73:40 - so for example we've identified four
73:42 - different points in both images that
73:43 - we've color coded that represent the
73:45 - same
73:46 - points on the physical book and
73:48 - therefore we call these points
73:49 - corresponding points since they
73:51 - correspond to the same physical location
73:53 - but are obviously represented by a
73:55 - different set of pixel coordinates in
73:56 - each image
73:57 - so given we have a set of points like
73:59 - these we could simply call an opencv
74:01 - function to compute the homography and
74:03 - then apply the homography as a
74:04 - transformation to the image on the left
74:06 - for example
74:08 - to effectively change its perspective to
74:10 - look like the image on the right
74:11 - i just keep in mind that
74:13 - four points is a minimum required and in
74:15 - practice we would want to find many more
74:17 - corresponding points
74:19 - uh but thankfully there are other
74:20 - functions in opencv that enable us to do
74:23 - just that
74:24 - so we'll take a look at those details
74:25 - further below in this notebook and just
74:28 - getting back to the document alignment
74:30 - example let's start taking a look at the
74:32 - actual code
74:37 - so here on lines two through three we're
74:39 - just importing some required modules
74:41 - and then the very next thing we're going
74:43 - to do is simply read in the images of
74:45 - the
74:46 - template form and also the scan form so
74:49 - we have those available to us
74:51 - and
74:52 - in this next section here we're simply
74:54 - displaying both of those images
74:56 - so the
74:57 - we've got the original form here on the
74:59 - left
75:00 - and then the
75:01 - photo of the form that we filled out on
75:04 - the table and and taken a photograph of
75:06 - and again our goal is to take this image
75:09 - and apply homography to it
75:11 - so that it lines up with this form here
75:13 - on the left
75:15 - so let's see how to do that
75:17 - the very first step in this process
75:19 - is finding
75:21 - some number of key points in both images
75:25 - and there's not a lot of code here but
75:26 - there's a lot going on that needs some
75:29 - explanation so
75:30 - lines two and three here are simply
75:32 - converting
75:33 - uh the images that we read into
75:35 - grayscale and the reason for that is
75:37 - that the the code that follows that is
75:39 - performing some feature extraction on
75:41 - these images only requires a great scale
75:43 - representation of the image
75:46 - and then there's this uh this code right
75:48 - here
75:49 - that is configuring an orb object from
75:52 - this orb create class
75:54 - so if you're not familiar with image
75:56 - features and feature extraction and
75:58 - computer vision just know that
76:01 - various algorithms have been invented
76:03 - over the years
76:04 - to extract what we call features from
76:06 - images and
76:08 - uh the objective there is to try to
76:10 - extract meaningful
76:12 - information that is contextually um
76:15 - related to the uh the image itself so
76:18 - typically we're looking for
76:20 - edges and corners and
76:23 - texture in images and we people have
76:26 - been tried to invent various ways to
76:28 - compactly represent that information so
76:31 - orb features are one way to do that and
76:34 - they're available in opencv
76:36 - so here we're going to create this orb
76:38 - object
76:39 - and then we're going to use that object
76:41 - to detect and compute
76:44 - key points and descriptors for each of
76:46 - the images so let's just go over this
76:49 - each of these call each of these
76:50 - function calls returns a list of key
76:52 - points
76:53 - and a list of associated descriptors so
76:55 - the key points are
76:57 - interesting features in each image that
77:00 - are usually associated with some sharp
77:02 - edge or corner
77:04 - and
77:05 - they're described by
77:07 - a set of pixel coordinates that describe
77:08 - the location of the of the key point
77:11 - the size of the key point in other words
77:13 - the scale of the key point and then also
77:15 - the orientation of the key point and
77:17 - then there's an associated list of
77:20 - descriptors for each key point and each
77:21 - descriptor is actually a vector of some
77:24 - information that describes the
77:27 - region around the key point which
77:29 - effectively acts as a signature for that
77:32 - key point so it's a it's a vector
77:34 - representation of the pixel information
77:37 - around the key point
77:39 - and the idea here is that
77:41 - if we're looking for
77:43 - the same key point in both images we can
77:45 - try to use the descriptors to match them
77:47 - up
77:49 - so let's uh let's scroll down here a bit
77:50 - further and
77:52 - just talk about these two
77:54 - displays so we've we've calculated the
77:57 - we've computed rather the um
77:59 - key points and descriptors for each
78:01 - image and here in these figures we're
78:04 - displaying just the key points so the
78:06 - key points are the um where all these
78:08 - red circles are key points
78:10 - the center of the circle is the location
78:12 - of the key point
78:14 - the size of the circle represents the
78:16 - scale of the key point and then the the
78:18 - line connecting the center of the circle
78:19 - to the outside of the circle represents
78:21 - the orientation of the key point
78:23 - so those details um aren't terribly
78:25 - important for this demonstration i just
78:27 - wanted to point out that all these red
78:28 - circles represent the the key points
78:31 - but associated with each of these key
78:32 - points is a
78:34 - vector representation of the image patch
78:36 - at that key point which we're not
78:38 - displaying
78:39 - but it's the
78:41 - descriptors that are actually used to
78:43 - match up these key points so notice that
78:46 - on the figure in the left there's all
78:47 - these red circles here on the form
78:49 - and on the figure to the right there's a
78:52 - lot of red circles in regions on the
78:54 - form that aren't even located here on
78:56 - the left so
78:57 - the list of key points for figure one
78:59 - and the list of key points for figure
79:01 - two are
79:02 - um
79:03 - they're overlapping but certainly
79:05 - there's probably some key points in both
79:07 - images that maybe are the same and those
79:09 - are the ones that we're going to try to
79:11 - try to find
79:13 - so that we can compute the homography
79:14 - between these two um image
79:16 - representations
79:18 - so
79:19 - so that's the introduction to key points
79:21 - now let's scroll down here a bit further
79:23 - and talk about how we match those key
79:25 - points
79:27 - so the first step in this matching
79:29 - process is to create a matcher object by
79:32 - calling this descriptor matcher
79:33 - underscore create function
79:35 - and we pass to that function some
79:37 - configurations that indicate the type of
79:40 - matching algorithm we're going to use
79:41 - which is brute force and then also
79:44 - the metric for computing the distance
79:46 - between the descriptors which is a
79:48 - hamming metric
79:50 - a distance measure and that's because
79:52 - the descriptors for uh orb are binary
79:55 - strings so we therefore require a
79:56 - hamming uh metric for that purpose
79:59 - and then uh so we use that matcher
80:01 - object to call the match function which
80:04 - then attempts to
80:06 - provide a list of the best matches
80:09 - associated with those list of
80:10 - descriptors and so now we get a data
80:12 - structure back here
80:14 - that contains the list of matches
80:16 - from the key points that we
80:18 - determined up above
80:20 - and then once we get that list we're
80:22 - going to sort the list based on the
80:24 - distance between the various descriptors
80:27 - and then on lines 9 and 10 here we're
80:30 - going to further limit that to the top
80:33 - 10 percent of the matches returned by
80:36 - the matching function here
80:40 - and we're going to use that now to draw
80:42 - the matches in this code below shown
80:44 - here in the image so uh we're calling
80:47 - this draw matches function and we're
80:49 - going to pass in the key points for
80:51 - image one as well as the image and the
80:53 - same for image two as well as this
80:56 - filtered list of matches computed above
80:59 - and if you uh take a look at these two
81:01 - images you can see that
81:03 - several key points in this image match
81:05 - the key points in this image for example
81:06 - on in the form over here there's a
81:08 - little image of a person and further
81:10 - down here it looks like an image of a
81:11 - house and you can see that there were
81:13 - several key points in both images
81:16 - that were in that local region and then
81:18 - this matching function determined that
81:20 - yes there were several here on this form
81:22 - that matched this form in other words
81:23 - the descriptors
81:25 - matched close enough for us to call that
81:27 - a match and so now we have a set of
81:29 - corresponding key points right
81:31 - but notice here for example right down
81:33 - here this lavender line is going up to
81:36 - some other location on the form to the
81:38 - right so that's that's not a match but
81:40 - it
81:41 - turned out that the descriptor for the
81:43 - key point here and the descriptor for
81:44 - this key point here were very close
81:46 - coincidentally and so it decided that
81:48 - that was a match
81:49 - it's okay to have some false positives
81:51 - here the important thing is that we have
81:52 - an overwhelming number
81:54 - of actual matches
81:56 - which will allow us to compute a
81:58 - homography
81:59 - so then the final a couple of steps here
82:02 - in the notebook are to
82:05 - first compute the homography
82:07 - so to do that we simply call this find
82:09 - homography function here
82:11 - and pass it both sets of key points that
82:13 - have been filtered by the matching
82:15 - process above
82:17 - and then there's an optional argument
82:19 - here which is the algorithm that's going
82:20 - to be used
82:21 - to compute the homography and here we're
82:24 - indicating the ransack algorithm which
82:27 - is definitely the one you want to use
82:28 - that's very robust to filtering out
82:31 - outliers left over from the matching
82:34 - process computed above and there's a
82:36 - little bit of code up here that requires
82:38 - us to change the format of the points so
82:40 - that um to comply with the uh this fine
82:43 - homography function but that's um that's
82:45 - a detail that the point here is that we
82:47 - can
82:47 - compute the homography from a set of uh
82:50 - corresponding key points and then
82:51 - finally once we have uh the homography
82:54 - h which is a three by three matrix
82:57 - we can uh call this uh function warp
83:00 - perspective
83:01 - on image two and recall image two was
83:06 - the image
83:07 - of the
83:08 - filled out form sitting there on the
83:10 - table
83:11 - and passing the homography
83:14 - and what we get back is the
83:16 - registered or aligned image
83:19 - as shown below here to the right so now
83:21 - we've we've effectively changed the
83:22 - perspective of that image on the table
83:25 - and it's very closely aligned to the
83:27 - image on the form here and now this is
83:29 - obviously a much simpler task to process
83:32 - uh you know automatically process this
83:34 - form on the right
83:35 - this form on the right can be compared
83:37 - to the form on the left
83:38 - and an algorithm could be written to
83:41 - it knows where the last name field is on
83:43 - the form so then it can easily
83:46 - recognize uh these characters here as
83:49 - the last name of the person that has
83:51 - filled out the form
83:52 - so that's one example of how you can use
83:56 - image alignment or image registration
83:59 - and there's many other applications as
84:01 - we mentioned earlier so you can see in
84:03 - very few lines of code you can get this
84:05 - up and running you can experiment with
84:07 - your own images and that's a lot of fun
84:09 - to do so and we encourage you to
84:12 - explore that more so we hope this was
84:13 - helpful to you and we'll see you next
84:15 - time
84:17 - in this video we're going to be
84:18 - describing how you can create panoramas
84:20 - from multiple images using opencv
84:23 - much of the processing pipeline used for
84:25 - creating panoramas is very similar to
84:27 - the steps we described in the image
84:29 - alignment video
84:31 - since panoramas require image alignment
84:33 - we still need to find key points and
84:34 - descriptors in each of the images
84:36 - and also determine their pairwise
84:38 - correspondences through a feature
84:40 - matching process and we also need to
84:42 - estimate the homographies to facilitate
84:45 - image warping
84:46 - and then once images have been
84:48 - transformed in this way we need an
84:50 - additional step to stitch and blend the
84:52 - images together so they look realistic
84:55 - and fortunately there's a high level
84:56 - convenience function in opencv that's
84:58 - available in the stitcher class that
85:00 - allows us to create panoramas by simply
85:02 - passing in a list of images
85:05 - however we do think it's important to
85:07 - understand the underlying concepts but
85:09 - since we covered much of this material
85:11 - in great detail in the image alignment
85:13 - video we're simply going to use the
85:15 - stitcher class in this example to show
85:17 - you just how easy it is to create
85:19 - panoramas with a single function call i
85:21 - just remember that images used to create
85:23 - panoramas need to be taken from the same
85:26 - vantage point ideally on a tripod that
85:28 - is panning around the optical axis of
85:30 - the camera
85:32 - and it's also important to take the
85:33 - photos at roughly the same time in order
85:35 - to minimize any lighting changes between
85:37 - the images so adhering to these
85:39 - suggestions will lead to the best
85:40 - results so let's get started and take a
85:42 - look at the code that's required here in
85:45 - this first cell block we're importing
85:47 - some required modules and then in this
85:50 - next code section
85:52 - we're using glob to retrieve the file
85:54 - names from a subdirectory
85:56 - and then in this for loop here we're
85:58 - simply reading in each of the file names
86:00 - and converting the images from bgr to
86:03 - rgb
86:05 - and then appending each image to a list
86:07 - of images
86:09 - and then in this next section here we're
86:11 - simply plotting each of the images so
86:13 - you can see the sequence here there's
86:15 - six images total
86:16 - and then finally in the last section
86:18 - here you can see that we're going to be
86:19 - able to create the panorama in just two
86:21 - lines of code so we do that by creating
86:24 - a stitcher object from the stitcher
86:26 - underscore create class and then we use
86:29 - that object to call the stitch method
86:31 - and we simply pass in the list of images
86:34 - and the result we get here is the
86:36 - panorama image so that's shown below uh
86:39 - the only thing we would mention at this
86:41 - point is that
86:42 - the return panorama includes these black
86:45 - regions here which are a result of the
86:47 - warping that was required to stitch the
86:49 - images together
86:51 - and we'd just like to mention that maybe
86:53 - one thing you might consider doing is
86:55 - writing your own code to
86:57 - programmatically crop out that black
86:59 - image
87:00 - you could use a combination of
87:02 - thresholding techniques uh bitmaps and
87:05 - contour finding uh to do that task so
87:08 - that's all we really wanted to cover in
87:09 - this section and uh thanks so much and
87:11 - we'll see you next time
87:14 - hi everyone in this video we're going to
87:16 - be talking about high dynamic range
87:18 - imaging also referred to as hdr imaging
87:21 - and i think the best way to get started
87:23 - with this is to just take a look at the
87:25 - simple example below here we have two
87:27 - photos of a young boy
87:29 - this photo below on the left was taken
87:32 - with an iphone in standard camera mode
87:35 - and in that case the metering system on
87:37 - the camera attempts to determine what
87:39 - the main subject matter of the photo is
87:41 - in this case the young boy
87:43 - and then it tries to set the exposure
87:45 - accordingly
87:46 - so it's done a nice job here the boy is
87:48 - properly exposed
87:50 - yet the background the sky
87:53 - and the clouds are completely washed out
87:56 - and then sometimes the opposite occurs
87:57 - sometimes you'll get a background that's
87:59 - properly exposed yet the the foreground
88:02 - is just far too dark and the reason this
88:04 - occurs is because the actual intensities
88:06 - in the real world scene far exceed the
88:09 - capability of the camera to record those
88:11 - values
88:12 - since most cameras only have an 8 bit
88:15 - per channel capability there just aren't
88:17 - enough bits to capture the full
88:20 - dynamic range of the scene
88:22 - so in contrast to that we have the image
88:24 - to the right this image was also taken
88:26 - with an iphone in hdr mode
88:29 - and in this case both the foreground and
88:32 - the background are all properly exposed
88:34 - and the photo looks fantastic
88:36 - so
88:37 - how exactly is this done well what the
88:39 - iphone does is it takes three
88:41 - photographs at different exposures and
88:44 - it takes them in quick succession so
88:45 - that there's no movement
88:47 - uh or almost no movement in between the
88:49 - three shots
88:50 - and then it takes those uh three what we
88:52 - call
88:54 - low dynamic range photos and merges them
88:57 - to come up with a hdr photo like the one
88:59 - shown here
89:00 - so that's the basic idea and we'll talk
89:02 - about this in a little more detail
89:05 - down here below with a different example
89:08 - so this is a common uh photo sequence of
89:11 - the old courthouse in st louis that's
89:13 - used to describe hdr imaging you can see
89:16 - that there's four different images here
89:18 - taken at different exposures
89:20 - the image to the far left uh
89:22 - underexposed quite a bit although that
89:24 - there is some
89:26 - area here in the lower portion of the
89:27 - building that looks properly exposed and
89:29 - might contain some useful detail
89:32 - and then
89:33 - further to the right here there's a
89:35 - little bit more of the building that's
89:36 - properly exposed
89:38 - still the center is nice too and then
89:40 - these other areas here might provide
89:43 - useful information
89:44 - and then even further to the right here
89:46 - now the buildings in the background
89:47 - start to have
89:49 - proper exposure and then finally to the
89:51 - right uh the well-lit portion here in
89:53 - the center is completely blown out but
89:55 - perhaps the um
89:57 - the background buildings and even the
89:59 - sky for example and then some of these
90:01 - areas in the foreground
90:03 - uh might contain useful information so
90:04 - the hope is that collectively this
90:07 - sequence of four images
90:09 - across all pixels in the image will
90:11 - contain some useful information that can
90:13 - be
90:14 - merged together to form a single
90:16 - hdr image
90:18 - with proper exposure for all of the
90:20 - pixels
90:21 - so let's take a look at some of the code
90:23 - that implements this example
90:26 - down here we're just importing some
90:27 - required modules and then right here
90:30 - we're defining a convenience function
90:31 - that's going to read the images and the
90:33 - exposure times for each image
90:35 - so in here we're just listing the file
90:37 - names of the four images and right down
90:40 - here we're
90:41 - setting the exposure times for each of
90:43 - those images we know what that is for
90:45 - this example but you could also extract
90:48 - that information from the metadata in
90:50 - each image and do that programmatically
90:52 - and then finally down here in this for
90:54 - loop we're just reading each of the
90:56 - images in
90:57 - and converting them to rgb
91:00 - and then returning the list of images
91:02 - and the exposure times
91:04 - so the next step in the process uh once
91:07 - we've read in the images is to make sure
91:09 - they're properly aligned
91:12 - and even though these images may have
91:13 - been taken in quick succession or even
91:16 - on a tripod in quick succession
91:18 - it's important that they be very
91:20 - accurately aligned down to the pixel
91:22 - level or even the sub pixel level
91:24 - so just as an example the image here to
91:26 - the left is an hdr image that was
91:28 - produced without alignment and you can
91:30 - see that the zoomed in section here at
91:32 - the top of the building
91:33 - has several ghosting artifacts and just
91:35 - doesn't look quite right and it's not a
91:38 - true representation of that portion of
91:40 - the image
91:41 - now contrast that with the hdr image
91:44 - produced on the right this was produced
91:46 - with alignment and you can see that the
91:48 - top portion of the building looks much
91:50 - more correct
91:52 - however since the images that are used
91:54 - in the sequence are taken at different
91:55 - exposures they actually look different
91:58 - and therefore
91:59 - standard alignment techniques just don't
92:01 - work however there is a special class in
92:03 - opencv that uses bitmaps for this
92:06 - purpose
92:07 - and that class is called create a line
92:10 - mtb for median threshold bitmap
92:13 - so down here we're going to create an
92:15 - align mtb object and then use that
92:17 - object to call the process method from
92:19 - that class
92:20 - and pass at the list of images and then
92:22 - get back the list of aligned images
92:24 - right here
92:25 - so
92:26 - once we've done the alignment of the
92:29 - images the next step in the process
92:32 - is to compute the camera response
92:33 - function and the reason we need to do
92:35 - this is because most cameras we use are
92:37 - not linear which means for example that
92:40 - if the radiance in a scene is doubled
92:41 - the pixel intensities recorded by the
92:43 - camera will not necessarily double and
92:45 - this presents a problem when we want to
92:47 - merge images taken at different
92:48 - exposures
92:50 - so for example suppose the response
92:52 - function was linear then the intensities
92:54 - of the input images could be simply
92:56 - scaled by their exposure times which
92:58 - would put them on the same radian scale
93:00 - and then we could simply compute an
93:02 - average intensity at every pixel
93:03 - location across those images to
93:06 - synthesize an hdr image
93:08 - however since the response function is
93:10 - not linear we need to estimate it so
93:12 - that we can first linearize the images
93:13 - before combining them
93:15 - however since the response function for
93:17 - various cameras are considered
93:19 - proprietary information by the camera
93:20 - manufacturers we need to actually use
93:22 - the images captured by the camera itself
93:25 - to estimate the response function
93:27 - and this is actually a rather involved
93:29 - optimization problem but fortunately
93:31 - opencv has two different classes that we
93:34 - can use for this purpose both named
93:36 - after the people that invented the
93:37 - algorithms so let's take a look at the
93:40 - code uh in opencv that accomplishes this
93:43 - uh the one we're going to be focusing on
93:46 - is create calibrate debevic there's
93:48 - another one by robertson in either case
93:51 - there's a class for each algorithm and
93:54 - here we're creating a calibrate devic
93:56 - object and then we're using that object
93:58 - to call the process method for that
94:00 - class and we pass in the list of images
94:03 - and the associated exposure times for
94:06 - those images and we get back
94:08 - the inverse camera response function
94:10 - here and so this next block of code here
94:12 - is simply plotting the camera response
94:14 - function
94:15 - and you can see at the lower intensity
94:17 - values the function is quite linear in
94:20 - this region here
94:21 - and then starts to become non-linear
94:24 - right about here and then finally at the
94:26 - higher end of the spectrum
94:27 - we start to see some clipping at 255 as
94:30 - the
94:31 - intensities in the actual scene exceed
94:34 - the recording limits of the camera
94:36 - also notice that the
94:37 - three channels are calibrated separately
94:39 - since the sensitivities are slightly
94:41 - different between them
94:43 - and so now we can use this function to
94:45 - linearize the input images by mapping
94:47 - the measured pixel intensity in those
94:49 - images to the calibrated intensity
94:51 - so that the images can then be merged
94:53 - appropriately so in this next section
94:55 - here we use a separate class for that
94:57 - purpose
94:58 - here we're calling the create mergedabit
95:00 - class to create an object and then using
95:03 - that object to call the process method
95:05 - in that class
95:06 - passing at the list of images the
95:08 - exposure times for each of the images
95:10 - and then the response function that we
95:12 - calculated up above
95:14 - and that method then returns the hdr
95:17 - image that we've been looking for
95:19 - so at this point it's worth mentioning
95:21 - that the merging process intentionally
95:23 - ignores pixel values close to zero or
95:25 - 255
95:26 - and the reason for that is that pixel
95:28 - values close to those extremes contain
95:30 - no useful information so it's common to
95:32 - apply a hat type weighting function to
95:35 - each of the input images to filter out
95:37 - those pixels
95:38 - from the merging process
95:40 - so just to briefly summarize because
95:42 - there are multiple images of the scene
95:44 - at different exposure settings the hope
95:46 - is that for every pixel we have at least
95:48 - one image that contains an intensity
95:51 - that is neither too dark nor too bright
95:53 - however there is one problem that still
95:55 - remains which is the intensity values
95:57 - are no longer in the zero to 255 range
96:01 - of course black is completely zero but
96:02 - hdr images can record light intensities
96:05 - from zero to essentially infinite
96:07 - brightness so because they have a fixed
96:09 - range they need to be stored as 32-bit
96:11 - floating point numbers
96:13 - and since our displays require 8-bit
96:15 - images we need one last step to bring
96:17 - the image intensities back down to the
96:19 - 0-255 range so that brings us to the
96:22 - final step in the process which is
96:23 - called tone mapping which refers to the
96:26 - process of mapping hdr images to 8-bit
96:29 - per channel
96:30 - images
96:31 - so there's several algorithms
96:32 - implemented in opencv for this purpose
96:36 - mostly designed to preserve as much
96:37 - detail as possible from the original
96:39 - image while converting it to 8 bits per
96:42 - channel
96:43 - but the main thing to keep in mind is
96:44 - that there's no correct way to perform
96:46 - tone mapping
96:47 - sometimes the goal of tone mapping is to
96:49 - achieve an aesthetically pleasing image
96:52 - that isn't necessarily realistic however
96:54 - the algorithms implemented in opencv
96:56 - tend to be
96:57 - fairly realistic
96:58 - yet they have some differences and also
97:01 - various parameters that are configurable
97:04 - for each of the algorithms
97:05 - so in this first example we're going to
97:07 - take a look at using drago's method to
97:09 - create a tone map
97:11 - and we start by calling the create
97:13 - tonemapdrago
97:14 - class
97:15 - and to create an object and then use
97:17 - that object to call the process method
97:19 - for that class and we simply pass it the
97:21 - hdr image itself
97:23 - and that returns the 8-bit per channel
97:25 - color image shown below here
97:28 - it does a very nice job of properly
97:30 - exposing
97:31 - all regions of the scene there
97:34 - it's very pleasing in my opinion and
97:37 - even the background the buildings there
97:39 - seem to be properly exposed so very nice
97:42 - result
97:43 - and then moving on to the next example
97:45 - this one is using reinhardt's method and
97:48 - it also looks very nice perhaps
97:51 - not as ascetically pleasing but
97:54 - certainly very realistic and with
97:56 - everything properly exposed
97:58 - and then finally there's one more
97:59 - example down here that's almost a
98:01 - combination of the two i'd say
98:03 - a little bit of the glowing here and in
98:05 - the center like the first image and
98:07 - again everything uh looking fairly
98:09 - realistic and and properly exposed
98:12 - so that's a summary of the
98:15 - hdr imaging process
98:17 - we covered a lot of detail but when you
98:19 - step back and look at how much code was
98:21 - required it wasn't very much so
98:24 - that's all we really wanted to cover in
98:25 - this section and we'll see you next time
98:27 - thank you
98:29 - in this video we're going to be talking
98:31 - about object tracking this is a really
98:33 - interesting topic and a lot of fun to
98:35 - experiment with and we hope you enjoy
98:37 - this demonstration so first of all what
98:39 - is tracking tracking usually refers to
98:43 - estimating the location of an object and
98:45 - predicting its location at some future
98:47 - point in time
98:48 - and in the context of computer vision
98:51 - that usually amounts to
98:52 - detecting an object of interest in a
98:54 - video frame and then predicting the
98:57 - location of that object in subsequent
98:59 - video frames and we accomplish this by
99:01 - developing both a motion model
99:04 - and an appearance model
99:05 - the motion model for example will
99:07 - estimate the position and velocity of a
99:10 - particular object and then use that
99:12 - information to predict its location in
99:14 - future video frames
99:16 - and then we can also use an appearance
99:17 - model which encodes what the object
99:19 - looks like and then search the region
99:22 - around the predicted location from the
99:24 - motion model to then fine tune the
99:26 - location of the object so the motion
99:28 - model is an approximation to where the
99:31 - object might be located in a future
99:33 - video frame and then the appearance
99:34 - model is used to fine-tune that estimate
99:38 - all of the
99:39 - code that we'll be using below is from
99:41 - the opencv api tracker class so we'll
99:44 - talk about that a little bit more as we
99:46 - scroll down through the notebook here
99:48 - so as a concrete example suppose we're
99:50 - interested in tracking a specific object
99:53 - like the race car identified here in the
99:54 - first frame of a video clip
99:56 - in order to initiate the tracking
99:58 - algorithm we need to specify the initial
100:00 - location of the object and to do this we
100:02 - define a bounding box shown here in blue
100:04 - which consists of two sets of pixel
100:06 - coordinates which define the upper left
100:08 - and lower right corners of the bounding
100:10 - box and then once the tracking algorithm
100:12 - is initialized with this information the
100:15 - goal is to then track the object in
100:16 - subsequent video frames by producing a
100:19 - bounding box in each new video frame so
100:23 - we'll talk more about this below but
100:24 - before we get started uh with the code
100:26 - description let's just take a look at
100:28 - the tracking algorithms available in
100:30 - opencv
100:31 - there's uh eight different algorithms
100:34 - listed here
100:35 - and we're not going to review the the
100:37 - details of each of these but it's worth
100:39 - noting that depending on your
100:40 - application one might be more suitable
100:42 - than the other
100:44 - for example some are more accurate some
100:46 - are faster
100:48 - some are more robust to occlusions of
100:51 - the object being tracked so that's worth
100:53 - keeping in mind when you experiment with
100:55 - all of these uh different algorithms
100:58 - and then one other thing that's worth
100:59 - mentioning is that the go turn model uh
101:02 - here is the only one that's deep
101:03 - learning based
101:05 - and we'll talk a little bit more about
101:06 - that further below
101:08 - so uh just as a preview to get started
101:10 - here i've got the um
101:12 - uh the test video clip right here and
101:14 - let's just play it uh one or two times
101:18 - so you'll notice that early on the car's
101:21 - appearance is relatively constant as
101:23 - well as its uniform motion but as it
101:25 - starts to make a turn here you'll see
101:27 - that we see the broadside portion of the
101:29 - car
101:30 - and then the lighting is starting to
101:32 - change quite a bit and then
101:34 - now it's getting smaller and smaller off
101:36 - into the distance so those types of
101:38 - things are going to represent some
101:39 - challenges uh for some of the tracking
101:41 - algorithms so we'll talk a little bit
101:43 - more about that
101:44 - so let's start taking a look at the
101:46 - first code block in this notebook
101:48 - here we're just importing some modules
101:50 - that are required and then on line 10
101:52 - we're indicating the file name for the
101:54 - video clip that we're going to process
101:57 - and then here we're defining some
101:59 - convenience functions that will allow us
102:00 - to render bounding box information on
102:02 - the output video stream as well as
102:05 - annotate the output video frames with
102:08 - some text
102:09 - and then recall
102:11 - earlier we described that one of the
102:13 - algorithms is the go turn model which
102:15 - requires an inference model so
102:17 - this block of code here is
102:21 - required to download that inference
102:22 - model and then
102:24 - this figure here is a very high level
102:26 - description of uh how the go turn
102:29 - tracker is
102:31 - trained and used so in the center here
102:33 - we're indicating that we have a
102:35 - pre-trained
102:37 - neural network model also known as an
102:39 - inference model
102:40 - and it takes as input two cropped images
102:43 - one from the previous frame and one from
102:44 - the current frame
102:46 - uh it uses the bounding box from the
102:48 - previous frame to crop both of these
102:50 - images and therefore the object of
102:52 - interest
102:53 - uh is located in the center of this
102:55 - previous frame
102:56 - and obviously if the object has moved um
102:59 - in the current frame then it won't be
103:01 - centered uh in this cropped image
103:03 - because we're using the bounding box
103:06 - from the previous frame to crop both of
103:08 - these images
103:09 - and then uh it's the job of the
103:11 - inference model to then predict
103:13 - uh what the bounding box is uh in the
103:16 - output frame here so that's just a high
103:18 - level description of
103:19 - how that works
103:21 - so let's scroll down a bit further here
103:22 - and take a look at this next code block
103:24 - this is where we're going to create a
103:26 - tracker instance
103:27 - and we start by defining a list of
103:29 - tracker types here where we're just
103:31 - indicating the list of string names that
103:33 - are available in the opencv api
103:36 - and then depending on the track or
103:38 - algorithm that you wanted to execute you
103:39 - would just set the appropriate index
103:41 - here into that list and since that's
103:43 - specified as two then we'd be indicating
103:45 - that we'd like to execute the kcf
103:48 - tracker in that list and this uh if else
103:51 - block here would then call the
103:53 - appropriate class
103:54 - to create the tracker object so in the
103:56 - case of uh the default we'd be calling
103:59 - the
104:00 - tracker kcf underscore create class to
104:04 - create a tracker object of that class
104:07 - so let's scroll down here to the next
104:09 - section uh in this block of code uh
104:12 - we're setting up the input output video
104:14 - streams so here on line two we're
104:16 - passing in the video input file name and
104:20 - creating a video input object and then
104:23 - on the next line we'll go ahead and read
104:24 - the first frame from that video file
104:27 - and then down here on lines 13 and 14
104:30 - we're doing a similar thing for the
104:32 - output video stream
104:34 - and creating a video out object which
104:37 - will then write results to
104:39 - from our tracking algorithms
104:42 - now in this section here as we talked
104:44 - about earlier we need to define a
104:46 - bounding box
104:47 - around the object that we're interested
104:49 - in tracking
104:50 - and we're accomplishing that here just
104:52 - manually notice that i'm specifying the
104:55 - two sets of pixel coordinates here for
104:56 - the upper left and lower right corners
104:58 - of that bounding box
105:00 - but in practice you would um either
105:02 - select that with a user interface or um
105:06 - or perhaps uh use the detection
105:07 - algorithm to detect objects of interest
105:10 - for tracking
105:11 - and do that programmatically uh so but
105:14 - for demonstration purposes here we're
105:16 - just going to set that box manually
105:19 - and then down here we're now ready to
105:21 - initialize the tracker so in order to do
105:24 - that we use the tracker object here and
105:26 - call the init function and we pass it
105:28 - the first frame of the video clip and
105:31 - then the bounding box uh that we defined
105:34 - manually up above
105:36 - okay and then once that's defined we're
105:38 - ready to enter a loop here to process
105:41 - all the frames in the video so this
105:43 - first line of code
105:44 - in the loop on line 2 is reading the
105:47 - next frame from the video clip
105:50 - and then on line 10 here we're going to
105:52 - pass that frame to the tracker update
105:54 - function
105:55 - and hopefully return a bounding box for
105:57 - the object
105:59 - that was detected
106:00 - so if we detect the object and we
106:02 - retrieve a bounding box from the update
106:04 - function
106:05 - uh then we'll go ahead and render a
106:07 - bounding box rectangle on the current
106:10 - frame and if we didn't detect the
106:12 - bounding box and this okay
106:14 - flag would be
106:16 - false and we'd simply
106:18 - annotate the frame with a tracking
106:20 - failure message indicated here
106:24 - and then further below we'll also
106:26 - annotate uh the video frame with the
106:28 - type of tracker that's being used and
106:30 - the frames per second that's been
106:31 - calculated and then write that frame out
106:34 - to the output video stream so that's all
106:36 - this loop does it um
106:38 - cycles through each frame in the video
106:40 - clip
106:41 - and calls the tracker update function
106:43 - and then annotates the frames and sends
106:45 - them to the output video stream
106:47 - so
106:48 - let's scroll down here a bit further and
106:49 - take a look at some results so this
106:51 - notebook has already been executed a few
106:53 - times for different trackers and so
106:55 - we're now we're just going to replay
106:57 - those results so all of these results
107:00 - shown here
107:01 - are the output video streams that have
107:03 - been annotated with tracker results
107:05 - and you can see in this first example
107:08 - this is the kfc tracker
107:10 - and i'm going to go ahead and play this
107:12 - and we'll take a look at how it performs
107:16 - so it looks like it's doing a fairly
107:17 - good job of tracking the car
107:20 - a little bit off-center but uh still
107:22 - maintaining track on what's um obviously
107:24 - the car in the video frame
107:26 - and then as the car rounds the corner
107:28 - it's uh it's doing okay and then uh
107:30 - shortly here we're gonna see that uh has
107:32 - a little bit of difficulty
107:34 - right here at the end it drops track on
107:36 - the car
107:37 - uh so let's take a look at the next
107:38 - example the next example here is the
107:41 - csrt tracker and we'll go ahead and take
107:44 - a look at that
107:47 - so this one does a little better job
107:49 - tracking the car with the bounding box
107:52 - encompassing
107:53 - most of the car and centered on the car
107:56 - pretty much and then as the car
107:59 - makes the turn here
108:01 - uh the box is on the front of the car
108:02 - but it's still got the car and track i'd
108:04 - say and then right there at the end it
108:06 - looks like it's having difficulty
108:09 - maintaining a precise location for the
108:11 - car
108:12 - so let's go down to the the final
108:14 - example and this is the go turn tracker
108:17 - again trained on a deep neural network
108:20 - offline
108:21 - so let's take a look at this
108:25 - so it looks like it's maintaining track
108:27 - as well
108:28 - the bounding box is a little narrower
108:30 - but centroid it on the on the car
108:33 - and then as it rounds the corner here it
108:35 - still maintains uh track the car is
108:38 - pretty much in the center of that
108:39 - bounding box uh for the most part
108:42 - it gets a little wider there but then as
108:44 - it tails off it uh it maintains uh track
108:47 - on the car right there at the end so out
108:49 - of the three examples
108:51 - of the go turn tracker probably uh did
108:53 - the best job of maintaining track uh
108:55 - throughout
108:56 - the entire video uh stream and
108:58 - especially right there at the end still
109:00 - able to keep the car
109:03 - essentially in the centroid of that
109:04 - bounding box
109:06 - so we hope that gives you a good feel
109:07 - for how to exercise the various tracking
109:10 - algorithms in opencv
109:11 - and especially the small amount of code
109:14 - that's required in order to get
109:15 - something up and running
109:17 - so we encourage you to experiment
109:19 - further try some of your own videos and
109:22 - experiment with the various algorithms
109:23 - and
109:24 - we hope this was helpful to you and
109:26 - we'll see you next time
109:30 - in this section we're going to show you
109:31 - how you can use a pre-trained neural
109:33 - network to perform face detection and to
109:36 - do that we're going to be using the
109:37 - opencv framework that will allow us to
109:40 - read in a pre-trained model and perform
109:42 - inference using that model
109:44 - so to get started
109:45 - there's a little bit of code here at the
109:46 - top of the script that sets the device
109:48 - index for the camera creates a video
109:51 - capture object
109:52 - and then creates an output window for
109:55 - sending all the results to the display
109:56 - however since we've covered this in a
109:58 - prior video we're going to skip that
110:00 - discussion
110:01 - and focus our attention here on line 47.
110:05 - so opencv has several convenience
110:07 - functions that allow us to read in
110:09 - pre-trained models that were trained
110:10 - using various frameworks so for example
110:13 - caffe tensorflow
110:15 - dark net and pytorch are all deep
110:17 - learning frameworks that allow you to
110:19 - design and train neural networks and
110:21 - thankfully opencv has built-in
110:23 - functionality to use pre-trained
110:24 - networks to perform inference
110:27 - so just to be clear you cannot use
110:28 - opencv to train a neural network but you
110:30 - can use it to perform inference on a
110:32 - pre-trained network and that's very nice
110:34 - for getting familiar with using neural
110:36 - networks and getting started
110:38 - so this function read net from caffe
110:42 - is a function that's specifically
110:44 - designed to read in a cafe model and it
110:47 - takes two arguments the first argument
110:48 - here is the prototext file which
110:50 - contains the network architecture
110:53 - information and then the next file is
110:55 - the caffe model file and that's a much
110:57 - larger file that contains the weights
110:59 - of the model that's been trained so
111:01 - notice here that we're pointing to these
111:03 - files on our local system however they
111:05 - can also be downloaded from the internet
111:07 - so let's take a look at the um the get
111:09 - repo that contains these models
111:11 - so here in this repo there's several
111:13 - scripts at this level and if you scroll
111:16 - down a bit uh you'll see that there's a
111:19 - download models script right here
111:22 - and if scroll down a little bit further
111:24 - you'll see that there's a readme file
111:26 - here that contains a description and
111:28 - instructions on how to use that script
111:30 - to download various models
111:32 - so it turns out that that script
111:33 - actually references a
111:36 - models.yaml file which is right here and
111:38 - it's instructive to go ahead and take a
111:39 - look at that
111:41 - so at the top of that file you'll see a
111:43 - block here that references the caffe
111:46 - model that we're actually going to be
111:47 - using here is the url to download the
111:49 - weights file
111:51 - and then there's several other
111:52 - parameters here
111:54 - related to how that model was trained
111:56 - and so we'll talk about these in a
111:58 - minute because we're going to reference
111:59 - these values in our script but just
112:01 - notice that there's a mean
112:03 - value a scale factor a height and width
112:06 - and also this rgb flag
112:09 - so let's go back to the script and
112:10 - continue on
112:13 - when we call this read net from cafe
112:15 - method it returns for us an instance of
112:18 - the network and we're going to use that
112:20 - object further below to perform
112:22 - inference on our test images from the
112:24 - video stream
112:26 - so this next section here is identifying
112:28 - the model parameters that were
112:30 - associated with how the model was
112:32 - trained
112:33 - and it's important that we're aware of
112:35 - these because any images that we pass
112:37 - through the model to perform inference
112:39 - on
112:40 - also need to be processed in the same
112:42 - way that the training images were
112:44 - processed so here we have the size of
112:46 - the input images that were used to train
112:48 - the model 300 by 300 and then here we
112:51 - have a list of mean values from each of
112:53 - the color channels across all the images
112:55 - that were used in training and then this
112:58 - confidence threshold
113:00 - is a value that you can set that will
113:02 - determine the sensitivity of your
113:03 - detections
113:05 - so then scrolling down here a bit
113:06 - further we enter this while loop
113:09 - and the first thing we do in the loop is
113:11 - read one frame at a time from the video
113:13 - feed
113:14 - and on line 59 i'm going to flip that
113:16 - frame horizontally just as a convenience
113:18 - for myself so that when i point to
113:19 - things in the field of view of the
113:21 - camera it's easier for me to do that but
113:23 - it has no consequence other than that
113:26 - and then on line
113:27 - 60 and 61 we're simply retrieving the
113:30 - size of the
113:32 - video frame
113:34 - and then on line 64 this is important uh
113:37 - here we're doing some pre-processing on
113:38 - the image frame calling this method blob
113:41 - from image
113:43 - so
113:43 - there are several arguments here and
113:45 - we'll go through these but this all this
113:47 - has to do with is doing some
113:49 - pre-processing on the input image and
113:51 - putting it in the proper format
113:53 - so that we can then perform inference on
113:55 - that image so it takes as input the
113:58 - image frame from the video stream
114:01 - this next argument is the scale factor
114:03 - and recall that the scale factor in that
114:05 - yaml file was one
114:07 - but that's not always the case when
114:09 - models are trained
114:10 - sometimes the images are scaled to
114:13 - different ranges and if that was the
114:14 - case this would have been something
114:16 - other than one
114:17 - then this is the input width and height
114:19 - of the images so that was 300 by 300 and
114:22 - we've identified that up above
114:24 - and then this is the mean value which is
114:27 - going to be subtracted from all the
114:29 - images
114:30 - and then there's this flag swap rb rb
114:33 - stands for red blue
114:35 - notice that that's equal to faults and
114:36 - the reason for that is that both cafe
114:38 - and opencv use the same convention
114:41 - for
114:42 - the three color channels but some models
114:44 - use a different convention and in those
114:45 - cases you'd have to swap the red and the
114:47 - blue channels
114:48 - and then finally there's this last input
114:50 - argument crop
114:53 - this last argument indicates that you
114:55 - can either crop your
114:57 - input image to be the correct size or
114:59 - you can resize it so because crop is set
115:02 - to false that means we're going to
115:03 - simply resize the image to be 300 by
115:05 - 300.
115:07 - and then this function call then returns
115:09 - a
115:10 - blob representation of the input image
115:13 - frame with all that pre-processing
115:14 - handled and then there's also a format
115:16 - change and then we pass that blob
115:19 - representation of the image to this
115:22 - function set input
115:23 - and that prepares it for
115:26 - for inference and then this very next
115:27 - line a net dot forward
115:30 - makes a forward pass through the network
115:32 - and is performing inference on this
115:35 - representation of our input image
115:38 - and then for some number of detections
115:40 - returned by the inference we're going to
115:42 - loop over all those detections and
115:45 - right here we're going to determine if
115:46 - the confidence for particular detection
115:48 - exceeds the detection threshold and if
115:51 - it does we'll proceed further and
115:54 - query the detections
115:56 - list for the bounding box coordinates of
115:58 - that particular detection
116:00 - and then the rest of the code here is um
116:03 - going to render a bounding box
116:06 - rectangle for the detection on the image
116:08 - frame right here and then we're also
116:10 - going to build a text string that
116:12 - indicates the confidence level for the
116:14 - detection
116:15 - and
116:16 - annotate the image frame
116:19 - using opencv rectangle and put text
116:21 - functions right here
116:23 - and then once we're done processing all
116:24 - the detections
116:26 - we'll finally call this get performance
116:28 - profile function which is going to
116:30 - return for us the time required to
116:33 - perform inference
116:34 - and we're going to convert that to
116:35 - milliseconds and then build another text
116:37 - string here and continue to annotate the
116:40 - frame with the amount of time that it
116:41 - took to perform the inference and then
116:44 - finally we're going to use im show
116:46 - to display that annotated frame to the
116:49 - output window
116:50 - so that's all there is there isn't much
116:52 - code required to perform inference on
116:54 - the model and in fact most of this code
116:56 - here is related to annotating the frame
116:59 - itself
117:00 - so at this point we're ready to go ahead
117:01 - and execute the script and when we do
117:03 - that we'll cycle through some
117:04 - demonstrations and see just how this
117:06 - performs
117:08 - so here you can see the model is
117:10 - detecting my face uh
117:13 - very nicely and i can um obscure my face
117:16 - a little bit with my hand and it still
117:17 - does a nice job
117:19 - of detecting my face
117:21 - the reason i like using the video stream
117:23 - is that it's just a lot of fun to
117:24 - experiment with that you can hold up
117:26 - images to the camera and
117:28 - experiment with the uh scale and
117:30 - orientation of the images in real time
117:33 - and so we're going to do that i've got a
117:35 - magazine here that i found a lot of
117:36 - interesting images in and so we'll cycle
117:39 - through that and we'll see what you
117:40 - think so i'll scooch out of the way here
117:41 - and we'll get started in just a second
117:48 - so in this first image here you can see
117:50 - the boy's face is in a downward pose and
117:52 - also his bangs are obscuring his
117:54 - forehead and even a portion of his eyes
117:57 - yet the model still performs nicely and
118:00 - detecting his face
118:01 - so we thought we'd start with this image
118:02 - and then progress to some that are a
118:04 - little bit more difficult
118:08 - in this next image here you can see the
118:10 - young girl's face is also in a downward
118:12 - pose but is also a profile view
118:15 - and then of course she's wearing eye
118:16 - glasses which may present an additional
118:18 - challenge yet the model still performs
118:21 - nicely
118:24 - these next couple of images have a
118:26 - mixture of the face as well as some
118:30 - graphics mixed in so kind of a mixed
118:32 - media obscuration of the face if you
118:35 - will and the model does very nicely on
118:37 - this one
118:38 - and here's another example on the
118:40 - opposing page
118:42 - and in this case notice the
118:44 - the different scales of the images that
118:45 - are being detected but the model still
118:48 - is handling things very nicely
118:50 - so this next one coming up is my
118:52 - favorite
118:54 - primarily because it's the most
118:55 - impressive so take a look at this
119:01 - as you can see the woman's face is
119:03 - heavily occluded and there's even been
119:04 - some manipulation of the image in the
119:07 - area around her eyes and also
119:09 - around her chin and mouth almost a
119:11 - blurring to some extent so those both
119:14 - represent significant challenges yet the
119:16 - model is able to detect her face fairly
119:19 - well
119:20 - and we hope this gets you really excited
119:22 - about computer vision and especially
119:24 - deep neural networks
119:25 - just remember that you don't have to
119:27 - train your own models you can use a
119:29 - pre-trained model like we've done here
119:30 - in this demonstration and write just a
119:32 - small amount of code to do your own
119:34 - testing with your own images so that's
119:36 - all we wanted to cover and we encourage
119:38 - you to do that and i will see you next
119:39 - time
119:42 - in this section we're going to describe
119:43 - how to perform deep learning based
119:45 - object detection
119:46 - and specifically we'll be using a neural
119:48 - network called single shot multi-box
119:50 - detection trained using tensorflow
119:53 - and like previous videos we're going to
119:54 - be using opencv to both read the model
119:56 - and perform inference on some sample
119:58 - test images
120:00 - if you look at this name here it says
120:01 - ssd which stands for single shot
120:03 - multi-box detection
120:05 - the single shot refers to the fact that
120:07 - we're going to make a single forward
120:08 - pass through the network to perform
120:10 - inference and yet detect multiple
120:11 - objects within an image and like other
120:13 - types of networks ssd models can be
120:15 - trained with different architectural
120:17 - backbones which essentially means you
120:19 - can model a single concept yet use
120:21 - different backbones depending on your
120:23 - application
120:24 - so in this case we're using a mobile net
120:26 - architecture which is a smaller model
120:27 - intended for mobile devices
120:29 - but before we get started i wanted to
120:31 - point out this resource here
120:33 - there's a tensorflow object detection
120:35 - model zoo at this url and if you go to
120:37 - that
120:38 - repository you can download a variety of
120:41 - different object detection models
120:43 - so we just wanted you to be aware of
120:44 - that
120:45 - in this particular case we're going to
120:46 - be using the
120:48 - ssd mobilenet v2 coco 2018
120:52 - archive listed here and if you extract
120:54 - that archive you'll see it has a
120:55 - structure like the one shown here and we
120:57 - simply wanted to point out that you only
120:59 - need one file from that archive and that
121:01 - would be the frozen inference graph
121:03 - right here which is the weights file
121:05 - for the model
121:07 - and there's actually two other files
121:08 - that we'll need to have in order to run
121:10 - this notebook so let's scroll down and
121:12 - take a look at those as well
121:15 - so right here we're specifying the three
121:17 - models that are required the frozen
121:19 - inference graph which we just described
121:21 - above and then there's a configuration
121:23 - file for the network that's indicated
121:25 - here with the dot pb text extension and
121:28 - then also the class labels for the data
121:31 - set that was used to train this model
121:32 - which is the coco 2018 data set
121:35 - you can actually google that coco data
121:37 - set and retrieve this
121:38 - class labels file from numerous places
121:41 - on the internet but in terms of this
121:43 - configuration file there's actually a
121:45 - script that you can use to generate this
121:47 - file from the frozen inference graph and
121:49 - that script is indicated right here
121:52 - uh we've already executed this notebook
121:54 - and so we have all these files locally
121:56 - in our system but we just wanted to
121:57 - review with you how to obtain uh each of
121:59 - these three files
122:01 - and then one thing that's worth pointing
122:02 - out at this point is take a look at the
122:05 - class labels for this file that we
122:07 - printed out down here in the lower
122:08 - portion of the screen
122:10 - notice the difference between a deep
122:12 - learning object detector and a
122:13 - traditional computer vision object
122:15 - detector we used to have a detector for
122:17 - every class so for example we had a face
122:18 - detector and a person detector and so on
122:21 - and those were all separate models but
122:23 - with deep learning models we have
122:24 - enormous capacity to learn so a single
122:26 - model can detect multiple objects over a
122:28 - wide range of aspect angles and scales
122:31 - which is the real beauty of deep
122:32 - learning so let's scroll down here a
122:34 - little bit further to the next section
122:36 - of the notebook
122:38 - so summarize here are the three steps
122:40 - that need to be performed first loading
122:42 - both the model and the input image into
122:44 - memory and then detecting objects using
122:46 - a forward pass through the network and
122:48 - then finally displaying the detected
122:51 - objects with bounding boxes and class
122:52 - labels and so the first step is
122:55 - indicated here where we're calling the
122:56 - opencv function read net from tensorflow
122:59 - and that takes as input a model file and
123:01 - the configuration file both of which we
123:03 - specified above
123:05 - and then that's going to return for us
123:06 - an instance of the network here which
123:09 - we'll use further below to perform
123:10 - inference
123:11 - next here we're defining a convenience
123:13 - function called detect objects and it
123:15 - takes as input the network instance and
123:17 - then the test image
123:19 - and then we've seen this before here
123:21 - there's another opencv function called
123:23 - blob from image
123:25 - and this takes as input the test image
123:27 - and then several other arguments that
123:28 - are related to pre-processing uh the
123:31 - test image i recall that when we prepare
123:34 - an image for inference we need to
123:36 - perform any pre-processing on that file
123:39 - that was performed on the training set
123:41 - and so this function contains several
123:43 - arguments related to the required
123:45 - pre-processing this first argument here
123:47 - is a scale factor and it's set to one uh
123:49 - which indicates that the training set
123:51 - didn't have any special
123:53 - scaling performed on it
123:55 - then here we're indicating the size of
123:56 - the training images and we're indicating
123:58 - that right here with the 300 so the test
124:01 - image will need to be reshaped according
124:03 - to this size
124:05 - and then the next argument is this mean
124:07 - value
124:08 - if the training images had um had a mean
124:10 - subtracted value applied to them then
124:12 - this would have been some other vector
124:14 - but since um those images don't require
124:16 - any mean subtraction we're simply
124:17 - indicating zeros here and this next
124:19 - argument here uh swap rb
124:21 - for whether or not we want to swap the
124:23 - red and the blue channels and then in
124:25 - this case we do want to do that since
124:27 - the training images used a different
124:28 - convention
124:29 - than what's used by opencv
124:32 - and then finally
124:33 - this crop flag is set to fault so that
124:35 - means that the images are simply going
124:37 - to be resized as opposed to cropping
124:38 - them to the right size
124:40 - and then this function returns for us a
124:42 - blob representation of that image that's
124:44 - been pre-processed so there's a
124:46 - pre-processing step and then there's
124:47 - also a
124:48 - format conversion step if you will
124:51 - and then this blob representation of the
124:53 - image is passed to the set input
124:55 - method to prepare the image for uh
124:58 - inference and then finally we perform
125:00 - inference on the test image by calling
125:02 - the forward method and that returns for
125:04 - us some number of objects that have been
125:07 - detected and then we'll return that from
125:08 - this function
125:10 - so there's a couple more convenience
125:11 - functions down here below so let's take
125:13 - a look at those
125:16 - this one here display text takes in the
125:19 - test image frame and then a text string
125:21 - and some coordinates so this is a
125:23 - function that will simply annotate a
125:25 - bounding box with the class label by
125:27 - drawing a black rectangle here and then
125:30 - annotating the frame with some text
125:32 - indicating the class label inside the
125:35 - black rectangle
125:36 - and then finally there's this display
125:38 - objects
125:40 - function and then it also takes in the
125:42 - test image and then a list of objects
125:44 - that were detected and then the
125:45 - threshold for detection here and here
125:49 - we're retrieving the shape of the input
125:51 - test image
125:52 - and then we're going to loop over all
125:54 - the objects that were detected by the
125:55 - network and retrieve their class ids and
125:58 - their scores
125:59 - and in this next section here we're
126:01 - further going to retrieve the
126:03 - coordinates for the bounding box of that
126:05 - object
126:06 - and convert those coordinates to the
126:08 - original test image coordinates
126:11 - and then finally if the score
126:13 - for this object is greater than our
126:14 - input threshold then we'll go ahead and
126:16 - annotate the frame
126:18 - with the class label i'm calling that
126:20 - display text function we just described
126:22 - above
126:23 - and then finally we're going to render
126:25 - the test image frame
126:26 - with the bounding box rectangle in white
126:29 - right here
126:30 - so let's take a look at some results
126:35 - so you can see here we're reading in a
126:37 - test image
126:38 - and now we're going to
126:40 - use the function we created above detect
126:42 - objects passing at the network instance
126:44 - here
126:45 - and the image we just read in here
126:47 - and the return from that function is a
126:49 - list of objects that have been detected
126:51 - and then we're going to call the display
126:53 - objects function passing in the test
126:56 - image and the array of objects and you
126:58 - can see the result down here there's all
127:00 - kinds of objects being detected there's
127:02 - a person here
127:04 - there's a bicycle here there's a car
127:05 - here
127:06 - there's cars off in the distance and
127:08 - even way out in the distance here you
127:09 - can see a traffic light's been detected
127:11 - so this is a very robust object
127:13 - detection algorithm has about 80 classes
127:17 - and let's take a look at another example
127:19 - down here below
127:21 - this is a sports scene so you can see
127:24 - the same sequence here reading in the
127:25 - image
127:26 - calling detect objects and then display
127:28 - objects
127:29 - and in this case we're getting uh both
127:31 - people in the image the bat
127:33 - the baseball glove which is really nice
127:35 - and then uh baseball but notice that the
127:38 - baseball actually has a false positive
127:40 - there's only one baseball there yet the
127:42 - detection algorithm is
127:45 - uh reporting two so there's a false
127:47 - positive there but other than that it's
127:49 - done a very nice job and let's let's
127:51 - take a look at one more example
127:56 - so here's another sports scene here and
127:58 - you can see it's detecting the soccer
127:59 - player here
128:00 - the soccer ball here
128:02 - and then there's a false positive here
128:04 - it thinks that the tip of his shoe is
128:06 - actually another sports ball
128:08 - and one thing we can do in cases like
128:10 - this is after you've established some
128:12 - number of false positives you can
128:14 - actually take these image examples
128:16 - and perform what's called hard negative
128:18 - mining by training the network with
128:20 - additional examples like this
128:23 - to reduce the number of false positives
128:25 - so we hope that's a nice introduction
128:26 - for you to object detection and that's
128:28 - all we wanted to cover in this section
128:30 - and we'll see you next time
128:33 - in this section we're going to show you
128:34 - how you can perform 2d human pose
128:37 - estimation on your own images and video
128:39 - by using a pre-trained model called open
128:41 - pose
128:42 - open pose was developed at the carnegie
128:44 - mellon perceptual computing lab and if
128:47 - you're not familiar with the pose
128:49 - estimation the figure below from the
128:50 - open pose research paper provides a nice
128:53 - graphic so essentially the problem
128:55 - requires taking an input image that may
128:57 - contain one or more people and then
128:58 - identifying the key points associated
129:00 - with the major joints in the human
129:02 - anatomy and then logically connecting
129:04 - those key points as shown in the figure
129:06 - on the right hand side here
129:08 - the model actually produces two types of
129:10 - output
129:12 - part confidence maps and part affinity
129:14 - maps however for the code demonstration
129:16 - below will only be using a single person
129:18 - in the image and therefore we'll only
129:20 - need to make use of the confidence maps
129:23 - which are also referred to as
129:25 - probability maps
129:26 - and we'll see how that's done further
129:28 - below
129:29 - so just a brief history for a long time
129:31 - human pose estimation was a very
129:32 - difficult problem to solve robustly
129:35 - especially on some of the more
129:36 - challenging benchmark cases
129:38 - the reason the problem can be hard is
129:40 - that joints are not always very visible
129:42 - there are numerous opportunities for
129:44 - occlusions of one type or another and
129:46 - then clothing or other objects can
129:48 - further obscure the image and then
129:49 - there's the added complexity of not only
129:51 - identifying key points but associating
129:53 - them with the right people if there's
129:55 - multiple people in the image
129:57 - however once deep learning was applied
129:58 - to this problem domain just a few years
130:00 - ago we really began to see dramatic
130:02 - improvements and it's been really
130:04 - exciting to see just how well these
130:05 - models now perform
130:07 - so in this demo we're going to be using
130:08 - the open pose cafe model that was
130:10 - trained on the multi-purpose image data
130:12 - set
130:13 - and we'll be doing that using a single
130:15 - image which we'll get to in just a
130:16 - minute but we wanted to point out that
130:18 - human pose estimation is often applied
130:20 - to video streams
130:21 - for various applications such as
130:23 - intelligent trainers for example so we
130:25 - wanted to just start with some example
130:27 - results on a video clip to whet your
130:28 - appetite and then we'll walk through the
130:30 - code for a single image implementation
130:32 - but just remember that the code can
130:33 - easily be adapted to process a video
130:36 - stream as we've shown in prior videos
130:38 - so just scrolling down here a little bit
130:40 - to this first example this video clip
130:42 - which we're going to show
130:44 - was processed using open pose and the
130:47 - open pose results have been overlaid on
130:49 - the video stream so let's take a look
130:52 - so all three hockey players are wearing
130:54 - bulky uniforms which is a challenge and
130:56 - then they're also including each other
130:58 - to some extent yet the model is
131:00 - performing uh pretty nicely if you just
131:02 - take a look at the results
131:04 - it's a lot of fun to work with and
131:07 - of course we'll be taking a look at a
131:08 - single image example but we thought it
131:10 - was instructive to show you
131:12 - how exciting that is to process videos
131:15 - so let's continue on and take a look at
131:17 - the rest of the notebook so in this
131:19 - first section here we're simply
131:21 - specifying the model
131:23 - here is the prototex file here and then
131:25 - the
131:26 - cafe model or the weights file right
131:28 - here
131:29 - we downloaded those already and have
131:31 - already executed this notebook but there
131:33 - are references in this notebook here for
131:35 - where you can download these files
131:38 - and then in this next section here we're
131:40 - specifying the number of points in the
131:41 - model
131:42 - and the associated uh linkage pairs here
131:45 - by their indices
131:47 - so these each of these
131:49 - blocks here refers to a linkage in the
131:52 - human anatomy
131:53 - and
131:54 - zero starts at the head one is the neck
131:56 - two is the right shoulder three is the
131:58 - right elbow and so forth so this is a
132:00 - mapping that the model uses during
132:02 - training and we're gonna need this
132:04 - mapping to process the output from the
132:06 - network
132:07 - further below
132:08 - and then right here on this line we're
132:11 - calling the read net from cafe we've
132:13 - seen that before in a previous video
132:15 - where we just pass in the prototex file
132:18 - and the weights file for the trained
132:19 - network
132:20 - and that creates for us uh instance of
132:23 - the network and we'll use that below for
132:25 - inference
132:26 - so now we're ready to read in our test
132:27 - image and we're doing that right here uh
132:30 - in this code block with imread and then
132:32 - we're also swapping the red and blue
132:34 - color channels here on the next line
132:37 - and then these two lines are retrieving
132:38 - the size of the image which we'll use
132:40 - further below
132:41 - so let's take a look at the image this
132:43 - is a picture of tiger woods hitting a
132:45 - driver
132:46 - from the rear view at the top of his
132:48 - backswing
132:50 - and the reason i chose this image is
132:51 - because it's a little bit challenging
132:53 - and makes a nice example
132:55 - his upper body notice is at right angles
132:58 - to his lower body so his lower body is
132:59 - facing to the right of the camera and
133:01 - his upper body is actually facing the
133:03 - camera
133:04 - and then his left arm is occluding his
133:06 - right shoulder so that's going to make
133:08 - things um a little more complicated
133:12 - and let's continue on uh to the next
133:15 - section
133:16 - so now we're at the point uh where we're
133:18 - ready to go ahead and pre-process our
133:20 - image i recall that when networks are
133:23 - trained they're trained um with training
133:26 - images that have a specific size and
133:30 - potentially some scaling performed on
133:32 - them and we need to make sure that
133:33 - whatever images we're using to perform
133:35 - inference on uh are pre-processed in the
133:38 - same way
133:39 - so here we're setting the net input size
133:42 - of 368 by 368
133:44 - and then we're calling the uh
133:47 - opencv function blob from image and
133:49 - recall from a previous video that this
133:51 - takes several arguments related to all
133:53 - this preprocessing
133:54 - and then it's also going to convert the
133:57 - image into a blob representation which
134:00 - will pass into this set input function
134:02 - uh to prepare the network for inference
134:06 - so let's review these arguments uh
134:07 - briefly so this first argument is the
134:10 - image itself and then the second
134:12 - argument is a scaling factor
134:15 - which is the same scaling factor that
134:16 - was applied to the training images so we
134:18 - need to perform that same transformation
134:20 - here on the input image
134:22 - and then here we're just indicating the
134:24 - net input size which we just
134:26 - talked about right here above 368 by
134:28 - 368.
134:30 - uh the there was no mean value
134:32 - subtracted from the um training images
134:34 - so we're simply uh indicating a vector
134:36 - of zeros here
134:38 - and then the swap red blue uh flag here
134:42 - set to true
134:43 - and uh we're not cropping we're going to
134:46 - resize our input image uh to match uh
134:50 - the size of the images that were used
134:51 - during training which was 368 by 368.
134:55 - so now we're ready to use the model to
134:57 - perform inference on our test image and
134:59 - we do that right here by calling the
135:00 - forward method
135:02 - and that returns for us the output from
135:04 - the network which consists of both
135:06 - confidence maps and affinity fields
135:09 - and as we mentioned earlier we're only
135:10 - going to be using the confidence maps
135:13 - for performing the
135:15 - key point detection in this
135:16 - demonstration and so for each point
135:19 - we're going to
135:20 - receive a probability map
135:22 - and then we're simply going to
135:24 - in this next two lines of code plot each
135:26 - of these probability maps and you'll see
135:28 - that these are color coded they're heat
135:30 - maps indicating the probability
135:32 - of the location of the detected key
135:35 - point and so red is a very high
135:37 - probability so in each of these uh
135:39 - probability maps you'll see this is the
135:41 - likely location for key point zero key
135:44 - point one key point two and so forth so
135:46 - remember this one corresponds to the
135:48 - head this corresponds to the neck this
135:50 - corresponds to the right shoulder and so
135:52 - forth
135:53 - so we can use these probability maps to
135:56 - overlay those key points
135:58 - on the original image and to do that
136:01 - we're going to have to scale these in
136:03 - the same scale as the input image and so
136:06 - that's
136:06 - what this next block of code is
136:08 - performing
136:10 - so right here we're using the uh
136:12 - output shape of the network in other
136:14 - words the shape of the probability maps
136:16 - and also the input shape of the test
136:18 - image to compute two scale factors
136:21 - x and y
136:23 - that we'll end up using below to
136:25 - determine the location of the key points
136:27 - in the actual test image
136:29 - but before we do that we're going to
136:30 - need to determine the location of the
136:32 - key points in the probability maps
136:35 - so that's what we're going to do in this
136:36 - next code block here this for loop is
136:39 - looping over all the key points
136:41 - and for each key point we're going to
136:42 - retrieve the probability map from the
136:45 - output array from the network and then
136:47 - we're going to call this opencv function
136:49 - min max location
136:52 - and pass it the probability map and this
136:54 - is going to return for us
136:56 - the
136:57 - location of the point associated with
136:59 - the maximum probability
137:02 - and
137:03 - so the coordinates of the point are in
137:05 - this variable here point
137:07 - and then once we have that location in
137:09 - the um
137:10 - probability map coordinates we're going
137:12 - to multiply it by the x and y scale
137:14 - factors we computed above
137:16 - to get the
137:18 - key point location in the original test
137:20 - image and then if the probability
137:23 - returned by this function is greater
137:25 - than some minimum threshold which we set
137:27 - above
137:28 - then we're going to go ahead and
137:31 - take that x y point now in the
137:34 - coordinates of the test image and then
137:35 - append it to a list of points
137:38 - and so now we're ready to
137:40 - render those points on the test image so
137:43 - let's scroll down here to our results
137:46 - first let's just take a look at the
137:47 - image so this is um
137:50 - the input image here with all the key
137:52 - points annotated
137:54 - on the frame and then the image to the
137:56 - right is um
137:58 - the same key points but without the
137:59 - numbers but with the linkages connected
138:01 - so two different views of the same data
138:03 - really
138:04 - and um if you take a look at this area
138:06 - that we knew was going to be difficult
138:07 - in other words the head was at zero the
138:09 - neck was at one
138:11 - the right shoulder at two the right
138:13 - elbow at three the right wrist at four
138:15 - it looks really nice even though the
138:17 - left arm is occluding the um right
138:19 - shoulder
138:20 - and uh i mean if you go back over here
138:22 - to the image on the right you can see
138:24 - the skeletal view and that looks pretty
138:26 - um spot on so it did a nice job of
138:30 - detecting the key points and and putting
138:32 - them together in a way that makes sense
138:34 - but let's just go ahead and walk through
138:36 - this code a little bit so up here on
138:38 - these first two lines we're just making
138:40 - a copy of the input image and one's
138:42 - gonna be um called points and the other
138:43 - one's gonna be called skeleton
138:45 - and then we're gonna loop over all the
138:47 - points
138:48 - that were
138:50 - that we just created in the for loop
138:51 - above
138:52 - and those are the coordinates of the key
138:54 - points in the test image coordinate
138:56 - frame and then we're going to use the
138:58 - opencv circle and put text functions
139:01 - to
139:02 - draw and label those points on the im
139:04 - points image which was the um
139:07 - image to the left down here
139:09 - and then further we're going to render
139:11 - the skeleton view uh that's displayed
139:13 - down here to the right with this for
139:15 - loop so we're looping over all the pose
139:17 - pairs which we defined further up in the
139:19 - notebook
139:21 - and then we're retrieving those pairs
139:22 - and we're going to set those to part a
139:24 - and part b here and then use those as
139:27 - indices into the points list that we
139:29 - created up above
139:31 - which contains the list of key point
139:33 - locations in the test image and now
139:35 - we're simply going to use opencv line
139:38 - and circle functions to
139:40 - draw a line from
139:42 - one joint to the next
139:44 - and color code it and then also draw a
139:46 - circle at the first
139:48 - key point in that link
139:50 - so and then here we're just
139:52 - using im show to display both these
139:54 - images below
139:55 - so that's all there is to it there
139:56 - wasn't much code in this notebook
139:58 - since we're leveraging the capability of
140:00 - opencv to perform inference for us
140:02 - really the code amounts to a few
140:04 - function calls
140:06 - and then a little bit of
140:08 - a logic to parse the outputs and and
140:11 - render the information on the original
140:14 - image
140:15 - and just for fun i went ahead and
140:18 - ran the same model on a photo of my son
140:22 - who also plays golf
140:24 - and this is a view of his follow through
140:27 - and
140:28 - the model does a pretty good job but
140:30 - i'll have you notice one thing here is
140:31 - that the um
140:33 - if you look over here to the right the
140:34 - neck and the right shoulder are off a
140:37 - little bit for some reason so uh this is
140:39 - the head this is point zero this is
140:41 - point one which should be right here and
140:44 - this is the right shoulder which should
140:46 - be over here but notice his right
140:47 - shoulder is actually
140:49 - occluded uh quite a bit by his back so
140:52 - it's almost not even visible so it's
140:54 - it's definitely a challenging pose
140:57 - and
140:59 - you know it was a lot of fun to try this
141:00 - out so
141:01 - the main point though is that you can
141:04 - use a pre-trained model
141:05 - leverage uh the inference capabilities
141:07 - of opencv
141:09 - and start playing around with your own
141:10 - images and video
141:12 - and uh
141:13 - we think you'll enjoy doing this and
141:15 - thanks so much and that's all we wanted
141:16 - to cover in this video and we'll see you
141:18 - next time well thanks everybody i hope
141:21 - you enjoyed the course that we just
141:23 - covered in the getting started series in
141:24 - computer vision we covered a lot of
141:26 - ground and a lot of material uh at a
141:29 - pretty good level for a getting started
141:30 - series and i think this is a good
141:32 - opportunity for us to talk with
141:35 - dr satya malik who's the ceo of
141:37 - opencv.org
141:39 - to get his uh take on how do you get a
141:41 - job in computer vision for example and
141:43 - some of the other course offerings uh
141:46 - that we offer on opencv.org
141:49 - thanks a lot bill uh it's a pleasure to
141:51 - talk to
141:52 - this audience because they have just
141:54 - completed their first steps in opencv
141:57 - and i can understand you know uh it's
142:00 - very exciting it's a very exciting field
142:02 - and the next thing uh people ask is how
142:05 - do you get a job in computer vision and
142:07 - ai
142:08 - and the path is uh you know you have to
142:10 - dedicate yourself you have to commit
142:12 - yourselves to learning uh various
142:14 - aspects of computer vision
142:16 - but there is a path through which you
142:18 - commit to that path you will find a job
142:21 - at the end of this path
142:23 - and as you know you know bill your own
142:26 - journey is one great example of that you
142:29 - started your journey in aeronautics and
142:32 - astronautics you did
142:33 - your masters from
142:35 - mit in that area and for the longest
142:37 - time you were in that area
142:39 - but then you made a switch because of
142:42 - your in your job you wanted to use ai
142:44 - but then gradually you made a switch to
142:47 - computer vision and ai
142:49 - i would you know i would in fact love to
142:50 - hear that we can start with that little
142:52 - story yeah sure i mean i'm excited to
142:54 - talk about that so i did i worked in the
142:57 - space and defense industry for many
142:58 - years
142:59 - uh very rewarding experience i loved the
143:02 - idea of space travel when i was a young
143:05 - child and ended up graduating with a
143:07 - degree in aeronautics and astronautics
143:09 - and
143:10 - it was a wonderful experience for me
143:12 - along the way i had the opportunity to
143:14 - work on a project that involved
143:16 - something completely unrelated to what i
143:18 - was doing i was working
143:19 - uh managed doing uh performing technical
143:21 - management on a project related to
143:23 - machine learning uh at the time and this
143:25 - was about eight or nine years ago
143:27 - and uh it's really uh exciting for me to
143:29 - see just how
143:30 - um stunning the results were to some of
143:33 - these use cases that we were working on
143:35 - it was a small research project and
143:37 - shortly after that project i uh began a
143:40 - journey on on expanding my continued
143:43 - education i studied machine learning and
143:45 - also image processing and as i moved
143:48 - forward with that i really uh felt a
143:50 - strong passion for working in the field
143:52 - of computer vision so i spent quite a
143:54 - bit of time uh taking extra classes
143:56 - while working full-time
143:58 - early morning study sessions and uh lots
144:00 - of weekends uh spent uh learning all
144:03 - this material
144:04 - and uh i'm happy to say that i've landed
144:06 - in a very nice place and i'm working in
144:08 - the field full time now
144:10 - and uh i really enjoy it thank you
144:13 - yeah so that that's you know uh and you
144:15 - took uh deep learning with pytorch which
144:17 - is one of our courses uh as well right
144:20 - right um so you know so what i was
144:23 - saying is that there is a path it's not
144:25 - necessarily easy especially if you are
144:28 - working full-time you have to dedicate
144:30 - your nights and weekends but there is
144:32 - definitely a path for people who are
144:34 - interested and if you're a student there
144:36 - is a very clear path right it's an a
144:38 - relatively easy path
144:40 - one one thing i like to say people is
144:43 - that if you want to learn about physics
144:46 - you have like 200 years of history in
144:49 - physics so you have to learn all that
144:51 - material to get started in physics right
144:53 - before you can contribute uh in physics
144:55 - but for
144:57 - computer vision it's a relatively new
144:59 - field computer vision you can say that
145:01 - even though research started in 1960s
145:05 - it's really now that
145:08 - algorithms that work in the real world
145:10 - are uh are available to the general
145:13 - public so you're literally talking about
145:15 - a decade uh of uh worth of techniques
145:18 - which is not
145:19 - you know which is not a lot frankly
145:21 - so
145:22 - for people who are trying to start in
145:24 - this area i would say that uh you know
145:27 - set aside and especially people who are
145:29 - not doing it full-time they should set
145:31 - aside anywhere between six months to a
145:34 - year
145:35 - to learn the material um to get you know
145:38 - completely uh and that's that's hard
145:40 - work right six months to a year of hard
145:43 - work nights and weekends
145:45 - to uh get their foothold in this field
145:47 - where you can think about making a
145:49 - career switch
145:51 - you know there are other people in our
145:52 - courses who were able to make a switch
145:55 - by taking just one course but i usually
145:58 - recommend that you know
146:00 - you need to be conver you need to
146:02 - understand traditional computer vision
146:04 - algorithms and you also need to learn
146:06 - deep learning algorithms
146:08 - so uh you need to have a flavor of both
146:10 - of those things let me actually start by
146:13 - explaining
146:14 - where uh what are the various aspects of
146:17 - ai so that we lay out the land and then
146:20 - it will be easy for beginners to
146:21 - understand what are the various topics
146:23 - we are talking about
146:25 - now uh the first question you know
146:27 - people ask is what is artificial
146:29 - intelligence
146:30 - so it's a very fuzzy term it doesn't uh
146:33 - it doesn't refer to a specific technique
146:35 - but in general whenever we try to make
146:37 - machines uh think like humans we call it
146:40 - artificial intelligence and there are
146:42 - various ways of solving this problem
146:44 - you can think about a rule-based system
146:46 - where you're encoding all the rules
146:48 - uh like in a chess game you could say
146:50 - that oh if you do this i'm going to
146:52 - do this step right
146:54 - and that used to be very popular uh a
146:56 - few decades back but then people
146:58 - realized that uh you can actually train
147:01 - the machine by giving it data not
147:03 - explicitly telling it you know what the
147:06 - rules are but it will automatically
147:08 - figure out things based on data so
147:10 - that's machine learning machine learning
147:12 - is a subset of ai techniques where we
147:15 - care about data
147:18 - the other part is deep learning you
147:19 - might you might have heard a lot about
147:21 - this new uh kind of technique called
147:24 - deep learning which is nothing but
147:26 - solving machine learning problems using
147:29 - a deep neural networks and you know in
147:31 - our course we tell you why it is called
147:33 - deep it's a very subtle uh
147:35 - thing why it is called deep neural
147:37 - networks
147:38 - but it is basically solving ai problems
147:41 - using neural networks so that's deep
147:43 - learning computer vision is actually
147:45 - there is an overlap of computer vision
147:48 - deep learning etc computer vision
147:50 - basically means the analysis
147:53 - of
147:54 - images and videos
147:55 - and it is different from image
147:57 - processing because in image processing
147:59 - you have an input image and the output
148:02 - is also an image uh you could be
148:04 - encoding an image you could be uh you
148:07 - know enhancing an image etc so that all
148:09 - is under uh image processing in computer
148:12 - vision usually we have an input image
148:15 - and the output is
148:18 - is information that we want so the image
148:20 - could be this video session
148:23 - but the output could be
148:24 - the faces that we detected right so it
148:27 - is image in and information out
148:31 - not only that in computer vision we also
148:33 - handle many things that have nothing to
148:35 - do with artificial intelligence for
148:37 - example in this course uh or in this
148:40 - video series you learned how to create
148:41 - panoramas that is a classical computer
148:44 - vision technique and it has nothing to
148:46 - do with artificial intelligence because
148:49 - you're stitching images together you're
148:50 - using the geometry of image formation
148:53 - you're using all these techniques which
148:55 - are not really machine learning or ai
148:58 - right but they are still very useful
149:00 - techniques so there is an overlap
149:02 - between
149:03 - artificial intelligence and computer
149:05 - vision but computer vision has
149:08 - does a lot more other things also for
149:10 - example the whole field of 3d computer
149:12 - vision
149:13 - uh
149:15 - there used to be very little ai in that
149:17 - now even ai is being used to enhance uh
149:20 - those fields also so that's uh the
149:23 - general lay of the land
149:26 - so from the artificial intelligence
149:28 - standpoint uh or the machine uh computer
149:30 - vision is like machine learning for
149:32 - images is one way i like to think about
149:34 - that component of it and also image
149:37 - understanding is another um
149:39 - way to think about uh computer vision or
149:42 - the the portion of it that's uh
149:44 - associated with machine learning and
149:45 - artificial intelligence yeah and there
149:47 - are other fields of ai for example
149:50 - natural language processing that's
149:52 - another field
149:53 - where you deal with text data
149:57 - we also have speech recognition and
150:00 - that's another big field you know for
150:02 - example when you call alexa there is an
150:05 - artificial intelligence module that
150:07 - reads recognizes your voice does the
150:10 - processing interprets it etc so that's
150:12 - the speech processing but
150:15 - among these different fields i think uh
150:17 - computer vision has the biggest
150:20 - potential
150:21 - because if you look at human visual
150:23 - system it it spends about 30 percent of
150:26 - its processing power on the visual part
150:30 - because visual information is so rich
150:32 - and we are also in a lucky spot that
150:33 - there are so many hundreds of millions
150:36 - of cameras uh in this world which are
150:39 - continuously gathering data so there is
150:41 - a lot of uh
150:43 - there's a lot of activity in the
150:44 - computer vision space
150:47 - you know even in aerospace for example
150:50 - uh for military and other applications
150:52 - there are ton of applications
150:56 - i mean everybody's mobile devices right
150:57 - everybody's mobile devices have cameras
150:59 - so there's all kinds of uh
151:01 - video and image processing taking place
151:03 - yeah and it's going to transform
151:05 - multiple industries uh we are going to
151:07 - see in manufacturing
151:09 - we we see we also do consulting work and
151:12 - we see people are using computer vision
151:14 - in manufacturing and agriculture
151:17 - in security obviously in autonomous
151:19 - driving that's a very big area so
151:22 - computer vision is everywhere and that's
151:24 - going to explode it's already on the
151:26 - rise there are so many jobs out there
151:28 - and we will
151:30 - we will show you
151:32 - right there
151:33 - in the video we will show you the
151:36 - pay grades of people who get jobs in
151:38 - this area
151:40 - you can see that people make more than a
151:42 - million dollars in uh you know companies
151:45 - like facebook etc of course these are
151:47 - senior people and these are level six
151:50 - engineers who
151:51 - uh you know who are accomplished they
151:53 - are at the cutting edge but it gives you
151:55 - a sense of an engineer is making this is
151:57 - not this is not like uh entrepreneur or
152:00 - this is not like a senior manager this
152:02 - is an ai engineer making that kind of uh
152:05 - money in big companies
152:07 - so that gives you a sense of you know
152:10 - why it is worthwhile taking a very hard
152:12 - look at these emerging fields as a
152:15 - career option so what about some of the
152:17 - the course offerings you want to talk
152:19 - about uh
152:20 - some of those and
152:22 - yeah sure so
152:23 - basically uh before even
152:26 - we go there right
152:28 - it's instructive to know what are the
152:30 - things that you need to learn
152:31 - to get into the field and what are the
152:34 - libraries you need to learn
152:36 - uh so the very first thing is that it's
152:38 - very important to be very good at uh at
152:42 - coding uh some of these uh
152:45 - algorithms right algorithms or building
152:47 - systems so you should be a very good
152:49 - programmer first because this is a very
152:51 - engineering oriented field you need to
152:54 - be able to write code and python is a
152:57 - great uh place to start but don't end
152:59 - there right take learn as much as
153:02 - possible if you want to learn c plus
153:03 - plus
153:04 - if you need to learn c plus plus learn
153:06 - it and because it expands your chances
153:09 - of uh getting a job
153:11 - so uh but let's start with python right
153:14 - you have suppose you have python
153:15 - expertise what do you need to learn to
153:18 - get a job first of all as i said that
153:21 - opencv is a fundamental library you have
153:24 - to have good expertise in using opencv
153:27 - that's number one and then there are two
153:29 - other libraries that are very important
153:31 - one is uh
153:33 - and this is a deep learning framework
153:36 - from
153:37 - facebook
153:38 - it's open source it was developed at
153:41 - facebook and uh the second one is
153:43 - tensorflow and kira's so this library is
153:46 - developed by google
153:48 - also open source these are all very good
153:51 - libraries and
153:53 - if you have mastery over these three
153:56 - libraries and you have mastery over
153:58 - computer vision
154:00 - and uh deep learning techniques
154:03 - i think
154:04 - there is
154:06 - no way in the world you will not get a
154:07 - job it's very easy to get a job once you
154:10 - have these uh
154:12 - two or three things under your belt
154:15 - so uh for from our course point of view
154:18 - computer vision one covers traditional
154:20 - computer vision applications uh
154:23 - and not so much deep learning we show
154:25 - you how to use deep learning uh in
154:27 - applications but we don't show you how
154:29 - to train deep learning models
154:31 - computer vision 2 is all about
154:33 - applications and there we
154:36 - we go over many different applications
154:38 - we don't even worry about you know which
154:40 - library you are choosing we expose you
154:42 - to several libraries that will allow you
154:44 - to you know you just build uh your
154:47 - arsenal of techniques and libraries and
154:50 - tools that you can use to build
154:52 - real-world applications
154:53 - and the third course is deep learning
154:55 - with pytorch where you go over the
154:58 - fundamentals of deep learning
155:01 - and
155:03 - we and it starts using pytorch and by
155:05 - the end of this year uh 2021 we will
155:08 - also launch uh deep learning with
155:10 - tensorflow and keras so basically it's
155:13 - all covered anything related to computer
155:16 - vision that involves deep learning it's
155:18 - covered in these courses we go over
155:20 - image classification object detection
155:23 - image segmentation pose estimation etc
155:26 - but these are very meaty courses in the
155:28 - sense that we go over uh all the
155:31 - theoretical details we uh you will learn
155:34 - about you know what is back propagation
155:36 - and things like that
155:37 - so it's
155:39 - once you take these courses i think uh
155:42 - it is equivalent to taking uh being
155:45 - getting a master's in computer vision
155:46 - and machine learning you will get that
155:49 - level of knowledge in fact i can very
155:51 - easily say that by the time i completed
155:53 - my masters
155:54 - i had i did not have this knowledge that
155:57 - people gain by taking computer vision
155:59 - one
156:00 - and deep learning let's say even if you
156:02 - take these two courses computer vision
156:04 - one and deep learning with pytorch
156:07 - i did not have that knowledge after my
156:09 - master's right so uh and i can say that
156:12 - without uh without any hesitation
156:14 - these are solid courses we took the best
156:17 - from
156:18 - uh you know various things we we are
156:20 - also very industry oriented right we are
156:22 - very very applications oriented so we
156:25 - picked the topics that are actually used
156:28 - in the industry and left out the things
156:30 - that are only of theoretical importance
156:33 - right so what about the prerequisites
156:35 - people might be wondering about what
156:36 - what's actually required to get started
156:38 - and maybe what are the
156:39 - potential learning tracks here with uh
156:41 - three or four courses that we offer
156:43 - yeah so the prerequisite is just uh you
156:46 - know
156:48 - intermediate level of knowledge and the
156:50 - python programming language once you
156:52 - have that you don't need any other uh
156:54 - prerequisites but if you're starting you
156:56 - know you have never tried uh opencv
156:58 - you're just starting out i would suggest
157:00 - that take our first course opencv for
157:03 - beginners and that is meant to be a
157:06 - quick you know
157:07 - a very short and fun course it is short
157:10 - it is affordable and it is uh something
157:13 - that you can try in a month
157:15 - uh and that will give you an idea you
157:17 - know do you actually enjoy building
157:19 - applications do you actually enjoy this
157:21 - field of uh computer vision and ai
157:23 - and once you have completed that course
157:25 - and you're certain that you want to
157:27 - invest uh you know time and energy into
157:30 - this field then i would say that uh take
157:33 - computer version one which is
157:36 - uh about classical computer vision we
157:38 - also cover some deep learning but it's
157:40 - very important to have the foundation uh
157:42 - ready a lot of people i see
157:45 - they directly jump into uh deep learning
157:48 - that's also an option
157:49 - but then what happens is that in real
157:51 - world
157:52 - there are many problems that you don't
157:54 - solve using deep learning it's just
157:56 - absurd to use that technique for solving
157:59 - uh some very easy problems in computer
158:02 - vision and if you don't have that
158:04 - foundational stuff you know you will try
158:06 - to look for a nail because you have this
158:08 - deep learning hammer exactly yeah so
158:11 - right so that is a pitfall that people
158:13 - should avoid
158:14 - so uh so computer vision one is the
158:17 - second one uh you can also skip opencv
158:20 - for beginners if you are convinced that
158:22 - oh you want to commit three to four
158:24 - months
158:24 - uh you can directly take computer vision
158:26 - one and then take deep learning with
158:29 - pytorch
158:31 - if you have more time to invest let's
158:33 - say you're a student
158:35 - then
158:36 - you should definitely take the whole you
158:38 - know
158:39 - four courses or at least uh start with
158:42 - computer vision 1 computer vision 2 deep
158:44 - learning with pytorch and then when deep
158:47 - learning with tensorflow and kiras comes
158:49 - along
158:50 - that is also
158:52 - going to be very useful
158:53 - so computer vision
158:55 - um there's uh must be a little bit
158:57 - overlap between computer vision one and
158:58 - computer vision two but computer vision
159:00 - two is more application focused yep uh
159:03 - is it considered a little bit more
159:04 - advanced or what are the prerequisites
159:06 - for that it's not advanced but we don't
159:09 - dive into a lot of theory in that we are
159:12 - more interested in teaching people about
159:14 - uh the tools right for example if we
159:17 - cover something like uh
159:20 - like a barcode or a qr code scanner we
159:23 - will tell you oh this is the library
159:25 - this is the best library to use for this
159:27 - application but we may not go into how
159:29 - qr code is actually read right right so
159:32 - we don't go into that level of detail uh
159:34 - similarly we cover applications related
159:37 - to faces
159:38 - and uh you know face swapping and things
159:40 - like that
159:41 - and
159:42 - in those cases we go over some theory
159:45 - enough for you to understand you know
159:46 - what's what's going on
159:48 - but uh we may not go into the
159:50 - theoretical detail of say uh facial
159:53 - landmark detection which can be
159:55 - mathematically challenging for people
159:57 - right right so
159:59 - yeah so it is based on you build
160:01 - applications right that's the main focus
160:03 - we will teach you how to build web
160:04 - applications for example
160:07 - so you get exposed to a wide variety of
160:09 - applications
160:11 - one thing that i noticed just in my own
160:13 - continued education in this area was
160:15 - that having a little bit of overlap is
160:17 - actually i found it to be a valuable
160:19 - experience because you might cover
160:21 - uh one particular topic in a class and
160:23 - then hit it again in another class maybe
160:26 - from a slightly different perspective or
160:27 - just the the passage of time and you're
160:31 - looking at the at the same topic again
160:32 - you know several months later from a
160:34 - slightly different vantage point i found
160:36 - is a actually very helpful so
160:39 - actually that's true about our deep
160:41 - learning with pytorch and deep learning
160:43 - with tensorflow courses as well
160:46 - you may ask you know uh if i have taken
160:48 - deep learning with pytorch does it make
160:50 - sense for me to take deep learning with
160:52 - tensorflow as well
160:53 - and the short answer is yes absolutely
160:56 - uh you know tensorflow is the most
160:58 - popular
160:58 - deep learning library in the world and
161:00 - pytorch is the one that is rising the
161:03 - fastest
161:04 - um a lot of people like using pytorch
161:06 - because it's very pythonic for for
161:08 - python developers the learning curve is
161:11 - uh very you know it's very gentle you
161:13 - can easily pick up uh pytorch but when
161:17 - you go to the industry right you're
161:19 - looking for a job right you want to
161:21 - offer the best uh things you have
161:25 - you know you cannot say pytorch is the
161:27 - library of my choice i don't work in
161:28 - tensorflow then people are not going to
161:30 - hire you the the people who actually use
161:33 - tensorflow
161:34 - as engineers we should not be married to
161:36 - the tools right we are there to solve
161:38 - problems and if
161:41 - taking this course deep learning with
161:43 - tensorflow the theory is already covered
161:45 - in deep learning with pytorch also so
161:48 - there will be an overlap and there will
161:50 - be a reputation
161:51 - but
161:52 - uh it will be in the context of a new
161:54 - course right so you are actually
161:57 - uh looking at uh
161:59 - from a different framework right
162:00 - tensorflow is a different framework and
162:02 - we have also added we are also adding
162:04 - new applications so that there is uh you
162:07 - get different applications with these uh
162:09 - two courses
162:10 - but with let's say 30 extra effort you
162:14 - learn a new framework right and now
162:16 - you have covered everything with pytorch
162:18 - and tensorflow you are all set in deep
162:21 - learning and you also know the theory
162:23 - from when you take either course we
162:25 - cover the theory that is necessary so i
162:28 - think that uh what you said uh it also
162:31 - is a revision of your theory when you
162:33 - take the second course
162:35 - and um you know people should not be
162:37 - married to tools and i i say this about
162:39 - other
162:42 - python and c plus also
162:44 - uh python is definitely the first and
162:46 - the easiest language uh to get started
162:49 - with uh ai
162:50 - but
162:51 - don't ignore c plus plus
162:54 - completely because when you go to the
162:56 - job market you may find that there are
162:58 - equal number of jobs in two areas right
163:01 - and you've already learned you've done
163:02 - the hard work of learning the basics the
163:04 - foundation
163:05 - and you're now you're just worried about
163:07 - you know i don't like this language or
163:09 - that language is hard language is just a
163:12 - way of solving problems right we should
163:14 - not be married to one particular
163:16 - language because as engineers we try to
163:18 - we want to see ourselves as problem
163:20 - solvers and
163:22 - whatever
163:23 - is the right tool for the problem we'll
163:25 - use that right
163:27 - and two of our courses computer vision
163:29 - one and computer vision two they are
163:32 - offered in both languages in fact when
163:33 - you purchase one course the other course
163:36 - is for free
163:37 - right so now you can actually compare
163:39 - code side by side to see uh oh this is
163:42 - how it is done in c plus plus version of
163:44 - opencv that's really great
163:47 - i like that feature especially i think
163:48 - that's great um so we talked about
163:51 - python versus c plus plus uh earlier but
163:54 - i think some people might be wondering
163:56 - uh obviously both would be good to have
163:58 - under your belt but some people might be
163:59 - wondering to what extent is either
164:02 - language used in industry and if you
164:04 - only knew one language very well or you
164:06 - had an affinity for one of those
164:07 - languages for for whatever reason um you
164:10 - know what is the market share for that
164:11 - language in industry
164:13 - yeah so um python is definitely it is
164:16 - fast becoming the language of scientific
164:18 - computing
164:19 - but then uh you know there is
164:21 - you you can easily learn the concepts
164:23 - using python and it will also be there
164:26 - are several jobs you know there are the
164:28 - large i would say 50 of the jobs would
164:31 - easily take somebody who has python
164:34 - skills but then there is this whole
164:36 - other world of embedded computer vision
164:38 - for example where you're trying to do
164:39 - computer vision on
164:41 - really not very powerful devices right
164:43 - you're not using a gpu even if you're
164:45 - using a gpu it's like an embedded gpu or
164:48 - something in those cases uh a lot of
164:51 - times these algorithms are implemented
164:53 - in c plus plus right and sometimes even
164:56 - in c right in
164:57 - c so that because you don't have access
164:59 - to that much computational power and
165:01 - that's a lot of you know that's also a
165:03 - very big market think about all the
165:05 - security cameras uh all kinds of devices
165:08 - which are embedded
165:09 - uh so
165:11 - i i mean
165:13 - if you love one language you know python
165:16 - is great you know try python you will
165:19 - gradually uh learn c plus plus over time
165:23 - what i want to emphasize is that don't
165:25 - uh
165:26 - think that you're a python developer
165:27 - only right think about yourself as an
165:30 - engineer and you will learn
165:32 - and uh whatever needs to be learned to
165:36 - make yourself fit for the job market
165:38 - all right so um you know one thing
165:40 - people might be wondering is there's
165:42 - certainly a lot of uh freely available
165:44 - course material
165:46 - uh on the internet um universities are
165:48 - offering all their court many top
165:50 - universities are offering many of their
165:52 - courses online for free
165:54 - there's lots of tutorials uh what how do
165:56 - people sort this out and what do your
165:58 - courses provide that maybe is not um
166:02 - available online for free
166:04 - yeah so i actually encourage people to
166:07 - look at all the free stuff uh first
166:09 - right because it builds uh the
166:12 - confidence it builds the momentum they
166:14 - start knowing what to expect you know
166:16 - what they want to learn uh so they start
166:18 - having an expectation what they want to
166:21 - learn so
166:22 - by all means you know there are a lot of
166:24 - free tutorials on opencv.org and also on
166:26 - learnopencv.com go ahead and try those
166:30 - and uh there are several other very good
166:32 - bloggers also who uh you can try
166:36 - and then there are free uh courses by
166:39 - universities as well try those as well
166:41 - but then at some point you know if you
166:43 - feel that you're missing a structure
166:45 - sometimes what happens is that you take
166:47 - all these pieces but you don't get a
166:49 - bigger picture you know
166:51 - uh pieces of computer vision but you're
166:54 - not confident enough to go and uh
166:57 - present yourself in front of a job
166:59 - interviewer because
167:00 - you know that you know you have learned
167:02 - in bits and pieces you have not
167:04 - connected the dots completely
167:06 - and that can be a blow to your
167:08 - confidence right when you take a course
167:10 - it is structured and you know that the
167:12 - important things have been covered and
167:14 - suppose the interviewer asks you
167:16 - something which is beyond what you have
167:17 - learned you can confidently say that oh
167:19 - i have not learned that that's okay but
167:21 - i know uh this thing i have gone in a
167:24 - through a structured path to learn all
167:26 - these things
167:27 - and in our courses we also cover uh you
167:29 - know
167:30 - there's a lot of code that people go
167:32 - through you you write a lot of code i
167:34 - was saying that you need to be very good
167:36 - at programming so you need to write a
167:38 - lot of code you
167:39 - you know read a lot of code so that's a
167:42 - very good practice we also give you
167:44 - assignments and projects and together
167:47 - that uh creates you know sense of
167:49 - urgency it creates a sense of
167:51 - responsibility that you have to finish
167:53 - this thing what happens with online
167:55 - material is that you take this and you
167:57 - read the code but you never write any
167:59 - code right you feel that you have
168:00 - understood it but if you take away that
168:02 - code you cannot ever write
168:05 - from scratch right you use that material
168:07 - as a crutch
168:08 - so when you start doing something like a
168:11 - project etc uh it actually comes
168:13 - together right you feel you need to
168:16 - job market it's basically a confidence
168:18 - game you need to have
168:20 - your expertise to a level where you're
168:22 - confident uh you can uh face an
168:25 - interviewer right
168:26 - and then we also uh not only that you
168:29 - know
168:30 - on the internet in a blog format it is
168:32 - simply not possible to cover things uh
168:35 - in depth right
168:37 - so a lot of times uh people gloss over
168:40 - things and we do that ourselves also
168:42 - it's simply not possible to go in a blog
168:44 - format you have to condense everything
168:47 - uh to be easily uh consumable
168:50 - you don't have that user uh for an hour
168:53 - right so you have to make sure that they
168:55 - learn whatever they want to learn in 10
168:58 - minutes
168:59 - and in doing so
169:01 - the medium is restricted in some sense
169:03 - so in courses we don't have that
169:05 - restriction we know that people are
169:06 - committed they are ready to spend time
169:08 - on this and so we take the time to
169:11 - explain in depth uh that's that's a very
169:14 - important thing as well and there is
169:16 - also peer pressure
169:18 - when you see that other people are you
169:20 - know asking questions in the course
169:21 - forum they are um
169:23 - they are enjoying the course it puts a
169:25 - little bit of pressure on you in a
169:26 - positive way
169:28 - which uh which propels you that i also
169:30 - want to do something uh you know well
169:32 - you're all in it together right it's a
169:33 - little bit of camaraderie too and uh i
169:36 - think you said it perfectly that
169:37 - confidence comes from mastery so you
169:40 - feel confident when you know that you've
169:41 - mastered one particular topic
169:43 - and then the other thing that you
169:44 - brought up is uh doing projects so uh um
169:48 - actually executing a project and taking
169:50 - what you've learned to create you know
169:51 - an extension of that
169:53 - uh is i think very valuable very
169:55 - rewarding and also it forces you not to
169:58 - skip steps so you could be reading a
170:00 - blog or watching a video online and not
170:02 - in your head yes this all makes sense
170:04 - but when it comes time to actually code
170:06 - something or create something a little
170:07 - bit different than what you learned
170:10 - if you actually have to program it and
170:11 - get your hands dirty it forces you not
170:13 - to skip steps or gloss over details that
170:16 - are actually required so
170:17 - right i think that's a very valuable
170:19 - experience
170:20 - so let's talk a little bit about what uh
170:22 - sort of jobs are available in industry i
170:24 - mean some immediate ones come to mind
170:26 - the entertainment industry perhaps uh
170:28 - medical imaging uh manufacturing but
170:31 - what are the different uh domains and
170:33 - and uh types of uh fields where computer
170:36 - vision is actually being used
170:38 - more recently
170:40 - so uh professor andrewing who is also
170:42 - the founder of coursera he likes to say
170:44 - that ai is like electricity so when
170:47 - electricity was invented it was used in
170:50 - for lighting purposes
170:52 - but
170:52 - within a few years it transformed
170:55 - multiple industries it was used in
170:56 - manufacturing in agriculture and a lot
170:59 - of different things and ai has the same
171:02 - power right it is used in certain
171:04 - industries right now but it is
171:06 - transforming multiple industries
171:07 - including manufacturing automotive you
171:10 - know uh
171:11 - autonomous driving cars
171:13 - um and
171:15 - agriculture we have a lot of people who
171:17 - are working on pest control who are
171:19 - working on removing weeds in an
171:22 - agricultural setting
171:24 - there are medical imaging is huge
171:27 - because now
171:29 - ei is doing better than radiologists in
171:32 - some sections right
171:34 - because
171:35 - when the data becomes available
171:37 - and it's a
171:39 - repetition some tasks which are
171:40 - repetitive in those cases you know we
171:43 - are ai is going to do better than humans
171:46 - over time as the data becomes
171:48 - available it is those creative fields
171:50 - you know let's say even in music
171:53 - and
171:54 - creating generating new images ai is
171:57 - learning from existing data and creating
171:59 - a new art form right but
172:03 - there are certain fields where ai is not
172:05 - going to take over uh for example um
172:09 - comedy is one example comedy is
172:12 - you cannot train uncommon you know the
172:15 - if you if you look at the jokes that are
172:17 - produced by an ai system they're usually
172:20 - pretty lame they try to rhyme something
172:22 - because
172:23 - for creating comedy you need things that
172:26 - come
172:27 - that's in that's not in a specific area
172:31 - you could tell a joke which combines uh
172:34 - you know something going on in the music
172:36 - industry with something that is going on
172:38 - in sports and when you put them together
172:40 - it is funny right right it's really hard
172:42 - to to
172:43 - how do you quantify that right right and
172:45 - it's not domain specific because you
172:47 - took something very different in a
172:49 - completely different domain and combined
172:50 - it with something uh very different and
172:53 - so those kinds of things which are one
172:54 - off right and it's even
172:57 - best comedians do not know what will
172:59 - what will fly right so they do a lot of
173:01 - tests themselves right right those kinds
173:03 - of fills fields are very difficult uh
173:05 - you know for ai but everywhere else
173:08 - where it is very structured like
173:10 - manufacturing it's a very structured
173:12 - environment uh warehouses uh it's a very
173:14 - structured environment a very controlled
173:16 - environment right so
173:18 - a lot of variation is removed in those
173:20 - cases so which makes it easier a lot of
173:21 - variations are removed and there are
173:23 - repetitive tasks that can generate a lot
173:25 - of data right the the key thing is that
173:28 - they are repetitive tasks the same task
173:30 - is happening over and over again no
173:32 - matter how complex the task you can put
173:35 - a camera there and
173:36 - get as much data as possible as as you
173:39 - want and ultimately that problem will be
173:41 - solved right
173:43 - so um i mean if you look at all the
173:45 - companies who are the companies hiring
173:47 - when i graduated back in 2006 i had a
173:50 - few options i could go to microsoft
173:52 - research i could go to google i could go
173:53 - to a few different places
173:55 - but now the people who come to us for
173:57 - consulting they are all over the place
174:00 - obviously these these big companies are
174:02 - still there microsoft google facebook
174:04 - adobe all these companies are still
174:06 - there who hire computer vision research
174:09 - engineers
174:10 - but then there are small companies one
174:12 - or you know like five people companies
174:15 - who are working on
174:17 - uh
174:18 - something very specific that you may not
174:19 - have thought about uh
174:21 - but they need computer vision expertise
174:23 - for example one of the first consulting
174:25 - projects uh i did
174:27 - they were sorting lego pieces so these
174:30 - lego pieces there are 10 000 of those we
174:31 - don't realize it but there are 10 000
174:34 - unique uh lego pieces and they wanted to
174:37 - identify with reliability which piece it
174:40 - is right
174:42 - so because they wanted a replacement
174:44 - system that if some piece is lost they
174:46 - can replace it
174:48 - so uh you can see that that is something
174:51 - that came out of the blue i had never
174:53 - thought about you know uh oh that's an
174:55 - application area right right
174:58 - another one was uh
175:00 - uh there were there is a company that is
175:02 - using uh uh computer vision for
175:05 - detecting
175:06 - the species of fish when they have uh
175:09 - you know when they catch fish they want
175:11 - to make sure just to you know what is
175:13 - the size of the fish what is the size
175:15 - what is the species that they have
175:16 - caught and things like that so uh to do
175:19 - that that analysis they are using
175:21 - computer vision
175:22 - uh microscopy companies we have worked
175:24 - on so many different applications uh
175:27 - even you know applications which have a
175:30 - huge impact
175:31 - like um one where they were identifying
175:34 - uh shooters uh in school
175:36 - and uh we had worked on a very proof of
175:39 - concept project uh with a company
175:42 - and recently a few weeks back i came to
175:45 - know that they have grown into a
175:47 - full-fledged company and they are
175:48 - offering the service right
175:51 - so
175:52 - my point is that computer vision is
175:53 - everywhere it's not just focused on
175:55 - these large companies but i just gave
175:58 - you several examples i'll add one more
176:01 - example there is a company
176:02 - that used
176:04 - us for detecting fraud in fashion
176:06 - merchandise like bags
176:08 - these bags are very expensive some of
176:10 - them are two thousand dollars
176:12 - for a small bag by a recognized brand
176:15 - and they want to protect the brand
176:17 - identity so they want to
176:19 - something that is two thousand dollars
176:21 - you can make a very good copy of that
176:23 - for a thousand dollars right and then
176:25 - sell it for two hundred dollars and
176:27 - still make a very good profit
176:28 - so um but fortunately these companies
176:32 - have very high standards trained a
176:34 - machine learning algorithm to know the
176:36 - difference between uh between a fraud a
176:39 - bag and the real bag
176:41 - so you can see now in the fashion
176:43 - industry we have worked and this is a
176:45 - small company you know i'm talking about
176:46 - a consulting company uh with
176:49 - 40 50 people
176:51 - we are receiving such uh
176:53 - diverse set of uh
176:55 - problems to work on we worked on sports
176:58 - analytics also where we are tracking
177:00 - soccer ball um
177:02 - uh in in a sports setting
177:04 - there are other companies who have
177:06 - approached us for baseball for golf and
177:09 - uh other you know projects like that one
177:12 - so
177:13 - uh
177:14 - it's a very diverse range uh we are
177:16 - lucky to be in this
177:18 - in this position i'm really really
177:20 - curious that the example that you
177:22 - pointed to with the with the counterfeit
177:24 - merchandise was that actually was that
177:26 - machine learning only or did that
177:27 - incorporate deep learning
177:29 - it was a deep learning project it was it
177:30 - was a deep learning project um
177:33 - and this company basically is in the
177:35 - business of selling second-hand
177:38 - they buy
177:39 - these
177:40 - handbags and then resell them so when
177:43 - they buy these
177:44 - handbags from people
177:47 - they have to make sure that these
177:48 - handbags are genuine
177:50 - and they have experts
177:53 - who go and check whether these things
177:54 - are genuine or not and that requires a
177:56 - lot of expertise right even to give a
177:59 - quote for this
178:01 - for this handbag how much should it be
178:03 - quoted you need to know the exact make
178:05 - and ma the model of the handbag
178:08 - which is
178:10 - the 10 000 of these again right so you
178:12 - have to automatically determine at least
178:14 - to give a get a ballpark you know which
178:16 - kind of handbag it is otherwise there
178:18 - are these experts who are you know tens
178:21 - of experts who are employed just to look
178:24 - at the these images and make sure that
178:26 - oh uh this is this kind of handbag right
178:29 - this is a gucci handbag of such and such
178:31 - model and therefore it should be priced
178:33 - at such and such
178:34 - um right and then the handbags comes in
178:37 - and you have to determine whether it is
178:38 - counterfeit or not and that requires a
178:41 - different level of expertise altogether
178:43 - because
178:44 - mistakes can be really
178:46 - expensive very expensive right yeah they
178:49 - are putting their name on that that this
178:50 - is a genuine item
178:52 - so we've covered quite a bit in this
178:54 - discussion today and i just curious if
178:56 - you had any final thoughts for people
178:57 - who are
178:58 - perhaps on the fence about getting into
179:00 - computer vision or wondering exactly you
179:02 - know how they might start
179:04 - um
179:06 - yeah so uh final thoughts uh first of
179:09 - all don't be afraid of taking a leap
179:12 - into this field uh the job opportunities
179:15 - are tremendous it's a very good uh
179:18 - career switch for people who are
179:20 - interested you know the first thing you
179:22 - have to answer for yourself is are you
179:24 - interested in computer vision and ai and
179:26 - if it you know sparks joy uh in you
179:30 - then this
179:32 - field is full of opportunities high
179:34 - paying jobs etc and take any learning
179:37 - path right it's not necessary that you
179:39 - take our courses which you can but jump
179:42 - into this uh this learning path try to
179:46 - learn as much as possible from free
179:48 - material and when you're ready come to
179:50 - our courses but even if you don't it
179:52 - doesn't matter right the main important
179:54 - thing is that you
179:56 - embark on this journey
179:58 - knowing that there are a lot of jobs
180:01 - available in this area and if you spend
180:04 - about six months
180:06 - to about a year and dedicate your time
180:08 - and energy into this
180:10 - you can become an expert an engineer who
180:13 - can uh create you know who can join this
180:16 - ai revolution so with that in mind i
180:18 - wish you all the best welcome to the ai
180:21 - revolution
180:25 - you