00:00 - Hey, I'm Andy from deep lizard.
00:02 - And in this course, we're going to learn how
to use Kerris, and neural network API written
00:07 - in Python and integrated with TensorFlow.
00:13 - Throughout the course, each lesson will focus
on a specific deep learning concept, and show
00:22 - the full implementation in code using the
keras API.
00:25 - We'll be starting with the absolute basics
of learning how to organize and pre processed
00:30 - data, and then we'll move on to building and
training our own artificial neural networks.
00:35 - And some of these networks will be built from
scratch, and others will be pre trained and
00:39 - state of the art models for which we'll fine
tune to use on our own custom data sets.
00:45 - Now let's discuss the prerequisites needed
to follow along with this course.
00:49 - From a knowledge standpoint, we'll give brief
introductions of each deep learning concept
00:53 - that we are going to work with before we go
through the code implementation.
00:58 - But if you're an absolute beginner to deep
learning, then we first recommend you go through
01:02 - the deep learning fundamentals scores on the
tables or Comm.
01:05 - Or if you're super eager to jump into the
code, then you can simultaneously take this
01:10 - course and the deep learning fundamentals
course, the deep learning fundamentals course
01:14 - will give you all the knowledge you need to
know to get acquainted with major deep learning
01:18 - concepts for which then you can come back
and implement and code using the keras API.
01:23 - And this course, in regard to coding prerequisites,
just some basic programming skills and some
01:28 - Python experience or all that's needed.
01:31 - On peoples.com.
01:32 - You can also find the deep learning learning
path.
01:35 - So you can see where this Kerris course falls
amidst all the deep lizard deep learning content.
01:40 - Now let's discuss the Course Resources.
01:43 - So aside from the videos here on YouTube,
you will also have video and text resources
01:48 - available on peoples are calm.
01:50 - And actually each episode has its own corresponding
blog and a quiz available for you to take
01:55 - and test your own knowledge.
01:57 - And you can actually contribute your own quiz
questions as well.
02:00 - And you can see how to do that on the corresponding
blog for each episode.
02:04 - Additionally, all of the code resources used
in this course, are regularly tested and maintained,
02:10 - including updates and bug fixes when needed.
02:13 - Download access to code files used in this
course, are available to members of the deep
02:17 - lizard hive mind.
02:18 - So you can check out more about that on tables
or calm as well.
02:22 - Alright, so now that we know what this course
is about, and what resources we have available,
02:26 - along with the prerequisites needed to get
started, let's now talk a little bit more
02:30 - about Kerris itself.
02:32 - Kerris was developed with a focus on enabling
fast user experimentation.
02:37 - So this allows us to go from idea to implementation
and very few steps.
02:41 - Aside from this benefit, users often wonder
why choose Kerris as the neural network API
02:47 - to learn or in general, which neural network
API should they learn?
02:51 - Our general advice is to not commit yourself
just to only learning one and sticking with
02:56 - that one forever, we recommend to learn multiple
neural network API's.
03:01 - And the idea is that once you have a fundamental
understanding of the underlying concepts,
03:06 - then the minor syntactical and implementation
differences between the neural network API's
03:11 - shouldn't really be that hard to catch on
to once you have at least one under your belt
03:15 - already, especially for job prospects.
03:18 - Knowing more than one neural network API will
show your experience and allow you to compare
03:23 - and contrast the differences between API's
and share your opinions for why you think
03:28 - that certain API's may be better for certain
problems, and others for other problems.
03:34 - Being able to demonstrate this will make you
a much more valuable candidate.
03:38 - Now, we previously touched on the fact that
Kerris is integrated with TensorFlow.
03:44 - So let's discuss that more.
03:45 - Now.
03:46 - Historically, Kerris was a high level neural
network API that you could configure to run
03:50 - against one of three separate lower level
API's.
03:55 - And those lower level API's were TensorFlow
viano and cntk.
04:00 - Later, though, Kerris became fully integrated
with the TensorFlow API, and is no longer
04:06 - a separate library that you can choose to
run against one of the three back end engines
04:11 - that we discussed previously.
04:13 - So it's important to understand that cares
is now completely integrated with the TensorFlow
04:17 - API.
04:18 - But in this course, we are going to be focusing
on making use solely of that high level Kerris
04:24 - API without necessarily making much use of
the lower level TensorFlow API.
04:29 - Now, before we can start working with cares,
then we first have to obviously get it downloaded
04:35 - and installed onto our machines.
04:37 - And because Kerris is fully integrated with
TensorFlow, we can do that by just installing
04:42 - TensorFlow Kerris will come completely packaged
with the TensorFlow installation.
04:47 - So the installation procedure is as simple
as running pip install TensorFlow from your
04:52 - command line, you might just want to check
out the system requirements on tensor flows
04:56 - website to make sure that your specific system
we The requirements needed for TensorFlow
05:01 - to install.
05:03 - Alright, so we have one last talking point
before we can get into the actual meat of
05:07 - the coding.
05:08 - And that is about GPU support.
05:11 - So the first important thing to note is that
a GPU is not required to follow this course,
05:17 - if you're running your machine only on a CPU,
then that is totally fine for what we'll be
05:22 - doing in the course.
05:23 - If however, you do want to run your code on
a GPU, then you can do so pretty easily.
05:28 - After you get through the setup process for
setting up your GPU to work with TensorFlow,
05:34 - we have a full guide on how to get the GPU
setup to work with TensorFlow on deep lizard
05:39 - Comm.
05:40 - So if you are interested in doing that, then
head over there to go through those steps.
05:43 - But actually, I recommend just going through
the course with a CPU if you're not already
05:48 - set up with a GPU.
05:50 - And like I said, the all the code will work
completely fine, run totally fine using only
05:55 - a CPU.
05:56 - But then after the fact, after you go through
the course successfully, then get the steps
06:00 - in order to work with the GPU if you have
one, and then run all the code that you have
06:05 - in place from earlier, run it on the GPU,
the second go round and just kind of look
06:10 - and see the kind of efficiency and speed ups
that you'll see in the code.
06:15 - Alright, so that's it for the Kerris introduction.
06:19 - Now we're finally ready to jump in to the
code.
06:22 - Be sure to check out the blog and other resources
available for this episode on the poser.com
06:28 - as well as the dibbles at hive mind where
you can gain access to exclusive perks and
06:33 - rewards.
06:34 - Thanks for contributing to collective intelligence.
06:36 - Now, let's move on to the next episode.
06:40 - Hey, I may be from deep lizard.
06:44 - In this episode, we'll learn how to prepare
and process numerical data that will later
06:49 - use to train our very first artificial neural
network.
06:59 - To train any neural network and a supervised
learning task, we first need a data set of
07:05 - samples along with the corresponding labels
for those samples.
07:09 - When referring to the word samples, we're
simply just talking about the underlying data
07:13 - set, where each data point in the set is referred
to as a sample.
07:18 - If we were to train a model to do sentiment
analysis on headlines from a media source,
07:23 - for example, then the labels that correspond
to each headline sample would be positive
07:30 - or negative.
07:32 - Or say that we were training an artificial
neural network to identify images as cats
07:36 - or dogs, well, then that would mean that each
image would have a corresponding label of
07:42 - cat or dog.
07:43 - Note that in deep learning, you may also hear
samples referred to as inputs or input data.
07:50 - And you may hear labels referred to as targets
or target data.
07:55 - When preparing a data set, we first need to
understand the task for which the data will
08:00 - be used.
08:01 - In our example, we'll be using our data set
to train an artificial neural network.
08:06 - So once we understand this, then we can understand
which format the data needs to be in, in order
08:12 - for us to be able to pass the data to the
network.
08:16 - The first type of neural network that we'll
be working with is called a sequential model
08:20 - from the cares API.
08:22 - And we'll discuss more details about the sequential
model in a future episode.
08:26 - But for now, we just need to understand what
type of data format the sequential model expects,
08:32 - so that we can prepare our dataset accordingly.
08:35 - The sequential model receives data during
training whenever we call the fit function
08:39 - on it.
08:41 - And again, we're going to go into more details
about this function in the future.
08:44 - But for now, let's check out what type of
data format the fit function expects.
08:49 - So if we look at the fit documentation here,
on tensor flows website, we can see the first
08:55 - two parameters that the fit function expects
are x and y.
08:59 - So x is our input data, our samples, in other
words, so this function expects x our input
09:06 - data to be in a NumPy array, a TensorFlow
tensor, a dict, mapping, a TF dot data data
09:13 - set or a karass generator.
09:16 - So if you're not familiar with all of these
data types, that's okay, because for our first
09:21 - example, we are going to organize our data
to be in a NumPy array.
09:25 - So the first option here in the documentation,
so this is for our input samples, but also
09:31 - the Y parameter that is expected by the fit
function is our data set that contains the
09:39 - corresponding labels for our samples.
09:41 - So the target data.
09:43 - Now the requirement for why is that it is
formatted as one of the above formats that
09:48 - we just discussed for x, but y needs to be
in the same format as x.
09:54 - So we can't have our samples contained in
a NumPy array, for example, and then have
09:58 - y Our target data or our labels for those
samples and the TensorFlow tensor.
10:04 - So the format of x and y, both need to match.
10:08 - And we're going to be putting both of those
into NumPy arrays.
10:12 - Alright, so now we know the data format that
the model expects.
10:16 - But there's another reason that we may want
to transform or process our data.
10:20 - And that is to put it in a format that will
make it easier or more efficient for the model
10:25 - to learn from.
10:26 - And we can do that with data normalization
or standardization techniques, data processing,
10:31 - and deep learning will vary greatly depending
on the type of data that we're working with.
10:37 - So to start out, we are going to work with
a very simple numerical data set to train
10:41 - our model.
10:43 - And later, we'll get exposure to working with
different types of data as well.
10:46 - Alright, so now we're ready to prepare and
process our first data set.
10:51 - So we are in our Jupyter Notebook.
10:52 - And the first step is to import all the packages
that we'll be making use of including NumPy
10:58 - random and some modules from scikit learn.
11:01 - Next, we will create two list one called train
samples, one called train labels, and these
11:08 - lists will hold the corresponding samples
and labels for our data set.
11:13 - Now about this data set, we are going to be
working with a very simple numerical data
11:18 - set.
11:19 - And so for this task, we're actually going
to create the data ourselves.
11:22 - Later, we'll work with more practical examples
and realistic ones where where we won't be
11:27 - creating the data, but instead downloading
it from some external source.
11:31 - But for now, we're going to create this data
ourselves for which will train our first artificial
11:37 - neural network.
11:39 - So as a motivation for this kind of dummy
data, we have this background story to give
11:44 - us an idea of what this data is all about.
11:47 - So let's suppose that an experimental drug
was tested on individuals ranging from age
11:53 - 13 to 100.
11:55 - In a clinical trial, this trial had 2100 participants
total, half of these participants were under
12:01 - the age of 65.
12:03 - And half were 65 years or older.
12:06 - And the conclusions from this trial was that
around 95% of the patients who were in the
12:10 - older population, so 65 or older, the 95%
of those patients experienced side effects.
12:18 - And around 95% of patients who were under
65 years old, experienced no side effects.
12:24 - Okay, so this is a very simplistic data set.
12:28 - And so that is the background story.
12:30 - Now in this cell, here, we're going to go
through the process of actually creating that
12:35 - data set.
12:36 - So what this is, is this first for loop is
going to generate both the approximately 5%
12:43 - of younger individuals who did experience
side effects, and the 5% of older individuals
12:50 - who did not experience side effects.
12:52 - So within this first for loop, we are first
generating a random number or a random integer
12:58 - rather between 13 and 64.
13:01 - And that is constituting as a younger individual
who is under 65 years of age, or yet who's
13:07 - under 65 years of age.
13:09 - And we are going to then append this number
to the train samples list.
13:16 - And then we append a one to the train labels
list.
13:19 - Now a one is representing the fact that a
patient did experience side effects.
13:25 - And a zero would represent a patient who did
not experience side effects.
13:29 - So then similarly, we jumped down to the next
line.
13:33 - And we are generating a random integer between
65 in 100, to represent the older population.
13:39 - And we are doing that Remember, this is in
our first for loop.
13:44 - So it's only running 50 times.
13:46 - So this is kind of the outlier group, these
5% of older individuals who did not experience
13:51 - side effects.
13:53 - So we then take that sample appended to the
train samples list, and append a zero to the
14:00 - corresponding train labels list.
14:03 - Since these patients were the older patients
who did not experience side effects.
14:07 - So then if we jumped down to the next for
loop, very similarly, we have pretty much
14:12 - the same code here, except for this is the
bulk of the group.
14:17 - And this for loop, we're running this for
the 95% of younger individuals who did not
14:22 - experience side effects, as well as the 95%
of older individuals who did experience side
14:28 - effects.
14:29 - So generating a random number between 13 and
six before and then appending that number,
14:34 - representing the age of the younger population
to the train samples list, and appending the
14:40 - label of zero.
14:41 - Since these individuals did not experience
side effects, we're appending zero to the
14:46 - train labels list.
14:48 - Similarly, we do the same thing for the older
individuals from 65 to 100.
14:53 - Except for since the majority of these guys
did experience side effects, we are appending
14:57 - a one to the train The labels list.
15:01 - So just to summarize, we have the samples
list that contains a bunch of integers ranging
15:06 - from 13 to 100.
15:09 - And then we have the train labels list that
has the labels that correspond to each of
15:14 - these individuals with the ages 13 to 100.
15:19 - The labels correspond to whether or not these
individuals experience side effects.
15:22 - So a samples list containing ages, a labels
list containing zeros and ones representing
15:27 - side effects, or no side effects.
15:29 - And just to get a visualization of the samples,
here, we are printing out all of the samples
15:34 - in our list.
15:35 - And we see that these are just all integers,
like we'd expect, ranging from 13 to 100.
15:42 - And then correspondingly, if we run our train
labels list and print out all of the data
15:50 - there, then we can see this list contains
a bunch of zeros and ones.
15:54 - Alright, so now the next step is to take these
lists and then start processing them.
15:59 - So we have our data generated.
16:01 - Now we need to process it to be in the format
for which we saw the fit function expects,
16:06 - and we discussed the fact that we are going
to be passing this data as NumPy arrays to
16:11 - the fit function.
16:13 - So our next step is to go ahead and do that
transformation here, where we are taking the
16:18 - train labels list and making that now a NumPy
array.
16:22 - Similarly, doing the same thing with the train
samples list.
16:26 - And then we use the shuffle function to shuffle
both are trained labels and trained samples
16:33 - respective to each other so that we can get
rid of any imposed order from the data generation
16:39 - process.
16:40 - Okay, so now the data is in the NumPy array
format that is expected by the fit function.
16:46 - But as mentioned earlier, there's another
reason that we might want to do further processing
16:50 - on the data.
16:51 - And that is to either normalize or standardize
it so that we can get it in such a way that
16:56 - the training of the neural network might become
quicker or more efficient.
17:02 - And so that's what we're doing in this cell.
17:04 - Now we are using this min max scalar object
to create a feature range ranging from zero
17:10 - to one, which we'll then use in this next
line to rescale our data from the current
17:17 - scale of 13 to 100, down to a scale of zero
to one, and then this reshaping that we're
17:25 - doing here is just a formality because the
fit transform function doesn't accept one
17:29 - D data by default.
17:30 - Since our data is one dimensional, we have
to reshape it in this way to be able to pass
17:36 - it to the fit transform function.
17:38 - So now if we print out the elements in our
new scaled train samples variable, which that's
17:44 - what we're calling our new scaled samples,
then we can print them out and see that now
17:50 - the individual elements are no longer integers
ranging from 13 to 100.
17:55 - But instead, we have these values ranging
anywhere between zero and one.
18:01 - So at this point, we have generated some raw
data, and then processed it to be in a NumPy
18:07 - array format that our model will expect.
18:10 - And then rescale the data to be on a scale
between zero and one, and an upcoming episode,
18:17 - we'll use this data to train our first artificial
neural network, be sure to check out the blog
18:22 - and other resources available for this episode
on deep lizard calm, as well as the deep lizard
18:27 - hive mind where you can gain access to exclusive
perks and rewards.
18:32 - Thanks for contributing to collective intelligence.
18:35 - Now, let's move on to the next episode.
18:40 - Hey, I'm Andy from Blizzard.
18:43 - And this episode will demonstrate how to create
an artificial neural network using a sequential
18:48 - model from the keras API integrated within
tensor flow.
18:59 - In the last episode, we generated data from
an imagined clinical trial.
19:04 - And now we'll create an artificial neural
network for which we can train on this data.
19:09 - Alright, so first things first, we need to
import all of the TensorFlow modules that
19:13 - we'll be making use of to build our first
model.
19:17 - And that includes everything that you see
here except for actually the last two of atom
19:22 - and categorical cross and vitry.
19:26 - categorical cross entropy, rather, those two
are going to be used when we train the model,
19:31 - not when we build it.
19:32 - But we're going ahead and bringing all the
imports in now.
19:35 - And then next, if you are running this code
on a GPU, then you can run this cell which
19:40 - will allow you to make sure that TensorFlow
is correctly identifying your GPU as well
19:46 - as enable memory growth.
19:49 - And there's a few lines on the blog where
you can check out what exactly that means
19:53 - and why you might want to do this.
19:55 - But if you are running a GPU then go ahead
and run this cell.
19:58 - So next, this is the The actual model that
we are building this is a sequential model.
20:05 - And that is kind of the most simplest type
of model that you can build using Kerris or
20:10 - TensorFlow.
20:11 - And a sequential model can be described as
a linear stack of layers.
20:15 - So if you look at how we're creating the model
here, that's exactly what it looks like.
20:20 - So we are initializing the model as an instance
of the sequential class.
20:26 - And we are passing in a list of layers here.
20:31 - Now, it's important to note that this first
dense layer that we're looking at, that is
20:36 - actually the second layer overall.
20:40 - So this is the first hidden layer.
20:42 - And that's because the input layer, we're
not explicitly defining using Kerris, the
20:48 - input data is what creates the input layer
itself.
20:52 - So the way that the model knows what type
of input data to expect, or the shape of the
20:58 - input data, rather, is through this input
shape parameter that we pass to our first
21:04 - dense layer.
21:05 - So through this, the model understands the
shape of the input data that it should expect.
21:10 - And then therefore it accepts that shape,
have input data, and then passes that data
21:15 - to the first hidden layer, which is this dense
layer here in our case, now, we are telling
21:22 - this dense layer that we want it to have 16
units.
21:25 - These units are also otherwise known as nodes
or neurons.
21:30 - And the choice of 16 here is actually pretty
arbitrary.
21:33 - This model overall is very simple.
21:36 - And with the arbitrary choice of nodes here,
it's actually going to be pretty hard to create
21:42 - a simple model, at least, that won't do a
good job at classifying this data, just given
21:48 - the simplicity of the data itself.
21:51 - So we understand that we're passing in, or
that we're specifying 16 units for this first
21:56 - hidden layer, we are specifying the input
shape, so that the model knows the shape of
22:01 - the input data to expect and then we are stating
that we want the relu activation function
22:08 - to follow this dense layer.
22:11 - Now, this input shape parameter is only specified
for our first hidden layer.
22:15 - So after that, we have one more hidden, dense
layer.
22:19 - This time, we are arbitrarily setting the
number of units for this dense layer to be
22:23 - 32.
22:25 - And again, we're following the layer by the
activation function value.
22:29 - And then lastly, we specify our last output
layer or our last layer, which is our output
22:35 - layer.
22:36 - This is another dense layer, this time with
only two units, and that is corresponding
22:42 - to the two possible output classes.
22:45 - Either a patient did experience side effects,
or the patient did not experience side effects.
22:51 - And we're following this output layer with
the softmax function, which is just going
22:56 - to give us probabilities for each output class.
23:01 - So between whether or not a patient experience
side effects or not, we will have an output
23:07 - probability for each class, letting us know
which class is more probable for any given
23:12 - patient.
23:14 - And just in case, it's not clear, this dense
layer here is what we call a densely connected
23:20 - layer or a fully connected layer, probably
the most well known type of layer and an artificial
23:26 - neural networks.
23:27 - Now in case you need any refresher on fully
connected layers, or activation functions,
23:32 - or anything else that we've discussed up to
this point, then just know that all of that
23:36 - is covered in the deep learning fundamentals
course, if you need to go there and refresh
23:40 - your memory on any of these topics here.
23:43 - Alright, so now we'll run this cell to create
our model.
23:47 - And now we can use model dot summary to print
out a visual summary of the architecture of
23:53 - the model we just created.
23:55 - So looking here, we can just see the visual
representation of the architecture that we
24:00 - just created in the cell above.
24:03 - All right, so now we have just finished creating
our very first neural network using this simple
24:08 - and intuitive sequential model type.
24:11 - In the next episode, we will see how we can
use the data that we created last time to
24:16 - train this network.
24:18 - Be sure to check out the blog and other resources
available for this episode on depot's or.com
24:23 - as well as the tables at hive mind where you
can gain access to exclusive perks and rewards.
24:29 - Thanks for contributing to collective intelligence.
24:32 - Now let's move on to the next episode.
24:36 - Hey, I'm Andy from deep lizard.
24:39 - In this episode, we'll see how to train an
artificial neural network using the keras
24:45 - API integrated with TensorFlow.
24:54 - In previous episodes, we went through steps
to generate data and also build an Artificial
25:00 - neural network.
25:01 - So now we'll bring these two together to actually
train the network on the data that we created
25:06 - and processed.
25:08 - Alright, so picking up where we were last
time in our Jupyter Notebook, make sure that
25:13 - you still have all of your imports included
and already ran so that we can continue where
25:18 - we were before.
25:19 - So first we have after building our model,
we are going to call this model compile function.
25:25 - And this just prepares the model for training.
25:28 - So it gets everything in order that's needed
before we can actually train the model.
25:33 - So first, we are specifying to the compile
function, what optimizer that we want to use,
25:38 - and we're choosing to use the very common
optimizer atom with a learning rate of 0.0001.
25:45 - And next we specify the type of loss that
we need to use, which is in our case, we're
25:51 - going to use sparse categorical cross in but
we're going to use sparse categorical cross
25:57 - entropy.
25:58 - And then lastly, we specify what metrics we
want to see.
26:01 - So this is just for the model performance,
what we want to be able to judge our model
26:07 - by and we are specifying this list, which
just includes accuracy, which is a very common
26:13 - way to be able to evaluate model performance.
26:16 - So if we run this cell, alright, so the model
has been compiled and is ready for training.
26:22 - And training occurs whenever we call this
fit function.
26:26 - Now recall earlier in the course, we actually
looked at the documentation for the split
26:30 - function, so that we knew how to process our
input data.
26:34 - So to fit the first parameter that we're specifying
is x here, which is our input data, which
26:39 - is currently stored in this scaled at train
samples variable, then y, which is our target
26:46 - data, or our labels are labels are currently
stored in the train labels variable.
26:52 - So we are specifying that here.
26:54 - Next, we specify our batch size that we want
to use for training.
26:58 - So this is how many samples are included in
one batch to be passed and processed by the
27:04 - network at one time.
27:05 - So we're setting this to 10.
27:08 - And the number of epochs that we want to run,
we're setting this to 30.
27:11 - So that means that the model is going to process
or train on all of the data in the data set
27:18 - 30 times before completing the total training
process.
27:22 - Next, we're specifying this shuffle, shuffle
parameter which we are setting to true.
27:29 - Now, by default, this is already set to true,
but I was just bringing it to your attention
27:33 - to show or to make you aware of the fact that
the data is being shuffled by default, when
27:40 - we pass it to the network, which is a good
thing, because we want any order that is inside
27:47 - of the dataset to be kind of erased before
we pass the data to the model so that the
27:53 - model is not necessarily learning anything
about the order of the data set.
27:57 - So this is true by default.
27:58 - So we don't necessarily have to specify that
I was just letting you know.
28:02 - And actually, we'll see something about that
in the next episode about why this is important
28:08 - regarding validation data, but we'll see that
coming up.
28:11 - The last parameter that we specify here is
verbose, which is just an option to allow
28:16 - us to see output from whenever we run this
fit function.
28:19 - So we can either set it to 01, or two, two
is the most verbose level in terms of output
28:25 - messages.
28:26 - So we are setting that here so that we can
get the highest level of output.
28:31 - So now let's run this cell so that training
can begin.
28:34 - All right, so training has just stopped.
28:36 - And we have run for 30 epochs.
28:40 - And if we look at the progress of the model,
so we're starting out on our first epoch,
28:44 - our loss value is currently point six, eight
and our accuracy is 50%.
28:49 - So no better than chance.
28:50 - But pretty quickly looking at the accuracy,
we can tell that it is steadily increasing
28:56 - all the way until we get to our last a POC,
which we are yielding 94% accuracy.
29:03 - And our loss has also steadily decreased from
the point six five range to now being at point
29:09 - two seven.
29:10 - So as you can see this model train very quickly,
with each epoch taking only under one second
29:17 - to run.
29:18 - And within 30 epochs, we are already at a
94% accuracy right.
29:23 - Now although this is a very simple model,
and we were training it on a very simple data,
29:28 - we can see that without much effort at all,
we were able to yield pretty great results
29:32 - in a relatively quick manner of time as well.
29:35 - In subsequent episodes, we'll demonstrate
how to work with more complex models as well
29:40 - as more complex data.
29:42 - But for now, hopefully this example served
the purpose of encouraging you on how easy
29:46 - it is to get started with Kerris.
29:49 - Be sure to check out the blog and other resources
available for this episode on V bowser.com.
29:54 - As well as the deep lizard hive mind where
you can gain access to exclusive perks and
29:59 - rewards.
30:00 - Thanks for contributing to collective intelligence.
30:03 - Now let's move on to the next episode.
30:08 - Hey, I'm Andy from deep lizard.
30:10 - In this episode, we'll demonstrate how we
can use tensor flows keras API to create a
30:16 - validation set on the fly during training.
30:23 - Before we demonstrate how to build a validation
set using Kerris, let's first talk about what
30:32 - exactly a validation set is.
30:34 - So whenever we train a model, our hope is
that when we train it that we see good results
30:40 - from the training output, that we have low
loss and high accuracy.
30:44 - But we don't ever train a model just for the
sake of training it, we want to take that
30:50 - model and hopefully be able to use it in some
way on data that it wasn't necessarily exposed
30:56 - to during the training process.
30:58 - And although this new data is data that the
model has never seen before, the hope is that
31:03 - the model will be good enough to be able to
generalize well on this new data and give
31:08 - accurate predictions for it, we can actually
get an understanding of how well our model
31:14 - is generalizing by introducing a validation
set during the training process to create
31:20 - a validation set.
31:21 - Before training begins, we can choose to take
a subset of the training set, and then separate
31:27 - it into a separate set labeled as validation
data.
31:31 - And then during the training process, the
model will only train on the training data,
31:36 - and then we'll validate on the separated validation
data.
31:40 - So what do we mean by validating?
31:42 - Well, essentially, if we have the addition
of a validation set, then during training,
31:48 - the model will be learning the features of
the training set, just as we've already seen.
31:52 - But in addition, in each epoch, after the
model has gone through the actual training
31:57 - process, it will take what it's learned from
the training data, and then validate by predicting
32:02 - on the data in the validation set, using only
what it's learned from the training data,
32:07 - though.
32:08 - So then during the training process, when
we look at the output of the accuracy and
32:12 - loss, not only will we be seeing that accuracy
and loss computed for the training set, we'll
32:18 - also see that computed on the validation set.
32:21 - It's important to understand though, that
the model is only alerting on or training
32:26 - on the training data.
32:28 - It's not taking the validation set into account
during training.
32:33 - The validation set is just for us to be able
to see how well the model is able to predict
32:38 - on data that it was not exposed to during
the training process.
32:42 - In other words, it allows us to see how general
our model is how well it's able to generalize
32:48 - on data that is not included in the training
data.
32:52 - So knowing this information will allow us
to see if our model is running into the famous
32:57 - overfitting problem.
32:59 - So overfitting occurs when the model has learned
the specific features of the training set
33:04 - really well, but it's unable to generalize
on data it hasn't seen before.
33:09 - So if while training, we see that the model
is giving really good results for the training
33:14 - set, but less than good results for the validation
set, then we can conclude that we have an
33:19 - overfitting problem, and then take the steps
necessary to combat that specific issue.
33:25 - If you'd like to see the overfitting problem
covered in more detail, then there is an episode
33:29 - for that in the deep learning fundamentals
course.
33:31 - Alright, so now let's discuss how we can create
and use a validation set with a karass sequential
33:37 - model, there's actually two ways that we can
create and work with validation sets with
33:42 - a sequential model.
33:43 - And the first way is to have a completely
separate validation set from the training
33:48 - set.
33:49 - And then to pass that validation set to the
model in the fit function, there is a validation
33:56 - data parameter.
33:57 - And so we can just set that equal to the structure
that is holding our validation data.
34:04 - And there's a write up in the corresponding
blog for this episode that contains more details
34:08 - about the format that that data needs to be
in.
34:10 - But we're going to actually only focus on
the second way of creating and using a validation
34:15 - set.
34:16 - This step actually saves us a step because
we don't have to explicitly go through the
34:20 - creation process, the validation set, instead,
we can get Kerris to create it for us.
34:27 - Alright, so we're back in our Jupyter Notebook
right where we left off last time.
34:31 - And we're here on the model dot fit function.
34:34 - And recall, this is what we use last time
to train our model.
34:37 - Now, I've already edited this cell to include
this new parameter, which is validation split.
34:44 - And what validation split does is it does
what it sounds like it splits out a portion
34:49 - of the training set into a validation set.
34:53 - So we just set this to a number between zero
and one.
34:57 - So just a fractional number to tell Kerris
How much of the training set we need to split
35:03 - out into the validation set.
35:06 - So here I'm splitting out 10% of the training
set.
35:09 - So it's important to note that whenever we
do this, the validation set is completely
35:14 - held out of the training set.
35:17 - So the training samples that we remove from
the training set into validation set are no
35:22 - longer contained within the training data
any longer.
35:26 - So using this approach, the validation set
will be created on the fly whenever we call
35:30 - the fit function.
35:32 - Now, there's one other thing worth mentioning
here.
35:34 - And remember last time, I discussed this shuffle
equals true parameter.
35:40 - And I said that by default, the training set
is shuffled whenever we call fit.
35:44 - So this shuffle equals true is already set
by default.
35:47 - But I was just bringing it up to let you know
that that the training set is being shuffled.
35:53 - So that is a good thing, we want the training
set to be shuffled.
35:57 - But whenever we call validation split in this
way, this split occurs before the training
36:03 - set is shuffled, meaning that if we created
our training set and say, we put all of the
36:11 - sick patients first and then the non sick
patients second, and then we say that we want
36:17 - to split off the last 10% of the training
data to be our validation data, it's going
36:24 - to take the last 10% of the training data.
36:27 - And therefore it could just take all of the
the second group that we put in the training
36:33 - set and not get any of the first group.
36:36 - So I wanted to mention that because although
the training data is being shuffled with the
36:41 - fit function, if you haven't already shuffled
your training data before you pass it to fit,
36:47 - then you also use the validation split parameter,
it's important to know that your validation
36:52 - set is going to be the last X percent of your
training set and therefore may not be shuffled
36:58 - and may yield some strange results because
you think that everything has been shuffled
37:02 - when really, it's only the training set has
been shuffled after the validation set has
37:07 - been taken out.
37:08 - So just keep that in mind the way that we
created our training set.
37:12 - Before this episode, we actually shuffled
the training data before it's ever passed
37:18 - to the fit function.
37:19 - So in the future, whenever you're working
with data, it's a good idea to make sure that
37:22 - your data is also shuffled beforehand, especially
if you're going to be making use of the validation
37:28 - split parameter to create a validation set.
37:31 - Alright, so now we'll run this cell one more
time calling the fit function.
37:34 - But this time, not only will we see loss and
accuracy metrics for the training set, we'll
37:40 - also see these metrics for the validation
set.
37:45 - Alright, so the model has just finished running
it's 30 epochs.
37:50 - And now we see both the loss and accuracy
on the left hand side, as well as the validation
37:56 - loss and validation accuracy on the right
hand side.
38:00 - So we can see, let's just look at the accuracy
between the two.
38:04 - They're both starting at around the same 50%
Mark, and going up gradually around the same
38:11 - rate.
38:12 - So we just scroll all the way to our last
epoch, we can see that the accuracy and validation
38:18 - accuracy are pretty similar with only 1% difference
between the two.
38:23 - And yet the loss values are similar as well.
38:27 - So we can see in this example that our model
is not overfitting, it is actually performing
38:34 - pretty well or just as well rather on the
validation set as it is on the training set.
38:39 - So our model is generalizing well.
38:41 - If however, we saw that the opposite case
was true, and our validation accuracy was
38:46 - seriously lagging behind our training accuracy,
then we know that we have a overfitting problem
38:51 - and we would need to take steps to address
that issue.
38:54 - Alright, so we've now seen how to train the
model how to validate the model, and how to
39:00 - make use of both training and validation sets.
39:03 - In the next episode, we're going to see how
to make use of a third data set that test
39:07 - data set.
39:08 - To use the model for inference.
39:10 - Be sure to check out the blog and other resources
available for this episode on depot's or.com
39:16 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
39:21 - rewards.
39:22 - Thanks for contributing to collective intelligence.
39:24 - Now let's move on to the next episode.
39:28 - Hey, I'm Andy from deep lizard.
39:32 - In this episode, we'll see how we can use
a neural network for inference to predict
39:37 - on data from a test set using TensorFlow keras
API.
39:43 - As we touched on previously, whenever we train
a model, the hope is that we can then take
39:52 - that model and use it on new data that it
has not seen before during training, and hopefully
39:58 - the model is able to generalize well.
40:00 - And give us good results on this new data.
40:03 - So as a simple example, suppose that we trained
a network to be able to identify images of
40:09 - cats or dogs.
40:10 - And so during the training process, of course,
we had a training set that, say, we downloaded
40:16 - from a website with 1000s of images of cats
and dogs.
40:20 - So the hope is later that if we wanted to,
we could maybe build a web app, for example.
40:24 - And we could have people from all over the
world, submit their dog and cat photos and
40:29 - have our model, tell them with high accuracy,
whether or not their animal is a cat or a
40:35 - dog.
40:36 - So I don't know why anyone would actually
make that web app. but you get the point.
40:39 - The hope is that even though the images that
are being sent in from people around the world
40:45 - have their own cats and dogs, even though
those weren't included in the training set
40:49 - that the model was originally trained on.
40:51 - Hopefully, the models able to generalize well
enough to understand from what it's learned
40:56 - about dog and cat features, that it can predict
that Mandy's dog is actually a dog and not
41:03 - a cat, for example, we call this process inference.
41:06 - So the model takes what it learned during
training, and then uses that knowledge to
41:11 - infer things about data that it hasn't seen
before.
41:14 - In practice, we might hold out a subset of
our training data to put in a set called the
41:20 - test set.
41:21 - Typically, after the model has been trained
and validated, we take the model and use it
41:26 - for inference purposes against the test set,
just as one additional step to the validation
41:31 - to make sure that the model is generalizing
Well, before we deploy our model to production.
41:38 - So at this point, the model that we've been
working with over the last few episodes has
41:43 - been trained and validated.
41:45 - And given the metrics that we saw during the
validation process, we have a good idea that
41:50 - the model is probably going to do a pretty
good job at inference on the test set as well.
41:55 - In order to conclude that though, we first
would need to create a test set.
41:59 - So we're going to do that now.
42:01 - And then after we create the test set, then
we'll use the model for inference on it.
42:05 - Alright, so we are back in our Jupyter Notebook.
42:08 - And now we're going to go through the process
of creating the test set.
42:11 - And actually, if you just glance at this code
here, you can see that this whole process
42:16 - of setting up the the samples and labels list
and then generating the data from the imagine
42:24 - clinical trial that we discussed in a previous
episode, we're then taking that generated
42:29 - data and putting it into a NumPy array format,
then shuffling that data, and then scaling
42:37 - the data to be on a scale from zero to one,
rather than from the scale of 13 to 100.
42:43 - So actually, that is the same exact process
using almost the same exact code, except for
42:49 - we're working with our tests, labels, and
test samples, variables rather than train
42:55 - labels and train samples.
42:57 - So we're not going to go line by line through
this code.
42:59 - If you need a refresher, go check out the
earlier episode where we did the exact same
43:03 - process for the training set.
43:06 - The important thing to take from this process,
though, is that the test set should be prepared
43:12 - and processed in the same format as the training
data was.
43:16 - So we'll just go ahead and run the cells to
test and are to create and process the test
43:23 - data.
43:24 - And now, we are going to use our model to
predict on the test data.
43:31 - So to obtain predictions from our model, we
call predict on the model that we created
43:37 - in the last couple of episodes.
43:39 - So we are calling model dot predict.
43:41 - And we are first passing in this parameter
x which we're setting equal to our scaled
43:47 - test samples.
43:48 - So that is what we created in just the line
above, where we scaled our test samples to
43:54 - be on a scale from zero to one.
43:56 - So this is the data that we want our model
to predict on.
43:59 - Then we specify the batch size, and we are
setting the batch size equal to 10, which
44:05 - is the exact same batch size that we use for
our training data whenever we train the model
44:10 - as well.
44:11 - And then the last parameter we're specifying
is this verbose parameter, we're setting this
44:15 - equal to zero, because during predicting there
is not any output from this function that
44:21 - we actually care about seeing or that is going
to be any use to us at the moment.
44:25 - So we're setting that equal to zero to get
no output.
44:28 - Alright, so then if we run this, so then our
model predicts on all of the data in our test
44:34 - set.
44:35 - And if we want to have a visualization of
what each of these predictions from the model
44:41 - looks like for each sample, we can print them
out here.
44:45 - So looking at these predictions, the way that
we can interpret this is for each element
44:51 - within our test set.
44:53 - So for each sample in our tests that we are
getting a probability that maps to either
45:00 - The patient not experiencing a side effect,
or the patient experiencing a side effect.
45:06 - So for the first sample in our test set, this
prediction says that the model is a 92, or
45:13 - is assigning a 92% probability to this patient,
not experiencing a side effect, and just a
45:21 - around 8% probability of the patient experiencing
a side effect.
45:26 - So recall that we said, no side effect experience
was labeled as a zero, and a side effect experienced
45:33 - was labeled as a one.
45:35 - So that is how we know that this particular
probability maps to not having a side effect
45:41 - because it's in the zeroeth index.
45:43 - And this specific probability maps to having
a side effect because it is in the first index.
45:50 - So if we're interested in seeing only the
most probable prediction for each sample in
45:55 - the test set, then we can run this cell here,
which is taking the predictions and getting
46:01 - the index of the prediction with the highest
probability.
46:06 - And if we print that out, then we can see
that these are a little bit easier to interpret
46:12 - than the previous output.
46:14 - So we can see for the first sample that the
prediction is zero, the second sample is a
46:19 - one.
46:20 - And just to confirm, if we go back up here,
we can see that the first sample indeed has
46:26 - the higher probability of a label of zero,
meaning no side effects.
46:31 - And the second sample has a higher probability
of one meaning that the patient did experience
46:38 - a side effect.
46:39 - So from these prediction results, we're able
to actually see the underlying predictions.
46:44 - But we're not able to make much sense of them
in terms of how well the model did add these
46:49 - predictions, because we didn't supply the
labels to the model during inference in the
46:54 - same way that we do during training.
46:56 - This is the nature of inference.
46:58 - A lot of times inference is occurring once
the model has been deployed to production.
47:02 - So we don't necessarily have correct labels
for the data that the model is inferring from.
47:08 - If we do have corresponding labels for our
test set, though, which in our case, we do,
47:13 - because we are the ones who generated the
test data, then we can visualize the prediction
47:17 - results by plotting them to a confusion matrix.
47:21 - And that'll give us an overall idea at how
accurate our model was at inference on the
47:26 - test data.
47:27 - We'll see exactly how that's done in the next
episode.
47:31 - Be sure to check out the blog and other resources
available for this episode on depot's or.com
47:37 - as well as the tables at hive mind where you
can gain access to exclusive perks and rewards.
47:42 - Thanks for contributing to collective intelligence.
47:45 - Now let's move on to the next episode.
47:50 - Hey, I'm Andy from deep lizard.
47:53 - In this episode, we'll demonstrate how to
use a confusion matrix to visualize prediction
47:58 - results from a neural network during inference.
48:02 - In the last episode, we showed how we could
use our train model for inference on data
48:12 - contained in a test set.
48:14 - Although we have the labels for this test
set, we don't pass them to the model during
48:19 - inference, and so we don't get any type of
accuracy readings for how well the model does
48:24 - on the test set.
48:25 - Using a confusion matrix, we can visually
observe how well a model predicts on test
48:30 - data.
48:31 - Let's jump right into the code to see exactly
how this is done.
48:34 - We'll be using scikit Learn to create our
confusion matrix.
48:37 - So the first thing we need to do is import
the necessary packages that we'll be making
48:41 - use of next week create our confusion matrix
by calling this confusion matrix function
48:47 - from scikit learn.
48:49 - And we pass in our test labels as the true
labels.
48:55 - And we pass in our predictions as the predictions
and that the confusion matrix at specs and
49:03 - recall this rounded predictions variable,
as well as the test labels.
49:07 - These were created in the last episode.
49:10 - So rounded predictions recall was when we
use the arg max function to select only the
49:16 - most probable predictions.
49:17 - So now our predictions are in this format.
49:20 - And then our labels are zeros and ones that
correspond to whether or not a patient had
49:25 - side effects or not.
49:27 - So next we have this plot confusion matrix
function.
49:30 - And this is directly copied from socket learns
website.
49:34 - There's a link to the site on the corresponding
blog where you can copy this exact function.
49:39 - But this is just a function that socket learn
has created to be able to easily plot in our
49:44 - notebook, the confusion matrix, which is going
to be the actual visual output that we want
49:49 - to see.
49:50 - So we just run this cell to define that function.
49:54 - And now we create this list that has the labels
that we will use on our computer.
50:00 - In matrix, so we want, the labels have no
side effects and had side effects.
50:06 - Those are the corresponding labels for our
test data, then we're going to call the plot
50:11 - confusion matrix function that we just brought
in and defined above from scikit learn.
50:16 - And to that we are going to pass in our confusion
matrix.
50:21 - And the classes for the confusion matrix which
we are specifying cm plot labels, which we
50:26 - defined just right above.
50:29 - And lastly, just the title that is going to
be the title to display above the confusion
50:34 - matrix.
50:35 - So if we run this, then we actually get the
confusion matrix plot.
50:40 - Alright, so we have our predicted labels on
the x axis and our true labels on the y axis.
50:47 - So the way we can read this is that we look
and we see that our model predicted that a
50:52 - patient had no side effects 10 times when
the patient actually had a side effect.
50:59 - So that's incorrect predictions.
51:00 - On the flip side, though, the model predicted
that the patient had no side effects 196 times
51:08 - that the patient indeed had no side effects.
51:10 - So the this is the correct predictions.
51:14 - And actually, generally reading the confusion
matrix, looking at the top left to the bottom
51:20 - right diagonal, these squares here and blue
going across this diagonal are the correct
51:26 - predictions.
51:27 - So we can see total that the model predicted
200 plus 196.
51:32 - So 396 correct predictions out of a total
of 420, I think, yes.
51:41 - So all these numbers added up equal 420 396
out of 420 predictions were correct.
51:49 - So that gives us about a 94% accuracy rate
on our test set, which is equivalent to what
51:56 - we were seeing for our validation, accuracy
rate during training.
52:00 - So as you can see, a confusion matrix is a
great tool to use to be able to visualize
52:04 - how well our model is doing edits predictions,
and also be able to drill in a little bit
52:09 - further to see for which classes it might
need some work, be sure to check out the blog
52:14 - and other resources available for this episode
on the poser.com as well as the deep lizard
52:20 - hive mind where you can gain access to exclusive
perks and rewards.
52:24 - Thanks for contributing to collective intelligence.
52:27 - Now let's move on to the next episode.
52:31 - Hey, I'm Andy from deep lizard.
52:35 - And this episode will demonstrate the multiple
ways that we can save and load a karass sequential
52:40 - model.
52:47 - We have a few different options when it comes
to saving and loading a karass sequential
52:51 - model.
52:52 - And these all work a little bit differently
from one another.
52:55 - So we're going to go through all of the options.
52:57 - Now.
52:58 - We've been working with a single model over
the last few episodes.
53:01 - And we're going to continue working with that
one.
53:03 - Now, I've printed out a summary of that model
to refresh your memory about which one we've
53:07 - been working with.
53:08 - But just make sure that in your Jupyter Notebook
that you have that model already created,
53:13 - because we're going to now show how we can
save that model.
53:15 - Alright, so the first way to save a model
is just by calling the Save function on it.
53:20 - So to save, we pass in a path for which we
want to save our model, and then the model
53:29 - name or the file name that we want to save
our model under with the H five extension.
53:34 - So this h5 file is going to be where the model
is stored.
53:39 - And this code here is just a condition that
I'm checking to see if the model is not already
53:43 - saved to disk first, then save it because
I don't want to continue saving the model
53:48 - over and over again, on my machine if it's
already been saved.
53:51 - So that's what this this condition is about.
53:54 - But this model dot save function is the first
way that we can save the model.
53:59 - Now when we save using this way, it saves
the architecture of the model, allowing us
54:04 - to be able to recreate it with the same number
of learnable parameters and layers and nodes
54:09 - etc.
54:10 - It also saves the weights of the model.
54:12 - So if the models already been trained, then
the weights that it has learned and optimized
54:18 - for are going to be in place within this saved
model on disk.
54:23 - It also saves the training configuration.
54:25 - So things like our loss and our optimizer
that we set whenever we compile the model,
54:31 - and the state of the optimizer is also saved.
54:34 - So that allows us if we are training the model,
and then we stop and save the model to disk,
54:40 - then we can later load that model again and
pick up training where we left off because
54:44 - the state of the optimizer will be in that
saved state.
54:48 - So this is the most comprehensive option when
it comes to saving the model because it saves
54:54 - everything, the architecture, the learnable
parameters and the state of the model.
55:00 - where it left off with training.
55:02 - So if we want to load a model later that we
previously saved to disk, then we need to
55:07 - first import this load model function from
TensorFlow Kerris dot models.
55:12 - And then we create a variable.
55:14 - In this case, I'm calling it a new model,
and then setting it to load model and then
55:19 - pointing to where our saved model is on disk.
55:23 - So then, if we run that, and then look at
a summary of our new model, we can see Indeed,
55:29 - it is an exact replica in terms of its architecture
as the original model up here that we had
55:37 - previously saved to disk.
55:39 - Also, we can look at the weights of the new
model, we didn't look at the weights ahead
55:44 - of time to be able to compare them directly.
55:47 - But this is showing you that you can inspect
the weights to look and comparatively see
55:52 - that the weights are actually the same as
the previous models weights.
55:56 - If you were to have taken a look at those
beforehand.
56:00 - We can also look at the optimizer just to
show you that although we never set an optimizer
56:05 - explicitly for our new model, because we are
loading it from the saved model, it does indeed
56:12 - use the atom optimizer that we set a while
back whenever we compiled our model for training.
56:18 - Alright, so that's it for the first saving
and loading option.
56:22 - And again, that is the most comprehensive
option to save and load everything about a
56:27 - particular model.
56:28 - The next option that we'll look at is using
this function called to JSON.
56:33 - So we call model.to.
56:34 - json, if we only need to save the architecture
of the model.
56:38 - So we don't want to set its weights or the
training configuration, then we can just save
56:43 - the model architecture by saving it to a JSON
string.
56:48 - So I'm using this example here, creating a
variable called JSON string, setting it equal
56:54 - to model.to.
56:55 - json.
56:56 - And remember, model is our original model
that we've been working with so far up to
57:01 - this point.
57:02 - So we call to JSON.
57:04 - And now if we print out this JSON string,
then we can see we get this string of details
57:13 - about the model architecture.
57:14 - So it's a sequential model.
57:16 - And then it's got the layers organized, with
the individual dense layers and all the details
57:22 - about those specific layers from our original
model.
57:26 - Now, if at a later point, we want to create
a new model with our older models, architecture,
57:34 - then if we save it to a JSON string, then
we can import the model from JSON function
57:40 - from TensorFlow Kerris dot models.
57:43 - And now we're creating a new variable called
model architecture.
57:47 - And we are loading in the JSON string using
the model from JSON function.
57:53 - So now we have this new model, which I'm just
calling model architecture.
57:59 - And if we look at the summary of that, then
again, we can see that this is identical to
58:05 - the summary of the original model.
58:08 - So we have a new model in place now, but we
only have the architecture in place.
58:13 - So you would have to retrain it to update
its weights.
58:16 - And we would need to compile it to get an
optimizer and our loss and everything like
58:20 - that defined, this only creates the model
from an architecture standpoint.
58:24 - Before moving on to our third option, I just
wanted to mention a brief point that we can
58:30 - go through this same exact process, but using
a YAML string instead of a JSON string.
58:37 - So the function to create a gamble string
is just model.to a gamble instead of to JSON.
58:45 - And then the function to load a YAML string
is model from a gamma model from a YAML instead
58:53 - of model from JSON.
58:55 - Alright, so our next option to save a model
is actually just to save the weights of the
59:01 - model.
59:02 - So if you only need to save the weights, and
you don't need to save the architecture, nor
59:06 - any of the training configurations, like the
optimizer or loss, then we can save solely
59:11 - models weights by using the Save weights function.
59:15 - So to do that, we just call model dot save
weights.
59:20 - And this looks exactly the same as when we
called model dot save, we're just passing
59:24 - a path on disk to where to save our model
along with the file name ending with an H
59:30 - five extension.
59:31 - So I'm calling this my model weights dot h
five.
59:34 - And again, we have this condition here where
I'm just checking if this H five file has
59:39 - already been saved to disk, otherwise, I'm
not going to keep saving it over and over
59:42 - again.
59:43 - Now, the thing with this is that when we save
only the weights, if we want to load them
59:48 - at a later time, then we don't have a model
already in place because we didn't save the
59:54 - model itself.
59:55 - We only save the weights.
59:56 - So to be able to bring in our weights to a
new model then we would then need to create
60:01 - a second model at that point with the same
architecture.
60:05 - And then we could load the weights.
60:07 - And so that's what we're doing in this cell.
60:09 - I'm defining this model called model two.
60:12 - And it is the exact same model from an architecture
standpoint as the first model.
60:17 - So if we run this, then at that point, we
have the option to load weights into this
60:22 - model.
60:23 - And the shape of these weights is going to
have to match the shape of what this model
60:29 - architecture is essentially.
60:31 - So we couldn't have a model with five layers
here to find, for example, and load these
60:35 - weights in because there wouldn't be a direct
mapping of where these particular weights
60:40 - should be loaded.
60:41 - So that's why we have the same exact architecture
here as our original model.
60:46 - So to load our weights, we call load weights.
60:48 - And we point to the place on disk where our
weights are saved.
60:52 - And then we can call get weights on our new
model, and see that our new model has been
60:58 - populated with weights from our original model.
61:03 - Alright, so now you know all the ways that
we can say various aspects of a karass sequential
61:08 - model.
61:09 - Be sure to check out the blog and other resources
available for this episode on people's are.com
61:15 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
61:20 - rewards.
61:21 - Thanks for contributing to collective intelligence.
61:23 - Now, let's move on to the next episode.
61:28 - Hey, I may be from people's or, in this episode,
we'll go through all the necessary image preparation
61:35 - and processing steps needed to train our first
convolutional neural network.
61:46 - Our goal over the next few episodes will be
to build and train a convolutional neural
61:51 - network that can classify images as cats or
dogs, the first thing that we need to do is
61:56 - get and prepare our data set for which we'll
be training our model.
62:00 - We're going to work with the data set from
the kaggle cats versus dogs competition.
62:05 - And you can find a link to download the dataset
in the corresponding blog for this episode
62:10 - on people's are.com.
62:11 - So we're mostly going to organize our data
on this programmatically.
62:15 - But there's a couple of manual steps that
we'll go through first.
62:18 - So after you've downloaded the data set from
kaggle, you will have this zip folder here.
62:24 - And if we look inside, then this is the contents
we have a zipped train folder, and a zipped
62:30 - test folder along with the sample submission
CSV.
62:33 - So we actually are not going to be working
with this, or this.
62:38 - So you can delete the test that as well as
the CSV file, we're going to only be working
62:43 - with this train zip.
62:46 - So we want to extract the high level cats,
or dogs versus cats at first, and then extract
62:52 - the train zip.
62:54 - So because that takes a while I went ahead
and did that here.
62:58 - So now I have just this train directory because
I moved the test directory elsewhere and deleted
63:04 - the CSV file.
63:06 - So now I have this extracted train folder.
63:10 - And I go in here and I have a nested train
folder.
63:12 - As you can see, I'm already in train once.
63:15 - Now I'm in train again.
63:17 - And in here we have all of these images of
cats, and dogs.
63:22 - Okay, so that is the way that the data will
come downloaded.
63:26 - The first step, or the next step that we need
to do now is to come into here and grab all
63:33 - of these images.
63:36 - And we're going to Ctrl x or cut all of these
and bring them up to the first train directory.
63:43 - So we don't want this nested directory structure.
63:47 - Instead, we're going to place them directly
within the first train directory.
63:51 - Alright, now all of our images have been copied
into our base train directory here.
63:56 - So this nested train directory that the images
previously belong to is now empty.
64:01 - So we can just go ahead and delete this one.
64:03 - Alright, so now our directory structure is
just this dogs vs. Cat top directory within
64:13 - we have a train directory.
64:15 - And then we have all of the images within
our train directory.
64:19 - Both of cats and dogs.
64:21 - The last step is to just move the dogs versus
cats directory that has all the data in it
64:27 - to be in the place on disk where you're going
to be working.
64:30 - So for me, I am in relative to my Jupyter
Notebook, I am within a directory called data
64:38 - and I have placed dogs versus cats here.
64:40 - Alright, so that's it for the manual labor
now everything else that will do to organize
64:45 - the data, and then later process the data
will be programmatically through code.
64:50 - Alright, so we're now here within our Jupyter
Notebook.
64:52 - And First things first, we need to import
all of the packages that we'll be making use
64:56 - of and all of the packages here are not just
This specific episode on processing the image
65:02 - data.
65:03 - But actually, these are all of the packages
that we'll be making use of over the next
65:07 - several episodes as we're working with cn
ns.
65:10 - Alright, so we'll get that taken care of.
65:13 - And now, just this cell here is making sure
if we are using a GPU that TensorFlow is able
65:19 - to identify it correctly, and we are an enabling
memory growth on the GPU as well.
65:24 - If you're not using a GPU, then no worries,
as mentioned earlier, you are completely fine
65:28 - to follow this course with a CPU only.
65:31 - Alright, so now we're going to pick back up
with organizing our data on disk.
65:36 - So assuming that we've gone through the first
steps from the beginning of this episode,
65:41 - we're now going to organize the data and to
train valid and test ders, which correspond
65:47 - to our training validation and test sets.
65:50 - So this group here is going to first change
directories into our dog vs. Cat directory.
65:56 - And then it's going to check to make sure
that the directory structure that we're about
66:01 - to make does not already exist.
66:03 - And if it doesn't, it's going to proceed with
the rest of this script.
66:07 - So the first thing that it's doing as long
as the directory structure is not already
66:11 - in place, is making the following directories.
66:15 - So we already have a train directory.
66:18 - So it's going to make a nested dog and cat
directory within train.
66:23 - And then additionally is going to make a valid
directory that contains dog and cat directories,
66:29 - and a test directory, which also contains
dog and cat directories.
66:34 - Now, this particular data set contains 25,000
images of dogs and cats.
66:40 - And that is pretty much overkill for the tasks
that we will be using these images for.
66:46 - In the upcoming episodes, we're actually only
going to use a small subset of this data,
66:52 - you're free to work with all the data if you'd
like.
66:54 - But it would take a lot longer to train our
networks and work with the data in general
66:58 - if we were using the entire set of it.
67:01 - So we are going to be working with a subset
consisting of 1000 images in our training
67:07 - set 200 in our validation set, and 100 in
our test set, and each of those sets are going
67:14 - to be split evenly among cats and dogs.
67:17 - So that's exactly what this block of code
here is doing.
67:21 - It's going into the images that are in our
dogs vs cat directory, and moving 500 randomly
67:29 - moving 500 cat images into our train cat directory
500 dog images into our train dog directory.
67:38 - And then similarly doing the same thing for
our valid, our valid our validation set, both
67:44 - for cat and dogs and then our test set, both
for cat and dogs with just these quantities
67:50 - differing regarding the amounts that I stated
earlier for each of the sets.
67:54 - And we're able to understand which images
are cats and which are dogs based on the names
68:00 - of the files.
68:01 - So if you saw earlier, the cat images actually
had the word cat in the file names and then
68:07 - the dog images had the word dog in the file
names.
68:10 - So that's how we're able to select dog images
and cat images here with this script.
68:15 - Alright, so after this script runs, we can
pull up our file explorer and look at the
68:22 - directory structure and make sure it is what
we expect.
68:25 - So we have our dogs vs cat directory within
our data directory here.
68:29 - So if we enter, then we have test train and
valid directories.
68:34 - Inside test, we have cat that has cat images,
and inside dog, we have dog images.
68:42 - If we back out and go to train, we can see
similarly.
68:47 - And if we go into valid we can see similarly.
68:51 - And you can select one of the folders and
look at the properties to to see how many
68:56 - files exist within the directory to make sure
that is it is the amount that we chose to
69:01 - put in from our script and that we didn't
accidentally make any type of error.
69:06 - So if we go back to the dogs vs cats the root
directory here, you can see we have all of
69:12 - these cat and dog images leftover.
69:15 - These were the remaining 23,000 or so that
were left over after we moved our subset into
69:23 - our train valid and test directories.
69:26 - So you're free to make use of these in any
way that you want or delete them or move them
69:30 - to another location.
69:31 - All of what we'll be working with are in these
three directories here.
69:35 - Alright, so at this point, we have obtained
the data and we have organized the data.
69:40 - Now it's time to move on to processing the
data.
69:44 - So if we scroll down, first, we are just creating
these variables here where we have assigned
69:51 - our train valid and test paths.
69:54 - So this is just pointing to the location on
disk where our different data sets reside.
70:00 - Now recall earlier in the course, we talked
about that whenever we train a model that
70:04 - we need to put the data into a format that
the model expects.
70:08 - And we know that when we train a karass sequential
model, that the model receives the data whenever
70:14 - we call the fit function.
70:16 - So we are going to put our images and do the
format of a karass generator.
70:21 - And we're doing that in this cell here.
70:24 - We're creating these train valid and test
batches, and setting them equal to image data
70:31 - generator dot flow from directory, which is
going to return a directory iterator.
70:38 - Basically, it's going to create batches of
data from the directories where our datasets
70:44 - reside.
70:45 - And these batches of data will be able to
be passed to the sequential model using the
70:51 - fit function.
70:52 - So now let's look exactly at how we are defining
these variables.
70:56 - So let's focus just on train batches for now.
70:59 - So we're setting train batches equal to image
data generator dot flow from directory.
71:05 - But first to image data generator, we are
specifying this pre processing function and
71:11 - setting that equal to tf Kerris dot applications
that VGG 16 dot pre process input.
71:18 - So I'm just going to tell you for now that
this is a function that is going to apply
71:24 - some type of pre processing on the images
before they get passed to the network that
71:30 - we'll be using.
71:31 - And we're processing them in such a way that
is equivalent to the way that a very popular
71:38 - model known as VGG 16, we're processing our
images in the same format as which images
71:44 - that get passed to this VGG 16 model our process.
71:48 - And we're going to talk about more about this
in a future episode.
71:51 - So don't let it confuse you now just know
that this is causing some type of processing
71:57 - to occur on our images.
71:58 - And we'll talk more about it in a future episode
and not stress on it now, because it's not
72:02 - necessarily very important for us right at
this moment, the technical details of that
72:07 - at least.
72:08 - So besides that, when we call flow from directory,
this is where we are passing in our actual
72:14 - data and specifying how we want this data
to be processed.
72:19 - So we are setting directory equal to train
path, which appear we defined the location
72:26 - on disk where our training set was under the
train PATH variable.
72:31 - And then we're setting target size equal to
224 by 224.
72:35 - So this is the height and width that we want
the cat and dog images to be resized to.
72:41 - So if you're working with an image data set
that has images of varying sizes, or you just
72:46 - want to scale them up or scale them down,
this is how you can specify that to happen.
72:51 - And this will resize all images in your data
set to be of this height and width before
72:56 - passing them to our network.
72:58 - Now we are specifying our classes, which are
just the classes for the potential labels
73:05 - of our data set.
73:07 - So cat or dog, and we are setting our batch
size to 10.
73:12 - We do the exact same thing for the validation
set.
73:16 - And the test set.
73:18 - Everything is the exact same for both of them,
except for where each of these sets live on
73:22 - disk as being specified here under the directory
parameter.
73:30 - And then the only other difference is here
for our test batches.
73:33 - We are specifying this shuffle equals false
parameter.
73:38 - Now, this is because whenever we use our test
batches later for inference to get our model
73:44 - to predict on images of cats and dogs after
training and validation has been completed,
73:50 - we're going to want to look at our prediction
results in a confusion matrix like we did
73:54 - in a previous video for a separate data set.
73:56 - And in order to do that, we need to be able
to access the unsettled labels for our test
74:02 - set.
74:03 - So that's why we set shuffle equals false
for only this set.
74:07 - For both validation and training sets, we
do want the data sets to be shuffled.
74:11 - Alright, so we run this and we get the output
of found 1000 images belonging to two classes.
74:19 - And that is corresponding to our train batches
found at 200 images belonging to two classes,
74:25 - which corresponds to valid valid batches,
and then the 100 belonging to two classes
74:31 - corresponding to our test batches.
74:32 - So that is the output that you want to see
for yourself.
74:35 - That's letting you know that it found the
images on disk that belong to both the cat
74:40 - and dog classes that you have specified here.
74:44 - So if you are not getting this at this point,
if you get found at zero images, then perhaps
74:48 - you're pointing to the wrong place on disk.
74:51 - You just need to make sure that it's able
to find all the images that you set up previously.
74:55 - Right and here we are just verifying that
that is indeed the case.
74:59 - Now Next, we are going to just grab a single
batch of images and the corresponding labels
75:05 - from our train batches.
75:09 - And remember, our batch size is 10.
75:11 - So this should be 10 images along with the
10 corresponding labels.
75:16 - Next, we're introducing this function plot
images that we're going to use to plot the
75:22 - images from our train batches that we just
obtained above.
75:26 - And this function is directly from tensor
flows website.
75:28 - So check the link in the corresponding blog
for this episode on people's or.com.
75:33 - To see, to be able to get to the tensor flows
site where exactly I pulled this off of.
75:40 - So we will define this function here.
75:42 - Alright, so now we're just going to use this
function to plot our images from our test
75:48 - batches here.
75:49 - And we're going to print the corresponding
labels for those images.
75:54 - So if we scroll down, we can see this is what
a batch of training data looks like.
76:01 - So this might be a little bit different than
what you expected, given the fact that it
76:05 - looks like the color data has been a little
bit distorted.
76:09 - And that's due to the pre processing function
that we called to pre process the images in
76:14 - such a way that in the same type of way that
images get pre processed for the famous VGG
76:20 - 16 model.
76:22 - So like I said, we're going to discuss in
detail what exactly that pre processing function
76:26 - is doing technically, as well as why we're
using it in a later video.
76:31 - But for now, just know that it's skewing the
RGB data in some way.
76:36 - So we can still make out the fact that this
is a cat.
76:40 - And this looks like a cat.
76:42 - This is the dog, dog, dog, dog cat.
76:45 - Yeah, so we can still kind of generally make
out what these images are, but the color data
76:50 - is skewed.
76:51 - But don't worry too much about the technical
details behind that.
76:53 - For right now, just know that this is what
the data looks like before we pass it to the
76:58 - model.
76:59 - And here are the corresponding labels for
the data.
77:02 - So we have these one hot encoded vectors that
represent either cat or dog.
77:09 - So a one zero represents a cat, and a zero,
a one represents a dog.
77:17 - Okay, so I guess I was wrong earlier with
thinking that this one was a dog.
77:22 - This one is a cat, because as we can see,
it maps to the 101 hot encoding.
77:29 - And if you don't know what I mean by one hot
encoding, then check out the corresponding
77:32 - video for that in the deep learning fundamentals
course on depot's or.com.
77:36 - But yeah, we can see that 01 is the vector
used to represent the label of a dog.
77:42 - So this one is a dog.
77:44 - And the next two are dogs as well, this one
and this one.
77:48 - Now, just a quick note about everything that
we've discussed up to this point, sometimes
77:53 - we do not have the corresponding labels for
our test set.
77:57 - So in the examples that we've done so far,
in this course, we've always had the corresponding
78:02 - labels for our test set.
78:04 - But in practice, a lot of times you may not
have those labels.
78:07 - And in fact, if we were to have used the downloaded
test directory that came from the kaggle download,
78:14 - then we would see that that test directory
does not have the images labeled with the
78:19 - cat or dog.
78:20 - So in this case, we do have the test labels
for the cat and dog images since we pulled
78:27 - them from the original training set from kaggle
that did have the corresponding labels.
78:32 - But if you don't have access to the test labels,
and you are wondering how to process your
78:38 - test data accordingly, then check the blog
for this episode on dibbles comm I have a
78:43 - section there that demonstrates what you need
to do differently from what we showed in this
78:47 - video if you do not have access to the labels
for your test set.
78:52 - Alright, so now we have obtained our image
data, organized it on disk and processed it
78:58 - accordingly for our convolutional neural network.
79:01 - So now in the next episode, we are going to
get set up to start building and training
79:05 - our first CNN.
79:07 - Be sure to check out the blog and other resources
available for this episode on the poser.com
79:13 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
79:18 - rewards.
79:19 - Thanks for contributing to collective intelligence.
79:21 - Now let's move on to the next episode.
79:26 - Hey, I'm Andy from deep lizard.
79:30 - And this episode will demonstrate how to build
a convolutional neural network and then train
79:34 - it on images of cats and dogs using tensor
flows integrated Kerris API.
79:41 - We'll be continuing to work with the cat and
dog image data that we created in the last
79:51 - episode.
79:52 - So make sure you still have all of that in
place, as well as the imports that we brought
79:56 - in in the last episode as well as we'll be
making use of those imports.
79:59 - For the next several videos of working with
CNN, so to create our first CNN, we will be
80:04 - making use of the Kerris sequential model.
80:08 - And recall, we introduced this model in an
earlier episode when we were working with
80:12 - just plain simple numerical data.
80:14 - But we'll continue to work with this model
here with our first CNN.
80:18 - So the first layer to this model that we pass
is a comma to D layer.
80:22 - So this is just our standard convolutional
layer that will accept image data.
80:28 - And to this layer, we're arbitrarily setting
the filter value equal to 32.
80:33 - So this first convolutional layer will have
32 filters with a kernel size of three by
80:39 - three.
80:40 - So the choice of 32 is pretty arbitrary, but
the kernel size of three by three is a very
80:45 - common choice for image data.
80:47 - Now, this first column to D layer will be
followed by the popular relu activation function.
80:54 - And we are specifying padding equals Same
here.
80:57 - And this just means that our images will have
zero padding, padding the outside so that
81:03 - the dimensionality of the images aren't reduced
after the convolution operations.
81:08 - So lastly, for our first layer only, we specify
the input shape of the data.
81:15 - And recall we touched on this parameter previously,
you can think of this as kind of creating
81:19 - an implicit input layer for our model.
81:23 - This comp 2d is actually our first hidden
layer, the input layer is made up of the input
81:28 - data itself.
81:30 - And so we just need to tell the model the
shape of the input data, which in our case
81:35 - is going to be 224.
81:36 - By 224.
81:38 - Recall, we saw that we were setting that target
size parameter to do 24 by 224.
81:44 - Whenever we created our valid test and train
directory iterators, we said that we wanted
81:51 - our images to be of this height and width.
81:54 - And then this three here is regarding the
color channels.
81:57 - Since these.
81:58 - Since these images are an RGB format, we have
three color channels, so we specified this
82:04 - input shape.
82:05 - So then we follow our first convolutional
layer with a max pooling layer, where we're
82:11 - setting our pool size to two by two and our
strides by two.
82:15 - And if you are familiar with Max pooling,
then you know this is going to cut our image
82:20 - dimensions in half.
82:22 - So if you need to know more about max pooling,
or you just need a refresher, same thing with
82:26 - padding here for zero padding, activation
functions, anything like that, then be sure
82:31 - to check those episodes out and the corresponding
deep learning fundamentals course on depot's
82:36 - or.com.
82:37 - So after this max pooling layer, we're then
adding a another convolutional layer, that
82:41 - looks pretty much exactly the same as the
first one except for four, we're not including
82:46 - the input shape parameter, since we only specify
that for our first hidden layer, and we are
82:52 - specifying the filters to be 64 here instead
of 32.
82:56 - So 64 is again, an arbitrary choice, I just
chose that number here.
83:01 - But the general rule of increasing functions
as you go until later layers of the network
83:08 - is common practice, we then follow this second
convolutional layer with another max pooling
83:13 - layer identical to the first one, then we
flatten all of this into a one dimensional
83:19 - tensor before passing it to our dense output
layer, which only has two nodes corresponding
83:26 - to cat and dog.
83:28 - And our output layer is being followed by
the softmax after activation function, which
83:34 - as you know, is going to give us probabilities
for each corresponding output from the model.
83:40 - Alright, so if we run this, we can then check
out the summary of our model.
83:46 - And this is what we have exactly what we built,
along with some additional details about the
83:52 - learnable parameters and the output shape
of the network.
83:56 - So these are things that are also covered
in the in the deep learning fundamentals course
84:01 - as well, if you want to check out more information
about learnable parameters and such.
84:05 - So now that our model is built, we can prepare
it for training by calling model dot compile,
84:10 - which again, we have already used in a previous
episode, when we were just training on numerical
84:16 - data.
84:17 - But as you will see here, it looks pretty
much the same as before, we are setting our
84:22 - optimizer equal to the atom optimizer with
a learning rate of 0.0001.
84:28 - And we are using categorical cross entropy.
84:32 - And we are looking at the accuracy as our
metrics to be able to judge the model performance.
84:38 - Just a quick note before moving on.
84:41 - We are using categorical cross entropy.
84:43 - But since we have just two outputs from our
model, either cat or dog, then it is possible
84:49 - to instead use binary cross entropy.
84:53 - And if we did that, then we would need to
just have a one single output node from our
84:58 - model instead of two And then rather than
following our output layer with the softmax
85:03 - activation function, we would need to follow
it with sigmoid.
85:07 - And both of these approaches using categorical
cross entropy loss with the setup that we
85:11 - have here, or using binary cross entropy loss
with the setup that I just described, work
85:17 - equally well, they're totally equivalent,
and will yield the same results, categorical
85:22 - cross entropy, and using softmax on the app,
as the activation function for the output
85:28 - layer, or just a common approach for when
you have more than two classes.
85:33 - So I like to continue using that approach,
even when I only have two classes just because
85:37 - it's general.
85:38 - And it's the case that we're going to use
whenever there's more than two classes anyway,
85:41 - so I just like to stick with using the same
type of setup, even when we only have two
85:46 - outputs.
85:47 - Alright, so after compiling our model, we
can now train it using model dot fit, which
85:52 - we should be very familiar with up to this
point.
85:55 - So to the fit function, we are first specifying
our training data, which is stored in train
86:00 - batches.
86:02 - And then we specify our validation data, which
is stored in valid batches.
86:07 - Recall, this is a different way of creating
validation data, as we spoke about in an earlier
86:13 - episode, we're not using validation split
here, because we've actually created a validation
86:19 - set separately ourselves before fitting the
model.
86:22 - So we are specifying that separated set here
as the validation data parameter, then we
86:28 - are setting our epochs equal to 10.
86:30 - So we're only going to train for 10 runs this
time and setting verbose, verbose equal to
86:36 - two so that we can see the most verbose output
during training.
86:41 - And one other thing to mention is that you
will see here that we are specifying x just
86:46 - as we have in the past, but we are not specifying
y, which is our target data usually.
86:52 - And that's because when data is stored as
a generator, as we have here, the the generator
86:59 - itself actually contains the corresponding
labels.
87:02 - So we do not need to specify them separately,
whenever we call fit, because they're actually
87:07 - contained within the generator itself.
87:10 - So let's go ahead and run this now.
87:12 - Alright, so the model just finished training.
87:14 - So let's check out the results.
87:16 - Before we do just if you get this warning,
it appears from the research I've done to
87:20 - be a bug within TensorFlow that's supposed
to be fixed in a in the next release actually,
87:25 - is what I read.
87:26 - So you can just safely ignore this warning,
it has no impact on our training.
87:31 - But if we scroll down and look at these results,
then we can see that by our 10th epoch, our
87:38 - accuracy on our training set has reached 100%.
87:41 - So that is great.
87:43 - But our validation accuracy is here at 69%.
87:47 - So not so great, we definitely see that we
have some overfitting going on here.
87:53 - So if this was a model that we really cared
about, and that we really wanted to use and
87:56 - be able to deploy to production, then in this
scenario, we will need to stop what we're
88:01 - doing and combat this overfitting problem
before going further.
88:05 - But what we are going to do is that we'll
be seeing in an upcoming episode, how we can
88:10 - use a pre trained model to perform really
well on this data.
88:15 - And that'll get us exposed to the concept
of fine tuning.
88:19 - Before we do that, though, we are going to
see in the next episode how this model holds
88:23 - up to inference at predicting on images in
our test set.
88:28 - Be sure to check out the blog and other resources
available for this episode on the poser.com
88:33 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
88:39 - rewards.
88:40 - Thanks for contributing to collective intelligence.
88:42 - Now let's move on to the next episode.
88:47 - Hey, I may be from deep lizard.
88:51 - And this episode will demonstrate how to use
a convolutional neural network for inference
88:55 - to predict on image data using tensor flows
integrated keras API.
89:06 - Last time, we built and trained our first
cnn against cat and dog image data, and we
89:12 - saw that the training results were great with
the model achieving 100% accuracy on the training
89:17 - set.
89:18 - However, it lagged behind by quite a good
bit at only 70% accuracy on our validation
89:24 - set.
89:25 - So that tells us that the model wasn't generalizing
as well as we hoped.
89:29 - But nonetheless, we are going to use our model
now for inference to predict on cat and dog
89:35 - images in our test set.
89:37 - Given the less than decent results that we
saw from the validation performance.
89:41 - Our expectation is that the model is not going
to do so well on the test set either it's
89:46 - probably going to perform at around the same
70% rate.
89:50 - But this is still going to give us exposure
about how we can use a CNN for inference using
89:56 - the Kerris sequential API.
89:58 - Alright, so we are back in Jupyter Notebook
and we need to make sure that we have all
90:03 - the code in place from the last couple of
episodes as we will be continuing to make
90:08 - use of both our model that we built last time,
as well as our test data from whenever we
90:13 - prepared the data sets.
90:16 - So the first thing we're going to do is get
a batch of test data from our test batches.
90:23 - And then we're going to plot that batch, we're
going to plot the images specifically, and
90:27 - then we're going to print out the corresponding
labels for those images.
90:31 - And we're using before we do that just a reminder,
this plot images function that we introduced
90:37 - in the last couple of episodes.
90:39 - Alright, so if we scroll down, we have our
test batches.
90:44 - Recall, we had the discussion about why the
image data looks the way that it does in terms
90:48 - of the color being skewed last time, but we
can see that just by looking even though we
90:54 - have kind of distorted color we have, these
are all actually cats here.
91:00 - And by looking at the corresponding label
for these images, we can see that they are
91:05 - all labeled with the one hot encoded vector
of one zero, which we know is the label for
91:11 - cat.
91:12 - So if you're wondering why we have all cats
as our first 10 images in our first batch
91:18 - here, that is because we're call whenever
we created the test set, we specified that
91:24 - we did not want it to be shuffled.
91:26 - And that was so that we could do the following.
91:28 - If we come to our next cell, and we run test
batches classes, then we can get a array that
91:35 - has all of the corresponding labels for each
image in the test set.
91:41 - So given that we have access to the ns shuffled
labels for the test set, that's why we don't
91:48 - want to shuffle the test set directly, because
we want to be able to have this one one direct
91:54 - mapping from the uncoupled labels to the test
data set.
92:00 - And if we were to shuffle the test data set,
every time we generated a batch, then we wouldn't
92:04 - be able to have the correct mapping between
labels and samples.
92:09 - So we care about having the correct mapping,
because later after we get our predictions
92:14 - from the model, we're going to want to plot
our predictions to a confusion matrix.
92:19 - And so we want the corresponding labels that
belong to the samples in the test set.
92:25 - Alright, so next, we're actually going to
go ahead and obtain our predictions by calling
92:30 - model dot predict just as we have an earlier
episodes for other data sets.
92:36 - And to x, we are specifying our test batches,
so all of our test data set, and we are choosing
92:44 - verbose to be zero to get no output whenever
we run our predictions.
92:50 - So here, we are just printing out the arounded
predictions from the model.
92:55 - So the way that we can read this is first,
each one of these arrays is a prediction for
93:02 - a single sample.
93:04 - So if we just look at the first one, this
is the prediction for the first sample and
93:10 - the test set.
93:11 - So this one, wherever there is a one with
each prediction is the one is the index for
93:17 - the output class that had the highest probability
from the model.
93:22 - So in this case, we see that the zeroeth index
had the highest probability.
93:28 - So we can just say that the label that the
model predicted for this first sample was
93:34 - a zero, because we see that there is a one
here in the zeroeth index.
93:39 - And if we look at the first element, or if
we look at the first label here, up here,
93:46 - for our first test sample, it is indeed zero.
93:51 - So we can eyeball that and see that the model
did accurately predict all the way down to
93:57 - here, because that's the first 123456 and
123456.
94:02 - Okay, so But then, whenever we see that the
model predicted the first index to be the
94:10 - highest probability, that means that the,
that the model predicted an output label of
94:15 - a one, and so that corresponds to dog.
94:19 - So it's hard for us to kind of draw an overall
conclusion about the prediction accuracy for
94:25 - this test set, just eyeballing the results
like this.
94:30 - But if we scroll down, then we know that we
have the tool of a confusion matrix that we
94:36 - can use to make visualizing these results
much easier, like we've seen in previous episodes
94:41 - of this course already.
94:43 - So we are going to do that.
94:44 - Now we're going to create a confusion matrix
using this confusion matrix function from
94:49 - scikit learn which we've already been introduced
to and we are passing in our true labels using
94:56 - test batches classes.
94:58 - Recall that we just touched on that a few
minute ago.
95:01 - And for our predictive labels, we are passing
in the predictions from our model, we are
95:07 - getting the we're actually passing in the
index of each.
95:13 - We're actually using arg max to pass in the
index of where the most probable prediction
95:19 - was from our predictions list.
95:21 - So this is something that we've already covered
in previous episodes for why we do that.
95:26 - So if we run that, we're now going to bring
in this plot confusion matrix, which we've
95:32 - discussed is directly from psychic Lauren's
website, link to that is, and the corresponding
95:37 - blog for this episode on people's are calm.
95:40 - This is just going to allow us to plot our
confusion matrix in a moment.
95:44 - And now if we look at the class indices, we
see that cat is first and dog is second.
95:51 - So we just need to look at that so that we
understand in which order, we should put our
95:56 - plot labels for our confusion matrix.
95:59 - And next we call plot confusion matrix and
pass in the confusion matrix itself, as well
96:06 - as the labels for the confusion matrix and
a title for the entire matrix.
96:11 - So let's check that out.
96:13 - Alright, so from what we learned about how
we can easily interpret a confusion matrix,
96:18 - we know that we can just look at this diagonal
here running from top left to bottom right,
96:23 - to see what the model predicted correctly.
96:26 - So not that great, the model is definitely
overfitting at this point.
96:30 - So like I said, if this was a model that we
were really concerned about, then we would
96:34 - definitely want to combat that overfitting
problem.
96:37 - But for now, we are going to move on to a
new model using a pre trained state of the
96:42 - art model called VGG 16.
96:45 - In the next episode, so that we can see how
well that model does on classifying images
96:49 - of cats and dogs.
96:51 - Be sure to check out the blog and other resources
available for this episode on the poser.com
96:56 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
97:02 - rewards.
97:03 - Thanks for contributing to collective intelligence.
97:05 - Now let's move on to the next episode.
97:11 - Hey, I'm Andy from be blizzard.
97:14 - And this episode will demonstrate how we can
fine tune a pre train model to classify images
97:19 - using tensor flows keras API.
97:28 - The pre train model that we'll be working
with is called VGG 16.
97:32 - And this is the model that won the 2014 image
net competition.
97:36 - In the image net competition, multiple teams
compete against each other to build a model
97:41 - that best classifies images within the image
net library.
97:46 - And the image net library is made up of 1000s
of images that belong to 1000 different classes
97:53 - using Kerris will import this VGG 16 model,
and then fine tune it to not classify on one
98:00 - of the 1000 categories for which it was originally
trained, but instead only on two categories,
98:07 - cat and dog.
98:09 - Note, however, that cats and dogs were included
in the original image net library for which
98:14 - VGG 16 was trained on.
98:17 - And because of this, we won't have to do much
tuning to change the model from classifying
98:23 - from 1000 classes to just the two cat and
dog classes.
98:26 - So the overall fine tuning that we'll do will
be very minimal.
98:30 - in later episodes, though, we'll do more involved
fine tuning and use transfer learning to transfer
98:36 - what a model has learned on an original data
set to completely new data and a new custom
98:42 - data set that we'll be using later.
98:44 - To understand fine tuning and transfer learning
at a fundamental level, check out the corresponding
98:49 - episode for fine tuning in the deep learning
fundamentals course on V blizzard.com. before
98:54 - we actually start building our model, let's
quickly talk about the VGG 16 pre processing
99:00 - that is done.
99:01 - Now recall we are here in our Jupyter Notebook.
99:03 - This is from last time when we plotted a batch
from our test data set.
99:08 - And a few episodes ago, we discussed the fact
that this color data was skewed as a result
99:13 - of the VGG 16 pre processing function that
we were calling.
99:17 - Well, now we're going to discuss what exactly
this pre processing does.
99:21 - So as we can see, we can still make out the
images as these here being all cats, but it's
99:27 - just the data the color data itself that appears
to be distorted.
99:31 - So if we look at the paper that the authors
of VGG 16 detailed the model in under the
99:39 - architecture section, let's blow this up a
bit, we can see that they state that the only
99:46 - pre processing we do is subtract the mean
RGB value computing on the trait computed
99:51 - on the training set from each pixel.
99:54 - So what does that mean?
99:55 - That means that they computed the mean red
value pixel for All of the training data.
100:02 - And then once I had that mean value across
each image in the training set, they subtracted
100:09 - that mean value from the red value, the red
pixel, each pixel in the image.
100:15 - And then they did the same thing for the green
and blue pixel values as well.
100:19 - So they found the green picks the Mean Green
pixel value among the entire training set.
100:26 - And then for each sample in the training set,
every green pixel, they subtracted that green
100:32 - value from it.
100:34 - So and same thing for the blue, of course.
100:36 - So that's what they did for the pre processing.
100:39 - So given that, that's how VGG 16 was originally
trained.
100:43 - That means now whenever new data is passed
to the model, that it needs to be processed
100:48 - in the same exact way as the original training
set.
100:52 - So Kerris already has functions built in for
popular models, like VGG 16, where they have
100:58 - that pre processing in place that matches
for the corresponding model.
101:03 - So that's why we were calling that model whenever
we process our cat and dog images earlier,
101:08 - so that we could go ahead and get those images
processed in such a way that matched how the
101:13 - original training set was processed, when
VGG was originally trained.
101:18 - Alright, so that is what that color distortion
is all about.
101:22 - So now let's jump back into the code.
101:24 - Now that we know what that's about, we have
an understanding of the pre processing.
101:28 - Now, Let's now get back to actually building
the fine tuned model.
101:33 - So the first thing that we need to do is download
the model.
101:37 - And when you call this for the first time,
you will need an internet connection because
101:40 - it's going to be downloading the model from
the internet.
101:44 - But after this, subsequent calls to this function
will just be grabbing this model from the
101:49 - download a copy on your machine.
101:52 - Alright, so the model has been downloaded
it now we are just running this summary.
101:57 - And we can see that this model is much more
complex than what we have worked with up to
102:02 - this point.
102:03 - So total, there are almost 140 million parameters
in this model.
102:09 - And on disk, it is over 500 megabytes.
102:13 - So it is quite large.
102:15 - Now recall I said that this VGG 16 model originally
was predicting for 1000 different image net
102:23 - classes.
102:24 - so here we can see our output layer of the
VGG 16 model has a 1000 different outputs.
102:31 - So our objective is going to simply be to
change this last output layer to predict only
102:39 - two output classes corresponding to cat and
dog, along with a couple other details regarding
102:44 - the fine tuning process that we'll get to
in just a moment.
102:47 - So now, we're actually going to just skip
these two cells here.
102:51 - These are just for me to be able to make sure
that I've imported the model correctly.
102:57 - But it is not relevant for any code here.
102:59 - It's just checking that the trainable parameters.
103:02 - And non trainable parameters are what I expect
them to be after importing.
103:06 - But this is not of concern at the moment for
us.
103:10 - So if we scroll down here, we're now going
to build a new model called sequential.
103:16 - Alright, now before we run this code, we're
actually just going to look at the type of
103:23 - model this is.
103:25 - So this is returning a model called model.
103:31 - So this is actually a model from the Kerris
functional API.
103:35 - We have been previously working with sequential
models.
103:40 - So we are in a later episode going to discuss
the functional API in depth, it's a bit more
103:46 - complicated and more sophisticated than the
sequential model.
103:50 - For now, since we're not ready to bring in
the functional model yet, we are going to
103:55 - convert the original VGG 16 model into a sequential
model.
104:01 - And we're going to do that by creating a new
variable called model here and setting this
104:07 - equal to an instance of sequential object.
104:11 - And we are going to then loop through every
layer and VGG 16.
104:18 - Except for the last output layer, we're leaving
that out, we're going to loop through every
104:22 - layer and then add each layer into our new
sequential model.
104:28 - So now we'll look at a summary of our new
model.
104:31 - And by looking at this summary, if you take
the time to compare the previous summary to
104:37 - this summary, what you will notice is that
they are exactly the same except for the last
104:44 - layer has been not included in this new model.
104:48 - So this layer here we have this fully connected
two layer is what this is here, with the output
104:56 - shape of 4096. here if we scroll Back up,
we can see that this is this layer here.
105:04 - So the predictions layer has not been included,
because when we were iterating over our for
105:09 - loop, we went all the way up to the second
last layer, we did not include the last layer
105:14 - of VGG 16.
105:16 - Alright, so now let's scroll back down.
105:19 - And we're now going to iterate over all of
the layers within our new sequential model.
105:27 - And we are going to set each of these layers
to not be trainable by setting layer dot trainable
105:34 - to false.
105:36 - And what this is going to do is going to freeze
the trainable parameters, or the weights and
105:41 - biases from all the layers in the model so
that they're not going to be retrained.
105:46 - Whenever we go through the training process
for cats and dogs, because VGG 16 has already
105:52 - learned the features of cats and dogs and
its original training, we don't want it to
105:56 - have to go through more training again it
since it's already learned those features.
105:59 - So that's why we are freezing the weights
here, we're now going to add our own output
106:06 - layer to this model.
106:07 - So remember, we removed the previous output
layer that had 10 output or that had 1000
106:12 - output classes rather.
106:14 - And now we are going to add our own output
layer that has only two output classes for
106:20 - cat and dog.
106:22 - So we add that now since we have set all of
the previous layers to not be trainable, we
106:29 - can see that actually only our last output
layer that's going to be the only trainable
106:34 - layer in the entire model.
106:37 - And like I said before, that's because we
already know that VGG 16 has already learned
106:42 - the features of cats and dogs during its original
training.
106:45 - So we only need to retrain this output layer
to classify two output classes.
106:50 - So now if we look at a new summary of our
model, then we'll see that everything is the
106:54 - same, except for now we have this new dense
layer as our output layer, which only has
107:00 - two classes, instead of 1000.
107:03 - From the original VGG 16 model, we can also
see that our model now only has 8000 trainable
107:11 - parameters, and those are all within our output
layer.
107:15 - As I said that our output layer is our only
trainable layer.
107:19 - So before actually, all of our layers were
trainable.
107:22 - If we go take a look at our original VGG 16
model, we see that we have 138 million total
107:30 - parameters, all of which are trainable, none
of which are non trainable.
107:35 - So if we didn't freeze those layers, then
they would be getting retrained during the
107:40 - training process for our cat and dog images.
107:42 - So just to scroll back down again and check
this out.
107:45 - We can see that now we still have quite a
bit of learnable parameters or a total parameters
107:52 - 134 million, but only 8000 of which are trainable.
107:56 - The rest are non trainable.
107:58 - In the next episode, we'll see how we can
train this modified model on our images of
108:04 - cats and dogs.
108:06 - Be sure to check out the blog and other resources
available for this episode on the bowser.com
108:11 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
108:16 - rewards.
108:17 - Thanks for contributing to collective intelligence.
108:20 - Now let's move on to the next episode.
108:24 - Hey, I'm Mandy from Blizzard.
108:27 - In this episode, we'll demonstrate how to
train the fine tuned VGG 16 model that we
108:33 - built last time on our own data set of cats
and dogs.
108:40 - Alright, so we're jumping straight into the
code to train our model.
108:46 - But of course, be sure that you already have
the code in place from last time as we will
108:50 - be building on the code that we have already
run previously.
108:54 - So, we are using our model here to first compile
it to get it ready for training.
109:01 - This model is the sequential model that we
built last time that is our fine tuned VGG
109:06 - 16 model containing all the same layers with
frozen weights except for our last layer which
109:12 - we have modified to output only two possible
outputs.
109:16 - So, we are compiling this model using the
atom optimizer as we have previously used
109:23 - with the learning rate of 0.0001 and a the
loss we are using again categorical cross
109:31 - entropy just like we have before and we are
using the accuracy as our only metric to judge
109:37 - model performance.
109:38 - So, there is nothing new here with this call
to compile.
109:42 - It is pretty much exactly the same as what
we have seen for our previous models.
109:47 - Now we are going to train the model using
model dot fit and we are passing in our training
109:54 - data set which we have stored in train batches.
109:57 - We are passing in our validation set Which
we have stored as valid batches.
110:03 - And we are only going to run this model for
five epochs.
110:06 - And we are setting the verbosity level to
two so that we can see the most comprehensive
110:12 - output from the model during training.
110:15 - So let's see what happens.
110:17 - Alright, so training has just finished, and
we have some pretty outstanding results.
110:22 - So just after five epochs on our training
data and validation data, we have an accuracy
110:29 - on our training data of 99%.
110:31 - And validation accuracy, right on par at 98%.
110:35 - So that's just after five epochs.
110:37 - And if we look at the first epoch, even our
first epoch gives, it gives us a training
110:43 - accuracy of 85%, just starting out, and a
validation accuracy of 93%.
110:50 - So this isn't totally surprising, because
remember, earlier in the course, we discussed
110:54 - how VGG 16 had already been trained on images
of cats and dogs from the image net library.
111:01 - So it had already learned those features.
111:02 - Now, the flight training that we're doing
on the output layer, is just to train VGG
111:07 - 16 to output only cat or dog at classes.
111:11 - And so it's really not surprising that it's
doing such a good job right off the bat in
111:16 - its first epoch, and even an even considerably
better, and it's fifth epoch at 99% training
111:24 - accuracy.
111:25 - Now we're called the previous CNN that we
built from scratch ourselves the really simple
111:29 - convolutional neural network, that model actually
did really well on the training data, reaching
111:35 - 100% accuracy after a small amount of epochs
as well.
111:39 - Where we saw it lagging though was with the
validation accuracy.
111:43 - So it had a validation accuracy of around
70%.
111:47 - Here we see that we are at 98%.
111:49 - So the main recognizable difference between
our very simple CNN and this VGG 16 fine tuned
111:57 - model is how well this model generalizes to
our cat and dog data in the validation set,
112:03 - whereas the model we built from scratch did
not generalize so well on data that was not
112:08 - included in the training set.
112:10 - In the next episode, we're going to use this
VGG 16 model for inference to predict on the
112:15 - cat and dog images in our test set.
112:18 - And given the accuracy that we are seeing
on the validation set here, we should expect
112:22 - to see some really good results on our test
set as well.
112:25 - Be sure to check out the blog and other resources
available for this episode on the poser.com
112:31 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
112:36 - rewards.
112:37 - Thanks for contributing to collective intelligence.
112:40 - Now let's move on to the next episode.
112:44 - Hey, I'm Andy from deep lizard.
112:48 - In this episode, we'll use our fine tuned
VGG 16 model for inference to predict on images
112:54 - in our test set.
112:58 - All right, we are jumping right back into
our Jupyter Notebook.
113:06 - Again, making sure that all the code is in
place and has run it from the previous episodes
113:11 - as we will be building on the code that has
been run there.
113:16 - So first things first is that we are going
to be using the model to get predictions from
113:21 - our test set.
113:23 - So to do that we call model dot predict which
we have been exposed to in the past.
113:28 - Recall that this model here is our fine tuned
VGG 16 model.
113:32 - And to predict we are passing our test set
which we have stored in test batches.
113:38 - And we are setting our verbosity level to
zero as we do not want to see any output any
113:44 - output from our predictions.
113:46 - Now recall Previously, we talked about how
we did not shuffle the test set for our cat
113:53 - and dog image data set.
113:54 - And that is because we want to be able to
access the classes here and uncheck old order
114:00 - so that we can then pass in the uncheck old
classes that correspond to the old sorry,
114:07 - the shuffled labels that correspond to the
ns shuffled test data, we want to be able
114:13 - to have those in a one to one mapping, where
the labels actually are the correct and shuffled
114:19 - labels for the unsettled data samples, we
want to be able to pass those to our confusion
114:24 - matrix.
114:25 - So this is the same story as we saw whenever
we were using our CNN that we built from scratch
114:31 - a few episodes back, we did the same process,
we're using the same dataset recall.
114:37 - And so now we are plotting this confusion
matrix in the exact same manner as before.
114:42 - So this is actually the exact same line that
we used to plot the confusion matrix A few
114:47 - episodes back when we plotted it on this same
exact test set for the CN n that we built
114:53 - from scratch.
114:54 - And just a reminder from last time recall
that we looked at the class indices of the
114:58 - test batches.
115:00 - So that we could get the correct order of
our cat and dog classes to use for our labels
115:06 - for our confusion matrix.
115:08 - So we are again doing the same thing here.
115:11 - And now we are calling the psychic learn plot
confusion matrix which you should have defined
115:16 - earlier in your notebook from a previous episode.
115:20 - And to plot confusion matrix we are passing
in our confusion matrix, and our labels defined
115:26 - just above as well as a general title for
the confusion matrix itself.
115:31 - So now, let's check out the plot so that we
can see how well our model did on these predictions.
115:36 - So this is what the third or fourth time that
we've used a confusion matrix in this course
115:41 - so far, so you should be pretty normalized
to how to read this data.
115:48 - So recall, the quick and easy way is to look
from the top left to the bottom right along
115:52 - this diagonal, and we can get a quick overview
of how well the model did.
115:57 - So the model correctly predicted a dog for
49 times for images that were truly dogs.
116:06 - And it correctly predicted a cat 47 times
for images that truly were cats.
116:13 - So we can see that one time it predicted a
cat when it was actually a dog.
116:18 - And three times it predicted a dog when images
were actually cats.
116:23 - So overall, the model incorrectly predicted
four samples, so that gives us 96 out of 100.
116:33 - Correct.
116:34 - Or let's see 96 correct predictions out of
100 total predictions.
116:39 - So that gives us an accuracy rate on our test
set of 96%.
116:44 - Not surprising given what we saw in the last
episode for the high level of accuracy that
116:49 - our model had on the validation set.
116:52 - So overall, this fine tuned VGG 16 model does
really well at generalizing on data that it
116:59 - had not seen during training a lot better
than our original model for which we build
117:03 - from scratch.
117:04 - Now recall that we previously discussed that
the overall fine tuning approach that we took
117:09 - to this model was pretty minimal since cat
and dog data was already included in the original
117:15 - training set for the original VGG 16 model.
117:18 - But in upcoming episodes, we are going to
be doing more fine tuning, more fine tuning
117:23 - than what we saw here for VGG 16.
117:25 - As we will be fine tuning another well known
pre trained model, but this time for a completely
117:30 - new data set that was not included in the
original data set that it was trained on.
117:35 - So stay tuned for that.
117:37 - Be sure to check out the blog and other resources
available for this episode on the poser.com
117:43 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
117:48 - rewards.
117:49 - Thanks for contributing to collective intelligence.
117:51 - Now let's move on to the next episode.
117:57 - Hey, I'm Andy from deep lizard.
118:00 - And this episode we'll introduce mobile nets
a class of a lightweight deep convolutional
118:05 - neural networks that are much smaller and
faster and size than many of the mainstream
118:10 - popular models that are really well known.
118:17 - neural nets are a class of small, low power,
low latency models that can be used for things
118:27 - like classification, detection, and other
things that cn ns are typically good for.
118:33 - And because of their small size, these models
are considered great for mobile devices, hence
118:39 - the name mobile nets.
118:41 - So I have some stats taken down here.
118:43 - So just to give a quick comparison, in regards
to the size, the size of the full VGG 16 network
118:50 - that we've worked with in the past few episodes
is about 553 megabytes on disk.
118:56 - So pretty large, generally speaking, the size
of one of the currently largest mobile nets
119:03 - is only about 17 megabytes.
119:06 - So that's a pretty huge difference, especially
when you think about deploying a model to
119:10 - run on a mobile app, for example, this vast
size difference is due to the number of parameters
119:17 - or weights and biases contained in the model.
119:20 - So for example, let's see VGG 16, as we saw
previously has about 138 million total parameters.
119:29 - So a lot.
119:30 - And the 17 megabytes mobile net that we talked
about, which was the largest mobile net currently
119:37 - has only about 4.2 million parameters.
119:40 - So that is much much smaller on a relative
scale than VGG 16 with 138 million.
119:46 - Aside from the size on disk being a consideration
when it comes to comparing mobile nets to
119:52 - other larger models.
119:54 - We also need to consider the memory as well.
119:57 - So the more parameters that a model has The
more space in memory it will be taking up
120:02 - also.
120:03 - So while mobile nets are faster and smaller
than the competitors of these big hefty models
120:10 - like VGG 16, there is a catch or a trade off,
and that trade off is accuracy.
120:18 - So, mobile nets are not as accurate as some
of these big players like VGG 16, for example,
120:25 - but don't let that discourage you.
120:27 - While it is true that mobile nets aren't as
accurate as these resource heavy models like
120:32 - VGG 16, for example, the trade off is actually
pretty small, with only a relatively small
120:38 - reduction in accuracy.
120:40 - And in the corresponding blog for this episode,
I have a link to a paper that goes more in
120:45 - depth to this relatively small accuracy difference.
120:48 - If you'd like to check that out further, let's
now see how we can work with mobile nets and
120:52 - code with Kerris.
120:53 - Alright, so we are in our Jupyter Notebook.
120:56 - And the first thing we need to do is import
all of the packages that we will be making
121:00 - use of which these are not only for this video,
but for the next several videos where we will
121:05 - be covering mobilenet.
121:07 - And as mentioned earlier in this course, a
GPU is not required.
121:10 - But if you're running a GPU, then you want
to run this cell.
121:13 - This is the same cell that we've seen a couple
of times already in this course earlier, where
121:18 - we are just making sure that TensorFlow can
identify our GPU if we're running one, and
121:23 - it is setting the memory growth to true if
we do have a GPU.
121:29 - So again, don't worry, if you don't have a
GPU, don't worry.
121:31 - But if you do, then run this cell, similar
to how we downloaded the VGG 16 model, when
121:37 - we were working with it in previous episodes,
we take that same approach to download mobile
121:41 - net here.
121:42 - So we call TF dot cares applications dot mobile
net dot mobile net, and that the first time
121:49 - we call it is going to download mobile net
from the internet.
121:52 - So you need an internet connection.
121:54 - But subsequent calls to this are just going
to be getting the model from a saved model
122:00 - on disk and loading it into memory here.
122:03 - So we are going to do that now and assign
that to this mobile variable.
122:08 - Now mobile net was originally trained on the
image net library, just like VGG 16.
122:14 - So in a few minutes, we will be passing some
images to mobile net that I've saved on this.
122:18 - They are not images from the image net library,
but they are images of some general things.
122:24 - And we're just going to get an idea about
how mobile net performs on these random images.
122:29 - But first, in order to be able to pass these
images to mobile net, we're going to have
122:33 - to do a little bit of processing first.
122:35 - So I've created this function called prepare
image.
122:39 - And what it does is it takes a file name,
and it then inside the function, we have an
122:45 - image path, which is pointing to the location
on disk, where I have these saved image files
122:51 - that we're going to use to get predictions
from mobile net.
122:56 - So we have this image path defined to where
these images are saved, we then load the image
123:02 - by using the image path, and appending, the
file name that we pass in.
123:07 - So say if I pass in image, one dot png here,
then we're going to take that file path, append
123:17 - one PNG here, pass that to load image, and
pass this target size of 224 by 224.
123:24 - Now, this load image function is from the
keras API.
123:29 - So what we are doing here is we are just taking
the image file, resizing it to be of size
123:34 - 224 by 224, because that is the size of images
that mobile net expects, and then we just
123:41 - take this image and transfer it to be in a
format of an array, then we expand the dimensions
123:48 - of this image, because that's going to put
the image in the shape that mobilenet expects.
123:54 - And then finally, we pass this new processed
image to our last function, which is TF Kerris,
124:02 - that application stop mobile net, that pre
process input.
124:06 - So this is a similar function to what we saw
a few episodes back when we were working with
124:11 - VGG 16, it had its own pre process input.
124:15 - Now mobile net has its own pre process input
function, which is processing images in a
124:22 - way that mobile net expects, so it's not the
same way as VGG 16.
124:26 - Actually, it's just scaling all the RGB pixel
values to be on a scale instead of from zero
124:33 - to 255, to be on a scale from minus one to
one.
124:37 - So that's what this function is doing.
124:39 - So overall, this entire function is just resizing
the image and putting it into an array format
124:46 - with expanded dimensions and then mobile net,
processing it and then returning this processed
124:52 - image.
124:53 - Okay, so that's kind of a mouthful, but that's
what we got to do two images before we pass
124:57 - them to mobile net.
124:58 - Alright, so we will just Define that function.
125:01 - And now we are going to display our first
image called one dot png, from our mobile
125:09 - net samples directory that I told you, I just
set up with a few random images.
125:14 - And we're going to plot that to our Jupyter
Notebook here.
125:18 - And what do you know, it's a lizard.
125:21 - So that is our first image.
125:23 - Now, we are going to pass this image to our
prepare image function that we defined right
125:30 - above that we just finished talking about.
125:32 - So we're going to pass that to the function
to pre process the image accordingly, then
125:37 - we are going to pass the pre processed image
returned by the function to our mobile net
125:43 - model, we're going to do that by calling predict
on the model, just like we've done in previous
125:48 - videos, when we've called predict on models
to use them for inference, then after we get
125:54 - the prediction for this particular image,
we are then going to give this prediction
125:59 - to this image net utils dot decode predictions
function.
126:04 - So this is a function from cares, that is
just going to return the top five predictions
126:10 - from the 1000 possible image net classes.
126:13 - And it's going to tell us the top five that
mobile net is predicting for this image.
126:19 - So let's run that and then print out those
results.
126:23 - And maybe you can have a better idea of what
I mean once you see the printed output.
126:27 - So we run this, and we have our output.
126:31 - So these are the top five in order results
from the imagenet classes that mobile net
126:36 - is predicting for this image.
126:38 - So it's assigning a 58% probability to this
image of being a an American chameleon 28%
126:47 - probability to green lizard 13% to a gamma.
126:51 - And then we have some small percentages here
under 1% for these other two types of lizards.
126:56 - So it turns out, if you're not aware, and
I don't know how you could not be aware of
127:01 - this, because everyone should know this.
127:03 - But this is an American chameleon.
127:05 - And I don't know I've always called these
things green and OLS.
127:09 - But I looked it up.
127:10 - And they're also known as American chameleon.
127:12 - So mobilenet got it right.
127:15 - So yeah, it assigned at 58% probability that
was the highest, most probable class.
127:21 - Next was green lizard.
127:22 - So I'd say that that is still really good
for that to be your second place.
127:26 - I don't know if green lizard is supposed to
be more general.
127:30 - But and then a gamma, which is also a similar
looking lizard, if you didn't know.
127:34 - So between these top three classes that it
predicted, this is almost 100%.
127:39 - between the three of these, I would say mobile
net did a pretty good job on this prediction.
127:44 - So let's move on to number two.
127:47 - Alright, so now we are going to plot our second
image.
127:52 - And this is a cup of well, I originally thought
that it was espresso, and then someone called
128:00 - it a cup of cappuccino.
128:02 - So let's say I'm not sure I'm not a coffee
connoisseur, although I do like both espresso
128:07 - and cappuccinos.
128:08 - This looks like it has some cream in it.
128:10 - So hey, now we're going to go through the
same process of what we just did for the lizard
128:16 - image where we are passing that image passing
this new image to our prepare image function,
128:22 - so that they undergoes all of the pre processing,
then we are going to pass the pre processed
128:27 - image to the predict function for our mobile
net model.
128:31 - Then finally, we are going to get the top
five results from the predictions for this
128:37 - model relative in regards to the imagenet
classes.
128:40 - So let's see.
128:41 - All right, so according to mobile net, this
is an espresso, not a cappuccino.
128:48 - So I don't know.
128:50 - But it predicts 99% probability of espresso
as being the most probable class for this
128:57 - particular image.
128:58 - And I'd say that that is pretty reasonable.
129:01 - So let me know in the comments, what do you
think is this espresso or cappuccino?
129:05 - I don't know if mobile or if image net had
a cappuccino class.
129:09 - So if it didn't, then I'd say that this is
pretty spot on.
129:12 - But you can see that the other the other four
predictions are all less than 1%.
129:18 - But they are reasonable.
129:20 - I mean, the second one is cup, third eggnog,
fourth coffee mug.
129:26 - Fifth, wooden spoon gets a little bit weird,
but there is wood, there is a circular shape
129:32 - going on here.
129:33 - But these are all under 1%.
129:35 - So they're pretty negligible.
129:36 - I would say mobilenet did a pretty great job
at giving a 99% probability to espresso for
129:43 - this image.
129:44 - All right, we have one more sample image.
129:46 - So let's bring that in.
129:48 - And this is a strawberry or multiple strawberries
if you call if you consider the background.
129:55 - So same thing we are pre processing the strawberry
image.
129:58 - Then we are getting a prediction For a mobile
net for this image, and then we are getting
130:02 - that top five results for the most probable
predictions among the 1000 image net classes.
130:10 - And we see that mobile net with 99.999% probability
classifies this image as a strawberry correctly,
130:21 - so very well, and the rest are well, well
under 1%.
130:25 - But they are all fruits.
130:27 - So interesting.
130:28 - Another really good prediction from mobile
net.
130:31 - So even with the small reduction in accuracy
that we talked about, at the beginning of
130:35 - this episode, you can probably tell from just
these three random samples that that reduction
130:40 - is maybe not even noticeable when you're just
doing tests like the ones that we just ran
130:45 - through.
130:46 - So in upcoming episodes, we're actually going
to be fine tuning this mobile net model to
130:51 - work on a custom data set.
130:53 - And this custom data set is not one that was
included in the original image net library,
131:00 - it's going to be a brand new data set, we're
going to do more fine tuning than what we've
131:04 - done in the past.
131:05 - So stay tuned for that.
131:07 - Be sure to check out the blog and other resources
available for this episode on deep lizard
131:11 - calm, as well as the deep lizard hive mind
where you can gain access to exclusive perks
131:17 - and rewards.
131:18 - Thanks for contributing to collective intelligence.
131:20 - Now let's move on to the next episode.
131:26 - Hey, I'm Mandy from deep lizard.
131:30 - And this episode, we'll be preparing and processing
a custom data set that we'll use to fine tune
131:36 - mobilenet using tensor flows keras API.
131:45 - Previously, we saw how well VGG 16 was able
to classify and generalize on images of cats
131:55 - and dogs.
131:57 - But we noted that VGG 16 was actually already
trained on cats and dogs, so it didn't take
132:02 - a lot of fine tuning at all to get it to perform
well on our cat and dog dataset.
132:07 - Now, with mobile net, we'll be going through
some fine tuning steps as well.
132:12 - But this time, we'll be working with a data
set that is completely different from the
132:16 - original image net library for which mobile
net was originally trained on.
132:20 - This new data set that we'll be working with
is a data set of images of sign language digits,
132:27 - there are 10 classes total for this dataset,
ranging from zero to nine, where each class
132:33 - contains images of the particular sign for
that digit.
132:38 - This data set is available on kaggle as grayscale
images, but it's also available on GitHub
132:44 - as RGB images.
132:46 - And for our particular task, we'll be using
the RGB images.
132:50 - So check out the corresponding blog for this
episode on Peebles or.com, to get the link
132:55 - for where you can download the data set yourself.
132:57 - So after downloading the data set, the next
step is to organize the data on disk.
133:03 - And this will be a very similar procedure
to what we saw for the cat and dog data set
133:08 - earlier in the course.
133:09 - So once we have the download, this is what
it looks like.
133:13 - It is a zipped folder called sign language
digits data set master.
133:18 - And the first thing we want to do is extract
all the contents of this directory.
133:24 - And when we do that, we can navigate inside
until we get to the dataset directory.
133:30 - And inside here we have all of the classes.
133:34 - And each of these directories has the corresponding
images for this particular class.
133:41 - So the what we want to do is we want to grab
zero through nine.
133:46 - And we are going to Ctrl x or cut these directories.
133:52 - And we're going to navigate back to the root
directory, which is here, we're going to place
134:00 - all of the directories zero through nine in
this route.
134:04 - And then we're going to get rid of everything
else by deleting.
134:08 - So now, one last thing.
134:10 - So I'm just going to delete this master.
134:13 - I don't necessarily like that.
134:15 - So we have sign language digits data set.
134:18 - And directly within we have our nested directories
consisting of zero through nine, each of which
134:24 - has our training data inside then the last
step is to move the sign language digits data
134:29 - set directory to where you're going to be
working.
134:32 - So for me that is relative to my Jupyter Notebook,
which lives inside of this deep learning with
134:38 - cares directory, I have a data directory and
I have the sign language digits data set located
134:45 - here.
134:46 - Now everything else that will do to organize
and process the data will be done programmatically
134:50 - in code.
134:51 - So we are in our Jupyter Notebook.
134:54 - Make sure that you do have the imports that
we brought in last time still in place because
134:58 - we'll be making use of those Now, this is
just the class breakdown to let you know how
135:04 - many images are in each class.
135:06 - So across the classes zero through nine, the
there are anywhere from 204 to 208 samples
135:14 - in each class.
135:16 - And then here I have just an explanation of
how your data should be structured up to this
135:21 - point.
135:22 - Now the rest of the organization, we will
do programmatically with this script here.
135:28 - So this script is going to organize the data
into train valid and test directories.
135:33 - So recall right now, we just have all the
data located in the corresponding classes
135:38 - of zero through nine.
135:40 - But the data is not broken up yet into the
separate data sets of train, test and validation.
135:46 - So to do that, we are first changing directory
into our sign language digits, data set directory.
135:53 - And then we are checking to make sure that
the directory structure that we're about to
135:58 - setup is not already in place on disk.
136:01 - And if it's not, then we make a train valid
and test directory, right within sign language
136:09 - digits dataset.
136:10 - So next we are then iterating over all of
the directories within our sign language digits
136:18 - dataset directory.
136:19 - So recall, those are directories labeled zero
through nine.
136:23 - So that's what we're doing in this for loop
with this range zero to 10.
136:26 - That's going from directory zero to nine,
and moving each of these directories into
136:32 - our train directory.
136:34 - And after that, we're then making two new
directories, one inside of valid with the
136:42 - directory for whatever place we're at in the
loop.
136:45 - So if we are on run number zero, then we are
making a directory called zero with invalid,
136:53 - and a directory called zero within test.
136:56 - And if we are on run number one, then we will
be creating a directory called one with invalid
137:03 - and one within test.
137:05 - So we do this whole process of moving each
class in to train and then creating each class
137:13 - directory empty, within valid and test.
137:17 - So at this point, let's suppose that we are
on the first run in this for loop.
137:23 - So in this range, here we are following add
a number zero.
137:27 - So here on this line, what we are doing is
we are sampling 30 samples from our train
137:35 - slash zero directory.
137:38 - Because up here we created or we moved the
class directory, zero into train.
137:46 - And now we're going to sample 30 random samples
from the zero directory within train, then,
137:55 - so we're calling these valid samples, because
these are going to be the samples that we
138:00 - move into our validation set.
138:01 - And we do that next.
138:03 - So for each of the 30 samples that we collected
from the training set randomly in this line,
138:11 - we're now going to be moving them from the
training set to the validation, zero set to
138:19 - the validation set in Class Zero.
138:23 - And then we do similarly the same thing for
the test samples, we randomly select five
138:29 - samples from the train slash zero directory.
138:34 - And then we move those five samples from that
train slash zero directory into the test zero
138:42 - directory.
138:43 - So we just ran through that loop using the
Class Zero as an example.
138:49 - But that's going to happen for each class
zero through nine.
138:53 - And just in case you have any issue with visualizing
what that script does, if we go into sign
138:58 - language, digits dataset, now let's check
out how we have organized the data.
139:03 - So recall we previously had classes zero through
nine all listed directly here within the this
139:12 - route folder.
139:13 - Now we have train valid and test directories.
139:17 - And within train, we moved the original zero
through nine directories all into train.
139:24 - And then once that was done, then we sampled
30 images from each of these classes and moved
139:31 - them into the valid directory classes.
139:35 - And then similarly, we did the same thing
for the test directory.
139:40 - And then once we look in here, we can see
that the test directory has five samples for
139:46 - zero, we see that it has five samples for
one.
139:52 - If we go check out the valid directories,
look at zero, it should have 30 zeros so See
140:00 - that here 30.
140:01 - So every valid directory has 30 samples, and
the training directory classes have not necessarily
140:10 - uniform samples because remember, we saw that
the number of samples in each class ranged
140:16 - anywhere from 204 to 209, I think.
140:20 - So the number of images within each class
directory for the training sample will differ
140:26 - slightly by maybe one or two images.
140:28 - But the number of images in the classes for
the validation and test directories will be
140:33 - uniform since we did that programmatically
with our script here.
140:37 - So checking this dataset out on disk, we can
see that this is exactly the same format,
140:42 - for which we structured our cat and dog image
data set earlier in the course, now, we're
140:48 - just dealing with 10 classes instead of two.
140:52 - And when we downloaded our data, it was in
a slightly different organizational structure
140:56 - than the cat and dog data that we previously
downloaded.
141:00 - Alright, so we have obtained the data, we've
organized the data.
141:04 - Now the last step is to pre process the data.
141:09 - So we do that first by starting out by defining
where our train valid and test directories
141:15 - live on disk.
141:16 - So we supply those paths here.
141:20 - And now we are setting up our directory iterators,
which we should be familiar with, at this
141:25 - point.
141:26 - Given this is the same format that we processed
our cat and dog images whenever we build our
141:31 - cnn from scratch, and we use the fine tuned
VGG 16 model.
141:37 - So let's focus on this first variable train
batches.
141:41 - First, we are calling image data generator
dot flow from directory which we can't quite
141:47 - see here.
141:48 - But we'll scroll into a minute to image data
generator, we are passing this pre processing
141:52 - function, which in this case is the mobile
net pre processing function.
141:58 - Recall, we saw that already in the last episode.
142:01 - And there we discussed how this pre processes
images in such a way that it scales the image
142:06 - data to be rather than on a scale from zero
to 255, to instead be on a scale from minus
142:12 - one to one.
142:13 - So then on image data generator, we call flow
from directory, which is blowing off the screen
142:20 - here.
142:21 - And we set the directory equal to train path,
which we have defined just above saying where
142:28 - our training data resides on disk, we are
setting the target size to 224 by 224, which
142:35 - recall just resizes any training data to be
a height or to have a height of 224 and a
142:42 - width of 224, since that is the image size
that mobile net expects, and we are setting
142:49 - our batch size equal to 10.
142:51 - So we've got the same deal for valid batches.
142:54 - And for test batches as well, everything exactly
the same except for the paths deferring to
143:00 - show where the validation and test sets live
on disk.
143:05 - And we are familiar now that we specify shuffle
equals false only for our test set so that
143:11 - we can later appropriately plot our prediction
results to a confusion matrix.
143:17 - So we run this cell, and we have output that
we have 17 112 images belonging to 10 classes.
143:25 - So that corresponds to our training set 300
images to 10 classes for our validation set,
143:31 - and 50 images belonging to 10 classes for
our test set.
143:36 - So I have this cell with several assertions
here, which just assert that this output that
143:42 - we find, or that we have right here is what
we expect.
143:46 - So if you are not getting this, then it's
perhaps because you are pointing to the wrong
143:51 - location on disk.
143:53 - So a lot of times if you're pointing to the
wrong location, then you'll probably get found
143:57 - zero images belonging to 10 classes.
143:59 - So you just need to check your path to where
your data set resides if you get that.
144:04 - Alright, so now our data set has been processed.
144:07 - We're now ready to move on to building and
fine tuning our mobile net model for this
144:12 - data set.
144:13 - Be sure to check out the blog and other resources
available for this episode on depot's or.com
144:18 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
144:24 - rewards.
144:25 - Thanks for contributing to collective intelligence.
144:27 - Now let's move on to the next episode.
144:32 - Hey, I'm Mandy from Blizzard.
144:35 - In this episode, we'll go through the process
of fine tuning mobile net for a custom data
144:40 - set.
144:43 - Alright, so we are jumping right back into
our Jupyter Notebook from last time.
144:53 - So make sure your code is in place from then
since we will be building directly on that
144:58 - now.
144:59 - So first thing we're going to do is we are
going to import mobile net just as we did
145:04 - in the first mobile net episode by calling
TF Kerris, the applications that mobile net
145:09 - dot mobile net, remember, if this is your
first time running this line, then you will
145:13 - need an internet connection to download it
from the internet.
145:16 - Now, let's just take a look at the model that
we downloaded.
145:21 - So by calling model dot summary, we have this
output here, that is showing us all of these
145:28 - lovely layers included in mobile net.
145:32 - So this is just to get a general idea of the
model because we will be fine tuning it.
145:38 - So the fine tuning process now is going to
start out with us getting all of the layers
145:46 - up to the sixth to last layer.
145:50 - So if we scroll up and look at our output
123456.
145:55 - So we are going to get all of the layers up
to this layer, and everything else is not
146:03 - going to be included.
146:04 - So all of these layers are what we are going
to keep and transfer into a new model, our
146:11 - new fine tune model.
146:13 - And we are not going to include the last five
layers.
146:16 - And this is just a choice that I came to after
doing a little experimenting and testing,
146:21 - the number of layers that you choose to include
versus not include whenever you're fine tuning,
146:26 - a model is going to come through experimentation
and personal choice.
146:31 - So for for us, we are getting everything from
this layer and above.
146:36 - And we are going to keep that in our new fine
tuned model.
146:40 - So let's scroll down.
146:41 - So we're doing that by calling mobile dot
layers, passing in that six to last layer
146:47 - and output, then we are going to create a
variable called output.
146:52 - And we're going to set this equal to a dense
layer with 10 units.
146:58 - So this is going to be our output layer.
146:59 - That's why it's called output and 10 units
due to the nature of our classes, ranging
147:05 - zero through nine.
147:08 - And this, as per usual is going to be followed
by a softmax activation function to give us
147:15 - a probability distribution among those 10
outputs.
147:19 - Now, this looks a little strange.
147:22 - So we're calling this and then we're like
putting this x variable next to it.
147:27 - So what's this about?
147:29 - Well, the mobile net model is actually a functional
model.
147:33 - So this is from the functional API from Kerris,
not the sequential API.
147:40 - So we kind of touched on this a little bit
earlier, whenever we fine tuned VGG 16, we
147:45 - saw that VGG 16 was also indeed a functional
model.
147:49 - But when we fine tuned it, we iterated over
each of the layers and added them to a sequential
147:54 - model at that point, because we weren't ready
to introduce the functional model yet.
148:00 - So here, we are going to continue working
with a functional model type.
148:05 - So that's why we are basically taking all
of the layers here up to the sixth the last.
148:11 - And whenever we create this output layer,
and then call this the previous layers stored
148:19 - in x here, that is the way that the functional
model works, we're basically saying to this
148:25 - output layer, pass all of the previous layers
that we have stored in X, up to this six to
148:34 - last layer and mobile net.
148:37 - And then we can create the model using these
two pieces x and output by saying by calling
148:43 - model, which is indeed a functional model
when specified this way, and specifying inputs
148:50 - equals mobile dot input.
148:52 - So this is taking the input from the original
mobile net model, and outputs equals output.
148:58 - So at this point, output is all of the mobile
net model up until the six the last layer
149:05 - plus this dense output layer.
149:08 - Alright, so let's run these two cells to create
our new model.
149:11 - Alright, so our new models now been created.
149:14 - So the next thing we're going to do is we're
going to go through and freeze some layers.
149:20 - So through some experimentation on my own,
I have found that if we freeze, all except
149:27 - for the last 23 layers, this appears to yield
some decent results.
149:32 - So 23 is not a magic number here, play with
this yourself and let me know if you get better
149:37 - results.
149:38 - But basically, what we're doing here is we're
going through all the layers in the model,
149:43 - and by default, they are all trainable.
149:46 - So we're saying that we want only the last
23 layers to be trainable.
149:52 - All the layers except for the last one, a
three, make those not trainable.
149:57 - And just so that You understand, relatively
speaking, there are 88 total layers in the
150:04 - original mobile net model.
150:06 - And so we're saying that we don't want to
train, or that we only want to train the last
150:11 - 23 layers in our new model that we built just
above.
150:15 - Recall, this is much more than we earlier
trained with our fine tuned VGG 16 model where
150:21 - we only train the output layer.
150:23 - So let's go ahead and run that now.
150:26 - And now let's look at a summary of our new
fine tuned model.
150:31 - So here, if we just glance it looks basically
the same as what we saw from our original
150:36 - summary.
150:37 - But we will see here that now now our model
ends with this global average pooling 2d layer,
150:46 - which recall before was the sixth the last
layer, where I said that we would include
150:50 - that layer and everything above it.
150:52 - So all the layers below the global average
pooling layer that we previously saw and the
150:57 - original mobile net summary are now gone.
151:00 - And instead of an output layer with 1000 classes,
we now have an output layer with 10 classes
151:07 - from the or corresponding to the 10 potential
output classes that we have for our new sign
151:14 - language digits dataset.
151:16 - If we compare the total parameters, and how
they're split amongst trainable and non trainable
151:23 - parameters, in this model with the original
mobile, mobile that model, then we will see
151:27 - a difference there as well.
151:29 - Alright, so now this model has been built,
we are ready to train the model.
151:34 - So the code here is nothing new.
151:37 - We are compiling the model in the same exact
fashion using the atom optimizer 0.00, with
151:45 - our little bit of luck, 0.0001 learning rate,
categorical categorical cross entropy loss
151:51 - and accuracy as our metric.
151:53 - So this, we have probably seen 2 million times
up to this point in this course.
151:59 - So that's exactly the same.
152:01 - Additionally, we have exactly the same fit
function that we are running to train the
152:06 - model.
152:08 - So we're passing in our train batches as our
data set, we are passing in validation batches
152:14 - as our validation data.
152:16 - And we are running this for 10 epochs.
152:20 - Actually, we're going to go ahead and run
this for 30.
152:22 - I had 10 here just to save time earlier from
testing, but we're going to run this for 30
152:27 - epochs.
152:28 - And we are going to set verbose equal to two
to get the most verbose output.
152:33 - Now, let's see what happens.
152:34 - All right, so our model just finished training
over 30 epochs.
152:38 - So let's check out the results.
152:41 - And if you see this output, and you're wondering
why the first output took 90 seconds, and
152:45 - then we get or the first epoch took 90 seconds,
and then we got it down to five seconds, just
152:50 - a few later, it's because I realized that
I was running on battery and not on my laptop
152:57 - being plugged in.
152:58 - So once we plug the laptop in, it beefed up
and started running much quicker.
153:03 - So let's scroll down and look basically just
like how we ended here.
153:09 - So we are at 100% accuracy on our training
set, and 92% accuracy on our validation set.
153:17 - So that is pretty freakin great considering
the fact that this is a completely new dataset,
153:24 - not having images that were included in the
original image net model.
153:28 - So these are pretty good results.
153:29 - A little bit overfitting here since our validation
accuracy is lower than our training accuracy.
153:36 - So if we wanted to fix that, then we could
take some necessary steps to combat that overfitting
153:42 - issue.
153:43 - But if we look earlier at the earlier epochs,
to see what kind of story is being told here,
153:49 - on our first epoch, our training accuracy
actually starts out at 74% among 10 classes,
153:56 - so that is not bad for a starting point.
154:00 - And we quickly get to 100% on our training
accuracy, just within four epochs.
154:06 - So that's great.
154:07 - But you can see that, at that point, we're
only at 81% accuracy for our validation set.
154:12 - So we have a decent amount of overfitting
going on earlier on here.
154:17 - And then, as we progress through the training
process, the overfitting is becoming less
154:22 - and less of a problem.
154:24 - And you can see that we actually at this point,
if we just look at the last eight epochs that
154:29 - have run here, we've not even stalled out
yet on our validation loss.
154:36 - It's not stalled out in terms of decreasing
and our value.
154:38 - Our validation accuracy has not stalled out
in terms of increasing so perhaps just running
154:44 - more epochs on this data will eradicate the
overfitting problem.
154:49 - Otherwise, you can do some tuning yourself
changing some hyper parameters around do a
154:56 - different structure of fine tuning on the
model.
154:58 - So freeze Morrow.
155:00 - Less than the last 23 layers for during the
fine tuning process, or just experiment yourself.
155:09 - And if you come up with something that yields
better results than this, then put it in the
155:12 - comments and let us know.
155:14 - So we have one last thing we want to do with
our fine tuned mobile net model, and that
155:18 - is use it on our test set.
155:21 - So we are familiar, you know the drill with
this procedure at this point, we have done
155:25 - it several times.
155:27 - So we are now going to get predictions from
the model on our test set.
155:32 - And then we are going to plot those predictions
to a confusion matrix.
155:37 - So we are first going to get our true labels
by calling test batches classes.
155:45 - We're then going to gain predictions from
the model by calling model dot predict and
155:50 - passing in our test set stored in test batches
here, setting verbose equal to zero, because
155:56 - we do not want to see any output from the
predictions.
155:59 - And now we are creating our confusion matrix.
156:03 - Using socket learns confusion matrix that
we imported earlier, we are setting our true
156:08 - labels equal to the test labels that we defined
just here above, we are setting our predicted
156:14 - labels to the arg max of our predictions across
x is one.
156:22 - And now we are going to check out our class
indices of the test batches just to make sure
156:27 - they are what we think they are.
156:29 - And they are of course, classes labeled zero
through nine.
156:33 - So we define our labels for our confusion
matrix here, accordingly.
156:39 - And then we call our plot confusion matrix
that we brought in earlier in the notebook
156:43 - and that we have used 17,000 Times up to this
point in this course, and we are passing in
156:50 - our confusion matrix for what to plot, we
are passing in our labels that we want to
156:57 - correspond to our confusion matrix, and giving
our confusion matrix, the very general title
157:03 - of confusion matrix, because hey, that's what
it is.
157:06 - So let's plot this.
157:07 - Oh, no.
157:08 - So plot confusion matrix is not defined?
157:11 - Well, it definitely is just somewhere in this
notebook, I must have skipped over here we
157:15 - go.
157:16 - Nope, here we are.
157:17 - Alright, so here's where plot confusion matrix
is defined.
157:20 - Let's bring that in.
157:22 - Now it is defined and run back here.
157:27 - So looking from the top left to the bottom
right, and now we see that the model appears
157:33 - to have done pretty well.
157:35 - So we have 10 classes total, with five samples
per class.
157:41 - And we see that we have mostly we have all
fours and fives across this diagonal, meaning
157:46 - that most of the time the model predicted
correctly.
157:50 - So for example, for a nine, five times out
of five, the model predicted an image was
157:55 - a nine when it actually was for an eight.
157:58 - However, only four out of five times did the
model correctly predict looks like one of
158:04 - the times the model, let's see, predicted
a one when it should have been an eight.
158:12 - But in total, we've got 12345 incorrect predictions
out of 50 total.
158:20 - So that gives us a 90% accuracy rate on our
test set, which is not surprising for us,
158:28 - given the accuracy that we saw right above
on our validation set.
158:32 - So hopefully this series on a mobile net has
given you further insight to how we can fine
158:38 - tune models for custom data set and use transfer
learning.
158:41 - To use the information that a model gained
from its original training set on a completely
158:47 - new task in the future.
158:49 - Be sure to check out the blog and other resources
available for this episode on the poser.com
158:54 - as well as the deep lizard hive mind where
you can gain access to exclusive perks and
159:00 - rewards.
159:01 - Thanks for contributing to collective intelligence.
159:03 - Now let's move on to the next episode.
159:07 - Hey, I'm Andy from deep lizard.
159:11 - And in this episode, we're going to learn
how we can use data augmentation on images
159:16 - using tensor flows.
159:20 - keras API.
159:25 - Data augmentation occurs when we create a
new data by making modifications to some existing
159:32 - data.
159:33 - We're going to explain the idea a little bit
further before we jump into the code.
159:37 - But if you want a more thorough explanation,
then be sure to check out the corresponding
159:41 - episode in the deep learning fundamentals
course on depot's or.com.
159:46 - For the example that will demonstrate in just
a moment, the data we'll be working with is
159:50 - image data.
159:51 - And so for image data specifically, data augmentation
would include things like flipping the image,
159:58 - either horizontally or vertically.
160:00 - It could include rotating the image, it could
include changing the color of the image, and
160:06 - so on.
160:07 - One of the major reasons we want to use data
augmentation is to simply just get access
160:13 - to more data.
160:14 - So a lot of times that not having access to
enough data is an issue that we can run into.
160:20 - And we can run into problems like overfitting
if our training data set is too small.
160:25 - So that is a major reason to use data augmentation
is to just grow our training set, adding augmented
160:31 - data to our training set can in turn reduce
overfitting as well.
160:36 - Alright, so now let's jump into the code to
see how we can augment image data using Kerris.
160:41 - Alright, so the first thing that we need to
do, of course, is import all of the packages
160:46 - that we will be making use of for this data
augmentation.
160:49 - Next, we have this plot images function, which
we've introduced earlier in the course, this
160:55 - is directly from tensor flows website.
160:58 - And it just allows us to apply images to our
Jupyter Notebook.
161:02 - So check out the corresponding blog for this
episode on deep lizard calm to get the link
161:06 - so that you can go copy this function yourself.
161:09 - Alright, so next we have this variable called
Gen, which is an image data generator.
161:17 - And recall, we've actually worked with image
data generators. earlier in the course, whenever
161:22 - we create our train at test and valid batches
that we were using for training seeing it
161:28 - and with this, though, we are using image
data generator in a different way.
161:32 - Here, we are creating this generator that
we are specifying all of these parameters
161:38 - like rotation, range, width, shift range,
heights of range, sheer range, zoom range
161:43 - channels of drains in horizontal flip.
161:46 - So these are all options that allow us to
augment our image data.
161:51 - So you need to check the documentation on
tensor flows website to get an idea of the
161:57 - units for these parameters, because they're
not all the same.
162:01 - So for example, rotation range here, I believe
is measured in radians.
162:07 - Whereas like this width, shift range is measured
as a percentage of the width of the image.
162:14 - So these are all ways that we can augment
image data.
162:18 - So this is going to be rotating the image
by 10 radians, this is going to be shifting
162:23 - the width of the image by 10%, the height
by 10%.
162:28 - Zooming in shifting the color channels, flipping
the image, so all sorts of things.
162:34 - So just all different ways that we can augment
image data.
162:39 - So there are other options to just be sure
to check out the image data generator documentation
162:44 - if you want to see those options.
162:46 - But for now, these are the ones that we'll
be working with.
162:49 - So we store that in our Jen variable.
162:52 - Next, we are going to choose a random image
from a dog directory that we had set up earlier
163:00 - in the course under the dogs versus cats dataset,
we're going to go into train into dog, then
163:07 - we are going to choose a random image from
this directory.
163:11 - And then we're going to set this image path
accordingly.
163:14 - So we're just going to set this to point to
whatever that chosen image was on disk, then
163:20 - we have this assertion here just to make sure
that that is indeed a valid file before we
163:25 - proceed with the remaining code.
163:27 - Now we're just going to plot this image to
the screen.
163:31 - And I'm not sure what this is going to be
since it is a random image from disk.
163:35 - So that is a cute looking.
163:38 - I don't know.
163:40 - Beagle, Beagle, basset hound mix, I don't
know, what do you guys think.
163:45 - So this is the random dog that was selected
from from our dog trained directory.
163:53 - Now we are creating this new variable called
all etre.
163:57 - And to our image data generator that we created
earlier called Jin, we're calling this flow
164:03 - function and passing our image in to flow.
164:08 - And this is going to generate a batch of augmented
images from this single image.
164:14 - So next, we are defining this og images variable,
which is going to give us 10 samples of the
164:22 - augmented images created by og etre here.
164:26 - Lastly, we are going to plot these images
using the plot images that we defined just
164:32 - above.
164:33 - All right, so let's zoom in a bit.
164:35 - All right, we can see now that first let's
take a look at our original dog Alright, so
164:42 - that is the original image.
164:44 - Now we can look and see that given those things
like rotation and width shift, and everything
164:51 - that we defined earlier, whenever we defined
whenever we defined our image data generator
164:57 - that has now been done to All of these images
in one random way or another so we can see
165:04 - kind of what's happening here.
165:05 - So for example, this particular image looks
like it has been shifted up some because we
165:11 - can see that the head of the dog is being
cut off a little bit.
165:16 - And this image, or let's see which way was
the dog originally facing, so its head is
165:21 - facing to the right.
165:23 - So yeah, so this image here has been flipped,
the dog is now facing to the left.
165:29 - And this image appears to be shifted down
some.
165:33 - And so some of these, like this one looks
like it's been rotated.
165:37 - So we can get an idea just by looking at the
images, the types of data augmentation that
165:42 - have been done to them.
165:44 - And we can see how this could be very helpful
for growing our dataset in general, because
165:50 - for example, say that we have a bunch of images
of dogs, but for whatever reason, they're
165:55 - all facing to the left.
165:58 - And we want to deploy our model to to be some
general model that will classify different
166:04 - dogs.
166:05 - But the types of images that will be accepted
by this model later might have dogs facing
166:09 - to the right as well.
166:10 - Well, maybe our model totally implodes on
itself whenever it receives a dog facing to
166:15 - the right.
166:16 - So through data augmentation, given the fact
that it is very normal for dogs to face left
166:21 - or right, we could augment all of our dog
images to have the data or to have the dogs
166:28 - also face in the right direction, as well
as the left direction to have a more have
166:35 - a more dynamic data set.
166:37 - Now, there is a note in the corresponding
blog for this episode on people's are calm,
166:42 - giving just the brief instruction for how
to save these images to disk if you want to
166:48 - save the images after you augment them, and
then add them back to your training set.
166:52 - So check that out if you're interested in
doing that to actually grow your training
166:56 - set.
166:57 - Rather than just plot the images here and
your Jupyter Notebook.
167:00 - By the way, we are currently in Vietnam filming
this episode.
167:03 - If you didn't know we also have a vlog channel
where we document our travels and share a
167:09 - little bit more about ourselves.
167:11 - So check that out at beetles or vlog on YouTube.
167:14 - Also, be sure to check out the corresponding
blog for this episode, along with other resources
167:19 - available on the blizzard.com.
167:21 - And check out the people's hive mind where
you can gain exclusive access to perks and
167:26 - rewards.
167:27 - Thanks for contributing to collective intelligence.
167:32 - I'll see you next time.