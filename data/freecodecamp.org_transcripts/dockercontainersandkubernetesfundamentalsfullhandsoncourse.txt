00:00 - gee Barrette teaches us Docker
00:01 - containers and kubernetes fundamentals
00:03 - course for beginners ghee is a developer
00:06 - and trainer with more than 25 years of
00:08 - experience he is a Microsoft MVP
00:11 - frequent conference speaker and was the
00:14 - leader of the montreal.net user group
00:16 - for more than 23 years all this is to
00:19 - say he is the perfect person to teach
00:21 - you about Docker and kubernetes
00:25 - [Music]
00:33 - welcome to this Docker containers and
00:35 - kubernetes fundamentals training course
00:39 - my name is gibaret and I'll be your host
00:42 - in your Learning Journey Into The
00:44 - Amazing World of containers
00:47 - I'm a full-time trainer with a developer
00:49 - background and I'm based in Montreal
00:51 - Canada and that's where my strange
00:54 - accent comes from
00:56 - I'm certified on kubernetes and also on
00:59 - terraform Azure AWS and Google Cloud
01:04 - I'm very honored to be a Microsoft MVP
01:07 - in the Azure expertise and a
01:10 - digitalocean navigator
01:13 - you can reach me via the contact page
01:15 - and on Twitter
01:19 - what should you expect from this course
01:22 - you will not become an expert just by
01:24 - taking this course this is an
01:26 - entry-level course that will provide you
01:28 - with a strong containers and kubernetes
01:31 - Foundation
01:32 - and you'll gain enough knowledge to make
01:34 - sound decision at work or in your
01:37 - projects
01:39 - throughout this course you will find
01:41 - lots of Amazon activities to practice
01:44 - what you've learned
01:46 - you will use Docker and kubernetes
01:49 - locally on your PC or Mac so there's no
01:53 - requirement to have an account with a
01:55 - cloud provider
01:58 - are there any prerequisites for this
02:01 - course not really if you're a developer
02:04 - a devops specialist an ID Pro or even a
02:08 - technical manager that's totally fine no
02:12 - previous Docker or kubernetes knowledge
02:14 - is required
02:18 - we will cover a lot of ground
02:21 - you will learn about containers Docker
02:24 - and D Docker registry
02:27 - you learn about the kubernetes objects
02:30 - like pods workloads and services
02:33 - that's a lot of material and the goal
02:36 - here is to get you from zero knowledge
02:38 - to a kubernetes ninja
02:41 - well at least provide you with enough
02:43 - knowledge to Aspire being a kubernetes
02:46 - ninja
02:48 - I want to say a big thank you for
02:51 - learning Docker and kubernetes using
02:53 - this course
02:56 - if you like the course you can help me
02:58 - by making a small donation this is the
03:01 - link to my buy me a coffee page
03:06 - you can of course buy one of my other
03:08 - courses where you'll learn to run
03:11 - containers on different Cloud providers
03:13 - services and use a managed kubernetes
03:16 - cluster in the cloud
03:19 - and finally I wish you all the best in
03:22 - your Learning Journey
03:27 - [Music]
03:33 - let's see how to set up your laptop or
03:35 - PC for this course
03:39 - you need a laptop PC or Mac with either
03:42 - Windows 10 Mac OS or Linux
03:45 - if you have a Mac with an apple silicon
03:48 - most tools should run perfectly
03:51 - I will use Visual Studio code with the
03:54 - docker extension to help build create
03:57 - and run containers
03:59 - vs code is a free IDE that runs on
04:02 - Windows Mac and Linux
04:05 - on Windows and Mac you'll need Docker
04:08 - desktop with kubernetes enabled
04:11 - on Linux refer to the documentation on
04:14 - how to install Docker and kubernetes on
04:17 - your distro
04:19 - you'll need a Docker of account
04:22 - and a few easy to install tools
04:25 - refer to the setup instructions located
04:28 - below this video
04:32 - the lab files are located in a git repo
04:35 - on GitHub
04:37 - simply open this URL in a browser
04:43 - click on the code button
04:46 - if you have git installed on your
04:48 - machine you can type git clone with the
04:52 - link displayed here
04:54 - and if you don't have git simply click
04:57 - on the download zip button to download
05:00 - the code as a zip file
05:03 - [Music]
05:08 - let's talk about the microservices
05:10 - concepts
05:12 - if we head to Wikipedia and take a look
05:15 - at the definition that we find over
05:17 - there it says that it's a variant of the
05:20 - service oriented architecture or SOA a
05:23 - structural style slash architecture
05:26 - and that it arranges in application as a
05:29 - collection of loosely coupled services
05:32 - so instead of a large monolithic system
05:35 - we have multiple smaller pieces
05:39 - in a microservices architecture services
05:41 - are fine-grained meaning that each of
05:44 - them have their own responsibilities
05:47 - and the protocol use are lightweight
05:50 - like an API exposed over HTTP or grpc
05:54 - for example
05:57 - if we look at the monolithic
05:59 - architecture
06:01 - these systems were usually built as one
06:04 - single unit
06:05 - an IDE would group multiple projects and
06:08 - we would compile the whole thing as one
06:10 - single unit
06:13 - they were also deployed as a single unit
06:15 - so we would need to copy everything all
06:19 - the files on on a server
06:23 - and if we had to scale the system we had
06:26 - to spin a new VM and copy deploy the
06:30 - whole system on that VM and same for a
06:33 - third and a Ford server
06:36 - an example of such a monolithic
06:39 - architecture is a treaty application
06:42 - even though the system was clearly
06:44 - separated into layers
06:46 - it was all tightly coupled from the web
06:49 - project we had to make a reference to
06:51 - the business layer project and the whole
06:54 - system would run in the same address
06:56 - space
07:00 - with macro Services we break our big
07:04 - system into smaller parts each with its
07:07 - own responsibility
07:10 - so let's say we have a class that deals
07:13 - with identity in our business layer we
07:16 - can extract that code and place it in
07:18 - its own microservice
07:21 - we can then scale each of these smaller
07:24 - pieces independently from each other
07:28 - there's no strong bound since we expose
07:30 - functionality through an API
07:34 - they can be written by smaller teams and
07:37 - each can use their own programming
07:40 - languages like go PHP C sharp
07:45 - and domain-specific data can be stored
07:48 - in separate databases
07:52 - so the way we would deploy a monolithic
07:55 - system is by deploying everything on a
07:58 - server all the dlls files needed to run
08:01 - the system
08:03 - we had to scale the deploy everything on
08:07 - more servers
08:11 - now let's compare that to microservices
08:13 - well
08:15 - microservices are deployed independently
08:19 - each can scale independently also
08:28 - need to scale back one service
08:30 - no problem
08:35 - so if you have an existing monolithic
08:37 - system how can you transform it into a
08:40 - microservices architecture
08:42 - well you need to break it into small
08:46 - units like the code that dealt with the
08:48 - identity in the in our business layer
08:53 - Martin Fowler author of the patterns of
08:56 - Enterprise application architecture book
08:59 - documented the way to achieve such a
09:02 - transformation using the Strangler
09:04 - pattern
09:05 - let's say our identity code is here in
09:08 - the Legacy system
09:10 - we can place a facade to Route the calls
09:13 - to it migrate the code
09:15 - and have the facade route the calls to
09:18 - the new macro service at some point
09:21 - as we go we will end up with less code
09:24 - in our Legacy system
09:26 - and when democracy and it's done we can
09:29 - get rid of the facade
09:31 - this pattern is very useful and you can
09:34 - learn more about it using the link in
09:37 - this slide
09:40 - and this concludes this lecture on the
09:42 - Marco Services Concepts
09:46 - [Music]
09:52 - let's talk about Marco Services
09:54 - anti-pattern because it's a thought
09:56 - Rosie I know it's kind of strange to
09:59 - talk about what can go wrong right away
10:01 - but I think it's very important
10:06 - first of all
10:08 - it's not some kind of magic pixie dust
10:10 - that you can sprinkle on top of a
10:12 - zigzing system and boom you get a
10:15 - beautiful Marco Services architect
10:17 - system
10:19 - it takes efforts and maturity to achieve
10:22 - this
10:24 - from one monolithic system you'll end up
10:28 - with a bunch of smaller pieces and that
10:30 - can add extra complexity
10:33 - a change to a microservice can have a
10:35 - domino effect and take your system down
10:39 - and what about securing all of these
10:41 - microservices
10:44 - it's also essential to use or introduce
10:47 - new processes in the organization
10:51 - like devops Ci CD and testing
10:55 - but be careful and don't try to
10:57 - implement everything at the same time
10:59 - it's a recipe for disaster
11:02 - take it step by step and make sure you
11:05 - have metrics in place to validate each
11:08 - of these steps
11:10 - and this concludes this lecture on
11:12 - microservices antibatterns
11:15 - [Music]
11:21 - let's talk about the microservices
11:22 - benefits and drawbacks
11:26 - since each microservice runs in its own
11:29 - address space
11:31 - there are less chances that if one of
11:33 - them goes down it takes the whole system
11:35 - down with it
11:38 - a microservice runs on open source
11:41 - Technologies so there's less Fender
11:43 - lock-in
11:45 - since they are smaller in most case they
11:49 - are easier to understand
11:52 - and that makes them faster to deploy
11:56 - and also easier to scale
12:02 - like we saw in the anti-pattern section
12:04 - there are some drawbacks
12:08 - complexity is added over mother's
12:11 - existence to resolve to her complexity
12:13 - issues
12:15 - so make sure your team is well trained
12:17 - and has made some proof of concept and
12:21 - make sure to start small adding one
12:25 - piece at a time
12:28 - testing might appear simpler since there
12:31 - are less functionality in a microservice
12:33 - to test
12:34 - but make sure to test the whole system
12:40 - deployment may appear simpler but one
12:43 - update can impact many Marco services
12:45 - and have a terrible domino effect
12:49 - ready to manage multiple databases
12:54 - calls between macro services will go
12:56 - through apis and this will add a bit of
12:59 - latency to all calls so make sure you
13:02 - test for that
13:05 - this key transition server will appear
13:08 - you'll make a call and it will fail or
13:11 - but try again 50 milliseconds later and
13:15 - it will work
13:16 - so make sure to implement some retry
13:18 - strategies in your code or by using a
13:21 - service mesh
13:24 - instead of one big point of failure
13:27 - you'll end up with multiple ones can
13:30 - your system survive if one microservice
13:32 - goes down
13:36 - and what about security
13:37 - are you okay for all these microservices
13:40 - can see and talk to each other
13:43 - so yes complexity is introduced for
13:47 - solving complexity issues
13:51 - and this concludes this lecture on the
13:53 - macro Services benefits and drawbacks
13:56 - thank you
13:58 - [Music]
14:04 - let's Now understand what is cloud
14:06 - native
14:08 - you may have heard the term Cloud native
14:10 - before
14:12 - but what it is exactly
14:15 - it's a way to architect and build
14:17 - complex systems taking advantage of
14:19 - modern development practices and the use
14:22 - of cloud infrastructure
14:25 - if we head to the cloud native
14:27 - Foundation website and look at the
14:29 - definition
14:31 - we see that it's quite a long one
14:35 - so let's break it into smaller parts
14:39 - Cloud native uses containers service
14:42 - meshes microservices immutable
14:44 - infrastructure and declarative apis
14:48 - we'll cover containers service meshes
14:50 - and macro services and the concept of
14:52 - immutability in this course
14:54 - but not how to build apis
14:58 - immutable infrastructure means that we
15:01 - usually never update something but we
15:03 - replace it with a newer version
15:08 - Loosely coupled systems mean that the
15:11 - functionalities are exposed through apis
15:15 - observable with the use of metrics
15:19 - creation and updates are automated
15:22 - and instead of making changes once every
15:25 - six months we deploy eye impact changes
15:28 - on a freaking basis
15:31 - and finally we use a series of Open
15:34 - Source projects to run our system
15:40 - when the cncf says to use open source
15:43 - projects they are not kidding
15:46 - this cncf landscape graph shows a ton of
15:49 - Open Source projects that you can use
15:52 - but don't worry you don't have to use
15:54 - them all
15:56 - the challenge really is to identify
15:58 - which one to use in the context of what
16:01 - you want and try to achieve
16:05 - and this concludes this lecture on cloud
16:07 - native
16:08 - head to the cncf website for more info
16:14 - [Music]
16:20 - let's go deeper in the cloud native
16:22 - Concepts
16:24 - Cloud native is about Speed and Agility
16:29 - the user wants new features right away
16:31 - without any downtime
16:34 - and the business wants faster release of
16:37 - feature to stay competitive
16:42 - a cloud native application architecture
16:44 - starts with clean code
16:48 - using domain-driven design techniques
16:52 - Markle services
16:56 - and kubernetes
16:58 - this course is all about microservices
17:00 - and kubernetes
17:02 - feel free to explore the concepts of
17:04 - clean code and DDD on your own
17:08 - with Cloud native we need to change
17:11 - mentalities
17:14 - infrastructure becomes immutable and
17:16 - disposable
17:18 - it is provision in minutes and Destroy
17:21 - as fast
17:23 - it is never updated but it's replaced
17:26 - with newer versions
17:28 - traditionally we would care about our
17:30 - virtual machines
17:32 - we would patch the OS update the apps
17:36 - with containers we create newer version
17:39 - with the software updates
17:42 - destroyed the previous running ones
17:44 - and replace them with the newer ones
17:48 - so the containers that you'll run will
17:50 - be more like cattle than pet
17:55 - of course this Cloud native thing is a
17:58 - lot easier when starting a new project a
18:02 - blank page or a green field
18:06 - however it's still possible with Legacy
18:09 - projects
18:13 - I really like the cloud native
18:15 - Foundation trail map because it breaks
18:19 - the journey to Cloud native into smaller
18:21 - measurable objectives
18:25 - you can set your own performance
18:27 - indicator to measure each steps to
18:29 - ensure a smooth Journey
18:33 - so let's take a look at the first steps
18:37 - your team must first learn how to
18:40 - cantonerize your application
18:43 - the developers and the IT Pros must know
18:46 - how to deploy and monitor containers
18:51 - you need to automate deployment through
18:53 - the use of continuous integration and
18:56 - continuous delivery techniques and tools
19:01 - you need to use an orchestrator like
19:03 - kubernetes
19:04 - and maybe deploy your application using
19:07 - L charts
19:11 - then you need to add observability so
19:13 - you can understand what's happening in
19:15 - your kubernetes clusters and be reactive
19:21 - use tools like service meshes to provide
19:24 - more functionalities inside your cluster
19:29 - Implement security through policies
19:34 - and wow these were just the first six
19:37 - steps Now understand that you don't have
19:41 - to implement all of this and especially
19:43 - not at the same time
19:45 - I really like this trail map because it
19:47 - breaks the journey into smaller steps
19:49 - that the management can understand and
19:51 - measure
19:55 - and this concludes this lecture on the
19:57 - cloud native Concepts
20:00 - [Music]
20:05 - let's take a look at the cncf website
20:08 - the website is located at cncf.io
20:13 - and from there you can take a look at
20:15 - the various projects maintained by the
20:17 - cncf
20:19 - information about on how to get
20:21 - certified
20:23 - Community Information like the
20:25 - conferences that the cncf organized each
20:27 - year
20:29 - so let's go back to the projects menu
20:32 - and you'll notice that projects are
20:34 - categorized in three categories sandbox
20:37 - incubating and graduated let's click
20:41 - here on this the second menu here
20:43 - and let's scroll down to the bottom and
20:48 - here you get the information about the
20:51 - meaning of these three categories and
20:53 - basically that's their maturity level
20:56 - sandbox projects are mostly newer
20:58 - projects uh well graduated projects our
21:02 - projects set conservatives Enterprises
21:04 - are more likely to use
21:07 - so let's take a look at the graduated
21:10 - ones we found here kubernetes
21:15 - Helm Jagger so let's click on kubernetes
21:18 - and basically that will show you the the
21:22 - project website
21:23 - let's take a look at the incubating ones
21:26 - here we find Linker D grpc let's click
21:32 - on grpc you can get more information
21:34 - about the grpc
21:37 - all right
21:38 - let's go back to this menu remember the
21:41 - trail map that I mentioned in the
21:43 - previous lecture well here it is cloud
21:46 - native trail map
21:49 - here's the nice diagram you can get more
21:52 - information you can get send that to
21:55 - friends colleagues
21:57 - and here's a link to the landscape
22:00 - diagram
22:03 - here it is it's super huge
22:07 - Let's uh let's click here on kubernetes
22:11 - and here we get very interesting
22:13 - information
22:14 - well you get the repository where the
22:16 - project is uh is stored you get the
22:19 - number of stars and the activity number
22:22 - of commits here
22:24 - you get you get the website address
22:28 - and also the Twitter handle here
22:32 - so you should follow the Twitter feed of
22:35 - the project that you're using so let's
22:37 - close this one
22:39 - let's click here on hell
22:42 - again
22:43 - website
22:45 - uh repository number of stars activity
22:49 - and the Twitter handle
22:57 - and this concludes this look at the cncf
22:59 - website
23:02 - thank you
23:08 - let's now talk about the containers
23:10 - Concepts
23:13 - containers containers containers
23:16 - they are everywhere but what are they
23:20 - exactly
23:23 - a container is a unit of deployment
23:27 - it contains everything needed for the
23:30 - code to run
23:31 - so the compile code the runtime system
23:35 - libraries and this is system tools
23:39 - you take a container push it on a server
23:42 - and it should run of course it can have
23:46 - some external dependencies like a
23:48 - database or a cache but the code
23:51 - deployed in it should run as is
23:57 - so why use containers
24:00 - because it is faster to deploy something
24:03 - small than something big like a complete
24:07 - monolithic system
24:10 - uh they use fewer resources they are
24:12 - smaller
24:15 - and since they are smarter you can fit
24:17 - more on the same server
24:21 - when using cicd techniques they are a
24:24 - lot faster to deploy
24:28 - you can run them anywhere
24:32 - and they are isolated from each other
24:34 - meaning that if one fails
24:37 - it will not take the whole system down
24:39 - with it
24:42 - so what exactly is virtualize
24:45 - let's compare virtual machines with
24:47 - containers
24:50 - a VM runs on some kind of Hardware where
24:54 - an OS is installed
24:56 - the OS hypervisor will let you create a
25:00 - virtual machine
25:01 - where you will install in OS
25:06 - so basically DVM virtualized the
25:10 - hardware
25:12 - and what's happening when a VM starts
25:15 - well you see the BIOS coming up and then
25:18 - the OS boots up
25:22 - and what about the size of that VM let's
25:25 - say we have a Windows Server VM
25:27 - it can take 12 gigabytes of RAM and 500
25:31 - gigabytes of hard drive space
25:34 - and how long does it take to boot
25:37 - well depending on multiple factors
25:39 - something like 5 to 10 minutes
25:44 - now let's compare that to containers
25:48 - we still have the hardware and the OS of
25:50 - course
25:52 - there's a container runtime installed uh
25:56 - in the OS
25:57 - and containers images are run in memory
26:01 - now compared to a VM a container does
26:05 - not have to boot
26:06 - because it will use the host OS kernel
26:11 - this means that container starts in
26:13 - seconds because they don't have to boot
26:18 - they also use a lot less memory and hard
26:21 - drive space since there's no OS
26:25 - a small container can take a hundred
26:28 - megabyte of hard drive space and run in
26:30 - 64 or 100
26:33 - megabyte of RAM
26:37 - so VM and containers
26:40 - virtual machine have a larger footprint
26:43 - they are slower to boot
26:46 - ideal for long running tasks
26:49 - container are lightweight they're quick
26:52 - to start they don't have to boot they're
26:54 - portable
26:55 - and they're ideal for short-lived tests
26:58 - because they you can spin one super fast
27:03 - so are containers replacing virtual
27:05 - machines
27:07 - our virtual machines obsolete
27:10 - absolutely no containers are just
27:13 - another tool in your toolbox
27:16 - and you need to find the right use case
27:18 - for them and also for VMS
27:24 - if you're old enough you must remember
27:26 - what the telephone booth is if not well
27:29 - before cell phones we used to make phone
27:32 - calls in these spoons by dropping a dime
27:35 - or a quarter anyways
27:37 - using a telephone boot analogy you can
27:40 - pack more containers on the same server
27:44 - than what's possible with virtual
27:46 - machines
27:49 - containers are made of layers
27:52 - you start with the base OS
27:56 - add customizations
27:58 - and add your applications
28:02 - let's take a deeper look at this
28:04 - screenshot
28:06 - the docker pull command retrieves and
28:09 - download a container image
28:11 - as you can see each layer is downloaded
28:14 - individually
28:18 - notice that each has a unique ID
28:21 - and that for the first ones Docker says
28:24 - that they already exist why is so
28:28 - Docker uses a local cache to store the
28:31 - container's images and if a layer
28:34 - already exists it will not be downloaded
28:37 - again
28:38 - the benefit is that if you pull version
28:40 - 2 of an image Docker will only download
28:43 - the layers not present in its cache
28:48 - one of the goal when creating container
28:50 - images is to create them with the
28:52 - smallest number of layers possible
28:55 - later on we'll see techniques on how to
28:58 - achieve that
29:00 - now can you write on these layers
29:04 - well no except for the top one because
29:08 - it is read right
29:10 - the lower ones are read only
29:15 - another concept is the container
29:18 - registry
29:20 - it's a centralized repository where you
29:23 - deploy the container images you create
29:26 - think Gita but for containers
29:30 - Docker as one called Docker up that
29:33 - provides public and private repositories
29:37 - and all major Cloud providers have
29:40 - container registry services
29:45 - the last container concept is the
29:48 - orchestrator
29:50 - an orchestrator allows us to manage
29:52 - scale monitor the containers that we run
29:56 - on our servers
29:58 - you can install your own
30:01 - or use a managed cluster offered by one
30:05 - of the cloud providers like AWS Azure
30:08 - Google Cloud
30:10 - we will come back to the orchestra
30:12 - Concepts after we have a better
30:14 - knowledge of containers
30:17 - and this concludes this lecture on the
30:19 - containers Concepts
30:23 - [Music]
30:28 - let's Now understand what is docker
30:32 - so what is stalker that may seem like a
30:36 - simple question but there's more to it
30:39 - there's Docker the company
30:42 - and Docker the platform
30:45 - document thinks the Mobi project an open
30:49 - source container runtime that follows
30:51 - the specs from the open container
30:53 - initiative
30:55 - doctors sold its Docker Enterprise
30:58 - division late 2019 to a company called
31:00 - mirantis
31:02 - so if you want to buy Enterprise support
31:04 - or get certified with docker you have to
31:08 - go through morentis
31:11 - Docker provides a container runtime
31:15 - that runs on Mac windows and Linux
31:19 - a command line tool to create and manage
31:22 - a containers
31:24 - a Docker file format for building
31:27 - containers
31:29 - and interestingly Windows lets you
31:32 - create both windows and Linux containers
31:36 - foreign
31:39 - if for some reason Docker doesn't seem
31:42 - to work on your machine try restarting
31:44 - it by using the restart menu from the
31:47 - system icon on Windows
31:50 - or by clicking on the debug icon in
31:53 - Docker desktop on Mac and windows and
31:56 - clicking on restart
31:57 - Docker desktop is very stable but I had
32:00 - some issues when my laptop was coming
32:02 - back from hibernation but that was a
32:06 - long time ago and I haven't had issues
32:08 - for a while
32:11 - and this concludes this lecture on
32:13 - docker
32:16 - [Music]
32:21 - the easiest way to run Docker on your
32:24 - machine is by running Docker desktop
32:27 - it's a free download available on
32:29 - docker.com so you click here on get
32:33 - started
32:35 - and you download the version for your OS
32:39 - so Windows Mac Linux here
32:42 - now if you're running Windows uh check
32:46 - the version of Windows that you're
32:48 - running
32:49 - if you're running Windows 10 version
32:52 - 2004 or a later version
32:55 - you can run what what's called windows
32:57 - subsystem for Linux version 2. so wsl2
33:03 - basically it allow you to run a Linux
33:06 - distribution right into your Windows
33:09 - installation so Docker desktop can run
33:13 - its container by installing a virtual
33:17 - machine inside hyper-v or if you have
33:20 - wsl2 install it will it will install
33:24 - that virtual machine
33:26 - inside the Linux distribution and
33:29 - everything will be a lot faster so
33:31 - that's the preferred way so just to
33:33 - prove a point here I'm going to launch
33:36 - my hyper-v
33:38 - miniature
33:41 - and as you can see I'm not running any
33:45 - virtual machine so my Docker desktop
33:48 - installation uses wsl2
33:52 - you'll find a link to this installation
33:54 - guide in the modules notes
33:58 - okay so let's take a look at the docker
34:01 - desktop I'm running Windows as you can
34:04 - see I can see my Docker desktop system
34:08 - tray icon here so for Mac User it'll be
34:12 - at the top of the screen of course I can
34:15 - right click on it and let's select
34:18 - dashboard
34:22 - all right so here I can see a list of
34:27 - um containers that are currently running
34:29 - I can stop them restart them delete them
34:33 - I can see a list of uh images that are
34:37 - installed on my machine here we'll come
34:40 - back to that later on there's a gear
34:44 - icon here that's the setting icon and in
34:49 - the general section here
34:51 - you can see that
34:54 - wsl2 is enabled so that's why Docker
34:58 - desktop doesn't use a Now preview
34:59 - virtual machine it uses the wsl2 to run
35:04 - the VM
35:06 - and there's the kubernetes menu here if
35:10 - I select it I can see that
35:13 - kubernetes is enabled so when you check
35:18 - that Docker desktop will download
35:20 - additional containers to run kubernetes
35:24 - right onto Docker desktop here so very
35:28 - useful
35:30 - there's a bug icon here which is the
35:33 - troubleshoot icon here so
35:36 - if at some point you're issuing Docker
35:40 - commands and the don't work or
35:44 - something's wrong uh running Docker
35:47 - Docker commands
35:49 - you can click here on the restart button
35:51 - here
35:53 - you can see my name here it means that
35:57 - I'm currently logged and if I right
35:59 - click down on the system play icon
36:03 - you can see that I'm currently logged in
36:06 - and I have the option to to sign out so
36:08 - what uh username and password did I use
36:11 - to to log in
36:14 - when you downloaded the uh the docker
36:17 - desktop you could have created a Docker
36:19 - account or a Docker Hub account so
36:22 - that's the same username and password so
36:25 - if you go to
36:27 - up.ducker.com and you logged in well
36:32 - that's the same account that is used
36:34 - here in Dr desktop
36:37 - thank you
36:39 - [Music]
36:44 - let's take a look at your first Docker
36:47 - CLI commands
36:50 - now throughout this course I will
36:52 - introduce you to various commands
36:55 - I will list them in what I call a cheat
36:58 - sheet list like what you find on the
37:01 - slide
37:02 - I will briefly explain what the commands
37:04 - are for
37:06 - and that will be followed by a concrete
37:09 - and Zone demonstration
37:11 - so when you install Docker desktop on on
37:14 - your Mac or PC
37:16 - it also installed the docker CLI tool
37:21 - our first command is Docker info
37:24 - this will display some information about
37:26 - the docker installation on your machine
37:30 - Docker version will display its version
37:34 - and Docker login will log you into a
37:37 - Docker registry
37:39 - by default this login command will log
37:41 - you into Docker Hub the registry from
37:44 - docker
37:47 - and this concludes this lecture on the
37:50 - docker CLI
37:53 - [Music]
37:58 - alright we need to open a new terminal
38:01 - window into Visual Studio code so let's
38:05 - select a terminal new terminal or you
38:08 - can use the shortcut Ctrl shift back
38:11 - tick key
38:13 - all right this will open a terminal
38:17 - window and let's take a look at the
38:20 - commands that we will run they're really
38:23 - basic
38:24 - it's just for testing that our Docker
38:28 - installation is working correctly so
38:30 - let's type Docker info
38:33 - this will give me some information about
38:35 - my current installation what's happening
38:38 - I have 47 containers three stuff 44
38:42 - running 25 images and so on and so on so
38:46 - information that that is quite useful
38:49 - for debugging purposes I can see that
38:51 - the virtual machine running that running
38:54 - Docker desktop is running as to CPU it
38:57 - has two gigabytes of memory allocated
39:01 - all right sounds good let's uh type now
39:06 - Docker version
39:09 - and this will uh give me some
39:10 - information about the version number of
39:13 - different parts of Docker desktop
39:16 - so again useful for debugging purposes
39:21 - these are not commands that you will run
39:23 - on a day-to-day basis but they're quite
39:26 - useful for troubleshooting the last
39:28 - command is Docker login so I'm just
39:33 - going to type Docker login without any
39:34 - username password see what's happening
39:37 - and well the command says that I am
39:40 - login successfully why is so
39:44 - well if I right click here on my Docker
39:49 - desktop I can see that I'm already
39:51 - logged in so that's why I didn't have to
39:55 - input or type any username password I
39:58 - was already logged in
40:02 - [Music]
40:07 - let's now see how to run containers
40:12 - the docker pull command lets you
40:14 - download an image from a registry
40:18 - Docker run will execute an image in
40:21 - memory as a container
40:24 - if the image is not present in the
40:26 - docker local cache it will be downloaded
40:29 - automatically
40:32 - using run with the Dash D flag will run
40:36 - the container in the background
40:38 - giving you your comment prompt or
40:39 - terminal back
40:43 - the start command will run a container
40:45 - in the stopped state
40:49 - Docker PS will list all the containers
40:52 - currently running
40:54 - and add the dash a flag to also lists
40:58 - all the stopped ones
41:02 - Docker stop will stop a container
41:05 - running but the container will still be
41:07 - in memory
41:08 - we will see how to remove them from
41:10 - memory in a few minutes
41:14 - Dr kill will well kill a container that
41:18 - might be stuck in memory
41:19 - you usually don't use this comment but
41:22 - it's useful to know
41:25 - Docker image and spec will give you some
41:28 - information about an image very useful
41:30 - for debugging purposes
41:33 - so you may notice that we have two
41:36 - parameters here
41:37 - one is called image name and the second
41:40 - one container name so what's the
41:42 - difference
41:44 - the image name is the name of the image
41:47 - as you find it in the container registry
41:50 - and the container name is the name of
41:53 - the running container
41:55 - so you run an image using its name and
42:00 - then interact with it using the running
42:02 - instance name
42:05 - the Run come in as an optional flag call
42:07 - dash dash name that lets you specify
42:10 - your name
42:12 - if you don't specify one Docker will
42:15 - Auto generate one for you
42:18 - you can set limits on the memory and the
42:22 - CPU that the container can use when
42:25 - using the Run command
42:29 - so how do you run a container
42:33 - using the docker run command you specify
42:36 - the image name as found in the container
42:39 - registry
42:41 - you specify a name for the running
42:43 - instance
42:45 - and with the published flag you map a
42:48 - port from your local OS to the port that
42:51 - the container is listening to
42:54 - you can list the running containers
42:56 - using Docker PS
42:59 - notice how we stop the container by
43:02 - using the running name and not by using
43:04 - the image name
43:06 - then we remove it from memory using the
43:10 - remove or RM command
43:13 - containers are not black boxes you can
43:16 - attach a shell to your terminal and run
43:18 - commands that will execute right inside
43:20 - the running container
43:22 - by using the dash it switch and the name
43:26 - of the program you want to run
43:29 - a Windows container well you can run
43:32 - Powershell
43:35 - using the docker container exact command
43:38 - you can attach to a running container
43:42 - here's a screenshot showing the docker
43:44 - run command
43:47 - and notice the terminal problem changing
43:49 - when attached to a container
43:54 - so how do we clean up things
43:58 - the remove command lets you remove a
44:02 - container from memory
44:04 - but first it must be in the stop state
44:08 - for the command to work
44:11 - here's the remove command getting a list
44:13 - of the stopped container and removing
44:16 - them all
44:19 - the images that you pull will be cached
44:22 - locally
44:24 - you can get a list of these images using
44:26 - the docker images command
44:31 - use the remove image or RMI command to
44:35 - delete an image from your machine
44:39 - after a while you may end up with a
44:41 - bunch of images
44:42 - to do some spring cleanup I use the
44:46 - system prune command
44:48 - this will delete all the image currently
44:51 - not in use so be careful using this
44:53 - command
44:58 - and this concludes this lecture on the
45:00 - docker CLI
45:03 - [Music]
45:07 - let's now run our first container we'll
45:10 - run an engine X web server so something
45:13 - pretty basic that'll be perfect for this
45:16 - this lab all right let's open a terminal
45:20 - so terminal new terminal or Ctrl shift
45:25 - back tick
45:28 - perfect
45:29 - so we'll run disk command so let's take
45:31 - a look at the command first
45:33 - so I'm going to run a Docker run
45:36 - and let's go at the end of the command
45:39 - this will be the image that will run so
45:42 - an nginx image
45:45 - we'll give it a name
45:47 - so the name will be web server that will
45:50 - be the name of the running instance
45:54 - will map local All Sport
45:59 - 8082d or that the container is listening
46:02 - to so port 80.
46:04 - n d 4G decimal so we can get our Command
46:08 - Prompt or terminal prompt back so let's
46:11 - run this
46:15 - ha something interesting is happening
46:17 - unable to find image nginx latest and
46:21 - you see that the docker as pull all the
46:24 - different layers locally here
46:27 - so now if I issue a Docker PS
46:32 - it will list the containers that are
46:34 - currently running so I have uh three
46:36 - containers running so the kubernetes
46:39 - dashboard and the metric server here so
46:42 - don't uh look at these let's focus on
46:46 - this one so this is the containers it is
46:49 - that we just launched the nginx image
46:53 - here
46:54 - it started 30 seconds ago
46:58 - we can see that the port 8080 local
47:01 - levels is mapped to the port 80 here and
47:06 - the name is web server
47:09 - so let's launch a web browser and let's
47:14 - type localhost 8080
47:19 - the container is actually running
47:22 - fantastic all right
47:27 - so we can get a list of the images
47:31 - installed on your machine here by using
47:34 - Docker images so I have a bunch I may
47:37 - have a lot more than you but let's focus
47:40 - on the last one here uh nginx the tag is
47:44 - latest image ID and this is the uh the
47:48 - size
47:50 - all right fantastic let's try to connect
47:54 - to uh to it so we'll issue a Docker
47:57 - container exact
48:00 - we'll give it the the name so the
48:04 - running instance and the program we want
48:06 - to run
48:11 - so look at the The Prompt now
48:14 - root at some some ID so the ID is the
48:20 - actual container ID so I'm logged in as
48:23 - root on that container so now I can
48:26 - issue some some commands so let's do in
48:29 - the last
48:32 - let's thy fell less been to see what's
48:36 - in there so different commands so I'm
48:39 - connected to to that running container
48:42 - that's pretty uh pretty cool so I can
48:43 - issue commands look at the logs if any
48:46 - and do some troubleshooting so this is
48:49 - super useful
48:51 - for debugging purposes uh we'll use that
48:54 - a lot in the various steps that we'll do
48:57 - together
48:58 - all right let's get out of there by
49:02 - typing exit
49:04 - all right you can just clear my screen
49:07 - here
49:08 - so our container is running how do we
49:11 - stop it we use the docker stop command
49:15 - but
49:17 - look at the the parameter that we use
49:20 - the name of the container that we use we
49:22 - use the running instance name not the
49:26 - name of the image so that's pretty
49:27 - important here
49:29 - Docker stop web server
49:32 - all right but the container is still in
49:36 - memory if I do a doctor PS
49:40 - it's not listed anymore as you're
49:43 - running container but if if I type
49:46 - darker es Dash a
49:50 - ha for all I can see that my my
49:54 - container is still in memory here
49:57 - so I need to remove it from uh from
50:00 - memory
50:01 - so we'll use the docker RM and the name
50:05 - of the running instance so RM for remove
50:10 - and now the container is no longer
50:15 - in memory awesome
50:18 - now
50:20 - the container is still well the image
50:23 - that was used to create the container is
50:26 - still on my machine
50:28 - so if I type darker images
50:34 - you see it's still it's still here
50:38 - so that takes 133 megabyte of this space
50:44 - if I want to get rid of that I use the
50:47 - RMI so remove image command and the name
50:52 - this time of the image not the name of
50:54 - the running instance because none are
50:58 - running right now
51:01 - and you see all the layers
51:04 - have been deleted
51:09 - [Music]
51:14 - let's now see how we can build
51:16 - containers
51:19 - Docker build that you create an image
51:21 - using a Docker file
51:24 - if you run the command in the same
51:26 - folder where the docker file is located
51:29 - simply use a DOT as the file name
51:34 - if the file is located in a different
51:37 - folder specify the location using the F
51:41 - flag
51:44 - the tag command let you assign a name to
51:47 - an image
51:49 - this tagging has two parts
51:51 - a name and a tag
51:54 - the tag is usually used to specify the
51:57 - version number
52:01 - so what is a Docker file
52:05 - well it's a text file listing the steps
52:09 - to build an image
52:11 - here's the simplest Docker file I can
52:13 - imagine
52:14 - two lines
52:16 - the from command specified the base
52:19 - image
52:21 - when building new images you always
52:23 - start from something already existing in
52:26 - this case an image with the nginx web
52:29 - server using the Alpine version
52:34 - and then the copy command copies
52:36 - everything from the current folder to a
52:39 - folder inside the container
52:43 - using the build command we create a new
52:47 - image specifying the docker file
52:50 - remember to use the dot when the file is
52:54 - located in the same folder
52:59 - here's another one this time a little
53:02 - bit more complex
53:04 - it is used to create an image running a
53:06 - node.js app
53:09 - let's take a look at it
53:12 - the from command specify the base image
53:18 - using the Run command we run the package
53:21 - manager inside the container to install
53:23 - node.js
53:26 - next we copy all the local files into a
53:30 - folder named SRC inside the container
53:35 - we use the Run command to do an npm
53:38 - install
53:40 - we then had some metadata in this case
53:43 - we tell the container to listen on port
53:45 - 8080
53:48 - and finally we tell the container what
53:51 - to run when starting
53:54 - so as you see this Docker file contains
53:56 - the stats needed to run our node.js app
54:02 - we saw what tagging was a moment ago
54:05 - let's explore this again
54:08 - using the docker tag command we name an
54:11 - image using a name and optionally a tag
54:17 - if you don't specify a repository name
54:21 - it will default to Docker Hub later on
54:25 - we'll see how to push images to
54:26 - different repositories and we'll have to
54:29 - specify the Ripple's Journey name when
54:31 - tagging our images
54:34 - and this concludes this lecture on how
54:36 - to create Docker images
54:41 - [Music]
54:45 - let's see how can Visual Studio code
54:48 - help us build and run containers
54:52 - so what is visual studio code and why
54:55 - talk about this tool in this course
54:58 - well it's a text and code editor
55:01 - it's free and open source
55:05 - it runs on Windows Mac and Linux
55:09 - and you can download it for free using
55:11 - this link
55:14 - so you will work a lot with text files
55:16 - creating Docker files and later on
55:18 - Docker compose and yaml files
55:22 - a tool like vs code will help you
55:25 - because you can install plugins that
55:27 - will make your life easier
55:30 - using a different text editor no problem
55:35 - in vs code you can install plugins by
55:39 - clicking on the extension icon in the
55:42 - left menu
55:44 - then you search for Docker and install
55:47 - the extension from Microsoft
55:52 - the extension lets you add Docker files
55:55 - to your projects using the command
55:57 - palette
55:59 - open it using the view menu or type Ctrl
56:02 - shift p
56:05 - type Docker add and select add Docker
56:09 - files to workspace
56:12 - the extension will ask you a few
56:15 - questions
56:16 - and we'll create the docker files for
56:19 - you
56:24 - vs code has a built-in terminal where
56:27 - you can type commands
56:29 - or you can run commands using the
56:32 - command palette
56:35 - here's another example when running a
56:38 - container
56:40 - and when using the command palette
56:43 - there's no magic there the extension
56:45 - will simply issue a command in the
56:48 - terminal
56:49 - but sometimes it's a great way to learn
56:55 - creating Docker files is okay but what I
56:58 - like the most is the UI provided by the
57:01 - extension helping me manage my container
57:06 - so if you click on the docker icon
57:10 - you can see the images installed on your
57:13 - computer
57:15 - and you can even see the containers
57:17 - currently running
57:21 - right click on an image to manage it
57:24 - same thing for the containers currently
57:27 - running very very very useful
57:32 - and this concludes this lecture on vs
57:35 - code
57:37 - thank you
57:43 - let's now use Visual Studio code to
57:45 - containerize a node.js express
57:47 - application
57:49 - so I already installed the docker
57:52 - extension in Visual Studio code but
57:55 - let's take a look at it so if I click
57:57 - here on extensions
57:58 - and let's search for docker
58:02 - here it is
58:04 - that's the one from Microsoft almost 6
58:08 - million install at the time of recording
58:11 - so that's the one
58:13 - all right so let's go back to our files
58:17 - so I have my node.js application here if
58:21 - I click here on package.json
58:23 - [Music]
58:25 - the name of my application is my Express
58:28 - app so that'll be important in a few
58:30 - seconds
58:32 - all right so let's first add the docker
58:37 - file to our project so we'll use the
58:40 - tooling provided by the extension to do
58:43 - that
58:45 - we'll go to the view menu command
58:47 - palette or you can use the shortcut Ctrl
58:50 - shift p
58:52 - right and we'll type Docker add and
58:57 - there are two options here Docker files
58:59 - or Docker compose files so we haven't
59:03 - looked at the docker compost file so
59:05 - let's select the first one Docker files
59:08 - the extension hack is asking us about
59:11 - the application platform so it's a
59:13 - node.js application but it can generate
59:15 - Docker files for net python Java C plus
59:19 - plus go and Ruby apps so let's select
59:22 - node.js
59:25 - all right and next next it's asking us
59:28 - where is the package.js file so here it
59:32 - is it set the root of my application so
59:34 - I'll select this one
59:37 - the port that the application is uh
59:40 - listening to 3000 perfect
59:43 - and do I need the optional Docker
59:46 - compost file no not at this time
59:50 - all right so the extension quickly added
59:54 - a the docker file and also the docker
59:58 - ignore file and also a folder for vs
60:02 - code so let's take a look at the docker
60:05 - ignore basically it's a list of files
60:09 - not to deploy in our container
60:13 - and the docker file has been created for
60:16 - me to use a node base image it copies
60:21 - everything into the Container the image
60:25 - and expose port a portrait 3000 perfect
60:29 - so let's now build this image
60:34 - so we'll go back to the command palette
60:37 - view command palette Ctrl shift p and
60:41 - this time we'll search for Docker build
60:44 - all right here it is Docker images build
60:48 - image
60:51 - all right so I just issue uh the command
60:54 - and look look at what's happening
60:57 - the extension is not a black box uh
61:00 - issuing some crazy or strange uh
61:04 - commands in the background it's just
61:05 - issuing a
61:07 - a Docker Bill command here
61:11 - and remember a few seconds ago I
61:14 - mentioned that the name of the
61:17 - application in the package.json file is
61:20 - is important well that's the name of our
61:23 - uh of our application here so the
61:25 - extension use it to generate the name
61:28 - for my image
61:31 - all right
61:33 - it's just a Docker Bill command nothing
61:36 - fancy but using this extension is a
61:38 - great way to learn about some uh some
61:41 - commands that you may want to use at a
61:44 - later time from the terminal prompt
61:47 - okay terminal will be reused by test
61:50 - press any key to close it let's press
61:53 - any key all right
61:57 - the image has been built let's run it
61:59 - now so view command palette
62:04 - okay Doctor run
62:08 - so we'll use the first one
62:11 - and uh there's a list of my images
62:16 - and here is my Express app
62:19 - perfect
62:21 - run that
62:22 - select image that's the latest the tag
62:26 - latest perfect
62:29 - and look at what's happening Docker run
62:37 - sport 3000 to the port that the
62:41 - container listening to so nothing fancy
62:44 - here is uh it's a comments that you can
62:46 - type yourself but uh the extension to
62:50 - that are for you so if I
62:53 - start my browser local ocean I go to
63:00 - Port 3000
63:03 - there it is
63:05 - so that worked
63:07 - perfect
63:09 - let's now close this
63:13 - let's now use the UI provided by the
63:16 - extension so in the left menu let's
63:19 - click on the docker icon here
63:22 - and from there you have a list of the
63:27 - running containers the images and the
63:31 - different Registries that you connected
63:33 - to all right so here's our Express hat
63:38 - my Express apply it is and it's running
63:42 - name the instance name is called magical
63:46 - underscore Direct
63:48 - uh why is so well when we issued a
63:52 - Docker run command using the command
63:55 - palette
63:56 - the extension didn't provide a name so
63:59 - uh one was generated automatically by by
64:03 - docker
64:04 - so from there I can view the logs attach
64:09 - shell
64:10 - inspect open in a browser so if I select
64:15 - that
64:16 - there it is that's my app listening on
64:20 - Port 3000
64:22 - uh I can stop it restart it remove it so
64:26 - let's uh let's remove it here
64:30 - so are you sure you want to remove
64:32 - container yes
64:34 - it's not in memory anymore it's not
64:36 - running anymore
64:38 - but if I look at the list of the images
64:42 - that I have here it is my Express app so
64:46 - I can right click on it
64:48 - and I have a few options I can run it
64:51 - inspect it pull push tag and even delete
64:55 - it so let's let's run run it
65:01 - okay and you see at the top my Express
65:04 - app
65:05 - and look at the name gifted underscore
65:08 - El beckyan
65:11 - because if you look if you uh inspect
65:15 - the docker command that was issued
65:18 - by the extension there's no name
65:21 - provided
65:23 - so Docker generated one for us
65:28 - all right so let's close this
65:31 - we can stop or remove it from memory
65:36 - and if we no longer need the image we
65:40 - can just remove it here
65:44 - are you sure you want to remove the
65:46 - image blah blah blah
65:49 - okay let's remove it
65:51 - and it's gone so
65:54 - what's happening in the background is
65:56 - that the extension is simply issuing a
65:59 - Docker remove image RMI to to remove the
66:04 - image from from this so nothing nothing
66:07 - really fancy the extension it's not a
66:10 - black box that is issuing some strange
66:13 - commands it's just regular and Docker
66:17 - commands but I really like the the UI
66:20 - provided here
66:22 - and so instead of uh typing uh okay
66:25 - let's uh let's try this let's
66:28 - um
66:29 - let's clear this and let's type Docker
66:33 - images
66:35 - here's a list of my all my images but
66:38 - here I have a nice UI listing all the
66:41 - the same images here so sometimes it's
66:44 - it's easier having a little UI to help
66:48 - you accomplish some tasks other times
66:52 - it's easier issuing the commands from
66:55 - the terminal or the command prompt so
66:58 - it's up to you you can use the UI or The
67:02 - Terminal
67:03 - thank you
67:04 - [Music]
67:09 - let's talk about data persistence
67:13 - containers are ephemers and stateless
67:17 - so you don't usually store data in them
67:21 - of course you can write data in a
67:24 - container but if you destroy one or if
67:28 - it crashes any data stored in it will be
67:31 - lost
67:32 - so it's okay to write some log files or
67:37 - scrap data that you don't want to keep
67:39 - as long as you understand that you will
67:42 - lose these files at some point in time
67:46 - to purchase data you need to store it
67:49 - outside the container in what we call a
67:51 - volume
67:52 - a value Maps an external folder or even
67:55 - a cloud storage service to a local
67:58 - folder inside your container
68:01 - so your app sees a volume just like any
68:03 - regular folder
68:06 - the OS in this diagram represent the
68:09 - server or the virtual machine where the
68:12 - container is running
68:13 - as you can see a local folder is mapped
68:17 - to the VM file system so they are stored
68:20 - in a volume where survive a container
68:22 - restart or crash
68:25 - there's still a chance that we can lose
68:28 - the data if the VM crashes so later on
68:31 - we'll see how we can use some type of
68:34 - external storage provided by the cloud
68:37 - provider
68:38 - but first thing first in the next
68:41 - lecture you'll see how to create a
68:43 - volume that maps to a VM file system
68:48 - and this concludes this structure on
68:50 - data persistence
68:53 - [Music]
68:55 - um
68:58 - let's see how to create volumes
69:01 - here's a cheat sheet listing the docker
69:04 - commands for managing volumes
69:07 - Docker create volume will create a new
69:09 - volume
69:11 - LS will list all the volumes
69:16 - volume and spec will get you information
69:18 - about a volume
69:20 - remove will delete a volume destroying
69:23 - all the files storing it
69:26 - and volume prune will delete all the
69:29 - volumes currently not mounted or not in
69:32 - use
69:33 - so be super careful careful using this
69:36 - command
69:40 - alright
69:41 - you first need to create a volume using
69:43 - the docker volume create command
69:48 - then when you run a container you need
69:51 - to use the V switch or to volume a
69:53 - parameter
69:55 - specifying the volume name a column and
69:58 - the name of a local folder that folder
70:02 - will be a logical folder in your code
70:06 - so your code will see just like any
70:08 - regular folder
70:11 - if you use the inspect command you'll
70:13 - see The Logical folder location in DVM
70:18 - instead of using a volume you can
70:21 - specify a local folder
70:23 - this is great for testing purposes let's
70:26 - say you started developing your service
70:28 - and that you want to test on your Dev
70:31 - machine
70:32 - if your code can read and write some
70:36 - files correctly you can use this kind of
70:39 - mapping but don't use that in production
70:43 - using the inspect command you can see
70:46 - the local folder path
70:50 - and this concludes this lecture on
70:52 - volumes
70:55 - [Music]
71:00 - a software system data outside of a
71:03 - running container
71:05 - so to do that we'll use a value
71:09 - so I'll open a terminal
71:13 - and we'll use the docker volume create
71:16 - and the name of the volume so this will
71:18 - create a volume
71:22 - perfect let's Now list the volumes on my
71:28 - machine
71:30 - so there's a few here the first four
71:33 - were created earlier
71:35 - this is the volume that we just created
71:39 - now let's run a ninja next image
71:44 - and let's attach a or Mappy a local
71:48 - folder to that volume so we'll use
71:50 - Docker run Dash D for detach we'll name
71:53 - our instance fault test with the Dash D
71:57 - for volume we'll use the volume name
72:01 - that we created earlier and map that to
72:04 - a folder called ATP on our nginx image
72:08 - all right
72:10 - to that
72:14 - okay excellent so that worked now let's
72:18 - connect to our running instance so
72:22 - Docker exec Dash it the name of the
72:25 - instance and it will run bash here
72:30 - perfect
72:31 - let's first do an LS to see if we see
72:36 - the app folder there it is so that's
72:39 - that folder is mapped to the volume so
72:43 - anything that we store or write in that
72:47 - folder will be written externally
72:50 - outside of the container so that will
72:54 - persist
72:55 - all right
72:57 - uh just for fun instead of
73:01 - doing a cat Let's uh install Nano inside
73:05 - the running instance so I'll do first in
73:08 - apt-get update
73:12 - remember this is uh this is running
73:14 - inside the the container and we'll
73:17 - install Nano app get the install Nano
73:21 - which is a small editor perfect
73:25 - LCD into the app folder
73:30 - in open Nano so Nano and we'll create a
73:36 - file called text test.txt
73:42 - hello
73:44 - volume
73:45 - [Music]
73:47 - all right and we'll use Ctrl o to write
73:52 - to to disk Ctrl o enter and control X to
73:59 - exit Nano perfect
74:03 - if I do an LS here
74:05 - I should see my file there it is
74:10 - test.txt so let's exit the running
74:15 - instance and what we'll do will stop the
74:19 - running instance and we'll remove it
74:21 - from memory
74:23 - stop it
74:25 - here
74:28 - and we will remove it from memory with
74:32 - our m
74:34 - okay so now the container or the
74:36 - instance is gone if we would have stored
74:39 - some data inside that container it would
74:42 - it would be lost at this point but we
74:45 - use a volume so the data was stored
74:47 - externally so let's try to create a
74:51 - second instance
74:54 - same thing using the same volume and
74:58 - let's exec to it
75:02 - perfect let's
75:05 - see what is in the app folder here is
75:10 - our file so let's do cat
75:13 - test.txt just to prove hello volume
75:18 - that work
75:20 - let's exit here
75:22 - this proved that by using a volume your
75:26 - data persists a container restart or
75:29 - crash here
75:32 - the data is still there
75:35 - until I remove the volume so if I issue
75:39 - a Docker volume removed and the name of
75:43 - the the volume you see doesn't work why
75:48 - the error says that the volume is in
75:51 - news
75:52 - interesting so I need to stop
75:56 - any container instance that is that is
76:00 - running
76:01 - and remove it from memory before
76:04 - deleting or removing
76:07 - a volume let's try uh again
76:11 - term Docker volume RM for remove by
76:16 - volume
76:16 - and this time it worked
76:19 - now another thing I want to show you let
76:25 - me create it again
76:30 - perfect and let's switch to the docker
76:35 - UI here if I click Docker here
76:39 - there's a section
76:41 - that list of the different volumes here
76:44 - so my vowel is listed here
76:47 - so here I can inspect I cannot look at
76:50 - the files but I can I can manage it so I
76:53 - can click here on remove that will
76:55 - delete the volume
76:58 - are you sure yes
77:04 - [Music]
77:09 - let's now see the yaml concepts
77:12 - yaml stands for yaml ain't markup
77:15 - language
77:17 - it's a way to serialize data so that
77:20 - it's readable by human beings
77:23 - it's the file format used by Docker
77:26 - compose and kubernetes
77:30 - here's the sample yaml file
77:34 - for a key value pair you specify the key
77:38 - a column
77:40 - data space and a value
77:44 - don't forget the space it's mandatory
77:49 - here are some nested values
77:52 - you specify child values using two space
77:55 - indentation
77:58 - and quotes are not needed for string
78:01 - values
78:05 - here's a list
78:07 - again the child elements are indented
78:10 - with two spaces
78:12 - and there's a space after the dash
78:18 - this is what we call the block style
78:24 - there's also a flow style that looks
78:27 - like Json
78:28 - so you may be tempted to use it if
78:31 - you're familiar with Jason but don't I
78:34 - never saw any sample or any
78:35 - documentation using this flow Style
78:42 - since it's easy to forget a space
78:46 - and you can spend quite some time
78:48 - figuring out why your yaml file doesn't
78:50 - work
78:51 - you can use tools like this linter
78:54 - available on yamlin.com
78:57 - it will parse your yaml and flag any
79:00 - errors very useful
79:04 - and this concludes this lecture on the
79:07 - yaml concepts
79:10 - [Music]
79:16 - let's now take a look at the docker
79:18 - compose Concepts
79:20 - let's say that your app is composed of
79:22 - multiple containers
79:24 - you run the front-end container using a
79:27 - Docker run command
79:29 - Docker run again for the backend
79:31 - container
79:33 - and again for the redis cache container
79:36 - so you end up issuing multiple Docker
79:39 - run commands to run your app
79:42 - would it be nice if you could deploy
79:44 - your app using one single command
79:48 - well that's the docker compost go
79:52 - to Define and run multi-containers
79:54 - application using a single yaml file
79:59 - there's a compost plugin that extends
80:02 - the docker CLI and let you run those
80:05 - Docker compose files
80:08 - these specifications are available here
80:12 - if you look at Docker compose before you
80:15 - may have seen that sometimes the
80:17 - commands are using an iPhone so Docker
80:21 - Dash compose
80:23 - and sometimes they do not
80:25 - why is this
80:28 - at the dockercon conference in 2022
80:32 - Docker announced the general
80:34 - availability of couples version 2. okay
80:38 - this means that there was a V1 before
80:41 - and the V1 command line tool was
80:45 - installed separately from the docker CLI
80:48 - it was built using python so you needed
80:51 - to have python installed to run compost
80:54 - V1
80:55 - and the syntax was Docker Dash compose
81:01 - couples V2 is a drop in replacement
81:04 - meaning that all the V1 commands are
81:07 - working as expected
81:11 - it's installed as the docker CLI plugin
81:14 - automatically by Docker desktop
81:17 - to use it you type Docker space compose
81:21 - no hyphen needed here
81:24 - it's written in go so no need to have
81:27 - python installed to run the command
81:31 - in summary it's simply a faster version
81:34 - of the docker compose tool shipped as a
81:38 - Docker plugin instead of a python
81:41 - application
81:43 - here's a Docker compose file
81:47 - there are three containers defined in it
81:50 - web API 1 web API 2 and API Gateway
81:56 - the name that you use here defined the
81:59 - network or host name for that container
82:03 - the code running inside your container
82:05 - can use these hostname to communicate
82:08 - between each container
82:12 - for each of them you specify the image
82:14 - to run you set the internal and external
82:18 - Port the container will listen on note
82:21 - that the API version is now optional so
82:24 - it's okay to skip it
82:28 - you may ask yourself should I use darker
82:30 - compose or not
82:32 - Docker compose is perfect for small
82:35 - workloads that don't require a full
82:37 - orchestrator like kubernetes
82:39 - it's perfect When developing and testing
82:42 - locally before deploying to kubernetes
82:46 - Some Cloud providers offer services that
82:49 - support Docker compose like app service
82:51 - on Azure and ECS on AWS
82:56 - and of course you can simply use a
82:58 - virtual machine or a VPS virtual private
83:01 - server with the digitalocean or linued
83:06 - and this concludes this lecture on the
83:09 - docker compost Concepts
83:14 - thank you
83:18 - LaSalle used Docker compose
83:22 - here's a cheat sheet listing some of the
83:24 - docker compose commands
83:26 - Docker composed build lets you build the
83:29 - containers as defined in your Docker
83:32 - compose yaml file
83:34 - if the file is located in another folder
83:37 - you can use the optional Dash F per
83:40 - meter and specify the files location
83:44 - start we'll start all the containers as
83:48 - defined in your yaml file
83:50 - step will stop them but they'll remain
83:54 - in memory
83:56 - up we'll do a build followed by a start
84:00 - this is super Endy
84:02 - use the Dash D parameter to run the
84:05 - command in the background and take back
84:07 - your terminal prompt
84:10 - PS will list what's running
84:14 - remove our RM we'll remove the
84:17 - containers currently from memory
84:20 - down we'll do a stop followed by a
84:25 - remove again this is super ND
84:29 - logs will display the logs for a
84:31 - container
84:34 - and you can open a session inside a
84:37 - container by running Docker compose exec
84:40 - the container name and the program to
84:42 - run
84:48 - the docker compose file is located
84:50 - inside a folder and if you run Docker
84:55 - compose up
84:56 - this will launch your application
85:00 - if you try to run a Docker compose up a
85:02 - second time from the same folder nothing
85:05 - will happen because the application is
85:08 - currently running
85:10 - if you want to run a second instance of
85:12 - your application it was impossible with
85:15 - Docker compose V1
85:18 - but with V2 you can use a project name
85:22 - to launch a second instance of your
85:25 - application from the same folder
85:28 - here's the cheat sheet for some of the
85:31 - new commands
85:32 - Docker compose dash dash project Dash
85:36 - name followed by a project name
85:39 - this will run a then sense of the
85:41 - application as a project
85:44 - the shortcut is much shorter to type
85:47 - Dash p instead of that Dash project Dash
85:50 - name
85:52 - you can list the project currently
85:54 - running by using compose LS
85:58 - CP will allow you to copy files from the
86:03 - containers so this is super ND to
86:05 - retrieve let's say log files
86:08 - and you can copy files to The Container
86:12 - so from your machine your desktop or
86:15 - laptop to The Container by using Docker
86:18 - compose CP The Source pad the container
86:21 - ID and the destination path
86:27 - here's an example imagine that the
86:29 - docker compost file is located in the
86:32 - same folder where you run these commands
86:36 - you simply use the up command to build
86:39 - and run the containers and to take them
86:42 - down simply use the down command
86:46 - and this concludes the structure on
86:49 - Docker compose
86:53 - [Music]
86:57 - in this lab we will deploy a Docker
87:00 - compose application
87:02 - let's take a look at our Docker
87:03 - compose.yaml file
87:06 - we have one section called services and
87:10 - under that section we Define two
87:14 - Services the first one is called Web
87:16 - Dash Fe F4 front end it's a python
87:21 - application and instead of using an
87:25 - image from Docker up will be building
87:28 - that image using the build parameter
87:32 - the dot means that the docker file is at
87:35 - the same level as the docker compose
87:37 - file so here it is
87:39 - it's a simple python application just
87:43 - one file app.pi and requirement.txt that
87:48 - we're copying on that base image
87:56 - it will be listening on Port 5000 and
87:59 - this is our second service it's the
88:02 - redis cache and this time we'll be using
88:05 - an image from Docker up
88:08 - all right let's open eight terminal
88:14 - and let's build the image using Docker
88:19 - compose build
88:26 - perfect my image was build now I can
88:29 - launch the application using Docker
88:31 - compose up and Dash D for detach now I
88:35 - could have skipped the build step
88:38 - because up does a build first and then a
88:44 - start so it's super ND
88:47 - so let's use Docker couples up
88:52 - and my application is up and running I
88:57 - can test it if I go to local OS 5000
89:02 - you visited me one time and do a few
89:06 - refreshes five time perfect
89:10 - okay I can list the running containers
89:13 - using Docker compose PS
89:19 - I can also use Docker PS since it will
89:23 - list the docker container currently
89:26 - running
89:30 - and I can look at the logs for my
89:34 - front-end service using Docker compose
89:37 - logs Dash F and the name of the service
89:44 - and if I
89:48 - move that a little bit
89:50 - and it F5 a few times you see
89:55 - new entries are logged perfect
89:59 - that works
90:01 - let's do a Ctrl C to terminate the log
90:06 - streaming
90:09 - and we'll use Docker compose LS to list
90:13 - the currently running projects
90:16 - I have one project running it's called
90:21 - l09-04 Docker compose well basically
90:24 - when I use Docker compost up I didn't
90:27 - specify a project name
90:29 - so Docker compose use the folder name as
90:33 - the project name
90:34 - now let's try to create a second
90:36 - instance of our application if we use
90:39 - Docker compose up Dash D again
90:46 - well Docker compose tell me that the
90:49 - application is running so it will not
90:52 - start a new version we can try to deploy
90:55 - our second version using a project name
90:58 - so Docker compose Dash p for project
91:01 - name
91:01 - we'll name it test up Dash D
91:06 - let's see what will happen
91:09 - starting oops we have an error here hmm
91:14 - bind for local OS 5000 failed Port is
91:19 - already allocated of course are my local
91:23 - old sport 5000 is in use right now
91:27 - so what I need to do
91:29 - I need to change that Port here the
91:32 - localhost port I'll use 5001 save the
91:36 - file
91:37 - and use the same command Docker compose
91:41 - Dash B project name test up Dash D
91:48 - and this time it worked awesome
91:52 - let's open our browser
91:54 - and let's go to Port 5001
91:59 - yes that worked
92:01 - a few refreshes 10 times let's go back
92:05 - to Port 5000 the first instance
92:08 - 15 times
92:10 - all right
92:16 - so let's do a Docker compose LS again
92:21 - to list our projects now we have two
92:23 - projects the first one that we deploy
92:26 - without specifying a project name and
92:28 - the second one with the project name
92:31 - test
92:32 - let's delete our for instance by using
92:35 - Docker compose down
92:38 - so I didn't specify a project name
92:40 - Docker compose use the folder name as
92:43 - the project name now if I do the docker
92:47 - composer list I should have only one
92:50 - project running yes it's test now I can
92:54 - delete that one using Docker compost
92:56 - specifying the project name and down
93:05 - let's list the projects again Docker
93:08 - composer less
93:11 - nothing running PS
93:14 - listening the containers nothing and why
93:18 - not try Docker PS
93:21 - nothing in memory
93:26 - [Music]
93:31 - in this lab we will deploy a Docker
93:34 - compose sample application
93:37 - that is composed of three services a web
93:40 - front-end build with react
93:43 - nodejs backend and the Maria DB database
93:48 - let's take a look at our Docker compose
93:51 - file
93:52 - this is a more complex than what we saw
93:55 - so far so let's try to break it into
93:58 - smaller pieces
94:01 - here we have the definition of our tree
94:03 - services if we start from the left we
94:05 - have the back-end service
94:08 - top right the database DB
94:12 - and the front end now if you look at the
94:15 - DB service you can see that we're
94:18 - referencing an image that will pull from
94:21 - Docker hub
94:23 - and the other two Services we're using
94:27 - the build parameter
94:29 - this means that we will build these two
94:32 - images
94:37 - looking at the backend service we can
94:39 - see that we specify build and context
94:44 - contacts with the value of backend
94:46 - backend is actually a subfolder where
94:51 - the docker file is located
94:56 - next we're defining two networks public
95:00 - and private
95:02 - we can see that our front-end service is
95:05 - using the public network
95:08 - the backend service is using public and
95:12 - private
95:13 - and the database service the DB service
95:16 - use only the private Network
95:21 - front end being in the public network
95:23 - cannot communicate directly with the DB
95:28 - service
95:29 - but backend being in both public and
95:33 - private can communicate with both DB and
95:38 - front-end services
95:42 - we're defining also two named volumes
95:45 - back in dash modules and DB Dash data
95:50 - and we can see highlighted in yellow
95:53 - that our DB service is using DB data and
95:57 - our backend service is using back-end
95:59 - modules
96:04 - highlighted in yellow we also see that
96:06 - we're using other volumes these volumes
96:10 - are scoped at the service level and are
96:13 - not shared between services
96:16 - to create an instance of our Docker
96:18 - composed application we simply use
96:20 - Docker couples app and to bring it down
96:22 - Docker compose down
96:26 - in Visual Studio code let's take a look
96:28 - at the docker compose file so it's
96:30 - called compose.yaml
96:33 - here we have our services section
96:36 - Network section where we Define two
96:39 - networks the name volumes back-end
96:44 - modules and DB data and secrets that we
96:47 - haven't seen yet where we're defining
96:49 - one key value pair so the key is DB
96:53 - password and we get the secret from a
96:56 - file the file is located in the folder
96:59 - called DB and it gets the value from a
97:03 - file called password.txt if we open the
97:07 - DB folder here's the file with the value
97:11 - the secret
97:13 - and that will be injected when we run
97:17 - Docker compose up
97:20 - okay let's go back to the services
97:23 - section and here we have our backend DB
97:27 - and front-end Services let's take a look
97:31 - at backend
97:32 - we're using the build a directive and we
97:36 - are setting the context to backend it
97:39 - points to this folder called backend
97:42 - here and the docker file is located
97:44 - inside that backend folder with the
97:48 - application
97:49 - same thing with the front-end service
97:53 - build context front-end and we're
97:57 - getting the docker file and the
97:59 - application from the front-end folder
98:04 - and what about the DB service well we're
98:08 - using an image that will pull from
98:10 - Docker hub
98:12 - okay let's build our two images
98:17 - we'll open a terminal
98:20 - and run Docker compose build
98:33 - our images were billed perfect now let's
98:37 - run the application by using Docker
98:39 - compose up Dash D
98:57 - ER perfect the application is this thing
98:59 - on Port 3000 so let's open a browser
99:04 - and type local OS 3000
99:09 - is our react application it's working
99:12 - awesome
99:15 - we can list the containers that are
99:19 - currently running by using Docker
99:21 - compose PS
99:23 - we should have three backend DB front
99:27 - end awesome
99:28 - let's take a look at the logs from the
99:31 - back-end service Docker compose logs
99:35 - Dash F backend
99:37 - and these are the logs for our backend
99:39 - service awesome
99:42 - we can type Ctrl C to stop the log
99:46 - streaming
99:47 - and we can take our application down by
99:51 - using Docker compose down
99:57 - this will stop and remove the containers
100:00 - from memory
100:02 - do we have something else in memory we
100:06 - should not
100:10 - perfect and even if I type doctor PS
100:15 - there's nothing however the volumes are
100:19 - still there when you do a doctor compose
100:21 - down it will remove the containers from
100:24 - memory but will not remove the volumes
100:27 - so if I open the docker desktop
100:30 - application and I click on volumes here
100:34 - I can see that I have a few well three
100:37 - volumes that were created a few minutes
100:39 - ago
100:41 - so I need to delete them manually I can
100:44 - select them right here and click on
100:47 - delete or I can do the same thing by
100:50 - clicking on the docker icon in vs code
100:53 - locating the volumes here
100:58 - and delete them
101:00 - find it easier to do that in Docker
101:04 - desktop because we see when the volumes
101:07 - were created so I'm pretty sure that
101:10 - these three are the ones that I need to
101:13 - delete I'll click on the delete button
101:16 - and confirm
101:20 - thank you
101:21 - [Music]
101:27 - let's take a look at some of the compose
101:30 - file features
101:33 - it's a good practice to set limits on
101:36 - the resources that your container will
101:38 - use
101:39 - in this example I light it in yellow we
101:42 - tell Docker to start the container with
101:45 - a quarter of a CPU and 20 megabytes of
101:48 - RAM
101:50 - the green section is the limits that we
101:53 - are allowing in this case half of a CPU
101:56 - and 150 megabytes of RAM
102:01 - to set an environment variable that will
102:04 - be injected in the running instance
102:07 - simply set the key value pair in the
102:10 - environment section
102:13 - those values can be overridden at the
102:16 - common line using the dash e per meter
102:21 - you can reference an environment
102:23 - variable using the daughter curly
102:27 - bracket syntax
102:28 - this way you can set the variable on
102:31 - your machine or server and use it
102:34 - directly in the compose file
102:38 - you can place the values in a file that
102:41 - you will name dot EnV located in the
102:46 - same folder as the compose file the
102:48 - compost command will automatically read
102:51 - the values from that file
102:55 - by default all containers specify in a
102:58 - compose file will see each other using
103:01 - their service names here we have two
103:04 - Services web and DB
103:09 - the code running in the web service can
103:12 - communicate with the second one using DB
103:16 - as the hostname and vice versa
103:23 - the web container is visible from
103:25 - outside of the docker network using the
103:29 - port number configured in the left
103:31 - portion of the ports value
103:34 - web is listening inside the docker
103:37 - Network on Port 80. DB can reach web on
103:41 - Port 80.
103:43 - finally DB only exposes one port number
103:48 - that's the internal port
103:51 - web can reach DB using Port 5432
103:56 - but DB is not visible from outside the
104:00 - docker Network
104:03 - if you have a compose application with
104:06 - multiple containers you can restrict who
104:09 - sees who by configuring networks in this
104:13 - example we're defining two Networks
104:16 - front end and back end
104:20 - proxy can see app because both are part
104:26 - of the front-end Network
104:29 - however proxy does not cdb because it's
104:34 - not part of the backend Network
104:40 - when using multiple containers you may
104:43 - want to start some of them first and
104:45 - wait until they are running before
104:47 - starting the other ones a typical use
104:50 - case is a database that you want to run
104:53 - before sorting the main application
104:58 - doing so is easy using the depends on
105:01 - parameter where you simply specify what
105:05 - is the service name that the service is
105:08 - dependent on in this example app depends
105:12 - on DB so compost will first start DB and
105:17 - when DB is running compose will then
105:20 - start app
105:24 - you can declare volumes in the volumes
105:26 - section
105:28 - these are called named volumes and they
105:31 - can be used by all these services that
105:34 - you are declaring in the compose file
105:37 - to use a volume from a service map it to
105:41 - a local folder using the volume name
105:44 - colon and the virtual path inside the
105:48 - container
105:50 - optionally you can make the mapping
105:52 - read-only by appending colon Ro to the
105:57 - mapping
106:00 - you can also create a mapping without
106:03 - using a named volume
106:06 - this mapping can't be shared across
106:08 - services
106:12 - it's also a good practice to set a
106:15 - restart policy
106:16 - let's say that you deploy your couple's
106:19 - app in a VM
106:20 - and at some point you need to install
106:23 - some OS batches and you need to restart
106:26 - or reboot the server
106:29 - what will happen to your compost app
106:32 - well if you don't specify a restart
106:35 - policy
106:36 - the one by default is no meaning that
106:39 - compose will not restart the containers
106:42 - if they were shut down by reboot
106:47 - you can set the policy to always this
106:50 - way compose restarts the containers
106:52 - until their removal
106:55 - on failure resource a container if the
106:58 - exact code indicates an error
107:01 - and lastly unless stop does a restart
107:05 - unless you stop or remove the containers
107:10 - and this concludes this look at some of
107:12 - the docker compose features
107:18 - [Music]
107:23 - let's now talk about container
107:25 - registries
107:26 - so what is the container registry it's a
107:30 - central repository for container images
107:34 - you build an image locally
107:36 - then you push and store the binary the
107:39 - different layers
107:41 - 2D Repository
107:43 - they can be private or public
107:47 - the default one is Docker hub
107:51 - Microsoft AWS and Google each offer
107:55 - container Registries as service
107:58 - the benefit of using a repository from
108:01 - your cloud provider is that the images
108:03 - are located near your app so no apps
108:08 - over the internet to retrieve the images
108:13 - so let's say we want to retrieve an
108:15 - image from Docker up
108:17 - we issue a Docker pull command and
108:20 - Docker downloads the images layers and
108:23 - store them in its local cache
108:27 - and this concludes this lecture on
108:29 - container registries
108:34 - [Music]
108:39 - let's see how to push and pull images to
108:42 - Docker up
108:45 - make sure you are logged in with your
108:49 - Docker user account to be sure simply
108:52 - type Docker login
108:54 - without a username and password
108:56 - Docker will tell you that you're already
108:59 - logged in if not enter your Docker
109:01 - username and password
109:04 - you need to tag an image with the
109:07 - repository name by default it's your
109:09 - username
109:11 - if you have created some organization in
109:13 - Docker hub
109:15 - prefix it to the name of the image
109:18 - in this example I want to push this
109:20 - image to my Kates Academy organization
109:24 - then use the push command and don't
109:28 - forget to specify the organization name
109:30 - it's part of the image name
109:34 - to retrieve the image we use the pull
109:36 - command with the image full name
109:42 - on Docker up public images are available
109:45 - for download to anyone
109:48 - if you don't want to share them you need
109:50 - to create a private Repository
109:54 - later on we will create one using our
109:57 - cloud provider
109:58 - and this concludes this lecture on
110:00 - Docker up
110:03 - [Music]
110:08 - let's now push our first image to Docker
110:11 - hub
110:12 - first thing first let's make sure that
110:15 - we can log in into Docker Hub so if you
110:18 - head to
110:20 - up.docker.com make sure you can log in
110:24 - also if I right click here on my Docker
110:28 - desktop icon I can see that I'm logged
110:31 - in
110:32 - perfect
110:36 - we will containerize a node.js Express
110:40 - application so we'll first add the
110:43 - docker file
110:44 - we'll use the tooling so view command
110:47 - palette
110:49 - if you can type Docker add and Docker
110:53 - files to workspace
110:57 - this is a node.js application
111:00 - the package.json file is located at the
111:03 - real so that's the correct one
111:05 - it's listening on power 3000 and we
111:09 - don't want Docker compose files
111:11 - perfect
111:13 - now we need to build the image so let me
111:16 - open a terminal
111:21 - perfect and we need to issue a Docker
111:24 - build command
111:25 - with the Dash D for tag parameter
111:30 - but notice here we need to prefix the uh
111:33 - the name of the image with our registry
111:36 - name
111:38 - if I select this
111:42 - I need to prefix that with
111:45 - my name here the name of my registry
111:51 - let me run that
112:06 - the image was built successfully now I
112:10 - need to use the push command to push
112:13 - that image onto Docker Hub again I'll
112:16 - select this
112:19 - and let's replace the registry name with
112:25 - mine
112:28 - use your home
112:32 - and see what's happening here Docker is
112:36 - pushing each layer to my Docker app
112:40 - account here
112:42 - all right
112:44 - no errors things went fine let's go back
112:48 - here I'm gonna refresh this page
112:52 - and here it is
112:54 - here's my
112:55 - um uh image here my Express image
112:58 - so I can click on this
113:02 - I can edit the
113:05 - um
113:05 - the information I can see that I have my
113:09 - tag is V1
113:11 - right I can get more information here I
113:15 - can here click on public view so by
113:18 - default the repositories on Docker Hub
113:21 - are public anyone can download and view
113:25 - and download your images this is the
113:28 - view for someone who would look at my
113:31 - image so with the pull command Docker
113:34 - pull
113:36 - let me go back here
113:39 - I can see the tags
113:42 - and so on and so on and if at some point
113:46 - you want to delete this you go into the
113:50 - settings tab here
113:53 - you scroll down and you can delete that
113:55 - repository here
113:59 - all right let's go back
114:03 - now let's try to pull that image to our
114:06 - computer here
114:08 - the first thing uh I will try to do is
114:12 - remove it from my computer
114:14 - let me
114:18 - type this RMI so remove image
114:22 - the image has gone completely so now
114:25 - let's try to pull it from uh from Docker
114:29 - uh
114:32 - okay
114:40 - Perfect Pull complete my image is is
114:44 - back here
114:46 - now
114:48 - you see that I use the V1 tag here to to
114:53 - tag my image with the with the version
114:55 - number so let's try to build a version 2
114:59 - of that image let me copy that
115:03 - and again let's replace this part this
115:07 - placeholder with my registry name
115:23 - image has been built so let's now push
115:26 - it
115:27 - two Docker uh
115:32 - same thing as we did before but this
115:37 - time we're pushing version two
115:45 - okay
115:47 - let's go back here let's go back to my
115:52 - Express or the general tab
115:56 - look here I have V1 and V2
116:01 - if I click here on tags
116:04 - I can see when the image was was pushed
116:09 - tags and people can download V1 or V2
116:20 - if I want to remove my images
116:23 - I use the RMI
116:25 - command so remove image I'll remove V1
116:29 - and also I remove V2
116:32 - and on Docker up if I no longer need
116:37 - this Repository
116:39 - I click here settings scroll down a
116:43 - little bit
116:44 - click on delete Repository
116:47 - and I need to enter the name of my repo
116:51 - just to make sure and click on delete
116:55 - and now it's gone
116:58 - [Music]
117:03 - time to introduce kubernetes
117:07 - so kubernetes or also known as Kates so
117:11 - the letter K followed by the number
117:14 - eight so eight letters
117:15 - and then the number s
117:17 - and it's pronounced Kate's so kubernetes
117:21 - is a project that was created at Google
117:25 - version one came in July 2015.
117:29 - it was the third generation of container
117:31 - scheduler from Google
117:33 - previous projects were Borg and Omega
117:37 - and Google donated the kubernetes to the
117:41 - cncf so now the development is
117:44 - supervised by the cncf
117:47 - is currently the leading container
117:50 - orchestration tool
117:52 - it's designed as a Loosely coupled
117:55 - collection of components for deploying
117:57 - managing and scaling containers
118:01 - it's vendor neutral so it's not attached
118:04 - to a single company and it runs on all
118:07 - Club providers
118:09 - and there's a huge community ecosystem
118:12 - around kubernetes
118:15 - so what kubernetes can do
118:19 - service Discovery load balancing
118:22 - it can bridge to the cloud providers
118:25 - storage services
118:27 - can provide rollout rollbacks
118:30 - capabilities
118:33 - and can monitor the health of the
118:35 - containers
118:36 - can manage configuration and secrets
118:40 - and the same API is available either in
118:45 - a on-premising solution or in every
118:48 - cloud provider
118:50 - so what can't kubernetes do
118:54 - it can deploy or build your code
118:58 - and it does not provide application
119:00 - Level services like databases service
119:03 - buses caches
119:07 - here's a quick look at the kubernetes
119:09 - architecture
119:10 - this diagram was taken from the
119:13 - kubernetes documentation
119:15 - we'll take a closer look at each
119:17 - component but for now let's just say
119:19 - that it's composed of a master node also
119:22 - called the control plane so that's the
119:25 - portion to the left
119:28 - and the control plane runs the
119:31 - kubernetes services and controllers
119:36 - and you have the worker nodes these node
119:39 - runs the containers that you'll deploy
119:41 - in the cluster
119:44 - so a container will run in a pod a pod
119:48 - runs in a node and all the nodes form a
119:52 - cluster
119:55 - and this concludes this intro to
119:58 - kubernetes
120:01 - [Music]
120:06 - let's see how you can run kubernetes
120:08 - locally
120:10 - so do you need to install a kubernetes
120:12 - cluster in the cloud or ask your it
120:14 - Department to install one in your
120:16 - Enterprise so you can test locally
120:19 - absolutely not there are many ways that
120:22 - you can run kubernetes on a desktop or
120:25 - laptop
120:27 - Docker desktop lets you run kubernetes
120:30 - macrocates from the makers of Ubuntu and
120:34 - minicube also let you run kubernetes
120:37 - altree requires that virtualization is
120:40 - enabled
120:42 - kind runs over Docker desktop and offer
120:46 - extra functionalities
120:49 - Docker desktop is limited to OneNote but
120:53 - it's usually not a problem
120:55 - Marco Cates kind and minicube can
120:59 - emulate multiple worker nodes
121:03 - on Windows Docker desktop lets you run
121:06 - both Linux and windows containers
121:10 - and you can't create and run Windows
121:12 - containers on Mac and Linux
121:16 - it runs on hyper-v or Windows subsystem
121:20 - for Linux
121:21 - so if you have Windows 10 version 2004
121:25 - or later
121:26 - it's the recommended way to run Docker
121:29 - desktop
121:32 - if fiber-v is enabled on your laptop or
121:35 - desktop you can't run another hypervisor
121:38 - at the same time
121:40 - and mini Cube used by default virtual
121:43 - box but it can also run on hyper-v
121:49 - you can install Docker desktop on
121:51 - Windows using App review and it will
121:54 - create a virtual machine named Docker
121:56 - desktop VM
121:58 - when you take the enable kubernetes
122:01 - checkbox in Docker desktop it will
122:05 - download additional containers to run
122:07 - kubernetes
122:10 - using Windows 10 version 2004 or later
122:13 - and if you have wsl2 installed you can
122:18 - tick the use the wsl2 base engine and
122:21 - Docker desktop will create its VM inside
122:24 - the Linux distro you install on your
122:27 - Windows machine
122:29 - note that this is the recommended way to
122:32 - run Docker desktop
122:35 - on Mac Docker desktop use the hyperkit
122:39 - lightweight hypervisor to run its VM
122:44 - mini cube is another popular option
122:48 - it does not require a Docker desktop it
122:51 - runs on Linux Mac in Windows and it
122:54 - requires an appervisor like a virtualbox
122:57 - here we can see mini Cube running on a
123:01 - Mac and its virtual machine in
123:04 - virtualbox
123:09 - if you need to install mini Cube on
123:11 - Windows but don't want to install
123:12 - virtualbox you can run minicube on
123:15 - hyper-v
123:17 - you need to create a network switch and
123:19 - start minicube with some extra
123:21 - parameters
123:25 - kind stands for kubernetes in Docker
123:29 - because it runs on top of Docker desktop
123:33 - kind lets you emulate multiple control
123:36 - planes and multiple worker nodes
123:39 - this is useful if you need to test node
123:42 - affinity
123:45 - and this concludes the sector on how to
123:47 - run kubernetes locally
123:51 - [Music]
123:56 - and this will be a super quick lab just
123:59 - to validate that our kubernetes
124:01 - installation is working locally
124:03 - so here I'm on Windows and I'm using
124:07 - Docker desktop and I installed
124:09 - kubernetes with Docker desktop
124:12 - in the system tray I can right click on
124:15 - the docker desktop icon on a Mac you can
124:18 - do that from the top of the the screen
124:21 - and I will select dashboard
124:27 - and from there we're going to click here
124:29 - on the gear icon the settings icon
124:34 - and I'm using wsl2 to run Docker desktop
124:38 - so that's the recommended way
124:40 - and here if I click on kubernetes
124:43 - uh enable kubernetes is checks so
124:45 - kubernetes is installed but is it
124:49 - running correctly let's find out let me
124:53 - go back here let's open a terminal
125:00 - and let's run uh this command uh Cube
125:03 - CTL cluster info
125:06 - that should give us a little information
125:08 - about what's running so kubernetes
125:10 - Master it as in green yay it's working
125:13 - so it's running at this address and Cube
125:16 - DNS is running at this address
125:18 - so by running qctl cluster info you get
125:22 - some information about the kubernetes
125:24 - installation and itself
125:29 - [Music]
125:34 - let's see how you can use the kubernetes
125:36 - CLI
125:39 - the kubernetes API server is a service
125:42 - running on the master node
125:45 - it exposes a rest API that is the only
125:49 - point of communication for kubernetes
125:52 - clusters
125:54 - you define the desired state in yaml
125:57 - files
125:58 - and let's say you want to run a x number
126:02 - of instances of a container in the
126:04 - cluster
126:06 - using the kubernetes CLI you then send
126:10 - that desired state to the cluster via
126:13 - the rest API
126:16 - other applications like a web dashboard
126:19 - can also communicate with the rest API
126:21 - to display the cluster state
126:26 - Cube CTL is the kubernetes CLI and it
126:29 - runs on Mac Linux and windows
126:33 - and you pick your choice of
126:34 - pronunciation cue control Cube cuddle
126:38 - Cube CDL doesn't matter
126:42 - it communicates with the API server
126:45 - and its connection information is stored
126:48 - in a config file under the dot Cube
126:52 - folder
126:56 - let's now see what a context is
127:00 - it's a group of access parameters that
127:03 - let you connect to a kubernetes cluster
127:06 - it contains the cluster name a user and
127:10 - a namespace
127:12 - the current context is the cluster that
127:16 - kubernetes commands will run against
127:19 - let's say that you can connect to three
127:22 - clusters cluster a cluster B and cluster
127:25 - C
127:27 - when you set the default context to
127:29 - Cluster B
127:30 - then all the cube CTL commands that you
127:34 - you will run will run against cluster B
127:40 - here's a cheat sheet for context
127:43 - commands
127:46 - Cube CDL config current context will get
127:50 - you the current context
127:53 - get context will list all of them
127:58 - Cube CL config use contacts and the
128:02 - context name Will Set the current
128:05 - contacts
128:08 - and delete contacts with the context
128:11 - name we'll delete the context from the
128:14 - config file
128:19 - there's a large ecosystem of free open
128:21 - source kubernetes tools that you can use
128:24 - Cube CTX is a good example
128:28 - it's a shortcut for the Cube's DL config
128:31 - use context command
128:33 - you simply type Cube CTX followed by the
128:37 - context name to quickly switch contacts
128:41 - it runs on Windows Mac and Linux
128:44 - very useful
128:47 - and this concludes the structure on the
128:49 - kubernetes CLI and the concept of
128:51 - context
128:55 - [Music]
129:00 - a context contains connection formation
129:03 - to a kubernetes cluster and you can have
129:06 - one or more than one context set on your
129:10 - machine it's super important to know how
129:13 - to figure out in which context you're
129:15 - currently in and how to change context
129:18 - so first thing first let's figure out in
129:20 - what context we're currently in I'm
129:22 - going to use cubectl config current
129:25 - context
129:28 - and this will print the name of the
129:31 - context we're currently in I'm currently
129:33 - in the docker desktop contact means that
129:36 - whenever I type Cube CTL commands it'll
129:38 - be applied to that kubernetes cluster
129:42 - all right I can have more than one
129:45 - context configure on my machine or on
129:48 - any machine or server to list them we
129:51 - use Cube CTL config get Dash context
129:56 - and here we can see that I have two
129:58 - contacts set on my machine Docker
130:01 - desktop which is the current context
130:03 - because it has that star in the current
130:06 - column and I have a second one called
130:09 - demo then one is a cluster that I
130:11 - created in the cloud
130:13 - what if I want to change from Docker
130:17 - desktop to demo
130:19 - okay let's use cubectl config use Dash
130:24 - context and the context name
130:29 - demo
130:32 - and now if
130:34 - I print again the current context
130:38 - there it is demo so whenever I'll be
130:41 - typing Cube CTL command
130:45 - DZ yaml files this command will be sent
130:48 - to My Demo cluster somewhere in the
130:52 - cloud
130:54 - a cool tool is Cube CTX because it
130:58 - allows you to well instead of using Cube
131:02 - CDL config blah blah blah blah blah
131:05 - basically it's a shortcut
131:08 - if I simply type a cube CTX
131:12 - that will print the context that I've
131:14 - configured on my machine so demo is
131:16 - green means that it's the current
131:18 - context and I can quickly change context
131:23 - using cubectx and the context name
131:28 - Docker desktop you know a little bit
131:31 - less keystroke to talk about anyway it's
131:34 - a fun too
131:36 - foreign
131:39 - context let's say uh you you your
131:42 - cluster has a long funky name and you
131:44 - want to rename it to make uh more sense
131:47 - you can use the cube CTL config rename
131:50 - contacts all name new name so let's try
131:53 - to rename our demo cluster here
131:57 - demo let's say
132:01 - it's on Azure so Azure demo
132:06 - okay let's use the cube CTX to print the
132:10 - contacts and there it is azure demo
132:15 - where is that context information stored
132:18 - it's stored locally on your machine so
132:21 - I'm on Windows let's see where it is
132:25 - it's on the C drive
132:28 - users
132:29 - your username
132:34 - under dot Cube deduct Cube folder
132:40 - and there it is config so if I right
132:43 - click on it
132:44 - and select opening with code
132:49 - we can see that it's a yaml file
132:54 - and it contains two entries here two
132:56 - clusters
132:59 - one is the Azure one and the second one
133:02 - is the
133:04 - Docker desktop one
133:06 - and this is the different context that I
133:10 - have that have so here is my Docker
133:13 - desktop context
133:15 - and here is my demo context
133:19 - well the cluster name is still demo but
133:22 - I renamed the context name to Azure Dash
133:27 - demo
133:28 - there it is
133:30 - all right
133:32 - let's go back here
133:35 - and let's say that I've deleted my
133:39 - cluster in the cloud I don't need it
133:41 - anymore
133:43 - I want to get rid of the contacts
133:46 - information I can use cubectl config
133:49 - delete contacts and the context name
133:52 - so let's try this
133:57 - Azure demo
134:00 - deleted context Azure demo from the
134:04 - config file so let's take a look here
134:08 - and you see that now I have only one
134:12 - contacts
134:14 - and here's the current context
134:17 - in this line here
134:19 - but let take a look here at the Clusters
134:24 - list
134:25 - My Demo cluster is still there
134:28 - right so it's not deleted automatically
134:31 - what you can do is simply edit that
134:34 - config file and remove the section no
134:37 - longer needed
134:40 - [Music]
134:45 - let's talk about the declarative and the
134:47 - imperative ways to create resources in
134:50 - kubernetes
134:52 - there are two ways that you can use when
134:54 - you want to create resources in
134:55 - kubernetes
134:57 - the declarative way and the imperative
135:01 - way
135:02 - using the imperative way you use cubectl
135:07 - to issue a series of command to create
135:09 - resources in the cluster
135:11 - this is great for learning testing and
135:14 - troubleshooting
135:16 - using the declarative way you define the
135:19 - resources needed in yaml files and use
135:23 - cubectl to send the content of these
135:27 - file as the desired state to the cluster
135:31 - so instead of a series of command
135:35 - this is reproducible and you can even
135:38 - store these yaml files in a source
135:41 - control system
135:43 - here we can see a series of command to
135:45 - create resources that's the imperative
135:48 - way
135:49 - you can create a pod using the Run
135:51 - command create a deployment or service
135:55 - using the create command
135:58 - using the declarative way
136:01 - you would use a yaml file to define the
136:04 - resource and then send the content of
136:06 - that file to the cluster to create these
136:09 - resources
136:10 - so what's a yaml file
136:14 - well it's a text file that contains
136:17 - properties that Define the resource
136:19 - it has some required properties like the
136:22 - API version
136:23 - the object kind that defines the type of
136:26 - object you want to create
136:28 - we'll take a look at these later on
136:32 - you can use the cube CTL create command
136:34 - to send the information to the
136:36 - kubernetes cluster
136:38 - we will take a deeper look at yaml files
136:41 - in the future lectures but right now you
136:44 - may be wondering
136:45 - do you need to type all that yaml
136:48 - manually
136:49 - the answer of course is no
136:53 - one way to get the correct syntax is to
136:56 - copy one from the official kubernetes
136:58 - documentation at kubernetes at IO slash
137:02 - Docs
137:03 - you then search for the object you want
137:06 - to create
137:07 - and click on the copy icon
137:11 - another way is to use templates offered
137:14 - with an editor like Visual Studio code
137:18 - let's say you create a new yaml file
137:21 - then you type control space and select
137:25 - the template to generate the Manifest
137:28 - that you can edit neat
137:32 - you can also use the kubernetes CLI to
137:36 - generate the ammo
137:38 - add dash dash dry run equals client and
137:43 - dash o for output
137:45 - specifying the yaml to Output the yaml
137:49 - to the console
137:52 - you can even send the output to a file
137:54 - using the greater than sign and a file
137:58 - name
138:01 - and this concludes this overview of the
138:04 - imperative and declarative ways to
138:06 - create resources in kubernetes
138:10 - [Music]
138:15 - let's deploy an nginx container using
138:18 - both the imperative way and the
138:20 - declarative way
138:22 - using the imperative way uh we're gonna
138:25 - type command called cubectl create I'm
138:28 - going to create a deployment I'm going
138:30 - to name our deployment my nginx1
138:33 - I'm going to specify a parameter called
138:36 - image where we will specify the image we
138:39 - want to run
138:40 - this will create a deployment
138:45 - and we can Cube CTL get a list of the
138:51 - deployments using Cube CTL get deploy
138:54 - and there it is
138:56 - now the second way is the declarative
138:58 - way
138:59 - instead of typing a command with all the
139:02 - parameters at the common line we're
139:05 - going to specify a yaml file where all
139:08 - the configuration options are stored
139:11 - cubectl create Dash f for file and the
139:15 - name of the file
139:17 - so let's run this
139:22 - okay
139:24 - it was created let's again type Cube CTL
139:28 - get deploy
139:29 - get both our deployments and we if we
139:32 - take a look at the yaml file well it's a
139:35 - yaml file the type deployment it has the
139:39 - my nginx2 name it has a bunch of
139:42 - parameters that will come back
139:44 - to that a little bit later but basically
139:47 - all the configuration parameters are
139:51 - stored in that yaml file what's cool
139:53 - with that concept is that it's you can
139:55 - put these files quite easily in a source
139:58 - control system
140:00 - all right uh let's do a little bit of a
140:03 - cleanup let's delete our deployment Cube
140:06 - CTL delete deployment my nginx one
140:13 - okay and I'm gonna use a the same
140:16 - command but using a shortcut this time
140:19 - Cube CTL delete
140:21 - instead of deployment
140:23 - and the name of our deployment
140:28 - okay and if I type again Cube CTL get
140:32 - deploy
140:34 - there's no deployments currently my
140:36 - namespace
140:39 - [Music]
140:44 - let's take a look at namespaces
140:48 - so what is a namespace
140:51 - it's a kubernetes resource that allow
140:53 - you to group other resources
140:56 - let's say you need to deploy your
140:57 - application in multiple environments
141:00 - like Dev test and prod
141:03 - well you can create a namespace for each
141:05 - of these environments
141:08 - they are like logical folders in which
141:11 - you group resources
141:13 - kubernetes creates the default namespace
141:15 - called well default
141:18 - objects in one namespace can access
141:21 - objects in different in a different one
141:23 - the kubernetes internal DNS assigned
141:26 - Network names to some resources
141:29 - deleting a namespace will delete all
141:32 - these child objects
141:34 - this is super useful when doing tests
141:37 - create a namespace in the morning
141:40 - create resources under that namespace
141:42 - during the day
141:44 - and at the end of the day simply delete
141:46 - that namespace
141:48 - this command lists all the namespaces
141:50 - existing in a cluster we'll take a look
141:53 - at this command in more detail
141:58 - okay
142:00 - first you create a namespace this yaml
142:03 - file Define a namespace called prod
142:08 - then you use that namespace when you
142:11 - create other resources
142:13 - in the metadata section you set the
142:16 - namespace key to the name of the
142:19 - namespace you want this resource to be
142:21 - created in
142:23 - so namespace colon prod
142:27 - you can assign Network policies and
142:30 - limit the resources that you can create
142:32 - in a namespace using the resource quota
142:35 - object
142:39 - here's a cheat sheet for the namespace
142:41 - commands
142:44 - so cubesatel get namespace list all the
142:48 - namespaces
142:49 - and if you don't want to type namespace
142:53 - each time you can use a shortcut an S so
142:57 - Cube CTL get an S is the same thing as
143:00 - cubectl get namespace
143:03 - you can set the current context to use a
143:06 - namespace in the next commands that
143:10 - you'll type by using cubectl config set
143:14 - contacts
143:15 - current with which will use the current
143:18 - context and then the namespace is called
143:21 - the name space that you want to use so
143:24 - the next command that you'll type will
143:27 - be under that namespace
143:32 - cubectl create NS and the name of the
143:34 - namespace so you create a namespace you
143:37 - delete it using cubectl delete in s and
143:40 - the namespace name
143:42 - and you can also list all the pods or
143:46 - the objects from another namespace so
143:49 - Cube CTL get parts or any objects and
143:53 - then you pass the flag dash dash all-
143:56 - namespaces that will list all the
143:59 - objects in all the different things
144:01 - spaces
144:05 - and this concludes this look at
144:07 - namespaces
144:10 - [Music]
144:15 - let's see how to list and switch between
144:19 - name spaces
144:23 - you can get a list of the namespaces
144:25 - using the cube CTL get namespaces
144:28 - command
144:30 - let me copy this
144:33 - and these are the namespaces currently
144:37 - created on my machine on my kubernetes
144:40 - cluster so default the cube node Cube
144:44 - public Cube system
144:45 - I can also use the shortcut for
144:49 - namespaces which is just few letters
144:52 - which is a lot faster to type
144:54 - and it will give you the exact same
144:56 - result here
145:00 - let me get a list of the pods running
145:05 - Cube CTL get parts
145:09 - no resources found in default namespace
145:15 - that I'm currently my contacts is using
145:19 - the namespace called default right and
145:23 - there's no pods running in there
145:26 - so what if I want to list the pods that
145:29 - are in the cube system namespace
145:33 - well I can use the same command qctl get
145:35 - pass specifying the namespace switch and
145:39 - specifying the name of the namespace
145:44 - let me copy this
145:48 - and there are many parts running here
145:51 - here I can use also the the shortcut so
145:54 - instead of typing dash dash namespace
145:56 - equal I can just use the shortcut dash n
146:01 - so only one dash here two dashes here
146:04 - and the name of the namespace so let's
146:07 - try this
146:09 - and it works
146:12 - perfect
146:13 - now what if I want to change from the
146:18 - default namespace or the namespace
146:20 - called default to a namespace called
146:23 - Cube system right and then apply all my
146:26 - commands to so all the objects will be
146:29 - created in that namespace well I can do
146:31 - that by using the cube CDL config set
146:35 - contacts dash dash current so here we're
146:39 - modifying the context
146:42 - and passing a switch called namespace
146:45 - and the name of the namespace we want to
146:48 - change
146:49 - so we want to change instead of being
146:52 - the default namespace all the time we
146:54 - want to be in the cube system
146:58 - okay and yeah let's see if uh if we run
147:04 - Cube City I'll get passed if we would
147:06 - get some pods of course because we're
147:10 - currently in that Cube system namespace
147:18 - okay now let's change back to the
147:21 - default namespace
147:26 - and let's get a list of the parts there
147:30 - should be none
147:31 - perfect
147:33 - okay so this is how you uh you change
147:36 - namespaces
147:39 - perfect
147:41 - of course we can create new namespaces
147:44 - by using the cube CTL
147:47 - uh create NS
147:50 - for short for namespace and a namespace
147:52 - name so hello
147:55 - okay uh Cube
147:59 - CTL get NS and my name space here was
148:05 - created seven seconds ago
148:08 - and I can delete a namespace using
148:10 - cubectl delete NS and the namespace name
148:16 - that and hello now
148:19 - a bit of a warning here uh if you have
148:22 - resources under that namespace these
148:26 - resources the buzz the containers
148:28 - whatever running will also be deleted so
148:31 - use this command with caution
148:34 - foreign
148:37 - take a couple of seconds
148:40 - okay let's get the name spaces and our
148:44 - hello namespace is gone
148:48 - [Music]
148:54 - let's now look at the masternode
148:58 - in nowadays the physical or virtual
149:00 - machine
149:01 - a group of nodes formed a cluster
149:05 - the masternode is also called the
149:08 - control plane
149:10 - the kubernetes services and controller
149:13 - are located on the control plane
149:16 - they are also called The Master
149:18 - components
149:20 - and you usually don't run your
149:22 - application containers on the masternode
149:28 - the key value data Store where the state
149:31 - of the cluster is stored
149:33 - the API server is the only component
149:36 - that communicates with its CD
149:41 - let's start with the API server
149:44 - it exposes a res interface and client
149:48 - tools like the kubernetes CLI
149:50 - communicates through that recipe
149:54 - it saves the state in a CD
149:57 - and all client interacts with the API
150:00 - server never directly with the data
150:03 - store
150:06 - hcd is the data store for storing the
150:10 - cluster state
150:11 - it's not a database but a key Value
150:14 - Store
150:16 - is the single source of truths inside
150:19 - kubernetes
150:23 - the cube control manager is the
150:25 - controller of controllers
150:28 - its job is to run the other kubernetes
150:30 - controllers
150:34 - the cloud control manager job is to
150:37 - interact with the cloud providers
150:40 - a check if nodes were created or deleted
150:44 - uh Route traffic create or delete load
150:48 - balancers
150:49 - and interact with the cloud providers
150:51 - storage services
150:55 - the cube scheduler watches for paths
150:58 - that are not created yet
151:00 - and selects a node for them to run on
151:04 - it checks for various rules and then
151:07 - assign the Pod creation to a node
151:12 - finally you can install various add-ons
151:15 - on the masternode
151:17 - these add-ons provide additional
151:19 - functionalities in your kubernetes
151:22 - cluster
151:24 - and this concludes this look at the
151:26 - masternode
151:30 - [Music]
151:35 - let's take a look at the worker nodes
151:38 - a node is a physical or visual machine
151:42 - a group of nodes forms a cluster
151:46 - there's a special node called the
151:48 - masternode sometimes called the control
151:51 - plane
151:52 - where the kubernetes services are
151:54 - installed
151:56 - the nodes running the containers are
151:59 - called the worker nodes
152:02 - when a worker node is added to the
152:04 - cluster
152:05 - some kubernetes services are installed
152:08 - automatically
152:09 - the container runtime
152:11 - the cubelet and the Q proxy
152:15 - these are Services necessary to run pods
152:18 - and they are managed by the master
152:21 - components on the masternode
152:25 - the cubelet manages the pod's life cycle
152:29 - and ensure that the containers described
152:31 - in the Pod specifications are running
152:33 - and are healthy
152:36 - the Q proxy is a network proxy that
152:40 - manages Network rules on nodes
152:43 - all Network traffic go through the cube
152:46 - proxy
152:48 - on each node you will find a container
152:51 - runtime
152:53 - kubernetes supports several container
152:55 - runtimes that implements the kubernetes
152:57 - container runtime interface
152:59 - specification or CRI
153:04 - one thing to note is that for kubernetes
153:07 - version previous to 1.19
153:11 - the Mobi container runtime was installed
153:14 - and was receiving the container runtime
153:16 - interface call through a shim
153:19 - because it did not fully implemented the
153:23 - specifications
153:25 - this is not ideal as it added an extra
153:29 - step
153:30 - starting with kubernetes version 1.19
153:33 - Mobi is no longer installed
153:36 - oh wait a minute
153:38 - Moby not installed that means that my
153:42 - Docker containers will no longer run if
153:45 - the docker container runtime is not
153:46 - installed right
153:48 - well the short answer is that your
153:51 - Docker images will run as is nothing to
153:55 - change its business as usual
153:59 - what change is that what you can do
154:01 - inside the cluster
154:04 - since the docker runtime is no longer
154:07 - installed
154:08 - you no longer can access the docker
154:11 - engine and issue Docker commands
154:13 - directly inside a node
154:17 - you'll have to use another tool called
154:19 - crease CTL but again that's if you SSH
154:23 - into a node and run commands directly on
154:27 - that note
154:28 - something that you don't do usually
154:33 - alright
154:34 - a node pool is a group of virtual
154:37 - machines all with the same size
154:41 - a cluster can have many node pools and
154:44 - each node pool can host virtual Machines
154:47 - of different sizes
154:50 - let's say that we have two node pools in
154:52 - our cluster the first one consists of VM
154:55 - without gpus and the second one with
154:58 - chip use
155:00 - remember that Docker desktop is limited
155:03 - to one node so basically you run the
155:05 - master components and all the
155:07 - application containers on the same node
155:12 - and this concludes this look at the
155:14 - worker nodes
155:18 - [Music]
155:22 - so let's get some information about our
155:25 - nodes I'm going to run a cube CTL get
155:28 - nodes
155:31 - and since I'm running on Docker desktop
155:34 - locally here
155:35 - I have only one node name is Docker
155:40 - desktop status is ready roll is master
155:44 - here's the revision the version number
155:47 - and it was installed 72 days ago all
155:51 - right
155:52 - I can get more information about the uh
155:55 - the node uh since I only have one node I
156:00 - can skip the node name per meter
156:05 - and here's a bunch of information
156:09 - the name the row some labels
156:13 - creation date
156:17 - um what else what else what else what
156:19 - else uh the capacity to CPUs
156:23 - uh number of PODS maximum number of
156:25 - parts that I can run 110
156:29 - the architecture the OS Linux
156:33 - architecture AMD
156:37 - the pods running and the CPU request CPU
156:42 - the Miss memory so all useful
156:45 - information when uh you need to
156:47 - troubleshoot and debug
156:50 - so that's pretty uh pretty interesting
156:52 - now I'm using a locally Docker desktop
156:56 - so I'm limited to one node Let's uh
157:00 - switch should
157:03 - to
157:04 - um My Demo cluster
157:08 - uh running in the cloud
157:12 - so let's do the same thing Cube CTL get
157:15 - notes
157:19 - I have more notes now so I have a
157:23 - cluster in in the cloud running three
157:27 - nodes in a node tool so one two three
157:31 - status ready agent and the version of
157:35 - kubernetes that is running that's
157:37 - interesting let me grab cubicity of the
157:41 - scrap node and let's uh let's say use
157:45 - this one
157:50 - so I can get
157:52 - the same information that I I was
157:55 - getting earlier but this one is running
157:58 - uh in the cloud
158:00 - that's the name
158:03 - role agent agent pool the different
158:06 - labels
158:08 - annotation
158:13 - number CPUs memory
158:18 - System Info it's running Ubuntu Linux
158:23 - AMD 64.
158:26 - uh what else will end the different
158:28 - parts
158:30 - that are running on this uh node
158:37 - [Music]
158:42 - let's take a look at parts
158:44 - so what are pots
158:47 - a pod is the smallest unit of work in
158:50 - kubernetes
158:51 - it encapsulate an application container
158:54 - and represent a unit of deployment
158:58 - paths can run one or more containers
159:02 - inside a pod containers shame the same
159:05 - IP address space and volumes
159:08 - and they communicate with each other
159:10 - using local laws that's inside the pod
159:15 - s are ephemeral deploying a pod is in
159:18 - atomic operation so it succeeds or not
159:23 - if a pod crashes it is replaced by a
159:27 - brand new one with a shiny new IP
159:29 - address
159:31 - you don't update the Pod that is
159:33 - currently running
159:34 - you create an updated version deleting
159:37 - the first one and deploying the new one
159:42 - you scale by adding more pods not more
159:46 - containers inside a pod
159:52 - I use this analogy in a previous lecture
159:55 - pods are like cattle there are ephemeras
159:58 - and you just replace them
160:02 - a note can run many pods
160:05 - and a part can run one or more
160:08 - containers
160:11 - if a pod runs multiple containers
160:14 - there's usually one that is the main
160:16 - worker where your application logic is
160:19 - located
160:21 - and the other ones are helper containers
160:23 - that provide services to the main worker
160:26 - we will come back to this concept in
160:29 - another lecture
160:32 - and this concludes this look at pods
160:37 - [Music]
160:42 - let's take a look at the Pod life cycle
160:45 - we'll start with the creation life cycle
160:48 - so when you issue a cube CTL create
160:51 - command to deploy a pod in your cluster
160:55 - the CLI sends the information to the API
160:58 - server
160:59 - and that information will be written
161:01 - into a CD
161:05 - the scheduler will wash for this type of
161:08 - the information
161:09 - look at the notes and find one where to
161:13 - schedule the Pod and write that
161:15 - information of course in a CD
161:18 - the cubelet running on the Node will
161:20 - watch for that information
161:23 - and issue a command to create an
161:25 - instance of the container inside a pod
161:28 - and finally the status will be written
161:31 - in a CD
161:34 - one thing to notice is that each time an
161:37 - operation is taking place inside the
161:39 - cluster the state is written in its CD
161:42 - so SCD is the single source of Truth in
161:46 - the cluster
161:48 - all right so let's take a look at the
161:51 - deletion life cycle now
161:54 - when you issue a cube City delete
161:56 - command to delete a pod from your
161:59 - cluster the CLI sends deformation of
162:02 - course to the API server
162:05 - that information will be written in SCD
162:10 - and notice that the grace period of 30
162:13 - seconds will be will be added
162:18 - so the cubelet picks that information
162:20 - and sends a terminate signal to the
162:23 - container
162:24 - if the container hangs it is killed
162:27 - after the 32nd grace period
162:30 - and finally the state is stored in a CD
162:38 - The Path State will give you a high
162:40 - level summary of Where the Pod is in its
162:43 - life cycle
162:46 - pending mean that the Pod is scheduled
162:48 - for creation
162:50 - but is not created yet
162:52 - if you run out of resources in your
162:55 - cluster kubernetes may not be able to
162:58 - create new paths and if this happens
163:01 - the parts will be in the pending state
163:07 - running means that the part is currently
163:09 - running
163:11 - succeeded means that the code exited
163:14 - without any errors
163:17 - fail means that the code inside the the
163:20 - Pod exited with the non-zero status
163:23 - so some kind of error occurred
163:27 - unknown means that kubernetes can
163:29 - communicate with the pod
163:32 - and finally crash loop back off oh I
163:36 - love this in his state name so krashu
163:40 - backup means that the Pod started then
163:43 - crash kubernetes started it again and
163:47 - then the Pod crashed again
163:48 - so kubernetes say okay hold on I'm
163:51 - stopping here
163:53 - so we'll take a look uh in the official
163:55 - letter of where to look for these states
164:01 - and this concludes this look at the bud
164:03 - life cycles
164:07 - [Music]
164:12 - let's see how to define and run pods
164:16 - to define a pod the declarative way you
164:19 - create a yaml file
164:22 - specifying but as the kind
164:25 - that's the type of resource you want to
164:27 - create
164:30 - you specify an image location in this
164:33 - case the nginx image will be pulled from
164:35 - Docker up that's the default container
164:38 - registry
164:40 - you set the port that the container will
164:43 - listen on
164:45 - you can add labels they are used to
164:48 - identify describe and group related sets
164:52 - of objects and resources
164:55 - you can set environment variables
164:58 - directly here
164:59 - might not be the best idea to place
165:01 - configuration values
165:04 - in a later lecture we'll see how we can
165:07 - externalize that by the use of a config
165:10 - map object
165:14 - you can even specify a command to run
165:17 - when the container starts
165:23 - if you have created a yaml file with
165:26 - your pod definition
165:28 - you use
165:30 - cubectlcreate-f specifying the yaml file
165:34 - location and name and this will create
165:37 - your pod the declarative way
165:40 - now if you don't have a yellow file and
165:43 - you just want to run a pod the
165:46 - imperative way you use qctl run you
165:50 - specify a name for your running pod Dash
165:53 - Dash image the image name
165:56 - you can specify a program to run in this
166:00 - case it's sh and dash C you can specify
166:04 - a parameter that you want to pass to the
166:08 - program
166:11 - qctl get parts will list all the pods
166:15 - that are currently running
166:17 - Dash o wide will get you the same
166:20 - formation but uh with a few columns more
166:25 - cubicity Of The Scribe part and the Pod
166:28 - name will show the pad information
166:34 - you can use qctl get part with the Pod
166:38 - name
166:39 - Dash offer output
166:41 - uh in yaml format and you can pipe that
166:44 - to a file name so this is pretty cool
166:47 - because
166:47 - it will extract the Pod definition in
166:51 - yaml and save it to a file
166:54 - so if in case you lost that yaml file
166:58 - that was used to create a pod well you
167:02 - can recreate it quite easily
167:05 - qctl exec Dash it specifying the Pod
167:09 - name and the program to run will get you
167:14 - inside that part in the interactive mode
167:19 - you delete a pod using cubectl delete
167:22 - Dash F specifying the yaml file or if
167:26 - you don't have the yaml file simply use
167:29 - cubectl delete pod and the path name
167:33 - that will result in the same as the
167:36 - previous command
167:41 - and this concludes this look at how to
167:44 - define and run pods
167:48 - [Music]
167:52 - in this lab we will run our first pods
167:56 - we'll start by using the imperative way
167:59 - we'll use the cube CTL run command
168:02 - specifying the image that we want to run
168:05 - in our case we want to run an nginx web
168:08 - server
168:09 - and we'll specify a name to the running
168:13 - instance
168:14 - my nginx
168:17 - let's run this
168:21 - pod my nginx created
168:24 - let's get a list of the running pods by
168:28 - using the cube CTL get pods command
168:32 - I have one pod running my nginx ready
168:36 - one of ones that is running it was
168:38 - created 11 seconds ago
168:41 - let's try to get more information by
168:44 - adding the dash o y per meter
168:48 - it's the same command but we'll get a
168:51 - little bit more information like the IP
168:53 - address of the Pod and the node where
168:56 - the Pod is currently running
168:59 - awesome
169:01 - if you want more information
169:03 - we'll use the cube CTL describe command
169:07 - with the type of object
169:10 - and the name of the running instance so
169:13 - cubectl describe the type pod and the
169:18 - name my nginx
169:22 - all right tons of cool information here
169:29 - the name of the object my internet the
169:32 - namespace where it's running
169:34 - uh the node where it's running start
169:37 - time any labels annotation the IP
169:40 - address information about the container
169:44 - any volumes restarts that happen
169:49 - and any volumes here
169:53 - and at the end we get the list of events
169:56 - that happened when the Pod was created
170:00 - it was first scheduled
170:03 - and then the image was pulled
170:07 - and then the image was successfully
170:10 - pulled
170:12 - and created
170:14 - and then started
170:18 - the cube CDL describe command should be
170:20 - the first thing you try when doing some
170:23 - troubleshooting here you will likely
170:26 - have some very useful information uh in
170:30 - case a pod doesn't start maybe
170:32 - the image is not available maybe the
170:36 - image didn't start correctly also Cube
170:40 - CDL describe command that's very very
170:42 - useful
170:45 - let's now delete our pod by using the
170:48 - qctl delete the object type pod and the
170:52 - name of the Running part
170:59 - should get my command line back in a few
171:03 - seconds
171:08 - all right
171:10 - let's now run a second pod this time a
171:14 - busy box image
171:16 - Cube CTL run the image busy box and the
171:21 - name of the running instance my box but
171:23 - we're adding extra parameters here Dash
171:26 - it dash dash the program we want to run
171:30 - this will open a session
171:33 - inside our running pod
171:39 - and look at the command prompt it's
171:41 - changed to a pound sign this means that
171:44 - now I can type commands that are running
171:47 - inside the pod LS for listing the
171:51 - folders and files I can run a
171:56 - base64 command here I can encode a
172:00 - string here and that happened inside the
172:04 - running container inside the Pod pretty
172:08 - cool
172:09 - to stop the session simply type exit
172:13 - and the this ends the session
172:17 - okay let's do a little bit of cleanup
172:21 - um in the busy box case uh it takes up
172:24 - to 30 seconds to uh to delete so we have
172:28 - two options here we can run the cube CTL
172:31 - delete the object type and the running
172:34 - AdSense and to get back our Command
172:36 - Prompt right away we can use the dash
172:40 - dash weight equal false per meter
172:42 - or if we simply want to kill the Pod
172:46 - brutally we use dash dash force and with
172:50 - a grace period of zero seconds
172:53 - let's run this
172:56 - and this will kill the Pod brutally
173:01 - all right
173:04 - let's now create a pod the declarative
173:07 - Way by using a yaml file
173:11 - we have a yaml file here called
173:12 - myhab.yamo
173:14 - the kind is a pod we give it a name my
173:18 - app dashboard
173:20 - a few labels
173:22 - the image we want to run
173:26 - any limits here four degree sources in
173:30 - CPU and memory
173:32 - the port that the container is listening
173:35 - to and some environment variable we're
173:38 - defining an environment variable called
173:40 - dbcon that will have a value of my
173:42 - connection string here
173:45 - awesome
173:46 - let's use the qctl create Dash F4 file
173:51 - and the name of our yaml file
173:54 - pod my
173:56 - app.pod created perfect let's run the
174:01 - qcl get pods command to get a list of
174:05 - our paths it's running
174:08 - perfect we can also describe
174:12 - our pod
174:15 - again
174:17 - same information that we saw earlier
174:21 - right the default names the namespace
174:27 - and here look we have our environment
174:30 - variable
174:33 - and the same events that we saw earlier
174:38 - previously we used the cube CTL run
174:43 - command with the dash it parameter to
174:48 - open a session to our BusyBox container
174:51 - now what is the Pod is currently running
174:53 - well we can use the cube CTL exec
174:57 - command with the same it
175:00 - per meter here
175:02 - we specify the Pod name and the program
175:05 - we want to run
175:08 - that will open a session to a part that
175:11 - is currently running
175:13 - okay let's output the
175:18 - dbcon environment variable my connection
175:21 - string awesome that worked
175:25 - let's exit or stop our session
175:29 - and this time we'll use the qctl delete
175:32 - command specifying our yaml file to
175:36 - delete our pod
175:44 - [Music]
175:49 - let's take a look at init containers
175:52 - let's say that your app has a dependency
175:55 - on something can be a database an API or
176:00 - some config files
176:02 - you want to initialize or validate that
176:05 - these exist before launching the app
176:08 - but you don't want to clutter your main
176:11 - logic with this type of infrastructure
176:13 - code
176:14 - so what do you do
176:17 - you can use an init container that lets
176:20 - you initialize a pod before an
176:23 - application container runs
176:25 - let's say that for the app container to
176:28 - run it requires a series of
176:30 - configuration files
176:32 - in the Pod definition you define a
176:34 - container that will run first this is
176:38 - the init container
176:40 - upon completion kubernetes will start
176:43 - the app container
176:45 - this is a great pattern for applications
176:47 - that have dependencies the init
176:49 - container job can be as simple as
176:52 - validating that a service or a database
176:55 - is up and running
176:56 - this keeps the infrastructure code out
176:59 - of the main logic
177:03 - init containers always run to completion
177:07 - you can have more than one and each bus
177:11 - complete successfully before the next
177:13 - ones starts
177:15 - if it fails the cubelet repeatedly
177:18 - restarts it until it succeeds unless
177:22 - it's restart policy is set to never
177:27 - probe are not supported as ended
177:29 - containers run to completion
177:33 - in this bad definition file we have the
177:36 - main application located in the
177:39 - containers section in green
177:41 - and the init containers in the init
177:43 - container section in yellow
177:45 - as you can see here we have two init
177:48 - containers they both watch for services
177:51 - to be up and running
177:54 - so the first one we will run to
177:56 - completion
177:58 - then the second one
178:01 - and finally kubernetes will start the
178:04 - app container
178:06 - and this concludes this look at init
178:09 - containers
178:12 - [Music]
178:16 - in this lab we will use an init
178:19 - container to modify the home page of an
178:23 - nginx container
178:26 - let's take a look at our yaml file
178:30 - it's manifest for a pod
178:34 - um we have two sections here containers
178:37 - and any containers let's first take a
178:39 - look at init containers
178:41 - we will use a BusyBox image
178:44 - and we'll run this command wget
178:48 - and will it that
178:51 - website right here and that website
178:55 - it's the ohm of the first website the
178:59 - home page of the first website is pretty
179:01 - simple just a few lines of HTML
179:05 - all right
179:07 - and we will save that HTML into a file
179:12 - called index.html into a volume called
179:16 - work directory
179:20 - and the nginx image
179:24 - will map that value
179:28 - and we'll serve that index.html page as
179:32 - its default web page
179:34 - so basically we're initializing our
179:38 - nginx
179:39 - container by using an init container
179:43 - here
179:44 - awesome
179:47 - let's open a terminal
179:54 - and let's deploy our application
179:58 - Cube CTL apply Dash F the name of our
180:02 - yellow file
180:04 - let's wait till the nginx image is up
180:11 - so if I do a doctor PS
180:16 - yet my nginx container is a
180:20 - oh let's uh open a session right into
180:23 - that nginx container
180:28 - and let's try to hit the default web
180:31 - page curl localhost
180:35 - and yep that worked we're serving the
180:39 - default uh web page of the CERN website
180:43 - here
180:44 - pretty cool
180:47 - let's type exit
180:50 - and let's
180:52 - do our cleanup
180:54 - cubesatel delete and our yellow file
181:01 - [Music]
181:06 - let's now look at selectors
181:10 - when defining kubernetes resources you
181:14 - can use labels
181:16 - these labels allow you to identify
181:19 - describe and group related set of
181:22 - objects or resources
181:24 - they are simply key value pairs that you
181:28 - define yourself
181:32 - in this part definition we see two
181:34 - labels
181:36 - app
181:37 - with the with a value of my app and type
181:41 - with a value of front end
181:45 - note that the app and type keys are not
181:49 - something defined by kubernetes they are
181:52 - defined by you for your application
181:57 - okay but what does labels have to do
182:01 - with selectors
182:04 - well selectors use labels to filter or
182:08 - select objects
182:11 - here we see a selector in this pod
182:13 - definition
182:15 - the selector type is node selector
182:19 - and the key value pair is this type
182:22 - equals super fast
182:25 - okay
182:26 - but how does that work
182:30 - here we have a path definition with the
182:33 - node selector set to this type equals
182:36 - super fast
182:39 - we're telling kubernetes that we want to
182:41 - run this bar on the Node that has a
182:45 - label set to this type equals super fast
182:52 - note a does have such label so
182:55 - kubernetes will schedule the Pod
182:57 - creation on that note
183:02 - the simplest way to picture what
183:04 - selectors do is by comparing them with a
183:07 - SQL query
183:09 - it would be something like select star
183:12 - from nodes where this type equals super
183:15 - fast
183:18 - and this concludes this look at the
183:20 - selectors concept
183:24 - [Music]
183:29 - in this lab we will test the selector
183:31 - concept
183:33 - we have two yaml files here the first
183:36 - one
183:37 - it contains the definition of a pod
183:40 - we will run an nginx web server
183:43 - listening on Port 80. and for the sector
183:47 - concept what is enter is is this section
183:50 - the label section so we have two labels
183:54 - defined here
183:55 - app set to my app
183:58 - type set to front dash n
184:03 - let's take a look at the second yaml
184:05 - file that's the service of name my
184:09 - service it's listening on Port 80 and
184:13 - targeting or redirecting the traffic to
184:16 - Port 80 that the Pod is listing on
184:20 - and let's look at this section selector
184:23 - app my app type front dash n so these
184:28 - are the same labels that we Define
184:31 - right here in our pod definition
184:36 - all right
184:37 - Let's test this concept
184:40 - I'll open a terminal
184:44 - and it will first deploy r
184:47 - by using cubesatel apply Dash F and the
184:50 - name of the yaml file
184:53 - foreign let's now deploy
184:57 - the service
184:59 - Cube CDL apply Dash f myservice.yaml
185:03 - all right how do we know that the
185:06 - service is connected to the Pod how do
185:10 - we know that the selection was
185:12 - successfully made
185:14 - to find that let's get the Pod IP
185:19 - address by using qcdl get pod Dash o
185:23 - wide
185:25 - and let's look at the IP address here
185:29 - 10.1.9.31 all right
185:33 - let's run this command cubectl get EP EB
185:36 - is short for endpoint and here's the
185:38 - name of our service my service so let's
185:40 - run this
185:44 - all right
185:45 - you see D in the endpoints column
185:50 - 10.1.9.31 that's the same IP address
185:54 - let's now try to port forward
185:57 - to that service
186:01 - okay we get an immediate result here and
186:05 - let's uh go to localhost
186:08 - port 8080 and that worked
186:11 - perfect let's go back here to our
186:14 - terminal and let's type Ctrl C to stop
186:18 - the port forward
186:20 - now let's try to break things a little
186:24 - bit
186:26 - we'll open the my app.yaml file and
186:30 - let's change the one of the labels so uh
186:34 - app will set it to my app to control s
186:38 - let's save that file
186:41 - and let's redeploy the file again the
186:46 - Pod again
186:47 - by using cubectl apply Dash F and the
186:50 - name of the yaml file
186:53 - all right
186:54 - let's check the endpoint again
186:58 - is it still working
187:01 - huh
187:02 - look at the endpoint colon none
187:07 - and if we try to pour forward again
187:13 - there's no immediate results so that
187:16 - doesn't work
187:18 - so disproved that both labels must match
187:23 - here the selector here boat labels must
187:27 - match the labels in in the Pod
187:30 - definition
187:31 - so that the selection can work
187:34 - all right
187:36 - let's type Ctrl C to stop this and let's
187:40 - do our little cleanup by deleting the
187:44 - service and then the pod
187:51 - thank you
187:56 - let's take a look at multi-container
187:58 - pods
188:00 - we sign a previous lecture that pass can
188:02 - run one or more containers
188:05 - that there's always a main worker and
188:08 - that the other containers are providing
188:10 - services to that main worker
188:12 - like saving data to a database
188:16 - and writing log files
188:18 - there are scenarios while
188:20 - multi-container Parts make sense and
188:22 - they are well documented in a series of
188:24 - patterns we will take a look at some of
188:26 - them in the next few slides
188:30 - with these sidecar pattern the helper
188:32 - container provides extra functionalities
188:34 - to the main worker
188:36 - let's say that our app writes some log
188:39 - files inside the pod
188:42 - the sidecar can copy these log files to
188:46 - some purchasing storage offered by the
188:48 - cloud provider
188:49 - this way the application code located
188:53 - inside the main worker is not cluttered
188:56 - with infrastructure code
188:58 - that code is located in the helper
189:01 - container
189:02 - and if you move from one cloud provider
189:04 - to another one well you simply replace
189:07 - or update that L per code
189:10 - this keeps your application code super
189:13 - clean
189:15 - our next pattern is the adapter let's
189:18 - say that our main worker outputs some
189:21 - complex monitoring information
189:24 - that the monitoring surface of our cloud
189:27 - provider cannot understand
189:29 - the adapter role would be to connect to
189:32 - the main worker
189:34 - simplify the data for the monitoring
189:37 - service
189:38 - again the code specific to the cloud
189:41 - provider service is located inside the
189:43 - helper container
189:46 - the Ambassador pattern is another type
189:51 - of the men of the middle role let's say
189:54 - that our application code needs to write
189:56 - to some nosql database but the code has
190:00 - no clue on how to do that
190:02 - no problem you can send that data to the
190:06 - Ambassador that in turn will write the
190:10 - data to the nosql data store
190:13 - the code specific to the data store
190:16 - service is located inside the helper
190:19 - container
190:21 - if you're curious about these patterns I
190:24 - suggest you get a copy of the design
190:26 - distributed systems book the other is
190:29 - Brennan Burns Brendan worked at the
190:32 - Google where he co-created kubernetes
190:35 - he now works at Microsoft
190:39 - so how can you define multi-container
190:41 - paths well if you remember the lecture
190:44 - about yellow files
190:46 - you see that the container sections is
190:49 - actually a list
190:51 - this means that you can Define multiple
190:53 - containers
190:55 - are added in yellow you see container
190:57 - number one and in green container number
191:00 - two and when you create a pod both
191:03 - containers will be created at the same
191:05 - time pretty cool
191:08 - here's a quick cheat sheet for
191:11 - multi-container parts
191:13 - so after you created your yaml file you
191:17 - simply use
191:18 - cubectlcreate-f specifying the yaml file
191:21 - name so same thing as creating a single
191:23 - container pod
191:26 - if you want to exec into one of the
191:28 - container you simply use Cube CTL exec
191:31 - Dash it the part name and specifying
191:36 - Dash C and the container name this way
191:39 - you can jump into one of the containers
191:43 - running inside the pod
191:46 - you get the logs for a container using
191:49 - cubectl logs the Pod name Dash C the
191:53 - container name
191:58 - and this concludes this look at
191:59 - multi-container pods
192:09 - let's take a look at some networking
192:10 - Concepts
192:13 - kubernetes is what we call a flat
192:15 - Network as a result most resources
192:18 - inside a cluster can see each other
192:21 - all containers within a pod can
192:23 - communicate with each other
192:25 - all pods can communicate with each other
192:28 - all nodes can communicate with all pods
192:31 - and all nodes
192:33 - paws are giving an ephemeral IP address
192:36 - while services are giving a persistent
192:38 - IP so this is quite important we'll come
192:41 - back to that later
192:43 - let's illustrate the cluster Network in
192:46 - blue here
192:49 - each pod gets an IP address and the
192:53 - containers inside a pod share the same
192:56 - address space
192:58 - the containers inside the same pod share
193:01 - the same IP address but each must be
193:04 - assigned a different port number
193:08 - they can communicate inside the Pod
193:10 - using localhost and the port number
193:16 - they can also share the shared volumes
193:22 - what about communication between pods
193:25 - can the container on the right talk to
193:29 - The Container inside the pod on the left
193:31 - using localhost
193:34 - no they can't
193:35 - they need to go through a service that
193:39 - will front the network traffic for the
193:42 - pod
193:44 - for external access to the cluster
193:46 - traffic goes through a load balancer
193:48 - service offered by the cloud provider
193:52 - in future lectures we'll look at
193:55 - different type of services that we can
193:57 - use in kubernetes
194:00 - and this concludes this quick look at
194:02 - networking Concepts
194:06 - thank you
194:12 - let's create a multi-container pod using
194:14 - a yaml file let's take a look at the
194:17 - yaml file
194:19 - so the kind is part
194:21 - the name will be two dash containers
194:25 - and in the container section we're
194:28 - defining two containers so the first one
194:30 - we'll use the nginx image and we'll name
194:34 - it my nginx and that web server will
194:39 - listen on Port 80.
194:42 - and we're defining a second container
194:45 - using the BusyBox image and we'll name
194:48 - it my box
194:50 - this one will listen on Port 81.
194:54 - and for the BusyBox container to stay up
194:57 - and running we need to tell it to to
195:00 - stay up by issuing a sleep command so
195:04 - it'll stay for an hour
195:06 - what we'll try to achieve is open a
195:09 - session on the BusyBox container and try
195:13 - to hit the nginx container so the the
195:17 - default web page served by the nginx
195:20 - container
195:22 - all right
195:24 - let's try to create our
195:29 - pod here using a cube CTL create and the
195:34 - name of our yaml file
195:36 - okay pad two dash containers created so
195:41 - let's try to see uh if they're running
195:46 - all right two out of two because we have
195:50 - two uh containers running in that pod
195:52 - status running 10 seconds uh the IP
195:56 - address so there's one IP address
195:57 - assigned to the to the pub
196:00 - and denote okay let's try to get a
196:03 - little bit more information by using the
196:06 - cube Ctrl describe pod and the name of
196:08 - the pod
196:10 - let's scroll up a little bit
196:14 - so the name is two containers then space
196:17 - is the default one the node where it's
196:21 - running uh the containers so the first
196:25 - one is my nginx which using a the nginx
196:29 - image
196:32 - we have information about the limits
196:34 - that we set earlier in the ml file my
196:38 - box using a BusyBox image listing on
196:42 - Port 81
196:43 - right there's a that sleep command
196:48 - so useful information here and also look
196:53 - at the events now
196:55 - so earlier when we add only one
196:58 - container inside a pod we would get just
197:01 - one set of events for polling and
197:05 - creating the container now we have two
197:08 - first one is the Pod was scheduled and
197:12 - the second one here is on the second
197:15 - land line
197:18 - um the nginx image was in the polling
197:20 - state it was Paul created started and
197:23 - then the busy box container was pulled
197:27 - and created
197:28 - okay
197:30 - perfect now let's try to open a session
197:33 - inside our BusyBox container so we'll
197:36 - use Cube CTL exec Dash it okay here's
197:41 - the trick now we need to specify the Pod
197:44 - name and then the container name that we
197:50 - want to to connect to
197:51 - and then the the program that we want to
197:55 - run that's the trick when you have
197:56 - multiple containers you need to specify
197:59 - the Pod name and then the container name
198:04 - all right let's try to do that
198:07 - oh looks good at last yeah
198:11 - it worked okay now we'll use wget to try
198:16 - to hit that default page serve ID nginx
198:21 - web server
198:23 - wget with the this flag and we'll call
198:27 - localhost
198:30 - all right welcome to nginx so that word
198:35 - now Outpost for a couple of seconds and
198:39 - why I'm gonna ask you why did that work
198:44 - why calling localhost worked
198:51 - so that work because let's go back to
198:55 - our yaml file
198:58 - the nginx container is listening on Port
199:01 - 80. right we don't have to specify a
199:06 - import number here
199:09 - if the nginx container would have
199:11 - listened to something different we would
199:13 - have to specify Here Local O's colon and
199:17 - the port number
199:19 - all right let's try to exit this
199:23 - perfect
199:25 - let's now delete our pod using Cube CTL
199:28 - delete and the name of the ml file and
199:30 - by using the force and grease grace
199:33 - period equals zero flag
199:35 - that will kill both containers
199:39 - immediately
199:42 - perfect
199:46 - [Music]
199:51 - this is a super short lecture just to
199:54 - introduce you to the concept of a
199:56 - workload
199:58 - so a workload is an application running
200:00 - on kubernetes
200:02 - all containers running in a kubernetes
200:04 - cluster must run in a workload
200:09 - the Pod is the atomic workload it
200:12 - represents a set of running containers
200:14 - and all workloads will create pods
200:18 - the replica set and the deployment will
200:20 - provide extra functionalities on top of
200:23 - the pod
200:24 - like the ability to Define how many
200:26 - instances of a part we want
200:30 - the stateful set and the Daemon set are
200:33 - specialized workloads and finally the
200:36 - job and crunch up offer tasks that run
200:39 - to completion
200:41 - these are short-lived tasks
200:45 - we will see each of these workloads in
200:48 - detail in the following lectures
200:51 - and this concludes this super short
200:53 - lecture on the workload concept
200:58 - [Music]
201:03 - let's take a look at replica set
201:07 - the replica set primary job is to manage
201:10 - d-pad replicas making sure that the
201:13 - desired number of instances are running
201:17 - this provides the self-e-link
201:19 - capabilities in kubernetes
201:22 - while you can create replica set the
201:26 - recommended workload is the deployment
201:28 - now welcome back to that in a moment
201:32 - so let's say that you want three
201:34 - instances of a pod to run at all time
201:38 - you create a replica set and specify
201:40 - that you want three replicas
201:43 - if for some reason one bird crashes
201:47 - kubernetes will replace it automatically
201:50 - without any human intervention I'm
201:53 - pretty cool eh
201:55 - let's see how to define a Rebecca set by
201:58 - starting with a pod
202:01 - we take the Pod definition except for
202:03 - the API version and kind
202:06 - and we place these values in the
202:09 - template section of the replica set
202:12 - the final result is a replica set yaml
202:15 - file
202:17 - so basically in the section highlighted
202:20 - in green
202:22 - you will find values specific to replica
202:24 - set and under the template section the
202:28 - values that Define the Pod you want to
202:30 - run
202:31 - here you set the desired number of
202:34 - instances with the replicas property
202:39 - again while you can create replica sets
202:41 - the recommended workload to use is the
202:44 - deployment because it provides extra
202:47 - functionalities on top of the replica
202:49 - set
202:51 - so why bother and learn about replica
202:54 - sets
202:55 - well in the deployment lecture you'll
202:58 - see that when you create a deployment
203:00 - that will also create a replica set in
203:03 - the background
203:04 - that's why it's important to learn about
203:06 - the replica sets functionalities
203:11 - here's a cheat sheet for replicasets
203:13 - command
203:15 - you create one using cubectl apply Dash
203:19 - F and the yabl file
203:24 - you get a list of the replicases by
203:26 - using cubectl get RS
203:31 - you get some information
203:33 - about the replica set by using Cube CTL
203:36 - describe RS and the replica set name
203:40 - you delete one using Cube CTL delete
203:43 - if you have the yaml file you specify
203:46 - Dash F and the yaml file name or if you
203:49 - don't have the yaml file simply by using
203:53 - the replica set name using cubectl
203:55 - delete RS and the replica set name
204:00 - and this concludes this look at Olympic
204:03 - assets
204:06 - [Music]
204:11 - let's create three instance of energy
204:14 - next container using the replica set
204:17 - template
204:19 - so let's take a look at our yaml file
204:22 - the type of object we want to create is
204:25 - the replica set
204:27 - this will be the name of the replica set
204:30 - rs-example
204:33 - we want three replicas running uh at the
204:37 - same time
204:39 - and uh we want three replicas of this
204:43 - containers
204:44 - uh name will be nginx and the image
204:47 - nginx uh colon Alpine so it's a smaller
204:52 - version and we Define the resources and
204:56 - also the ports that each is listening on
205:01 - all right
205:02 - let's try to uh create that so Cube CTL
205:07 - apply or create Dash F and the name of
205:11 - our EML file
205:15 - okay we'll pick a set created
205:18 - let's take a look at our running pods
205:23 - and have three pods running okay and
205:28 - look at the names that were
205:30 - um that were assigned to the each of
205:32 - these spots RS Dash example and then is
205:36 - some kind of a unique number here
205:41 - each one must have a different or unique
205:45 - name so each one is ready running and
205:48 - look here each one has a different IP
205:51 - address
205:52 - perfect so let's take a look at our
205:56 - replica sets that we've created so Cube
205:59 - CDL get RS
206:01 - so there's one
206:03 - rs-example
206:06 - um three desired three current three
206:09 - ready
206:10 - everything is green everything's okay
206:14 - Let's uh now describe our replica set so
206:20 - Cube CTL describe RS for replica set and
206:23 - the name of our replica set
206:25 - so let me paste that
206:29 - right let's scroll up a little bit
206:32 - so the name is RS example uh it's
206:36 - running in the default namespace
206:40 - um any labels or annotations are listed
206:43 - here number of replicas that we set so
206:46 - three current three desired and the the
206:49 - current part the status S3 running
206:52 - and zero failed
206:56 - to The Container information so nginx we
206:59 - want to run the nginx Alpine image the
207:02 - sync on Port 80 the limits and so on and
207:05 - this these are the events that were
207:09 - raised here so each pod was successfully
207:13 - created here
207:17 - all right so last thing we need to
207:22 - delete what created using qctl delete
207:25 - Dash F and the name of the EML file
207:31 - thank you
207:36 - let's take a look at deployments
207:40 - we'll start by comparing pods and
207:42 - deployments
207:44 - Buzz don't self-heal meaning that if a
207:47 - pod dies kubernetes will not replace it
207:50 - automatically
207:52 - using a pod definition only one instance
207:55 - can be created and you can't update and
207:58 - roll back pods
208:01 - deployment can
208:04 - a deployment manages a single part
208:07 - template so you create one for each
208:10 - microservice you want to run
208:13 - when creating a deployment this will
208:15 - also create a replica set in the
208:17 - background
208:18 - but while you can see it
208:21 - you don't interact with the replica set
208:23 - directly you let the deployment manage
208:26 - that for you
208:29 - to summarize the replica set provides
208:32 - the self-ealing and scaling
208:34 - functionalities while the deployment
208:37 - provides updates and rollback
208:39 - functionalities
208:43 - let's take a look at the deployment
208:45 - definition
208:46 - you define the desired number of
208:48 - instances with the replicas property
208:52 - this will be used by the underlying
208:54 - replica set
208:56 - you set the number of iteration you need
208:59 - to keep using the revision history limit
209:01 - property
209:04 - and you set the update strategy in the
209:06 - strategy section
209:08 - you can set the strategy type to a
209:11 - rolling update this way kubernetes will
209:14 - cycle when updating the pods
209:16 - the other strategy is recreate
209:19 - kubernetes will take all existing pods
209:22 - down before creating the new ones
209:26 - we'll have a dedicated lecture on this
209:29 - topic later on
209:32 - like a replica set we start with a pod
209:35 - definition
209:36 - and we start the metadata section in the
209:40 - template section of the deployment
209:48 - the final deployment definition looks
209:51 - like this
209:52 - highlighted in green we see the
209:54 - properties specific to the deployment
209:56 - and in yellow the ones defining the Pod
210:00 - we want to run
210:03 - here's a cheat sheet for deployments
210:05 - command
210:08 - so if you don't want to use a yaml file
210:12 - you can create a deployment using the
210:16 - imperative way so you use Cube CTL
210:18 - create deploy you specify a name
210:22 - then with the image property you you
210:25 - specify the image
210:27 - replicas the number of replicas you want
210:30 - to run and you can specify also other
210:33 - properties like the port number that the
210:37 - paths will listen on
210:39 - if you have a yaml file well you simply
210:42 - use Cube CTL apply Dash F and the yaml
210:45 - file
210:46 - you get a list of the deployments using
210:50 - cubectl get deploy
210:52 - and you get the deployment info using
210:55 - cubectl describe deploy and the
210:58 - deployment name
211:00 - since a deployment will create also a
211:03 - replica set you can get the list of the
211:06 - replica set using cubectl get RS and you
211:11 - delete a deployment by using a EML file
211:13 - so Cube CTL delete Dash F the name of
211:16 - the yaml file or if you don't have it
211:19 - simply use Cube CTL delete deploy and
211:22 - the deployment name
211:26 - and this concludes this look at
211:29 - deployments
211:30 - thank you
211:32 - [Music]
211:37 - let's use the deployment template to
211:39 - create three instances of an nginx
211:43 - container
211:44 - so let's take a look at the EML file
211:49 - this time we want to use the deployment
211:52 - Kai
211:53 - type of object we want to create is the
211:55 - deployment
211:56 - we name it deploy example we want three
212:00 - instances three replicas
212:03 - and we want to keep three versions three
212:06 - replica sets versions in the history in
212:09 - site kubernetes
212:12 - I know if you scroll down a little bit
212:13 - we see that we want to run an nginx uh
212:16 - the Alpine version because it's a little
212:18 - bit smaller and we name it nginx we set
212:23 - some resource limits and each pod will
212:26 - listen on Port 80. all right quite
212:30 - similar to the replica set template that
212:32 - we saw earlier except maybe for uh this
212:38 - parameter here
212:40 - all right
212:41 - now let's create our deployment using
212:45 - cubesatel apply or create Dash F deploy
212:49 - example
212:51 - and let's get a list of the pods that
212:55 - are currently running Cube CTL getbots
212:58 - Dash o wide
213:00 - and yes we have three parts three lines
213:04 - here so look at the names uh given to
213:08 - each bot so deploy example that's the
213:11 - name of the deployment object
213:14 - the the name of the deployment object
213:16 - Dash something unique so to make sure
213:19 - that each part has a unique thing
213:21 - kubernetes as a random number like this
213:26 - only one container is running inside
213:28 - each bot uh it's in running State each
213:32 - pod gets its own IP address
213:35 - so that's perfect
213:37 - okay let's now try to describe our pods
213:43 - so we can use the cube CDL describe pod
213:47 - and deploy example name but we just saw
213:52 - that the the names work a little bit
213:56 - different let's see if this works
213:59 - yep it worked
214:02 - it worked because uh the name is share
214:04 - across uh these three parts and so we
214:09 - get information about each one of these
214:12 - let's say I just need the information
214:14 - about that particular part I can use
214:16 - that name uh the unique name also so
214:19 - instead of deploy example I'm gonna use
214:23 - the full name
214:25 - and there it is f uh the information
214:29 - about that pod
214:32 - just that button
214:33 - all right
214:35 - let's now get some information about the
214:38 - deployment
214:40 - currently uh inside my cluster so
214:43 - cubesatel get deploy I have only one
214:48 - deploy Dash example three out of three
214:50 - ready up to date available so
214:53 - everything's uh looking good
214:56 - and I can describe my deployment using
215:00 - cubectl describe the name of the the
215:03 - type of the object and then its name
215:07 - all right so let's go a little bit we
215:11 - have the name the namespace where it's
215:13 - running any labels annotations number of
215:17 - replicas three desired three updated
215:20 - three total three available perfect
215:23 - strategies rolling update
215:25 - okay because we haven't set a running
215:28 - update strategy that's the default one
215:32 - we'll come back to that uh later on the
215:36 - path template
215:37 - nginx Alpine listening on Port 80.
215:41 - right and the events here
215:47 - all right
215:49 - since a deployment will automatically
215:53 - create a replica set let's take a look
215:56 - at the replica set that was created so
215:59 - Cube CTL get RS
216:02 - and for sure we have one replica set
216:04 - that was created by the deployment if we
216:07 - desired three current we're ready
216:08 - everything looks okay and we can also
216:11 - describe our Republic asset since we
216:14 - have only one we can uh use Cube CTL
216:18 - describe RS or we can use its full name
216:23 - we copy that
216:27 - and here I have the replica set
216:31 - description
216:33 - and finally we need to do a little bit
216:36 - of cleanup we delete our deployment
216:40 - using Cube CTL delete and with the name
216:44 - of the yaml file
216:49 - thank you
216:54 - let's take a look at demon sits
216:57 - the demon set is a specialized workload
217:00 - its role is to ensure that an instance
217:02 - of a pod is running on all nodes
217:06 - the paths are scheduled by the scheduler
217:09 - controller and run by the demon
217:11 - controller
217:13 - as nodes are added to the cluster
217:16 - the pods are added to them
217:19 - typical views are running some kind of
217:21 - helper service like a log collection or
217:25 - some kind of monitoring
217:27 - well let's illustrate that in this
217:30 - cluster we have two nodes
217:32 - and the demon workload ensures that an
217:35 - instance of a pod is running on each one
217:38 - of these nodes
217:41 - so you define a demon set in a yellow
217:44 - file
217:45 - you can specify that you don't want to
217:47 - schedule a pod on the master node by
217:49 - using a toleration
217:51 - same thing if you want to run the part
217:54 - on specific node pools
217:58 - here's a cheat sheet for the demon sets
218:00 - command
218:01 - so you create a demon set using a yaml
218:05 - file with the cube CTL apply Dash F
218:08 - command
218:09 - you get a list of demon set using Cube
218:12 - CDL get DS
218:14 - you get some information about the
218:17 - running demon set using cubectl describe
218:20 - DS and the name of the demon set
218:23 - and when you want to delete a demon set
218:26 - you either use a yaml file using cubectl
218:30 - delete Dash F and the name of the yaml
218:32 - file or the name of the running Daemon
218:36 - set by using Cube CTL delete DS and the
218:39 - demon set name
218:42 - and this conclude this look at demon
218:46 - sets
218:48 - [Music]
218:54 - let's run a BusyBox image as a demon set
218:58 - to ensure that we have one instance of
219:03 - that container running on each node in
219:07 - our cluster
219:09 - so let's take a look at the yaml file
219:13 - all right
219:14 - the kind is demon set we give it a name
219:18 - demon set Dash example
219:20 - and the container that we'll run is a
219:24 - busy box and here in the Toleration
219:27 - section
219:29 - we specify that
219:32 - we don't want to schedule
219:35 - a demon set on
219:38 - a node role
219:41 - that is mastered so we don't want to to
219:44 - run that demon said that the Busy box uh
219:48 - on the control plane on the masternode
219:51 - all right
219:54 - Let's uh
219:56 - deploy our demon sets so here I'm I'll
220:01 - be deploying that demand set on a
220:03 - cluster in the cloud that has three
220:05 - nodes because if I would try to do that
220:08 - on Docker desktop with just one note
220:10 - won't be interesting so let's try to do
220:13 - that doesn't matter what cloud provider
220:15 - I'm using right now
220:17 - okay uh let's uh get a
220:22 - a cube City I'll get parts we'll come
220:24 - back to uh the rest just after that so
220:29 - here I have three uh Parts demon example
220:33 - with a unique name here let's uh add the
220:38 - dash
220:40 - oh wide
220:44 - a flag
220:46 - Let's uh examine the node column so you
220:52 - can see that each pod is running on a
220:57 - unique node so 0 3 and 4 here
221:03 - and this selector is interesting because
221:06 - it if you have multiple objects you can
221:11 - basically filter out or just select what
221:15 - you want so here selector says the app
221:18 - uh equals the demon set example that's
221:22 - the name of our object here
221:25 - so that word one instance of our busy
221:28 - box is running on each node
221:32 - let me add a fourth node
221:43 - all right so my note count is now four
221:48 - let's go back to visual studio code and
221:52 - let's rerun uh the get paths command
221:56 - and for sure I have a Ford instance of
222:01 - my BusyBox container running as a demon
222:05 - set on the new note without me doing
222:08 - anything it was deployed automatically
222:11 - because I selected the demon set object
222:15 - type
222:19 - now we simply need to delete that demon
222:22 - set using a cube CTL delete
222:29 - [Music]
222:35 - let's take a look at stateful sets
222:38 - let's say that you run a database inside
222:41 - your kubernetes cluster
222:43 - traffic gets higher and you need to
222:46 - scale that database
222:48 - so you create more instances
222:51 - the main one can read and write while
222:55 - the other instances are in read-only
222:57 - mode and use replicas of the main
223:00 - database
223:01 - this is a complex scenario to implement
223:06 - the stateful set role is to help you
223:09 - solve this complex problem
223:12 - for parts that need to maintain state
223:15 - and unlike a deployment a stateful set
223:19 - maintains a sticky identity with a
223:22 - persistent identifier for each of the
223:24 - pods
223:26 - and if a part dies it will be replaced
223:30 - by another one using the same identifier
223:35 - the stateful set will create the parts
223:37 - in sequence
223:39 - and delete them in reverse order
223:44 - typical use are for Network related
223:47 - services that maintain some State and
223:50 - also for databases
223:55 - each pod gets a unique identifier using
223:58 - the stateful set name plus the sequence
224:01 - number
224:02 - and if one dies it is replaced by a new
224:06 - one but with the same identifier
224:12 - the pot creation is done in a ordered
224:14 - way meaning that the first one will be
224:17 - created then the second one and so on
224:21 - and when they are deleted they are
224:24 - deleted in the reverse order
224:27 - note that the persistent volumes are not
224:30 - automatically deleted
224:33 - foreign
224:36 - set you need to use a needless service
224:39 - you define one by setting the cluster IP
224:43 - value to none
224:45 - the stateful set will then refer to the
224:48 - atlas service
224:50 - and you define cloud storage in the
224:53 - volume claim templates section
224:57 - let's represent this slide as you can
225:00 - see only the first spot lets you read
225:03 - and write so how can you reach it if you
225:08 - want to write to the database
225:10 - well you use the pad network name in
225:15 - this case the instance name my SQL Dash
225:19 - zero
225:21 - plus the service name so dot MySQL
225:26 - when reading simply use the service name
225:29 - this will load balance the calls across
225:32 - all instances
225:36 - a bit of warning here
225:39 - containers are stateless by default and
225:42 - stateful sets are offering a solution
225:45 - for a stateful scenario but a lot of
225:48 - work has to be done on top of that
225:52 - a better approach might be to use the
225:54 - cloud provider database Services instead
225:57 - of trying to fit a stateful scenario
225:59 - inside your kubernetes cluster
226:03 - lastly deleting a sitful set will not
226:06 - delete the PVCs you have to do this
226:08 - manually
226:11 - here's a cheat sheet for stateful set
226:13 - commands so you create one using the
226:17 - cube CTL apply Dash F and the yaml file
226:21 - you get a list of set full sets using
226:24 - cubectl get STS you describe one using
226:28 - cubectl describe SCS and the set for set
226:31 - name
226:32 - and you delete a simple set using the
226:36 - delete command either using a yaml file
226:39 - or the name of the stateful set
226:45 - and this concludes this look at stateful
226:47 - sets
226:51 - [Music]
226:56 - let's now create a stateful set
226:59 - let's take a look at the state we'll set
227:01 - that EML file there are two parts in
227:04 - this CML file the first part is the
227:06 - creation of the Headless service
227:09 - the the kindest service
227:13 - but the trick here is to set cluster IP
227:18 - To None like this this will create an
227:22 - endless service
227:24 - all right second part is to create our
227:26 - stateful set so the kind set will set
227:29 - give it a name nginx Dash SCS and here
227:34 - we're referencing our Atlas service here
227:37 - so the name of our Atlas service
227:39 - assigned to the service name property
227:43 - we want three replicas
227:46 - right of a nginx image
227:50 - and we're creating a claim on a search
227:54 - class name called Azure file so the
227:57 - cloud provider here is not important
227:59 - that will work on any cloud provider so
228:03 - I already have a search class name
228:05 - called Azure file we'll use the read
228:09 - write once access mode and we'll use
228:12 - when when gigabyte of storage and we'll
228:15 - Mount this to a folder in VAR www
228:22 - all right
228:25 - so let's create our Seattle set here
228:28 - Cube CTL apply Dash if
228:30 - stateful set Dot yaml and quickly let's
228:34 - do a cube CTL get but a white and here
228:39 - you can see that the first
228:42 - instance is running
228:45 - the second one is running the third one
228:48 - is pending so each instance will be
228:52 - created in a sequence so the first one
228:55 - zero the second one one the third one
228:59 - two you can see by the age of each
229:04 - instances so 19 seconds 14 and 9 seconds
229:08 - they are created in a sequence and
229:12 - deleted in a reverse order also
229:16 - here we can see that um
229:19 - there's an IP address assigned to each
229:22 - of these
229:25 - let's take a look at the PVCs we should
229:28 - have a PVC for each of these instances
229:34 - and yes SCS 0 1 and 2. so uh we can do a
229:41 - mapping one to one here so for uh scs0
229:46 - we have a PVC call also scs0 to prove
229:50 - that let's uh describe the second one
229:53 - let's say so Q
229:56 - the L
229:58 - describe paths and the number two and
230:03 - let's see what the PVC is assigned what
230:06 - claim is assigned
230:09 - so here
230:11 - the volume claim name scs-2
230:16 - awesome
230:19 - okay
230:22 - what we'll do we'll create a file in the
230:25 - in that instance the instance number two
230:27 - and we'll delete the the Pod the Pod
230:31 - will be recreated automatically and
230:34 - we'll see if the file still exists we'll
230:37 - also modify the default web page served
230:41 - by nginx and see how we can reach that
230:45 - that file the default web page from
230:50 - another instance
230:52 - so let's open a session on the nginx CS2
231:00 - perfect
231:02 - Let's uh CD in VAR
231:07 - are volume and let's uh simply type Echo
231:12 - and pipe that to
231:15 - a file called echo.txt
231:19 - I do it LS my file is there get
231:23 - Hello dot txt yeah perfect
231:28 - okay first step creating that file uh
231:32 - Second Step modifying the default web
231:35 - page let's CD into the user chair nginx
231:39 - HTML folder
231:44 - and let's do an LS here
231:47 - so here's the default page served by
231:50 - nginx index.html
231:54 - we will brutally replace that file by
232:00 - using cat and piping that to the file
232:04 - name
232:06 - and typing hello
232:08 - enter Ctrl D on Windows to save the file
232:15 - the file is there if I simply do a cat
232:18 - index.html
232:21 - yeah okay the file has been brutally
232:24 - replaced
232:26 - okay we'll close our session on this
232:29 - instance so scs2 let's close our ins our
232:33 - session
232:35 - and let's open
232:38 - a session on the instance zero
232:43 - okay
232:49 - awesome
232:51 - let's try to it that default HTML page
232:55 - but on the instance number two so nginx
232:59 - as CS2 so to do that we'll need to use
233:04 - the web the web address nginx scs2 so
233:09 - that's the name of the instance dot the
233:12 - name of the atlas service
233:16 - and that word
233:18 - would only the
233:22 - hard name word
233:26 - no you need a combination of boat
233:29 - the instance name and the atlas service
233:33 - also
233:34 - all right let's exit our session here
233:40 - okay let's try to delete uh the uh
233:45 - instance number two
233:53 - okay let's do a q that's CTL
233:57 - get but
234:01 - we have a new instance
234:03 - uh that was created uh seven seconds ago
234:07 - but the uh the name of the instance is
234:10 - still the same
234:11 - so instead of a random number here by
234:16 - using a stateful set this will ensure
234:19 - that the names will be the same
234:22 - okay so let's
234:25 - open a session
234:27 - on the instance number two let's LS far
234:32 - and here's our file
234:37 - awesome
234:38 - let's do our cleanup so we'll delete the
234:41 - stateful set and we need to manually
234:45 - delete dpvcs because simply deleting
234:48 - this table set will not delete the PVCs
234:52 - so let's do that
235:00 - [Music]
235:05 - let's take a look at jobs
235:09 - are for short-lived task workloads
235:13 - you start a job it executes and succeeds
235:17 - or fails so job don't say memory they
235:20 - don't wait for traffic
235:22 - a job creates one or more pod and
235:25 - ensures that a specific number of them
235:28 - successfully terminate
235:30 - the job tracks the successful part
235:33 - completions and then marks the job as
235:36 - complete when the desired number of
235:38 - completion is reached
235:40 - when using multiple Parts the job will
235:43 - create them one after the other
235:46 - they can also run in parallel
235:50 - this is a job definition
235:53 - you define how many paths you want to
235:55 - run at the same time
235:57 - you can set a deadline if needed
236:02 - and the number of completions to reach
236:04 - to Mark the job as complete
236:08 - and you should set the restart policy to
236:12 - never
236:15 - so here's a cheat sheet for the jobs
236:17 - command
236:19 - you create a job the imperative way
236:22 - using Cube CTL create job the job name
236:26 - and the image name
236:28 - using the declarative way with a yaml
236:31 - file you use cubectl apply Dash F into
236:34 - the yaml file name
236:36 - you list the jobs by using cubectl
236:40 - GitHub
236:41 - you get some information by using
236:44 - cubectl describe job and the job that is
236:47 - currently running
236:49 - and you delete jobs with either a yaml
236:53 - file using cubectl delete Dash F or with
236:56 - the job name Cube CTL delete job and the
237:00 - job name
237:03 - and this concludes this look at jobs
237:09 - thank you
237:14 - in this lab we'll create a simple job so
237:18 - let's take a look at the chub yaml file
237:21 - so the kind is chopped so that's the
237:25 - type of object we're creating
237:27 - uh the name we're giving it hello and
237:31 - what we will run is a busy box container
237:36 - and when the container starts it will
237:40 - Echo uh hello front the job
237:44 - perfect
237:46 - something sip simple
237:49 - Let's uh run this Cube CTL apply with
237:55 - the name of our yaml file
237:58 - perfect the job was created
238:01 - okay let's get a list of the jobs
238:07 - there's one
238:08 - hello completion one so it ran
238:13 - and duration was two seconds okay
238:17 - let's do a cube CDL describe job since
238:21 - we have only one job that will do the
238:24 - work if not we can type the job name
238:28 - hello
238:31 - and uh but but let's take a look at name
238:34 - hello namespace default
238:38 - annotation and so on and so on it ran
238:42 - One Time One succeeded
238:45 - so that's our container
238:48 - successfully create and complete it
238:52 - perfect
238:53 - now
238:55 - we
238:58 - can get a list of the pods using cubectl
239:02 - get that and this is uh the Pod that was
239:07 - created to run the job it's still there
239:10 - you see the status completed
239:13 - but it's still there so we can get to to
239:19 - examine the log in case something went
239:21 - wrong
239:22 - so we can do Cube
239:25 - CTL
239:26 - logs and the name of the part here
239:33 - hello from the job
239:37 - that worked
239:39 - so let's do our cleanup let's delete the
239:41 - job
239:46 - is deleted do we have any uh Parts left
239:51 - none any chops left
239:56 - none perfect
240:01 - [Music]
240:06 - let's take a look at crunch apps
240:09 - a crunch up is a workload that runs jobs
240:12 - on a schedule
240:14 - it's an extension of the job that we saw
240:17 - in the previous lecture
240:19 - the schedule is defined using a crown
240:22 - like syntax in UTC
240:24 - and you can get more information about
240:26 - the crown syntax in this Wikipedia page
240:30 - here's a crunch out definition
240:33 - you set the scheduled parameter to a
240:36 - cron schedule
240:38 - so how do you know if a crunch up ran
240:41 - successfully
240:42 - well you need to look at the job history
240:45 - by default the last three successful
240:48 - jobs
240:49 - and the last fill job are kept
240:54 - the paths will be in a stop State and
240:57 - you'll be able to look at their logs
241:00 - foreign
241:01 - if you don't want to keep any history
241:03 - you can set the successful job history
241:06 - limit to zero
241:11 - so here's a cheat sheet for the crown
241:14 - job commands
241:15 - you can create one using the imperative
241:18 - way
241:20 - and if you have a yellow file you use
241:23 - the cube CTL apply Dash F and the name
241:25 - of the yaml file
241:27 - you can get the list of decron jobs
241:29 - currently
241:31 - running using cubectl get the CJ
241:36 - you can get some information with the
241:39 - cube CDL describe CJ and you delete the
241:42 - crunch up using its CML file using Cube
241:44 - CTL delete Dash F and the yaml file name
241:48 - and if you don't have the yellow file
241:50 - name you can delete it using cubectl
241:53 - delete CJ and the crunch up that is
241:56 - currently running
241:59 - and this concludes this look at run jobs
242:04 - [Music]
242:09 - so let's create a crunch up we'll take a
242:13 - look at the yellow file
242:16 - kind is the crunch up I'm going to name
242:20 - it hello Cron
242:22 - and we will give it a schedule
242:27 - only Stars which means that it will run
242:29 - every 60 seconds every minute that's the
242:33 - default
242:34 - and we're going to run a busy box image
242:38 - and we it will Echo this string
242:43 - okay
242:45 - so let's create our
242:49 - job using cubectl apply
242:53 - and we can get a list of the crown jobs
242:57 - using cubectl get print jobs
243:01 - okay so here we have the name the
243:04 - schedule if it's suspended it's if it's
243:07 - active the last last time it ran and we
243:12 - can get some information using cubectl
243:15 - describe run job and we pass the its
243:20 - name hello Dash Cron
243:25 - here
243:26 - this is super useful for troubleshooting
243:29 - the name the default name space it runs
243:32 - on in the default namespace
243:34 - we have the schedule here
243:37 - how many jobs does it keep in its
243:39 - history how many failed job is kept also
243:45 - the the command that will run
243:50 - oh
243:52 - you can get a list of the pods
243:55 - okay uh one per Ran So elocron with a
244:00 - unique uh value here so it has uh
244:04 - completed I'm gonna pause
244:07 - um to let it run a few times
244:11 - all right uh the job ran three times
244:15 - well it the last one is still uh running
244:18 - uh container creating Let me refresh
244:21 - that so it's completed okay now
244:25 - we can get the logs by using cubectl
244:30 - logs and then the
244:33 - job name
244:35 - dpod name so hello from the crown chart
244:39 - so by default the last three run of the
244:42 - job are kept in the history and you can
244:45 - configure that in the yaml file
244:51 - let's delete our crunch up here
244:58 - perfect and if again we type Cube CTL
245:03 - get pads uh all the parts in in the
245:07 - history basically are also deleted when
245:10 - you delete the crunch up
245:15 - [Music]
245:20 - let's take a look at rolling updates
245:23 - in a previous lecture we saw that using
245:26 - deployments you can set the number of
245:29 - paths instances using the replicas
245:32 - parameter
245:34 - and set a number of previous iterations
245:36 - of the deployment to keep in kubernetes
245:40 - we also saw that there are two update
245:42 - strategies running a date and recreate
245:47 - recreate is quite simple kubernetes will
245:50 - shut down all the running paths and
245:52 - create new ones after that
245:54 - running update will cycle through
245:56 - updating parts
245:58 - all right let's illustrate that using
246:01 - recreate all previous paths are deleted
246:06 - and the New Path will be created after
246:08 - that
246:09 - this means that there might be a small
246:11 - period of time where your microservice
246:13 - might not be responsive
246:17 - using the routing of this strategy
246:20 - a pod is deleted and replaced by a new
246:23 - one
246:24 - then the next one and so on
246:29 - there are two values that you can set to
246:32 - help you with this process
246:34 - Max search will tell kubernetes how many
246:38 - parts can be created over the desired
246:42 - number of pods
246:43 - let's say that you you want three
246:45 - instances in total
246:47 - setting Max search to 1 will allow
246:51 - creation of one additional pod on top of
246:54 - these three desired ones and this while
246:58 - the rolling update is running
247:01 - Max unavailable is the opposite
247:04 - is the maximum number of pulse that can
247:08 - be unavailable during the update process
247:13 - note that if you don't specify an update
247:16 - strategy in the deployment manifest
247:18 - communities will use a default strategy
247:21 - of running update with Max Surge and Max
247:24 - unavailable both set to 25 percent
247:31 - so let's say that we want three
247:33 - instances of a bot and we set max search
247:37 - and Max unavailable to 1.
247:40 - we're telling kubernetes that it's okay
247:42 - to create one additional part on top of
247:46 - the three desired one
247:48 - and that's it's okay to have one part
247:51 - less than the three desired one
247:56 - when done the previous replica set is
247:59 - kept
248:00 - you set how many you want to keep with
248:03 - the revision history limit property
248:07 - here's a cheat sheet for running updates
248:11 - you create of course your deployment
248:14 - using cubectl apply Dash F and the name
248:17 - of your yaml file
248:19 - you get the progress of the update using
248:22 - cubectl rollout status
248:26 - you get the history of the deployment
248:28 - using cubectl rollout history deployment
248:31 - and the deployment name
248:35 - you can roll back a deployment using
248:37 - qctl rollout undo and the deployment
248:41 - name that will roll back to the previous
248:45 - version
248:46 - or if you want to roll back to a
248:49 - specific revision number
248:51 - you add the two dash revision parameter
248:58 - and this concludes this look at running
249:01 - updates
249:04 - [Music]
249:09 - in this lab we will create a deployment
249:11 - and later on update it to a new version
249:14 - using a rolling update
249:17 - so let's take a look at the yaml file
249:21 - the kind of object we're using easy
249:24 - deployment call Lo Dash dab we want
249:28 - three replicas and we're using a rolling
249:31 - update strategy here and setting a Max
249:34 - search to 1 and Max unavailable to one
249:40 - we will deploy a container called Lo
249:43 - Dash app yeah hello app and uh that'll
249:47 - be version one and later on we will
249:50 - update it to version 2. okay
249:54 - so let's create our deployment here
249:59 - okay we can get a the deployment status
250:03 - by using Cube CTL rollout status
250:06 - and the deployment name
250:09 - uh the deployment was successfully
250:11 - rolled out let's take a look at our
250:15 - pods running
250:17 - okay so I have three instances perfect
250:20 - yellow depth uh three times excellent
250:26 - let's describe the uh the deployment so
250:30 - Cube CDL describe uh deploy and the
250:34 - deployment name
250:36 - and let's try to find if we can get some
250:40 - information about the strategy yeah here
250:44 - it is rolling update strategy Max
250:47 - unavailable oh wow the strategy type is
250:49 - right here uh rolling update
250:52 - running update strategy Max and
250:54 - available One Max search to one so you
250:59 - can get that information that we set
251:02 - earlier in the yml files okay
251:08 - let's now see if we have a replica set
251:15 - here's our replica set
251:19 - and let's Now update
251:24 - our yaml file and change the version of
251:26 - Hello app to version 2. so just update
251:30 - to 2 and save the file
251:35 - and we will use cubesatel apply and with
251:40 - the same yaml file
251:43 - now what I'm going to do
251:45 - how I'm using right now A K9s it's a
251:50 - terminal dashboard in a terminal sorry
251:53 - and to get the visual view of what's
251:58 - what will happen basically so right now
252:01 - I have three parts in green these are
252:04 - the ones that are deployed let's apply
252:08 - our new deployment I'm going to switch
252:13 - quickly to K9s and you can see oh it
252:17 - happens so fast
252:18 - but
252:19 - is you saw that the the new pods were
252:23 - were created and the old ones were uh
252:26 - were shut down
252:29 - all right
252:30 - here I can have a a deployment status
252:38 - everything's fine if not we would have
252:40 - uh some information if the deployment
252:42 - would take longer we would have some
252:45 - information printed here
252:47 - okay how many uh replica sets do we have
252:53 - we have two
252:55 - uh the the current one so that's the
252:59 - current one and the previous one
253:02 - by default uh three uh versions are kept
253:06 - uh in history
253:07 - we can get the deployment history by
253:10 - using cubectl rollout history and the
253:13 - deployment name
253:18 - okay version one for version two
253:22 - okay we're at version two and
253:28 - um we want to downgrade basically our
253:30 - roll back to the previous version so you
253:33 - can do
253:34 - that by using Cube CTL rollout undo and
253:38 - the deployment name so by default it
253:41 - will roll back to the previous version
253:44 - or if you have multiple versions and you
253:46 - know what version you want to roll back
253:49 - to you can specify the flag to revision
253:52 - and the revision number as we see here
253:58 - foreign
254:00 - let's do this one
254:02 - well either one will do the same thing
254:07 - okay let me switch to K9s and see what's
254:10 - happening oh the other one the the older
254:14 - while the version two is uh terminating
254:16 - new version is created
254:20 - and we can get a deployment status here
254:26 - everything successfully wrote
254:29 - okay
254:31 - Let's uh now take a look at our replica
254:35 - sets
254:37 - so we still have two
254:39 - right
254:40 - the current one is now
254:43 - the the first one that we uh that we
254:46 - deployed
254:49 - all right and we can do our cleanup by
254:53 - deleting the deployment using the yaml
254:56 - file
254:59 - [Music]
255:05 - let's take a look at blue green
255:07 - deployments
255:09 - so let's say that version 2 of our macro
255:12 - service contains some breaking changes
255:14 - like a different database schema
255:18 - what do you do
255:20 - using the rolling update strategy you'll
255:22 - have both version 1 and version 2 of
255:25 - your app running at the same time
255:27 - that might not work at all
255:31 - so using the blue green deployment
255:33 - pattern that might help solve that
255:36 - problem
255:37 - blue identify what's in production and
255:41 - green identify a new version currently
255:43 - deployed but not yet in production
255:48 - notice that the pulse label contains the
255:50 - version number
255:52 - when ready simply update the service
255:55 - definition to point to the new version
255:59 - and now green is now in production
256:03 - so green becomes blue and blue becomes
256:08 - green
256:10 - oh great this means that this pattern is
256:13 - solving the new database schema problem
256:15 - well not entirely you may still have to
256:19 - deal with some downtime while you update
256:22 - your database
256:24 - and also another drawback is that since
256:27 - both version of the macro services will
256:30 - be up and running at the same time
256:32 - you need to have enough free resources
256:34 - in your cluster to make this possible
256:39 - and this concludes this look at the blue
256:41 - green deployments pattern
256:46 - [Music]
256:51 - in this lab we'll create a blue green
256:53 - deployment
256:55 - we have three yaml files here so let's
256:57 - take a look at the lodep V1
257:02 - it's a deployment
257:04 - we want three replicas of a container of
257:09 - a an image called Hello app
257:12 - 1.0 and here we're setting a label
257:16 - of app lov1
257:20 - now let's take a look at the second one
257:24 - it's basically the same thing so
257:27 - deployment three replicas
257:30 - but it will use version 2 of our hello
257:35 - App application and we're setting the
257:38 - this label Here app to lov2
257:44 - all right
257:45 - let's take a look at our cluster IP
257:48 - manifest file so it kind of service and
257:52 - here's the selector app hello V1
257:57 - we will deploy that and later on we will
258:00 - change
258:01 - the cluster IP manifest to point to the
258:05 - newer version
258:07 - okay so let's deploy version one of our
258:12 - pods
258:15 - and let's also deploy
258:18 - our cluster IP service
258:22 - so let's take a look at the list of the
258:25 - paths currently running there are three
258:28 - pods and also there should be
258:32 - uh if I type Cube City I'll get SVC
258:37 - one cluster IP as we see front that's
258:41 - the one I just deployed perfect
258:44 - so let's do a quick port forward
258:48 - to uh to connect to our cluster IP
258:51 - service so we'll port forward uh the
258:54 - port 8080 that the cluster IP is
258:56 - listening to to localhost 8080.
259:00 - here cubesatel port forward service the
259:06 - name of our service 8080 28080 and let's
259:10 - hit local OS on port 8080 here
259:15 - okay
259:16 - so here hello world version
259:20 - 1.0 excellent day
259:24 - okay
259:26 - let's now deploy version two so uh right
259:29 - in my phenomenal I'll hit Ctrl C on my
259:33 - windows keyboard to break that and uh
259:36 - gain my terminal back perfect
259:40 - let's deploy
259:43 - version two
259:47 - okay and let's get a list of our pods
259:53 - currently running should be six
259:55 - so I have both versions uh in memory at
259:59 - the same time
260:00 - okay so that's one of the drawback of
260:02 - this uh this technique this blue green
260:04 - deployment technique
260:06 - okay let's now it did the cluster IP
260:10 - manifest file so we're we will change
260:13 - the selector to select app on Lo
260:18 - V2
260:19 - let's save the file
260:22 - and let's update our cluster IP service
260:27 - by using cubectl apply and the name of
260:31 - the yaml file
260:34 - yes
260:38 - okay
260:40 - Let's uh port forward again
260:46 - right let's hit that local OS again
260:50 - and there you go V2
260:55 - it worked
260:58 - let's do a little bit of her clean up uh
261:01 - let's delete our first deployment our
261:03 - second deployment and also decluster IP
261:06 - service
261:07 - you can select the three lines at the
261:09 - same time
261:14 - [Music]
261:19 - let's take a look at the concept of
261:21 - service in kubernetes
261:25 - first what problem do Services try to
261:29 - solve
261:31 - well if the pod in green need to reach
261:35 - the pod in purple
261:37 - you need to use its IP address
261:41 - the problem is that pass yet ephemeral
261:44 - IP addresses
261:46 - if the part in purple dies well you
261:49 - replace it
261:50 - and the new one will have a different IP
261:52 - address
261:54 - so we need a way to make these calls
261:57 - between pods a lot more robust
262:02 - so back to the service what exactly is
262:05 - the service well it's a kubernetes
262:07 - object that you define in a yaml file
262:11 - but on my pods
262:14 - that have ephemeral IP addresses
262:16 - Services get durable IP addresses and
262:20 - also they get DNS names
262:23 - they serve as ways to access Parts
262:27 - they're kind of a middleman
262:29 - and the target pods using selectors
262:35 - here we have four pods and a service
262:39 - the service select the pods that have
262:41 - the Zone label equals to prod and the
262:45 - version label equal to one
262:49 - the first part satisfies the selector
262:53 - the second one also
262:55 - but not the third one
262:58 - and not the last one
263:00 - so only the first two are selected
263:05 - so let's say we have two instances of a
263:08 - pod and we place a service in front of
263:11 - them
263:12 - if another part needs to reach these
263:14 - ones
263:15 - it will go through the surface
263:19 - and then the service will load balance
263:21 - the request to the instances
263:26 - in kubernetes we can use these Services
263:29 - the cluster IP the node port and the
263:33 - load balancer
263:35 - note that the cluster IP is the default
263:38 - service
263:39 - we will look at them in more details in
263:41 - the next lectures
263:44 - and this concludes this quick look at
263:46 - the concept of services in kubernetes
263:51 - [Music]
263:57 - let's take a look at the cluster IP
264:00 - so what is a cluster IP well it's the
264:04 - default service in kubernetes
264:07 - its visibility is cluster internal this
264:11 - means that it's not possible to use a
264:13 - cluster IP service to reach a macro
264:16 - service from the internet from outside
264:18 - the cluster
264:20 - in the cluster IP definition you can set
264:23 - two different ports
264:25 - Port is the port that the service will
264:27 - listen on
264:29 - and Target Port is the port that
264:32 - deselected pods are listening on so the
264:36 - cluster IP will route incoming traffic
264:39 - to that board
264:41 - in this CML file the service listens on
264:44 - Port 80 and Route the traffic to Port
264:47 - 8080.
264:49 - traffic is load balanced across the
264:52 - selected paths
264:54 - so when do you use a cluster IP service
264:57 - well to provide a durable way to
265:01 - communicate with pods but from inside
265:04 - the cluster
265:06 - so let's illustrate this
265:09 - here we have a cluster IP service
265:11 - fronting three instances of a pod
265:15 - it's impossible to reach it from outside
265:18 - the cluster but it's okay it's visible
265:21 - from inside the cluster
265:26 - this cluster IP will listen on Port ad
265:29 - and select the paths using these two
265:32 - labels
265:34 - since the selected pods are listening on
265:37 - port 8080 the service Target Port must
265:40 - also be set to 8080.
265:44 - this way the parts in green that want to
265:47 - communicate with the ones in purple well
265:50 - they go through the cluster IP service
265:52 - on Port 80 and the service route the
265:55 - traffic to Port 8080.
265:59 - so let's say you have multiple Marco
266:02 - Services uh you place a cluster IP in
266:06 - front of each of them
266:08 - because a cluster IP service IP address
266:11 - is durable
266:14 - while the pods ones are ephemeral
266:18 - here's a cheat sheet for cluster IP
266:21 - so the first two commands are imperative
266:24 - commands let's say you already have a
266:28 - pod running and you want to expose it
266:30 - using a cluster ID
266:32 - so you would use Cube CTL expose Oh
266:36 - short for path the Pod name specifying
266:38 - the the port and the Target Port and you
266:41 - can also give a name to your to your
266:44 - service
266:45 - if you have a deployment you can also
266:48 - use Cube CTL expose deploy the
266:51 - deployment name specifying the port and
266:54 - the target board so both commands are
266:57 - imperative commands
266:59 - if you have a yaml file you would use
267:03 - Cube CTL apply Dash F and specifying the
267:07 - EML file name
267:08 - you can get a list of the services
267:11 - running using cubectl get SVC
267:16 - I get a little bit more information
267:17 - specifying uh the flag Dash o and wide
267:23 - you can also describe the service using
267:25 - a cube CTL describe SVC and the service
267:28 - name
267:29 - and you can delete the cluster IP
267:32 - service using the yaml file with the
267:35 - cube CTL delete Dash F and the name of
267:38 - the yaml file or cubectl delete SVC and
267:42 - the name of the service
267:45 - and this concludes this look at the
267:48 - clusterity
267:51 - [Music]
267:56 - in this lab we will deploy an nginx
267:58 - container front it with a cluster IP
268:01 - service and then deploy a BusyBox
268:04 - container open a session on that BusyBox
268:07 - container and try to hit the web page
268:10 - served by the nginx container but
268:14 - through the cluster IP service all right
268:18 - let's take a look at our yaml file we'll
268:22 - start with the deployment one
268:25 - kind is a deployment we want three
268:28 - instances of the Pod and we will run the
268:35 - nginx image the Alpine version and it
268:38 - will list it on Port 80. now we're
268:41 - setting two labels here and the cluster
268:44 - IP service will you will select uh the
268:49 - pods using these two labels so app
268:51 - example environment prod all right let's
268:56 - take a look at our cluster IP
268:59 - kind service
269:02 - it's gonna listen on port 8080 and it
269:06 - will redirect the traffic to Port 80 on
269:11 - the nginx containers all right
269:15 - the selector is here so app example
269:18 - environment broad so that will select
269:21 - our pods uh in our deployment
269:25 - perfect
269:26 - so let's try to do that first let's uh
269:30 - deploy the service
269:36 - and let's deploy the
269:42 - nginx containers
269:47 - let's also deploy the busy box
269:52 - and we can take a look at this it's a
269:56 - kind of pod the name is my box and uh
270:01 - it'll run a busy box image okay
270:05 - so now let's get a list of our Bots
270:10 - currently running
270:12 - we should add four
270:15 - one two three four so the first three
270:18 - ones are the deployment the nginx images
270:22 - and the fourth one is the BusyBox
270:26 - all right
270:28 - let's try to connect to the BusyBox
270:32 - container open a session by using
270:35 - cubectl exec my box Dash it and the name
270:40 - of program we want to run perfect at
270:42 - work by type LS yep okay let's try to
270:46 - use the service to reach the nginx pods
270:51 - so wget
270:53 - and HTTP SVC example colon 880 so let's
271:00 - try to run that and see if it works
271:03 - okay that worked why did it work so what
271:07 - is that name here
271:10 - if we go back to the cluster IP
271:13 - definition
271:14 - that's the name of our service
271:17 - and it's listening on Port 8080.
271:23 - the service name Colin d uh the port
271:28 - that is listening on
271:31 - and that's it we can now exit our
271:34 - session on our busy box and we can
271:37 - delete
271:40 - our resources the cluster IP the
271:43 - deployment and the pod
271:45 - thank you
271:47 - [Music]
271:52 - let's take a look at the note Port
271:54 - service
271:56 - what is a note port a noteboard extends
272:00 - the cluster IP service and provides
272:02 - extra functionalities
272:05 - its visibility is internal like a
272:08 - cluster IP but also external to the
272:11 - cluster
272:13 - you can set a third port using the note
272:15 - Port property
272:17 - this is the port that the service will
272:19 - listen on outside the cluster
272:22 - note that the port must be in a range
272:25 - between 30 000 and 32 767.
272:30 - and if you don't specify a note Port
272:32 - value well kubernetes will assign one
272:35 - randomly
272:38 - you then set the port and the Target
272:40 - Port Properties like you do with a
272:43 - cluster IP
272:45 - this sounds like a good way to expose
272:48 - our macro services to external traffic
272:51 - but this range between 30 000 and 32
272:55 - 767 it's kind of annoying because you
272:59 - can't set it to let's say port 80.
273:03 - and One requirement for using note ports
273:06 - is that nodes must have public IP
273:09 - addresses
273:12 - to access your service simply specify
273:15 - any node Port IP address plus the note
273:19 - port
273:20 - and the traffic will be routed to the
273:23 - right note Port service inside the
273:26 - cluster
273:28 - the way it works is that you set the
273:31 - pods and the service just like you did
273:34 - before with the cluster IP but this time
273:36 - you also specify a port number in the
273:39 - note Port property
273:42 - external communication uses the node IP
273:45 - address and the port set with the node
273:47 - Port property
273:49 - internal communication uses the port set
273:52 - in the port property just like a cluster
273:54 - IP
273:57 - now let's take a look at our note Port
273:59 - cheat sheet
274:00 - if you already have a pod running in
274:03 - your cluster and you want to expose it
274:05 - using a note Port service simply use the
274:08 - cube CTL expose bow the Pod name
274:12 - specifying the port and the Target Port
274:15 - and the note Port as the type
274:19 - now you may wonder where do you specify
274:24 - the note port number
274:26 - well you can't there's no properties
274:30 - letting you set know that value between
274:33 - 30 000 and 32 767.
274:38 - so communities will assign one randomly
274:41 - for you
274:43 - same thing for a deployment let's say
274:45 - you have a deployment already running in
274:47 - your cluster and you want to expose it
274:49 - using a note Port you use cubectl expose
274:52 - deploy the deployment name
274:55 - Port Target Port type which is note port
274:59 - and you can specify your name also
275:04 - you can define a node port in a yaml
275:06 - file and deploy it using cubesatel apply
275:10 - Dash F the name of the yaml file you
275:12 - list the services using cubectl get SVC
275:16 - get more info adding Dash o wide
275:20 - he can describe your service using
275:22 - cubesatel describe SVC and the service
275:25 - name
275:26 - if you have a yaml file you can delete
275:28 - it using the that file using cubectl
275:31 - delete Dash F the name of the yaml file
275:33 - or you can delete your service using its
275:36 - name with qctl delete SVC and the
275:39 - service name
275:42 - and this concludes this look at the note
275:45 - Port service
275:48 - [Music]
275:53 - in this lab we will expose a deployment
275:56 - using a note Port service we have two
275:59 - yable files let's take a look at them
276:01 - the first one is for the deployment we
276:04 - will deploy an engine X image the Alpine
276:06 - version listening on Port 80.
276:09 - and we will need to replicas perfect
276:14 - let's take a look at the noteboard yaml
276:17 - file kind is service
276:20 - and the type is noteboard
276:23 - and the selector will select our
276:25 - deployment
276:27 - and here we set our node Port 32 410
276:31 - okay
276:34 - now let's open a terminal
276:39 - and we'll start by the deployment
276:42 - cubesatel apply
276:45 - and our yaml file next our service Cube
276:49 - CTL apply
276:51 - noteboard.yaml awesome let's make sure
276:55 - that our pods are running
276:59 - Cube CTL getbots Dash o wide
277:03 - awesome two parts two instance of our
277:06 - nginx container
277:08 - all right now since we're using Docker
277:11 - desktop the docker desktop node is
277:14 - mapped to local OS to reach the note
277:17 - Port service we need to use local OS
277:20 - plus the note port
277:23 - let's try that
277:25 - local Post
277:29 - 32 410
277:33 - and it worked awesome
277:36 - Now when using a cloud provider you
277:39 - would need to get a node IP address
277:41 - instead of using the Local Host
277:45 - you would get that IP address by using
277:48 - cubectl get nodes Dash o wide
277:52 - and here in the external IP address
277:55 - colon you would find the external IP
277:59 - address of the node
278:01 - awesome let's do our cleanup let's
278:04 - delete our note port and our deployment
278:12 - [Music]
278:18 - let's take a look at the concept of
278:20 - surfaces in kubernetes
278:23 - what problem do Services try to solve
278:25 - well if the pad in green need to reach
278:29 - the purple one
278:30 - it needs to use its IP address
278:33 - the problem is that pods are ephemeral
278:37 - if the purple one dies you need to
278:41 - replace it
278:42 - and the new one will have a different IP
278:45 - address
278:46 - we need a way to make these calls
278:48 - between pods more robust
278:53 - so what exactly is a service
278:56 - a service is a kubernetes object that
278:59 - you define in a yaml manifest
279:03 - unlike parts that have ephemeral IP
279:06 - addresses
279:07 - Services gets durable IP addresses and a
279:11 - DNS name
279:13 - they serve as a way to access paths and
279:16 - they target pods using selectors
279:20 - here we have four pods and a service
279:25 - the surface selects the paths that have
279:27 - the Zone label equals to prod and the
279:31 - version label equals to V1
279:35 - the first part satisfies the selector
279:39 - the second one also
279:41 - but not the third one and the last one
279:45 - only the first two are selected
279:51 - let's say that we have two instances of
279:53 - a pod and we place a service in front of
279:56 - them
279:57 - if another part needs to reach these
279:59 - parts it will go through the service and
280:04 - the surface will load balance the
280:06 - requests to these instances
280:10 - in kubernetes we can use these services
280:13 - D cluster IP the note Port the load
280:18 - balancer and the Ingress
280:21 - the cluster IP is the default service
280:24 - its visibility is internal only
280:28 - the note Port can expose a pod outside
280:30 - the cluster
280:32 - the load balancer and the Ingress are
280:35 - similar Services they let you expose
280:38 - applications outside of the cluster
280:41 - one operates at D layer 4 and the other
280:44 - at layer 7.
280:47 - L4 L7 what's that
280:50 - download balancer operates at the layer
280:53 - 4. that's the TCP transport level so
280:57 - that's very low in the transport stack
281:00 - it means that the load balancer can do
281:02 - simple operations like round robin
281:05 - routing
281:06 - the Ingress operates at the higher level
281:09 - in the transport stack
281:11 - think of protocols like HTTP or SMTP
281:16 - it's more intelligent so you can
281:18 - configure complex routing rules okay no
281:22 - worries if this sounds complex for now
281:25 - simply remember that an Ingress is like
281:28 - a load balancer but more intelligent
281:32 - and this concludes this look at the
281:34 - concept of services
281:39 - [Music]
281:44 - in this lab we will create a load
281:47 - balancer service
281:48 - but you may be may be asking yourself
281:51 - we're not using a cloud provider right
281:54 - now how can that work
281:57 - well Docker desktop is helping us it
282:00 - will emulate the load balancer service
282:03 - so we can test our load balancer locally
282:07 - awesome let's take a look at the
282:10 - application
282:11 - it's a simple deployment we want to
282:15 - replicas two instances of an nginx image
282:18 - super simple
282:20 - and the the load balancer yaml file
282:24 - kind is service the type is load
282:27 - balancer
282:28 - it will listen on port 8080 and redirect
282:31 - traffic to the Pod that is listening on
282:33 - Port 80.
282:34 - all right
282:36 - all right so let's open a terminal
282:38 - okay
282:43 - and let's deploy the app
282:49 - and the load balancer
282:54 - perfect let's make sure that our pod are
282:58 - running by using Cube CDL getbots
283:02 - yes I have two uh my coupons are here
283:05 - perfect now
283:07 - to get the IP address of the load
283:10 - balancer we use Cube CTL get SVC Dash o
283:14 - wide
283:17 - foreign
283:22 - S as the IP address
283:25 - using a cloud provider load balancer
283:28 - service you would find here instead of
283:31 - local OS the public IP address of the
283:35 - load balancer so let's test this open a
283:39 - browser and type localhost 8080
283:46 - and that works we reach our nginx pod
283:50 - awesome
283:51 - let's do our cleanup let's delete our
283:54 - load balancer
283:57 - and our application
284:04 - [Music]
284:09 - this is an introduction to the
284:11 - persistent Concepts in kubernetes
284:15 - we saw this slide earlier
284:17 - containers are ephemeris and stateless
284:19 - and any data stored in them is deleted
284:22 - when the container is destroyed
284:25 - so we need to find a way to store data
284:28 - outside the containers if you want to
284:30 - keep that data
284:33 - so volumes let containers store data
284:36 - into external storage systems
284:39 - these are storage services offered by
284:42 - the cloud providers
284:44 - Defenders create plugins according to a
284:48 - specification called the container
284:50 - storage interface
284:52 - and there are two ways to use storage in
284:54 - the cloud the static way and the dynamic
284:57 - way we have separate lectures on these
285:01 - later on
285:03 - all right the cloud providers create
285:04 - plugins
285:06 - to expose their storage Services as
285:09 - persistent volumes and storage class
285:13 - these two are kubernetes objects
285:17 - next we will look at the static and
285:20 - dynamic ways
285:24 - [Music]
285:29 - let's see how to use the static way
285:33 - persistent volumes or PV and persistent
285:37 - volume claims are PVCs
285:40 - are two kubernetes objects that lets you
285:43 - define and use external storage
285:48 - a purchasing volume represents a storage
285:51 - resource that is available cluster wide
285:54 - and is provisioned by the cluster
285:56 - administrator
285:58 - you then use a persistent volume claim
286:01 - to claim the persistent volume
286:05 - a part will then use the PVC to mount a
286:10 - local folder
286:13 - PVCs can be used by multiple parts and
286:17 - inside the parts all the containers are
286:20 - sharing the same volume
286:24 - there are many persistent volumes
286:27 - plugins available some are offered by
286:30 - the cloud providers
286:31 - the one highlighted in yellow called
286:34 - hostpath is a special one
286:37 - it's a plugin available with kubernetes
286:39 - that allow you to do local testing
286:42 - and it's not mapped to a cloud provider
286:45 - storage service
286:48 - it will not work in a multi-node cluster
286:51 - but it's super useful for local testing
286:57 - here's the main drawback of persistent
286:59 - volumes let's say that the cluster admin
287:03 - provision 100 gigabytes of storage
287:07 - and that the Pod only requires a small
287:10 - portion of this storage let's say just
287:13 - one gigabyte of that 100 gigabytes in
287:17 - total so just one gigabyte well
287:20 - too bad for the other pods because the
287:23 - volume is used exclusively by the Pod
287:26 - who has the claim on it
287:30 - this can be a waste of precious
287:32 - resources and we will see how storage
287:35 - class get around this problem in the
287:38 - next lecture
287:40 - okay in the meantime let's focus on the
287:43 - PV and the PVC
287:46 - you first select the cloud provider
287:48 - storage service you want to use
287:51 - then you create a persistent volume and
287:55 - set the required capacity let's say here
287:58 - 10 gigabytes
288:00 - you then create a PVC so a claim
288:04 - that refers to the persistent value
288:08 - and finally you use the PVC from your
288:12 - pod and mount a local folder on it
288:19 - there's an inputs and property that you
288:21 - must be aware of it's the reclaim policy
288:27 - set a delete all data will be lost when
288:31 - the claim on the volume is released
288:34 - and this is the default value so be
288:37 - aware of this
288:40 - if you want to keep your files when the
288:42 - PVC is released
288:44 - you have to set the reclaim policy to
288:46 - retain
288:48 - again the default value is delete so be
288:52 - careful and be aware of this
288:56 - there are three access modes possible
288:59 - using read write mini the volume can be
289:03 - mounted as read write by many parts
289:06 - using read-only mini the volume can be
289:09 - mounted read-only by many parts
289:13 - and finally with read write once the
289:16 - volume can be mounted as real read write
289:19 - Sorry by one single path and the other
289:23 - parts will be in read-only mode
289:27 - this might be useful if you have a main
289:30 - worker that writes data and the other
289:32 - pods simply read the data
289:37 - you define a person in volumes using the
289:41 - purchasing volume kind and you specify
289:44 - the capacity
289:45 - the access mode and the reclaim policy
289:48 - in the spec section
289:52 - in this example the ospad plugin is used
289:55 - to access local storage
289:58 - remember to only use hostpat for local
290:02 - testing and refer to the storage
290:04 - provider documentation and on how to
290:07 - create a persistent volume specific for
290:11 - their storage service
290:15 - you then Define a claim so a persistent
290:18 - value claim making sure that the access
290:20 - mode match the one set in the processing
290:25 - value
290:26 - in this case the claim is for 8
290:29 - gigabytes out of the possible 10
290:32 - gigabytes set on the persistent volume
290:35 - this means that no one can claim the
290:37 - remaining two gigabytes until the claim
290:40 - is released
290:44 - in the volume section of your pod
290:46 - definition
290:47 - simply refer to the PVC and mount a
290:51 - local folder on it
290:59 - a persistent volume can have these
291:02 - states
291:03 - available meaning that the resource is
291:06 - free and not currently in use
291:09 - bound the volume is bound to a claim so
291:13 - it's in use it's not available anymore
291:16 - release the claim has been deleted but
291:19 - the resource is not yet reclaimed by the
291:22 - cluster
291:23 - and finally failed well something's
291:26 - wrong
291:29 - here's a cheat sheet for the PV and PVC
291:32 - commands
291:33 - using a yaml file you can create either
291:37 - a PV or PVC by using cubectl apply Dash
291:40 - F and the name of the yaml file
291:43 - you get the install persistent volume
291:45 - using cubecti Cube CTL getpv
291:49 - the claims using cubectl get PVC you can
291:53 - describe them cubectl describe PV or PVC
291:57 - with their name you can delete them
291:59 - using their yaml file cubicle delete F
292:04 - and the name of the yaml file or by
292:07 - using their name so Cube CTL delete PV
292:10 - and the PV name or the PVC name
292:16 - and this concludes this section as the
292:19 - static way next we'll take a look at the
292:21 - dynamic way
292:24 - [Music]
292:29 - in this lab we will create a persistent
292:31 - volume a persistent volume claim and use
292:35 - a pod to mount a local folder on that
292:40 - storage
292:41 - we will create that in Docker desktop
292:45 - locally using the host path plugin all
292:49 - right so let's first take a look at the
292:51 - persistent volume yaml file
292:54 - the kind is purchasing value we give it
292:58 - a name pv001
293:00 - and a storage capacity 10 megabyte
293:05 - access mode read write once and we set
293:08 - the processing volume reclaim policy to
293:11 - retain
293:12 - and use that host Pat plugin here and to
293:17 - map to a folder in the docker desktop
293:20 - virtual machine to data here
293:24 - all right
293:25 - let's take a look at the PVC
293:28 - kind persistent volume claim we give it
293:32 - the name my claim read write access mode
293:36 - must be the same as the PV persistent
293:38 - volume so let's double check read write
293:42 - once
293:44 - on the PV read write once on the PVC
293:46 - awesome
293:48 - we request 10 megabytes of storage so we
293:51 - record the full capacity we could have
293:54 - chosen a lesser value if you want all
293:58 - right
293:59 - let's take a look at the Pod now
294:02 - so it's uh the kind of pod it's a busy
294:07 - box
294:08 - and uh we make a reference to the claim
294:12 - here in the volumes section we give it a
294:15 - name and we reference the persistent
294:19 - volume claim called my claim
294:23 - this guy here
294:25 - okay
294:28 - and we use that name mypd and we mount
294:33 - it to a local folder called demo we
294:37 - should see magically appear a folder
294:40 - called demo inside our BusyBox container
294:46 - okay let's deploy our persistent volume
294:51 - right
294:54 - percent in volume pv01 created awesome
294:58 - let's look at the PV
295:01 - qctl getpv
295:05 - name
295:06 - pv001 capacity 10 megabyte read write
295:10 - once reclaim policy to retain it's
295:13 - available it's not claimed
295:16 - uh awesome so let's now deploy the claim
295:21 - the PVC
295:25 - persistent volume claim my claim created
295:28 - awesome Cube CTL get PVC
295:34 - my claim it's bound to the volume called
295:38 - pv001 capacity 10 megabytes
295:41 - read write once and let's again take a
295:45 - look at the PV to see if something has
295:48 - changed
295:50 - yep it's now bound the status is bound
295:54 - to the claim called
295:57 - my claim running in the default name
296:00 - space
296:01 - awesome
296:05 - so let's now uh deploy our pod
296:12 - okay my PC box was created let's connect
296:16 - to it using Cube CTL exec the name of
296:20 - the instance so the Pod Dash it and the
296:23 - program you want to run
296:25 - so let's do NLS and see if we see a demo
296:29 - folder and there it is we have our demo
296:34 - folder
296:36 - Let's uh CD into that folder and let's
296:39 - create a a file inside
296:44 - cat and we'll pipe that to uh lo.txt
296:48 - let's type hello world world
296:54 - uh if I can type
296:57 - Ctrl D to exit and save the file Let's
297:01 - do an LS to see if the file was created
297:04 - perfect
297:05 - okay
297:07 - let's exit this session
297:11 - and now let's delete the pod
297:15 - let's delete the part by using cubectl
297:17 - delete Dash F pod and since the busy box
297:20 - takes uh 30 seconds to shut down we will
297:24 - force it to do it right away we don't
297:28 - want to wait
297:29 - okay
297:31 - Let's uh deploy it again well
297:35 - Cube CTL
297:39 - rctl get the pods no resource awesome
297:45 - it's really dead
297:47 - Let's uh deploy it again
297:51 - okay
297:52 - Let's uh open a session
297:57 - CD demo LS all right and let's get that
298:02 - file
298:06 - hello a car cat cat nut car
298:12 - hello world awesome it worked
298:16 - Let's uh exit our session
298:21 - let's now do our cleanup we'll delete
298:24 - our bud
298:27 - and then we will delete the PVC
298:35 - right
298:38 - and then the PV
298:44 - so you can't delete the PV before the
298:47 - PVC well you can issue the command but
298:50 - the the command will be in kind of a
298:53 - weight State uh until uh the PVC has
298:57 - been released
299:02 - [Music]
299:07 - let's continue our journey into
299:09 - persistence by looking at the dynamic
299:12 - way
299:13 - so here's a new object the storage class
299:16 - and the search class represent a storage
299:19 - resource that is available cluster wide
299:21 - and is provisioned by the cluster
299:23 - administrator
299:25 - you don't have to set a capacity
299:28 - and it eliminates the need for the admin
299:30 - to pre-provision a persistent value
299:35 - now compared with processing volumes
299:38 - where once a claim has been made
299:41 - the remaining capacity becomes
299:43 - unavailable
299:45 - well the storage class can support many
299:48 - claims many persistent volume claims
299:53 - so you first select the cloud provider
299:56 - storage service that you want to use
300:00 - you create a storage class
300:02 - so here no need to specify a capacity
300:06 - then you create a PVC the claim that
300:10 - refers to the storage class and now you
300:14 - specify the required capacity
300:17 - and finally you use the PVC in your pod
300:21 - and mount a local folder
300:27 - like a persistent volume there's an
300:30 - important property that you must be
300:32 - aware of it's the reclaim policy set at
300:36 - delete all data will be lost when the
300:39 - claim is released and it's the default
300:43 - value also like the persistent volume so
300:47 - be aware
300:48 - if you want to keep your files when the
300:51 - PVC is released you have to set the
300:53 - reclaim policy to retain again the
300:56 - default value is delete
301:00 - again three access modes possible and
301:03 - they are set using the PVC not the
301:06 - storage class
301:08 - read write mini the volume can be
301:10 - mounted as read write by many pods read
301:14 - only mini the volume can be mounted
301:17 - read-only by many pods
301:19 - and lastly read write once the volume
301:23 - can be mounted as read write by a single
301:26 - part and the other parts will be in
301:29 - read-only mode
301:31 - useful if you have a main worker that
301:33 - writes data and the other pods simply
301:35 - read the data
301:40 - so
301:41 - you first start defining a storage class
301:45 - specifying the cloud provider driver
301:48 - with the provisioner property and
301:52 - additional settings in the parameters
301:54 - section
301:55 - so refer to the source provider
301:57 - documentation on how to create a
302:00 - specific storage class further storage
302:02 - service
302:05 - you then Define a PVC specifying an
302:09 - access mode
302:11 - and the source capacity required
302:14 - in this claim uh the claim is for 5
302:18 - gigabytes
302:19 - but more PVCs can be created over that
302:23 - storage class
302:26 - then simply refer to the PVC in your
302:30 - path definition and map a local folder
302:33 - on it
302:36 - in summary the main benefits of a
302:38 - storage class versus a purchasing volume
302:40 - is that with a storage class you don't
302:43 - have to define a capacity
302:45 - and multiple Claims can be made
302:50 - here's a cheat sheet for storage class
302:52 - at Men's so you create your storage
302:56 - class using a yaml file using Cube CTL
303:00 - apply Dash F and your the name of your
303:02 - yaml file
303:03 - you get a list of your storage classes
303:06 - or PVCs using get the SC for search
303:11 - class and get PVC
303:14 - you get the search as information by
303:17 - using cubectl described as C and the
303:20 - class name
303:21 - you delete your search class and PVC
303:24 - using cubectl delete Dash F and the yaml
303:27 - file name or you delete your search
303:31 - class using delete SC and the class name
303:34 - or delete PVC and the PVC name
303:40 - and this concludes this section about
303:44 - persistence using the dynamic way
303:49 - [Music]
303:54 - let's see how to store configuration
303:56 - values using config Maps
304:00 - in a previous lecture we saw that it was
304:03 - possible to place configuration values
304:05 - directly in the environment section of a
304:08 - pod definition
304:11 - but what if we need to change that value
304:15 - well we have to edit the Manifest and
304:19 - redeploy the container
304:22 - also usually it's not a best practice to
304:26 - tie an object with its configuration so
304:29 - how can we externalize these values
304:33 - the config map object allow you to
304:37 - decouple and externalize configuration
304:40 - values
304:41 - the key value pairs are then injected in
304:44 - the containers as environment variables
304:48 - they can be created from yaml files
304:52 - a series of text files or even folders
304:56 - containing files
304:59 - they are static meaning that if you
305:01 - change the value the containers that
305:05 - refer to these values have to be
305:07 - restarted to get these refresh values
305:12 - using a yaml file you define a config
305:16 - map and place the key value pairs in the
305:19 - data section
305:22 - you can even specify multi-line values
305:24 - using the pipe character
305:28 - in the EnV section environment section
305:31 - of the container definition
305:33 - you define an environment variable and
305:37 - by using value from an config map key
305:41 - ref
305:42 - you reference the config back name and
305:46 - the key as defined in the config map
305:52 - so in the config map key ref section
305:55 - name refers to the config map name and
305:59 - key refers to a key in the config map
306:05 - earlier I mentioned that this is a
306:07 - static process meaning that the values
306:10 - are injected when kubernetes starts the
306:13 - container
306:14 - this means that if you make a change to
306:18 - a config map value
306:20 - inside the container the original values
306:23 - stay the same until you restart the
306:26 - container
306:28 - to get around this you can map a volume
306:31 - on a config map Yes you heard it right
306:35 - mounting a volume
306:37 - this solves the static issue and updates
306:40 - are reflected in containers
306:43 - each key value pair is seen as a file in
306:48 - the mounted directory
306:50 - so we start with a config map
306:53 - then use a volume to mount it to a local
306:58 - folder inside our pod our container
307:02 - the result is that all key value pairs
307:05 - are now scenes as file
307:08 - the name of the file being the key
307:11 - and the value being inserted in the file
307:16 - while this sounds cool it also means
307:19 - that you'll have to refactor your code
307:21 - so instead of reading environment
307:23 - variables
307:24 - you'll have to read files
307:26 - so is it worth it you'll have to figure
307:29 - out that by yourself
307:32 - here's the config Maps cheat sheet so if
307:37 - you're adventurous you can create a
307:40 - config map from the command line that's
307:42 - the imperative Way by using Cube CTL
307:45 - create config map you give it a name and
307:48 - with the from Dash literal parameter you
307:51 - specify the key value pairs so you can
307:53 - specify multiple key value pairs of the
307:56 - same line
307:57 - or you can use a good old yaml file and
308:01 - use cubectl apply Dash F and the name of
308:04 - your yaml file you can create a config
308:07 - map using cubectl Create CM specify your
308:10 - name and specifying a the name of a text
308:16 - file containing multiple key value pairs
308:21 - also you can create a config maps from a
308:25 - folder so if you have multiple files
308:29 - inside your folders you can create a
308:31 - config map from that
308:33 - you can get a list of the config maps by
308:36 - using cubectl getcm
308:39 - you can output the config map in a yaml
308:43 - file by using cubectl get CM the name of
308:46 - the config map and the dash o parameter
308:49 - with the yaml and you can pipe that to
308:53 - to a file name
308:55 - of course you delete a config map by
308:58 - using its yaml file using cubectl delete
309:02 - Dash F and the name of the yaml file
309:08 - and this concludes this uh look at
309:12 - config Maps
309:16 - [Music]
309:21 - in this app we're going to create a
309:23 - config map and use a pod that will
309:25 - reference a value stored in that config
309:29 - map
309:30 - let's take a look at the config map
309:34 - so the kind is config map
309:37 - we have a name
309:38 - cm-example and in the data section we
309:43 - have two key value pairs date as set to
309:47 - Michigan and City and our board all
309:50 - right
309:51 - so let's now take a look at our pod
309:55 - tiny spot
309:57 - uh it's a busy box
310:01 - and here in the environment section
310:05 - we declare in environment variable that
310:09 - we will call City
310:12 - and we are getting the value from
310:17 - a config map key ref
310:20 - right and we specified the name of the
310:24 - config map so cm-example
310:27 - that's the name of the config map
310:32 - and the key is City
310:36 - so here
310:37 - that's the key right there
310:40 - okay
310:43 - again environment section
310:46 - we Define a new environment variable
310:49 - that we will call City we get the value
310:52 - from config map key ref
310:55 - specifying the name of the config map
310:57 - and the key
311:00 - awesome
311:02 - let's create our config map
311:09 - okay Cube CDL gets cm to get information
311:13 - about our config map
311:16 - so CM example
311:18 - two data
311:21 - okay that doesn't give us much
311:24 - information well about the data itself
311:27 - so let's do a cube Studio describe
311:30 - config map
311:31 - CM example
311:36 - okay
311:38 - we have the name the namespace where it
311:42 - is any labels annotations and here we
311:46 - have uh the data section City and
311:49 - Airport State Michigan
311:53 - and if for some reason you want to
311:56 - Output that as a yamo uh Cube CDL get
312:00 - config map the name of the config map
312:03 - Dash o output in yaml
312:07 - uh to recreate the config map using this
312:16 - let's now deploy the pod
312:18 - or BusyBox
312:22 - perfect let's open a session
312:26 - Cube CDL exec
312:28 - my box
312:29 - Dash it and the program you want to run
312:33 - and let's display the city environment
312:37 - variable let's Echo that Echo dollar
312:40 - sign City and there it is an arbor so
312:44 - that worked foreign
312:46 - exit
312:51 - and we can do a little bit of cleanup we
312:55 - can delete our config map
312:59 - and we can delete our busy box spot
313:06 - [Music]
313:11 - let's see how to use the secrets object
313:15 - in kubernetes you will find many times
313:18 - of Secrets types
313:20 - the default one is the OPAC type
313:24 - and it is very similar to the config
313:27 - Maps object that we saw in the previous
313:29 - lecture
313:31 - you can also store credentials to
313:34 - connect to private container registries
313:37 - authentication secrets
313:40 - and even certificates
313:43 - in this lecture we will focus on the
313:45 - OPEC Secret
313:49 - like config Maps secrets are used to
313:51 - store configuration values so they are
313:54 - somewhat identical to config Maps except
313:57 - that these store values as base64
314:00 - encoded strings
314:03 - and it's important to understand that
314:05 - base64 is a way to encode strings
314:08 - and it is not an encryption algorithm
314:12 - this means that Secrets stored in
314:15 - kubernetes can be decoded quite easily
314:19 - yeah great since these secrets are Noah
314:23 - very secret should you use them
314:26 - well the answer depends on the type of
314:29 - information you want to store
314:31 - it might be okay to store a connection
314:33 - to a database but it might not be for
314:36 - something more sensitive
314:39 - you can protect Secrets using role-based
314:43 - access control policies are back
314:46 - or you can store them elsewhere all
314:49 - Cloud providers offered ways to store
314:51 - secrets in Seeker Vault services
314:55 - that you can retrieve firm kubernetes
314:58 - you can also use a third party tool like
315:00 - the very popular Vault product from
315:02 - archicup just be aware that the
315:06 - kubernetes default secret the OPEC one
315:09 - is not encrypted by default in
315:12 - kubernetes
315:15 - so you can define a secret in a manifest
315:18 - and use a base64 encoded strings as the
315:22 - values
315:24 - or use the command line where you can
315:27 - use plain text strings
315:30 - easier
315:33 - in the path definition you simply get
315:36 - the secret value using the secret key
315:38 - ref section
315:40 - this is very similar to config Maps
315:44 - and again similar to config maps you can
315:46 - mount a volume on top of Secrets
315:53 - here's the container registry secret and
315:57 - you can Define it using a yaml file or
315:59 - with the CLI
316:01 - next in the path definition you
316:04 - reference the credentials in the image
316:07 - pull Secrets section
316:11 - here's a cheat sheet for Secrets
316:13 - commands
316:15 - so you can create a secret the
316:17 - imperative way at the common line if you
316:20 - want using cubesdl Create secret generic
316:23 - and then the secret name and you pass
316:26 - the key value pairs with the from Dash
316:29 - literal per meter you can of course
316:32 - create one using a yaml file so cubesdl
316:36 - apply Dash F and the name of the yaml
316:38 - file you can get a list of the secrets
316:41 - using cubectl gets secrets
316:44 - you can output the secret to a yaml file
316:50 - by using cubectl get Secrets the secret
316:53 - name and the dash o yaml parameter and
316:58 - you can pipe that to a a file you can
317:01 - delete the secret using a yaml file or
317:05 - using the secret name using cubectl
317:08 - delete secrets and with the secret name
317:13 - and this concludes this look at secrets
317:19 - [Music]
317:24 - in this lab we'll create a secret and
317:27 - from a pod we will reference secret and
317:31 - use them as environment variable
317:34 - let's take a look at our secrets at yaml
317:37 - file
317:38 - the type or the kind is secret we give
317:42 - it a name secrets and in the data
317:45 - section we have key value pairs so
317:47 - username to some value and password to
317:51 - some value notice that the
317:53 - values but must be base64 encoded you
317:59 - cannot put a non-encoded string here
318:02 - how do you do that on Windows base64 the
318:07 - the tool is not installed by default so
318:10 - you can use these two websites basics of
318:13 - foreign code.org decode.org
318:16 - or you can install base64 using
318:19 - chocolati as shoko installed base64 on
318:22 - macro Linux well it's already installed
318:25 - so you simply do something like that you
318:28 - Echo your string and you pipe that to
318:33 - base 64.
318:36 - and that will encode the string and let
318:40 - me copy that
318:42 - and to decode Echo
318:46 - uh the encoder string and you pipe that
318:49 - again to base64
318:53 - Dash D for decode and voila
318:58 - all right
319:05 - from the Pod now let's take a look at
319:09 - our pod yaml definition
319:11 - kindness part it's a busy box and here
319:17 - in the environment variable we're
319:19 - creating two environment variable the
319:22 - first one is called username and it gets
319:25 - its value from a secret key ref
319:30 - and the name references the secret name
319:35 - here
319:37 - and the value well the key is one of
319:41 - these two
319:42 - so username or password Here
319:47 - a case key reference here the username
319:50 - from the secrets Secret
319:53 - and second one is password a the
319:56 - environment variable is called password
319:57 - and we get the value from the secrets
320:00 - secret and the key is password awesome
320:06 - let's create the secret
320:13 - all right let's get a list of the
320:17 - secrets
320:20 - here's our secret if it has two data it
320:24 - was created six seconds ago we can
320:27 - describe it cubicity of describe the
320:31 - object type and then it name
320:35 - and here what do we have the name
320:40 - namespace and well we don't see the
320:44 - secret just the the keys here
320:49 - and what if we use Cube CTL get secrets
320:56 - and output that to demo
321:00 - the
321:04 - secret data password say
321:09 - uh
321:11 - doing a describe would not allow us to
321:14 - see the values but using get secret and
321:18 - outputting that to a yaml will allow us
321:21 - to retrieve the actual values
321:26 - okay
321:27 - let's now deploy our busy box
321:34 - right let's open a session
321:40 - and let's Echo uh username
321:45 - D username and the password
321:52 - whoops Echo
321:55 - uh
321:57 - password
321:59 - my password so that that works
322:03 - the uh the values the secrets are
322:06 - decoded when they're injected into into
322:10 - the pods
322:12 - let's exit that and let's delete our
322:16 - secret
322:19 - and let's delete our busy box
322:25 - [Music]
322:30 - let's talk about observability
322:33 - if you deploy a container using a
322:36 - deployment and it crashes kubernetes
322:38 - will create a brand new instance of the
322:41 - pod
322:42 - this works because kubernetes monitors
322:45 - the infrastructure
322:46 - but what about your application
322:49 - if your app crashes well kubernetes will
322:52 - look at the Pod health and see that it's
322:55 - still running
322:56 - so from the infrastructure point of view
322:59 - everything is working fine your bud is
323:02 - still up and running but your code
323:04 - inside the Pod has crashed
323:08 - would it be nice if kubernetes could
323:10 - monitor the application health
323:13 - well you can achieve this by configuring
323:15 - probes
323:18 - the startup probe informs a kubernetes
323:21 - that the container has started and it's
323:22 - now okay to send traffic to it
323:25 - the Readiness probe enforced kubernetes
323:28 - that the container is now ready to
323:30 - accept traffic
323:32 - let's say that when your app starts it
323:35 - needs to execute a series of steps like
323:38 - getting some configuration values from a
323:40 - database creating some files and so on
323:43 - and so on and let's say that this
323:46 - startup sequence takes around 30 seconds
323:49 - so even if your container is up and
323:52 - running your code is not ready to accept
323:55 - traffic
323:56 - so using a Readiness probe you tell
323:59 - kubernetes to wait 30 seconds before
324:01 - starting to send traffic
324:05 - lastly the liveness probes tell
324:07 - kubernetes if your app is still running
324:10 - and if not kubernetes will kill the Pod
324:13 - and replace it with a brand new one
324:18 - here's a part definition with the three
324:20 - possible probes
324:22 - the starter probe tell kubernetes to
324:25 - wait for 10 seconds before making an
324:27 - HTTP call to a page called health
324:29 - the failure threshold tells kubernetes
324:32 - to try three times
324:36 - the Readiness probe tell kubernetes to
324:39 - wait initially five seconds before
324:40 - probing and making a TCP call on port
324:44 - 8080 and then check every 10 seconds
324:50 - the liveness probe tell kubernetes to
324:52 - wait initially for 15 seconds before
324:54 - probing by making a TCP call on port
324:57 - 8080 and then check every 20 seconds
325:01 - note that the Readiness probe will run
325:04 - during the whole pod life cycle
325:07 - but will these two Conflict at some
325:10 - point
325:12 - yes and no they will run simultaneously
325:15 - but a fail probe will result in
325:18 - different actions from kubernetes
325:21 - failing a Readiness probe will tell
325:24 - kubernetes to stop sending traffic to
325:26 - the pod
325:28 - but the Pod is still alive right
325:30 - while failing a liveness probe will tell
325:34 - kubernetes to restart the pod
325:40 - how does kubernetes probe the containers
325:43 - the cubelet will do the probing using
325:45 - the method you configure
325:48 - with exec action kubernetes will run a
325:50 - command inside a container
325:53 - with TCP socket action kubernetes check
325:56 - if a TCP socket port is open
325:59 - and with HTTP get action kubernetes
326:02 - perform nhdp get
326:06 - here's an exec action you're telling
326:09 - kubernetes to run a cat command on a
326:12 - file called healthy in the TMP folder
326:18 - here's a TCP socket action
326:21 - you're telling kubernetes to check if
326:23 - there's an open socket on port 8080
326:28 - and finally an HTTP get action you're
326:31 - telling kubernetes to do an HTTP get on
326:34 - the Health page on Port 8080.
326:39 - and this concludes this look at
326:43 - observability
326:46 - [Music]
326:50 - in this lab we will set a liveness probe
326:54 - so let's take a look at our yaml file
326:58 - we will deploy a pod that we'll call
327:01 - liveness Dash example is going to be a
327:04 - PC box
327:06 - and this is where we're setting the
327:09 - probe
327:10 - type is liveness Pro
327:13 - we're asking to do an exacto so to run a
327:17 - command inside our container
327:21 - and the command is cat
327:24 - and the parameter is that file so under
327:26 - the TMP folder the file is called
327:30 - healthy no extension
327:32 - basically we're asking kubernetes
327:35 - to run that command if that command is
327:40 - successful if the file exists well the
327:44 - probe is successful if the file doesn't
327:47 - exist the probe will fail okay
327:52 - uh initial delay seconds we're asking
327:55 - kubernetes to wait for five seconds
327:56 - before starting to probe and then period
327:59 - seconds to five we're asking kubernetes
328:02 - to probe every five seconds
328:06 - the last parameter is failure threshold
328:09 - set to two basically we're telling
328:11 - kubernetes that the liveness probe will
328:14 - fail when two probes will fail
328:19 - now for uh the purpose of this lab we
328:24 - will set something a little bit funky uh
328:28 - just so we're able to do this test
328:31 - we're um we're running a command when
328:35 - the container starts uh it's dutch and
328:38 - basically this will create that healthy
328:41 - file
328:42 - and then we're telling the container to
328:46 - wait for 15 seconds
328:49 - and then to delete that file
328:53 - right and just a little trick too so
328:56 - we'll be able to to quickly see the
328:58 - effect of the lavenous probe
329:01 - okay
329:03 - Let's uh deploy our pod
329:12 - and let's quickly do a cube CTL describe
329:16 - pod
329:19 - all right successfully pull the image
329:22 - the container was created
329:25 - okay so let's
329:27 - do that again a few times and we'll see
329:30 - what happens
329:31 - just rerun the command out
329:34 - liveness profile cat can't open okay two
329:38 - times over five seconds
329:40 - and now kubernetes is killing the Pod
329:43 - awesome
329:46 - and uh
329:53 - is still killing
330:00 - so you see that here the the Pod was in
330:04 - an unhealthy State here
330:06 - so now it's in the killing mode oh
330:10 - and then uh kubernetes is pulling again
330:14 - the BusyBox image and starting a new one
330:19 - right
330:23 - and then
330:25 - it's on LT again
330:27 - and the process starts uh starts again
330:33 - all right let's do our cleanup let's
330:36 - delete our pod and we'll force it
330:41 - you don't want to wait for it to end
330:43 - perfect
330:47 - [Music]
330:52 - let's take a look at some dashboards
330:56 - while it's fine to use a terminal to get
330:59 - a view of your cluster You may wish to
331:02 - use a graphical user interface instead
331:05 - luckily there are many options available
331:07 - we will take a look at these three
331:09 - popular and free options the kubernetes
331:13 - dashboard the lens desktop application
331:16 - and K9s a dashboard that runs in the
331:20 - terminal
331:21 - let's start with the kubernetes
331:23 - dashboard
331:24 - it's a web UI that you can install as an
331:28 - add-on inside your cluster
331:31 - it's not installed by default by Docker
331:34 - desktop and also by most Cloud providers
331:38 - the rule of thumb is if you don't need
331:40 - it don't install it
331:43 - a because it runs inside the cluster so
331:46 - you need to find a way to expose it over
331:48 - the internet
331:49 - and B well because of that it's a known
331:53 - Vector of attack
331:55 - that being said the kubernetes dashboard
331:58 - that you see the various resources
332:00 - deployed inside your cluster
332:02 - simply select the type in the left menu
332:06 - and you can also edit them by clicking
332:09 - on the edit icon
332:11 - and you can edit the yaml file and click
332:14 - on update to change the Manifest file
332:20 - lens is a IDE that runs locally on Mac
332:25 - windows and Linux so you need to install
332:28 - it on your OS
332:30 - on top of viewing the resources and
332:32 - editing the yaml Manifest
332:34 - you can also use a built-in editor and
332:37 - also a built-in terminal
332:40 - lens is maintained by mirantis and you
332:44 - can download it from this URL
332:48 - here's the overview dashboard that lets
332:51 - you see a resource count
332:54 - like the kubernetes dashboard you can
332:57 - select a resource click on the edit
333:00 - button
333:01 - and it did the Manifest directly in lens
333:07 - you can also type commands using the
333:10 - built-in terminal
333:14 - K9s is a cool text dashboard that runs
333:18 - in a terminal
333:20 - and you can install it on Windows Mac
333:23 - and Linux
333:26 - it might sound strange to run the
333:28 - dashboard in a terminal but this makes a
333:31 - lot of sense Kina NS is super light
333:34 - starts in an instant and gives you a
333:38 - clean view of all your resources
333:40 - you can get the information about the
333:43 - cluster the resources and you can take a
333:48 - or make a series of action like deleting
333:51 - a resource viewing the logs
333:54 - here we have a list of the pods
333:56 - currently running in the default
333:58 - namespace
334:00 - want to view the services that are
334:02 - currently running simply type
334:04 - colon
334:06 - and type SVC to list the objects type
334:12 - foreign pressing s while a part is
334:16 - selected will open a shell
334:20 - you can also set a port forward by
334:23 - typing shift f
334:26 - you can even see the part logs
334:30 - I really like this K9s dashboard in a
334:33 - terminal and use it all the time
334:37 - and this concludes this look at the
334:39 - kubernetes dashboards
334:42 - thank you
334:44 - [Music]
334:49 - in this lab we'll take a look at Lens so
334:52 - lens is a free dashboard that you can
334:54 - install on Windows Mac and Linux here's
334:57 - the URL
334:59 - k8s
335:00 - lenss.dev let's take a look at the
335:03 - website
335:04 - from there you can install it it runs on
335:09 - Mac windows and Linux so you can
335:12 - download the setup files here
335:15 - or if you're using a security on Windows
335:18 - you can use shoko install lens or Brew
335:22 - on Mac and Brew install cast lens
335:26 - all right
335:28 - I've already launched a lens here and by
335:33 - default it should take you to your
335:34 - Docker desktop cluster if not or if you
335:39 - want to select another cluster click on
335:42 - the hamburger menu to the left and
335:46 - select add cluster
335:49 - Lance will look at the cube config file
335:54 - and you'll be able to select your
335:58 - clusters from that drop down list so
336:01 - here I have my demo cluster in the cloud
336:04 - so I can select that
336:06 - and click on ADD cluster
336:12 - I can be connected to multiple clusters
336:16 - here's my Docker desktop here's my demo
336:20 - so let me switch back to dockerdist up
336:24 - here
336:25 - okay
336:30 - so first thing first uh let's deploy
336:33 - something on our cluster
336:34 - here I have a yaml file
336:36 - it's a deployment
336:39 - we'll have three replicas of an image
336:43 - called Hello app okay nothing fancy here
336:50 - let's deploy
336:53 - this all right and let me switch back to
336:58 - uh lens
337:00 - here
337:04 - now let's click here on workloads
337:08 - and overview
337:10 - so we should see three parts running
337:14 - here one deployment and one replica set
337:19 - I can either from the top menu select
337:22 - path or from this workload menu select
337:27 - paths here and the other type of objects
337:31 - here
337:32 - so here are my three parts
337:35 - I have one deployment and one replica
337:39 - set if I click on an object I get more
337:44 - information
337:45 - labels annotations and so on and so on
337:48 - so
337:49 - something that you would have at the
337:52 - terminal at the common line by typing
337:54 - Cube CTL describe uh replica set or
337:59 - deployment or pod and the object name
338:02 - here
338:03 - but here is presented in a nice UI
338:09 - all right
338:11 - let's take a look at our pods here okay
338:14 - let's see what we can do let's say
338:17 - we want to delete this one so I can
338:21 - select it like this and from the Ellis
338:24 - menu to the right I can open that and
338:28 - select remove
338:30 - right or I can remove it
338:33 - right there
338:35 - okay let's do that
338:37 - remove item hello yep
338:40 - remove it and since it's a deployment uh
338:44 - kubernetes will create a new One
338:47 - automatically
338:50 - pretty cool uh let's uh take a look at
338:53 - the logs
338:56 - we can see the logs from there we can
339:00 - open a shell
339:06 - so at last
339:09 - a pretty cool front directly from that
339:11 - UI if I click on edit
339:14 - well I can edit the Manifest file
339:19 - and click on Save and close that will
339:22 - update that object
339:26 - let me cancel that
339:29 - close close this there's a built-in
339:32 - terminal so by default you have a small
339:35 - icon here called terminal if you click
339:39 - on it if you click on this open button
339:42 - that will open the terminal so I can
339:46 - type command Cube C Cube CTL get the
339:53 - pods
339:54 - right
339:58 - you can try to delete an object directly
340:01 - here so let me copy this object name
340:07 - Cube CTL delete uh
340:12 - and let's paste that
340:15 - we should see right right away we see uh
340:18 - something happening uh at the top of the
340:21 - screen
340:27 - all right
340:28 - uh what's cool is that if you have more
340:30 - than one cluster you can switch between
340:31 - these clusters so here let me uh just to
340:35 - prove it
340:37 - clear and the
340:40 - qctl uh get the nodes
340:45 - okay on Docker desktop I have only one
340:47 - node let me select My Demo cluster
340:51 - my terminology open it
340:55 - there's no deployment yet but let's just
340:59 - do a cube CTL
341:02 - get nodes
341:05 - and for sure I have three nodes in this
341:08 - in this cluster
341:09 - that's pretty cool
341:12 - just by selecting the cluster I can
341:15 - switch and I have a terminal that is in
341:17 - the right context
341:19 - pretty cool
341:22 - so I just scratched the surface there's
341:25 - a lot more information that we can get
341:27 - like a configuration the config Maps the
341:30 - secrets
341:33 - what the network services that are
341:37 - installed here I have a cluster IP
341:39 - service
341:42 - storage so the persistent volume claims
341:45 - persistent volume storage classes so
341:48 - here in my cluster in the cloud I have
341:51 - four search classes defined for me the
341:55 - namespaces and so on and so on all the
341:59 - information that you would get from the
342:02 - command line using cubectl you get the
342:05 - same information but from a nice UI
342:13 - so once you're done exploring don't
342:15 - forget to delete your deployment
342:19 - thank you
342:21 - [Music]
342:26 - in this lab we will use K9s a super
342:29 - great dashboard running inside a
342:32 - terminal so you can get more information
342:34 - about the K9s at the website
342:39 - k9scli.io
342:41 - here's the website nice logo
342:47 - and you can look at the documentation
342:50 - and the how to install K9s from that
342:55 - website if you're on Windows
342:59 - you can install it using chocolaty so
343:02 - Chico install K9s on Mac OS Brew install
343:05 - K9s and on Linux take a look at the
343:08 - documentation
343:10 - all right
343:12 - Let's uh first deploy something in our
343:16 - clusters
343:17 - here in our yaml file I have a simple
343:20 - deployment with three replicas of a
343:24 - simple
343:26 - container or image called Hello app
343:29 - nothing fancy here
343:31 - let's deploy that uh right away
343:38 - okay and let's open a terminal or come
343:42 - in line comment prom whatever you name
343:46 - it
343:47 - and let me type K9s
343:56 - all right let me stretch that a little
343:59 - bit
344:02 - well it's super cool it's a dashboard
344:04 - running inside the terminal it's super
344:07 - light and it gives you tons of
344:09 - functionality so here
344:12 - I have my deployment so I'm in the
344:15 - default namespace you can look at my my
344:18 - deployment I can enter on an object I
344:23 - get more information type Escape go back
344:29 - I have information about my cluster uh I
344:33 - can issue some some commands so Ctrl D I
344:37 - can delete a a pod let me do that let me
344:41 - kill that poor part here
344:45 - uh are you sure yes
344:49 - see you have feedback visual feedback of
344:52 - of uh what's happening
344:56 - the Pod that I deleted was kill was in
345:00 - the terminating State and the new one
345:02 - was uh created uh right away
345:06 - I can yeah it D to describe
345:12 - the resource escape to uh to go back
345:17 - Ctrl K to kill I just deleted one but I
345:20 - can do a Ctrl k
345:25 - there we go
345:27 - I can see the log so let's let's switch
345:30 - to the second one type l
345:34 - and I can look at the uh the logs
345:37 - I can open a shell also by typing here
345:43 - the
345:45 - S letter doing the less
345:49 - right you can type exit go back I can uh
345:54 - even configure a port forward
345:59 - pretty cool here I can look at the yaml
346:01 - file also
346:07 - let me type Escape
346:14 - here we have the parts that are listed
346:16 - but if I type Colin I can change that
346:20 - let's say let me type deploy
346:22 - so here's my deployment now if I type e
346:28 - I will edit the deployment and let's
346:31 - change the number of senses or replicas
346:34 - from three to four
346:37 - and let's close that
346:42 - right and now I have four out of four so
346:46 - let's uh I'll type Colin again and type
346:50 - but
346:53 - here are my four pods
346:58 - super interesting it's a free tool it's
347:02 - super light it's super fast and I always
347:06 - have one open so I can see
347:09 - visually what's happening uh inside my
347:12 - cluster when I'm issuing some some
347:14 - commands
347:16 - let's go back uh to uh our page here our
347:21 - Visual Studio code and let's simply
347:24 - delete our deployment and let me switch
347:27 - back right away here
347:30 - and they're gone
347:33 - pretty cool too
347:37 - [Music]
347:41 - let's see how to scale pods
347:44 - the horizontal pad to scalar is a
347:47 - kubernetes feature that allows you to
347:49 - scale the number of Parts up and down
347:54 - it uses the metric server to gather the
347:57 - pods utilization
347:59 - pods app must have requests and limits
348:02 - defined
348:03 - the HPA checks the metric server every
348:07 - 30 seconds and scale the number of PODS
348:10 - according to the minimum and maximum
348:13 - number of replicas defined
348:17 - to prevent racing conditions the HPA
348:19 - Waits some period of time after a
348:22 - scaling event by default this delay on
348:25 - scale-up events is 3 minutes and the
348:29 - delay on scaled down events is 5 minutes
348:34 - in this part definition you specify the
348:37 - CPU and memory requests and limits
348:40 - the request is what's allocated at first
348:42 - and the limit is what you allow the part
348:45 - to burst to
348:47 - in this example the Pod will start with
348:49 - 64 megabyte of ram but can burst up to
348:52 - 128 megabyte if needed
348:56 - you configure the HPA using a manifest
348:59 - specifying the deployment you want to
349:01 - scale
349:02 - the Min and max number of replicas
349:06 - and the metric you want the HPA to scale
349:09 - on in this case we tell the HPA to kick
349:12 - in when the average CPU utilization is
349:16 - above 50 percent
349:19 - here's the cheat sheet for the HPA
349:21 - commands
349:22 - so you can create one using the
349:24 - imperative way using cubectl autoscale
349:27 - deployment the name and the metric and
349:32 - replicas number
349:34 - you can create one using the yaml file
349:40 - you can get the autoscaler status by
349:43 - using cubectl get HPA and the HPA name
349:46 - and of course you can delete the HPA
349:49 - using the yaml file or cube CTL delete
349:52 - HPA with its name
349:55 - foreign
349:58 - this lecture about scaling pods
350:03 - [Music]
350:07 - in this lab we will use the horizontal
350:11 - particle scalar to scale a pod
350:14 - for the HP it worked it needs some data
350:16 - some metrics coming from the metric
350:19 - server and by default it's not installed
350:22 - by Docker desktop
350:24 - just make sure I'll open the terminal
350:28 - and what we'll do we'll get a list of
350:32 - the parts running in the cube system
350:34 - namespace
350:38 - Accord DNS at CD Cube API server proxy
350:43 - provisioner no nothing that looks like
350:46 - metric server okay
350:49 - to install it you need to run this yaml
350:53 - file coming from this git repo on
350:57 - kubernetes-6 special integers group and
351:00 - Metric server
351:02 - but
351:04 - you need to make a small modification to
351:08 - to it
351:10 - let's take a look at the components that
351:12 - yaml file
351:14 - need to do you need to edit it if you
351:17 - download it directly from the git repo
351:22 - and you locate the deployment section
351:25 - there it is deployment
351:29 - and what you need to do is add this
351:31 - parameter
351:33 - a cubelet dash insecure-tls
351:37 - if not the metric server will not run on
351:41 - Docker desktop all right
351:44 - let's deploy our metric server cubectl
351:49 - apply Dash F components Dot yaml
351:54 - awesome
351:56 - what we can do we can run again Cube
352:00 - City I'll get pod in the cube system
352:02 - namespace
352:05 - aha metrics server there it is it might
352:10 - take a few a couple of minutes for the
352:12 - metric server to start running
352:15 - now that the metric server is running
352:17 - let's take a look at our deployment
352:20 - uh it's a deployment and what we'll
352:23 - deploy is a web server called HPA Dash
352:27 - example uh listening on Port 80. so it's
352:30 - a simple web server that will return a
352:32 - web page nothing fancy
352:35 - what we'll do uh will also deploy a busy
352:39 - box and from that busy box will it that
352:42 - web server uh in a loop and that should
352:45 - generate some traffic
352:48 - awesome let's
352:50 - deploy our
352:53 - web server
352:58 - let's get a list of the pods running in
353:01 - the default namespace awesome it's
353:04 - running
353:05 - let's enable our Auto scaler so Cube CTL
353:09 - Auto scale a deployment called HP Dash
353:12 - deployment and a metric called CPU
353:15 - percent
353:16 - and we want a minimum of one instance
353:19 - and a maximum of four
353:22 - okay
353:25 - let's validate that we have an HPA Cube
353:28 - CTL get HPA that will list all the hpas
353:33 - running
353:34 - on my cluster there's only one awesome
353:38 - let's now deploy our busy box
353:45 - and let's connect
353:48 - or open a session on that busy box
353:52 - perfect
353:54 - and here's our endless loop that will uh
353:57 - it the web server
354:00 - that should generate some traffic
354:03 - okay let's take a look at our
354:07 - K9s
354:09 - I have my deployment here my HPA
354:12 - deployment and my busy box
354:14 - and area we have three instances of our
354:19 - deployment right now
354:20 - awesome
354:21 - so the HPA worked and now what I can do
354:25 - is start the loop by hitting Ctrl C
354:30 - and I'll type exit to exit my busy box
354:36 - and from there I can delete my HPA
354:42 - but be careful when you do that I'll
354:45 - delete my HP here if we take a look at
354:48 - the deployments there's still three
354:50 - nothing will scale that down since the
354:54 - HP uh has been deleted
354:57 - let's delete our busy box
355:02 - and let's delete our deployment that
355:04 - should delete all three instances
355:07 - take a look here yep all three are
355:10 - terminating
355:12 - and optionally you can delete the metric
355:15 - server by using cubectl delete and the
355:19 - components.yaml file here
355:24 - [Music]
355:30 - we are at the end of this Docker
355:32 - containers and kubernetes fundamental
355:35 - scores
355:36 - congratulations you are now an official
355:40 - kubernetes Ninja
355:41 - [Applause]
355:44 - the next steps for you would be to
355:47 - deploy containers in the cloud using
355:49 - services from a cloud provider
355:52 - these courses will teach you how to do
355:55 - that on Google cloud and Azure and also
355:58 - on smaller Cloud providers like Linux
356:00 - and digitalocean
356:03 - the best part is that each offer free
356:06 - credit usage when creating new accounts
356:09 - this way you can create a managed
356:11 - kubernetes service in the cloud without
356:14 - breaking the bank
356:17 - if you enjoyed the course you can help
356:19 - me by making a small donation this is
356:22 - the link to my buy me a coffee page
356:26 - I want to say a big thank you for
356:29 - learning Docker and kubernetes using my
356:31 - course and I wish you all the best