00:01 - hi everyone
00:02 - my name is vincent and this is a photo
00:05 - of me
00:06 - in this series of videos i am going to
00:09 - explain psychic learn to you
00:12 - psychic learn is the go-to library for
00:14 - machine learning with an amazing
00:16 - ecosystem of plugins
00:18 - and in this series of videos i will
00:20 - highlight some of the most important
00:21 - parts to understand if you're going to
00:23 - be working with this tool
00:26 - what's important to note is that these
00:27 - videos i'm about to show you originally
00:29 - appeared on calm code
00:32 - this website features short videos about
00:34 - a topic in a series so what i've done is
00:36 - i've concatenated the videos about
00:38 - scikit-learn together for this free code
00:42 - camp video
00:44 - that means that this long video is split
00:46 - up into different sections and each
00:48 - section will highlight a different but
00:51 - relevant so i could learn topic now note
00:54 - that all the code for the videos that
00:55 - you're about to see
00:57 - is available on github
01:00 - you'll find a link in the show notes and
01:02 - you can download each notebook yourself
01:03 - locally to run them but you can also use
01:06 - a service like google colab as well
01:09 - so let's talk about the topics that we
01:11 - will be discussing in this video
01:14 - in the first section we'll discuss some
01:16 - high level topics involved with psychic
01:18 - learn they are mainly there to help you
01:20 - appreciate how to construct machine
01:22 - learning pipelines but the videos will
01:24 - also try to help explain to you why
01:26 - machine learning is still hard in
01:28 - practice
01:30 - after all machine learning tends to go
01:32 - beyond just optimizing a model
01:36 - after that we've got a segment where we
01:38 - will talk about pre-processing tools
01:41 - you can use a scikit-learn model but the
01:43 - performance of the model really depends
01:45 - a lot on how the data is pre-processed
01:48 - and understanding this pre-processing
01:50 - step tends to make your models a lot
01:52 - better
01:54 - next
01:55 - if you want to judge a model you'll also
01:56 - need to think about how to quantify its
01:58 - qualities
02:00 - so
02:00 - in this section we'll talk about the
02:03 - metrics that scikit-learn offers but
02:05 - also how to build custom metrics to get
02:07 - your machine learning models on for your
02:10 - own specific use case
02:12 - after that we're going to discuss meta
02:14 - estimators the idea here is that while
02:17 - you can pre-process data as a step in
02:20 - your machine learning pipeline you
02:22 - sometimes also want to apply post
02:24 - processing
02:26 - in order to properly explain this though
02:28 - we will have to discuss these things
02:30 - called meta estimators
02:33 - then finally i would like to demonstrate
02:35 - a machine learning library that
02:37 - integrates with scikit-learn that tries
02:39 - to make machine learning a little bit
02:40 - more
02:41 - human
02:42 - the tool is called human learn
02:45 - and full disclaimer it's a tool that i
02:48 - made myself the goal of this library is
02:51 - to show you how you might be able to
02:53 - benchmark the domain knowledge that's
02:54 - inside of your company before you resort
02:56 - to machine learning
02:59 - now i hope that these topics indeed
03:00 - sound interesting to you
03:02 - but let's now get started with
03:04 - understanding psychic learn
03:08 - scikit-learn is possibly the most used
03:10 - machine learning tool in the world and
03:12 - in this series of videos i would like to
03:14 - explain the general flow of how to use
03:16 - scikit-learn when you want to make
03:17 - predictions
03:19 - now for reasons that will become clear
03:21 - later we are going to use a very
03:23 - specific version of scikit-learn in this
03:26 - series of videos
03:28 - in particular we'll be using version
03:30 - 0.23.0
03:32 - and if you're in a jupiter notebook you
03:34 - can use this command to make sure that
03:36 - you've got this specific version
03:37 - installed
03:38 - it should also be said that this series
03:41 - of videos is different than some of the
03:43 - other series of videos on this website
03:45 - in this series of videos it is extra
03:47 - important that you watch all of the
03:49 - videos not just the first few
03:52 - you actually need to watch all of the
03:53 - videos to get a proper understanding of
03:55 - how to use scikit-learn appropriately
03:58 - also the scikit-learn ecosystem is vast
04:01 - and it's impossible to do it justice in
04:03 - just a short series of videos the goal
04:06 - of this series of videos is more to give
04:08 - you a bit of an overview
04:10 - on how in general you should think and
04:12 - work inside a psychic learn but there
04:14 - will be other series of videos that will
04:17 - go more in depth in a certain aspect of
04:19 - psychic learn that i will not be able to
04:21 - do justice in just this series of videos
04:24 - that said let's dive in
04:27 - the way scikit-learn works is you start
04:29 - with some data
04:31 - you then eventually give it to a model
04:34 - the model will learn from it and then
04:36 - you will be able to make predictions
04:38 - that's the general flow
04:40 - however let's be a little bit specific
04:43 - just giving data to a model is a bit
04:45 - vague
04:46 - so what do we mean by giving data to a
04:48 - model
04:49 - now typically if we have a data set
04:52 - that's useful for predictions then we
04:54 - can split the data set up into two parts
04:56 - and the common notation is to call one
04:59 - part of the data x and the other part of
05:01 - the data y
05:03 - now typically my data set x represents
05:06 - everything that i'm using to make a
05:08 - prediction
05:09 - and my data set y contains the
05:11 - prediction that i'm interested in making
05:14 - the use case that we're going to deal
05:16 - with in a moment has to do with house
05:19 - price prediction so you can imagine that
05:21 - this y
05:23 - data set that contains the house prices
05:26 - and x over here contains
05:28 - information about the house
05:31 - things like square feet how big is the
05:34 - garden that sort of thing
05:36 - if you split your data up in this
05:37 - fashion the next thing that you can do
05:40 - is you can pass that to your model
05:42 - and then it's the model's job to learn
05:44 - the pattern such that we can predict y
05:47 - using x this x and y notation comes from
05:51 - scientific articles and because of the
05:54 - notation that scikit-learn adheres to
05:56 - it's also the notation that i will use
05:57 - in my videos what we'll do next is go
06:00 - back to our notebook and get ourselves a
06:02 - data set that psychic learn provides
06:04 - that gives us our x and y
06:08 - given that we have scikit-learn
06:09 - installed what i can do is i can say
06:11 - from sklearn
06:13 - datasets
06:15 - i can type load
06:17 - press autocomplete and then i see a
06:19 - whole lot of data sets that are at my
06:21 - disposal and these data sets are meant
06:23 - for benchmarking and educational
06:25 - purposes and what i'll just go ahead and
06:26 - do is i'll load the boston data set
06:29 - that's a data set that contains house
06:31 - prices of boston i believe in the 70s
06:35 - now if i call the load boston data set
06:37 - as is
06:38 - it's going to give me a dictionary with
06:40 - lots of items in it
06:42 - however there's a parameter that we can
06:43 - set called return x y and if we set that
06:46 - to true then we get two arrays out
06:52 - one array represents the house prices
06:54 - and these house prices are in thousands
06:56 - of dollars and these are all properties
06:59 - of the houses
07:01 - one nice thing that i can do is i can
07:03 - type x comma y equals
07:05 - and then i've got my x and my y that i
07:08 - can go ahead and use
07:10 - inside of a scikit-learn model
07:12 - so
07:13 - what we are about to do is
07:16 - make a model appear inside a cycle learn
07:18 - but just conceptually it is nice to
07:20 - point out what a model does actually
07:23 - when you create a model it hasn't seen
07:25 - any data yet there has to be this moment
07:27 - where we declare that this model over
07:30 - here
07:30 - learns from data
07:32 - and in psychic learn that means that
07:33 - there are two phases
07:35 - there's phase one where we create the
07:38 - model and then there's phase two where
07:40 - the model has to
07:42 - learn from the data in scikit-learn all
07:45 - models are just python objects
07:47 - and then this learning over here in
07:49 - scikit-learn terminology that is often
07:52 - called the dot fit step
07:55 - and during that fit step we typically
07:57 - pass it the x as well as the
07:59 - y data set
08:01 - it's important to know that these are
08:03 - two separate phases that the model has
08:04 - to go through if we wanted to be able to
08:07 - make predictions but given that we've
08:09 - highlighted this overview let's go back
08:11 - to the notebook and show you what the
08:13 - code might look like
08:15 - so let's load up our first model
08:18 - scikit-learn comes with many but one of
08:20 - my favorite ones is the k-nearest
08:22 - neighbor model
08:25 - and you can find it in the neighbors sub
08:27 - module
08:28 - and i will grab this k neighbor's
08:30 - regressor
08:32 - now for the creation step what i will do
08:35 - is i will
08:36 - have a variable called model
08:38 - and that will point to this model object
08:42 - now at the moment there hasn't been a
08:44 - phase where the model was able to learn
08:46 - anything so if i were not to call model
08:48 - dot predict
08:50 - x
08:51 - as in hey model could you perhaps
08:53 - predict why using x then right now i
08:56 - will get an error
08:58 - and that's because the model has not
09:00 - been fitted yet
09:01 - however if i were now to say hey model
09:03 - please fit yourself
09:05 - x comma y
09:07 - now by calling this
09:08 - k neighbors will learn from the data as
09:11 - well as possible such that if i now
09:13 - wanted to make a prediction it is able
09:15 - to do so
09:18 - and you can verify yourself that the
09:20 - number of predictions that we're making
09:22 - is equal to the number of rows in our x
09:25 - array
09:28 - now i'll discuss later what this machine
09:31 - learning model does exactly
09:33 - because you might wonder how the
09:34 - predictions get made
09:36 - but one thing i would just briefly like
09:38 - to point out is that i can also use
09:40 - another model
09:41 - say from scikit-learn
09:43 - linear model i'm just going to import
09:46 - linear regression
09:48 - but the beauty of scikit-learn is that
09:50 - even though this is a completely
09:52 - different machine learning model that
09:54 - internally works very differently
09:57 - the api is still exactly the same
09:59 - if i were now to replace this
10:02 - k neighbor's regressor with this linear
10:04 - regressor then
10:05 - i can run all the cells and sure the
10:08 - predictions will be different because
10:10 - it's a different model the internals
10:12 - work differently but the api is exactly
10:14 - the same it's still.fit dot predict
10:18 - and it's this what makes scikit-learn
10:20 - very nice to use
10:22 - you can stick to a general api and not
10:24 - have to worry too much about the
10:26 - internals and that is very nice
10:31 - so i'm back in a jupiter notebook and
10:33 - what i've got is my trusty k nearest
10:35 - neighbor model
10:36 - and i am fitting that model such that
10:39 - later on i can make predictions with it
10:42 - and what i've got now is this array of
10:44 - predictions but what i've also got is
10:46 - this array of original values these were
10:49 - the true values so the same
10:52 - so one thing that i typically like to do
10:54 - is just make a scatter plot
10:56 - i will show the predictions on the
10:58 - x-axis
10:59 - and the predicted values on the y-axis
11:04 - and by looking at this we get a bit of
11:05 - an impression of how well the model
11:08 - is doing it's just a plot but it does
11:11 - give me an impression that in general it
11:13 - seems to pick up something of a signal
11:16 - and that there indeed is some form of
11:17 - correlation happening as well when our
11:19 - model says the house price should be
11:21 - high it seems to be
11:23 - high in reality as well
11:25 - it's not perfect though and it's a
11:27 - little bit noisy here
11:29 - and there's a couple of reasons for why
11:30 - that may be
11:32 - but
11:33 - let's now talk about how this
11:34 - k-neighbor's regressor works
11:37 - let's consider a simpler version of the
11:39 - dataset and the data set contains
11:42 - a square feet that a house has
11:45 - as well as let's say
11:47 - the proximity to school
11:50 - let's say it's these two
11:51 - and let's assume that if you have a big
11:53 - house
11:55 - and proximity to schools is relatively
11:57 - small then
11:59 - these red dots indicate that these are
12:01 - indeed the expensive houses and you can
12:04 - imagine maybe the cheaper houses being
12:06 - over here
12:08 - now the way that a prediction is made
12:10 - when you're using a k neighbor's
12:11 - regressor is let's say i want to make a
12:13 - prediction for
12:15 - this point right over here now what the
12:17 - model will do is it will start looking
12:19 - for its nearest neighbors
12:21 - let's say we're looking for the nearest
12:23 - five
12:24 - i suppose that will be these
12:27 - if i look at the distance from the point
12:29 - that i'm trying to predict
12:33 - to all the other points in this data set
12:34 - i think these might be the nearest
12:36 - neighbors
12:38 - now the prediction for this data point
12:39 - will be
12:40 - the average of these five neighbors that
12:42 - we found
12:44 - and that is how a prediction is made
12:46 - but here is where the tricky thing can
12:48 - happen it might be that the proximity to
12:51 - a school here is in miles
12:54 - and
12:55 - it might very well be the case that
12:57 - you might have one mile over here two
12:59 - miles over here
13:01 - and three over here
13:03 - but these square feet that we have on
13:05 - this axis well that can be well in the
13:07 - thousands
13:09 - so that means that
13:11 - the distance that will be used to find
13:13 - neighbors will be very different on this
13:16 - axis compared to this
13:18 - axis because
13:21 - this x-axis just features larger numbers
13:23 - it also means that that axis will have a
13:25 - much bigger effect in our end
13:27 - predictions and that might not be what
13:28 - we want
13:30 - so this means we have to rethink what
13:32 - our model actually is
13:36 - at the moment this is our high level
13:38 - overview of what a machine learning
13:40 - model is
13:41 - still data going into the model and then
13:43 - we get a prediction eventually but maybe
13:45 - we have to rethink what a model exactly
13:48 - is here because if i think about the
13:50 - example that we just saw
13:52 - maybe
13:53 - before this data set x goes into this
13:56 - model over here
13:58 - maybe we would have to do some
13:59 - pre-processing first
14:01 - we saw that the square footage can be in
14:03 - the thousands and maybe school proximity
14:05 - can be
14:06 - in singletons
14:08 - so there might be something to be said
14:10 - about applying pre-processing before it
14:12 - touches the model
14:14 - so let's draw that
14:17 - so here's the redrawn schematic the idea
14:19 - is that we take our data set x and
14:22 - before we give it to our k nearest
14:23 - neighbor we apply some sort of scaling
14:26 - just to make sure that the effect that
14:27 - each column can have on the prediction
14:29 - is on the level playing field and doing
14:32 - this will make this k nearest neighbor
14:35 - predict rather differently
14:37 - and when you think about it that way
14:39 - maybe we need to redefine what we think
14:41 - a model is
14:43 - before we said that it is this k nearest
14:46 - neighbor that is the model but maybe we
14:49 - should expand that idea
14:53 - maybe everything inside of this box is
14:56 - the model
14:57 - if the preprocessing has a large effect
14:59 - on the model itself
15:01 - then for all intents and purposes we
15:03 - would like that to be part of our entire
15:05 - system so that said
15:08 - maybe it's more this pipeline that i've
15:10 - drawn over here that should be regarded
15:12 - as the model
15:13 - and it just so happens that in
15:15 - scikit-learn we have this notion of a
15:17 - pipeline and this pipeline also has the
15:21 - api where we can call dot fit on the
15:23 - entire pipeline as well as dot predict
15:26 - once it's trained
15:28 - the reason why that is so nice is
15:31 - because
15:32 - this pre-processing step also has to
15:34 - learn from the data in order to properly
15:37 - scale and normalize
15:38 - and by putting everything into a
15:40 - pipeline we are able to just handle that
15:43 - automatically such that we still have to
15:45 - interface with one object instead of
15:47 - many
15:48 - so i hope this overview paints a clear
15:50 - picture of why we like to have a new
15:52 - definition of what a model is and what
15:54 - i'll do now is i'll just implement this
15:56 - in the jupyter notebook
15:58 - so let's first import the parts that we
16:00 - need from scikit-learn i will need to
16:02 - import from the pre-processing module
16:05 - something that can do scaling and for
16:07 - that i will just use this standard
16:09 - scalar object
16:10 - next what i'll do is i will also import
16:14 - the pipeline object and that allows me
16:16 - to chain processing steps after each
16:18 - other
16:19 - so what i will do now having imported
16:21 - these tools is start a new pipeline
16:24 - object
16:25 - it needs a list of tuples
16:27 - and it's a
16:28 - pair of a name as well as a step
16:33 - do keep in mind that you have to pass
16:35 - the object over here not the class so
16:38 - it's important that you
16:39 - use these brackets
16:41 - after we've done scaling we would like
16:43 - to use our nearest neighbor
16:48 - and this is the pipeline what i can now
16:51 - do is just call pipe.fit x comma y and
16:54 - this entire pipeline will now train and
16:57 - fit itself and what i can go ahead and
16:59 - do
17:00 - is i can replace the model that i had
17:02 - originally over here with the pipeline
17:05 - that's now also scaled and when i run
17:07 - this we should see a new
17:09 - graphic appear as well
17:13 - so i don't know about you but this does
17:16 - look a bit better because there's less
17:18 - noise
17:19 - there is one
17:21 - other issue though that we've just
17:22 - introduced
17:24 - so let's have a look at what's actually
17:26 - happening now because we're cheating i'm
17:28 - telling the pipeline to go ahead and
17:30 - predict using this data set x
17:33 - but note that that's the same data set
17:35 - that we're using in the dot fit moment
17:38 - we are learning from the same data as we
17:41 - are judging on
17:43 - and the k nearest neighbor will do
17:44 - something now that's cheeky
17:46 - suppose that i want to make a prediction
17:48 - for a point that's let's say over here
17:51 - what i will then do is i will grab the
17:52 - five nearest neighbors
17:55 - and then i will make a prediction for
17:57 - this one point by taking the average
18:01 - but we're not making a prediction like
18:02 - this in our scatter chart the point that
18:05 - i'm trying to predict is a point that's
18:07 - in our original data set as well
18:09 - so what we're actually doing now is
18:11 - we're saying
18:12 - suppose i want to predict this point
18:15 - what are the nearest five neighbors
18:17 - and well the nearest five neighbors
18:19 - would include these four points
18:22 - as well as this original point
18:24 - so that means that i'm literally using
18:27 - the data point that i'm trying to
18:28 - predict in order to figure out if i'm
18:30 - doing the prediction well
18:32 - as far as judging whether or not a model
18:34 - is good this chart over here is giving
18:37 - us
18:37 - a view that is too optimistic
18:40 - and i can force it too
18:43 - what i'll quickly do now is i'll just
18:44 - change the pipeline a little bit
18:47 - to emphasize what's currently going
18:49 - wrong
18:50 - this k neighbor's regressor has a few
18:52 - settings
18:53 - and in particular we have this number of
18:55 - neighbors that we can set so let's
18:57 - change this number from 5 to 1.
19:03 - and i will run every single step now
19:05 - again
19:06 - just to show you what the effect is
19:11 - if i only select one neighbor
19:14 - then the chart falsely suggests that
19:16 - we're making a perfect prediction
19:18 - but the model here is only able to do
19:20 - that because it's allowed to memorize
19:22 - the original data the nearest neighbor
19:25 - here is the original data point
19:27 - so this chart doesn't tell us anything
19:29 - about how it might predict points that
19:31 - are not in the original data set and
19:33 - this is a big issue
19:34 - we want our model to predict data that
19:36 - it's not seen before
19:38 - and we cannot trust charts and
19:40 - statistics where the model is allowed to
19:42 - predict on the same data that it's
19:43 - trained on
19:45 - and that brings us to
19:47 - two issues
19:48 - the first issue is how can we get a fair
19:51 - comparison going for ourselves but
19:53 - perhaps another issue is
19:55 - how do i go about picking an appropriate
19:57 - number of neighbors here and when you
19:59 - consider thoughts like this
20:01 - you might realize that we have to review
20:03 - our understanding of what a model is
20:05 - one more time
20:07 - currently this is our belief of what a
20:10 - model is
20:11 - we have this pipeline and there can be
20:13 - multiple steps inside of it
20:16 - but now we have another issue because
20:18 - now we might have
20:20 - some settings for the k nearest neighbor
20:23 - i might want to try out the system where
20:24 - we have one neighbor or two
20:26 - all the way up until 10.
20:28 - and i would like to pick this
20:30 - number of neighbors setting
20:32 - such that my model makes the best
20:34 - predictions
20:35 - and in order to figure out which
20:37 - predictions are the best one thing that
20:39 - we can do
20:40 - is we can compare our prediction with
20:42 - the original label
20:44 - but as we've seen in a previous video we
20:47 - have to be really careful here we don't
20:49 - want to judge the model on the same data
20:51 - set as we're learning from
20:54 - and with that in mind
20:55 - maybe we should do a trick with a data
20:57 - set just to keep the methodology clean
21:00 - and here's the idea i'm going to
21:03 - cut this data set up
21:06 - into let's say three different segments
21:09 - and what i'm also going to do is i'm
21:10 - going to copy both data sets three times
21:14 - and here's the idea first i'm going to
21:16 - say well let's give this
21:18 - the predict name and let's do it over
21:20 - here
21:21 - i'll set the predict name here
21:24 - in the second set and i'll put predict
21:27 - down here in the third and i'm going to
21:31 - declare the other parts for training
21:35 - the first time around
21:37 - this
21:38 - part of the data set
21:40 - is going to be used for training
21:43 - and then given that trained model i can
21:45 - use this portion of the data that's not
21:48 - been used for training
21:49 - to test how well my predictions are
21:51 - going
21:52 - and in the next
21:54 - data copy i'm going to repeat the
21:56 - exercise but
21:58 - a different portion of the data is going
22:00 - to be used
22:02 - for prediction as well as training
22:04 - and finally the same thing happens here
22:06 - as well
22:08 - the idea here being i'm going to call
22:10 - dot fit
22:13 - dot predict here
22:14 - but i'm fitting on the green part and
22:16 - i'm predicting on the red part
22:19 - this prevents me from ever predicting on
22:22 - data that i've used during training
22:25 - but it does allow me to judge
22:27 - in the predict section of my data how
22:30 - well my predictions are
22:32 - and the idea essentially is if i just
22:33 - repeat this
22:35 - then maybe
22:36 - i'll have a pretty good metric of
22:38 - performance for when
22:40 - i had one neighbor selected
22:43 - when i had two neighbors selected and
22:45 - when i had 10 neighbors selected
22:48 - however
22:49 - in scikit-learn
22:50 - all of this is something that the
22:52 - pipeline will not handle for you
22:54 - instead
22:56 - there is a different object
23:01 - and the name of this object is a grid
23:03 - search
23:05 - cv object
23:07 - the idea behind it is that you can give
23:08 - this grid search a pipeline
23:10 - and you can also give it a grid like
23:12 - this number of neighbors over here and
23:15 - internally it will perform cross
23:17 - validation which is the procedure that
23:19 - i've explained here
23:21 - and by performing this cross validation
23:23 - we have a methodology that is somewhat
23:26 - sound
23:27 - and what i would argue here is that
23:28 - maybe this grid search object maybe that
23:31 - is the model
23:33 - that we should be thinking about
23:35 - and in scikit-learn also this grid
23:37 - search has a dot fit
23:40 - as well as a dot predict
23:42 - method attached
23:44 - so let's turn this pipeline into a
23:47 - proper grid search
23:48 - the first thing i'll need to do is make
23:50 - sure that i have my grid search cv
23:53 - object imported and you can import it
23:55 - from sklearn
23:58 - model selection
23:59 - and the object that you're interested in
24:01 - is this grid search cv object
24:05 - and given that we have this imported now
24:07 - what i can do is i can start a new grid
24:09 - search object
24:11 - to get started with it i need to pass an
24:13 - estimator and an estimator is something
24:15 - that has a dot fit as well as a predict
24:18 - and the pipeline that i've made earlier
24:21 - this one over here will do just fine
24:23 - next what i gotta pass is a parameter
24:25 - grid
24:26 - and this parameter grid is going to
24:28 - represent all the settings that we would
24:30 - like to
24:31 - go over in our pipeline in particular
24:34 - the one that we're interested in
24:35 - changing is this number of neighbors in
24:37 - this k neighbor's aggressor
24:39 - now to set the grid we need to have the
24:41 - name of that parameter and the easiest
24:43 - way to get there is to
24:45 - use the get params method that is on
24:47 - every scikit-learn
24:49 - estimator including this pipeline
24:54 - and when you run this you will see all
24:56 - the settings that you're able to tweak
24:59 - in particular you'll notice that there
25:00 - is this
25:02 - number of neighbors
25:04 - property that's on this
25:06 - model
25:07 - and the name model that i have here
25:10 - corresponds with this name of this
25:12 - pipeline step here
25:13 - and the number of neighbors that i have
25:15 - here corresponds with
25:17 - the number of neighbors that are a
25:20 - parameter in this object over here
25:23 - for our intents and purposes though the
25:25 - only thing i need to grab is this one
25:27 - but know that you can grab more here if
25:29 - you would like to change more parameters
25:33 - and what i'm going to do is i'm going to
25:35 - say hey you know what
25:37 - these are all the values that i would
25:39 - like you to check
25:42 - and i would really like this grid search
25:43 - to also do cross validation so let's set
25:46 - the cross validation parameter to three
25:49 - and now this is my model
25:55 - oop
25:56 - forgot a comma there
25:58 - now as far as grit search goes i would
26:00 - like to mention that what we're doing in
26:02 - this one over here is relatively basic
26:06 - i will leave links to other video series
26:08 - where we're going more in depth but the
26:11 - main thing i want to give you as a high
26:13 - overview of what will you end up doing
26:15 - when you have a grid search like this
26:17 - well
26:18 - you are simply just gonna call
26:21 - model.fit xy
26:23 - just like every other estimator we've
26:25 - seen so far except in the grid search
26:27 - what will happen is we will have all
26:29 - sorts of settings and cross-validation
26:32 - that's happening on our behalf so we
26:34 - don't have to write that code ourselves
26:36 - however
26:37 - once this has trained there is this very
26:39 - interesting property available called cv
26:42 - results
26:43 - and note that this property ends with an
26:44 - underscore
26:46 - the grid search will train
26:48 - and for every cross validation for every
26:51 - setting it's keeping track of a couple
26:53 - of numbers
26:55 - and what i can do is i can just
26:57 - give that dictionary that we have as
26:59 - output there to a pandas data frame
27:03 - and here what you see
27:05 - is all sorts of statistics
27:07 - you see how long it took to fit the
27:09 - entire thing
27:11 - and for every
27:12 - parameter that we have
27:14 - and for every cross validation split
27:16 - that we've made
27:17 - we can see
27:19 - how well it did on a certain score
27:22 - and we can also see which result was the
27:24 - best at this point in time it seems like
27:27 - we found that this might be the best
27:29 - setting and that's interesting and at
27:30 - this point in time we can spend a little
27:32 - bit of time
27:33 - analyzing this generated data set to
27:36 - figure out why that might be
27:38 - the main point i'm trying to make so far
27:41 - though is that with only
27:44 - a very small amount of code over here
27:47 - we have a fairly mature pipeline and we
27:51 - also have a proper set of lego bricks to
27:53 - build machine learning systems
27:55 - and if you'll be using scikit-learn a
27:57 - lot this is the pattern that you are
27:59 - eventually going to be aiming for
28:03 - we have a proper pipeline that we can go
28:04 - ahead and tweak quite easily it's clear
28:07 - what steps are happening the steps are
28:09 - reproducible as well
28:10 - and as far as methodology goes we can
28:13 - argue that we're doing a couple of
28:14 - things right because we're using this
28:16 - grid search object over here
28:19 - so try to stick to this pattern whenever
28:21 - you're using scikit-learn the system of
28:24 - fit predict that scikit-learn offers and
28:26 - the way that it allows you to construct
28:27 - pipelines is something to appreciate
28:31 - that said though we could ask ourselves
28:34 - have we now found a model that can go to
28:37 - production have we done our work
28:39 - are we now proper data scientists and
28:41 - the answer is no and i have to highlight
28:45 - why
28:46 - and this is also the reason why it's
28:48 - very important that you watch all of the
28:50 - videos in this series
28:54 - so far we have been using the
28:55 - scikit-learn api appropriately
28:58 - now i use the word appropriately here in
29:00 - the sense that we've been using its
29:03 - building blocks in the right way we
29:05 - click together a pipeline where you've
29:06 - been using a grid search and so far i
29:08 - would argue these are all good things
29:11 - as far as an approach to a data science
29:13 - project though we could not have done it
29:15 - worse and i would like to explain to you
29:17 - why we've been using this load boston
29:20 - data set but during the entire analysis
29:22 - we've not taken any time or even
29:24 - bothered to look what's actually inside
29:26 - of this data set now when i just run
29:28 - load boston like so you will notice that
29:31 - i get a dictionary one of the things
29:34 - inside of this dictionary is this
29:35 - description tag and what i can do is i
29:38 - can get a description of all the
29:39 - variables that are in the data set
29:42 - now if i want to get a nice output i
29:45 - have to print what is returned to me
29:47 - here so i'll do that
29:49 - and what i can now do is i can have a
29:51 - little look at what we're actually
29:52 - dealing with so first of all you can
29:55 - kind of wonder
29:56 - is 506
29:59 - houses enough to give us a lot of
30:01 - confidence in our model
30:03 - maybe not
30:04 - we can also ask ourselves the question
30:06 - hey from what year is this data it might
30:09 - be that this data is really old and that
30:10 - the world has moved on in a way that
30:12 - this data set doesn't really represent
30:14 - what's currently happening
30:16 - that's also a valid concern
30:18 - but it gets worse
30:20 - we also never bothered to look at what
30:22 - attributes are actually in the data set
30:24 - x that we're using to make a prediction
30:26 - so we have things like crime in the
30:28 - neighborhood
30:29 - we have things like how industrious is
30:31 - the area but the one that i was really
30:34 - concerned with that when i first looked
30:36 - at this is this one
30:38 - apparently this is a data set where the
30:40 - proportion of blacks in your town is
30:43 - something that is actually being used to
30:44 - predict a house price
30:46 - now i don't know about you but i'm
30:48 - having a really hard time coming up with
30:50 - a use case for this data set that is
30:52 - appropriate
30:53 - looking at this feature it's clear to me
30:56 - that we have a potential for a racist
30:58 - algorithm and that is a property i don't
31:00 - want to have in production
31:02 - now there's a lot of things that can go
31:04 - wrong when we deploy a machine learning
31:06 - model and we've discussed methodology
31:08 - and these are fine topics it's good to
31:11 - worry about that but grid search is
31:13 - simply not enough
31:16 - and what has been bothering me is that
31:18 - this data set has been used for so long
31:20 - in so many different courses without
31:23 - even looking at the variables that are
31:25 - in here putting a model that has this
31:28 - feature in it is incredibly naive
31:31 - and i think as a profession it really
31:33 - helps if we just do better
31:35 - this is also why scikit-learn has now
31:37 - chosen to remove this data set
31:40 - load boston after a few releases will be
31:43 - gone
31:44 - and this was the reason why i had to pin
31:47 - the version number of scikit-learn in
31:49 - the beginning mainly because the load
31:51 - boston data set in a future release will
31:53 - no longer be available
31:56 - remember this chart that we made halfway
31:58 - in this video series
31:59 - it contains our predicted values versus
32:02 - our true values
32:04 - and this chart was generated because we
32:06 - were using the same data set that we
32:07 - trained on as we evaluated on
32:10 - and the danger here is that the chart
32:12 - suggested that our model was better even
32:15 - though the model was effectively leaking
32:17 - information if we had blindly trusted
32:20 - this model then the risk is that this
32:22 - model might have gone to production with
32:24 - very high expectations and the results
32:26 - clearly would not have been great the
32:28 - only way to catch these sorts of things
32:30 - in practice is to remain skeptical you
32:33 - should always feel free to distress the
32:34 - model and to try to test every weakness
32:36 - that you can come up with only after a
32:39 - long period and lots of stress tests are
32:41 - you allowed to put a little bit of faith
32:43 - into the model that you have
32:45 - and note that the same thing that
32:46 - happened in this chart also happened to
32:48 - us while we were doing the grid search
32:50 - the grid search introduced a methodology
32:53 - and we certainly have statistical
32:55 - concerns when it comes to a model if it
32:57 - has poor predictive power then it won't
32:59 - be very useful in practice but we should
33:01 - maybe mention here that there is a
33:03 - danger in doing this grid search
33:06 - it may give you the impression that
33:08 - you're doing the right thing
33:09 - after all numbers go up and you might
33:12 - get optimistic about the quality of your
33:14 - model and it's exactly this optimism
33:16 - that might be causing you to develop a
33:18 - blind spot
33:19 - it's the time during which you make
33:21 - these optimistic charts as well as when
33:23 - you see numbers go up in your grid
33:24 - search that you forget to think about
33:26 - things like hey what's actually in my
33:29 - data
33:30 - and i hope the load boston data set as
33:32 - we've seen in the previous video
33:33 - makes it clear that you cannot just
33:35 - blindly put any data into a model you
33:38 - actually have to spend some effort and
33:39 - try to understand what you have at hand
33:42 - there are many things that can and have
33:44 - gone wrong in the application of machine
33:46 - learning models and i would argue that
33:48 - it's also your responsibility to educate
33:50 - yourself to make sure that it doesn't
33:51 - happen to you
33:54 - if the output of a machine learning
33:55 - model
33:56 - is your responsibility then so is the
33:59 - data going in
34:01 - so please use scikit-learn and its
34:03 - amazing lego brick-like features but
34:06 - also understand that so i could learn
34:08 - typically is the easy part of the
34:09 - profession the hard part is
34:11 - understanding the story behind the data
34:14 - set and to understand what might go
34:16 - wrong when you put a model into
34:18 - production and it means that it will be
34:19 - good to make sure that you're up to date
34:21 - on themes of ethics in algorithmic
34:24 - design but also topics like feedback
34:27 - mechanisms and to consider fallback
34:29 - scenarios for when things go wrong in
34:31 - production
34:37 - that concludes the psychic learn portion
34:39 - of this video
34:40 - this would be a good time to grab a
34:42 - drink or have a quick break and in the
34:44 - next portion we'll talk about
34:46 - pre-processing tools inside of
34:48 - scikit-learn
34:51 - quite typically in scikit-learn you'll
34:53 - have your labels and you'll have your
34:55 - data that you want to use to make a
34:56 - prediction
34:57 - and then both of these eventually will
34:59 - be passed to
35:01 - a model
35:04 - and then you'll have something that is
35:05 - able to make a prediction
35:08 - but the idea of what a model is can be
35:10 - extended here
35:12 - because
35:13 - if you think about what happens
35:15 - more often than not it's a pretty good
35:17 - idea to first transform your data that
35:20 - you're using for the prediction
35:23 - and the reason for doing that is because
35:24 - the model performance will just be a bit
35:26 - better by doing so
35:30 - so what i figured might be a good idea
35:32 - was spend some videos explaining some of
35:34 - the more frequent transformers that
35:36 - people tend to use and also show that
35:38 - it's important to not forget about these
35:40 - transformers because they do really
35:42 - matter in your pipeline so what i've
35:44 - done here is i've started the notebook
35:46 - i've imported numpy pandas and black.lib
35:49 - and i've imported this dataset called
35:51 - drawdata1
35:53 - and it's a csv file it has a column x a
35:55 - column y and a column z
35:59 - and z has two values either a and b
36:02 - and the idea is that these are just some
36:05 - numeric numbers and we're trying to
36:06 - predict this one column over here
36:09 - and using map.lib we can show what the
36:11 - dataset looks like
36:14 - so we've got a group of data points over
36:16 - here
36:17 - and another group of data points over
36:19 - here
36:21 - and we should notice that there seems to
36:23 - be like a small group of outliers over
36:25 - here as well as a small group of
36:27 - outliers over here
36:29 - but another thing that you should be
36:31 - aware of is that the y-axis over here is
36:35 - on a completely different scale
36:37 - as the x-axis over here
36:41 - and that can be something that's
36:42 - bothersome the effect that these axes
36:45 - will have depends on your algorithm but
36:47 - in general you can imagine that
36:49 - algorithms are sensitive for this kind
36:50 - of thing
36:52 - so
36:52 - a large chunk of your pre-processing in
36:54 - this case
36:56 - is going to revolve around
36:57 - scaling
36:59 - we want to rescale this data such that
37:01 - there's still information in there but
37:03 - it's just numerically a bit more stable
37:05 - because the x and y axes are just a
37:07 - little bit more in line with each other
37:10 - and as you might be able to guess from
37:11 - the name a standard way of doing this
37:14 - is using the standard scalar from
37:17 - scikit-learn and what it does is for
37:19 - each column it is going to calculate the
37:23 - mean as well as the variance the idea
37:26 - here being that if you have a data point
37:27 - x and you subtract the mean of x from it
37:30 - and then you divide by the square root
37:32 - of the variance
37:34 - well then you're going to have something
37:35 - that revolves around zero and this will
37:37 - also have a variance that's kept at bay
37:40 - so what i'll now do is i'll just go
37:41 - ahead and use the standard scalar to
37:43 - rescale the dataset that you see here
37:47 - to use a standard scalar we first have
37:49 - to import it
37:52 - and next what we have to do is we have
37:54 - to create a scalar object but from here
37:57 - we should be able to call fit transform
38:00 - and give it our data set x
38:02 - and this will be
38:04 - our new transformed data set and what
38:07 - i'll just go ahead and do is i'll just
38:09 - copy this plot code over here and put
38:12 - that down here
38:14 - this way i can just feed it the new
38:17 - values for x
38:21 - and we can see what it looks like so one
38:23 - thing you should notice at this point is
38:25 - that these axes numerically are much
38:29 - more similar
38:33 - but it's not exactly perfect though
38:35 - it seems that this spread is about 8
38:38 - units
38:42 - whereas the spread over here
38:44 - is more like three and a half
38:47 - and we can also observe that there's
38:49 - nothing really happening with these
38:51 - outliers
38:55 - so the standard scaler is doing things
38:57 - we like but it does make you wonder is
38:59 - there maybe another way of scaling this
39:03 - to further demonstrate what might be a
39:05 - weakness of the standard scaler i
39:07 - figured i would generate a data set to
39:08 - make the point just a little bit more
39:10 - tangible so what i've got here is just
39:12 - some data that has a couple of outliers
39:14 - on one end
39:16 - and what i'm going to do now is i'm
39:18 - going to say well let's take that data
39:20 - set
39:21 - let's subtract from that the mean of the
39:23 - data set
39:26 - and let's then
39:28 - divide by the standard deviation
39:33 - and whenever i run this i have slightly
39:35 - different data because i'm simulating
39:37 - data
39:38 - but as i'm running this over and over
39:40 - you should notice a few things
39:44 - and yes for starters definitely the
39:47 - numbers we have here on the x-axis these
39:49 - are definitely
39:51 - scaled
39:53 - so you could argue that's a good thing
39:55 - but the downside is we still have
39:57 - outliers
39:59 - and depending on the algorithm that
40:00 - you're using outliers will make life a
40:03 - little bit harder for you
40:05 - so let's instead conceptually come up
40:07 - with a different way of normalizing
40:09 - where these outliers are just a little
40:10 - bit less of a problem
40:13 - so let's say that this is my original
40:15 - data set
40:16 - now what i could do is i could calculate
40:18 - the mean value which would probably be
40:20 - around here
40:22 - and i could then say oh let's
40:23 - standardize around that
40:24 - but let's calculate some other values
40:26 - instead let's let's ignore the mean for
40:28 - now
40:29 - instead what i could do is i could
40:31 - calculate the quantiles
40:32 - i could imagine for example that the
40:34 - 50th percent quantile is over here and
40:37 - that means that 50
40:39 - of all the data is on this side of the
40:41 - line and 50 of the data is on that side
40:44 - of the line
40:46 - i could imagine that maybe the 25th
40:49 - quantile is over here
40:51 - and that means that 25 of all the data
40:53 - is on the left side and 75 is on the
40:56 - right side
40:57 - and i think that the 75th quantile will
41:01 - be somewhere over here
41:02 - and let's say the 99th that might be
41:04 - over here this is something that we
41:07 - could go ahead and calculate
41:09 - and if i were now to think hey how can i
41:11 - project that onto something that's
41:13 - normalized well i could have a number
41:16 - line down below
41:17 - and i would have
41:19 - the number 50 halfway the number 100
41:21 - would be all the way to the right the
41:23 - number zero will be over here
41:25 - i would have 25 over here and 75 over
41:27 - here
41:28 - and i hope you can see that there's a
41:30 - mapping here
41:37 - and notice when i scale it this way that
41:39 - then in this scaled representation
41:43 - the distance from the outlier to the
41:45 - 75th quantile is a lot smaller
41:51 - and that means that by using quantiles
41:54 - as opposed to means and standard
41:55 - deviations
41:57 - we may be able to get a more robust
41:59 - pre-processing step if there's outliers
42:02 - in here
42:04 - so let's use this idea as a
42:05 - pre-processing step and see what the
42:07 - effect is on our data set
42:11 - so from scikit-learn pre-processing i
42:13 - can now import the quantile transformer
42:16 - and what i'm going to go ahead and do is
42:18 - i'm going to replace the standard scaler
42:20 - with that quantile transformer
42:23 - but before i run this
42:25 - notice the
42:26 - axes
42:28 - the numbers that are the minimum and the
42:30 - maximum
42:31 - and also notice
42:32 - these outlier clusters
42:35 - i'm now going to run this and show the
42:37 - effect
42:39 - now there's a warning typically
42:41 - scikit-learn likes to calculate a
42:43 - thousand quantiles
42:45 - and the data set that we gave it doesn't
42:47 - have enough data for it so what i'm just
42:50 - going to go ahead and do
42:51 - is turn that number of quantiles
42:55 - into a
42:56 - hundred
42:58 - but notice
43:00 - that the minimum as well as the maximum
43:03 - on both axes are exactly equal to zero
43:06 - and one
43:08 - and you might also recognize that the
43:09 - clusters that we saw earlier are still
43:11 - in the data it's just that they have
43:13 - less of a profound effect
43:15 - and the reason for that is because now
43:17 - we're using quantiles to transform and
43:20 - scale as opposed to using the mean and
43:22 - standard deviation
43:24 - in the previous video we showed that
43:26 - when you take your x matrix and you pass
43:29 - it through
43:30 - a transformer
43:32 - the quantile transformer that you can
43:34 - get a very different output
43:36 - and what i would like to show in this
43:38 - video
43:39 - is that when you take that output and
43:41 - then pass that to a model
43:44 - that then the predictions are also going
43:45 - to be
43:46 - very different
43:50 - so what i have done is i have made this
43:53 - plot output function
43:55 - and what the function does it allows you
43:57 - to pass through a transformer
44:00 - and the function will then run all of
44:02 - this it's going to train a model a k
44:05 - nearest neighbor model
44:06 - and it will then also produce some
44:08 - predictions
44:09 - and i just would like to show you what a
44:11 - profound effect this transformer can
44:13 - make in a predictive pipeline as well
44:18 - so i'll first run it with a standard
44:20 - scaler
44:22 - and what you can see here is
44:24 - the original data that we start out with
44:27 - followed by the transformed data
44:30 - and then we will see the predicted data
44:32 - which is this final plot over here
44:35 - and what you can see is that
44:37 - everything around this area over here
44:40 - would get predicted as the yellow class
44:42 - and
44:43 - pretty much everything else will be
44:45 - predicted as the purple one
44:48 - let's now compare that to the quantile
44:50 - transformer
44:54 - because the transform data is much less
44:57 - influenced by the outliers
45:00 - you'll notice that the model is bound to
45:02 - think that there's just this dividing
45:04 - line between the two classes and that's
45:06 - how we're gonna classify everything
45:08 - and if i were to compare the predicted
45:10 - data plot you'll really see a big
45:12 - difference
45:15 - and in this particular case i might
45:16 - argue as far as numerical stability goes
45:19 - the quantile transformer does have
45:21 - benefits
45:23 - but it would still be a good idea to
45:24 - verify this with a grid search but i
45:26 - hope that you can imagine that the
45:28 - quantile transformer even with the grid
45:30 - search is just going to be more stable
45:33 - in the long run
45:36 - because these
45:37 - groups of outliers are not going to have
45:40 - as much of an effect anymore
45:43 - let's now have a look at a different
45:44 - data set in this case i'm looking at
45:47 - dronedata2.csv
45:49 - and this dataset is special in the sense
45:51 - that it's a dataset that represents a
45:54 - classification task that is not linearly
45:57 - separable
45:59 - it's not possible for me to draw a
46:01 - single line
46:02 - such that on one side of the line i'll
46:04 - have purple points and on the other side
46:06 - of the line i'll have yellow points
46:11 - and this might make you think that a
46:13 - logistic regression is not going to be
46:15 - the right choice for this task
46:18 - you may need to have a different
46:19 - algorithm here to get a good classifier
46:22 - so let's see if that is accurate
46:27 - so what you see right now
46:28 - is a pipeline that has a quantile
46:31 - transformer first as a pre-processing
46:33 - step and then a logistic regression
46:36 - and the best thing the logistic
46:37 - regression has been able to do is come
46:39 - up with this line to separate the two
46:41 - classes
46:43 - and
46:44 - this is obviously a bad classifier these
46:46 - two groups should be the same color and
46:48 - the line should not separate this way
46:51 - but maybe
46:53 - we can fix this with preprocessing
46:55 - instead
46:56 - if i consider the two axes that i have
46:58 - at my disposal here then essentially
47:00 - what is happening is the logistic
47:02 - regression will get x1 and x2 and at the
47:05 - moment it can only use those two columns
47:08 - to come up with a separating line but
47:10 - what if i generate x1 times x2 as
47:13 - another feature that's something that
47:16 - the logistic regression might be able to
47:17 - use
47:19 - and what about x1 to the power of 2 and
47:21 - x2 to the power of 2. what you could say
47:25 - is that we can limit ourselves to the
47:26 - linear features
47:28 - but we can also just use those linear
47:30 - features to come up with
47:32 - non-linear features that our model might
47:35 - be able to use
47:37 - so let's
47:39 - change the pipeline
47:40 - such that we add these features in as
47:42 - well and then see what the effect is
47:47 - from scikit-learn pre-processing i can
47:50 - import the polynomial features
47:53 - and just start a new object over here
47:57 - now note that i'm not changing any of
47:59 - the input variables here but one thing
48:00 - that's nice to point out is that i am
48:03 - only calculating the interactions at the
48:05 - moment and i'm only calculating this for
48:07 - a degree of 2 but i could increase this
48:10 - if i wanted to but let's just see what
48:12 - the effect of just this is
48:15 - booyah and if i were to scroll up now to
48:18 - compare
48:19 - i would argue we have a near perfect
48:20 - classification
48:22 - and granted this is on the train set so
48:24 - it's cheating a little bit
48:26 - but what i do hope to have shown here is
48:28 - how much of an effect a single
48:30 - pre-processing step could have in your
48:32 - pipeline
48:33 - the effects can actually be quite
48:34 - drastic
48:36 - so far we've been doing a lot of
48:37 - pre-processing on numeric data but you
48:40 - can also imagine that we have data
48:42 - that's like this where maybe we have
48:45 - classes low medium high risk and then it
48:48 - will be nice if we can do some
48:49 - pre-processing such that this text data
48:52 - becomes numeric data as well
48:55 - the most common technique for that is
48:57 - the
48:58 - one hot encoder
49:00 - now what this encoder will do
49:02 - is it will
49:04 - be able to take in
49:06 - an array of text or categories and
49:08 - transform that out to something that is
49:10 - indeed numeric now if you just run this
49:13 - as is you're going to get a data
49:15 - structure that's known as a sparse
49:17 - matrix
49:18 - there's a setting though that we can
49:20 - change
49:21 - such that the sparsity
49:23 - is false and then we can actually see
49:25 - what is inside of it
49:29 - note that the first two rows are
49:31 - indicated as low
49:33 - and we can see that indeed they share
49:35 - the same column
49:38 - then we see high over here
49:40 - which is listed there and then we see
49:43 - medium below and that is listed there
49:46 - so we can see a form of correspondence
49:48 - and that is something that is indeed
49:49 - useful and the most common use case for
49:52 - this is if this is let's say the class
49:54 - that you would like to predict
49:56 - then
49:57 - this
49:58 - numeric representation that is going to
50:00 - be the y
50:02 - array that you're going to pass to
50:03 - psychic learn because this is something
50:06 - that psychic learn can use to train
50:07 - numerically on
50:11 - there is some behavior to be aware of
50:13 - though and this is not super relevant if
50:15 - you're generating labels but it is
50:18 - relevant if you're using this onenote
50:19 - encoder to encode information for the
50:22 - data that will predict the label
50:25 - let's say i grab the encoder now and i
50:26 - ask it to transform something and what
50:28 - i'm asking it to transform is something
50:31 - that it's never seen before
50:33 - so notice that i'm asking it to give me
50:35 - a label for zero
50:37 - but zero does not appear in this set
50:40 - and that is the set where we did perform
50:43 - a fit on
50:44 - so we might wonder what's going to
50:46 - happen here
50:48 - well we're going to get a big fat value
50:50 - error
50:51 - so it's saying value error found unknown
50:54 - category 0.
50:56 - so essentially it is telling us
50:58 - you're not allowed to give me data that
51:01 - i've never seen before
51:03 - we can change the setting for this
51:04 - though
51:05 - because at the moment the handle unknown
51:07 - parameter is set to error
51:10 - but we can change that
51:13 - such that it's set to ignore
51:16 - and now if i run this
51:18 - it's not going to give me an error
51:20 - and what it's doing is it's saying well
51:23 - these are all zeros or another way of
51:25 - saying that
51:27 - zero is neither low high or medium so we
51:30 - can just go ahead and give it this
51:32 - zero array back
51:34 - now one thing to finally note about that
51:36 - is that this is a very useful setting if
51:39 - you're generating your x matrix so to
51:41 - say
51:42 - but you don't want to do that if you're
51:44 - generating your y labels because those
51:47 - are things that you want to have very
51:49 - strict control over
51:51 - in this series of videos i've shown you
51:53 - some of the pre-processing steps that
51:55 - are available
51:57 - but a very convenient way for you to
52:00 - play around with more of them and to get
52:01 - a better understanding is to go to this
52:04 - website called drawdata.xyz
52:07 - and full disclaimer it's a website that
52:09 - i made but it's a website that allows
52:11 - you to
52:12 - quite literally
52:14 - make a drawing of a little bit of data
52:19 - and that way you can play with it
52:23 - from your jupyter notebook and playing
52:25 - with preprocessors is the best way to
52:27 - learn about them now what you can go
52:29 - ahead and do from here once you've drawn
52:31 - your data set that you're interested in
52:33 - you can click this download csv button
52:35 - to download this file locally but what
52:38 - you can also do is you can copy the csv
52:41 - to your clipboard
52:43 - what you can then do is you can type
52:46 - pandas.read
52:47 - clipboard
52:49 - and then this will be able to read from
52:51 - your clipboard the only thing you have
52:53 - to do manually is you gotta set the
52:55 - separator to a comma because i think the
52:57 - clipboard is typically reading in from
52:59 - excel
53:00 - but what i can now do is just run this
53:03 - and lo and behold
53:05 - the data set that i was just drawing is
53:08 - now available to me here and this is a
53:10 - really nice way to just get a little bit
53:12 - playful with so i can learn
53:14 - pre-processing steps and pipelines
53:16 - i hope that this series of videos has
53:19 - inspired you to check out these
53:20 - pre-processing steps a little bit more
53:22 - in depth but mainly i would like you to
53:24 - remember that these pre-processing steps
53:27 - really do matter they can have a
53:28 - profound effect on what your algorithm
53:31 - is going to end up doing
53:35 - and that concludes the pre-processing
53:38 - part of this video
53:40 - next we will move on to metrics
53:45 - i was looking around for a fun data set
53:48 - when i found one on kaggle it's a
53:50 - relatively well-known data set about
53:52 - credit card fraud and i figured it'd be
53:54 - a nice example to explain something
53:56 - about scikit-learn
53:59 - because kaggle is giving me a data set
54:02 - and this data set consists of a label
54:05 - that i would like to learn as well as
54:07 - the data that i'll be using to predict
54:09 - that label
54:13 - and typically you would start building
54:14 - your model but you're not going to build
54:16 - one typically you're going to make a few
54:20 - so you're going to have model a
54:23 - all the way down
54:24 - to model z
54:26 - and these models might be the same type
54:28 - of model but they're going to have
54:29 - different hyper parameters and for all
54:31 - intents and purposes they will be
54:33 - different models because of it
54:35 - and all these different models are going
54:37 - to have a different prediction
54:42 - and what we would like to do
54:44 - is pick the best model
54:47 - and in scikit-learn you would use a grid
54:48 - search for this and what the grid search
54:50 - will do is it will take all of these
54:53 - predictions
54:55 - and it will compare them
54:57 - against
54:58 - our original label
55:00 - using
55:01 - a
55:02 - metric
55:03 - and it's important you get that metric
55:05 - right select the wrong metric and you're
55:07 - going to pick the wrong model
55:09 - so i figured it'd be useful to spend a
55:12 - few videos explaining how psychic learn
55:15 - goes about this metric because it is a
55:17 - really important part of your machine
55:19 - learning pipeline
55:21 - so let's check the data set before we
55:24 - talk about metrics
55:26 - i downloaded the data set into a csv
55:28 - file that's on my downloads folder and
55:30 - i'm reading it in with pandas and the
55:33 - data set is a little bit big so i'm only
55:34 - reading in the first 80 000 rows
55:37 - the data set has a lot of columns and
55:40 - most of them are anonymized
55:44 - at the end of the data frame we see a
55:46 - class that's the thing we're interested
55:47 - in predicting accurately and we see an
55:50 - amount
55:51 - the idea behind this data set is that we
55:52 - have all sorts of anonymized features
55:55 - that's represented by column v1 all the
55:57 - way down to v
55:59 - 28
56:00 - but all of these features describe
56:02 - characteristics of a transaction
56:04 - and we also know the amount that was in
56:06 - this transaction
56:08 - now to get this data frame into
56:10 - scikit-learn it helps to have it in
56:12 - numpy and that's what i've done below
56:15 - x contains all the columns that have a v
56:17 - in it and y is the column over here what
56:20 - i'd like to predict
56:22 - and to emphasize what kind of data set
56:24 - this is let's print some useful
56:26 - information so let's print the shape
56:31 - and let's also list the number of fraud
56:33 - cases
56:37 - so this looks like data that's ready for
56:38 - psychic learn but we have to be aware of
56:41 - one phenomenon and that is the fact that
56:43 - the number of fraud cases you know it's
56:46 - about 200 and that is out of 80 000
56:49 - cases
56:51 - so it's safe to say that this data set
56:54 - is unbalanced there are way more cases
56:57 - with
56:58 - fraud than without
57:00 - and that is something to keep in mind
57:03 - so keeping that in mind let's start with
57:05 - a simple model and i'll just use
57:08 - logistic regression for now
57:12 - so first i'm going to import it
57:18 - and then i'm going to say hey please fit
57:21 - on that data and make a prediction as
57:23 - well
57:26 - now when i run this i get an error
57:28 - saying that the
57:30 - total number of iterations has been
57:31 - reached and what's probably happening
57:34 - here is that because this data set is so
57:35 - imbalanced it's not converging within
57:38 - the number of iterations that it has
57:40 - so what i'll do is i'll just set the
57:42 - maximum number of iterations to be
57:44 - higher i think the number of iterations
57:45 - initially is a hundred but let's set it
57:47 - to a thousand
57:49 - okay so that works
57:51 - what i'll now do is i'll take the sum
57:53 - over all of the predictions that i have
57:56 - so this is by no means a perfect metric
57:59 - but what i would like to observe now is
58:00 - if i'm overfitting on just the train set
58:03 - then the model detects fewer cases
58:07 - than i actually have in my data set
58:11 - so without grid search i can already
58:12 - tell that this model could be doing
58:14 - better
58:15 - so let's add a setting that might allow
58:17 - us to
58:18 - move the algorithm in the direction that
58:20 - we're interested in
58:23 - and for logistic regression one
58:25 - convenient way of going about that is to
58:27 - say hey let's specify the class weight
58:30 - now the class weight is a dictionary
58:33 - that allows me to specify how much
58:35 - weight to assign to each class
58:39 - and in particular the way that you
58:40 - should read it
58:41 - is that for class
58:43 - 0 that will be the non-fraud cases then
58:47 - we assign a weight of 1.
58:52 - but for my class of one which would be
58:55 - the fraud cases
58:57 - i'm saying well let's give that double
58:58 - the weight the idea being that we're
59:01 - gonna get more fraud cases selected so
59:03 - let's run this and see what the effect
59:05 - is
59:08 - and booyah i'm able to find more fraud
59:10 - cases this way
59:12 - so this is a pretty good place to start
59:14 - i have a setting here that i would like
59:16 - to optimize and that means that right
59:18 - now i can start worrying about the grid
59:20 - search and about the metrics
59:23 - so now that this basic example works
59:26 - let's get a grid search going so we can
59:28 - find the best value for this class
59:30 - weight
59:32 - and i'll start with basic settings
59:34 - let's first import it
59:37 - and next let's define our grid search
59:40 - i'll first need to pass it an estimator
59:42 - and that's our logistic regression in
59:44 - this case
59:45 - and let's also set the maximum number of
59:48 - iterations to a thousand
59:50 - the next thing i have to do is i have to
59:51 - set the parameters
59:53 - and i'm interested in changing this
59:54 - class weight
59:57 - so now i have to give it a list of
59:59 - settings to loop over and in particular
60:01 - the settings have to be dictionaries
60:03 - like so
60:07 - and let's use a list comprehension for
60:09 - that
60:11 - so next let's specify how many cross
60:12 - cross-validations we want to do and
60:15 - i'll just say 4 for now
60:17 - and
60:18 - i have a couple of cores on this machine
60:20 - so i'll set the number of jobs to -1 and
60:22 - that way this grid search can occur in
60:24 - parallel
60:26 - my grid is now defined and what i can do
60:28 - is i can tell my grid to go ahead and
60:30 - fit
60:34 - and it's done training
60:36 - so i'll start a new cell here
60:40 - because this grid object it has a cv
60:43 - results property now and this contains
60:46 - all of the results from the cross
60:47 - validation
60:51 - and that's a dictionary with lots of
60:53 - values
60:56 - but i can easily turn that into a data
60:58 - frame
61:04 - so when i look at these results i see
61:06 - the class weight appear
61:09 - and i also see the
61:11 - scores
61:12 - so for every cross validation split we
61:15 - see this score
61:17 - and it's this score that grid search is
61:19 - using to pick the best model
61:21 - but then we gotta wonder
61:23 - how has it come up with this score
61:26 - because we didn't specify any metrics in
61:28 - our grid search
61:29 - yet it is able to find this score right
61:32 - here
61:35 - look there's no metrics
61:39 - now where that score comes from is from
61:42 - the
61:43 - model
61:44 - i've got a logistic regression right
61:45 - here
61:46 - and what i can do is i can ask for the
61:48 - scoring method that is in there
61:52 - so we see that there's a bound method
61:53 - called score
61:55 - and if i use
61:56 - two question marks then i can see the
61:58 - implementation of it
62:00 - so this is the implementation and i see
62:01 - a doc string
62:04 - and if i look at the implementation it
62:07 - says that from metrics it's importing
62:09 - the accuracy score internally
62:12 - so that means that this logistic
62:14 - regression
62:15 - it has a score
62:17 - and unless we specify otherwise the
62:20 - score for logistic regression is just
62:21 - going to be accuracy
62:23 - and when i look down below
62:27 - at this mean test score
62:29 - then that makes sense
62:31 - the model is predicting no fraud most of
62:34 - the time so we're getting a really high
62:36 - accuracy there
62:37 - but this is not the metric that i might
62:39 - be interested in though so let's change
62:42 - that
62:43 - so let's import some things from the
62:46 - scikit-learn metrics module
62:48 - and in particular i'll just import the
62:50 - precision score
62:52 - and the
62:54 - recall score
62:56 - now the way these score functions work
62:57 - and i'll take precision as a first
62:59 - example
63:00 - is you can pass it the
63:02 - true values those are the values that it
63:05 - should predict
63:06 - and you can give it the predicted values
63:09 - so i'll just predict some
63:13 - and so far so good
63:15 - i can do the same thing with recall
63:19 - now precision and recall both measure
63:21 - different things
63:23 - what recall will tell me
63:25 - is it will tell me
63:26 - did i get all the fraud cases and
63:30 - precision is saying
63:31 - given that i predict fraud
63:34 - how accurate am i
63:37 - and you can imagine in an extreme
63:38 - example
63:39 - if i were to say
63:41 - hey let's predict that every single case
63:44 - is a fraud case
63:45 - well then the recall score is going to
63:47 - be really high and the precision score
63:49 - is going to be really low
63:52 - in another extreme case suppose i find
63:54 - one candidate that's a fraudulent
63:56 - candidate but nobody else gets predicted
63:59 - as fraud then i'll have a really high
64:01 - precision and a really low recall
64:05 - and you can imagine that if you're going
64:06 - to optimize for either of these two
64:08 - you're going to get a substantially
64:10 - different algorithm and in this case the
64:12 - crux is do we care more about false
64:14 - positives or false negatives
64:18 - for now i'll assume that we care a
64:19 - little bit more about precision
64:21 - so what i will do is i will take these
64:24 - two metric functions and add them to the
64:26 - grid search
64:28 - now let's add precision and recall to
64:30 - the grid search
64:32 - to add it to the grid search we have to
64:33 - pass a
64:34 - scoring dictionary and i can say hey
64:37 - i've got my precision score
64:40 - and i've got my recall score
64:42 - there's one extra thing we have to do
64:44 - though and that is if you want to use
64:46 - these functions inside of a grid search
64:48 - then we have to pass that to the make
64:51 - score function
64:52 - first
64:54 - we'll discuss later why they make this
64:56 - distinction
64:58 - but by doing this we now say hey
65:01 - let's add these metrics now as well
65:04 - the one extra thing that i have to pass
65:05 - now as well is the refit if i just tell
65:09 - so i could learn hey these are the extra
65:11 - scores that i want you to keep track of
65:13 - so i could learn will do it
65:15 - but if i want this grid search to select
65:17 - the best model based on one of these
65:19 - scores then i have to explicitly mention
65:22 - which score it has to optimize over
65:25 - so let's run this
65:27 - and i just ran
65:29 - and one of the effects that you now see
65:31 - is that we have
65:33 - the test precision that's now listed
65:35 - here
65:38 - as well as a test recall score
65:43 - one extra thing that we could add now is
65:46 - these are the test scores which are
65:48 - useful but sometimes it's also nice to
65:50 - see the train scores as well so we can
65:52 - set a flag that we want to also get the
65:54 - train scores and our cross-validation
65:56 - results
65:59 - and if we now scroll all the way to the
66:01 - back
66:02 - we should at some point see
66:05 - yes
66:06 - scores for the train set as well
66:09 - since the grid search is now pretty well
66:11 - set up it will be good to do a proper
66:14 - run
66:14 - so i will change two things
66:16 - for starters i'm just going to increase
66:18 - the number of cross validations
66:21 - by having more cross validations this
66:23 - will take longer to run but we should
66:25 - have more accurate metrics coming out
66:28 - next what i'll do is i will replace this
66:30 - range 4
66:32 - with a numpy linear space
66:36 - and that allows me to say hey let's
66:38 - start at 1 let's end at 20 and let's
66:41 - have 30 steps in between
66:43 - this should give me a higher resolution
66:45 - on the effect of this class weight
66:49 - and again by setting this value v higher
66:51 - i'm telling the algorithm to focus on
66:54 - the fraud cases
66:56 - so i will now run this
66:57 - and when it's done running i'll show
67:00 - some charts that summarize this
67:03 - so it is now done running and
67:05 - i've made a few charts
67:09 - this is the first one
67:10 - it shows the test results
67:13 - we have the weight
67:16 - on this axis
67:17 - of the class weight
67:19 - and we see the two scores on the y-axis
67:23 - and if you want to have a really good
67:24 - precision you have to be on this end of
67:26 - the spectrum
67:27 - and if you want to have a really good
67:28 - recall you have to be on this end of the
67:30 - spectrum
67:31 - and note
67:32 - that if you want to have a balance
67:34 - between the two now that you're
67:36 - somewhere in the middle
67:38 - so this is interesting but you might
67:40 - wonder what do the scores from the train
67:42 - sets tell us
67:48 - and again we have our class weight here
67:51 - and our scores
67:52 - but we get a completely different
67:54 - picture
67:58 - so it's a good reminder that
68:00 - cross-validating results is a good idea
68:03 - because i've got two metrics now
68:05 - scikit-learn is able to optimize either
68:07 - of them
68:08 - so at the moment i'm able to either
68:10 - optimize for precision
68:12 - which is going to give me the model over
68:14 - here with very very high precision and
68:16 - very low recall
68:18 - or i can say hey pick the model with the
68:20 - best recall and then i have the opposite
68:22 - problem
68:24 - maybe instead i want to be here in the
68:26 - middle
68:28 - and there are two ways of going about
68:30 - that
68:31 - one might be to go for another metric
68:33 - that's inside a psychic learn
68:35 - in this particular case the f1 score is
68:38 - something that you might be interested
68:39 - in because the f1 score represents a
68:41 - balance between precision and recall but
68:44 - it might be more interesting instead to
68:46 - make our own
68:47 - scikit-learn does not support every
68:49 - single metric out there so it is good to
68:51 - be able to write your own sometimes
68:53 - and in this case i think it might be
68:55 - cool to have one that selects the
68:57 - minimum
68:58 - out of the recall and precision score
69:02 - if i select the best model based on this
69:05 - metric then i will have a model that
69:07 - balances the two
69:09 - so let's go and implement this
69:13 - now you might remember that the
69:15 - precision score function that we had
69:17 - that's the function that we're using
69:18 - over here but the signature of that
69:20 - function was that we said hey there's
69:23 - these true labels going in
69:25 - and there are these predicted labels
69:27 - going in and the output of this function
69:29 - was a number
69:30 - so let's use that as a template to
69:33 - create our own and i will call this the
69:35 - min
69:36 - recall precision
69:39 - and if i want to calculate the minimum
69:41 - between precision and recall i'll first
69:43 - calculate the
69:46 - recall next i'll calculate the precision
69:52 - and then i'll return the minimum of the
69:53 - two
69:56 - let's now add this
69:58 - to our grid search
70:04 - i'll just call that the minimum of both
70:06 - and again i gotta pass that to the make
70:08 - scorer
70:10 - and let's now also say that the grid
70:12 - search has the refit on it let's now run
70:15 - this
70:18 - it's done running now and let's have
70:20 - another look at the charts
70:22 - i updated them while the grid search was
70:24 - running
70:26 - so one thing that's interesting is it
70:28 - does seem that the
70:29 - grid search would now pick a model
70:31 - that's around this region over here
70:34 - i can't exactly see where but it makes
70:35 - sense it's
70:36 - close to where the two are balanced
70:40 - so that's interesting
70:43 - but you might wonder well
70:45 - the green line well that green line
70:47 - that's
70:48 - always lower than either of the two
70:52 - i might wonder well why is that
70:54 - i'll leave it as a small exercise for
70:56 - yourself to figure out why
70:59 - and you can check the appendix for the
71:00 - answer
71:01 - in the previous video we've made our own
71:05 - metric here
71:06 - and then we used it in the grid search
71:09 - over here
71:10 - but before we were able to use it we
71:12 - first had to pass it to this make scorer
71:15 - function
71:17 - and it wasn't just our own function we
71:18 - also did this for the precision and the
71:20 - recall as well
71:22 - so what's up with that
71:26 - so to show what is happening there
71:29 - let's make
71:30 - a scorer
71:32 - using our min precision recall function
71:35 - and then just let's just have a look at
71:37 - the implementation
71:40 - so the make score function apparently
71:43 - takes the min recall precision function
71:45 - that we had
71:46 - that's this one
71:49 - and it takes that function and then
71:51 - turns it into a predict scorer object
71:55 - and it has a signature
71:58 - in goes an estimator
71:59 - x y true
72:01 - and some form of sample weight
72:06 - so one way of looking at this is to say
72:08 - well
72:10 - we start with a metric function
72:12 - that accepts y true
72:15 - the true labels and y predicted the
72:17 - predicted labels
72:19 - and what makes score is doing is it's
72:22 - turning that into
72:24 - something else some other callable
72:27 - object
72:28 - where i can pass it the estimator
72:31 - my x data set my entire y labels and a
72:35 - sample weight as well
72:37 - and this is what the make score function
72:40 - does
72:42 - the idea behind it is that sometimes you
72:44 - would like to use the function this way
72:46 - if you're in a notebook and you just
72:48 - want to quickly use a metric then
72:50 - calling that directly can be useful
72:52 - but it would be a shame if you had to
72:54 - rewrite that function such that the grid
72:56 - search will be able to use it
72:58 - because the grid search likes to have
73:00 - something callable where you also have
73:02 - access to the estimator now what i just
73:04 - quickly want to do
73:06 - is rewrite this function over here
73:09 - such that i no longer need
73:12 - make scorer
73:13 - just to demonstrate that this is indeed
73:15 - the case and we'll do that using this
73:18 - signature over here
73:22 - so i've copied the signature
73:24 - this is now the input
73:26 - and let's now fill in this function
73:29 - well what i can do is i can first
73:31 - calculate the
73:32 - predicted y values from this estimator
73:36 - that is being passed in
73:38 - so that means i can just say s dot
73:40 - predict
73:42 - and now
73:43 - i should no longer need this make scorer
73:47 - so let's run this
73:51 - and it ran
73:53 - and let's now just check if we get the
73:54 - same chart
73:59 - excellent
74:01 - when you're looking at this min recall
74:03 - precision function then there's probably
74:06 - a lot of parts that you recognize
74:08 - for example the estimator that we have
74:10 - here
74:11 - that is going to be a logistic
74:14 - regression like we've defined here
74:16 - with the correct settings for the class
74:18 - weight and and you can also imagine that
74:20 - we have this x here and this y true and
74:23 - that these will be the train or test set
74:26 - labels and data sets
74:28 - but you might wonder well what's up with
74:30 - this sample weight over here
74:32 - and sample weight well that's an extra
74:35 - feature that can be passed along
74:37 - certain machine learning models allow
74:39 - you to say well this row is more
74:42 - important than this other row that's
74:44 - different than the class weight the
74:46 - class weight that we have over here that
74:48 - says hey
74:49 - this class should get more attention but
74:52 - the sample weight allows us to pass data
74:56 - that says this row is more important
74:57 - than that row
74:59 - and in the case that we're dealing with
75:00 - fraud where we have rows that resemble
75:03 - financial transactions you could maybe
75:05 - say that maybe the transaction amount
75:09 - maybe that's a valid setting the really
75:11 - big transactions that have millions of
75:13 - dollars in it well sure if there's fraud
75:16 - there that's much more important to
75:18 - catch than if it's only about a single
75:20 - dollar
75:21 - so one thing that i could do is i could
75:23 - rewrite this entire function to take
75:25 - that sample weight into account
75:28 - one thing that i would need to do though
75:29 - is i would also need to
75:31 - give a sample weight
75:33 - and that's something i have to do over
75:34 - here
75:36 - so let's say i'll do that
75:41 - and what i can do here is i can say hey
75:43 - take that original data frame and take
75:45 - the amount column that is the amount of
75:48 - the financial transaction
75:50 - and just for numerical stability what
75:52 - you can do is you can also maybe not
75:54 - take the exact amount but the logarithm
75:56 - of it this way if you have really big
75:58 - transactions then you prevent that we're
76:00 - overfitting on it
76:01 - and what i'm going to do now is i'm just
76:03 - going to run this and i'll leave this
76:05 - the way it is i'm not changing this
76:09 - if i have a principled way of taking
76:11 - that sample weight and translating that
76:14 - to a metric i want to optimize
76:16 - then that is definitely something i can
76:17 - now do here which is good that's a
76:20 - benefit that gives me flexibility but i
76:23 - don't have the financial expertise to
76:24 - come up with a good rule now so what i
76:26 - just want to briefly show is if we
76:28 - actually do this
76:30 - what is the effect
76:32 - because even though we're not changing
76:33 - the metric adding this effect will
76:35 - change the algorithm
76:37 - so i'll run this now and show you the
76:39 - summary charts to see what changed
76:45 - so just for reference this is the
76:47 - original chart
76:49 - and this is where the balancing point is
76:51 - between precision and recall and let's
76:54 - now see how this compares so the dashed
76:56 - line is where the balance was before
76:59 - and the dotted line is where the balance
77:00 - is now so adding sample weight will
77:03 - definitely influence the algorithm here
77:06 - but it's good to know that we could also
77:08 - use the sample way for our metric
77:10 - if we wanted to
77:14 - so let's consider a new approach to this
77:16 - problem
77:17 - we could consider that perhaps
77:20 - fraud
77:21 - is like
77:22 - an outlier
77:24 - it's a rare event
77:26 - but it's also something out of the
77:27 - ordinary so one could wonder sure we can
77:30 - have a classifier go in there and
77:32 - predict whether or not something is
77:33 - fraud but if we just have an outlier
77:36 - detection algorithm might that not
77:38 - correlate with fraud
77:40 - so what i would now like to do is
77:42 - replace the logistic regression with an
77:44 - outlier detector and then adapt these
77:46 - metrics such that we can check if this
77:48 - is the case
77:51 - so let's first write a little bit of
77:53 - code that can handle outlier detection
77:56 - i'll just start by importing the
77:57 - isolation forest algorithm
78:00 - and because it's an outlier detection
78:02 - algorithm when i call dot fit it doesn't
78:04 - need a label it just needs the data set
78:08 - i can still use it to make predictions
78:10 - though
78:11 - so let's just run this
78:13 - when i look at this output it seems like
78:15 - everything here is a 1 as if it's always
78:17 - predicting fraud but that's not the case
78:20 - to show that i'll just quickly import a
78:22 - counter object
78:23 - and i'll just count this how often a one
78:26 - occurs and how often another number
78:27 - occurs
78:29 - and that's a thing psychic learn assumes
78:32 - that a one represents not an outlier and
78:36 - minus one that that does represent an
78:38 - outlier
78:39 - so if i want to translate these outlier
78:41 - detections
78:43 - what i got to do is i got to say well if
78:45 - these predictions are equal to negative
78:46 - 1
78:47 - then i want to have a 1 because that
78:49 - indicates a label for fraud
78:51 - and 0 otherwise
78:54 - now with that observation done
78:56 - let's just briefly go back here
79:00 - and look at our metrics because a lot of
79:03 - stuff now will break
79:04 - these recall and precision scores
79:07 - they expect zero or one values at the
79:10 - moment
79:11 - not negative one so if i were to put an
79:14 - isolation force here the metrics would
79:16 - break that is unless
79:19 - i write my own variant over here
79:22 - so i'll just quickly go ahead and do
79:24 - that
79:27 - and there we are i have made two
79:30 - functions over here
79:31 - that don't need the make scorer like we
79:34 - saw before
79:35 - and by doing it this way these
79:37 - predictions are now turned from outlier
79:40 - predictions into fraud label predictions
79:44 - what i've now also changed is i have
79:46 - removed the logistic regression here and
79:48 - put in the isolation forest
79:50 - and i have now also put in the
79:52 - contamination factor here as the hyper
79:54 - parameter that i want to tune
79:56 - again i'll not go into the details on
79:58 - what that means but we could say it's
80:00 - something i want to optimize
80:02 - finally the two functions i have here
80:05 - again are being put into the scoring
80:07 - methods down here below
80:11 - and here's the small amazing thing about
80:14 - this
80:14 - notice again that this isolation forest
80:18 - when you train an isolation force you
80:21 - call it via fit and then you pass it
80:24 - an x matrix without y labels per se
80:28 - but because we are passing in y labels
80:30 - below here anyway
80:32 - we are able to use them here
80:36 - in our own custom metrics
80:38 - and that is something really flexible
80:42 - that means that we can pass it because
80:44 - that allows us to use outlier detection
80:46 - algorithms as if they were classifiers
80:50 - so what i'll now just quickly do is run
80:52 - this
80:53 - and then make a similar chart like what
80:55 - we had before
80:58 - so here's the similar chart on the
81:00 - x-axis again we have our hyper parameter
81:03 - and on the y-axis we have our scores
81:06 - and when looking at this it does seem
81:08 - that even though the outlier detection
81:10 - algorithm is able to detect some things
81:12 - it's not able to detect as well as the
81:15 - logistic regression that we had before
81:18 - that said this can still be a useful
81:19 - feature to have in your pipeline but
81:21 - that is outside the scope of these
81:23 - series of videos
81:24 - what is amazing here is that we are able
81:26 - to use metrics to quickly judge if an
81:29 - outlier model would be useful in this
81:32 - classification problem
81:35 - in this series of videos i hope to have
81:37 - been able to show you how you can use
81:39 - metrics in your scikit-learn pipelines
81:41 - and that it actually offers a very
81:43 - flexible api
81:45 - that said i do think it's fair to point
81:47 - out that you don't always have to make
81:49 - your own metrics very often the metrics
81:52 - that scikit-learn has will be relevant
81:55 - to your project too
81:56 - you can have a scroll around there's
81:58 - metrics for classification but also for
82:00 - regression and it's definitely worth to
82:02 - spend some time here on the
82:04 - documentation page
82:06 - before wrapping up though i do want to
82:07 - point out one small danger with the
82:10 - current approach
82:12 - the way that we model things is by
82:14 - taking our data frame passing it through
82:16 - a grid search to get our model while
82:19 - also checking the metrics and in terms
82:21 - of methodology this is a fair approach
82:24 - but there is one big danger and that's
82:27 - especially true in the case of fraud
82:30 - do we really trust these labels
82:33 - especially in the case of fraud
82:35 - it may be safe to assume that we don't
82:37 - have a complete picture of all the fraud
82:39 - cases it's a cat and mouse game so when
82:41 - i see a label for fraud i think i can
82:44 - assume that that's accurate but when
82:46 - there's no fraud that might just be
82:48 - fraud that's undetected and if we think
82:50 - a step further probably the labels that
82:53 - i have
82:54 - those are the labels of the easy cases
82:56 - those were the cases that got caught and
82:58 - if we then train a model that optimizes
83:01 - for the easier cases then we have to be
83:03 - honest we might have a model that has a
83:05 - blind spot for the harder to detect
83:08 - fraud cases
83:10 - so i must stress it is really important
83:12 - that we keep track of our metrics that
83:14 - we take that very serious because it has
83:16 - a huge impact on how we pick the model
83:18 - but typically it is not enough we still
83:20 - need to concern ourselves with the
83:22 - quality of these labels
83:24 - because if the labels are wrong then so
83:26 - is your metric
83:29 - in the previous video we were using the
83:32 - make score function
83:35 - to take our custom metric
83:38 - and to turn that into something that the
83:40 - grid search was able to use
83:43 - and before delving in depth in what the
83:45 - make scorer function does exactly i
83:47 - figured it'd be good to point out that
83:49 - when it comes to customization there's a
83:51 - couple of settings that you can still
83:52 - add
83:53 - not every metric will have the rule
83:56 - where greater is better
83:58 - if you take mean squared error for
84:00 - example in that case
84:02 - greater will be worse so
84:04 - that is something you want to specify
84:06 - otherwise the grid search might pick the
84:08 - wrong model
84:09 - and there's also metrics that depend on
84:12 - having a
84:13 - probability measure being there as
84:15 - opposed to just a class and if you need
84:18 - the probability measure for your metric
84:20 - that's also something you ought to
84:21 - specify
84:23 - so for example
84:25 - let's say i take this min recall
84:26 - precision function and if greater was
84:29 - worse for this metric you could specify
84:32 - greater is better
84:33 - is false
84:35 - and then you would pass this in your
84:37 - grid search
84:38 - now note that this is of course a bit of
84:40 - a detail but there will be moments when
84:43 - you need it so i figured it'd be good to
84:44 - at least mention it
84:49 - let's now move on to a relatively fancy
84:52 - feature of psychic learn
84:54 - the ability to work with meta models
85:00 - usually in scikit-learn what you'll do
85:02 - is you'll make a pipeline
85:05 - and that pipeline will have
85:06 - pre-processing tools
85:08 - maybe some featurizers
85:10 - and after that you'll have a machine
85:13 - learning model
85:16 - and the idea is that this pipeline is
85:17 - just a very convenient way to
85:19 - connect everything here however
85:22 - if you think about what you might want
85:24 - to have happen inside of a pipeline
85:26 - then there might be some steps you would
85:28 - like to have after you've made your
85:30 - model maybe there's some post processing
85:33 - you would like to do
85:34 - the way that the scikit-learn pipeline
85:36 - works however is that you're able to
85:38 - have many featurizers and pre-processing
85:40 - tools chained together but at the end it
85:43 - stops with this model
85:45 - the thing that can do fit predict that
85:47 - should be the final thing in your
85:48 - pipeline so that might make you wonder
85:51 - how can we go about some of these extra
85:53 - steps and post processing tools we might
85:57 - have things that we would like to grid
85:58 - search so it will be nice if there's a
86:00 - trick up our sleeves that allows us to
86:03 - still have access to these sorts of
86:05 - techniques in our scikit-learn pipeline
86:08 - over here now it's a bit of an advanced
86:10 - trick but the way to go about this is to
86:13 - think about meta estimators
86:17 - scikit-learn has a few of them but
86:19 - there's a lot of them implemented in
86:20 - other tools like scikit lego
86:23 - full disclosure this is a tool that i
86:25 - made
86:26 - but the idea is to have an estimator
86:28 - that can take this model
86:30 - and add extra behavior to it
86:33 - it's a powerful modeling technique so in
86:35 - this series of videos what i would like
86:37 - to do is highlight a few of these meta
86:39 - estimators and show you how you can use
86:42 - them to make your modeling pipelines a
86:44 - bit more expressive
86:46 - scikit-learn has a classifier that's
86:48 - known as the voting classifier and it's
86:51 - an example of a meta estimator
86:53 - to help explain what it does though
86:55 - let's consider this classification task
86:58 - it's definitely an artificial data set i
87:01 - am using the make classification method
87:03 - to generate it but what you can see here
87:05 - is that i've got these blobs of yellow
87:08 - points
87:09 - as well as purple points
87:11 - and what you can see is that there are
87:14 - some purple points around here where
87:16 - there's mainly yellow points and vice
87:18 - versa there's
87:19 - also a few yellow points around here
87:22 - now there are two kinds of models i
87:24 - suppose that you could make here if we
87:27 - were to say train a logistic regression
87:29 - on this data set then effectively what
87:31 - the logistic regression would do
87:33 - is it would pick the direction where it
87:35 - can make the most difference
87:37 - and then create one separating line in
87:40 - this case it probably
87:41 - will have a separating line somewhere
87:43 - over here
87:44 - and it will say that everything on this
87:46 - end of the line is supposed to be the
87:48 - yellow class and everything on this side
87:50 - of the line is supposed to be the purple
87:52 - one a different algorithm near his
87:54 - neighbor
87:55 - would do something slightly different
87:57 - whenever it needs to predict a point
87:58 - let's say a point over here
88:00 - it would look at all the neighbors let's
88:02 - say the nearest five and then make a
88:04 - prediction and that would mean that in
88:06 - this region over here it might predict a
88:09 - purple point and where the logistic
88:11 - regression is a little bit too general
88:14 - as in it's splitting up everything here
88:16 - on the left and the right you can
88:17 - imagine that the nearest neighbor is
88:19 - maybe too specific at times
88:21 - and you might want to have a way to
88:23 - balance these two things out and being
88:25 - able to balance these two that is
88:27 - something that the voting classifier can
88:29 - do
88:30 - the idea behind the voting classifier is
88:32 - that you can give it a list of
88:34 - estimators
88:35 - and you can also give it a weight
88:38 - for each estimator
88:40 - and that might allow you to say well i
88:42 - want this estimator to be weighted twice
88:44 - as heavily as this one
88:46 - the nice thing about this is that this
88:48 - weight over here is something that you
88:50 - can use inside of your grid search and
88:51 - that's really nice because that means
88:53 - you don't manually have to specify these
88:55 - weights you can have the grid search
88:57 - determine the best way to balance
88:59 - different models for your data set
89:05 - and here's an example of how the voting
89:07 - classifier works on this data set i've
89:10 - got my voting classifier over here that
89:12 - i've imported beforehand
89:14 - and then i kind of submit something that
89:16 - resembles a pipeline it's a list of
89:18 - tuples with the name that i associate
89:21 - with an estimator and then the estimator
89:24 - itself note that the first classifier
89:26 - going in here is this logistic
89:28 - regression and the second classifier
89:30 - over here is this k neighbors classifier
89:33 - but what i'm able to specify is i'm able
89:35 - to say well
89:36 - these are the weights i would like to
89:38 - associate with both models and i would
89:40 - like you to do soft voting and soft
89:44 - voting over here
89:45 - effectively means that we are averaging
89:47 - the predict proba values
89:50 - now to show the effect of this i've
89:52 - created a couple of charts this chart
89:54 - over here shows the original data and
89:56 - this chart over here shows the proba
89:59 - predictions of the first model this one
90:03 - and you can see we get the behavior that
90:05 - we expect there's a line that's
90:07 - separating everything and you're either
90:09 - on the left hand or right hand side of
90:11 - it
90:12 - this chart represents the predictions
90:13 - from the k neighbors classifier and you
90:16 - can definitely see
90:18 - that it's zooming in on a couple of
90:20 - areas we see darker colors appear in the
90:22 - yellow region and we also see lighter
90:25 - colors appear in the dark purple region
90:27 - and what you can see in this third chart
90:31 - are the predictions from this voting
90:33 - classifier
90:34 - and you can see that we basically smooth
90:36 - the predictions of these two models what
90:38 - i can do
90:40 - is i can say well
90:41 - let's give a somewhat higher weight to
90:44 - the k nearest neighbor
90:46 - model and by doing that you can see that
90:49 - these two charts are now definitely more
90:51 - similar
90:53 - but i can also give a higher weight to
90:55 - the logistic regression classifier as
90:57 - well
90:58 - so there's two things to observe here
91:01 - first of all there's merits to having
91:03 - such a voting classifier you can combine
91:06 - different models that work different
91:07 - ways this way which is nice but moreover
91:10 - the main thing that's happening here
91:11 - code wise is that we have an estimator
91:13 - here
91:14 - that takes as input
91:16 - other psychic learn estimators
91:19 - this voting classifier is adding
91:21 - behavior to both of these models and
91:24 - that can be a powerful modeling strategy
91:28 - note that if we put this voting
91:29 - classifier inside of a pipeline that
91:31 - this is still the final model at the end
91:34 - of it but we are able to add behavior
91:37 - because models can be used as input here
91:39 - and this way of thinking about models
91:41 - gives us a lot of flexibility and
91:44 - expressiveness
91:45 - let's now say that we have a slightly
91:47 - different data set
91:49 - again it's an artificial one made with
91:51 - the make blobs function in scikit-learn
91:54 - but given that we have a data set like
91:56 - this
91:56 - let's now talk about a different
91:58 - consideration for models let's say that
92:00 - we have a logistic regression that we'd
92:02 - like to fit on this data set you might
92:04 - get a separating line that's
92:06 - somewhere over here let's say
92:08 - the way that the separating line works
92:11 - if i were to draw it out on
92:13 - this axis over here
92:17 - if i were to draw what the probability
92:19 - is that
92:20 - my predicted value is equal to 1 which
92:22 - i'm associating with these yellow points
92:25 - then the predicted curve would look
92:26 - something like this
92:29 - now what's happening here is we have a
92:31 - threshold
92:32 - around where the probability is larger
92:35 - or lower than 0.5 when we actually say
92:38 - to which class a point belongs to you
92:41 - could wonder though
92:43 - what if we just
92:44 - move that threshold just slightly
92:47 - maybe move it somewhere over here
92:51 - maybe at 0.7
92:53 - if we were to do that then this
92:55 - classification line would move upwards
92:58 - it would move over here
93:02 - that's the new one
93:03 - and that's the old one
93:05 - now the consequence of doing this is
93:07 - that your model might become less
93:09 - accurate overall
93:11 - but whenever we do predict that it's
93:13 - going to be a yellow point using this
93:15 - new line we're probably more sure of it
93:18 - in this region over here
93:19 - there's some purple points and there's
93:21 - some yellow points
93:22 - in this region over here there's still a
93:24 - couple of purple points left but it's
93:26 - less
93:27 - so by tuning this threshold over here
93:31 - we might have a nice knob to turn
93:34 - to exchange precision and recall in our
93:37 - model and the ability to do this is
93:39 - provided by the thresholder meta model
93:42 - that you can find in the scikit lego
93:44 - package
93:49 - the way that it works is similar to the
93:51 - voting classifier we have a meta model
93:53 - over here
93:54 - and as input it accepts an estimator
93:56 - in this case logistic regression
93:58 - thresholder accepts any
94:00 - cycler model but it only works in binary
94:04 - classification cases what you're then
94:06 - able to do is you can set this threshold
94:07 - value over here which again is something
94:10 - that you can put inside of a
94:11 - scikit-learn grid search
94:13 - and here's the effect
94:17 - you can see that if we set a very low
94:19 - threshold that maybe the optimal line
94:21 - that we would have had over here kind of
94:24 - moves down a bit
94:26 - over here
94:27 - this setting i assume will be great for
94:29 - recall but bad for precision and if we
94:32 - move the threshold to the other
94:33 - direction over here then we might have
94:36 - something that's very high precision but
94:38 - very low recall
94:41 - what i've got here is an example of how
94:43 - you can maybe use this inside of a grid
94:45 - search i have my modeling pipeline over
94:47 - here
94:48 - inside of that modeling pipeline i've
94:50 - got my one model
94:52 - and this model has this threshold
94:55 - parameter
94:56 - i can refer to that using model
94:58 - underscore underscore in my parameter
95:00 - grid here which i am using in my grid
95:02 - search
95:03 - and what i'm doing is i'm just looping
95:05 - over all sorts of values of this
95:07 - threshold and i'm keeping track of the
95:09 - precision the recall as well as the
95:12 - accuracy and if we scroll down to the
95:14 - results of this grid search we see some
95:16 - interesting things
95:18 - the blue line over here represents the
95:20 - precision
95:21 - and if we look at the threshold value
95:23 - then indeed it seems that the higher the
95:25 - threshold the more picky we are and then
95:27 - also
95:28 - the better the precision is
95:32 - as expected this does come at the cost
95:34 - of this recall over here which is the
95:37 - orange line and we can definitely see it
95:39 - plummet whenever we have a high
95:41 - threshold value now keep in mind that
95:44 - we're doing this on the test sets we're
95:46 - not reporting train numbers here
95:48 - something that's interesting is that for
95:49 - this particular data set it seems that
95:52 - as long as we remain let's say in this
95:55 - range of thresholds the accuracy doesn't
95:57 - really change too much
95:59 - but the precision and recall curves do
96:02 - so that means that you don't have to
96:04 - give up your accuracy
96:06 - to get a bit of a boost in either
96:08 - precision or recall and the nice thing
96:10 - about this threshold is that it's
96:11 - something that's easy to interpret and
96:14 - at the same time it's also a nice
96:15 - example of what i would call post
96:18 - processing
96:19 - in order to tune the threshold i need to
96:22 - have the model ready and it's this sort
96:25 - of post-processing steps that are
96:27 - probably best implemented as a meta
96:30 - model as a model that accepts another
96:32 - model as input
96:35 - what i've got here is a somewhat large
96:38 - pipeline that is predicting the weight
96:40 - of a chicken over time it's being
96:43 - applied to a data set that has a diet
96:45 - column
96:46 - that says something about what the
96:48 - chicken's eating different chickens have
96:50 - different foodstuffs and also time
96:53 - the idea is that chickens gain weight
96:55 - over time but they might gain more or
96:58 - less depending on the diet that they
97:00 - have
97:01 - the way that i'm going about that
97:03 - is i am selecting the diet column
97:06 - and i am one hot encoding that
97:09 - next what i'm doing is i'm taking the
97:11 - time column and i'm just passing that
97:14 - through a standard scaler schematically
97:17 - what is happening is whenever i get my
97:19 - new data set
97:20 - it is going into this pipeline that has
97:22 - a feature union
97:24 - which is
97:25 - splitting up the
97:26 - features into two buckets
97:28 - the time feature and the diet feature
97:31 - the time feature gets scaled the diet
97:34 - one gets encoded
97:35 - and then this becomes the
97:38 - data set that i can use for my machine
97:40 - learning and it's this data set that is
97:42 - being used down below here
97:45 - for my linear regression
97:47 - and here's what the predictions look
97:48 - like
97:51 - for every diet you see a new prediction
97:54 - line
97:55 - now the downside of the way that we've
97:58 - encoded our data
97:59 - is that this linear regression sees the
98:02 - one-hot encoded variable for these diets
98:05 - and the only way that it can deal with
98:06 - it is to see it as a constant shift and
98:10 - if we think about the possible effect
98:11 - that a diet could have then i hope that
98:14 - you might agree that we're not really
98:15 - modeling it the right way here the
98:17 - effect of the diet might be something
98:19 - else than adding a mere constant
98:22 - so then the question is can we maybe
98:25 - reuse this diet in a different way
98:28 - maybe instead of adding a feature that
98:30 - is one hot encoded for every diet
98:33 - can't we maybe
98:34 - group
98:36 - per this diet
98:37 - and then train a different model for
98:39 - each
98:46 - you would have model 1 for diet 1
98:48 - model 2 for diet 2 etc
98:52 - and then maybe what we can do
98:54 - is we can say well
98:55 - whenever there's a new data set x coming
98:58 - into
98:59 - this model
99:02 - then what needs to happen is this
99:05 - internal grouping needs to figure out
99:07 - what diet this new data belongs to and
99:10 - then make a prediction using the
99:12 - appropriate model
99:14 - this behavior can also be implemented as
99:16 - a meta model and scikit lego has an
99:19 - implementation of exactly this
99:22 - and here's an implementation and the
99:25 - effect in this case we have a data frame
99:27 - that's going into this
99:28 - model over here so that means that i can
99:31 - use the column name here to describe the
99:33 - group but if it was a numpy like data
99:36 - structure then you can also refer to the
99:38 - column indices over here you'll note
99:40 - that when we do this
99:42 - the effect of this diet isn't simply a
99:44 - constant
99:45 - instead it's training a different
99:48 - linear regression each of these lines
99:51 - has their own intercept and their own
99:53 - slope and i hope that you can imagine
99:55 - that making models this way can be
99:57 - reasonable the main risk that you have
99:59 - to keep in mind is that it is possible
100:02 - that maybe one of these groups
100:04 - has way more data than another one of
100:07 - these groups
100:08 - so that is something to keep in mind if
100:10 - you're interested in doing something
100:12 - like this
100:14 - let's now consider a time series task
100:16 - there's a basic one over here and one of
100:18 - the properties of this time series is
100:20 - that it's also changing over time
100:22 - there's a seasonal pattern in here for
100:24 - sure
100:25 - but the shape of the seasonal effect is
100:27 - definitely
100:28 - getting amplified as time goes on
100:32 - so let's say you want to maybe model
100:33 - this
100:34 - one way of modeling this is that we can
100:37 - say well let's have a look at every
100:38 - month in the year and let's try to
100:40 - calculate the mean
100:42 - of every month and then that can be our
100:44 - model and what we can do is we can use
100:46 - the grouped predictor for that as well
100:49 - as a dummy regressor
100:51 - and if we were to then calculate the
100:53 - mean
100:54 - for every month the prediction would
100:56 - look something like this
100:59 - again note that we're using the dummy
101:00 - regressor from scikit-learn here and the
101:03 - grouped predictor that we saw in a
101:04 - previous video when you have a look at
101:06 - these predictions though you'll notice
101:08 - that something is definitely off it
101:10 - seems that we're really good at
101:11 - predicting this middle year
101:13 - but we're undershooting in the more
101:16 - recent years and we're overshooting in
101:18 - the far past so that means that we have
101:21 - to wonder well what can we maybe do here
101:24 - to make this just a little bit better
101:26 - and it's pretty good to observe that
101:27 - this is something reasonably general in
101:30 - this particular case i'm definitely
101:31 - looking at a time series but it's not
101:33 - unreasonable to consider that the world
101:35 - is a moving target no matter what you're
101:37 - trying to predict there's something to
101:39 - be said to maybe focus in on the more
101:41 - recent data and maybe to forget the data
101:44 - that's in the past
101:45 - now
101:46 - the interesting thing is that
101:47 - scikit-learn actually provides a small
101:50 - mechanism to deal with this
101:52 - not every model allows for this but some
101:54 - models do
101:56 - the dummy regressor is one of these
101:58 - models though and what we're interested
102:00 - in is a parameter that's tucked away in
102:02 - this fit method
102:05 - if you look at the signature you'll
102:07 - notice that the fit method has this
102:09 - simple weight parameter it's a parameter
102:12 - that's set to none in most cases but
102:14 - what you can do with it is you can say
102:16 - well how much do i weigh my data points
102:19 - let's say that this is my big input x
102:21 - and my array y that i would like to
102:23 - predict then i can have another array of
102:26 - sample weights and what that allows me
102:28 - to do is it allows me to say well
102:31 - this data point over here that's worth
102:33 - 0.1
102:35 - this one is worth 0.2 and maybe this
102:38 - data point is worth 10. the idea behind
102:41 - these weights is that you can say that
102:42 - maybe this data point is way more
102:44 - important to predict than maybe this one
102:46 - or this one and if you can specify this
102:49 - for every row in your data set then this
102:53 - gives you an option to customize now for
102:55 - our purposes it would be nice to have a
102:56 - meta model that can automatically put
103:00 - values for these sample weights into
103:02 - whatever we're trying to predict
103:04 - and in particular something that's
103:05 - general is that we might say you know
103:07 - what let's just do some exponential
103:09 - smoothing if we assume that this data
103:12 - set here is sorted such that everything
103:14 - on this end is the most recent data and
103:16 - everything on this end is the old data
103:19 - then maybe we can say that there's a
103:20 - decay parameter
103:22 - that says that everything that's
103:23 - happening here is super important but as
103:26 - we move to the past it gets less and
103:28 - less important now this idea of adding a
103:30 - decay
103:32 - that is a meta model that's also made
103:34 - available by psychic lego
103:37 - and here's the implementation and you
103:39 - got to pay attention because we're
103:41 - actually using two meta models here
103:43 - we're starting out with our good old
103:45 - dummy regressor but then we say well we
103:47 - want to add decay to it what we're doing
103:49 - is we're saying well the more recent
103:51 - data points matter more than the old
103:53 - ones note that if you want to do this
103:55 - your data set does need to be sorted
103:57 - beforehand next what i'm doing is i'm
103:59 - saying okay take that model but now i
104:02 - want you to calculate
104:04 - the mean not on the entire data set but
104:07 - per month and what's nice about this
104:09 - example is that we can demonstrate that
104:11 - we start with a very basic model but by
104:14 - adding this meta behavior to it we are
104:17 - quite expressive in what sort of model
104:18 - we would like to end up with and we can
104:20 - see the effect over here as well
104:22 - the orange line over here is what we
104:24 - would get if we didn't do any
104:27 - decay
104:28 - and what you can see here is this green
104:29 - line
104:30 - which fits the more recent data very
104:32 - well but it doesn't really fit the old
104:34 - data as much it doesn't really pay
104:36 - attention to that and that's because
104:38 - we're adding a decay that's saying well
104:40 - ignore
104:42 - the far away history and focus on the
104:44 - more recent events
104:47 - one thing to keep in mind here is that
104:48 - by applying this trick you will get this
104:51 - awkward situation where it is possible
104:53 - that your training data has way worse
104:56 - performance than your testing data and
104:59 - that is because you're probably testing
105:01 - towards the future
105:02 - and it's the future that we're actually
105:03 - paying more attention to in our model so
105:05 - don't be too surprised if that's
105:07 - something that you see
105:08 - but the main thing that i hope that you
105:09 - recognize here is that it's very nice to
105:11 - be able to
105:12 - take models that we already know and
105:14 - understand and just add
105:16 - small interpretable behaviors to it it's
105:20 - that that gives us the opportunity to
105:22 - customize a model to the use case that
105:24 - we're interested in and not only will
105:26 - that make our models more accurate but
105:28 - it also makes them just slightly more
105:30 - interpretable which is always a good
105:32 - thing
105:34 - in this final segment we will talk about
105:36 - a tool called human learn
105:38 - and the whole goal of this segment is to
105:40 - showcase that you don't always need
105:43 - machine
105:44 - learning
105:47 - back when data science was really just
105:49 - starting out it wasn't really common to
105:51 - use machine learning algorithms instead
105:53 - what was more common is you would start
105:56 - with a data set and then you would get a
105:58 - domain expert to come up with some
105:59 - business rules and this would give you a
106:01 - system that could automatically assign
106:04 - labels to new data that came in
106:07 - these days though it's a little bit more
106:08 - common to not write your own rules but
106:11 - instead to have a machine learning
106:12 - algorithm figure them out you would
106:14 - collect data as well as labels
106:17 - and then it will be the job of the
106:19 - machine learning algorithm to figure out
106:21 - what the appropriate rules are
106:23 - the question is though in this
106:25 - transition from a rule-based system to a
106:27 - machine learning model have we maybe
106:29 - lost something along the way and there's
106:31 - a couple of issues if you think about it
106:33 - for starters machine learning isn't
106:35 - perfect it might be that the machine
106:37 - learning model is very accurate but that
106:39 - it has behavior that we aren't
106:41 - necessarily proud of
106:43 - think of themes like fairness and it
106:45 - might also be a little bit ineffective
106:47 - odds are that if you're starting out
106:49 - with data science and you've got a use
106:51 - case that there's also a domain expert
106:53 - around who can pretty much tell you what
106:56 - rules already should matter if nothing
106:58 - else
106:59 - it would be nice if we can construct
107:01 - rule-based systems in such a way that we
107:03 - can easily benchmark it against machine
107:05 - learning models so that got me thinking
107:08 - maybe we need to have a tool that makes
107:10 - it easier to write these rule-based
107:12 - systems and that is why i started this
107:14 - new open source project called human
107:16 - learn the goal of the project is to
107:18 - offer scikit-learn compatible tools that
107:20 - make it easier to construct and
107:22 - benchmark these rule-based systems
107:26 - it is made in such a way that it's also
107:28 - easy for you to combine these rules into
107:30 - machine learning models as well and in
107:32 - this series of videos i would like to
107:33 - highlight some of the main tools of the
107:35 - package and how you can get it working
107:37 - in your daily flow
107:39 - what i've got here is a jupiter notebook
107:41 - and one thing to observe is that at the
107:43 - top i have this pip install human learn
107:46 - command and it's this command that makes
107:49 - this ulearn package available to me what
107:52 - i'm doing in this notebook so far is
107:53 - somewhat basic the main thing is i'm
107:56 - loading in this low titanic function and
107:58 - that allows me to load in this data set
108:00 - about the titanic disaster the goal of
108:03 - the dataset is to predict whether or not
108:05 - a passenger survived so we have a column
108:08 - here that we would like to predict
108:09 - that's binary you survived or you didn't
108:12 - and there's a couple of things that we
108:13 - have at our disposal in order to predict
108:15 - that we have the passenger class we've
108:18 - got the name
108:20 - gender age
108:22 - and what we can do is we can take all of
108:24 - this data and put it into a machine
108:25 - learning model and look for a prediction
108:27 - however if i were to come up with a
108:29 - simple domain rule that might also go
108:31 - ahead and work
108:32 - then i could argue that perhaps i can
108:34 - also just have a look at this column
108:36 - here that tells me how much somebody
108:38 - paid something's telling me that the
108:40 - people who paid more for a ticket are
108:42 - probably also the ones who were perhaps
108:44 - more protected odds are that they were
108:47 - on the upper decks and i can definitely
108:48 - imagine that they had better access to
108:51 - lifeboats than everyone else on the ship
108:53 - so that means that i could already come
108:55 - up with a rule-based system i could say
108:59 - hey let's just have a threshold on this
109:01 - fare and everyone above the threshold
109:04 - survived and everyone below the
109:05 - threshold didn't probably the easiest
109:07 - way to implement this logic in python is
109:10 - to write it down as a function
109:12 - which is what i've done here i'm
109:14 - assuming that a data frame goes in as
109:16 - the first argument
109:18 - and then i have this threshold parameter
109:20 - and
109:21 - effectively all the logic
109:23 - of predicting is happening in here
109:26 - if you paid more than the threshold then
109:28 - i'm assuming that you survived and i'm
109:30 - returning in numpy array
109:33 - and what i could do
109:35 - is i could just say hey
109:37 - let's apply that fair based function to
109:39 - my input data x that i've defined here
109:44 - and i will get all sorts of predictions
109:46 - here
109:47 - the issue with this is that
109:49 - this is a function it is not something
109:52 - that scikit-learn is able to accept as a
109:55 - classifier and that means that i cannot
109:58 - use it in a grid search or pipeline and
110:00 - that's a bummer because i would really
110:02 - like to be able to grid search this
110:05 - threshold hyper parameter over here
110:07 - lucky for us though there's a tool
110:09 - inside of humanlearn that addresses this
110:13 - the tool that we want to use now is
110:14 - called a function classifier
110:17 - and i'm importing it here
110:19 - what the function classifier will do is
110:21 - two things it will take a function that
110:24 - you've written that outputs a prediction
110:26 - and it will turn it into a scikit-learn
110:28 - compatible classifier the second thing
110:31 - that it will then do is it will also
110:33 - make sure that any arguments that you've
110:35 - defined
110:36 - here when creating the function
110:38 - classifier that overlap with the
110:41 - arguments in the function itself
110:43 - that those arguments can be grid
110:46 - searched so to say and to show you how
110:48 - it works what i've done here is i've
110:50 - created a somewhat simple grid search
110:53 - i'm doing two cross validations and i
110:56 - have a parameter grid here over the
110:57 - threshold effectively i'm looping over
111:00 - lots of values between 0 and 100 and
111:03 - what i'm doing is i'm keeping track of
111:05 - the accuracy the precision and the
111:07 - recall
111:09 - and what this grid search will do is it
111:10 - will pick the best hyper parameter based
111:12 - on the accuracy
111:14 - when i call dot fit over here this
111:16 - system will run and then i can inspect
111:19 - the grid search results
111:22 - the grid search just ran
111:24 - and i'm taking the
111:26 - results from it
111:27 - turning it into a data frame and then
111:28 - making a chart what you see on the
111:30 - x-axis over here is the threshold value
111:34 - that we set on the fare column and then
111:36 - you see three lines the blue line is the
111:40 - the orange line is the precision and the
111:42 - green line stands for the recall one
111:45 - thing that we can see is that when we
111:47 - increase the threshold the precision
111:49 - goes up that's the orange line and this
111:52 - suggests that the rule that we started
111:54 - with isn't half bad people who paid more
111:57 - for the ticket are probably more wealthy
111:59 - and are therefore perhaps more likely to
112:01 - get to a lifeboat quicker now as a model
112:04 - this isn't necessarily perfect the
112:07 - recall that we have down below here
112:09 - decreases the higher we set the
112:11 - threshold and that's also because
112:12 - there's less and less people that paid a
112:15 - certain amount for it but all in all
112:17 - this is already kind of nice because it
112:19 - allows us to test if machine learning
112:22 - models can make better or worse
112:24 - decisions than domain experts
112:27 - and quantifying that can be nice a final
112:30 - thing to mention about this is how
112:33 - general this is
112:35 - we can come up with any python function
112:37 - that we like here that has any logic
112:39 - that can cause a prediction and then any
112:42 - argument that we have in the function
112:44 - can be used as a hyper parameter inside
112:47 - of a grid search
112:49 - and this in practice can be very very
112:53 - powerful
112:55 - the function classifier is meant to be
112:57 - very customizable that means that you
113:00 - can do more than just come up with a
113:03 - system that can make predictions you can
113:05 - also use it to add behaviors to existing
113:07 - models for example let's say that i
113:10 - already have a classifier and that i
113:12 - also have a outlier detection system
113:15 - something that might be really sensible
113:16 - is that you first check for a new data
113:18 - point whether or not it is an outlier if
113:21 - it is that you then say well let's not
113:23 - make a prediction let's fall back and if
113:26 - it's not an outlier it might also be
113:27 - good to check if the model's certainty
113:30 - or the proba that's in the model if that
113:32 - is high enough and if it isn't that
113:35 - might also be a nice reason to
113:37 - perform a fallback scenario only when
113:39 - these two checks pass
113:41 - then should we take the action that the
113:43 - original classifier takes
113:45 - if you want to construct something like
113:46 - this and you already have an outlier
113:48 - detection model as well as a classifier
113:52 - then the function classifier object can
113:54 - also be used to very easily declare
113:57 - systems that do exactly this
114:01 - to give an example i have written down
114:02 - the pseudo code that you might need to
114:04 - construct something like this i'm
114:06 - assuming that you already have your
114:08 - outlier detector and i'm also assuming
114:10 - that you already have your classifier
114:12 - but then if you want to construct the
114:13 - diagram that we had before
114:16 - the only thing you have to do is declare
114:17 - this one make decision function
114:21 - inside of this function the first thing
114:22 - that needs to happen is you first need
114:24 - to have a classifier that makes a
114:26 - prediction on all the data points that
114:28 - are passed in
114:29 - next if there is any doubt that is to
114:32 - say we calculate the probability values
114:35 - for each class
114:36 - and if the maximum of all those
114:38 - probability values is less than a
114:40 - threshold
114:41 - well
114:42 - then we are going to
114:44 - not use the predictions that we had
114:47 - instead we are going to override it by
114:49 - saying well the class is now doubt
114:51 - fallback and the numpyware function
114:54 - makes it really convenient to write this
114:56 - logic
114:57 - we also like to make sure that it's not
114:59 - an outlier so we apply a similar trick
115:01 - if we have an outlier detection model
115:04 - that's pre-trained we can make a
115:05 - prediction and if the prediction says
115:07 - it's an outlier then we can assign
115:10 - another label instead of the
115:12 - original predictions
115:14 - this gives us our make decision function
115:17 - and our function classifier makes it
115:20 - very easy to then accept that function
115:22 - and we now have a fallback model that
115:25 - you can use in scikit-learn grid search
115:26 - like you're used to
115:28 - now i hope you agree that designing
115:30 - systems like this
115:31 - is beneficial in practice it is really
115:33 - nice that we can add our own little
115:35 - logic in front of a existing machine
115:38 - learning model because that allows us to
115:40 - give a behavior that we are interested
115:43 - in i also hope it makes it clear that
115:45 - the goal of this package isn't to make
115:47 - rule-based systems that act as if
115:49 - they're just predictors
115:50 - instead we want to construct rule-based
115:52 - systems using pre-existing machine
115:54 - learning models the idea being here that
115:56 - this gives us a nice balance between
115:59 - natural intelligence and artificial
116:01 - intelligence
116:02 - let's consider another way of
116:04 - constructing rule-based systems
116:07 - and to explain this i will be using a
116:09 - data set that is in the scikit lego
116:13 - package now to get the data that we're
116:15 - interested in what we got to do is we
116:17 - got to import this load penguins
116:20 - function all we can do then is we can
116:22 - call the function
116:24 - like you see here
116:25 - and this gives us a data frame the goal
116:28 - of this data set is to predict the
116:30 - species of penguin based on the
116:33 - properties of the penguin like the body
116:35 - weight or the length of the flipper
116:38 - to get a different view of the data set
116:40 - though what i'm doing below is i am
116:43 - using this module inside of human learn
116:47 - the experimental interactive module to
116:50 - generate some interactive charts
116:53 - and here you can see one of these
116:55 - interactive charts
116:57 - i'll just zoom out a bit
116:59 - and the interactive chart that you see
117:01 - here
117:02 - shows these different
117:04 - classes appear for the different kinds
117:06 - of penguins that we have
117:11 - now one reason why people like doing
117:12 - machine learning is because it's kind of
117:14 - hard to come up with a rule-based system
117:17 - that can handle data points like this
117:20 - writing an if-else statement that will
117:22 - perfectly describe this area over here
117:25 - is relatively tricky but what if we
117:27 - could just go ahead and draw it instead
117:34 - if i'm to consider this data set over
117:36 - here i can argue that it's not really
117:38 - that complex
117:39 - as a human i can kind of draw the area
117:42 - where i expect the green dots to be in
117:44 - and i can pretty much eyeball what the
117:46 - algorithm should look like and that
117:48 - allows me to draw out these polygons to
117:51 - get started with drawing i got a
117:52 - selected color that i'm interested in
117:55 - in this case red which corresponds to
117:57 - this class
117:58 - and then i can double click
117:59 - and click
118:01 - and click
118:02 - click click
118:03 - click and when i'm done making the
118:05 - polygon i can double click once more
118:08 - and then i've got a shape i can click
118:10 - and drag this shape if i'm interested
118:12 - and what i can also do is i can use this
118:14 - edit button over here
118:16 - to edit the shape
118:18 - but the main point i want to make here
118:20 - is that we can also declare rules by
118:23 - drawing them
118:24 - and this is just one drawing but let's
118:27 - also make another one
118:35 - and now i've got these two drawings that
118:37 - as far as i'm concerned should serve as
118:39 - a pretty okay classification algorithm
118:42 - now to use this as a classification
118:44 - algorithm it's useful to point out one
118:46 - more thing note that we added a chart to
118:50 - this clf object over here
118:53 - and that object is this
118:56 - interactive charts object
118:59 - the one that took the data frame as
119:01 - input and
119:02 - this species as a label column
119:05 - what i can do
119:06 - is i can take that object and ask it for
119:09 - all the polygons that i've drawn so far
119:14 - using this json blob we can reconstruct
119:17 - the drawings that we made and we can use
119:19 - a point-in-poly algorithm to check if a
119:23 - new point is in one of these polygons at
119:26 - any given point and if that's the case
119:29 - we can make the appropriate
119:31 - classification to get this to work
119:33 - though we're going to have to import
119:35 - something called an interactive
119:36 - classifier the idea behind this
119:38 - classifier is that we can define a
119:40 - scikit-learn compatible model
119:44 - from this json blob
119:48 - and what we can then do is we can
119:50 - generate our x and y data sets as we
119:53 - would normally from a data frame and
119:55 - then we could use this model as if it
119:57 - was any cycler model that we're used to
120:02 - and to emphasize how these
120:04 - classifications work i've added a little
120:06 - bit of code that makes the matplotlib
120:08 - charts and you can verify yourself that
120:10 - the plots that you see here do
120:13 - correspond with the drawings made
120:15 - earlier in this notebook
120:17 - it deserves to be mentioned though that
120:18 - these drawn models have properties that
120:21 - you might not have thought of up front
120:23 - and some of these properties are
120:24 - actually quite beneficial
120:26 - what i'm doing now is i'm taking the
120:28 - exact same model as i drew before
120:31 - but now i'm having it predict a new
120:34 - example
120:35 - over here
120:37 - and this new example is predicted with a
120:39 - class of addly and we can see that it's
120:42 - doubting between two classes in
120:45 - particular
120:46 - if i were to scroll up now
120:49 - then i can also explain why
120:52 - compared to this chart the new example
120:54 - point that i've made is somewhere over
120:56 - here
121:00 - whereas for the other chart
121:03 - the same point will be somewhere around
121:05 - here
121:06 - so that explains why it's having trouble
121:08 - making a proper classification
121:12 - but let's now say
121:14 - that the data that i'm getting in it's
121:17 - incomplete it's missing a value
121:20 - well in this case if this was the value
121:23 - that's missing
121:25 - then i could scroll back up
121:28 - to here
121:29 - and then where before the point was
121:31 - inside of this polygon it now isn't
121:33 - anymore so this chart is not giving us
121:36 - any more information for our
121:38 - classification task however
121:41 - because these two
121:44 - variables are definitely known
121:46 - we can definitely still say that the
121:48 - penguin is in this region and from a
121:50 - machine learning perspective that means
121:52 - that our drawings are robust against
121:55 - missing data
121:58 - we now see that we're predicting a new
122:00 - class and that's because we are now
122:03 - zooming in on these two features and no
122:05 - longer using
122:06 - these two
122:08 - there is another use case for these
122:10 - drawn machine learning models so the
122:12 - same
122:13 - if we have a look at the drawing right
122:14 - here you'll notice that there's a couple
122:16 - of points that
122:18 - kind of fall outside the polygon that we
122:20 - drew
122:21 - and in this particular case we can
122:23 - perhaps make the argument
122:25 - maybe some of these points are outliers
122:27 - especially if we were to zoom out
122:31 - i hope you recognize that this machine
122:33 - learning model really only has a comfort
122:35 - zone around here
122:38 - if we were to see points that are
122:40 - definitely on the outskirts of this
122:41 - comfort zone
122:43 - then it'd be nice if we can
122:45 - have a system that says hey that's an
122:47 - outlier we should treat those points
122:49 - differently
122:53 - luckily for us what we can do is we can
122:54 - take the same drawing that we use to
122:56 - make a classifier and use it as an
122:58 - outlier detection system
123:00 - where before we would check whether or
123:02 - not a point was inside of a polygon in
123:04 - order to make a decision the outlier
123:06 - detector will check whether or not the
123:08 - point is outside of polygons and there's
123:10 - a hyper parameter we can specify the
123:12 - minimum number of polygons that a point
123:14 - has to be in in order for it not to be
123:16 - an outlier but to give a simple demo of
123:18 - this feature
123:20 - you'll notice below here that i am using
123:22 - the interactive outlier detector as
123:25 - opposed to the classifier and then i'm
123:27 - following very similar steps as before
123:29 - but the main thing that you'll notice
123:31 - now
123:31 - is that when i make
123:33 - the chart of the predictions
123:36 - that
123:37 - all the points on the outskirts are
123:39 - indeed now classified as an outlier and
123:42 - again what we could do is we can make
123:44 - changes to
123:45 - this drawing over here and then we would
123:47 - get different results
123:50 - in all the drawings that we've made so
123:51 - far
123:52 - we've had the luxury of this label that
123:55 - was available
123:57 - however you can also imagine situations
123:59 - where we don't have a label readily
124:01 - available and in these situations it
124:04 - might be nice to use this drawing
124:05 - mechanic to actually assign labels to
124:08 - data points so as a demo let's just add
124:11 - group
124:12 - 1 and group
124:15 - 2
124:16 - as labels here
124:17 - because the labels parameter here is now
124:20 - a list these interactive charts will no
124:22 - longer internally check for a column and
124:24 - instead it will assume that these are
124:26 - the
124:28 - values that you want to assign
124:30 - so if i now were to run this
124:33 - you'll notice that the colors in the
124:35 - chart here are gone i do however
124:39 - still have the ability
124:41 - to
124:42 - make a drawing
124:43 - like before
124:46 - and i can use this drawing to make an
124:48 - outlier detector or a classifier but
124:52 - what i can also do is use this drawing
124:54 - in a pre-processing step
124:59 - human learn comes with this interactive
125:01 - preprocessor and just like in the
125:03 - examples before it is able to read in
125:06 - this chart data
125:07 - but instead of acting as a predictor it
125:10 - can act as either a scikit-learn
125:13 - transformer
125:14 - but it can also be used in a pandas
125:16 - pipeline as you see below here
125:24 - in the case of pandas what will happen
125:26 - is it will add
125:27 - two new columns with counts how often a
125:31 - data point was in a polygon and if i
125:33 - were to use the scikit-learn flow
125:35 - instead i would get the same counts but
125:39 - as a numpy array
125:42 - now this interactive charts api
125:45 - is relatively experimental currently it
125:47 - only allows for 2d charts but you can
125:49 - imagine that other visuals will also be
125:51 - added in future things like histograms
125:54 - can also be used to make selections the
125:56 - main utility of this library though is
125:58 - to maybe start thinking a little bit
126:00 - differently about machine learning
126:01 - models and to maybe start thinking about
126:03 - rule-based systems instead
126:05 - the goal isn't to throw machine learning
126:07 - out the window but instead to use
126:09 - machine learning algorithms with some
126:11 - business rules around them such that
126:13 - literally machine learning models can
126:15 - play by the rules
126:18 - i hope you enjoyed watching these videos
126:20 - on scikit-learn it's a vast ecosystem
126:22 - and we've only really been able to
126:24 - scratch the surface here but i hope that
126:26 - i've been able to explain enough for you
126:28 - to either get started or to explore the
126:30 - tool more if you enjoyed these videos
126:32 - then i would like to mention that there
126:34 - is some other machine learning content
126:35 - out there that you might also appreciate
126:38 - in particular i make a lot of videos on
126:40 - the calm code website so definitely feel
126:42 - free to check that out
126:45 - also if you're interested in learning
126:47 - more about natural language processing
126:49 - then you might also appreciate the
126:51 - videos i've been making on the official
126:53 - spacey channel in this series of videos
126:55 - i'm trying to make a model that can
126:57 - attack programming languages and if that
126:59 - sounds interesting definitely feel free
127:00 - to check that out as well
127:02 - and if you're interested in learning
127:04 - more about natural language processing
127:06 - definitely feel free to check out the
127:08 - algorithm whiteboard channel on youtube
127:11 - it's a playlist that i maintain on
127:13 - behalf of my employer raza and you'll
127:15 - find many examples and explanations of
127:18 - algorithms there as well
127:20 - if your interest is to learn more about
127:22 - psychic learn though then i might also
127:24 - have some other recommendations for you
127:27 - for starters
127:28 - definitely feel free to check out free
127:30 - code camp i've been in industry for over
127:32 - seven years but i still find many useful
127:36 - courses and hints on a wide variety of
127:38 - topics on this website
127:40 - which includes related toolkits like
127:43 - numpy and pandas
127:45 - second i might also recommend checking
127:47 - out the pi data youtube channel
127:50 - understanding how psychid learn works is
127:52 - great but sometimes you would also like
127:53 - to hear anecdotes and lessons learned
127:56 - from applying psychic learning practice
127:58 - and the pi data channel
128:00 - on youtube is a great resource for just
128:02 - that
128:03 - now the third and final resource that i
128:05 - really recommend is the psychic learn
128:09 - documentation page
128:10 - scikit-learn is a gigantic library and
128:13 - there's a lot of features that to this
128:15 - day i still discover basically just by
128:17 - reading the documentation but there's
128:19 - also something specific about the
128:21 - documentation that i would like to
128:22 - highlight
128:23 - if you go to the documentation page
128:25 - you'll find a couple of sub-sections
128:27 - there is a user guide that you can
128:29 - follow
128:30 - but there's also a subsection over here
128:32 - under more
128:33 - where it reads related packages
128:37 - you see
128:38 - scikit-learn isn't just a package at
128:41 - this stage you could also say it's an
128:42 - ecosystem
128:43 - and there are many different projects
128:45 - for specific use cases that might
128:48 - seriously help out there's
128:50 - experimentation frameworks
128:53 - tools for model selection
128:56 - there's also extra support for tools
128:58 - that have to do with time series
129:01 - and although the list of tools here can
129:03 - be intimidatingly large i really do
129:06 - recommend just having a look here simply
129:08 - to get a feel of what you could do with
129:10 - psychic learn
129:12 - so i hope you'll give this documentation
129:13 - page a glance and once more
129:16 - i hope that you'll be able to use
129:17 - scikit-learn to do meaningful work with
129:19 - machine learning thanks for listening