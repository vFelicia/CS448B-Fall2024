00:01 - Hey, this is Andrew Brown from exam Pro. And
I'm bringing you another free Eva's certification
00:05 - course. And this one happens to be the most
popular in demand. And it is the solutions
00:09 - architect associate certification. So if you're
looking to pass that exam, this is the course
00:14 - for you. And we are going to learn a broad
amount of database services, we're going to
00:20 - learn how to build applications that are highly
available, scalable or durable. And we're
00:24 - going to learn how to architect solutions
based on the business use case. Now, if you're
00:28 - looking at this course, and it's 2020, and
you're wondering if you can use it to pass,
00:33 - you definitely can because the video content
here was shot in late 2019. The only difference
00:39 - is that the AWS interface has changed a little
bit in terms of aesthetic, but it's more or
00:44 - less the same. So you can definitely use this
course, if you want a high chance of passing,
00:49 - I definitely recommend that you do the hands
on labs here in your own AWS account. If you
00:56 - enjoyed this course, I definitely want to
get your feedback. So definitely share anything
01:00 - that you've experienced throughout this course
here. And if you pass, I definitely want to
01:05 - hear that as well. And I also hope you enjoy
this course. And good luck studying.
01:12 - Hey, this is Andrew Brown from exam Pro. And
we are going to look at the solution architect
01:21 - associate and whether it's a good fit for
us to take the certification. So the first
01:27 - thing I want you to know is that this kind
of role is for finding creative solutions
01:31 - by leveraging cloud services instead of reinventing
the wheel. It's all about big picture thinking.
01:36 - So you're going to need broad knowledge across
multiple domains. It's great for those who
01:40 - get bored really easily. And so you're gonna
have to wear multiple hats. And it's really
01:45 - less about how are we going to implement this
and more about what are we going to implement,
01:51 - okay, so you would come up with an architecture
using multiple different cloud services, and
01:56 - then you would pass it on to your cloud engineers
to actually go implement, it's not uncommon
02:01 - for a solution architect to be utilized within
the business development team. So it's not
02:07 - quite unusual to see solution architects being
very charismatic speakers and extroverts,
02:12 - because they're going to have to talk to other
companies to collaborate with, alright, and
02:18 - just to really give you a good idea of what
a solution architect does, they're going to
02:23 - be creating a lot of architectural diagrams.
So here, I just pulled a bunch from the internet,
02:26 - and you can see kind of the complexity and
how they tie into different services, you're
02:31 - going to require a lot of constant learning,
because AWS is constantly adding new services
02:36 - and trying to figure out how they all fit
together is a common thing. And advice that
02:40 - I get from some senior solution architects
at large companies, is you're always thinking
02:45 - about pricing, and you're always thinking
about can you secure a whatever that is okay,
02:49 - but at best is gonna have their own definition
there, which is all about the five pillars,
02:55 - which comes into the well architected framework.
But you know, we'll learn that as we go along
03:00 - here. Okay, so let's talk about what value
do we get out of the solution architect associate?
03:06 - Well, it is the most popular at a certification
out of every single one. It's highly in demand
03:12 - with startups, because you can help wherever
help is needed startups, from small to medium
03:17 - size, just need people to fill any possible
role. And because you're gonna have broad
03:21 - knowledge, you're going to be considered very,
very valuable, it is recognized as the most
03:25 - important certification at the associate level,
and it's going to really help you stand out
03:30 - on a resumes, I would not say the associate
is going to help you increase your salary
03:36 - too much. But you're definitely going to see
a lot more job opportunities to see those
03:40 - increase in salaries, you're gonna have to
get those pros and specialty certifications.
03:45 - Okay, so if you're still not sure whether
you should take the solution architect associate,
03:51 - let me just give you a little bit more information.
So it is the most in demand a certification.
03:57 - So it has the most utility out of any other
certification because of that broad knowledge.
04:03 - It's not too easy, but it's not too hard.
So it's not too easy, in the sense that, you
04:07 - know, the information you're learning is superficial,
it's actually going to be very useful on the
04:11 - job. But it's also not that hard. So you're
not going to risk failing the exam because
04:16 - you don't know the nitty gritties of all the
services, okay, it requires the least amount
04:21 - of technical knowledge. So if you're really
more of a, a academic or or theory based learner,
04:29 - instead of having that hands on experience,
you're going to excel here taking the solution
04:33 - architect associate. And again, when in doubt,
just take this certification because it gives
04:39 - you the most flexible future learning path.
So I always say that if you aren't sure what
04:45 - specialty you want to take, take the solution
architect associate. So you get to familiarize
04:50 - yourself with all the different kinds of roles
that you can encounter. So if you're definitely
04:54 - thinking about doing big data security, machine
learning, I would absolutely think a to do
05:00 - Take the solution architect associate first.
Of course, you can always do the solution
05:03 - architect professional, if you want to keep
on going down this specific path. And if you
05:09 - are new to AWS and cloud computing in general,
that I strongly recommend that you take the
05:15 - CCP before taking the solution architect associate
because it's a lot easier. And it's going
05:22 - to give you more foundational knowledge so
that you're going to have a really easy time
05:26 - with this exam. And it specifically is the
direct upgrade path. So all that stuff you
05:32 - learn in the CCP is directly applicable to
the Solution Architect associate. So how much
05:39 - time are we going to have to invest in order
to pass the solution architect
05:43 - associate. And this is going to depend on
your past experience. And so I've broken down
05:48 - three particular archetypes to give you an
idea of time investment. So if you are already
05:53 - a cloud engineer, you're already working with
AWS, you're looking at 20 hours of study,
05:58 - you could pass this in a week, okay, but that's
if you're using AWS on a day to day basis,
06:04 - if you are a bootcamp grad, it's going to
take you one to two months. So we're looking
06:09 - between 80 to 160 hours of study. If you have
never used AWS or heard of it, then you probably
06:16 - should go take the certified cloud practitioner
first, it's going to make things a lot easier,
06:21 - which has a lot more foundational information,
you might start this here and be overwhelmed,
06:27 - because you feel that you're missing information.
So you will probably want to go there first.
06:31 - If you are a developer, and you've been working
in the industry for quite a few years, but
06:35 - maybe you've just never used AWS, then you're
looking at one month of study. So that's about
06:42 - 80 hours of study. Okay, and so that will
give you an idea how much time you need to
06:47 - commit. Okay, so let's just touch on the exam
itself here. So the exam itself is going to
06:54 - cost $150, for you to take and that's in USD,
you have to take it at a test center that
07:01 - is partnered with AWS. So you will have to
go through the portal there and book it and
07:07 - then you'll be going down to that test center
to right at the exam gives you 130 minutes
07:13 - to complete it. There's a 65 questions on
the exam, the passing score is around 72%.
07:20 - And once you have the certification, it's
going to be valid for three years. All right.
07:24 - So hopefully, that gives you a bit of perspective
and whether the solution architect associate
07:29 - is right for you. Here I have on the right
hand side, the exam guide. And I'm just going
07:39 - to walk you quickly through it just so you
get a kind of a breakdown of what it is that
07:43 - AWS recommends that we should learn and how
this this exam is broken up in terms of domains,
07:52 - and also its scoring. Okay, so here on the
left hand side, we're going to first look
07:56 - at the content outline. Okay, if we just scroll
down here, you can see it's broken up into
08:00 - five domains. And we get a bunch a bunch more
additional information. Okay, so we have a
08:07 - design resilient architectures, design performance
architectures, specify secure applications
08:12 - and architectures designed cost optimized
architectures, and define operational excellent
08:17 - architectures. Now I highlighted the word
in there resilient performance, secure cost,
08:22 - optimizing operational excellence, because
this actually maps to the five pillars of
08:27 - the well architected framework, which is a
recommended read for study here, okay. So
08:35 - there is a rhyme and rhythm to this layout
here, which we will talk about when we get
08:40 - to the white paper section. But let's just
look inside of each of these domains. So for
08:44 - resilient architecture, you have to choose
reliable and resilient storage. So there we're
08:49 - talking about elastic block store in s3 and
all the different storage options available
08:57 - to us design, how to design decoupling mechanisms
using AWS services. So they're talking about
09:03 - application integration, such as Sq s, and
SNS. Then we have a design how to or determine
09:09 - how to design a multi tier architecture solution.
Maybe they're hinting there at once as multi
09:15 - tier. So when you have tiers you, you'd have
your database layer, your web layer, your
09:21 - load balancing layer, okay, so that's probably
what they mean by tiers. Determine how to
09:24 - design high available availability or fault
tolerant architectures. So that's going to
09:29 - be knowing how to use row 53. Load Balancing
auto scaling groups, what happens when an
09:34 - AZ goes out what happens when a region goes
out? That kind of stuff, okay. The next thing
09:39 - is design, performance architecture. So choose
performance storage, and databases. So that's
09:45 - just going to be knowing Dynamo DB versus
RDS versus redshift, okay? That we can apply
09:51 - caching to improve performance. That's going
to know that dynamodb has a caching layer
09:55 - that's going to be knowing how to use elastic
cache or maybe using cloud Front to cache
10:00 - your static content, then we have designed
solutions for elasticity and scalability.
10:06 - So that sounds pretty much like auto scaling
groups to me, okay. And then we got specify
10:13 - secure applications in architecture. So determine
how to secure application tiers. So again,
10:18 - there's three tiers database, web network,
or load balancing. There's obviously other
10:22 - tiers there. But just knowing when to check
box to turn on security for those services
10:27 - and how that stuff works. From a general perspective,
okay,
10:31 - Turman, how do you secure data, so just knowing
data at rest, like data in transit? Okay,
10:39 - then defining the networking infrastructure
for a single VPC application. This is about
10:43 - knowing v PCs inside and out, which we definitely
cover heavily. I'm a solution architect associate
10:49 - and all the associate certifications, because
it's so darn important that we have designed
10:54 - cost optimize architecture. So determine how
to design cost optimized storage, determine
10:59 - how to design cost, optimize, compute, we're
talking about storage, they're probably really,
11:03 - really talking about s3, s3 has a bunch of
storage classes that you can change and they
11:07 - get cheaper, the further down you go, and
knowing when and how to use that for compute,
11:13 - maybe they're talking about just knowing when
to use us different kinds of ECU instances,
11:19 - or maybe using auto scaling groups to reduce
that cost to, to scale out when you don't
11:24 - have a lot of usage. Then the last one here
is design, operational, excellent architecture.
11:29 - So design features and solutions that enable
opera enable operational excellence, okay,
11:34 - and so you can see, and I'm not even exactly
sure what they're saying here. But that's
11:38 - okay. Because it's worth 6%. Okay, it's definitely
covered in the course, it's just, it's a funny
11:45 - word in one there, I never could remember
what they're saying there. Okay. But you can
11:49 - see the most important one here is designing
resilient architecture. Okay, so that's the
11:54 - highest one there. And the last two is cost
and operational excellence. So you're not
11:59 - going to be hit with too many cost questions,
but you just generally have to know, you know,
12:03 - when it makes sense to use x over y. Alright.
So yeah, there's the outline, and we will
12:09 - move on to the next part here. And that's
the response types. Okay. So this exam, I
12:15 - believe, has a 65 questions. I don't think
it actually states it in here. But generally,
12:20 - it's 65. Okay, a lot of times when you take
the exams, they'll actually have additional
12:25 - questions in there that are not scored, because
they're always testing out new questions.
12:30 - Questions are going to come in two formats,
multiple choice. So we're going to have the
12:33 - standard one out of four. And then we're going
to have multiple response, which is going
12:38 - to be choose two or more out of five or more,
okay, generally, it's always two out of five.
12:44 - But I guess sometimes you could have three
out of six. All right. And so just be aware
12:51 - of that. Now, the passing score for this is
going to be 720 points out of 10,000 points.
12:58 - Okay, so they have this point system, and
so 720 is passing. So the way you can think
13:02 - about it, it's 72%, which is a C minus two
pass. All right, I put a tilde there, I know
13:08 - it looks a bit funny there. But the Tilda
means to say like about or around because
13:12 - that value can fluctuate. So the thing is,
is that it's not exactly 72%, you could go
13:19 - in and get 72%, and fail, you can go and get
75% and fail, it just depends on how many
13:24 - people are taking the exam, and they're going
to adjust it based on how you feel or passing
13:29 - or failing, okay, but it doesn't, it doesn't
fluctuate too far from this point, okay, it's
13:33 - not gonna be like, you have to get 85%. Alright.
And then just the last thing here is the white
13:38 - paper. So each of us recommends white papers
for you to read. And they're not very clear
13:43 - here. So they do architecting for the cloud
as best practices, that's when you should
13:48 - definitely read. It's not a very difficult
read. So it's on the top of your reading list.
13:52 - And then there's Eva's well, architected architected
webpage. And so that web page contains a bunch
13:58 - of white papers. And this is the full list
here. Okay, so we have the well architected
14:03 - framework, which talks about the five pillars
and then then they actually have a white paper
14:07 - for each pillar. And then there's these other
ones down below, which are kind of new additions.
14:12 - So the question is, do you have to read all
of these things? No. In fact, you should just
14:17 - probably just read the top one here as well
architecture framework, and you could read
14:20 - half of that and you'd still be good. It is
great to dove dive into these. These ones
14:25 - here. So um, there are still listed here.
The last ones here are definitely 100% optional.
14:30 - I do not believe they are on the exam. But
again, they just tell you to go to the entire
14:34 - page. So it is a bit confusing there. So hopefully,
that gives you a bit of a breakdown so you
14:40 - are prepared. what's ahead of you for study.
14:44 - Hey, this is Angie brown from exam Pro. And
we are looking at simple storage service,
14:51 - also known as s3, which is an object based
storage service. It's also serverless storage
14:56 - in the cloud. And the key thing is you don't
have to worry about falling systems or display
15:01 - to really understand three, we need to know
what object storage is. And so it is a data
15:05 - storage architecture that manages data as
objects, as opposed to other storage architectures,
15:11 - other architectures being file systems, where
you manage data as files within a file hierarchy,
15:18 - or you have block storage, which manages data
as blocks, when within sectors and tracks,
15:23 - the huge benefit to object storage is you
don't have to think about the underlying infrastructure,
15:28 - it just works. And with s3, you have practically
unlimited storage. So you just upload stuff,
15:34 - you don't worry about disk space. s3 does
come with a really nice console that provides
15:40 - you that interface to upload and access your
data. And the two most key key components
15:46 - to s3, our s3 objects and s3 bucket so objects
are is what contains your data. And they're
15:52 - kind of like files. And an object is composed
of a key value of version ID and metadata.
15:57 - So the key is just the name of the file or
the object, the value is actually the data
16:02 - itself made up as a sequence of bytes. The
version ID, if you're using versioning, then
16:06 - you have to enable that on s3, then each object
you upload would have an ID. And then you
16:12 - have metadata, which is just additional information
you want to attach to the object. An s3 object
16:17 - can be zero bytes in size and five terabytes.
Please note that I really highlighted zero
16:23 - bytes, because that is a common Trick question
on the exam, because a lot of people think
16:28 - you can't do zero bytes, and you definitely
can. And then you have buckets and buckets,
16:33 - hold objects, they are kind of like top level
folders or directories. Buckets can also have
16:38 - folders, which in turn hold objects. So buckets
have a concept called folders. And so you
16:43 - can have had those objects directly in that
bucket. Or in those folders. When you name
16:49 - an s3 bucket, it's using a universal namespace.
So the bucket names must be unique. It's like
16:54 - having a domain name. So you have to really
choose unique names. Okay. So the concept
17:04 - behind storage classes is that we get to trade
retrieval time accessibility and durability
17:09 - for cheaper storage. And so when you upload
data to s3, by default, it's using standard.
17:14 - And it's really, really fast. It has 99 point
99% availability, it has 11 nines of durability,
17:20 - and it replicates your data across three availability
zones. And as we go down, this list is going
17:25 - to get cheaper, we're going to skip over until
intelligent tearing, we'll come back to that.
17:30 - And we're gonna look at standard and frequency
access, also known as IAA. So it's just as
17:34 - fast as standard. The trade off here is that
it's cheaper if you access files less than
17:39 - once a month. There isn't an additional retrieval
fee when you access that data. But the cost
17:45 - overall is 50%. Less than standard. So the
trade off here is you're getting reduced availability,
17:51 - then you have one zone IAA. And as the name
implies, it only runs your data or only replicate
17:56 - your data in one AZ so you don't you have
reduced durability. So there is a chance that
18:02 - your data could get destroyed, a retrieval
fee is applied just like an AI a, your availability
18:09 - is going to drop down to 99.5%. Okay, then
you have glacier and glaciers for long term
18:16 - cold storage.
18:18 - It's for the trade off here, though, is that
the retrieval is going to take minutes to
18:22 - an hour, but you get extremely, extremely
cheap storage. There also is a retrieval fee
18:27 - applied here as well. And glacier normally
is like pitched kind of like its own service.
18:33 - But really, it's an s3 service, then you have
glacier deep archive. And this is just like
18:39 - glacier except now it's going to take 12 hours
before you can access your data. Again, it's
18:45 - very, very, very, very cheap at this level.
It's the cheapest tier here. And so this is
18:52 - really suited for long archival data. Now
we glossed over intelligent, tearing, but
18:58 - let's talk about it. So what it does is it
uses machine learning to analyze your object
19:03 - usage and determine the appropriate storage
class. So it's going to decide for you what
19:08 - storage class you should use so that you save
money, okay, and so that is all of the classes
19:14 - and we're going to compare them in a big chart
in the next slide. I just have here up the
19:23 - comparison of storage classes just to make
it a bit easier for you to see what's going
19:27 - on here. So you can see across the board we
have durability at the 11 nines across all
19:31 - services. There is reduced durability in one
zone I A but I guess it's trying to say that
19:37 - maybe it has 11 nines in that one zone. I'm
not sure so that one confuses me a bit. But
19:42 - you have to think that if you're only running
one zone, there has to be reduced durability.
19:47 - For availability, it's 99.9% until we hit
one zone IAA. For glacier and glacier deep
19:53 - archive. It's not applicable because it's
just going to take a really long time to access
19:57 - those files. So availability is like indefinitely
Hello, we're not going to put a percentage
20:01 - on that. For azs. It's going to run in three
or more azs. From standard to standard ay
20:09 - ay ay ay. Actually, across the board. The
only one that is reduced is for one zone IAA,
20:14 - I always wonder, you know, if you're running
in Canada Central, it would only use the two
20:18 - because there's only two availability zones
there. So it's always a question I have on
20:23 - the top of my head. But anyway, it's always
three or more azs, you can see that there
20:29 - is a capacity charge for standard IAA. and
above, for there is a storage duration charge
20:37 - for all the tiers with the exception of standard.
And then you have your retrieval fees, which
20:43 - are only going to come in your eyes and your
glacier, okay. And then you have the latency.
20:47 - That's how fast you can access files. And
you can see Ms means milliseconds. So for
20:52 - all these tiers, it's super, super fast. And
you know, it's good to just repeat, but you
20:57 - know, AWS does give a guarantee of 99.99%
availability, it has a guarantee of 11 nines
21:08 - durability. Alright, so there you go, that
is the big. Now we're taking a look at s3
21:18 - security. So when you create a bucket, they're
all private by default. And AWS really obsesses
21:24 - over not exposing public buckets, they've
changed the interface like three or four times,
21:29 - and they now send you email reminders telling
you what buckets are exposed because it's
21:33 - a serious vulnerability for AWS, and people
just seem to keep on leaving these buckets
21:37 - open. So when you create a new bucket, you
have all public access denied. And if you
21:43 - want to have public access, you have to go
check off this for either for your ACLs or
21:48 - your bucket policies. Okay.
21:50 - Now, in s3, you can turn on logging per request.
So you get all the detailed information of
21:57 - what objects were accessed or uploaded, deleted
in granular detail. log files are generated,
22:03 - but they're not putting the same bucket, they're
putting in a different bucket. Okay. Now to
22:08 - control access to your objects. And your buckets,
you have two options. We have bucket policies
22:15 - and access control lists. So access control
lists came first, before bucket policies,
22:19 - they are a legacy feature, but they're not
depreciated. So it's not full pot to use them.
22:24 - But they're just simpler in action. And sometimes
there are use cases where you might want to
22:27 - use them over bucket policies. And so it's
just a very simple way of granting access,
22:32 - you right click in a bucket or an object and
you could choose who so like there's an option
22:39 - to grant all public access, you can say lists
the objects, right the objects, or just read
22:42 - and write permissions. And it's as simple
as that now bucket policies are, they're a
22:47 - bit more complicated, because you have to
write a JSON document policy, but you get
22:52 - a lot more rich, complex rules. If you're
ever setting up static s3, hosting, you definitely
22:58 - have to use a bucket policy. And that's what
we're looking at here. This is actually an
23:01 - example website where we're saying allow read
only access, forget objects to this bucket.
23:07 - And so this is used in a more complicated
setup. But that is the difference. So bucket
23:11 - policies are generally used more, they're
more complex and ACLs are just simple. And
23:16 - there's no foolproof. So we talked about security
and very big feature about that is encryption.
23:26 - And so when you are uploading files to s3,
it by default uses SSL or TLS. So that means
23:31 - we're going to have encryption in transit.
When it comes to server side encryption, actually
23:36 - what is sitting on the actual data at rest.
We have a few options here. So we have SSE
23:45 - a s, we have SSE kms. And SSE C, if you're
wondering what SSE stands for, it's server
23:51 - side encryption. And so for the first option,
this is just an algorithm for encryption.
23:56 - So that means that it's going to be 256 bytes
in length or characters in length when it
24:02 - uses encryption, which is very long. And s3
is doing all the work here. So it's handling
24:06 - all the encryption for you. Then you have
kms, which is key management service, and
24:11 - it uses envelope encryption. So the key is
then encrypted with another key. And with
24:17 - kms. It's either managed by AWS or managed
by you the keys itself, okay, then you have
24:22 - customer provided keys, this is where you
provide the key yourself. There's not an interface
24:27 - for it here, it's a bit more complicated.
But you know, all you need to know is that
24:31 - the C stands for customer provided, then you
have client side encryption, there's no interface
24:36 - or anything for that. It's just you encrypting
the files locally and then uploading them
24:40 - to s3 or looking at s3 data consistency. Sorry,
I don't have any cool graphics for here because
24:50 - it's not a very exciting topic, but we definitely
need to know what it is. So when you put data
24:55 - or you write data to s3, which is when you're
writing new objects, The consistency is going
25:01 - to be different when you are overwriting files
or deleting objects. Okay, so when you send
25:08 - new data to s3, as a new object, it's going
to be read after write consistency. What that
25:14 - means is as soon as you upload it, you can
immediately read the data and it's going to
25:18 - be consistent. Now when it comes to overwriting
and deleting objects, when you overwrite or
25:25 - delete, it's going to take time for s3 to
replicated to all those other azs. And so
25:30 - if you were to immediately read the data,
s3 may return to you an old copy. Now, it
25:35 - only takes like a second or two for it to
update. So there, it might be unlikely in
25:40 - your use case, but you just have to consider
that that is a possibility. Okay. So we're
25:50 - taking a look at cross region replication,
which provides higher durability in the case
25:57 - of a disaster, okay, and so what we do is
we turn it on, and we're going to specify
26:02 - a destination bucket in another region, and
it's going to automatically replicate those
26:06 - objects from the region source region to that
destination region. Now, you can also have
26:12 - it replicate to a bucket in another AWS account.
In order to use this feature, you do have
26:18 - to have versioning turned on in both the source
and destination buckets.
26:27 - Now in s3, you can set on a bucket versioning.
And what versioning does is it allows you
26:33 - to version your objects, all right. And the
idea here is to help you prevent data loss
26:39 - and just keeping track of versions. So if
you had a file, here, I have a an image called
26:44 - tarok. Nor the name is the same thing as a
key, right, it's gonna have a version ID.
26:50 - And so here we have one, which is 111111.
And when we put a new file, like a new object
26:58 - with the exact same key, it's going to create
a new version of it. And it's going to give
27:03 - it a new ID, whatever that ID is 121212. And
the idea is now if you access this object,
27:11 - it's always going to pull the one from the
top. And if you were to delete that object,
27:15 - now it's going to access the previous one.
So it's a really good way of protecting your
27:21 - data from and also, if you did need to go
get an older version of it, you can actually
27:26 - get any version of the file you want, you
just have to specify the version ID. Now when
27:33 - you do turn on s3, versioning, you cannot
disable it after the fact. So you'll see over
27:38 - here it says enabled or suspended. So once
it's turned on, you cannot remove versioning
27:43 - from existing files, all you can do is suspend
versioning. And you'll have all these baseball's
27:49 - with one version, alright.
27:53 - So s3 has a feature called Lifecycle Management.
And what it does is it automates the process
28:00 - of moving objects to different storage classes
or deleting them altogether. So down below
28:05 - here, I have a use case. So I have an s3 bucket.
And I would create a lifecycle rule here to
28:09 - say after seven days, I want to move this
data to glacier because I'm unlikely to use
28:15 - that data for the year. But I have to keep
it around for compliancy reasons. And I want
28:19 - that cheaper cost. So that's what you do with
lifecycle rule, then you create another lifecycle
28:24 - rule to say after a year, you can go ahead
and delete that data. Now, Lifecycle Management
28:30 - does work with a version and it can apply
to both current or previous version. So here
28:35 - you can see you can specify what you're talking
about when you're looking at a lifecycle rule.
28:45 - So let's look at transfer acceleration for
s3. And what it does is it provides you with
28:49 - fast and secure transfer of files over long
distances between your end users and an s3
28:54 - bucket. So the idea is that you are uploading
files and you want to get them to s3 as soon
29:00 - as possible. So what you're going to do is
instead of uploading it to s3, you're going
29:04 - to send it to a distinct URL for a edge location
nearby. an edge location is just a data center
29:10 - that is as close as you as possible. And once
it's uploaded there, it's going to then accelerate
29:16 - the uploading to your s3 bucket using the
AWS backbone network, which is a an optimized
29:22 - network path. Alright. And so that's all there
is to it. So pre signed URLs is something
29:32 - you're definitely going to be using in practicality
when you're building web application. So the
29:37 - idea behind it is that you can generate a
URL which provides you temporary access to
29:41 - an object to either upload or download object
data to that endpoint. So presale URLs are
29:47 - commonly used to provide access to private
objects. And you can use the CLR SDK to generate
29:54 - pre signed URLs Actually, that's the only
way you can do it. So here using the COI you
29:58 - can see I'm specifying the actual object and
I'm saying that it's going to expire after
30:02 - 300. I think that's seconds. And so, anyway,
the point is, is that, you know, it's only
30:10 - going to be accessible for that period of
time. And what it's going to do is gonna generate
30:15 - this very long URL. And you can see it actually
has an axis axis key in here, sets the expiry
30:21 - and has the signature. Okay, so this is going
to authenticate a temporary temporarily to
30:27 - do what we want to do to that object of dairy,
very common use cases, if you have a web application,
30:33 - you need to allow users to download files
from a password protected part of your web
30:37 - app, you'd also expect that those files on
s3 would be private. So what you do is you
30:44 - generate out a pre signed URL, which will
expire after like something like five seconds,
30:48 - enough time for that person to download the
file. And that is the concept. So if you're
30:58 - really paranoid about people deleting your
objects in s3, what you can do is enable MFA
31:03 - delete. And what that does is it makes you
require an MFA code in order to delete said
31:11 - object. All right, now in order to enable
MFA, you have to jump through a few hoops.
31:16 - And there's some limitations around how you
can use it. So what you have to do is you
31:21 - have to make sure version is turned on your
bucket or you can't use MFA delete. The other
31:26 - thing is that in order to turn on MFA delete,
you have to use the COI. So here down below,
31:31 - I'm using the C ally. And you can see the
configuration for versioning. I'm setting
31:34 - it to MFA delete enabled. Another caveat is
that the only the bucket owner logged in as
31:39 - the root user can delete objects from the
bucket. Alright, so those are your three caveats.
31:45 - But this is going to be a really good way
to ensure that files do not get deleted by
31:49 - act. Hey, this is Andrew Brown from exam Pro,
and welcome to the s3 Follow along where we're
31:58 - going to learn how to use s3. So the first
thing I want you to notice in the top right
32:02 - corner is that s3 is in a global region. So
most services are going to be region specific.
32:08 - And you'd have to switch between them to see
the resources of them, but not for s3, you
32:12 - see all your buckets from every single region
in one view, which is very convenient. That
32:17 - doesn't mean that buckets don't belong to
a region, it's just that the interface here
32:21 - is
32:22 - global.
32:23 - Okay, so we're going to go ahead and create
our first bucket and the bucket name has to
32:26 - be unique. So if we choose a name that's already
taken by another database user, we're not
32:30 - gonna be able to name it that and the name
has to be DNS compliant. So it's just like
32:34 - when you register for a domain name, you're
not allowed certain characters. So whatever
32:40 - is valid for a domain name, or URL is what's
going to be valid here. So I'm going to try
32:44 - to name it exam Pro. And it's going to be
in this region here. And we do have all these
32:47 - other options. But honestly, everybody always
just goes ahead and creates and configures
32:51 - after the fact, we're gonna hit Create. And
you're gonna notice that it's gonna say this
32:54 - bucket name has already been taken, and it
definitely has been because I have it in my
32:59 - other AWS account here. So I'm just gonna
go ahead and name it something else, I'm gonna
33:05 - try 000. Okay, I'm gonna hit Create. And there
we go. So we have our own bucket. Alright,
33:12 - now if we wanted to go ahead and delete this
bucket, I want you to do that right away here,
33:17 - we're going to go ahead and delete this bucket,
it's gonna pop up here, and it's going to
33:20 - ask us to put in the name of the bucket. So
I was just copy it in here, like this. And
33:24 - we're going to delete that bucket. Okay, so
that's how you create a bucket. And that's
33:28 - how you delete a bucket. But we're gonna need
a bucket for to learn about s3. So we're gonna
33:32 - have to go ahead and make a new bucket here.
So I'm going to put in exam pro 000. And we
33:38 - will go ahead and create that. And now we
have our buckets. So great. And we'll go click
33:43 - into this bucket. And there we go. So let's
start actually uploading our first file. So
33:52 - I prepared some files here for upload. And
just before I upload them, I'm just gonna
33:57 - go create a new folder here in this bucket.
Okay. And I have a spelling mistake there.
34:05 - Great. And so I'm gonna just go upload these
images. Here. They are from Star Trek The
34:10 - Next Generation. They're the characters in
the show. And so I all I have to do here is
34:14 - click and drag, and then I can go ahead and
hit upload down here. And they're just going
34:20 - to go ahead and upload them. Alright. And
we'll just give that a little bit of time
34:25 - here. And they are all in Great. So now that
I have all my images here in my bucket, I
34:32 - can click into an individual one here. And
you can see who the owner is, when it was
34:37 - uploaded, uploaded the storage class, the
size of it. And it also has an object URL,
34:42 - which we're going to get to in a moment here.
All right. So but if we want to actually just
34:46 - view it in the console here, we can click
open and we can view it or we can hit download.
34:52 - Alright, and so that's going to download that
file there. But then we have this object URL
34:56 - and this was why we're saying that you have
to have unique you Unique a bucket names because
35:02 - they literally are used as URLs. Okay, so
if I were actually to take this URL and try
35:06 - to access it, and I actually just had it open
here a second ago, you can see that we're
35:11 - getting an Access denied because by default,
s3 buckets are private. Okay? So if we wanted
35:18 - to make this public, so anyone could access
this URL, we want to hit the make public button,
35:22 - we're going to see that it's disabled. So
if we want to be able to make things public,
35:29 - we're gonna have to go to our bucket up here
at the top here, go to the properties, or
35:33 - sorry, I should say permissions. And we're
going to have to allow public access. Okay,
35:40 - so this is just an additional security feature
that eight of us has implemented, because
35:45 - people have a really hard time about making
things public on their bucket and getting
35:50 - a lot of bad stuff exposed. So we're gonna
go over here and hit edit, and we have a bunch
35:54 - of options. But we're first going to untick
block all public access, and this is gonna
35:59 - allow us to now make things public. So I hit
save, okay, and I go to type in confirm. Okay.
36:08 - And so now, if I go back to our bucket here
into the enterprise D and data, I now have
36:13 - the ability to make this public. So I'm going
to click make public. And so now, this file
36:18 - is public. And if I were to go back here and
refresh, okay, so there you go. So now I could,
36:25 - I could take this link and share it with you,
or anybody, and anyone in the world can now
36:29 - view that file. Okay. Great. So now that we
learned how to upload a file, or files, and
36:38 - how to make a file public, let's learn about
versioning.
36:46 - So we uploaded all these different files here.
But let's say we had a newer version of those
36:52 - files. And we wanted to keep track of that.
And that's where version is going to help
36:55 - us in s3. So, you know, we saw that I uploaded
these characters from Star Trek The Next Generation.
37:03 - And there, we actually have a newer images
here of the characters, not all of them, but
37:07 - some of them. And so when I upload them, I
don't want the old ones to vanish, I want
37:13 - it to keep track of them. And that's where
virgin is going to come into play. So to turn
37:17 - on versioning, we're going to go to exam pro
000. And we're going to go to properties here
37:22 - and we have a box here to turn on versioning,
I just want you to notice that when you turn
37:26 - on versioning, you cannot turn it off, you
can only suspend it, okay. So what that means
37:32 - is that objects are still going to have version
history, it's just that you're not gonna be
37:35 - able to add additional versions, if you turn
it off, we'll go ahead and we're going to
37:39 - enable versioning here, and versioning is
now turned on. And you're going to notice
37:43 - now that we have this versions tab, here we
go hide and show and it gives us additional
37:48 - information here for the version ID. Okay,
so I'm going to go into the enterprise D,
37:52 - I hit show here, so I can see the versions.
And now you're gonna see, we've got a big
37:56 - mess here. Maybe we'll turn that off here
for a second. And we're going to go ahead
37:59 - and upload our new files here, which have
the exact same name, okay, so I'm gonna click
38:03 - and drag, and we're gonna hit upload, and
Oh, there they go. Okay, so we're gonna upload
38:11 - those files there. And now let's hit show.
And we can see that some of our files where
38:15 - we've done some uploading there have have
additional versions. So you're gonna notice
38:20 - that the first file here actually has no version
ID, it's just No, okay, but the latest version
38:26 - does, it's just because these were the, the
initial files. And so the initial files are
38:30 - going to have null there. But you know, from
then on, you're going to have these new version
38:37 - IDs. Okay, so let's just see if we can see
the new version. So we're looking at data
38:41 - before. And what we want to see here is what
he looks like now. So I click open, and it's
38:47 - showing us the latest version of data. All
right. Now, if we wanted to see the previous
38:51 - version, I think if we drop down here, see
vs latest and prior, so we have some dates
38:55 - there, we click here, we hit open, and now
we can see the previous version of data. Okay.
39:01 - Now, one other thing I want to check here
is if we go up to this file here, and we were
39:07 - to click this link, is this going to be accessible?
No, it's not. Okay. Now, let's go look at
39:12 - the previous example. Now we had set this
to be public, is this one still public? It
39:18 - is okay, great. So what you're seeing here
is that when you do upload new files, you're
39:22 - not going to inherit the original properties,
like for the the public access, so if we want
39:28 - data to be public, we're going to have to
set the new one to be public. So we're going
39:31 - to drop down the version here and hit make
public. And now if we go open this file here,
39:36 - he should be public. So there you go, that
is versioning. Now, if we were to delete this
39:40 - file out of here, let's go ahead and delete
data out of here. Okay, so I'm going to hit
39:46 - actions and I'm going to delete data. So I'm
going to hit Delete. Notice it says the version
39:51 - ID here, okay? So I hit delete, and data is
still here. So if we go into data,
39:57 - and we hit open, okay, we're Now we get the
old data, right? So we don't have
40:04 - the previous versions. I'm pretty sure we
don't. So we just hit open here open. Great.
40:08 - And if I go here and go this one open, okay,
so the specified version does not exist. So
40:14 - we, it still shows up in the console, but
the the file is no longer there. Okay? Now
40:19 - let's say we wanted to delete this original
data file, right? Can we do that, let's go
40:24 - find out, delete, and we're gonna hit delete,
okay, and still shows the data's there, we're
40:30 - gonna hit the refresh, we're going to go in
there, and we're going to look at this version
40:34 - and open it. Okay, and so you can still see
it's there. So the thing about versioning
40:39 - is that it's a great way to help you protect
from the deletion of files. And it also allows
40:47 - you to, you know, keep versions of stuff and
those properties, you know, again, does not
40:51 - carry over to the next one. So, now that we've
learned about versioning, I think it'd be
40:56 - a bit of fun to learn about encryption. Okay.
Actually, just before we move on to encryption,
41:01 - I just want to double check something here.
So if we were to go to versions here, and
41:05 - I was saying, like, the initial version here
is normal, what would happen if we uploaded
41:09 - a file for the first time? That because remember,
these were uploaded, before we turned on?
41:15 - versioning? Right. And so they were set to
novel what happens when we upload a new file
41:19 - with versioning turned on? Is it going to
be normal? Or is it going to have its own
41:22 - version ID? Okay, that's a kind of a burning
question I have in the back of my mind. And
41:27 - so I added another image here, we now have
Keiko, so we're going to upload kago. And
41:32 - we're going to see is it going to be no, or
is going to have an ID and so look, it actually
41:36 - has a version ID. So the only reason these
are no is because they existed prior to versioning.
41:43 - Okay, so if you see no, that's the reason
why. But if you have versioning turned on,
41:49 - and then from then on, it's always going to
have a version ID. Actually, just before we
41:53 - move on to encryption, I just want to double
check something here. So if we were to go
41:57 - to versions here, and I was saying like, the
initial version here is novel, what would
42:01 - happen if we uploaded a file for the first
time? That because remember, these were uploaded,
42:08 - before we turned on? versioning? Right. And
so they were set to novel what happens when
42:11 - we upload a new file with versioning turned
on? Is it gonna be normal? Or is it gonna
42:15 - have its own version ID? Okay, that's a kind
of a burning question I have in the back of
42:19 - my mind. And so I added another image here,
we now have Keiko and so we're gonna upload
42:24 - Keiko, and we're gonna see is it going to
be no? Or is it going to have an ID and so
42:28 - look, it actually has a version ID. So the
only reason these are null is because they
42:34 - existed prior to versioning. Okay, so if you
see no, that's the reason why. But if you
42:41 - have versioning turned on, and then from then
on, it's always going to have a version of
42:46 - it. Alright, so let's explore how to turn
on server side encryption, which is very easy
42:54 - on s3. So we're going to go back to our bucket,
go to our properties, and then click on default
42:59 - encryption. And by default, you're gonna see
we don't have any encryption turned on. But
43:02 - we can turn on server side encryption using
a Aes 256, which is uses a 256 algorithm in
43:11 - length or key in length. I'm always bad on
that description there. But the point is,
43:15 - it's 256 something in length for encryption,
and then we have AWS kms, we're going to turn
43:21 - on a Aes 256, because that's the easiest way
to get started here. But look at the warning
43:25 - up here, it says the property, this property
does not affect existing objects in your bucket.
43:30 - Okay. And so we're going to turn on our encryption.
So we got our nice purple checkmark. And we're
43:34 - going to go back to our bucket to see if any
of our files are encrypted, which I don't
43:38 - think they're going to be based on that message
there. So we're going to go and check out
43:41 - data. And we can see that there is no server
side encryption. Okay. So in order to turn
43:48 - it on for existing files, I would imagine
it's gonna be the same process here, we'll
43:52 - go to properties. We're going to have encryption
here, and we're going to turn on a Aes 256.
43:56 - So you can see that you can set individual
encryption profiles. And you can also do it
44:02 - per bucket. And so we're going to go ahead
there and encrypt data there. Alright, so
44:08 - now if we were to go access this URL, do we
have permissions even though it is set to
44:13 - public? So remember, data is public, right?
But can we see it when encryption is turned
44:17 - on? And apparently, we totally totally can.
So encryption doesn't necessarily mean that
44:23 - the files aren't accessible, right? It just
because we have made this file public, it
44:28 - just means that when they're at rest on the
servers on AWS, there are going to be encrypted.
44:34 - Okay. So, you know, that is how easy it is
to turn on encryption. Now when it comes to
44:41 - accessing files via the csli and kms. There
is a little bit more work involved there.
44:47 - So you know, for that there's going to be
a bit more of a story there. But, you know,
44:52 - if we do get to see alive, we'll talk about
that. Okay.
44:57 - Now, I want to To show you how to access private
files using pre signed URL. But just before
45:05 - we get to that, I figured this is a good opportunity
to learn how to use the COI for s3. And we'll
45:11 - work our way to a pre signed URL. So here
I have my terminal open. And I already have
45:17 - the a vcli installed on my computer here.
So what we're going to do first is just list
45:22 - all the buckets within our Eva's count. So
we can do ABS s3 Ls LS stands for list. And
45:28 - then what's going to do is we're going to
see that single bucket that we do actually
45:31 - have here, if we wanted to see the contents
of it, we can just type AWS s3 Ls, and then
45:36 - provide the bucket name. And it's gonna show
us its content, which is a single folder.
45:42 - And then if we wanted to see within that folder,
you kind of get where we're going here, we
45:45 - can put that on the end there and hit enter.
Okay, and then we're gonna see all our files.
45:50 - So that's how easy it is to use. ls, you're
going to notice over here, I do have a very
45:56 - slightly different syntax here, which is the
using the s3 protocol here in the front. This
46:02 - is sometimes as needed for certain commands,
which are going to find out here with CP in
46:07 - a moment. But not all commands require it,
okay, like so for instance, in LS, we've admitted
46:12 - that protocol there, but yeah, moving on to
copying files, which CP stands for, we can
46:19 - download objects to and from our desktop here.
So let's go ahead and actually go download
46:24 - a Barkley from our bucket here. So I'm just
going to clear this here, and type A, this
46:29 - s3, CP, we're gonna use that protocol, we
definitely have to use it for CP or will air
46:34 - out and we'll do exam Pro, there was a zero
enterprise enterprise D here. And then it's
46:43 - going to be Barclay, okay. And we're just
going to want to download that, that fellow
46:49 - there to our desktop. Okay, and then we're
just gonna hit enter there. And it's just
46:55 - complain, because I typed it in manually.
And I have a spelling mistake, we need an
46:59 - R there. And it should just download that
file. Great. So if we go check our desktop,
47:05 - there we go, we've downloaded a file from
our s3 bucket. Now, we want to upload a file.
47:09 - So down in here, I have an additional file
here called Q, and I want to get that into
47:14 - my bucket vcli, it's going to be the same
command, we're just going to do it in the
47:18 - reverse order here. So we're gonna do Avis
s3, CP, and we're first gonna provide the
47:22 - file locally, we want to upload here, and
that's going to be enterprise d, q dot jpg,
47:28 - and we're going to want to send that to s3.
So we have to specify the protocol of the
47:33 - bucket name, the folder here that it's going
to be in enterprise D, make sure it won't
47:38 - be spelling mistakes this time. And we're
just going to put q dot jpg, okay. And we're
47:42 - going to send out there to s3 and you can
see it's uploaded, we're going to refresh.
47:47 - And there it's been added to our s3 bucket.
Great. So now we know how to list things and
47:54 - upload or download things from s3. And now
we can move on to pre signed URL. So we saw
47:59 - earlier that was data, we had access to data
here because he was public. So if we were
48:06 - to click this fellow here, we can access him
right. But let's say we wanted to access a
48:11 - queue that we just uploaded, right? And so
by default, they are private. Okay, so if
48:16 - I was to open this, I'm not going to be able
to see it, it's access tonight, which is a
48:20 - good, good, good and sane default. But let's
say I wanted to give someone temporary access.
48:26 - And this is where pre signed URLs come. So
pre signed URLs, what it's going to do is
48:30 - going to generate a URL with the credentials
that we need to be able to temporarily temporarily
48:36 - access it. Okay. So if I were to copy this
AWS s3 pre signed URL here, and what we'll
48:43 - just type it out, it's not a big deal here.
And we're going to try to get access to this,
48:50 - this Q file here. So we're going to want to
do enterprise D.
48:57 - And we're gonna say q dot j, jpg, and we're
gonna put an expires on there expires, like
49:01 - by default, I
49:02 - think it's like an hour or something. But
we want it to expire after 300 seconds. So
49:06 - people like these aren't these links aren't
staying around there. Again, they're temporary,
49:10 - right? And I'm just going to hit enter there.
Um, and I've made a mistake, I actually forgot
49:16 - to write the word pre signed in there. Okay,
what's going to do, it's going to spit back
49:19 - a URL. So if we were to take this URL, right,
and then supply it up here, now we actually
49:24 - have access. So that's a way for you to provide
temporary access to private files. This is
49:29 - definitely a use case that you'd have if let's
say you had paid content behind, like a web
49:34 - application that you'd have to sign up to
gain access. And this is how you give them
49:39 - temporary access to whatever file they wanted.
And I just wanted to note, I think this is
49:44 - the case, where if we were to actually open
this here, so again, if we have this URL that
49:49 - has the Access ID, etc, up there, but if we
were to open it up via this tab, I think it
49:54 - doesn't exact same thing. So it has a security
token here. So I guess maybe it's not exactly
49:59 - the same thing, but I was hoping maybe this
was actually also using a pre signed URL here.
50:03 - But anyway, the point is, is that if you want
to access temporary files, you're going to
50:07 - be using pre signed URLs.
50:14 - So we uploaded all our files here into this
bucket. And when we did, so it automatically
50:20 - went to the storage class standard by default.
So let's say we want to change the storage
50:25 - class for our objects, we're not going to
do that at the bucket level, we're going to
50:29 - do it at the object level here. So we're gonna
go to properties here for Gaiden. And all
50:35 - we have to do here is choose the class that
we want to standardize. And we're going to
50:40 - hit save, and now we can start saving money.
So that's all it takes to switch storage classes.
50:46 - But let's say we want to automate that process.
Because if we were handling a lot of log files,
50:51 - maybe after 30 days, we don't really need
need them anymore, but we need to hold on
50:55 - to them for the next seven years. And that's
where lifecycle policies are going to come
50:59 - in play. So what we're going to do is we're
going to go back to our bucket, we're going
51:02 - to go to management. Here, we have lifecycle,
and we're going to add a new lifecycle rule.
51:07 - So we'll say, here, we'll do just for something
simple, say so after 30 days, so 30 day rule.
51:13 - And we could limit the scope of what files
we want. So if we wanted to say just enterprise
51:17 - D here, we could do enterprise D, okay, that's
not what I'm going to do. I'm just going to
51:23 - say, all files within the actual bucket here,
go next. And then we can choose the storage
51:28 - class. So transition. So here, we have to
decide whether it's the current version of
51:32 - the previous versions, okay, and so I'm just
gonna say it's gonna be the current version.
51:37 - All right, always the current version here,
we're going to add a transition, and we're
51:40 - going to move anything that's in standard
into standard ay ay ay. and it's going to
51:44 - be after 30 days, I don't think you can go
below that, if I try seven here, see, so the
51:49 - minimum value here has to be 30. So we're
gonna have to set it to 30. I think we saw
51:54 - those, those, those minimums in the actual
storage class when we were setting them. So
52:01 - if you're wondering what those are, they're
probably over there. But we'll just hit next
52:05 - here. And so then after we're seeing that
it's been transitioned, after 30 days, it's
52:11 - going to move to move to that. And we can
also set an expiration. So we don't necessarily
52:19 - need to set this but this is if we wanted
to actually delete the file. So after current
52:22 - days, we could then say, to completely delete
the files, which is not what we're going to
52:28 - do, we're just going to hit next. And click
that. And now we have a rule that was going
52:35 - to automate the moving of our files from one
storage class to another. Alright, so there
52:41 - you go.
52:43 - So we're gonna learn how to set up cross region
replication here. And so this is going to
52:50 - allow us to copy one files to from a bucket
to another bucket. And this could be in another
52:55 - region and in another AWS account, okay, so
there's a few different possibilities here
53:02 - as to why we'd want to do that. But let's
just learn how to actually do it. And so we
53:06 - did, we're going to need to create a replication.
But before we can do that, we're going to
53:09 - need a destination bucket. So we're going
to go back here to s3, we're going to create
53:14 - a new bucket, I'm just going to call it exam
pro BBB. And I'm going to set this to Canada
53:19 - Central, okay, if this name is not available,
you're just going to have to come up with
53:23 - your own names here. But just make sure you're
saying it to another region for this, this
53:28 - example here. And so now we have a bucket
in the States and in Canada, and we're almost
53:33 - ready to go, we just have to make sure that
we have versioning turned on in both buckets,
53:37 - both the source and destination. So we'll
go here to our new bucket, turn on versioning.
53:41 - Okay, and I already know that we have versioning
turned on in our source. But we'll just take
53:46 - a quick look here. So here it is turned on.
And so now we are ready to turn on cross region
53:52 - replication. So we'll go ahead and create
our rule in our source bucket, our source
53:56 - bucket is selected here. Then we'll go next.
And we will choose our destination bucket.
54:02 - And so now we have a couple options here,
which can happen during replication. So we
54:08 - can actually change the storage class, which
is a good idea if you want to save money.
54:11 - So the other bucket is just like your backup
bucket. And you don't plan to really use those
54:16 - files, you probably would want to change the
storage class there to save money. And you
54:22 - can also send the this to someone else's bucket
in another AWS account. So maybe you your
54:27 - use case is this bucket has a files and you
want to provide it to multiple clients. And
54:33 - so you've used that replication rule to replicate
it to their, their buckets, okay, but we're
54:38 - just this is going to be our buckets for the
time being. And we're going to go ahead here
54:41 - and create a new role. And we'll just call
it cc R. us to Canada. Okay. I will just create
54:50 - a new rule. So we have permissions to go ahead
and do that there. And we'll get a nice little
54:55 - summary here and hit save. And we will wait
and we'll cross our fingers as As the replication
55:00 - figuration was not found, so this sometimes
happens, it's not a really big deal here.
55:05 - So just go back to replication. And it actually
did work, I think. So, sometimes what happens
55:11 - is, the roll isn't created in time. So you
know, sometimes it's green, and sometimes
55:18 - it's red, but just come back and double check
here because it definitely is set. So now
55:21 - we have replication set up. So now we're going
to learn how to set up bucket policies. So
55:31 - we can create custom rules about the type
of access we want to allow to our buckets.
55:36 - So in order to do so we're going to go to
our exam pro 000 bucket, we're going to go
55:40 - to permissions, and we're going to go to bucket
policy, okay. And so this is where we're going
55:44 - to provide a policy in the format of Jason
file here, it's very hard to remember these.
55:50 - So luckily, they have a little policy generator
down here, I'm gonna open it in a new tab.
55:52 - And we're going to drop this down to s3, and
we're going to define what kind of access
55:59 - control we want to have. So let's say we wanted
to deny anyone being able to upload new files
56:05 - in this bucket. I don't know why you'd want
to do that. But maybe there's a use case.
56:08 - So we're gonna say denied, we're gonna give
it a asterik here. So we can say this applies
56:14 - to everyone. The service is going to be s3,
of course, and we're going to look for the
56:18 - actions here. So we're just going to look
for the puts. So we'll say put bucket ACL.
56:25 - And there should just be a regular put in
here. Oh, that's, that's bucket ACL. We want
56:29 - objects. So we say bucket, put object and
put object ACL so we can't upload files. And
56:34 - we'll have to provide the Arn and they give
you a bit of an indicator as to what the format
56:38 - is here. That's going to be exam pro hyphen,
000, at Ford slash Asterix, so it's gonna
56:44 - say any of the files within that bucket. And
we're gonna go add that statement and generate
56:49 - that policy. And now we have our JSON. Okay,
so we'll copy that, go back over here, paste
56:55 - it in, save it, cross your fingers, hope it
works. And it has saved Yeah, so you don't
57:00 - get like a response here. I'm just gonna save
again, and then just refresh it to just be
57:04 - 100% sure here and go back to your bucket
policy. And there it persists. So our bucket
57:08 - policy is now in place. So we should not be
able to upload new files. So let's go find
57:13 - out if that is actually the case. So here
I am in the overview here and enterprise D.
57:19 - And I want to upload a new file to to see
so let's go to our enterprise D and we have
57:24 - a new person here we have Tom Alok, he is
a Romulan, and we do not want him in the enterprise
57:30 - D here. So we're going to drag it over here
and see what happens, we're going to hit upload.
57:37 - Okay. And you're going to see that it's successfully
uploaded. Okay,
57:42 - so we're going to go ahead and do a refresh
here and see if it actually is there. And
57:48 - it looked like it worked. But I guess it didn't,
because we do have that policy there. So there
57:52 - you go. But let's just to be 100% sure that
our policy actually is working, because I
57:58 - definitely don't see it there. We're gonna
go back there. And we're just going to remove
58:01 - our policy. Okay, so we're gonna go ahead
here and just delete the policy, right? It's
58:06 - where the interface shows it to you as if
you know, it's actually working. And so Tom
58:10 - lock is definitely not in there. But our policy
has been removed. So now if we were to upload
58:14 - it, Tom should be able to infiltrate the enterprise
D bucket here, we're going to do an upload
58:19 - here. Okay. Let's see if we get a different
result. And there it is. So there you go.
58:25 - So our bucket policy was working, you can
see that ABS can be a little bit misleading.
58:32 - So you do have to double check things that
happened for me all the time. But there you
58:37 - go, that is how you set up.
58:44 - So we are on to the s3 cheat sheet. And this
is a very long cheat sheet because s3 is so
58:49 - important to the eight of us associate certification,
so we need to know the service inside and
58:55 - out. So s3 stands for simple storage service.
It's an object based storage, and allows you
59:02 - to store unlimited amounts of data without
worrying of the underlying storage infrastructure.
59:07 - s3 replicates data across at least three availability
zones to ensure 99.9% availability and 11
59:14 - nines of durability on just contain your data.
So you can think of objects like files, objects,
59:20 - objects can be sized anywhere from zero bytes
to five terabytes, I've highlighted zero bytes
59:25 - in red because most people don't realize they
can be zero bytes in size. Buckets contain
59:31 - objects and buckets can also contain folders,
which can in turn contain objects. And you
59:37 - can also just think of buckets themselves
as folders. Buckets names are unique across
59:42 - all AWS accounts. So you can treat them like
domain names. So there you your bucket name
59:48 - has to be unique within within the entire
world. When you upload a file to s3 successfully
59:54 - then you'll receive an HTTP 200 code Lifecycle
Management feature So this allows you to move
60:01 - objects between different storage classes.
And objects can be deleted automatically based
60:07 - on a schedule. So you will create lifecycle
lifecycle rules or policies to make that happen,
60:14 - then you have versioning. So this allows you
to have
60:17 - version IDs on your objects. So when you upload
a new object, the overtop of an existing object,
60:25 - the old objects will still remain, you can
access any previous object based on their
60:30 - version ID. When you delete an object, the
previous object will be restored. Once you
60:35 - turn on versioning cannot be turned off, it
can only be suspended. Then we have MFA delete.
60:41 - So this allows you to enforce all delete operations
to require an MFA token in order to delete
60:48 - an object, so you must have versioning turned
on to use this, you can only turn on MFA delete
60:54 - from the ADC Li and it's really just the root
account or the root user who's allowed to
60:59 - delete these objects. All new buckets are
private by default, logging can be turned
61:05 - on on a bucket. So you can track all the operations
performed on objects. Then you have access
61:11 - control, which is configured using either
bucket policies or access control lists. So
61:16 - we have bucket policies, which are Jason documents,
which let you write complex control access.
61:23 - Then you have ACLs. And they are the legacy
legacy method. They came before bucket bucket
61:28 - policies. And they're not depreciated. So
there's no full part in using them, but they're
61:32 - just not used as often anymore. And it allows
you to grant object access to objects and
61:37 - buckets with simple actions. And so now we're
on to the security portion. So security in
61:44 - transit is something you have with s3, because
all the files uploaded are done over SSL.
61:49 - And so you have SSC, which stands for server
side encryption. s3 has three options for
61:54 - SSE. We have SSE A S. And so s3 handles the
key itself, and it uses a Aes 256 algorithm
62:04 - as the encryption method, then you have SSE
kms. And as the name implies, it is using
62:10 - a key management service, which is an envelope
encryption service. And so AWS manages the
62:15 - the key and so do you, then you have SSE C,
and the C stands for work customer. So it's
62:21 - a customer provided key, you actually upload
the key, and you have full control over key
62:26 - but you also have to manage that key. All
right, s3 doesn't come with client side encryption,
62:31 - it's up to you to encrypt your files locally,
and then upload them to s3, you could store
62:36 - your client side key in kms. So that is an
option for you. But it's not that important
62:41 - to actually have here on the cheat sheet.
You have also cross region replication. This
62:47 - allows you to replicate files across regions
for greater durability, you must have versioning
62:52 - turned on in the source and destination bucket
in order to use cross region replication.
62:57 - And you can replicate a source bucket to a
bucket in another AWS account, then you have
63:04 - transfer acceleration. This provides fast
and secure uploads from anywhere in the world
63:08 - data is uploaded via a distinct URL to an
edge location. And data is then transported
63:13 - to your s3 bucket via the AWS backbone network,
which is super fast, then you have pre signed
63:18 - URLs. And this is a URL generated via the
HSC ally or SDK, it provides temporary access
63:23 - to write or download to an object like data
to that actual object via that endpoint. Pre
63:29 - signed URLs are commonly used to access private
objects. And the last thing is our storage
63:34 - classes. And we have six different kinds of
storage classes started with standard. And
63:39 - that's the default one. And it's fast. It's
has 99.99% availability, 11 nines of durability,
63:45 - you access files within the milliseconds and
it replicates your data across at least three
63:50 - azs. Then you have the intelligent tear, tearing
storage class. And this uses XML to analyze
63:55 - your objects usage and determine the appropriate
storage to help you save money and just move
64:00 - to those other storage classes which recovering
now, then you have standard and frequency
64:05 - access. Review to IAA it's just as fast as
standard. It's cheaper to access files, if
64:12 - you're only accessing files less than once
a month. So just one file in the month, if
64:17 - you access it twice. Now it's the same class
of standard probably a little bit more because
64:20 - there's an additional retrieval fee. When
you try to grab those files. It's 50% less
64:27 - than standard. The trade off here is reduced
availability, then you have one zone IAA.
64:32 - And as the name implies, it's not replicating
cross three azs just at least three Z's. It's
64:37 - only in one az, so it's going to be super
fast. And the trade off here is it's going
64:43 - to be 20% cheaper than standard IAA, but now
you also have reduced durability. And again,
64:49 - it has a retrieval fee. Then you have glacier
and Glazier is for long term cold storage.
64:55 - It's archival storage, and it's a very very,
very cheap the tradeoff here is that it's
65:01 - going to take between minutes to hours for
you to actually access your files if you need
65:06 - them. Then you have glacier deep archive,
it is the cheapest, the cheapest solution
65:10 - or storage class on our list. And you can't
access your files for up to 12 hours. So if
65:18 - that's how long it's going to take before
you can use them. So that is the s3 cheat
65:21 - sheet. It was a very long cheat sheet. But
there's a lot of great information here. So
65:25 - hey, this is Angie brown from exam Pro. And
we are looking at AWS snowball, which is a
65:34 - petabyte scale data transfer service. So move
data onto a database via a physical briefcase
65:40 - computer. Alright, so let's say you needed
to get a lot of data onto AWS very quickly
65:46 - and very inexpensively. Well, snowball is
going to help you out there because if you
65:51 - were to try and transfer 100 terabytes over
a high speed internet to AWS, it could take
65:56 - over 100 days where with a snowball, it will
take less than a week. And again, for cost.
66:02 - If you had to transfer 100 terabytes over
high speed internet, it's gonna cost you 1000s
66:05 - of dollars, where snowball is going to reduce
that cost by 1/5. Now we'll just go through
66:10 - some of the features of snowball here, it
does come with an E ink display, it kind of
66:14 - looks like your shipping label, but it is
digital, which is kind of cool. It's tamper
66:19 - and weatherproof. The data is encrypted end
to end using tivity. Six bit encryption, it
66:24 - has a Trusted Platform Module, TPM is just
this little chip here. And as it says here,
66:30 - endpoint device that stores RSA encryption
keys specific to host systems for hardware
66:34 - authentication. So that's a cool little hardware
feature. And for security purposes, data transfers
66:39 - must be completed within 90 days of the snowball
being prepared. And this data is going to
66:43 - come into s3, you can either import or export.
So not only can you you know use this to get
66:50 - data into the cloud, it can be a way for you
to get data out of the cloud it snowball comes
66:54 - in two sizes, we have 50 terabytes and 80
terabytes. Now you don't get to utilize all
66:58 - the space on there. So in reality, it's really
42 terabytes and 72 terabytes. And you're
67:03 - going to notice that it said this was a petabyte
scale migration, while they're suggesting
67:09 - that you use multiple multiple snowballs to
get to petabytes. So you don't you don't transport
67:15 - petabytes in one snowball, it's going to take
multiple snowballs to do that. Alright. Now
67:24 - we're going to take a look here at Ada snowball
edge, which again, is a petabyte scale data
67:28 - transfer service move data on database via
a physical briefcase computer, but it's going
67:33 - to have more storage and onsite compute capacity
capabilities. So just looking at snowball
67:38 - edge here in greater detail, you're going
to notice one aesthetic difference is that
67:42 - it has these little orange bars, maybe that's
the way to distinguish snowball from snowball
67:45 - edge, but it's similar to snowball, but with
more storage and with local processing. So
67:50 - going through these features, instead of having
an ink display, it's going to have an LCD
67:55 - display. So again, it's the shipping information.
But with other functionality. The huge advantage
68:00 - here is that it can undertake local processing
and edge computing workloads. It also has
68:05 - the ability to cluster so you can get a bunch
of these noble edges and have them work on
68:10 - a single job kind of like having your own
little mini data center up to five to 10 devices.
68:15 - And it comes in three options for device configuration.
So you can optimize for storage compute or
68:19 - with GPU optimization. And the CPU amounts
are going to change in this device based on
68:25 - what you need. And snowball edge comes in
two sizes. We have 100 terabytes, that's 83
68:30 - terabytes of usage space. And then we have
the clustered version, and it's lesser, a
68:35 - fewer amounts of terabytes. But of course,
you're gonna be using this in clusters. So
68:39 - there's good reasons for that. So there you
go. That's still no, now we're taking a look
68:47 - here at snowmobile, and it is a 45 foot long
shipping container pulled by a semi trailer
68:52 - track, it can transfer up to 100 petabytes
personal mobiel. So in order to get to exabytes,
68:58 - you're gonna need a few of these, but it definitely
is feasible. And it has some really cool security
69:02 - features built in. We have GPS tracking, alarm
monitoring, 24 seven video surveillance and
69:07 - an escort security vehicle while in transit.
Now that is an optional feature. I don't know
69:11 - if it costs more, but it's definitely sounds
really cool. So you know, just to wrap this
69:16 - up here, eight of us personnel will help you
connect your network to the snowmobile. And
69:21 - when data transfer is complete, we'll drive
it back data bus and import it into s3 or
69:25 - s3 Glacier. So
69:26 - there you are. I'm for the cheat sheet and
it's for snowball, snowball edge and snowmobile.
69:35 - So let's jump into it. So snowball and snowball
edge is a rugged container, which contains
69:39 - a storage device. snowmobile is a 45 foot
long ruggedized shipping container pulled
69:44 - by a semi trailer truck snowball and snowball
edge is for petabyte scale migration, whereas
69:50 - snowmobile is for exabyte scale migration.
So the advantages here with snowball is low
69:56 - cost so 1000s of dollars to transfer 100 terabytes
over high speed internet snowball comes at
70:01 - 1/5 of the price, then we have speed 100 terabytes
over 100 days to transfer over high speed
70:05 - internet. Or you can use snowball, which takes
less than a week. And so then we talked about
70:10 - snowball here. So snowball comes in two sizes,
we have 50 terabytes and 80 terabytes, but
70:14 - the actual usable space is less so it's 42
and 72. Then you have snowball edge, it comes
70:19 - in two sizes, we have 100 terabytes and 100
terabytes clustered. And then the usability
70:23 - here is gonna be 83 and 45. snowmobile comes
in one size 100 petabytes per vehicle, and
70:30 - you can both export or import data using snowball
or snowmobile Okay, and that also includes
70:35 - snowball edge there, you can import into s3
or glacia snowball edge can undertake local
70:41 - processing and edge low edge computing workloads.
snowball edge can use be used in a cluster
70:47 - in groups of five to 10 devices. And snowball
edge provides three options for device configurations,
70:52 - we have storage optimized, compute optimized
and GPU optimized and the variation there
70:58 - is going to be how many CPUs are utilized
and GPU have is going to have more GPUs on
71:02 - board there. So there you go, that that is
your snowball, snowball edge and snowmobile.
71:09 - Hey, this is Andrew Brown from exam Pro. And
we are looking at Virtual Private Cloud known
71:17 - as VPC. And this service allows you to provision
logically isolated sections of your database
71:24 - cloud where you can launch eight of his resources
in a virtual network that you define.
71:35 - So here we are looking at an architectural
diagram of a VPC with multiple networking
71:40 - resources or components within it. And I just
want to emphasize how important it is to learn
71:46 - VPC and all components inside and out because
it's for every single aidable certification
71:52 - with the exception of the cloud practitioner.
So we definitely need to master all these
71:57 - things. So the easiest way to remember what
a VPC is for is think of it as your own personal
72:02 - data center, it gives you complete control
over your virtual networking environment.
72:07 - All right, so the idea is that we have internet,
it flows into an internet gateway, it goes
72:12 - to a router, the router goes to a route table,
the route table passes through knakal. And
72:17 - the knakal sends the traffic to the public
and private subnets. And your resources could
72:23 - be contained within a security group all within
a VPC. So there's a lot of moving parts. And
72:27 - these are not even all the components. And
there's definitely a bunch of different configurations
72:31 - we can look at. So looking at the core components,
these are the ones that we're going to learn
72:36 - in depth, and there are a few more than these,
but these are the most important ones. So
72:41 - we're going to learn what an internet gateway
is. We're gonna learn what a virtual private
72:45 - gateway is route tables, knackles, security
groups, public and private subnets, Nat gateway
72:52 - and instances customer gateway VPC endpoints
and VPC peering. So, this section is very
72:59 - overwhelming. But you know, once you get it
down, it's it's pretty easy going forward.
73:05 - So we just need to master all these things
and commit them to memory. Now that we kind
73:13 - of have an idea, what's the purpose of VPC,
let's look at some of its key features, limitations
73:19 - and some other little things we want to talk
about. So here on the right hand side, this
73:22 - is the form to create a VPC, it's literally
four fields. It's that simple. You name it,
73:27 - you give it an address, you can also give
it an additional ipv6 address. You can't be
73:33 - or it's either this and this. And you can
set its tendencies to default or dedicated,
73:38 - dedicated, meaning that it's running on dedicated
hardware. If you're an enterprise, you might
73:43 - care about that. This is what the ipv6 cider
block would look like because you don't enter
73:47 - it in Amazon generates one for you. So v PCs
are region specific. They do not span regions,
73:54 - you can create up to five epcs per region.
Every region comes with a default VPC, you
73:59 - can have 200 subnets per VPC, that's a lot
of subnets. You can create, as we said, Here,
74:06 - an ipv4 cider block, you actually have to
create one it's a requirement. And in addition
74:10 - to you can provide an ipv6 cider block. It's
good to know that when you create a VPC, it
74:17 - doesn't cost you anything. That goes the same
for route tables, knackles, internet gateway
74:21 - security groups subnets and VPC peering. However,
there are resources within the VPC that are
74:28 - going to cost you money such as Nat gateways,
VPC endpoints, VPN gateways, customer gateways,
74:32 - but most the time you'll be working with the
ones that don't cost any money so that there
74:37 - shouldn't be too much of a concern of getting
over billed. One thing I do want to point
74:41 - out is that when you do create a VPC, it doesn't
have DNS host names turned on by default.
74:47 - If you're wondering what that option is for
what it does is when you launch easy two instances
74:52 - and so here down below, I have an easy to
instance and it will get a public IP but it
74:58 - will only get a Public DNS, which looks like
a domain name, like an address, and that's
75:03 - literally what it is. But if this isn't turned
on that easy to instance, won't get one. So
75:08 - if you're wondering, why isn't that there,
it's probably because your host names are
75:12 - disabled and they are disabled by default,
you just got to turn that.
75:19 - So we were
75:20 - saying earlier that you get a default VPC
for every single region. And the idea behind
75:24 - that is so that you can immediately launch
EC two instances without having to really
75:29 - think about all the networking stuff you have
to set up. But for Eva certification, we do
75:34 - need to know what is going on. And it's not
just a default dBc It comes with other things
75:39 - and with specific configurations. And we definitely
need to know that for the exams. So the first
75:44 - thing is it creates a VPC of cider block size
16. We're going to also get default subnets
75:50 - with it. So for every single AZ in that region,
we're going to get a subnet per AZ and there
75:56 - gonna be a cider block size 20. It's going
to create an internet gateway and connect
76:00 - it to your default VPC. So that means that
our students is going to reach the internet,
76:05 - it's going to come with a default security
group and associated with your default VPC.
76:09 - So if you launch an EC two instance, it will
automatically default to the security group
76:14 - unless you override it. It will also come
with by by default, a knakal. And associated
76:19 - with your VPC, it will also default DHCP options.
One thing that it's implied is that you It
76:28 - comes with a main route table, okay, so when
you create a VPC, it automatically comes to
76:32 - the main route table. So I would assume that
that comes by default as well. So there are
76:37 - all people.
76:43 - So I just wanted to touch on this 0.0 dot
zero forward slash zero here, which is also
76:47 - known as default. And what it is, is it represents
all possible IP addresses. Okay, and so you
76:54 - know, when you're doing a device networking,
you're going to be using this to get the GW
77:01 - to have a route like routing traffic to the
GW to the internet. When you're using a security
77:06 - group, when you set up your inbound rules,
you're going to set 0.0 dot 0.0 to allow any
77:11 - traffic from the internet to access your public
resources. So anytime you see this, just think
77:18 - of it as giving access from anywhere or the
internet. Okay. We're looking at 
77:27 - VPC peering, which allows you to connect one
VPC to another over direct network route using
77:32 - private IP addresses. So the idea is we have
VPC, a VPC Zb. And we want to treat it so
77:40 - like the they behave like they're on the same
network. And that's what VPC peering connection
77:44 - allows us to do. So it's very simple to create
a peering connection, we just give it a name,
77:50 - we say V, what we want is the requester. So
that could be VP ca and then we want as the
77:55 - acceptor which could be VP CB, and we can
say whether it's in my account, or another
78:00 - account, or this region or another region.
So you can see that allows v PCs from same
78:05 - or different regions to talk to each other.
There is some limitations around the configuration.
78:12 - So you know, when you're peering, you're using
star configuration, so you'd have one central
78:16 - VPC and then you might have four around it.
And so for each one, you're going to have
78:20 - to have a peering connection. There's no transitive
peering. So what does that mean? Well, the
78:26 - idea is like, let's say VPC c wants to talk
to VPC, B, the traffic's not going to flow
78:32 - through a, you actually would have to create
another direct connection from C to B. So
78:39 - it's only to the nearest neighbor, where that
communication is going to happen. And you
78:44 - can't have overlapping cider blocks. So if
these had the same cider block, this was 172
78:49 - 31. This was 172 31, we're gonna have a conflict
and we're not gonna be able to talk to each
78:54 - other. So that is the VPC peering in a nutshell.
78:57 - Alright, so we're taking a look here at route
tables. The route tables are used to determine
79:06 - where network traffic is directed, okay. And
so each subnet in your V PC must be associated
79:13 - with a route table. And a subnet can only
be associated with one route table at a time,
79:18 - but you can associate multiple subnets subnets
with the same route table. Alright, so now
79:24 - down below, I have just like the most common
example of where you're using route tables.
79:29 - And that's just allowing your easy two instances
to gain access to the internet. So you'd have
79:35 - a public subnet where that easy to instance
resides and that's going to be associated
79:40 - with a route table. That read route table
is going to have us routes in here. And here
79:44 - you can see we have a route, which has the
internet gateway attached that allows access
79:52 - to the internet. Okay, so there you go. That's
all there is to it. We're taking a look at
80:01 - Internet gateway internet gateway allows your
VPC access to the internet and I N GW does
80:06 - two things. It provides a target in your VPC
route tables for internet routable traffic.
80:12 - And it can also perform network address translation
Nat, which we'll get into in another section
80:17 - for instances that have been assigned a public
ipv4 address. Okay, so down below here, I
80:24 - have a representation of how I GW works. So
the idea is that we have internet over here
80:31 - and to access the internet, we need an internet
gateway, but to route traffic from our EC
80:36 - two instances or anything, they're gonna have
to pass through a route table to get to a
80:40 - router. And so we need to create a new route
in our route table for the ID W. So I gwi
80:47 - hyphen, Id identifies that resource, and then
we're going to give it 0.0 point zero as the
80:54 - destination. Alright, so that's all there
is to it. So we talked about how we could
81:03 - use Nat gateways or Nat instances to gain
access to the internet for our EC two instances
81:08 - that live in a private subnet. But let's say
you wanted to SSH into that easy to essence,
81:14 - well, it's in a private subnet, so it doesn't
have a public IP address. So what you need
81:18 - is you need an intermediate EC two instance
that you're going to SSH into. And then you're
81:23 - going to jump from that box to this one, okay?
And that's why bastions are also known as
81:27 - jump boxes. And this institute instance for
the bastion is hardened. So it should be very,
81:34 - very secure, because this is going to be your
point of entry into your private EC two instances.
81:40 - And some people might always ask, Well, if
a NAT instance, like Nat gateways, we can't
81:45 - obviously turn into bastions, but a NAT instance
is just a new situation, it's Couldn't you
81:50 - have it double as a bastion, and the possibility
of it is possible, but generally the way you
81:57 - configure NATS and also, from a security perspective,
you'd never ever want to do that, you'd always
82:02 - want to have a different EC two instance,
as your Bastion. Now, there is a service called
82:10 - SYSTEMS MANAGER, session manager and it replaces
the need for bastions so that you don't have
82:15 - to launch your own EC two instances. So generally,
that's recommended in AWS. But you know, bastions
82:21 - are still being commonly used throughout a
lot of companies because it needs to meet
82:26 - whatever their requirements are, and they're
just comfortable with them. So there you go.
82:32 - So we're gonna take a look at Direct Connect,
and Direct Connect is in aid of a solution
82:38 - for establishing dedicated network connections
from on premise locations to AWS, it's extremely
82:44 - fast. And so depending on what configuration
you get, if it's in the lower bandwidth, we're
82:49 - looking between 1550 megabytes to 500 megabytes,
or the higher bandwidth is one gigabytes to
82:56 - 10 gigabytes. So the transfer rate to your
on premise environment, the network to AWS,
83:02 - is it considerably fast. And this can be really
important if you are an enterprise and you
83:07 - want to keep the same level of performance
that you're used to. So yeah, the takeaway
83:13 - here with Direct Connect is that it helps
reduce network costs increase bandwidth throughput,
83:17 - it provides a more consistent network experience
than a typical internet internet based connection.
83:23 - Okay, so that's all.
83:28 - We're looking
83:29 - at VPC endpoints, and they're used to privately
connect your V PC to other Ada services, and
83:35 - VPC endpoint services. So I have a use case
here to make it crystal clear. So imagine
83:40 - you have an EC two instance, and you want
to get something from your s3 bucket. So what
83:44 - you normally do is use the ABS SDK and you
would make that call, and it would go out
83:50 - of your internet gateway to the internet back
into the AWS network to get that file or or
83:58 - object out of s3. So wouldn't it be more convenient
if we could just keep the traffic within the
84:04 - AWS network and that is the purpose of a VPC
endpoint. It helps you keep traffic within
84:11 - the EBS network. And the idea is now because
it does not leave a network, we do not require
84:16 - a public IP address to communicate with these
services. I eliminates the need for an internet
84:21 - gateway. So let's say we didn't need this
internet gateway, the only reason we were
84:24 - using it was to get to s3, we can now eliminate
that and keep everything private. So you know,
84:29 - there you go. There are two types of VPC endpoints
inter interface endpoints and gateway endpoints.
84:34 - And we're going to get into that.
84:40 - So we're going to look at the first type of
VPC endpoint and that is interface endpoints.
84:46 - And they're called interface endpoints because
they actually provision an elastic network
84:50 - interface, an actual network interface card
with a private IP address, and they serve
84:55 - as an entry point for traffic going to a supported
service. If you read a bit more about interface
85:01 - endpoints, they are powered by AWS private
link. There's not much to say here, that's
85:06 - just what it is. So Access Services hostname
is easily securely by keeping your network
85:10 - traffic within a bus network. This is always
confused me this branding of Eva's private
85:15 - link. But you know, you might as well just
think of interface endpoints, and it is prevalent
85:19 - to be in the same thing. Again, it does cost
something because it is speeding up and he
85:23 - and I, and so you know, it's it's point 01
cents per hour. And so over a month's time,
85:30 - if you had it on for the entire time, it's
going to cost around $7.50. And the interface
85:36 - endpoint supports a variety of native services,
not everything. But here's a good list of
85:41 - them for you.
85:42 - The second type of VCP endpoint is a gateway
endpoint and a gateway endpoint. It has a
85:52 - target for a specific route in your row table
used for traffic destined for a supported
85:57 - database service. And this endpoint is 100%.
Free because you're just adding something
86:02 - to your row table. And you're going to be
utilizing it mostly for Amazon s3 and dynamodb.
86:08 - So you saw that first use case where I showed
you that we were getting the AC units in stock
86:15 - yesterday that was using a gateway endpoint.
So there you go. Here we are at the VPC endpoint
86:25 - cheat sheet and this is going to be a quick
one, so let's get to it. VPC endpoints help
86:30 - keep traffic between awa services within the
AWS network. There are two kinds of VPC endpoints
86:36 - interface endpoints and gateway endpoints.
interface endpoints cost money, whereas gateway
86:41 - endpoints are free interface endpoints uses
a elastic network interface in the UI with
86:48 - a private IP address part. And this was all
powered by private link. gateway endpoints
86:53 - is a target for a specific route in your route
table. And interface endpoints support many
86:59 - ad services, whereas gateway endpoints only
support dynamodb and s3.
87:10 - So we're going to take a look at VPC flow
logs, which allows you to capture IP traffic
87:14 - information in and out from your network interfaces
within your VPC. So you can turn on flow logs
87:20 - at three different levels. You can turn it
on at the VPC level, which we're doing right
87:24 - here. You can turn it on at a specific subnet,
or you can turn it on for a specific network
87:29 - interface. The idea is this all trickles down.
So the turn off VBC is it's monitoring everything
87:34 - below. And same thing with subnets. To find
VPC flow logs, you just go to go to the VPC
87:40 - console, and there's gonna be a tab for flow
logs. Same thing with subnets and network
87:44 - interface, we're gonna be able to create that
flow log. And so here is that forms Do you
87:48 - have an idea what you can do here. So the
idea is you can choose to filter for only
87:53 - the accepted or rejected or all. So I'm saying
it all, and it can deliver those logs to cloudwatch
88:00 - logs, they can also deliver them to an s3
bucket If you would prefer them to go there
88:05 - instead. So you know, that's the general stuff.
But once you create a flow log, you can't
88:10 - really edit it, all you can do is delete it.
So there you go. So we now know what VPC flow
88:20 - logs are for. But let's actually take a look
at what VPC flow logs look like. And so here
88:25 - I have the structure up here of the data that
is stored on a VPC flow logs so it stores
88:30 - these on individual lines. And immediately
below, we actually have an example of a VPC
88:36 - flow log. And this is the full description
of all these attributes. And these are pretty
88:40 - straightforward. The only thing I really want
you to go away with here is that the fact
88:44 - that it stores the source IP address and the
destination IP address. There's some exam
88:50 - questions and the probably at the pro level
or the specialty level where we're talking
88:56 - about VPC flow logs. And the question might
have to do with like, you know, just the VPC
89:00 - flow logs contain host names or does it contain
IP addresses? And, and the answer is it contains
89:06 - IP addresses. So that's the big takeaway here
that I wanted to show. So now that we've learned
89:15 - everything about VPC flow logs, here's your
cheat sheet for when you go sit the exam.
89:20 - So the first thing is VPC flow logs monitors
the in and out traffic of your network interfaces
89:24 - within your VPC. You can turn on flow logs
at the VPC subnet or network interface level.
89:31 - VPC flow logs cannot be tagged like other
resources. You cannot change the configuration
89:36 - of a flow log after it's been created. You
cannot enable full logs for epcs which are
89:41 - appeared within your VPC unless it's the same
account. VPC flow logs can be delivered to
89:46 - s3 or cloud watch logs. VPC flow logs contain
the source and destination IP addresses so
89:52 - not the host names okay. And there's some
instance traffic that will not get monitored.
89:57 - So instance traffic generated by Contact the
Avis DNS servers, Windows license activation
90:03 - traffic from instances traffic to and from
instance metadata addresses DHCP traffic any
90:09 - traffic to the reserved IP address of the
default VPC router. So there you go. Andrew
90:20 - Brown from exam Pro, and we are looking at
network access control lists, also known as
90:23 - knackles. It is an optional layer of security
that acts as a firewall for controlling traffic
90:29 - in and out of subnets. So knackles act as
a virtual firewall at the subnet level. And
90:36 - when you create a VPC, you automatically get
a knakal by default, just like security groups
90:42 - knackles have both inbound and outbound rules
of the difference here is that you're going
90:47 - to have the ability to allow or deny traffic
in either way. Okay, so for security groups,
90:53 - you can only allow whereas knackles, you have
deny. Now, when you create these rules here,
91:00 - it's pretty much the same as security groups
with the exception that we have this thing
91:03 - called rule number And rule number is going
to determine the order of evaluation for these
91:09 - rules, and the way it evaluates is going to
be from the lowest to the highest, the highest
91:14 - rule number, it could be 32,766. And AWS recommends
that when you come up with these rule numbers,
91:23 - you use increments of 10 or 100. So you have
some flexibility to create rules in between
91:29 - if you need be, again, subnets are at the
subnet level. So in order for them to apply,
91:34 - you need to associate subnets to knackles.
And subnets can only belong to a single knakal.
91:40 - Okay, so yeah, where you have security groups,
you can have a instances that belong to multiple
91:48 - ones for knackles. It's just a singular case,
okay? Alright,
91:55 - we're just gonna look at a use case for knackles.
Here, it's going to be really around this
92:00 - deny ability. So let's say there is a malicious
actor trying to gain access to our instances,
92:06 - and we know the IP address, well, we can add
that as a rule to our knakal and deny that
92:11 - IP address. And let's say we know that we
never need to SSH into these instances. And
92:18 - we just want to additional guarantee in case
someone Miss configures, a security group
92:23 - that SSH access is denied. So we'll just deny
on port 22. And now we have those two cases
92:29 - covered. So there you go. So we're on to the
knackles cheat sheets. Let's jump into it.
92:38 - So network access control list is commonly
known as nachal. v PCs are automatically given
92:44 - default knakal, which allow all outbound and
inbound traffic. Each subnet with any VPC
92:49 - must be associated with a knakal. subnets
can only be associated with one knakal. At
92:54 - a time associating a subnet with a new knakal
will remove the previous Association. If a
92:59 - knakal is not exclusively associated with
a subnet, the subnet will automatically be
93:03 - associated with the default knakal knakal
has inbound and outbound rules just like security
93:09 - groups, rules can either allow or deny traffic,
unlike security groups, which can only allow
93:15 - knackles or stateless. So that means your
it's going to allow inbound traffic and also
93:19 - outbound. When you create a knakal, it will
deny all traffic by default knackles contain
93:26 - a number numbered list of rules that gets
evaluated in order from lowest to highest.
93:31 - If you need to block a single IP address,
you could you could be a knackles. You cannot
93:37 - do this via security groups because you cannot
have deny actions. Okay, so there you go.
93:46 - Hey, it's Andrew Brown from exam Pro, we are
looking at security groups. And they help
93:52 - protect our EC two instances by acting as
a virtual firewall controlling the inbound
93:57 - and outbound traffic, as I just said, security
groups acts as a virtual firewall at the instance
94:02 - level. So you would have an easy to instance
and you would attach to it security groups.
94:06 - And so here is an easy to instance. And we've
attached a security group to it. So what does
94:12 - it look like on the inside for security groups,
each security group contains a set of rules
94:17 - that filter traffic coming into. So that's
inbound, and out of outbound to that easy
94:23 - to instance. So here we have two tabs, inbound
and outbound. And we can set these are rules,
94:28 - right. And we can set these rules with a particular
protocol and a port range. And also who's
94:33 - allowed to have access. So in this case, I
want to be able to SSH into this YSU instance,
94:39 - which uses the TCP protocol. And the standard
port for SSH is 22. And I'm going to allow
94:46 - only my IP so anytime you see forward slash
32 that always means my IP. All right. So
94:53 - that's all you have to do to add inbound and
outbound rules. There are no deny rules. So
94:59 - all traffic is blocked by default unless a
rule specifically allows it. And multiple
95:05 - instances across multiple subnets can belong
to a security group. So here I have three
95:10 - different EC two instances, and they're all
in different subnets. And security groups
95:14 - do not care about subnets, you just assign
EC two instance, to a security group. And,
95:21 - you know, just in this case, and they're all
in the same one, and now they can all talk
95:24 - to each other, okay?
95:28 - You're I have three security group scenarios,
and they all pretty much do the same thing.
95:34 - But the configuration is different to give
you a good idea of variation on how you can
95:38 - achieve things. And so the idea is we have
a web application running on a situ instance.
95:43 - And it is connecting to an RDS database to
get its information running in a private subnet.
95:48 - Okay. And so in the first case, what we're
doing is we have an inbound rule on the SU
95:55 - database saying allowing for anything from
5432, which is the Postgres port number, for
96:03 - this specific IP address. And so it allows
us these two instance to connect to that RDS
96:09 - database. And so the takeaway here is you
can specify the source to be an IP range,
96:13 - or specific IP. And so this is very specific,
it's forward slash 32. And that's a nice way
96:18 - of saying exactly one IP address. Now in the
second scenario, it looks very similar. And
96:24 - the only difference is, instead of providing
an IP address as a source, we can provide
96:28 - another security group. So now anything within
the security group is allowed to gain access
96:34 - for inbound traffic on 5432. Okay, now, in
our last use case, down below, we have inbound
96:43 - traffic on port 80, and inbound traffic on
port 22, for the SG public group, and then
96:49 - we have the EC two instance and the RDS database
within its own security group. So the idea
96:54 - is that that EC two instance is allowed to
talk to that RDS database, and that EC two
96:59 - instance is not exposing the RDS database
to it well wouldn't, because it's in a private
97:06 - subnets, that doesn't have a public IP address.
But the point is, is that this is to instance,
97:10 - now is able to get traffic from the internet,
it's also able to accept someone from like
97:18 - for an SSH access, okay. And so the big takeaway
here is that you can see that an instance
97:23 - can belong to multiple security groups and
rules are permissive. So when we have two
97:27 - security groups, and this one has allows,
and this is going to take precedence over
97:32 - su stack, which doesn't have anything, you
know, because it's denied by default, everything,
97:36 - but anything that allows is going to override
that, okay, so you can nest multiple security
97:41 - groups onto one EC two instance. So just keep
that stuff. There are a few security group
97:51 - limits I want you to know about. And so we'll
look at the first you can have up to 10,000
97:55 - security groups in a single region, and it's
defaulted to 2500. If you want to go beyond
98:00 - that 2500, you need to make a service limit
increase request to Eva support, you can have
98:06 - 60 inbound rules and 60 outbound rules per
security group. And you can have 16 security
98:11 - groups per EMI. And that's defaulted to five.
Now, if you think about like, how many scripts
98:19 - Can you have on an instance? Well, it's depending
on how many annise are actually attached to
98:24 - that security group. So if you have to realize
that it's attached to a security group, then
98:28 - by default, you'll have 10. Or if you have
the upper limit here, 16, you'll be able to
98:32 - have 32 security groups on a single instance.
Okay, so those are the limits, you know, I
98:38 - thought were worth telling. So we're gonna
take a look at our security groups cheat sheet.
98:46 - So we're ready for exam time. So security
groups act as a firewall at the instance level,
98:51 - unless allowed, specifically, all inbound
traffic is blocked by default, all outbound
98:57 - traffic from the instance is allowed by default,
you can specify for the source to be either
99:02 - an IP range, a single IP address or another
security group. security groups are stateful.
99:08 - If traffic is allowed, inbound is also allowed
outbound. Okay, so that's what stateful means.
99:14 - Any changes to your security group will take
effect immediately. ec two instances can belong
99:18 - to multiple security groups. security groups
can contain multiple EC two instances, you
99:24 - cannot block a specific IP address security
groups. For this, you need to use knackles.
99:29 - Right? So again, it's allowed by default,
sorry, everything's denying you're only allowing
99:36 - things okay. You can have up to 10,000 security
groups per region default is 2500. You can
99:41 - have 60 inbound and 60 outbound rules per
security group. And you can have 16 security
99:46 - groups associated to that. And I default is
five and I can see that I added an extra zero
99:51 - there. So don't worry when you print out your
security scripts to cheat it will be all correct.
99:57 - Okay.
99:59 - We're looking
100:01 - at network address translation, also known
as Nat. And this is the method of remapping
100:05 - one IP address space into another. And so
here you can see we have our local network
100:12 - with its own IP address space. And as it passes
through the NAT, it's going to change that
100:18 - IP address. Well, why would we want to do
this? Well, there's two good reasons for this.
100:22 - So if you have a private networking need to
help gain outbound access to the internet,
100:27 - you need to use a NAT gateway to remap those
private IPS. If you have two networks, which
100:32 - have conflicting network addresses, maybe
they actually have the same, you can use a
100:37 - NAT to make the addresses more agreeable for
communication. So when we want to launch our
100:48 - own Nat, we have two different options in
AWS, we have Nat instances, and Nat gateways.
100:53 - So we're just going to go through the comparison
of these two. So before Nat gateways, all
100:58 - there was was Nat instances, and so you have
to configure that instance, it's just the
101:05 - regular situ instance, to do that remapping.
And so luckily, the community came up with
101:10 - a bunch of Nat instances. And so through the
aect marketplace, you go to community am eyes,
101:17 - you can still do this. And some people have
use cases for it. And you can launch a NAT
101:22 - instance, okay, and so in order for Nat instances
to work, they have to be in a public subnet,
101:27 - because there it has to be able to reach the
internet, if it wasn't a private subnet, there's
101:31 - no way it's going to get to the internet.
So you would launch a NAT instance there,
101:36 - and there you go, there'd be a few more steps
to configuration. But that's all you need
101:40 - to know. Now, when we go over to Nat gateways,
um, it's a managed service. So it's going
101:46 - to set up that easy to instance for you, you're
not going to have access to it, Avis is going
101:50 - to win 100% manage it for you. But it's not
just going to launch one, it's going to have
101:54 - a redundant instance for you. Because when
you launch your own Nat instances, if for
101:59 - whatever reason it gets taken down, then you'd
have to run more than once. And now you have
102:05 - to do all this work to make sure that these
instances are going to scale based on your
102:10 - traffic or have the durability that you need.
So for Nat gateways, they take care of that
102:17 - for you. And again, you would launch it in
a public subnet. The only thing that Nat gateway
102:24 - doesn't do is it doesn't launch them automatically
across other azs for you. So you need to launch
102:30 - a NAT gateway per AZ but you do get redundancy
for your instances. So those are the two methods.
102:36 - And generally you want to use Nat gateways
when possible, because it is the new way of
102:40 - doing it, but you could still use the legacy
way of doing it. So we're on to the NAT cheat
102:51 - sheet and we have a lot of information here.
It's not that important for the solution architect
102:55 - associate, it would definitely come up for
the sysops. Some of these details might matter.
103:03 - So we'll just go through this here. So when
creating a NAT instance, you must disable
103:07 - source and destination checks on the instance.
NAT instances must exist any public subnet,
103:13 - you must have a route out of the private subnet
to the NAT instance, the size of a NAT instances
103:19 - is determined how much traffic can be handled.
Highveld bill availability can be achieved
103:25 - using auto scaling groups, multiple subnets
in different Z's and automate failover between
103:30 - them using a script so you can see there's
a lot of manual labor. When you want to have
103:35 - availability and durability and scalability
for Nat instances, it's all on on to you.
103:39 - And then we'll go look at Nat gateway so Nat
gateways are redundant inside an availability
103:44 - zone so they can survive failure of EC to
have an EC two instance. You can only have
103:49 - one Nat gateway inside one AZ so they cannot
span multiple eyzies starts at five gigabytes
103:56 - per second and scales all the way up to 45
gigabytes per second. NAT gateways are the
104:01 - preferred setup for enterprise systems. There
is no requirement to patch Nat gateways and
104:06 - there's no need to disable source and destination
checks for the NAT gateway. Unlike Nat instances,
104:11 - Nat gateways are automatically assigned a
public IP address route tables for the NAT
104:16 - gateway must be updated resources. in multiple
easy's sharing a gateway will lose internet
104:22 - access if the gateway goes down unless you
create a gateway in each AZ and configure
104:27 - route tables accordingly. So there you go.
That is your net. Hey, this is Andrew Brown
104:35 - from exam pro and we are starting to VPC follow
along and this is a very long section because
104:42 - we need to learn about all the kind of networking
components that we can create.
104:46 - So we're going to learn how to create our
own VPC subnets, route tables, internet gateways,
104:51 - security groups, Nat gateways knackles we're
going to touch it all okay, so it's very core
104:57 - to learning about AWS And it's just great
to get it out of the way. So let's jump into
105:03 - it. So let's start off by creating our own
VPC. So on the left hand side, I want you
105:08 - to click on your VPC. And right away, you're
gonna see that we already have a default VPC
105:13 - within this region of North Virginia. Okay,
your region might be different from mine,
105:18 - it doesn't actually does kind of matter what
region you use, because different regions
105:23 - have different amounts of available azs. So
I'm going to really strongly suggest that
105:29 - you switch to North Virginia to make this
section a little bit smoother for you. But
105:35 - just notice that the default VPC uses an ipv4
cider, cider block range of 172 31 0.0 forward
105:43 - slash 16. Okay, and so if I was to change
regions, no matter what region will go to
105:49 - us, West, Oregon, we're going to find that
we already have a default VPC on here as well.
105:56 - And it's going to have the same a cider block
range, okay. So just be aware that at best
106:01 - does give you a default VPC so that you can
start launching resources immediately without
106:06 - having to worry about all this networking,
and there's no full power with using the default
106:10 - VPC, it's totally acceptable to do so. But
we definitely need to know how to do this
106:15 - ourselves. So we're going to create our own
VPC. Okay, and so I'm a big fan of Star Trek.
106:20 - And so I'm going to name it after the planet
of Bayshore, which is a very well known planet
106:25 - in the Star Trek universe. And I'm going to
have to provide my own cider block, it cannot
106:30 - be one that already exists. So I can't use
that 172 range that AWS was using. So I'm
106:37 - gonna do 10.0 dot 0.0, forward slash 16. And
there is a bit of a rhyme and rhythm to choosing
106:44 - these, this one is a very commonly chosen
one. And so I mean, you might be looking at
106:49 - this going, Okay, well, what is this whole
thing with the IP address slash afford 16.
106:54 - And we will definitely explain that in a separate
video here. But just to give you a quick rundown,
106:58 - you are choosing your IP address that you
want to have here. And this is the actual
107:04 - range, and this is saying how many IP addresses
you want to allocate. Okay. Um, so yeah, we'll
107:10 - cover that more later on. And so now we have
the option to set ipv6 cider, or a cider block
107:17 - here. And so just to keep it simple, I'm going
to turn it off. But you know, obviously, ipv6
107:22 - is supported on AWS. And it is the future
of, you know, our IP protocol. So it's definitely
107:30 - something you might want to turn on. Okay,
and just be prepared for the future there,
107:34 - that we have this tendency option, and this
is going to give us a dedicated hosts. For
107:38 - our VPC, this is an expensive, expensive option.
So we're going to leave it to default and
107:43 - go proceed and create our VPC. And so there
it has been created. And it was very fast,
107:49 - it was just instantaneous there. So we're
going to click through to that link there.
107:53 - And now we can see we have our VPC named Bayshore.
And I want you to notice that we have our
107:59 - IP V for cider range, there is no ipv6 set.
And by default, it's going to give us a route
108:06 - table and a knakal. Okay, and so we are going
to overwrite the row table because we're going
108:12 - to want to learn how to do that by ourselves.
knackles is not so important. So we might
108:17 - just glossed over that. But um, yeah, so there
you are. Now, there's just one more thing
108:22 - we have to do. Because if you look down below
here, we don't have DNS resolution, or DNS,
108:29 - or sorry, DNS hostnames is disabled by default.
And so if we launch an EC two instance, it's
108:34 - not going to get a, a DNS, DNS hostname, that's
just like a URL. So you can access that ecsu
108:42 - instance, we definitely want to turn that
on. So I'm going to drop this down to actions
108:46 - and we're going to set a host names here to
enabled okay. And so now we will get that
108:52 - and that will not cause this pain later down
the road. So now that we've created our VPC,
108:57 - we want to actually make sure the internet
can reach it. And so we're going to next learn
109:03 - about internet gateways. So we have our VPC,
but it has no way to reach the internet. And
109:09 - so we're going to need an internet gateway.
109:11 - Okay, so on the left hand side, I want you
to go to internet gateway. And we are going
109:16 - to go ahead and create a new one. Okay. And
I'm just going to call it for internet gateway,
109:22 - bay shores and people do it w e do it w doesn't
hurt. And so our internet gateway has been
109:27 - created, and so we'll just click through to
that one. And so you're gonna see that it's
109:32 - in a detached state. So internet gateways
can only be attached to a very specific VP,
109:37 - VPC, it's a one to one relationship. So for
every VPC, you're going to have an internet
109:41 - gateway. And so you can see it's attached
and there is no VPC ID. So I'm going to drop
109:46 - this down and attach the VPC and then select
Bayshore there and attach it and there you
109:51 - go. Now it's attached and we can see the ideas
associated. So we have an internet gateway,
109:57 - but that still doesn't mean that things within
our network can Reach the internet, because
110:01 - we have to add a route to our route table.
Okay, so just closing this tab here, you can
110:06 - see that there already is a route table associated
with our VPC because it did create us a default
110:12 - route table. So I'm just going to click through
to that one here to show you, okay, and you
110:17 - can see that it's our main route tables, it's
set to main, but I want you to learn how to
110:20 - create route tables. So we're going to make
one from scratch here. Okay. So we'll just
110:24 - hit Create route table here. And we're just
going to name it our main route table or RG,
110:32 - our internet road table, I don't know doesn't
matter. Okay, we'll just say RT, to shorten
110:38 - that there and we will drop down and choose
Bayshore. And then we will go ahead and create
110:42 - that route table. Okay, and so we'll just
hit close. And we will click off here. So
110:47 - we can see all of our route tables. And so
here we have our, our, our main one here for
110:52 - Bayshore. And then this is the one we created.
Okay, so if we click into this route table
110:57 - here, you can see by default, it has the full
scope of our local network here. And so I
111:04 - want to show you how to change this one to
our main. So we're just going to click on
111:09 - this one here and switch it over to main,
so set as main row table. So the main row
111:13 - table is whenever you know, just what is going
to be used by default. All right, and so we'll
111:19 - just go ahead and delete the default one here
now, because we no longer need it. Alright,
111:25 - and we will go select our new one here and
edit our routes. And we're going to add one
111:29 - for the internet gateway here. So I'm gonna
just drop down here, or sorry, I'm just gonna
111:34 - write 0.0 dot 0.0, forward slash, zero, which
means let's take take anything from anywhere
111:39 - there. And then we're going to drop down,
select internet gateway, select Bayshore,
111:42 - and hit save routes. Okay, and we'll hit close.
And so now we, we have a gateway. And we have
111:50 - a way for our subnets to reach the internet.
So there you go. So now that we have a route
111:55 - to the internet, it's time to create some
subnets. So we have some way of actually launching
112:00 - our EC two instances, somewhere. Okay, so
on the left hand side, I want you to go to
112:06 - subnets. And right away, you're going to start
to see some subnets. Here, these are the default
112:09 - ones created with you with your default VPC.
And you can see that there's exactly six of
112:15 - them. So there's exactly one for every availability
zone within each region. So the North Virginia
112:20 - has six azs. So you're going to have six,
public subnets. Okay, the reason we know these
112:27 - are public subnets. If we were to click on
one here and check the auto assign, is set
112:31 - to Yes. So if a if this is set to Yes, that
means any EC two instance launch in the subnet
112:38 - is going to get a public IP address. Hence,
it's going to be considered a public subnet.
112:43 - Okay. So if we were to switch over to Canada
Central, because I just want to make a point
112:48 - here, that if you are in a another region,
it's going to have a different amount of availability
112:54 - zones, Canada only has two, which is a bit
sad, we would love to have a third one there,
112:58 - you're going to see that we have exactly one
subnet for every availability zone. So we're
113:02 - going to switch back to North Virginia here.
And we are going to proceed to create our
113:07 - own subnets. So we're going to want to create
at least three subnets if we can. So because
113:13 - the reason why is a lot of companies, especially
enterprise companies have to run it in at
113:17 - least three availability zones for high availability.
Because if you know one goes out and you only
113:23 - have another one, what happens if two goes
out. So there's that rule of you know, always
113:27 - have at least, you know, two additional Okay,
so we're going to create three public subnets
113:33 - and one, one private subnet, we're not going
to create three private subnets, just because
113:37 - I don't want to be making subnets here all
day. But we'll just get to it here. So we're
113:41 - going to create our first subnet, I'm going
to name this Bayshore public, okay, all right,
113:47 - and we're going to select our VPC. And we're
going to just choose the US East one, eight,
113:53 - and we're going to give it a cider block of
10.0 dot 0.0 forward slash 24. Now, notice,
113:59 - this cider range is a smaller than the one
up here, I know the number is larger, but
114:03 - from the perspective of how many IP addresses
114:06 - it allocates, there's actually a fewer here,
so you are taking a slice of the pie from
114:11 - the larger range here. So just be aware, you
can set this as 16, it's always going to be
114:15 - less, less than in by less, I mean, a higher
number than 16. Okay, so we'll go ahead and
114:22 - create our first public subnet here. And we'll
just hit close. And this is not by default
114:27 - public because by default, the auto sign is
going to be set to No. So we're just going
114:31 - to go up here and modify this and set it so
that it does auto assign ipv4 and now is is
114:39 - considered a public subnet. So we're going
to go ahead and do that for our B and C here.
114:44 - So it's going to be the same thing Bayshore
public,
114:47 - be,
114:48 - okay, choose that. We'll do B, we'll do 10.0
dot 1.0 24. Okay. And we're going to go create
114:57 - that close. And we're going to that auto assign
that there. All right. And the next thing
115:07 - we're going to do is create our next subnet
here so Bayshore How boring I beige or public.
115:14 - See? And we will do that. And we'll go to
see here, it's going to be 10.0 dot 2.0, Ford
115:21 - slash 24. Okay, we'll create that one. Okay,
let it close. And we will make sure did I
115:28 - set that one? Yes, I did that I said that
one, not as of yet. And so we will modify
115:33 - that there, okay. And we will create a another
subnet here, and this is going to be a beige,
115:40 - your private a, okay. And we are going to
set that to eight here. And we're going to
115:50 - set this to 10.0 dot 3.0 24. Okay, so this
is going to be our private subnet. Alright,
115:59 - so we've created all of our subnets. So the
next thing we need to do is associate them
116:03 - with a route table, actually, we don't have
to, because by default, it's going to use
116:07 - the main, alright, so they're already automatically
associated there. But for our private one,
116:13 - we're not going to be wanting to really use
the, the, the the main route table there,
116:19 - we probably would want to create our own route
table for our private subnets there. So I'm
116:24 - just gonna create a new one here, and we're
gonna just call it private RT. Okay, I'm going
116:29 - to drop that down, choose Bayshore here. And
we're going to hit close, okay. And the idea
116:35 - is that the, you know, we don't need the subnet
to reach the internet. So it doesn't really
116:39 - make sense to be there. And then we could
set other things later on. Okay, so what I
116:45 - want you to do is just change the association
here. So we're gonna just edit the route table
116:48 - Association. And we're just going to change
that to be our private one. Okay. And so now
116:53 - our route tables are set up. So we will move
on to the next step. So our subnets are ready.
117:04 - And now we are able to launch some EC two
instances. So we can play around and learn
117:08 - some of these other networking components.
So what I want you to do is go to the top
117:13 - here and type in EC two. And we're going to
go to the EC two console. And we're going
117:19 - to go to instances on the left hand side.
And we're going to launch ourselves a couple
117:23 - of instances. So we're going to launch our
first instance, which is going to be for our
117:27 - public subnet here. So we're going to choose
t to micro, we're going to go next, and we
117:32 - are going to choose the Bayshore VPC that
we created. We're going to launch this in
117:39 - the public subnet here public a, okay, and
we're going to need a new Im role. So I'm
117:46 - just going to right click here and create
a new Im role because we're going to want
117:49 - to give it access to both SSM for sessions
manager and also, so we have access to s3.
117:56 - Okay, so just choosing EC to there. I'm going
to type in SSM, okay, SSM, there it is at
118:04 - the top, that will type in s3, we're gonna
give it full access, we're going to go next,
118:10 - we're going to go to next and we're going
to just type in my base, your EC two. Okay.
118:17 - And we're going to hit Create role. Okay,
so now we have the role that we need for our
118:24 - EC two instance, we're just going to refresh
that here, and then drop down and choose my
118:29 - beige or EC two. Okay, and we are going to
want to provide it a script here to run. So
118:38 - I already have a script pre prepared that
I will provide to you. And this is the public
118:42 - user data.sh. All this is going to do. And
if you want to just take a peek here at what
118:48 - it does, I guess they don't have it already
open here. But we will just quickly open this
118:53 - up here. all it's going to do is it's going
to install an Apache server. And we're just
118:58 - going to have a static website page here served
up. Okay, and so we're going to go ahead and
119:06 - go to storage, nothing needs to be changed
here. We're going to add, we don't need to
119:12 - add any tags. We're gonna go to security group
and we're going to create a new security group,
119:15 - I'm going to call it um, my my beige your
EC two SG, okay. And we're going to make sure
119:29 - that we have access to HTTP, because this
is a website, we're going to have to have
119:34 - Port 80 open, we're going to restrict it down
to just us.
119:39 - And we could also do that for SSH, so we might
as well do that there as well. Okay, we're
119:45 - going to go ahead and review and launch this
EC two instance and already have a key pair
119:49 - that is created. You'll just have to go ahead
and create one if you don't have one there.
119:53 - And we'll just go ahead and launch that instance
there. Okay, great. So now, we have this EC
119:59 - two instance here. Which is going to be for
our public segment. Okay. And we will go ahead
120:05 - and launch another instance. So we'll go to
Amazon Lex to here, choose teach you micro.
120:10 - And then this time we're going to choose our
private subnet. Okay, I do want to point out
120:16 - that when you have this auto assign here,
see how it's by by default disabled, because
120:21 - it's inheriting whatever the parents have
done has, whereas when we set it, the first
120:25 - one, you might have not noticed, but it was
set to enable, okay. And we are going to also
120:29 - give it the same role there, my beige or EC
two. And then this time around, we're going
120:34 - to give it the other scripts here. So I have
a private script here, I'm just going to open
120:39 - it up and show it to you. Okay, and so what
this script does, is a while it doesn't actually
120:44 - need to install Apache, so we'll just remove
that, I guess it's just old. But anyway, what
120:49 - it's going to do is it's going to reset the
password on the EC to user to chi win. Okay,
120:54 - that's a character from Star Trek Deep Space
Nine. And we're also going to enable password
120:59 - authentication. So we can SSH into this using
a password. And so that's all the script does
121:06 - here. Okay, and so we are going to go ahead
and choose that file there. And choose that.
121:13 - And we will move on to storage, storage is
totally fine. We're not going to add tags,
121:18 - secure groups, we're gonna actually create
a new security group here. It's not necessarily
121:22 - necessary, but I'm going to do anyway, so
I'm gonna say my private, private EC two,
121:32 - SD, maybe put Bayshore in there. So we just
keep these all grouped together note, therefore,
121:40 - it's only going to need SSH, we're not going
to have any access to the internet there.
121:44 - So like, there's no website or anything running
on here. And so we'll go ahead and review
121:47 - and launch. And then we're going to go launch
that instance, and choose our key pair. Okay,
121:53 - great. So now we're just going to wait for
these two instances to spin up here. And then
121:59 - we will play around with security groups and
knackles. So just had a quick coconut water.
122:09 - And now I'm back here and our instances are
running, they don't usually take that long
122:13 - to get started here. And so we probably should
have named these to make it a little bit easier.
122:18 - So we need to determine which is our public
and private. And you can see right away, this
122:22 - one has a public public DNS hostname, and
also it has its ip ip address. Okay, so this
122:31 - is how we know this is the public one. So
I'm just going to say, base your public. Okay.
122:36 - And this one here is definitely the private
one. All right. So we will say a base your
122:40 - private. Okay. So, yeah, just to iterate over
here, if we were to look, here, you can see
122:48 - we have the DNS and the public IP address.
And then for the private, there's nothing
122:53 - set. Okay, so let's go see if our website
is working here. So I'm just going to copy
122:58 - the public IP address, or we can take the
DNS when it doesn't matter. And we will paste
123:02 - this in a new tab. And here we have our working
website. So our public IP address is definitely
123:08 - working. Now, if we were to check our private
one, there is nothing there. So there's nothing
123:12 - for us to copy, we can even copy this private
one and paste it in here. So there's no way
123:16 - of accessing that website is that is running
on the private one there. And it doesn't really
123:22 - make a whole lot of sense to run your, your
website, in the private subnet there. So you
123:29 - know, just to make a very clear example of
that, now that we have these two instances,
123:34 - I guess it's a good opportunity to learn about
security groups. Okay, so we had created a
123:38 - security group. And the reason why we were
able to access this instance, publicly was
123:44 - that in our security group, we had an inbound
rule on port 80. So Port 80, is what websites
123:49 - run on. And when we're accessing through the
web browser, there's and we are allowing my
123:54 - IP here. So that's why I was allowed to access
it. So I just want to illustrate to you what
123:59 - happens if I change my IP. So at the top here,
I have a VPN, it's a, it's a service, you
124:05 - can you can buy, a lot of people use it so
that they can watch Netflix in other regions.
124:10 - I use it for this purpose not to watch Netflix
somewhere else.
124:14 - So don't get that in your mind there. But
I'm just going to turn it on. And I'm going
124:19 - to change my IP. So I get I think this is
Brazil. And so I'm going to have an IP from
124:24 - Brazil here shortly once it connects. And
so now if I were to go and access this here,
124:29 - it shouldn't work. Okay, so I'm just going
to close that tab here. And it should just
124:34 - hang Okay, so it's hanging because I'm not
using that IP. So that's how security groups
124:41 - work. Okay, and so I'm just going to turn
that off. And I think I should have the same
124:45 - one and it should resolve instantly there.
So great. So just showing you how the security
124:50 - groups work for inbound rules. Okay, for outbound
rules, that's traffic going out to the internet,
124:56 - it's almost always open like this. 0.0 dot
0.0 right. Because you'd want to be able to
125:01 - download stuff, etc. So that is pretty normal
business. Okay. So now that now that we can
125:09 - see that maybe we would like to show off how
knackles work compared to security groups
125:13 - to security groups. As you can see if we were
just to open this one up here, okay. security
125:21 - groups, by default, only can allow things
so everything is denied. And then you're always
125:28 - opening things up. So you're adding allow
rules only, you can't add an explicit deny
125:33 - rule. So we're knackles are a very useful
is that you can use it to block a very specific
125:39 - IP addresses, okay, or IP ranges, if you will.
And you cannot do that for a security group.
125:44 - Because how would you go about doing that?
So if I wanted to block access just to my
125:47 - IP address, I guess I could only allow every
other IP address in the world except for mine.
125:53 - But you can see how that would do undue burden.
So let's see if we can set our knakal to just
125:59 - block our IP address here. Okay. So security
groups are associated with the actual EC two
126:05 - instances, or? So the question is, is that,
how do we figure out the knackles and knackles
126:12 - are associated with the subnets. Okay, so
in order to block our IP address for this
126:17 - EC, two instance, we have to determine what
subnet it runs in. And so it runs in our beige
126:22 - or public a right and so now we get to find
the knakal, that's associated with it. So
126:27 - going up here to subnets, I'm going to go
to public a, and I'm going to see what knackles
126:31 - are associated with it. And so it is this
knakal here, and we have some rules that we
126:36 - can change. So let's actually try just blocking
my IP address here. And we will go just grab
126:41 - it from here. Okay. All right. And just to
note, if you look here, see houses forward
126:49 - slash 32. That is mean, that's a cider block
range of exactly one IP address. That's how
126:53 - you specify a single IP address with forward
slash 32. But I'm going to go here and just
126:59 - edit the knakal here, and we are going to
this is not the best way to do it. So I'm
127:07 - just going to open it here. Okay.
127:08 - And because I didn't get some edit options
there, I don't know why. And so we'll just
127:14 - go up to inbound rules here, I'm
127:15 - going to add a new rule. And it goes from
lowest to highest for these rules. So I'm
127:20 - just going to add a new rule here. And I'm
going to put in rule 10. Okay, and I'm going
127:28 - to block it here on the side arrange. And
I'm going to do it for Port 80. Okay, so this
127:36 - and we're going to have an explicit deny,
okay, so this should, this should not allow
127:42 - me to access that easy to instance, any any
longer. Okay, so we're going to go back to
127:47 - our instances here, we're going to grab that
IP address there and paste it in there and
127:51 - see if I still have access, and I do not okay,
so that knakal is now blocking it. So that's
127:55 - how you block individual IP addresses there.
And I'm just going to go back and now edit
128:00 - the rule here. And so we're just going to
remove this rule, and hit save. And then we're
128:05 - going to go back here and hit refresh. Okay.
And I should now have access on I do. So there
128:11 - you go. So that is security groups and knakal.
So I guess the next thing we can move on to
128:15 - is how do we actually get access to the private
subnet, okay, and the the the ways around
128:23 - that we have our private EC two instance.
And we don't have an IP address, so there's
128:29 - no direct way to gain access to it. So we
can't just easily SSH into it. So this is
128:34 - where we're going to need a bastion. Okay,
and so we're going to go ahead and go set
128:38 - one up here. So what I want you to do is I
want you to launch a new instance here, I'm
128:45 - just gonna open a new tab, just in case I
want this old tab here. And I'm just going
128:50 - to hit on launch instance here. Okay, and
so I'm going to go to the marketplace here,
128:55 - I'm gonna just type in Bastion. And so we
have some options here, there is this free
129:00 - one Bastion host, SSH, but I'm going to be
using guacamole and there is an associated
129:04 - cost here with it, they do have a trial version,
so you can get away without paying anything
129:09 - for it. So I'm just going to proceed and select
guacamole. And anytime you're using something
129:14 - from the marketplace, they generally will
have the instructions in here. So if you do
129:17 - view additional details here, we're going
to get some extra information. And then we
129:23 - would just scroll down here to usage information
such as usage instructions, and we're going
129:29 - to see there is more information. I'm just
going to open up this tab here because I've
129:33 - done this a few times. So I remember where
all this stuff is okay, and we're just going
129:37 - to hit continue here. Okay, and we're going
to start setting up this instance. So we're
129:41 - going to need a small so this one doesn't
allow you to go into micros. Okay, so there
129:46 - is an associated cost there. We're going to
configure this instance, we're going to want
129:50 - it in the same VPC as our private, okay, when
we have to launch this in a public subnet,
129:58 - so just make sure that you select the public
one Here, okay. And we're going to need to
130:04 - create a new Im role. And this is part of
guacamole these instructions here because
130:08 - you need to give it some access so that it
can auto discover instances. Okay? And so
130:14 - down here, they have the instructions here,
and they're just going to tell you to make
130:17 - an IM role, we could launch a cloudformation
template to make this, but I would rather
130:20 - just make it by hand here. So we're going
to grab this policy here, okay. And we are
130:27 - going to make a new tab and make our way over
to I am, okay. And once we're in I am here,
130:36 - we're going to have to make this policy. So
I'm going to make this policy. Okay, unless
130:39 - I already have it. Let's see if it's already
in here. New, okay, good. And I'm gonna go
130:46 - to JSON, paste that in there, review the policy,
I'm going to name it, they have a suggestion
130:50 - here, what to name it, Glock AWS, that seems
fine to me. Okay, and here, you can see it's
130:56 - gonna give us permissions to cloud watch an
STS. So we'll go ahead and create that policy.
131:00 - It says it already exists. So I already have
it. So just go ahead and create that policy.
131:04 - And I'm just going to skip the step for myself.
Okay, and we're just going to cancel there.
131:10 - So I'm just going to type walk. I don't know
why it's not showing up, says it already exists.
131:17 - Again, so yeah, there it is. So I already
have that policy. Okay, so I couldn't hit
131:22 - that last step. But you'll be able to get
through that no problem. And then once you
131:25 - have it, you're gonna have to create a new
role. So we're going to create a role here,
131:29 - and it's going to be for EC two, we're going
to go next. And we're going to want I believe
131:34 - EC to full access is that the right Oh, read
only access, okay. So we're going to want
131:40 - to give this easy to read only access. And
we're also going to want to give it that new
131:44 - GWAC role. So I'm going to type in type AWS
here. Oh, that's giving me a hard time here,
131:50 - we'll just copy and paste the whole name in
here. There it is. And so those are the two,
131:55 - two policies you need to have attached. And
then we're just going to name this something
132:00 - here. So I'm gonna just call it my Glock,
Bastion. Okay, roll here. I'm going to create
132:07 - that role. Okay, and so that role has now
been created, we're going to go back here,
132:14 - refresh the IM roles, and we're going to see
if it exists. And there it is my Glock Bastion
132:18 - role. Am I spell bashing wrong there, but
I don't think that really matters. And then
132:22 - we will go to storage. There's nothing to
do here, we'll skip tags, we'll go to security
132:28 - groups. And here you can see it comes with
some default configurations. So we're going
132:32 - to leave those alone. And then we're going
to launch this EC two instance. Okay. So now
132:38 - we're launching that it's taking a bit of
time here, but this is going to launch. And
132:43 - as soon as this is done, we're going to come
back here and actually start using this Bastion
132:50 - to get into our private instance. So our bashing
here is now already in provisioned. So let's
132:57 - go ahead and just type in Bastion, so we don't
lose that later on, we can go grab either
133:02 - the DNS or public IP, I'll just grab the DNS
one here. And we're going to get this connection,
133:07 - not private warning, that's fine, because
we're definitely not using SSL here. So just
133:12 - hit advanced, and then just click to proceed
here. Okay, and then it's might ask you to
133:16 - allow we're going to definitely say allow
for that, because that's more of the advanced
133:20 - functionality, guacamole there, which we might
touch in. At the end of this here, we're going
133:24 - to need the username and password. So it has
a default, so we have block admin here, okay.
133:29 - And then the password is going to be the name
of the instance ID. All right, and this is
133:34 - all in the instructions here. I'm just speaking
you through it. And then we're going to hit
133:38 - login here. And so now it has auto discovered
the instances which are in the VPC that is
133:45 - launched. And so here, we have Bayshore, private.
So let's go ahead and try to connect to it.
133:50 - Okay. So as soon as I click, it's going to
make this shell here. And so we'll go attempt
133:55 - and login now. So our user is easy to user.
And I believe our password is KI wi n Chi
134:03 - win. And we are in our instance, so there
you go. That's how we gain access to our private
134:10 - instance here. Just before we start doing
some other things within this private easy
134:16 - to I just want to touch on some of the functionality
of Bastion here, or sorry, guacamole, and
134:20 - so why you might actually want to use the
bastion. So it does, it is a hardened instance,
134:26 - it does allow you to authenticate via multiple
methods. So you can enable multi factor authentication
134:32 - to use this. It also has the ability to do
screen recordings, so you can really be sure
134:38 - what people are up to, okay, and then it just
has built in audit logs and etc, etc. So,
134:43 - there's definitely some good reasons to use
a bastion, but we can also use a sessions
134:47 - manager which does a lot of this for us with
the exception of screen recording within the
134:53 - within AWS. But anyway, so now that we're
in our instance, let's go play around here
134:57 - and see what we can do. So now that we are
in this private EC two instance, I just want
135:08 - to show you that it doesn't have any internet
access. So if I was to ping something like
135:11 - Google, right, okay, and I'm trying to get
information here to see how it's hanging,
135:16 - and we're not getting a ping back, that's
because there is no route to the internet.
135:21 - And so the way we're going to get a route
to the internet is by creating a NAT instance
135:26 - or a NAT gateway. Generally, you want to use
a NAT gateway, there are cases use Nat instances.
135:31 - So if you were trying to save money, you can
definitely save money by having to manage
135:35 - a NAT instance by herself. But we're gonna
learn how to do Nat gateway, because that's
135:39 - the way to this wants you to go. And so back
in our console, here, we are in EC two instances,
135:46 - where we're going to have to switch over to
a V PC, okay, because that's where the NAT
135:51 - gateway is. So on the left hand side, we can
scroll down and we are looking under VPC,
136:00 - we have Nat gateways. And so we're going to
launch ourselves a NAT gateway, that gateways
136:04 - do cost money. So they're not terribly expensive.
But you know, at the end of this, we'll tear
136:10 - it down, okay. And so, the idea is that we
need to launch this Nat gateway in a public
136:16 - VPC or sorry, public subnet, and so we're
gonna have to look, here, I'm gonna watch
136:20 - it in the beige or public a doesn't matter
which one just has to be one of the public
136:24 - ones. And we can also create an elastic IP
here. I don't know if it actually is required
136:30 - to sign a pipe network, I don't know if it
really matters. Um, but we'll try to go ahead
136:36 - and create this here without any IP, no, it's
required. So we'll just hit Create elastic
136:41 - IP there. And that's just a static IP address.
So it's never changing. Okay. And so now that
136:46 - we have that, as associated with our Nat gateway,
we'll go ahead and create that. And it looks
136:51 - like it's been created. So once your Nat gateway
is created, the next thing we have to do is
136:54 - edit your route table. So there actually is
a way for that VPC to our sorry, that private
137:00 - instance to access the internet. Okay, so
let's go ahead and edit that road table.
137:06 - And so we created a private road table specifically
for our private EC two. And so here, we're
137:12 - going to add it the routes, okay. And we're
going to add a route for that private or to
137:18 - that Nat gateway. Okay. So um, we're just
going to type in 0.0 dot 0.0, Ford slash zero.
137:27 - And we are then just going to go ahead, yep.
And then we're going to go ahead and choose
137:31 - our Nat gateway. And we're going to select
that there, and we're going to save that route.
137:37 - Okay, so now our Nat gateway is configured.
And so there should be a way for our instance
137:43 - to get to the internet. So let's go back and
do a ping. And back over here, in our private
137:48 - EC two instance, we're just going to go ahead
and ping Google here. Okay, and we're going
137:53 - to see if we get some pings back, and we do
so there you go. That's all we had to do to
137:58 - access the internet. All right. So why would
our private easy to instance need to reach
138:03 - the internet. So we have one one inbound traffic,
but we definitely want outbound because we
138:08 - would probably want to update packages on
our EC two instance. So if we did a sudo,
138:14 - yum, update, okay, we wouldn't be able to
do this without a outbound connection. All
138:18 - right. So it's a way of like getting access
to the internet, only for the things that
138:22 - we need for outbound connections. Okay. So
now that we've seen how we can set an outbound
138:32 - connection to the Internet, let's talk about
how we could access other AWS services via
138:37 - our private EC two instance here. So s3 would
be a very common one to utilize. So I'm just
138:42 - going to go over to s3 here, I'm just going
to type in s3, and open this in a new tab,
138:46 - I'm going to try to actually access some s3
files here. Okay. And so I should already
138:52 - have a bucket in here called exam pro 000.
And I do have some images already in here
138:58 - that we should be able to access. And we did
get that Iam role permissions to access that
139:03 - stuff there. So the ACL, the ACL, I should
be already pre installed here. And so we'll
139:09 - just type in AWS s3, and it should be, if
we wanted to copy a file locally, we'll type
139:16 - in CP. And we're going to need to actually
just do LS, okay, so we'll do LS here, okay.
139:22 - I don't think we have to go as advanced as
copying and doing other stuff here. But you
139:27 - can definitely see that we have a way of accessing
s3 via the COI. So what would happen if we
139:34 - removed that Nat gateway, would we still be
able to access s3? So let's go find out. All
139:39 - right. I think you know the answer to this,
but let's just do it. And then I'll show you
139:43 - a way that you can still access s3 without
a NAT gateway. All right. So we're going to
139:49 - go ahead here and just delete this Nat gateway,
it's not like you can just turn them off so
139:52 - you have to delete them. And we'll just wait
till that finishes deleting here. So our Nat
139:58 - gateway has deleted after a few months. here
just hit the refresh button here just in case
140:02 - because sometimes it will say it's deleting
when it's already done. And you don't want
140:05 - to be waiting around for nothing. So
140:07 - let's go back to our EC two instance here.
We'll just clear the screen here. And now
140:12 - the question is, will we be able to access
AWS s3 via the via the CLR here, okay, and
140:20 - so I hit Enter, and I'm waiting, waiting,
waiting. And it's just not going to complete
140:24 - because it no longer has any way to access
s3. So the way it works when using the COI
140:30 - through UC Davis is it's going to go out out
to the internet out of the universe network,
140:34 - and then come back into nativas network to
then access s3. And so since there is no outbound
140:41 - way of connecting to the internet, there's
no way we're going to be able to connect to
140:45 - s3. Okay, so it seems a little bit silly,
because you'd say, Well, why wouldn't you
140:49 - just keep the traffic within the network because
we're already on an easy to within AWS network,
140:54 - and s3 is with Ava's network. And so that
brings us to endpoints, which is actually
141:00 - how we can create a like our own little private
tunnel within the US network, so that we don't
141:06 - have to leave up to the internet. So let's
go ahead and create an endpoint and see if
141:10 - we can connect to s3 without having outbound
Connect. So we're going to proceed to create
141:14 - our VPC endpoints on the left hand side, you're
going to choose endpoints, okay. And we're
141:20 - going to create ourselves a new endpoint.
And this is where we're going to select it
141:25 - for the service that we want to use. So this
is going to be for s3. But just before we
141:29 - do that, I want you to select the VPC that
we want this for down below. And then we're
141:34 - going to need this for s3. So we'll just scroll
down here and choose s3. Okay, and we're going
141:39 - to get a blotch options here. Okay. And so
we're going to need to configure our route
141:46 - tree, our route table. So we have that connection
there. And it's going to ask what route table
141:51 - Do you want put it in, and we're going to
want to put it in our private one, because
141:54 - that's where our private easy to instance
is. And then down below, we will have a policy
141:59 - here. And so this is going to be great. So
we will just leave that as is, and we're going
142:04 - to hit Create endpoint. Okay, so we're gonna
go back and hit close there. And it looks
142:10 - like our endpoint is available immediately
there. And so now we're going to go find out
142:15 - if we actually have access to s3. So back
over here we are in our private EC two instance.
142:22 - And I'm just going to hit up and see if we
now have access and look at that. So we've
142:26 - created our own private connection to s3 without
leaving the egress network.
142:36 - Alright, so we had a fun time playing around
with our private EC two instance there. And
142:43 - so we're pretty much wrapped up here for stuff.
I mean, there's other things here, but you
142:47 - know, at the associate level, it's, there's
not much reason to get into all these other
142:52 - things here. But I do want to show you one
more thing for VP C's, which are VPC flow
142:58 - logs, okay, and so I want you to go over to
your VPC here, okay, and then I just want
143:04 - you to go up and I want you to create a flow
log, so flow logs will track all the, the
143:10 - traffic that is going through through your
VPC. Okay, and so it's just nice to know how
143:16 - to create that. So we can have it to accept
reject, or all I'm going to set it to all
143:20 - and it can either be delivered to cloudwatch
logs, or s3 Cloud watch is a very good destination
143:24 - for that. In order to deliver that, we're
going to need a destination log group on I
143:31 - don't have one. So in order to send this to
a log group, we're going to have to go to
143:37 - cloud watch, okay. We'll just open this up
in a new tab here. Okay. And then once we're
143:44 - here in cloud watch, we're going to create
ourselves a new cloud watch log, alright.
143:50 - And we're just going to say actions create
log group, and we'll just call this Bayshore,
143:55 - your VPC flow logs or VPC logs or flow logs,
okay. And we will hit Create there. And now
144:03 - if we go back to here and hit refresh, we
may have that destination now available to
144:09 - us. There it is. Okay, we might need an IM
role associated with this, to have permissions
144:14 - to publish to cloud watch logs. So we're definitely
going to need permissions for that. Okay.
144:22 - And I'll just pop back here with those credentials
here in two seconds. So I just wanted to collect
144:27 - a little bit of flow log data, so I could
show it off to you to see what it looks like.
144:31 - And so you know, under our VPC, we can see
that we have flow logs enabled, we had just
144:35 - created that a log there a moment ago. And
just to get some data, I went to my EC two
144:40 - instances, and we had that public one running
right. And so I just took that IP address.
144:44 - And I just started refreshing the page. I
don't know if we actually looked at the actual
144:48 - webpage I had here earlier on, but here it
is. And so I just hit enter, enter, enter
144:53 - here a few times. And then we can go to our
cloud watch here and look for the log here
144:58 - and so we should have some streams, okay.
And so if we just open that up, we can kind
145:04 - of see what we have here. And so we have some
raw data. So I'll just change it to text.
145:09 - And so here we can see the IP address of the
source and the destination and additional
145:14 - information, okay, and that we got a 200 response.
Okay, so there you go. I'm just wanting to
145:21 - give you a little quick peek into there.
145:29 - So now we're done the VPC section, let's clean
up whatever we created here, so we're not
145:33 - incurring any costs. So we're gonna make our
way over two EC two instances. And you can
145:38 - easily filter out the instances which are
in that. That VPC here by going to VPC ID
145:44 - here, and then just selecting the VPC. So
these are the three instances that are running,
145:48 - and I'm just going to terminate them all.
Because you know, we don't want to spend up
145:52 - our free credits or incur cost because of
that bash in there. So just hit terminate
145:57 - there, and those things are going to now shut
down. We also have that VPC endpoint still
146:03 - running, just double check to make sure your
Nat gateway isn't still there. So under the
146:08 - back in the VPC section here, just make sure
that you had so and there we have our gateway
146:15 - endpoint there for s3. So we'll just go ahead
and delete that. I don't believe it cost us
146:19 - any money, but it doesn't hurt to get that
out of the way there. We'll also check our
146:23 - elastic IPS. Now it did create a e IP when
we created that gateway, IPS that are not
146:28 - being utilized cost us money. So we'll go
ahead and release that tip. Okay, we'll double
146:34 - check our gateway. So under Nat gateways,
making sure nothing is running under there,
146:40 - and we had deleted previously, so we're good,
we can go ahead and delete these other things
146:44 - not so important. But we can go ahead and
attempt to delete them, it might throw an
146:49 - error here. So we'll see what it says. Nope,
they're all deletes. That's great. So those
146:55 - are deleted now. And then we have our route
table. So we can delete those two route tables,
147:00 - okay. And we can get rid of our internet gateway,
so we can find that internet
147:06 - gateway.
147:08 - There it is, okay, we will detach it, okay.
And then we will go ahead and delete that
147:15 - internet gateway, okay. And we'll go attempt
to delete our, our actual VPC now, we'll see
147:23 - if there's any dependencies here. So if we
haven't deleted all the things that it wants,
147:27 - from here, it's going to complain. So there
might be some security groups here, but we'll
147:30 - find out in a second. Oh, just delete the
forest. Great. So it just deleted it there.
147:34 - So we're all cleaned up. So there you go.
147:40 - Hey, this is Andrew Brown from exam Pro. And
we are looking at identity access management
147:47 - Iam, which manages access of AWS users and
resources. So now it's time to look at I am
147:57 - core components. And we have these installed
identities. And those are going to be users
148:02 - groups and roles. Let's go through those.
So a user is an end user who can log into
148:06 - the console or interact with AWS resources
programmatically, then you have groups, and
148:11 - that is when you take a bunch of users and
you put them into a logical grouping. So they
148:15 - have shared permissions. That could be administrators,
developers, auditors, whatever you want to
148:19 - call that, then you have roles and roles,
have policies associated with them. That's
148:24 - what holds the permissions. And then you can
take a role and assign it to users or groups.
148:29 - And then down below, you have policies. And
this is a JSON document, which defines the
148:34 - rules in which permissions are loud. And so
those are the core components. But we'll get
148:39 - more in detail to all these things. Next.
So now that we know the core components aren't,
148:45 - let's talk about how we can mix and match
them. Starting at the top here, we have a
148:48 - bunch of users in a user group. And if we
want to on mass, apply permissions, all we
148:54 - have to do is create a role with the policies
attached to that role. And then once we attach
148:59 - that role to that group, all these users have
that same permission great for administrators,
149:03 - auditors, or developers. And this is generally
the way you want to use Iam when assigning
149:12 - roles to users. You can also assign a role
directly to a user. And then there's also
149:19 - a way of assigning a policy, which is called
inline policy directly to a user. Okay, so
149:25 - why would you do this? Well, maybe you have
exactly one action, you want to attach this
149:30 - user and you want to do it for a temporary
amount of time. You don't want to create a
149:34 - managed role because it's never it's never
going to be reused for anybody else. There
149:38 - are use cases for that. But generally, you
always want to stick with the top level here.
149:42 - A role can have multiple policies attached
to it, okay. And also a role can be attached
149:48 - to certain AWS resources. All right. Now,
there are cases where resources actually have
149:55 - inline policies directly attached to them,
but there are cases where You have roles attached
150:02 - to or somehow associated to resources, all
right. But generally, this is the mix and
150:07 - match of it. If you were taking the eight
of a security certification, then this stuff
150:14 - in detail really matters. But for the associate
and the pro level, you just need to conceptually
150:19 - know what you can and cannot do. All right.
So in I am you have different types of policies,
150:29 - the first being managed policies, these are
ones that are created by AWS out of convenience
150:34 - for you for the most common permissions you
may need. So over here, we'd have Amazon easy
150:39 - to full access, you can tell that it's a managed
policy, because it says it's managed by AWS,
150:44 - and an even further indicator is this orange
box, okay? Then you have custom customer managed
150:50 - policies, these are policies created by you,
the customer, they are edible, whereas in
150:54 - the Manage policies, they are read only. They're
marked as customer managed, you don't have
150:58 - that orange box. And then last are inline
policies. So inline policies, you don't manage
151:04 - them, because they're like they're one and
done. They're intended to be attached directly
151:08 - to a user or directly to a, a resource. And
they're, and they're not managed, so you can't
151:16 - apply them to more than one identity or resource.
Okay, so those are your three types of roles.
151:27 - So it's now time to actually look at a policy
here. And so we're just going to walk through
151:32 - all the sections so we can fully understand
how these things are created. And the first
151:36 - thing is the version and the version is the
policy language version. If this changes,
151:40 - then that means all the rules here could change.
So this doesn't change very often, you can
151:46 - see the last time was 2012. So it's going
to be years until they change it, if they
151:50 - did make changes, it probably would be minor,
okay, then you have the statement. And so
151:55 - the statement is just a container for the
other policy elements. So you can have a single
151:59 - one here. So here I have an array, so we have
multiples. But if you didn't want to have
152:04 - multiples, you just get rid of the the square
brackets there, you could have a single policy
152:10 - element there. Now going into the actual policy
element, the first thing we have is Cid and
152:14 - this is optional. It's just a way of labeling
your statements. So Cid probably stands for
152:19 - statement identifier, you know, again, it's
optional, then you have the effect, the effect
152:25 - can be either allow or deny. And that's going
to set the conditions or the the access for
152:31 - the rest of the policy. The next thing is
we have the action. So actions can be individualized,
152:38 - right. So here we have I am and we have an
individual one, or we can use asterisk to
152:43 - select everything under s3. And these are
the actual actions the policy will allow or
152:49 - deny. And so you can see we have a deny policy,
and we're denying access all to s3 for a very
152:56 - specific user here, which gets us into the
principal. And the principal is kind of a
153:01 - conditional field as well. And what you can
do is you can specify an account a user role
153:05 - or federated user, to which you would like
to allow or deny access. So here we're really
153:10 - saying, hey, Barkley, you're not allowed to
use s3, okay, then you have the resource,
153:15 - that's the actual thing. That is we're allowing
or denying access to so in this case, it's
153:21 - a very specific s3 bucket. And the last thing
is condition. And so condition is going to
153:26 - vary based on the based on the resource, but
here we have one, and it does something, but
153:35 - I'm just showing you that there is a condition
in here. So there you go, that is the makeup
153:39 - of a policy, if you can master master these
things, it's going to make your life a whole
153:45 - lot easier. But you know, just learn what
you need to learn.
153:54 - So you can also set up password policies for
your users. So you can set like the minimum
153:58 - password length or the rules to what makes
up a good password, you can also rotate out
154:04 - passwords, so that is an option you have as
well. So it will expire after x days. And
154:08 - then a user then must reset that password.
So just be aware that you have the ability
154:12 - to password.
154:19 - Let's take a look at access keys. Because
this is one of the ways you can interact with
154:23 - AWS programmatically either through the ad
vcli or the SDK. So when you create a user
154:28 - and you say it's allowed to have programmatic
access, it's going to then create an access
154:33 - key for you, which is an ID and a secret access
key. One thing to note is that you just can
154:39 - only have up to two access keys within their
accounts down below, you can see that we have
154:45 - one, as soon as we add a second one, that
grey button for create access key will vanish.
154:49 - And if we want more we would either have to
we'd have to remove keys, okay. But you know,
154:54 - just be aware that that's what access keys
are and you can make them inactive and you're
154:58 - only allowed to have Let's quickly talk about
MFA. So MFA can be turned on per user. But
155:08 - there is a caveat to it where the user has
to be the one that turns it on. Because when
155:14 - you turn it on, you have to connect it to
a device and your administrator is not going
155:17 - to have the device notes on the user to do
so there is no option for the administrator
155:22 - to go in and say, Hey, you have to use MFA.
So it cannot be enforced directly from an
155:29 - administrator or root account. But what the
minister can do if if if they want is they
155:35 - can restrict access to resources only to people
that are using MFA, so you can't make the
155:40 - user account itself have MFA. But you can
definitely restrict access to API calls and
155:46 - things like that. And this is Andrew Brown
from exam Pro, and we are going to do the
155:55 - I am follow along. So let's make our way over
to the IM console. So just go up to services
156:00 - here and type in Im, and we will get to learning
this a right right away. So here I am on the
156:09 - IM dashboard, and we have a couple things
that a device wants us to do. It wants us
156:14 - to set MFA on our root account. It also wants
us to apply an IM password policy, so that
156:21 - our passwords stay very secure. So let's take
what they're saying in consideration and go
156:26 - through this. Now I am logged in as the root
user. So we can go ahead and set MFA. So what
156:32 - I want you to do is drop this down as your
root user and we'll go manage MFA. And we
156:37 - will get to this here. So this is just a general
disclaimer here to help you get started here.
156:48 - I don't ever want to see this again. So I'm
just going to hide it. And we're going to
156:51 - go to MFA here and we're going to activate
MFA. So for MFA, we have a few options available.
156:57 - We have a virtual MFA, this is what you're
probably most likely going to use where you
157:00 - can use a mobile device or computer, then
you can use a u2f security key. So this is
157:08 - like a UB key. And I actually have an OB key,
but we're not going to use it for this, but
157:13 - it's a physical device, which holds the credentials,
okay, so you can take this key around with
157:17 - you. And then there are other hardware mechanisms.
Okay, so but we're going to stick with virtual
157:23 - MFA here. Okay, so we'll hit Continue. And
what it's going to do is it's going to you
157:29 - need to install a compatible app on your mobile
phone. So if we take a look here, I bet you
157:34 - authenticator is one of them. Okay. So if
you just scroll down here, we have a few different
157:40 - kinds. I'm just looking for the virtual ones.
Yeah. So for Android or iPhone, we have Google
157:46 - Authenticator or authy, two factor authentication.
So you're going to have to go install authenticator
157:52 - on your phone. And then when you are ready
to do so you're going to have to show this
157:56 - QR code. So I'm just going to click that and
show this to you here. And then you need to
158:00 - pull out your phone. I know you can't see
me doing this, but I'm doing it right now.
158:05 - Okay. And I'm not too worried that you're
seeing this because I'm going to change this
158:10 - two factor authentication out here. So if
you decide that you want to also add this
158:15 - to your phone, you're not going to get too
far. Okay, so I'm just trying to get my authenticator
158:20 - app out here, and I'm gonna hit plus and the
thing and I can scan the barcode, okay. And
158:28 - so I'm just going to put my camera over it
here. Okay, great. And so is is saved the
158:35 - secret. All right, and so it's been added
to Google Authenticator. Now, now that I have
158:40 - it in my application, I need to enter in to
two consecutive MFA codes. Okay, so this is
158:46 - a little bit confusing. It took me a while
to figure this out. The first time I was using
158:49 - AWS, the idea is that you need to set the
first one. So the first one I see is 089265.
158:56 - Okay, and so I'm just going to wait for the
next one to expire, okay, so there's a little
159:02 - circle that's going around. And I'm just waiting
for that to complete to put in a second one,
159:07 - which just takes a little bit of time here.
159:11 - Still going here. Great. And so I have new
numbers. So the numbers are 369626. Okay,
159:25 - so it's not the same number, but it's two
consecutive numbers, and we'll hit assign
159:30 - MFA. And now MFA has been set on my phone.
So now when I go and log in, it's going to
159:37 - ask me to provide additional code. Okay, and
so now my root account is protected. So we're
159:43 - gonna go back to our dashboard, and we're
gonna move on to password policies. Okay.
159:48 - So let's take the recommendation down here
and manage our password policy. Okay. And
159:53 - we are going to set a password policy. So
password policy allows us to enforce some
159:58 - rules that we want to have on Your users.
And so to make passwords a lot stronger, so
160:03 - we can say it should require at least one
upper letter, one lowercase letter, or at
160:09 - least one number, a non non alphanumeric character,
enable the password expiration. So after 90
160:17 - days, they're going to have to change the
password, you can have password expiration
160:21 - requires the administrator reset, so you can't
just reset it, the admin will do it for you
160:27 - allow users to change their own password is
something you could set as well. And then
160:30 - you could say prevent password reuse. So for
the next five passwords, you can't reuse the
160:34 - same one. Okay? So and I would probably put
this a big high numbers, so that is a very
160:39 - high chance they won't use the same one. Okay,
so, um, yeah, there we go. We'll just hit
160:44 - Save Changes. And now we have a password policy
in place. Okay. And so that's, that's how
160:52 - that will be. So to make it easier for users
to log into the Iam console, you can provide
160:57 - a customized sign in link here. And so here,
it has the account ID, or I think that's the
161:04 - account ID but we want something nicer here.
So we can change it to whatever you want.
161:08 - So I can call it Deep Space Nine. Okay. And
so now what we have is if I spelt that, right,
161:15 - I think so yeah. So now that we have a more
convenient link that we can use to login with,
161:20 - okay, so I'm just going to copy that for later,
because we're going to use it to login. I
161:24 - mean, obviously, you can name it, whatever
you want. And I believe that these are just
161:29 - like, I'm like picking like your Yahoo or
your your Gmail email, you have to be unique.
161:35 - Okay, so you're not gonna be at least Deep
Space Nine, as long as I have to use I believe.
161:40 - But yeah, okay, so maybe we'll move on to
actually creating a. So here I am under the
161:46 - Users tab, and I am, and we already have an
existing user that I created for myself, when
161:50 - I first set up this account, we're going to
create a new user so we can learn this process.
161:54 - So we can fill the name here, Harry Kim, which
is the character from Star Trek Voyager, you
162:00 - can create multiple users in one go here,
but I'm just gonna make one. Okay, I'm going
162:04 - to give him programmatic access and also access
to the console. So you can log in. And I'm
162:10 - going to have an auto generated password here,
so I don't have to worry about it. And you
162:13 - can see that it will require them to reset
their password and they first sign in. So
162:18 - going on to permissions, we need to usually
put our users within a group, we don't have
162:23 - to, but it's highly recommended. And here
I have one called admin for edit, which has
162:27 - add administrator access, I'm going to create
a new group here, and I'm going to call it
162:32 - developers okay. And I'm going to give them
power access, okay, so it's not full access,
162:38 - but it gives them quite a bit of control within
the system. Okay. And I'm just going to create
162:43 - that group there. And so now I have a new
group. And I'm going to add Harry to that
162:49 - group there. And we will proceed to the next
step here. So we have tags, ignore that we're
162:55 - going to review and we're going to create
Harry Kim the user. Okay. And so what it's
163:00 - done here is it's also created a secret access
key and a password. Okay, so if Harry wants
163:07 - that programmatic access, he can use these
and we can send the an email with the, with
163:12 - this information along to him. Okay.
163:14 - And, yeah, we'll just close that there. Okay.
And then we will just poke around here in
163:18 - Harry Kim for a little bit.
163:20 - So just before we jump into Harry Kim, here,
you can see that he has never used his access
163:24 - key. He, the last time his password was used
was today, which was set today. And there
163:29 - is no activity and he does not have MFA. So
if we go into Harry Kim, we can look around
163:34 - here. And we can see that he has policies
applied to him from a group. And you can also
163:40 - individually attach permissions to him. So
we have the ability to give them permissions
163:44 - via group. Or we can copy permissions from
existing user or we can attach policies directly
163:49 - directly to them. So if we wanted to give
them s3, full access, we could do so here.
163:54 - Okay. And then we can just apply those permissions
there. And so now he has those permissions.
164:01 - We also have the ability to add inline policies.
Okay, so in here, we can add whatever we want.
164:06 - And so we could use the visual editor here
and just add an inline policy. Okay, so I'm
164:11 - just trying to think of something we could
give him access to some ABC to okay, but he
164:14 - already has access to it because he's a power
user, but we're just going through the motions
164:17 - of this here. So I'm gonna select EC two,
and we're going to give him just a list access,
164:24 - okay. And we're going to say review policy,
okay. And so we have full, full access there.
164:31 - And we can name the policy here so we can
say, full Harry, okay. Full hair, you see,
164:39 - too. And we'll go ahead and create that policy,
their maximum policy size exceeds for Harry
164:46 - Kim, I guess it's just that it has a lot of
stuff in there. So I'm gonna go previous,
164:51 - okay, and I guess it's just a very large policy.
So I'm just going to pare that down there.
164:57 - Okay. Again, this is just for show So it doesn't
really matter what we select here. And then
165:02 - we'll go ahead and review that policy, and
then we will create that policy. Okay. And
165:07 - so here we have an inline policy, or a policy
directly attached. And then we have a manage
165:12 - policy, okay. And then we have a policy that
comes from a group. Alright. So that's policies,
165:20 - we can see what group he belongs to here and
add them to additional groups, tags or tags,
165:25 - we can see his security credentials here.
So we could manage whether we want to change
165:30 - whether he has full access or not retroactively.
And we can fiddle with his password or reset
165:36 - it for him here. And we can manage MFA. So
we can set MFA for this user. Normally, you
165:44 - want the user to do it, by themselves, because
if you had to set MFA for them, as administrator,
165:49 - they'd have to give me their phone right to
set this up. But I guess if they had, if you
165:53 - had a yubikey, that set up the yubikey for
them, and they give them the UB key. And then
165:59 - we have the access keys. So you can have up
to two access keys within your account here.
166:03 - Okay, so I can go ahead and create a another
one, it's a good habit to actually create
166:07 - both of them, because for security purposes,
if you take the ADA security certification,
166:13 - one way of compromising account is always
taking up that extra slot, you can also make
166:17 - these inactive. Okay? So if this becomes an
active, you can set them all right. But see,
166:22 - we still can't create any additional keys,
we have to hit the x's here. And so then we
166:27 - can create more access keys. Okay, if we were
using code commit, we could upload their SSH
166:34 - key here. And so same thing, we can generate
a credentials for code commit. And then there's
166:41 - access advisor. This is gives you general
advice of like what access they have, I think
166:47 - Suzy can scroll down here and see what do
they actually have access to? And when did
166:51 - they last access something? Okay. And then
there's the Arn to Harry Kim. So it's something
166:56 - that we might want to utilize there. So we
got the full gambit of this here. Okay. And
167:01 - so I'm just going to go ahead and delete Harry,
because we're pretty much done here. Okay.
167:07 - Great. And so there we are. So that was the
run through with users. So just to wrap up
167:13 - this section, we're just going to cover a
rules and policies here. So first, we'll go
167:17 - into policies. And here we have a big list
of policies here that are managed by AWS,
167:22 - they say they're managed over here, and you
can definitely tell because they're camelcase.
167:26 - And they also have this nice little orange
box, okay. And so these are policies, which
167:31 - you cannot edit, they're read only, but they're
a quick and fast way for you to start giving
167:36 - access to your users. So if we were just to
take a look at one of them, like the EC 214,
167:41 - or maybe just read only access, we can click
into them. And we can see kind of the summary
167:46 - of what we get access to. But if we want to
actually see the real policy here, we can
167:51 - see it in full. Alright. And so we do have
some additional things here to see actually
167:57 - who is using this policy. So we can see that
we have a roll named
168:03 - that I created here for a different follow
along and it's utilizing this policy right
168:07 - now. We also have policy versions. So a policy
can have revisions over time, and to have
168:13 - up to five of them, okay, so if you ever need
to roll back a policy or just see how things
168:17 - have changed, we can see that here. And we
also have the access advisor, which tells
168:21 - us who is utilizing this. So again, for Amazon
ECS, we're seeing that role being utilized
168:27 - for this custom role that I've created. Okay,
so let's just actually copy the the Jason
168:33 - here, so we can actually go try and make our
own policy. Okay, because we did create a
168:37 - policy for Harry Kamba, it would be nice to
actually create one with the JSON here. So
168:41 - we'll go back to the policy here. And we'll
create a new policy, we'll go to the JSON
168:45 - editor here, and we will paste that in. And
we do cover this obviously, in the lecture
168:49 - content, but you know, you have to specify
the version, then you need a statement. And
168:54 - the statement has multiple, multiple actions
within here that you need to define. Okay.
169:02 - And so here we have one that has an allow,
allow effect, and it is for the action, easy
169:07 - to describe. And it's for all possible resources.
Okay, so we're going to go ahead and create
169:12 - that there. And we're just going to name this
as my read, only easy to access, okay. We're
169:20 - just gonna go ahead and create that policy.
Okay. And so we have that policy there. I'm
169:27 - just going to search it there quickly. And
you can see that this is customer manage,
169:30 - because it doesn't have the orange box. And
it says that it's customer managed. All right.
169:35 - So let's just go ahead and go ahead and create
a role now. So we can go ahead and create
169:39 - a role. And so generally, you want to choose
the role, who this role is for. We're gonna
169:45 - say this is for EC two, okay, but you could
also set it for another ABC account. This
169:50 - is for creating cross rules. Then you have
web identity and SAML. We're gonna stick with
169:55 - services here and go to EC two and now we
have the option You need to choose their policies
170:00 - and we can create or sorry, choose multiple
policies here. So I'm going to do my read,
170:04 - only easy to hear, okay? But we could also
select on them, and I guess three, okay. And
170:11 - I'm just going to skip tags because they don't
care. And we're going to go next review, I'm
170:14 - going to say my role, alright, and shows the
policies that were attached there. And now
170:19 - we can create that role. Okay. And so we now
have that role. And so we can now attach it,
170:24 - we can attach that role to resource such such
as when we launched got easy to instance,
170:30 - we could assign it that way, or, you know,
affiliate with a user but yeah, there you
170:39 - go.
170:41 - We're onto the IM cheat sheets. Let's jump
into it. So identity access management is
170:45 - used to manage access to users and resources
I am is a universal system, so it's applied
170:50 - to all regions at the same time, I am is also
a free service. A root account is the account
170:56 - initially created when AWS is set up, and
so it has full administrator access. New Iam
171:01 - accounts have no permissions by default until
granted, new users get assigned an access
171:06 - key ID and secret when first created when
you give them programmatic access. Access
171:11 - keys are are only used for the COI and SDK,
they cannot access the console. Access keys
171:17 - are only shown once when created, if lost,
they must be deleted and recreated again,
171:21 - always set up MFA for your root accounts.
Users must enable MFA on their own administrators
171:26 - cannot turn it on for each user, I am allows
you, you to Set password policies to set minimum
171:34 - password requirements or rotate passwords.
Then you have Iam identities, such as users,
171:39 - groups and roles and we'll talk about them
now. So we have users, those are the end users
171:43 - who log into the console or interact with
AWS resources programmatically, then you have
171:48 - groups. So that is a logical group of users
that share all the same permission levels
171:52 - of that group. So think administrators, developers,
auditors, then you have roles, which associates
171:58 - permissions to a role, and then that role
is then assigned to users or groups. Then
172:03 - you have policies. So that is a JSON document,
which grants permissions for specific users
172:08 - groups, roles to access services, policies
are generally always attached to Iam identities,
172:13 - you have some variety of policies, you have
managed policies, which are created by AWS,
172:18 - that cannot be edited, then you have customer
managed policies, those are policies created
172:22 - by you that are editable and you have inline
policies which are directly attached to the
172:27 - user. So there you go, that is I am. Hey,
this is Andrew Brown from exam Pro. And we
172:36 - are looking at Amazon cognito, which is a
decentralized way of managing authentication.
172:41 - So think sign up sign in integration for your
apps, social identity providers, like connecting
172:46 - with Facebook, or Google. So Amazon cognito
actually does multiple different things. And
172:53 - we are going to look at three things in specific,
we're going to look at cognito user pools,
172:57 - which is a user directory to authenticate
against identity providers, we're going to
173:01 - look at cognito identity pools, which provides
temporary credentials for your users to access
173:06 - native services. And we're gonna look at cognito
sync, which syncs users data and preferences
173:10 - across all devices. So let's get to
173:17 - so to fully understand Amazon cognito, we
have to understand the concepts of web identity
173:21 - Federation and identity providers. So let's
go through these definitions. So for web identity
173:26 - Federation, it's to exchange the identity
and security information between an identity
173:31 - provider and an application. So now looking
at identity provider, it's a trusted provider
173:38 - for your user identity that lets you authenticate
to access other services. So an identity provider
173:43 - could be Facebook, Amazon, Google, Twitter,
GitHub, LinkedIn, you commonly see this on
173:49 - websites where it allows you to log in with
a Twitter or GitHub account, that is an identity
173:54 - provider. So that would be Twitter or GitHub.
And they're generally powered by different
174:00 - protocols. So whenever you're doing this with
social social accounts, it's going to be with
174:04 - OAuth. And so that can be powered by open
ID Connect, that's pretty much the standard
174:08 - now, if there are other identity providers,
so if you needed a single sign on solution,
174:14 - SAML is the most common one. Alright. So the
first thing we're looking at is cognito. User
174:23 - pools, which is the most common use case for
cognito. And that is just a directory of your
174:29 - users, which is decentralized here. And it's
going to handle actions such as signup sign
174:34 - in account recoveries, that would be like
resetting a password account confirmation,
174:39 - that would be like confirming your email after
sign up. And it has the ability to connect
174:44 - to identity providers. So it does have its
own like email and password form that it can
174:48 - take, but it can also leverage. Maybe if you
want to have Facebook Connect or Amazon connect
174:54 - and etc. You can do that as well. The way
it persists a connection after it authenticate
175:00 - is that generates a JW t. So that's how you're
going to persist that connection. So let's
175:04 - look at more of the options so that we can
really bake in the utility here of user pools.
175:09 - So here left hand side, we have a bunch of
different settings. And for attributes, we
175:13 - can determine what should be our primary attribute
should be our username when they sign up,
175:18 - or should it be email and phone phone number.
And if it is, you know, can they sign up or
175:26 - sign in, if the email address hasn't been
verified, or the conditions around that, we
175:30 - can set the restrictions on the password the
length, if it requires special characters,
175:35 - we can see what kind of attributes are required
to collect on signup, if we need their birthday,
175:39 - or email or etc. It has the capabilities of
turning on MFA. So if you want multi factor
175:44 - authentication, very easy way to integrate
that if you want to have user campaigns, so
175:49 - if you're used to like sending out campaigns
via MailChimp, you can easily integrate cognito
175:53 - with pinpoint, which is user campaigns, right.
And you also can override a lot of functionality
176:01 - using lambda. So anytime like a sign up or
sign in, or recovery passwords triggered,
176:06 - there is a hook so that you can then trigger
lambda to do something with that. So that's
176:10 - just some of the things that you do with cognito
user pools. But the most important thing to
176:13 - remember is just it's a way of decentralizing
your authentication that's for for user pool.
176:20 - All right. So now it's time to look at cognito
identity pools, identity pools provide temporary
176:27 - natives credentials to access services such
as dynamodb, or s3, identity pools can be
176:32 - thought of as the actual mechanism authorizing
access to the AWS resources. So you know,
176:38 - the idea is you have an identity pool, you're
gonna say, who's allowed to generate those
176:43 - AWS credentials, and then use the SDK to generate
those credentials. And then that application
176:48 - can then access those at the services. So
just to really hit that home here, I do have
176:54 - screenshots to give you an idea what that
is. So first, we're going to choose a provider.
176:59 - So our provider can be authenticated. So we
can choose cognito, or even a variety of other
177:04 - ones, or you can have it on authenticated.
So that is also an option for you. And then
177:10 - after you create that identity pool, they
have an easy way for you to use the SDK. So
177:15 - you could just drop down your platform and
you have the code and you're ready to go to
177:18 - go get those credentials. If you're thinking
did I actually put in my real, real example
177:24 - or identity pool ID there? It's not, it's
not I actually go in and replace all these.
177:28 - So if you're ever wondering and watching these
videos, and you're seeing these things I always
177:32 - replaced.
177:37 - We're going to just touch on one more, which
is cognito. Sync. And so sync lets you sync
177:42 - user data and preferences across all devices
with one line of code cognito, uses push notifications
177:47 - to push those updates and synchronize data.
And under the hood, it's using simple notification
177:52 - service to push this data to devices. And
the the data which is user data and preferences
178:00 - is key value data. And it's actually stored
with the identity pool. So that's what you're
178:04 - pushing back and forth. But the only thing
you need to know is what it does. And what
178:08 - it does is it syncs user data and preferences
across all devices with one line of code.
178:17 - So we're onto the Amazon cognito cheat sheets,
and let's jump into it. So cognito is a decentralized
178:22 - managed authentication system. So when you
need to easily add authentication to your
178:26 - mobile or desktop apps, think cognito. So
let's talk about user pools. So user pool
178:31 - is the user directory allows users to authenticate
using OAuth two ipds, such as Facebook, Google
178:36 - Amazon to connect to your web applications.
And cognito user pool isn't in itself an IPD.
178:43 - All right, so it can be on that list as well.
User pools use JW T's to persist authentication.
178:50 - Identity pools provide temporary database
credentials to access services, such as s3
178:55 - dynamodb, cognito. Sync can sync user data
preferences across devices with one line of
179:00 - code powered by SNS web identity Federation,
they're not going to ask you these questions.
179:04 - But you need to know what these are exchange
identity and security information between
179:08 - identity provider and an application. identity
provider is a trusted provider for your user
179:13 - to authenticate or sorry to identify that
user. So you can use them to dedicate to access
179:19 - other services. Then you have Oh IDC is a
type of identity provider which uses OAuth
179:24 - and you have SAML which is a type of identity
provider which is used for single sign on
179:28 - so there you go. We're done with cognito Hey,
this is Angie brown from exam Pro, and we
179:37 - are going to take a look here at AWS command
line interface also known as COI, which control
179:42 - multiple at the services from the command
line and automate them through scripts. So
179:48 - COI lets you interact with AWS from anywhere
by simply using a command line. So down below
179:53 - here I have a terminal and I'm using the AWS
CLI which starts with AWS so to get installed
180:00 - on your computer. AWS has a script, a Python
script that you can use to install the COI.
180:06 - But once it's installed, you're going to now
have the ability to type AWS within your terminal,
180:10 - followed by a bunch of different commands.
And so the things that you can perform from
180:14 - the CLR is you could list buckets, upload
data to s3, launch, stop, start and terminate
180:19 - easy to instances, update security groups
create subnets, there's an endless amount
180:23 - of things that you can do. All right. And
so I just wanted to point out a couple of
180:28 - very important flags, flags are these things
where we have hyphen, hyphen, and then we
180:31 - have a name here. And this is going to change
the behavior of these COI commands. So we
180:37 - have output and so the outputs what's going
to be returned to us. And we have the option
180:40 - between having Jason table and plain text.
I'm for profiles, if you are switching between
180:47 - multiple AWS accounts, you can specify the
profile, which is going to reference to the
180:52 - credentials file to quickly let you perform
CL actions under different accounts. So there
181:00 - you go. So now we're going to take a look
at eight of a software development kit known
181:06 - as SDK. And this allows you to control multiple
AWS services using popular programming languages.
181:13 - So to understand what an SDK is, let's go
define that. So it is a set of tools and libraries
181:18 - that you can use to create applications for
a specific software package. So in the case
181:24 - for the Ava's SDK, it is a set of API libraries
that you you that let you integrate data services
181:31 - into your applications. Okay, so that fits
pretty well into the distribution of an SDK.
181:36 - And the SDK is available for the following
languages. We have c++, go Java, JavaScript,
181:41 - dotnet, no, Jess, PHP, Python, and Ruby. And
so I just have an example of a couple of things
181:48 - I wrote in the SDK. And one is a no Jess and
one is Ruby into the exact same script, it's
181:54 - for ABS recognition for detecting labels,
but just to show you how similar it is among
182:01 - different languages, so more or less the,
the syntax is going to be the same. But yeah,
182:07 - that's all you need to know, in order to use
the line SDK, we're gonna have to do a little
182:15 - bit work beforehand and enable programmatic
access for the user, where we want to be able
182:20 - to use these development tools, okay. And
so when you turn on programmatic access for
182:25 - user, you're going to then get an access key
and a secret. So then you can utilize these
182:30 - services. And so down below, you can see I
have an access key and secret generated.
182:35 - Now, once you have these, you're going to
want to store them somewhere. And you're going
182:38 - to want to store them in your user's home
directory. And you're going to want them within
182:44 - a hidden directory called dot AWS, and then
a file called credentials. Okay, so down below
182:49 - here, I have an example of a credentials file.
And you'll see that we have default credentials.
182:55 - So if we were to use CLR SDK, it's going to
use those ones by default if we don't specify
182:59 - any. But if we were working with multiple
AWS accounts, we're going to end up with multiple
183:05 - credentials. And so you can organize them
into something called profiles here. And so
183:09 - I have one here for enterprise D, and D, Space
Nine. So now that we understand pragmatic
183:14 - access, let's move on to learning about Ceylon.
Hey, this is Andrew Brown from exam Pro. And
183:23 - we are going to do the COI and SDK follow
along here. So let's go over to I am and create
183:30 - ourselves a new user so we can generate some
database credentials. So now we're going to
183:36 - go ahead and create a new user. And we're
going to give them programmatic access so
183:42 - we can get a key and secret. I'm going to
name this user Spock. Okay. And we're going
183:48 - to go next here. And we're going to give them
developer permissions, which is a power user
183:53 - here. Okay, and you can do the same here,
if you don't have that, just go ahead and
183:58 - create that group. Name it developers and
just select power access there, okay? power
184:04 - with a capital I guess, there, okay. But I
already created it earlier with our I am,
184:10 - I am walk through there or follow along. So
we're going to skip over to tags and a review.
184:17 - And we're gonna create that user. Alright.
And so now we're gonna get an Access ID and
184:22 - secret. So I actually want to hold on to these.
So I'm just gonna copy that there. And we're
184:26 - gonna move over here and just paste that in.
Okay. And so we're just going to hold on to
184:33 - these and now we need an environment where
we can actually do a bit of coding and use
184:38 - the CLA and the best place to do that in AWS
is cloud nine. So we're gonna make our way
184:43 - over to cloud nine, and we're gonna spin ourselves
up a new environment, okay. So here we are,
184:50 - and still have these two other environments
here. I just seem to not be able to get rid
184:55 - of them. Generally, they they do delete with
very little trouble because I messed with
184:59 - the cloudformation stack, they're sticking
around here, but you won't have this problem.
185:03 - So I'm just going to create a new environment
here. And we are going to go ahead and call
185:08 - this Spock Dev. Okay, and we're gonna go to
the next step. And I'm just going to continuously
185:13 - ignore these silly errors. And we're going
to use the smallest instance here, the TT
185:17 - micro, which will be on the free tier, okay,
and we're going to use Amazon Linux. And this,
185:24 - cloud nine will actually spin down after 30
minutes Non Us. So if you forget about it,
185:29 - it will automatically turn itself off, start
us off, which is really nice. And we'll go
185:33 - ahead and go to next step. And it's going
to then just give us a summary here, we'll
185:39 - hit Create environment. And now we just have
to wait for this environment to start up.
185:45 - So we'll just wait a few minutes here. So
our cloud nine environment is ready here.
185:54 - Okay. And we have a terminal here, and it's
connected to any c two instance. And the first
185:58 - thing I'm going to do is I just can't stand
this white theme. So I'm gonna go to themes
186:02 - down here, go to UI themes, and we're going
to go to classic dark, okay, and that's gonna
186:06 - be a lot easier on my eyes here. And so the
first thing we want to do is we want to plug
186:11 - in our credentials here so that we can start
using the CLA. So the COI is already pre installed
186:16 - on this instance here. So if I was to type
AWS, we already have access to it. But if
186:21 - we wanted to learn how to install it, let's
actually just go through the motions of that.
186:25 - Okay, so I just pulled up a couple of docs
here just to talk about the installation process
186:30 - of the COI, we already have the COI installs
of a type universe, it's already here. So
186:35 - it's going to be too hard to uninstall it
just to install it to show you here, but I'm
186:39 - just going to kind of walk you through it
through these docks here just to get you an
186:41 - idea how you do it. So the COI requires either
Python two or Python three. And so on the
186:48 - Amazon Linux, I believe that it has both.
So if I was to type in pi and do virgin here,
186:53 - okay.
186:54 - Or just Python, sorry, I'm always thinking
of shorthands. This has version 3.6, point
187:00 - eight, okay. And so when you go ahead and
install it, you're going to be using Pip.
187:05 - So PIP is the way that you install things
in Python, okay, and so it could be PIP or
187:11 - PIP three, it depends, because there used
to be Python two, and southern Python three
187:15 - came out, they needed a way to distinguish
it. So they called it PIP three. But Python
187:19 - two is no longer supported. So now PIP three
is just Pip. Okay, so you know, he's got to
187:25 - play around based on your system, okay. But
generally, it's just Pip pip, install ATSC
187:30 - Li. And that's all there is to it. And to
get Python install, your system is going to
187:34 - vary, but generally, you know, it's just for
Amazon, Linux, it is a CentOS, or Red Hat
187:40 - kind of flavor of Linux. So it's going to
use a Yum, install Python. And just for all
187:45 - those other Unix distributions, it's mostly
going to be apt get Okay, so now that we know
187:52 - how to install the CLA, I'm just gonna type
clear here and we are going to set up our
187:56 - credentials. Alright, so we're gonna go ahead
and install our credentials here, they're
188:06 - probably already installed, because cloud
nine is very good at setting you up with everything
188:10 - that you need. But we're going to go through
the motions of it anyway. And just before
188:13 - we do that, we need to install a one thing,
cloud nine here. And so I'm going to install
188:18 - via node package manager c nine, which allows
us to open files from the terminal into Cloud
188:25 - Nine here. And so the first thing I want you
to do is I want to go to your home directory,
188:28 - you do that by typing Tilda, which is for
home, and forward slash, OK. And so now I
188:34 - want you to do l LS, hyphen, LA, okay. And
it's going to list everything within this
188:38 - directory, and we were looking for a directory
called dot AWS. Now, if you don't have this
188:43 - one, you just type it MK Dir. And you do dot
AWS to create it, okay, but already exists
188:48 - for us. Because again, cloud nine is very
good at setting things up for us. And then
188:53 - in here, we're expecting to see a credentials
file that should contain our credential. So
188:56 - typing see nine, the program we just installed
there, I'm just going to do credentials here,
189:01 - okay. And it's going to open it up above here.
And you can already see that it's a set of
189:06 - credentials for us, okay. And I'm just going
to flip over and just have a comparison here.
189:11 - So we have some credentials. And it is for
I don't know who, but we have them and I'm
189:18 - going to go ahead and add a new one, I'm just
going to make a new one down here called Spark,
189:24 - okay. All right. And basically, what I'm doing
is I'm actually creating a profile here, so
189:31 - that I can actually switch between credentials.
Okay. And I'm just going to copy and paste
189:41 - them in here. Alright, and so I'm just going
to save that there. And so now I have a second
189:49 - set of credentials within the credential file
there, and it is saved. And I'm just going
189:54 - to go down to my terminal here and do clear.
And so now what I'm going to do is I'm going
189:58 - to type in AWS s Three LS, and I'm going to
do hyphen, hyphen profile, I'm going to now
190:04 - specify Spock. And that's going to use that
set of credentials there. And so now I've
190:10 - done that using sparks credentials, and we
get a list of a bucket stuff. Okay? So now
190:15 - if we wanted to copy something down from s3,
we're going to AWS s3, CP. And we are going
190:23 - to go into that bucket there. So it's going
to be exam Pro, enterprise D, I have this
190:30 - from memory. And we will do data dot jpg,
okay. And so what that's going to do is it's
190:38 - going to download a file, but before I actually
run this here, okay, I'm just going to CD
190:43 - dot dot and go back to my home directory here.
Okay. And I'm just going to copy this again
190:50 - here and paste it and so I should be able
to download it. But again, I got to do a hyphen
190:53 - having profile specifies proc Spark, because
I don't want to use the default profile there.
190:58 - Okay, um, and, uh, complain, because I'm missing
the G on the end of that there, okay?
191:05 - And it's still complaining. Maybe I have to
do s3 forward slash forward slash,
191:16 - huh?
191:17 - No, that's the command. Oh, you know why?
It's because when you use CP, you have to
191:22 - actually specify the output file here. So
you need your source and your destination.
191:27 - Okay, so I'm just gonna type Spock, or sorry,
data, sorry, data dot JPG there. Okay. And
191:33 - that's going to download that file. So, I
mean, I already knew that I had something
191:37 - for AWS there. So I'm just going to go to
AWS to show you that there. So if you want
191:42 - to do the same thing as I did, you knew you
definitely need to go set up a bucket in s3.
191:47 - Okay. So if I just go over here, we have the
exam, pro 00, enterprise D, and we have some
191:53 - images there. Okay. So that's where I'm grabbing
that image from. And I could just move this
191:59 - file into my environment directory, so I actually
can have access to it there. Okay, so I'm
192:04 - just going to do MB data. And I'm just going
to move that one directory up here. Okay.
192:11 - All right. And so now we have data over here,
okay. And so you know, that's how you'd go
192:18 - about using the CLA with credentials. Okay.
Yeah, we just open that file there if we wanted
192:24 - to preview it. Okay. So now let's, let's move
on to the SDK, and let's use our credentials
192:29 - to do something programmatically. So now that
we know how to use the COI, and where to store
192:38 - our credentials, let's go actually do something
programmatically with the SDK. And so I had
192:44 - recently contributed to database docs, for
recognition. So I figured we could pull some
192:48 - of that code and have some fun there. Okay.
So why don't you do is go to Google and type
192:53 - in Avis docs recognition. And we're going
to click through here to Amazon recognition,
192:59 - we're going to go to the developer developers
guide with HTML, apparently, they have a new
193:03 - look, let's give it a go. Okay, there's always
something new here. I'm not sure if I like
193:08 - it. But this is the new look to the docks.
And we're gonna need to find that code there.
193:13 - So I think it is under detecting faces here.
And probably under detecting faces in an image,
193:21 - okay. And so the code that I added was actually
the Ruby and the no GS one, okay, so we can
193:26 - choose which one we want. I'm going to do
the Ruby one, because I think that's more
193:30 - fun. And that's my language of choice. Okay.
And so I'm just going to go ahead and copy
193:35 - this code here. Okay. And we're going to go
back to our cloud nine environment, I'm going
193:41 - to create a new, a new file here, and I'm
just going to call this detect faces, ooh,
193:50 - keep underscore their faces.rb. Okay. And
I'm just going to double click in here and
193:56 - paste that code in. Alright. And what we're
going to have to do is we're going to need
194:01 - to supply our credentials here, generally,
you do want to pass them in as environment
194:07 - variables, that's a very safe way to provide
them. So we can give that a go. But in order
194:13 - to get this working, we're going to have to
create a gem file in here. So I'm just going
194:16 - to create a new file here, because we need
some dependencies here. And we're just going
194:21 - to type in gem file, okay. And within this
gem file, we're going to have to provide the
194:27 - gem recognition. Okay, so I'm just gonna go
over here and supply that there. There is
194:33 - a few other lines here that we need to supply.
So I'm just gonna go off screen and go grab
194:37 - them for you. Okay, so I just went off screen
here and grabbed that extra code here. This
194:42 - is pretty boilerplate stuff that you have
to include in a gem file. Okay. And so what
194:46 - this is going to do, it's going to install
of AWS SDK for Ruby, but specifically just
194:51 - for recognition. So I do also have open up
here the AWS SDK, for Ruby and for no GS,
194:58 - Python, etc. They all
195:01 - I have one here. And so they tells you how
you can install gems. So for dealing with
195:06 - recognition here, I'm just going to do a quick
search here for recognition. Okay, sometimes
195:11 - it's just better to navigate on the left hand
side here. Alright, and so I'm just looking
195:16 - for a recognition. Okay, and so if we want
to learn how to use this thing, usually a
195:20 - lot of times with this, it's going to tell
you what gem you're gonna need to install.
195:23 - So this is the one we are installing. And
then we click through here through client,
195:27 - and then we can get an idea of all the kind
of operations we can perform. Okay, so when
195:32 - I needed to figure out how to write this,
I actually went to the CLR here, and I just
195:35 - kind of read through it and pieced it together
and looked at the output to figure that out.
195:39 - Okay, so nothing too complicated there. But
anyway, we have all the stuff we need here.
195:44 - So we need to make sure we're in our environment
directory here, which is that Spock dev directory.
195:50 - So we're going to tilde out, which goes to
our home directory environment. Okay, we're
195:55 - gonna do an ls hyphen, LA. And just make sure
that we can see that file there and the gem
195:59 - file, okay, and then we can go ahead and do
a bundle install. All right, and so what that's
196:04 - going to do is it's going to now install that
dependency. so here we can see that installed
196:09 - the EVAs SDK, SDK, core and also recognition.
Okay. And so now we have all our dependencies
196:15 - to run the script here. So the only thing
that we need to do here is we need to provide
196:20 - it an input. So here, we can provide it a
specific bucket and a file, there's a way
196:28 - to provide a locally, we did download this
file, but I figured what we'll do is we'll
196:31 - actually provide the bucket here. So we will
say what's the bucket called exam pro 000.
196:41 - And the next thing we need to do is define
the key. So it's probably the key here. So
196:45 - I'm going to do enterprise D. Okay. And then
we're just going to supply data there. All
196:52 - right. And we can pass these credentials via
the environment variables, we could just hard
196:57 - code them and paste them in here. But that's
a bit sloppy. So we are going to go through
197:01 - the full motions of providing them through
the environment here. And all we have to do
197:06 - that is we're just going to paste in, like
so. Okay. And we're just going to copy that.
197:14 - That's the first one. And then we're going
to do the password here. Oops. Okay. And hopefully,
197:22 - this is going to work the first time and then
we'll have to do bundle, exactly. detect faces,
197:28 - okay. And then this is how these are going
to get passed into there. And assuming that
197:32 - my key and bucket are correct, then hopefully,
we will get some output back. Okay.
197:37 - All right, it's just saying it couldn't detect
faces here, I just have to hit up here, I
197:43 - think I just need to put the word Ruby in
front of here.
197:46 - Okay, so my bad. Alright, and we is it working.
So we don't have the correct permissions here.
197:57 - So we are a power user. So maybe we just don't
have enough permission. So I'm just going
198:00 - to go off screen here and see what permissions
we need to be able to do this. So just playing
198:07 - around a little bit here, and also reading
the documentation for the Ruby SDK, I figured
198:12 - out what the problem was, it's just that we
don't need this forward slash here. So we
198:17 - just take that out there, okay, and just run
what we ran last there, okay. And then we're
198:22 - going to get some output back. And then it
just shows us that it detected a face. So
198:27 - we have the coordinates of a face. And if
we used some additional tool there, we could
198:34 - actually draw overtop of the image, a bounding
box to show where the face is detected. There's
198:38 - some interesting information. So it detected
that the person in the image was male, and
198:43 - that they were happy. Okay. So, you know,
if you think that that is happy, then that's
198:49 - what recognition thinks, okay. And it also
detected the face between ages 32 and 48.
198:55 - To be fair, data is an Android, so he has
a very unusual skin color. So you know, it's
199:01 - very hard to do to determine that age, but
I would say that this is the acceptable age
199:07 - range of the actor at the time of, so it totally
makes sense. Okay. Um, so yeah, and there
199:14 - you go. So that is the pragmatic way of doing
it. Now, you don't ever really want to ever
199:20 - store your credentials with on your server,
okay? Because you can always use Iam roles,
199:25 - attach them to EC two instances, and then
that will safely provide credentials onto
199:30 - your easy two instances, to have those privileges.
But it's important to know how to use the
199:35 - SDK. And whenever you're in development working
on your local machine, or maybe you're in
199:39 - cloud nine environment, you are going to have
to supply those credentials. Okay. So there
199:45 - you go. So now that we are done with our ADA,
or SC Li and SDK, follow along here, let's
199:53 - just do some cleanup. So I'm just going to
close this tab here for cloud nine. And we're
199:57 - going to go over to cloud nine and we're going
to just delete that and Now again, it's not
200:01 - going to be bad for you to have it hanging
around here, it's not going to cause you any
200:04 - problems, it's going to shut down on its own.
But you know, if we don't need it, we might
200:08 - as well get rid of it. Okay. And so I'm just
gonna have the word delete here. And hopefully
200:13 - this one deletes as long as they don't fiddle
and delete these security groups before it
200:17 - has an opportunity to delete. That should
have no problem for me here. And then we're
200:21 - just going to go to our Im role, or sorry,
our user there. And what we really want to
200:26 - do is since they're not being used anymore,
we want to expire those credentials. But I'm
200:31 - actually going to also go ahead and delete
the user here. So they're going to be 100%
200:36 - gone there. Okay, so there, that's all the
cleanup we had to do.
200:41 - over onto the AWS ccli SDK cheat sheet so
let's jump into it. So COI stands for command
200:50 - line interface SDK stands for software development
kit. The COI lets you enter interact with
200:56 - AWS from anywhere by simply using a command
line. The SDK is a set of API libraries that
201:02 - let you integrate Ada services into your applications.
promatic access must be enabled per user via
201:08 - the Iam console to UCLA or SDK into its config
command is used to set up your database credentials
201:15 - for this Eli, the CLA is installed via a Python
script credentials get stored in a plain text
201:23 - file, whenever possible use roles instead
of at this credentials. I do have to put that
201:27 - in there. And the SDK is available for the
following programming languages c++, go Java,
201:31 - JavaScript, dotnet, no GS, PHP, Python and
Ruby. Okay, so for the solution architect
201:38 - associate, they're probably not going to ask
you questions about the SDK, but for the developer,
201:43 - there definitely are. So just keep that in.
Hey, this is Andrew Brown from exam Pro. And
201:52 - we are looking at domain name systems also
abbreviated as DNS, and you can think of them
201:56 - as the phonebook of the internet. DNS translates
domain names to IP addresses, so browsers
202:02 - can find internet resources. So again, domain
name servers are a service which handles converting
202:08 - a domain name such as exam prodotto, into
a rentable Internet Protocol address. So here
202:14 - we have an example of an ipv4 address. And
this allows your computer to find specific
202:18 - servers on the internet automatically, depending
what domain name you browse. so here we can
202:24 - see it again. So we see example, CO, it looks
up the the domain name and determines that
202:31 - it should go this IP address, which should
go to this server. So that is the process.
202:36 - So we need to understand the concept of Internet
Protocol, also known as IP. So IP addresses
202:44 - are what are uniquely used to identify computers
on a network. And it allows communication
202:50 - between using them. And that is what the IP
is. And so IPS come in two variations. We
202:55 - have ipv4, which you're probably most familiar
with, because it's been around longer. And
203:00 - then we have ipv6, which looks a bit unusual,
but there are definitely benefits to it. So
203:04 - ipv4 is an address space with 32 bits. And
this is the amount of available addresses
203:10 - that are out there. And the issue with ipv4
is we're running out of IP addresses, okay,
203:16 - because it's based on this, this way of writing
numbers, and there's a limit to how many numbers
203:22 - are here. So to come up, or to combat that
solution. That's where ipv6 comes in. And
203:27 - so ipv6 uses an address space of 128 bits.
And it has up to 340 on the sillian potential
203:36 - addresses. And so basically, they've invented
a way so we will not run out of addresses,
203:42 - okay, and this is what that address looks
like. So it's big and long, it's not as easy
203:46 - to look look at as an IP, ipv4 address, but
we're never gonna run out of them. So you
203:53 - know, come the future. We're gonna see this
implemented more, you can definitely use ipv6
203:58 - on AWS, as well as ipv4, but you know, this
is future. So domain registers are the authorities
204:10 - who have the ability to assign domain names
under one or more top level domains. And if
204:14 - you're thinking about what are some common
registrar's, we have them listed down below.
204:19 - So you've probably seen them before like Hostgator,
GoDaddy, domain.com. AWS is also their own
204:25 - domain register with route 53. And name cheap,
okay, domain domains get registered through
204:30 - the internet, which is a service provided
by the internet Corporation for Assigned Names
204:35 - and Numbers, I can and enforces the uniqueness
of domain names all over the internet. And
204:41 - when you register a domain name, it can be
found publicly in the central who is database.
204:45 - So if you've ever wondered who owns a domain,
there's a high chance that you could go type
204:49 - it in who is and they might have the registers
contact information. Now you can pay additional,
204:55 - or in the case of revenue three, I don't think
there's any additional money to keep that
204:57 - information private. But that's it. You have
a registered domain name. And you wonder why
205:02 - somebody is calling you out of the blue, maybe
they've been looking you up through here.
205:05 - So there you go domain. So we're looking
205:11 - at the concept of top level domains. And so
if you've ever typed a domain name in and
205:16 - you're wondering what that.com is, that is
the top level domain name. And there are domains
205:21 - that also have second level domains. So in
the example, of.co.uk the.co is the second
205:27 - level, top level domain names are controlled
by the Internet Assigned Numbers Authority.
205:33 - And so anytime there's new ones, they're the
ones who are the number one authority on new
205:40 - top level domains. These domains are managed
by different organizations. And it would surprise
205:47 - you to see that there's hundreds upon hundreds
of top level domains. And you might not even
205:52 - know about them, because these companies are
just sitting on them. But like, you can see
205:56 - Disney has one for dot ABC. And then we have
a dot Academy one, and also AWS has their
206:02 - own, which is dot AWS. Okay, so there you
go, that's top level. So when you have a domain
206:12 - name, you're gonna have a bunch of records
that tell it what to do. And one that's very
206:16 - important and is absolutely required is the
SLA, the start of authority. And the SLA is
206:21 - a way for the domain admins to provide additional
information about the domain such as how often
206:27 - it's updated, what's the admins email address,
if there was a failure with responding to
206:33 - master? How many seconds should it tried to
fault failover to the secondary namespace,
206:39 - so it can contain a bunch of information,
and you can see it on the right hand side
206:43 - here. And you don't necessarily have to provide
all the information. But those are all the
206:48 - options. And it comes in the format of one
big long string. So you can see, we can see
206:55 - the format here. And we have an example. And
then we got an eight it was example. So there
207:00 - you go. And yeah, there you can only actually
have one, so a record within a single zone.
207:06 - So you can't have more than one. But yeah,
it's just to give additional information.
207:11 - It's absolutely.
207:16 - So now we're gonna take a look at address
records. And a records are one of the most
207:19 - fundamental types of DNS records. And the
idea is that you're going to be able to convert
207:25 - the name of a domain directly into an IP address.
Okay. So if you had testing domain comm, and
207:32 - you wanted to point it to the ipv4 address
50 221 6834, you'd use an a record for it.
207:41 - And one more thing I want to know about is
that you can use naked domain names, or root
207:47 - domains, that's when you don't have any www.no.
subdomains as an A record. Okay. So canticle
207:58 - names, also known as C names are another fundamental
DNS record used to resolve one domain name
208:03 - to another rather than an IP address. So if
you wanted, for example, to send all your
208:10 - naked domain traffic to the www route, you
could do so here, right? So here we have the
208:17 - we're specifying the naked domain, and we're
going to send it to look like the www domain
208:24 - for some reason, for some reason, I have four
W's in here. But you can't give me a hard
208:28 - time because at least that error is consistent.
But yeah, that's all there is to it. So a
208:33 - records are IP addresses, and C names are
domain. So besides the SLA, the second most
208:43 - important record or records are the name server
records. And they're used by top level domain
208:49 - servers to direct traffic to DNS to the DNS
server contain the authoritive DNS records.
208:55 - If you don't have these records, your domain
name cannot do anything. Typically, you're
209:00 - gonna see multiple name servers provided as
redundancy. Something like with GoDaddy, they're
209:06 - gonna give you two, with AWS, you're gonna
have four. But the more the merrier. Okay,
209:11 - and so you can see an example down below here.
So if you're managing your DNS records with
209:17 - route 53, DNS records for the domain would
be pointing to AWS servers, and there we have
209:22 - four we have a.com, it's probably the US dotnet.
I'm not sure that is co n.org. Okay, so we
209:28 - have a lot of redundancy there. Oh, now I
want to talk about the concept of TTL Time
209:36 - To Live and this is the length of time that
a DNS record gets cached on resolving servers
209:42 - or the user's own local machine. So the lower
the TTL, the faster the changes to DNS records
209:47 - will propagate across the internet. TTL is
always measured in seconds under ipv4, so
209:54 - you're gonna see more TTL is here. So if it's
not super clear, it will make sense further
209:59 - into this.
210:05 - So it's time to wrap up DNS with another cheat
sheet. So let's get to it. So Domain Name
210:10 - System, which is DNS is an internet service
that converts domain names into routable IP
210:16 - addresses. We have two types of internet protocols.
We have ipv4, which is a 32 bit address space
210:22 - and has a limited number of addresses. And
then we have an example of one there. And
210:26 - then we have ipv6, which is 128 bit address
space and has unlimited number of addresses.
210:32 - And we also have an example there as well.
Then we talked about top level domains. And
210:36 - that's just the last part of a domain like.com,
then you have second level domains. And this
210:42 - doesn't always happen. But it's usually the
second last part of the domain. So in.co.uk,
210:47 - it's going to be the.co. Then there we have
domain registers. These are third party companies
210:52 - who register don't you register domains through,
then you have name servers, they're the servers
210:57 - which contain the DNS records of the domain,
then we have some records of interest here.
211:03 - So we have the SLA. This contains information
about the DNS zone and associated DNS records,
211:08 - we have a records, these are records which
directly convert a domain name into an IP
211:13 - address, then we have a C name records. And
these are records, which lets you convert
211:17 - a domain name into another domain name. And
then we have TT ELLs, and it's the time it
211:23 - takes for DNS records, or it's the time that
a DNS record will be cached for a cache for
211:29 - and the lower that time means, the faster
it will propagate or update. Okay, and there
211:37 - you go. Hey, this is Andrew Brown from exam
Pro. And we are looking at remedy three, which
211:43 - is a highly available and scalable domain
name service. So whenever you think about
211:48 - revenue three, the easiest way to remember
what it does is think of GoDaddy or Namecheap,
211:53 - which are both DNS providers. But the difference
is that revenue three has more synergies with
211:59 - AWS services. So you have a lot more rich
functionality that you could do on on AWS
212:03 - than you could with one of these other DNS
providers. What can you do with revenue three,
212:09 - you can register and manage domains, you can
create various record sets on a domain, you
212:13 - can implement complex traffic flows such as
bluegreen, deploys, or fail overs, you can
212:18 - continuously monitor records, via health checks,
and resolve epcs outside of AWS.
212:30 - So here, I have a use case, and this is actually
how we use it at exam Pro is that we have
212:34 - our domain name, you can purchase it or you
can have refer to three manage the the name
212:40 - servers, which allow you to then set your
record sets within route 53. And so here we
212:46 - have a bunch of different record sets for
subdomains. And we want those sub domains
212:51 - to point to different resources on AWS. So
for our app, our app runs behind elastic load
212:56 - balancer. If we need to work on an ami image,
we could launch a single EC two instance and
213:01 - point that subdomain there for our API, if
it was powered by API gateway, we could use
213:06 - that subdomain for that, for our static website
hosting, we would probably want to point to
213:10 - CloudFront. So the WW dot points to a CloudFront
distribution. And for fun, and for learning,
213:15 - we might run a minecraft server on a very
specific IP, probably would be elastic IP
213:20 - because we wouldn't want it to change. And
that could be Minecraft exam pro quo. So there's
213:24 - a basic example. But we're gonna jump into
all the different complex rules that we can
213:30 - do in revenue three here. So in the previous
use case, we saw a bunch of sub domains, which
213:40 - were pointing to AWS resources, well, how
do we create that link so that a remedy three
213:45 - will point to those resources, and that is
by creating record sets. So here, I just have
213:51 - the form for record sets. So you can see the
kind of the types of records that you can
213:55 - create, but it's very simple, you just fill
in your sub domain or even leave the naked
213:59 - domain and then you choose the type. And in
the case for a this is allows you to point
214:03 - this sub domain to a specific IP address,
you just fill it in, that's all there is to
214:09 - it. Okay, now, I do need to make note of this
alias option here, which is a special option
214:15 - created by AWS. So here in the next slide
here, we've set alias to true. And what it
214:21 - allows us to do is directly select specific
AWS resources. So we could select CloudFront,
214:27 - Elastic Beanstalk, EOB, s3 VPC API gateway,
and why would you want to do this over making
214:34 - a traditional type record? Well, the idea
here is that this alias has the ability to
214:41 - detect changes of IP addresses. So it continuously
keeps pointing that endpoint to the correct
214:46 - resource. Okay. So that's normally when if,
if and whenever you can use alias always use
214:53 - alias because it just makes it easier to manage
the connections between resources via roughly
214:59 - three records. That's, and the limitations
are listed here as follows.
215:07 - So the major advantage of Route 53 is it's
seven types of routing policies. And we're
215:15 - going to go through every single one here.
So we understand the use case, for all seven.
215:21 - Before we get into that a really good way
to visualize how to work with these different
215:26 - routing policies is through traffic flow.
And so traffic flow is a visual editor that
215:30 - lets you create sophisticated routing configurations
within route 53. Another advantage of traffic
215:36 - flow is that we can version these policy routes.
So if you created a complex routing policy,
215:42 - and you wanted to change it tomorrow, you
could save it as version one, version two,
215:45 - and roll, roll this one out or roll back to
that. And just to play around traffic flow,
215:50 - it does cost a few dollars per policy record.
So this whole thing is one policy record.
215:55 - But they don't charge you until you create
it. So if you do want to play around with
215:58 - it, just just create a new traffic flow, and
name it and it will get, you'll get to this
216:04 - visual editor. And it's not until you save
this. So you can play around with this to
216:08 - get an idea of like all the different routing
rules and how you can come up with creative
216:11 - solutions. But now that we've covered traffic
flow, and we know that there are seven routing
216:16 - rules, let's go deep and look at what we can
do. We're gonna look at our first routing
216:26 - policy, which is the simple routing policy.
And it's also the default routing policy.
216:31 - So when you create a record setting here,
I have one called random, and we're on the
216:35 - a type here. Down below, you're gonna see
that routing policy box that's always by default
216:41 - set to simple. Okay, so what can we do with
simple The idea is that you have one record,
216:47 - which is here, and you can provide either
a single IP address or multiple IP addresses.
216:53 - And if it's just a single, that just means
that random is going to go to that first IP
216:57 - address every single time. But if you have
multiples, it's going to pick one at random.
217:02 - So it's good way to make like a, if you wanted
some kind of random thing made for a to b
217:08 - testing, you could do this. And that is as
simple as it is. So there you go. So now we're
217:18 - looking at weighted routing policies. And
so what a weighting routing policy lets you
217:23 - do is allows you to split up traffic based
on different weights assigned. Okay, so down
217:28 - below, we have app.example.co. And we would
create two record sets in roughly three, and
217:34 - they'd be the exact same thing, they both
say app.example.com. But we'd set them both
217:38 - to weighted and we give them two different
weights. So for this one, we would name it
217:42 - stable. So we've named that one stable, give
it 85%. And then we make a new record set
217:46 - with exact same sub domain and set this one
to 15% call experiment, okay. And the idea
217:53 - is that when ever traffic, any traffic hits
app.example.co, it's going to look at the
217:58 - two weighted values. 85% is gonna go to the
stable one. And for the 15%, it's going to
218:03 - go to the experimental one. And a good use
case for that is to test a small amount of
218:07 - traffic to minimize impact when you're testing
out new experimental features. So that's a
218:12 - very good use case for a weighted routing.
Now we're going to take a look at latency
218:21 - based routing. Okay, so lane based routing
allows you to direct traffic based on the
218:27 - lowest network latency possible for your end
user based on a region. Okay, so the idea
218:32 - is, let's say people want to hit AP dot exam
protocol. And they're coming from Toronto.
218:38 - All right, so coming from Toronto, and the
idea is that we have, we've created two records,
218:44 - which have latency with this sub domain, and
one is set to us West. So that's on the west
218:50 - coast. And then we have one central Canada,
I believe that's located in Montreal. And
218:55 - so the idea is that it's going to look here
and say, Okay, which one produces the least
218:58 - amount of latency, it doesn't necessarily
mean that it has to be the closest one geographically,
219:02 - just whoever has the lowest return in milliseconds
is the one that it's going to route traffic
219:09 - to. And so in this case, it's 12 milliseconds.
And logically, things that are closer by should
219:13 - be faster. And so the, so it's going to route
it to this lb, as opposed to that one. So
219:20 - that's, that's how latency based routing works.
So now we're looking at another routing policy,
219:29 - this one is for failover. So failover allows
you to create an Active Passive setup in situations
219:34 - where you want a primary site in one location,
and a secondary data recovery site and another
219:40 - one. Okay, another thing to note is that revenue
three automatically monitors via health checks
219:45 - from your primary site to determine if that
that endpoint is healthy. If it determines
219:51 - that it's in a failed state, then all the
traffic will be automatically redirected to
219:56 - that secondary location. So here, we have
done following examples, we have app.example.co.
220:02 - And we have a primary location and a secondary
one. All right. And so the idea is that
220:11 - roughly three, it's going to check and it
determines that this one is unhealthy based
220:15 - on a health check, it's going to then reroute
the traffic to our secondary locations. So
220:20 - you'd have to create, you know, two routing
policies with the exact same. The exact same
220:27 - domain, you just set which one is the primary
and which one is the secondary, it's that
220:32 - simple. So here, we are looking at the geolocation
routing policy. And it allows you to direct
220:40 - traffic based on the geolocation, geographical
location of where the request is originating
220:45 - from. So down below, we have a request from
the US hitting app dot exam pro.co. And we
220:52 - have a a record set for a geolocation that
set for North America. So since the US is
220:59 - in North America, it's going to go to this
record set. Alright. And that's as
221:05 - simple as that.
221:09 - So we're going to look at geo proximity routing
policy, which is probably the most complex
221:14 - routing policy is a bit confusing, because
it sounds a lot like geolocation, but it's
221:18 - not. And we'll see shortly here, you cannot
create this using record sets, you have to
221:25 - use traffic flow, because it is a lot more
complicated, and you need to visually see
221:29 - what you're doing. And so it's gonna be crystal
clear, we're just going to go through here
221:33 - and look at what it does. So the idea is that
you are choosing a region. So you can choose
221:40 - one of the existing Eva's regions, or you
can give your own set of coordinates. And
221:44 - the idea is that you're giving it a bias around
this location, and it's going to draw boundaries.
221:49 - So the idea is that if we created a geo proximity
routing for these regions, this is what it
221:55 - would look like. But if we were to give this
120 5% more bias, you're gonna see that here,
222:00 - it was a bit smaller, now it's a bit larger,
but if we minus it, it's going to reduce it.
222:04 - So this is the idea behind a geo proximity
where you have these boundaries, okay. Now,
222:08 - just to look at in more detail here, the idea
is that you can set as many regions or points
222:17 - as you want here. And so here, I just have
two as an example. So I have China chosen
222:22 - over here. And this looks like we have Dublin
shows. So just an idea to show you a simple
222:27 - example. Here's a really complicated one here,
I chose every single region just so you have
222:31 - an idea of split. So the idea is you can choose
as little or as many as you want. And then
222:37 - you can also give it custom coordinates. So
here I chose Hawaii. So I looked at the Hawaii
222:41 - coordinates, plugged it in, and then I turned
the bias down to 80%. So that it would have
222:45 - exactly around here and I could have honed
it in more. So it just gives you a really
222:49 - clear picture of how geo proximity works.
And it really is boundary based and you have
222:55 - to use traffic flow for that. So the last
routing policy we're going to look at is multivalue.
223:05 - And multivalue is exactly like simple routing
policy. The only difference is that it uses
223:12 - a health check. Okay, so the idea is that
if it picks one by random, it's going to check
223:17 - if it's healthy. And if it's not, it's just
going to pick another one by random. So that
223:21 - is the only difference between multivalue
and simple. So there you go.
223:30 - Another really powerful feature of Route 53
is the ability to do health checks. Okay,
223:34 - so the idea is that you can go create a health
check, and I can say for AP dot exam dot prodotto,
223:40 - it will check on a regular basis to see whether
it is healthy or not. And that's a good way
223:45 - to see at the DNS level if something's wrong
with your instance, or if you want to failover
223:50 - so let's get into the details of here. So
we can check health every 30 seconds by default
223:56 - and we can it can be reduced down to 10 seconds,
okay, a health check and initiate a failover.
224:01 - If the status is returned unhealthy, a cloudwatch
alarm can be created to alert you of status
224:07 - unhealthy, a health check can monitor other
health checks to create a chain of reactions.
224:14 - You can have up to 50 in a single AWS account.
And the pricing is pretty affordable. So it's
224:21 - 50 cents. So that's two quarters for per endpoint
on AWS. And there are some additional features,
224:27 - which is $1 per feature. Okay.
224:29 - So if you're using route 53, you might wonder
well, how do I route traffic to my on premise
224:40 - environment and that's where revenue three
resolver comes into play, formerly known as
224:45 - dot two. resolver is a regional service that
lets you connect route DNS queries between
224:50 - your VBC and your network. So it is a tool
for hybrid environments on premises and cloud.
224:56 - And we have some options here. If we just
want to do inbound and outbound inbound only
224:59 - or outbound Only. So that's all you really
need to know about it. And that's how you
225:04 - do hybrid networking. So now we're taking
a look at revenue three cheat sheet, and we're
225:14 - going to summarize everything that we have
learned about roughly three. So revenue three
225:18 - is a DNS provider to register and manage domains
create record sets, think GoDaddy or namecheap.
225:23 - Okay, there's seven different types of routing
policies, starting with simple routing policy,
225:27 - which allows you to input a single or multiple
IP addresses to randomly choose an endpoint
225:33 - at random, then you have weighted routing,
which splits up traffic between different
225:37 - weights, assign some percentages, latency
based routing, which is based off of routing
225:41 - traffic to the based on region for the lowest
possible latency for users. So it's not necessarily
225:46 - the closest geolocation but the the lowest
latency, okay, we have a failover routing,
225:51 - which uses a health check. And you set a primary
and a secondary, it's going to failover to
225:56 - the secondary if the primary health check
fails, you have geolocation, which roads traffic
226:01 - based on the geolocation. So this is based
on geolocation would be like North America
226:07 - or Asia, then you have geo proximity routing,
which can only be done in traffic flow allows
226:13 - you to set biases so you can set basically
like this map of boundaries, based on the
226:19 - different ones that you have, you have multi
value answer, which is identical to a simple,
226:24 - simple routing, the only difference being
that it uses a health check. In order to do
226:29 - that. We looked at traffic flow, which is
a visual editor for changing routing policies,
226:34 - you conversion those record those policy records
for easy rollback, we have alias record, which
226:39 - is a device's Smart DNS record, which detects
IP changes freedoms resources and adjusts
226:44 - them automatically always want to use alias
record, when you have the opportunity to do
226:48 - so you have route 53 resolver, which is a
hybrid solution. So you can connect your on
226:54 - premise and cloud so you can network between
them. And then you have health checks, which
227:00 - can be created to monitor and and automatically
failover to another endpoint. And you can
227:05 - have health checks, monitor other health checks
to create a chain of reactions, for detecting
227:10 - issues for endpoints. Hey, this is Angie brown
from exam Pro. And we are looking at elastic
227:22 - Compute Cloud EC two, which is a cloud computing
service. So choose your last storage memory,
227:27 - network throughput and then launch and SSH
into your server within minutes. Alright,
227:32 - so we're on to the introduction to EC two.
And so EC T was a highly configurable server.
227:37 - It's a resizeable compute capacity, it takes
minutes to launch new instances, and anything
227:42 - and everything I need is uses easy to instance
underneath. So whether it's RDS or our ECS,
227:49 - or simple system manager, I highly, highly
believe that at AWS, they're all using easy
227:55 - to okay. And so we said they're highly configurable.
So what are some of the options we have here?
227:59 - Well, we get to choose an Amazon machine image,
which is going to have our last so whether
228:04 - you want Red Hat, Ubuntu windows, Amazon,
Linux or Susi, then you choose your instance
228:09 - type. And so this is going to tell you like
how much memory you want versus CPU. And here,
228:14 - you can see that you can have very large instances.
So here is one server that costs $5 a month.
228:21 - And here we have one that's $1,000 a month.
And this one has 36 CPUs and 60 gigabytes
228:27 - of memory with 10 gigabyte performance, okay,
then you add your storage, so you could add
228:33 - EBS or Fs and we have different volume types
we can attach. And then you can configure
228:39 - your your instance. So you can secure it and
get your key pairs. You can have user data,
228:44 - Im roles and placement groups, which we're
all going to talk about starting. All right,
228:51 - so we're gonna look at instance types and
what their usage would be. So generally, when
228:57 - you launch an EC two instance, it's almost
always going to be in the T two or the T three
229:01 - family. And
229:02 - yes, we have all these little acronyms which
represent different types of instance types.
229:08 - So we have these more broad categories. And
then we have subcategories, or families have
229:13 - instances that are specialized. Okay, so starting
with general purpose, it's a balance of compute
229:18 - and compute memory and networking resources.
They're very good for web servers and code
229:22 - repository. So you're going to be very familiar
with this level here. Then you have compute
229:27 - optimized instances. So these are ideal for
compute bound applications that benefit from
229:33 - high performance processors. And as the name
suggests, this is compute it's going to have
229:38 - more computing power. Okay, so scientific
modeling, dedicated gaming servers, and ad
229:42 - server engines. And notice they all start
with C. So that makes it a little bit easier
229:46 - to remember. Then you have memory optimized
and as the name implies, it's going to have
229:50 - more memory on the server. So fast performance
for workloads that process large data sets
229:55 - in memory. So use cases in memory caches in
memory databases, Real Time big data analytics,
230:02 - then you have accelerated optimized instances,
these are utilizing hardware accelerators
230:07 - or co processors. They're going to be good
for machine learning, computational finance,
230:13 - seismic analysis, speech recognition, really
cool. Future tech uses a lot of accelerated
230:18 - optimized instances. And then you have storage
optimized. So this is for high sequential
230:23 - reads and write access to very large data
sets on local storage. Two use cases might
230:27 - be a no SQL database in memory or transactional
databases or data warehousing. So how is it
230:33 - important? How important is it to know all
these families, it's not so important to associate
230:38 - track at the professional track, you will
need to know themselves, all you need to know
230:42 - are these general categories and what and
just kind of remember, which, which fits into
230:47 - where and just their general purposes. All
right.
230:57 - So in each family of EC two instance types,
so here we have the T two, we're gonna have
231:02 - different sizes, and so we can see small,
medium, large x large, I just wanted to point
231:07 - out that generally, the way the sizing works
is you're gonna always get double of whatever
231:12 - the previous one was, generally, I say generally,
because it does vary. But the price is almost
231:17 - always double. Okay, so from small to medium,
you can see the ram has doubled, the CPU has
231:22 - doubled for medium large, isn't exactly doubled.
But for here, the CPU has doubled. Okay, but
231:28 - the price definitely definitely has doubled,
almost nearly so it's almost always twice
231:34 - inside. So general rule is, if you're wondering
when you should upgrade, if you need to have
231:39 - something, then you're better off just going
to the next version.
231:44 - So we're gonna look at the concept called
instance profile. And this is how your EC
231:50 - two instances get permissions. Okay? So instead
of embedding your AWS credentials, your access
231:56 - key and secret in your code, so your instance
has permissions to access certain services,
232:01 - you can attach a role to an instance via an
instance profile. Okay, so the concept here
232:07 - is you have any situ instance, and you have
an instance profile. And that's just the container
232:11 - for a role. And then you have the role that
actually has the permissions. Alright. And
232:17 - so I do need to point out that whenever you
have the chance to not embed a those credentials,
232:23 - you should never embed them. Okay, that's
like a hard rule with AWS. And anytime you
232:28 - see an exam question on that, definitely,
always remember that the way you set an instance
232:33 - profile tuneecu instance, if you're using
the wizard, you're going to see the IM role
232:38 - here. And so you're going to choose, you're
going to create one and then attach it. But
232:42 - there's one thing that people don't see is
they don't see that instance profile, because
232:46 - it's kind of like this invisible step. So
if you're using the console, it's actually
232:50 - going to create it for you. If you're doing
this programmatically through cloud formation,
232:54 - you'd actually have to create an instance
profile. So sometimes people don't realize
232:57 - that this thing exists. Okay. We're gonna
take a look here at placement groups, the
233:07 - placement groups let you choose the logical
placement of your instances to optimize for
233:11 - communication performance, or durability.
And placement groups are absolutely free.
233:15 - And they're optional, you do not have to launch
your EC two instance in within a placement
233:19 - group. But you do get some benefits based
on your use case. So let's first look at cluster
233:24 - so cluster PACs instances close together inside
an AZ and they're good for low latency network
233:30 - performance for tightly coupled node to node
communication. So when you want servers to
233:34 - be really close together, so communication
superfast, and they're well suited for high
233:38 - performance computing HPC applications, but
clusters cannot be multi az, alright, then
233:44 - you have partitions. And so partitions spread
instances across logical partitions. Each
233:49 - partition does not share the underlying hardware.
So they're actually running on individual
233:54 - racks here for each partition. They're well
suited for large distributed and replicated
233:59 - workloads, such as Hadoop, Cassandra, and
Kafka, because these technologies use partitions
234:04 - and now we have physical partitions. So that
makes total sense there, then you have spread
234:09 - and so spread is when each instance is placed
on a different rack. And so, when you have
234:15 - critical instances that should be kept separate
from each other. And this is the case where
234:21 - you use this and you can spread a max of seven
instances and spreads can be multi az, okay,
234:27 - whereas clusters are not allowed to go multi
AZ. So there you go. So user data is a script,
234:37 - which will automatically run when launching
easy to instance. And this is really useful
234:41 - when you want to install packages or apply
updates or anything you'd like before the
234:45 - launch of a an instance. And so when you're
going through the easy to wizard, there's
234:49 - this advanced details step where you can provide
your bash script here to do whatever you'd
234:55 - like. So here I have it installing Apache,
and then it starts that server. If you were
235:00 - Logging into an ECU instance. And you didn't
really know whether user data script was performed
235:07 - on that instance, on launch, you could actually
use the this URL at 169 24 169 24. If you
235:14 - were to curl that within that easy to instance,
with the user data, it would actually return
235:19 - whatever script was run. So that's just good
to know. But yeah, user data scripts are very
235:23 - useful. And I think you will be using one.
235:31 - So metadata is additional information about
your EC two instance, which you can get at
235:37 - runtime. Okay. So if you were to SSH into
your EC two instance, and run this curl command
235:42 - with latest metadata on the end, you're going
to get all this information here. And so the
235:47 - idea is that you could get information such
as the current public IP address, or the app
235:53 - ID that was used to watch the students, or
maybe the instance type. And so the idea here
235:58 - is that by being able to do this programmatically,
you could use a bash script, you could do
236:02 - somebody with user data metadata to perform
all sorts of advanced Ada staging operations.
236:07 - So yeah, better data is quite useful and great
for debugging. So yeah, it's time to look
236:17 - at the EC two cheat sheet here. So let's jump
into it. So elastic Compute Cloud EC two is
236:21 - a cloud computing service. So you configure
your EC two by choosing your storage, memory
236:26 - and network throughput, and other options
as well. Then you launch an SSH into your
236:31 - server within minutes. ec two comes in a variety
of instance types specialized for different
236:36 - roles. So we have general purpose, that's
for balance of compute memory and network
236:40 - resources, you have compute optimized, as
the name implies, you can get more computing
236:44 - power here. So ideal for compute bound applications
that benefit from high performance processors,
236:49 - then you have memory optimized. So that's
fast performance for workloads that process
236:53 - large data sets in memory, then you have accelerated
optimized hardware accelerators or co processors,
236:59 - then you have storage optimized that's high
sequential read and write access to very large
237:04 - data sets on local storage, then you have
the concept of instant sizes. And so instant
237:09 - sizes generally double in price and key attributes.
So if you're ever wondering when it's time
237:13 - to upgrade, just think when you're need double
of what you need that time to upgrade, then
237:19 - you have placement groups, and they let you
choose the logical placement of your instances
237:23 - to optimize communication performance, durability,
and placement groups are free, it's not so
237:27 - important to remember the types are because
I don't think we'll come up with a solution
237:31 - architect associate. And then we have user
data. So a script that will be automatically
237:35 - run when launching EC two instance, for metadata.
Metadata is about the current instance. So
237:41 - you could access this metadata via a local
endpoint when SSH into an EC two instance.
237:46 - So you have this curl command here with metadata,
and metadata could be the instance type, current
237:52 - IP address, etc, etc. And then the last thing
is instance profile. This is a container for
237:56 - an IM role that you can use to pass roll information
to an easy to instance, when the instance
238:01 - starts. Alright, so there you go, that's easy.
So we're gonna take a look at the EC two pricing
238:11 - model. And there are four ways we can pay
with EC two, we have on demand spot, reserved
238:17 - and dedicated. And we're going to go through
each section and see where each one is.
238:22 - We're gonna take first a look at on demand
pricing. And this is whenever you launch an
238:31 - EC, two instance, it's going to by default
use on demand, and so on demand has no upfront
238:36 - payment and no long term commitment. You're
only charged by the hour or by the man, it's
238:41 - going to vary based on ecsu instance type.
And that's how the pricing is going to work.
238:45 - And you might think, okay, what's the use
case here? Well, on demand is for applications
238:49 - where the workload is short term spike, you're
in predictable. When you have a new app for
238:54 - development, or you want to just run an experiment,
this is where on demand is going to be a good
238:58 - fit for you. Know, we're taking a look at
reserved instances, also known as RI and these
239:06 - are going to give you the best long term savings.
And it's designed for applications that have
239:12 - steady state predictable usage or require
reserved capacity. So what you're doing is
239:17 - you're saying to AWS, you know, I'm gonna
make a commitment to you. And I'm going to
239:21 - be using this over next period of time, and
they're going to give you savings. Okay, so
239:25 - this reduced pricing is going to be based
on three variables, we have term class offerings,
239:28 - and payment options. We'll walk through these
things to see how they all work. So for payment
239:33 - options, we have standard convertible and
scheduled standard is going to give us the
239:36 - greatest savings with 75% reduced pricing.
And this is compared to obviously to on demand.
239:43 - The thing here though, is that you cannot
change the ri attributes, attributes being
239:47 - like instance type, right? So whatever you
have, you're you're stuck with it. Now if
239:51 - you needed a bit more flexibility, because
you might need to have more room to grow in
239:55 - the future. You'd look at convertible so the
savings aren't gonna be as great. We're looking
239:59 - at it. To 54%. But now you have the ability
to let's say, change your instance type to
240:04 - a larger size, you can't go smaller, you can
always go larger. And you're, but you're going
240:09 - to have some flexibility there, then there's
scheduled, and this is when you need to reserve
240:13 - instance, for a specific time period, this
could be the case where you always have a
240:18 - workload that's predictable every single Friday
for a couple hours. And the idea is by telling
240:24 - you Ws that you're going to be doing out on
schedule, they will give you savings there
240:27 - that's going to vary. The other two things
is term and payment options. So the terms
240:32 - is how long are you willing to commit one
year or three year contract, the greater the
240:37 - terms, the greater the savings, and you have
payment options, so you have all the front
240:42 - partial upfront and no upfront, no friends,
the most interesting one, because you could
240:46 - say, you know, I'm going to use the server
for a year, and you and you'll just pay at
240:51 - the end of the month. And so that is a really
good way of saving money. Right off the bat,
240:57 - a lot of people don't seem to know that. So
you know, mix those three together. And that's
241:01 - going to change the the outcome there. And
I do here have a graphic to show you that
241:05 - you can select things and just show you how
they would estimate the actual cost for you.
241:13 - A couple things you wanted about reserved
instances that can be shared between multiple
241:16 - accounts within a single organization and
unreserved, our eyes can be sold in the reserved
241:22 - instance marketplace. So if you do buy into
one or through your contract, you're not fully
241:26 - out of luck, because you can always try to
resell it to somebody else who might want
241:30 - to use it. So there you go. Now we're taking
a look at spot instances, and they have the
241:39 - opportunity to give you the biggest savings
with 90% discount compared to on demand pricing.
241:44 - There are some caveats, though. So eight of
us has all this unused compute capacity, so
241:48 - they want to maximize utility of their idle
servers, it's no different than when a hotel
241:53 - offers discounts to fill vacant suites, or
when the plane offers discounts to fill vacant
241:59 - seats. Okay, so there's just easy two instances
lying around, it would be better to give people
242:04 - discounts than for them to do nothing. So
the only caveat though is that when you use
242:10 - spot instances, if another customer who wants
to pay on demand a higher price wants to use
242:17 - it. And they need to give that capacity to
that on demand user, this instance can be
242:23 - terminated at any given time, okay. And that's
going to be the trade off. So just looking
242:28 - at termination, termination conditions down
below, instances can be terminated by Avis
242:32 - at any time. If your instance is terminated
by AWS, you don't get charged for the partial
242:39 - hour of usage. But if you were to terminate
an instance, you will still be charged for
242:43 - any hour that it ran. Okay, so there you go.
That's the little caveat to it. But what would
242:50 - you use abundances for if it can if these
incidents could be interrupted at any time?
242:54 - Well, they're designed for applications that
have flexible Start and End Times or applications
242:58 - that are only feasible at very low compute
costs. And so you can see I pulled out the
243:04 - configuration graphic when you make spot.
So it's saying like, Is it for load balancing
243:09 - workloads, flexible workloads, big data workloads
are defined duration workloads. So you can
243:12 - see there is some definitions as to what kind
of utility you would have there. But there
243:19 - you are.
243:21 - So we're taking a look at dedicated host instances,
which is our most expensive option with EC
243:27 - two pricing models. And it's designed to meet
regulatory requirements when you have strict
243:32 - server bound licensing that won't support
multi tenancy or cloud deployments. So to
243:37 - really understand dedicated hosts, we need
to understand multi tenant versus single tenant.
243:43 - So whenever you launch an EC two instance,
and choosing on demand or or any of the other
243:48 - types beside dedicated hosts, it's multi tenant,
meaning you are sharing the same hardware
243:53 - as other AWS customers, and the only separation
between you and other customers is through
243:58 - virtualized isolation, which is software,
okay, then you have single tenant and this
244:03 - is when a single customer has dedicated hardware.
And so customers are separated through physical
244:09 - isolation. All right. And so to just compare
these two I think of multi tenant is like
244:14 - everyone living in an apartment, and single
tenant is everyone living in a house. Right?
244:19 - So, you know, why would we want to have our
own dedicated hardware? Well, large enterprises
244:25 - and organizations may have security concerns
or obligations about sharing the same hardware
244:29 - with other AWS customers. So it really just
boils down to that with dedicated hosts. It
244:37 - comes in an on demand flavor and a reserved
flavor. Okay, so you can save up to 70%. But
244:43 - overall, dedicated hosts is way more expensive
than our other EC two pricing. Now we're on
244:54 - to the CTU pricing cheat sheet and this one
is a two pager but we'll make our way through
244:58 - it. So EC two has four pricing models we have
on demand spot reserved instances also known
245:03 - as RI and dedicated looking first at on demand,
it requires the least commitment from you.
245:09 - It is low cost and flexible, you only pay
per hour. And the use cases here are for short
245:15 - term spiky, unpredictable workloads or first
time applications, it's going to be ideal
245:20 - when you want workloads that cannot be interrupted,
whereas in spot, that's when you can have
245:24 - interruption and we'll get to that here shortly.
So onto reserved instances, you can save up
245:29 - to 75% off, it's gonna give you the best long
term value. The use case here are steady state
245:35 - or predictable usage. You can resell unused
reserved instances and the reserved instance
245:40 - marketplace the reduced pricing is going to
be based off of these three variables terms
245:44 - class offering and payment option. So for
payment terms, we have a one year or a three
245:48 - year contract. With payment options, we can
either pay all upfront, partial upfront or
245:52 - no upfront. And we have three class offerings,
we have standard convertible and scheduled.
245:57 - So for standard we're gonna get up to 75%
reduced pricing compared to on demand. But
246:02 - you cannot change those ra attributes meaning
like, if you want to change to a larger instance
246:06 - type, it's not going to be possible, you're
stuck with what you have. If you wanted more
246:10 - flexibility we have convertible where you
can get up to 54% off, and you get that flexibility.
246:16 - As long as those ra attributes are greater
than or equal in value, you can change those
246:21 - values, then you have scheduled and this is
used. This is for reserved instances for specific
246:26 - time periods. So maybe you want to run something
once a week for a few hours. And the savings
246:30 - here are gonna vary. Now onto our last two
pricing models, we have spot pricing, which
246:36 - is up to 90% off, it's gonna give you the
biggest savings, what you're doing here is
246:40 - you're requesting spare computing capacity.
So you know, as we said earlier, it's like
246:45 - hotel rooms where they're just trying to fill
the vacant suites. If you are if you're comfortable
246:52 - with flexible Start and End Times spot price
is going to be good for you. The use case
246:56 - here is if you can handle interruptions, so
servers randomly stopping and starting, it's
247:01 - a very good use case is for non critical background
jobs. instances can be terminated by alias
247:07 - at any time, if your instance is terminated
by a device, you won't get charged for that
247:12 - partial hour of usage. If you terminate that
instance, you will be charged for any hour
247:17 - that it ran in, okay. And the last is dedicated
hosting, it's the most expensive option. And
247:22 - it's just dedicated servers, okay? And so
it can be it can be utilized in on demand
247:29 - or reserves you can save up to 70% off. And
the use case here is when you need a guarantee
247:34 - of isolette hardware. So this is like enterprise
requirements. So there you go, we made it
247:39 - all the way through ECP.
247:41 - Hey, this is Andrew Brown from exam Pro. And
we are looking at Amazon machine images ami
247:49 - eyes, which is a template to configure new
instances. So an ami provides the information
247:54 - required to launch an instance. So you can
turn your EC two instances into ami. So that
248:00 - in turn, you can create copies of your servers,
okay. And so an ami holds the following, it's
248:06 - going to have a template for the root volume
for the instance. And it's either going to
248:09 - be an EBS snapshot, or an instant store template.
And that is going to contain your operating
248:15 - system, your application server, your applications,
everything that actually makes up what you
248:19 - want your AR ami to be, then you have launch
permissions, the controls which AWS accounts
248:25 - can use for the AMI to launch instances, then
you have block device mapping, that specifies
248:32 - volumes to attach to the instances when it's
launched. Alright, and so now I just have
248:36 - that physical representation over here. So
you have your EBS snapshot, which is registered
248:41 - to an ami and then you can launch that ami
or make a copy of an ami to make another ami,
248:46 - okay, and analyze our region specific. And
we're going to get into that shortly. I just
248:55 - wanted to talk about the use cases of ami,
and this is how I utilize it. So ami is help
249:01 - you keep incremental changes to your OS application
code and system packages. Alright, so let's
249:05 - say you have a web application and or web
server, and you create an ami on it. And it's
249:10 - going to have some things you've already installed
on it. But let's say you had to come back
249:14 - and install Redis because you want to run
something like sidekick, or now you need to
249:18 - install image magic for image processing.
Or you need the cloudwatch agent because you
249:22 - wanted to stream logs from your EC two instance
to cloud watch. And that's where you're going
249:27 - to be creating those revisions, okay, and
it's just gonna be based on the names. amye
249:32 - is generally utilized with SYSTEMS MANAGER
automation. So this is a service which will
249:38 - routinely Patreon eyes with security updates.
And then you can bake those ami so they can
249:43 - quickly launch them, which ties into launch
configuration. So when you're dealing with
249:49 - auto scaling groups, those use launch configurations
and launch configurations have to have an
249:53 - ami. So when you attach an ami to launch configuration
and you update the launch configuration in
249:57 - your auto scaling group, it's going to roll
out Those updates to all those multiple instances.
250:02 - So just to give you like a bigger picture
how ami is tied into the AWS ecosystem.
250:10 - So I just quickly wanted to show you the Avis
marketplace. And so again, the marketplace
250:18 - lets you purchase subscriptions to vendor
maintained amies. There can also be free ones
250:22 - in here as well. But generally they are for
paid. And they come in additional costs on
250:27 - top of your UC to instance. So here if you
wanted to use Microsoft's deep learning ami,
250:33 - you couldn't, you'd have to pay whatever that
is per hour. But generally people are purchasing
250:39 - from the marketplace security hardened ami
is a very popular so let's say you had to
250:44 - run Amazon Linux and you wanted it to meet
the requirements of level one cis. Well, there
250:50 - it is in the marketplace, and it only costs
you point 02. I guess that's two cents per
250:55 - hour or $130 $130 per year. Yeah, so just
wanted to highlight.
251:05 - So when you're creating an ami, you can create
an ami from an existing ecsu instance, it's
251:09 - either running or stopped, okay, and all you
have to do to create an ami is drop down your
251:14 - actions, go to image and create image. And
that's all there is to it.
251:20 - Now we're gonna look
251:24 - at how we would go about choosing our ami
and so AWS has hundreds of ami as you can
251:28 - search and select from. And so they have something
called the community ami which are free ami
251:33 - is maintained by the community. And then we
also have the ad bus marketplace, which are
251:38 - free or paid ami is maintained by vendors.
And so here in front of me, this is where
251:43 - you would actually go select an ami. But I
wanted to show you something that's interesting
251:47 - because you can have an ami, so let's say
Amazon Lex two. And if you were to look at
251:52 - it in North Virginia, and compare it to it
in another region, such as Canada Central,
251:57 - you're going to notice that there's going
to be some variation there. And that's because
251:59 - amies even though they are the same they they
are different to meet the needs of that region.
252:05 - And so um, you know, you can see here for
Amazon looks to North Virginia, we can launch
252:10 - it in x86 or arm, but only in Canada central
is 64 bit. Alright, so the way we can tell
252:17 - that these API's are unique is that they have
ami IDs, so they're not one to one. And so
252:23 - am eyes are region specific. And so they will
have different ami IDs per region. And you're
252:29 - not going to just be able to take it an ID
from another region and launch it from another
252:33 - region, there is some some things you have
to do to get an ami to another region. And
252:38 - we will talk about that. The most important
thing here I just want to show you is that
252:42 - we do have hundreds of API's to choose from.
And there is some variation between regions.
252:47 - So when choosing ami that we do have a lot
of options open to us to filter down what
252:52 - it is that we're looking for. And so you can
see that we could choose based on our LS whether
252:57 - the root device type is EBS, or instant store,
whether it's for all regions, recurrent regions,
253:03 - or maybe the architecture, so we do have a
bunch of filtration options available to us.
253:07 - amies are categorized as either backed by
EBS or backed by instant store this is a very
253:12 - important option here. And you're going to
notice that in the bottom left corner on there,
253:18 - I just wanted to highlight because it is something
that's very important.
253:26 - So
253:27 - you can also make copies of your ami. And
this feature is also really important when
253:30 - we're talking about am eyes because they are
region specific. So the only way you can get
253:35 - an ami from one region to another is you have
to use the copy command. So you do copy ami
253:40 - and then you'd have the ability to choose
to send that ami to another region. So there
253:45 - we are. So we're onto the AMI cheat sheets.
Let's jump into it. So Amazon machine image,
253:56 - also known as ami provides the information
required to launch an instance am eyes are
254:00 - region specific. If you need to use an ami
in another region, you can copy an ami into
254:06 - the destination region via the copy ami command.
You can create amies from an existing UC two
254:12 - instance that's either running or stopped
then we have community amies and these are
254:17 - free ami is maintained by the community then
there are 80 of us marketplace. amies and
254:22 - these are free or paid subscription ami is
maintained by vendors am eyes have ami IDs,
254:29 - the same ami so if you take Amazon links to
will vary in both ami ID and options, such
254:35 - as the architecture option in different regions,
they are not exactly the same, they're different.
254:41 - Okay, an ami holds the following information,
it's going to have a template for the root
254:46 - volume for the instance. So that's either
going to be an EBS snapshot or an instance
254:49 - store template. And that will contain operation
system, the application server and application
254:55 - data Okay. And then you have the launch permissions
that controls which ad was accounts can be
255:01 - used with the AMI to launch instances, and
a blocked device mapping that specifies the
255:07 - volume to attach to the instances when it's
launched. So there is your MIT ci and good
255:14 - luck. Hey, this is Andrew Brown from exam
Pro. And we are looking at auto scaling groups.
255:21 - So auto scaling groups lets you set scaling
rules, which will automatically launch additional
255:25 - EC two instances or shutdown instances to
meet the current demand. So here's our introduction
255:31 - to auto scaling groups. So auto scaling groups
revia to as G contains a collection of EC
255:37 - two instances that are treated as a group
for the purpose of automatic scaling and management.
255:42 - And automatic scaling can occur via capacity
settings, health check replacements, or scaling
255:49 - policies, which is going to be a huge topic.
255:56 - So the simplest way to use auto scaling groups
is just to work with the capacity settings
256:00 - with nothing else set. And so we have desired
capacity, Min, and Max. Okay, so let's talk
256:06 - through these three settings. So for min is
how many easy two instances should at least
256:11 - be running, okay, Max is the number of easy
two instances allowed to be running and desired
256:17 - capacity is how many easy two instances you
ideally want to run. So when min is set to
256:22 - one, and let's say you had a new auto scaling
group, and you lost it, and there was nothing
256:27 - running, it would always it would spin up
one. And if that server died, for whatever
256:31 - reason, because when it was unhealthy, or
just crashed, for whatever reason, it's always
256:35 - going to spin up at least one. And then you
have that upper cap, where it can never go
256:40 - beyond two, because auto scaling groups could
trigger more instances. And this is like a
256:47 - safety net to make sure that you know, you
just don't have lots and lots of servers running.
256:51 - And desired capacity is what you ideally want
to run. So as you will try to get it to be
256:57 - that value. But there's no guarantee that
it will always be that value. So that's capacity.
257:04 - So another way that auto scaling can occur
with an auto scaling group is through health
257:11 - checks. And down here, we actually have two
types, we have EC two and lb. So we're gonna
257:16 - look at EC two first. So the idea here is
that when this is set, it's going to check
257:20 - the seatoun instance to see if it's healthy.
And that's dependent on these two checks,
257:24 - that's always performed on DC two instances.
And so if any of them fail, it's going to
257:29 - be considered unhealthy. And the auto scaling
group is going to kill that EC two instance.
257:34 - And if you have your minimum capacity set
to one, it's going to then spin up a new EC
257:40 - two instance. So that's the that's easy to
type. Now let's go look at the lb type. So
257:45 - for the lb type, the health check is performed
based on an E lb health check. And the E lb
257:50 - can perform a health check by pinging an end
like an endpoint on that server could be HTTP
257:56 - or HTTPS, and it expects a response. And you
can say I want a 200 back at this specific
258:01 - endpoint or so here. That's actually what
we do. So if you have a web app, you might
258:05 - make a HTML page called health check. And
it should return 200. And if it is, then it's
258:11 - considered healthy. If that fails, then the
auto scaling group will kill that EC two instance.
258:16 - And again, if your minimum is set to one is
going to spin up a healthy new EC two instance.
258:25 - final and
258:29 - most important way of scaling gets triggered
within an auto scaling group is scaling policies.
258:33 - And there's three different types of scaling
policies. And we'll start with target tracking
258:37 - scaling policy. And what that does is it maintains
a specific metric and a target value. What
258:43 - does that mean? Well, down here, we can choose
a metric type. And so we'd say average CPU
258:48 - utilization. And if it were to exceed our
target value, and we'd set our target value
258:52 - to 75%. Here, then we could tell it to add
another server, okay, whenever we're adding
258:58 - stuff, that means we're scaling out whenever
we are removing instances, we're moving servers,
259:02 - and we're scaling in okay. The second type
of scaling policy is simple scaling policy.
259:10 - And this scales when alarm is breached. So
we create whatever alarm we want. And we would
259:14 - choose it here. And we can tell it to scale
out by adding instances, or scale in by removing
259:20 - instances. Now, this scaling policy is no
longer recommended, because it's a legacy
259:24 - policy. And now we have a new policy that
is similar but more robust. To replace this
259:31 - one, you can still use it but you know, it's
not recommended and but still in the console.
259:35 - But let's look at the one that replaces it
called scaling policies with steps. So same
259:40 - concept you scale based on when alarm is breach,
but it can escalate based on the alarms value,
259:46 - which changes over time. So before where you
just had a single value here, we could say
259:51 - well, if we have this, this alarm and the
value is between one and two, then add one
259:58 - instance and then when it goes between two
And three, then add another instance, or when
260:02 - exceeds three to beyond, then add another
instance. So you can, it helps you grow based
260:08 - on that alarm, that alarm as it changes, okay.
So earlier I was showing you that you can
260:20 - do health checks based on l bees. But I wanted
to show you actually how you would associate
260:26 - that lb to an auto scaling group. And so we
have classic load balancers. And then we have
260:31 - application load balancer and network load
balancer. So there's a bit of variation based
260:35 - on the load bouncer how you would connect
it, but it's pretty straightforward. So when
260:40 - in the auto scaling group settings, we have
these two fields, classic load balancers and
260:43 - target groups. And for classic load balancers,
we just select the load balancer, and now
260:48 - it's associated. So it's as simple as that
it's very straightforward, but with the new
260:52 - ways that there's a target group that's in
between the auto scaling group and the load
260:56 - balancer, so you're associating the target
group. And so that's all there is to it. So
261:01 - that's how you associate. So to give you the
big picture on what happens when you get a
261:11 - burst of traffic, and auto scaling occurs,
I just wanted to walk through this architectural
261:15 - diagram with you. So let's say we have a web
server and we have one EC two instance running,
261:20 - okay, and all of a sudden, we get a burst
of traffic, and that traffic comes into roughly
261:24 - three, revenue three points to our application
load balancer application load balancer has
261:29 - a listener that sends the traffic to the target
group. And we have this these students since
261:33 - which is associated with that target group.
And we have so much traffic that it causes
261:37 - our CPU utilization to go over 75%. And once
it goes over 75%, because we had a target
261:45 - scaling policy attached, that said anything
above 75%, spin up a new instance. That's
261:50 - what the auto scaling group does. And so the
way it does is it uses a launch configuration,
261:55 - which is attached to the auto scaling group.
And it launches a new EC two instance. So
261:59 - that's just to give you the, like, full visibility
on the entire pipeline, how that actually
262:06 - works.
262:08 - So when you have an auto scaling group, and
it launches in institutions, how does it know
262:13 - what configuration to use to launch a new
new ECU essence, and that is what a launch
262:19 - configuration is. So when you have an auto
scaling group, you actually set what a launch
262:25 - configuration you want to use. And a launch
configuration looks a lot like when you launch
262:29 - a new EC two instance. So you go through and
you'd set all of these options. But instead
262:34 - of launching an instance, at the end, it's
actually just saving the configuration. Hence,
262:39 - it's called a launch configuration. A couple
of limitations around loss configurations
262:43 - that you need to know is that a launch configuration
cannot be edited once it's been created. So
262:49 - if you need to update or replace that launch
configuration, you need to either make a new
262:53 - one, or they have this convenient button to
clone the existing configuration and make
262:58 - some tweaks to it. There is something also
known as a launch template. And they are launching
263:05 - figurations, just but with versioning. And
so it's AWS is new version of lock configuration.
263:10 - And you know, generally when there's something
new, I might recommend that you use it, but
263:15 - it seems so far that most of the community
still uses launch configuration. So the benefit
263:21 - of versioning isn't a huge, doesn't have a
lot of value there. So, you know, I don't
263:27 - I'm not pushing you to use launch templates,
but I just want you to know the difference
263:31 - because it is a bit confusing because you
look at it, it looks like pretty much the
263:34 - same thing. And it just has version in here
and we can review the auto scaling group cheat
263:43 - sheet. So an S g is a collection of up to
two instances group for scaling and management
263:48 - scaling out is when you add servers scaling
is when you remove servers scaling up is when
263:54 - you increase the size of an instance so like
you'd update the launch configuration with
263:57 - a larger size. The size of an ASC is based
on the min max and desired capacity. Target
264:03 - scaling policy scales based on when a target
value of a metric is breached. So example
264:08 - average CPU utilization exceeds 75% simple
scaling policy triggers a scaling when an
264:13 - alarm is breach. Scaling policy with steps
is the new version simple scaling policy allows
264:18 - you to create steps based on escalation alarm
values. desired capacity is how many instances
264:24 - you want to ideally run. An ESG will always
launch instances to meet the minimum capacity.
264:30 - health checks determine the current state
of an instance in nasg. health checks can
264:34 - be run against either an EOB or needs to do
instance, when an auto scaling when an auto
264:40 - scaling group launches a new instance it will
use a launch configuration which holds the
264:44 - configuration values of that new instance.
For example, maybe the AMI instance type of
264:49 - roll launch configurations cannot be edited
and must be cloned or a new one created. Launch
264:55 - configurations must be manually updated in
by editing the auto scaling group. setting.
265:00 - So there you go. And that's everything with
auto scale. Hey, it's Andrew Brown from exam
265:09 - Pro. And we are looking at elastic load balancers,
also abbreviated to lb, which distributes
265:15 - incoming application traffic across multiple
targets such as you see two instances containers,
265:20 - IP addresses or lambda functions. So let's
learn a little bit What a load bouncer is.
265:24 - So load balancer can be physical hardware
or virtual software that accepts incoming
265:29 - traffic and then distributes that traffic
to multiple targets. They can balance the
265:34 - load via different rules. These rules vary
based on the type of load balancers. So for
265:39 - elastic load balancer, we actually have three
load balancers to choose from. And we're going
265:44 - to go into depth for each one, we'll just
list them out here. So we have application
265:47 - load balancer, network, load balancer, and
classic load balancer.
265:54 - Understand the flow of traffic for lbs, we
need to understand the three components involved.
266:00 - And we have listeners rules and target groups.
And these things are going to vary based on
266:05 - our load balancers, which we're going to find
out very shortly here. Let's quickly just
266:08 - summarize what these things are. And then
see them in context with some visualization.
266:13 - So the first one our listeners, and they listen
for incoming traffic, and they evaluate it
266:19 - against a specific port, whether that's Port
80, or 443, then you have rules and rules
266:26 - can decide what to do with traffic. And so
that's pretty straightforward. Then you have
266:31 - target groups and target groups is a way of
collecting all the easy two instances you
266:36 - want to route traffic to in logical groups.
So let's go take a look first at application
266:41 - load bouncer and network load balancer. So
here on the right hand side, I have traffic
266:45 - coming in through repartee three, that points
to our load balancer. And once it goes, our
266:50 - load balancer goes to the listener, it's good
check what port it's running on. So if it's
266:54 - on port 80, I have a simple rule here, which
is going to redirect it to Port 443. So it's
267:00 - gonna go this listener, and this listener
has a rule attached to it, and it's going
267:04 - to forward it to target one. And that target
one contains all these two instances. Okay.
267:11 - And down below here, we can just see where
the listeners are. So I have listener at 443.
267:15 - And this is for application load balancer,
you can see I also can attach a SSL certificate
267:21 - here. But if you look over at rules, and these
rules are not going to appear for network
267:26 - load balancer, but they are going to appear
for a lb. And so I have some more complex
267:30 - rules. If you're using lb, it simply just
forwards it to a target, you don't get more
267:36 - rich options, which will show you those richer
options in a future slide. But let's talk
267:42 - about classic load balancer. So classic load
balancer is, is much simpler. And so you have
267:47 - traffic coming in it goes to CLB. You have
your listeners, they listen on those ports,
267:52 - and you have registered targets. So there
isn't target groups, you just have Lucy see
267:59 - two instances that are associated with a classic
load balancer. Let's take a deeper look at
268:09 - all three load balancers starting with application
load balancer. So application load balancer,
268:14 - also known as a lb is designed to balance
HTTP and HTTPS traffic. It operates at layer
268:21 - seven of the OSI model, which makes a lot
of sense because layer seven is application
268:26 - lb has a feature called request routing, which
allows you to add routing rules to your listeners
268:31 - based on the HTTP protocol. So we saw previously,
when we were looking at rules, it was only
268:37 - for a lb that is this is that request routing
rules, you can attach a web application firewall
268:43 - to a lb. And that makes sense because they're
both application specific. And if you want
268:49 - to think of the use case for application load
balancer, well, it's great for web applications.
268:58 - So now let's take a look at network load balancer,
which is designed to balance TCP and UDP traffic.
269:04 - It operates at the layer four of the OSI model,
which is the transport layer, and it can handle
269:09 - millions of requests per second while still
maintaining extremely low latency. It can
269:13 - perform cross zone load balancing, which we'll
talk about later on. It's great for you know,
269:19 - multiplayer video games, or when network performance
is the most critical thing to your application.
269:30 - Let's take a look at classic load balancers.
So it was AWS is first load balancer. So it
269:34 - is a legacy load balancer. It can balance
HTTP or TCP traffic, but not at the same time.
269:41 - It can use layer seven specific features such
as sticky sessions. It can also use a strict
269:46 - layer for for bouncing purely TCP applications.
So that's what I'm talking about where it
269:52 - can do one or the other. It can perform cross
zone load balancing, which we will talk about
269:57 - later on and I put this one in here. Because
it is kind of an exam question, I don't know
270:02 - if it still appears, but it will respond with
a 504 error in case of timeout if the underlying
270:08 - application is not responding. And an application
can be not respond spawning would be example
270:12 - as the web server or maybe the database itself.
So classic load balancer is not recommended
270:18 - for use anymore, but it's still around, you
can utilize it. But you know, it's recommended
270:23 - to use nlb or lb, when possible.
270:32 - So let's look at the concept of sticky sessions.
So sticky Sessions is an advanced load balancing
270:37 - method that allows you to bind a user session
to a specific EC two instance. And this is
270:43 - useful when you have specific information
that's only stored locally on a single instance.
270:47 - And so you need to keep on sending that person
to the same instance. So over here, I have
270:52 - the diagram that shows how this works. So
on step one, we wrote traffic to the first
270:59 - EC two instance, and it sets a cookie and
so the next time that person comes through,
271:03 - we check to see if that cookie exists. And
we're gonna send it to that same EC two instance.
271:08 - Now, this feature only works for classic load
balancer and application load balancer, it's
271:13 - not available for nlb. And if you need to
set it for application load balancer, it has
271:20 - to be set on the target group and not individually
easy to instance. So here's a scenario you
271:30 - might have to worry about. So let's say you
have a user that's requesting something from
271:35 - your web application, and you need to know
what their IP address is. So, you know, the
271:40 - request goes through and then on the EC two
instance, you look for it, but it turns out
271:44 - that it's not actually their IP address. It's
the IP address of the load balancer. So how
271:49 - do we actually see the user's IP address?
Well, that's through the x forwarded for header,
271:55 - which is a standardized header when dealing
with load balancers. So the x forwarded for
272:01 - header is a command method for identifying
the originating IP address of a connecting,
272:06 - or client connecting to a web server through
HTTP proxy or a load balancer. So you would
272:10 - just forward make sure that in your web application
that you're using that header, and then you
272:17 - just have to read it within your web application
to get that user's IP address. So we're taking
272:26 - a look at health checks for elastic load balancer.
And the purpose behind health checks is to
272:31 - help you route traffic away from unhealthy
instances, to healthy instances. And how do
272:37 - we determine if a instances unhealthy waltz
through all these options, which for a lb
272:42 - lb is set on the target group or for classic
load balancers directly set on the load balancer
272:47 - itself. So the idea is we are going to ping
the server at a specific URL at a with a specific
272:54 - protocol and get an expected specific response
back. And if that happens more than once over
273:01 - a specific interval that we specify, then
we're going to mark it as unhealthy and the
273:05 - load balancer is not going to send any more
traffic to it, it's going to set it as out
273:09 - of service. Okay. So that's how it works.
One thing that you really need to know is
273:15 - that e lb does not terminate unhealthy instances
is just going to redirect traffic to healthy
273:21 - instances. So that's all you need to know.
So here, we're taking a look at cross zone
273:29 - load balancing, which is a feature that's
only available for classic and network load
273:33 - balancer. And we're going to look at it when
it's enabled, and then when it's disabled
273:36 - and see what the difference is. So when it's
enabled, requests are distributed evenly across
273:41 - the instances in all the enabled availability
zones. So here we have a bunch of UC two instances
273:46 - in two different Z's, and you can see the
traffic is even across all of them. Okay?
273:51 - Now, when it's disabled requests are distributed
evenly across instances, it's in only its
273:57 - availability zone. So here, we can see in
az a, it's evenly distributed within this
274:04 - AZ and then the same thing over here. And
then down below, if you want to know how to
274:09 - enable cross zone load balancing, it's under
the description tab, and you'd edit the attributes.
274:13 - And then you just check box on cross zone
load balancing. Now we're looking at an application
274:23 - load balancer specific feature called request
routing, which allows you to apply rules to
274:27 - incoming requests, and then for to redirect
that traffic. And we can check on a few different
274:32 - conditions here. So we have six in total.
So we have the header host header source IP
274:37 - path is to be header is to be header method,
or query string. And then you can see we have
274:42 - some then options, we can forward redirect
returned to fixed response or authenticate.
274:47 - So let's just look at a use case down here
where we actually have 1234 or five different
274:53 - examples. And so one thing you could do is
you could use this to route traffic based
274:57 - on subdomain. So if you want an app to sub
domain app to go to Target, prod and QA to
275:03 - go to the target QA, you can do that you can
either do it also on the path. So you could
275:07 - have Ford slash prod and foresights qa and
that would route to the respective target
275:11 - groups, you could do it as a query string,
you could use it by looking at HTTP header.
275:18 - Or you could say all the get methods go to
prod on a why you'd want to do this, but you
275:22 - could and then all the post methods would
go to QA. So that is request request routing
275:27 - in a nutshell.
275:32 - We made it to the end of the elastic load
balancer section and on to the cheat sheet.
275:37 - So there are three elastic load balancers,
network application and classic load balancer.
275:42 - an elastic load balancer must have at least
two availability zones for it to work. Elastic
275:48 - load balancers cannot go cross region you
must create one per region lbs have listeners
275:54 - rules and target groups to route traffic and
OBS have listeners and target groups to route
275:59 - traffic. And CL B's use listeners and EC two
instances are directly registered as targets
276:05 - to the CLB. For application load balancer,
it uses HTTP s or eight or without the S traffic.
276:13 - And then as the name implies, it's good for
web applications. Network load balancer is
276:17 - for TCP, UDP, and is good for high network
throughput. So think maybe like multiplayer
276:22 - video games, classic load balancer is legacy.
And it's recommended to use a lb, or nlb when
276:29 - you can, then you have the x forwarded for.
And the idea here is to get the original IP
276:35 - of the incoming traffic passing through the
lb, you can attach web application firewall
276:40 - to a lb. Because you know web application
firewall has application the name and nlb
276:45 - and CLB do not, you can attach an Amazon certification
manager SSL certificate. So that's an ACM
276:53 - to any of the L B's for to get SSL. For a
lb you have advanced request routing rules,
277:01 - where you can route based on subdomain, header
path and other SP information. And then you
277:07 - have sticky sessions, which can be enabled
for CLB or lb. And the idea is that it helps
277:12 - the session remember, what would you say to
instance, based on cookie.
277:18 - All right, so it's time to get some hands
on experience with EC two and I want you to
277:25 - make your way over to the ECG console here
by going up to services and typing EC two
277:32 - and click through and you should arrive at
the EC to dashboard. Okay, I want you to take
277:38 - a look on the left hand side here because
it's not just easy two instances that are
277:42 - under here, we're going to have ami eyes.
Okay, we're going to have elastic block store,
277:48 - we're going to have some networking security.
So we have our security groups, our elastic
277:52 - IPS are key pairs, we're going to have load
balancing, we're going to have auto scaling,
277:56 - okay, so a lot of times when you're looking
for these things, they happen to be under
278:00 - the sea to console here, okay, but um, now
that we've familiar, familiarize ourselves
278:07 - with the overview here, let's actually go
ahead and launch our first instance. Alright,
278:13 - so we're gonna proceed to launch our first
instance here, and we're gonna get this really
278:17 - nice wizard here. And we're gonna have to
work our way through these seven steps. So
278:22 - let's first start with choosing your ami,
your Amazon machine image. And so an ami,
278:26 - it's a template that contains the software
configuration. So the operating system, the
278:30 - application server and applications required
to launch your instance, right. And we have
278:36 - some really same choices here we have Amazon
Linux two, which is my go to, but if you wanted
278:42 - something else, like red hat or or Susi or
Ubuntu or or Microsoft Windows, eight of us
278:49 - has a bunch of amies that they support they
manage, okay, and so if you were to pay for
278:56 - AWS support, and you were to use these ami
is you're gonna get a lot of help around them.
279:01 - Now if you want more options, besides the
ones that are provided from AWS, there is
279:05 - the marketplace and also community ami. So
if we go to community am eyes, and we're just
279:09 - going to take a peek here, we can see that
we can filter based on less than architecture
279:15 - etc. But let's say we wanted a WordPress ami,
something was pre loaded with WordPress, we
279:20 - have some here. And if we wanted to get a
paid paid one, one that is actually provided
279:27 - by a vendor and that they support it and you
pay some money to make sure that it's in good
279:31 - shape, maybe as good security or just kept
up to date. You can do that. So here's WordPress
279:35 - by bitnami. Alright, and that's a very common
one to launch for WordPress on here. But you
279:40 - know, we're just going to stick to the defaults
here and go back to the Quickstart and we're
279:44 - going to launch an Amazon Linux two ami, alright,
and we'll just proceed to a clicking select
279:50 - here. So now we're on to our second option,
which is to choose our instance type, okay,
279:56 - and this is going to determine how many CPUs
you're going to be using. realizing how much
280:00 - memory you're going to have are going to be
backed by an EBS or instant store. And also
280:05 - Are there going to be limitations around our
network performance. And so you can filter
280:10 - based on the types that you want. And we learned
this earlier that there are a bunch of different
280:14 - categories, we're going to stay in the general
purpose family here, which are the first that
280:18 - are listed, we're always going to see T two
and T two micro is a very same choice and
280:22 - also a free choice if we have our free tier
here. Okay, and so we will select it and proceed
280:30 - to instance details. So now it's time to configure
our instance. And the first option available
280:38 - to us is how many instances we want to run.
So if you wanted to launch 100, all at once,
280:43 - you can do so and put it in an auto scaling
group, or they were going to stick with one
280:46 - to be cost effective, then we have the option
here to turn this into a spot instance. And
280:53 - that will help us save a considerable amount
of money. But I think for the time being we
280:57 - will stick with on demand. Next we need to
determine what VPC and subnet we're going
281:04 - to want to launch this into. I'm going to
stick with the default VPC and the default
281:08 - sub network, I should say it will pick one
at random for me, we're definitely going to
281:12 - want to have a public IP address. So we're
going to allow that to be enabled, you could
281:18 - put this easy to insert into a placement group,
you'd have to create a placement group first.
281:23 - But we're not going to do that because we
don't have the need for it. And we're going
281:26 - to need an IM role. So I'm going to go ahead
here and right click and create a new tab
281:30 - here and create a new Im role for us to give
this easy to some permissions. And so we're
281:36 - gonna go to UC to here, we're going to go
next, I want you to type in SSM for system,
281:42 - simple SYSTEMS MANAGER. And we're going to
get this one role for SSM. We hit Next, we're
281:47 - going to go next to review, I'm going to say
my EC to EC to URL, okay. And we're gonna
281:55 - create that role, close this tab, and we're
gonna hit that refresh there and then associate
281:59 - this Im role to our instance here. The reason
I did that, and is because I want us to have
282:06 - the ability to use simple SYSTEMS MANAGER
sessions manager, because there are two different
282:11 - ways that we can log into our EC two instance,
after it is launched, we can SSH into it,
282:16 - or we can use SYSTEMS MANAGER in order to
use this sessions manager, we're going to
282:20 - have to
282:21 - have that Im role with those permissions,
the default behavior will it will shut down,
282:27 - that's or do a stop, that's good to me. If
we wanted detailed monitoring, we could turn
282:31 - that on would cost additional, if we want
to protect against accidental accidental termination,
282:36 - that is also a very good option. But again,
this is a test here. So we'll probably be
282:40 - tearing this down pretty quick. So we don't
need to have that enabled, then we have Tennessee
282:45 - option. So if we get a dedicated host, that's
going to be very expensive. But that'd be
282:50 - very good if you are on, you know, an enterprise
organization that has to meet certain requirements.
282:56 - So there are use cases for that. And the last
option here is under the advanced details,
283:01 - you might have to drop this down and open
it. And this is that script that allows us
283:06 - to set something up initially when the server
launches. And we have a script here that we
283:11 - want to run. So I'm going to just copy paste
that in there. And what it's going to do is
283:15 - when the EC two instance launches, it's going
to set us up without a proxy server and started
283:20 - that server. So we have a very basic website.
Okay. And so yeah, we're all done here, we
283:26 - can move on to storage. So now we're going
to look at adding storage to our EC two instance.
283:31 - And by default, we're going to always have
a root volume, which we cannot remove. But
283:36 - if we wanted to add additional volumes here
and choose their, their mounting directory,
283:41 - we could do so there. And we do have a lot
of different options available to us for volume
283:47 - types. But we only want one for the sake of
this tutorial. So we'll just remove that we
283:53 - can set the size, we're gonna leave it to
eight. If we want to have this root volume
283:59 - persist on the case that the CPU is terminated,
so the volume doesn't get deleted, we can
284:03 - uncheck box, this is a very good idea. In
most cases, you want to keep that unchecked.
284:10 - But for us, we want to make cleanup very easy.
So we're going to keep that on. And then we
284:14 - have encryption. And so we can turn on encryption
here using kms. The default key, we're just
284:20 - going to leave it off for the time being and
yeah, we'll proceed to tags here. And we're
284:25 - just gonna skip over tags. Now tags are good
to set. Honestly, I just I'm very lazy, I
284:31 - never set them. But if you want to group your
resources across a city or anything, you want
284:36 - to consistently set tags, but we're just gonna
skip that and go on to security groups. So
284:42 - now we need to configure some security groups
here for this easy to instance. And then we
284:47 - don't have any, so we're gonna have to create
a new one, but we could choose an existing
284:50 - one if there was one, but we're gonna go ahead
and name it here because I really don't like
284:55 - the default name. So we'll just say my SG
four For EC two, okay, and we're gonna set
285:04 - some inbound rules here. And so we have SSH
and that stuff, something that we definitely
285:09 - want to do, I'm going to set it to my IP,
because I don't want to have the ability for
285:14 - anyone to SSH into this instance, I want to
really lock it down to my own. We're gonna
285:18 - add another instance here, because this is
running an Apache server. And we do want to
285:22 - expose the Port 80 there, so I'm going to
drop that down, it's going to automatically
285:27 - like Port 80. And we do want this to be internet
accessible. But if we want to be very explicit,
285:31 - I'm just gonna say anywhere, okay, and I'm
gonna make a note here, just saying, like
285:35 - my home, and, you know, for paci, you know,
it doesn't hurt to put these notes in here.
285:42 - And then we'll go ahead and review. So now
it's time to just review and make sure everything
285:48 - we set in the wizard here is what we want
it to be. So you know, we just review it.
285:53 - And if you're happy with it, we can proceed
to launch. So now when you hit launch, it's
285:57 - going to ask you to create a key pair, okay.
And this is going to be so that we can actually
286:02 - SSH into the instance and access it. So I'm
going to drop down here, I'm going to create
286:06 - a new key pair pair here. And I'm going to
call it o m, I'm going to call it my EC two.
286:14 - And I'm going to go ahead and download that
pair there. And we're going to go ahead and
286:18 - launch that instance. Alright. And so now
it says that that instance, is being created.
286:24 - So we can go ahead and click on the name here.
And so now it is spinning up, I do suggest
286:29 - that you do on tick here so that we can see
all of our ECS instances. And what we're going
286:33 - to be waiting for is to move from a pending
state into a green state, I forget the name
286:38 - of it, I guess enabled or created. And then
after it goes green, we're going to be waiting
286:44 - for our two status checks to pass. And if
those two status checks pass, which will show
286:49 - up under here, we have our system says check
out our instance status check. That means
286:54 - the instance is going to be ready and available,
it's going to run that user data script that
286:59 - we have. So once again, at one with these
two checks are done. It's Oh, sorry, green
287:03 - is running. Great. So now we're waiting for
these checks. And once they are complete,
287:08 - we're going to be able to use our public IP
address here, either this one here, or even
287:12 - this DNS record, and we're going to see if
our, our
287:17 - if our, our servers running. Okay, so if that
works, then we will be in good shape here.
287:23 - Okay, so we're just going to wait for that
session. So our two checks have passed, meaning
287:33 - that our instance is now ready to attempt
to access via either the public IP or the
287:39 - public DNS record, we can use either or I
like using the public DNS record. And so I'm
287:43 - just going to copy it using the little clipboard
feature there, make a new tab in my browser,
287:48 - and paste it in there. And I'm going to get
the Apache test page, meaning that our user
287:53 - data script worked, and it's successfully
installed and started Apache here. So that's
287:57 - all great. Now, um, so now, you know, our
instances are in good working order. But let's
288:03 - say we had to log into our instance to debug
it or do something with it. That's where we're
288:08 - gonna have to know how to either SSH in, which
is usually the method that most people like
288:12 - to do. Or we can use simple systems sessions
manager, which is the recommended way by AWS.
288:18 - Okay, so I'm gonna show you both methods,
methods here, starting with sessions manager.
288:24 - But just before we do that, I just want to
name our instance something here to make our
288:28 - lives a little bit easier here, you just have
to click the little pencil there and rename
288:32 - it. So I'm just rename it to my server. And
we'll pop down services here and type SSM
288:38 - for simple SYSTEMS MANAGER and make a new
tab here, click off here. So we have our space
288:45 - here, and we will go to the SYSTEMS MANAGER
console. Now, on the left hand side, we are
288:51 - looking for sessions manager, which is all
the way down here. And we're just going to
288:55 - click that. And we are going to start a new
session. So we're going to choose the instance
289:00 - we want to connect to. So just hit start instance
here. Okay. And here, we have my server, which
289:08 - is the instance we want to connect to, we'll
hit start session, and it will gain access
289:12 - to this instance, immediately. So there is
next to no wait. And we are in this instance,
289:17 - the only thing I don't like is that it logs
you in as the root user, which to me is very
289:21 - over permissive. But we can get to the correct
user here. For for Amazon links to instances,
289:27 - it always has an EC to user, that is the user
that you want to be doing things under. So
289:31 - I'm just going to switch over to that user
here just quickly here. Okay, and so now I'm
289:37 - the correct user and I can go about doing
whatever it is that I want to do here. Okay.
289:42 - Right. So, so there you go. So that's the
sessions manager, I'm just going to terminate
289:49 - here to terminate that instance. And again,
the huge benefit here is that you get sessions
289:54 - history so you can get a history of who has
logged in and done like gone into server to
290:00 - do something. And, you know, the other thing
is that you don't have to share around that
290:05 - key pair. Because when you launch a sudo instance,
you only have that one key pair, and you really
290:09 - don't want to share that around. So this does
remove that obstacle for you. Also, because
290:16 - people have to log in the console, there are
people leave your company, you know, you're
290:20 - also denying them access there, and you don't
have to retract that key pair from them. So
290:25 - it is a lot easier to use sessions manager.
The downside, though, is that it just has
290:31 - a very simplistic terminal within the browser.
So if you're used to more rich features from
290:36 - your, your OS terminal, that is a big downside.
That's why people still SSH in, which is now
290:43 - the next method we are going to use to gain
access to our EC two instance. So in order
290:49 - to do that, we are going to need a terminal
Okay, and I just moved that, that key pair
290:54 - onto my desktop, when you download, it probably
went to your download. So just for convenience,
290:59 - I've moved it here. And what we're going to
do is we are going to use our SSH command
291:04 - here, and we're going to log in as the aect
user because that's the user you should be
291:08 - logging
291:09 - in as, as soon as you have to log in as when
you SSH into it, we're just going to grab
291:12 - the public IP, now we could use the public
291:15 - DNS record, but the IP address is a bit shorter
here, so it's a bit nicer. And then we're
291:19 - gonna use a hyphen flag to specify the private
key that we want to pass along. That's on
291:25 - our desktop here, I'm actually already on
the desktop, so I don't have to do anything
291:29 - additional here. So we're just going to pass
along, so we're gonna hit enter, okay. And
291:35 - we're just going to wait here, and we got
a Permission denied. Now, if this is the first
291:38 - time you've logged the server, it might ask
you to for a fingerprint where you will type
291:42 - in Yes. Okay, it didn't ask me that. So that's
totally fine. But you're going to see that
291:47 - it's giving me a 644, because the the private
key is to open so it is required that your
291:54 - private key files are not accessible by others.
So database wants to really make sure that
291:58 - you lock down those permissions. And so if
we just take an LS hyphen, Li here, we can
292:03 - see that it has quite a few permissions here.
And we could just lock that down by typing
292:07 - chmod 400. Okay. And then if we just take
a look here, again, now it's locked down here.
292:14 - So if we try to SSH in again, okay, we should
have better luck, this time around. It's not
292:19 - as fast as sessions manager, but it will get
you in the door there. There we are. And we
292:23 - are logged in as the user, okay, and we'd
go about our business doing whatever it is
292:28 - that we'd want to do. Now, we did talk about
in the, the the actual journey about user
292:34 - data metadata, and this is the best opportunity
to take a look there. So we have this private,
292:41 - this private addresses only accessible when
you're inside your EC two instance. So you
292:45 - can gain additional information. First one
is the user data one, okay, so if I just was
292:49 - to paste that in there, and it's just curl
a cp 169 254, once it's done to the four latest
292:55 - user data, and if I were to hit enter, it
will return the script that actually was a
293:01 - performed on launch. Okay, so if you were
to debug and easy to instance, and it wasn't,
293:06 - you know, you're like working for another
company, and you didn't launch that instance,
293:08 - and you really wanted to know, what was, uh,
what was performed on launch, you could use
293:13 - that to find out, then we have the metadata
endpoint, and that is a way for us to get
293:18 - a lot of different rich information. So it's
the same thing, it's just gonna have metadata
293:21 - on the end there with with a Ford slash, and
you have all these different options. Okay.
293:27 - So let's say we wanted to get the IP address
here of this instance, we could just append
293:32 - IP public ipv4 on there, okay. And there you
go. So, you know, that is how you log into,
293:40 - into an instance via SSH or, or such sessions
manager. And that's how we get some user data
293:45 - and metadata after the fact. So when we had
launched a cc two instance, we didn't actually
293:54 - encrypt the root volume, okay. And so if you
were to launch an instance, I just want to
294:00 - quickly just show you what I'm talking about
here. And we go to storage here. We just didn't
294:06 - we didn't go down here and select this. Alright,
so let's say we had to retro actively apply
294:10 - encryption, well, it's not as easy as just
a drop down here, we have to go through a
294:15 - bunch of steps, but we definitely need to
know how to do this. Okay, so how would we
294:18 - go about that? Well, we go to a volumes on
the left hand side here, and we'd find our
294:24 - running volume. So here is the volume that
we want to encrypt that is unencrypted. And
294:30 - so what we do is we would first create a snapshot
of it, okay. And so I would say, um, we'll
294:37 - say my volume. Okay. We'll go ahead and create
that snapshot. And we'll go back here, and
294:44 - we're just going to wait for this snapshot
to complete it's going to take a little bit
294:50 - of time here. But once it comes back here,
we'll move on to the next step. So our progress
294:55 - is at now at 100%. And we can see that this
snapshot is unencrypted. Okay, so what we're
295:01 - gonna do is we're gonna go up to actions at
the top here, make a copy of the snapshot.
295:06 - And this is where we're gonna have the ability
to apply encryption. Alright, and we're just
295:10 - going to use the default kms key here, which
is a pretty good default to use. And we're
295:15 - gonna hit copy. And that's going to now initiate
a copy here. So we'll just visit our snapshots
295:21 - page ever. Now we're just going to wait for
this to create our copy. So our progress is
295:27 - at 100%. It's encrypted, even though it's
it's pending and 0% down there. Sometimes
295:32 - if you hit refresh, that will just get your
interface up to date here. I'm just going
295:36 - to turn this off here. So you can see that
we have our volume here. And then we have
295:41 - our, our snapshot there. I don't really like
the name of the description, I wonder if I
295:45 - can change that. No, so that's fine, though.
But anyway, we now have our unencrypted and
295:52 - our encrypted volume. So if we wanted to have
this encrypted one launch, all we're gonna
295:57 - have to do here is launch the seats aect,
or sorry, create a an image from this volume
296:04 - here. So I'm just gonna hit Create image.
And this is
296:08 - going to create an image from our EBS snapshot,
okay, so that's going to be our ami. And I'm
296:13 - just gonna say, my server here, okay. And,
yeah, this all looks good to me. And we'll
296:21 - just go ahead and hit Create. Alright, and
we'll click through here to our ami. And so
296:27 - we're just going to wait here, actually, I
think it's instantly created. So our ami is
296:32 - ready to launch. Okay. So if we want to now
have our server with a version that is encrypted,
296:39 - we can just go ahead here and launch a new
instance. And it's going to pull up our big
296:43 - interface here. And we'll just quickly go
through it. So t to micro is good. We have
296:49 - one instance, we're going to drop down to
our, our AC t roll here, we're going to have
296:54 - to again, copy our startup script here. Okay.
I'm actually I guess not, because if we if
297:02 - we created a snapshot of our instance, this
would already be installed. So we don't have
297:06 - to do that again. So that's good. And then
we'll go to our storage, and there is our
297:10 - encrypted volume. Okay, we'll go to security
groups, and we're just going to select that
297:14 - existing one there. And then we're gonna go
to review, and we're going to launch, okay,
297:19 - and we're gonna choose the existing one. So
we'll say launch instance. Okay. And we'll
297:24 - go back here, and we just check box off here,
we're gonna actually have two instances running,
297:28 - you can see I have a few terminated ones,
those are just old ones there. But this is
297:32 - the one we have running here. So once this
is running, we'll have this here. And we'll
297:37 - just double check to make sure it's working.
But we'll talk about, you know, how do we
297:41 - manage launching multiple instances here next.
So our new instance is now a running, I'm
297:51 - just going to name it to my new server. So
we can distinguish it from our old one. And
297:56 - I want to see if that root device is actually
encrypted, because that was our big goal here.
298:01 - And so we're going to open this up in a new
tab here and look at our, our volume and is
298:07 - indeed encrypted. So we definitely were successful
there. Now, the main question, is our server
298:12 - still running our Apache test page here? So
I'm going to grab the new IP for the new server
298:18 - and take a look here, and we're going to find
that Apache isn't running. So you know, what
298:22 - happened? Why is it not working? And there's
a really good reason for that. If we go over
298:25 - to our script, when we first use this user
data script on our first server, what it did
298:31 - was it installed Apache, and then it started
Apache, okay. But that doesn't mean that it
298:37 - kept Apache running if there was a restart.
So the thing is, is that when we when we made
298:43 - an a, an ami, or like a copy of our volume,
it had the installed part, but there's nothing
298:50 - that says on launch, start up, start Apache,
okay, so what we need to do is we need to
298:56 - enable this command, which will always have
a paci start on boot, or stop or start or
299:04 - restart. Okay. So let's go ahead and actually
turn that on and get our server running. And
299:09 - we are going to use sessions manager to do
that. So we'll go back to SYSTEMS MANAGER
299:13 - here. If it's not there, just type SSM and
you can just right click and make a new tab
299:18 - there. And we're going to go down to sessions
manager. And we are going to start a new session,
299:23 - we are going to choose my new server if you
named it that makes it a lot easier to find
299:27 - it. And it will get us into that server lickety
split. And we are going to switch to the EC
299:34 - to user because we don't want to do this as
root. And what we're going to do is first
299:38 - start our service because it'd be nice to
see that it is working. So we'll go back to
299:42 - this IP here and is now working. And now we
want to work on reboots. I'm going to copy
299:47 - this command, paste it in, hit on and it's
going to create a symlink for the service.
299:53 - Okay, and so now when we restart the server,
this should work. So what I want you to do
300:00 - I want you to just close this tab here, leave
this one open. And we will go back to this
300:05 - server here. And we're going to reboot it.
So we're going to go, reboot. And so if our
300:12 - command does what we hope it will do, that
means that it will always work on reboot.
300:18 - Okay.
300:19 - Great. So it is, it should be rebooting now.
I'm pretty sure. And let me just do that one
300:27 - more time.
300:28 - Sure, you want to reboot? Yes. There we go.
Was it really that fast? Oh, I think it just
300:34 - booted really fast. Okay, so I guess it's
finished booting. And we'll just go here,
300:39 - and it's still working so great. So I always
get that confused, because if you stop and
300:44 - start instance, it takes a long time reboots
can be very quick. So now that we have this
300:50 - instance, here, the only only issue is that
if we were to create a copy of this instance,
300:56 - we want to bake in that new functionality.
So we need to create a new ami of that instance.
301:01 - Okay, so what we're gonna do is we're gonna
go to our images and create a new image, and
301:05 - we're gonna call this, my servers 000. And
we're gonna say what we did to this. So what
301:13 - we were doing was ensuring Apache restarts
on boot. On reboot, okay, and then we will
301:24 - create our image, okay. And we will let that
image proceed there, it failed. Oh, I've never
301:32 - I never get failure. That's interesting. Um,
well, that's fine. We'll just do a refresh
301:37 - here. Honestly, I've never had an image fail
on me. So what I'm going to do is I'm just
301:42 - going to try that one more time here. My servers,
there was a zero, restart Apache on reboot,
301:54 - okay. And we will create that image again.
Okay, we'll go back here. And we'll just see
302:02 - if that creates it, there just takes a little
bit time. And so I'll come back here. So sometimes
302:06 - instances can fail or like ami, or snapshots
can fail, and it's just AWS. So in those cases,
302:12 - you just retry again. But it rarely happens
to me. So. But yeah, we'll see you after this
302:18 - is done. Our ami is now available. And if
I was to remove this filter, here, we'd see
302:27 - our original ami, where we installed, install
the paci and, but it doesn't run it by default.
302:33 - So if we were to launch this one, we'd have
our problems that we had previous but this
302:36 - one would always start Apache, okay, now we
have a couple servers here, I want you to
302:42 - kill both of them because we do not need them
anymore. Okay, we're going to terminate them.
302:46 - And we're going to learn about auto scaling
groups. All right. So whenever we want to
302:52 - have a server always running, this is going
to be a good use case for it. So before we
302:56 - can go ahead and create an auto scaling group,
I want you to go create a launch configuration,
303:01 - okay. And so I just clicked that down there
below, and we are going to create a configuration,
303:06 - and we can choose our ami, but we want to
actually use one, one of our own ami, so I'm
303:11 - gonna go my ami here, and I'm going to select
this one, which is the Apache server, it's
303:15 - going to be T two micro, we want the role
to be my EC two, we're going to name this
303:21 - my server lc 000. LC stands for launch configuration
there. That's just my convention, you can
303:31 - do whatever you want. We're going to have
this volume encrypted by default, because
303:36 - if you use an ami that is encrypted, you can't
unencrypted. Okay, we'll go to our security
303:42 - groups. And we will drop this down and select
the security group we created previously here.
303:47 - Yeah, miss that one, we'll go to review. And
we will create our launch configuration, choose
303:53 - our existing key pair there and launch our
or sorry, create our launch configuration.
303:57 - Okay, so we've created launch configuration,
you'll see that process was very similar to
304:02 - an EC two instance. And that was just that
was saving all the settings because an ami
304:06 - doesn't save all the settings rights and a
launch configuration saves all of those settings.
304:10 - So now that we have our launch configuration,
we can go ahead and launch an auto scaling
304:15 - group, okay. And this is going to help us
keep a server always continuously running.
304:20 - So we'll go ahead and create our our auto
scaling group, we're going to use our launch
304:23 - configuration here. We're gonna go next steps.
We're going to name it we'll just say as G
304:28 - or say my server is G. ACS SD stands for auto
scaling group, we're going to have one an
304:34 - instance of one size, we're going to launch
it into default subnet, we are going to have
304:37 - to choose a couple here. So we'll do a and
b. Let's check advanced details. We're going
304:42 - to leave that alone. We're going to go to
the next step. And we're going to leave that
304:47 - alone. We're going to see notifications. We're
gonna leave that alone tags alone. Oops, I
304:53 - think I went backwards here review and we
are going to create that auto scaling group.
304:57 - So now we're going to hit close and we're
going to check out that ESG here. And so look
305:01 - at that it's set to one desired one min and
one max. Okay, it's using that ami. So now.
305:06 - Now what it's going to do, it's going to just
start spinning up that server. So the way
305:12 - this works, if I just go to the Edit options,
here, we have these three values, okay, and
305:16 - so minimum is the minimum number of instances
the
305:18 - auto scaling group should have at any time,
so there's not at least one server running.
305:22 - Okay, it's going to start it up. And we can
never have beyond one server. So there's a,
305:27 - there's a chance where if you have auto scaling
groups, it would try to trigger and go beyond
305:31 - the max. So this Max is like a, like a safety
so that we don't end up with too many servers.
305:36 - And then we have desired capacity. And that's
the desired number of instances we want to
305:41 - be running. And this value actually changes
based on auto scaling groups, it will adjust
305:46 - it. So generally, you want to be at this number,
etc. A lot of times I'll have this. Yeah,
305:51 - exactly. You know, I might have like two,
for very simple applications, and then this
305:56 - would be one in one. Okay. But anyway, this
instance, is automatically starting, okay.
306:02 - And it looks like it's in service. It doesn't
assume me it's running. Because if I go to
306:05 - instances over here, okay, and we take a look,
here we have it, and it's initializing. All
306:12 - right. So what is the big benefit to USGS
is the fact that, you know, if this instance
306:20 - got terminated for any reason, the ASU will
spin up another one, right. So we're just
306:24 - going to wait a little bit of time here for
this, to get it to two checks here. And then
306:30 - we will attempt to kill it and see if the
auto scaling group spins up a new one. So
306:34 - our instances running that was launched our
auto scaling group. And let's just double
306:38 - check to make sure that it's working. It's
always good to do sanity checks here in our
306:42 - Apache page is still operational. So now the
real question is, is that if we are to terminate
306:49 - this instance, will the auto scaling group
launch a new instance, because it should,
306:53 - it should detect that it's unhealthy and launch
a new one. So this is terminating here, and
306:58 - we're gonna go to our auto scaling group here.
And we are going to just see if it's going
307:07 - to monitor, so it's saying that it's terminating,
so it can actually tell that it's terminating,
307:13 - and it's unhealthy, okay. And so it's going
to determine that there are no instances it's
307:17 - going to start up another one here shortly.
Okay. So we are just going to give it a little
307:23 - bit of time here. And so now we have no instances
running, right. And so it should detect very
307:30 - shortly, okay. And there is that health grace
check period. So we are just waiting a little
307:35 - bit of time here. Okay, and great. So now
it's starting up a new EC two instance, because
307:39 - it determined that we're not running. So our
ASC is working as expected. Okay. So, yeah,
307:45 - there you go. So I think maybe the next thing
we want to do is, let's say we wanted to change
307:50 - our Apache page, that index to have some kind
of different text on there. And we can learn
307:57 - how to actually update that. So we'll have
to create a new oma ami and then swap out
308:02 - our launch configuration so that the auto
scaling group can update that page. So we'll
308:06 - just go back here. And we'll just wait for
this to complete so we can SSH in. And we
308:11 - will do that next. So we had terminated our
previous instance. And the auto scaling group
308:16 - spun up a new one, and is now running. So
let's just double check to make sure our party
308:19 - page is still there. And it is and so now
let's go through the process of figuring out
308:25 - how to update this page, so that when we spin
up new instances, with our auto scaling group,
308:30 - they all have the latest changes, okay, so
what we're going to have to do is we're going
308:35 - to need to update our launch configuration,
but we're also gonna have to bake a new ami.
308:39 - And even before we do that, we need to SSH
or get into an instance and update the default
308:45 - page. So what we're going to do is go to services
and type SSM and open up SYSTEMS MANAGER,
308:51 - okay. And we'll just click off here, so this
is not in our way, and we will go down to
308:57 - sessions manager here. Okay, start a new session,
we're going to choose the unnamed one because
309:04 - that was the one last by the auto scaling
group. And we will instantly get into our
309:10 - instance here and we are going to have to
switch over to the user because we do not
309:14 - want to do this as root. Okay, and so if my
memory is still good, Apache stores their
309:23 - their pages in HTML, var ww HTML in it is
great. And so we're just going to create a
309:30 - new page here. So I'm gonna do sudo touch
index HTML, it's going to create a new file.
309:35 - Alright, and now we're going to want to edit
that I'm going to use VA you can use nano
309:39 - here nano is a lot easier to use VI is every
single thing is a hotkey. So you might regret
309:46 - launching and vi here, but this is what I'm
most familiar with. So I'm going to just open
309:49 - that here. And I already have a page prepared.
And obviously you'll have access to this too,
309:55 - for this tutorial series. And I'm just going
to copy this in here and we're going to Paste
310:02 - that there. And I'm just going to write and
quit to save that file. Okay, and then we're
310:06 - going to go back,
310:07 - we'll kill this. Before we kill it, let's
just double check to make sure it works. So
310:12 - we're going to go back to this instance, grab
that IP, paste it in there. And there we are,
310:16 - we have our Star Trek reference page from
the very famous episode in the pale moonlight
310:22 - here. So our page is now been replaced. Great.
So now that we have this in place, the next
310:28 - thing we need to do is get it so that when
we launch a new auto scaling group, it will
310:32 - have that page, right. And so I said that
we need to create an ami. So let's we're going
310:38 - to do so we are going to get the current instance
here. And we are going to create a new image
310:44 - and we are going to follow our naming convention,
I can't remember what I called it. So I'm
310:49 - just going to go double check it here quickly
here. Because you only get the opportunity
310:53 - to name this one. So you want to get them
right. So we will go here and name it. What
310:59 - was it my server my image, I'm getting confused.
Now my server, okay. So we'll say my server
311:06 - 001, I really wish they'd show you the previous
names there. And we'd say, update default
311:12 - party, or create our own custom index HTML
page for a party. Okay, so there we go. And
311:22 - we are going to create that image. Great.
And we will go to View pending ami here. And
311:28 - we will just wait until that is complete.
And we will continue on to updating our launch
311:32 - configuration. So our image there is now available.
If we were just to click off here, we can
311:37 - see we have our ami, so this one has the Apache
server where it doesn't restart on boot. So
311:43 - you have to manually do it. This one starts
up Apache, but it has the default page. And
311:48 - then this one actually has our custom page.
Okay, so we need to get this new ami into
311:52 - our auto scaling group. So the way we're going
to do that is we're going to update the launch
311:56 - configuration. Okay, so launch configurations
are read only, you cannot edit it and change
312:02 - the AMI. So we'll have to create a copy of
that launch template, or some last configuration,
312:08 - careful, there's launch templates, it's like
the new way of doing launch configurations.
312:12 - But people still use launch. Launch config.
So we're just gonna go to actions here and
312:18 - create a copy of launch configuration. Okay,
and it's gonna go all the way to the end step,
312:23 - which is not very helpful. But we're going
to go all the way back to the beginning here.
312:26 - And we're going to remove our zeros zero here
was click that off there, we'll choose one,
312:31 - it probably will warn us saying Yeah, do you
want to do this? Yes, of course. And we're
312:36 - going to leave all the settings the same,
we're going to go to configure details, because
312:40 - it does a stupid copy thing here. And I'm
just going to name it one. And you got to
312:44 - be careful here because you do not get the
opportunity to rename these. So it's just
312:48 - nice to have consistency there. And we will
just proceed to add storage to make sure everything's
312:52 - fine. Yes, it's opposite encrypted security
group, it tried to make a new one. So we're
312:57 - going to be careful there and make sure it
uses our existing one, we don't need to continuously
313:01 - create lots of security groups that we don't
need, we're going to review it, we're going
313:05 - to create the lock configuration we're going
to associate with our key pair as usual. Okay,
313:10 - and now this launch configuration is ready.
So now that we have a new launch configuration,
313:14 - in order to launch this, what we need to do
is go to our auto scaling group, okay. And
313:21 - what we will do is, we're going to edit it,
okay, and we are just going to drop it down
313:27 - and choose one. Alright. So now what we can
do is we can either, we'll just hit Save here,
313:34 - but let's say we want this new instance to
take place. What we can do here is just terminate
313:40 - this old one, and now the new the new auto
scaling group should spin up and use the new
313:44 - launch configuration. Okay, I'm just paranoid
here. And you should always be in a device,
313:48 - I'm just going to double check to make sure
that I did set that correctly. Sometimes I
313:52 - hit save, and it just doesn't take effect.
And this is the new launch configuration being
313:55 - set for ASD. So we'll go back here. Okay.
And we're just going to stop this instance.
314:03 - And the new one that's spun up should have
that page that we created there. So I'll just
314:08 - terminate here. And I'll talk to you here
in a moment we get back, ok. Our new instances
314:19 - running here, and let's take a peek to see
if it's showing our web page here. And it
314:23 - is, so we are in good shape here. We successfully
updated that launch configuration for the
314:29 - auto scaling group. And so now anytime a new
instance is launched, it will have all all
314:33 - our latest changes. So auto scaling groups
are great because, you know, they just ensure
314:39 - that there's always at least one server running.
But what happens if the entire availability
314:43 - zone goes out? All right, it's not. It's not
going to help if the auto scaling group is
314:48 - set in that AZ. So what we're going to have
to do is create high availability using the
314:54 - load balancers so that we can run instances
in more than one
314:58 - one AZ at any given time. Okay, so let's go
ahead and layer in our auto scaling group
315:03 - into our load balancer. So what I want you
to do is make a new tab here, and we are going
315:07 - to create a load balancer. Okay, it's not
terribly hard, we'll just get through here.
315:12 - And we'll just create a load balancer here.
And we have three options provided to us application
315:18 - load balancer network load balancer, and classic
load balancer, okay. And we're going to make
315:22 - an application load balancer. Alright, and
I'm just going to name this al are my, my
315:27 - lb, okay, and it's going to be internet facing,
we're going to use ipv4 because that's the
315:34 - easiest to work with, we're gonna use the
HTTP Port 80 protocol, because we don't have
315:39 - a custom domain name. So we're gonna just
have to stick with that. We're gonna want
315:43 - to run this in multiple azs. It's always good
to run it at least three public AZ. So that's,
315:50 - I'm going to select those there. Okay, we're
going to proceed to security here, prove your
315:56 - load balancers, security settings don't seem
to have any here. We'll we'll go next here.
316:00 - And we are going to create a new security
group actually. And this is going to be the
316:06 - security group for the lb here. So we'll say
Alp. Let's what's the SD, that's always thing
316:16 - I like to put on the on the end there. And
we can leave the default description in there.
316:20 - And so we want it. So anything on port 80
is accessible from anywhere. So to me, that
316:27 - is a good rule. And we will go to the next
step here. And we will have to create our
316:33 - target group. Okay, so the target group is
what points to the actual easy two instances,
316:39 - okay, so we will just make a new one, and
we'll just call it my target group. And we
316:46 - will call it the production one here, because
you can have multiple levels to say tg prod,
316:51 - because you can have multiple target groups,
okay. And it's going to be for instances where
316:56 - use HTTP protocol, the health check is actually
going to be the default page. That's pretty
317:00 - good to me. And we're going to go ahead and
register some targets. And so here we can
317:05 - individually register targets, but we actually
want to associate it via the auto scaling
317:11 - group. So we're not going to do it this way.
We're just gonna go next and create this load
317:16 - balancer here. And it takes a very little
time to do so. Okay. And I mean, it says it's
317:21 - provisioning. So I guess we'll have to wait
a little bit here. But what we want to do
317:25 - is we want to get those auto scaling groups
associated with our target group. Okay. And
317:29 - so the way we'll go about doing that is we're
going to go to our auto scaling group. And
317:35 - we are going to go ahead and go to actions,
edit this here. And we are going to associate
317:41 - to tg prod. So that's how it's going to know
how to the load balancing browser, it's good
317:47 - to know how to associate with the the auto
scaling group. And we will also change your
317:51 - health check to EOB. Because that is a lot
better. Here. We're going to save that there.
317:56 - And we are going to go back to load balancers
and see how that is progressing. Remember
318:00 - how long it takes for an lb to spin up. Generally,
a very quick still says it's provisioning.
318:05 - But while that is going, we have an opportunity
to talk about some of the settings here. So
318:09 - the load bouncer has listeners, right. And
so it created a listener for us through that
318:14 - wizard there. And so we have one here that
listens on port 80. And they always have rules.
318:20 - And that rule is it's going to always forward
to the the target group we had selected. So
318:26 - we wanted to edit it or do some more things
with those rules, we can hit rules here. and
318:30 - here we can see Port 80. So if by default,
everything by default, is going to be Port
318:36 - 80, it's going to then forward it to that
target group. So if we wanted to here add
318:40 - another rule here, we could add rules such
as all sorts of kinds. So we could say if
318:47 - the path here was secret page, okay, we could
then make an action and forward it to a target
318:56 - group that that has some very special servers
in it. Okay, so there are some very advanced
319:00 - rules that we can set in here, we're going
to leave them alone, I just wanted to give
319:04 - you a little a little tour of that. And so
yeah, we're just gonna have to wait for this
319:09 - to finish here. And once it's done provisioning,
we're going to see if we can get traffic through
319:15 - our load balancer. So our lb is ready. Now
just make sure you press that refresh up there,
319:21 - because a lot of the times these things are
ready and you're just sitting there because
319:25 - the UI does not refresh. So always hit the
refresh there once in a while. And let's just
319:31 - see if our load balancer is working and routing
traffic to our single instance there. So down
319:37 - below, we do have our DNS DNS name. So this
is a way that we would access our load balancer
319:44 - there and there you go. So now everything's
being routed through the load balancer. Now,
319:49 - if we were to go back
319:50 - to this EC two instance here, okay, we might
want to actually restrict traffic so that
319:56 - you can't ever directly go to the instance
only through the load balancer. Alright, so
319:59 - I'm just going to copy this IP here. Okay,
and so I'm able to access this through the
320:04 - IP address, which is not too bad. But let's
say I didn't want to be able to access it
320:09 - through Yeah, through here, okay, so it always
has to be directly through the load balancer.
320:15 - And the way we can do that is we can just
go adjust this auto scaling group, or sorry,
320:19 - the security group. So that it it denies traffic
from Port 80. Now, I kind of like having these,
320:26 - the the security group around for the CTO
instance. And so what I want to do is I want
320:33 - to actually create a new security group just
for the auto scaling group. Okay, so we'll
320:36 - go to security groups here. And we will create
a new security group and we're going to call
320:41 - it my I don't think I was very consistent
here. So yeah, yeah, kinda. So we'll say,
320:48 - my ESG security group. And so for this, we
are going to allow us to use SSH. Honestly,
321:00 - we don't really need to SSH anymore because
we can use SYSTEMS MANAGER. But if the case
321:05 - we wanted to do that, we can set that rule
there. And so we will allow inbound traffic
321:11 - that way. And we will also allow inbound traffic
for Port 80, but only only from the load balancer.
321:18 - So in here, we can actually supply a security
group of another one here. So for the load
321:24 - balancer, I don't remember what's called Oh,
I can move this nice. And so the other one
321:28 - is called a lb. So I'm just going to start
typing a lb here. And now we are allowing
321:34 - any traffic, just from on port 80. From that
load bouncer. Okay, so we're going to hit
321:40 - Create there, oh, I gotta give it a description.
So my SD scritti. Group, okay. I know, you
321:48 - always have to provide those descriptions,
it's kind of annoying. Okay, and so now what
321:53 - we're gonna do is we're gonna go back to our
auto scaling group, we might actually, we
322:01 - might have to make a new launch configuration,
because there's no the the the security group
322:05 - is associated with the launch configuration.
So I think that's what we're gonna have to
322:09 - do here. So we're gonna have to create a new
ami, or new launch configuration, you can
322:14 - see this is a very common pattern here, copy
the launch configuration here. And we're going
322:19 - to want to go back, we're gonna want to use
the same, sorry, we're gonna wanna use the
322:24 - same ami, we're not changing anything here,
we just want to make sure that it's using
322:27 - the new security group here. So we will go
to the ASC one here. Okay. And I think that's
322:36 - all I want to do. I'm just gonna double check,
make sure all our settings are the same. Oh,
322:39 - yeah, does this stupid copy thing. So we'll
just do 002 there. And yeah, everything is
322:44 - in good shape, we will just double check here.
Yep. And we will all create that new launch
322:50 - configuration there, close it. Okay, we'll
go to our auto scaling group. And we're going
322:56 - to go to actions, edit, as always, and go
to version two here. Okay, and we are going
323:02 - to save it. Alright. And the next thing we're
going to do is we are going to terminate this
323:09 - instance here, because we want the new security
group to take effect, okay. So we're going
323:14 - to terminate that instance, if we stopped,
it also would do it. But we want to get rid
323:17 - of the server here. So we'll terminate it,
we're going to go back to our auto scaling
323:22 - group, okay, because we want to by default,
run at least in three azs. To get that, that
323:27 - full availability there. So what I'm going
to do is I'm going to change our desire to
323:33 - three, our min to three and our max to three.
Okay? And, um, I'm going to add an additional
323:42 - subnet here, we need See here, okay, so that
I can actually do that. And, yeah, now I'm
323:48 - kind of expecting this AZ to, to launch in
all three, okay. If I just go back here and
323:56 - edit here. Yeah. So now all three Z's appear
there. Because if we didn't have that there,
324:02 - I don't think the load balancer would have
been able to do that it would have launched
324:04 - two and one is the one another. So now we
have those there. And what we're gonna do
324:10 - is we're just going to wait for these three
servers to spin up here. Okay. There's two,
324:17 - where's my third one? Give me a third one.
But yeah, we set it to three. So the third
324:24 - one will will appear here shortly. And we
will just resume here once they all appear.
324:28 - So our three EC two instances have launched,
I was a bit worried about the last one, but
324:33 - it eventually made its way here and look at
the azs. One is an A one is in B and one is
324:38 - in C. So we have high availability. So if
two available availability zones go out, we're
324:45 - always going to have a third server running.
So we're going to be in very good shape. Now,
324:48 - the other thing we were doing is we wanted
to make sure that people couldn't directly
324:52 - access the servers and had to be through the
load balancer. So let's go discover that right
324:56 - now. So if I pull up this IP should change
I'll be accessible via the IP or public DNS
325:02 - here and I try this here. It's never loading.
That's great. That's what we want. Okay, so
325:07 - the question is, is now if we go to our load
balancer, do, we still have access to our
325:11 - our instances here, through that DNS record
here, okay. So we're gonna copy that. And
325:18 - we do. So, you know, the good reason for that
is that we always want to restrict our traffic
325:24 - through like this narrow pipeline. Because
if everything always passes through the load
325:28 - balancer, then we can get richer analytics
on and put things in front of there. One thing
325:33 - we can do with a load balancer is attach a
laugh, web application firewall, which is
325:38 - a really good thing to do. And so if you have
people accessing things directly, not through
325:45 - the load balancer, then they wouldn't pass
it a laugh, okay, so it's just creating those
325:49 - nice choke points there. And so now that we
have our lb, I guess the next step would be
325:55 - really to serve this website up via a custom
domain name. So yeah, let's do a bit with
326:02 - route 53 and get a custom domain.
326:09 - So now it's time to learn how to use route
53 to get a custom domain name, because, you
326:15 - know, we do have this URL for our website,
but it's kind of ugly. And we want to go custom
326:20 - here. And we want to learn how to integrate
router d3 into our load balancer. So let's
326:25 - go up at the top here, type a route 53. Okay,
and we're going to register our domain. Now,
326:30 - this does cost money. So I guess you could
skip it or just watch here. But you know,
326:35 - to get the full effect, I really do think
you should go out and purchase a very inexpensive
326:40 - domain. And so
326:41 - we're gonna go to the top here, and I'm going
to register a new domain, I'm going to get
326:44 - a.com if I can, unless there's something cheaper.
326:47 - I mean, there are cheaper ones. I guess I
don't have to be so frugal, okay, and I'm
326:55 - going to try to get I know what domain I want.
And as always, it's going to be Star Trek,
326:59 - I'm gonna see if I can get the friendly lion.
So we'll type in frame your lines here and
327:03 - check if it's available. And it is, so that's
going to be our domain name. So I'm going
327:08 - to add it to my cart here. We'll have our
subtotal. We'll hit Continue. And now we have
327:12 - a bunch of information to fill in. And I just
want to point out down below that you can
327:18 - have privacy protection turned on. So normally,
with other services like GoDaddy, you'd have
327:23 - to pay an additional fee in order to have
yourself protected. So it doesn't display
327:29 - your information on who is okay. So if you're
wondering, I'm talking about if you go to
327:33 - who is here.
327:36 - Domain tools, I believe it this is this one
here? I'm not sure why Oh, here it is. There
327:42 - we go. I clicked the wrong one.
327:45 - Okay, and so if we were to go to who is here,
we can generally look up anything we want.
327:50 - So if we typed in like google.com, okay. I
say I'm not a robot. Okay. Generally, there
328:00 - is like additional information here. And it
can provide like someone's phone number and
328:05 - the company here. And sometimes you want to
keep this information, private. And so that
328:11 - is what this option here is going to do. So
if you've ever had a random call from somebody,
328:15 - you wonder how they got your phone number,
maybe you registered a domain name, and it
328:19 - wasn't a had privacy turned on. But anyway,
I'm gonna go ahead and fill this out. And
328:24 - then I'm going to show you the steps afterwards.
Okay. All right. So we're on to the next page
328:29 - here. And I did substitute my private information
here. And if you do call this number, you
328:33 - are looking forward to some tasty, tasty pizza
hot in Toronto. And so on this page here,
328:42 - it's gonna ask us if we want to automatically
renew our domain, I'm gonna say yes, because
328:45 - I think I want to keep this domain. And I'm
just going to agree to the terms and conditions,
328:49 - it's going to cost $12 USD. Unfortunately,
it's not Canadian dollars. So it's a little
328:53 - bit more expensive. But for me, it is worth
it, we're going to go ahead and complete our
328:57 - purchase. Okay, and so it says we've registered
the domain. And so it has been successfully
329:05 - registered. So there we go. Cool. So I just
want to show you that the domain is in pending.
329:10 - So we are just waiting to get some emails
from AWS. And once we get those emails, we
329:16 - will confirm them. And we should have our
domain very shortly. They say it can take
329:20 - up to three days, I've never had to wait that
long to get a domain it's pretty quick, especially
329:24 - with dot coms. So as those emails come in,
I will then switch over to our email and show
329:28 - you what those look like. So our first email
has arrived is from Amazon registar. And it's
329:34 - just it's a confirmation. And there's nothing
for us to confirm. It's just saying, hey,
329:38 - you are now using Amazon register. That's
the thing that actually registers the domains
329:41 - behind revenue three. And this is not really
the email we care about, but I just want to
329:46 - show you so if you're wondering, you know
what it's about. Okay, so our second email
329:50 - came in here. It didn't take too long here.
I think I made maybe waited about 15 minutes
329:55 - and it says I've successfully registered the
Frankie Alliance calm and So now, we are ready
330:01 - to start using this domain. So we're gonna
go back to row 53. Here, I'm just going to
330:05 - get out of my email. So our domain name is
registered. And we can see it appearing under
330:10 - the register domains, it's no longer in the
pending state. So let's go all the way up
330:14 - to hosted zones, because either of us will
will have created one by default for us. And
330:19 - we can go ahead and start hooking up this
domain to our load balancer. So I'm going
330:23 - to click in to the Frankie Alliance, okay.
And right off the bat, we have some ns records,
330:29 - we have an SLA record. So they've really set
us up here. And we're going to create our
330:34 - first record set. And we're going to want
to hook up our www dot, okay, and we are going
330:43 - to want to use alias and we're going to choose
the target, and we're gonna choose our E lb.
330:47 - Okay, and so we're gonna leave it on simple
routing. And we're gonna create that. And
330:52 - now our www dot should start pointing to our
domain name, I'm pretty sure this takes effect,
330:58 - like immediately, this is a new domain name.
So I'm not sure if it has to propagate through
331:02 - all the DNS records around the world. So this
doesn't work. I'm not going to get too upset
331:06 - about this here, but I'm gonna cross my fingers
and can't be reached, okay, so it doesn't
331:12 - work just as of yet. So what I'm going to
do is, I'm just going to give it a little
331:16 - bit of time here, just to see if it does take
effect here, because everything is hooked
331:21 - up correctly. And we will be back here shortly.
So I took a quick break there had some coconut
331:27 - water came back refreshed, and now our website's
working, I didn't have to do anything, it
331:31 - just sometimes takes a little bit time to
propagate over the internet, those those changes
331:36 - there. So now our next thing we need to resolve
is the fact that this isn't secure. Okay.
331:42 - And so AWS has a server certificate, a service
called Amazon certification manager, certificate
331:48 - manager, and it'll allows you to get free
SSL certificates. But just be sure, when you
331:53 - do go to the service, we're going to click
on provision certificates and not private,
331:58 - these ones are very expensive, they're $500.
Initially, and you really want to just provision
332:04 - a certificate, I wish they'd say free or public
over here. So it's less confusing, you're
332:08 - only ever going to see the splash screen once,
for the first time you've ever created a certificate
332:13 - within a zone. So hopefully, that's not too
confusing there. But it does ask you again,
332:17 - if you want public or private, you definitely
definitely want the public certificate, not
332:21 - the private, okay, so we're going to request
a certificate, and we are going to put our
332:25 - domain name in. So just to cover all our bases
here, I'm going to put in the naked domain,
332:31 - I'm also going to put in a wildcard. So all
our sub domains are included here, this is
332:36 - going to catch all cases. And we'll just hit
next here,
332:40 - we're going to use DNS validation. Email validation
is the older mechanism here for validation.
332:46 - Everyone does DNS validation, okay, and we're
gonna hit review, then we're gonna hit confirm
332:52 - request. And this is going to start spinning
here. And now what's gonna ask for us for
332:58 - to do is to validate that we have ownership
of that domain. And so we can verify this
333:04 - here. And luckily, we can just drop this down
and create a record in row feet, roughly three.
333:09 - So this is what they do, they put a C name
in your, your DNS records here. And that's
333:15 - how we know that we own that domain name.
And we're gonna do it for both here. Okay,
333:20 - and so now we're just gonna have to wait for
these to validate here and it shouldn't take
333:24 - too long. So I took a quick break there had
some coconut water came back refreshed, and
333:34 - now our website's working, I didn't have to
do anything, it just sometimes takes a little
333:37 - bit time to propagate over the internet, those
those changes there. So now our next thing
333:43 - we need to resolve is the fact that this isn't
secure. Okay. And so AWS has a server certificate,
333:49 - a service called Amazon certification manager,
certificate manager and allows you to get
333:55 - free SSL certificates. But just be sure, when
you do go to the service, we're going to click
334:00 - on provision certificates and not private,
these ones are very expensive. They're $500.
334:07 - Initially, and you really want to just provision
a certificate, which let's say free or public
334:12 - over here, so it's less confusing. You're
only ever going to see the splash screen once,
334:16 - for the first time you've ever created a certificate
within a zone. So hopefully, that's not too
334:21 - confusing there. But it does ask you again,
if you want public or private, you definitely
334:24 - definitely want the public certificate, not
the private, okay, so we're going to request
334:28 - a certificate, and we are going to put our
domain name in. So I'm going to do this and
334:33 - just so I don't type it wrong, and we're going
to go back to row 53 and grab our domain name,
334:40 - okay. And there it is. So I'm just going to
copy it there. Go back here, and I'm going
334:46 - to give it wildcard so it just saves you a
lot of time if you wildcard it Okay, so you
334:50 - don't have to keep on adding certificates
for subdomains you don't cover but we have
334:55 - wildcard it so we're gonna hit next. And then
we need to validate that we own That domain
335:01 - name. And since we're using record three,
it's going to be very easy, we can just use
335:04 - the DNS validation method, nobody uses email
anymore. It's like very old method. So we're
335:09 - always going to do DNS. And so we're going
to go to the next step here and say review.
335:13 - And we're going to say, confirm and request.
And now we just need to confirm ownership.
335:17 - So they will issue us a free SSL certificate.
So we see pending validation, okay, we're
335:22 - going to drop down here. And what we're going
to have here is a button. And what this is
335:26 - going to do is it's going to automatically
create the C name record in roughly three,
335:30 - this is the way certificate manager is going
to confirm that we own the domain, because
335:35 - we can add it to the domain, that means we
must own it. And so they have a one button
335:39 - press for us to do that there. So that's a
very convenient, okay, and this should not
335:44 - take too long to confirm. So we'll hit continue
here. Okay, and it's in pending validation.
335:50 - And so we're just going to wait a little bit
here, just like we did for the website update.
335:56 - And we'll do I will do a refresh because it
says a sack page, so you will have to hit
336:00 - this a few times. So that certificate has
been issued, the console didn't directly take
336:06 - me to the screen. So I did have to go to the
top here, type ACM, like this, and to get
336:12 - here and hit refresh. But again, this only
takes a few minutes when you are using route
336:16 - 53. And so um, you know, just be aware, that's
how long it should take. And so now that it's
336:21 - been issued, we can go ahead and attach it
to our load balancers. So we're gonna go back
336:26 - to the C two console. On the left hand side,
we're going to go to load balancers. And we're
336:32 - going to make sure our load balancers selected
there, go to listeners, and we're going to
336:36 - add another 14443, which is SSL. And we're
going to forward this to our target production
336:44 - group here. Okay. And then this is where we
get to attach your SSL certificate. So we're
336:48 - going to drop that down to Frankie Alliance,
and we're going to hit save. Okay, and so
336:52 - now, we're able to listen on port four, or
sorry, 4443. Here, we do have this little
336:58 - caution symbol is saying that we can't actually
receive inbound traffic for 443. So we're
337:03 - gonna have to update our security group. So
going down to a security group here, we will
337:07 - click it,
337:08 - and it is the alrb security group that we're
looking for. So this one here, we're going
337:12 - to go to inbound rules
337:13 - we're going to edit, we're going to add a
new rule, and we're going to set it for HTTP
337:17 - s. So there we go. So now, we can accept traffic
on 443. And it is attached there. So now we
337:26 - should be able to, we should be able to have
a protected or secure URL there when we access
337:34 - our domain name. So I'm just gonna grab the
name. So don't make any spelling mistakes
337:37 - here. And we'll paste it in here. And there
we go. It's secure. So um, yeah, so there
337:43 - we are. Okay, great. So we're all done this
fall along here. And I just want to make sure
337:47 - that we just do some cleanup here to make
sure that we aren't being charged for things,
337:51 - we don't need any more. So the first thing
we're going to do is we're going to just terminate
337:55 - our load balancer there, which is not too
difficult to do. So we'll just go ahead there
338:01 - and go to actions and just go ahead and delete,
and delete that, it should be pretty quick.
338:06 - And wow, that was very fast. So now on to
our auto scaling groups. So that's the next
338:11 - thing, we need to go ahead and delete there.
And so we're just gonna drop down to actions
338:16 - there and go delete. Now, when you delete
the auto scaling group, it will automatically
338:20 - delete those easy two instances there. But
it's good to keep a little eye on them there.
338:24 - So we're going to pop over to here for a minute.
And you can see they're not terminating just
338:29 - yet. So we're gonna wait on that auto scaling
group. And so once that scaling group has
338:35 - been deleted, and this might take a little
bit of time here, it will definitely get rid
338:39 - of those easy two instances for us. That took
an incredibly long time to delete that auto
338:43 - scaling group. I don't know why. But we'll
go back to our instances here. And we will
338:48 - see if they are still running. So you can
see they've all been terminated. So when that
338:52 - auto scaling group is deleted, it's going
to take down the EC two instances with it,
338:56 - we're probably also going to want to go to
route 53. And remove those, those dead endpoints
339:03 - there because there is a way of compromising
those, if you are a very smart, so we'll just
339:10 - go ahead and delete that record because it's
pointing to nothing right now. Right. And
339:13 - so there you go, that was our cleanup. So
we're all in good shape. And hopefully you
339:18 - found the the section.
339:21 - Hey, this is Andrew Brown from exam Pro. And
we are looking at elastic file system Fs,
339:29 - which is a scalable elastic cloud native NFS
file system. So you can attach a single file
339:34 - system to multiple EC two instances and you
don't have to worry about running out or managing
339:39 - disk space. Alright, so now we are looking
at EF s and it is a file storage service for
339:45 - easy to instances, storage capacity is going
to grow up to petabytes worth and shrink automatically
339:51 - based on your data stored so that's why it
has elastic in its name that drive is going
339:56 - to change to meet whatever the demand you
are Having stored now, um, the huge huge advantage
340:04 - here is that you can have multiple EC two
instances in the same VPC mount to a single
340:09 - e Fs volume. So it's like they're all sharing
the same drive. And it's not just easy two
340:14 - instances, it's any any of them in a single
VPC. That's amazing. And so in order for you
340:20 - easy two instances, to actually mount FSX,
it does have to install the NFS version 4.1
340:26 - client, which totally makes sense, because
Fs is using that protocol, the NFS version
340:32 - four protocol. And so Fs, it will create multiple
targets in all your VPC subnets. So this is
340:38 - how it's able to allow you to mount in different
subnets or different Z's here. And so we'll
340:46 - just see a create a bunch of Mount points
and that's what you will mount. And the way
340:50 - it builds, it's going to, it's going to be
based on the space that you're using, it's
340:54 - going to be 30 cents for a gigabyte, month
over month reoccurring. Okay. So there you
341:00 - go. Hey, this is Angie brown from exam Pro,
and we are going to do a quick EF s follow
341:10 - along. So we're going to launch to EC two
instances, connect them both IE Fs and see
341:14 - if they can share files between them from
a one DFS, okay, so what we're going to need
341:20 - to do is make our way to the DFS console here,
okay. And we're going to get here and we're
341:27 - going to create a new file system, we are
going to launch this in our default VPC, you're
341:31 - going to see that it's going to create mount
targets for every single availability zone
341:37 - for us here, okay. And so it's also going
to use the default security group. Okay, and
341:43 - so we're gonna go next, we're gonna skip tags,
we do have the ability to do Lifecycle Management.
341:48 - So this is no different than s3, allowing
you to reduce costs by moving stuff that you're
341:53 - not using as frequently into infrequent access.
So you get a cheaper, cheaper storage cost
341:58 - there. So that's really nice. We're going
to be comfortable with using bursting here.
342:04 - We're going to stick with general purpose,
and we're going to turn on encryption, okay,
342:07 - it's good practice to use encryption here,
especially when you're doing the certifications.
342:11 - You want to know what what you can encrypt.
Okay, so, AWS has a default kms key for us.
342:17 - So we can go to next here. And we're just
going to do a quick review. And we're going
342:21 - to create that file system. Okay. So it's
going to take a little bit of time here to
342:26 - create that file system. Well, while that's
going, I think we can go prep our EC two instances.
342:30 - Okay. So yeah, it's just going to take a bit
of time. So if we are looking at our Fs volume
342:37 - here, we can still see that our mount targets
are creating. So let's go ahead and create
342:40 - those EC two instances. So in the ECG console,
you can get there just by typing EC two, and
342:46 - we are going to launch a couple of instances.
So we will choose Amazon Lex two, because
342:50 - that's always a good choice, we'll stick with
T two micros because that is part of the free
342:54 - tier, we're going to go next. And we're going
to launch two of these fellows here. Okay,
342:59 - and we got to make sure that we are launching
this in the same VPC as a DFS, which is the
343:03 - default one, okay? We're not going to worry
about what which subnet because we're gonna
343:07 - be able access it from anywhere. And I want
you to create a new role here. So we're gonna
343:12 - go here and create a new role. Okay, and we've
created this multiple times through our fall
343:17 - alongs. But just in case you've missed it,
we're going to go ahead and create a role
343:20 - go to EC to go next. And we're going to choose
SSM for SYSTEMS MANAGER, okay, and we're going
343:27 - to hit next and then next, and then we're
going to type in a my will say Fs DC to role,
343:35 - okay, and we will create that role. Alright.
And so that's going to allow us to access
343:39 - these instances via session manager, which
is the preferred way over SSH. So we will
343:44 - just hit refresh here and then see if it is
there. And there it is. Okay, and we're just
343:50 - going to need to also just attach a user data
script here, because we need to configure
343:56 - these instances here to
343:58 - to be able to use EF s, okay, so there is
this little thing you do have to install.
344:04 - And so we're just gonna make that easy here,
it's gonna install it for both our instances.
344:09 - Alright, and so we're gonna go ahead and go
to storage. And then we're going to go to
344:13 - tags, and we're gonna go to security groups,
and we're gonna make a new security group
344:18 - for this here. Okay? So I'm just going to
call it my EC to a u Fs. sg, alright. And
344:26 - I mean, we don't really plan on doing much
with this here. So I'm just going to go ahead
344:31 - and review and launch and then we will get
our key pair here. Not that our key pair matters
344:36 - because we are going to use sessions manager
to gain access to these instances. Okay, so
344:42 - now as these are spinning up, let's go take
a look back over here and see if these endpoints
344:46 - are done creating. Now it says they're still
creating but you know, the ages constantly
344:50 - can never trust it. So we'll go up here and
do a refresh here and see if they are done
344:55 - crazy and they are all available. Okay? So
we do actually have something instructions
345:00 - on how to set these up here with us. So if
we just click here, it's going to tell us
345:05 - that we need to install this here onto our
EC two instances for it to work. And we have
345:10 - absolutely done that. And then when we scroll
down here, we need to then mount the EF Fs.
345:17 - Okay, and so since we are using encryption,
we're going to have to use this command here.
345:22 - Okay, and so that's what we're going to do,
we're going to have to use sessions manager
345:27 - to log in, and then mount it using the following
command. Okay, so we're going to go back to
345:33 - EC two instances, and we're just going to
wait till those style checks are complete.
345:37 - Okay, so as these are initializing here, I
bet we should probably go set our security
345:42 - groups. So I'm going to go over to security
groups here. And I'm going to look for our
345:46 - default one, because I believe that's what
we launched, our DFS is in, I probably should
345:50 - have created our own security group here,
but that's totally fine. So it's going to
345:53 - be 2198. Okay, and so we're, that's what we're
looking for is 2918. And it is this one here,
346:01 - and it's the default here, okay, and we're
just going to add an inbound rule here. And
346:04 - we're going to have a Roy here from an old
an old following here, this will just remove
346:09 - that. And we'll look for NFS. Okay, that's
gonna set it for 2049. And now we just need
346:14 - to allow the security group for our, our EC
to there's so I believe we called it and so
346:20 - I just start typing mine. So we have my EC
two Fs SG. And so now we shouldn't have any
346:25 - issues with mounting, because we probably
do need that to gain access. Okay, and so
346:31 - we're gonna go and now we'll go back and see
how our instances are doing. So it looks like
346:35 - they're ready to go. So now that they're,
they're in good shape, we can actually go
346:39 - gain access to them. So what we're going to
do is we're going to go to SYSTEMS MANAGER,
346:45 - so type SSM and make a new tab here, okay.
And we're going to wait for this to load here
346:52 - and on the left hand side, we're going to
go to sessions manager. And we're going to
346:56 - start a session. And so there are both of
our instances, we probably should go name
347:00 - them, it'll just make our lives a lot easier.
So I'm going to say EC two Fs, a, okay. And
347:06 - then we have E, F, E, AC to DFS B. All right,
so we'll just go back here, do a refresh.
347:13 - And there are two instances. So we need to
launch an instance for this one, okay. And
347:18 - I'm gonna have to start there. And then I'm
gonna have to make another session here and
347:22 - start it for a B. All right. Okay, and so
now we have these two here, and we're going
347:29 - to have to switch to the correct user. So
we are as root user, we don't want to be that
347:33 - we want to be the easy to user. Okay, so we
will switch over to EC two for a consuming
347:39 - this one's a doesn't really matter at this
point, because you know, doesn't, but it will
347:44 - just switch over here to the user here. Alright,
so now we are set up here, and we can go ahead
347:51 - and mount mounted there. So it's just a matter
of copying this entire command. So we are
347:57 - going to need the sudo in there. So just make
sure you include it. And we're just going
348:01 - to paste that in there. And we're going to
mount, okay, and it just says it doesn't exist.
348:06 - So we'll just Alright, so that can fail because
it has nothing to mount to. So we actually
348:12 - have to create a directory for it. Okay, so
just here in the home directory, I'm just
348:15 - going to go make MK dir, and type in Fs. Okay,
and then we will just go up, and then we will
348:23 - mount it. And so now it should mount to that
new directory. So it is mounted. We're going
348:27 - to do the same story over here. Okay, so just
to save us some time, I'm just going to copy
348:32 - these commands over, okay. All right. Oops,
we forgot the M there on the front. Okay.
348:39 - And then we will just copy over the same command
here. And we will also mount Fs, okay, so
348:45 - they should both be mounted now. And so I'm
just going to do an ls, and we're going to
348:48 - go into that directory there. Okay. And we
are just going to create any kind of files,
348:53 - I'm gonna say, touch on touch, base, your
348:57 - dot txt. Okay, so I've touched that file,
we cannot do that there. So I'll just type
349:04 - in sudo. And so I'll do an ls within that
directory. And so that file is there. So now
349:07 - if I go over to this one here, and do an ls
and go into Fs, and do an ls, there is that
349:13 - file. Okay. So that's how you can access files,
across instances using DFS. So that's all
349:20 - you need to know. So we're, we are done. Okay,
so we'll just terminate this instance here,
349:24 - okay. And we will also terminate this instance
here. Great, we will close that sessions manager,
349:29 - we will just kill our instances here. Okay.
Because we are all done. And we'll go over
349:35 - to DFS. So it's only only costs for when we're
consuming stuff, but since we're done with
349:40 - it, let's go ahead and tear this down. Okay,
and we need to just copy this fella in here.
349:47 - And we're all good to go. So there you are,
that is EF s. Alright, so we're onto the ZFS
349:56 - cheat sheet here. So elastic file system.
VFS supports the network files. System version
350:00 - four protocol you pay per gigabyte of storage
per month. volumes can scale to petabyte size
350:06 - storage volumes will shrink and grow to meet
current data stored. So that's why it's elastic
350:13 - can support 1000s of concurrent connections
over NFS. Your data is stored across multiple
350:18 - agencies within a region. So you have good
durability there can mount multiple EC two
350:24 - instances to a single Fs as long as they're
all in the same VPC. It creates a mount points
350:29 - in all your VPC subnets. So you can mount
from anywhere within your VPC and it provides
350:34 - read after write consistency. So there you
go. That's 
350:42 - the
350:43 - Hey, this is Angie brown from exam Pro. And
we are looking at elastic block store also
350:46 - known as EBS, which is a virtual hard drive
in the cloud create new volumes attached easy
350:51 - to instances, backup via snapshots and easy
encryption. So before we jump into EBS, I
350:57 - wanted to lay some foundational knowledge
that's going to help us understand why certain
351:01 - storage mediums are better than others based
on their use case. So let's talk about IOP.
351:07 - So IOP stands for input output per second
it is the speed at which non contiguous reads
351:12 - and writes can be performed on a storage medium.
So when someone says hi IO, there, they're
351:17 - saying that this medium has the ability to
do lots of small fast reads and writes. Then
351:22 - we have the concept of throughput. So this
is the data transfer rate to and from the
351:27 - storage medium in megabytes per second. Then
you have bandwidth, which sounds very similar,
351:32 - but it's different. And so bandwidth is the
measurement of total possible speed of data
351:36 - movement along the network. So to really distinguish
between the throughput and the bandwidth,
351:41 - we're going to use the pipe in water example.
So think of bandwidth as the pipe and throughput
351:46 - as the water. Okay, so now let's jump into
EBS. So we are now on to talking about EBS
351:55 - and it is a highly available durable solution
for attaching persistent block storage volumes
352:00 - to easy to instances, volumes are automatically
replicated within their AZ to protect them
352:05 - from component failure. And we have five types
of EBS storage to choose from we have general
352:10 - purpose provision, I Ops, throughput, optimized
HDD, cold, HDD and EBS. Magnetic, okay, and
352:19 - so we do have some short definitions here,
but we're going to cover them again here.
352:24 - Alright, so we're gonna look at the different
volume types for EBS, and just try to understand
352:32 - their use cases. And so again, there are five
types, and we're gonna go through each one
352:36 - starting with general purpose. And it is as
the name implies, good for general usage without
352:41 - specific requirements. So you're gonna be
using this for most workloads like your web
352:45 - apps, and has a good balance between price
and performance for the actual attributes
352:50 - underneath it, it can have a volume size between
one gigabytes and 16 terabytes, and a max
352:55 - I ops of 16,000 per second. Moving on to provision
I ops SSD, it's really good when you need
353:02 - fast input and output, or the more verbose
description is when you need mission critical,
353:09 - low latency or high throughput. So it's not
just eye Ops, it's also high throughput as
353:14 - well, it's going to be great for large databases.
So you know, think RDS, or Cassandra. And
353:22 - the way you know, when you should start using
provision I ops if you exceed 16,000, I often
353:28 - see that's where the limit is for general
purpose. So when you need to go beyond that,
353:32 - you're going to want to use this one, or if
the throughput is greater than 250 megabytes
353:37 - there as well. Now, the volume size here can
between four gigabytes and 16 terabytes, and
353:43 - we can have a max I ops of 64,000. Okay, moving
on to our hard disk drives, we have throughput,
353:50 - optimized HDD, and it is a low cost. It's
designed for frequency, frequently accessed.
353:58 - data and the throughput and throughput intensive
workloads, it's going to be great for data
354:02 - warehousing, big data and log processing stuff
where we have a lot of data. The volume size,
354:08 - you can see by default is a lot larger, we
have 500 gigabytes to 15 terabytes in the
354:13 - max I ops is 500. Moving over to cold HDD,
this is the lowest costing hard drive here.
354:22 - It's less for less frequently used workloads,
you're going to have things for storage, okay,
354:27 - so if you want to put backups or file storage,
this is going to be a good drive for that.
354:33 - And it has the same volume size as the throughput
one is just not as the throughput is lower
354:40 - here, and we're have a max I ops of 250. Moving
on to EBS magnetic, we're looking at very
354:49 - inexpensive storage for long term archival
storage, where you're going to have between
354:55 - 500 gigabytes and 15 terabytes and a max I
ops as As 40 to 200, okay, and so generally
355:03 - it's using previous generation hard drives.
So that's why we get that low cost there.
355:07 - But yeah, there is the full spectrum of volume
types for you here, on on EBS.
355:15 - So we're gonna look at some different storage
volumes starting with hard disk drives. So
355:23 - hard disk drives, is a magnetic storage that
uses rotating platters on an actuator arm
355:28 - and a magnetic head. So look, we got this
round thing, an arm and a little needle, or
355:33 - head on the end of it remind you something,
it's like a record player, right? So h acds
355:38 - is very good at writing a continuous amount
of data. So the idea is that once the arm
355:43 - goes down, you have lots of data, it's really
good at just writing a bunch of data. Where
355:48 - hard disk drives do not accelerate is when
you have many small reads and writes. Okay,
355:53 - so that's high IO, okay. And the reason why
is just think about the arm, the arm would
355:58 - have to like, lift up, move to where it needs
to right, go down and then right, and then
356:03 - lift up again. And so there's a lot of physical
movement there. And that's going to limit
356:08 - its ability to have high i O. So hard disk
drives are really good for throughput, because
356:15 - again, it's continuous amounts of data. And
so that means fast amounts of data being written
356:20 - continuously. So it's gonna be great for throughput.
But you know, the caveat here is does it does
356:25 - have physical moving parts, right? Now we're
taking a look at our next storage volume,
356:34 - solid state drives SSDs. And they're different
because they don't have any physical moving
356:39 - parts, they use integrated circuits to transport
the data and store it on to things like flash
356:46 - memory here. And so SSDs are typically more
resistant to physical shock. They're very
356:52 - quiet, because there's no moving parts. And
they have quicker access times and lower latency.
356:57 - So they're really, really good at frequently
reads and writes. So they're going to have
357:00 - a high i O, they can also have really good
throughput. But generally when we're thinking
357:05 - about SSD, we just think hio Okay, so there
you go. So looking at our last storage volume,
357:14 - here, we have magnetic tape. And if you've
ever seen an old computer, you've seen magnetic
357:18 - tape because they look like film, right? You
have these big reels of magnetic tape, and
357:24 - we still use them today, because they are
highly durable. They last for decades. And
357:29 - they're extremely cheap to produce. So if
it isn't broke, why throw it out. Now, we
357:35 - don't really use magnetic tape in this way
anymore with big spools, or sorry, reels,
357:41 - what we do is we have a tape drive down below
and you can insert a cartridge into it, which
357:46 - contains the magnetic tape. Okay, so there
you go. So I want to talk about how we can
357:56 - move our volumes around. So if you wanted
to move your volume from one AZ to another,
358:05 - what you'd have to do is you have to create
a snapshot. And then once you create that
358:10 - snapshot, you'd create an ami from that snapshot.
And then you could launch an EC two instance
358:15 - into another availability zone. Now for regions,
there's a little bit more work involved, it's
358:21 - going to be the same process to begin with,
we're going to create a snapshot. And from
358:25 - there, we're going to create an ami from that
snapshot. But in order to get into another
358:30 - region, we're going to have to copy that ami.
So we're gonna use that copy ami command into
358:35 - region B, and then we're going to launch IBC
two instance. And so that's how we're going
358:39 - to get our volumes from one region to another.
Okay.
358:48 - So now we're taking a look at how to encrypt
our root volume. And so when you create an
358:53 - EC two instance, there is a little through
the launch wizard, there is a storage step.
358:58 - And so here we can see our storage volume,
that's going to be our root. And we actually
359:03 - have a box over here where we just drop it
down and encrypt it based on the method that
359:08 - we want. This wasn't possible prior, I don't
know how many years ago was maybe last year
359:14 - or something. But um, you weren't able to
encrypt a volume on creation, but you definitely
359:19 - can now. Now what happens if we have a volume
that we created that was unencrypted, and
359:26 - now we want to apply encryption to it? Well,
we're gonna have to go through a little bit
359:31 - more effort here. And here are the steps below.
So the first thing we're going to do is take
359:34 - a snapshot of that unencrypted volume. And
then once we have that snapshot, we're going
359:39 - to copy or use the copy command to create
another snapshot. And with that, we actually
359:46 - will have the option to encrypt it. And so
we will encrypt that copy giving us an encrypted
359:52 - snapshot. And then from there, we will launch
a new EC two instance with that encrypted
359:57 - ami and then launch a new EC two instance
from That ami and so that's how we're gonna
360:01 - get an encrypted root volume. So there you
go. So when you launch an EC two instance,
360:11 - it can be backed by either an EBS volume or
an instance store volume. And there's going
360:15 - to be some cases when you want to use one
or over the other, but 99.9% of the time,
360:20 - you're going to be wanting to use an EBS volume.
And so the EBS volume is a durable block level
360:26 - storage device that you can attach to a single
EC two instance. Now the instant store volume
360:31 - is a temporary storage type located on disk
that are physically attached to a host machine.
360:39 - And so the key word here is one is durable
and one is temporary. All right. And you know,
360:45 - anytime we talk about instant stores, a Ferial
is another word that comes up. And so if you
360:51 - ever see that word, it means lasting for a
very short time. Okay, so that makes total
360:54 - sense why they're called that sometimes. And
so for an EBS volume, if you, if you want
361:00 - to use it within institutions, it's going
to the volume is going to be created from
361:03 - an EBS snapshot with an instance or store
volume, it's going to be created from a template
361:08 - stored in s3. Now, the way you use these volumes
is going to also affect the behavior you see
361:14 - too, because you can stop and start an EC
two instance. And the data is going to re
361:19 - persist when it starts back up. Again, when
you look at an instance store volume, you
361:24 - cannot stop the instance, if you terminate
it, you're going to lose all that data because
361:27 - it's a temporary storage type, you might be
able to reboot it, but you won't be able to
361:33 - stop the volume, okay, so you might just have
reboot and terminate as your only two options
361:40 - there. And also, you know, when an EC two
instance launches up, it goes through status
361:44 - checks, and so the one it goes through is
like a host check. If that were to fail, then
361:50 - also the data would be lost there. But generally,
when you're spinning up an EC two instance,
361:55 - you don't have any data you're not too worried
about any way, but that's just a consideration
361:58 - to list out there. And so we're gonna talk
about the use cases. So, so EBS volumes are
362:03 - ideal when you want data to persist. In most
cases, you'll want to use an EBS, EBS backed
362:09 - volume. And for instance store it's ideal
for temporary backup for storing an applications
362:14 - cash logs or random data. So this is data
where when the server is not running you,
362:21 - it should go away. Okay, so there you go.
That's the difference between EBS volumes
362:27 - and instant store
362:29 - volume.
362:30 - So we're done with EBS, and we're looking
at the EBS cheat sheet so let's jump into
362:36 - it. So elastic block store EBS is a virtual
hard disk, and snapshots are a point in time
362:41 - copy of that disk. volumes exist on EBS snapshots
exist on s3. snapshots are incremental only
362:48 - changes made since the last snapshots are
moved to to s3. Initial snapshots of an EC
362:53 - two instance will take a large crate then
subsequent snapshots of taking snapshots of
362:58 - a volume the EC two instance should be stopped
before snapshotting you can take snapshots
363:03 - while the instance is still running, you can
create amies from volumes or from snapshots.
363:09 - EBS volumes are just define that what they
are here a durable block level storage device
363:14 - that you can attach to a single EC two instance.
EBS volumes can be modified on the fly so
363:19 - you can increase their storage type or their
volume size. volumes always exists in the
363:23 - same AZ as easy to instance. And then looking
at instance store volumes a temporary storage
363:28 - type located on disks that are physically
attached to a host machine. Instant storage
363:32 - volumes are serial meaning that cannot be
if they cannot be stopped. If the host fails
363:38 - and you lose your data. Okay, EBS backed instances
can be stopped and you will not lose any data.
363:44 - By default root volumes are deleted on termination.
EBS volumes can have termination protection,
363:50 - so don't delete the volume on termination.
And snapshots or restored encrypted volumes
363:54 - will also be encrypted. You cannot share a
snapshot if it has been encrypted and unencrypted
364:00 - snapshots can be shared with other Eva's counts
or made public. So there you go. That's your
364:08 - EBS tg
364:10 - Hey, this is Angie brown from exam Pro. And
we are looking at CloudFront, which is a CDN
364:16 - a content distribution network. It creates
cache copies of your website at various edge
364:20 - locations around the world. So to understand
what CloudFront is, we need to understand
364:26 - what a content delivery network is. So a CDN
is a distributed network of servers, which
364:30 - deliver webpages and content to users based
on their geographical location, the origin
364:34 - of the web page and a content delivery server.
So over here, I have a graphical representation
364:40 - of a CDN specifically for CloudFront. And
so the idea is that you have your content
364:46 - hosted somewhere. So here the origin is s3
and the idea is that the server CloudFront
364:52 - is going to distribute a copy of your website
on multiple edge locations which are just
364:59 - servers around the world that are nearby to
the user. So when a user from Toronto tries
365:04 - to access our content, it's not going to go
to the s3 bucket, it's going to go to CloudFront.
365:09 - And CloudFront is going to then route it to
the nearest edge location so that this user
365:16 - has the lowest latency. And that's the concept
behind.
365:25 - So it's time to look at the core components
for CloudFront. And we'll start with origin,
365:30 - which is where the original files are located.
And generally, this is going to be an s3 bucket.
365:36 - Because the most common use case for CloudFront
is static website hosting. However, you can
365:40 - also specify origin to be an easy to instance,
on elastic load balancer or route 53. The
365:46 - next thing is the actual distribution itself.
So distribution is a collection of edge locations,
365:51 - which define how cache content should behave.
So this definition, here is the thing that
365:58 - actually says, Hey, I'm going to pull from
origin. And I want this to update the cache,
366:04 - whatever whatever frequency or use HTTPS,
or that should be encrypted. So that is the
366:10 - settings for the distribution. And then there's
the edge locations. And an edge location is
366:14 - just a server. And it is a server that is
nearby to the actual user that stores that
366:20 - cache content. So those are the three components
to so we need to look at the distribution
366:30 - component of CloudFront in a bit more detail,
because there's a lot of things that we can
366:34 - set in here. And I'm not even showing you
them all, but let's just go through it. So
366:39 - we have an idea, the kinds of things we can
do with it. So again, a distribution is a
366:43 - collection of edge locations. And the first
thing you're going to do is you're going to
366:48 - specify the origin. And again, that's going
to be s3 ec two lb or refer d3. And when you
366:53 - said your distribution, what's really going
to determine the cost and also how much it's
366:58 - going to replicate across is the price class.
So here, you can see, if you choose all edge
367:03 - locations, it's gonna be the best performance
because your website is going to be accessible
367:06 - from anywhere in the world. But you know,
if you're operating just in North America
367:11 - and the EU, you can limit the amount of servers
it replicates to. There are two types of distributions,
367:17 - we have web, which is for websites, and rtmp,
which is for streaming media, okay, um, you
367:24 - can actually serve up streaming video under
web as well. But rtmp is a very specific protocol.
367:30 - So it is its own thing. When you set up behaviors,
there's a lot of options we have. So we could
367:35 - redirect all the traffic to be HTTPS, we could
restrict specific HTTP methods. So if we don't
367:41 - want to have puts, we can say we not include
those. Or we can restrict the viewer access,
367:47 - which we'll look into a little bit more detail
here, we can set the TTL, which is time to
367:52 - expiry, or Time To Live sorry, which says
like after, we could say every two minutes,
367:57 - the content should expire and then refresh
it right, depending on how, how stale we want
368:01 - our content to be. There is a thing called
invalidations in CloudFront, which allow you
368:06 - to manually set so you don't have to wait
for the TTL. to expire, you could just say
368:10 - I want to expire these files, this is very
useful when you are pushing changes to your
368:16 - s3 bucket because you're gonna have to go
manually create that invalidation. So those
368:19 - changes will immediately appear. You can also
serve back at error pages. So if you need
368:25 - a custom 404, you can do that through CloudFront.
And then you can set restrictions. So if you
368:31 - for whatever reason, aren't operating in specific
countries, and you don't want those countries
368:38 - to consume a lot of traffic, which might cost
you money, you can just restrict them saying
368:43 - I'm blocking these countries, or or you could
do the other way and say I only whitelist
368:48 - these countries, these are the only countries
that are allowed to view things from CloudFront.
368:57 - So there's one interesting feature I do want
to highlight on CloudFront, which is lambda
369:01 - edge and lambda edge are lambda functions
that override the behavior of requests and
369:05 - responses that are flowing to and from CloudFront.
And so we have four that are available to
369:11 - us, we have the viewer requests, the origin
requests, the origin response, and the viewer
369:16 - response, okay, and so on our on our CloudFront
distribution under probably behavior, we can
369:22 - associate lambda functions. And that allows
us to intercept and do things with this, what
369:27 - would you possibly use lambda edge for a very
common use case would let's say you have protected
369:34 - content, and you want to authenticate it against
something like cognito. So only users that
369:41 - are within your cognito authentication system
are allowed to access that content. That's
369:47 - actually something we do on exam pro for the
video content here. So you know, that is one
369:52 - method for protecting stuff, but there's a
lot of creative solutions here with you can
369:57 - use lambda edge, you could use it to serve
up a to b testing websites, so you could have
370:05 - it. So when the viewer request comes in, you
have a roll of the die, and it will change
370:09 - what it serves back. So it could be, it could
set up a or set up B. And that's something
370:14 - we also do in the exam pro marketing website.
So there's a lot of opportunities here with
370:18 - lambda edge. I don't know if it'll show up
in the exam, I'm sure eventually it will.
370:22 - And it's just really interesting. So I thought
it was worth
370:24 - an hour talking about cloud front protection.
So CloudFront might be serving up your static
370:35 - website, but you might have protected content,
such as video content, like on exam Pro, or
370:41 - other content that you don't want to be easily
accessible. And so when you're setting up
370:45 - your CloudFront distribution, you have this
option to restrict viewer access. And so that
370:49 - means that in order to view content, you're
going to have to use signed URLs or signed
370:54 - cookies. Now, when you do check this on, it
actually will create you an origin identity
370:58 - access, and oh AI. And what that is, it's
a virtual user identity that it will be used
371:04 - to give CloudFront distributions permission
to fetch private objects. And so those private
371:10 - objects generally mean from an s3 bucket that's
private, right. And as soon as that set up,
371:16 - and that's automatically set up for you. Now
you can go ahead and use sign URLs inside
371:20 - cookies. So one of these things well, the
idea behind it is a sign URL is just a URL
371:26 - that CloudFront will provide you that gives
you temporary access to those private cash
371:31 - objects. Now, you might have heard of pre
signed URLs, and that is an s3 feature. And
371:37 - it's similar nature. But it's very easy to
get these two mixed up because sign URLs and
371:42 - pre signed URLs sound very similar. But just
know that pre signed URLs are for s3 and sign
371:48 - URLs are for CloudFront, then you have signed
cookies, okay. And so it's similar to sign
371:54 - URLs, the only difference is that you're you
passing along a cookie with your request to
372:02 - allow users to access multiple files, so you
don't have to, every single time generate
372:07 - a signed cookie, you set it once, as long
as that cookie is valid and pass along, you
372:12 - can access as many files as you want. This
is extremely useful for video streaming, and
372:17 - we use it on exam Pro, we could not do video
streaming protected with sign URLs, because
372:22 - all the video streams are delivered in parts,
right, so a cookie has to get set. So that
372:29 - that is your options for protecting cloud.
It's time to get some hands on experience
372:36 - with CloudFront here and create our first
distribution. But before we do that, we need
372:40 - something to serve up to the CDN. Okay, um,
so we had an s3 section earlier, where I uploaded
372:47 - a bunch of images from Star Trek The Next
Generation. And so for you, you can do the
372:52 - same or you just need to make a bucket and
have some images within that bucket so that
372:57 - we have something to serve up, okay. So once
you have your bucket of images prepared, we're
373:02 - going to go make our way to the CloudFront
console here. And so just type in CloudFront,
373:06 - and then click there. And you'll get to the
same place as me here, we can go ahead and
373:11 - create our first distribution. So we're gonna
be presented with two options, we have web
373:16 - and rtmp. Now rtmp is for the Adobe Flash
media server protocol. So since nobody really
373:22 - uses flash anymore, we can just kind of ignore
this distribution option. And we're going
373:26 - to go with web, okay. And then we're going
to have a bunch of options, but don't get
373:30 - overwhelmed, because it's not too tricky.
So the first thing we want to do is set our
373:34 - origin. So where is this distribution going
to get its files that wants to serve up, it's
373:39 - going to be from s3. So we're going to click
into here, we're going to get a drop down
373:42 - and we're going to choose our s3 bucket, then
we have path, we'll leave that alone, we have
373:47 - origin ID, we'll leave that alone. And then
we have restrict bucket access. So this is
373:50 - a cool option. So the thing is, is that let's
say you only want two people to access your,
373:58 - your bucket resources through CloudFront.
Because right now, if we go to s3 console,
374:02 - I think we made was data public, right? And
if we were to look at this URL, okay, this
374:09 - is publicly accessible. But let's say we wanted
to force all traffic through cloud front,
374:13 - because we don't we want to the conference
can track things. So we get some rich analytics
374:18 - there. And we just don't want people directly
accessing this ugly URL. Well, that's where
374:22 - this option comes in, restrict bucket access,
okay, and it will, it will create an origin
374:27 - identity access for us, but we're gonna leave
it to No, I just want you to know about that.
374:30 - And then down to the actual behavior settings,
we have the ability to redirect HTTP to HTTPS.
374:36 - That seems like a very sane setting. We can
allow these to be methods, we're only going
374:40 - to be ever getting things we're never going
to be put or posting things. And then we'll
374:46 - scroll down, scroll down, we can set our TTL.
The defaults are very good. And then down
374:50 - here, we have restrict viewer access. So if
we wanted to restrict the viewer access to
374:54 - require signed URLs of site cookies to protect
access to our content, we'd press yes here
375:00 - But again, we just want this to be publicly
available. So we're going to set it to No.
375:04 - Okay. And then down below, we have distribution
settings. And this is going to really affect
375:08 - our price of the cost we're going to pay here,
as it says price class, okay,
375:13 - and so we can either distribute all copies
of our files to every single edge location,
375:20 - or we can just say US, Canada, Europe, or
just US, Canada, yeah, Europe, Asia, Middle
375:25 - East Africa, or just the the main three. So
I want to be cost saving here. So I'm really
375:30 - going to cost us a lot anyway. But I think
that if we set it to the lowest cost here
375:33 - that it will take less time for the distribution
to replicate here in this tutorial go a lot
375:38 - faster, okay, then we have the ability to
set an alternate domain name, this is important
375:43 - if we are using a CloudFront certificate,
and we want a custom domain name, which we
375:49 - would do in a another follow along but not
in this one here. Okay, and if this was a
375:55 - website, we would set the default route here
to index dot HTML. Okay, so that's pretty
375:59 - much all we need to know here. And we'll go
ahead and create our distribution, okay, and
376:03 - so our distribution is going to be in progress.
And we're going to wait for it to distribute
376:07 - those files to all those edge locations. Okay,
and so this will just take a little bit of
376:12 - time here, he usually takes I don't know,
like three to five minutes. So we'll, we'll
376:16 - resume the video when this is done. So creating
that distribution took a lot longer than I
376:25 - was hoping for, it was more like 15 minutes,
but I think the initial one always takes a
376:30 - very long time. And then then after whenever
you update things, it still takes a bit of
376:35 - time, but it's not 15 minutes, more like five
minutes. Okay. But anyway, um, so our distribution
376:40 - is created. Here, we have an ID, we have a
domain name, and we're just going to click
376:44 - in to this distribution. And we're going to
see all the options we have here. So we have
376:49 - general origins, behaviors, error pages, restrictions,
and validations. And tags. Okay, so when we
376:54 - were creating the distribution, we configured
both general origins and behaviors all in
376:59 - one go. Okay. And so if we wanted to override
the behaviors from before, we just looked
377:04 - at it here, we're not going to change anything
here. But I just want to show you that we
377:07 - have these options previous. And just to see
that they are broken up between these three
377:11 - tabs here. So if I go to Edit, there's some
information here and some information there.
377:16 - Okay. So now that we have our distribution
working, we have this domain name here. And
377:21 - if we had, if we had used our own SSL, from
the Amazon certification manager, we could
377:27 - add a custom domain, but we didn't. So we
just have the domain that is provided with
377:32 - us. And this is how we're going to actually
access our our cache file. So what I want
377:35 - you to do is copy that there. I'm just going
to place it here in a text editor here. And
377:42 - the idea here is we want to then, from the
enterprise D poll, one of the images here.
377:47 - So if we have data, we'll just take the front
of it there, okay. And we are going to just
377:52 - assemble a new URL. So we're going to try
data first here, and data should work without
377:57 - issue. Okay. And so now we are serving this
up from CloudFront. So that is how it works
378:02 - now, but data is set to public access. Okay,
so that isn't much of a trick there. But for
378:11 - all these other ones, I just want to make
sure that he has public access here and it
378:15 - is set here yep to public access. But let's
look at someone that actually doesn't have
378:20 - public access, such as Keiko, she does not
have public access. So the question is, will
378:26 - CloudFront make files that do not have public
access set in here publicly accessible, that's
378:31 - what we're going to find out. Okay. So we're
just going to then assemble an another URL
378:36 - here, but this time with Keiko, okay, and
we're gonna see if we can access her All right.
378:42 - Okay, oops, I copied the wrong link. Just
copy that one more time. Okay, and there you
378:48 - go. So Keiko is not available. And this is
because she is not publicly accessible. Okay.
378:54 - So just because you create a CloudFront distribution
doesn't necessarily mean that these files
378:57 - will be accessible. So if we were to go to
Keiko now and then set her to public, would
379:04 - she be accessible now through CloudFront?
Okay, so now she is all right. So so just
379:11 - keep that in mind that when you create a CloudFront
distribution, you're going to get these URLs
379:16 - and unless you explicitly set the objects
in here to be publicly accessible, they're
379:21 - not going to be publicly accessible. Okay.
But yeah, that's all there is to it. So we
379:25 - created our conference. So we need to touch
on one more thing here with CloudFront. And
379:33 - that is invalidation. So, up here we have
this Keiko image which is being served up
379:38 - by CloudFront. But let's say we want to replace
it. Okay, so in order to replace images on
379:44 - CloudFront, it's not as simple as just replacing
an s3. So here we have Keiko, right, and this
379:50 - is the current image. And so let's say we
wanted to replace that and so I have another
379:55 - version of Keiko here. I'm just going to upload
it here. And that's going to replace the existing
379:59 - One, okay.
380:03 - And so I'm just going to make sure that the
new one is here. So I'm just going to right
380:08 - click or sorry, gonna hit open here, make
sure it's set to public. And then I'm just
380:12 - going to click the link here. And it still
now it's the new one, right, so here we have
380:17 - the new one. And if we were to go to the CloudFront
distribution and refresh, it's still the old
380:22 - image, okay, because in order for these new
changes to propagate, you have to invalidate
380:27 - the old, the old cache, okay, and that's where
invalidation is come into play. So, to invalidate
380:33 - the old cache, we can go in here to create
invalidations. And we can put a wildcard to
380:38 - expire everything or we could just expire.
Keiko. So, for Keiko, she's at Ford slash
380:45 - enterprise D. So we would just paste that
in there. And we have now created an invalidation.
380:49 - And this is going to take five, five minutes,
I'm not going to wait around to show you this
380:55 - because I know it's going to work. But I just
want you to know that if you update something
380:59 - in order, in order for it to work, you have
to create a validation. So it's time to look
381:08 - at the CloudFront cheat sheet. And let's get
to it. So CloudFront is a CDN a content distribution
381:13 - network. It makes websites load fast by serving
cache content that is nearby CloudFront distributes
381:19 - cached copies at edge locations, edge locations
aren't just read only you can actually write
381:25 - to them. So you can do puts to them. We didn't
really cover that in the core content. But
381:29 - it's good to know CloudFront has a feature
called TTL, which is time to live and that
381:34 - defines how long until a cache expires. Okay,
so if you set it to expire every hour, every
381:42 - day, that's how fresh or I guess you'd say
how stale your content is going to be. When
381:47 - you invalidate your cash, you're forcing it
to immediately expire. So just understand
381:52 - that invalidations means you're you're refreshing
your cash, okay? refreshing the cast does
381:58 - cost money because of the transfer cost to
update edge locations, right. So if you have
382:03 - a file, and it's and it's expired, it then
has to then send that file to 1020, whatever
382:09 - amount of servers it is, and there's always
that outbound transfer cost, okay? origin
382:14 - is the address of where the original copies
of your files reside. And again, that can
382:18 - be a three EC two, lb raffa, d three, then
you have distribution, which defines a collection
382:24 - of edge locations and behavior on how it should
handle your cache content. We have two types
382:29 - of distributions, we have the web distribution,
also known as web, which is for static website
382:35 - content. And then you have rtmp, which is
for streaming media, again, that is a very
382:40 - specific protocol, you can serve up video
streaming via the web distribution. Then we
382:46 - have origin identity access, which is used
to access private s3 buckets. If we want to
382:51 - access cash content that is protected, we
need to use sign URLs or signed cookies, again,
382:57 - don't get signed roles confused with pre signed
URLs, which is an s3 feature, but it's pretty
383:01 - much the same in terms of giving you access
to something, then you have lambda edge, which
383:07 - allows you to pass each request through a
lambda to change the behavior of the response
383:12 - or the request. Okay, so there you go. That
is cloud front in a nutshell. Hey, this is
383:21 - Andrew Brown from exam Pro. And we are looking
at relational database service RDS, which
383:26 - is a managed relational database service and
supports multiple SQL engines, easy to scale,
383:32 - backup and secure. So jumping into RDS RDS
is a relational database service. And it is
383:38 - the AV solution for relational databases.
So there are six relational database options
383:44 - currently available to us. So we have Amazon
Aurora, which we have a whole section dedicated
383:48 - on MySQL, Maria dB, Postgres, which is what
we use it exam Pro, Oracle and Microsoft SQL.
384:00 - So let's look at what we can do for encryption,
so you can turn on encryption at rest for
384:03 - all RDS engines, I've noticed that you might
not be able to turn on encryption for older
384:04 - versions of some engine. So sometimes this
option is not available, but generally it
384:05 - always is. And also, when you do turn on encryption,
it's also going to encrypt, as well as your
384:06 - automated backups, your snapshots and your
read replicas related to that database. And
384:07 - encryption is handled by AWS key management
service kms, because it always is. So you
384:08 - can see it's as simple as turning on encryption,
and you can either use the default key or
384:09 - provide another kms key that you were taking
a look here at RDS backups. Okay, so we have
384:10 - two solutions available to us starting with
automated backups, we're going to do is you're
384:11 - going to choose a retention period between
one and 35 days. Now generally, most people
384:12 - we're going to set this to seven, and if you
were to set it to zero, that's actually how
384:13 - you turn
384:14 - it off. So when they say that, automated backups
are enabled, Default, they just mean that
384:15 - they fill it in with like seven by default
for you. And you can just turn that to zero,
384:16 - it's going to store transaction logs throughout
the day, all the data is going to be stored
384:17 - inside s3, and there is no additional charge
for those. Those backups, okay, you're going
384:18 - to define when you want backups do occur through
a backup windows. So here you can see UTC
384:19 - six, and the duration can't be longer than
a half an hour. And then storage and IO may
384:20 - be suspended during backup. So just understand
that you might have some issues during that
384:21 - period of time. So you might really want to
choose that time carefully. Now, the other
384:22 - way is manual snapshots. And all you have
to do is drop down actions and take a snapshot.
384:23 - So it is a manual process. Now, if your primary
database or your RDS was instance was deleted,
384:24 - you're still going to have the snapshot. So
if you do want to restore a previous snapshot,
384:25 - you totally can do that. Okay, they don't
go when you delete the RDS.
384:26 - So let's learn how to actually restore a backup
now. And it's as simple as dropping down actions
384:27 - here and choosing restore to point in time.
So when recovering, AWS will take the most
384:28 - recent daily backup and apply transaction
log data relevant to that day. This allows
384:29 - point in time recovery down to a second inside
the retention period. backup data is never
384:30 - restored over top of an existing instance,
what it's going to do is when you restore
384:31 - an automated backup, or manual snapshot, it's
going to create a new instance, from that
384:32 - create, for the created restored database,
okay. And so when you do make this new restored
384:33 - RDS instance, it's going to have a new DNS
endpoint. And you are going to have to do
384:34 - a little bit of manual labor here because
you're going to want to delete your old instance,
384:35 - and then use this new endpoint for your applications.
Okay. So we're going to be looking at multi
384:36 - AZ deployment. And this ensures your database
remains available if another AZ becomes unavailable.
384:37 - So what it's going to do, it's going to make
an exact copy of the database in another availability
384:38 - zone, and it is automatically going to synchronize
those changes from the primary database, the
384:39 - master database over to the standby database.
Okay. So the thing about this standby is that
384:40 - it is a slave database, it is not receiving
any real time traffic, it is just there's
384:41 - a backup to take the place of the master database
in the case of the AZ goes down. So over here
384:42 - we have automatic failover protection. So
if the AZ does go down, then failover will
384:43 - occur, it's going to point there's like a
URL or address that that says that points
384:44 - to the database. So it's going to point to
the slave and the slave is going to be promoted
384:45 - to master and now it is your master database.
All right, so that's multi AZ going to take
384:46 - a look at Reed replicas, and they allow you
to run multiple copies of your database. These
384:47 - copies only allow reads, so you can't do writes
to them. And it's intended to alleviate the
384:48 - workload of your primary database, also known
as your master database to improve performance.
384:49 - Okay, so in order to use read replicas, you
must have automatic backups enabled. And in
384:50 - order to create a replica, you're just dropping
down actions here and hitting create read
384:51 - replica as easy as that. And it uses a synchronous
replication between your master and your read
384:52 - replica. So you can have up to five replicas
of the database, each read replica will have
384:53 - its own DNS endpoint. You can have multi AZ
replicas, cross region replicas, and even
384:54 - replicas of replicas. replicas can be promoted
to their own database, but this will break
384:55 - replication, which makes a lot of sense, and
no automatic failover. So if the primary copy
384:56 - fails, you must manually update your roles
to point at copy.
384:57 - So now it's time to compare multi AZ and read
replicas because it's very important to know
384:58 - the difference between the two. So for replication,
multi AZ has synchronous replication and read
384:59 - replicas have a synchronous replication for
what is actually active on multi AZ it's just
385:00 - gonna be the primary instance. So the standby
doesn't do anything, it's just there. If the
385:01 - primary the primary instance becomes unavailable,
then it becomes the primary instance where
385:02 - read replicas the primary and all the replicas
are being utilized. Okay, we're going to have
385:03 - autom automated backups are taken from the
standby word read replicas there are no backups
385:04 - configured by default, multi AZ as the name
implies, will span to azs. within a single
385:05 - region. replicas are within a single AZ but
they can be multi AZ cross az, or cross region.
385:06 - Okay. When upgrades are occurring, it's going
to happen on the primary database for multi
385:07 - AZ when upgrade upgrades. To create and read
replicas, it's going to be independent from
385:08 - the source instance. And then lastly, we have
failover. So automatic failover will happen
385:09 - to standby. And so for read replicas, you're
not going to have that automatic failover,
385:10 - you're gonna have to manually promote a, one
of those replicas to become the standalone
385:11 - data. Hey, it's Angie brown from exam Pro,
and we are looking at Amazon RDS. And so we
385:12 - are going to create our own RDS database,
as well as an aurora database, as well as
385:13 - looking how to migrate from RDS to Aurora.
And also maybe we'll look at some backup solutions
385:14 - there and some of the other superflous features
that RDS supplies us. If you're wondering
385:15 - how do you get to the console here, you go
the top here, type RDS, and click that and
385:16 - you will end up in the same place as I am.
So let's get to it and create our first database.
385:17 - Okay, and we're going to do that by going
on the left hand side here to databases, and
385:18 - clicking Create database. So here we are in
the RDS creation interface. And the first
385:19 - thing we're presented with is with crates,
standard crate and easy crate, I assume that
385:20 - this would eliminate some options for us,
we're going to stick with standard, because
385:21 - we want to have full control over what we're
doing here of the next thing is to choose
385:22 - your engine. And so we're going to Aurora
later. And we're going to spin up a Postgres
385:23 - database, which is very popular amongst Ruby
on Rails developers, which is my primary web
385:24 - framework that I like to use, then under templates,
this is a more configuration for you that
385:25 - allows you to get started very easily. So
if you leave this to production here, I want
385:26 - to show you the cost because it's laughably
really inexpensive. It's $632, because it's
385:27 - doing a bunch of stuff here is running more
than one AZ it's using a very large EC two
385:28 - instance, and it has provisioned I Ops, okay.
And so for our use case, I don't think we
385:29 - want to spend $632. But if you were like an
enterprise, it makes sense why they would
385:30 - do that. But if you aren't paying attention,
that's very expensive. And there's obviously
385:31 - the free tier, which is what we will want
to use here. But we will configure this so
385:32 - that it will end up as free tier here. And
we will learn the options as we go through
385:33 - it. So the first thing is we're going to have
to have a database, so we'll just keep our
385:34 - database name as database one, then we need
to set a master password. So we will set it
385:35 - as Postgres. And we'll see if we can get to
get away with that. Obviously, when you make
385:36 - your real password, you'd use a password generator,
or let it auto generate one. So it's very
385:37 - long in length. But I just want to be able
to work with this database very easily for
385:38 - the purpose of you know, this, this follow
along, okay, then we have our DB instance
385:39 - size. So for the DB instance size, you can
see it's set to standard classes m, I would
385:40 - say the standard class is more like burstable,
which are t twos. And so this is what we're
385:41 - used to when we're saving money. So we have
the T two micro if you are a very small startup,
385:42 - you would probably be starting on T two micro
and do totally fine for you. So we're going
385:43 - to change it to T two micro okay. And the
next thing is storage class. so here we can
385:44 - choose I provisioned I ops. And so we have
faster, oh, I Ops, right, but we're gonna
385:45 - go general here, because we don't need, we
don't need that crazy amount of I ops there.
385:46 - And that's going to reduce storage, there
is this ability to do storage, auto scaling.
385:47 - So this is kind of nice, where it dynamically
will scale your database to your needs. Um,
385:48 - I think I will just leave that on, I don't
see why we wouldn't want to keep that on unless
385:49 - there's additional cost to don't believe there
is. Then there's multi AZ and so that would
385:50 - set up a nother database for us in another
availability zone in a standby, I don't think
385:51 - we need that. So we're going to turn that
off. But just to show you how easy it is to
385:52 - turn that on, then we're going to need to
choose our VPC, it's very important, whatever
385:53 - web application you are deploying that your
your RDS databases in the same VPC or it's
385:54 - going to be a bit of trouble trying to connect
to it. So we'll just leave that in the default
385:55 - there. There's some additional
385:56 - connectivity options here. And so we is a
subnet group. So we're gonna have to default,
385:57 - we're, we're gonna ask whether we want this
to be publicly accessible. Generally, you're
385:58 - not going to want a public IP address. But
if we want to interact with this database
385:59 - very easily, I'm going to set it to Yes, for
the for the sake of this follow along, because
386:00 - it would be nice to put some data into this
database, interact with it with table plus,
386:01 - and then we'll go ahead and delete it. Okay.
Then down below, we have a VPC security group,
386:02 - I'm thinking that it will probably create
one for us by default, so we'll leave it with
386:03 - the default one, which is totally fine. We
can choose our preference for AZ I don't care,
386:04 - we'll leave it by default. And then we have
the port number 5432, which is the standard
386:05 - port number there. You might want to change
it just for the sake of security reasons.
386:06 - Because if you change the port number then
people have to guess what it is. And there's
386:07 - some additional configurations here. So what
we have is the initial database name, if you're
386:08 - not specify when RDS does not create a database
Okay, so we probably want to name our database
386:09 - here. So I'm just going to name the database
one here. You can also authenticate using
386:10 - IMDb authentication. So if that is one way
you want to authenticate to your database,
386:11 - that is definitely a really nice way of doing
that. So you might want to have that checkbox
386:12 - on, then you have backups, okay, and so backups
are enabled automatically, and they're set
386:13 - for seven days. If you want to turn backups,
which I definitely do, I'm gonna set it to
386:14 - zero days, if we had left that on and created
our RDS instance, it would take forever to
386:15 - create because immediately after it starts
up, it's going to then create a backup. And
386:16 - that just takes a long time, you can set the
backup window here and select when you want
386:17 - to have it run, there is the chance of interruptions
during a backup window. So you definitely
386:18 - want to pick this one, it's it's not the most
important usage by your users, we can enable
386:19 - performance insights, I'm pretty sure I thought
that was only accessible for certain classes
386:20 - of database performance, advanced database
performance monitoring, and offers free tier
386:21 - for seven days of rolling, okay, sure, we'll
turn it on. But at one point, you had to have
386:22 - a higher tier to be able to use this, then
we have the retention period here for performance
386:23 - insights, I guess it's seven days we'll leave
that there appear appears to be encrypted
386:24 - by default. That seems like a good thing there.
There's an account, Id kind of ignore my kindly
386:25 - ignore my account ID for this account. But
this is a burner account. So it's not like
386:26 - you're going to be able to do anything with
that ID. We have enhanced monitoring, I don't
386:27 - need that. So I'm going to turn that off,
that seems kind of expensive. We can export
386:28 - our logs, that is a good thing to have. We
can have it so it automatically upgrades minor
386:29 - versions, that is a very good thing to set,
you can also set the window for that, we can
386:30 - turn on deletion protection, I'm not going
to have that on because I'm going to want
386:31 - to delete this lickety split. And so there
we go. And so it says 1544. But I know that
386:32 - this is free tier because it's using the T
to micro and so we get things so even though
386:33 - it doesn't say it's free, I definitely know
it's free. But this gives you the true costs.
386:34 - So after your feature would run out, this
is what it would cost about $15 per month
386:35 - at the lowest tier, the lowest thing you can
get on on AWS for for your RDS instance. Okay,
386:36 - so let's go ahead and create that database.
So I failed creating my database, because
386:37 - it turns out database is a reserved word for
this engine. And that's totally fine. So we're
386:38 - gonna have to scroll up here and change the
database name. And so I'm just going to change
386:39 - it to Babylon five. And we're going to go
ahead and create that. And this time, we're
386:40 - going to get our database. And so now we're
just waiting for our database to be created
386:41 - here. And so we just saw the region AZ pop
into existence here, and it is currently being
386:42 - created, you might have to hit refresh here
a few times. So we'll just wait a little bit
386:43 - here until this status has changed. So our
database is available. And now we can actually
386:44 - try to make a connection to it and maybe put
in some SQL and run a query. And so before
386:45 - we can try to even make a connection, we need
to edit our security group, okay, because
386:46 - we are going to need access via Port 5432
to connect to that instance there. So we'll
386:47 - just edit our inbound rules. And we're going
to drop down and look for Postgres in here.
386:48 - So there's 5432. And we're gonna set to only
rip because we don't want make this publicly
386:49 - accessible. And we will hit save, okay. And
so now if we want to make connection, we should
386:50 - have no trouble here. I'm just going to close
that tab, and we're going to have to collect
386:51 - some information, you're going to need to
use a tool such as table Plus, if you are
386:52 - on Mac or Windows, it's free to download and
install if you're on Linux, or you could use
386:53 - B, I think it's called D Beaver. Okay, so
that's an open source
386:54 - SQL tool here. And so I'm gonna just make
a new connection here. And we're just going
386:55 - to choose Postgres, we're going to fill in
some information. So I called this Babylon
386:56 - five. Okay, and that was the name of the database
as well, Babylon five. All right, and the
386:57 - username was Postgres. And the very, not secure
password is, is Postgres as well, again, if
386:58 - you are doing this for production, or any
case, you should really generate a very long
386:59 - password there. And then we need the host,
the host is going to be this endpoint here,
387:00 - okay. And the port is 5432 by default, so
I don't have to do anything special here.
387:01 - I'm just gonna hit test and see if we connect.
Okay, and so it went green. So that is great.
387:02 - I'm gonna hit save. So I can save myself some
trouble here. I will just double, double click
387:03 - and make a connection. There's a bit of latency
here when you are connecting to RDS and just
387:04 - running things. So if you don't see things
immediately, I just give it a little bit time
387:05 - or hit the refresh here. But I already have
a SQL script prepared here. I'm just going
387:06 - to show it to you. So this is a script and
what it does, actually should have been shouldn't
387:07 - have been called the Babylon five database
because I'm mixing Star Trek with Babylon
387:08 - five. Ridiculous, right? But this is a bunch
of starship classes from Star Trek and a bunch
387:09 - of starships from Star Trek here. And I'm
going to run the script to get us some data
387:10 - here. Okay, and so if we do it for table plus,
we're gonna go import from SQL dump. And I
387:11 - have it on my desktop here called Starfleet
ship registry, I'm gonna hit open. Okay, I'm
387:12 - going to import, I'm just going to run that
script and import us our data here into Postgres.
387:13 - Now, if you're using a different database
type is MySQL, Oracle, I can't guarantee that
387:14 - this will work. But it will definitely work
for Postgres, because SQL does vary based
387:15 - on engines, okay, and so it says it's successfully
done, it even tells us to do a refresh here,
387:16 - there is a nice refresh button up there that
you can click, and we're gonna wait for our
387:17 - tables to appear. So there we are, we have
our ship classes. And we also have our starships.
387:18 - Okay, I just want to run one query here to
make sure queries are working, I'm sure it's
387:19 - gonna work. And we're gonna go over here,
and we're just going to get out, we're going
387:20 - to want to pull all the starships that are
of ship class defiant. Okay, and we'll just
387:21 - make a new query here and or run the old say,
run all Okay, and so there you go. So we're
387:22 - getting data. So that's how you can connect
to your RDS database, you just have to open
387:23 - up that that port number there, if you are
to connect this to your web application, what
387:24 - you probably want to do for your security
group, is to just allow 5432 to the security
387:25 - group of the web application. Okay, so here,
I gave my access to my IP, right. But you
387:26 - know, you just have whatever your your security
group is, you know, here. So if you had one
387:27 - for your EC two instances, your auto scaling,
auto scaling group that holds two instances,
387:28 - you just put that in there. Alright. So now
that we have our database running, I figured
387:29 - it'd be cool to go check out performance insights,
which I'm really excited about, because this
387:30 - service used to be only available to a certain
expensive tier, so you had to pay, I don't
387:31 - know, it was like, like a T to large before
you can utilize this. But now it looks like
387:32 - AWS has brought it down all the way to the
T to micro. And it just gives you some rich
387:33 - performance insights into your application
here. So here actually ran that query, it
387:34 - actually shows me kind of the performance
over time. So this is really great to see
387:35 - here. I bet if I was to perform another query,
it would repair so I could probably just run
387:36 - the same one here. And we could just change
it to a different class. So we're just going
387:37 - to go here and change it. Let's pick one at
random like this one here. Okay. And I'll
387:38 - just run that there. And so I ran that query.
And I'm not sure how real time this is, because
387:39 - I've actually never had a chance to use it
until now, because I just never, never wanted
387:40 - to upgrade for that there. So it looks like
it does take a little bit of time for those
387:41 - queries to appear. But I did run a query there.
So I bet it will come in, it probably is he
387:42 - says fast past five minutes. So I'm going
to assume that it's at a five minute interval.
387:43 - So if we waited five minutes, I'm sure this
query would show up there. But just nice to
387:44 - know that you can get these kind of rich analytics
here, because normally, you'd have to pay
387:45 - for data dog or some other third party service.
And now it's free with AWS. So I just want
387:46 - to quickly show you that you can reserve instances
with RDS just like EC two, and you can start
387:47 - saving money. So just go to the reserved instances
tab here
387:48 - and go to purchase a reserved DB instances.
And we're going to have to wait a little bit
387:49 - of time here for this to load, it's probably
because it's getting the most up to date information
387:50 - about pricing. And so what we're going to
do is just go through this here and just kind
387:51 - of get an idea of the difference in cost.
So we're going to drop down and choose Postgres
387:52 - as our database, I always seem to have to
select that twice. Okay, but now I have Postgres
387:53 - selected, we are using a T to micro Okay,
we are not doing multi AZ one term seems great
387:54 - to me, we will first start with no upfront,
we only want one DB instance. And we'll look
387:55 - at what we're getting. So here, it's going
to tell us what we're going to save. So it's
387:56 - gonna say it's at 0.0 14 cents per hour. So
to compare this, I have the pricing up here.
387:57 - So for a tea to micro, it is a point 00 18
there, okay? And so that's your savings there.
387:58 - So if you just fiddle with this, you'll see
now it's 007, which is considerably cheaper,
387:59 - and you have all up front and that can't be
right. So 000 I guess that would be the case
388:00 - because you you've already paid for it. So
there'll be no hourly charge that makes total
388:01 - sense. But now you have an idea of what that
cost is for the year. So for $111 your, your
388:02 - your, your cost is totally covered for you.
And so if we wanted to actually calculate
388:03 - the full cost here, we would just go here
and generally the number is between watching
388:04 - grab the full full price here to get a comparison.
Where is our TT micro buddy here? Here it
388:05 - is. So I always just do 730 times because
that's generally how many hours there are
388:06 - in a month, seven 730 by that so that you
have basically a $14 charge. So you say 14
388:07 - times 12. Okay. And so it's $168 for the year.
So if you're paying upfront for one year,
388:08 - we are saving about 50 bucks. If we go on
for three years, we're saving more money,
388:09 - I'm not gonna do the math on that, but you
get the idea. So just be aware those options
388:10 - are available to us at the T to micro stage,
it's not a huge impact. But when you get to
388:11 - these larger instances, you realize you definitely
want your savings. Okay, so yeah. So I'm going
388:12 - to show you how to create a snapshot for your
database, it's pretty darn straightforward.
388:13 - We're going to go into our database, go to
our maintenance and backups. If we had backups,
388:14 - you know, they'll be turned on here. And so
just to take a snapshot, which is the manual
388:15 - process of backing up, we can name our snapshot,
whatever he wants, like to say, first snapshot
388:16 - there. Okay, and then we'll just press take
snapshot. And it's just going to go from into
388:17 - creating state. And we're just going to now
wait for that snapshot to complete. So our
388:18 - snapshot is now available to us. And so there's
a few things we can do with it. This only
388:19 - took about seven minutes. I didn't wait that
long for the snapshot here. But if we go to
388:20 - the top here to actions, there's a few things
we can do, we can restore our snapshots. So
388:21 - that's the first thing we're going to look
at here. And so you're gonna be presented
388:22 - with a bunch of options to pretty much spin
up a new RDS instance here. And so the reason
388:23 - why you might want to do this is you have
a database, and you have outrun or outlived
388:24 - the size that you're currently using. So if
you're using that T to micro, which is super
388:25 - small here, we'll just use, we'll just show
t three micro here as an as an example. And
388:26 - you wanted to increase the neck size, you
would do so. And you could also switch to
388:27 - multi AZ change your storage type
388:28 - here etc. And then you could restore it, which
will spin up a new RDS instance, okay. And
388:29 - then you just kill your old one and move your
endpoints over to this one. Alright.
388:30 - So that's one thing we can do here. With restoring
a snapshot, the other is migrating a snapshot,
388:31 - okay. And so we'll look into that next here.
So just before we get onto a migrate snapshot,
388:32 - let's take a look at copy and share. So copy
allows you to move your snapshot to another
388:33 - region. So if you need to migrate your snapshot
somewhere else, this is how you're going to
388:34 - go about doing that. And then you can also
enable encryption. So if you don't have encryption
388:35 - enabled, this is a good opportunity for you
to encrypt your snapshots. So when you launch
388:36 - an RDS instance, it will be encrypted. Okay,
just like an easy to instance. And so then
388:37 - we have the ability to share. So now let's
say you wanted to make this snapshot available
388:38 - to other people via other AWS accounts, where
you'd add their ID here. And so now they would
388:39 - be able to reference that snapshot ID and
utilize it. Or you can also set it to public.
388:40 - So that anyone, anyone could access the snapshot.
But you know, we're we're just going to leave
388:41 - that alone, just so you are aware of those
two options. Now the one that is of most interest
388:42 - is migrating. So this is how you're going
to create an aurora database, okay, so you
388:43 - can just directly create an aurora database,
but if you wanted to migrate from your RDS,
388:44 - Postgres to Aurora, Postgres, this is how
you're going to go about it. Okay, so we're
388:45 - just going to choose, obviously, Aurora, Postgres,
because we're dealing with a Postgres database
388:46 - here, that we have our engine version, okay.
So this is an opportunity where we could upgrade
388:47 - our version, we're going to change our instance
class. Now, Aurora instances are a lot larger
388:48 - than your normal RDS instances. So we're not
going to have a teaching micro here, you might
388:49 - want to skip this step, because it is kind
of expensive, and you might forget about it.
388:50 - So you don't want to leave this thing running.
So down below, I'm gonna just choose t to
388:51 - medium because that is the least expensive
option I have here. And I'm just going to
388:52 - end up doing this anyway. So it's not a big
deal. Then we can choose our VPC, we're going
388:53 - to leave it to the default here, we can make
it publicly accessible. I'm gonna leave it
388:54 - publicly accessible here, because I don't
care. Um, and yeah, so there you go. And we'll
388:55 - just scroll down here, and we will migrate.
Okay. And so you might get a complaint here,
388:56 - sometimes I get that. And so what I normally
do is I just go ahead and hit migrate again.
388:57 - Okay. Let me just drop down the version, maybe
it won't let us do it for version 10.6. Okay,
388:58 - and we'll hit migrate one more time. Funny,
as soon as you choose 10.7, you have to re
388:59 - choose your instance class there. So I'll
go back to T two t three medium there and
389:00 - now hit migrate. Okay, so now it's going to
go ahead and create that cluster there. So
389:01 - it's going to go ahead and create that cluster.
You can see here, we had a two to previous
389:02 - failed attempts there when I hit a save there,
so those will vanish. But we're just going
389:03 - to wait a while for this to spin up. So our
migration has completed. And so our RDS instance
389:04 - is now running on Aurora. So let's just take
a quick peek inside of here, it did take a
389:05 - considerable amount of time, I think I was
waiting about like 20 minutes for this Aurora
389:06 - instance to get up here. And so right away,
you're gonna see that we have a cluster. And
389:07 - then we have the writer underneath.
389:08 - We have
389:09 - two endpoints, one for writing and one for
reading. And you can obviously create your
389:10 - own custom endpoints here. But we're just
going to go back here and I just want to show
389:11 - you that you can connect to this database.
So going back to table plus, we're going to
389:12 - create a new connection and inherited all
the settings from our previous database. So
389:13 - just grabbing the reader endpoint here, I'm
just going to paste in the host name. We called
389:14 - the user was Postgres. The password was Postgres.
Not very secure password. By the way, the
389:15 - database is called Babylon five. Okay, and
we'll just say this is our Aurora, Aurora,
389:16 - Babylon five. babbie. Ilan five. I don't know
why having such a hard time spelling that
389:17 - today. is okay. I think I spelt it wrong there.
Oh, okay. But anyway, let's just test our
389:18 - connection to see here if it works definitely
have spelled something wrong here. Okay, there
389:19 - we go. So it's the same the same credentials,
right, just the host has changed. And I can
389:20 - obviously connect to that. And we will see,
we'll have read only access to our data there.
389:21 - So yeah, it's the same process. Yeah, and
there you go. So just just to peek around
389:22 - here, you can create additional readers. So
you know, so you have more read replicas,
389:23 - we also have this option for activity stream,
which is for auditing all the activity. So
389:24 - this might be for an enterprise requirement
there for you. But we're pretty much done
389:25 - with this cluster here. So I'm just going
to go to databases here. And I'm just going
389:26 - to terminate it here. And so when we want
to terminate that here, we have, we can go
389:27 - down here and just delete, and we just type
in delete me. Okay, and that's going to take
389:28 - out the whole thing here. Okay. So once this
is done here, we'll just have to hit refresh
389:29 - there. And this will take a considerable long
time, see, it's deleting both, then this URL
389:30 - will be gone here. So yeah, there you are.
So um, we created an RDS Postgres database,
389:31 - we connected to it. We created, we migrated
it to Aurora. But I wanted to show you a little
389:32 - bit more with Aurora. Because I don't feel
like we got to look at all the options here.
389:33 - And we're only going to be able to see that
by creating a new instance here. So we're
389:34 - going to stick with the standard create there,
we're going to have Amazon Aurora, we have
389:35 - the option between MySQL and Postgres, we're
going to select Postgres, and which is going
389:36 - to have on the version 10.7 there. And what
I really want to show you here is this database
389:37 - feature setting. So we had this set here,
which had one writer and multiple readers.
389:38 - And so you're continuously paying for
389:39 - for
389:40 - Aurora there, and it's very expensive. But
we have this option called serverless. And
389:41 - serverless is a very inexpensive option. So
let's say we were building a web application,
389:42 - and it was in development, so only some few
clients were using it, or it was only just
389:43 - be using being used sporadically throughout
the month, not a lot of usage, then serverless
389:44 - is going to be a very cost effective option
for us to use Aurora and also a way for us
389:45 - to scale up to using Aurora when we need to
full time. Okay, so what I'm going to do is
389:46 - just go and set up a serverless. Aurora database
here, we're going to have a call database
389:47 - to we're going to also call it Postgres give
it that very weak password. And this is the
389:48 - big thing here. So we have this capacity setting.
So this is only showing up because we have
389:49 - serverless I'm pretty sure if we checkbox
that off, it doesn't appear. So now just we
389:50 - just choose our DB instance size, okay, but
we're gonna go back up here and go to serverless.
389:51 - And so the idea here is that we are choosing
I believe it's called ACU is the acronym for
389:52 - this capacity in here, but we're gonna choose
our minimum and our maximum. So at our minimum,
389:53 - we want to use two gigabytes of RAM and a
maximum we want that okay, and we have some
389:54 - scaling options here, which we're going to
ignore, we're going to launch this in our
389:55 - default VPC. And then he did they do have
a clear warning here. Once you create your
389:56 - database, you cannot change your VPC selection.
But I mean, that's the case of these two instances
389:57 - or whatever you can't you always have to create
a new one right? But I guess some people aren't
389:58 - aware of that. We are going to leave these
alone here. There is this option here for
389:59 - websites Data API. So this allows you to access
and run SQL via an HTTP endpoint. This is
390:00 - extremely convenient way of accessing your,
your, your database here, and it's only available
390:01 - here because we are using serverless. Okay.
And this is the same thing with like the Query
390:02 - Builder. So if you're using the query builder,
query editor, which is called that there,
390:03 - by having this enabled, then we can do both,
okay, and we can have a retention period,
390:04 - I'm going to set it to one day, I wish I could
set it to zero. But with Aurora, you have
390:05 - to have something set up, you can't have backups
turned off. And it's going to have encryption
390:06 - by default. So you can see that it's really
making sure that we make all the smart decisions
390:07 - here, and we have deletion protection, I'm
gonna turn that off, because I definitely
390:08 - want to be able to delete this, and we're
gonna hit create database. Okay, so there
390:09 - you go, we're going to just wait for that
to create, and then we're going to see how
390:10 - we can use it with the query editor here,
maybe loaded up with some data, and etc. So
390:11 - our service is now available to us. And so
let's go actually connect to it and play around
390:12 - with our server. And in order to connect to
Aurora serverless is a little bit different,
390:13 - because you have to be within the same VPC,
we're not going to be able to use table plus.
390:14 - So in order to do or to connect to it, we're
gonna have to launch an EC two instance. But
390:15 - to make things really easy, we're going to
use Cloud Nine, okay, because cloud nine is
390:16 - a an ID that's backed by easy to instance,
it already has the MySQL client installed.
390:17 - So it's going to make it really easy for us.
So what I want you to do is go to services
390:18 - here and type in cloud nine. And we will make
our way over to the cloud nine console here.
390:19 - And we'll create ourselves a new environment.
And so I'm just going to call this MySQL,
390:20 - Aurora serverless, okay, because that's all
we're gonna use this for. Okay. And we're
390:21 - gonna hit next step. And we're going to create
a new EC two instance, we're gonna leave it
390:22 - at T two micro, the smallest instance there,
we're going to launch it with Amazon Linux,
390:23 - it's going to shut down automatically after
30 minutes. So that's great for us. And we'll
390:24 - go ahead and hit next step. Okay, and then
we will create that environment. And now we
390:25 - just have to wait for that ID to spin up here.
Okay. So it shouldn't take too long. just
390:26 - takes a few minutes. All right. So our cloud
nine environment here is ready here. And down
390:27 - below, we have our environment. And so I can
type MySQL, okay. And you can see that the
390:28 - client is installed, but we didn't actually
specify any information. So there's no way
390:29 - it's going to connect anything here. But let's
go ahead and let's go to our RDS because we
390:30 - need to prepare this so we can actually make
a connection here for cloud nine. So let's
390:31 - go into the database here and grab this endpoint
here. And I've actually prepped a little follow
390:32 - over here with the stuff that we need. So
we're gonna need to prepare this command.
390:33 - But before we even do this, okay, we are going
to need to update our security group, okay,
390:34 - because we're going to need to grant access
to the security group of that EC, or that
390:35 - cloud nine environment. So also, on the right
hand side, we'll open our left hand side,
390:36 - we'll open up school groups, again here. And
we're going to look for this, this person's
390:37 - or this a cloud environment. So we have one,
here, it's this one up here. Okay, and so
390:38 - I just need this, the actual name, the group
ID of the security group. And we'll go back
390:39 - to our serverless, or our service security
group here, and we're going to edit it here,
390:40 - this looks like it's using the default one,
which is kind of a mess, we shouldn't be using
390:41 - this one. But I'm going to go ahead here and
just remove those, and we'll drop down and
390:42 - choose MySQL, MySQL, Aurora, if I can find
it in here. There it is. Okay, and we'll just
390:43 - paste that in there. And so that is going
to allow the cloud nine environment to connect
390:44 - to, or have permission to connect to the Aurora
serverless there. So going back to our environment,
390:45 - now we're ready to try out that line. So here
is our line here. And so we're just going
390:46 - to copy that whole thing in there and paste
it in. Okay, it's gonna prompt for password,
390:47 - and we made it password 123. And there we
are. So we're connected to our database there.
390:48 - And so if we wanted to create whatever we
want, it would just be as we were doing with
390:49 - Postgres there. So there you go. That's, that's
how you create a Aurora serverless database.
390:50 - And that's how you would go about connecting
to it. So now it's just time to do a bit of
390:51 - cleanup here. So we are not incurring any
costs. Now, Aurora serverless, doesn't cost
390:52 - any money while it's running. So it's not
going to cost you anything. I did terminate
390:53 - these other instances earlier on. So you just
have to go to the top here and hit Delete.
390:54 - And we don't want to create a final snapshot
and we will delete that cluster. The other
390:55 - thing that we need to consider deleting is
this cloud nine environment again, it will
390:56 - automatically shut down after 30 minutes.
So it's not going to cost you things the long
390:57 - term but you know just to keep it out of your
account. You can go ahead here and delete.
390:58 - You can see I was attempting An earlier one
here with Aurora serverless Postgres that
390:59 - didn't work and I messed up the cloudformation
templates, I can't get rid of that one, but
391:00 - this one will delete here and that's all I
got to do to clean up for this section of
391:01 - the RDS cheat sheet and this one is a two
pager. So RDS is a relational database service
391:02 - and its AWS solution for relational databases.
Artists instances are managed by AWS so you
391:03 - cannot SSH into the VM running the database.
There are six relational database options
391:04 - currently available. So we have Aurora, MySQL,
Marya dB, Postgres, Oracle and Microsoft SQL
391:05 - Server. Multi AZ is an option you can turn
on which makes an exact copy of a database
391:06 - and another AZ that that is only a standby
for multi AZ it is automatically synchronizes
391:07 - changes in the database over to the standby
copy. Multi AZ has automatic failover protection
391:08 - so if one AZ goes down, failover will occur
and the standby slave will be promoted to
391:09 - master. Then we have read replicas replicas
allow you to run multiple copies of your database.
391:10 - These copies only allow reads and no writes
and is intended to alleviate the workload
391:11 - of your primary database to improve performance.
replicas use a synchronous replication, you
391:12 - must have automatic backups enabled to use
read replicas. You can have up to five read
391:13 - replicas you can combine read replicas. With
multi AZ you can have read replicas in another
391:14 - region. So we have cross region read replicas,
read replicas can be promoted to their own
391:15 - database. But this breaks replication. You
can have read replicas of read replicas, RDS
391:16 - has two backup solutions. We have automated
backups and database snapshots to be manual
391:17 - snapshots. But it means the same thing. So
automated backups, you choose a retention
391:18 - period between one and 35 days, there is no
additional cost for backup storage, you define
391:19 - your backup window, then you have manual snapshots.
So you might you manually create backup backups.
391:20 - If you delete your primary, the manual snapshots
will still exist, and you can they can be
391:21 - restored. When you restore an instance it
will create a new database, you just need
391:22 - to delete your old database and point traffic
to the new restore database. And you can turn
391:23 - on encryption at rest for RDS via kms. So
there you go. That's
391:24 - it. This is Angie brown from exam Pro. And
we are looking at Aurora which is a fully
391:25 - managed Postgres or MySQL compatible database,
designed by default to scale and is fine tuned
391:26 - to be really really, really fast. Looking
more here at Aurora, it combines the speed
391:27 - and availability of a high end database with
the simplicity and cost effectiveness of an
391:28 - open source database. So Aurora can either
run on MySQL or Postgres compatible engines.
391:29 - And the advantage of using Aurora over just
a standard RDS, Postgres or MySQL engine,
391:30 - is the fact that it's fine tuned to be super
for performance. So if you're using MySQL,
391:31 - it's five times faster than your traditional
MySQL. And the Postgres version is three times
391:32 - more performant than the traditional Postgres.
And the big benefit is the cost. So it's 1/10
391:33 - 10th the cost of other solutions offering
similar performance and availability.
391:34 - So let's talk about Aurora scaling, which
is one of its managed features. So it starts
391:35 - with 10 gigabytes of storage initially, and
can scale in 10 gigabyte increments all the
391:36 - way up to 64 terabytes, so you have a lot
of room for growth here. And storage is auto
391:37 - scaling. So just happens automatically. for
computing power computing, resources can scale
391:38 - all the way up to 32 VPC use, and up to 244
gigabytes of memory.
391:39 - Let's take a look at aurors availability and
you can see that it's extremely available
391:40 - because it runs six copies of your data across
three availability, availability zones, with
391:41 - two in each single AZ Okay, so if you were
to lose two copies of your data, it would
391:42 - not affect right availability. If you were
to lose three copies of your data, it would
391:43 - not affect read availability. So this thing
is super super bomb. Now looking at fault
391:44 - tolerance and durability for Aurora backups
and failover are handled automatically if
391:45 - you wanted to share your data to another Eva's
account snapshots can be shared. It also comes
391:46 - with self healing for your storage so data
blocks and disk are continuously scan for
391:47 - errors and repaired automatically. Looking
at replication for Aurora There are two types
391:48 - of replicas available we have Amazon or more
replicas in MySQL read replicas knows for
391:49 - MySQL we can only have up to five for performance
impact on primary is high. It does not have
391:50 - auto Automatic failover However, it does have
support for user defined replication delay,
391:51 - or for different data or schema versus primary.
So you have to decide for yourself, which
391:52 - one makes more sense for you. But just for
exams, you might need to know there's two
391:53 - different types. If for whatever reason they
had you looked into Aurora pricing, you'd
391:54 - find out, it's really expensive if you aren't
using it for high production applications.
391:55 - So if you're a hobbyist, like me, and you
still want to use Aurora, Aurora has Aurora
391:56 - serverless, which is just another mode that
it runs in. And the advantage here is that
391:57 - it only runs when you need it to and it can
scale up and down based on your applications
391:58 - needs. And so when you set serverless, in
the database features, you're going to have
391:59 - this capacity settings, so you can set the
minimum and maximum capacity for a work capacity
392:00 - units, also abbreviated as ACU. And so here
it's between two and 384 ac use. And that's
392:01 - what it's going to charge you based on only
when it's consumed. So when would you want
392:02 - to use a word serverless, what's really good
for low volume blog sites, maybe a chatbot.
392:03 - Maybe you've built an MVP that you are demoing
out to clients, so it's not used very often,
392:04 - but you plan on using your word down the road.
So that's the use case for Aurora. It works
392:05 - with both MySQL and Postgres. For over a year,
Postgres wasn't there, but now it is here.
392:06 - There are some limitations on the versions
of Postgres and MySQL are bicycles, you can
392:07 - use it, it used to be only MySQL 5.6. But
last time I checked, I saw 5.6 and 5.7. For
392:08 - my school. And for Postgres, I saw a lot of
versions. So there is a lot of flexibility
392:09 - there for you. But there are some limitations
around that. There's also other things that
392:10 - it can't do that Aurora can do. But it's a
big long list. I'm not going to listen here,
392:11 - but I just want you to know, the utility of
Aurora thermal. We've finished the raw section
392:12 - and now on to the raw cheat sheet where we're
going to summarize everything that we've learned.
392:13 - So when you need a fully managed Postgres
or MySQL database that needs to scale, have
392:14 - automatic backups, high availability, and
fault tolerance. Think Aurora, Aurora can
392:15 - run on my skull or Postgres database engines.
Aurora, MySQL is five times faster over regular
392:16 - MySQL, and Aurora, Postgres is three times
faster over regular Postgres. Aurora is 1/10.
392:17 - The cost over its competitors with similar
performance availability options, over replicate
392:18 - six copies of your database across three eyzies.
Aurora is allowed up to 15 Aurora replicas
392:19 - on Aurora database can span multiple regions
via Aurora global database. Aurora serverless,
392:20 - allows you to stop start Aurora and scale
automatically while keeping costs low. And
392:21 - the ideal use case for serverless is for new
projects or projects with infrequent database
392:22 - usage. So there you go. That's everything
you need to know about.
392:23 - We are looking at Amazon redshift, which is
a fully managed petabyte size data warehouse.
392:24 - So what we use a data warehouse for we would
use it to analyze massive amounts of data
392:25 - via complex SQL queries. Amazon redshift is
a columnar store database. So to really understand
392:26 - what redshift is, we need to understand what
a data warehouse is to understand what a data
392:27 - warehouse is, it's good to compare it against
a database and understand this, we need to
392:28 - set some foundational knowledge and understand
what a database transaction is. So let's define
392:29 - a database transaction. A transaction symbolizes
a unit of work performed within a database
392:30 - management system. So an example of a transaction
or reads and writes that's as simple as that.
392:31 - And for database and data warehouse, they're
going to treat transactions differently. And
392:32 - so for a database, which we have an online
transactional processing system and OLTP,
392:33 - the the transactions are going to be short.
So look at the bottom here, we say short transaction.
392:34 - So that means small and simple queries with
an emphasis on writes. Okay, so why would
392:35 - we want short transactions? Well, for OLTP?
Well, a database was built to store current
392:36 - transactions, and enables fast access to specific
transactions for ongoing business processes.
392:37 - So they're just talking about I have a web
app. And we need to be very responsive for
392:38 - the current user for reads and writes. Okay,
and so that could be adding an item to your
392:39 - shopping list. That could be sign up that
could be doing any sorts of thing in a web
392:40 - application. And generally, these are backed
by a single source. So a single source would
392:41 - be Postgres on could be running on RDS. And
so that's the idea behind a database. So if
392:42 - we go over to the data warehouse side, it
runs on an online analytical processing system,
392:43 - an OLAP. And all apps are all about long transaction
so long and complex SQL queries with an emphasis
392:44 - on reads. So a data warehouse is built to
store large quantities of historical data.
392:45 - and enable fast and complicated complex queries
across all data. So the utility here is business
392:46 - intelligence tools generating reports. And
a data warehouse isn't a single source it
392:47 - is it takes data from multiple sources. So
dynamodb, EMR, s3, Postgres all over the place,
392:48 - data is coming into one place so that we can
run complex queries, and not too frequently.
392:49 - So now that we know what a data warehouse
warehouse is, let's talk about the reasons
392:50 - why you'd want to use redshift. So redshift
the Pricing starts at 25 cents per hour with
392:51 - no upfront costs or commitments. It scales
up to petabytes, petabytes of data for $1,000
392:52 - per terabyte per year. redshift is price less
than 1/10. The cost of most similar services
392:53 - redshift is used for business intelligence
redshift uses OLAP redshift is a columnar
392:54 - store database. It was the second time we've
mentioned this. And we really need to understand
392:55 - what a column or storage database is to really
understand the power behind redshift and data
392:56 - warehouses. So columnar storage for database
is database tables is an important factor
392:57 - in optimizing an analytic query performance
because it drastically reduces the overall
392:58 - disk IO requirements and reduces the amount
of data you need to load from the disk. So
392:59 - columnar storage is the reason why redshift
is so darn fast. And we're going to look at
393:00 - that in more detail here. So let's really
cement our knowledge with redshift and show
393:01 - a use case example. So here I have, I want
to build my own business intelligence tool.
393:02 - And I have a bunch of different sources. So
I have data coming from EMR, I have data coming
393:03 - from s3, I have data coming from dynamodb.
And I'm going to copy that data however I
393:04 - want. There's a copy command, I'm going to
copy that data into redshift. Okay, so but
393:05 - once that data is in there, you say, Well,
how do I interact and access redshift data?
393:06 - Normally, you know, most services use the
ABS SDK. But this case, we're not using native
393:07 - SDK, because we just may need to make a generic
SQL connection to redshift. And so if we were
393:08 - using Java, and generally you probably will
be using Java, if you're using redshift, you'd
393:09 - be using j JDBC, or ODBC, which are third
party libraries to connect and query redshift
393:10 - data. So, you know, I said columnar storage
is very important to redshifts performance.
393:11 - And so let's conceptually understand what
that means. So what would we normally use
393:12 - with a database would be reading via the rows,
whereas an in an OLAP, or reading versus columns,
393:13 - because if we're going to be looking at a
lot of data and crunching it, we it's better
393:14 - to look at it at columns, okay. Because that
way, if we're reading columns, that allows
393:15 - us to store that data as the same database
datatype for allow for easy compression, that
393:16 - means that we're going to be able to load
data a lot quicker. And because we're always
393:17 - looking at massive amounts of data, at the
same time, we can pull in only the columns
393:18 - that we need in bulk, okay,
393:19 - and so that's gonna give us much faster performance
for our use case, which is like business intelligence
393:20 - tools. So redshift configuration, you can
set it up in two different cluster types.
393:21 - So you have single node, which is a great
way to get started on redshift, if you don't
393:22 - have a lot of money you want to play around,
you can just launch a single node of 160 gigabytes,
393:23 - or you can launch an multimo, multi node.
And so when you launch a multi node, you always
393:24 - have a leader node, and then you have compute
nodes. And you can add up to 128 compute nodes,
393:25 - so you have a lot of computing power behind
you. Now, I just want to point out that when
393:26 - you do spin up redshift and multi node, you're
gonna see there's a maximum set of 32. And
393:27 - I just said, there's 128. So what's going
on here? Well, it's just one of those same
393:28 - defaults, where AWS wants to be really sure
that you want more than 32. Because you know,
393:29 - if you come in day one, someone had 128, they
want to make sure that they have the money
393:30 - to pay for it. So if you need more than 32
nodes, you just have to go ask a request for
393:31 - a certain service limit increase. Now besides
there being different cluster types, there's
393:32 - also different node types. And so we have
two that are labeled here we have DC dense
393:33 - compute and dense storage DS, okay. And they
are as what they say they are one is optimized
393:34 - for computing power, and one is optimized
for storage. So you know, depending on your
393:35 - use case, you're going to choose what type
of node you want. Notice that there are no
393:36 - smalls or micros, we all we only start at
large here. Because if you're doing redshift,
393:37 - you're working with large amounts of data.
So you know, that makes total sense. compression
393:38 - is something that is the most important thing
in terms of speed. So redshift uses multiple
393:39 - compression techniques to achieve a significant
compression. Relative to traditional relational
393:40 - data stores. Similar data is stored sequentially
on disk. It does not require indexes or materialized
393:41 - views, which saves a lot of space compared
to traditional systems when loading data to
393:42 - An empty table data is sampled. And the most
appropriate compression scheme is selected
393:43 - automatically. So this is all great information
for the exam, you know, it's not so important
393:44 - remember this. So you know what redshift is,
is utilized for these Nitty gritties at F
393:45 - or the associate is not so, important. redshift
processing. So, redshift uses massively parallel
393:46 - processing, which they aggrandize or initializes
MPP, it automatically distributes data and
393:47 - query loads across all nodes, and lets you
easily add new nodes to your data warehouse
393:48 - while still maintaining fast query performance.
So yeah, it's easy to add more compute power
393:49 - on demand. Okay, and so we got redshift backups
of backups are enabled by default, with a
393:50 - one day retention period, and retention periods
can be modified up to 35 days. All right.
393:51 - redshift always attempts to maintain at least
three copies of your data. One is the original
393:52 - copy. The second is a replica on the compute
nodes. And then the third is a backup copy
393:53 - of s3. And so redshift also could asynchronously
replicate your snapshots to s3 in a different
393:54 - region. So you know, if you need to move your
data region per region, you have that option
393:55 - as well. For redshift billing, the compute
node hours, the total number of hours ran
393:56 - across all nodes in the billing period, build
one unit per node per hour, and you're not
393:57 - charged for the leader nodes per hour. So
when you spin up a cluster, and you only have
393:58 - one compute node, and one one leader node,
you're just paying for the compute node. For
393:59 - backups, backups are stored on s3 and you're
billed the s3 storage fees, right. So you
394:00 - know, just same same thing as usual. And data
data transfer build only only transfers within
394:01 - a VPC not outside up outside of it. Okay.
redshift security. So we have data in transit
394:02 - encrypt using SSL data rest, we can encrypt
using a Aes 256 encryption. database encryption
394:03 - can be applied using kms. Or you can use Cloud
HSM and here you can see it's just as easy
394:04 - as applying it. redshift availability. So
redshift is single AZ super important to remember
394:05 - this because a lot of services are multi AZ
but redshift is not one of them maybe in the
394:06 - future, but maybe not. To run in multi AZ
you would have to run multiple redshift clusters
394:07 - in a different AZ with with same inputs. So
you're basically just running a clone, it's
394:08 - all manual labor, right? So there's no managed
automatic way of doing multi AZ snapshots
394:09 - can be restored to a different AZ in the event
394:10 - of an outage occurs. And just to wrap everything
up, we have a really good redshift cheat sheet
394:11 - here definitely recommend you print this out
for your exam. And we're going to go through
394:12 - everything again. So data can be loaded from
s3 EMR. dynamodb, or multiple data sources
394:13 - on remote hosts. redshift is columns columnar
store database, which can give you SQL like
394:14 - queries and is an is an OLAP. redshift can
handle petabytes worth of data redshift is
394:15 - for data warehousing. redshifts most common
use cases business intelligence redshift can
394:16 - only run in one AZ so it's a sink, it's single
AZ it's not multi AZ redshift can run via
394:17 - a single node or multi node for clusters.
A single node is 150 gigabytes in size. A
394:18 - multi node is comprised of the leader node
and multiple compute nodes. You are billed
394:19 - per hour for each node excluding the leader
node in multi node, you're not billed for
394:20 - the leader node. just repeating that again
there, you can have up to 128 compute nodes.
394:21 - Again, I said earlier that the maximum by
default was 32. But they're not going to ask
394:22 - you what the default is. redshift has two
kinds of node types dense compute and dense
394:23 - storage. And it should be pretty obvious when
you should use one or the other redshift attempts
394:24 - to back backup your data three times the original
on the compute node on a three similar data
394:25 - is stored on a disk sequentially for faster
reads. Read of data database can be encrypted
394:26 - via kms, or cloud HSM backup retention is
default to one day and can be increased to
394:27 - a maximum of 35 days. redshift can asynchronously
backup to your snaps to your backup via snapshot
394:28 - to another region delivered via s3. And redshift
uses massively parallel processing to distribute
394:29 - queries and data across all loads. And in
the case of an empty empty table when importing
394:30 - redshift will sample the data to create a
schema. So there you go. That's redshift in
394:31 - a nutshell, and that should help you for the
exams.
394:32 - Hey, this is Andrew Brown from exam Pro. And
we are looking at Dynamo DB which is a key
394:33 - value and document database, a no SQL database
which can guarantee consistent reading rights
394:34 - at any scale. So let's just double check a
couple things before we jump into Dynamo dB.
394:35 - So what is a no SQL database? Well today It
is neither relational and does not use SQL
394:36 - to query the data for results, hence the no
SQL part, no SQL databases, the method of
394:37 - how they store data is different. dynamodb
does both key value store and document store.
394:38 - So key value stores when you simply have a
key, and a value and nothing more. And then
394:39 - document store is where you have structured
data, right? So this whole thing here would
394:40 - be a single value in the database. So again,
dynamodb is a no SQL key value and document
394:41 - database for internet skills applications.
It has a lot of functionality behind it is
394:42 - fully managed multi region. Multi master durable,
is a durable database, built in security,
394:43 - Backup and Restore. And in memory caching.
The big takeaway why you'd want to use dynamodb
394:44 - is that you just say what you need, you say
I need 100 reads per second, or 100 writes
394:45 - per second, and you're guaranteed to get that
it's just based on what you're willing to
394:46 - pay. Okay, so scaling is not an issue here,
it's just do you want to pay that amount for
394:47 - whatever capacity you need. So when we're
talking about durability, dynamodb does store
394:48 - its data across three regions. And we definitely
have fast reads and writes because it's using
394:49 - SSD drives. So that's the level of durability.
And the next thing we're going to look into
394:50 - is the consistency because it replicates data
across different regions, you could be reading
394:51 - a copies of data, and we might run into inconsistency.
So we need to talk about those caveats.
394:52 - So I just wanted to touch on table structure
here. Because Dynamo DB does use different
394:53 - terminologies instead of what a relational
database uses. So instead of a roll, they
394:54 - call it an item instead of a column or a cell
or whatever you want to call it, they just
394:55 - call it an attribute. And then the other most
important thing is the primary key which is
394:56 - made up with a with a partition key and a
sort key. And that's all you need to know
394:57 - for the solution architect associate for the
other certifications, we have to really know
394:58 - this stuff. But this is all we need to know
for this case.
394:59 - So consistency is something that's a very
important concept when we're dealing with
395:00 - dynamodb. Because when data is written to
the database, it has to then copy it to those
395:01 - other regions. And so if someone was reading
from region C, when an update was occurring
395:02 - there is that there's that chance that you're
reading it before it has the opportunity to
395:03 - write it. Okay. And so dynamodb gives us a
couple options to give us choices on our use
395:04 - case. And we'll go through the two. And so
the first one is eventual consistent reads,
395:05 - which is the default functionality. And the
idea here is when copies are being updated,
395:06 - it is possible for you to read and be returned
inconsistent copy, okay. But the trade off
395:07 - here is the reads are fast, but there's no
guarantee of consistency, all copies of data,
395:08 - eventually will become generally consistent
within a second. Okay, so the here that the
395:09 - trade off is that you know, you could be reading
it before it's updated. But generally it will
395:10 - be up to date. So you have to decide whether
that's the trade off you want. That's default
395:11 - option. The other one is strongly consistent
reads, okay, and this is one all copies are
395:12 - being updated and you attempt to read it,
it will not return a result until all copies
395:13 - are consistent, you have a guarantee of consistency.
But the trade off is higher latency, so slower
395:14 - reads, but that the reads are going to be
as slow as a second because all copies of
395:15 - data will be consistent within a second. So
if you can wait up to a second in the case
395:16 - of a right, then that's what you'll have to
do. for eventual consistent reads. If you
395:17 - can tolerate something being consistent because
it's not important, then those are your two
395:18 - options. So we're on to the dynamodb cheat
sheet. If you are studying for the developer
395:19 - associate this would be two pages long but
since this is for the solution architect associate
395:20 - it this is a lot shorter. Okay, so dynamodb
is a fully managed no SQL key value and document
395:21 - database. applications that contain large
amounts of data but require predictable read
395:22 - and write performance while scaling is a good
fit for dynamodb dynamodb scales with whatever
395:23 - read and write capacity you specify per second
dynamodb can be set to have eventually consistent
395:24 - reads which is the default option and strongly
consistent reads, eventually consistent reads,
395:25 - data is returned immediately, but data can
be inconsistent copies of data will be generally
395:26 - consistent within one second. Strongly consistent
reads will wait until data is consistent data
395:27 - will never be inconsistent, but latency will
be higher, only up to a second though. Copies
395:28 - of data will be consistent within within a
guarantee of one second or at you know exactly
395:29 - one second dynamodb stores three copies of
data on SSD drives across three regions. And
395:30 - there you go, that's all you need. Hey, this
is Andrew Brown. And we are looking at Eva's
395:31 - cloud formation, which is a templating language
that defines AWS resources to be provisioned,
395:32 - or automating the creation of resources via
code. And all these concepts are called infrastructure
395:33 - as code which we will cover again in just
a moment here. So to understand cloud formation,
395:34 - we need to understand infrastructure as code
because that is what cloudformation is. So
395:35 - let's reiterate over what infrastructure is
code is. So it's the process of managing and
395:36 - provision computer data centers. So in our
case, it's AWS, through machine readable definition
395:37 - files. And so in this case, it's cloudformation,
template YAML, or JSON files, rather than
395:38 - the physical hardware configuration or interactive
configuration tools. So the idea is to stop
395:39 - doing things manually, right. So if you launch
resources in AWS, you're used to configuring
395:40 - in the console all those resources, but through
a scripting language, we can automate that
395:41 - process. So now let's think about what is
the use case for cloud formation. And so here,
395:42 - I have an example, where let's pretend that
we have our own minecraft server business,
395:43 - and people sign up on our website and pay
a monthly subscription, and we will run that
395:44 - server for them. So the first thing they're
going to do is they're gonna tell us where
395:45 - they want the server to run. So they have
low latency and what size of servers so the
395:46 - larger the server, the more performant the
server will be. And so they give us those
395:47 - two inputs. And then we somehow send that
to a lambda function, and that lambda function
395:48 - triggers to launch a new cloudformation stack
using our cloud formation template, which
395:49 - defines, you know, how to launch that server,
that easy to instance, running Minecraft and
395:50 - a security group and what region and what
size. And when it's finished creating, we
395:51 - can monitor maybe using cloud watch events
that it's done, and using the outputs from
395:52 - that cloud formation stack, send the IP address
of the new minecraft server to the user, so
395:53 - they can log in and start using their servers.
So that's way of automating our infrastructure.
395:54 - So we're gonna look at what a cloudformation
template looks like. And this is actually
395:55 - one we're going to use later on to show you
how to launch a very simple Apache server.
395:56 - But cloudformation comes in two variations.
It comes in JSON, and YAML. So why is there
395:57 - two different formats? Well, JSON just came
first. And YAML is is an intent based language,
395:58 - which is just more concise. So it's literally
the same thing, except it's in that base.
395:59 - So we don't have to do all these curlies.
And so you end up with something that is,
396:00 - in length, half the size. Most people prefer
to write YAML files, but there are edge cases
396:01 - where you might want to use JSON. But just
be aware of these two different formats. And
396:02 - it doesn't matter which one you use, just
use what works best for you. Now we're looking
396:03 - at the anatomy of a cloud formation template.
And so these are made up of a bunch of different
396:04 - sections. And here are all the sections listed
out here. And we'll work our way from top
396:05 - to bottom. And so the first one is metadata.
So that allows you to provide additional information
396:06 - about the template, I don't have one of the
example here and I rarely ever use metadata.
396:07 - But you know, it's just about additional information,
then you have the description. So that is
396:08 - just describing what you want this template
to do. And you can write whatever you want
396:09 - here. And so I described this template to
launch new students, it's running Apache,
396:10 - and it's hard coded work for us East one,
then you have parameters and parameters is
396:11 - something you can use a lot, which is you
defining what inputs are allowed to be passed
396:12 - within to this template at runtime. So one
thing we want to ask the user is what size
396:13 - of instance type Do you want to use, it's
defaulted to micro, but they can choose between
396:14 - micro and nano. Okay, so we can have as many
parameters as we want, which we'll use throughout
396:15 - our template to reference, then you have mappings,
which is like a lookup table, it maps keys
396:16 - to values, so you can change your values to
something else. A good example of this would
396:17 - be, let's say, you have a region. And for
each region, the image ID string is different.
396:18 - So you'd have the region keys mapped to different
image IDs based on the region. So that's a
396:19 - very common use for mappings. Then you'd have
conditions these are like your FL statements
396:20 - within your template don't have an examples
here. But that's all you need to know. Transform
396:21 - is very difficult to explain, if you don't
know macros are but the idea it's like applying
396:22 - a mod to the actual template. And it will
actually change what you're allowed to use
396:23 - in the template. So if I define a transform
template, the rules here could be wildly different,
396:24 - different based on what kind of extra functionality
that transform adds. We see that with Sam,
396:25 - the serverless application model is a transform.
So if you ever take a look at that you'll
396:26 - have a better understanding of what I'm talking
about there. Then you have resources which
396:27 - is the main show to the whole template. These
are the actual resources you are defining
396:28 - that will be provisioned. So think any kind
of resource I enroll etc. instance lamda RDS
396:29 - anything, right? And then you have outputs
and outputs is, it's just what you want to
396:30 - see as the end results. So like, when I create
the server, it's we don't know the IP address
396:31 - is until it spins it up. And so I'm saying
down here, get me the public IP address. And
396:32 - then in the console, we can see that IP address,
so that we don't have to, like look at the
396:33 - PC to console pull it out. The other advantage
of outputs is that you can pass information
396:34 - on to other cloudformation templates or created
like a chain of effects because we have these
396:35 - outputs. But the number one thing you need
to remember is what makes a valid template.
396:36 - And there's only one thing that is required,
and that is specifying at least one resource.
396:37 - All these other fields are optional, but resource
is mandatory, and you have to have at least
396:38 - one resource. So if you're looking for cloudformation
templates to learn by example, Ava's quickstarts
396:39 - is a great place to do it, because they have
a variety of different categories, where we
396:40 - have templates that are pre built by AWS partners
and the APN. And they actually usually show
396:41 - the architectural diagram, but the idea is
you launch the template, you don't even have
396:42 - to run it, you can just press a button here
and then actually see the raw template. And
396:43 - that's going to help you understand how to
connect all this stuff together. Because if
396:44 - you go through the ages documentation, you're
going to have to spend a lot of time figuring
396:45 - that out where this might speed that up if
this is your interest. So I just wanted to
396:46 - point that out for you. It's not really important
for the exam, it's not going to come up as
396:47 - an exam question. It's just a learning resource
that I want you to
396:48 - consider.
396:49 - We're on to the cloudformation cheat sheet,
please consider that this is specific for
396:50 - the solution architect associate. Whereas
for the SIS ops associate, this would be a
396:51 - much longer cheat sheet because you have to
know it more in detail. I did add a few additional
396:52 - things we did not cover in the core content
just in case they do creep up on the exam.
396:53 - I don't think they will, but I threw them
in there just in case. And so let's get through
396:54 - this list. So when being asked to automate
the provisioning of resources, think cloudformation.
396:55 - When infrastructure as code is mentioned,
think cloud formation. cloudformation can
396:56 - be written in either JSON or YAML. When cloudformation
encounters an error, it will roll back with
396:57 - rollback and progress again, might not show
up an example, I'm putting it in their cloudformation
396:58 - templates larger than half a megabyte, arc
too large. In that case, you'd have to upload
396:59 - from s3. So the most important thing is you
can upload templates directly or you can provide
397:00 - a link to an object in an s3 bucket. Okay,
nested stacks help you break up cloudformation
397:01 - templates into smaller reusable templates
that can be composed into larger templates.
397:02 - At least one resource under Resources must
be defined for a cloudformation template to
397:03 - be valid. And then we talk about all the sections.
So we have metadata that's for extra information
397:04 - about your template description that describes
what your template should do parameters how
397:05 - you get users inputs into the template, transforms
and applies macros, outputs, these are values
397:06 - you can use to important to other stacks.
So it's just output variables. mappings is
397:07 - like a lookup table. So it maps keys to values
resources, define the resources you want to
397:08 - provision. And again, I repeat it, at least
one resources required. All these other sections
397:09 - are optional, and conditions, which these
are like your FL statements within your cloudformation
397:10 - templates. So there you go. We're all done
with cloudformation. Hey, this is Andrew Brown
397:11 - from exam Pro. And we are looking at cloudwatch,
which is a collection of monitoring services
397:12 - for logging, reacting and visualizing log
data. So I just want you to know that cloud
397:13 - watch is not just one service, it's multiple
services under one name. So we have cloudwatch,
397:14 - logs, cloudwatch metrics, cloud watch events,
cloud watch alarms, and cloud watch dashboards,
397:15 - I'm not going to go through the list here,
because we're going to cover each section
397:16 - and then we'll cover it in the cheat sheet.
But just so you know, the most important thing
397:17 - to know is that their cloud watch is not a
single service, it's multiple services. So
397:18 - it's time to look at cloudwatch logs, which
is the core service of cloud watch. All the
397:19 - other cloud services are built on top of this
one. And it is used to monitor store and access
397:20 - your log file. So here we have a log file.
And logs belong within a log group that cannot
397:21 - exist outside of the Law Group. So here I
have one called production dot log, which
397:22 - is a Ruby on Rails application. And it contains
multiple log files over a given period of
397:23 - time, and this is inside of those log files.
And we have the ability to like filter that
397:24 - information and do other things with it. So
log files are stored indefinitely by default,
397:25 - and they never expire. Okay, so you don't
ever have to worry about losing this data.
397:26 - And most Eva's services are integrated cloudwatch
logs by default. Now there are some cases
397:27 - where there's actually multiple cases where
you have to turn on cloud watch logs, or you
397:28 - have to add Iam permissions. So like when
you're creating a lambda function, the default
397:29 - permissions allow you to write to logs. But
the thing is, you wouldn't normally realize
397:30 - that you're enabling it. So anyway, that's
cloud watch. So now we're going to take a
397:31 - look at cloudwatch metrics, which is built
on top of logs. And the idea behind this is
397:32 - it represents a time ordered set of data points,
or you can think of it as a variable to monitor
397:33 - so within the logs, we'll have that data and
extracts it out as data points, and then we
397:34 - can graph it, right. So in this case, I'm
showing you for an EC two instance. So you
397:35 - have some network in coming into that EC two
instance. And you can choose that specific
397:36 - metric, and then get a visual of it. So that
is cloudwatch metrics. Now, these metrics
397:37 - are predefined for you. So you don't have
to do anything. to leverage this, you just
397:38 - have to have logs enabled on specific services.
And these metrics will become available when
397:39 - data arrives. Okay.
397:40 - So now we're going to take a look at cloudwatch
events, which builds off of metrics and logs
397:41 - to allow you to react to your data and and
take an action on that, right. So we can specify
397:42 - an event source based on an event pattern
or a schedule, and that's going to then trigger
397:43 - to do something in a target, okay. And a very
good use case for this would be to schedule
397:44 - something that you'd normally do on a cron
tab. So maybe you need to backup a server
397:45 - once a day. So you trigger that, and then
there's probably like, EBS snapshot in here.
397:46 - But you know, that's the idea behind it here.
Okay, so you can either trigger based on a
397:47 - pattern or a timeframe. And it has a lot of
different inputs here, it's not even worth
397:48 - going through them all. But EBS snapshot,
and lambda are the most common. So we're looking
397:49 - at cloudwatch metrics, and we had a bunch
of predefined ones that came for us. But let's
397:50 - say we wanted to make our own custom metric,
well, we can do that. And all we have to do
397:51 - is by using the avsc ally, the command line
interface, or the SDK, software development
397:52 - kit, we can programmatically send data for
custom metrics. So here I have a custom metric
397:53 - for the enterprise D, which is namespace and
Starfleet. And we're collecting dimensions
397:54 - such as hole integrity, shields and thrusters.
Okay, so we can send any kind of data that
397:55 - we want on and publish that to cloudwatch
metrics. Now another cool feature about custom
397:56 - metrics is that it opens the opportunity for
us to have high resolution metrics, which
397:57 - can only be done through custom metrics. So
if you want data, a lot like edit even more
397:58 - granular level of blow one minute, with high
resolution metrics, you can go down to one
397:59 - second, and we have these intervals, you can
do it one second, five, second, 10, second,
398:00 - 30 seconds. But generally, you know, if you
can turn it on, you're probably gonna want
398:01 - to go as low as possible. The higher the frequency,
the more it's going to cost you. So do have
398:02 - that in consideration. But the only way to
get high resolution metrics is through a custom
398:03 - metric. So now we're taking a look at cloudwatch
alarms, which triggers a notification based
398:04 - on a metric when it is breached based on a
defined threshold. Very common use case is
398:05 - building alarms. It's like one of the first
things you want to do when you set up your
398:06 - AWS account. And so here we have some options
when we go set our alarm. So we can say whether
398:07 - it's static, or its anomaly, what is the condition?
So does it trigger this question, when it's
398:08 - greater than equal then lower than etc? And
what's the amount so you know, my account,
398:09 - I'm watching for $1,000, if it's under $1,000,
I don't care. And if it goes over, please
398:10 - send me an email about it. And that is the
utility there. So there you go. cloudwatch
398:11 - alert. So now it's time to look at Cloud watch
dashboards, which, as the name implies, allows
398:12 - you to create dashboards. And this is based
off of cloudwatch metrics. So here, we have
398:13 - a dashboard in front of it. And we add widgets.
And we have all sorts of kinds here, graphs,
398:14 - bar charts, and etc. And you drag them on
you pick your options, and then you have to
398:15 - just make sure you hit that Save button. And
there you go. So it's really not that complicated.
398:16 - Just when you need a visualization of your
data. You know, think about using cloud watch.
398:17 - So I just wanted to quickly touch on availability
of data, and how often cloudwatch updates
398:18 - the metrics that are available to you because
it varies on service and the one we really
398:19 - need to know is these two because it does
creep been to a few exam questions. So by
398:20 - default, when you're using EC two, it monitors
at a five minute, minute interval. And if
398:21 - you want to get down to one minute, you have
to turn on detailed monitoring, which costs
398:22 - money, okay? For all other services, it's
going to be between one minute to three minute
398:23 - to five minute, there might be a few other
services that have detailed monitoring, I
398:24 - feel like the elastic cache might have it.
But generally, all you have to worry about
398:25 - is for EC to the majority of services are
by default one minute. So that's why I just
398:26 - had to really emphasize this because you see,
two does not default to one minute, it's five
398:27 - minutes. And to get that one minute, you have
to turn on detailed monitoring. I just want
398:28 - to make you aware that cloudwatch doesn't
track everything you'd normally think it would
398:29 - track for an easy to incidence. And specifically,
if you wanted to know like your memory utilization,
398:30 - or how much disk space was left on your server,
it does not track that by default. Because
398:31 - those are host level metrics. Those
398:32 - are more in detailed metrics. And in order
to gather that information, you need to install
398:33 - the cloud watch agent and the cloud watch
agent is a script, which can be installed
398:34 - via SYSTEMS MANAGER run command, it probably
comes pre installed on Amazon, Linux one and
398:35 - Amazon x two. And so you know, if you need
those more detailed metrics, such as memory
398:36 - and disk space, you're gonna have to install
that. But these ones you already have by default,
398:37 - so there is disk usage, there is network usage
and CPU usage. The disk usage here is limited.
398:38 - I camera what they are the top my head, but
it's not like disk space, like, am I 40% left
398:39 - in disk space, okay. So you know, just be
aware of these two things, okay.
398:40 - Hey, this is Andrew Brown from exam Pro. And
we are going to do a very short follow along
398:41 - here for cloudwatch. So if you're taking the
solution, architect associate, you don't need
398:42 - to know a considerable amount about cloud
watch in terms of details, that's more for
398:43 - the sysop associate. But we do need to generally
know what's going on here. So maybe the first
398:44 - thing we should learn how to do is create
an alarm. Okay, so alarms are triggers based
398:45 - on when certain thresholds, metrics trigger
a certain threshold. So I have a bunch here
398:46 - already, because I was creating some dynamodb
tables, whenever you create dynamodb, you
398:47 - always get a bunch of alarms, I'm going to
go ahead here and create a new alarm, the
398:48 - most common alarm to create is a building
a building alarm. So maybe we could go ahead
398:49 - and do that. So under billing, we're going
to choose total estimated charge, we're gonna
398:50 - choose USD, I'm going to select metric, okay.
And, you know, we can choose the period of
398:51 - time we want it to happen, we have the static
and anomaly detection, we have whether we
398:52 - need to determine when it should get triggered.
So we would say when we go over $1,000, okay,
398:53 - we should get a Rubes $1,000 USD. Okay. So
it's not letting me fill it in there. There
398:54 - we go. When we hit that metric there, then
we should get an email about it. All right.
398:55 - And so we are going to go ahead and just hit
next there. And for this alarm to work, we
398:56 - need to have an SNS topic. Okay, so we're
gonna create a new topic here. And I'm just
398:57 - gonna say Andrew at exam pro.co. All right.
And what we'll end up doing here is we'll
398:58 - just hit next, oops, we have to create topic
button there. Okay, so it's created that topic,
398:59 - we'll hit next. And we'll just define this
as billing alarm. Okay. And we will hit next
399:00 - here, and we will create the alarm. And so
now we have an alarm. So anytime billing goes
399:01 - over $1,000, it's going to send us an email,
it's very unlikely this is going to happen
399:02 - within this account. Because I'm not spending
that much here. It does have to wait for some
399:03 - data to come in. So it will say insufficient
data to begin with. And it's still waiting
399:04 - for pending confirmation. So it is waiting
for us to confirm that SNS topic. So I'm just
399:05 - going to hop over to my email and just confirm
that for you very quickly here. And so in
399:06 - very short amount of time I've received here
a subscription. So I'm just gonna hit confirm
399:07 - subscription here. Okay, it's just going to
show that I've confirmed that, okay. And I'm
399:08 - just going to go ahead and close that here.
And we'll just give this a refresh. Alright,
399:09 - so that pending confirmation is gone. So that
means that this billing alarm isn't an OK
399:10 - status, and it is able to send me emails,
when that does occur. Okay, so there's a few
399:11 - different ways to set alarms, sometimes you
can directly do it with an EC too. So I just
399:12 - want to show you here, okay. So we're just
going to go to over DC to I don't think we
399:13 - have anything running over here right now.
Okay, and so I'm just going to launch a new
399:14 - instance because I just want to show that
to you. We're going to go to Amazon Linux
399:15 - two. We're going to go to configuration next.
Now there is this option here for detailed
399:16 - monitoring. And this is going to provide monitoring
at for every minute as opposed to every five
399:17 - minutes by default. Okay, so this does cost
additional money. But I'm just going to turn
399:18 - it on here. For the sake of this, follow along.
Okay, I'm just going to give it a key pair
399:19 - here. And then I'm just going to go to View
instances here. And I just want to show you
399:20 - that under the monitoring tab, we do get a
bunch of metrics here about this EC two instance.
399:21 - And if you wanted to create an alarm, it's
very convenient, you can actually just do
399:22 - it from here. So if you have an EC two instance,
and you want to send an alarm for this here,
399:23 - it could be for a variety thing. So take action,
so send a notification here. And also, you
399:24 - could stop the instance. So we could say,
when the CPU utilization goes over 50%, shut
399:25 - down the server. Okay. And so that's one very
easy way to create alarms. All right. Okay,
399:26 - um, so, you know, just, it's, it's good to
know that a lot of services are like that,
399:27 - I bet if we went over to dynamodb, I bet it's
the same thing. So if we go over to Dynamo
399:28 - dB, okay, and we go to tables here, and we
were using this for a another tutorial here
399:29 - and we create an alarm, okay, it's the same
story. So you're gonna want to take a peek
399:30 - at different services, because they do give
you some basic configurations, though that
399:31 - make make it very easy to set up alarms, you
can, of course, always do it through here.
399:32 - But it's a lot easier to do that through a
lot of the services. Okay. And so I think
399:33 - maybe what we'll do here is look at events.
Next, we're going to take a look now at cloudwatch
399:34 - events, which has been renamed to Amazon event
bridge. So these are exactly the same service.
399:35 - So AWS added some additional functionality,
such as the ability to create additional event
399:36 - buses, and to use partner event sources. And
so they gave it a rebranding, okay.
399:37 - So the way it works is, and we'll just do
this through cloudwatch events here. And then
399:38 - we'll also do it through the new interface.
But the idea is, you generally will create
399:39 - rules within cloud watch. And so you have
the ability to do from an event pattern, okay?
399:40 - Or from a schedule. All right. So we are going
to actually just do from schedule, because
399:41 - that's the easiest one to show here. And so
based on a schedule, we could say every day,
399:42 - all right, once a day, I want to create a
backup of a aect volume. So we have a bunch
399:43 - of options here. Okay. And this is a very
common one. So I actually have a minecraft
399:44 - server that I run, I like to backup the volume
at least once a day. And so this is the way
399:45 - I would go about doing that. So here, I just
have to supply it. Oops, we actually want
399:46 - to do the snapshot here. And so I would just
have to supply the volume. So here I have
399:47 - an easy to instance running from earlier in
this fall along here. And so I'm just going
399:48 - to provide the volume ID there. Okay. And
so once that is there, I can hit configure
399:49 - details and say EBS snapshot, okay. Or we
just say volume, snapshot doesn't matter.
399:50 - Just I'm being picky here. And we'll create
that. And so now we have that rule. So once
399:51 - a day, it's going to create that snapshot
for us, we're gonna go ahead and do that event
399:52 - bridge, you're gonna see it's the same process.
It's just this new UX, or UI design. Whether
399:53 - it is a improvement over the old one is questionable,
because it's a very thing that people always
399:54 - argue about with AWS is the changes to the
interface here, we're going to see the same
399:55 - thing of that pattern and schedule, I'm going
to go to schedule here, we're going to choose
399:56 - a one day, alright. And you can see now we
choose our Event Bus. And so we're whenever
399:57 - we're creating rules here, it's always using
the default Event Bus. But we can definitely
399:58 - create other event buses and use partner events.
Okay. And we're just gonna drop this down
399:59 - here and choose Create a snapshot here. I
don't know if it's still my clipboard it is,
400:00 - there we go. And we'll just create that. So
you're gonna see that we can see both of them
400:01 - here. Okay, so we can see UBS one and snapshot.
And if we go back to our rules here, we should
400:02 - be able to see both, is it just the one or
both? Yeah, so they're both so you can just
400:03 - see that they're the exact same service. Okay.
And just to wrap up talking about Amazon event
400:04 - bridge, I just want to show you that you can
create multiple event buses here. So if we
400:05 - go and create an event bus, you can actually
create an event bus that is shared from another
400:06 - AWS account. Okay, so you could actually react
to within your system, an event from another
400:07 - actual account, which is kind of cool. And
then you also have your partner event sources.
400:08 - So here you could react to data with a data
dog or something with has to do with login.
400:09 - So you know, there are some ways to react
cross account. Okay, so that's just the point
400:10 - I wanted to make there. All right. And we're
going to just check one more thing out here,
400:11 - which is a cloud watch dashboard. So college
dashboards allows you to create a bunch of
400:12 - metrics and put it on a dashboard. So I'm
just going to make one here my EC to dashboard
400:13 - because we do have an EC two instance running
400:14 - And what we can do here is just start adding
things. So I could add a line graph, okay.
400:15 - And we do have a running EC two instance.
So we should be able to get some information
400:16 - there. So let's say per instance metrics here,
and we will see if we can find anything that's
400:17 - running should be something running here,
I actually, you know what, I think it's just
400:18 - this one here, we didn't name that instance.
That's why I'm not seeing anything there.
400:19 - Okay, I'm just gonna create that there. And
so you know, a lot, a lot of stuff is happening
400:20 - with that instance. So that's why we're not
seeing any data there. But if there was, we
400:21 - would start to see some spikes there. But
all you need to know is that you can create
400:22 - dashboards in these, we can create widgets
based on metric information. And just be sure
400:23 - to hit that save dashboard button. It's very
non intuitive. This interface here, so maybe
400:24 - this will get a refresh one day, but yeah,
there you go. That's dashboards. So that wraps
400:25 - up the cloud watch section here. So what we're
gonna want to do is, we're just going to want
400:26 - to tear down whatever we created. So let's
go to our dashboard. And I believe we can
400:27 - delete it, how do we go about doing it, we
go to delete dashboard, these dashboards us
400:28 - get like a few free but they do cost in the
long term, then we're going to tear down are
400:29 - on our alarm, because these actually alarms
do cost money. If you have a lot of them,
400:30 - so Mize will get rid of ones that we aren't
using. Okay, then we will go to our rules
400:31 - here. And we will just go ahead and delete
these rules, okay, I'm just disabling them,
400:32 - I actually want to delete them. Okay, and
I believe I started an EC two instance. So
400:33 - we're gonna just go over to our instances
here, and terminate. Okay, so there we go.
400:34 - That's a full cleanup there. Of course, if
you're doing the SIS Ops, where you have to
400:35 - know, cloud watch in greater detail, but this
is just generally what you need to know, for
400:36 - the solution architect associate. And likely
the developer is you're on to the cloud watch
400:37 - cheat sheet. So let's jump into it. So Cloud
watch is a collection of monitoring services.
400:38 - We have dashboards, events, alarms, logs and
metrics, starting with logs. First, it is
400:39 - the core service to all of cloud watch. And
it logs data from Ada services. So a very
400:40 - common thing that you might love would be
CPU utilization. Then we go on to metrics
400:41 - and metrics builds off of logs, and it represents
a time ordered set of data points. It is a
400:42 - variable to monitor. So let's go back to CPU
utilization and visualize it as a line graph.
400:43 - That is what metrics is done, we go on to
cloudwatch events, which triggers an event
400:44 - based on a condition, a very common use case
is maybe you need to take a snapshot of your
400:45 - server every hour, I like to think of events
as a serverless crontab, because that's how
400:46 - I use it. Then you have alarms, which triggers
notifications based on a metric when a defined
400:47 - threshold is breached. So a very common use
case are a building alarm. So if we go over
400:48 - $1,000, I want an email about it, you got
to tell me, then you got cloudwatch dashboards,
400:49 - as the name implies, it's a dashboard. So
it creates visualizations based on metrics.
400:50 - There are a couple of exceptions when we're
dealing with end to end cloud watch. And the
400:51 - first is that it monitors at an interval of
five minutes. And if you want to get that
400:52 - one minute interval, you have to turn on detailed
monitoring, most services do monitor at one
400:53 - minute intervals. And if they don't, it's
going to be the one three or five minute interval.
400:54 - logs must belong to a log group cloudwatch
agents need to be installed on EC to host
400:55 - if you want to get memory usage or decides
because that doesn't come by default. You
400:56 - can stream custom log files to to cloud watch
logs. So maybe if you're gonna have a Ruby
400:57 - on Rails app, you have a production log, and
you want to get that in cloud watch logs,
400:58 - you can do that. And then the last thing is
cloud watch metrics. metrics allow you to
400:59 - track high resolution metrics. So that you
can have sub minute intervals, tracking all
401:00 - the way down to one second. So if you need
something more granular, you can only do that
401:01 - through cloud metrics. So there you go. That's
the cloud watch.
401:02 - Hey, this is Angie brown from exam Pro. And
we are looking at Cloud trail which is used
401:03 - for logging API calls between AWS services.
And the way I like to think about this service.
401:04 - It's when you need to know who to blame. Okay,
so as I said earlier, cloud trail is used
401:05 - to monitor API calls and actions made on an
AWS account. And whenever you see these keywords
401:06 - governance, compliance, operational auditing
or risk auditing, it's a good indicator, they're
401:07 - probably talking about Eva's cloud trail.
Now, I have a record over here to give you
401:08 - an example of the kinds of things that cloud
trail tracks to help you know how you can
401:09 - blame someone What's up, something's gone
wrong. And so we have the where, when, who
401:10 - and what so the where, so we have the account
ID what, like which account did it happen
401:11 - in and Have the IP address of the person who
created that request the lens. So the time
401:12 - it actually happened, the who. So we have
the user agent, which is, you know, you could
401:13 - say, I could tell you the operating system,
the language, the method of making this API
401:14 - call the user itself. so here we can see Worf
made this call, and, and what so to what service,
401:15 - and you know, it'll say what region and what
service. So this service, it's using, I am
401:16 - here, I in the action, so it's creating a
user. So there you go, that is cloud trail
401:17 - in a nutshell.
401:18 - So within your database account, you actually
already have cloud trail logging things by
401:19 - default, and it will collect into the last
90 days under the event history here. And
401:20 - we get a nice little interface here. And we
can filter out these events. Now, if you need
401:21 - logging be on 90 days. And that is a very
common use case, which you definitely want
401:22 - to create your own trail, you'd have to create
a custom trail. The only downside when you
401:23 - create a custom trail is that it doesn't have
a gooey like here, such as event history.
401:24 - So there is some manual labor involved to
visualize that information. And a very common
401:25 - method is to use Amazon, Athena. So if you
see cloud trail, Amazon, Athena being mentioned
401:26 - in unison, there's that reason for that, okay.
So there's a bunch of trail options, I want
401:27 - to highlight and you need to know these, they're
very important for cloud trail. So the first
401:28 - thing you need to know is that a trail can
be set to log in all regions. So we have the
401:29 - ability here, say yes, and now, no region
is missed. If you are using an organization,
401:30 - you'll have multiple accounts, and you want
to have coverage across all those. So in a
401:31 - single trail, you can check box on applied
to my entire organization, you can encrypt
401:32 - your cloud trail logs, what you definitely
want to do using server side encryption via
401:33 - key management service, which abbreviate is
SSE kms. And you want to enable log file validation,
401:34 - because this is going to tell whether someone's
actually tampered with your logs. So it's
401:35 - not going to prevent someone from being able
to tamper from your logs. But it's going to
401:36 - at least let you know how much you can trust
your logs.
401:37 - So I do want to emphasize that cloud trail
can deliver its events to cloudwatch. So there's
401:38 - an option After you create the trail where
you can configure, and then it will send your
401:39 - events to cloud watch logs. All right, I know
cloud trail and cloud watch are confusing,
401:40 - because they seem like they have overlapping
of responsibilities. And there are a lot of
401:41 - aidable services that are like that. But you
know, just know that you can send cloud trail
401:42 - events to cloud watch logs, not the other
way around. And there is that ability to.
401:43 - There are different types of events in cloud
trail, we have measurement events, and data
401:44 - events. And generally, you're always looking
at management events, because that's what's
401:45 - turned on by default. And there's a lot of
those events. So I can't really list them
401:46 - all out for you here. But I can give you a
general idea what those events are. So here
401:47 - are four categories. So it could be configuring
security. So you have attached rule policy,
401:48 - you'd be registering devices, it would be
configuring rules for routing data, it'd be
401:49 - setting up logging. Okay. So 90% of events
in cloud trail are management events. And
401:50 - then you have data events. And data events
are actually only for two services currently.
401:51 - So if you were creating your trail, you'd
see tabs, and I assume as one, they have other
401:52 - services that can leverage data events, we'll
see more tabs here. But really, it's just
401:53 - s3 and lambda. And they're turned off by default,
for good reason. Because these events are
401:54 - high volume. They occur very frequently. Okay.
And so this is tracking more in detail s3,
401:55 - events, such as get object, delete object
put object, if it's a lambda, it'd be every
401:56 - time it gets invoked. So those are just higher
there. And so those are turned off by default.
401:57 - Okay. So now it's time to take a quick tour
of cloud trail and create our very own trail,
401:58 - which is something you definitely want to
do in your account. But before we jump into
401:59 - doing that, let's go over to event history
and see what we have here. So AWS, by default
402:00 - will track events in the last 90 days. And
this is a great safeguard if you have yet
402:01 - to create your own trail. And so we have some
event history here. And if we were just to
402:02 - expand any of them doesn't matter which one
and click View event, we get to, we get to
402:03 - see what the raw data looks like here for
a specific event. And we do have this nice
402:04 - interface where we can search via time ranges
and some additional information. But if you
402:05 - need data Now beyond 90 days, you're going
to have to create a trail. And also just to
402:06 - analyze this, because we're not going to have
this interface, we're gonna have to use Athena
402:07 - to really make sense of any cloud trail information.
But now that we have learned that we do have
402:08 - event history available to us, let's move
on to creating our own trail. Let's go ahead
402:09 - and create our first trail. And I'm just going
to name my trail here exam pro trail, I do
402:10 - want you to notice that you can apply a trail
to all regions, and you definitely want to
402:11 - do that, then we have management events, where
we can decide whether we want to have read
402:12 - only or write only events, we're going to
want all of them, then you have data events.
402:13 - Now these can get expensive, because s3 and
lambda, the events that they're tracking are
402:14 - high frequency events. So you can imagine
how often someone might access something from
402:15 - an s3 bucket, such as a get or put. So they
definitely do not include these. And you have
402:16 - to check them on here to have the inclusion
of them. So if you do want to track data events,
402:17 - we would just say for all our s3 buckets,
or specify them and lambdas are also high
402:18 - frequency because we would track the invocations
of lambdas. And you could be in the 1000s
402:19 - upon millions there. So these are sanely not
included by default. Now down below, we need
402:20 - to choose our storage location, we're going
to let it create a new s3 bucket. For us,
402:21 - that seems like a good choice, we're going
to drop down advanced here at it because it
402:22 - had some really good tidbits here. So we can
turn on encryption, which is definitely something
402:23 - we want to do with kms. And so I apparently
have a key already here. So I'm just gonna
402:24 - add that I don't know if that's the default
key. I don't know, if you get a default key
402:25 - with cloud trail, usually, you'd have one
in there. But I'm just going to select that
402:26 - one there, then we have enable log file validation.
So we definitely want to have this to Yes,
402:27 - it's going to check whether someone's ever
tampered with our logs, and whether we should
402:28 - not be able to trust her logs. And then we
could send a notification about log file delivery,
402:29 - this is kind of annoying, so I don't want
to do that. And then we should be able to
402:30 - create our trail as soon as we name our bucket
here. So we will go ahead and just name it
402:31 - will say exam pro trails, assuming I don't
have one in another account. Okay, and so
402:32 - it doesn't like that one, that's fine. So
I'm just going to create a new kms key here.
402:33 - kms keys do cost a buck purse, if you want
to skip the step you can totally do. So I'm
402:34 - just going to create one for this here called
exam pro trails.
402:35 - Okay. Great. And so now it has created that
trail. And we'll just use the site here. And
402:36 - then maybe we'll take a peek here in that
s3 bucket when we do have some data. Alright,
402:37 - I do want to point out one more thing is that
you couldn't set the the cloud watch event
402:38 - to track across all organizations, I didn't
see that option there. It's probably because
402:39 - I'm in a sub account. So if I was in my, if
you have an alias organization, right, and
402:40 - this was the root account, I bet I could probably
turn it on to work across all accounts. So
402:41 - we didn't have that option there. But just
be aware that it is there. And you can turn
402:42 - a trail to be across all organizations. So
I just had to switch into my route organization
402:43 - account, because I definitely wanted to show
you that this option does exist here. So when
402:44 - you create a trail, we have applied all regions,
but we also can apply to all organizations,
402:45 - which means all the accounts within an organization.
Okay. So you know, just be aware of that.
402:46 - So now that our trail is created, I just want
you to click into and be aware that there's
402:47 - an additional feature that wasn't available
to us when we were creating the trail. And
402:48 - that is the ability to send our cloud trail
events to cloud watch logs. So if you want
402:49 - to go ahead and do that, you can configure
that and create an IM role and send it to
402:50 - a log or cloud watch log group. There are
additional fees apply here. And it's not that
402:51 - important to go through the motions of this.
But just be aware that that is a capability
402:52 - that you can do with patrol. So I said earlier
that this will collect beyond 90 days, but
402:53 - you're not going to have that nice interface
that you have an event history here. So how
402:54 - would you go about analyzing that log, and
I said you could use Amazon Athena. So luckily,
402:55 - they have this link here. That's going to
save you a bunch of setup to do that. So if
402:56 - you were to click this here, and choose the
s3 bucket, which is this one here, it's going
402:57 - to create that table for you and Athena, we
used to have to do this manually, it was quite
402:58 - the pain. So it's very nice that they they've
added this one link here, and I can just hit
402:59 - create table. And so what that's going to
do, it's going to create that table in Athena
403:00 - for us and we can jump over to Athena. Okay.
And um, yeah, it should be created here. Just
403:01 - give it a little refresh here. I guess we'll
just click Get Started. I'm not sure why it's
403:02 - not showing up here. We're getting the splash
screen. But we'll go in here and our table
403:03 - is there. So we get this little goofy tutorial.
I don't want to go threat. But on that table
403:04 - has now been created. And we have a bunch
of stuff here. There is a way of running a
403:05 - sample query, I think he could go here and
it was preview table. And that will create
403:06 - us a query. And then we it will just run the
query. And so we can start getting data. So
403:07 - the cool advantage here is that if we want
to query our data, just like using SQL, you
403:08 - can do so here. And Athena, I'm not doing
this on a day to day basis. So I can't say
403:09 - I'm the best at it. But you know, if we gave
this a try here and tried to query something,
403:10 - maybe based on event type, I wonder if we
could just like group by event type here.
403:11 - So that is definitely a option. So we say
distinct. Okay, and I want to be distinct
403:12 - on maybe, event type here.
403:13 - Okay.
403:14 - doesn't like that little bit, just take that
out there. Great. So
403:15 - there we go. So that was just like a way so
I can see all the unique event types, I just
403:16 - take the limit off there, the query will take
longer. And so we do have that one there.
403:17 - But anyway, the point is, is that you have
this way of using SQL to query your logs.
403:18 - Obviously, we don't have much in our logs,
but it's just important for you to know that
403:19 - you can do that. And there's that one button,
press enter to create that table and then
403:20 - start querying your logs. So we're onto the
cloud trail cheat sheet, and let's get to
403:21 - it. So Cloud trail logs calls between eight
of us services. When you see the keywords
403:22 - such as governance, compliance, audit, operational
auditing and risk auditing, it's a high chance
403:23 - they're talking about cloud trail when you
need to know who to blame. Think cloud trail
403:24 - cloud trail by default logs events data for
the past 90 days via event history. To track
403:25 - beyond 90 days, you need to create a trail
to ensure logs have not been tampered with,
403:26 - you need to turn on log file validation option.
Cloud trail logs can be encrypted using kms.
403:27 - Cloud trail can be set to log across all service
accounts in an organization and all regions
403:28 - in an account. Cloud trail logs can be streamed
to cloud watch logs, trails are outputted
403:29 - to s3 buckets that you specify cloud trail
logs come in two kinds. We have a management
403:30 - events and data events, management events,
log management operations, so you know, attach
403:31 - roll policy, data events, log data operations
for resources. And there's only really two
403:32 - candidates here s3 and lambda. So think get
object delete object put put object did events
403:33 - are disabled by default when creating a trail
trail log trail logs in s3 and can be analyzed
403:34 - using Athena I'm gonna have to reword that
one. But yeah, that is your teaching. Hey,
403:35 - this is Andrew Brown from exam Pro. And we
are looking at AWS lambda, which lets you
403:36 - run code without provisioning or managing
servers. And servers are automatically started
403:37 - and stopped when needed. You can think of
as lambdas as serverless functions, because
403:38 - that's what they're called. And it's pay per
invocation. So as we just said it was lambda
403:39 - is a compute service that lets you run code
without provisioning or managing servers.
403:40 - Lambda executes your code only when needed
and scales automatically to a few or to 1000
403:41 - lambda functions concurrently. In seconds,
you pay only for the compute time you consume,
403:42 - there is no charge when your code is not running.
So the main highlights is lambda is cheap,
403:43 - lambda is serverless. And lambda scales automatically.
Now, in order to use lambda, you are just
403:44 - uploading your code and you have up to seven
options that are supported by AWS. So we have
403:45 - Ruby, Python, Java, go PowerShell, no GS,
and C sharp. If you want to use something
403:46 - outside of this list, you can create your
own custom runtimes that are not supported.
403:47 - So eight support is not going to help you
with them, but you can definitely run them
403:48 - on it. So when we're thinking about how to
use Avis lambda, there is a variety of use
403:49 - cases because lambda is like glue, it helps
you connect different services together. And
403:50 - so I have to use use cases in front of you
here. So the first is processing thumbnail.
403:51 - So imagine you are a web service, and users
are allowed to upload their profile photo.
403:52 - So what you normally do is you would store
that in an s3 bucket. Now you can set event
403:53 - triggers on s3 buckets so that it would go
trigger a lambda and then that image would
403:54 - get pulled from that bucket. And using something
like Sharpe j s or image magic, you could
403:55 - then take that profile photo and then crop
it to a thumbnail and then store it back into
403:56 - the bucket. Okay, another use case would be
contact email form. So this is the same thing
403:57 - example if user email form, it's exactly this.
When you fill in the contact email form, it
403:58 - sends that form data to an API gateway. endpoint,
which then triggers a lambda function, and
403:59 - then you have this lambda function that evaluates
whether the form data is valid or not. If
404:00 - it's not valid, it's gonna say, hey, you need
to make these corrections. Or if it if it's
404:01 - good, it's going to then create a record in
the in our dynamodb table, which is the records
404:02 - are called items. And it's also going to send
out an email notification to the company so
404:03 - that we know that you've contacted us via
SNS. All right, so there you go.
404:04 - So to invoke a lambda to make it execute,
we can either use the AWS SDK, or we can trigger
404:05 - it from another database service. And we have
a big long list here. And this is definitely
404:06 - not the full list. So you can see that we
can do API gateway, we just showed that with
404:07 - the email contact form, if you have IoT devices,
you could trigger a lambda function, your
404:08 - maybe you want your Echo Dot using an Alexa
skill would trigger a lambda, Al B's CloudFront,
404:09 - cloudwatch, dynamodb, kinesis, s3, SNS Sq
s. And I can even think of other ones outside
404:10 - of here, like guard duty and config, there's
a bunch, okay, so you can see that a dress
404:11 - integrates with a lot of stuff, it also can
integrate with third party party, or partnered
404:12 - third party database partners. And that's
powered through Amazon event bridge, which
404:13 - is just event bridges very much like cloud
watch events. But with some additional functionality
404:14 - there. And you can see we can integrate with
data dog one, login page pager duty. So just
404:15 - to give you a scope of the possible triggers
available, I just want to touch on lambda
404:16 - pricing here, quickly. So the first million
requests like the first functions, you execute
404:17 - our free per month, okay? So if you're a startup,
and you're not doing over a million requests
404:18 - per month, and a lot aren't, you're basically
not paying anything for compute after that
404:19 - is 20 cents per additional million requests.
So very, very inexpensive. There, the other
404:20 - costs to it, besides just how often things
are requested. It's also you know, how many
404:21 - how long the duration is. So the first 400
gigabytes of seconds is free. Thereafter,
404:22 - it's going to be this very, very small amount,
for every gigabyte per second, okay, this
404:23 - also is going to this value is going to change
based on the amount of memory you use, I bet
404:24 - you This is for the lowest amount 128 megabytes,
most of the times, you're not going to see
404:25 - yourself increasing beyond 512. That's really
high. But yeah, I always feel that I'm between
404:26 - 128 and 256. Now just to do a calculation
just to give an idea of total pricing. So
404:27 - let's say we had a lambda function that's
at 128 megabytes with the lowest, we have
404:28 - 30 million executions per month, those are
requests, and the duration is 200 milliseconds.
404:29 - For those lambda functions, we're only paying
$5.83. So you can see that lambda is extremely
404:30 - inexpensive. So I just wanted to give you
a quick tour of the actual ad bus lamda interface,
404:31 - just so you can get an idea how everything
works together. So you would choose your runtime.
404:32 - So here we're using Ruby. And then you can
upload your code, you have different ways,
404:33 - you can either edit it in line, so they have
like CLOUD NINE integrated here. So you can
404:34 - just start writing code. If it's too large,
then you either have to upload in a zip or
404:35 - provided via s3. So there are some limitations.
And the larger gets eventually you'll end
404:36 - up on s3, when you want to import a lambda,
and then you have your triggers. And there's
404:37 - a lot of different triggers. But for this
lambda function, it's using dynamodb. So when
404:38 - a record is inserted into dynamodb, it goes
to dynamodb streams. And then it triggers
404:39 - this lambda function. And then we have on
the right hand side, the outputs and those
404:40 - outputs. It's the lambda function that actually
has to call those services. But you create
404:41 - an IM role. And whatever you have permissions
to it will actually show you here on the right
404:42 - hand side. So here Here you can see this lambda
is allowed to interact with cloud watch logs,
404:43 - dynamodb and kinesis. firehose. So there you
go. We are looking at default limits for AWS
404:44 - lambda. It's not all of them, but it's the
ones that I think are most important for you
404:45 - to know. So by default, you can only have
1000 lambdas running concurrently. Okay,
404:46 - so if you want to have more, you'd have to
go ask AWS support. It's possible there could
404:47 - be an exam question where it's like, Hey,
you want to run X amount of lambdas. And they're
404:48 - not running or something. This could be because
of that limit. You are able to store temporary
404:49 - files on a on a lambda as it's running, and
has a limit of up to 500 megabytes. When you
404:50 - create a lambda by default, it's going to
be running in no VPC. And so sometimes there
404:51 - are services such as RDS where you can only
rack them if You are in the same VPC. So you
404:52 - might actually have to change the VPC. In
some use cases, when you do set a lambda to
404:53 - a VPC, it's going to lose internet access,
I'm not to say that you cannot expose it because
404:54 - it's in a security group. So there might be
some way to do it. But that is a consideration
404:55 - there, you can set the timeout to be a maximum
of 15 minutes. So you don't really if you
404:56 - had to go beyond 15 minutes, and this is where
you probably want to use fargate, which is
404:57 - similar to Avis lambda, but there's a lot
more work in setup and your your charge per
404:58 - second as opposed to milliseconds. But just
be aware, if you need anything beyond 50 milliseconds,
404:59 - you're gonna want fargate. And the last thing
is memory. So you can set memory memory starts
405:00 - at 128 megabytes and goes up all the way to
3008 megabytes, the more megabytes you use,
405:01 - the more expensive it's going to be paired
with how long the duration is, and it This
405:02 - goes up in 64 megabyte increments, okay. So
there you go, there's the most important do
405:03 - you know. So one of the most important concepts
to ages lambda is cold starts, because this
405:04 - is one of the negative trade offs of using
serverless functions. So, you know, database
405:05 - has servers pre configured, so they're just
lying around, and they're in a turned off
405:06 - state for your runtime environment. So when
a lambda is invoked, the servers need to then
405:07 - be turned on, your code needs to be copied
over. And so during that time, there's going
405:08 - to be a delay when that function will initially
run. And that's what we call a code cold start.
405:09 - So over here, you can see I have a lambda
function, it gets triggered, and there is
405:10 - no server for it to run on. And so what's
gonna happen is that servers gonna have to
405:11 - start and we're going to copy that code, and
there's gonna be a period of delay. Now, if
405:12 - you were to invoke that function, again, what's
going to happen is, it has to be recent, right?
405:13 - If if the same function, so the codes already
there, and and the servers already running,
405:14 - then you're going to not have that delay,
that cold starts not going to be there. And
405:15 - that's when your servers actually warm. All
right. So, you know, serverless functions
405:16 - are cheap, but everything comes with the trade
off. And so serverless functions, cold starts
405:17 - can cause delays in the user experience. And
this is actually a problem directly for us
405:18 - on exam Pro, because we didn't use serverless
architecture, because we wanted everything
405:19 - to be extremely fast, because, you know, using
other providers, we weren't happy with the
405:20 - delay and experience. Now there are ways around
cold starts, which is called pre warming.
405:21 - So what you can do is you can invoke the a
function so that it starts prematurely so
405:22 - that when someone actually uses it, it's always
going to stay warm, or you can take a lambda
405:23 - and then give it more responsibility so that
more things are passing through it so it stays
405:24 - warm, more constant. And you know that cold
starts is becoming less and less issue at
405:25 - going forward because cloud providers are
trying to find solutions to reduce those times,
405:26 - or to mitigate them, but they are still a
problem. So just be very aware of this one
405:27 - caveat to thermal. We're on
405:28 - to the lamda cheat sheet. So lambdas are serverless
functions, you upload your code and it runs
405:29 - without you managing or provisioning any servers.
Lambda is serverless. You don't need to worry
405:30 - about the underlying architecture. Lambda
is a good fit for short running tasks where
405:31 - you don't need to customize the OS environment.
If you need long running tasks greater than
405:32 - 15 minutes, and a custom OS environment then
consider using fargate. There are seven runtime
405:33 - language environments officially supported
by lambda, you have Ruby, Python, Java, Node
405:34 - JS, C sharp PowerShell and go. You pay per
invocation. So that's the duration, the amount
405:35 - of memory used, rounded up to the nearest
100 milliseconds, and you are at and you're
405:36 - also paid based on the amount of requests
so the first 1 million requests per month
405:37 - are free. You can adjust the duration timeout
to be up to 15 minutes, and the memory up
405:38 - to 3008 megabytes, you can trigger lambdas
from the SDK or multiple AWS services, such
405:39 - as s3 API gateway dynamodb. Lambda is by default
run in no VPC to interact with some services,
405:40 - you need to have your lambdas in the same
VPC. So, you know, in the case of RDS, you'd
405:41 - have to have your lambda in the same VPC as
RDS lambdas can scale to 1000 concurrent functions
405:42 - in a second 1000 is the default if you want
to increase this you have to go make an EVA
405:43 - service limit increase with Ada support and
lambdas have cold starts if a function has
405:44 - not been recently executed, there will be
a delay. Hey, this is Andrew Brown from exam
405:45 - pro and we are looking at simple queue service
also known as Sq s which is a fully managed
405:46 - queuing service that is enables you to decouple
and scale micro services, distributed systems
405:47 - and serverless applications. So to fully understand
SQL, we need to understand what a queueing
405:48 - system is. And so a queueing system is just
a type of messaging system, which provides
405:49 - asynchronous communication and decouples.
Processes via messages could also be known
405:50 - as events from a sender and receiver. But
in the case for a streaming system, also known
405:51 - as a producer and consumer. So, looking at
a queueing system, when you have messages
405:52 - coming in, they're usually being deleted on
the way out. So as soon as they're consumed
405:53 - or deleted, it's for simple communication,
it's not really for real time. And just to
405:54 - interact with the queue. And the messages,
they're both the sender and receiver have
405:55 - to pull to see what to do. So it's not reactive.
Okay, we got some examples of queueing systems
405:56 - below, we have sidekick, sq, S, rabbit, rabbit
and queue, which is debatable because it could
405:57 - be considered a streaming service. And so
now let's look at the streaming side to see
405:58 - how it compares against a queueing system.
So a streaming system can react to events
405:59 - from multiple consumers. So like, if you have
multiple people that want to do something
406:00 - with that event, they can all do something
with it, because it doesn't get immediately
406:01 - deleted, it lives in the Event Stream for
a long period of time. And the advantage of
406:02 - having a message hang around in that Event
Stream allows you to apply complex operations.
406:03 - So that's the huge difference is that one
is reactive and one is not one allows you
406:04 - to do multiple things with the messages and
retains it in the queue. One deletes it and
406:05 - doesn't doesn't really think too hard about
what it's doing. Okay, so there's your comparative
406:06 - between queuing and streaming. And we're going
to continue on with Sq s here, which is a
406:07 - queueing system. So the number one thing I
want you to think of when you think of SQL
406:08 - says application integration, it's for connecting
isolette applications together, acting as
406:09 - a bridge of communication, and Sq s happens
to use messages and queues for that you can
406:10 - see Sq s appears in the Ava's console under
application integration. So these are all
406:11 - services that do application integration Sq
S is one of them. And as we said it uses a
406:12 - queue. So accuse a temporary repository for
messages that are waiting to be processed,
406:13 - right. So just think of going to the bank,
and everyone is waiting that line, that is
406:14 - the queue. And the way you interact with that
queue is through the Avis SDK. So you have
406:15 - to write code that was going to publish messages
to the queue. And then when you want to read
406:16 - them, you're going to have to use the AWS
SDK to pull messages. And so Sq S is pull
406:17 - based, you have to pull things, it's not pushed
based, okay.
406:18 - So to make this crystal clear, I have an SQL
use case here. And so we have a mobile app,
406:19 - and we have a web app, and they want to talk
to each other. And so using the Avis SDK,
406:20 - the mobile app sends a message to the queue.
And now the web app, what it has to do is
406:21 - it has to use the Avis SDK and pull the cue
whenever it wants. So it's up to the this
406:22 - app to code in how frequently it will check.
But it's going to see if there's anything
406:23 - in the queue. And if there is a message, it's
going to pull it down, do something with it
406:24 - and report back to the queue that it's consumed
it meaning to tell the queue to go ahead and
406:25 - delete that message from the queue. All right,
now this app on the mobile left hand side
406:26 - to know whether it's been consumed, it's going
to have to, on its own schedule, periodically
406:27 - check to pull to see if that message is still
in the queue, if it no longer is, that's how
406:28 - it knows. So that is the process of using
SQL between two applications. So let's look
406:29 - at some SQL limits starting with message size.
So the message size can be between one byte
406:30 - to 256 kilobytes, if you want it to go beyond
that message size, you can use the Amazon
406:31 - SQL extended client library only for Java,
it's not for anything else to extend that
406:32 - necessarily up to two gigabytes in size. And
so the way that would work is that the message
406:33 - would be stored in s3 and the library would
reference that s3 object, right? So you're
406:34 - not actually pushing two gigabytes to SQL
is just loosely looking to something in an
406:35 - s3 bucket. Message retention. So message retention
is how long SQL will hold on a message before
406:36 - dropping it from the queue. And so the message
retention by default is four days, and you
406:37 - have a message retention retention that can
be adjusted from a minimum of 60 seconds to
406:38 - a maximum of 14 days.
406:39 - SQL is a queueing system. So let's talk about
the two different types of queues. We have
406:40 - standard queue which allows for a nearly unlimited
number of transactions per second when your
406:41 - transaction is just like messages, and it
guarantees that a message will be delivered
406:42 - at least once. However, the trade off here
is that more than one copy of the message
406:43 - could be Potentially delivered. And that would
cause things to happen out of order. So if
406:44 - ordering really matters to you just consider
there's that caveat here with standard queues,
406:45 - however, you do get nearly unlimited transactions.
So that's a trade off. It does try to provide
406:46 - its best effort at to ensure messages stay
generally in the order that they were delivered.
406:47 - But again, there's no guarantee. Now, if you
need a guarantee of the, the ordering of messages,
406:48 - that's where we're going to use feefo, also
known as first in first out, well, that's
406:49 - what it stands for, right. And the idea here
is that, you know, a message comes into the
406:50 - queue and leaves the queue. The trade off
here is the number of transactions you can
406:51 - do per second. So we don't have nearly unlimited
per second where we have a cap up to 300.
406:52 - So there you go.
406:53 - So how do we prevent another app from reading
a message while another one is busy with that
406:54 - message. And the idea behind this is we want
to avoid someone doing the same amount of
406:55 - work that's already being done by somebody
else. And that's where visibility timeout
406:56 - comes into play. So visibility timeout is
the period of time that it meant that messages
406:57 - are invisible DSU Sq sq. So when a reader
picks up that message, we set a visibility
406:58 - timeout, which could be between zero to 12
hours. By default, it's 30 seconds. And so
406:59 - no one else can touch that message. And so
what's going to happen is that whoever picked
407:00 - up that message, they're going to work on
it. And they're going to report back to the
407:01 - queue that, you know, we finished working
with it, it's going to get deleted from the
407:02 - queue. Okay. But what happens, if they don't
complete it within the within the visibility
407:03 - timeout frame, what's going to happen is that
message is now going to become visible, and
407:04 - anyone can pick up that job, okay. And so
there is one consideration you have to think
407:05 - of, and that's when you build out your web
apps, that you you bake in the time, so that
407:06 - if if the job is going to be like if it's
if 30 seconds have expired, then you should
407:07 - probably kill that job, because otherwise
you might end up this issue where you have
407:08 - the same messaging being delivered twice.
And that could be an issue. Okay, so just
407:09 - to consideration for visibility. Don't ask
us we have two different ways of doing polling
407:10 - we have short versus long. Polling is the
method in which we retrieve messages from
407:11 - the queue. And by default, SQL uses short
polling, and short polling returns messages
407:12 - immediately, even if the message queue being
pulled is empty. So short polling can be a
407:13 - bit wasteful, because if there's nothing to
pull, then you're just calling you're just
407:14 - making calls for no particular reason. But
there could be a use case where you need a
407:15 - message right away. So short polling is the
use case you want. But the majority of use
407:16 - cases, the majority of use cases, you should
be using long polling, which is bizarre, that's
407:17 - not by default, but that's what it is. So
long polling waits until a message arrives
407:18 - in the queue, or the long pole timeout expires.
Okay. And long polling makes it inexpensive
407:19 - to retrieve messages from the queue as soon
as messages are available, using long polling
407:20 - will reduce the cost because you can reduce
the number of empty receives, right. So if
407:21 - there's nothing to pull, then you're wasting
your time, right? If you want to enable long
407:22 - polling if you have to do it within the SDK,
and so what you're doing is you're setting
407:23 - the receive message request with a wait time.
So by doing that, that's how you set long
407:24 - polling. Let's take a look at our simple queue
service cheat sheet that's going to help you
407:25 - pass your exam. So first, we have Sq S is
a queuing service using messages with a queue
407:26 - so think sidekick or rabbit mq, if that helps
if you know the services, sq S is used for
407:27 - application integration. It lets you decouple
services and apps so that they can talk to
407:28 - each other. Okay, to read Sq s, you need to
pull the queue using the ABS SDK Sq S is not
407:29 - push based. Okay, it's not reactive. sq s
supports both standard and first in first
407:30 - out FIFO queues. Standard queues allow for
unlimited messages per second does not guarantee
407:31 - the order of delivery always delivers at least
once and you must protect against duplicate
407:32 - messages being processed feefo first in first
out maintains the order messages with a limit
407:33 - of 300. So that's the trade off there. There
are two kinds of polling short by default
407:34 - and long. Short polling returns messages immediately
even if the message queue is being pulled
407:35 - as empty. Long polling waits until messages
arrive in the queue or the long pole time
407:36 - expires. in the majority of cases long polling
is preferred over short polling majority okay.
407:37 - Visibility timeout is the period of the time
that messages are invisible to the Sq sq.
407:38 - messages will be deleted from the queue after
a job has been processed. Before the visibility
407:39 - timeout expires. If the visibility timeout
expires in a job will become visible to The
407:40 - queue again, the the default visibility timeout
is 30 seconds, timeout can be between zero
407:41 - seconds to a maximum of 12 hours. I highlighted
that zero seconds because that is a trick
407:42 - question. Sometimes on the exams, people don't
realize you can do it for zero seconds, sq
407:43 - s can retain messages from 60 seconds to 14
days by default, is four days. So 14 days
407:44 - is two weeks. That's an easy way to remember
it. Message sizes can be between one byte
407:45 - to two and 56 kilobytes. And using the extended
client library for Java can be extended to
407:46 - two gigabytes. So there you go, we're done
with SQL.
407:47 - Hey, this is Andrew Brown from exam Pro. And
we are looking at simple notification service
407:48 - also known as SNS, which lets you subscribe
and send notifications via text message email,
407:49 - web hooks, lambdas Sq s and mobile notification.
Alright, so to fully understand SNS, we need
407:50 - to understand the concept of pub sub. And
so pub sub is a publish subscribe pattern
407:51 - commonly implemented in messaging systems.
So in a pub sub system, the sender of messages,
407:52 - also known as the publisher here, doesn't
send the message directly to the receiver.
407:53 - Instead, they're going to send the messages
to an Event Bus. And the event pumps categorizes
407:54 - the messages into groups. And then the receiver
of messages known as the subscriber here subscribes
407:55 - to these groups. And so whenever a new message
appears within their subscription, the messages
407:56 - are immediately delivered to them. So it's
not unlike registering for a magazine. All
407:57 - right, so, you know, down below, we have that
kind of representation. So we have those publishers,
407:58 - and they're publishing to the Event Bus which
have groups in them, and then that's gonna
407:59 - send it off to those subscribers, okay, so
it's pushing it all along the way here, okay,
408:00 - so publishers have no knowledge of who their
subscribers are. Subscribers Do not pull for
408:01 - messages, they're gonna get pushed to them.
messages are instead automatically immediately
408:02 - pushed to subscribers and messages and events
are interchangeable terms in pub sub. So if
408:03 - you see me saying messages and events, it's
the same darn thing. So we're now looking
408:04 - at SNS here. So SNS is a highly available,
durable, secure, fully managed pub sub messaging
408:05 - service that enables you to decouple microservices
distributed systems and serverless applications.
408:06 - So whenever we talking about decoupling, we're
talking about application integration, which
408:07 - is like a family of Ada services that connect
one service to another. Another service is
408:08 - also Sq s. And SNS is also application integration.
So down below, we can see our pub sub systems.
408:09 - So we have our publishers on the left side
and our subscribers on the right side. And
408:10 - our event bus is SNS Okay, so for the publisher,
we have a few options here. It's basically
408:11 - anything that can programmatically use the
EVAs API. So the SDK and COI uses the Avis
408:12 - API underneath. And so that's going to be
the way publishers are going to publish their
408:13 - messages or events onto an SNS topic. There's
also other services on AWS that can trigger
408:14 - or publish to SNS topics cloud watch, definitely
can, because you'd be using those for building
408:15 - alarms. And then on the right hand side, you
have your subscribers and we have a bunch
408:16 - of different outputs, which we're going to
go through. But here you can see we have lambda
408:17 - Sq s email, and HTTPS protocol. So publishers
push events to an SNS topic. So that's how
408:18 - they get into the topic. And then subscribers
subscribe to the SNS topic to have events
408:19 - pushed to them. Okay. And then down below,
you can see I have a very boring description
408:20 - of SNS topic, which is it's a logical access
point and communication channel. So that makes
408:21 - a nap. That makes sense. So let's move on.
We're gonna take a deeper look here at SNS,
408:22 - topics and topics allow you to group multiple
subscriptions together, a topic is able to
408:23 - deliver to multiple protocols at once. So
it could be sending out email, text, message,
408:24 - HTTPS, all the sorts of protocols we saw earlier.
And publishers don't care about the subscribers
408:25 - protocol, okay, because it's sending a message
event, it's giving you the topic and saying,
408:26 - you figure it out, this is the message I want
to send out. And it knows what subscribers
408:27 - it has. And so the topic, when it delivers
messages, it will automatically format it
408:28 - for the message according to the subscribers
chosen protocol. Okay. And the last thing
408:29 - I want you to know is that you can encrypt
your topics via kms key management service.
408:30 - And you know, so it's just as easy as turning
it on and picking your key.
408:31 - So now we're taking a look at subscriptions.
And subscriptions are something you create
408:32 - on a topic, okay, and so here I have a subscription
that is an email subscription. And the endpoint
408:33 - is obviously going to be an email. So I provided
my email there. If you want to say hello,
408:34 - give send me an email. And it's just as simple
as clicking that button and filling in those
408:35 - options. Now you have to choose your protocol.
And here we have our full list here on the
408:36 - right hand side. So we'll just go through
it. So we have a sheet phps. And you're going
408:37 - to want to be using this for web hooks. So
the idea is that this is usually going to
408:38 - be an API endpoint to your web applications
that's going to listen for incoming messages
408:39 - from SNS, then you can send out emails. Now,
there's another service called ACS, which
408:40 - specializes in sending out emails. And so
SNS is really good for internal email notifications,
408:41 - because you don't necessarily have your custom
domain name. And also, the emails have to
408:42 - be plain text only. There's some other limitations
around that. So they're really, really good
408:43 - for internal notifications, maybe like billing
alarms, or maybe someone signed up on your
408:44 - platform you want to know about it, then they
also have an email JSON. So this is going
408:45 - to send you JSON via email, then you have
Sq s. So you can send an SNS message to Sq
408:46 - s. So that's an option you have there. You
can also have SNS trigger a lambda functions.
408:47 - So that's a very useful feature as well. And
you can also send text messages that we'll
408:48 - be using the SMS protocol. And the last one
here is platform application endpoints. And
408:49 - that's for mobile push. So like a bunch of
different devices, laptops, and phones have
408:50 - notification systems in them. And so this
will integrate with that. And we're just gonna
408:51 - actually talk about that a bit more here.
So I wanted to talk a bit more about this
408:52 - platform application endpoint. And this is
for doing mobile push. Okay, so we have a
408:53 - bunch of different mobile devices, and even
laptops that use notification systems in them.
408:54 - And so here you can see a big list, we have
ADM, which is Amazon device messaging, we
408:55 - have Apple, Badu Firebase, which is Google.
And then we have two for Microsoft. So we
408:56 - have Microsoft push, and Windows push, okay.
And so you can with this protocol, push out
408:57 - to that stuff. And the advantage here, you're
gonna when you push notification messages
408:58 - to these mobile endpoints, it can appear in
the mobile app just like message alerts, badges,
408:59 - updates, or even sound alerts. So that's pretty
cool. Okay, so I just want you to be aware
409:00 - of that. Alright, so on to the SNS cheat sheet.
So simple notification service, also known
409:01 - as SNS, is a fully managed pub sub messaging
service. SNS is for application integration.
409:02 - It allows decoupled services and apps to communicate
with each other. We have a topic which is
409:03 - a logical access point and communication channel,
a topic is able to deliver to multiple protocols.
409:04 - You can encrypt topics via kms. And then you
have your publishers, and they use the EVAs
409:05 - API via the CLA or the SDK to push messages
to a topic. Many Eva's services, integrate
409:06 - with SNS and act as publishers. Okay, so think
cloud watch and other things. Then you have
409:07 - subscriptions. So you can subscribe, which
consists subscribe to topics. When a topic
409:08 - receives a message, it automatically immediately
pushes messages to subscribers. All messages
409:09 - published to SNS are stored redundantly across
multi az, which isn't something we talked
409:10 - in the core content, but it's good to know.
And then we have the following protocols we
409:11 - can use. So we have HTTP, HTTPS. This is great
for web hooks into your web application. We
409:12 - have emails good for internal email notification.
Remember, it's only plain text if you need
409:13 - rich text. And custom domains are going to
be using sts for that. Then you have email
409:14 - JSON, very similar to email just sending Jason
along the way. You can also send your your
409:15 - SNS messages into an ESA s Sq sq. You can
trigger lambdas you can send text messages.
409:16 - And then the last one is you have platform
application endpoints, which is mobile push,
409:17 - okay. And that's going to be for systems like
Apple, Google, Microsoft, Purdue, all right.
409:18 - Hey, this is Andrew Brown from exam Pro. And
we are looking at elastic cache, which is
409:19 - used for managing caching services, which
either run on Redis or memcached. So to fully
409:20 - understand what elastic cache is, we need
to answer a couple questions. And that is,
409:21 - what is caching what is an in memory data
store. So let's start with caching. So caching
409:22 - is the process of storing data in a cache.
And a cache is a temporary storage area. So
409:23 - caches are optimized for fast retrieval with
the trade off. The data is not durable, okay.
409:24 - And we'll explain what not like what it means
when we're saying it's not durable. So now
409:25 - let's talk about in memory data store, because
that is what elastic cache is. So it's when
409:26 - data is stored in memory. So memory, literally
think RAM, because that's what it's going
409:27 - in. And the trade off is high volatility.
Okay. So when I say it's very volatile, that
409:28 - means low durability. So what does that mean?
It just means the risk of data being lost,
409:29 - okay, because, again, this is a temporary
storage area. And the trade off is we're going
409:30 - to have fast access to that data. All right.
So that is generally what a cache and memory
409:31 - data store is. So with elastic cache, we can
deploy, run and scale popular open source
409:32 - compatible in memory data stores. One cool
feature is that it will frequently identify
409:33 - queries that you Use often and will store
those in cash so you get an additional performance
409:34 - boost. One caveat, I found out when using
this in production for my own use cases is
409:35 - that alasa cash is only accessible to two
resources offering in the same VPC. So here
409:36 - I have an easy to instance, as long as the
same VPC can connect to elastic cache, if
409:37 - you're trying to connect something outside
of AWS, such as digitalocean, that is not
409:38 - possible to connect that elastic cache. And
if it's outside of this VPC, you're not gonna
409:39 - be able to make that connection, probably
through peering or some other efforts, you
409:40 - could do that. But you know, generally, you
want to be using elastic cache or the servers
409:41 - that use it to be in the same VPC. And we
said that it runs open source compatible in
409:42 - memory data stores. And the two options we
have here are mem cache, and Redis. And we're
409:43 - going to talk about the difference between
those two in the next slide. for lots of cash,
409:44 - we have two different engines, we can launch
we have memcached, and Redis. And there is
409:45 - a difference between these two engines, we
don't really need to know in great detail,
409:46 - you know, all the differences. But we do have
this nice big chart that shows you that Redis
409:47 - takes more boxes, then mem cache. So you can
see that Redis can do snapshots, replication,
409:48 - transact transactions, pub sub, and support
geospatial support. So you might think that
409:49 - Redis is the clear winner here. But it really
comes down to your use case. So mem cache
409:50 - is generally preferred for caching HTML fragments.
And mem cache is a simple key value store.
409:51 - And that trade off there is that even though
it's simpler and has less features, it's going
409:52 - to be extremely fast. And then you have Redis
on the other side, where he has different
409:53 - kinds of operations that you can do on your
data in different data structures that are
409:54 - available to you. It's really good for leaderboards,
or tracking unrenewed notification, any kind
409:55 - of like real time cached information that
has some logic to it. Redis is going to be
409:56 - your choice there. It's very fast, we could
argue to say who is faster than the other
409:57 - because on the internet, some people say Redis
is overtaking memcached, even in the most
409:58 - basic stuff, but generally, you know for for
the exam, memcached is technically are generally
409:59 - considered faster for HTML fragments, okay.
But you know, it doesn't really matter because
410:00 - on the exam, they're not gonna really ask
you to choose between memcached and Redis.
410:01 - But you do need to know the difference. So
we are on to the elastic cache cheat sheet.
410:02 - It's a very short cheat sheet, but we got
to get through it. So elastic cache is a managed
410:03 - in memory caching service. Elastic cache can
launch either memcached or Redis. mem cache
410:04 - is a simple key value store preferred for
caching HTML fragments is arguably faster
410:05 - than Redis. Redis has richer data types and
operations, great for leaderboards, geospatial
410:06 - data, or keeping track of unread notifications,
a cache is a temporary storage area. Most
410:07 - frequently identical queries are stored in
the cache and resources only within the same
410:08 - VPC may connect to lots of cash to ensure
low latency. So there you go. That's the last
410:09 - thing.
410:10 - So now we're taking a look at high availability
architecture, also known as h A. And this
410:11 - is the ability for a system to remain available.
Okay. And so what we need to do is we need
410:12 - to think about what could cause a service
to become unavailable. And the solution we
410:13 - need to implement in order to ensure high
availability. So starting with number one,
410:14 - we're dealing with the scenario where when
an availability zone becomes unavailable,
410:15 - now remember, an AZ is just the data center.
So you can imagine a data center becoming
410:16 - flooded for some reason. And so now all the
servers there are not operational. So what
410:17 - would you need to do? Well, you need to have
easy two instances in another data center.
410:18 - Okay. And so how would you route traffic from
one AZ to another? Well, that's where we will
410:19 - use an elastic load balancer so that that
way we can be multi AZ. Now, what would happen
410:20 - if two ACS went out? Well, then you'd need
a third one. And a lot of enterprises have
410:21 - this as a minimum requirement, we have to
be running at least in three azs. Okay, moving
410:22 - on to our next scenario, what happens when
a region becomes unavailable? So let's say
410:23 - there is a meteor strike. It's a very unlikely
scenario. But um, we need a scenario which
410:24 - would take out an entire region, all the data
centers in that geographical location. And
410:25 - so what you're going to need is going to need
instances running in another region. So how
410:26 - would you facilitate the routing of traffic
from one region to another, that's going to
410:27 - be route 53. Okay, so that's the solution
there. Now, what happens when you have a web
410:28 - application that becomes unresponsive because
of too much traffic? All right. So if you're
410:29 - having too much traffic coming to your platform,
well, then you're probably going to need more
410:30 - EC two instances to handle the demand. And
that's where we're going to use auto scaling
410:31 - groups which have the ability to scale based
on the amount of traffic that's coming in
410:32 - and So now what happens if we have an instance
that becomes unavailable, because there's
410:33 - an instance failure. So something with the
hardware or the virtual software is failing,
410:34 - and so it's no longer healthy? Well, again,
that's where we can have auto scaling groups,
410:35 - because we can say, we will set the minimum
amount of instances. So let's say we have
410:36 - always three running to handle the load minimum.
And if one fails, then it's going to spin
410:37 - up another one. And also, of course, the lb
would run traffic to other instances and other
410:38 - AZ. So we have high availability. And now
we have our last scenario here. So what happens
410:39 - when our web application becomes unresponsive
due to distance and geographical location?
410:40 - So let's say someone's accessing our web application
from Asia, and we are in North America. And
410:41 - the distance is causing unavailability? Well,
we have a couple options here, we can use
410:42 - a CloudFront. And so CloudFront could cache
our static content or even our dynamic content
410:43 - to some degree. So that there's there's content
nearby to that user, which gives them back
410:44 - availability, or we could just be running
our our content, or sorry, our servers in
410:45 - another region that's nearby. And we use route
53 routing policy for geo geolocation. So
410:46 - it's, so if we have servers that are in Asia,
that it's going to route traffic to that those
410:47 - servers. Okay, so there you go. That's the
rundown for high availability.
410:48 - We're looking at scale up versus scale out.
So when utilization increases, and we're reaching
410:49 - capacity, we can either scale up known as
vertical scaling, or we can scale out known
410:50 - as horizontal scaling. So in the case for
scale up, all we're doing is we're just increasing
410:51 - the instance size to meet that capacity. And
the trade offs here is this is going to be
410:52 - simple to manage, because we're just increasing
the instance. But we're gonna have lower available
410:53 - availability. So if a single instance fails,
the service is going to become unavailable.
410:54 - Now for scale out known as horizontal scaling,
while we're going to do is we're going to
410:55 - add more instances. And so the advantage here
is we're gonna have higher availability, because
410:56 - if a single instance fails, it doesn't matter,
we're gonna have more complexity to manage.
410:57 - So more servers means more, more of a headache,
okay? So what I would suggest you to do is
410:58 - you generally want to scale out first to get
more availability. And then you want to then
410:59 - scale up so that you keep simplicity, okay.
So you do want to use both these methods,
411:00 - it's just it has to do on the specific scenario
that's in front of you.
411:01 - Hey, this is Angie brown from exam Pro. And
we are looking at Elastic Beanstalk, which
411:02 - allows you to quickly deploy and manage web
apps on AWS without worrying about infrastructure.
411:03 - So the easiest way to think of Elastic Beanstalk
is I always compare it to Heroku. So I always
411:04 - say it's the Heroku of AWS. So you choose
a platform, you upload your code, and it runs
411:05 - without little worry for developers knowing
the actual underlying infrastructure, okay.
411:06 - It just does not recommend it for production
applications, when each of us is saying that
411:07 - they really are talking about enterprise or
large companies. for startups, you know, I
411:08 - know ones that are still using it three years
in, so it's totally fine for those use cases.
411:09 - But if you do see exam questions where they're
talking about, like, you have a workload,
411:10 - it's just for developers, they don't want
to have to really think about what they're
411:11 - doing Elastic Beanstalk is going to be that
choice. So what kind of what kind of things
411:12 - does Elastic Beanstalk set up for you? Well,
it's going to set up you a load balancer,
411:13 - auto scaling groups, maybe the database, easy
to instance, pre configured with the platform
411:14 - that you're running on. So if you're running
a Rails application, you choose Ruby. If you're
411:15 - running large Ral you choose PHP, you can
also create your own custom platforms and
411:16 - run them on Elastic Beanstalk. Another thing
that's really important to know is that Elastic
411:17 - Beanstalk can run Docker eyes environments.
So here we have multi container and Docker.
411:18 - It does have some nice security features where
if you have RDS connected, it can rotate out
411:19 - those passwords for you. And it does have
a couple of deployment methodologies in there.
411:20 - By default, it's in place, but it can also
do bluegreen deployment, and it can do monitoring
411:21 - for you. And down below, you just see these
little boxes, that's just me showing you that
411:22 - when you go into Elastic Beanstalk, you'd
have all these boxes to do some fine tuning
411:23 - but more or less, you just choose if you want
high availability or you want it to be cheap,
411:24 - and it will then just choose all these options
for you. So that is all you need to know about
411:25 - Elastic Beanstalk for the solution architect
is Andrew Brown from exam Pro, and we are
411:26 - going to learn how to utilize Elastic Beanstalk.
So we can quickly deploy, monitor and scale
411:27 - our applications quickly and easily. Okay,
so we're going to go ahead here and hit get
411:28 - started. And we're going to upload or create
a new application here we're going to name
411:29 - it Express j s because that's what we're going
to be utilizing here. Express is just example,
411:30 - a To choose a platform to be no j s, okay,
and so now we're at this option where we can
411:31 - utilize a sample application, which is totally
something we can do. Or we can upload our
411:32 - own code. So for this, I really want you to
learn a few of the caveats of Elastic Beanstalk.
411:33 - And you're only going to learn that if you
upload your own code, you can definitely just
411:34 - do sample application, just watch the videos
to follow along. But the next thing we're
411:35 - going to do is we're going to prep a application
where I have a sample repo here, we're going
411:36 - to go and talk about some of the things we
need to configure and upload in the next video.
411:37 - All right. Alright, so I prepared this expressjs
application. So we can learn the caveats of
411:38 - Elastic Beanstalk. And I even have the instructions
here, if you didn't want to just use this
411:39 - premade one and you want to go the extra effort
to make your own, I do have the instructions
411:40 - here emitting how to install no GS, you have
to figure that out for yourself. But um, you
411:41 - know, just using this application here, we're
going to go ahead and you can either download
411:42 - the zip, or clone it, I'm going to clone it,
because that's the way I like to do it. And
411:43 - we'll go over to our terminal here. And I'm
just going to clone that to my desktop. Okay.
411:44 - And it won't take long to here. And there
we go. We have it cloned. We'll go inside
411:45 - of it, I'm just going to open up the folder
here. So you can
411:46 - get an idea of the contents of this Express
JS application. So with most applications,
411:47 - we just want to make sure that it runs before
we upload it here. So I'm going to do npm
411:48 - install. Okay, it's gonna install all the
dependencies, you just saw a node modules
411:49 - directory created. And I'm just going to run
my application, I have a nice little script
411:50 - here. To do that, it's going to start on localhost.
And here's our application. So it's a very
411:51 - simple application references a very popular
episode of Star Trek The Next Generation.
411:52 - And so we're going to go back and just kill
our application here. And now we're going
411:53 - to start preparing this application for for
Elastic Beanstalk. So when you have an Elastic
411:54 - Beanstalk application, it needs to know how
to actually run this application, okay. And
411:55 - the way it's going to go ahead and do that
is through a hidden directory with a couple
411:56 - of hidden files. So if you scroll down in
my example, here, I'm going to tell you that
411:57 - you need to create a couple, you need to create
a hidden folder called Eb extensions. And
411:58 - inside that folder is going to contain different
types of configuration files. And it's going
411:59 - to tell Elastic Beanstalk how to run this
application. So for no GS, we want it to execute
412:00 - the NPM start command, which is going to start
up the server to run this application. We
412:01 - also need it to serve static files. So we
have another configuration file in here to
412:02 - serve those static files. Now this.eb extensions,
it's actually already part of the repository,
412:03 - so you don't have to go ahead and create them.
But a very common mistake that people have
412:04 - with Elastic Beanstalk is they fail to upload
that hidden folder, because they simply don't
412:05 - see it. Okay. So if you are on your on a Mac
here, you can hit Control Shift period, which
412:06 - is going to show those hidden folders for
Windows and Unix, you're gonna have to figure
412:07 - that out for yourself here. But just be aware
that you need to include this folder for packaging.
412:08 - Alright, so now we know that our application
runs on and we can see all the files we need,
412:09 - we're going to go ahead and packages. So I'm
going to grab what we need here. We don't
412:10 - need the docks. That's just something that
I added here just to get that nice graphic
412:11 - in there for you. And I believe that's all
we need, we could exclude the readme, we're
412:12 - going to exclude the dot get directory because
sometimes they contain sensitive credentials.
412:13 - But the most important thing is this.eb. Extension.
So I'm going to go ahead here and zip this
412:14 - into an archive. Okay, and so now I have that
ready to upload. Okay. And just one more caveat
412:15 - here is that you saw me do an npm install
to install the dependencies for this application.
412:16 - Well, how is Elastic Beanstalk? going to do
that, okay, so it actually does it automatically.
412:17 - And this is pretty much for most environments
have it's going to be a Ruby application,
412:18 - it's going to do bundle install. And if you
have requirements and your Django application
412:19 - is going to do it, but for Elastic Beanstalk
for a no Jess, it's going to automatically
412:20 - run npm install for you. Okay, so you don't
have to worry about that. Um, but anyway,
412:21 - we've prepared the archive. And so now we
can proceed to actually uploading this archive
412:22 - into Alaska. So I left this screen open, because
we had to make a detour and package our code
412:23 - into an archive. And so now we're ready to
upload our code. So just make sure you have
412:24 - upload your code selected here and we'll click
Upload. And just before we upload, I want
412:25 - you to notice that you can either upload a
local archive or you can provide it via a
412:26 - s3 URL and you will have to do this once you
exceed 512. megabytes, which isn't a very
412:27 - hard thing to do, because applications definitely
get larger. So just be aware of that. But
412:28 - we still have a very small application here,
it's only five megabytes, five, five megabytes
412:29 - here. So we can definitely upload this directly.
I do want to point out that we did zip the
412:30 - node modules, and this directory is usually
very large. So I bet if I excluded this, this
412:31 - would have been less than a megabyte. But
for convenience, I just included it. But we
412:32 - did see previous that Elastic Beanstalk does
do an npm install automatically for us. So
412:33 - if we had admitted this, it would be installed
in the server, okay. But I'm just going to
412:34 - upload this archive, it is five megabytes
in size. So it will take a little bit of time
412:35 - here, when we hit the upload button, but just
before we do, we need to set our version label.
412:36 - Now I'm going to name this version 0.0. point
one. And it's a good idea to try to match
412:37 - the versioning here with your Git tags, because
in git, you can tag specific cabinets with
412:38 - specific versions, okay, and we're going to
go ahead and upload this, okay. And this will
412:39 - just take a little bit of time, my Internet's
not the fastest here. So five megabytes is
412:40 - going to take a minute or so here.
412:41 - Okay, great. So we've uploaded that code here.
And we have version 0.0. point one. And so
412:42 - now we can talk about more advanced configuration,
we could go ahead and create this application.
412:43 - But I want to just show you all the little
things that you can configure in Alaska. So
412:44 - let's look at a bit more advanced configurations
here and just make sure that we aren't getting
412:45 - overbilled because we spun up resources, we
didn't realize we're gonna cost us money.
412:46 - So the preset configuration here is on the
low cost here. So it's going to be essentially
412:47 - free. If we were to launch this here, which
is great for learning, but let's talk about
412:48 - if we actually set it to high availability.
So if we set it to high availability, we're
412:49 - going to get a load balancer. So a load bouncer
generally costs at least $15 USD per month.
412:50 - So by having a low cost, we're saving that
money there. And when we set it to high availability,
412:51 - it's going to set it in an auto scaling group,
okay, between one to four instances, with
412:52 - low cost, it will only run a single server,
you can see that is set to a tee to micro,
412:53 - which is the free tier. And we could adjust
that there if we want. And then we have updates.
412:54 - So the deployment method right now is all
at once. And so if we were to deploy our application,
412:55 - again, let's say it's already been uploaded,
and we deploy it again, using all at once,
412:56 - we're going to have downtime, because it's
going to take that server off offline, and
412:57 - then put a new server up with the code in
order to deploy that code. And so we can actually
412:58 - use bluegreen deployment to mitigate that,
and I'm just going to pop in here to show
412:59 - you so all at once means that it's going to,
it's going to shut down and start up a new
413:00 - server in place. And then immutable means
it's going to create a new server in isolation,
413:01 - okay, so just be aware of those options there.
But um, there's that. And we can also create
413:02 - our database and attach it here as well. Sometimes,
that is a great idea. Because if you create
413:03 - an RDS database, so here, I could select like
MySQL and Postgres, right, and you provide
413:04 - the username and password. But the advantage
of creating your art RDS database with Elastic
413:05 - Beanstalk is that it's going to automatically
rotate your RDS passwords for you for for
413:06 - security purposes. So that's a very good thing
to have here. I generally do not like creating
413:07 - my RDS instances with Elastic Beanstalk, I
create them separately and hook them up to
413:08 - my application. But just be aware that you
can go ahead and do that. And I think that's
413:09 - like the most important options there. But
we're just going to make sure that we are
413:10 - set to the low cost free tier here with T
to micro, okay. And we'll go ahead and create
413:11 - our app. And here we go. And so now, what
we're going to see is some information here
413:12 - as it creates our application. And this does
take a few minutes here anytime you launch,
413:13 - because it has to launch at two instance.
But it always takes about, you know, three
413:14 - to five minutes to spin up a fresh instance.
So I will probably clip this video. So this
413:15 - proceeds a lot quicker here. So that deploy
finished there. And it redirected me to this,
413:16 - this dashboard here. So if you are still on
that old screen, and you need to get to the
413:17 - same place as me just go up to express j s
sample up here and just click into your environment
413:18 - and we will be in the same place. So did this
work. So it created us a URL here and we will
413:19 - view it and there you go. Our application
is running on Elastic Beanstalk. Alright.
413:20 - Now if you're looking up here and saying well,
what if I wanted to get my custom domain here?
413:21 - That's where row 53 would come into play.
So in roughly two, three, you would point
413:22 - it to your elastic IP, which is the case here,
because we created a single instance to be
413:23 - cost saving, and attached an elastic IP for
us. If we had done the high availability option,
413:24 - which created a load bouncer, we would be
pointing roughly three, two, that load balancer.
413:25 - And that's how we get our custom domain on
Elastic Beanstalk. And let's just quickly
413:26 - look at what it was doing as it was creating
here. So if we go to events, we're gonna get
413:27 - all the same information. As we were in that
prior, you know, that black terminal screen
413:28 - where it was showing us progress. It's the
exact same information here. So it created
413:29 - an environment for us it, um, had environment
data that it uploaded to s3, it created a
413:30 - security group, it created an elastic IP on
and then it spun up that easy to essence and
413:31 - it took three minutes. And this is what I
said it would take between three to five minutes
413:32 - to spin up an EC two instance, if we had chose
to create an RDS instance, in our configuration
413:33 - to create that initial RDS always takes about
10 to 15 minutes because it has to create
413:34 - that that initial backup. But then from then
on, if we did other deploys, would only take
413:35 - the three to five minutes. Okay, so there
you go. That's all we need to really know
413:36 - for Elastic Beanstalk for the solution architect.
413:37 - So just so we're not wasting our free tier
credits, we should tear down this Elastic
413:38 - Beanstalk environment. So I'm going to go
up here to actions. And we are going to terminate
413:39 - this environment here. And we're going to
have to provide its name so it's up here.
413:40 - So I'm just going to copy it, paste it in
and hit terminate, okay, and this is a bit
413:41 - slow. But we'll let it go here. And it's going
to hopefully destroy this environment. Sometimes
413:42 - it does fail, you'll have to give it another
try there. But once that is done, then you
413:43 - might want to go ahead and destroy it. Delete
the application, okay? So, but it's not necessarily
413:44 - sorry to delete the application. It's just
necessary to destroy this environment here,
413:45 - because this is actually running instances.
All right. So we'll just wait here. shouldn't
413:46 - take too long, just a couple minutes. All
right. So we finished terminating, and it
413:47 - redirected me here. And we can see the previous
terminated environment. And now just to fully
413:48 - clean everything up here, we can delete the
application. Now, there's no cost to keep
413:49 - this application around. It's really the environments
that contain that running resources. But just
413:50 - to be tidy here, we'll go ahead and delete
that there. And we'll provide its name and
413:51 - should be relatively quick, they're in Great.
So the environment and also the application
413:52 - is destroyed. So we're fully or fully cleaned
up. So onto the Elastic Beanstalk cheat sheet.
413:53 - And this is very minimal for the solution
architect associate, if you're doing other
413:54 - exams, where Elastic Beanstalk is more important,
because there's going to be like two pages,
413:55 - okay, so just keep that in mind. But let's
get through this cheat sheet. So Elastic Beanstalk
413:56 - handles the deployment from capacity provisioning,
and load balancing, auto scaling, to application
413:57 - health monitoring when you want to run a web
app, but you don't want to have to think about
413:58 - the underlying infrastructure. You want to
think Elastic Beanstalk. It costs nothing
413:59 - to use Elastic Beanstalk only the resources
it provision. So RDS lb easy to recommend
414:00 - it for test or development apps, not Rebecca
recommended for production use, you can choose
414:01 - from the following pre configured platforms,
you got Java, dotnet, PHP, node, js, Python,
414:02 - Ruby, go and Docker. And you can run Docker
eyes environments on Elastic Beanstalk. So
414:03 - there you go.
414:04 - Hey, this is Angie brown from exam Pro. And
we are looking at API gateway, which is a
414:05 - fully managed service to create, publish,
maintain, monitor and secure API's at any
414:06 - scale. So API gateway is a solution for creating
secure API's in your cloud environment at
414:07 - any scale. So down below, I have a representation
of how API gateway works. So on the left hand
414:08 - side, you'd have your usual suspects, you'd
have your mobile app, your web app, or even
414:09 - an IoT device. And they would be making HTTP
requests to your API, which is generally a
414:10 - URL. And so API gateway provides you a URL
that it generates, so that you can do that.
414:11 - And then in API gateway, you create your endpoint.
So here I have like endpoints for tasks with
414:12 - different methods. And the idea here is that
it's likely to create these virtual endpoints
414:13 - so that you can then point them to AWS services,
the most common common use service is lambda.
414:14 - But yeah, so the easiest way to think about
API gateway and this is really 80 of us this
414:15 - definition is that the API acts as the front
door for applications to access data, business
414:16 - logic and functionality for back end services.
So it's just a virtual, a bunch of virtual
414:17 - endpoints to connect to AWS So let's just
talk about some of the key features API gateway.
414:18 - So API gateway handles all tasks involving
accepting and processing up to hundreds of
414:19 - 1000s of concurrent API calls, including traffic
management, authorization and monitoring.
414:20 - So it allows you to track and control any
usage of your API, you can throttle requests
414:21 - to help prevent attacks, you can expose atbs
endpoints to define a RESTful API is highly
414:22 - scalable. So everything happens automatically
and is cost effective. And you can send each
414:23 - API endpoint to a different target, and maintain
multiple versions of your API.
414:24 - All right, so let's look at how we actually
configure API and the components involved.
414:25 - So the first most important thing are resources.
Okay, so resources over here is Ford slash
414:26 - projects. And so you know, they literally
just are URLs, that's what resources are.
414:27 - And in a API gateway project, you're gonna
want to be creating multiple multiples of
414:28 - these resources, right, because you're not
gonna just have one endpoint. And so here
414:29 - we have a forward slash projects. And underneath,
you can see we have another resource, which
414:30 - is a child of this Parent Resource, which
would create this full URL here for you, you
414:31 - see this weird hyphen ID syntax that is actually
a variable. So that would be replaced with
414:32 - like three or four. But you know, what you
need to know is that resources are URLs, and
414:33 - they can have children, okay? Then what you're
going to want to do is you're going to want
414:34 - to apply methods to your resources. So methods
are your HTTP methods. So it's the usual write,
414:35 - delete patch, post, put options head, okay.
And you can define multiple resources. So
414:36 - if you wanted to do a get to projects, ID,
you could do that. And you could also do a
414:37 - post. And now those are unique endpoints.
So both GET and POST are going to have different
414:38 - functionality. But yeah, you just need to
define all that stuff, right? So yeah, a resource
414:39 - can have multiple methods, resources can have
children, you know. And so once we've defined
414:40 - our API using resources and methods, the next
thing is to actually get our API published.
414:41 - And in order to do that, you're going to need
a bunch of different stages setup. And so
414:42 - stages is just the is just like a way of versioning,
your, your API for published versions. And
414:43 - you're normally going to do this based on
your environment. So you would have like production,
414:44 - QA, for quality assurance, staging, maybe
you'd have one for developers. So yeah, so
414:45 - you'll create those stages. Now once you create
a stage, you're going to get a unique URL
414:46 - that's automatically generated from AWS. So
here, I have one, and this is the called the
414:47 - invoke URL, and this is the endpoint, you're
actually going to hit. So you do this, like
414:48 - Ford slash prod. And then whatever your endpoints
are. So we saw in the previous example, we
414:49 - had Ford slash tasks and projects, you just
depend on there and make the appropriate method,
414:50 - whatever it is, GET or POST. And that's how
you're gonna interact with API gateway. Now,
414:51 - you might look at this and say, I don't really
like the look of this URL, I wish I can use
414:52 - a custom one, you can definitely do that in
API gateway. So you could make this like API
414:53 - dot exam pro CO, instead of this big ugly
URL. But again, for each staging, they're
414:54 - going to have one. So here, it's prod. So
there'd be one here, QA and staging. All right.
414:55 - And so in order to deploy these versions to
the staging, you'd, you'd go to your actual
414:56 - API, and you'd have to do deploy API action.
Every time you make a change, you have to
414:57 - do the deploy API actually doesn't automatically
happen. That's something that confused me
414:58 - for some time, because you think you made
this endpoint? Why aren't they working? It's
414:59 - generally because you have to do so we looked
at how to define the API, and also how to
415:00 - deploy it. The last thing is actually how
do we configure those endpoints. So when you
415:01 - select the method for your resource, you're
going to choose your integration type. And
415:02 - so we have a bunch of different ones. So we
got lambda HTTP and Mach one, you can send
415:03 - it to another ad of a service, which this
option is very confusing, but it's supposed
415:04 - to be there. And you have VPC link. So I would
go to your on premise environment or your
415:05 - local data center or local. Network. Okay.
So you do have those integration types. And
415:06 - then once you do that, the options are going
to vary, but the most common one is lambda
415:07 - function. And so we're going to see is generally
this and the idea is that you get to configure
415:08 - for the the request coming in and the response
going out. Okay, so you could apply authorizations,
415:09 - yes is off none. So you can make it so they
have to authenticate or Authorize. And then
415:10 - you can say you have some configuration lambda,
and you have some manipulation on the response
415:11 - going out. Okay.
415:12 - So get a bit of cost saving here and a bit
of less of a burden on your API, what you
415:13 - can do is you can turn on API gateway cache,
so you can cache the results of common endpoints.
415:14 - Okay. So when enabled on the stage API gateway
caches response from your endpoint for a specific
415:15 - TTL. Okay, so let's just a period of time
have expired right, or Time To Live. API gateway
415:16 - responds to requests by looking up the responses
from the cache. So instead of making a request
415:17 - to the endpoint, okay, and the reason you're
gonna want to do this is going to reduce the
415:18 - number of calls To your endpoint, which is
going to save you money. And it's going to
415:19 - prove latency for the requests made to your
API, which is going to lead to a great experience.
415:20 - So definitely, you might want to turn on.
So we're gonna look at cores now. And this
415:21 - stands for cross origin resource sharing.
And this is to address the issue of same origin
415:22 - policy. So same origin policy protects us
against Xs s attacks, but there are times
415:23 - when we actually do want to access things
from another domain name, okay, that's not
415:24 - from our own. And so that's what corps allows
you to do causes kind of like this document
415:25 - or these header files, which say, Okay, this
domain is okay. To run scripts on. Okay. And
415:26 - so an API gateway, this is something we're
going to be commonly turning on. Because by
415:27 - default, cores is not enabled. And so you're
gonna have to enable it for the entire API
415:28 - or particular endpoints, like the cores that
you want to like the headers, you want to
415:29 - be passed along with those endpoints. And
so here, you'd say, Okay, well, posts are
415:30 - allowed options are allowed and see where
it says access cross origin allow, you're
415:31 - doing a wildcard saying everything's allowed.
Okay? So that's how you would set it. But
415:32 - you know, just understand what causes and
korres is a is these headers that say, this
415:33 - domain is allowed, allowed access to run these
things from this location, okay. And so of
415:34 - course, is always enforced by the client cores
being the browser. Okay, so the core, that
415:35 - means that the browser is going to look for
cores, and if it has cores, then it's going
415:36 - to, you know, do something, okay. So there
you go. So there's this common vulnerability
415:37 - called cross site scripting, exe, s s attacks.
And this is when you have a script, which
415:38 - is trying to be executed from another website,
on your website. And it's for malicious reasons.
415:39 - Because a lot of people when they're trying
to do something malicious, they're not going
415:40 - to be doing it from your site, because that's
from you. It's gonna be from somebody else.
415:41 - So in order to prevent that, by default, web
browsers are going to restrict the ability
415:42 - to execute scripts that are cross site from
another site. But in order to allow scripts
415:43 - to be executed, you need a same origin policy,
okay? That's the concept of the browser saying,
415:44 - Okay, these scripts are allowed to execute
from another website. So again, web browsers
415:45 - do enforce this by default. But if you're
using tools, such as postman and curl, they're
415:46 - going to ignore same origin policy. So if
you're ever wondering why something's not
415:47 - working, cross site, it's going to likely
be this.
415:48 - Right, so we're on to the API gateway cheat
sheet. So API gateway is a solution for creating
415:49 - secure API's in your cloud environment at
any scale, create API's that act as a front
415:50 - door for applications to access data, business
logic or functionality from back end services,
415:51 - API gateway thralls, API endpoints at 10,000
requests per second. We didn't mention that
415:52 - in the core content, but it's definitely exam
question that might come up where they're
415:53 - like, Oh, you have something, you're going
beyond 10,000. And it's not working? Well.
415:54 - That's the reason why is that there's a hard
limit of 10,000 requests per second. And then
415:55 - you have to ask for a increase a service level
increase at the support stages allow you to
415:56 - have multiple published versions of your API.
So prod staging QA. Each stage has an invoke
415:57 - URL, which is the endpoint you use to interact
with your API, you can use a custom domain,
415:58 - domain for your invoke URL. So it could be
API dot example code to be a bit prettier.
415:59 - You need to publish your API via the deploy
API action, you choose which which stage you
416:00 - want to publish your API. And you have to
do this every single time you make a change.
416:01 - It's annoying, but you have to do it. Resources
are URLs. So just think forward slash projects.
416:02 - resources can have child resources, resources.
So the child here being hyphen, Id edit hyphen,
416:03 - hyphen, hyphen is like a syntax is saying
this is a custom variable. That could be three,
416:04 - four to sign an exam question, but it's good
for you to know. You define multiple methods
416:05 - on your resources. So you're gonna have your
get post, delete, whatever you want. cores
416:06 - issues are common with API gateway cores can
be enabled on all or individual endpoints.
416:07 - caching improves latency and reduces the amount
of calls made to your endpoint. Same origin
416:08 - policies help to prevent excess attacks, same
origin policies ignore tools like postman
416:09 - or curl. So similar to policies just don't
work with those or don't work. But it just
416:10 - the ease that so you can work with those tools.
cores is also enforced by the client client
416:11 - would be the browser. So cores, the browser
is going to definitely look for course headers
416:12 - and interpret them. You can require authorization
to to your API via Ava's cognito or a custom
416:13 - lambda. So just so you know, you can protect
the calls to
416:14 - Hey, this is Andrew Brown from exam Pro, and
we are looking at Amazon kinesis, which is
416:15 - a scalable and durable real time data streaming
service. As to ingest and analyze data in
416:16 - real time from multiple sources. So again,
Amazon kinesis is AWS is fully managed solution
416:17 - for collecting, processing and analyzing street
streaming data in the cloud. So when you need
416:18 - real time, think kinesis. So some examples
where kinesis would be of use stock prices,
416:19 - game data, social media data, geospatial data,
clickstream data, and kinesis has four types
416:20 - of streams, we have kinesis data streams,
kinesis, firehose delivery streams, kinesis,
416:21 - data analytics, and kinesis video analytics,
and we're going to go through all four of
416:22 - them. So we're gonna first take a look at
kinesis data streams. And the way it works
416:23 - is you have producers on the left hand side,
which are going to produce data, which is
416:24 - going to send it to the kinesis data stream,
and that data stream is going to then ingest
416:25 - that data. And it has shards, so it's going
to take that data and distributed amongst
416:26 - its shards. And then it has consumers. And
so consumers with data streams, you have to
416:27 - manually configure those yourself using some
code. But the idea is you have these two instances
416:28 - that are specialized to then consume that
data and then send it to something in particular.
416:29 - So we have a consumer that is specialized
to sending data to redshift than dynamodb
416:30 - than s3, and then EMR, okay, so whatever you
want the consumer to send it to, it can send
416:31 - it wherever it wants. But the great thing
about data streams is that when data enters
416:32 - into the stream, it persists for quite a while.
So it will be there for 24 hours, by default,
416:33 - you could extend it up to 160 68 hours. So
if you need to do more with that data, and
416:34 - you want to run it through multiple consumers,
or you want to do something else with it,
416:35 - you can definitely do that with it. The way
you pay for kinesis data streams, it's like
416:36 - spinning up a new CPU instance, except you're
spinning up shards, okay. And that's what's
416:37 - going to be the cost there. So as long as
the shard is running, you pay X amount of
416:38 - costs for X amount of shards. And that is
kinesis data. So onto kinesis firehose delivery
416:39 - stream, similar to data streams, but it's
a lot simpler. So the way it works is that
416:40 - it also has producers and those producers
send data into kinesis firehose. The difference
416:41 - here is that as soon as data is ingested,
so like a consumer consumes that data, it
416:42 - immediately disappears from the queue. Okay,
so data is not being persisted. The other
416:43 - trade off here is that you can only choose
one consumer. So you have a few options, you
416:44 - can choose s3, redshift, Elasticsearch, or
Splunk, generally, people are going to be
416:45 - outputting to s3. So there's a lot more simplicity
here. But there's also limitations around
416:46 - it. The nice thing though, is you don't have
to write any code to consume data. But that's
416:47 - the trade off is you don't have any flexibility
on how you want to consume the data, it's
416:48 - very limited. firehose can do some manipulations
to the data that is flowing through it, I
416:49 - can transform the data. So if you have something
where you want it from JSON, you want to convert
416:50 - it to parkette. There are limited options
for this. But the idea is that you can put
416:51 - it into the right data format, so that if
it gets inserted into s3, so maybe Athena
416:52 - would be consuming that, that it's now in
parkette file, which is optimized for Athena,
416:53 - it can also compress the file. So just simply
zip them, right. There's different compression
416:54 - methods, and it can also secure them. So there's
that advantage. The big advantage is firehose
416:55 - is very inexpensive, because you only pay
for what you consume. So only data that's
416:56 - ingested is what you what you pay for you
kind of think of it like I don't know, even
416:57 - lambda or fargate. So the idea is you're not
paying for those running shards, okay. And
416:58 - it's just simpler to use. And so if you don't
need data retention, it's a very good. Okay,
416:59 - on to kinesis video streams. And as the name
implies, it is for ingesting video data. So
417:00 - you have producers, and that's going to be
sending either video or audio encoded data.
417:01 - And that could be from security cameras, web
cameras, or maybe even a mobile phone. And
417:02 - that data is going to go into kinesis video
streams, it's going to secure and retain that
417:03 - encoded data so that you can consume it from
services that are used for analyzing video
417:04 - and audio data. So you got Sage maker recognition,
or maybe you need to use TensorFlow or you
417:05 - have a custom video processing or you have
something that has like HL based video playback.
417:06 - So that's all there is to it. It's just so
you can analyze and process a video streams
417:07 - applying like ml or video processing service.
417:08 - Now we're gonna take a look at kinesis data
analytics. And the way it works is that it
417:09 - takes an input stream and then it has an output
stream. And these can either be firehose or
417:10 - data streams. And the idea is you're going
to be passing information data analytics What
417:11 - this service lets you do is it lets you run
custom SQL queries so that you can analyze
417:12 - your data in real time. So if you have to
do real time reporting, this is the service
417:13 - you're going to want to use. The only downside
is that you have to use two streams. So it
417:14 - can get a little bit expensive. But for data
analytics, it's it's really great. So that's
417:15 - all there is. So it's time to look at kinesis
cheat sheet. So Amazon kinesis is the ADA
417:16 - solution for collecting, processing and analyzing
streaming data in the cloud. When you need
417:17 - real time, think kinesis. There are four types
of streams, the first being kinesis data streams,
417:18 - and that's a you're paying per shard that's
running. So think of an EC two instance, you're
417:19 - always paying for the time it's running. So
kinesis data streams is just like that data
417:20 - can persist within that stream data is ordered,
and every consumer keeps its own position,
417:21 - consumers have to be manually added. So they
have to be coded to consume, which gives you
417:22 - a lot of custom flexibility. Data persists
for 24 hours by default, up to 168 hours.
417:23 - Now looking at kinesis firehose, you only
pay for the data that is ingested, okay, so
417:24 - think of like lambdas, or fargate. The idea
is that you're not paying for a server that's
417:25 - running all the time. It's just data, it's
ingested, data immediately disappears. Once
417:26 - it's processed consumer, you only have the
choice from a predefined set of services to
417:27 - either get s3, redshift, Elasticsearch, or
Splunk. And they're not custom. So you're
417:28 - stuck with what you got kinesis data analytics
allows you to perform queries in real time.
417:29 - So it needs kinesis data streams or farfalle
firehose as the input and the output, so you
417:30 - have to have two additional streams to use
a service which makes it a little bit of expensive.
417:31 - Then you have kinesis video analytics, which
is for securely ingesting and storing video
417:32 - and audio encoder data to consumers such as
Sage maker, recognition or other services
417:33 - to apply machine learning and video processing.
to actually send data to the streams, you
417:34 - have to either use kpl, which is the kinesis
Producer library, which is like a Java library
417:35 - to write to a stream. Or you can write data
to a stream using the ABS SDK kpl is more
417:36 - efficient, but you have to choose what you
need to do in your situation. So there is
417:37 - the kinesis cheat sheet.
417:38 - Hey, this is Andrew Brown. And we are looking
at AWS storage gateway, which is used for
417:39 - extending and backing up on premise storage
to storage gateway provides you seamless and
417:40 - secure integration between your organization's
on premise IT environment, and ABS AWS storage
417:41 - infrastructure, we can securely store our
data to the newest cloud. And it's scalable
417:42 - and cost effective in uses virtual machine
images in order to facilitate this on your
417:43 - on premise system. So it supports both VMware
ESXi and Microsoft Hyper V. And once it's
417:44 - installed and activate, you can use Eva's
console to create your gateway. Okay, so there
417:45 - is an on premise component and a cloud component
to connect those two things. And we have three
417:46 - different types of gateways, which we're going
to get into now. So the three types of gateways,
417:47 - we have file gateway, which uses NFS, or SMB,
which is used for storing your files in s3,
417:48 - then you have volume gateway, which is using
iSCSI. And this is intended as a backup solution.
417:49 - And we have two different methods of storing
those volumes. And then the last is tape gateway,
417:50 - which, which is for backing up your virtual
tape library. Here we're looking at file gateway.
417:51 - And what it does is it allows you to use either
the NFS, or SMB protocol, so that you can
417:52 - create a mount point so that you can treat
s3, just like a local hard drive or local
417:53 - file system. And so I always think of file
gateway as extending your local storage onto
417:54 - s3. Okay, and there's some details here we
want to talk about, so ownership permissions,
417:55 - and timestamps are all stored with an s3 metadata.
For the objects that are associated with the
417:56 - file, Once the file is transferred to s3,
it can be managed as a native s3 objects and
417:57 - bucket policies versioning lifecycle Lifecycle
Management, cross region replication apply
417:58 - directly to your object stored in your bucket.
So not only do you get to use s3, like a normal
417:59 - file system or hard drive, you also get all
the benefits of s3.
418:00 - Now we're going to look at the second type
of storage gateway volume gateway. So volume
418:01 - gateway presents your application with disk
volumes using internet small computer systems
418:02 - interface. So iSCSI block protocol. Okay,
so the idea is that you have your local storage
418:03 - volume and using this protocol through storage
gateway, we're going to able to interact with
418:04 - s3 and store a backup of our storage volume
as an EBS snapshot, this is going to depend
418:05 - on the type of volume gateway we use because
there are two different types and we'll get
418:06 - into that out of the slide. But let's just
get through what we have here in front of
418:07 - us. So the data is written to the volumes
and can be asynchronously backed up as a point
418:08 - in time snapshot of the volume and stored
in cloud as an EBS snapshots. snapshots are
418:09 - incremental backups that capture only change
blocks in the volume. All snapshots, storage
418:10 - is also compressed to help minimize your storage
charges. So I like to think of this as giving
418:11 - you the power of EBS locally, because if you
were to use EBS on AWS. It does all these
418:12 - cool things for you, right, but so it's just
treating your local drives as like EBS drives,
418:13 - and it's doing this alter s3. So let's let's
go look at the two different types here. So
418:14 - the first type is volume gateway for storage
vault volumes. And the key thing is that it's
418:15 - where the primary data is being stored. Okay,
so the primary data is stored locally while
418:16 - asynchronously backing up the data to AWS.
So you're all your local data is here, and
418:17 - then you just get your backup on AWS. So it
provides on premise applications with low
418:18 - latency access to the entire data set while
still providing durable offset backups. It
418:19 - creates storage volumes and mounts them as
ice FCS devices from your on premise servers.
418:20 - As we saw in the last illustration, any data
written to the stored volumes are stored on
418:21 - your on premise storage hardware. That's what
this is saying here with the primary data.
418:22 - EBS snapshots are backed up to a database
s3 and stored volumes can be between one gigabyte
418:23 - to 16 terabytes in size. Let's take a look
at cached volume. So the difference here between
418:24 - stored volumes cache volumes is the primary
data stored on AWS. And we are caching the
418:25 - most frequently accessed files. So that's
the difference here. And the key thing to
418:26 - remember between storage volume or storage
volumes and cache volumes is where the primary
418:27 - data is. So why would we want to do this?
Well, it minimizes the need to scale your
418:28 - on premise storage infrastructure while still
providing your applications with low latency
418:29 - data access, create storage volumes up to
32 terabytes in size and attach them as I
418:30 - SCSI devices, from your on premise servers,
your gateway stores data that you will write
418:31 - to these volumes in s3 and retain recently
read data in your on premise storage. So just
418:32 - caching those most frequently files, gateway
cache and upload buffer storage cache volumes
418:33 - can be between one gigabyte and 32 gigabytes
in size. So there you go, that is volume gateway.
418:34 - We're looking at the third type of storage
gateway tape gateway. And as the name implies,
418:35 - it's for backing up virtual tape libraries
to AWS. So it's a durable cost effective solution
418:36 - to archive your data in AWS. You can leverage
existing tape based backup application infrastructure,
418:37 - stores data virtual tape cartridges that you
create on your tape gateway, each tape gateway
418:38 - is pre configured with a media changer and
tape drives. I know I'm not showing that in
418:39 - here. But you know, I think it's just better
to see the simpler visualization, but which
418:40 - are available to your existing client backup
applications. Ice SCSI devices, you add tape
418:41 - cartridges as you need to archive your data,
and it supports these these different tape
418:42 - tape services. Okay, so got veem you got backup,
exact net backup. There's also one from Symantec,
418:43 - Symantec that's called Backup Exec, I don't
know, used to be there got bought out, I don't
418:44 - know. So I just listed this one. So maybe
I made a mistake there. But it's not a big
418:45 - deal. But the point is, is that you have virtual
tape libraries, and you want to store them
418:46 - on s3, and it's going to be using s3 Glacier
because of course that is for long, long storage.
418:47 - So there you go. That's it.
418:48 - So we're at the end of storage gateway. And
here I have a storage gateway cheat sheet
418:49 - which summarizes everything that we've learned.
So let's start at the top here, storage gateway
418:50 - connects on premise storage to Cloud Storage.
So it's a hybrid storage solution. There are
418:51 - three types of gateways file gateway, volume,
gateway and tape gateway, Vol gateway, lets
418:52 - s3 act as a local file system using NFS or
SMB. And the easy way to think about this
418:53 - is think of like a local hard drive being
extended into s3. Okay. Volume gateway is
418:54 - used for backups and has two types stored
and cached. Stored volume gateway continuously
418:55 - backs backs up local storage to s3 as EBS
snapshots, and it's important for you remember
418:56 - that the primary data is on premises that's
what's going to help you remember the difference
418:57 - between stored and cached. Storage volumes
are between one gigabyte to 16 terabytes in
418:58 - size, cache volume gateway caches the most
frequently used files on premise and the primary
418:59 - data is stored on s3 again, remember the difference
between where the primary data is being stored.
419:00 - cache volumes are one gigabytes, between three
to gigabytes in size and tape gateway backs
419:01 - up virtual tapes to s3 Glacier for long archival
storage. So there you go, we're all done with
419:02 - storage. Hey, this is Andrew Brown. And we
are going to do another follow along. And
419:03 - this is going to touch multiple services.
The core to it is lambda, but we're going
419:04 - to do static website hosting, use dynamodb
use SNS and API gateway, we're all going to
419:05 - glue it together, because I have built here
a contact form, and we are going to get it
419:06 - hosted and make it serverless. Okay, so let's
get to it. So, um, we're gonna first try to
419:07 - get this website here hosted on s3, okay,
and so what I want you to do is make your
419:08 - way to the s3 console, you can just go up
to services here and type s3 and click here
419:09 - and you will arrive at the same location.
And we're going to need two buckets. So I've
419:10 - already registered a domain called Frankie
Lyons calm here in route 53. Okay, and we're
419:11 - going to have to copy that name exactly here
and create two buckets, okay, and these buckets
419:12 - are going to have to be the exact name as
the domain name. So we're going to first do
419:13 - the naked domain, which is just Frankie Lyons,
calm, okay. And then we're going to need to
419:14 - do the second one here, once this creates,
it's taking its sweet time, we're gonna have
419:15 - to do that with the sub domain, okay. And
so now we have both our buckets, okay. And
419:16 - so now we're going to click into each one
in turn on static website hosting. So going
419:17 - to management over here, or sorry, properties,
there is a box here called static website
419:18 - hosting. And we're going to have this one
redirect to our subdomain here. So we'll do
419:19 - ww dot, I'm not even gonna try to spell that.
So I'm just gonna copy paste it in there.
419:20 - Okay. And we're just going to hit save. Alright,
and so we have a static website hosting redirect
419:21 - set up here. And then we're going to go to
back to Amazon s3 and turn on static website
419:22 - hosting for this one here. So we're going
to go to properties, and set static website
419:23 - hosting and use this bucket. And we're going
to make it index dot HTML error dot html.
419:24 - And yeah, that's good. So now we have our
other stuff turned on, this is going to need
419:25 - to be public facing because it is a bucket.
So we're going to go over to our permissions
419:26 - here and edit the block public access. And
we're going to have to hit Save here, okay.
419:27 - And we just need to type confirm. Okay. And
now we should be able to, we should be able
419:28 - to upload content here. So let's go ahead
and upload our website here. So I do have
419:29 - it on my, my desktop here under a folder called
web. So this is all the stuff that we need
419:30 - to run it probably not the package dot JSON
stuff. So I'm just going to go ahead here
419:31 - and grab this, okay. And we're just going
to click and drag that there. And we'll just
419:32 - upload it. Okay, and that won't take too long
here. And now if we want to preview the static
419:33 - website hosting, we're going to go to our
properties here, and just right click on this
419:34 - endpoint to or I guess you can right click,
we'll just copy it. Okay. And we'll just give
419:35 - it a paste and give it a look here. So we're
getting a 403 forbidden, um, this shouldn't
419:36 - be the case, because we have it. Oh, you know,
it's not WW. Oh, no, and just www. So that's
419:37 - a bit confusing, because we should have this
turned on. So I think what it is, is that
419:38 - I need to update the bucket policy. Okay,
so I'm just going to go off screen here and
419:39 - grab the bucket policy, it's on the database
documentations, I just can't remember it off
419:40 - the top my head. So I just did a quick Google
on static website hosting bucket policy. And
419:41 - I arrived here on a device docs. And so what
we need is we need this policy here. Okay.
419:42 - And so I'm just going to go copy it here.
And I'm going to go back to s3, and I'm going
419:43 - to paste in this bucket policy. Now I do need
to change this to match the bucket name here.
419:44 - So we'll just copy that here at the top. Okay,
and so what we're doing is we're saying allow,
419:45 - allow read access to all the files within
this bucket, okay. And this said, we can name
419:46 - it whatever you want, it's actually optional,
I'm just gonna remove it to clean it up here,
419:47 - okay. And we should be able to save this.
Okay, and we are and now this bucket has public
419:48 - access. So if we go back to this portal three
here, and do refresh, our website is now up.
419:49 - So there is a few other things we need to
do. So this form when we submit it, I wanted
419:50 - to send off an email via SNS, and I also want
it to, I want it to also stored in Dynamo
419:51 - dB, so we have a reference of it. So let's
go ahead and set up an SNS topic and then
419:52 - we'll proceed to do Alright, so let's make
our way over to SNS here. So I'm just gonna
419:53 - go back up to the top here. Just click down
services here and type SNS and we're going
419:54 - to open this up in a new tab. Because it's
great to have all these things open. And we'll
419:55 - just clear out these ones here. Okay? And
we're going to get to SNS here, and what I'm
419:56 - going to do is on the first time I'm here,
so I get this big display here, but a lot
419:57 - of times, you can just click the hamburger
here and get to what you want. So I'm just
419:58 - going to go to topics on the left hand side,
because that's what we need to create here.
419:59 - I'm going to create a topic. And I'm going
to name this topic, um, Frankie Alliance.
420:00 - Okay, so I'm just going to grab that domain
name here. Okay, I'm just gonna say topic
420:01 - here. And I don't need an optional display
name, I guess it depends, because sometimes
420:02 - it's used in the actual email here that's
actually displayed. So I'm just going to copy
420:03 - this here and just put F, and here, Frankie
Alliance, okay, I think we can have uppercase
420:04 - there. And we have a few options here, we
can encrypt it, I'm not going to bother with
420:05 - that, we can set our access policy, we're
gonna leave that, by default, we have this
420:06 - ability to do retries, we're not doing HTTP,
we're going to be using this for email. So
420:07 - this doesn't matter. And, you know, the rest
are not important. So I'm going to hit Create
420:08 - topic here. Okay, and what that's going to
do is that's going to create an Arn for us.
420:09 - And so we're going to have to come back to
this later to utilize that there. Okay. But
420:10 - what we're going to need to do is if we want
to receive emails from here, we're going to
420:11 - have to subscribe to this topic. So down below,
we'll hit the Create subscription here. And
420:12 - we're going to choose the protocol. And so
I want it to be email, and I'm
420:13 - going to choose Andrew at exam pro.co. All
right, I'm just gonna drop down here, see
420:14 - if there's anything else no, nothing important.
And
420:15 - I'm just gonna hit Create subscription. So
what what's that that is going to do? It's
420:16 - going to send me a confirmation email to say,
hey, do you really want to subscribe to this?
420:17 - So you're going to get emails, I'm going to
say yes, so I'm just going to flip over to
420:18 - my email here, and go ahead and do that. Alright,
and so here came the email was nearly instantaneous.
420:19 - And so I'm just going to hit the confirmation
here, okay. And now that's going to confirm
420:20 - my subscription. Okay, so that means I'm going
to now receive emails if something gets pushed
420:21 - to that topic. All right. So yeah, if we go
back to SNS here, you can see it was in a
420:22 - pending state, if we just do a refresh here.
Okay, now we have a confirmation. So there
420:23 - you go. Um, now we can move on to creating
our Dynamo DB table. Alright, so now that
420:24 - we have SNS, let's proceed to create our Dynamo
DB table. So I want to go to services at the
420:25 - top here, type in Dynamo dB. And we will open
this up in a new tab because we will have
420:26 - to come back to all these other things here.
And we're just gonna wait for this to load
420:27 - and we're going to create ourselves a dynamodb
table. So we'll hit Create. And I'm going
420:28 - to name this based off my domain name. So
I'm gonna say Frankie Alliance. And we need
420:29 - to set a partition key. So a good partition
key is something that is extremely unique,
420:30 - like a user ID, or in this case, an email.
So we'll use that as email. And then for the
420:31 - sort key, we are going to use a created date.
Okay, so there is no date time, data structure
420:32 - here in dynamodb. So we'll just have to go
with a string, and that's totally fine. And
420:33 - there are some defaults here. So it says no
secondary indexes, provision, capacity five,
420:34 - and five, etc, etc. So we're just going to
turn that off, and we're gonna override this
420:35 - ourselves. So there is the provisioned, and
we can leave it at provision, I'd rather just
420:36 - go Yeah, we'll leave it at provision for the
time being. But I'm going to override these
420:37 - values. So I'm just gonna say one and one,
okay. And the reason why is just because I
420:38 - don't imagine we're gonna have a lot of traffic
here. So being able to do one Read, read and
420:39 - writes per second should be extremely capable
for us here, right? So this should be no issue.
420:40 - And then we'll just go down below, this is
all fine. We could also encrypt this at rest,
420:41 - I'm just gonna leave that alone. Okay, and
that all looks good to me. So I'm going to
420:42 - hit Create there. Okay, and so this table
is just going to create here and so what we're
420:43 - looking for is that Arn. So once we have the
Arn for the table, then we will be able to
420:44 - go ahead and hook that up into our lambda
code. Okay. So yeah, this all looks great.
420:45 - Um, so I guess maybe the next thing is now
to actually get the lambda function they're
420:46 - working. So maybe we'll do that or I guess
we could go ahead and actually put this behind
420:47 - CloudFront and hook up the domain. I think
we'll do that first. Okay, so we're gonna
420:48 - go and do some CloudFront here, sem rush.
420:49 - So I guess the next thing here is to actually
get our, our proper domain here, so we're
420:50 - not using the AWS one. So I've actually already
registered a domain here and I might actually
420:51 - include it from another tutorial here are
those steps here. So if you feel that there
420:52 - is a jargon section, it's just because I am
bringing that in over here. But we already
420:53 - have the domain name. And so once you have
your domain name, that means that you can
420:54 - go ahead and start setting up CloudFront and
ACM. So we're gonna want to do ACM first.
420:55 - So we're going to type in ACM here. Okay,
that's Amazon certificate manager. And that's
420:56 - how we're going to get our SSL domain, make
sure you click on the one on the left hand
420:57 - side provision certificates, because this
one is like $500. Starting, so it's very expensive.
420:58 - So just click this over here to provision
just make sure again, that is the public certificate
420:59 - and not the private private, again, is very
expensive, we're going to hit request,
421:00 - we're going to put the domain name in. So
we have the domain name there, I'm always
421:01 - really bad at spelling it. So I'm just gonna
grab it here. Let's really hold it, we don't
421:02 - need spelling mistakes.
421:03 - And we're going to have the naked domain.
And we're also going to do wildcard. And so
421:04 - just by doing this, we're going to cover all
our bases of the naked and all all subdomains.
421:05 - So I strongly recommend that you go ahead
and do this, when you are creating your certificates,
421:06 - we're going to hit Next we're going to use
DNS validation email is just a very old way,
421:07 - nobody really does it that way anymore. We're
going to hit review, we're gonna hit confirm
421:08 - request, okay. And so what's happening here
is that it's going it's now in pending validation.
421:09 - And so we need to confirm that we own this
domain. And so we need to add a record to
421:10 - our domain name, since our domain name is
hosted on Route 53, that's going to make it
421:11 - very easy to add these records, it's going
to be one click of a button. So I'm just gonna
421:12 - go ahead here and hit create, and then go
here and hit Create. Okay, and so um, yeah,
421:13 - this shouldn't take too long, we'll hit continue,
okay. And we're just going to wait for this
421:14 - to go pending to issued, okay, this is not
going to take very long, it takes usually
421:15 - a few minutes here. So we're just going to
wait, I'm going to go grab a coconut water,
421:16 - and I'll be back here shortly. Alright, so
I'm back here, and it only took us a few minutes
421:17 - here. And the status has now issued, so meaning
our SSL certificate is ready for use. So that
421:18 - means we can now create our CloudFront distribution.
So what I want you to do is go up to here
421:19 - and type in CloudFront. Okay. And we're going
to make our way over to CloudFront. So, here
421:20 - we are in CloudFront. And we're just going
to create ourselves a new distribution, we
421:21 - have web and rtmp, we're not going to be using
rtmp. This is for Adobe Flash media server.
421:22 - And very rarely does anyone ever use Adobe
anymore, so it's going to have to be the web
421:23 - distribution. And we're gonna have to go through
the steps here. So the first thing is, we
421:24 - need to select our actual bucket here. So
we are going to be doing this for the www,
421:25 - okay. And we're going to restrict bucket access,
because we don't want people directly accessing
421:26 - the website via this URL here, we want to
always be through the domain name. So that's
421:27 - what this option is going to allow us to do.
It's going to create a new origin identity,
421:28 - we can just let it do as it pleases, we need
to grant permission. So I'm gonna say yes,
421:29 - update my bucket policy. So that should save
us a little bit of time there. Now on to the
421:30 - behavior settings, we're going to want to
redirect HTTP to HTTPS, because really, no
421:31 - one should be using HTTP, we are going to
probably need to allow we'll probably have
421:32 - this forget. And head I was just thinking
whether we need post and patch. But that's
421:33 - only four if we were uploading files to s3
through education. So I think we can just
421:34 - leave it as get get in head. So we're fine
there. We're just going to keep on scrolling
421:35 - down here, we're not gonna restrict access
to this is a public website, we're just going
421:36 - to drop down and choose US, Canada and Europe,
you can choose best performance, I just feel
421:37 - this is going to save me some time because
it does take a long time for this thing to
421:38 - distribution to create. So the fewer edge
locations, I think the less time it takes
421:39 - to create that distribution, we're going to
need to put our alternate domain name in here.
421:40 - So that is just our domain names, we're going
to put www dot and again, I don't want to
421:41 - spell it wrong. So I'm just going to copy
it here manually. Okay. Back to CloudFront
421:42 - here and we'll just do Frankie lines calm.
Now we need to choose our custom SSL, we will
421:43 - drop down here and choose Frankie Alliance
Comm. Okay, and we need to set our default
421:44 - route object that's going to be index dot
HTML. That's how it knows to look at your
421:45 - index HTML page right off the bat. Okay, and
that's everything. So we're gonna hit Create
421:46 - distribution. And luckily, there are no errors.
There are no errors. So we are in good shape
421:47 - here. I'm not sure why it took me here. So
I'm just going to click here to see if it
421:48 - actually created that distribution. It's very
strange, it usually takes you to the distribution
421:49 - page there. Okay, but it is creating that
distribution. Okay, so we're gonna wait in
421:50 - progress. This does take a considerable amount
of time. So go take a shower, go take a walk,
421:51 - go watch an episode of Star Trek, and we will
be back here shortly. So our distribution
421:52 - is created. It took about 20 minutes for this
to create. I did kind of forget to tell you
421:53 - that we have to create two distributions.
So sorry about that, but we're going to have
421:54 - to go ahead and make another one. So we have
one here for the www, but we're going to need
421:55 - one for the naked domain. So I want you to
go to create distribution and go to web. And
421:56 - for the domain name, it's going to be slightly
different. Okay, so instead of selecting the
421:57 - bucket, what I want you to do is I want to
go back to s3. And I want you to go to the
421:58 - bucket with the naked domain. And we're going
to go to properties here, okay. And at a static
421:59 - website hosting, I don't want you to copy
everything. For the end point here with the
422:00 - exception of the HTTP, colon, forward slash
forward slash, okay. And we're just going
422:01 - to copy that. And we're going to paste that
in as the domain name. All right, so we're
422:02 - not going to autocomplete anything that I
want you to hit tab, so that it autocompletes
422:03 - this origin ID here. And then we can proceed.
So we will redirect HTTP to HTTPS.
422:04 - We'll just scroll down here. So this is all
good. The only thing is, we want to change
422:05 - our price class to you the first one here,
okay, we're going to need to put that domain
422:06 - name in there. So we'll just copy it here
from s3, and paste that in, we're going to
422:07 - need to choose our custom SSL drop down to
our SSL from ACM. And we'll leave everything
422:08 - else blank. And we'll create that distribution.
So now it's going to be another long wait.
422:09 - And I will talk to you in a bit here. So after
waiting 20 minutes, our second distribution
422:10 - is complete, I want you to make note that
for the naked domain, we were pointing to
422:11 - that endpoint for the static s3, website hosting,
and for the WW, we are pointing to the actual
422:12 - s3 bucket, this doesn't matter. Otherwise,
this redirect won't work. Okay, so just make
422:13 - sure this one is not set to the bucket. Alright.
So now that we have our two distributions
422:14 - deployed, we can now start hooking them up
to our custom domain name. Alright, so I want
422:15 - you to make your way over to route 53, we're
going to go to hosted zones on the left hand
422:16 - side, we're going to click into that domain
name. And we're going to add two record sets,
422:17 - we're gonna add one record set for the naked
domain, and then the www Alright, so we're
422:18 - gonna leave that blank there, choose alias.
And we're going to drop down here, and we're
422:19 - going to choose that CloudFront distribution.
Now, there are two distributions, there's
422:20 - no chance of you selecting the incorrect one,
because it's only going to show the one that
422:21 - matches for the domain. Alright, so we'll
hit create for the naked domain, and then
422:22 - we'll add another one. And we'll go to www,
and we're gonna go to alias, and we're going
422:23 - to choose the www CloudFront distribution.
All right, and now that those are both created,
422:24 - we can go ahead and just grab this domain
name here and give it a test. Okay, and so
422:25 - it's working, we're on our custom domain name.
Now, definitely, you want to check all four
422:26 - cases. So with, with and without the WW. And
with and without the s for SSL. And then in
422:27 - combination of so that one case works will
try without the WW. Okay, it redirects as
422:28 - expected, okay, and we will try now without
the SSL and make a domain. And it works as
422:29 - expected. And we will just try it without
the S here. And so all four cases work, we
422:30 - are in great shape. If anyone ever reports
to the your website's not working, even though
422:31 - it's working for you just check all those
four cases, maybe there is a issue there.
422:32 - Alright. So now that we have route 53, pointing
to our distributions and our custom domain
422:33 - hooked up, we need to do a little bit of work
here with our www. Box bucket there because
422:34 - when we first created this policy, we added
this statement here, which allowed access
422:35 - to this bucket. And then when we created our
CloudFront distribution, we told it to only
422:36 - allow access from that bucket. So it added
this, this statement here, all right. And
422:37 - so this one was out of convenience, because
we weren't using a custom domain. And so we
422:38 - only want this website to be accessible through
CloudFront. And this is still allowing it
422:39 - from the original endpoint domain. So if we
were to go to management, I'm just going to
422:40 - show you what I mean, I'm sorry, properties.
And we were to go to this endpoint here. All
422:41 - right, we're going to see that it's still
accessible by this URL. So it's not going
422:42 - through CloudFront. And we don't want people
directly accessing the bucket. We want everything
422:43 - to go through CloudFront. So we get statistics
and other fine tuned control over access to
422:44 - our website here. So what I want you to do
is I want you to go and back to your permissions
422:45 - here and go to bucket policy and remove that
first statement. Okay. And we're going to
422:46 - hit save. All right, and we're going to go
back to this endpoint, and we're going to
422:47 - get a refresh. And we should get a 403. If
you don't get it immediately, immediately.
422:48 - Sometimes chrome caches the browser's just
try another browser or just hit refresh until
422:49 - it works. Because if you definitely have removed
that bucket policy, it should return a 403.
422:50 - So now, the only way to access it and we'll
go back to the original one here is through
422:51 - the domain name. So there you go. It's all
hooked up here and we can now proceed to actually
422:52 - working with the landlord. Alright, so now
it's time to work with AWS lambda. And so
422:53 - I prepared a function for you here in the
For a folder called function, and the idea
422:54 - is we have this form here. And when we submit
it, it's going to go to API gateway and then
422:55 - trigger this lambda function. Alright. And
those, that data will be inputted into this
422:56 - function here. So it gets passed in through
event. And then it parses event body, which
422:57 - will give us the return Jason of these fields
here, that we're going to use validate, which
422:58 - is a, it's a third party library that we use
to do validations. Okay, so if I just open
422:59 - up the constraints, file here, this uses,
this actually validates all these fields.
423:00 - And so whether the input is valid or not,
it's going to either return an error to this
423:01 - forum saying, hey, you have some mistakes.
If it is successful, then what it's going
423:02 - to do. Alright, it's going to call this success
function, and then it will call insert, record
423:03 - and send email. So these two things are in
two separate files here. So one is for Dynamo
423:04 - dB, and one is for SNS. So when we say insert
record, we're saying we're going to insert
423:05 - a record into the dynamodb table that we created.
And then for SNS, we're going to send that
423:06 - email off. Alright, so this is a slightly
complex lambda function. But the reason I
423:07 - didn't just make this one single file, which
could have been very easy is because I want
423:08 - you to learn at the bare minimum of of a more
complex lambda functions, such as having to
423:09 - upload via zip, and dealing with dependencies.
All right, so now that our, our we have a
423:10 - little walkthrough there, let's actually get
this lambda function into the actual AWS platform.
423:11 - All right. So before we can actually upload
this to AWS, we have to make sure that we
423:12 - compile our dependencies. Now, I could easily
do this locally. But just in case, you don't
423:13 - have the exact same environment as myself,
I'm gonna actually show you how to do this
423:14 - via cloud nine. All right, so what I want
you to do is, we're just going to close that
423:15 - tab here. And I want you to close these other
ones here and just leave one open. And we're
423:16 - going to make our way over to cloud nine.
423:17 - Okay, and so just before we create this environment,
can you double check to make sure that you
423:18 - are in the correct region. So I seem to sit
on the border of US East, North Virginia,
423:19 - and Ohio, and sometimes it likes to flip me
to the wrong region. So I'm gonna switch it
423:20 - to North Virginia, this is super important,
because when we create our lambda functions,
423:21 - if they're not in the same region, we're going
to have a bunch of problems. All right, so
423:22 - just make sure that's North Virginia, and
go ahead and create your environment. Okay.
423:23 - And I'm going to name this based off our domain
name. So I
423:24 - should probably have that tab open there.
So I'm just gonna open up route 53. There,
423:25 - okay. And I'm just going to copy the domain
name here. Okay, and I'm going to name it
423:26 - for Randy Alliance.
423:27 - Alright. And we're going to go to a next step.
And we are going to choose to micro because
423:28 - it's the smallest instance, we're gonna use
Amazon Lex, because it's packed with a bunch
423:29 - of languages pre installed for us. Cloud Nine
environments do shut off after 30 minutes,
423:30 - so you won't be wasting your free credits.
Since it is a T two micro it is free tier
423:31 - eligible. And the cost of using cloud nine
is just the cost of running the cloud, the
423:32 - actual EC two instance underneath to run the
environment. So we're going to hit next step
423:33 - here, we're going to create the environment
here. And now it will only take a few minutes,
423:34 - and I will see you back shortly here. Oh,
so now we are in cloud nine. And I'm just
423:35 - going to change the theme to a darker because
it's easier on my eyes, I'm also going to
423:36 - change the mode to vim, you can leave it as
default vim is a complex keyboard setup. So
423:37 - you may not want to do that. And now what
we can do is we can upload our code into Cloud
423:38 - Nine. So that we can install the dependencies
for the specific Node JS version we are going
423:39 - to need, so I want you to go to File and we're
going to go to upload local files. And then
423:40 - on my desktop, here, I have the contact form,
it's probably a good idea, we grab both the
423:41 - web and function, the web contains the actual
static website, and we will have to make adjustments
423:42 - to that code. So we are just prepping ourselves
for a future step there. And so now that code
423:43 - is uploaded, okay, so what we're looking to
do is we want to install the dependencies,
423:44 - okay, because there, we need to also bundle
in whatever dependencies within this function,
423:45 - so for it to work, and in order to know what
to install with, we need to know what version
423:46 - of Node JS we're going to be using. And the
only way we're going to know is by creating
423:47 - our own lambda function. Alright, so what
I want you to do is just use this other tab
423:48 - here that I have open, and I'm going to go
to the lambda, the lambda interface here,
423:49 - and we are going to go ahead and create our
first. All right, so let's proceed to create
423:50 - our lambda function. And again, just make
sure you're in the same region so North Virginia
423:51 - because it needs to be the Same one is the
cloud nine environment here. So we're going
423:52 - to go ahead and create this function and we're
going to need to name it, I'm going to name
423:53 - it the offeror engi. Alliance, okay? And I'm
gonna say contact form. Alright. And I believe
423:54 - I spelt that correctly there, I'm just using
that as a reference up here. And we need to
423:55 - choose a runtime. So we have a bunch of different
languages. So we have Ruby, Python, Java,
423:56 - etc, etc, we were using no Gs. And so now
we, you can use 10, or eight point 10, it's
423:57 - generally recommended to use 10. But there
are use cases where you might need to use
423:58 - eight, and six is no longer supported. So
that used to be an option here, but it is
423:59 - off of the table now, so to speak. So we need
to also set up permissions, we're not going
424:00 - to have a role. So we're going to need to
create one here. So let's go ahead and let
424:01 - this lambda create one for us. And we will
hit Create function, okay. And we're just
424:02 - gonna have to wait a few minutes here for
it to create our lambda function. Okay, not
424:03 - too long. And here's our function. Great.
So it's been created here. And the left hand
424:04 - side, we have our triggers. So for our case,
it's going to be API gateway. And on the right
424:05 - hand side, we have our permissions. And so
you can see, by default, it's giving us access
424:06 - to cloud watch logs. And we are going to need
dynamodb and SNS in here. So we're going to
424:07 - have to update those permissions, just giving
a quick scroll down here, we actually do have
424:08 - a little cloud nine environment embedded in
here. And we could have actually done all
424:09 - the code within here. But I'm trying to set
you up to be able to edit this web page. And
424:10 - also, if the if your lambda function is too
large, then you actually can't use this environment
424:11 - here. And so you'd have to do this anyway.
So I figured, we might as well learn how to
424:12 - do it with the cloud nine way, alright, but
you could edit it in line, upload a zip file,
424:13 - so as long as it's under 10 megabytes, you
can upload it and you should, more or less
424:14 - be able to edit all those files. But if it
gets too big, then you have to supply it via
424:15 - s3. Alright, so, um, yeah, we need to get
those additional permissions. And so we're
424:16 - going to need to edit our, our roll, which
was created for us by default. All right,
424:17 - let's make our way over to I am so just gonna
type in I am here. And once this loads here,
424:18 - we're going to go to the left hand side, we're
going to go to a roles, and we're going to
424:19 - start typing Frankie. So if he are, okay,
there's that role. And we're going to add,
424:20 - attach a couple policies here. So we're gonna
give it we said SNS, right, we'll give it
424:21 - full access. And we're
424:22 - gonna give it dynamodb. And we'll give it
full access. Now, for the associate level,
424:23 - it's totally appropriate to use full access.
But when you become a database professional,
424:24 - you'll learn that you'll want to pair these
down to only give access to exactly the actions
424:25 - that you need. But I don't want to burden
you with that much. Iam knowledge at this
424:26 - time, all right, but you'll see in a moment,
because when we go back to, sorry, lambda
424:27 - function, and we refresh here, okay, I'm just
hitting the manual refresh there, we're gonna
424:28 - see what we have access to. So this is now
what we have access to, and we have a bunch
424:29 - of stuff. So we don't just have access to
dynamodb. We have access to Dynamo DB accelerator,
424:30 - we're not going to use that we have access
to easy to we don't need that we, we have
424:31 - access to auto scaling, we probably don't
need that data pipeline. So that's the only
424:32 - problem with using those full access things
as you get a bit more than what you want.
424:33 - But anyway, for our purposes, it's totally,
totally fine. Okay, and so now that we have
424:34 - our role, we want to get our code uploaded
here. So what I want you to do is I want you
424:35 - to go back to cloud nine. All right, and we're
going to bundle this here. So down below,
424:36 - we are in the correct directory environment.
But just to make sure we are in the same place,
424:37 - I want you to type cd Tilda, which is for
home, and then type forward slash environment.
424:38 - Okay, and then we're gonna type forward slash
function. All right, I want to show you a
424:39 - little a little thing that I discovered is
that this top directory that says Frankie
424:40 - Alliance is actually the environment directory,
for some reason, they name it differently
424:41 - for your purpose. But just so you know, environment
is this folder here. Okay. And so now we know
424:42 - our node version is going to be 10. I want
you to type in nvm, which stands for node
424:43 - version manager type NBN list, and we're going
to see what versions of node we have installed
424:44 - and which one is being used. And by default,
it's using 10. So we're already in great shape
424:45 - to be installing our dependencies in the right
version. So I want you to do npm install,
424:46 - or just I, which is the short form there.
Okay. And it's going to install all the dependencies
424:47 - we need. We can see they were installed under
the node modules directory there. So we have
424:48 - everything we want. And so now we just need
to download this here and bring it into our
424:49 - lambda function. So we are going to need to
zip this here and then upload it to the lamda
424:50 - interface here. So what I want you to do is
I want you to right click the function folder
424:51 - here and click download and it's going to
download this to our download. folder here,
424:52 - okay. And then I want you to unzip it, okay,
because we actually just want the contents
424:53 - of it, we don't want this folder here. Alright.
And the idea is that we are just including
424:54 - this node modules, which we didn't have earlier
here. And I'm just going to go ahead and compress
424:55 - that. And then we're going to have an archive.
And I want you to make your way back to the
424:56 - lamda interface here. And we're going to drop
down and upload a zip. Alright, and we are
424:57 - going to upload that archive. Alright, and
then we will hit save so that it actually
424:58 - uploads the archive, it doesn't take too long,
because you can see that it's less than a
424:59 - megabyte. And so we can access our files in
here. All right. And again, if this was too
425:00 - large, then it would actually not allow us
to even edit in here, and we'd still have
425:01 - to do cloud nine. Alright. So now that our
code is uploaded, now we can go and try and
425:02 - run it, or better yet, we will need to learn
how to sync it back to here, so that we can
425:03 - further edit it. Okay. So what I want to just
show you quickly here in cloud nine is if
425:04 - you go to the right hand side to AWS resources,
we have this lambda thing here. And again,
425:05 - if we were in the wrong region, if we were
in US East two, in our cloud nine environment,
425:06 - we wouldn't be able to see our function here.
But here's the remote function. And what we
425:07 - can do is if we want to continuously edit
this, we can pull it to cloud nine, and edit
425:08 - it here and then push it back and upload it.
So this is going to save us the trouble of
425:09 - continuously zipping the folder. Now, you
could automate this process with cloudformation,
425:10 - or other other serverless frameworks. But,
you know, I find this is very easy. It's also
425:11 - a good opportunity to learn the functionalities
of cloud nine. So now that we have the remote
425:12 - function here, I just want you to press this
button here to import the lambda function
425:13 - to our local environment. And it's saying,
Would you like to import it? Yes, absolutely.
425:14 - Okay. And so now, this is the function here,
and this is the one we're going to be working
425:15 - with. So we can just ignore this one here.
425:16 - All right. And so whenever we make a change,
we can then push this back, alright. And we
425:17 - might end up having to do that here or not.
But I just wanted to show you that you have
425:18 - this ability. All right. So actually, let's
actually try sinking it back, we're just going
425:19 - to make some kind of superficial change something
that doesn't matter, I just want to show you
425:20 - that you can do it. So we're going to just
type in anything here. So I'm just going to
425:21 - type in for reggy. Just a comment, okay. And
I'm going to save this file here and see how
425:22 - that was a period. Now it's green. So that
says it's been changed. And what I'm going
425:23 - to do is I'm going to go up here and click
this, and I'm going to re import this function.
425:24 - Alright, so I'm just going to hit this deploy.
Alright, and that's going to go ahead and
425:25 - send the changes back, alright, to the remote
function here. Okay. And I'm just going to
425:26 - hit our refresh here. All right. And then
what I'm going to do is I'm going to go back
425:27 - to the land environment here, I'm just going
to give it a refresh here. And let's see if
425:28 - our comment shows up. Okay, and so there you
are. So that's how we can sync changes between
425:29 - cloud nine in here. And again, if this file
was too large, we wouldn't even be able to
425:30 - see this. So Cloud Nine would be our only
convenience for doing this. So now that we
425:31 - know how to do that, let's actually learn
how to test our function. So let's proceed
425:32 - to doing that. So now let's go ahead and test
our function out. And so we can do here is
425:33 - create a new test event. Alright, and so I've
prepared one here for us. Okay, so we have
425:34 - a JSON with body. And then we have a string
of five JSON, because that's how it would
425:35 - actually come in from API gateway. All right,
and so I'm just going to do a contact form,
425:36 - test contact form, okay? And hit Create there.
And so now we have this thing that we can
425:37 - test out. All right, and so I'm just going
to go ahead and hit test, and we're going
425:38 - to get a failure and that's totally, totally
fine because if we scroll down, it actually
425:39 - tells us that we have an undefined a table
name. Alright. And the reason why we have
425:40 - an undefined table name is because we actually
haven't configured our SNS topic, like it's
425:41 - our an ID or the dynamodb table. So what we're
going to need to do is we're gonna need to
425:42 - supply this function with that information
and I believe we have them as environment
425:43 - variables, that's how they're specified. So
if I was to go back to cloud nine here, and
425:44 - we were to look maybe in Dynamo dB, I'm just
looking at where we actually configure the
425:45 - actual table name here. Okay, um, there it
is. So see where it says process envy table
425:46 - name, so that means is expecting a down here
in the environment. We are expecting under
425:47 - our environment variables where our environment
variables, here they are, so we're expecting
425:48 - a table name. And we're also for SNS, we are
expecting the topic Arn. Okay, so we need
425:49 - to Go grab those two things there. And we
will have better luck this time around. So
425:50 - I'm going to go and look for dynamodb here.
Okay, and I'm going to also get SNS. And we
425:51 - will go ahead and swap those in. So for our
table, it's just called Frankie Alliance.
425:52 - So that's not too complicated. Okay. And we
will just apply it there. And then for SNS,
425:53 - we might actually have an additional topic
here since the last time we were here, and
425:54 - we need the Arn. So we're going to go in here
and grab this. Okay, we need the Arn. All
425:55 - right. And so I'll paste that in there. And
we will hit save. Okay, and we will give this
425:56 - a another trial. So let's go ahead and hit
that test button and see what we get this
425:57 - time around. Fingers crossed. And look, we
got a success here. All right. So if we were
425:58 - to wanting to take a look at the results of
this, if we go down to monitoring here, okay,
425:59 - and we go to View cloud watch logs, we can
actually see our errors and our success rates.
426:00 - Okay. So here, we have a couple logs, I'm
just gonna open up this one here. Alright.
426:01 - And so we can see here that the body was passed
along. And it inserted into the dynamodb.
426:02 - table there. And it also did the SNS response.
And so these are all just the console logs
426:03 - that I have within the actual code. So if
you're wondering where these are coming from,
426:04 - it's just these console logs here. Okay, so
I set those up so that I'd have an idea for
426:05 - that. So let's actually go take a look at
dynamodb and see if our record exists. And
426:06 - there it is. So it's added to the database.
So now the real question is, have we gotten
426:07 - an email, notification. And so I'm just going
to hop over to my email, and we're going to
426:08 - take a quick look now.
426:09 - Alright, so here I am in my email, and we
literally got this email in less than a minute.
426:10 - And here is the information that has been
provided to us. So there you go, our lambda
426:11 - function is working as expected. And so now
that we have that working, the next big thing
426:12 - is actually to hook up our actual actual form
to this lambda function. And so in order to
426:13 - do that, we are going to need to set up API
gateway. So that that form has somewhere to
426:14 - send its data to. So let's proceed over to
API gateway. And we are going to create our
426:15 - own API gateway, Kool
426:16 - Aid. Okay, hello, let's go. So here we are
at the API gateway. And if you need to figure
426:17 - out how to get here, just type API up here
in the services. And we will be at this console.
426:18 - And we will get started. And we'll just hit
X here, I don't care what they're saying here.
426:19 - And we're going to hit new API, make sure
it's rest. And we're going to name it as we
426:20 - have a naming everything here. So I'm going
to call this for Engie. Alliance, okay. And
426:21 - the endpoint is original, that's totally fine
here. And we will go ahead and create that
426:22 - API. So now our API has been created. And
we have this our default route here, okay.
426:23 - And so we can add multiple resources or methods,
we can totally just work with this route here.
426:24 - But I'm going to add a resource here, and
I'm going to call it a transmit, okay. Okay.
426:25 - And I'm just going to name it the same up
here. And we're also going to want to enable
426:26 - API gateway course, we do not want to be dealing
with course issues. So we will just checkbox
426:27 - that there. And we'll go ahead and create
that resource. Okay, and so now we have a
426:28 - resource. And by default, we'll have options.
We don't care about options, we want to have
426:29 - a new method in here, and we are going to
make it a post. Okay. And we are going to
426:30 - do that there. And it's actually going to
be a lambda function. That's what we want
426:31 - it to have. All right. And do we want to use
the lambda integration proxy requests will
426:32 - be proxy to lambda with request details available
in your event? Yes, we do. That sounds good
426:33 - to me. And then we can specify the lamda region
is US East one, and then the lambda function.
426:34 - So here, we need to supply the lambda function
there. So we're going to make our way back
426:35 - to that lambda function there. So we have
this SNS topic, I'm just going to go over
426:36 - here and go back to lambda. And we're going
to grab the name of that lambda function and
426:37 - go back to API gateway and supply it there.
Okay, and then we're going to go ahead and
426:38 - save that. Yes, we are cool with that. Okay.
And we'll hit Save there. And so now our lambda
426:39 - function is kind of hooked up here. We might
have to fiddle with this a little bit more.
426:40 - But if we go back to our our lamda console
here and hit refresh. Now on the left hand
426:41 - side, we should see API gateway. So API gateway
is now a trigger. Okay? So yeah, we will go
426:42 - back to API gateway here. Alright, so now
to just test out this function to see if it's
426:43 - working, I'm going to go to test here and
we have some things we can fill in like query
426:44 - strings, headers, and the request body. So
flipping back to here, you're going to probably
426:45 - wonder why I had this up here. Well, it was
to test for this box here. And so I'm just
426:46 - gonna just slightly change it here. So we'll
just change it to Riker. Okay, and we're gonna
426:47 - say bye. Okay. All right. And so, you know,
now that I have this slight variation here,
426:48 - I'm just going to paste that in there. And
hit test. Okay. And it looks like it has worked.
426:49 - So we can just double check that by going
to a dynamodb here and doing a refresh here.
426:50 - And so the second record is there. Obviously
in cloud watch, if we were to go back here
426:51 - and refresh, okay, we are going to have updated
logs within here. Alright, so not sure if
426:52 - the logs are showing up here. There is Riker.
So he is in there. So our API gateway endpoint
426:53 - is now working as expected. Okay. So now what
we need to do is we need to publish this API
426:54 - gateway
426:55 - endpoint. Okay, so let's go ahead and do that.
All right. Okay. Hello, let's go. So in order
426:56 - to publish our API, what we're going to need
to do is deploy it. Okay. So anytime you make
426:57 - changes, you always have to deploy your API.
So I'm going to go ahead here and hit deploy
426:58 - API. And we're going to need to choose a stage,
we haven't created any stages yet. So I'm
426:59 - going to go here, I'm going to type prod,
which is short for production, okay, very
427:00 - common for people to do that. You could also
make it pro if you like, but prod is generally
427:01 - what is used. And I'm gonna go ahead and deploy
that. Okay. And so now it is deployed. And
427:02 - now I have this nice URL. And so this URL
is what we're going to use to invoke the lambda.
427:03 - So all we have to do is copy this here, and
then send a put request to our sorry, post
427:04 - request to transit here. So what we're going
to do is copy this URL here, okay. And we're
427:05 - going to go back to our cloud nine environment,
and we are going to go to our web in here.
427:06 - Okay. And in this, we have our HTML code,
and we have a function called form Submit.
427:07 - And so if we were to go into the JavaScript
here, okay, there is a place to supply that
427:08 - in here. And it's probably going to be Oh,
where did I put it? Um, it is a right here.
427:09 - So on the form submit, it takes a second parameter,
which is URL. All right. And so actually,
427:10 - it's just all right over here, I made it really
easy for myself here, and I'm just going to
427:11 - supply it there. Okay, and we're going to
need strings around that. Otherwise, it's
427:12 - going to give us trouble, it's going to have
to be a double quotations. Okay. And so now
427:13 - this form submit is going to submit it to
that endpoint, it has to be transmit, of course.
427:14 - Okay. So, yeah, there we go. I'm just going
to double check that transmit that looks good
427:15 - to me, I'm just gonna double check that it's
using a post it is. So yeah, that's all we
427:16 - need to do here. So now that we've changed
our index HTML, we're going to need to update
427:17 - update that in CloudFront, and invalidate
that. So let's make our way over to s3 and
427:18 - upload this new file. Okay. All right. So
the first thing we're going to need to do
427:19 - is download this file here. So we're just
gonna go ahead and download that new index
427:20 - HTML. And we're going to need to use one of
our other tabs here, we'll take up the cloud
427:21 - watch one here, we don't need to keep all
these things open. And we're going to make
427:22 - our way to s3, okay. And once we make it into
s3, we're going to go into the www dot francke.
427:23 - Alliance a.com. And we're going to upload
that new file, I believe it's in my downloads.
427:24 - So I'm just gonna go down here and show in
Finder, and here it is. So I'm just going
427:25 - to drag it on over Okay, upload that file.
Okay, and so now that file has been changed,
427:26 - but that doesn't mean that CloudFront has
been updated. So we have to go to our friendly
427:27 - service called CloudFront. Okay, and we're
going to need to invalidate that individual
427:28 - file there. So we're gonna go to www, and
we're gonna go into validations create. Now,
427:29 - we could put an asterisk, but we know exactly
what we're changing. So we're gonna do index
427:30 - dot HTML, and we're going to invalidate it.
And we're gonna just wait for that validation
427:31 - to finish Okay, and then we will go test out
our form. So after waiting a few minutes here,
427:32 - our invalidation is complete. And so let's
go see if our new form is hooked up. So we're
427:33 - going to need to go to the domain name. And
I was have a hard time typing it so I'm just
427:34 - going to call it Copy, Paste it directly here,
okay, just gonna go route 53 and grab it for
427:35 - myself. And there it is. Come on, there we
are. Okay. And so I'm just going to then paste
427:36 - it in there. Okay, so here's our form. And
I just want to be 100%. Sure, because you're
427:37 - using when you're working with Chrome and
stuff, things can aggressively cache there.
427:38 - So see, it's still using the old one. But
we've definitely definitely updated it. So
427:39 - I'm just going to give it a hard refresh here.
Okay, and so now it is there. So just make
427:40 - sure you check that before you go ahead and
do this so that you save yourself some frustration.
427:41 - All right. So now it's the moment of truth
here, and we are going to go ahead and fill
427:42 - up this form and see if it works. So I'm gonna
put in my name Andrew Brown. Okay, and we're
427:43 - gonna just put in exam Pro. Andrew exam pro.co,
we're gonna leave the phone number blank there.
427:44 - I'm going to say, Federation, I want to buy
something. Can I buy a spaceship? Okay. Whoa,
427:45 - boy. And we're going to now hit transmit,
okay, and it's transmitting there, and it
427:46 - says success.
427:47 - Okay, so we're gonna go to dynamodb, and do
a refresh there, and we can see that it's
427:48 - inserted. So there you go, we're all done
all the way there. If we wanted to test the
427:49 - validation, we could totally do so as well.
So I'm just gonna hit Submit here. And here,
427:50 - it's throwing an error. So we're, we're done.
So we went through everything we created dynamodb,
427:51 - an SNS topic, a lambda function, we used cloud
nine, we hooked up API gateway, we created
427:52 - a static website hosting. And we backed it
all by CloudFront using route 53. So we did
427:53 - a considerable amount to get this form. So
you know, that was quite impressive there.
427:54 - So I guess, now that we're done, let's go
ahead and tear all this stuff down. So now
427:55 - it's time to tear down and do some cleanup
here. All the resources we're using here pretty
427:56 - much don't cost any money or shut themselves
down. So there's, we're not going to be in
427:57 - trouble if we don't do this. But, you know,
we should just learn how to delete things
427:58 - here. So first, I'm going to go to dynamodb,
I'm going to go ahead and delete that table.
427:59 - Okay, and I don't want to create a backup.
So we'll go ahead and do that. Then we'll
428:00 - make our way over to SNS, okay. And we are
going to go ahead and delete that topic. So
428:01 - there it is, and we will just delete it, okay,
and we will put in a delete me. And then we
428:02 - will make our way over to lamda. All right.
And we are going to go ahead and delete that
428:03 - lambda function. And then we are going to
make our way over to Iam. And I am roles aren't
428:04 - that big of a deal, but you might not want
to keep them around. So we will just go in
428:05 - here and type in Frankie here. Okay, and we
will delete that role. And then we were running
428:06 - an ECG, or any situ instance via cloud nine.
So I'm just going to close that there. Let's
428:07 - tap here. And I'm gonna just type in, in here,
cloud nine, okay. And we are going to terminate
428:08 - that instance, there, you can see I have a
few others that did not terminate, but we'll
428:09 - go ahead and delete that there. And we will
just type delete. Okay, and that's going to
428:10 - delete on its own. You will want to double
check that there. But I want to get through
428:11 - this all with you here. So we're not going
to watch that. And then we want to delete
428:12 - our API gateway here. Why don't we delete
this, um, we'll go up to the top here. I rarely
428:13 - ever delete API. So we'll go here and enter
the name of the API before commit couldn't
428:14 - just be delete, right? Sometimes it's delete
me, sometimes it's etc, etc, they give you
428:15 - all these different ways of deleting, we'll
hit Delete API. So now that is deleted there.
428:16 - Um, we need to delete our CloudFront. Okay,
so we'll go here. And we have two distributions
428:17 - here. So in order to delete them, you have
to first disable them. And this takes forever
428:18 - to do Okay, so once they're disabled, there'll
be off and then you can just delete them,
428:19 - I'm not going to stick around to show you
how to delete them. As long as they're disabled,
428:20 - that is going to be good enough, but you should
take the extra time and delete them 2030 minutes
428:21 - when these decide to finish, okay, and we
want to delete our ACL if we can. Okay, so
428:22 - we'll make our way over there. So I'm gonna
hit delete, and it won't let me until those
428:23 - are deleted. So we'll wait till those are
fully deleted, then you just go ahead to here
428:24 - and delete that. I'm not going to come back
and show you that it's just not not worth
428:25 - the time here. Some of these things just take
too too long for me. Okay, and then we'll
428:26 - go into fringe Alliance. We will go ahead
and remove these Records here, okay, you don't
428:27 - want to keep records around that are pointing
to nothing. So if those CloudFront distributions
428:28 - are there, there is a way of people can compromise
and have those resources point to somewhere
428:29 - else. So you don't want to keep those around,
then we're going to go to s3, okay. And we're
428:30 - going to go ahead and delete our buckets.
Generally, you have to empty them before you
428:31 - can delete them. So I'm going to go here.
I don't know if databases made this a little
428:32 - bit easier now, but generally, you'd always
have to empty them before you can delete.
428:33 - So I'll hit Delete there. And we will try
this one as well. Okay. Oops, www dot. Okay,
428:34 - look at that. Yeah, that's nice. You don't
have to hit empty anymore. So yeah, I think
428:35 - we have everything with the exception of CloudFront
there and the ACM again, once they're disabled,
428:36 - you delete them, and then you delete your
ACM. So yeah, we have fully cleaned up here.
428:37 - And hopefully, you really enjoyed this. Alright,
so now it's time to book our exam. And it's
428:38 - always a bit of a trick to actually find where
this page is. So if you were to search eight,
428:39 - have a certification and go here.
428:40 - Alright, and then maybe go to the training
overview, and then click get started, it's
428:41 - going to take you to at bis dot training,
and this is where you're going to register
428:42 - to take the exam. So in the top right corner,
we are going to have to go ahead and go sign
428:43 - in. And I already have an account. So I'm
just going to go and login with my account
428:44 - there. So I'm just gonna hit sign in there.
Okay, and we're just going to have to provide
428:45 - our credentials here. So I'm just going to
go ahead and fill mine in, and I will see
428:46 - you on the other side and just show you the
rest of it. Alright, so now we are in the
428:47 - training and certification portal. So at the
top, we have a one stop training. And to get
428:48 - to booking our exam, we got to go to certification
here. And then we're going to have to go to
428:49 - our account. And we're going to be using the
certain metrics, third party service that
428:50 - actually manages the certifications. So we're
going to go to our certain metrics account
428:51 - here. And now we can go ahead and schedule
our exam. So we're going to schedule a new
428:52 - exam. And down below, we're going to get a
full list of exams here. So it used to just
428:53 - be psi. And so now they all have psi Pearson
VUE, these are just a network of training
428:54 - centers where you can actually go take and
sit the exam, for the CCP, you can actually
428:55 - take it from home. Now it's the only certification
you can take from home, it is a monitored
428:56 - exam. But for the rest, they have to be done
at a data center. And so I'm just going to
428:57 - show you how to book it either with psi or
a Pearson VUE here. And again, they have different
428:58 - data centers. So if you do not find a data
center in your area, I'll just go give Pearson
428:59 - VUE a look so that you can actually go book
that exam. So let's go take a look at an exam.
429:00 - So maybe we will book the professional here.
So I'm just going to open this in a tab and
429:01 - open that in a tab and we're going to review
how we can book it here through these two
429:02 - portals. So let's take a look at psi, this
is the one I'm most familiar with. Okay, because
429:03 - Pearson VUE wasn't here the last time I checked.
But so here you can see the duration and the
429:04 - confirmation number, you want to definitely
make sure you're taking the right exam. Sometimes
429:05 - there are similar exams like the old ones,
that will be in here. So just be 100%. Sure,
429:06 - before you go ahead and do that, and go and
schedule your exam. And so it's even telling
429:07 - you that there is more than one available
here, and that's fine. So we'll just hit Continue.
429:08 - Okay. And then from here, we're going to wait
here and we're going to select our language,
429:09 - okay. And then we get to choose our data centers.
So the idea is you want to try to find a data
429:10 - center near you. So if I typed in Toronto
here, so we'll enter a city in here like Toronto,
429:11 - I don't know why thinks I'm over here. And
I'm just going to hit Toronto here. And we're
429:12 - going to search for exam centers. Okay, and
then we are going to have a bunch of over
429:13 - here. So the closest one in Toronto is up
here. So I'm gonna click one. Alright, and
429:14 - it's going to show me the available times
that I can book. So there's not a lot of times
429:15 - this week, generally you have to it has to
be like two, three days ahead. Every time
429:16 - I booked exam, it's never been the next day.
But here we actually have one it's going to
429:17 - vary based on the test center that you have
here. We're going to go ahead here and this
429:18 - one only lets you do Wednesdays and Thursdays.
So if we had the Thursday here at 5pm. Okay,
429:19 - and then we would choose that and we would
continue. Okay, and then we would hit Continue
429:20 - again. Alright, and so the booking has been
created and in order to finalize it, we just
429:21 - have to pay that it is in USD dollars, okay.
So you'd have to just go and fill that out.
429:22 - And once that's filled out and you pay it,
then you are ready to go sit that exam. So
429:23 - that's how we do with psi and then we're gonna
go take a look over at Pearson VUE. So I'm
429:24 - just gonna go ahead and clear this. Because
I'm not serious about booking an exam right
429:25 - now. Okay. And we'll go take a look how we
do it with Pearson VUE. So here we are in
429:26 - the Pearson VUE section to book. And you first
need to choose your preferred language. I'll
429:27 - choose English because that's what I'm most
comfortable with. And we're going to just
429:28 - hit next here. And the next thing it's going
to show us is the price and we will say schedule
429:29 - this exam. All right. And now we can proceed
to scheduling. Okay, so we'll just proceed
429:30 - to scheduling, it's given me a lot of superfluous
option. and here we can see locations in Toronto.
429:31 - Okay, so here are test centres. And we do
actually have a bit of variation here. So
429:32 - you can see there are some different offerings,
you might also see the same data center, so
429:33 - I can choose this one here. Okay, and it lets
you select up to three to compare the availability.
429:34 - So sure, we will select three, and we will
hit next. Okay, we'll just wait a little bit
429:35 - here. And now we are just going to choose
when we want to take that exam there. So we
429:36 - do have the three options to compare. And
so you know, just choose that 11 time, okay.
429:37 - And so then we would see that information
and we could proceed to checkout.