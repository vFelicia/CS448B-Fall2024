00:02 - welcome
00:03 - today's talk is the human nature of
00:07 - failure and resiliency by vm
00:10 - please give her a warm welcome
00:14 - [Applause]
00:19 - hey hi oh my gosh the microphone is on
00:22 - um hey everyone i have a lot to cover so
00:24 - we're just going to dive right in
00:26 - who am i why am i here my name is vm
00:29 - brassuer but because we're all friends
00:30 - here you can call me vicky plus the
00:32 - virtual machine jokes have gotten old
00:35 - please don't think yours is original
00:36 - because it's not
00:38 - i'm the author of this uh which is as
00:41 - far as we can tell the very first book
00:42 - on how to contribute to free and open
00:44 - source software
00:46 - i am an author and community moderator
00:48 - for opensource.com
00:50 - the vice president of the open source
00:52 - initiative
00:53 - i'm an open source policy and strategy
00:55 - freelancer and i do training on how to
00:58 - do this sort of stuff right here
01:01 - but before i did all of this i used to
01:03 - lead software engineering departments
01:05 - and while i was doing that i started
01:07 - studying failure as a bit of a hobby i
01:09 - did a lot of research into failure
01:13 - this is a link to a bibliography of all
01:16 - that research currently there are 205
01:19 - items in this bibliography it adds up to
01:21 - approximately 3 500 pages worth of
01:23 - preacher
01:24 - research into failure
01:26 - now this talk is going to take all of
01:28 - that and give you the greatest hits the
01:30 - things that are common across all those
01:32 - 3 500 pages of research into failure i'm
01:35 - going to give it to you today in
01:36 - approximately 40 minutes
01:38 - here they are here's a quick summary
01:39 - these are the themes that are common
01:41 - across all of that research and what i'm
01:43 - going to share with you today like i
01:45 - said there are thousands of pages here
01:47 - i'm not going to get into everything
01:51 - um i'm also not going to give you the
01:53 - secret serum that will prevent failure
01:55 - for you from here on out i'm just not
01:57 - going to do it because it's not possible
02:00 - i will however give you a list of
02:02 - questions of things you should consider
02:04 - when you want to start succeeding with
02:06 - failure i am only going to give you an
02:09 - overview again because dude 3 500 pages
02:11 - what the hell
02:13 - so um
02:14 - there will hopefully be time for
02:16 - questions if not now there is a break
02:18 - afterwards so catch me afterwards and we
02:19 - can talk about failure it's absolutely
02:21 - fascinating
02:22 - so another thing i need to mention as a
02:25 - limitation of this talk it does go into
02:27 - a lot of psychology
02:29 - and what i have found is most of the
02:32 - research almost all of the research is
02:34 - done in a western mindset using western
02:37 - psychology on western individuals and
02:40 - unrelated recent studies have shown that
02:43 - a lot of these psychological concepts
02:45 - that we
02:46 - in western cultures believe are solid
02:49 - truths don't actually hold true as well
02:52 - for europe for eastern asian african
02:55 - cultures so
02:58 - just be aware there is a bias here for
03:01 - western minds
03:02 - and finally do save questions for the
03:04 - end which is something i really don't
03:05 - have to say at pycon but i include this
03:07 - slide anyway
03:08 - so
03:10 - let's get into those common themes and
03:12 - we're going to start with complexity
03:14 - the world is complex this shouldn't come
03:16 - as a surprise to anyone people are
03:18 - complex hell the complexity hiding in
03:21 - something as simple as chocolate chip
03:22 - cookies
03:23 - is really quite mind-blowing
03:26 - we as humans however we don't like
03:29 - complexity and that leads us to some
03:31 - very unfortunate tendencies for starters
03:34 - we have a tendency to ignore complexity
03:36 - completely la la la la la nothing to see
03:39 - here moving on
03:41 - everything around us is constantly
03:43 - changing constantly evolving yet
03:45 - research shows we prefer to operate as
03:47 - though we are in a static environment we
03:50 - will start a project we'll take a
03:52 - snapshot in our brain of how things are
03:55 - right then
03:56 - and then we'll operate within the
03:57 - snapshot while the rest of the world
03:59 - moves on without us
04:01 - as you can imagine this can lead to some
04:04 - problems
04:06 - research also shows that we dislike
04:08 - variation we like things to be just so
04:11 - and we like them to stay that way hence
04:13 - that snapshot we also prefer to operate
04:16 - in serial rather than parallel because
04:19 - that minimizes complexity and it allows
04:22 - us to focus our efforts on what we then
04:24 - consider to be the best option
04:27 - so we focus on the static now rather
04:29 - than the ever changing complexity of the
04:31 - future and the parallel projects
04:37 - so we can't know everything and we like
04:39 - to think we do
04:40 - but we really can't and because we're
04:42 - more comfortable with what we know we
04:45 - avoid everything we don't and this
04:47 - this is a problem um we don't actually
04:49 - know as much as we think we know we are
04:51 - each and every one of us a walking
04:53 - talking done in kruger
04:55 - film festival
04:58 - so we don't know
04:59 - uh we think we know more than we do and
05:02 - because of that we have a tendency to
05:04 - overlook a lot of things and these are
05:06 - the unknown unknowns
05:08 - and rather than just some silly thing a
05:11 - politician said once this is actually a
05:13 - real psychological concept it's a latent
05:16 - error
05:17 - latent errors are overlooked near misses
05:20 - latent errors look an awful lot like
05:23 - successes because they haven't failed
05:26 - yet
05:27 - and that's because latent errors have
05:28 - yet to meet their enabling condition
05:31 - a an enabling condition is that trigger
05:34 - that turns a latent error to the dark
05:36 - side of failure
05:38 - so uh there is a very good example i
05:41 - like to use for latent errors
05:44 - meeting their enabling conditions now i
05:46 - won't be able to see when you raise your
05:48 - hand but i'm going to ask you to raise
05:49 - your hands anyway
05:50 - how many of you out there
05:53 - remember the deep water horizon uh
05:56 - tragedy that happened a few years ago
05:58 - yeah the oil rig in the gulf of mexico
06:00 - went boom bad thing
06:02 - so what happened there
06:04 - was you had two
06:06 - latent errors really there was a lot
06:08 - going on but we're going to minimize
06:09 - things because again not going into
06:11 - detail we had two latent errors you had
06:13 - somebody performing a cementing
06:15 - procedure and that cementing procedure
06:17 - was filling a pipe with cement and while
06:20 - he was doing so he was doing it wrong
06:24 - normally that's fine that's perfectly
06:25 - fine he was doing it the way that they
06:27 - always do it and there was never a
06:28 - problem but he was doing it wrong and
06:30 - because of that gas was escaping from
06:33 - the pipe that's latent error number one
06:35 - incorrect cementing procedure latent
06:38 - error number two
06:39 - somebody was welding nearby
06:42 - usually not a problem you can have these
06:43 - things in close proximity all the time
06:46 - and it's an oil rig right someone's
06:48 - always cementing and someone is always
06:50 - welding not a problem okay so two latent
06:54 - errors
06:55 - and then we have our enabling condition
06:56 - something that does not happen very
06:58 - often especially not on the high seas
07:01 - and that is it was a windless day
07:05 - so thanks to that windless day
07:07 - that welding that spark caught the gas
07:10 - there was an explosion it killed 11
07:13 - people
07:14 - 16 others were critically injured very
07:17 - very bad and we had the worst
07:19 - environmental disaster our country has
07:21 - ever seen because of latent errors
07:24 - meeting their enabling condition
07:29 - now it is possible to spot
07:31 - latent errors
07:33 - um you can look for them in your
07:34 - postmortems do you do postmortems you
07:36 - probably should
07:38 - as you're doing that as you're looking
07:39 - at your postmortem and you're looking at
07:41 - how things went ask the group so what
07:44 - went right
07:45 - we've been looking at what went wrong
07:47 - but what went right
07:48 - pretty much by chance those are probably
07:51 - your latent errors and things you should
07:52 - consider for your next project but
07:54 - here's the problem we're really bad at
07:57 - postmortems
07:58 - we just don't know how to do them
08:01 - so we focus on symptoms rather than
08:04 - causes
08:06 - when we do look for causes we look for
08:08 - just one we look for root cause i'm
08:11 - sorry folks gonna tear down your little
08:13 - house of cards here there is never just
08:15 - one root cause
08:18 - um we also have selection bias
08:21 - we prefer to look at the easy things the
08:23 - things we can actually answer very
08:24 - quickly because it makes us feel
08:25 - accomplished
08:27 - another form of selection bias that we
08:29 - have
08:31 - is that we only postmortem the things
08:33 - that went wrong
08:35 - rather than the things that went right
08:37 - as well if you post more than the things
08:38 - that went right you're more likely to
08:40 - spot those latent errors that never met
08:42 - their enabling conditions but we don't
08:44 - do that for us postmortems are things
08:46 - that you only do when goes wrong
08:52 - so what do we do
08:53 - how can we minimize these tendencies we
08:55 - have with just avoiding complexity well
08:58 - for starters do pre-mortems
09:00 - before you even start your project
09:03 - do a pre-mortem rather than a
09:05 - post-mortem and start looking for things
09:08 - that might go wrong start looking for
09:10 - latent errors come up with contingency
09:12 - plans for all of them
09:14 - get skeptics in the room who will
09:16 - question all of your assumptions we'll
09:17 - talk a lot more about that in a moment
09:20 - and then make your plan in your
09:22 - pre-mortem but then plan to change the
09:25 - plan because if you remember you take
09:27 - that snapshot right and we like to work
09:29 - inside the snapshot once in a while
09:32 - plan to pause
09:35 - bring your snapshot into modern times
09:38 - and take it again and once you take that
09:40 - new snapshot you might have to adjust
09:42 - your project accordingly
09:45 - now let's dive into assumptions
09:49 - man
09:51 - nearly every single piece of research in
09:54 - that bibliography has terrible things to
09:57 - say about assumptions
09:58 - um this is one of those lessons we never
10:00 - seem to learn for some reason we always
10:02 - assume and we always assume we're
10:04 - assuming and then we're okay with
10:05 - assuming we assume research has nothing
10:07 - nice to say about assumptions at all
10:10 - for starters assumptions are usually
10:13 - based on incorrect and invalid
10:15 - information we often base them on those
10:18 - snapshots we take in our brain
10:20 - and those snapshots again they're static
10:22 - whereas the world has moved on so our
10:25 - snapshots are wrong our assumptions are
10:27 - wrong
10:28 - we then use those assumptions as a basis
10:30 - for a set of heuristics which we apply
10:32 - to real world situations right now
10:35 - and that's not going to work how many of
10:37 - you have tried to estimate how long it's
10:39 - going to take to
10:41 - complete a project
10:43 - how many of you are actually right
10:44 - probably none of you if i could see your
10:46 - hands and you were you had them still up
10:48 - you would be liars every one of you um
10:51 - other really popular assumptions that we
10:53 - make in technology that we probably
10:55 - should stop doing
10:57 - user needs what is our user actually
11:00 - trying to accomplish we think we know
11:02 - but we really don't and yet we assume we
11:05 - move on and we develop things
11:08 - what business are we really in this is
11:10 - something startups never seem to answer
11:12 - and they keep failing because of it they
11:15 - don't know why they're in business
11:18 - assumptions usually
11:20 - are based upon a poor problem definition
11:24 - we are so bad at this i am a freelancer
11:26 - now i work with companies to help them
11:28 - with their open source strategy and
11:29 - compliance and all that sort of stuff
11:30 - and they come to me and they say vicky
11:32 - we want to do open source and i say
11:34 - great i want to help you do open source
11:37 - why do you want to do it
11:39 - what do you want to get out of it what
11:40 - problem are you looking to solve for
11:42 - your business and almost entirely except
11:45 - for one client i'm working with right
11:46 - now who is so amazing
11:48 - they don't have an answer they don't
11:49 - know why they want to do this and this
11:51 - is common we never really know why we're
11:54 - doing stuff we have a problem we're
11:56 - looking to solve but we haven't defined
11:57 - that problem and so we're basing our
12:00 - assumption on the problem that is not
12:02 - defined
12:03 - and so if we haven't defined our problem
12:06 - and we're basing our assumptions on an
12:07 - undefined problem we're basing our
12:09 - solutions on an undefined problem and
12:11 - we're naturally going to fail because
12:13 - we're doing the wrong thing
12:14 - it is absolutely impossible in
12:16 - technology to fail fast if you don't
12:18 - define success
12:19 - you must know what you're trying to
12:20 - accomplish because if you don't define
12:23 - success you can't define failure you
12:25 - can't fail fast
12:27 - if you don't know what failure even is
12:29 - now we in tech like to say that we don't
12:31 - have time for that
12:33 - holy crap no we are totally agile we are
12:36 - just iterative and it's great you know
12:39 - we're gonna just build and move
12:41 - fast and break stuff and it's amazing
12:44 - but the thing is we don't really we we
12:46 - have the time to do that
12:49 - but we don't have time to check the
12:50 - assumptions we have the time to do that
12:54 - and to build the wrong thing for the
12:55 - wrong person then scrap it
12:57 - because we don't check our assumptions
12:59 - it's terrible
13:02 - so you do have to question your
13:04 - assumptions as you are doing this entire
13:06 - process always question them and that's
13:08 - because psychologists have found over
13:10 - and over again that unquestioned
13:12 - assumptions become facts in the minds of
13:14 - the hearers we have all seen this in
13:16 - play
13:16 - when you're sitting there in a meeting
13:18 - and the person at the top of the desk at
13:20 - the top of the table there says so how
13:22 - long is it going to take to do this
13:24 - someone else at the table raises their
13:25 - hand they say we assume this will take
13:28 - two weeks but we need to confirm
13:31 - research shows that for almost everyone
13:34 - in that room especially the person at
13:35 - the head of the table they don't hear
13:37 - anything after two weeks
13:40 - it's it's just a psychological thing we
13:42 - don't hear
13:43 - and it needs to we need to confirm
13:47 - so this is really common in all cultures
13:50 - across uh all industries but we in
13:52 - technology seem to get bitten by it a
13:54 - lot
13:57 - so
13:58 - assumptions they're horrible they're
14:00 - terrible they are in fact the root of
14:01 - all evil they're not the next one is
14:03 - actually the root of all evil what do we
14:05 - do to minimize the impact
14:07 - well
14:09 - please define your problem
14:11 - please not only define your problem but
14:13 - communicate the definition of the
14:15 - problem because otherwise you will be
14:17 - sitting in that room with all those
14:18 - people
14:19 - and you'll ask them
14:21 - what are we trying to accomplish
14:23 - if each and every person in that room
14:25 - does not have the exact same answer
14:28 - then you still have assumptions that you
14:29 - need to fix
14:31 - and communication that you need to fix
14:33 - as well
14:35 - do pre-mortems list and verify not only
14:39 - those latent errors don't just look for
14:40 - those but explicitly call out every
14:43 - single assumption
14:44 - that you are making be very obvious
14:46 - about it it's great to have assumptions
14:48 - they're shortcuts they allow us to move
14:50 - more quickly
14:51 - but be aware of your assumptions
14:54 - and once you have those write them down
14:57 - and you need to do this because you
14:59 - later need to go back and revisit them
15:01 - throughout the process
15:03 - just check in every once in a while on
15:05 - those assumptions because again you're
15:07 - working in your snapshot you need to
15:08 - update your snapshot
15:10 - to the now and when you're updating your
15:12 - snapshot to the now also go through all
15:14 - of your assumptions because some of them
15:17 - might no longer be assumptions now you
15:18 - might have knowledge
15:20 - rather than an assumption but you're not
15:22 - going to know that if you don't verify
15:27 - and this actually is the root of all
15:29 - evil
15:30 - businesses clubs projects any group of
15:33 - human beings
15:34 - every piece of research shows that human
15:37 - beings are the primary cause of failure
15:42 - surprising huh but we can't exactly get
15:44 - rid of them i highly recommend you do
15:46 - not get rid of human beings so that is
15:48 - not the solution here
15:53 - we are kind of a problematic species um
15:56 - we are fairly arrogant
15:58 - as the top of the food chain we like to
16:00 - think um ellen langer is a psychologist
16:03 - at harvard she was the first person to
16:06 - uh to publish a paper on this
16:08 - it's called illusion of control
16:11 - we as humans again arrogant we like to
16:14 - think we have a lot of control over
16:17 - things and we don't we over estimate the
16:19 - impact our actions can have
16:21 - and we overestimate even what we can
16:23 - just influence let alone control
16:26 - and because of that we ignore latent
16:29 - errors because we think we are in
16:30 - control
16:32 - again near misses remember deepwater
16:35 - horizon now if we look for latent errors
16:38 - if we're not actively ignoring them
16:40 - we can do something about latent errors
16:43 - we can stop that man from doing his
16:45 - cementing procedure incorrectly we can
16:48 - stop that man from doing the welding we
16:50 - can control these two things because we
16:52 - can see them and go
16:54 - that's not going to go well
16:56 - and we can control that
16:58 - but what we can never do is we can never
17:01 - make the wind blow we can never control
17:04 - enabling conditions and therefore we can
17:07 - never control fully any situation we're
17:10 - in we can think we can but we're wrong
17:13 - we just can't do it
17:16 - so how is your team organized your team
17:19 - your project your company how is it
17:21 - actually physically structured and
17:23 - organized you know what's your hierarchy
17:26 - and all that goodness
17:27 - because that organization can have a
17:30 - really dramatic effect on the its
17:31 - success rate
17:33 - and on its reaction to failures
17:36 - are you in silos are you in
17:38 - organizational silos who here knows what
17:40 - i mean by organizational silo
17:43 - hey not quite everyone um the other
17:44 - people are probably just doing twitter
17:46 - but um
17:48 - so organizational silos you've got this
17:50 - team here team
17:52 - c
17:53 - b and a and they're all working on their
17:55 - own little things very happily going
17:57 - along but they're not communicating
18:00 - nobody really knows what anyone else is
18:02 - doing and this is a problem it dilutes
18:04 - responsibility for failure
18:07 - let's say something goes wrong over here
18:08 - in team a
18:10 - there's a failure in team a and it could
18:12 - possibly affect teams b and c but
18:15 - there's no communication they don't tell
18:17 - them that so teams b and c are going to
18:18 - fail as well because nobody has shared
18:21 - information with them
18:23 - silos hamper communication they obscure
18:26 - information and they're a problem and
18:28 - they increase failures within an
18:30 - organization
18:31 - another problem is when your team is
18:34 - organized around processes
18:37 - and let's say you've got one team that's
18:39 - dedicated 100 to testing and this team
18:41 - is writing that team is designing and
18:43 - this team is you know deploying
18:46 - they're all around processes
18:48 - now what happens if the process is
18:51 - what's causing the failure
18:54 - what do you do then well studies show
18:56 - that we as human beings are less likely
18:59 - to change the process because it also
19:01 - might mean we have to change the
19:03 - organizational structure in some way
19:06 - and so we don't do it and we continue
19:08 - using broken processes because of it and
19:10 - that leads to a lot more failures
19:13 - now both with silos and with uh with
19:17 - these process organized things it does
19:19 - require a reorganoid to fix
19:21 - unfortunately and those are we probably
19:24 - all know reorgs are never fun
19:26 - but it will help reduce failures if you
19:28 - have multi-disciplinary teams
19:31 - because you get a lot more communication
19:33 - across teams if you have people who
19:36 - cross
19:37 - procedural boundaries within a team
19:39 - and you get a lot more people who are
19:41 - asking questions and questioning
19:43 - assumptions
19:49 - so regardless of the structure of your
19:51 - team your group your project whatever it
19:53 - is regardless of their structure how
19:55 - does it react when something fails
19:58 - does it punish people for failures
20:01 - this is pretty common for people to be
20:02 - punished for failures
20:04 - if that happens
20:06 - it's actually a bigger problem than the
20:07 - failure itself because it increases the
20:11 - the impact of the failure
20:13 - that's because if people are afraid of
20:15 - being punished for failure
20:17 - they stop reporting failures
20:20 - and if people stop reporting failures
20:23 - it is impossible for you to learn from
20:25 - failures because that is the only way to
20:27 - prevent failures is to have failures and
20:30 - learn from them
20:34 - unshared failures are just the
20:36 - experience of an individual and that's
20:38 - it and i can stand right here on this
20:41 - kind of squeaky podium thing um i can
20:43 - stand right here and i can gain
20:45 - experience
20:47 - it just happens to me
20:49 - now if i want to learn from that
20:51 - experience it takes an extra step
20:53 - that's an activity that's something i
20:55 - have to manually do the experience comes
20:58 - at me for free whether i like it or not
21:00 - it's my choice whether i learn from it
21:02 - and for failure it's the same thing we
21:05 - can't learn from failures
21:07 - if we don't share them
21:09 - you don't have the information to share
21:14 - bigger problem or another problem with
21:16 - people
21:17 - being afraid of punishment for failure
21:19 - is that they stop innovating they stop
21:22 - trying new things
21:24 - and i'm not just talking like that
21:27 - silicon valley disrupting sort of
21:29 - innovation i'm not talking about that
21:32 - talking about the kind of innovation
21:33 - that actually matters
21:34 - the innovation of streamlining a
21:36 - workflow the innovation of trying a new
21:39 - library just small everyday innovations
21:42 - that add up to a great deal of
21:45 - improvement in your life we stop doing
21:48 - it
21:48 - because we're afraid we're going to get
21:50 - punished should it fail
21:52 - so that's a big problem
21:55 - i've had people say don't worry
21:57 - we don't have that problem i never hear
21:59 - about failures at all
22:03 - really did you just hear what i said
22:05 - people oh if you're not hearing about it
22:07 - doesn't mean it's not happening just
22:09 - means they're hidden
22:11 - and that's worse than not having
22:12 - failures at all
22:18 - another common problem with humans
22:20 - and also might i add my favorite
22:23 - cognitive
22:24 - bias i do as a matter of fact have a
22:27 - favorite cognitive bias um normalization
22:29 - of deviance uh diane vaughn
22:31 - is an american sociologist
22:34 - she studied the space shuttle challenger
22:35 - explosion and she is the first person to
22:38 - write about the normalization of
22:39 - deviance yey diane
22:42 - so normalization
22:44 - getting difficult to speak up here
22:46 - normalization of deviance is when you
22:47 - have a latent error
22:50 - lying right there in the open
22:52 - and rather than picking it up and
22:54 - putting it away you just step over it
22:57 - you just avoid it you know you just
22:59 - completely ignore la la la there is no
23:01 - latent error there
23:03 - right this is a big problem um an
23:05 - example i like to give of normalized
23:07 - deviance
23:08 - is when let's say you're a new
23:10 - programmer at a a company you're tailing
23:14 - log files as one is want to do and
23:15 - you're looking at log files and this
23:16 - error keeps popping up and you're like
23:18 - holy what is this error omg and you
23:20 - run to the senior developer and you're
23:22 - like hey
23:23 - what's this error and they say don't
23:25 - worry
23:26 - it happens all the time
23:28 - we just ignore it
23:30 - um
23:31 - a as somebody who used to run software
23:33 - engineering departments i'm going to
23:34 - tell you to get that out of the log
23:36 - because it's noise that's blocking my
23:38 - signal so okay that's problem number one
23:40 - but if it's an actual legitimate problem
23:42 - if that error in your log is legit
23:46 - then i'm sorry you are one race
23:48 - condition away from catastrophe
23:50 - that's a latent error that you are
23:52 - ignoring you have normalized that
23:54 - deviance
23:56 - so normalization of deviance we do it
23:58 - all the time
24:00 - we are very familiar with
24:02 - pressure and stress in technology um we
24:06 - have this unfortunate tendency across
24:08 - all of humanity but particularly in
24:10 - technology um also medical
24:13 - we do a lot of this
24:15 - we push ourselves too hard we try doing
24:18 - too much in too little time usually for
24:20 - no good purpose at all
24:22 - and this leads to a lot of problem the
24:23 - obvious problems are fatigue driven
24:26 - errors
24:27 - read the studies in the bibliography
24:29 - about
24:30 - the impact of this on your health care
24:33 - and you will this is real this is a real
24:35 - big problem
24:37 - but it also is a problem because but
24:39 - when we're fatigued when we're tired
24:41 - when we're stressed out
24:43 - we have a tendency research shows to
24:46 - rely more on untested assumptions we
24:49 - will make wild ass guesses and run with
24:52 - them
24:52 - and that's perfectly fine we think it's
24:54 - cool it's okay i'm totally stressed i've
24:56 - got to do i've just got to move on
24:58 - but it's a problem and it leads to a lot
24:59 - more failures
25:04 - so
25:07 - remember when i mentioned that we like
25:09 - to focus on just that one thing we're
25:10 - working in serial rather than parallel
25:12 - right we also have a really hard time
25:15 - not seeing that one thing through to the
25:17 - end
25:19 - uh which is a big problem if we haven't
25:21 - bothered to define what the end is
25:23 - frankly so we're just going to keep on
25:25 - going we're never going to stop because
25:27 - we don't know what done looks like
25:29 - and that's a big problem obviously but
25:32 - then we will have the sunk cost fallacy
25:34 - where people are like we've taken so
25:36 - much time and put in so much effort we
25:38 - should just see it through
25:40 - right
25:41 - okay maybe but
25:43 - usually that's not the case
25:46 - um we also have a problem with killing
25:48 - things before they fail
25:49 - when we've got project champions
25:52 - those people who are in the group and
25:54 - they've got this reality distortion
25:56 - field you know and they're just waving
25:58 - the flag we can do it we can do it and
26:01 - they just get this group think of
26:03 - everyone involved in the project who
26:05 - they get this deeply deeply held belief
26:07 - that yeah
26:09 - we can do it yeah we can succeed even
26:12 - when all evidence is to the contrary
26:14 - people have blind faith that yes
26:17 - we can do this when there's no proof
26:20 - that they actually can and so what you
26:22 - end up with is the shambling zombie
26:24 - project that never dies
26:26 - another reason we can't pull the plug is
26:29 - we aren't taught how to communicate
26:32 - we think we're being very nice by
26:34 - allowing something to continue
26:36 - rather than telling people i'm sorry we
26:37 - have to pull the plug on your project
26:39 - there are really good easy empathetic
26:42 - ways to tell people that we're pulling
26:44 - the plug on your project you're not
26:46 - being a jerk by doing that you can say
26:48 - it in a jerky way and then yeah you are
26:50 - a jerk but we don't teach people how to
26:52 - do this properly and so
26:55 - projects go on and on and on and finally
26:58 - they fail and everyone's upset
27:01 - so what do we do about this what do we
27:02 - do about i already mentioned we can't
27:04 - get rid of humans that's not one of the
27:06 - options
27:09 - organizational changes dealing with
27:11 - people anything dealing with people is
27:13 - hard i'm in management trust me it's
27:15 - hard
27:17 - they're hard to start they're even
27:18 - harder to finish
27:20 - so keep in mind this is
27:22 - these are going to be very difficult
27:23 - things but they're usually worthwhile
27:26 - for starters
27:27 - research shows again and again and again
27:29 - any sort of organizational initiative if
27:31 - it doesn't get support from the top this
27:33 - is where grassroots does not necessarily
27:36 - work you have to get support from the
27:38 - top
27:39 - or else your organizational change will
27:41 - not succeed
27:43 - ideally your support from the top on
27:45 - this failure stopping initiative should
27:48 - be the leaders discussing their own
27:50 - failures
27:51 - their leaders
27:52 - showing what they've learned from their
27:54 - failures and showing people it's okay
27:58 - and that starts to create something that
28:00 - we call an environment of psychological
28:02 - safety
28:03 - because people see it's okay to fail
28:07 - now again experience is automatic
28:09 - learning is not
28:11 - mistakes are also not automatic
28:14 - mistakes are only mistakes because
28:16 - somebody points at it and says yo that's
28:18 - a mistake
28:19 - before that it's just a learning
28:21 - opportunity it's just experience
28:24 - now
28:25 - learning it develops intuition it
28:27 - develops skills
28:29 - how many of you out there are senior
28:31 - technical staff
28:34 - okay i can actually see these hand a
28:36 - little bit so i lied earlier um you can
28:38 - put your hands down now i can guarantee
28:40 - you didn't become senior technical staff
28:43 - because of your age
28:45 - you became senior technical staff
28:47 - because dude you've seen some man
28:52 - okay and
28:54 - i mean you've experienced failure
28:56 - but you learned from it you developed
28:59 - intuition and skill
29:01 - and you're using those past failures to
29:04 - prevent future failures
29:06 - so that's really important we need the
29:09 - failures to do this but you can't do
29:11 - that if you're not in an environment
29:13 - where it's safe to make the failure
29:17 - studies finally are coming out all about
29:20 - psychological safety
29:22 - and you can find them in the
29:23 - bibliography
29:24 - and we are showing that psychologically
29:27 - safe teams are the ones that the most
29:30 - productive they are the most innovative
29:32 - they are the most cost effective so
29:34 - aside from taking care of your team
29:36 - being just the right thing to do in a
29:38 - humane way
29:40 - if that's not good enough for you oh
29:42 - honey please it is so good for your
29:44 - bottom line
29:47 - so there's lots of good reasons to
29:48 - create psychologically safe teams
29:53 - so as you're doing your premortems your
29:55 - postmortems as you're having all these
29:57 - discussions around your project
29:59 - bring in outside people
30:01 - and not just outside from other teams
30:04 - other development teams but i'm talking
30:06 - you know bring in customer service bring
30:08 - in users bring in marketing and finance
30:11 - bring in people who don't think the same
30:13 - way you do
30:15 - and have them in your premortem have
30:16 - them in your post-mortem they're going
30:18 - to ask a lot of really interesting
30:20 - questions about things that you have
30:21 - been ignoring and glossing over
30:24 - these are project skeptics these are
30:26 - these are truth seekers and they are
30:29 - really valuable they help surface a lot
30:32 - of things that you otherwise wouldn't
30:34 - see and are latent errors or failures
30:37 - waiting to happen
30:41 - you can also make your projects just
30:43 - more survivable
30:44 - you know just if you're going to try
30:46 - stuff make it easier to try stuff first
30:50 - of all
30:51 - the easiest project to survive is when
30:53 - you don't even start
30:56 - so
30:56 - ask yourself do we really need to do
30:58 - this do we really need to reinvent the
31:01 - wheel again
31:03 - or can't we just use a library that
31:04 - someone else has used
31:06 - right so don't start them at all
31:09 - as manager i like this one build in
31:12 - checkpoints and milestones into every
31:14 - project
31:15 - and if you do funding for a project if
31:18 - you're in a big corporation that does
31:19 - that sort of funding thing um
31:22 - then only fund to a checkpoint otherwise
31:24 - just make this checkpoint a place where
31:26 - everyone in the entire organization
31:28 - knows this is a no go no-go situation if
31:32 - you're not actually moving us towards
31:34 - our goal
31:35 - at that checkpoint
31:37 - we will probably cancel you but that's
31:39 - okay
31:40 - and make sure everyone knows that's okay
31:43 - it's not a sign that you have failed in
31:45 - any way just means we're going to move
31:46 - on to something else
31:48 - it's fine
31:50 - and if you do have something that is
31:52 - potentially not very survivable if you
31:54 - have a project that is going to be a
31:56 - little more um
31:58 - complicated or involved
32:00 - spin it off get it away if you have a
32:02 - project that's going to explode
32:04 - honey get it away from the things that
32:06 - might be hit by the shrapnel
32:08 - you know because otherwise that you're
32:10 - going to have ripple effects from that
32:12 - potential failure should it actually
32:13 - fail
32:16 - all right so let's talk about some ways
32:19 - that you can work with failure
32:23 - experiments are controlled failure and
32:25 - they are amazing
32:27 - we literally as human beings would not
32:29 - be here without failure and that's
32:31 - because of evolution
32:32 - and life forms as in all other things no
32:36 - change is possible without some failure
32:38 - evolution itself is just a constant
32:41 - series of works for now solutions
32:43 - and maybe in a thousand years we'll have
32:45 - a better works for now solution it's
32:47 - just constant series of these so your
32:50 - experimentation should also be similarly
32:53 - uh continuous if you want your business
32:54 - and your project to evolve
32:57 - as you're doing your experiments
33:00 - you have something in mind we're testing
33:02 - for this we want it to look like this
33:03 - and this is your success right redefine
33:07 - that success
33:09 - don't have your success for your
33:10 - experiment be we
33:13 - got the outcome that we were hoping for
33:15 - have success for your experiment mean oh
33:17 - we learned something really good that we
33:20 - can use for future experiments
33:22 - so seek the truth first and traditional
33:25 - success second
33:29 - now experiments can be very risky
33:31 - remember the shrapnel
33:33 - right but that doesn't mean they all
33:35 - have to be risky think of experiments
33:38 - like your financial portfolio
33:40 - balance them with high risk and low risk
33:44 - because otherwise you're going to put
33:45 - your company your team your project all
33:47 - these things at jeopardy if everything
33:49 - is high risk
33:51 - also like investing
33:53 - do it for the long term
33:55 - right you don't just do that day trading
33:57 - and assume you're going to make millions
33:59 - of dollars because that's not the way
34:01 - trading works
34:03 - if you invest in the long term with your
34:05 - experiments you are experimenting
34:07 - multiple times you're not just trying
34:08 - once having it not get the outcome you
34:10 - want and then walking away done we tried
34:12 - it it's not going to work
34:15 - because that's that's not the way you
34:16 - can have successful experiments
34:19 - the nature of successful experiments
34:21 - um we we do like to say fail fast
34:24 - most of us don't really know what that
34:26 - entails but it does start with making
34:27 - things small and survivable again i
34:30 - talked about survivable with that nice
34:31 - little shrapnel metaphor
34:34 - but if we keep experiments small
34:36 - not only are they quicker to iterate
34:38 - upon
34:39 - but it also means that the outcomes are
34:42 - very very closely situated to the
34:44 - actions
34:46 - which means we can more easily determine
34:48 - cause and effect
34:52 - when you go into an experiment assume
34:54 - they're going to fail i just said assume
34:55 - i'm sorry we're not supposed to assume
34:57 - anything
34:58 - i will acknowledge that we are assuming
34:59 - here and i will confirm it later
35:01 - but expect that some of them will fail
35:04 - because if you don't then you're going
35:06 - to be very disappointed with your
35:07 - experiments
35:10 - and do have explicit success and failure
35:12 - criteria know what you're looking for
35:15 - and if you don't know that you can't
35:16 - know when you're not hitting that mark
35:19 - you can't know what failure looks like
35:21 - and to make everybody again more
35:23 - psychologically safe
35:25 - have an exit strategy
35:27 - one of the things that research finds is
35:29 - that people are unwilling to say that
35:32 - their experiment failed because they
35:34 - don't know what's going to happen to
35:35 - them personally at the end
35:37 - well my project failed so what am i
35:39 - going to get canned
35:41 - you you can come up with an exit
35:43 - strategy for these things you can make
35:44 - people feel more secure they know what's
35:46 - going to happen to them at the end and
35:48 - it's not bad
35:49 - but if you don't have an exit strategy
35:50 - people will be uncomfortable
35:53 - okay so introspection which is really
35:56 - not a great title for this slide but
35:58 - we're going to work with it
36:01 - you each and every one of you beautiful
36:02 - individuals in this this hall
36:05 - you want to get better from failure you
36:06 - want to learn from it
36:08 - the best way to do that
36:09 - is to actively start inspecting your
36:12 - failure environment
36:14 - so what sort of questions should you be
36:15 - asking
36:18 - what are you trying to accomplish if you
36:19 - can't answer that and if everyone in
36:22 - your team in your project can't answer
36:24 - that the same way you might have a
36:26 - problem
36:27 - step number one make sure you know what
36:29 - you're doing
36:30 - what's your user trying to accomplish if
36:32 - you're doing this for yourself whatever
36:34 - your project is that's one thing but if
36:36 - you're doing it for some external entity
36:38 - or person make sure you don't assume you
36:41 - know the answer to this question
36:44 - what are those assumptions
36:46 - list them out be very explicit about it
36:49 - because you want to check in on them to
36:51 - make sure they still hold true or not
36:53 - true or you still need more information
36:57 - have you looked for latent errors
36:59 - probably not because most people don't
37:04 - what does failure look like
37:07 - most people
37:08 - if you think of this at all you think of
37:10 - it in the success case but it's not
37:12 - always obvious it's not simply black and
37:15 - white that failure is the opposite of
37:17 - what you had defined as success so be
37:19 - very explicit what failure looks like
37:23 - and then be very clear with people
37:24 - what's going to happen should their
37:26 - experiment fail
37:28 - because then they will feel more safe
37:30 - even trying because there's that giant
37:32 - question mark hanging over their heads
37:34 - do you have a post-mortem process do you
37:36 - have a pre-motor process figure that out
37:38 - in advance so everybody knows there is a
37:40 - preset expectation that you will be
37:42 - doing this
37:44 - how does your organization treat failure
37:46 - if it punishes failure you can try to
37:48 - change your organization but frankly
37:50 - between you me and the wall get out
37:55 - because there are some things that you
37:57 - just can't change on your own
37:59 - and if your organization is pointing
38:01 - fingers and blaming people and punishing
38:03 - for failure that's one of the things
38:04 - that's going to be very difficult for
38:06 - you to change
38:07 - as is an a culture that doesn't provide
38:09 - psychological safety
38:11 - if you're not in a psychologically safe
38:14 - environment if you don't feel you have
38:16 - the freedom and the support to
38:18 - experiment and to fail
38:20 - then you might need to find a job that
38:22 - will allow you to do that
38:24 - now the answers to all of these
38:26 - questions
38:27 - they're going to be different for each
38:28 - and every project each and every team
38:30 - each and every company right and so
38:32 - therefore the solutions are also going
38:34 - to be different for each and every
38:35 - project and team and company and that's
38:37 - why there is no silver bullet here
38:39 - that's why i can't just hand you this
38:41 - happy little serum that's going to make
38:43 - all failure go away forever it's
38:45 - actually really hard to do this properly
38:47 - you do have to think everything through
38:50 - so please just take the time
38:52 - ask these questions answer these
38:54 - questions and communicate your failures
38:57 - across the entire
38:58 - organization for those of us those of
39:01 - you who have been snapping pictures all
39:02 - this of all the slides you're going to
39:04 - be very disappointed to learn they're
39:05 - already available online
39:08 - so they're here at internet archive
39:12 - also here is a link to that bibliography
39:14 - i mentioned my contact information
39:16 - twitter handle at the top free note irc
39:18 - nick in the middle and my email address
39:22 - um and of course my book i will leave
39:24 - this up while we do q a so are there any
39:26 - questions but not comments disguised as
39:29 - questions
39:33 - please come on up to the microphone
39:36 - it looks like we have three and a half
39:38 - minutes
39:41 - maybe four minutes
39:53 - uh thank you for the talk thank you uh
39:55 - you're welcome er
39:57 - oops
39:58 - live demo a lot of these complexity
39:59 - analyses are always concerning about
40:01 - what happens when things go wrong with
40:03 - complexity and i feel like the other
40:05 - half of the coin is
40:06 - can complexity work for us like are
40:08 - there anything like latent successes and
40:11 - i think a lot of people would call this
40:12 - luck but i'm curious if anything in the
40:14 - research has talked about latent
40:16 - successes or how interacting variables
40:19 - might contribute to our success
40:21 - they they do talk a lot about variables
40:23 - um very few of them
40:25 - think of them or discuss them in context
40:27 - of latent successes
40:30 - because nobody's interested in that
40:31 - frankly we're all very interested in the
40:33 - failures because we love watching a
40:35 - train wreck
40:37 - it's just one of those things we do in
40:38 - humanity right we we love clicking
40:40 - through on those horrible click bait you
40:42 - know
40:43 - things that we see on buzzfeed so no
40:45 - that's it's actually a really good
40:46 - perspective though which is why um
40:49 - i recommend that in your post-mortem you
40:52 - also look at the successes
40:54 - right because those are very telling as
40:55 - well they can tell you a lot about a
40:58 - what you did wrong but also they can
40:59 - help find the latent errors
41:02 - um
41:04 - next
41:05 - hi um for those of us who are not at the
41:08 - top or close to the top
41:09 - what tips if any of you have for trying
41:11 - to convince upwards to adopt things like
41:14 - this or how do we be more
41:16 - helpful towards
41:17 - leaders who do want to adopt things
41:20 - ideas like this
41:21 - you see this is really weird because i'm
41:23 - used to repeating the questions and i
41:24 - don't have to
41:25 - so uh so how do you manage up
41:27 - essentially yeah uh it depends
41:30 - i don't know the people you're working
41:32 - with and the answer to how do i manage
41:34 - up or how do i manage it all
41:36 - is one of ethical manipulation
41:40 - and
41:41 - in order to ethically manipulate you
41:42 - have to know which buttons to push and
41:44 - those buttons are different for everyone
41:46 - but it you do have to understand what
41:48 - they value
41:49 - right uh do they value
41:52 - certain things like you know success
41:55 - probably you know and and so you can
41:57 - couch things in a term in terms
42:00 - of things that will push their buttons
42:01 - that will help them understand this
42:03 - isn't just for us this is also going to
42:05 - help you
42:06 - and help show them how this is going to
42:09 - support everyone in the group but it can
42:12 - often help to
42:13 - um lead by example here and rather than
42:16 - going up first go lateral
42:19 - and work within your team
42:21 - and have your team be this shining
42:23 - example of collaboration and
42:24 - communication and you know pre-mortems
42:27 - and post-mortems and documentation oh
42:29 - for love a dog please doc everything um
42:32 - you know and that can help then become
42:35 - your pilot project
42:37 - and you can point to it it's very
42:39 - helpful to point to something and say
42:40 - look this is successful
42:42 - can you help me scale this up and
42:44 - they're more likely to listen to you at
42:46 - that point cool thank you you're welcome
42:49 - hello hi thanks
42:52 - um so you mentioned spin it off for a
42:54 - high risk project um
42:56 - that kind of felt like just basically
42:58 - delegating off like people who are going
43:00 - to die in an explosion to die what can
43:03 - you say a little more about that
43:05 - um so
43:06 - this often is done with companies as
43:09 - they are uh
43:11 - they want to do some sort of skunk works
43:13 - type thing
43:14 - and it's not going to work in their
43:16 - existing organization because it's going
43:18 - to make other people jealous that
43:20 - they're not working on the skunk works
43:22 - it might actually impact their work on
43:25 - other products you know and so they will
43:27 - spin it off into another company to
43:28 - another team into a lab
43:31 - and they will spin it off like that now
43:33 - while i do say explosion and shrapnel
43:36 - that does imply that i assume which we
43:39 - shouldn't do i assume it's going to fail
43:42 - and it might not
43:43 - so i'm not necessarily sending people
43:45 - off to their deaths
43:47 - because it could be a very successful
43:49 - situation and often spinning these
43:52 - projects out in that way gives people a
43:54 - lot more freedom than they would
43:56 - otherwise so they can experiment more
43:58 - and do things that they wouldn't
44:00 - otherwise be able to do in the typical
44:02 - structure of the group they were in
44:04 - so there it does give you a lot more
44:06 - freedom in that way and again have an
44:09 - exit strategy
44:11 - let people know what happens if this
44:14 - fails
44:15 - what happens if we can't make this
44:16 - experiment work
44:18 - right we're going to bring you all back
44:19 - in you're going to work on this we're
44:20 - going to move you to another lab
44:22 - so people know that they're not just
44:23 - going to get cut
44:25 - okay thank you welcome
44:28 - we are have 40 minutes or 40 seconds if
44:31 - there's any more questions otherwise
44:36 - great thank you all thank you