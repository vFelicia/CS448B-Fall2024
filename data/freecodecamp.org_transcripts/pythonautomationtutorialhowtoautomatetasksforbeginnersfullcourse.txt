00:00 - In this course, you will learn how to automate 
a bunch of different things in Python. You will  
00:04 - learn about web scraping, automating downloads, 
extracting from PDFs, automated image processing,  
00:11 - building an automated new summarizer and more. 
The instructor for this course is Abdul, also  
00:17 - known as one little coder. He has been creating 
courses for a while and is a great teacher.  
00:22 - Welcome to the section one of hands on with tools 
to automate stuff in Python. In this section,  
00:30 - we'll build Hacker News Headlines, emailer will 
begin with the understanding of the basics of web  
00:36 - scraping. Then we'll set up a system environment 
by installing the required Python packages.  
00:42 - Then we'll move on to understand the project 
architecture. And then we'll start scraping  
00:46 - project with Hacker News front page. And then 
finally, we'll complete the email section  
00:52 - to finish the tool building so the tool 
can send us the Hacker News Headlines.  
00:59 - In this video, we'll learn about project 
architecture of building an automated Hacker  
01:05 - News Headlines email. This architecture starts 
with getting the content of the Hacker News  
01:12 - website front page, so we'll use the Python 
package request to get the get request  
01:19 - to extract the content of the website. Once 
we have the content of the website in place,  
01:24 - we'll use a Python package beautifulsoup to scrape 
the recorded content. So the required content,  
01:30 - I mean, the components like title, link, score, 
domain name, etc. So we'll use beautifulsoup. To  
01:39 - extract these required components from the content 
that we had extracted from the previous step.  
01:46 - The next step is to build the email body or the 
content of the email. From the scraped content.  
01:53 - We'll arrange it in such a way that the email 
body looks like there is a title there is a link,  
01:58 - there is a number so that the email body looks 
exactly like a news presentation. And once we  
02:04 - have the email body ready, we'll move on to the 
email authentication section, where we will use  
02:10 - your Gmail ID. to authenticate the email section, 
we'll use the Python package SMTP lib to set up  
02:19 - an SMTP library authentication system where we'll 
use a gmail authentication once the authentication  
02:26 - is set up. And then we have provided or email id 
and then other necessary information. Finally,  
02:32 - we are going to send that email using the email 
body that we have set up. So ultimately, what  
02:37 - we are doing is we are extracting content from 
the web page. And then we are going to take the  
02:42 - required components and then use the component to 
build an email body and then use the email body in  
02:48 - the email that we compose and then we sent it to 
the required users. In the next video, we'll start  
02:55 - to learn how to set up our Python environment in 
such a way that we have all the required packages.  
03:02 - In this video, we'll learn how to set up 
our Python environment in such a way that  
03:07 - we have all the required packages. These are 
the following packages that we will be using  
03:12 - in this particular project. Request package 
is used for HTTP requests. beautifulsoup is  
03:20 - used for web scraping. SMTP lib is used for 
email authentication and email transaction,  
03:26 - email dot mine is used for creating the email 
body. And then finally date time is used for  
03:33 - accessing or manipulating date and time. But these 
packages SMTP lib email dot mine on day 10 comes  
03:40 - by default with your Python installation. So what 
we have to do is there are two external libraries  
03:46 - request package and beautifulsoup that we are 
supposed to install in our Python environment.  
03:51 - So let us go ahead and see how to install 
those two packages that are required. First,  
03:57 - open your terminal and then make sure that you 
have Python installed already. Once you have  
04:03 - Python installed already, you can start with 
PIP three install requests. Once you enter it,  
04:11 - Python is going to look up for requests and then 
it is going to get a request package from pi pi  
04:19 - which is the repository where all the packages 
are available and then it is going to install in  
04:23 - your machine. To check if request packages 
installed. Let us invoke our Python three  
04:28 - console and then check input requests. As you 
can see it has successfully imported so let us  
04:35 - exit and now move ahead with the next package. The 
next package that we are supposed to installers,  
04:41 - Pip three install Beautiful Soup for now again 
Beautiful Soup is getting downloaded from pi pi  
04:51 - and then Beautiful Soup is getting installed. So 
as we can see Beautiful Soup is installed now, let  
04:56 - us go ahead and invoke our Python three console 
and then To check if beautifulsoup is installed  
05:02 - As you can see Beautiful Soup even though is the 
package name while you are importing the package  
05:07 - in your Python session, you have to use bs for 
as you can see bs four is successfully installed.  
05:12 - You can even check if a particular object is 
getting imported bs from VS code input beautiful.  
05:19 - So, as you can see B is capital here and s also 
also capital. So it has successfully imported.  
05:25 - So let us exit our Python environment. For now 
we have successfully installed beautifulsoup for  
05:33 - and then request package. Both are the 
external libraries that are required for  
05:38 - this particular project. And all other packages 
like SMTP lib, email, dot mime and date time  
05:43 - are inbuilt present in our Python setup. 
But let us make sure that those packages  
05:48 - are available also. So let us invoke our 
Python console once again, Python three.  
05:56 - Let us clear our terminal 
First, open Python three,  
06:00 - and then do import SMTP lib SMTP 
lib is successfully imported.  
06:12 - Let us input email.my email that mine was 
successfully imported. Let us input date,  
06:13 - time, date time is also successfully imported. So 
this tells us that all the required packages are  
06:19 - available in the Python environment that we have 
got. So we are good to go ahead with our project.  
06:29 - In this video, they start to learn how to code 
the project script. For this code editing,  
06:36 - I'm using Python Community Edition. 
But you can use any ID of your choice.  
06:43 - So make sure that Python installation is proper 
in your machine and Python is added in the system  
06:49 - path, then you can use any code editor to do the 
same thing that I'm going to show you right now.  
06:56 - We'll start with importing all 
the packages that are required.  
06:59 - And if you remember, those are 
the packages that we installed.  
07:03 - In the previous section. I will start with import 
requests, which is for our HTTP request. And then  
07:11 - we'll import Beautiful Soup from bs for Next, 
we'll move on to importing SMTP lib. And then  
07:20 - we'll input two objects from email mime. 
And then finally we'll import date time.  
07:28 - Once all the inputs are finished, we'll start 
with extracting the current date time, which  
07:35 - is the system date time. The reason we are using 
date time to extract the current date time is to  
07:43 - create an email subject line where it will show us 
the appropriate date when the email was sent. This  
07:50 - is for us to make sure that the same email get 
doesn't get overwritten every day. So that we have  
07:57 - an understanding that every day we are receiving 
a new email from the automated email. So the next  
08:03 - step is to create an empty Python object with 
nothing, it's a string object with nothing in it,  
08:09 - which is going to be used as an email content 
placeholder. Once we are ready with this thing,  
08:14 - we can move on to start creating a new 
function where we will extract the Hacker News  
08:21 - components that we are required. First, let us 
create a function called extract underscore news,  
08:28 - which will take one argument which is a URL that 
it is required to keep the user updated will print  
08:35 - the user message saying extracting Hacker News 
stories. Then we'll create another temporary  
08:41 - placeholder, which is again an empty string. So 
this temporary placeholder is going to be used to  
08:48 - assign value to this content, which is the actual 
email body that we want. So the first line that we  
08:54 - want in our email body is to say that this is a 10 
top stories, it N stands for Hacker News. And then  
09:01 - we are going to display a 10 top stories and it 
is going to be a bold text. And then we are going  
09:08 - to have line breaks and then we are going to show 
star star star star just to make it more readable  
09:15 - once this line is defined. Now we will move on to 
get the content of the URL, the URL that we are  
09:22 - going to pass to this function when we call this 
function. So we are going to use request packets  
09:28 - get function to get the content of this URL and 
then store it in the response object. The content  
09:35 - once we get from the get function is actually in 
response body HTTP response, which will contain  
09:44 - content the actual content that is required for 
us which is the content of the webpage. So we  
09:50 - are going to use the method content on the object 
response to store the actual content in content.  
09:56 - Remember, this is a global object which is 
called And this is a local object within this  
10:02 - particular function, which means these two are 
different. So the scope of this particular content  
10:08 - lies only within this function. So do not get 
confused with this content and this content.  
10:14 - Now, we are going to use this content that we 
extracted using the response body. And we are  
10:20 - going to use the HTML parser to extract or make 
a soup out of it. from that particular soup,  
10:27 - what we are going to be interested is about the 
components that we are going to require in this  
10:34 - particular project. So, to understand what are 
the components that we are going to required,  
10:39 - we have to see the website structure. So, in the 
next video, we will see the website structure of  
10:45 - the Hacker News front page to see what are 
the components that we would be required  
10:50 - to extract using this beautifulsoup function? 
In this video, we'll see what are the components  
10:59 - that we need from the Hacker News front page 
website. And as you can see from this code,  
11:05 - this is the URL that we are going to use. And 
this is the Hacker News front page URL. So let  
11:11 - us go to our browser. So in this case, I'm using 
Mozilla Firefox. So let us go to our browser,  
11:16 - and then open Hacker News front page. And this 
is how the site actually looks. This is one of  
11:21 - the most popular websites on the internet. And 
this was started by Paul Graham was a very famous  
11:27 - personality, and internet entrepreneur. And he 
also runs an incubator called Y Combinator. So  
11:34 - this is a website that has been read by 1000s 
and 1000s of people every day. And our objective  
11:39 - is to extract this content and automatically send 
an email to us so that we can see why only when  
11:45 - there is an important or interesting content 
for us to see, then we can go to the website,  
11:50 - that is the objective of this entire project. 
So as we can see, this website, we can see  
11:55 - a header in this website or a navigation bar. 
And then we have list, which is very similar to  
12:01 - how, you know Reddit kind of website look. So 
in this setup, there are some components that  
12:06 - we could be interested in to understand what is 
the link? Or what is the information about First,  
12:12 - we need this one, which is the actual title 
of the link, then we would be also interested  
12:17 - in knowing the points which shows how popular 
that particular link is. So to know what are  
12:23 - the contents that we should be escaping from this 
website, we have to first open our web inspector.  
12:30 - To open web Inspector, you can 
either press F 12 in your keyboard,  
12:35 - or you can right click in your machine, the 
browser using your mouse and then click inspect  
12:41 - element. So click Inspect Element, then you will 
get this particular page opened, which will give  
12:47 - you a sense of how the web page is designed 
to let us slightly increase the size of this.  
12:54 - And let us pick the pick an element tool. So now 
we have picked up this tool to understand what are  
13:02 - the components that we need from this particular 
Hacker News front page, which we will use in the  
13:08 - code to extract that particular component from the 
content that we have already extracted. So pick  
13:14 - this thing and then go here, as you can see this, 
as you hover, you can actually see the CSS value,  
13:21 - which is also called the selector. As you can see, 
moreover, you can see a dot story link here you  
13:28 - can see a dot h and user a dot some score value, 
then you can actually see a then you can see a  
13:35 - so what we can see is we can click mouse and then 
see how we don't have to actually click we can  
13:42 - just hover around and we can actually see how the 
chord changes and then the first area of interest  
13:47 - for us was the title. So who are your most of them 
and click the button. As you can see, once you  
13:56 - hover there what you actually get is you get an A 
anchor text with the class story link and which is  
14:04 - present inside a table with class title. So, 
the first point that we are looking for is  
14:12 - anchor text with story link. So let us go to our 
code and see what we have written. So what is our  
14:19 - area of interest is we are trying to extract 
everywhere where we have got db, db is in HTML  
14:29 - is the actual sale inside a table HTML table HTML 
table is created using this tag table. A table row  
14:39 - is created using this tag BR and then the values 
the actual sales inside the table row is using  
14:45 - created using the tag the D in this step what 
we are trying to do is we are trying to tell  
14:52 - beautiful su to find all TD from this particular 
soup that we have just created. de su. So, we are  
15:01 - using the function find all to find everything 
that is TD, but as you can see, we are trying  
15:07 - to find everywhere where there is TD with class 
title. So, as you can see, we are trying to find  
15:14 - everywhere there there is class title and then 
we are trying to extract the components of it  
15:21 - everywhere there is class title and then 
we are trying to extract the components  
15:25 - or the values inside it that is exactly 
what we're doing in this function  
15:30 - soup dot find all DD and then the attributes 
the HTML attributes that we are looking for this  
15:36 - class title and then we align we align 
is something that you can actually see,  
15:44 - this is what we are trying to find out from this 
particular webpage. So, we are trying to find  
15:50 - everything that is TD and we are trying to 
extract that using this attribute class title  
15:58 - and then we are saying we align should be empty 
the first attribute class should have value title,  
16:04 - the second attribute we align should have 
nothing in it. So, just let us see it once again.  
16:09 - Once you click the inspector tool, and then 
however on the page, you click on the title  
16:15 - that you want, and you can actually see the 
title The TD plus attribute holds the value title  
16:22 - and then the we align like this one is not present 
yet. So, this is to eliminate the junk and then  
16:29 - extract the component that we want. So, once we do 
that, we are trying to find wherever we are trying  
16:36 - to convert the entire thing as a text. So, to see 
that we have to understand one more thing, what  
16:42 - we are actually seeing is this is okay. So, we are 
extracting TD, which has class title, but that is  
16:49 - only for the first element, but what we need is 
we need all the links, we need all the 30 links  
16:56 - from this particular page for us to to do that, we 
are trying to put this entire thing in a for loop  
17:02 - and then we are using this function enumerate 
just for one simple purpose because in the final  
17:07 - email that we want, we want numbers 123456 until 
30. So, for that purpose, we are trying to use  
17:16 - enumerate which will give us the actual number the 
index value and also the value of this output. So  
17:23 - we are using a for loop and we are using a 
numerate to say okay, I want all the values  
17:30 - that is output of this thing, which will give 
us all the text all the scraped output and  
17:35 - then we are saying enumerated so that we have 
the index value and we also have the actual  
17:40 - tag that is extracted from this page. And once 
we do that, we are entering into the for loop  
17:46 - trying to build the email content actual content. 
So as we just discussed, we are trying to  
17:52 - create the table row number with this value I and 
as you can see, Python is a zero index language.  
18:00 - So what we are trying to do is we are trying 
to say okay, I plus one which will give us  
18:05 - one for the first row and then two three 
until 30 so on and then we are going to  
18:11 - we are trying to actually have a nice looking 
format just like this, which will separate the  
18:16 - index number from the actual title and then we are 
trying to convert everything. What we are trying  
18:22 - to do is we are trying to convert everything that 
we just extracted using dot txt into a text. So we  
18:28 - are trying to say okay, you have given me the 
tag, but I don't want the entire day, I just  
18:33 - want that text inside the tag. So the value inside 
the tag. So we are using tag dot txt to do that.  
18:40 - And then now we need a line break. So we are going 
to use br which is an HTML tag for the line break.  
18:45 - And then one more thing is required here, which 
is if you can notice in this particular page,  
18:50 - you have got all the title letters again open our 
whip inspector and you have got all the title. But  
18:57 - at the end of the page, you actually have another 
title which is TD class title, we aligned nothing,  
19:03 - but this value is more. So in order to avoid this 
more getting captured in our final output email  
19:10 - body, we are trying to eliminate this saying 
that we want everything one to 30 except when  
19:17 - there is a value more that is what exactly we 
are doing. We are saying we want everything  
19:23 - except when the tag is not equal to more. We 
are saying give me everything when the tag is  
19:30 - not equal to more. And then we are concatenating 
it for every row. So this for loop is executing  
19:36 - for every row and then every row value is getting 
added in the CNT. And at the end of this function,  
19:42 - we are returning this object Python object that 
we created, which was an empty placeholder string,  
19:48 - as CMT. To recap, this particular 
function, this function is to extract  
19:55 - the front page links or title or the components 
that we wanted So we are creating a function  
20:01 - called extract underscore news, where we are 
passing on the URL. And then we are creating a  
20:06 - nice title, which says agent top stories, 
and then we are extracting the content,  
20:11 - we are using beautifulsoup to extract make a soup 
out of it. And then we are using soup dot find all  
20:18 - to find all the teeny tags with attributes class 
title, and then we align nothing. And then we are  
20:25 - trying to create rows using this thing. And while 
we are creating rows, we also noticed that there  
20:31 - is one final row which has a value more which we 
do not want. So we are excluding that. And then  
20:37 - finally we are returning the entire object, TNT as 
the result of this function. In the next section,  
20:44 - we'll see how to call the function, and 
then we'll move on to composing the email.  
20:50 - In the previous video, we learned how 
to build a custom function that we use  
20:55 - for extracting the news from Hacker News friend 
page. In this video, we'll see how to call it  
21:00 - function, how to finish the email content, and 
then how to start with email authentication.  
21:05 - So to start with, we can see the function that we 
tried to build in the previous section is called  
21:11 - extract underscore news. That takes one argument 
which is a URL to invoke the function or to call  
21:17 - the function, we are going to say extract 
underscore news. And then we are going to  
21:21 - pass on the URL of that Hacker News friend 
page as a string, once we do this thing,  
21:28 - this function gets executed and then whatever 
is returned in C and D will get assigned to this  
21:33 - particular CNT. And as we saw in the previous 
section, this C and T is a local object whose  
21:39 - scope is within this function. And this C and 
T is part of a global object, which has scope  
21:45 - in the entire code. Once we have this end, what we 
are going to do is we are going to append this end  
21:52 - to the content placeholder that we created. So we 
are saying just content plus equal to which is as  
21:59 - equal and as content is equal to content plus. So 
instead of this thing, we are going to simply say  
22:05 - content CNT plus equal to c NT at the end of 
the email body, then we are trying to put empty  
22:12 - lines with dashes to denote that the email is 
finished. And then finally, we are going to  
22:17 - add two more lines and then say this is the 
end of the message. This is just for us to  
22:21 - make the email more professional, more useful in 
understanding where the email starts with this  
22:27 - thing and where the email actually ends. Once we 
have this finished, what we are going to now do  
22:33 - is we are going to start with the email composing 
state. And then the first step of email composing  
22:39 - is to create the parameters that is required for 
email authentication, as we saw in the project  
22:46 - architecture section, so once the email 
composition, the email body is ready, we  
22:51 - are going to start with the email authentication. 
For email authentication, there are five important  
22:57 - parameters that we have to define first, what 
is the email server, the SMTP server that you  
23:02 - are going to use? Second, what is the port 
number? Third, what is the from address email  
23:08 - address where you want to send the email? What 
is the address where you want to send the email,  
23:14 - and then finally, the password of the address from 
address from where you want to send that email.  
23:20 - One thing that you have to keep in mind is this 
to address could be actually a list where you  
23:25 - want to send this email to multiple recipients. 
So in this particular project, we will see how to  
23:30 - send this email to yourself so that you can keep 
yourself updated with Hacker News Headlines every  
23:36 - day. But actually, you can even send this email 
to multiple person provided that you give a list  
23:43 - of email ids list. When I mean list, it's actually 
a Python list. So to start with, we are going to  
23:49 - use a Gmail account for this particular step. So 
we are going to stay SMTP gmail.com, which is the  
23:57 - SMTP email server for Gmail. And for Gmail, the 
port number is 587. And then the next thing is  
24:04 - the from email ID, which should be in character 
as a string. So for the sake of this particular  
24:12 - project, I'm going to use my gmail account from 
where I want to send my email. And then also,  
24:18 - this is the same email ID for which I want to send 
this email to remember, the to address could be  
24:25 - a list of email ids, where we have multiple email 
ids so that this one email could be sent to a lot  
24:31 - of people. And then finally we have the password 
that we are going to use for this email account.  
24:38 - So you enter the password that is required to log 
into this account. And then this will complete the  
24:45 - parameters that we wanted. Once we have this in 
place. The next thing is we are going to create  
24:52 - the message body. So the message body that we want 
is a mime multipart. So we are creating an empty  
24:58 - object with using function mind multipad. And 
then we have to add the subsequent components  
25:04 - of an email, an email is supposed to have an 
important thing, which is called an email subject.  
25:09 - So to create an email subject, there are multiple 
things that we can actually do. The first nyovest  
25:15 - thing that we can actually do is we can have a 
title, that doesn't change. But the disadvantage  
25:21 - with that is, in an email client, like Gmail, 
or outlook, if you have the same title, every  
25:26 - email that comes next day gets folded in the same 
conversation, instead of having a different email,  
25:32 - the same email will be there, and then subsequent 
emails will be added as a conversation. So to  
25:38 - avoid that, and also for us to understand when did 
we see that particular email, what we are going  
25:45 - to do is, we are going to create a dynamic email 
subject and the way we are going to do that is, as  
25:50 - you can remember from the previous videos, we had 
created a new object Python object that called now  
25:57 - from the date time package, which will return the 
system date the current date. So what we are doing  
26:03 - here is we are saying okay, this is my emails, 
which says top news stories, hm automated email,  
26:10 - that's well and good. Next what we are trying to 
do is we are trying to actually create the date  
26:14 - object, str now dot they will give you the day. 
The next one is str node month. The next one is  
26:22 - str node here. So what we are trying to do is we 
are actually trying to create an email subject  
26:27 - line that has the date, component, date, day, and 
then your year. So once we have this thing, we are  
26:34 - going to assign it in this mime multipart that we 
created as subject. The next is the from address  
26:40 - from the next store address as to and once we 
are done with this thing, we are going to attach  
26:47 - the email body the email body that we created 
as message MSG dot attach. As you can notice,  
26:54 - here, we are trying to make this an HTML email. If 
you remember, we had used Be bold as HTML tags to  
27:01 - make our email look slightly more better than a 
normal text email. And that is why we are using  
27:09 - mime text should be content HTML, and then we are 
attaching that content to the email. So with this,  
27:15 - our email body is currently ready. Now we are 
moving on to the authentication section we are we  
27:20 - are printing the message that initializing server. 
Once we have the server components in place,  
27:26 - the next step that we are going to do is we're 
going to call SMTP function from the SMTP lib  
27:33 - package. And then we are saying okay, this is my 
server, this is my port, I'm going to send it to  
27:37 - server and then this function set underscore 
debug level. One is to say whether we want  
27:43 - to see debug messages if the server has issue 
in connecting if the server has any problem.  
27:50 - If the authentication is not successful, do you 
want to see the error messages or not. So if you  
27:55 - do not want to see the error messages, you can set 
zero. If you want to see the error messages, you  
27:59 - can set one which will help you in debugging. Once 
we have that thing, we are going to initiate this  
28:05 - over with Hello. And then we are going to start a 
TLS connection which is a secured connection. And  
28:12 - then once that is done, we are going to log in 
from ID using the password that we have given.  
28:19 - Once the login is successful, then finally we 
are going to send the email that we have composed  
28:24 - from this ID to this ID where the message that 
we have created is sent as a string using the  
28:31 - function as underscore string. Once the message 
is sent successfully, we are going to print a user  
28:37 - message email sent. And then finally, we are going 
to quit from the server that we just initialized.  
28:44 - So we are going to initialize the server using the 
server and port detail that we just created. We  
28:49 - are going to set the debug level one to understand 
the error messages, we are going to initiate the  
28:55 - transaction with the server starting with ello 
and then starting with TLS server and then we  
29:01 - are going to log into the from and the ID email 
id and then using the password. And then finally  
29:06 - we are going to send the email from this ID to 
this ID or set of IDs that we have created here.  
29:12 - And then finally, we are going to send it as a 
message using this as underscore string function.  
29:18 - And then finally we are printing a user message 
and then we are going to quit from the server.  
29:23 - In the next video we'll see how the email actually 
looks and then how do we execute the script. In  
29:30 - the previous section, we completed the actual code 
that was required for this particular project.  
29:36 - But before we move on to executing the code, there 
is one change that you have to do if you are going  
29:41 - to use your Gmail account. If you are going to 
use your custom SMTP like your company email id,  
29:47 - or you're going to have your own email server you 
probably would not need to know this thing. But if  
29:52 - you are going to use your Gmail account to send 
an automated email, this is one mandatory step  
29:57 - that you have to do otherwise, your email would 
draw authentication error. So what is that thing.  
30:03 - So what you have to do is you have to go to 
your email account. So what you have to do is  
30:08 - you have to go to my account.google.com slash 
security. So this is what you have to open,  
30:14 - you have to go to my account.google.com slash 
security and then you could do Security tab.  
30:20 - Once you get into the Security tab, you will 
see something called less secure access,  
30:26 - you'll see less secure app access onto your 
scrawled on the top page looks like this.  
30:31 - So once you go at the end, you will see 
less secure app access to click this button.  
30:39 - Right now I have it on but for you It should be 
ideally off. So what you have to do is you have  
30:46 - to turn it on. So read this message carefully 
wants to Google is trying to tell you that you  
30:50 - are trying to enable your email sign in for less 
secure technology. And this project that we are  
30:58 - doing it is calling it as less secure technology 
because it doesn't use two factor authentication,  
31:04 - unlike your mobile phone or 
mobile app or something else.  
31:07 - So Google is trying to let you 
know that you are trying to access  
31:12 - or give access to your Gmail login for less secure 
app. And this is how exactly the message is for  
31:19 - all the email automation project that you will do. 
So that is completely fine. But make sure if you  
31:24 - have two factor authentication, this is not going 
to work properly, you have to find another way,  
31:29 - which you can find on Google forums. But for 
normal login, if you have a normal login,  
31:35 - this is what you have to do, you have to 
go to my account.google.com slash security,  
31:41 - you go click this button. And then by default, 
it will be like this for you, which is off.  
31:48 - And then you have to go here and then turn it 
on. Once you turn it on, you'll see this message.  
31:57 - And then you will have an ELO 
color. Let me refresh the page,  
32:02 - it will take slightly some time for Google to 
refresh your on to off logged on to probably  
32:10 - have to wait executing this until that. So now 
you can actually see that this is refreshed.  
32:15 - And then it is showing with an exclamatory 
Mark, which is lately a warning sign to say  
32:21 - that you have enabled your Gmail login or less 
secure apps. Once you are done with this step.  
32:29 - Now you can go ahead and then execute your 
code. And then let us open go to the Python  
32:36 - Community Edition that we were using Let us 
open the terminal. So please note, you can  
32:41 - even go to your system terminal and then do this 
thing. Or even you can use your Python terminal.  
32:47 - For the first time we'll use your Python terminal 
to see what are the error messages that we are  
32:51 - getting if you are getting some error messages. 
Or if you are not going to get any error messages,  
32:57 - then we can probably you know automate this 
entire thing using Windows Task Scheduler,  
33:02 - or a bash script, that would be just simply run 
on your terminal or shell. So to start with,  
33:08 - what we have to do is we let us see what 
are all the files that we have, these are  
33:11 - the files that we have in this thing. And this 
is the particular file that we are of interest,  
33:16 - we will say Python three, and then we are 
pasting the file name and then executing it.  
33:22 - As you can see, these are the error messages 
or user messages that we were printing.  
33:26 - And now because we had enabled debug level one, 
this message has been sent. And you can see that  
33:34 - it is showing that this is first extracting Hacker 
News stories composing email initiating server,  
33:40 - all the IP address related details and starting 
the TLS server. And then it is saying though,  
33:46 - okay, SMTP has started. And then from this email, 
you're sending it to this email. And then this is  
33:52 - your email body starting with automated email. 
And then finally you're finishing the email,  
33:57 - and then you're receiving a message that email 
sent and then the email connection is closed,  
34:03 - we'll see how to do the same thing using your 
terminal. To open the terminal that you have  
34:08 - taught with the terminal that you have like 
in my case, I'm going to open my Mac terminal.  
34:13 - And then I'm going to first navigate to 
the place where I have got the quotes.  
34:18 - Once you navigate to the folder where you 
have got the quotes, now check what are the  
34:22 - files you have got. So these are the files 
that we have got. So copy the file name.  
34:27 - Now open Python three, and then filename. You are 
executing first extracting the news composing the  
34:34 - email. And it is the same set of messages that 
we have seen. So in this video, we learned how to  
34:43 - enable the Google setting that will allow us to 
send automated emails through Gmail. And then we  
34:49 - also saw how to execute our script, both in the 
Python and also in our terminal. So the way we  
34:55 - execute is using this code, Python three which 
is to exit Python three or invoke Python three  
35:01 - console and then the file name. In the next video, 
we'll actually see how the email looks like.  
35:08 - In this video, we'll see how the email that we 
sent using the previous automated script actually  
35:14 - looks like. Let us go to our email. As you can 
see, Google has sent me a critical security alert,  
35:20 - which is just to notify that I had tried to 
enable My Account Login for less secure apps.  
35:26 - And you know, that is completely fine for us to 
see and then ignore. And then the next thing is,  
35:32 - we can see the email that we sent. As you can 
see, you have received two emails, because the  
35:37 - first email was sent using the terminal inside pi 
charm. The second email was sent using the shell,  
35:42 - the actual terminal that is in your voice, 
which is your command prompt or terminal.  
35:46 - So let us open the email with a slightly zoom 
out to see how the email actually looks like.  
35:52 - As you can see, this is the subject of the email 
where this is static, what we created. And then  
35:57 - this is the date that is the current system 
date. And then you can see a bold email title,  
36:03 - not the subject, actually mail title. And you can 
also see that email was sent from to this email.  
36:10 - And you can actually see another important thing 
that this email has fallen initiate your inbox  
36:16 - and not inside your spam box. So what happens is, 
if you do not have all the email companies that  
36:22 - mime components that we set up, your email might 
end up in your spam not in your inbox. So make  
36:27 - sure that you have got all the email components 
that we described in the code setup, right? Now  
36:33 - you can see that you have a title, then you have 
all the formatting that we did, then you have the  
36:38 - number that we used using the enumerate index i 
and then we have this formatting the separator,  
36:45 - and then we have the title. And then finally, 
we have the domain name with the link. In fact,  
36:51 - as you can see, we have 30. And then finally, we 
have this thing, and then we finally say end of  
36:56 - message. Let us open the next email also, to see 
Okay, the first one is this. The second one is  
37:03 - this. And then we have all the 30 items. And then 
finally, we are seeing it is in definisi latest  
37:08 - ones, go to the Hacker News site, and then verify 
what is it. As you can see, bullshitters is the  
37:14 - first we'll show this is the first out to setup 
setting up an ad block, putting up an ad block.  
37:22 - There's no third probably it has changed by the 
time we send an email. Now how do I hide from a  
37:27 - surveillance? How do I hide from a surveillance, 
this makes sure that the email that we sent  
37:33 - is the actual Hacker News for this time during 
the file for 2019. At this time, 25 for 2019.  
37:42 - So in this video, we saw how the actual email 
that we sent using the previous script looks like  
37:49 - and in this section, we have learned how to build 
an automated Hacker News headline e mailer, which  
37:57 - now further you can extend to probably a Windows 
Task Scheduler to send it every day morning,  
38:03 - or using a bash script scheduler or a cron 
job to automatically send this email even  
38:10 - without executing. So in the video, start of the 
video, the previous section end we learned how  
38:16 - to execute the script. But once you automate 
it using a task scheduler or a bash script,  
38:22 - or a cron job, then probably you would not have to 
even open or run the script every day, the script  
38:27 - would be automatically executed. There is only one 
important thing that you have to keep in your mind  
38:33 - before you know we close this section is that this 
email script will contain your email password.  
38:40 - So make sure before you upload it on GitHub or 
before you share it with your friends the same  
38:45 - script that you remove your password so that 
no one else knows your password. So in this  
38:52 - entire project, what we learned 
is, how to scrape a website,  
38:56 - how to extract the components that we want, how to 
build an email and how to automatically send that  
39:02 - email from our gmail account. Hope you enjoyed 
this section. Let us see in the next section.  
39:09 - In this section, too, we'll learn how to 
build a TED talk video downloader, we'll see  
39:16 - how to install requests package and understand 
how to use request package for HTTP requests.  
39:25 - With that, we'll build a basic script to download 
the video of a given a TED talk. And then we'll  
39:32 - store the video in our local machine. Then we'll 
generalize the code to download any TED Talk video  
39:40 - given the URL, and we'll ultimately 
package the script as a C light tool.  
39:48 - In this video, we'll see all the packages 
that we'll be using in this project.  
39:53 - The first and foremost package that we are going 
to use in this project is requests request is a  
40:00 - package that will help us get the web content. And 
the request. The name comes from HTTP requests,  
40:07 - which is how the communication between a 
server and client happens in a HTTP protocol,  
40:15 - the client sends a request to the server, and 
then the server response back since the response  
40:22 - as a result, typical HTTP request contains the 
actual request. And then the header lines, like  
40:30 - authentication, and then an optional empty message 
body. So sometimes you can have the information  
40:37 - that is required to be passed on in that message 
body, the package that we are going to use for  
40:43 - that is requests in Python. So let us see how to 
install that required package inside our computer.  
40:52 - So open your terminal. And then as you might have 
seen before, because we are using Python three,  
40:58 - we should use PIP three install request. This 
will install request package from pi pi. So now  
41:06 - as you can see, a course package is successfully 
installed. To verify that we can open our Python  
41:12 - three console, and then import request and then 
see that it has been successfully installed.  
41:19 - Thank you for listening, we'll see the next video 
about another package, which is Beautiful Soup.  
41:26 - In this video, we'll see about Beautiful Soup. 
Beautiful Soup is another important package that  
41:32 - we'll be using in this project. Beautiful Soup 
is used to extract data out of HTML and XML,  
41:40 - primarily Beautiful Soup is used for web scraping. 
So the request package that we saw in the last  
41:45 - video will give us a content from the webpage. But 
as you might have guessed, the content is an HTML  
41:51 - or XML format, in which websites are designed. So 
Beautiful Soup is the package that will give us  
41:58 - the formatted content, the extract of whatever we 
want from the HTML or XML file that we requested  
42:05 - from the request package as a get request. Let us 
see how to install Beautiful Soup package inside  
42:12 - our Python environment. Beautiful so goes by the 
name Beautiful Soup for so open your terminal  
42:21 - type PIP three, install and Beautiful Soup for 
everything in small case, once you click enter  
42:28 - Beautiful Soup will be collected from pi pi and 
then it is going to get successfully installed  
42:33 - in your local machine. To verify whether 
beautifulsoup is installed. Let us open  
42:38 - up our Python environment which is Python three, 
and then import bs four. As you might have seen,  
42:46 - the package that we installed was beautiful 
soup, but when we are going to call it we are  
42:50 - going to use beautiful bs for us Beautiful Soup. 
And especially the function that we are going to  
42:57 - use the object that we are going to use from 
Beautiful Soup is cumbias for input beautiful.  
43:07 - This is the object that we are going to use from 
the package Beautiful Soup. So as you can see,  
43:14 - and you install it, you have to call it 
beautifulsoup for when you import it you  
43:18 - have to call bs for and then from that this is a 
specific object that we are going to use inside  
43:24 - our code. Let us get into the next video where 
we'll see how to build the basic code. Thank you.  
43:31 - In this video, we'll see how to build the first 
version of the TED Talk video download it, as  
43:38 - we have seen in the previous videos, we have 
successfully installed request package and we have  
43:42 - successfully also install Beautiful Soup package. 
So let us start with the code. Let us import the  
43:48 - required packages first in the header section. 
And then let us move on to the code. The first  
43:52 - package that we are going to use this request. So 
let us import requests. And the second package is  
43:57 - beautiful. So as we saw in the Beautiful Soup 
package video, we are going to import Beautiful  
44:03 - Soup the object from the package bs four. And 
once we do that, the next package that we will be  
44:09 - requiring is our E which is for regular expression 
manipulation. So regular expression, as you might  
44:16 - have known is just to do pattern matching. And 
then finally, the package that we're going to use  
44:21 - is Sis, which is for argument parsing, which is to 
generalize the code for using multiple URLs as a  
44:29 - combined package. So let us start with the code. 
So as we have imported the required packages in  
44:35 - our header section, we'll move on to the further 
section. We'll see this exception handling  
44:40 - in the next video. Meanwhile, we'll use a URL that 
is hard coded. So here there is a URL of a TED  
44:48 - talk. And that is defined in the object URL. So 
the first step is to use request package to send  
44:56 - a get request to get the content of the URL. And 
stored in the object. So, because this is going  
45:05 - to be a long process, it is also good to Minter 
message to the user who is using this package,  
45:12 - the project that we are developing to 
indicate that the download is about to start,  
45:17 - once the request packages successfully get all 
the content from the URL of this TED Talk, the  
45:24 - next thing that we are going to do is we are going 
to use beautifulsoup. To create a soup out of the  
45:29 - content that we have got. As you can see here, 
the response of the get request is stored in our  
45:38 - which is the Python object. But here, when we are 
going to use it with beautifulsoup, we are going  
45:43 - to save our content, it is because the response 
contains a lot of things like the status code, the  
45:50 - result of the request. So in this entire response 
body, the only thing that we are interested  
45:55 - is our dot content, which is the actual content 
of the website URL that we extracted. And once  
46:02 - we are going to use beautifulsoup. To assign it 
to the soup object, the next step is to identify  
46:10 - the exact location where we have got the mp4. 
To understand that let us actually see the  
46:16 - source code of the doc page. When you open an 
Tiktok, just like this, when you press Ctrl u,  
46:25 - you will get the actual source of the project. 
The page in this page, we have to see where  
46:32 - mp4 is present. So we'll say Control F mp4. As you 
can see, this is the place where you have mp4 and  
46:41 - we are interested to extract this particular 
URL. But before that, we have to see where  
46:46 - the exact location, this mp4 is present in this 
entire page. So we'll just scroll to the first.  
46:54 - And we'll see that this entire content is present 
inside a script that starts with top page in it.  
47:00 - And that's exactly what we are going to use 
our beautifulsoup code to find within this  
47:06 - entire page. So entire content is present in soup. 
And then we are going to say inside soup find  
47:13 - everywhere where you have script. And inside that 
script using rejects regular expression we are  
47:18 - going to find for this particular word, and then 
we are going to store the result inside result.  
47:24 - And as you can see from this entire script, this 
script contains a lot of text. And the only part  
47:31 - that we are interested in is a proper mp4 file. 
So we are building a regular expression pattern to  
47:36 - say that it should start with the URL should start 
with HTTPS, and then it will contain also mp4. And  
47:44 - then we are going to assign the result in result 
underscore mp4. At this point, you might have got  
47:50 - a lot of results. So what we are going to do is 
we are going to split everything based on one  
47:56 - separator and then we are going to take the first 
output after the split as the proper mp4 URL,  
48:03 - because as you can see, with the mp4, you 
actually see medium quality, light quality,  
48:09 - high quality, so not bothering about the quality 
of the videos, we are going to just take the first  
48:15 - URL after the split. And then finally we are going 
to print a message that we are going to download  
48:21 - video from the URL. And then we have to have a 
file name. So to get the file name dynamically,  
48:27 - we are going to use the URL title or the file 
name which is also present in here. And then  
48:34 - the final step, we are again going to use GET 
request to extract the content of the URL,  
48:41 - which is currently the mp4 file. And then we 
are going to use that content to write using  
48:49 - F dot write and then we are going to save in 
the output file. So as you might have seen,  
48:54 - this is the mp4 file which should end with dot 
mp4, which we extracted from here this part  
49:00 - and then we are going to use F dot write 
to write the content of the mp4 file in  
49:06 - the output file that we wanted, which is an mp4 
file and then we are going to print a message  
49:10 - that the download process is finished. So in this 
video, we learned how to build the entire code,  
49:16 - a generic first version of the code that will 
help us in downloading the mp4 video of a TED  
49:23 - x Ted video from ted.com using requests and Bs 
for Thank you for listening. In the next video,  
49:33 - we'll see how to generalize this code 
so that it can be packaged as a CI tool  
49:38 - that anyone can pass a URL instead of hard coding 
it and then download the video. In this video,  
49:45 - we'll see how to generalize the code that we 
built using the last video for a better CLA  
49:52 - tool. So what do we mean by UCLA to a CLA tool is 
nothing but you can have a package the code that  
49:59 - we develop Have as one line in your terminal, 
and then you can get the output of that without  
50:07 - having the need of entering, you know, your PI 
charm or without having the need of editing the  
50:12 - code. So as you might have seen in the last video, 
we actually hard coded the URL, we gave the URL as  
50:17 - part of the code. But that is not going to help 
us in long term because not every time you want  
50:22 - to open a text editor, enter the URL, and then 
recompile the entire or execute the entire Python  
50:28 - code. So for that purpose, what we are going to 
do is we are going to generalize this code. And as  
50:33 - we saw in the last video, that is exactly why we 
are going to use this particular module of Python,  
50:38 - which is just so first, we need to check whether 
someone is giving a particular URL as part of the  
50:46 - execution command. So for that, we are going to 
include this exception handling module in the  
50:52 - quote, exception is nothing but unexpected error. 
So that's why we are calling it Exception Handling  
50:57 - how to handle the exception. So in this section, 
what we are doing is we are checking sis dot arg v  
51:03 - arg stands for argument that is passed with the 
code execution. So we are seeing if the length  
51:09 - of sis.rb is more than one then we take the first 
argument that is passed with the code execution  
51:16 - is we are going to pass this message sis dot 
exists. Exit message that says error, please enter  
51:23 - the TED talk you are to demonstrate the output of 
this let us go to terminal, let us take the code  
51:30 - that we saved using the previous section. And 
then we'll use Python three, and then type this  
51:36 - and see what is the error that you are getting 
the error that you are getting this error, please  
51:39 - enter the TED Talk URL. So that is exactly what is 
happening here. So we have written this exception  
51:46 - handling module that will tell us error, please 
enter the TED Talk URL, which will show up when  
51:53 - someone is just executing the name executing 
the file name without passing any argument,  
51:59 - which is what we are checking here. So to check 
how the code executes properly, let us actually  
52:04 - take a proper URL of a talk and then we'll 
see how to download this entire video as this  
52:12 - is becoming a CLA tool. Now let us save this code. 
Let us copy this video URL that we just used hard  
52:19 - coded in the previous video. So now we'll go 
to terminal we'll do the same Python three  
52:27 - a talk downloader, and then we'll paste the video 
URL, the actual data URL, and let us Press Enter.  
52:34 - And as you can see, these are the messages that 
we have given in the previous section download  
52:39 - about the start and end download has started, 
we extracted the URL name and then storing in  
52:44 - this particular name, and then it says download 
process finished. At the start of this video,  
52:49 - you might have seen that we had only four files 
in this particular video or in this particular  
52:54 - folder. But now, as we do LS you can actually 
see we have one extra file which is an mp4 file,  
53:01 - which is what we have downloaded. Let us go 
into our finder and see we had all these four  
53:06 - files. And now as part of this thing, we have 
this file also which is actually a tic toc.  
53:13 - So what we have seen is, we actually built a 
first draft code first version of the code that  
53:19 - did not have argument parsing where we hard 
coded the URL. In this video, we learned how to  
53:25 - generalize this video generalize this code, which 
will include URL as part of an argument and that  
53:33 - URL will be used to download the video and we also 
saw how to handle the exception when someone is  
53:39 - not giving the URL as part of the execution 
in the terminal. Thank you for listening.  
53:47 - In this section, we'll learn how to build a 
table extracted from the PDF file format. pdf  
53:53 - is one of the most prevalent file formats that 
we deal with in our daily life. Anyone who works  
53:59 - in data science would know that extracting table 
from PDFs is one of the most boring manual tasks  
54:06 - than one have to deal with. In this section. 
We'll start with basics of PDF file format,  
54:12 - then we'll learn how to install the required 
Python modules for extracting the PDF.  
54:18 - Then we'll actually do the coding part to 
extract table from PDF. Then finally, we'll learn  
54:24 - a bit about panda's data frame and then using 
panda's data frame to write the table that we just  
54:29 - extracted into a CSV file. Thank you. In the next 
video, we'll start with basics of PDF file format.  
54:39 - In this video, we'll learn the basics of PDF file 
format. pdf stands for portable document format,  
54:46 - which is a file format developed by Adobe in 
the 1990s. This file format was developed to  
54:53 - present documents that include text, graphics and 
images, independent of the software and hardware  
54:59 - and operating systems as seen, let us say whether 
it is Apple Mac or Microsoft Windows, a document  
55:06 - should look the same in both the operating systems 
and both the hardware, hence, PDF was developed.  
55:13 - The first version 1.0 of PDF was introduced in 
1993. pdf is based on the PostScript language,  
55:23 - each PDF file encapsulates a complete description 
of a fixed layout flat document. The way  
55:30 - that text and graphics are embedded on the PDF 
are based on the layout not with any structured  
55:36 - format. The general structure of a PDF file is 
composed of four main components header body cross  
55:44 - reference table trailer, the header contains just 
one line that identifies the version of the PDF,  
55:51 - for example, percentage PDF 1.5. This indicates 
that this PDF belongs to the version 1.5.  
56:00 - The trailer contains pointers to the cross 
reference table and two key objects contained  
56:05 - in the trailer dictionary. It ends with percentage 
percentage Evo F. To identify end of file, e o f  
56:14 - stands for end of file. The cross reference table 
contains pointers to all the objects included  
56:21 - in the PDF. It identifies how many objects 
are in the table, where the objects begin,  
56:27 - and its length in bytes. The body contains all 
the object information, for example, object  
56:35 - informations like fonts, images, words, bookmarks, 
form fields, and so on. So these objects  
56:44 - are mapped using the cross reference table and 
thus this forms the structure of the PDF. So far  
56:51 - in this video, we learned the basics of a PDF file 
format and the general structure of a PDF file.  
56:57 - In the next video, we'll learn how to install 
the required Python packages for this project.  
57:04 - In this video, we'll learn how to install the 
required Python packages for this project. So  
57:10 - we need three Python packages for this particular 
project. The first one is Jupiter, the second one  
57:16 - is Camelot. And then the third one is shebang, 
which we are going to use for data visualization.  
57:21 - So to install Jupiter notebook, we have to 
open our terminal and then use the Jupyter  
57:26 - Notebook installation command. Before that, 
let us see a bit about Jupyter Notebook.  
57:31 - Jupyter Notebook is an open source web application 
that allows you to create and share documents  
57:36 - that contain live code visualization and narrative 
text. So Jupyter Notebook is one of the most  
57:42 - preferred IDs are notebooks used in data science 
community. And the reason is Jupyter Notebook lets  
57:49 - you write code and also the narrative text in 
the form of markdown in the same file format.  
57:55 - Also Jupyter Notebook lead says upload Jupyter 
Notebooks rendered file which is a markdown on  
58:02 - web. So if you are going to maintain a web blog, 
which is markdown based, so you can export Jupyter  
58:08 - Notebooks markdown file and then upload it on web. 
Alternatively, if you want a Python file, not a  
58:13 - Jupyter Notebook, just a Python file to share 
it with your peers or automation. Then Jupyter  
58:19 - Notebook also lets you download the file that you 
have written the notebook file into.pi format,  
58:25 - so let us go ahead and then install Jupiter 
notebook. Open your shell or terminal  
58:32 - where you would be doing this 
installation if you're using Mac,  
58:36 - open your terminal and if you are using Windows, 
open your command prompt. So once you open your  
58:41 - command prompt, as we have seen in the previous 
videos, if you have Python three then you have to  
58:45 - type PIP three for installation of any Python 
package and then type install and Jupiter.  
58:52 - Once you type enter, This command will install 
Jupyter Notebook on our machine. So it seems  
58:57 - that Jupyter Notebook has been installed. So let 
us validate whether Jupyter Notebook has been  
59:01 - successfully installed. So let us type Jupyter 
Notebook Enter to invoke the Jupyter Notebook.  
59:08 - As you can see, Jupyter Notebook has been 
successfully installed. Now for us to shut  
59:12 - down this notebook. Let us go to the terminal and 
then type Ctrl C. So type Ctrl C in your keyboard  
59:21 - to shut down this Jupyter Notebook and it asks 
you whether you want to shut down press why that  
59:26 - the Jupyter Notebook has been successfully shut 
down so you can see the shutdown configuration.  
59:31 - So the next package that we would like to install 
is Camelot. Camelot is the Python package that we  
59:38 - will be using to extract tables from PDF. Camelot 
is an open source package that is available on  
59:45 - pi pi. So the same way that we install Jupiter 
notebook we can use PIP to install Camelot,  
59:51 - Camelot is the package that we have preferred 
in this project to extract table from PDFs.  
59:58 - So let us go ahead and Then install Camelot 
package. So now once again type PIP three install  
60:06 - Camelot but the thing with the Camelot packages 
instead of just typing Camelot you need to install  
60:11 - Camelot pi The reason is there is already a Python 
package in the name of Camelot which is not this  
60:17 - package. So, these package developers decided to 
put it in the name Camelot hyphen p y. So, the  
60:23 - package that we should be installing is Camelot 
dash p y even though the package name is Camelot,  
60:30 - we have installed it like this. So, let us type 
into that the package gets installed on our local  
60:35 - machine. So, we see that the package has been 
installed. So, let us verify whether Camelot has  
60:40 - been installed successfully. So, let us open our 
Python Client repple. Once we have Python Client,  
60:47 - let us try to import Camelot. So, Camelot has 
been successfully imported without any error,  
60:53 - which means Camelot has been successfully 
installed. So, let us exit the Python console.  
60:59 - And then the next package that we are interested 
in is C one c one is the package that we are going  
61:05 - to use for data visualization. So, as part of this 
project, once we extract the table from the PDF,  
61:11 - we are going to visualize it so that the data 
science workflow is completed. So seaborne is  
61:15 - the package that we will be installing. So, let 
us install c one, let us go ahead and open our  
61:22 - terminal. Let's clear the screen and then type 
PIP three, install c mon. Once you type enter,  
61:29 - c one is going to get installed. So C one 
ultimately requires matplotlib as a dependency.  
61:34 - So if you have got matplotlib already on your 
machine, so C one wouldn't install it again.  
61:39 - But if you have not got it, no problem not plot 
lib would also get installed on your machine.  
61:45 - So let us clear the terminal and then verify 
whether seaborne has been successfully installed.  
61:50 - So open Python three, input c one. As you can see 
c one has been successfully imported, which means  
61:58 - c one installation is successful. So in this 
video, we have seen that we installed the required  
62:04 - Python packages, we install three packages, 
which are Jupiter or Jupiter notebook, Camelot  
62:11 - for extracting table from PDF and then C one for 
data visualization. So in the next video, we'll  
62:17 - start with the coding of how to extract table from 
the PDF. Thank you for listening. In this video,  
62:25 - we'll learn how to extract table from a PDF 
file. Before we start with the coding part,  
62:30 - let us try to understand what other Python 
modules are available for the same purpose.  
62:36 - The first one is tabular. tabular is one of the 
most widely used PDF extraction library. tabular  
62:42 - is actually based on a Java library in the same 
name tabular. So, this one that we are talking  
62:46 - about is a Python binding for the Java library. 
The next one is PDF lumber, then PDF tables  
62:53 - and PDF table extract. So, all these libraries 
are available as an alternate for the library  
62:59 - that we have picked for this particular project. 
So even though all these libraries are available,  
63:04 - we have selected Camelot to go ahead 
with so the reason we selected Camelot is  
63:10 - because of the following reasons. The first 
main reason is you are in control. So unlike  
63:17 - other libraries and tools, which gives you a nice 
output or failed miserably, so there is no in  
63:22 - between. So either it gives you a nice output or 
it fails miserably. Camelot gives you the power to  
63:28 - tweak the table extraction with hyper parameters, 
which means if you do not get any output,  
63:33 - then you can tweak your hyper parameters to get at 
least some output show that not everything in the  
63:39 - PDF table extraction becomes manual. The reason 
is because since everything in the real world  
63:45 - is actually fuzzy, including PDF table extraction 
is also fuzzy. You need to have control over  
63:51 - the hyper parameters to tweak how you want to 
extract the table from the PDF. The second one is  
63:57 - bad tables can be discarded based on the metrics 
like accuracy and whitespace. So Camelot gives  
64:04 - you these metrics accuracy and whitespace so 
that you don't have to manually look at each  
64:08 - table to select the good table and discard the 
bad table. The next reason is, the table output  
64:15 - that you get out of Camelot is a panda's data 
frame. panda's data frame is one of the most  
64:20 - widely used Python module for data analysis 
and data science, which means the output of  
64:26 - the Camelot library could be seamlessly integrated 
into any ETL workflow or a data analysis workflow,  
64:33 - which exhaustingly uses Camelot or Python. The 
last reason is because Camelot lets you export the  
64:40 - extracted PDF table into multiple file formats, 
including JSON, XML and HTML. Let us say that the  
64:48 - table that you extracted from the PDF file 
format you wanted it to be published online,  
64:52 - which means you ultimately want HTML file 
format. So instead of sitting and hard coding,  
64:58 - HTML file format A table HTML table Camelot lets 
you export the PDF table that you just extracted  
65:05 - into a HTML file. So this way, Camelot helps you 
with being in control, discarding bad tables.  
65:13 - And then using a panda's data frame, which is 
easily seamlessly integrated with an existing  
65:19 - data analysis workflow. And then finally exporting 
the file format into a different file format.  
65:24 - So this is the reason why we pick Camelot ahead of 
the other packages that we just mentioned. So let  
65:30 - us move ahead and then learn how Camelot is going 
to help us in extracting tables from PDF. The PDF  
65:38 - from which we would like to extract data is this 
PDF. This has been downloaded from un website,  
65:44 - which is economic and Human Development indicators 
for India. So this is a fact sheet with multiple  
65:50 - tables. As you can see, you have one table here, 
you have another table here, so multiple tables  
65:55 - with multiple columns. So for this particular 
purpose, we are interested in extracting the  
66:01 - values row 2021 and 22 of this table, which is 
literacy rate. So let us go ahead and see how we,  
66:09 - we are going to extract this particular table and 
then do a little bit of data visualization with  
66:14 - this, before we move on to the actual coding, 
because this is the first time we are going to  
66:19 - use Jupyter Notebook in our course. So let us see 
a bit of overview about Jupyter Notebook. So open  
66:26 - your terminal, which is Windows command prompt 
or Mac terminal, and type Jupiter notebook. Once  
66:33 - you take Jupyter Notebook, it would internally 
evoke a server and then your Jupyter Notebook,  
66:39 - which this interface would open. And then 
for you to create a new Jupyter Notebook,  
66:44 - click here new and then type Python three, click 
this Python three. So once you enter here, this is  
66:50 - how the structure of a Jupyter Notebook would 
look like. So this is the title which you can  
66:54 - add to say, Okay, my first Jupyter Notebook. So 
once we have renamed it, this is how the Jupyter  
67:01 - Notebook would look like. So this in the Jupyter 
Notebook would be called as a cell. This cell can  
67:06 - have primarily two values. So it can have a code 
value where you can write your Python code. Or it  
67:11 - can have a markdown where you write your narrative 
text or documentation. So let's start with the  
67:16 - documentation and say, This is my first Jupyter 
Notebook. And then let's say this is a heading.  
67:24 - So once you are done with this thing, this is how 
it looks like. So now let us go ahead and write a  
67:29 - small Python code. As you all know, Python can be 
also used as a calculator, which means you can do  
67:36 - basic arithmetic operation. So let us go ahead and 
do a little bit of arithmetic operation which says  
67:41 - three into three. Once you are done with this 
code, you can press Shift Enter, like this,  
67:47 - and then the output will be displayed. Or maybe 
if you do not want to use the keyboard shortcut,  
67:52 - you can say okay, four minus three, which is 
probably one and then we'll go ahead and click  
67:58 - Run button here, which will show us output one. So 
this way, you know that you can have documentation  
68:05 - or narrative text and then code in the same 
file. And then this is the advantage of Jupyter  
68:10 - Notebook. And one of the reasons why we prefer 
Jupyter Notebook for this particular project.  
68:15 - Now, let us go ahead and start executing 
the actual code that we would like to write  
68:20 - for extracting table from the UN report that we 
just saw. So to start with, we should name the  
68:26 - Jupyter Notebook, which is a good practice. 
So in this case, we can name it extracting  
68:32 - table from PDF. So whatever you would like to name 
you can name it so I've named it extracting table  
68:36 - from PDF. The first Excel let us start with 
importing the Camelot package. In this case,  
68:43 - I'm importing the Camelot package with an alias 
which is cm which will help us easily call that  
68:48 - evoke that package. So let us go here and say 
Shift Enter. On the Jupyter Notebook successfully  
68:54 - installed, you get this thing probably let's say 
if you have made a mistake instead of Camelot,  
68:58 - we have said cam as cm. Now, you will get an error 
that this module is not found, because there is  
69:05 - no such package cam in this particular Python 
environment. So once we successfully invoke or  
69:11 - call that package, we will not get any error, but 
the package has been imported. So next step is for  
69:17 - us to see what are all the files available in the 
environment. So we see these are the files that we  
69:23 - have in that environment. So we have the PDF file 
that is available in the current folder. We have  
69:30 - the CSV and XLS, which I just executed before the 
project and I have it in place. And then we have  
69:38 - a bunch of other files. So now let us go ahead and 
then import the file. There are two ways that you  
69:44 - can read PDF one, you can read it directly from 
web like from where we have downloaded the PDF,  
69:50 - or you can read it from your local machine. So 
the first argument is you read you give the Python  
69:57 - the PDF file name the second argument In this 
flavor, which is there are two ways Camelot  
70:03 - can parse your PDF file one is called stream. 
The other one is called a dice mod. These have  
70:09 - different variety of ways to how to parse a PDF 
file. And in this particular case, we will prefer  
70:16 - lattes. And then we are explicitly telling 
Camelot that we have two pages one is page one,  
70:21 - the second one is page two. So we are going 
to use the function named read underscore PDF  
70:27 - from Camelot and then we are going to read the 
PDF file. So let us execute this shift enter. Once  
70:33 - we execute this thing, we are writing it in the 
Python object input underscore PDF. So we can see  
70:39 - that this has been successfully executed without 
any error. So now, let us see what is inside that  
70:45 - input underscore PDF. So this gives you a 
table list object with four values inside it,  
70:50 - which means there are four tables that has been 
extracted from this function from this video.  
70:56 - And this has been put inside this into input 
underscore PDF as a table file. So for us  
71:02 - to know what are the individual properties 
individual dimensions of this particular PDF  
71:08 - extraction process, we will see four n in input 
underscore PDF, so we are iterating in through  
71:15 - input underscore PDF to see what is inside it. 
So let us go ahead and execute this thing. So  
71:20 - this shows that we have four PDF table extracted 
files, which is first one is the table is with  
71:27 - the dimension four by three, which means four 
rows, three columns, second known as 15 by three,  
71:32 - the third one is 14 by four, and then the fourth 
one is 13 by three, so our area of interest is  
71:38 - the last part of the first page, as you saw in the 
previous PDF display. So we are going to say okay,  
71:44 - I want this third one. Since Python is a zero 
indexed language, we will say two input underscore  
71:50 - PDF of two. And then we are saying Do you mean 
it as a data frame. So once we write this thing,  
71:56 - this is what we get. As you can see, this is our 
area of interest, which is the literacy rate,  
72:02 - and which is the index value 1112, and 13. So 
what we are going to do now is we are going to say  
72:08 - okay, I want input underscore PDF of two, 
and then from that I want the data frame.  
72:13 - And in that give me the location leaven to 14 
and then give me three columns, which is 123.  
72:20 - So I want 123 11 1213. That is what we are 
specifying here. Once we execute this thing,  
72:27 - we can see how the different looks like. So this 
is how the extractor table looks like. So let us  
72:33 - do a little bit of table formatting. Before we 
do table formatting. We have understood that  
72:39 - this data frame that Camelot gives us as 
part of a panda's data frame. So let us  
72:45 - have a little bit of understanding of pandas. 
pandas is the data manipulation package that  
72:51 - is widely used the most widely used for Python, 
and pandas helps you read a CSV, write a CSV,  
72:58 - read an Excel write in Excel, do a little bit of 
reformatting in case if you want to, you know,  
73:03 - do a little bit of data analysis, pandas will 
help you do data preparation data pre processing.  
73:09 - So like that, what we are going to do is we 
are going to use a panda's function which is  
73:13 - reset index to drop this index value 1112 and 
three, and then come up with our own index value,  
73:20 - which is by default 0123. So let us go ahead and 
say, okay, for this data frame, dot t does reset  
73:28 - underscore index drop is equal to true and then 
we'll assign it to the data frame here. So let  
73:33 - us go ahead and execute this thing. And then let's 
see how the output looks like. Now as you can see,  
73:39 - from 11 1213, it has become 012. Now as 
you can see, there are three columns,  
73:45 - but the column names are 123, which is not 
very intuitive if you want to write a table.  
73:50 - So what we'll do is we'll manually put the table 
name. So from the table, you can you can see that  
73:55 - this is 2001 This is 2011. And these are the KPIs 
that we are interested in. So what we'll do is  
74:01 - we'll say Okay, the first column is Kp, second 
is 2,001/3 is 2011. Let us execute this thing.  
74:08 - And then let us see how the output looks like. 
So from one to three, now it has become kPa  
74:13 - to those n one and two those 11 then the next 
step is for us to do any kind of data analysis  
74:19 - with this thing, we need to convert this one which 
is actually a string into a number format. And the  
74:24 - number format that we are going to use is float 
because this is a decimal point. So from string,  
74:29 - we are going to save for 2001 and 2011. convert 
everything to float. So once we do this thing,  
74:37 - we are reassigning it to the same old data 
frame, and then the output even though it  
74:41 - looks same internally from string, it has become 
a character. The next step is for us to write  
74:48 - the output as a CSV file. So because this 
file name is already available, we'll say  
74:54 - Pac output dot CSV. So once we have written the 
CSV, we can go ahead and See using ls command  
75:01 - to see how the current working directory looks 
like. So as you can see, we have packed underscore  
75:08 - output dot CSV, which is what we just wrote using 
pandas function, which is to underscore CSV.  
75:14 - Once we are done with this thing, I would like to 
add another information that pandas is not just  
75:19 - letting you write it as a CSV, but it can help you 
write it as an Excel file. So let us go ahead and  
75:25 - then use this function to underscore Excel on this 
panda's data frame to write it as an excellent,  
75:31 - let's say, packed output Excel with the extension 
dot x LS x. And then once we execute this thing,  
75:41 - let us see how the current directory looks like. 
So in the previous setup, you had only packed  
75:46 - underscore output dot CSV. But now you can also 
see that at underscore output underscore Excel  
75:53 - dot XLS x. So this is how the Excel file looks 
like. So as we write it, so now what we can do is  
76:01 - we would like to import this in our current 
working directory in this current Python session,  
76:07 - so that we can do some data analysis. So our 
objective in this project is to read table  
76:13 - and then write that output table as a 
CSV, which we have already achieved.  
76:18 - But as a bonus, I would like to also show you 
why we would be requiring such a data frame in  
76:24 - the first place, because we want to do some data 
analysis, some data visualization from the PDF  
76:30 - file, which we cannot do it directly. So we are 
extracting the table from the PDF as a CSV as a  
76:37 - data frame, which is then we are converting it as 
a CSV. And then we are doing some data analysis  
76:42 - with this. In this case, we'll build a bar graph 
with this. So let us go ahead and call our panda's  
76:47 - data frame, which is required for us to read 
the CSV. So as we call the panda's data frame,  
76:54 - we'll say PD dot read underscore CSV. And what 
is the name of the file that we wrote. It is  
77:00 - packed underscore output dot CSV. So 
we'll replace this name with this name.  
77:05 - And then we'll say read underscore CSV. And 
then we are assigning it to a Python object  
77:09 - which is the DF two and then we'll display how 
it looks like. So once we execute this thing.  
77:15 - We can see this is how it looks like with an index 
value because we just read it, then we'll call a  
77:22 - data visualization library c one c one is one of 
the most widely used data visualization library.  
77:28 - Seaborn is actually built on matplotlib for 
better visualizations. So we'll go ahead and  
77:33 - call Seabourn with an area's SNS once we are done 
with that, executed, Seabourn is now imported.  
77:40 - So, for us to build a data visualization, we 
have to change the format the shape of how the  
77:46 - data frame looks like to a different shape. So, 
we are going to use the pandas function melt,  
77:51 - which will convert this data frame from the wide 
format to a long format. So, this is called to be  
77:58 - a wide format. Now, we are going to convert it to 
a long format. Now, let us execute this thing once  
78:03 - we execute df underscore melt is available now, 
let us see how df underscore meant is available.  
78:09 - So as you can see, this is a wide format. Now, 
this is a long format frame where 2001 and 2011  
78:17 - from being the column name that has become the row 
value and the value that we have given us here and  
78:23 - percentage. Now the DF underscore melted is 
available, let us go ahead and then make a bar  
78:29 - plot. So we are going to use acns dot bar plot for 
making a bar plot the x axis should contain the Kp  
78:36 - value, the y axis should contain the percentage 
and the Q which is the grouping variable,  
78:42 - which is 2011 and 2001. For us to compare how 
it has been different for 2011 and in 2001.  
78:50 - Let us execute this thing. As you 
can see, now it has generated a plot  
78:55 - with two bars, the blue color represent to version 
one, the orange color represents 2011. And then  
79:02 - with three KPIs that we just built, literacy 
rate male literacy rate or female literacy rate.  
79:08 - So this is how the overall literacy rate and as 
you can see the gap between 2001 and 2011 female  
79:14 - literacy rate is huge, which means there has 
been a tremendous growth between 2001 and 2011  
79:21 - in the rate of female literacy rate. So this is 
what we have understood from this project that  
79:27 - we had a raw PDF file which was unreadable 
as it is not a structured information. What  
79:34 - we have done is we have used Camelot to read the 
PDF, which is to be technical, we parse the PDF,  
79:41 - we extracted tables, specifically we extracted 
four tables. We went to the table of our interest,  
79:47 - which is index 1112 and three, and then we did a 
little bit of data pre processing using pandas.  
79:53 - Once we did the data pre processing, 
we went ahead and then we wrote  
79:58 - the data frame. into a CSV file. And we also 
experimented with writing it as an Excel file.  
80:04 - Once we were done with this Excel file, we finally 
went and did a little bit of data pre processing,  
80:11 - which is reshaping the data from a wide 
format to long format. And then finally,  
80:15 - we explored the data as a data visualization to 
understand some valuable insight from the PDF that  
80:22 - we have returned show for. In this video tutorial, 
we learned how to build a table extracted from  
80:28 - PDF. So we started with understanding the PDF 
file formats. Then we went ahead and installed  
80:34 - Camelot and Jupyter Notebook Python packages. 
Then we understood how to extract PDF table.  
80:40 - And then we saw basics of panda's data frame 
to write and read CSV. And then we went ahead  
80:47 - with seaborne to do some visualization. So at 
the end of this project, we have a successful  
80:52 - visualization, we have the output table as 
CSV and Excel. And then we also learned how  
80:58 - to extract table from any PDF. So thank you for 
listening. I'll see you in the next section.  
81:05 - In this section, we'll learn how to 
build an automated bulk resume parser.  
81:10 - Going through resumes, and extracting relevant 
information from those resumes is one of the most  
81:16 - essential tasks a manager has to go through before 
hiring new resources. In this section, we'll learn  
81:22 - how to build an automated bulk resume a parser 
that can go through multiple resumes and extract  
81:28 - relevant information from them, and convert 
them into a structured tabular format. with  
81:32 - the click of a button. We'll start the section 
by understanding different formats of resumes,  
81:38 - and marking relevant information that we would 
like to extract a brief overview of packages and  
81:45 - the installation of those packages. Then we'll 
see the basics of regular expression in Python,  
81:50 - and also the basic overview of spacey 
functions. And then we'll move on to  
81:55 - build the code to extract relevant information. 
And then finally, we'll complete the script.  
82:00 - To make it one click command level tool. 
Let us go ahead and then see the sections.  
82:07 - In this video, we'll learn the different formats 
of resumes. And then we'll mark essential  
82:13 - information that we would like to extract in this 
project. So as you can see, on my screen, I've got  
82:19 - two different types of resumes. The first one is 
a single column, which has content one by one. And  
82:27 - then the second one is a double column, which 
means in one page, they have got two columns,  
82:33 - and then the experiences and other details are 
scattered across the columns. So as you can see,  
82:40 - a resume can have me multiple types. So 
it is up to the creator of the resume,  
82:47 - essentially the one who is seeking for a job to 
have the format that he or she likes. But it is  
82:53 - essential for the recruiting manager to completely 
go through the resume to extract essential parts  
82:59 - of it. So what do we mean by essential parts? 
The first one is I would say that the name,  
83:05 - the name, to whom this resume belongs 
is the most essential part because  
83:10 - if you ever want to shortlist this resume, you 
need to understand who is that person. And then  
83:15 - the second thing is, if you ever want to shortlist 
a resume, you just do not want to know their name,  
83:21 - but also you need to be able to contact them. And 
then the two key information to contact a person,  
83:27 - one their email ID, and then the second one their 
phone number. So as you can see in this resume,  
83:33 - the name is first of all mentioned in the top 
left, but in this regime as the name is in the  
83:37 - center position. And in this resume, the email 
id is in the top right, but in this resume,  
83:43 - it is all centrally aligned. So as you can see, 
we have totally listed down three elements. The  
83:49 - first one is name, the second one is email 
id. And then the third one is a phone number.  
83:53 - These are the three information significant 
information that we would like to extract  
83:57 - from this resume. But more than this, what 
we want to do is we want to have a criteria  
84:03 - for which we want to extract this resume, 
which means for example, let us say you are  
84:07 - recruiting for a position called data scientist. 
And for a position called data scientists you  
84:13 - need to have relevant resume is who have the 
essential skills of a data scientist and that is  
84:18 - the most important information that we would like 
to see in the resume extraction project. So for  
84:23 - that purpose, we are going to extract skills from 
this resume, especially to say technical skills.  
84:29 - So the things that we are going to extract is most 
importantly the technical skills from the resume,  
84:35 - then the name, phone number, email id irrespective 
of how or where these informations are present  
84:42 - in a particular resume. We are going to extract 
this information using this particular project.  
84:47 - Before we move on, we have to also understand 
one more thing. That resume itself is a file  
84:53 - and then the file could have multiple formats. 
For example, a resume could be a simple image of  
84:58 - JPEG or PNG The resume a could be a doc exe, which 
is Microsoft Word, or the resume could be of PDF.  
85:05 - In this particular project, we are going 
to only deal with the resumes of PDF type.  
85:10 - Because once we have written a script for PDF file 
format, it is not very tough for you to convert  
85:17 - every other format into a PDF format. Let's say 
you can convert a JPEG to a PDF format, you can  
85:23 - convert a docx to a PDF format. So, that is one of 
the reasons why we have picked PDF format as one  
85:31 - condition where we will build this project upon so 
PDF is the file format that we are going to use.  
85:37 - And also that we are going to use resumes 
of different types it could be single or  
85:41 - double column. And then the information 
is that we are going to extract a skill  
85:45 - name, email id and phone number. In the next 
video, we'll see the architecture of this project,  
85:51 - and then the required Python modules 
and how to install those Python modules.  
85:56 - In this video, we'll see the architectural 
overview of this project. So in this project  
86:02 - will take three PDF files shall resume is in 
three PDF files. And we will store it in our local  
86:08 - folder. So what we are going to do is we are going 
to take one PDF file from this folder, and then  
86:16 - we'll convert this PDF into text. And then 
we'll do natural language processing and  
86:20 - pattern matching to extract relevant information 
that are required the relevant information that  
86:25 - we saw in the previous video, which are 
name, email id, phone number, and skills.  
86:31 - And then we'll use these relevant information 
to populate a structured tabular format.  
86:36 - And then finally, we'll write the output in a CSV 
format. Meanwhile, while we are doing PDF to text,  
86:43 - we also would save those PDF files as text files 
for future reference. We'll iterate this process  
86:51 - until all the files in the current 
directory which is the folder  
86:55 - are completed. Sure to repeat, we are going to 
take PDF files, convert them into PDF to text,  
87:03 - then do natural language processing and pattern 
matching. And then we are going to populate them  
87:07 - in a structured tabular format. And then we are 
going to write it in a CSV. So for this purpose,  
87:13 - these are the packages that we are going to use. 
The first one is PDF minor for PDF to text, then  
87:20 - spacey for natural language processing, then our 
E rejects package that is for pattern matching,  
87:28 - and pandas per output CSV saving. Meanwhile, we 
also would use another package called ODS. This  
87:37 - is for operating system manipulation, which 
is highly required for us to iterate through  
87:43 - multiple files and in the current working 
directory, and also to save the output files  
87:48 - in the required folder. So the packages that 
we will be using are PDF minor, spacey, r, e,  
87:56 - OAS, and pandas. So of all these five packages, 
packages, ie underwears come by default installed  
88:04 - on your Python operating system. So of all these 
five packages, packages, Ari and OAS are already  
88:13 - installed by default in your Python environment. 
So it is required for us to install PDF minor,  
88:19 - spacey, and pandas. As we just saw, of the five 
required Python packages. Two packages come by  
88:26 - default with your Python installation. Those 
are our UI, and OAS. So we'll go ahead and see  
88:34 - how to install the rest of the packages which 
is PDF minor, spacey, and pandas to start with  
88:41 - PDF minor is a package that we are going to use to 
convert a PDF into text. And the package that we  
88:48 - are going to use is called PDF minor dot six 
for that reason is because PDF minor is the  
88:55 - actual package name whose development stopped 
before python 2.7. So there was a requirement  
89:02 - to support the latest version of Python. And that 
is where this fork as you can see, this is a fork  
89:08 - of PDF minor. And this is called PDF minor dot 
six. So anytime you're going to deal with PDF,  
89:14 - and then you want the PDF to be converted into a 
text format, and then you have got Python version  
89:20 - which is latest, mostly three, then you have 
to install PDF miner dot six, not PDF miner.  
89:27 - So as we have always seen how to install a Python 
package in this project also will use a terminal  
89:34 - or shell or Command Prompt. And then we'll use 
PIP to install the required Python package.  
89:39 - So let us open our terminal or in case if 
you have got windows mission command prompt  
89:44 - and type PIP three, install PDF minor dot six. So 
as I've got already this package on my machine,  
89:53 - this is successfully shown that it has been 
satisfied. Now let us open our Python terminal  
89:58 - to see if PDF Minor has been successfully 
installed. So let us try import PDF minor.  
90:05 - You can notice this difference that when we 
install the package, it is called PDF minor dot  
90:10 - six. But when we import it it is as same as PDF 
minor. So the only reason they have got this dot  
90:15 - six is to differentiate between the older version 
of PDF minor and the newer version of PDF minor.  
90:21 - So let us type PDF, import PDF minor and then 
press enter and then you can see that it has been  
90:28 - successfully imported without any error, which 
means PDF minor has been successfully imported.  
90:33 - The next Python package that 
we would like to install is  
90:37 - spacey. spacey is the library that we are going 
to use for natural language processing. In fact,  
90:42 - spacey is one of the most popular natural 
language processing libraries in Python and it is  
90:48 - widely used in the industry for natural language 
processing. There are a lot of features of spacey  
90:53 - for example, the tokenization is very good, it has 
got a good named entity recognition, it has got a  
90:58 - very good language support like 49 languages are 
supported. It also comes with pre trained models,  
91:04 - which helps us do a lot of natural language 
processing without training our own model. So  
91:10 - let us go ahead and install spacey. So to install 
spacey, we can see that again we will use PIP and  
91:17 - then we'll do install spacey. And if you remember 
one of the things that I've told from our previous  
91:23 - sections, we use PIP three because our Python 
version is Python three in case if you have got  
91:29 - Python you have to use the so PIP three install 
spacey. So as you can see requirement is already  
91:36 - satisfied because I've got this package already. 
So it has been successfully installed. Let us  
91:41 - clear our terminal and then open Python three to 
see if spacey has been successfully installed.  
91:48 - So Python three import spacey. spacey has been 
successfully imported, let us exit. But there  
91:55 - is one more thing that we have to do with respect 
to spacey, which is download the language model.  
92:00 - So a natural language processing can work better 
only if the language model is available, which  
92:05 - usually has got all these words and then the 
part of speech the named entity recognition  
92:11 - all this stuff. So for us to download the default 
language model English language model for spacey,  
92:18 - we have to use this command, which we'll 
put in our terminal. And because we have  
92:24 - got Python three, we'll do Python three spacey 
download. So, once this is successfully done,  
92:31 - our language model is successfully installed. 
So we can see that the language model has been  
92:36 - installed. And then it is also telling us that 
the way we have loaded our language models,  
92:41 - spacey dot load the language model. So let us just 
check that once. Let us open our Python terminal  
92:50 - and then say import spacey and then copy 
this and then say okay, my NLP is equal to  
92:58 - Yeah, the language model has been successfully 
loaded, which means we have got successfully  
93:03 - installed spacey and that language model that is 
required for us to do English natural language  
93:08 - processing. And then the final library 
that we are going to install is pandas.  
93:13 - pandas is the library, the go to library for any 
tabular data manipulation, and pandas is one of  
93:20 - the most widely used library in data science for 
data manipulation. So it is much easier for us  
93:26 - to install pandas. As the name suggests, we'll 
do let's clear our terminal to three installed  
93:31 - pandas. We can see that the requirement has 
been satisfied. Let us open our Python terminal  
93:38 - unsay input pandas. pandas has been successfully 
imported without any error, which means we have  
93:45 - successfully installed the packages that 
are required for us, which is PDF minor,  
93:50 - and then spacey, and then pandas. But we also 
saw that there are other two libraries that comes  
93:57 - by default in the current Python installation 
that we have got. So let us just verify  
94:02 - whether those packages are available. So the 
first package that we saw was our E, which  
94:08 - is for regular Express manipulation. So we'll do 
import IE. Yep, that's been successfully imported.  
94:14 - And then the next package that we saw was 
was for operating system manipulation,  
94:19 - to you know, find files, pass through folders. 
So we'll do input was, and as we can see,  
94:26 - these two libraries are successfully 
imported, which means we are all set  
94:30 - with all the libraries that are required 
for us to proceed with this project.  
94:34 - And then we'll see how to code. In the next 
section. We'll see basics of regular expression  
94:42 - and natural language processing overview 
and then we'll move on to coding. Thank you.  
94:48 - In this video, we'll learn basics of 
regular expression. Regular Expression  
94:52 - could be also called as regex or red X or 
reg x. So whatever you would like to call it  
94:59 - is a sequence of characters that define a search 
pattern. That search pattern is usually the  
95:04 - combination of characters and meta characters. 
So the meta characters are like this cap,  
95:10 - dollar dot pipe or curly braces. So these meta 
characters define the syntax around characters  
95:16 - to create the search pattern, which is what we 
call as regular expression. regular expression  
95:22 - is one of the toughest programming concepts 
that is not very familiar. So there are a lot  
95:29 - of names around regular expressions on internet, 
you can just refer to it to see how tough it is.  
95:35 - So just to understand regular expression, 
we'll go through a little bit basics,  
95:39 - but regular expression is so old that 
anytime you Google for regular expression,  
95:44 - you would find the answer for which you're 
searching for. So, the basics of expression is  
95:50 - primarily to understand the meta characters, 
the meta character pipe, the pipe operator  
95:56 - defines a Boolean or so, if you want to compare 
between two words, whether it is gi AY or GY,  
96:05 - then you would use a pipe operator in your 
regular expression. So, then parenthesis, let  
96:10 - us assume that you do not want to use individual 
different words, but you want to just use or for  
96:16 - one particular character, then you would use gr, 
open parenthesis a pipe e close parenthesis, which  
96:24 - is to say I want gr and then it could be either 
A or D and then I want to wipe So, this is how  
96:30 - my pattern should look like. And this is what we 
call a grouping regular expression metacharacters  
96:36 - also has something called quantification, 
which is to say, how many or how the number  
96:42 - of elements or tokens should be present in the 
pattern that I would like to see. For example,  
96:47 - the question mark indicates zero or one 
occurrence of the preceding element, which means  
96:54 - let us say you have got color CEO elbow you 
question mark and our which means it could  
97:01 - match both CEO elbow our our CFO elbow, you our 
while question mark is about one Li one occurrence  
97:08 - as strict which is star is about multiple 
occurrences. Which means if you say a B star C,  
97:16 - it could be either a, b, c, or it could 
be either ABC, or it could be either ABC  
97:24 - or so on. So it doesn't strictly count only one 
occurrence of the preceding character. But it  
97:30 - actually has n number of preceding characters 
that you could have. But the condition is that  
97:34 - you have to have a and b and c. So this is the 
basic concept of regular expression. But you could  
97:42 - actually refer more online to understand more of 
regular expression. But for us to proceed with  
97:47 - this project, I think this is good enough for us 
to see how to build a basic regular expression,  
97:52 - or at least to understand the regular expression 
that we might get from internet. So now, from  
97:58 - the basics of regular expression. As such, we'll 
move forward to see how these regular expressions  
98:03 - could be used in Python for us to do pattern 
matching. And as we saw in the previous video,  
98:08 - the package that we are going to use for regular 
expression in Python is R E. So Ari has these  
98:15 - major functions match search, find all split, 
sub and compile. So all of these you will see  
98:22 - compile is something that would be mostly used to 
create a regular expression, and then compile it,  
98:28 - and then use that compiled regular expression to 
find and search for everything else. And match is  
98:37 - when you would use if you want to match the first 
word and search, you don't care about whether it  
98:42 - is first or second word, but you will want to 
find it anywhere it isn't. And find all again it  
98:47 - has its own purpose. So let us open our terminal 
and then see a little bit of regular expression,  
98:53 - how we would like to do it. So you can open either 
your terminal or your Python console on pi charm  
98:58 - or Jupyter Notebook. But for simplicity, 
I'm just invoking Python from my terminal,  
99:04 - then say import IE. And let us create a 
sample text. Okay, my text is going to say  
99:10 - best Python course this is my text. And now 
I would like to see where the word based  
99:18 - if the word based occurs in my text, so first 
I'm going to say re dot match of paste, which  
99:26 - is my search pattern here, in this case, regular 
expression. Comma, I'm going to save my text  
99:32 - and then enter. So as you can see, it has replied 
with a real object, understand zero to four which  
99:39 - means I've got the word best. And the match 
is best. But let's say we want to test it  
99:45 - with two different words either best or let's say 
good match but we are going to use pipe operator  
99:53 - to say good so it says the matches best 
because the text is saying We have got  
100:00 - best Python course, let us create a second text, 
which is text to. And in this case, we'll say good  
100:07 - Python course. And then we'll use the same regular 
expression that we used before which is best pipe  
100:13 - good, which means either best or good and 
then say text to in this case, it says that  
100:19 - it has matched with good, and then it is the same 
space. Now what we'll do is instead of using match  
100:26 - in this will use good best Python course 
aspects too. And then instead of using match,  
100:34 - we'll use search to see what output it gives 
us. As you can see, it has matched good,  
100:43 - because we have got either based or good and then 
index two and the position is good based Python  
100:50 - course, let us assume now we want to see both both 
of these in the given text. So now we'll say T dot  
100:58 - find all which will result us both 
good and best. So, in this case,  
101:04 - we wanted to see whether both these words are 
present and then because both these words are  
101:08 - present, it is returning us both good 
and best that it is present in the text.  
101:13 - And in case if the pattern that we have given 
the regular expression that we have given us,  
101:16 - let's say something like this, and then it 
is not available and then we will not get  
101:20 - any result which is we have got an empty 
list. So these are the main functions  
101:24 - that we are going to use in this particular 
project. So this is a very brief introduction  
101:31 - of regular expression, and also how to use regular 
expression in Python for pattern matching. In  
101:37 - the next video, we will see very basics of 
natural language processing using spacey.  
101:44 - In this video, we will learn basics of natural 
language processing using spacey. So let us open  
101:50 - our Jupyter Notebook and create a new Jupyter 
Notebook. So you can do this by going to File  
101:56 - and then new notebook and select Python three 
once you have this new notebook. Let us now  
102:01 - see a little bit about spacey. So as we saw in 
the previous video, spacey is one of the most  
102:07 - popular used natural language processing library. 
So we'll import the library using import spacey.  
102:16 - Once that is done, the very first step, 
the next step that we are supposed to do is  
102:21 - load our language model. So if you 
remember after installing spacey library,  
102:26 - we downloaded the English language model. 
So if you remember, after installing spacey  
102:32 - Python library, we actually downloaded the 
English language model because a language  
102:36 - model is essential for a lot of natural language 
processing tasks. So, we are going to load the  
102:43 - language model that we downloaded in the previous 
section. So we will use spacey dot load and then  
102:49 - put the name of the language model that you have 
downloaded. For example, let us assume that you  
102:53 - have downloaded a language model for a different 
language like German, so you will have a different  
102:57 - name instead of E. And so in that case, put the 
language model name that you have downloaded.  
103:02 - And then as I need to NLP. So once this is 
successfully executed, now what you can do  
103:08 - is you can define the text that you have. So 
the text that I've got is I've got a couple of  
103:13 - sentences from Google Wikipedia page for us to 
explore this package. So I'm going to use this  
103:20 - as text and then I'm assigning it to text. So if 
you print text you will see okay, this is my text.  
103:26 - So what are we going to do now is now that the 
entire thing is ready. So we have imported the  
103:31 - package, we have loaded the language model 
and then the input text on which we have to  
103:35 - do natural language processing is ready. So the 
first step in natural language processing is  
103:40 - annotation. So what you're going to do 
is you're going to let the language model  
103:44 - annotate your input text, so that the input 
text is created in such a way that it knows  
103:50 - what is a part of speech tagging in it, it 
knows where are the named entity recognition,  
103:55 - it tokenizers it it creates word vectors it can 
do a lot of other things. So the first step is  
104:01 - use the language model because you loaded the 
language model and presented to NLP on the text  
104:07 - input text that you have got. So the way you do 
is NLP off text and then you store the result  
104:12 - in doc. So once you store the result in dog the 
first very first step that any natural language  
104:18 - processing would require you to do is tokenization 
what is tokenization tokenization is nothing but  
104:25 - splitting the input text based on token so for 
example, so in this case what we are going to do  
104:31 - is we are going to split the entire text word by 
word so word is the token for us here. So you can  
104:36 - do the sentence organization. You can do this even 
paragraph tokenization but in this case we are  
104:41 - going to do word tokenization. So say we have got 
doc so let us print how the doc looks like once I  
104:49 - print doc. I'm not getting anything other than 
this thing because this is what it is printing,  
104:54 - but internally doc has been annotated using the 
natural language processing light library and  
105:00 - language model that we have got. So what are we 
going to do is we are going to say for token and  
105:04 - doc print token so we are going to iterate through 
doc and then say okay print each and every token  
105:09 - by default, which is a word token. So we 
what we see is, we see one by one word by  
105:14 - word. And then okay the word organization has been 
successfully then, once the word tokenisation has  
105:20 - been successfully done, we can do a lot of things. 
For example, you can build a workload if you want,  
105:25 - you can build a unigram or if you want, you can 
have a bi gram also combining the words. In case  
105:31 - let's say you want to visualize the sentence, 
instead of doing word tokenization, you can do  
105:35 - a sentence tokenization. So the opportunities 
are a lot so you can do anything you want.  
105:40 - To move ahead with the spacey library, what we can 
further do is as we saw, when you annotate your  
105:46 - input text using the language model that you have 
got you get out of speech also part of speeches  
105:50 - to say okay, this this word is a noun, this word 
is a verb, this word is an adjective. So all these  
105:55 - things, so what we are going to do now is we are 
going to say okay, for token and Doc, the same  
105:59 - thing that we saw before, what we are saying onely 
if that token dot POS underscore, which is to say  
106:06 - that this is the part of speech, which is noun, 
then print the token. So print all the tokens,  
106:12 - which is known that is what we are going to do 
here. So you see that you have got founder Money,  
106:16 - money, Angel, all these things. It is say instead 
of now let's say we want to work. So once you  
106:20 - exude this, you get was funded. But in case if 
you want objective, you put a DJ, and then you get  
106:27 - all the objectives that you have got. So this 
is how you extract a particular part of speech,  
106:32 - for example, you're doing text summarization, 
or you're doing any other text technique,  
106:36 - you want to extract topic from it. So you're going 
to only go for nouns and adjectives. So the same  
106:41 - way for you to identify a particular part of 
speech. So this is what you're going to use,  
106:45 - iterate through the document. For every token, 
you're going to say, give me the product speech.  
106:51 - So the next important thing or in fact, the most 
important thing for this particular project is  
106:56 - named entity recognition. So what does named 
entity recognition, named entity recognition is  
107:01 - nothing but identifying entity of that particular 
word. For example, Google is an organization,  
107:08 - August 1998 is a date. When you 
have money dollars, it is money,  
107:12 - when you see a person's name, it is a person. So 
identifying, you know putting a context around the  
107:17 - word instead of just simply saying, whether it is 
a grammatically noun or verb or adverb adjective.  
107:23 - So you're trying to say okay, this is what 
I have identified this entity. So that is  
107:27 - what we are calling us named entity recognition. 
And this is much, much easier to do and spacey,  
107:33 - because of the language model that 
we have got under spacey has made it  
107:37 - so easy for us to do it in one single 
function. So what we are going to do is,  
107:41 - we are going to say dot.ns, which will apply the 
entity recognition on the dog. So in the previous  
107:47 - section, you would have seen that we have done it 
only on dog because we were iterating through the  
107:52 - actual words. But in this case, we want to iterate 
through the entities. So we are saying dog.ns,  
107:57 - which is for entities, and then we are iterating 
over it with entity. And then we are saying okay,  
108:03 - print entity dot text, the actual word, and then 
print entity dot label underscore, which means  
108:09 - the label of the entity that has been recognized. 
So I'm printing the word, and I'm printing the  
108:16 - label of the entity, which is organization or data 
or whatever it is. So once I print this thing,  
108:22 - this is what I get. So once I print this thing, 
so we have entity dot txt, entity dot label  
108:28 - underscore, I'm going to print this and once I 
print this thing, and this is what I get, so I get  
108:32 - Google org, or augusztus. August 1998 is a date 
and Google is an organization, Jeff Bezos CEO of  
108:40 - Amazon is a person and then Stanford University 
is an organization. So this is the way what we are  
108:46 - going to do is we are going to take the text and 
then identify important entities that are present  
108:51 - in this text. So this is the basic overview of 
natural language processing. Using spacey Of  
108:57 - course, natural language processing is an emerging 
area in research and development. And of course,  
109:02 - one of the most widely, you know anticipated areas 
in machine learning and artificial intelligence.  
109:08 - So learning natural language processing 
with just five cells in Jupyter Notebook is  
109:12 - completely impossible, but just wanted to give you 
a flavor of what we might be doing in this course,  
109:17 - so that you have some idea of you know how to 
proceed further or if you want to extend this  
109:21 - particular project to include different set of 
skills, or different variables that we have not  
109:28 - captured in this resume parsing project. So you 
can use this understanding that we have covered  
109:32 - in this video to extend this particular project 
to you know, wider objective. So in this video,  
109:39 - we successfully learned the basics of 
natural language processing using spacey  
109:43 - we saw a couple of spacey functions how 
to load spacey, how to learn natural  
109:46 - language processing the library English language 
library, and then we also saw a little bit about  
109:51 - tokenization and named entity recognition. In 
the next video we will actually start with the  
109:56 - coding part of resume parsing project which 
we are interested In this particular project  
110:02 - In this video, we'll learn how to code the actual 
project of resume parsing. Before we get into  
110:09 - coding, there is a little bit of understanding of 
folder structure is required. So let us see our  
110:15 - folders are organized for this particular project, 
we need to create two different folders that are  
110:20 - essential for this particular project. The first 
one is resume his resume is is the folder where we  
110:26 - will have all the input files all the resumes that 
we have got in this project. So in this project,  
110:31 - we have taken three resumes and all those three 
resumes will be present inside this folder  
110:36 - resumes, then the next folder that we would need 
is output folder, which will have two subfolders.  
110:42 - The first one is txt. txt. txt will contain all 
the text converted format of those resumes. So  
110:50 - once the resume a PDF is read, and then converted 
into a txt that is stored inside this text folder,  
110:57 - and then once the entire parsing is done, all 
this content is converted into a structured  
111:02 - tabular format, which is a CSV and then 
it is stored inside this folder CSV. So  
111:07 - first, we need resume is folder where we will 
have all the PDFs that we want to be parsed.  
111:12 - Second, we will have output folder inside 
output folder, we will have two subfolders.  
111:18 - One is txt and then the second one is CSV. Once 
you are done with creating these folders, the next  
111:25 - important file that we need for this particular 
project is PDF to txt dot p y. So the file that  
111:32 - we need is PDF to txt dot p y. And how do we 
get this file. This is a file that is present  
111:40 - inside the PDF miner library. So we need to get 
this file from the PDF miner library. So let us  
111:46 - go ahead and then open the PDF minor PDF minor 
dot six to be precise PDF minor dot six GitHub  
111:54 - repository. And then let us go inside the tool 
section. So open PDF, miners GitHub repository,  
112:02 - and then get into Tools section. So from this tool 
section, you can see that you have PDF to text  
112:10 - dot p y, click this. And then you get 
RAW file. Once you click the raw file.  
112:17 - You have this option when you press Ctrl S, or 
if you're using Mac Command S or you can use your  
112:24 - Firefox to say OK, save pages, which will tell you 
okay, how do I have to save the file. So once you  
112:32 - have this file, saved in your local drive 
the folder where you have what this project,  
112:38 - now we are ready to go further with the 
coding. So there are three essential things  
112:43 - to folders, main folders, output and resumes. 
And inside output we want txt and CSV to folders.  
112:50 - And then we need to get this PDF to text dot p 
y from the PDF minor GitHub repository and then  
112:56 - store it inside the current working directory 
the current folder where this project is setup.  
113:01 - Once this is done, let us go ahead and open our 
Jupyter Notebook. So until now, what we saw is  
113:08 - how the folder structure is organized, and what 
is the essential file that we need that we source  
113:13 - just from PDF miner GitHub repository. 
Once this is done, let us go ahead and open  
113:20 - our Jupyter Notebook. Once you open the Jupyter 
Notebook, please create a new notebook by just  
113:25 - going into file, new notebook Python three. Once 
you do this thing, your new notebook will be ready  
113:32 - in the new notebook as we have a better coding 
practice in the every project that we have done.  
113:39 - Once you create the new notebook by 
going to File, New notebook Python three,  
113:46 - we will have a new notebook. And as a better 
coding practice, we will start with importing the  
113:52 - library that are required and then we'll create 
functions that are acquired and then finally  
113:56 - will invoke those functions and we have got the 
entire project set up in the Jupyter Notebook. So  
114:01 - this is the flow that we are going to follow while 
creating the Jupyter Notebook. So the first one is  
114:07 - we are going to load all the packages that are 
required for this project. So if you remember  
114:12 - from the previous videos that we need five 
essential packages. The first one is spacey  
114:18 - for natural language processing, PDF miner for 
PDF to text. re r e for rejects, works well as  
114:28 - operating system OAS for file manipulation and 
then finally pandas for CSV tabular format.  
114:37 - So pandas for CSV tabular format, once this 
is done, let us execute this thing. Next we  
114:44 - are going to input the file PDF to dot txt dot 
p y. So next we are going to import PDF to text  
114:52 - dot p y. Next we are going to import PDF to 
text dot p y that we just downloaded from the  
114:59 - Google tab repository that we have kept it in our 
current working directory is a project folder. So  
115:04 - what we are going to do is we are going to simply 
say import PDF to txt. And then we are going to  
115:09 - execute this, this is successfully executed, 
which means we have got the file in the right  
115:14 - Drive folder location. So the next task is the 
first function that we are going to create is  
115:22 - for converting PDF to txt, and we'll call 
this function convert underscore PDF. So this  
115:32 - function is going to take one argument. So this 
function is going to take one argument, which is  
115:38 - the file name. So the file name is going to be 
given inside this function, and then we'll see  
115:43 - what is this function is going to do. So the first 
thing is once we get the file name that is passed  
115:47 - into this function, we are going to say okay, 
split the file name, and then had dot txt. So,  
115:56 - imagine you have got a file name, let us 
say, okay, observe Majid, which is my name,  
116:04 - and PDF, this is our typical resume might look 
like. So what we are trying to do is we are trying  
116:08 - to create the output file name. So because when 
we are going to convert the PDF into text, we also  
116:16 - want to save the text in the folder that we just 
saw. So what we want to do is we want to create  
116:22 - an output file name using this input file name. 
So when we use always dot path dot split text  
116:27 - on the input file name, what we get is we get two 
items just like this. So we get two items. So what  
116:33 - we are trying to do is we are going to take the 
first one because Python is zero index language,  
116:39 - we are going to take the first one like 
this, and then we are saying okay plus text,  
116:44 - which uses the new output file name for the 
text file, and then we are assigning this name  
116:49 - to the output file name. So understand that we 
got the input file name like this, a PDF file,  
116:55 - and we are trying to remove that extension 
dot PDF. And then we are trying to append this  
117:00 - new expression that is dot txt. And then this new 
extension dot txt is appended to this filename,  
117:07 - and then this name is getting assigned to this 
output underscore file name. Once this is done,  
117:14 - we also have to define where do we want 
to save this file. So as we saw just now,  
117:20 - in the folder structure that we are going to have 
an output folder. And inside that output folder,  
117:24 - we are going to have all the txt files. So what 
we are going to write here is we are saying Okay,  
117:31 - give me the path, which is output slash txt 
slash and then this output file name that we  
117:38 - just created. So our output file is going to be 
saved in this file path where it is output slash  
117:43 - txt slash, and then the file name output file 
name with dot txt extension. So this one,  
117:49 - we are going to assign it in output underscore 
file path. The next thing is the PDF to txt that  
117:57 - we just imported, it has a main function, 
and then it takes a couple of arguments.  
118:03 - So what we want to do is we want to save this 
file in that particular location. So we are  
118:07 - going to say okay, the arguments that I'm passing 
is the file name, which I just received, and then  
118:13 - the output file name, which is passed down with 
this argument, hyphen, hyphen, outfile, and then  
118:19 - the output file path where it has to be saved. So, 
this is the function that helps us converting PDF  
118:25 - to text and save it in the given location, which 
is what we create here. So, the output location is  
118:33 - created here, output file name is created here. So 
the input file, which is the dot PDF file is given  
118:39 - through this F and then the output file is saved. 
Once that is done, we would like to present a  
118:44 - message to the user saying that the file has been 
successfully saved this is just for reference.  
118:49 - And finally, we are going to return the 
output file path and then we are reading  
118:54 - it as an input file. So what we are doing 
is in the same function, we are outputting,  
119:01 - the read file so that is why we have got open the 
file name and then dot read which is a function  
119:08 - to read any file in Python. So ideally, we 
could have done it in two lines to say okay,  
119:14 - I'm reading the file, and then I'm passing 
that read object the file object as returning,  
119:19 - but in to save space and also for simplicity, 
which is one of the core philosophies of Python  
119:24 - to have simple code, what we are trying to do is 
we are trying to do it in same line. So the file  
119:30 - path which we are reading after opening, 
so, this is what we are going to return.  
119:35 - So, in this function convert underscore PDF, there 
are five things that we are doing the first thing  
119:40 - is we are creating the output file name. Second 
thing is we are creating the output file path.  
119:44 - And then the third thing is we are converting PDF 
to txt and saving it in the given location which  
119:49 - is the output file path. Then we are printing 
a user message to say that this is successfully  
119:54 - done and then finally we are returning the read 
file opened and read file with To be just saved  
120:01 - and then with this, this function is done, let us 
execute this function. And this is just for sample  
120:06 - we don't need I'll delete this cell. Once this is 
done, now we are going to use spacey to load the  
120:13 - language model. So let us load the language model 
in this line which we just saw, load the language  
120:20 - model. Once we have the language model, we are 
going to create an output file structure. So  
120:28 - we saw that the ultimate objective of this 
project is to capture four important content  
120:34 - four important components from the resume and 
then make it a structured document which is name,  
120:40 - phone, email skills, and then these are going to 
be the four columns that we have in our output  
120:46 - tabular format. And for that, we are creating a 
dictionary a Python dictionary using the curly  
120:53 - braces. So if you remember your basics of Python, 
a Python dictionary is created using curly braces,  
120:59 - a Python list is created using your square 
brackets. So, what we are trying to do is we are  
121:04 - trying to create a result dictionary and then we 
are also creating four placeholder values names,  
121:12 - phones, emails skills, and then we are making it a 
list so that when we extract these information, we  
121:18 - can put that particular component the respective 
component for example, let us assume that we have  
121:24 - extracted name from the first resume a secondary 
email address, so we have three names, and those  
121:29 - three names will go into the placeholder list that 
we have created names, the phone numbers will go  
121:34 - into phones, the email ids will go into emails 
and then the skills will go into skills. So, with  
121:40 - this, we are ready with the placeholder the type 
of output that we would like to have Once this is  
121:47 - done, we are getting into the core function which 
is going to extract the content from the resuming.  
121:54 - So with the placeholder output now, we are moving 
forward to see how to define the function that  
122:00 - will do the extraction the core for component 
extraction for this particular project. Now  
122:05 - that we have got the output placeholder in place, 
which is a dictionary and then couple of lists to  
122:11 - put the content inside it extracted content inside 
it, we are moving forward to do the actual core  
122:18 - component extraction part of this particular 
project. And this function will call it as  
122:23 - parse underscore content, which is to parse 
the content that we have, and the argument is  
122:29 - text. So this function receives text, if you 
remember, what we returned from this function is  
122:36 - the Convert PDF function is we opened the file 
and then we read it, which is a file object  
122:43 - and then in this function, we are going to read a 
text. So that is how we are interlinking those two  
122:47 - functions, and we'll see it in the future. So, the 
first thing that is required for us is to define  
122:53 - what skill set that we are expecting to extract 
from this project. So extract from the resume.  
122:59 - So, the first thing that we have to define is what 
are the set of skills the skill set that we would  
123:04 - like to extract from the resuming. So, considering 
a data science setup, what we are trying to do is  
123:11 - we are trying to see okay, I want Python and if 
you remember your rejects, you might remember  
123:16 - that a pipe operator signifies or condition, so 
Python or Java or SQL or Hadoop or tableau. So,  
123:24 - these are the five things that we are trying to 
extract as skills from a particular resume and we  
123:30 - are trying to see whether this particular resume 
has any of these things or all of these things.  
123:35 - The next thing is phone number. So, what 
we are trying to do is we are trying to  
123:40 - create projects, that projects should be able to 
capture phone number and this phone number rejects  
123:47 - has been extracted from this StackOverflow 
answer. So, I would like to give credit to that  
123:53 - answer that has created a regex that can handle 
multiple different types of phone numbers. So,  
124:00 - we saw we want this a skill set. So we are 
compiling this expression and then saving it  
124:06 - in skill set and this is the regex to capture 
the phone number and then we are compiling it  
124:11 - and saving it in for now. And then what we are 
trying to do is we are trying to take the text  
124:19 - that we just got inside this function and then we 
are trying to do annotation. So if you remember  
124:26 - the basics of spacey video The first thing that 
we have to do is annotation. Once that is done  
124:32 - what we are trying to do is we are trying to 
extract two entities from the annotated the past  
124:40 - are the natural language processing text. So the 
text on which the natural language processing has  
124:46 - been done on the text is nothing but the resume 
content okay. So from the past content or from  
124:53 - the extracted text content, now we are trying to 
do two things. The first thing is extract name.  
124:58 - The second thing is E So, totally we want four 
components, which is name, email, phone number  
125:04 - and skill set. And we just saw creating regular 
expressions for skill set and phone number.  
125:09 - And now we are trying to extract name and email id 
and to start with the name what we are trying to  
125:15 - do is entity any named entity recognition has 
one particular level which is called a person  
125:21 - that signifies the name of a person. So, what 
we are trying to do is we are trying to do  
125:28 - if the entity label is person, then give me the 
entity text and assign it in name. So, this in  
125:36 - Python is called a list comprehension, which 
means, instead of writing a for loop in multiple  
125:41 - steps, you can write it in a single step. So, what 
we are doing is we are doing a list comprehension  
125:45 - and then we are saying okay, whenever the 
entity recognized entity, post label is person,  
125:52 - then give me that entity text. And then we are 
saying okay, what if a resume has multiple names,  
125:58 - for example, first, the name of the person would 
be on the top of the resume, probably their dad's  
126:03 - name and mom's name is there. So for that, 
with that assumption that the person's name is  
126:08 - always on top of the resume, you're saying you 
mean the first name that you detect. So we are  
126:13 - trying to take the first name that is detected 
in the resume and then as any to name. Next,  
126:18 - we are also printing the name for us to have some 
reference whose resume may have passed. Next is  
126:24 - we are trying to do email id extraction. So what 
we are trying to do is we are trying to say okay,  
126:30 - for every word, the tokenized word in the 
document. If the word is like email show  
126:37 - spacey has such an attribute, like underscore 
email, if this word like underscore email,  
126:44 - which is returning a boolean value, either true 
or false. If it is true, then give me the word.  
126:50 - And I'm telling, okay, give me the first word, 
there will be multiple email ids. But I don't  
126:55 - care about the second and third email id, at least 
for this particular project. So what I'm saying is  
126:59 - okay, give me the first email id and then store 
it in email, and then print the email id again,  
127:05 - for a reference. Now that we have built the 
regular expression for skill set and phone number,  
127:11 - it is time for us to use the expression 
that we have compiled and then extract.  
127:16 - So there is one thing that we have to notice, we 
are trying to convert the text enter text into  
127:21 - a lowercase before even proceeding further with 
the regular expression. This is to solve the case  
127:26 - issue. So it could be like for example, let's say 
this is Python, someone could have written Python  
127:30 - as capital P y to H and someone could have 
written SQL as SQL capital or small letter  
127:36 - Java could be with J capital. So to solve this, 
all these issues, we are normalizing all the texts  
127:42 - or downgrading all the text to a lower 
case. And then we are saying okay,  
127:46 - rejects find all where you have phone number 
pattern, and then the string on which you have  
127:52 - to find this text, which is what the argument that 
we got. And then after converting into lowercase,  
127:58 - and then we are assigning the output to form after 
converting it into a string object. So, simply  
128:05 - we have the text, we are converting into a 
lower text, the text is the resume matrix,  
128:10 - we are converting everything into lowercase. And 
then we are trying to find all everywhere in this  
128:16 - resume with this regular expression. And then 
we are converting that result into a string and  
128:22 - then as entering into a foreign object. And then 
the same thing that we are trying to do here is  
128:27 - find all the skill set text lower, and then assign 
it to a skills underscore list because we will  
128:34 - have multiple skills one resume could have Python 
and Java both. So that is why we are calling it as  
128:38 - skills underscore list. Also, one thing that 
we have to notice in one particular resume,  
128:44 - we could have multiple instances of Python and 
Java. For example, let us assume in the technical  
128:49 - skill section, someone has mentioned Python, but 
also while describing the project, they might have  
128:54 - used the word Python. So what we would end up 
getting here is in the skills underscore list,  
128:59 - we would end up getting Python two times 
and we don't want to record Python two times  
129:03 - for that matter, because we want to see 
whether Python is present or not present.  
129:07 - So, what we are trying to do is we are trying to 
say okay, converted to a dictionary, which means  
129:12 - of course, it will have only unique 
elements. And then we are converting  
129:16 - that into a string and then assigning it to 
unique underscore skills underscore list. So,  
129:23 - in this entire section, we have defined the 
regular expression for skill set and for number  
129:28 - we have used spacey for annotation and then we 
have used the annotated text to extract a Persian  
129:33 - and then as Enter to name to see whether there 
is anything like email then as Enter to email  
129:38 - and then we have used phone number and skill set 
regular expression that we just compiled to find  
129:43 - everywhere where we have got and then we have got 
the skills and unique skills. So right now, after  
129:50 - this, what we are trying to do is we are trying 
to append these names in the placeholder list  
129:56 - that we just created. So these are the four empty 
placeholder lists. recreated, so we are trying to  
130:01 - append all these that we just extracted in those 
lists. And then finally, we are trying to print a  
130:06 - message or message to the user to say, extraction 
has been completed successfully. So for a small  
130:13 - summary, this is the function, the core function 
where we are extracting the four components,  
130:18 - it takes one argument, which is the text of 
the resume. And then initially we are compiling  
130:23 - the regular expression for skill set and phone 
number. Next, we are annotating the text document  
130:28 - using natural language processing of spacey, 
and then we are extracting name, and then we  
130:33 - are extracting email, and then we are extracting 
phone number and then we are extracting skills,  
130:39 - and then making unique skills out of it. 
And then we are appending all those values  
130:43 - in the placeholder list that we created in the 
previous cell. And then finally printing our user  
130:47 - message to say extraction has been completed. 
So let us execute this thing and it has been  
130:53 - successfully executed. In the next line, what we 
are trying to do is we are trying to say okay,  
130:59 - now I've got two main functions, one to convert 
all the PDF to text. And also, you know,  
131:05 - meanwhile, save that txt file in the particular 
folder. Second, use the text content and extract  
131:11 - whatever I wanted the four components name, 
phone number, email id skills. And with this set,  
131:17 - now what I have to do is, I have to make this 
project work on multiple files rather than  
131:23 - only one PDF file. So that center objective of 
doing an automated bulk resume a parser, right,  
131:28 - so you don't really want to use a code just to 
pass one Li one resume because a human being Of  
131:33 - course, could be better in passing one resume, 
but assume that you are a manager and then you  
131:37 - have got 100 resumes or 200 resumes or 50 resumes. 
And this is the case, we want to have an automated  
131:44 - resume a parser bulk resume a parser and that 
is exactly what we are trying to do here. So  
131:49 - what we are trying to do here is we are trying to 
list down all the files inside the folder resume.  
131:55 - As we just saw, we have three files. 
Inside resume, we have three resume files,  
132:00 - and then we are saying okay, lists down all the 
files and iterate through each of the file calling  
132:06 - it has file. So we will take individual resumes, 
and we are calling it file. And then we are trying  
132:12 - to just validate whether the file name ends with 
dot PDF. So in the same folder, you could have,  
132:18 - let's say, forgotten to convert a docx file into 
PDF. So what we are trying to do is we are trying  
132:24 - to add an extra layer of validation to say, Okay, 
I want only PDF, because right now this code is  
132:29 - built only to function with PDF. And if you have 
got a docx, you have to manually convert a docx  
132:35 - to a PDF or you can find a lot of script online to 
convert docx to PDF. So we are saying okay, if the  
132:42 - file name ends with the dot PDF, then first print 
the user message that we are reading the file  
132:48 - and then read the file. Convert underscore PDF 
is the function that the function that we just  
132:53 - created, invoking that and then the path where we 
have all the files is this one. So the first file,  
132:59 - the first iteration, it will be the name of the 
first file. In the second iteration, it will  
133:03 - be the name of the second file. And in the third 
iteration, it will be the name of the third file.  
133:07 - And as we saw in this convert underscore PDF, 
we are returning a file object, we opened the  
133:13 - file using the path and also we read it. So we 
are assigning that output inside this object  
133:21 - called txt. And then now we are passing this txt 
to the function that we just built here. So this  
133:28 - takes one text object as argument. So we are 
passing that as the parameter here saying okay,  
133:36 - parse underscore content. txt, once we run this, 
okay, first resume is read. Allison Parker's  
133:43 - resume is read. And then the output is saved 
successfully. Allison Packers, right is the name,  
133:49 - email id. So these are all hypothetical names 
that do not exist. So it's a disclaimer that  
133:54 - this is these are all hypothetical resumes, and do 
not reflect to any living human being. So this is  
134:00 - the first resume PDF is red text to successfully 
saved the exact same message that we printed here.  
134:07 - And then we have the name, which we also printed 
here. And then we print the email ad, which is  
134:13 - here. And then we finally say extraction has been 
successful. So this is first resume. Then this is  
134:20 - second resume, reading john domnick. saving it 
as txt name, email id extraction completed. And  
134:28 - then finally, actually miles reading, saving it as 
text extracting name, email ID, which is what we  
134:35 - printed, we didn't print phone number and skills 
and then extraction complete successfully. So now  
134:40 - that we know that we have got three resumes, all 
those three resumes have been successfully read,  
134:45 - what we are going to do is we are going to use 
these placeholder values which is now populated  
134:50 - with all those names. So to give you some 
perspective, names will have all the names phone  
135:00 - We'll have all the phone numbers, skills, we'll 
have all the skills. And then of course, emails,  
135:06 - we'll have all the emails. Now that we have got 
all the values of this placeholders populated. Now  
135:12 - we are going to say okay, assign these into these. 
So this is a dictionary with it is a key value  
135:18 - pair, and this is the key and then we are going 
to put this against the respective key as values.  
135:25 - So what we are doing here is we are saying, 
okay, the value of this key should be names,  
135:30 - the value of key, this key should be phones. and 
the value of this key email key should be emails.  
135:35 - And then finally skills, then to see how does it 
look, let us execute this, then say result. And  
135:42 - that's good, which is the result dictionary. So 
as you can see, the starts with a curly brace ends  
135:46 - with the curly brace, this is the key, and this is 
all the values, this is the key, this is all the  
135:51 - values. So this is how a typical dictionary 
in Python will look like. And then finally,  
135:56 - we have to convert into a tabular format, which we 
will do using pandas DataFrame. So in this video,  
136:04 - we saw how to start with importing libraries 
define two important functions. One is converting  
136:10 - PDF to text. The second one is the code the 
main function, the it's like the engine of the  
136:16 - entire project, which is parsing the content and 
extracting required components. And then finally  
136:21 - storing the required components into a Python 
dictionary, which in the next video, we'll see how  
136:27 - to convert into a tabular format using pandas, and 
also to save that CSV as an Excel file CSV file.  
136:35 - In this video, we'll see how to convert the 
dictionary that we created into a tabular format  
136:42 - using pandas. And we will also see how to then 
finally save this entire thing as a script to run  
136:50 - on a bulk, you know, set of files in a particular 
folder. So to start with, this is where we left in  
136:57 - the previous video where we had created a result 
underscore dict, which is a dictionary Python  
137:00 - dictionary, with all the essential content that we 
just extracted from the three resumes that we had  
137:06 - got in the race image folder. So to move ahead 
with, we are going to use pandas, which we just  
137:12 - imported at the start import pandas as PD is what 
we used. pd is an alias. So we are going to use  
137:19 - that alias PD dot data frame. So the beauty of 
pandas is that panda's data frame is nothing but  
137:28 - a Python dictionary internally. So it is easier 
for us to convert result underscore date into a  
137:34 - data frame just by invoking this function called 
Data Frame. From this lspd pd dot data frame.  
137:43 - We're framus the SS capital D and therefore 
capital and pass result underscore dict  
137:49 - as any to result underscore df df stands for 
data frame or you can give any name that you  
137:53 - would like to and then I'm printing df, which says 
okay, this is my name. This is my phone number.  
138:00 - This is my email. And these are the skills that 
I've got. So Alison has got it on Tableau and  
138:05 - Java. JOHN Dominic has got How do Python and Java 
actually Myles has got SQL and tableau. And then  
138:13 - the next step is for us to save this entire thing 
into a CSV file. As we saw in the previous video,  
138:20 - the folder structure that we've got, where 
output has two folders subfolders. One is text,  
138:26 - which stores all the text files that are 
converted from PDF, and in the CSV where  
138:32 - we have got the output CSV, we are going 
to say okay, save the output CSV there.  
138:38 - And then we can go ahead and open the folder 
and see that the output CSV is present here.  
138:46 - So until now, what we have seen is take the 
dictionary convert into a tabular format,  
138:51 - which is a panda's data frame, and then 
save that data frame into a CSV file.  
138:57 - But this Jupyter Notebook is good for us to 
prototype. But now, the objective is, we have  
139:04 - a folder full of resumes. And then you know, 
you want to give this to someone who cannot use  
139:09 - proper Python coding, and then they should be 
able to convert all those PDFs into structured  
139:15 - content. And for that purpose, we are going 
to convert this Jupyter Notebook into a Python  
139:19 - script and then use the Python script to convert 
all those PDFs into text. So let us go ahead  
139:25 - and then open file, and then go to download as and 
then do.py. This will give us the Python script.  
139:34 - But before we do that, we have to remove 
those instances that are not required.  
139:39 - For example, we have a lot of places where 
we have printed these things, which is  
139:44 - quite unnecessary. So we'll delete these cells. 
You can use x To delete a cell, or probably  
139:49 - you can go here and then try and delete cell. So 
I'll use x, delete, delete, delete, delete. Cool,  
139:59 - and then I'm going to Delete this dictionary. So 
we are done with this as of now, and we can save  
140:06 - and checkpointed just for our reference, 
then what we can do further is we can go  
140:12 - download Python read poi, so we'll save the file, 
and then make sure that you have got the file  
140:20 - inside the folder, the current project folder. So 
bring that file here, the current project folder,  
140:26 - where we have got all the files. So now, 
you should have the folders that we defined  
140:33 - with the resume is with the input files, that 
resume is with all the PDF regimens that we want  
140:38 - to be parsed. And then the output folder 
with two subfolders, CSV and txt. txt.  
140:46 - And then also, the PDF to txt dot 
p y should be inside this folder.  
140:51 - And this is the notebook that we just created 
to create this entire project. And then finally,  
140:58 - the Python script of this Jupyter Notebook. 
With this, we are good to go that we have got  
141:05 - resume underscore passing with this script will 
use this script on command level as a CLA tool  
141:11 - to automate this entire process of converting PDFs 
into a structured format of valuable content. But  
141:17 - before we do that, let us go ahead and then delete 
these things because these were created when we  
141:24 - ran this particular code in the Jupyter Notebook. 
But to make sure that the project is successful,  
141:29 - we'll delete this. And also we'll delete 
the output file, we have got an output file,  
141:33 - we'll delete this output file also. So right now, 
let us just validate, we have got three resumes,  
141:39 - Alison Parker, Ashley Mays, Dominic, john Dominic. 
And in fact, you can notice that this is all three  
141:46 - different formats. This is a double column, this 
is a single page resume. So we have got different  
141:53 - resume formats. And then output does not have 
anything, output is empty with two subfolders,  
141:58 - empty subfolders. And then we are all set to test 
our script on this the resumes folder. To see  
142:06 - the code that we just have created using the 
Jupyter Notebook the Python script that we just  
142:10 - downloaded from the Jupyter Notebook, it could 
be used for automation, so open the terminal. Or  
142:15 - if you are using Windows, please open your command 
prompt. And then make sure that you are inside the  
142:22 - project folder. So my project folder name is 
resuming parsing, I can use LS to check Okay,  
142:29 - or if you are using Windows, check the command 
to see where you have got your current folder. So  
142:36 - you can see that this is my current folder. And 
then it shows that I've got this file regime  
142:41 - underscore parsing in place. They've also got the 
folders like output resumes, and the main PDF to  
142:48 - txt dot p file. So with this, what we can do is 
we can say resume a underscore parsing dot p y  
142:57 - but before we do that, we have to say Python 
three phase resume underscore parsing.py. So the  
143:05 - same name that we have given here to say Python 
three ratio parsing that py. Once you press enter,  
143:11 - we'll see all the details that we have done. So 
for example, this is exactly what we saw. First  
143:16 - reading Allison Parker's the first resume output 
is txt saved name, email extraction completed  
143:23 - second resume at john Dominic, john Dominic 
txt saved name email id successfully completed  
143:30 - and then the third resume actually miles CSV 
reading PDF saving txt name email ad successfully  
143:39 - completed now we can see okay LS nothing has 
changed. Let us enter into resumes folder.  
143:47 - And then we will see okay, LS we can see that 
there are three files which is what we used as  
143:52 - input. Now let us come out you see the space dot 
dot coming out and then doing LS just to validate  
144:00 - where we are and then let us get into output. And 
then you can see LS, we have two folders, CD txt  
144:09 - and then LS we can see that we have got three txt 
files. So initially before we started with this  
144:15 - execution of the script, we did not have these 
txt files. Now we have this txt file which means  
144:22 - the conversion of PDF to txt has been successful. 
Now let us also validate whether we have got the  
144:27 - structured information in the form of CSV. So let 
us go out cd space dot.we are again inside output.  
144:36 - We'll get into CSV and then say okay, 
we have got the CSV inside CSV, LS.  
144:42 - We have got parsed underscore resumes dot 
CSV, which means the script that we executed  
144:50 - has completely done what we did with the 
notebook which means the script is perfect,  
144:56 - that the automation is successful, that it picks 
all the resumes folder resume. And it converts  
145:03 - everything into txt and saves inside txt. And then 
it also finally creates one CSV file, which is  
145:09 - the parsed resumes file. So now let us go ahead 
and go to output, open CSV now that we find CSV  
145:18 - in this folder, let us right click it and open 
the CSV with Microsoft Excel for us to see, okay,  
145:24 - this is Excel, the CSV has been opened, you 
can see that okay name is their phone number,  
145:30 - their email it is their skill is there? And 
how can you use the output of this project? Let  
145:36 - us assume that you are HR manager or you are the 
recruiting manager, and you want to use the output  
145:41 - of this project and you have got, let's say maybe 
100 names like that. The way you can use it is,  
145:46 - right now you have a requirement for a SQL 
Developer, SQL Developer, what can you do?  
145:52 - You can just go there, apply this filter and say, 
Okay, I want someone with SQL. What do you get?  
146:03 - Let's say you go to the header, let us format 
this header a little bit. And then you go to  
146:11 - the header, and then say filter, and I 
want someone with SQL, then you see Oh,  
146:18 - you've got Ashley miles with SQL skillset. And 
additionally, Ashley miles also has tableau.  
146:24 - So maybe let us pick up Ashley, my email ad and 
then male and ask if he would be interested in  
146:30 - joining our company for an interview or let us 
assume that you have another requirement where  
146:36 - you want someone with Tableau and then in this 
case, you will go put a filter and say one blue,  
146:45 - then you see okay, Alison Parker right. And Ashley 
Mize has Tableau skill set. And then you know,  
146:50 - now let us prioritize that these two resumes for 
interview and then go ahead and then call them for  
146:55 - interview. So this is the main objective of this 
project. So if you have got a lot of resumes, like  
147:01 - 50 resumes, it is nearly impossible for one human 
being to literally go through all the resumes.  
147:05 - But using an automated script, a bulk resume a 
parser that we have built using this project,  
147:10 - what we can do is we can extract the essential 
skill set that we want. And as you know, you can  
147:16 - change the skill set like you wanted in the regex 
expression that we built, and then filter it using  
147:21 - Excel to say, Okay, these are the resumes that I'm 
going to focus instead of just going randomly with  
147:27 - all the 50 resumes. So in this section, we learned 
how to build an automated bulk resume parser  
147:34 - using natural language processing and regular 
expression. We also alongside learned basics  
147:40 - of regular expression and how to implement it in 
Python. And we also saw introduction to spacey,  
147:46 - and natural language processing using spacey. 
And then once we successfully built the script  
147:52 - to completely convert a resume a which 
is in PDF and an unstructured format into  
147:57 - a structured tabular format. We saved it 
in CSV and we saw how we can use that CSV  
148:02 - to prioritize resumes for selecting the 
right resume for the job requirement.  
148:08 - Thank you for watching this video, 
and we'll see the next section.  
148:14 - In this section, we'll learn how to build an image 
typeconverter. Converting images from one image  
148:20 - type to another image type like PNG to JPG JPG 
to PNG, or BMP to PNG is one of the most wanted  
148:28 - tools that every one of us expect to have handy. 
To build such a tool, we'll start learning with  
148:34 - basic image manipulation in Python, then we'll 
understand what are the Python packages used for  
148:39 - image manipulation. And then finally, we'll build 
a tool that helps us do image type conversion.  
148:45 - The section contains following topics, what are 
the different types of an image what is an image  
148:51 - type converter, Introduction to image manipulation 
in Python, and the Python packages used further.  
148:57 - And then finally, we'll build a script project 
that will help us do image type conversion. In  
149:03 - the next video, we'll learn what are the different 
types of image file formats, and its details.  
149:11 - In this video, we'll learn what are the different 
type of image file formats, and its details. image  
149:17 - file formats are standardized means of organizing 
and storing digital images. The image file  
149:23 - format is usually identified by the image file 
format extension that comes with the file name,  
149:29 - and image file format is required to store data 
in an uncompressed or compressed or vector format.  
149:37 - Once rasterized, an image becomes a grid of 
pixels, each of which has a number of bits to  
149:42 - designate its color equal to the color depth of 
the device displaying it. In general, an image  
149:50 - file format defines how data is stored the image 
data is stored in that particular file format. And  
149:57 - image compression can be of two types. lossless 
compression The other one is a lossy compression.  
150:03 - In a lossless compression the image and file 
format is change the compression is usually  
150:10 - lossless, which means there is no information loss 
in a lossy compression. The algorithm preserves a  
150:17 - representation of the original uncompressed image 
that may appear to be a perfect copy but it is not  
150:23 - so often lossy compression able to achieve 
smaller file sizes than lossless compression  
150:28 - and it is highly preferred when you're going to 
transfer image from one place to another place  
150:34 - where you need to compress the image. What are 
the various different types of image file format,  
150:41 - the most widely used and in internet the file 
format that has been highly preferred to be  
150:47 - used for Image Transfer is JPEG. jpg stands 
for joint photographic experts group,  
150:54 - which is a lossy compression which means JPEG 
is a compression algorithm that stores image  
151:00 - data in a compressed format. After JPEG on of the 
highly preferred file format is PNG. png stands  
151:09 - for Portable Network graphics file format. png was 
originally created as an alternative for GIF, or  
151:16 - GIF, however you want to share it give stands for 
graphics Interchange Format. In the next video,  
151:23 - we'll see what are the different Python packages 
that we'll be using in this particular project.  
151:29 - In this video will learn about the different 
types of Python packages that we will be  
151:34 - using in this particular project. For image 
manipulation, we are going to use a package called  
151:41 - PPI L stands for Python imaging library. Python 
imaging library is one of the most popular Python  
151:48 - package which is free for image manipulation 
in Python. However, there was no recent  
151:53 - support from ti l for any Python version that is 
greater than three, which means PL supports only  
152:01 - Python version that is lesser than 2.7. So for 
this, someone has formed a friendly fork of PL  
152:08 - repository, and that is called pillow pillow now 
supports any new latest Python version that is  
152:14 - greater than three below was created by Alex Clark 
and its contributors to pillow is the library for  
152:20 - image manipulation that we are going to use in 
this particular project below follows the same  
152:26 - syntax as PL you have to make sure in a computer 
where you have installed below that you do not  
152:32 - have bi l below can be installed using PIP which 
we'll see later on. The next package that we are  
152:38 - going to use in this project is globe. globe is 
simply for Unix style path manipulation. So to  
152:46 - identify the files images in our current folder, 
we are going to use globe. So we are going to use  
152:51 - Glo to identify the current image files that is 
in our current directory. And then we are going  
152:57 - to use pillow for converting into from one 
format to another format. In the next video,  
153:03 - we'll learn how to install the required Python 
packages and loading them into a project.  
153:09 - In this video, we'll learn how to install the 
required Python packages and loading them into  
153:15 - our project. As we discussed earlier, the back 
end that we are going to use is called below. So  
153:21 - let us open our terminal. If you are going to use 
Mac, open your terminal. Or if you're going to use  
153:26 - Windows, open your command prompt. And make sure 
that in your computer, if you have Python three,  
153:33 - you're going to use PIP three. So if you're going 
to have Python three, use PIP three or if you're  
153:39 - going to have fight them less than 3.0 use PIP 
so in my computer, I've got python 3.7. So I'm  
153:44 - going to use PIP three, install and below. Make 
sure that your pee in pillow isn't capital letter  
153:51 - and then press enter the installation procedure 
will start now. So right now, you can see that  
153:57 - below has been installed successfully for 
you to verify a pillow has been installed  
154:01 - successfully. You can open Python console here 
using Python three and then import pillow.  
154:11 - The reason that pillow is not found here is 
because we also saw that pillow follows the  
154:17 - same syntax as pa l but it is just a simple folk. 
But in this case, if you are going to use pillow,  
154:23 - you have to just simply input bi l make 
sure that you have only one version of  
154:28 - pillow installed in your machine so that 
there is no clash among the packages.  
154:34 - This is the only package that was required for 
us to install. The other package that we saw  
154:39 - below is already available which also we can 
verify by using import glow. It has been imported,  
154:46 - which means the package is available already by 
default in python 3.0. In the next video, we'll  
154:52 - start with the coding part of creating our image 
typeconverter script in this video We'll learn how  
155:01 - to code or image type converter script to begin 
with, open your PI charm or any other Python ID  
155:07 - that you are going to use for this particular 
project. Once you open your ID or pi charm in  
155:12 - my particular case, go to File and click New to 
create a new Python file. Once you click New,  
155:20 - you'll get all these options and select Python 
file to create a new Python file, you might have  
155:25 - to name the file upfront. So give a meaningful 
name like image conversion new.pi. For ease of  
155:32 - process, I've already created the code and I'll 
take you through section by section. The first  
155:38 - section we are going to load the library that 
we are going to use in this particular project.  
155:43 - As we discussed earlier, you under the package 
name that we are going to use is called pillow,  
155:49 - because it is a fork of p i e l library. So we 
are going to use p L. So from PL we want to import  
155:57 - the class object image. And then we are going to 
import blob, which is to identify the files with  
156:04 - a particular extension. So in this step, we are 
going to use print globe dot globe. And we are  
156:10 - going to use this small regular expression pattern 
that tells us that anything that starts with  
156:17 - anything, and then followed by a dot, and 
then finally ends with a PNG, which means  
156:22 - we are going to tell Python that please give me 
the list of files that have an extension PNG.  
156:29 - To understand this, let us see the current working 
directory of the current working directory offers.  
156:36 - We have three PNG files. As you can see, the first 
one is Batman logo. The second one is Powerpuff  
156:41 - Girls, and the third one is Tom and Jerry dot 
png. So all these PNG files will be displayed.  
156:47 - Once we run this code. The next section, we 
are going to iterate through the PNG files,  
156:53 - each and every file. And then we are going to open 
the image file and then assign it to a new Python  
156:59 - variable called iron. Once we assign it to the new 
Python variable called Iam, we are going to use  
157:05 - this is to apply a method called convert where 
we are converting into tune to its RGB file  
157:12 - format. Our GB stands for red, green, blue, which 
forms the complete color that we usually see.  
157:20 - So in this step, we're going to convert 
the image that we read, which is a PNG  
157:26 - image after getting assigned into a new object, 
we're going to convert it into its RGB format.  
157:32 - Please note that image conversion can also 
have RGB, but RGBA is not something that  
157:40 - we are going to use here for that purpose 
because JPEG is a file format that cannot  
157:45 - retain transparency. A stands for alpha alpha 
represents transparency. So for a JPEG image,  
157:53 - it has only three properties, which is 
our G B. But for a PNG image, for example,  
157:58 - if you're going to convert from a JPG to PNG 
image, you need to have RGB A, which will convert  
158:04 - the image into a new file format along with this 
attribute alpha, which stands for transparency,  
158:10 - which is not required for our current use 
case. So we will use RGB format and convert  
158:15 - the input image. And then finally, while saving 
the image, we are going to use the same file name,  
158:22 - which is what we read. And then we are just going 
to replace the extension from PNG to jpg. And then  
158:28 - it also gives us the flexibility of setting the 
quality value depending upon how large the images.  
158:34 - So if you want the image to be more compressed low 
quality, if you want to upload it online, then you  
158:39 - can reduce the quality which means the size of 
the image will also be reduced. Because we have  
158:44 - given it in a for loop, it is going to happen for 
all the files that we have got. So let us go ahead  
158:50 - and run the code. So to run the code, we can go 
here and pi charm and then just execute the code.  
158:56 - As you can see, the code has completely executed 
and then it has displayed all the PNG that we had.  
159:02 - And then it also has finished with exit code 
zero. As you might have noticed, initially,  
159:08 - when we had opened this finder, the Explorer where 
we had all the files, we had only PNG file format,  
159:14 - but you can see now that we have also created 
new JPG file format. And to notice the difference  
159:21 - when you open Batman Lego file you can actually 
see that there is no background in there,  
159:26 - which means it is completely transparent, which 
is one of the attributes of a PNG image. But when  
159:32 - you actually see the JPEG image, which is this 
one, you actually see that the entire background  
159:37 - has been filled with the black color. That is what 
happens when we had done this conversion where the  
159:42 - attribute alpha has been lost. So in this project, 
we learned how to import the Python image library,  
159:49 - how to find PNG in the current working directory, 
and then how to convert this image from one file  
159:55 - format to another file format with different 
quality level outcomes. impression level. In the  
160:01 - next video, we'll learn how to execute our Python 
project that we just created using the terminal.  
160:09 - In this video, we'll learn how to execute a Python 
project using terminal or Command Prompt. What we  
160:16 - did in the previous project has been imported 
below package. And then we iterated through all  
160:22 - the PNG files in our current directory. And then 
we converted all those PNG files into a JPEG.  
160:27 - But the problem with that in sharing the code 
is that someone has to have the knowledge of  
160:32 - Python to open the text editor, or to open pi 
charm and then run the code. To avoid that,  
160:38 - we can convert this enter code into a Python 
executable file, which is something that we have  
160:44 - already created in the previous project. So the.py 
file that we had created in the previous project  
160:51 - is what we are going to use in our terminal 
or bash, or shell, a Windows command prompt,  
160:57 - and then use that py file to execute and then 
convert everything within our shell itself. So  
161:04 - this way, we can convert the entire project into a 
single click command line utility. So let us first  
161:11 - get into our current working directory where we 
have got all the files under code. In my case,  
161:17 - image type conversion is where I've got 
image underscore conversion is where  
161:22 - I've got my code, and then project file to check 
that we can use their Linux command ls to see what  
161:30 - are all the files in our current directory. 
So as you can see, we have three PNG files,  
161:35 - and we have the Python file that we had created 
in the previous project. So to execute this file,  
161:41 - let us first copy this file name, right click 
Copy. And once the file name is copied, remember  
161:50 - whether you have got Python, or just Python three, 
which means if you have got Python less than 3.0,  
161:57 - you have to use this command Python. Or if you 
have to, if you have Python version greater  
162:02 - than 3.0, you have to use Python three. So in my 
case, I've got Python version 3.0. So I'm going  
162:09 - to use Python three here as the first command. And 
then I'm going to put my file name which is image  
162:16 - underscore conversion dot p y. This command is 
going to execute image underscore conversion.py,  
162:23 - which will iterate through these PNG files and 
create new JPEG files. So let us see. So as we  
162:29 - have executed, in just a microsecond, this nta 
code has got executed. And then we can see that  
162:36 - the pipe PNG files have got listed, and then 
the code is completely successfully executed.  
162:41 - Now let us see whether the new files are there 
available in the current working directory.  
162:46 - So to check that we can use the same command ls, 
and then see, as you can see, we have new files,  
162:52 - which has this extension called JPG and JPEG and 
jpg. So let us go to the folder where we have got  
163:02 - all these files. As you can see, initially, we 
had only PNG files. And after this execution,  
163:08 - we have all these JPG files. To just verify this, 
once again, we'll delete all these JPG files.  
163:16 - And then we'll go back and then check using 
a list. Yes, there is no JPG files in this  
163:21 - current directory. And then we'll run the same 
command again, to execute the image conversion  
163:28 - Python file. I'm going to use Python three 
because my Python version is greater than 2.7.  
163:35 - So once we executed this, we can go to the current 
working directory, and then see that we have got  
163:40 - new JPG files. And as we see last time, this 
Batman Lego has got no background, which is one  
163:48 - of the properties of PNG files we saw but alpha 
and the JPEG version of the same Batman Lego has  
163:55 - got a black color background, which means the JPG 
file has replaced the transparent background with  
164:02 - a black color. So this project, we learned how to 
create an image type converter using Python using  
164:10 - the library pi L, which stands for Python image 
library. And then we converted that code into a  
164:18 - shell script where we can execute in one line that 
the entire conversion will happen for all the PNG  
164:25 - files in our current working directory. Thank 
you for listening. See you in the next section.  
164:31 - In this section, we'll learn how to build an 
automated new summarizer. The reason we call it  
164:36 - an automated new summary is because the machine 
learning algorithm is doing the summarization  
164:41 - technique for us with no manual effort of going 
through the long text of news. News summarization  
164:47 - is nothing but the text summarization of news. 
We'll start seeing an introduction to text  
164:52 - summarizer and its techniques. we'll implement 
one such text summarization procedure and Python.  
164:58 - With that, we would have extracted this summarise 
text of the news and thus we would have got our  
165:03 - automated news summaries are in place. Let us 
go ahead and start seeing the course section.  
165:09 - In this video we will learn about take 
summarization. Take summarization is the process  
165:14 - of extracting meaningful text that is shorter 
in length from a huge chunk of larger text.  
165:19 - Using algorithms powered by natural 
language processing and machine learning.  
165:23 - Take summarization is actually one of the most 
exciting fields in machine learning and natural  
165:28 - language processing, which is NLP. Automated text 
summarization allows engineers and data scientists  
165:33 - to create software's and tools that can 
quickly find and extract keywords and phrases  
165:39 - from documents thus creating a summarized takes 
take summarizes are implemented in a variety of  
165:44 - web applications and mobile applications 
to deliver summarized content, or news.  
165:50 - And the example is there is an app in short, which 
is one of the most popular news apps in India  
165:55 - that delivers a summarized text from a larger 
use. Take summarization usually are of two types,  
166:02 - the technique is of two types. The first 
one is extraction. And then the second  
166:06 - one is abstraction. The extraction technique, the 
automated system or algorithm that we have built,  
166:12 - extracts objects from the entire collection of 
text without modifying the objects, which means  
166:18 - it extracts key phrases from the entire text that 
we have given and then ranks those sentences based  
166:24 - on its importance. And then finally gives a text 
summarized format of text using only those most  
166:31 - important sentences. So in this case, there is no 
modification of objects that are present inside  
166:37 - the actual text that has been provided. The second 
technique is abstraction, under abstraction,  
166:44 - instead of just merely copying the information 
from the given text. What it does is it  
166:49 - actually pair up phrases the entire section. So 
it takes the entire text, paraphrases the section,  
166:55 - and then finally identify key words and then key 
phrases, and then uses natural language processing  
167:02 - to create a new text which is more meaningful, 
and also covers the context. And then finally,  
167:07 - it uses the summarized text of the original text 
that is given. So there are two techniques. The  
167:13 - first one is extraction, which actually returns 
the entire document without modifying the objects  
167:19 - but a minimalistic version of using only important 
sentences. The second one is it breaks down the  
167:25 - entire sentence, multiple objects, and then it 
builds meaningful sentences using natural language  
167:30 - processing by paraphrasing the sections, and then 
we get the summarize text. In the next video,  
167:36 - we'll learn about what kind of technique we 
are going to use in this particular project,  
167:41 - what Python package we are going to use for that 
purpose, and how to install that Python package to  
167:46 - proceed further. Thank you. In the previous video, 
we learned a bit about text summarization. In this  
167:52 - video, we will see what kind of text summarization 
Are we going to use, and then the Python library  
167:57 - required for that. So text summarization is 
of two types. As we saw in the previous video,  
168:02 - one is extraction. The second one is abstraction. 
So in this particular project, we'll use  
168:07 - extraction method for text summarization. And the 
Python package that we are going to use is called  
168:12 - Jensen. So the Jensen implementation of text 
summarization is based on a popular algorithm  
168:19 - called text rank. Text rank algorithm is a 
graph based ranking model for text processing.  
168:25 - An important aspect of text rank is that it 
does not require deep linguistic knowledge,  
168:30 - which means text rank model is highly portable 
to any other domain or language. Which means if  
168:36 - you have built a model using English language, 
you can use the same algorithm for a different  
168:41 - language without any further major changes. Jensen 
is also known as topic modeling in Python gensim  
168:50 - is a pythonic library for topic modeling, document 
indexing and similarity retrieval for large text  
168:56 - corpora. The target audience for Jensen is the 
one who uses natural language processing and  
169:02 - information retrieval or information extraction. 
gensim is particularly one of the most popular  
169:09 - Python libraries especially for topic modeling. 
gensim is been open sourced by a company called  
169:15 - Ray technologist or a ray technologists, let us 
see how to install gensim on our computer, open  
169:22 - your terminal or if you're using Windows, open 
your command prompt. And then type crypt three as  
169:28 - we have seen before, if you're using Python 
three, you need to type PIP three install  
169:33 - if you already have got gensim so use for 
upgrade Jensen, once you type this on enter  
169:39 - gensim would start getting downloaded on your 
machine as Jensen is related to topic modeling and  
169:45 - then much more. There are many pre trained models 
that comes with gentlemen language models also.  
169:51 - So for that purpose gensim is quite heavy and it 
will take some time to get installed. So as we can  
169:55 - see gensim has got installed successfully. So let 
us clear that up. mil and open our Python console  
170:02 - to see if Jensen has been successfully installed. 
So import gensim Yeah, as you can see gensim  
170:10 - has been successfully installed without any 
error. So we can now exit Jensen. So Jensen  
170:15 - is the library that we are going to use for text 
summarization, automatic summarization. But for  
170:21 - us to get the text itself because we are going to 
do new summarization, we need to extract the text  
170:28 - from news which means the news is published on 
internet and we need to extract text from news  
170:33 - and to extract text from news we are going to use 
something called a beautiful so as we have seen in  
170:40 - the previous sections beautifulsoup is the library 
that we are going to use for extracting text from  
170:46 - internet. As we can see, it could be from HTML or 
XML. So Beautiful Soup is the library that we are  
170:52 - going to use for web scraping, which is to extract 
text from internet to install beautifulsoup we  
170:59 - need to type pip install Beautiful Soup for so 
let us once again open our terminal or shell  
171:06 - and clear the text that we have gotten there and 
then installed beautiful so PIP three install  
171:12 - Beautiful Soup work because it is the latest 
version of Beautiful Soup. So we have to type  
171:17 - Beautiful Soup for so as you can see here the 
terminal that I have already got Beautiful Soup  
171:23 - installed in the previous section. So my Beautiful 
Soup requirement has been already satisfied.  
171:28 - If you have not got beautiful soup, Beautiful Soup 
would get freshly installed on your machine. Let  
171:34 - us open our Python console. And then try to 
see if we have got Beautiful Soup installed.  
171:40 - So let us save from bs for import Beautiful 
Soup, which is the object of our interest. So  
171:47 - as you can see beautifulsoup the object from the 
package beautiful. So for it's been successfully  
171:54 - imported. So remember this always when we 
install Beautiful Soup, it should be installed  
171:59 - as Beautiful Soup for all and small critters. 
But when you are importing Beautiful Soup,  
172:04 - it is from this package bs four and then you input 
the object beautiful. So let us exhibit evil soup.  
172:11 - So in this video, we learned a little bit about 
the kind of text summarization technique that we  
172:18 - are going to use in this project. The package name 
Jensen that we are going to use in this project,  
172:24 - how to install Jensen, and then a little bit 
about Jensen and we also saw that we need  
172:28 - beautifulsoup the Python package beautifulsoup 
for web scraping, which is to extract  
172:33 - text from the web, you are the news 
that we are going to be of our interest.  
172:39 - In the next video, we'll learn how to extract the 
news text from the internet using beautifulsoup.  
172:46 - In this video, we'll see how to use Beautiful 
Soup the Python package Beautiful Soup  
172:52 - to extract text from the internet news source. 
So to begin with, let us import the packages  
172:58 - that we want for text extraction. The first one 
is beautiful. So as we know before Beautiful Soup  
173:04 - is a web scraping library that we are going 
to use to extract text from a new source. The  
173:10 - second package that we are going to use is called 
request. The request package is used to extract a  
173:16 - web content Beautiful Soup is used to parse the 
text content that we extracted using a request  
173:22 - package. So after we import the packages from bs 
four input Beautiful Soup from request input gate,  
173:30 - so the reason why we are importing a particular 
object or a function from a package instead of  
173:34 - loading the entire package is because memory 
management if we have got a huge package like  
173:41 - Jensen, and if we import the entire package, then 
the primary memory will be occupied with a huge  
173:47 - memory chunk of this particular package. So it 
is always better to import only the packages  
173:53 - only the functions only the objects that are our 
interest rather than importing the entire package.  
173:58 - So likewise, Beautiful Soup object is of our 
interest and in the function request that we  
174:03 - wanted from the request package. So once we do 
that thing, let us create a custom function. The  
174:09 - purpose of this function is to extract one Li the 
text component, so extract only the text component  
174:16 - from paragraph text. So in a typical website, a 
web URL you will see text spread across different  
174:23 - tags. It could be in a span tag, it could be in 
a h1 tag h2 tag h3 tag, or it could be in a div,  
174:30 - it could be anywhere. So the tag that is of our 
interest is paragraph tag, which is a HTML tag  
174:36 - which is denoted by this symbol P. So what we are 
going to do is we are going to create a custom  
174:42 - function. The first step in the function is the 
function is reading the URL. So the function is  
174:48 - trying to get a sense HTTP request to get the URL 
using request package and then it stores it in  
174:54 - page. Once it stores it in page we are going 
to use beautifulsoup to parse the content Using  
175:01 - a XML parser. So we are using l XML parser to 
parse the content that we just extracted using the  
175:09 - get function and then we are storing it in soup. 
So next step is we are going to identify why only  
175:15 - the paragraph text, and then we are going 
to save it in text. The reason we are using  
175:20 - lambda function is to iterate these four different 
tags. The reason is, because in a particular page,  
175:26 - there could be different paragraph text. So 
we are using soup dot find all to find all  
175:32 - the paragraph text and then we are using lambda 
function to iterate this page or text to all this  
175:37 - paragraph function that we found out. And then 
we are finally using join to join all the text  
175:42 - and store it in a text object. The next thing is, 
it is always good to present the title. So what  
175:48 - we are going to do is, apart from the text that we 
extracted, we are going to extract the title also,  
175:53 - the reason we are using soup, low title, dots, 
strings is because the title might sometimes  
176:00 - come with escape strings like slash in the denotes 
end of line or slash tab that denotes a tab space.  
176:07 - So to strip out those strings, we are using soup 
dot title dot strip strings. And then again, we  
176:12 - are using join to join all the words that we have 
got and then store it in title. And then finally,  
176:18 - we are returning this as title comma text, 
which will be read as a tupple. So finally,  
176:25 - we are sending it as a tupple. And then the 
custom function that we wanted to create is done,  
176:29 - the function name is get underscore onely 
underscore txt, and then the argument that we  
176:34 - are passing is the URL that we want to extract 
the text from. So once this function is done,  
176:40 - let us execute this. And then the next step is the 
news URL from which we are going to extract the  
176:46 - text. This is the URL from where we are extracting 
the News, the news is from a very popular media  
176:53 - publishing site called works. The news title is 
California is cracking down on the gig economy.  
176:58 - And then this is what we are going to 
use to extract and summarize text from  
177:02 - once we take this URL, and then we put this URL 
within the function that we just created. So  
177:08 - you can see, the function that we created is 
called get underscore only underscore text.  
177:13 - And then we are passing this URL as a character or 
string, the string argument is passed on to this  
177:20 - URL. And let us execute this URL. As you can see, 
it just finished executing. And let us print the  
177:26 - text object to see if it has successfully scraped 
extracted the text. So you can see that the text  
177:33 - is available now. And let us just see how large 
this text is. In other words, how many number of  
177:40 - words we have got. So once we execute this thing, 
the way we find it out is we take the string,  
177:46 - and then we split the strings using str dot 
split as words and then we are using the length  
177:53 - of the words to see how many words we have got. So 
you might doubt Why have I used text one instead  
178:00 - of just text. So as we created the function before 
I mentioned that we are returning it as a tupple.  
178:07 - A tupple is a different type of Python object 
from a list. So a tupple is an immutable  
178:12 - object. So the reason we have returned it as 
a tupple is because we wanted to return title  
178:18 - and takes in the same expression. So that is 
why we have returned both. So if you see text  
178:27 - length of text, it will show you two which means 
it is a tupple with two objects in it. So you can  
178:34 - also see it opens with an open bracket and it ends 
with a close bracket as opposed to a list which  
178:41 - would open with a square bracket and close with 
the square bracket. And that is the reason why  
178:45 - we are excluding this because text of zero would 
be the title and text of one is the actual text  
178:52 - that we want. So text of zero would be the title 
and the text of one is what we actually wanted.  
179:00 - And that is why we are using text of one to use 
string split and then split it by words and then  
179:06 - calculate the number of words entered. So in 
this video, we learned how to use beautifulsoup  
179:13 - and request to extract the text from the new 
source. And we also saw what is the length of the  
179:21 - text the extracted text which is 1600 25. In 
the next video, we will learn how to do text  
179:28 - summarization using Jensen. In this video, we'll 
see how to do text summarization automated text  
179:36 - summarization of that extracted text 
that we did it in the previous video.  
179:40 - So as a first step, we need to import all the 
required Python packages. The Python package that  
179:46 - we are going to use here is Jensen as we know 
before and within gensim. We are interested of  
179:52 - two functions one is summarize the second one is 
keywords. So from Jensen, we are going to import  
179:57 - summarize and keywords. So Say do from Jensen dot 
summarization towards summarizer import summarize,  
180:06 - and then from Jensen dot summarization import 
keywords once you have typed this in execute  
180:12 - your Jupyter notebooks. So once this is 
executed, your Python package has been  
180:17 - successfully imported. So the next step is for 
us to do text summarization, text summarization,  
180:24 - as it might look very complicated gensim is 
offering us this in a single function. So, how  
180:30 - we can do this is the first one is the function 
called summarize, and then we have to pass the  
180:35 - text to this. So, the text that we have extracted 
in the previous step as we saw as a tupple. So,  
180:43 - what we have to do is we have to do text dot 
one, and then the summarize function as a first  
180:49 - argument it takes text the actual text content 
as a second argument it actually takes ratio.  
180:55 - And then the third argument is word count. The 
thing with this is it can be either done with  
181:00 - ratio or with the word God not with both. So, if 
you supply both with means ratio, let us say ratio  
181:07 - is equal to point 01. So, the ratio is nothing but 
the ratio of text summarised text that you want,  
181:14 - as opposed to the original text. So, in text 
summarization, we use summaries as a function,  
181:20 - we can either use ratio or word count as 
argument to extract to limit the amount of text  
181:27 - that we wanted. So, let us start with the first 
method, which we'll use using word count. So,  
181:35 - using summarize function, we are passing the text 
which is of text one, and then we are using the  
181:40 - word count that I want us 100 which means I do not 
want more than 100. So, in the previous video, we  
181:46 - saw that we have totally 1600 25 words, and then 
we are going to extract only 100 words out of it,  
181:54 - which is meaningful as summarized text. So, to 
make it a little bit, the cosmetic changes to  
182:00 - the output, we are going to say okay, this is 
the title and then we are using text of zero  
182:05 - to print the title and then we are saying 
this is somebody and then this embrace text.  
182:09 - Once we execute this thing, we see that we 
have got the title of walks. And then we are  
182:15 - getting the summarized text which says okay, 
the State Assembly has passed a bill which  
182:20 - makes it harder for companies to label worker as 
an independent contractor instead of employees,  
182:24 - which is what usually happens in gig economy. 
So, as a first step, we have extracted our text  
182:30 - and the number of words that we have got is let us 
say, is 98. So we limited our word count to 100.  
182:39 - And then we have got 98 words. So this is the 
first method where we have extracted the text  
182:47 - using word count as an argument to put a 
threshold. In the second method, what we are  
182:52 - going to do is we are going to say I do not want 
to put a threshold of word count, what I want is  
182:57 - I'm giving a ratio, the ratio can be anything 
between zero and one. And then within this,  
183:03 - it will use as an approximation as the ratio of 
the original text which is 1600 25. And then given  
183:09 - this ratio, it will give us how much text that we 
are going to be given. So what we are going to do  
183:16 - is we are going to use the same function, text 
off one because we just wanted the text and then  
183:21 - we are going to say ratio of point one and then 
see how much are we getting. So you can see that  
183:27 - you've got the title, and then you've got the 
text. If you want to reduce this text further,  
183:32 - we can say instead of point 01, we can say point 
zero, let's say 7.07, which further reduces their  
183:40 - takes. If you are interested in reading more 
text instead of point 01, we can say point  
183:46 - two which is giving us more text point one giving 
us less text. So as you can see that we have  
183:54 - extracted the summary using a different 
method other than specifying the word count.  
183:59 - But still it is good for us to see how much it 
has reduced. So what we'll do is we'll copy this  
184:05 - function, which is where we have got the output 
text. And then we'll say we are going to put it as  
184:11 - summarized. Text is equal to this is an we'll 
execute this once we execute this, we have got  
184:19 - the summary sticks. Now, we are going to use the 
summarize text and then put it inside our text  
184:26 - length function to see how many words we have 
got we have got 217 words. In the previous  
184:32 - muttered we got 98 words because we had set a 
threshold using the word count. In this method,  
184:38 - we have set a threshold using the ratio and then 
we have got 217 words. So this is how we have  
184:44 - extracted summaries text, but sometimes not just 
summarize text is enough but you also want to see  
184:51 - the keywords that are more important that the 
algorithm has found out. So what we are going  
184:56 - to do in this step is we are going to extract 
the keywords that is of more importance. And  
185:00 - we are going to use the same method. So, we are 
going to say keywords, which is the function name  
185:06 - for the first argument is the text that we are 
passing the original takes. The second argument  
185:10 - is we are setting a threshold using the ratio. And 
then the third argument says whether you want to  
185:15 - do lemmatization or not. So, to understand what is 
lemmatization So, let us first exude this function  
185:22 - without lemmatization function. So, once you 
exude this thing you might see okay the critical  
185:27 - keywords are drivers, code codes, workers, 
worker states state contractors contractor  
185:35 - So, even though it gives us keywords, 
sometimes you might see the reputation  
185:39 - one has quote unquote, state and states 
contractors and contract This is because we have  
185:47 - not done lemmatization lemmatization is a process 
of taking a word and then converting it to a root  
185:53 - word. For example, workers would be converted 
into worker coats would be converted into code  
185:59 - contractors would be converted into contractor 
which means we would not see duplication because  
186:03 - of that just one extra word. So, let us go ahead 
and then do lemmatization Lemma ties through,  
186:11 - it's a Boolean flag. Once we execute this thing, 
now, we have got the new set of keywords, which is  
186:16 - just quotes Ober, contractor, business worker, 
and there is no plural form and singular form  
186:23 - because we have done lemmatization we can do this 
for that other method also. So in this video, we  
186:30 - have learned how to successfully do automated text 
summarization using the Jensen function summarize  
186:36 - and also we learned how to do it using two 
different methods. One is using ratio, the second  
186:42 - one is using word count. Apart from this, we also 
learned how to do Tech's keyword extraction. So  
186:49 - one is text summarization. The next one is keyword 
extraction. To find out more relevant or important  
186:55 - keywords, for example, if you are going to run a 
Google AdWords campaign for this particular text,  
187:00 - then you need to understand what are the keywords 
critical keywords that are presented in this  
187:04 - particular word, text, and then we'll run the 
campaign we'll do the bidding accordingly.  
187:09 - So in the next video, we'll learn how to make this 
a complete project and then we'll see a summary.  
187:17 - In this video, we'll see how you can take 
this further forward. So, as you can see,  
187:22 - what we have done is we have used a Jupyter 
Notebook to create this particular project.  
187:26 - So Jupyter Notebook is good if we want to 
incorporate text, which is narrative as markdown  
187:32 - and also the code and also the output. But Jupyter 
Notebook is not applicable for every purpose.  
187:38 - So for example, let us take that you want to do 
this project as an automation. So what you want  
187:43 - to do is you want to take a particular URL, and 
then you want to shedule this in your computer  
187:48 - to get the summarized text from a news 
source every day, let us say morning.  
187:52 - So in that case, what you can actually do 
is you can go to your Jupyter Notebook file,  
187:57 - and then you can download this Jupyter Notebook 
as a.py file. So instead of having it as a Jupyter  
188:03 - Notebook, you can download this as a.py file, 
which means you are going to get a Python code.py  
188:10 - file, which you can use your Windows Task 
Scheduler or Mac Automator, to schedule it  
188:16 - every day at a certain point of time. So this is 
one thing that you can do this in this project.  
188:21 - The second thing, what you can do is, instead 
of a Jupyter Notebook, you can use this Jupyter  
188:27 - Notebook to create a more generalized version of 
this project. And then you can convert it as a  
188:32 - command line project. So where you can just 
invoke this project with an argument of URL,  
188:38 - and then it will give you a summarized text. 
So in this project in this URL, if you see,  
188:44 - we have hard coded the URL. But instead of hard 
coding the URL, what you can actually do is  
188:50 - you can use this as an argument that could be 
paused at the command level. And then what you  
188:54 - can do is you can use your terminal to invoke this 
Python project, passing the desired URL in the  
189:01 - terminal as an argument and then you can extract 
the text as an output. So there are two things  
189:06 - that you can do further with this project. One is 
extracting the Python code and then scheduling it  
189:12 - so that you get the news into your inbox every 
day. Or instead of that you can have it as a  
189:16 - Python project, the same.py file, but in a more 
generalized format instead of hard coding the URL.  
189:22 - And then you can get the extracted summarized 
text whenever you supply a URL to this particular  
189:28 - project. But if you do not wish to do any of 
these things, you can just keep it as this Jupyter  
189:33 - Notebook. And then what you can do is whenever you 
want to change the URL, you can give a different  
189:37 - URL here and then there under text and the text to 
this algorithm, this notebook would give you the  
189:42 - desired output where you can use to read the news 
of a summarized format. Like just like headlines  
189:48 - are how the in shorts app which we saw that 
most popular news application in India would  
189:54 - be doing to send a card of text, which is a 
very summarized version of the actual news.  
190:00 - In this video, we'll learn how to take this 
project further. So far. In this section,  
190:06 - we learned how to build an automated new 
summarizer. We started with understanding  
190:12 - what is the new summarizer and what kind of 
techniques are available. And then we learned  
190:16 - about the Python packages required for it. And 
then finally, we did new summarizer using Jensen  
190:23 - and the text extracted using beautifulsoup. I 
hope you enjoyed the project. Thank you very much.