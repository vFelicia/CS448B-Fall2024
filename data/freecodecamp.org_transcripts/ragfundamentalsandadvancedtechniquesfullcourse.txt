00:00 - this course will guide you through the
00:02 - basics of retrieval augmented generation
00:06 - or rag starting with its fundamental
00:08 - concepts and components you'll learn how
00:11 - to build a rag system for chatting with
00:14 - documents explore Advanced Techniques
00:17 - and understand the pitfalls of naive rag
00:20 - Paulo created this course he is a senior
00:23 - software engineer and experienced
00:25 - teacher in this video I'm going to go
00:27 - through a quick introduction of rag rag
00:30 - stands for retrieval augmented
00:33 - generation now if you have never heard
00:35 - of rag no worries that's what I'm going
00:38 - to be doing in this video the main idea
00:41 - is that when you use a large language
00:43 - model but a large language model
00:45 - essentially is a model that was trained
00:47 - on certain um data so for instance if
00:51 - you go to chat GPT and you type in what
00:54 - is the capital of France and of course
00:56 - it will give you the capital of France
01:00 - because it was trained on information
01:02 - about including in this case the
01:05 - capitals of countries in the world but
01:08 - if you were to ask chaj what is the name
01:11 - of my first dog of course chaj wouldn't
01:14 - know because it's using that large
01:16 - language model the model that was
01:19 - trained on something that is not related
01:21 - to your information information that it
01:24 - is particular to you that is specific to
01:27 - you and that of course is an it a
01:29 - problem and and rag essentially allows
01:31 - us to take our own information our own
01:33 - data databases video textol information
01:37 - RW data or unstructured data as they
01:40 - call it sort of inject to the large
01:42 - language model so now the large language
01:45 - model has more information including
01:49 - your own information and so now when you
01:51 - ask questions related to your specific
01:53 - data you are able to get the answer from
01:56 - the large language model because it's
01:58 - able to connect to your data that you
02:00 - have injected happy day so you get the
02:03 - right answer so that is the idea of rag
02:05 - so that's what we're going to be doing
02:06 - this mini course or in this video and I
02:09 - hope you enjoy it all right let's go
02:11 - ahead and get started in order for you
02:12 - to follow along in this course you need
02:14 - to have your development environment
02:17 - setup particularly I expect you to of
02:19 - course have python setup on your machine
02:22 - also vs code or any other code editor of
02:25 - your preference but I will be using vs
02:27 - code so I would encourage you to also
02:29 - use code but that is not a requirement
02:32 - also make sure that we have an open AI
02:34 - account which means uh you also need to
02:36 - create have an API key that way you're
02:39 - able to follow along if you want to
02:42 - actually do the Hands-On with me which I
02:45 - believe I want to believe that's what
02:47 - you're going to be doing so go ahead and
02:48 - have all those things set up and we
02:51 - should be good now for you to set up the
02:53 - open account again you can just go to
02:56 - open.com and go through the process if
02:58 - you haven't done that already ready and
03:00 - just set you up create an account and
03:02 - create an open API key which then we'll
03:05 - be using in this course and if you are
03:09 - wanting to install python you don't have
03:12 - python installed it's very simple just
03:13 - follow this link and they have all of
03:16 - the things or all of the directions you
03:18 - will need to set up python on your
03:21 - machine so I encourage you to go through
03:23 - that and have everything set up okay so
03:25 - they have python for Windows Mac Linux
03:27 - and everything this is all in case you
03:29 - don't have anything set up but go ahead
03:32 - and do that if you don't have that set
03:34 - up and I'll see you
03:37 - next all right so let's go ahead and
03:39 - start doing the Deep dive on rag so I
03:43 - know that most of you who are here may
03:46 - already know what rag is and that's
03:47 - wonderful but I'm going to do just a
03:50 - quick Deep dive overview so that we are
03:53 - have some sort of a summary overview
03:56 - again of what rack is so we're going to
03:59 - look at what is rag the motivation
04:01 - behind Rag and also
04:04 - advantages um now what is rag rag stands
04:08 - for retrieval augmented generation so
04:11 - the key points here is that we have
04:13 - retrieval augmented and generation these
04:16 - are the key points here retrieval
04:18 - augmented and generation so the idea is
04:21 - that we have a system that retrieves
04:23 - information we have also way of
04:25 - augmented whatever we are passing
04:27 - through as well as then push that
04:30 - information into a machine quote unquote
04:32 - that will generate a result so rag has
04:37 - two main components which is the
04:39 - retriever the retriever what it does it
04:41 - identifies and retrieves relevant
04:44 - documents and then we have the generator
04:46 - well it takes retrieve documents and the
04:49 - input query to generate coherent and
04:52 - contextually relevant responses because
04:55 - that is the whole idea to get coherent
04:57 - and contextually relevant responses
05:00 - these are the main components of rag but
05:03 - we still haven't defined rag really so
05:05 - what is a rag so the definition will go
05:07 - as follow a framework that combines the
05:10 - strengths of retrieval based systems and
05:13 - generation based models to produce more
05:16 - accurate and contextual relevant
05:18 - response and we have the keys again the
05:20 - keywords contextual relevant response
05:22 - but that is the whole uh goal of rag
05:26 - okay that sounds great but translating
05:27 - all of that we would say efficient way
05:29 - to customize an llm a model language
05:32 - model large language model with your own
05:35 - data well what that means is what are we
05:37 - doing really is that as we know a large
05:39 - language model like GPT and many others
05:41 - out there they only know so much okay so
05:45 - what we doing is we we are injecting our
05:47 - own data into this large language model
05:50 - so that it knows more than the things
05:52 - that it knows that was trained on so now
05:55 - the large language model is going to
05:57 - know about specific contextual data in
06:00 - addition to what it was trained on let's
06:03 - look at an overview of rag who have
06:06 - documents these documents are cut into
06:09 - small chunks and then these chunks are
06:12 - put through an embedding large language
06:14 - model so to create embedding essentially
06:17 - and then that is what is created
06:20 - embeddings and those embeddings are set
06:22 - okay so now the question or the query
06:25 - comes in goes through the same process
06:27 - transforms to embedding and then we have
06:29 - have this embedding which then is used
06:31 - to go ahead and find in our retrieval
06:34 - system in our Vector database most
06:36 - similar items which then is pushed into
06:40 - a general large language model which
06:42 - knows how to take that information in
06:45 - this case the most similar results with
06:47 - the question this case The Prompt and
06:50 - get the response that is needed that
06:53 - we're looking for so that is how a rag
06:56 - Works notice here when we say rag
06:58 - retrieval augment generation that means
07:01 - that the generated response is augmented
07:04 - by the data I retrieved from the
07:06 - documents in our case hence the name rag
07:09 - so really if you want to do a deep dive
07:12 - into naive rag this is what happens so
07:14 - we have the documents and these
07:15 - documents are going through the phase of
07:18 - parsing and pre-processing so
07:20 - essentially cut them up into smaller
07:22 - documents uh this is the chunking
07:24 - process and then we pass them around
07:26 - into smaller chunks and those are passed
07:29 - through through the embedding model to
07:31 - create vectors out of these chunks okay
07:34 - so we're vectorizing those chunks and
07:37 - then that is what it's saved into a
07:39 - vector store or a vector database so
07:41 - this is the part of indexing that
07:44 - happens here of course is the indexing
07:47 - part as I have shown you this is the
07:49 - part where we cut the documents and
07:52 - pre-process everything and chunk it up
07:55 - and then create those embeddings or
07:57 - vectorize those chunks and save them
08:00 - into a vector store and then what
08:03 - happened is then we have a user who has
08:06 - a query or question of some sort and
08:08 - that also has to go through the
08:10 - embedding model to vectorize that query
08:14 - and then that is actually what is what
08:16 - is sent to search into the vector
08:18 - database so we have vectors and vectors
08:21 - that are easily uh used in a fact
08:25 - database to do all sort of things mainly
08:27 - to search and then the information is
08:30 - retrieved the relevant documents are
08:32 - retrieved or packed up in this case with
08:34 - prompt as well as the relevant documents
08:37 - as I said and the query but notice here
08:40 - this is the different part phase this is
08:42 - the augmentation phase of the rag so we
08:45 - augmenting we're adding something to
08:48 - what we had before so not only we have a
08:50 - query but we also have prompt which is
08:52 - part of the query and relevant documents
08:54 - and so forth okay so once that is
08:57 - augmented we pass that information
08:59 - through a large language model so it
09:02 - could be any kind of large language
09:04 - model and then that's when the response
09:06 - is generated which is then returned to
09:08 - the
09:10 - user all right so now you have the
09:12 - basics of understanding what rag is how
09:15 - really rag works the idea is that we
09:18 - have our own documents we're going to go
09:20 - through the process of extracting those
09:22 - documents splitting those up and then
09:25 - pass them through the large language
09:27 - model of course we're going to be saving
09:29 - that into a vector database now if you
09:32 - don't know what a vector database is I
09:34 - actually have yet another video where I
09:36 - talk about Vector databases somewhere at
09:38 - the top here okay so go ahead and check
09:39 - that out so we're going to do a Hands-On
09:41 - here while I'm going to show you how to
09:43 - use rag to create a system a rag system
09:47 - that allows us to pass through some
09:50 - documents in this case we're going to be
09:52 - a bunch of articles that we going to be
09:55 - reading in saving those to a vector
09:57 - database and then form rag to start
10:01 - conversing or in this case querying our
10:04 - documents so we can ask questions and
10:07 - get the correct answers along with the
10:10 - large language model in this
10:11 - demonstration here I'm going to be using
10:14 - open AI which means that you need to go
10:16 - and have an openai API key for you to be
10:20 - able to do this with me now if you don't
10:22 - want to use open AI you can use other
10:24 - large language models out there and
10:26 - things will be a little bit different of
10:28 - course but the main idea is going to be
10:30 - the same all right okay let's go ahead
10:32 - and get started and so I have this
10:35 - project called rag intro and have a few
10:38 - things here one of the important things
10:39 - is that you have here the openi API key
10:43 - so you need to get that you need to have
10:45 - that then of course I have the app.py
10:48 - which is empty at this point so this is
10:49 - where we're going to start doing our
10:51 - magic now before we do that I need to
10:53 - make sure that I have a virtual
10:55 - environment if you want to learn more
10:57 - about python I have a full video of 1
11:00 - hour or so that's you can go ahead and
11:02 - check it out also you'll see somewhere
11:05 - here all right or you can search on my
11:08 - channel you will find that all right so
11:10 - I have my virtual environment created
11:13 - there and let's go ahead and say Source
11:15 - VNV and activate that real quick so we
11:18 - have that set up so now it's active we
11:21 - are going to install a few dependences
11:24 - the first one that I need here uh let's
11:26 - see I have my cheat sheet here I have so
11:31 - the first one that we need is the
11:33 - python. EnV so pip install I'm going to
11:37 - pass that this is going to allow us to
11:40 - retrieve information from our virtual
11:44 - environment file okay and then next I'm
11:47 - going to get the open AI because we're
11:50 - going to be using open AI so I say p
11:53 - install open
11:56 - AI okay and because of the nature of
11:59 - large language models and rag system we
12:02 - need to save this information this data
12:04 - that we're going to split up these
12:06 - documents into a vector database if you
12:08 - don't know what a vector database is I
12:10 - do have a course um that talks about
12:13 - Vector databases and so there are many
12:15 - kinds of vector databases we're going to
12:17 - be using chroma DB which is light and
12:20 - easy to use so I'm going to say pip
12:24 - install chroma DB as such so we have
12:27 - that set up for us
12:30 - I'm going to go ahead and import few
12:31 - things that I need here now just to make
12:34 - sure that this is fast I'm not going to
12:36 - type everything because you should have
12:38 - access to this
12:41 - code the OS because we're going to be
12:43 - needing that to access operating system
12:47 - folders and files and so forth and other
12:49 - functions I have chrom ADB and I have
12:51 - EnV here to load all of environment
12:55 - variables and of course I'm going and
12:57 - importing embedding function we're going
12:59 - to use that to embed create embeddings
13:02 - because those are the representations of
13:04 - our data that need to go that we need to
13:07 - have in order to put that into our
13:09 - database Vector database of course we
13:12 - have open AI here which we're going to
13:13 - be using soon right all right so next
13:15 - what we'll do here I'm going to load all
13:18 - of our environment variables and then
13:20 - I'm going to set up the open key in the
13:24 - my from my environment variable as I
13:27 - said from our environment
13:31 - variable from our environment file there
13:35 - and then what we're going to do is we're
13:36 - going to create the function the
13:39 - embedding function this is what it's
13:41 - going to allow us to create those
13:44 - embeddings again once we chop up all of
13:47 - our data which I'm going to show you in
13:49 - a second here we want to transform that
13:52 - into embeddings these zeros and ones
13:55 - Vector space and then that is what's
13:57 - going to be saved into the vector
13:59 - database the chroma Vector database and
14:03 - when you do that when instantiating this
14:05 - embedding function you need to P pass
14:07 - the API key OPI key because it needs to
14:09 - know what model it's going to be used to
14:12 - do that and we're going to pass the
14:14 - actual model name which going to be text
14:15 - embedding three small so this is just a
14:17 - very small light embedding embedding
14:22 - functional system that allows us to
14:24 - embed create embeddings and next I'm
14:26 - going to go ahead and of course
14:27 - initialize the chroma client persistence
14:30 - so I want to be able to persist or in
14:33 - this case I want to be able to save the
14:35 - actual database now looking at the data
14:38 - here you see that I also have these news
14:40 - articles so this is where I have all of
14:43 - these articles news articles that I
14:45 - found online these is what we're going
14:47 - to be using as the documents and then we
14:51 - are going to chop it all up put it into
14:55 - a database right not a normal database
14:58 - this is going to be be a vector database
15:00 - and then we're going to use other
15:01 - techniques to start conversing talking
15:05 - and getting the documents that we need
15:07 - to answer the questions that we are
15:09 - asking so what are we doing here we are
15:12 - initializing the chromer client you can
15:14 - see it's very simple really you say
15:15 - chroma persistent client and then we
15:19 - pass the path we want this to have now I
15:23 - said chroma persistent storage this is
15:25 - kind of long but you can make it shorter
15:26 - if you want and then collection name add
15:29 - whatever name we want and then now we
15:32 - actually say chroma get or create
15:34 - collection which means this function
15:36 - allows us to create the actual
15:38 - collection collection is just a table or
15:40 - document where we can put all of these
15:42 - documents or tables in this case Okay
15:45 - and then we need to pass the embedding
15:46 - function notice that now we are passing
15:48 - what we initiate or instantiated at the
15:51 - top here the actual open AI embedding
15:54 - function that is going to allow us to
15:57 - create those embeddings right vector
15:59 - of the vector embeddings along with the
16:03 - collection name there we go so now we
16:04 - have this collection that indeed we
16:08 - created with chroma all right so let's
16:11 - go ahead
16:12 - and create our client this is our
16:15 - openingi client we pass the API key and
16:18 - the openingi key of course so now we
16:20 - have our client we can do all sort of
16:22 - things meaning we can for instance say
16:25 - client uh dot I think it's
16:27 - chat. open what is that called
16:30 - completions and I can go and create and
16:33 - then here I can pass a few things such
16:35 - as the model I believe let's say model
16:38 - and I going say gpt3 turbo and I can
16:42 - pass messages and rolls and everything
16:45 - in fact let's just do that real quick
16:46 - here so you can see this client working
16:49 - you can see we have the messages and the
16:51 - system says your helpful assistant what
16:53 - is human life expectancy in United
16:55 - States well that's pretty good let's go
16:57 - ahead and see if this works making sure
16:59 - that you have everything set up of
17:01 - course so I'm going to say print
17:05 - actually let me put this say res or res
17:09 - like this and say I can say res that and
17:13 - I can go to choices and go to message
17:15 - and they get the content all right so if
17:18 - I go ahead and run this I should be able
17:20 - to get something so python like this so
17:24 - should let us know that indeed we have
17:26 - everything set up and we should get some
17:28 - result results in a second okay looks
17:30 - like we have some issues here rest
17:32 - choices message let's just go and get
17:35 - the response the whole payload I think
17:37 - I'm missing the
17:40 - actual object it's okay let's run again
17:43 - okay so we can see that we have the
17:44 - response chat completion and went ahead
17:48 - to went ahead and got as of 2020 the
17:51 - average life expectancy in the United
17:53 - States is around 78.8 years old and so
17:56 - forth okay so we're getting a payload to
17:58 - tell us that that this is actually
17:59 - working okay of course you can get to
18:02 - the actual payload if you want this
18:04 - content here if you go through I can go
18:07 - straight to content I believe like that
18:09 - okay same
18:11 - thing go ahead and run we should get the
18:14 - actual content I think okay there we go
18:16 - so now we get the actual content as of
18:18 - 2020 blah blah blah and so forth okay so
18:20 - at least we know that this is working
18:21 - that's all we really wanted that's not
18:23 - our end goal so I'm going to get rid of
18:25 - that okay so the first thing we need to
18:27 - do is of course to load our documents
18:29 - from our articles as you can see here we
18:31 - need to load all of them and then start
18:34 - doing something so I have all of the
18:36 - code already so that we don't spend too
18:40 - much time and I can and you'll have
18:42 - access to all this code anyway and what
18:45 - I'm going to do it's a function that I
18:47 - created before this so what are we doing
18:49 - here we're loading documents from a
18:50 - certain directory so I have a print
18:54 - statement here just to give us what's
18:55 - happening and I go through in this case
18:59 - I know that all of these articles
19:00 - documents end with txt they are txt
19:04 - files it's kind of hard to see you just
19:06 - have to believe me you can see here
19:08 - there are txt files okay so that's what
19:11 - we're doing we're going through and
19:13 - start loading all of them and return the
19:16 - actual documents right it's going to be
19:18 - a list of documents all right so the
19:20 - next thing is we need to split this
19:22 - documents once we get them we got to
19:25 - have to split them up so that then we
19:27 - can pass them through into our database
19:30 - so I already have a function that does
19:32 - just that you can see we pass the split
19:35 - the text and we say the chunk size 1,000
19:38 - and the overlap is 20 the overlap
19:40 - essentially says once we split these
19:42 - documents we want to make sure that they
19:43 - overlap right overlap like this that way
19:46 - the contextual meaning of each piece of
19:49 - text is overlapped which means it's kept
19:52 - the context is kept because once we
19:54 - split all this up these documents in
19:56 - small chunks they're going to be very
19:58 - distant so the more overlap we have you
20:00 - can see that the more context we'll have
20:03 - kept the less the less overlap so that
20:07 - is the idea really and so we just go
20:09 - through the splitting process and then
20:11 - we return the chunks okay so we have
20:13 - those functions we're going to be using
20:14 - soon and so now we're going to go ahead
20:16 - and load documents from the directory so
20:19 - the directory as you see here is uh
20:22 - let's
20:23 - see right I should have said news
20:26 - articles okay so I'm just going to
20:28 - remove this like that should have put
20:31 - under data but it's okay so is under
20:33 - news articles which is this guy here and
20:36 - it's going to go ahead and get all of
20:38 - them and then for your documents I'm
20:41 - going to go and then I'm going to call
20:43 - load documents from directory pass the
20:45 - path which is this one here and at this
20:47 - point we should have all the documents
20:49 - I'm going to go ahead and print real
20:51 - quick here so we can hopefully
20:53 - see we should have the length of the
20:56 - documents once we have those
20:59 - documents loaded right so in this case
21:01 - here you know that this should return a
21:05 - list of documents because documents is
21:07 - indeed a list okay we should have
21:09 - something so let's go ah save this and
21:11 - I'm going
21:12 - to quickly run
21:15 - again okay loaded 21 documents very good
21:18 - so it went and got all of those
21:20 - documents that we loaded in essentially
21:24 - all of these documents here so they're
21:26 - about 21 now once we have these
21:29 - documents of course now it's time for us
21:31 - to do something else what we need to do
21:32 - really is to
21:34 - get those documents split up so I'm
21:37 - going to go ahead and do that so now I
21:41 - create the split documents into chunks
21:43 - so I have a list and I go through those
21:46 - documents that we just received and then
21:48 - we call the split text split text is
21:51 - indeed what we have here so it's going
21:53 - to go through and return another chunks
21:55 - which is a list of those all those
21:58 - documents that are split but remember we
22:00 - have this overlap for each document to
22:03 - continue having the context okay very
22:07 - good so we do all that stuff and then I
22:10 - can go ahead and say print
22:14 - again split documents so this should
22:17 - give me all of the documents chunk split
22:20 - documents into the length of chunks okay
22:23 - because I should have something at this
22:24 - point let's run
22:27 - again okay so you can see it went
22:29 - through the process splitting docks
22:31 - splitting docks and this is telling me
22:33 - how many splits I got because that is
22:35 - what indeed I asked length and so forth
22:38 - okay so we know this is working which is
22:41 - essentially what we want okay so the
22:43 - next function I need here is a function
22:47 - that will generate those actual
22:48 - embeddings because remember once we
22:50 - split up all the documents we need to
22:52 - take those splits that we did here and
22:54 - create embeddings this is what's
22:57 - actually saved into our database into
22:59 - our Vector database so I have a function
23:01 - here that is going to be helpful for us
23:03 - to use so essentially what this does is
23:06 - we use the we use open AI to create
23:10 - those embeddings from the text okay
23:12 - that's what we're doing here so you can
23:14 - say client embeddings and create and we
23:16 - pass the text the pieces that we are
23:19 - putting through and then we say the
23:21 - model that we want to use to create
23:23 - those embeddings that's all we're doing
23:24 - here and then we get those embeddings
23:26 - and return them okay this is going to be
23:28 - help full in a second here then I'm
23:30 - going to generate the actual embeddings
23:32 - why because I already have the function
23:34 - so to generate embeddings we go through
23:37 - all the chunked documents yes you
23:39 - remember these guys here and then as we
23:42 - go through we call these get open ey
23:45 - embeddings and we pass the actual
23:48 - information through to then create a
23:51 - document embedding field so each time we
23:54 - go we actually creating those embeddings
23:56 - so we can actually print to see our
23:58 - embeddings so I can say Doc embedding
24:02 - and what will happen is let's go ahead
24:04 - and run real quick so you can see we're
24:06 - going to go through the whole process
24:07 - splitting and look at that it's creating
24:09 - those
24:10 - embeddings we'll take a bit and in a
24:12 - second here we should
24:14 - see actual
24:17 - embeddings so after a little while so be
24:19 - patient but this will take a while you
24:21 - can see now we have the embeddings for
24:22 - Vector spaces right and so there we go
24:26 - of all the documents now we have all the
24:28 - embeddings this is these vectors that
24:31 - actually we're able to then add to uh
24:34 - into the database so this is good so I'm
24:36 - going to go ahead and clear this so I
24:38 - just wanted to show you okay so now that
24:40 - we have our embeddings let's go ahead
24:42 - and comment this out so we don't run
24:44 - that let's go ahead and insert each one
24:47 - of these embeddings into our database
24:50 - okay so of course I have the code for
24:52 - that so for each one of these we're
24:53 - going to because we know this chunked
24:55 - documents which is what we have here has
24:58 - all of the information what we do now is
25:01 - that we going to get those chunks the
25:03 - real chunks before we embed anything add
25:06 - into our Vector their base and at the
25:09 - same time we're going to add the actual
25:11 - documents along with the embeddings so
25:14 - now we're going to have these chunks of
25:16 - the documents these little pieces not
25:18 - embedding these are just the text chunks
25:21 - and then we're going to have the actual
25:22 - embeddings they are going to be sitting
25:25 - on our database ah very cool then and
25:28 - we're going to create a function to
25:30 - query our documents so I have all of
25:32 - that and I'm going to just copy that and
25:35 - put it here so we don't have to do all
25:38 - the things so the idea here is that
25:40 - query documents now this query documents
25:43 - is very simple we pass in the question
25:45 - like tell me about GPT 4 something like
25:48 - that anything pertaining to our data
25:50 - that we've just saved okay and then we
25:52 - say how many results we're expecting to
25:54 - receive how many documents essentially
25:56 - because what will happen is we're going
25:57 - to be able to retrieve the documents
26:00 - corresponding to the query that we're
26:02 - passing in right so in the background
26:04 - what will happen is the database is
26:06 - going to be able to go and search do
26:09 - similarity search until it finds what is
26:13 - congruent with the question that we have
26:16 - inserted that's pretty cool right and so
26:18 - we say in collection. query passing the
26:21 - question and the number of results we
26:23 - want documents that we want and then we
26:26 - put in a variable and then we stract the
26:28 - relevant chunks from that uh from our
26:33 - list of documents because this result
26:35 - here is going to have the list of
26:37 - documents that's why we can go through
26:39 - those documents and get the relevant
26:41 - chunks and then once we have them we
26:43 - just return those relevant chunks I have
26:46 - this other code here you can check it
26:47 - out this is going to just give us the
26:49 - distance between the relevancy so
26:52 - essentially tell us how close to the
26:55 - actual answer this doc doents are okay
27:00 - and you can play with that okay so next
27:02 - what we'll do once we have this done I'm
27:05 - going to then of course have a function
27:07 - that will generate the response because
27:09 - think about it we have taken the
27:12 - documents that we have we Cho them up we
27:15 - put them we created a vector database
27:18 - and then we put inside of that Vector
27:20 - database but before that we were able to
27:22 - create embeddings because we want to
27:24 - save those embeddings CU it's easier for
27:27 - the search store happen right for the
27:30 - right document once we ask the question
27:32 - and so now we want to generate the
27:34 - actual response so now we are going to
27:35 - use the large language model again open
27:38 - AI in this case to do all the work with
27:42 - all these pieces that we have right now
27:44 - so as you can see here we pass in the
27:45 - question and we need the relevant chunks
27:48 - right we are taking the relevant chunks
27:51 - that we created we were able to query
27:53 - the database right and then we are
27:56 - passing that along in this with the
27:58 - question so now we have the question our
28:00 - question asking questions about these
28:02 - documents right whatever and then now we
28:05 - have this only relevant chunks that were
28:08 - passed through the large language model
28:10 - and the large language model now has
28:13 - more information or more knowledge of
28:17 - what we want to get the answer from
28:19 - right and so that is what's happening so
28:21 - here I'm creating the context
28:23 - essentially I'm getting this relevant
28:25 - chunks and joining in with other stuff
28:27 - and then I have a prompt here a prompt
28:29 - for our large language model to say hey
28:31 - this is what you need to be aware of
28:33 - when you are answering these questions
28:35 - you're an assistant for question
28:37 - answering tasks use the following pieces
28:39 - of data blah blah blah retrieve context
28:41 - to answer question if you don't know
28:43 - please say I don't know and things like
28:45 - that now prompt is actually its own
28:47 - thing and you have to be really good at
28:49 - prompting to get the right result from
28:51 - large language model and then of course
28:53 - we pass that context and the question we
28:56 - need to pass those two things right so
28:58 - now the large language model will have
29:00 - the question that we asking we typed in
29:02 - and then we'll have the relevant
29:04 - documents because we've parsed that
29:05 - through already you see okay and then we
29:09 - call the create again we go to the
29:10 - client chat completions and create of
29:13 - course the actual now we are going to
29:15 - the actual model say hey here's the
29:17 - information go ahead and give me the
29:19 - answer that's all we doing here okay we
29:21 - pass in the prompt and as well as the
29:23 - question and then we get the answer this
29:27 - is what this will return so now it's
29:29 - time for us to check out to see how this
29:31 - will work so now I'm going to have here
29:34 - this query that I'm going to start doing
29:37 - here so here example question tell me
29:41 - about AI replacing T TV writers in
29:43 - strike now I know this question that
29:46 - would work because in one of these
29:47 - documents here we talk about AI
29:50 - replacing jobs and so forth and so I'm
29:52 - going to see if this works so here's
29:54 - what's happening I have the question and
29:56 - I need to get relevant
29:58 - chunks right from the document in this
30:01 - case from database well I call the query
30:04 - documents and I pass the question so
30:06 - it's going to go ahead and query the
30:07 - documents that are in dator base finding
30:10 - documents that are relevant to the
30:12 - question that we asking which is this
30:14 - here right and once we get this relevant
30:17 - chunks we're going to need that along
30:19 - again with the question the first
30:21 - question and the relevant chunks we got
30:23 - to get the answer right because this
30:25 - generate response here this is where we
30:27 - just talked about is going to go ahead
30:29 - and pull in the context relevant chunks
30:32 - as well as the question create a prompt
30:35 - and then pass that through that prompt
30:37 - and then call the large language model
30:39 - to then ask answer that question and we
30:43 - get that answer okay let's see if this
30:46 - works and then we're going to print the
30:47 - answer
30:48 - here all right let's run this again and
30:50 - see if this works so you're going to go
30:52 - through the whole process of course it
30:54 - will generate everything and one thing
30:56 - also in the beginning of of course we'll
30:58 - go through the process but once you run
30:59 - once because we will have that data and
31:01 - everything uh we should be able to just
31:04 - comment out the first part of this code
31:07 - essentially so everything is good but in
31:09 - any case everything will still work
31:11 - should work because we have that data
31:14 - already the other thing you will notice
31:15 - is that now we should have you can see
31:18 - now we have this chroma persistent
31:20 - storage which is indeed the chroma SQL
31:22 - I3 which is the database that we created
31:25 - this is the this is the actual chroma
31:29 - database pretty cool pretty cool indeed
31:31 - okay so we have that set up so this will
31:33 - take a little bit of course okay now it
31:36 - says retrieving relevant chunks and
31:38 - voila says here TV writers are currently
31:40 - on strike due to the Writers Guild of
31:43 - America demanding regulations and the
31:45 - use of um let's see on the use of AI in
31:48 - writer's rooms so the writer's Guild
31:50 - blah blah blah so all this information
31:52 - actually pertains to the articles that
31:55 - we have here so AI replace TV writers if
31:58 - I click here you will see that indeed I
32:01 - should have something related to that
32:03 - let's see okay regulate the use of AI
32:06 - and clear work and so forth so it goes
32:08 - ahead and looks at the correct other
32:11 - ones here that relate to exactly that so
32:14 - for instance I can go ahead and ask
32:15 - something else let's see let's say
32:19 - something about data bricks okay so them
32:22 - say tell me about data
32:26 - bricks and let's let's go
32:29 - ahead and go through the
32:31 - process it went through the whole
32:33 - process inserting chunks into DB blah
32:35 - blah blah went through all of that we
32:37 - know that and at some point it went
32:39 - ahead and say returning relevant chunks
32:41 - and then of course we hit the large
32:43 - language model and says dat bricks is a
32:46 - data AI an AI company that recently
32:49 - acquired ocara blah blah blah and so
32:51 - forth so just like that we're able to
32:54 - take the information our own data in
32:58 - this case here this could be anything
33:00 - our own data in this case and we pared
33:03 - all that through we extracted everything
33:05 - we created these little chunks of this
33:08 - data and then we used opening eye API to
33:13 - actually create the embeddings that's
33:15 - very important and then we save that
33:17 - into a vector database this is very
33:19 - important also this is not a normal
33:21 - database is this is a vector database
33:24 - and then we're able to search that
33:25 - Vector database according to the
33:27 - question that we passing through it and
33:29 - then we got the right chunks right and
33:32 - then we pass those chunks of documents
33:35 - and the question and pass that through
33:37 - the large language model and then we're
33:39 - able to get the answer this is the power
33:42 - as you see here because now we're able
33:44 - to take our own data and sort of inject
33:47 - into the large langage model so that we
33:50 - are able to ask questions about our
33:53 - particular data and I hope you can see
33:55 - so many use cases that you can use this
33:58 - particular uh rag system here to help
34:02 - you with analyzing data and so forth all
34:06 - right so now that you have the basics
34:08 - the fundamentals of rag how to create a
34:11 - simple rag system that allows you to
34:13 - converse or chat with your own documents
34:16 - so essentially injecting some
34:18 - information your custom information your
34:20 - data with the large language model so
34:23 - you can converse and start talking
34:25 - chatting and getting response
34:28 - conveniently response that is attached
34:31 - or that is congruent with your own data
34:34 - okay so you know how to do that and of
34:37 - course now you know how to create of
34:39 - course a database a vector database
34:42 - which is very important for us to be
34:44 - able to save those pieces of information
34:47 - of our data data documents in this case
34:51 - and then save all of that information
34:53 - along with the embeddings which is
34:56 - really important because all of this is
34:58 - being saved in a vector space which is
35:01 - easier for the vector database to be
35:04 - able to find things faster things that
35:08 - have meaning and relevancy if you don't
35:10 - think about you think that this is where
35:12 - it ends but obviously this is not where
35:14 - it ends because you will see that rag as
35:17 - it is right now we call it naive rag it
35:20 - has its own pitfalls and so now we are
35:24 - ready to move forward and learn of these
35:27 - pit FS that rag naive rag which what is
35:31 - what we've been doing the pitfall that
35:33 - it has and then we're going to learn and
35:35 - Implement a technique or certain
35:37 - techniques that will take our rag system
35:40 - to the next level okay so we can
35:43 - actually get consistent results because
35:46 - as it is you can get some results that
35:49 - are not very consistent and results that
35:51 - may not be necessarily congruent or
35:54 - related semantically with your query and
35:58 - that is what we're going to be touching
35:59 - on next okay let's go ahead and do that
36:02 - so naive rag is the most simple
36:05 - straightforward way of dealing with
36:08 - large language models so essentially it
36:11 - has indexing retrieval and generation so
36:14 - indexing it is the process of cleaning
36:16 - up and extracting data from the
36:18 - documents just like as we saw earlier
36:20 - and then we have the retrieval so this
36:22 - is the part where we turn questions into
36:24 - a vector a vector space and that's what
36:27 - use for comparison which allows us to
36:30 - retrieve closely related chunks which
36:32 - then are pushed with the query into the
36:36 - generation phase which then the query
36:39 - choose chosen documents are combined
36:42 - into a prompt so we have prompt the
36:44 - query and the uh documents that were
36:47 - chosen push to the model to generate an
36:50 - answer so this is the naive rag so
36:53 - really if you want to do a deep dive
36:55 - into naive rag this is what happens so
36:58 - we have the documents and these
36:59 - documents are going through the phase of
37:02 - parsing and pre-processing so
37:04 - essentially cut them up into smaller
37:06 - documents uh this is the chunking
37:08 - process and then we pass them around
37:10 - into smaller chunks and those are passed
37:13 - through the embedding model to create
37:15 - vectors out of these chunks okay so
37:18 - we're vectorizing those chunks and then
37:21 - that is what it's saved into a vector
37:23 - store or a vector database so this is
37:26 - the part of indexing that happens here
37:29 - of course is the indexing part as I have
37:32 - shown you this is the part where we cut
37:34 - the documents and pre-process everything
37:37 - and chunk it up and then create those uh
37:40 - embeddings or vectorize those chunks and
37:43 - save them into a vector store and then
37:47 - what happened is then we have a user who
37:49 - has a query or question of some sort and
37:52 - that also has to go through the
37:54 - embedding model to vectorize that query
37:58 - and then that is actually what is what
38:00 - is sent to search into the vector
38:02 - database so we have vectors and vectors
38:05 - that are easily uh used in a V database
38:09 - to do all sort of things mainly to
38:11 - search and then the information is
38:13 - retrieved the relevant documents are
38:15 - retrieved or packed up in this case with
38:18 - prompt as well as the relevant documents
38:21 - as I said and the query but notice here
38:23 - this is the different part phase this is
38:26 - the augmentation phase of the rag so we
38:29 - augmenting we're adding something to
38:32 - what we had before so not only we have a
38:34 - query but we also have prompt which is
38:36 - part of the query and relevant documents
38:38 - and so forth okay so once that is
38:40 - augmented we pass that information
38:43 - through a large language model so it
38:46 - could be any kind of large language
38:47 - model and then that's when the response
38:50 - is generated which is then returned to
38:52 - the user so this is how naive uh rag
38:57 - works works as wonderful as this sounds
39:00 - and looks uh there's some issues with
39:02 - this well naive rag has some challenges
39:06 - some pitfalls some drawbacks the first
39:08 - one is that we have limited contextual
39:11 - understanding so for example if a user
39:14 - asks about the impact of climate change
39:17 - on polar bears so a naive rag might
39:21 - retrieve documents broadly discussing
39:23 - climate change on polar bears separately
39:26 - but will f fail to find the most
39:28 - relevant documents discussing both
39:31 - topics in context okay so that is a
39:34 - problem because naive rag models often
39:37 - retrieve documents base solely on
39:40 - keyword matching or basic semantic
39:44 - similarity which can lead to retrieving
39:47 - irrelevant or partially relevant
39:49 - documents and then of course we get
39:51 - inconsistent relevance and quality of
39:54 - retrieved documents so what that means
39:57 - that the quality and relevance of the
39:59 - retrieved documents can vary
40:00 - significantly because navag models uh
40:03 - may not rank the documents effectively
40:06 - which leads to poor quality inputs to
40:09 - the generative model and the third one
40:12 - is that we have poor integration between
40:13 - retrieval and generation what that means
40:16 - is that uh in naive rag systems the
40:19 - Retriever and the generator components
40:21 - often operate independently without
40:24 - optimizing their interactions so you can
40:27 - see two things are work independently
40:29 - they're not optimized to work together
40:31 - that's a problem so this lack of synergy
40:34 - could lead to suboptimal Performance
40:36 - where the generative model doesn't fully
40:39 - Leverage The retrieved information so as
40:42 - an example the generative model might
40:45 - generate a response that actually
40:47 - ignores critical context that was
40:49 - provided by the retrieve documents which
40:52 - in this case will result in generic or
40:56 - off-topic an ERS also we have
40:58 - inefficiency uh handling of large scale
41:01 - data naive raich systems May struggle
41:05 - with scaling to large data sets because
41:07 - of its inefficient retrieval mechanisms
41:10 - which leads to slower response times and
41:13 - decreased performance for example in a
41:16 - large knowledge base a naive retriever
41:19 - might take too long to find rant
41:22 - documents or even miss critical
41:24 - information due to inadequate index
41:27 - and search strategies we also have lack
41:30 - of robustness and adaptability the issue
41:34 - here is that naive rag models often lack
41:37 - mechanisms to handle ambiguous or even
41:40 - complex queries robustly so they're not
41:43 - adaptable uh to changing contexts or uh
41:47 - user needs without significant manual
41:50 - intervention you can imagine if a user
41:53 - query is vague or multifaceted a naive
41:56 - rag might might retrieve documents that
41:58 - partially address different aspects of
42:01 - the query but fail to provide a coherent
42:05 - and comprehensive answer okay so these
42:07 - are some of the main drawbacks or
42:10 - challenges or even pitfalls uh of naive
42:13 - rag okay let's go ahead and break it
42:15 - down uh each one of these pitfalls of
42:18 - naive rag so we can have some better
42:20 - ideas okay so let's look at the limited
42:23 - contextual understanding as we focused
42:25 - on in the last
42:27 - uh lecture how does that even look like
42:30 - again we understand in naive rag limited
42:32 - contextual understanding focus on
42:35 - keyword matching or basic semantic
42:38 - search in this case retrieving
42:39 - irrelevant or partially relevant
42:41 - documents so essentially as I said we
42:44 - have a query for instance that says uh
42:46 - the impact of climate change on polar
42:48 - bears and so the idea is that that
42:50 - question is
42:52 - transferred through the whole process
42:54 - this naive rag which we don't have to go
42:56 - through the whole details what happens
42:58 - there as we know it so this is just an
43:00 - illustration goes through all of that
43:02 - and then it just retrieves non-relevant
43:05 - docs on both topics okay so that is the
43:09 - limited contextual understanding that is
43:12 - lacking which is one of pitfalls of
43:15 - naive rag okay because naive rag models
43:18 - often retrieve documents based solely on
43:21 - the keyword matching or basic semantic
43:24 - similarity which you can see here can
43:27 - lead to retrieving irrelevant or
43:29 - partially relevant documents
43:31 - non-relevant docs on both topics now the
43:35 - next one we talked about is the
43:36 - inconsistent relevance and quality of
43:39 - retrieved documents so the problem is
43:42 - the same with naive rag we have this
43:44 - plethora of varying in quality and
43:47 - relevance documents which means we have
43:49 - poor quality inputs for the model again
43:52 - the same example we'll have a query that
43:54 - goes through and that says latest
43:57 - research on AI ethics for instance and
43:59 - so that goes through the naive rag
44:02 - process and then we may end up getting
44:05 - outdated or less credible resources
44:08 - because the quality and relevance of the
44:10 - retrieve documents can also vary
44:12 - significantly so naive rag models may
44:15 - not rank the documents effectively which
44:18 - in this case will lead to poor quality
44:21 - inputs for generative model which in
44:24 - this case you know will get very poor
44:26 - results
44:27 - the next one is that we discuss is the
44:29 - poor integration between retrieval and
44:31 - generation and so we know that because
44:34 - of the nature of naive Rags the
44:37 - retriever I should say and the generator
44:39 - components often operate independently
44:42 - they are operating alone without
44:45 - optimizing the interactions within
44:48 - themselves which as you know if two
44:49 - systems are working not in conjunction
44:53 - the Synergy can lead to suboptimal
44:55 - Performance which means we end up with
44:58 - very unoptimized documents or
45:01 - information that is being passing
45:03 - through so again we have in this case
45:05 - the retrieved documents and these
45:07 - retriev documents are of course used
45:09 - through the naive rag system and what
45:12 - happens that is that we may lose what's
45:14 - actually important what is relevant and
45:17 - we end up with very generic or even
45:19 - off-topic answers through this whole
45:22 - process because these two systems are
45:24 - working the generator in this case
45:26 - generator and the retriever are
45:28 - components that are working
45:29 - independently without optimizing their
45:31 - interactions and the next one is
45:33 - inefficient handling of large scale
45:35 - let's look at the overview the idea is
45:37 - that using naive rag systems uh these
45:39 - tend to struggle with scaling to large
45:42 - data sets because of the inefficient
45:45 - retrieval mechanisms which lead to very
45:47 - slow response times and of course
45:49 - decreased performance because it takes
45:52 - too long in larger scale data of course
45:54 - sets and so forth to find relevant box
45:57 - which of course leads to missing
45:59 - information due to bad indexing
46:01 - essentially so essentially it's the same
46:03 - thing we have the index process which
46:05 - creates of course these index uh data
46:08 - structures which then in a grand scheme
46:10 - of things is going to have trouble
46:13 - trying to find the right information in
46:15 - a very large knowledge base if you have
46:17 - a lot of documents and so forth because
46:19 - it's taking too long and of course it
46:21 - may end up missing critical information
46:24 - because of this inadequate indexing and
46:27 - search strategies okay so going to the
46:29 - lack of robustness and adaptability this
46:32 - is another one models often lack
46:34 - mechanism to handle ambiguous or complex
46:37 - queries so remember that queries are not
46:39 - always very direct so you can have a
46:42 - query that is loaded per se it's
46:45 - ambiguous it has more information more
46:47 - questions that it's a little bit complex
46:50 - so with the naive rag they don't just
46:54 - have the mechanisms to handle that
46:56 - because they're not adaptable so that is
46:58 - the problem here's an example so we have
47:00 - a query tell me more about index funds
47:03 - and anything related to finances so this
47:06 - is a little bit loaded this query isn't
47:08 - it and you can see that naive Rags don't
47:10 - have a way of deciphering through this
47:13 - query to get to the bottom of the actual
47:16 - query or um to get the correct coherent
47:20 - and comprehensive answer so these are
47:22 - the drawbacks uh doing a little bit of a
47:24 - deep dive into each one of the drawbacks
47:28 - or challenges uh or pitfalls of using
47:32 - naive racks so in summary naive rag have
47:36 - of course some pitfalls as we've seen
47:39 - and we can subdivide those pitfalls in
47:41 - two categories we have first the
47:43 - retrieval challenges so in this case
47:46 - they lead to the selection of misaligned
47:49 - or even irrelevant chunks which of
47:51 - course leads to missing of crucial
47:54 - information which as you know it's not a
47:56 - good thing and then we have the other
47:57 - side which is the generative challenges
47:59 - so under this umbrella we have issues
48:03 - that the model might struggle with
48:05 - hallucination and have issues with
48:07 - relevance toxicity or bias in its
48:11 - outputs so these are the two umbrellas I
48:15 - would say of the drawbacks that naive
48:19 - frag brings to the table retrieval
48:22 - challenges as well as generative
48:24 - challenges
48:27 - now let's talk about the solutions here
48:29 - in this case we are going to go from the
48:31 - naive rag that we've looked at and it's
48:34 - pitfalls now we're going to look at
48:36 - Advanced rag techniques that help us
48:39 - make this whole process that uh makes
48:42 - this whole process more efficient okay
48:44 - for our Rag and so Advanced rag
48:47 - techniques and their Solutions now let's
48:50 - look at the first
48:52 - one now first let's look at advanc Rags
48:54 - benefits what is it important the beauty
48:57 - here is that they introduce the specific
48:59 - improvements this plethora of
49:02 - improvements that allow us to overcome
49:04 - the limitations of Na rag as we've seen
49:09 - previously the main goal here is that
49:11 - with Advanced rag benefits we are
49:13 - focusing on enhancing retrieval quality
49:17 - so Advanced rag employs the following
49:20 - strategies the first one is
49:21 - pre-retrieval so idea is to improve the
49:26 - indexing structure and users query
49:29 - because that is very important the first
49:30 - step if you don't have good indexing
49:33 - structure and users query then
49:35 - everything else falls apart and also
49:38 - we're going to improve the data details
49:41 - in this case organizing index is better
49:43 - adding extra information augmentation
49:46 - part of it now and aligning things
49:48 - correctly and then we have the second
49:50 - part which is the post retrieval at this
49:52 - stage we are combining what we got from
49:55 - the pre- retrieval stage okay the data
49:59 - that we got there with the original
50:01 - query so we're combining all all of that
50:03 - to finalize this augmentation part of
50:06 - the rag in this case we're going to be
50:08 - we could be reranking to highlight the
50:11 - most important content or doing all
50:13 - other techniques goes ahead and to
50:16 - enhance our retrieval processes now
50:19 - there are many Advanced rack techniques
50:21 - out there have done a lot of studies and
50:24 - written papers that you can go and take
50:26 - take a look uh one thing I'll will let
50:28 - you know of is that as you go through
50:30 - all of these other different techniques
50:32 - even Beyond this course you will realize
50:34 - that most of them tend to overlap and
50:37 - sometimes the namings can also overlap
50:39 - and that's okay but the main idea is the
50:41 - same which is having techniques that
50:44 - allow for a better workflow of our rack
50:48 - systems so first of all I'm going to
50:50 - look at here is the query expansion in
50:53 - this case going to be with generated
50:55 - answers what does that really really
50:56 - means is that we're going to generate
50:59 - potential answers to the query in this
51:01 - case we're going to actually use a large
51:03 - language model to get relevant context
51:06 - so with the query expansion as an
51:10 - advanced retrieval technique uh it's
51:12 - used to improve the relevance again keep
51:16 - in mind these keywords it's used to
51:19 - improve the relevance of search results
51:23 - by
51:24 - augmenting the original query with
51:26 - additional terms or phrases because
51:29 - these additional terms are selected
51:31 - based on various strategies such as
51:33 - synonyms expansion related terms or
51:37 - contextually similar words so the goal
51:39 - is to really capture more relevant
51:41 - documents that might not match the
51:44 - original query terms in this case will
51:47 - not match them exactly but are
51:49 - semantically
51:50 - related so to show you here here's a
51:53 - simple diagram so you have a query and
51:56 - this query has to go through a large
51:59 - language model to create an answer this
52:01 - is a I would call an hallucinated answer
52:05 - okay and then we take that answer that's
52:08 - goes through the vector base and then we
52:10 - concatenate the answer and the original
52:12 - query and use that as a new query which
52:16 - you pass through the vector database in
52:18 - this case retrieval system and then you
52:21 - return your query results which was
52:24 - passed through the large language model
52:26 - again to get the actual results the
52:29 - answer so that's all we're doing here is
52:32 - that we are using the large language
52:34 - model in this case to hallucinate a
52:36 - little bit on that first query and then
52:39 - we concatenate the original query or in
52:43 - this case the answer that answer that we
52:45 - got right and the original query and use
52:48 - that as a new query which then we pass
52:50 - through the fact database the retrieval
52:52 - system and then return the query results
52:55 - again pass those through large language
52:57 - model again to get the results so what
52:59 - happens here is that with this expansion
53:02 - here we have a few use cases right this
53:04 - query expansion with generated answers
53:07 - we can use that in the information
53:09 - retrieval can imagine with this system
53:11 - now we can enhance the effectiveness of
53:14 - search engines because we're providing
53:16 - more comprehensive search results and
53:19 - also in question answering systems
53:21 - because now we're improving the
53:23 - retrieval of relevant documents or p
53:26 - messages that potentially help in
53:28 - answering user queries in e-commerce
53:32 - search because increases the accuracy
53:35 - right and relevance of product search by
53:39 - expanding user queries with related
53:42 - terms academic research it makes sense
53:44 - because now we are able to find more
53:47 - relevant papers by expanding their
53:50 - search queries with related scientific
53:52 - terms and Concepts okay so let's go
53:55 - ahead do a Hands-On so essentially again
53:57 - this is diagram we have the query pass
53:59 - that through a large language model and
54:02 - we get an answer so a hallucinated
54:04 - answer and that's okay but that's going
54:06 - to be used of course concatenate with
54:08 - the answer and the previous query and we
54:10 - get some results for our from our
54:13 - Defector database which then we pass all
54:16 - of that into a large language model and
54:19 - we get an answer so in the next video
54:21 - we're going to see how all that works in
54:24 - code okay so I have a project here here
54:26 - you should have all access to all of
54:28 - this code and data and everything and at
54:32 - the top here we have this data folder
54:33 - which has this Microsoft annual
54:36 - report okay so if you click on that it's
54:38 - just a large PDF with about 126 or 116
54:43 - pages so it talks about Microsoft annual
54:46 - report 2023 okay so I just found this
54:50 - and I thought would be a good idea to
54:52 - use that for our demonstration here okay
54:54 - so we'll have access to all of this and
54:56 - also make sure that uh you create a
54:58 - virtual environment on your project so
55:00 - this works as well as having in this
55:03 - case an open AI API key you should be
55:06 - able to go and create an account with
55:08 - open Ai and get a key so if you want to
55:11 - follow
55:12 - along and so forth I've got this helper
55:15 - utils other utility methods and so forth
55:19 - okay all right so the first thing let's
55:21 - go ahead and do some importing here or
55:25 - installing some dependen es first I'm
55:27 - going to say pip
55:29 - install
55:31 - chroma DB because that's what we're
55:33 - going to be using and next let's pip
55:37 - install Pi PDF because we're going to be
55:40 - using that as well to read a PDF file
55:43 - and extract everything and let's go and
55:45 - say
55:49 - pip I'm going to
55:52 - install open AI okay so let's go ahead
55:55 - and create a new file here let's call
55:57 - this
55:59 - XP say expansion insert that py okay all
56:03 - right so I already have the code so I'm
56:06 - going to just get parts of it and we go
56:08 - from there first I'm going to add some
56:10 - imports here get PD Pi let's make sure
56:14 - that everything is set up okay we got
56:17 - the our PI PDF PDF reader we have open
56:20 - the eye all of that and I'm going to set
56:23 - up the environment variables as well
56:26 - well so open your IPI key make sure you
56:29 - have that set up inside of your. EnV
56:32 - file it's very important first let's go
56:34 - ahead and read our Microsoft annual
56:37 - report it's under data can see it's
56:40 - under here okay should have access to
56:42 - all of that so that's what we're doing
56:44 - here and then PDX text I'm extracting
56:47 - everything from the pages filter the
56:50 - empty strings real quick and while I'm
56:52 - here let's go ahead and run this real
56:54 - quick to see if this works I'm going to
56:56 - just just print this PDF text that we
56:59 - get here filter out the mty
57:02 - strings and I'm passing through the word
57:05 - wrapper so we can see so I'm going to
57:06 - print that out let's see what's going to
57:07 - happen let's go ahead and run
57:13 - this okay so I need to import Panda as
57:16 - pandas so that's
57:21 - something okay make sure you import
57:23 - pandas as well let's go ahead and run
57:27 - okay so we can see that we're able
57:30 - to extract our PDF and we get all of
57:33 - that response so this is really good
57:35 - okay so now that we have the information
57:37 - that we need which means we extracted
57:39 - the text I'm just want to comment this
57:41 - out we extracted our PDF we got our that
57:44 - information from our document what we
57:46 - need to do next is we want to be able to
57:51 - split the text chunks and to do that
57:54 - we're going to use um L chain which is a
57:58 - framework that allows us to deal with
58:00 - with large language models and do all
58:02 - sort of things okay so uh first let's go
58:04 - ahead and get that so pip
58:09 - install link
58:16 - chain lank chain splitter as such so I
58:20 - can go ahead and get the recursive
58:22 - character splitter as well as the
58:24 - sentence transformance token text
58:26 - splitter because we're going to need
58:27 - those two before we uh get everything
58:31 - and start embedding everything okay so
58:33 - let's go ahead and take care of
58:35 - splitting our document using the
58:39 - recursive character Tex splitter which
58:41 - we are passing the chunk size th000 and
58:44 - chunk overlap of zero and then here we
58:47 - are uh splitting the text by getting all
58:51 - of the actual PDF text that we have
58:54 - created from here okay so taking that
58:57 - this is where we're splitting it and
58:59 - we're going to get the character splits
59:01 - so I can also come here back here I'm
59:03 - going to print the character split text
59:06 - all right the 10th one and then I'm
59:08 - going to show the total chunks so I'm
59:10 - going to save this and let's do a quick
59:13 - run okay so we can see that we have
59:16 - indeed the information that we wanted to
59:18 - see this word wrap of the character
59:21 - splits right in this case just the the
59:23 - 10th one and we see that's the one that
59:26 - we got and then we have total chunks is
59:29 - 410 chunks that were splitted that's
59:32 - very good next we use the sentence
59:35 - Transformers token text splitter to
59:37 - split the text into chunks of about 256
59:41 - tokens and then we're going to set the
59:43 - chunk overlap to zero okay now the
59:47 - reason why we need to do this is because
59:49 - we need to be mindful of the token size
59:53 - for limitations that the large longer
59:55 - Mar models could impose then we'll be
59:58 - able to use to generate the embeddings
60:00 - which we need Okay so let's do that so
60:04 - here I'm creating the token splitter by
60:06 - using the sentence Transformers token
60:08 - text splitter I'm passing overlap zero
60:10 - and the tokens per chunk 256 and then
60:13 - we're going to go ahead and actually
60:14 - start doing the splitting so here I
60:17 - create the text token split text and I
60:20 - Loop through all of the
60:22 - pieces and I split them up and now let's
60:26 - go ahead and print some information out
60:28 - so we can see the difference here so I'm
60:30 - going to save
60:35 - this so notice the first one the total
60:37 - chunk when we first did it all right
60:41 - about total chunks is for 10 and now you
60:44 - can see the difference that for the
60:47 - chunk the total chunks is going to be a
60:49 - little bit elevated which makes sense
60:51 - okay so looks like good not import
60:52 - sentence Transformer from python
60:54 - packages need to be order sentence okay
60:57 - uh yes I need to go ahead and install
60:59 - that's right so let's go ahead and say
61:00 - copy that and first of all say pip
61:03 - install send this Transformer so we have
61:07 - that okay we have that let's run
61:11 - this okay so first we have the 410 total
61:14 - chunks but next we should have a little
61:16 - bit more because of what we just did
61:19 - okay so you can see it's a little bit
61:20 - more now 419 as opposed to 410 so very
61:23 - good so that is the total chunks of the
61:25 - splits that we did here okay this is the
61:28 - token split text okay so now it's time
61:31 - that we have since we have all the
61:33 - tokens we can go ahead and start the
61:36 - embedding process so first of all we're
61:38 - going to import chroma DB and then we're
61:40 - going to use chroma DB and get the
61:42 - embedding function we could have used
61:44 - some other embedding functions using
61:46 - open Ai and so forth but I'm going to
61:48 - use the uh send Transformer embedding
61:50 - function for
61:52 - this demonstration here so we going to
61:55 - import that and then I'm going to
61:57 - actually instantiate the sentence
61:59 - Transformer embedding function as you
62:01 - see here and just because I want to see
62:02 - I'm going to go ahead and print the
62:06 - token splits token because come as a
62:08 - list the 10th one here the 10th chunk to
62:12 - show so we can print the actual
62:14 - embeddings of the 10th chunk okay so
62:17 - just to avoid having a lot of things
62:19 - printing out I'm going to just comment
62:20 - out all of these other that we had
62:23 - before
62:26 - okay so you can see now we have the
62:28 - embeddings for the 10th chunk okay not
62:31 - very uh useful but at least we see that
62:33 - things are actually working so the
62:34 - embedding function is indeed
62:38 - working okay so we know that the
62:40 - embedding function is working so we no
62:41 - longer to need to see it so I'm going to
62:43 - go this out going to leave it there and
62:45 - next what we'll do is we're going to
62:47 - instantiate the chroma client and create
62:49 - a collection and give it a name and
62:52 - passing of course the embedding function
62:53 - so that we can embed uh each one of of
62:55 - these items that are passed along so
62:58 - chroma going to create a client and then
63:00 - create a function in this case I'm going
63:02 - to just give this Microsoft collection
63:04 - you can name whatever you want and then
63:06 - embedding function it's going to be
63:07 - passing the embedding function which is
63:09 - going to be attached when we create the
63:12 - chroma database okay and once we have
63:15 - that we can extract embeddings of the
63:18 - token splits by just going through like
63:22 - this okay and then
63:26 - we are going to use that where we get
63:29 - all of these embeddings of the token
63:31 - splits and that's what we're going to be
63:33 - adding to our collection right so to do
63:35 - so I'm going to just copy this to do so
63:38 - we do this so we say chroma that add and
63:41 - we're going to add these IDs as we go
63:43 - through and the actual documents so the
63:44 - documents going to be the token splits
63:47 - right and I'm going to just say chroma
63:50 - uh collection count and for now let's go
63:52 - ahead and print just say count
63:57 - and say count like that so we can
64:03 - see okay so we can see the count is for
64:06 - 19 so essentially what we had before
64:08 - very good so at least we know that it's
64:09 - working okay so I don't need to do any
64:11 - of that and so what we'll do next is
64:14 - we're going to create the actual query
64:16 - we've extracted the document split it
64:18 - all up and then added that putting that
64:20 - through an embedding function so we have
64:23 - the embeddings and then we added all of
64:24 - that inside of our chroma DB okay our
64:28 - Vector store so I'm going to have a
64:30 - query here what was the total revenue
64:31 - for the year for instance right and then
64:34 - what we'll do is we're going
64:36 - to query using the collection so
64:41 - collection the query we pass the query
64:43 - text which could be we could pass more
64:46 - than one query that's why we pass as a
64:49 - list and then the number of results that
64:51 - we want to get okay and then retrieved
64:53 - documents I'm going to say results and
64:55 - get docment as such and so what I will
64:57 - do is for us to be able to see I'm going
65:00 - to Loop through all these documents so
65:03 - we can see what we get let's run
65:08 - again all right so you can see that we
65:10 - got some of the documents okay so very
65:13 - good revenue and this year and of course
65:16 - these are split up and that's the whole
65:18 - idea is that they may not make sense or
65:20 - where they start but you can see we have
65:22 - one two three four and a few documents
65:26 - that we getting here all right so this
65:28 - is indeed working so now we know we're
65:31 - getting all the documents so it's a
65:33 - matter of trying to figure out okay how
65:35 - do we see how this technique Works
65:38 - meaning that the first query and we got
65:40 - the documents that we've just retrieved
65:42 - and as well as creating in this case
65:44 - we're going to generate the actual
65:47 - queries right the augmented query as we
65:49 - see because that's the whole idea of
65:50 - this technique okay first thing is I'm
65:52 - going to uh create a function called
65:55 - augment query generated so so this is
65:57 - where I'm going to use the augment query
66:00 - generated to generate an actual answer
66:03 - so we're going to use of course a large
66:05 - language model in this case open Ai and
66:08 - generate this actual answers or one
66:12 - answer in this case so first of all I'm
66:14 - going to have to create a client for
66:16 - open AI so I'm going to copy
66:18 - that and so this augmented query
66:21 - generator what it does is allows us to
66:23 - pass the query and we're setting up the
66:25 - model to the GPT 3.5 turbo you can
66:28 - change that if you want and then I have
66:29 - the prompt here that you are a helpful
66:31 - expert Financial research assistant I
66:34 - provide an example answer because we
66:35 - want an answer to given question that
66:38 - might be found in a document like an
66:40 - annual report so we're prompting it
66:42 - making sure that it knows what to do and
66:45 - then we pass the message here for the
66:46 - prompt here for the system is where we
66:48 - created here to know this is what you
66:50 - need to do what he need to be knowledge
66:53 - knowledgeable uh on and then passing the
66:56 - query so the query is going to be the
66:57 - question the first query that we pass
66:59 - along okay and then we use of course uh
67:02 - the completion API that create pass it
67:05 - the model and the message and we get
67:07 - that response so essentially here so
67:09 - this is where we are looking at this
67:12 - part here so we're going to generate
67:13 - this one answer which we then we're
67:15 - going to take with the query the answer
67:17 - we got go through our uh DB and we got
67:21 - the query results and then put all of
67:23 - that together into the large language
67:25 - model and and get the answer all right
67:27 - so we have that function which we'll be
67:28 - using soon okay so next what we'll do
67:31 - here is we're going to use create an
67:33 - original query here for instance what
67:35 - was the total profit for the year and
67:37 - how does it compare to previous year so
67:39 - this is the original query and then I
67:41 - have this hypothetical query or answer I
67:43 - should say and so I'm going to use the
67:45 - augmented query generating the function
67:47 - and I'm going to pass the original query
67:49 - which is this one here and then I'm
67:51 - going to join those two queries right
67:53 - the original and the hypothetical answer
67:55 - answer so Regional and hypothetic answer
67:59 - and I'm going to print them out so we
68:00 - can see how those will look so I'm going
68:03 - to go ahead and run
68:06 - this you can see here we have this what
68:08 - was the total profit for the year and
68:10 - how does it compared to the previous
68:12 - year very good but then we have this
68:15 - total profit look at this we got an
68:17 - answer so we hallucinated we used large
68:19 - L model to create that answer the answer
68:22 - is the total profit for the year was 10
68:25 - billion and so forth and um by increased
68:27 - sales and successful cost cutting
68:29 - initiative implemented throughout the
68:31 - year very good so now we have the query
68:34 - and we have the answer that was
68:36 - hallucinated right that was created
68:38 - which was part is part of this technique
68:41 - so now we are golden because we have
68:44 - those two and so once we have those two
68:46 - it's time for us to then pull all of
68:49 - that into our chroma collection and
68:52 - query using those two disjoint queries
68:55 - right right in this case I call joint
68:57 - query but it's original query and the
68:59 - hypothetical answer so I'm going to say
69:02 - chroma collection I'm going to put in a
69:03 - variable result because we're going to
69:05 - use that to just retrieve the documents
69:06 - right so I can say query I can pack
69:08 - query text what is the query text well
69:11 - in this case going to be joint query
69:12 - because that is what I got so the
69:14 - original query and the hypothetical
69:17 - answer which is what we just received
69:19 - from the large language model now you
69:21 - could have created yourself a list of or
69:24 - one question for that
69:26 - answer but it's always nice to use the
69:28 - large language model because then it can
69:30 - create something that is more useful you
69:32 - don't have to think about it all right
69:34 - so now we have this joint query which is
69:37 - that and then we say we want Five
69:39 - results and most importantly we want to
69:41 - include documents as well as embeddings
69:44 - in our results all right because you
69:47 - will see that the reason why I want
69:48 - embeddings you will see because we're
69:50 - going to use actual those embeddings to
69:52 - create a graph a graph that will show
69:55 - the relationship of those embeddings
69:57 - visually so we can see how this works
69:59 - all right so then we have retrieve
70:01 - documents in this case I'm going to go
70:03 - ahead and show those retrieve documents
70:06 - all right so I'm going to print them out
70:08 - let's run and there we go so so the idea
70:11 - we are getting the documents that were
70:14 - passed through in this case we have the
70:17 - query uh the original query as doents
70:19 - that we receive as well as the generated
70:22 - answer put them together and then we
70:24 - pass them through our collection to say
70:27 - okay this is the query text The Joint
70:29 - query and we want Five results so
70:31 - include documents and embedding so we
70:34 - have all the documents that are related
70:36 - to the query so we are seeing the query
70:40 - results from the answer concatenated
70:42 - with the query the original query go
70:44 - through the factor database and we get
70:46 - the query results so now that we have
70:48 - our retrieved documents which is exactly
70:50 - what we want we want to project this
70:53 - data set um on a nice graph so we can
70:57 - see the relationship we can see the
70:59 - improvements that we have uh in the
71:01 - documents that we get that are related
71:03 - to the original query and for that we
71:06 - are going to import a few things here so
71:08 - I'm going to command V I'm going to
71:10 - first of all create embeddings go to the
71:12 - chroma and we've seen this before right
71:15 - I'm going to get the embeddings because
71:17 - we're going to be using those and then
71:18 - I'm going to use the umap library to
71:21 - actually create the projections and for
71:25 - that we need to install say pip
71:28 - install umap dlearn Okay this Library
71:32 - okay it's all done put import um map so
71:35 - we can use it at the bottom here and I'm
71:38 - going to say from hper TS import project
71:41 - embeddings there we go okay so now we
71:44 - have this projected data set embeddings
71:46 - now not notice that we're getting these
71:47 - embeddings here from our chroma
71:50 - collection and because when we saved we
71:52 - saved also embeddings along with other
71:55 - information okay so that's the beauty
71:57 - okay so I'm going to use project
71:58 - embeddings and this is a function from
72:01 - our UTS as well the um map transform so
72:05 - next what we'll do we're going to
72:06 - retrieve the embeddings okay as you see
72:08 - there so I'm going to retrieve
72:10 - embeddings from our results which is our
72:13 - collection query there and then I'm
72:16 - going to get the original query
72:19 - embedding by calling the embedding
72:21 - function and passing the original query
72:23 - that we saw before and getting the
72:25 - augmented query embedding this is for us
72:27 - to see the differences of the results
72:30 - that we get between original query
72:32 - embedding in this case the original
72:34 - query and the joint query which is the
72:36 - original query uh the query plus the
72:39 - answers that we hallucinated to put
72:41 - together okay that's all we're doing
72:43 - here and then we are going to create
72:46 - some projection variables here so here
72:50 - I'm creating an object to project
72:52 - original query embedding by calling
72:54 - project embedding ings and passing the
72:56 - same thing the query embedding for the
72:58 - original and passing to map Transformer
73:00 - and doing the same for project augmented
73:02 - query embeddings here we're passing the
73:04 - actual augmented query embedding which
73:07 - is what we have here okay and then
73:09 - projecting the retrieved embeddings so
73:12 - I'm just projecting them all right and
73:14 - next we're just going to import Matt
73:16 - plot lib like such we don't have that
73:18 - let's go ahead and pip install so that
73:22 - we're able to see the figures see the
73:24 - graphs and everything okay once that is
73:26 - all set up then I'm going to go ahead
73:28 - and just plot everything so now here I'm
73:30 - plotting the projected query and
73:32 - retrieve documents in the embedding
73:34 - space so this is going to be just a 2d
73:36 - and you know that embeddings uh the
73:38 - vector space can be M it's multi n is
73:41 - multi-dimensional so but we're going to
73:42 - use 2D to facilitate so we can actually
73:45 - see the differences here that's what I'm
73:47 - doing here scatter and creating
73:49 - different uh points in our graph and
73:52 - then I'm going to go ahead and show
73:55 - graph okay let's go ahead and run this
73:57 - real quick so you can actually see what
73:59 - we're talking
74:01 - about so this is beautiful so you can
74:04 - see here this is a
74:06 - two-dimensional Vector space that shows
74:09 - all of the information that we've
74:11 - plugged in so what we have here is we
74:15 - the Red X as you see here is the
74:16 - original query and all of these gray
74:20 - dots are the data set embeddings now the
74:23 - retrieved documents are close goes to
74:25 - the augmented query what is the
74:27 - augmented query well this is the
74:28 - augmented query the orange x is the
74:32 - augmented query which contains as you
74:35 - remember the hypothetical in this case
74:37 - the hallucinated answer and the original
74:41 - query and then these Green Dots or
74:43 - circles here these are the retrieved
74:47 - documents so you can see that the
74:48 - retrieved documents are close to the
74:51 - augmented query in the embedding space
74:54 - which is Improvement whereas here is the
74:58 - original one and we are really far away
75:01 - from all the retrieve documents so that
75:04 - tells you that indeed this technique
75:05 - works so the bottom line here is that
75:07 - the augmented query is closer again to
75:10 - the retrieved documents than the
75:13 - original query in the embedding space it
75:16 - tells us that using hallucinated answer
75:19 - which we hallucinated meaning we asked
75:21 - the large language model to create that
75:23 - answer first beforehand was part of the
75:25 - query to improve the retrieval results
75:29 - because the augmented query is closer to
75:32 - the retrieve documents than the original
75:34 - query in the betting space looking at
75:37 - this you can see that is quite an
75:38 - improvement now again this is not
75:41 - perfect of course but you see that we
75:44 - can use it to refine the results that we
75:46 - get from our previous naive rag using
75:51 - this technique here then we get
75:53 - something that is a little bit B more
75:55 - closer to the actual documents right the
75:58 - most relevant documents that were
76:00 - retrieved so what I need you to do is to
76:02 - ask a different question and see how
76:04 - this graph will show the improvements uh
76:07 - between the original query and the
76:10 - augmented
76:11 - query okay so we just saw how to use the
76:14 - query expansion with generated answers
76:17 - as a advanc as an advanced rack
76:19 - technique to generate potential answers
76:22 - to query using a large language model
76:24 - and to to get relevant context so
76:26 - essentially we have the query and then
76:28 - we pass that through the llm to generate
76:30 - an answer and then we combine all of
76:33 - that so the query original query and the
76:36 - answer that we received to get query
76:39 - results and then of course that helps to
76:41 - pass through the large L model and to
76:43 - get answer to see how in a 2d
76:46 - dimensional space how the documents
76:49 - relevant documents are closer to theed
76:52 - query instead of the original query now
76:56 - what are the query expansion use cases
76:59 - well this can be used in the information
77:01 - retrieval system or in the question and
77:04 - answering systems as well as e-commerce
77:07 - search and also the academic research as
77:10 - we've seen so what I want you to do next
77:12 - is to create different queries and look
77:15 - at the differences that are plotted so
77:18 - you can start seeing how query expansion
77:19 - with generated answers works it's a
77:23 - really good technique obviously it's not
77:24 - perfect but you can see the difference
77:26 - in many cases that are not negligible at
77:30 - all there are really good differences
77:32 - that perhaps you can see the power or
77:35 - the usability of using this query
77:38 - expansion as an advanced rack
77:42 - technique okay so now let's look at the
77:44 - quer expansion again but now with
77:47 - multiple queries so the idea is that we
77:49 - use the large language model or the
77:51 - model to hallucinate or generate add
77:54 - additional queries that might help
77:56 - getting the most relevant answer just
77:59 - like before we have a query and we pass
78:01 - that through the large language model
78:03 - and then we actually get more queries so
78:06 - essentially we use a large language
78:08 - model to suggest additional queries and
78:12 - then we retrieve results for original
78:14 - and new queries through the database or
78:17 - vector space and that's what we actually
78:20 - pass through and then we send all those
78:23 - response to the large language model for
78:26 - a final relevant answer so that is the
78:30 - overall workflow how the core expansion
78:32 - with multiple queries works so
78:34 - essentially we have more queries instead
78:37 - of one answer in this case we're
78:39 - actually creating queries as opposed of
78:41 - creating answers in an ed shell we have
78:44 - the original query analysis part that
78:46 - happens so we're analyzing the original
78:49 - user query to understand its intent and
78:52 - context and then we have the subquery
78:54 - gener ation so this is where we generate
78:56 - multiple subqueries that expand on
78:59 - different aspects or interpretations of
79:02 - the original query so we have breath of
79:05 - different subqueries now this can be
79:07 - done using synonym expansion related
79:10 - terms or contextually similar phrases
79:12 - you could have in your organization or
79:15 - in your business you could have a list
79:17 - of these subqueries but we're going to
79:19 - use the large language model to
79:21 - hallucinate those subqueries and then we
79:24 - have the document retrieval so now we
79:27 - retrieve the documents for each subquery
79:29 - separately that is the beauty here and
79:31 - then we have the combination this is the
79:33 - aggregation side of things where we
79:35 - combine the documents retrieved from all
79:38 - subqueries that way ensuring a diverse
79:40 - and a more comprehensive set of relevant
79:43 - documents and then of course the
79:46 - response generation this is where we use
79:49 - the aggregated documents to generate a
79:51 - more informative and contextually relev
79:54 - an response here are some use cases for
79:57 - quer expansion exploring data analysis
80:00 - so this could be uh a way of helping
80:03 - analysts explore different facets of
80:06 - data by generating varied subqueries uh
80:09 - we have academic research again we've
80:12 - seen this before to provide researchers
80:14 - with different angles on a research
80:17 - question by generating multiple
80:19 - subqueries customer support this helps
80:22 - in covering all aspects of a user's
80:25 - query by breaking it down into small uh
80:29 - specific subqueries Healthcare
80:32 - information system so helping uh with
80:35 - retrieving comprehensive medical
80:36 - information by expanding queries to to
80:40 - include related symptoms or treatments
80:43 - and diagnosis let's go ahead and put all
80:46 - together in code so I put together this
80:49 - expansion queries atpy it should have
80:51 - access to all of this anyway and let's
80:53 - get started so I went ahead and did some
80:56 - imports so most of this actually is
80:58 - going to be exactly the same as we had
81:00 - before but I'm going to go through again
81:02 - just to for
81:04 - completion Okay so we've imported
81:06 - everything we have the open AI client
81:08 - and everything is
81:09 - good and first we are going to do what
81:12 - we did before which is to read
81:15 - everything through uh using the PDF
81:18 - reader to get our data okay so we get
81:21 - our data Microsoft angle report PDF and
81:24 - and extract those texts okay from the
81:28 - page and we're going to filter the empty
81:30 - strings as you see here okay so I'm not
81:33 - going to run this again and then I'm
81:35 - going to split all of those pieces into
81:39 - smaller right chunks as you can see here
81:42 - and I'm using the recursive character
81:45 - text split from Lang chain and we're
81:47 - going to always be using the sentence
81:50 - Transformers token text spater as we saw
81:53 - before so essentially really the same
81:55 - thing that we did before okay use the
81:57 - sentence Transformer to get split into
82:01 - tokens as you see here that's what we're
82:02 - doing we have the text splits and we
82:05 - have all set up in this list here we are
82:09 - looping through and adding that to that
82:11 - list
82:12 - of token splits and next we are going to
82:16 - import chroma DB and all of that again
82:19 - so I know this is kind of Overkill but
82:21 - I'm going to put all of this for
82:23 - completion okay okay I'm going to so now
82:25 - we're creating our embedding function
82:27 - using the sentence transform embedding
82:29 - function and so we instantiated our
82:31 - chroma DB CLI okay and all of this is
82:35 - good next we're going to add all of
82:37 - those embeddings into our chroma DB but
82:40 - first we're going to extract the
82:41 - embeddings of the tokens as we did
82:44 - before and we added all of those okay we
82:47 - have the count there as
82:49 - well and now I'm going to add the query
82:52 - pretty much the same as we saw before so
82:54 - what was the total revenue for the year
82:57 - pass that query through our collection
82:59 - to see what we get so collection query
83:03 - we pass the query until here let's go
83:05 - ahead and make sure that we actually
83:07 - getting the documents so I'm just going
83:09 - to Loop through all the retriev
83:11 - documents and
83:13 - see okay very good the same thing we've
83:15 - seen before we're getting all of our
83:18 - documents at least five 1 2 3 4 and five
83:24 - next let have a multi-query function
83:27 - that will be responsible for generating
83:29 - the multi- queries so the difference
83:31 - here is that in our prompt in this
83:33 - generate multiquery we still pass the
83:36 - actual query and the model is defaulted
83:39 - at GPT 3.5 turbo and here the prompt
83:43 - here we saying you're a knowledgeable
83:44 - Financial research assistant and your
83:46 - users are inquiring about an anual
83:48 - report but for the given question
83:51 - propose up to five relevant or related
83:54 - question to assist them in finding the
83:56 - information they need now this is very
83:58 - important because everything is driven
84:01 - through the prompt so your prompt should
84:04 - be something that you put some thought
84:06 - into it and the important thing here is
84:08 - that you need to make sure that we have
84:10 - a prompt that encourages the model to
84:12 - actually generate related questions or
84:15 - queries so make sure that the prompt
84:17 - must include different aspects of the
84:22 - topic and the variety of questions
84:25 - that will help the model understand the
84:27 - topic better provide concise single
84:30 - topic questions without compounding
84:32 - sentences that cover a variety or
84:34 - various aspects of the topic I ensure
84:37 - each question is complete and directly
84:39 - related to the original inquiry very
84:41 - important to have a well refined prompt
84:44 - of course we pass the prompt and the
84:46 - query which is going to be passed
84:48 - through the function let's go ahead and
84:50 - generate the multi-query so at the
84:53 - bottom here I'm going to just paste so I
84:56 - have the original query what details can
84:58 - you provide about the factors that led
85:00 - to revenue growth for instance and so
85:02 - the augmented queries here I call the
85:05 - generate multiquery and pass to original
85:07 - because that is what it needs so at this
85:09 - point I should get result that will give
85:12 - me what I need step through and get
85:14 - augmented queries okay so I'm going to
85:16 - just show
85:18 - here so let's query through all the
85:20 - augmented queries we're going to get and
85:23 - print them out and at the top let's make
85:25 - sure that I have con okay very good this
85:27 - is already commented out just going
85:30 - through the process and soon we should
85:32 - see and just like that you can see we
85:34 - have our augmented queries so here is
85:37 - the first one how do changes in pricing
85:39 - strategy impact Revenue growth what role
85:43 - did new product launches play in driving
85:45 - Revenue growth where were there any
85:48 - specific marketing or advertising
85:49 - campaigns that significantly
85:52 - contributed that significantly
85:54 - contributed to revenue growth how did
85:57 - changes in customer demographics
85:59 - influence renew uh Revenue growth that
86:02 - Partnerships or collaborations with
86:04 - other companies impact Revenue growth
86:06 - this is wonderful so you can see that
86:09 - our large language model able to plug in
86:12 - from the query that we passed along uh
86:15 - the original query we're able to plug in
86:18 - and get at least five new queries five
86:22 - new um augmented
86:25 - queries now that we see that we have our
86:28 - augmented queries because we look
86:30 - through and we saw of them all of those
86:32 - were generated let's go ahead and join
86:35 - or concatenate the original query with
86:37 - the augmented queries okay let's go
86:39 - ahead and do that let's go ahead and
86:42 - print join query so we can
86:48 - see okay so we can see that we have the
86:51 - augmented queries still loading that's
86:53 - very good but then we have this list
86:56 - that will contain each one of our
86:58 - augmented queries with the original
87:02 - query okay so you can see here the what
87:05 - details can you provide about the
87:07 - factors that led to revenue growth and
87:09 - then we have the augmented query what
87:11 - were the key products which is the first
87:14 - one here so for each one of these we are
87:16 - concatenating or attaching to the
87:19 - original queries as well as the
87:21 - augmented query so we are concatenating
87:23 - both of them and you can see they're all
87:25 - in a list now we're going to go ahead
87:27 - and get this joint query which will have
87:30 - of course the original query and the
87:32 - augmented queries through
87:34 - our collection in this case through our
87:36 - Vector database and query so let's do
87:40 - that and there we go so now we have
87:42 - retrieved results and next thing what we
87:45 - need to do because we might get multiple
87:49 - duplicates now because of the nature of
87:51 - things we might end up getting multiple
87:53 - duplic duplicates we can now remove
87:56 - duplicates and Sally sanitize the list
87:59 - that comes from our retrieved documents
88:02 - from our collection. query here okay so
88:05 - to do that let's go ahead and so now
88:08 - we're going to go ahead and just go
88:10 - through and sanitize our list to remove
88:13 - the duplicates and let's go ahead and
88:16 - output results documents okay okay let's
88:19 - go ahead and run this so should go and
88:21 - output result documents for each query
88:23 - going to add add that query and then
88:25 - result in Loop through and get each one
88:27 - of those results okay let's go ahead and
88:28 - run this real
88:30 - quick okay there we go there's a lot
88:34 - here looks like gibberish but exactly uh
88:36 - that's exactly what we're doing so we're
88:38 - outputting the results documents so at
88:40 - this point we have passed everything
88:42 - through the query and now we're looking
88:44 - through and get the query results so all
88:46 - the documents that we were're able to
88:49 - retrieve through the queries
88:51 - concatenated with the query the query
88:55 - all right so next step really is to go
88:57 - through pull all that information pass
89:00 - that through a large language model so
89:02 - we can get the answer all right so now
89:05 - what we can do is just plot everything
89:07 - so we can see the results right in a
89:09 - graph so let's go ahead and project
89:12 - everything okay so we pass the
89:14 - embeddings just like we did before we're
89:15 - getting all the embeddings from our
89:19 - collection and then we instantiating the
89:21 - umap transform so that we can have that
89:24 - to uh pass through the data set
89:27 - embeddings and project embeddings to
89:30 - project everything okay so same thing
89:32 - we've seen before so next we're going to
89:34 - visualize results in edding space so by
89:37 - creating the original cor embedding and
89:39 - then the augmented query embedding so we
89:41 - can see the differences we're going to
89:43 - project the original query and augmented
89:46 - queries in embedding space but because
89:48 - because of the nature of things we're
89:49 - going to flatten the list of the
89:51 - retrieval retrieve documents to a single
89:54 - list because the project embeddings
89:57 - expects indeed a single list of
90:01 - embeddings so that's what we're doing
90:02 - there and what we'll do next is we're
90:05 - going to go ahead and retrieve uh the
90:07 - embeddings themselves and result
90:10 - embeddings just looping through all of
90:12 - them and get them and we're going to go
90:15 - ahead and project those embeddings the
90:19 - result embeddings and you map transform
90:20 - the we passing through as you see there
90:23 - and of course as always we're going to
90:24 - go ahead and plot everything using the
90:27 - the mat plot lib so we're going to have
90:30 - to import
90:32 - that and then we're going to go ahead
90:35 - and pass it through the plot figure the
90:39 - plot object and project everything okay
90:41 - let's go ahead and run and we should see
90:43 - if all goes
90:44 - well something
90:46 - projected again you can see here are our
90:49 - results everything is pretty much
90:51 - squished in Red X which is the original
90:55 - queries and then we have the orange axis
90:58 - which are the augmented new queries that
91:01 - were generated okay by the large
91:03 - language model the green circles are the
91:06 - retrieved documents results by the
91:09 - vector database search okay and the gray
91:12 - dots are the data sets embeddings now
91:14 - the retrieve documents are close to the
91:17 - augmented query as you can see here all
91:20 - these yellow or orange X's are the
91:23 - augmented query and you can see most of
91:25 - them are really aggregated around the
91:28 - augmented query which is really good it
91:30 - tells us again that we're able to see
91:32 - the closeness and not so close to the
91:36 - original one even though there's this
91:38 - closeness between the original and the
91:40 - augmented queries here but the actual
91:42 - documents that are relevant are
91:44 - agglomerated or are around the augmented
91:48 - query as you see okay much better so you
91:50 - can see that we have 1 2 3 4 5 6 gu six
91:55 - augmented queries five or six or some
91:57 - sort something like that and of course
92:00 - we only have one original query so this
92:03 - is probably better for you to visualize
92:05 - so we see that with the query expansion
92:07 - we are able to retrieve more relevant
92:10 - documents that we might have missed with
92:13 - the original query which gives us a
92:15 - better understanding of the topic and
92:17 - helps in answering the original query
92:20 - because the model generated queries the
92:22 - hallucinated queries have helped us in
92:25 - capturing different aspects of the
92:28 - original query and provide a more
92:30 - comprehensive view of the topic so the
92:33 - model generated queries help in
92:35 - capturing different aspects of the
92:37 - original query so now we have a more
92:40 - comprehensive view of the topic and can
92:42 - better answer the original query
92:44 - especially for cases where we have
92:47 - complex topics like Financial reports
92:49 - and so forth now there's a downside to
92:52 - this the downside of the this approach
92:54 - is that the model generated queries
92:56 - might not always be relevant or useful
92:59 - and can sometimes introduce noise in the
93:02 - search results so it's important to
93:04 - carefully evaluate the generated queries
93:07 - and the retrieved documents okay so in
93:10 - the next videos uh we are going to
93:12 - tackle this issue of noise that I'm
93:14 - talking about in the search results by
93:16 - using a more advanced technique that
93:19 - will allow us to rank in this case all
93:22 - of these documents so we can get the
93:25 - relevant feedback so before we move
93:28 - forward here I would like for you to do
93:31 - a challenge so what I want you to do is
93:33 - to play with different prompts and
93:35 - queries uh to see what results you get
93:38 - each time so keep refining The Prompt
93:41 - and see the results so this is very
93:43 - important because as we've talked about
93:46 - the reprompt is what guides everything
93:48 - for you to be able to get those multiple
93:50 - queries that are related to the query
93:53 - which is going to influence of course
93:56 - the documents that you end up getting
93:58 - from the vector database so we just
94:01 - finish going through the query expansion
94:03 - in this case with multiple queries which
94:06 - allows us to use the large language
94:08 - model to alistate or generate additional
94:11 - queries that might help us getting the
94:15 - most relevant answer so this is the
94:17 - overall flow where we create queries
94:21 - from the large language model and then
94:22 - we concatenate
94:24 - the the original query with the
94:26 - augmented queries as we call it and then
94:29 - we extract the relevant information from
94:31 - the factor database the square results
94:34 - and then we can use that to do all sort
94:36 - of things to get the actual answer of
94:38 - course use cases for qu expansion range
94:41 - from exploring data analysis academic
94:44 - research we've seen this before customer
94:46 - support Healthcare information systems
94:49 - and so forth as I said there are some
94:50 - downsides to this technique because
94:54 - there's lots of results that means
94:56 - queries might not always be relevant or
94:59 - useful because we end up having noise
95:02 - hence we need another technique to find
95:04 - relevant results so once we get these
95:07 - results from this technique we should
95:10 - probably go a little bit further and
95:13 - find what we just received as a result
95:16 - from this technique okay very well I
95:18 - hope you enjoyed this mini course I made
95:21 - it for you and that you see the whole
95:24 - picture of rag systems now the idea here
95:27 - is that you take this and you build your
95:29 - own rag systems and understanding now
95:33 - the techniques the Advanced Techniques
95:35 - that you can use to make your rag
95:37 - systems even better so that they don't
95:39 - hallucinate as much and you get the
95:42 - correct or the most the closest um
95:45 - information or uh pieces of information
95:48 - from your documents that then you pass
95:50 - through the large language model so then
95:52 - you have a full Fuller response that you
95:55 - confident that it is exactly what is
95:57 - being retrieved from the system so thank
96:01 - you so much for being here so if you're
96:03 - interested in learning more about rag
96:05 - about AI agents and Python and
96:08 - programming in general I do have a
96:11 - channel called Vinci bits so it's right
96:13 - here and also I'm working on a larger
96:15 - more comprehensive course and or courses
96:19 - related to AI large language models and
96:23 - creating AI based applications from Zero
96:26 - to Hero right and so if you're
96:28 - interested I should have a link
96:30 - somewhere in the descriptions where you
96:32 - can go and drop your email in the
96:34 - mailing list in the waiting list I
96:36 - should say and as soon as I have
96:37 - everything set up you will be the first
96:40 - one to be notified so again thank you so
96:44 - much for your time and till next time be
96:47 - well