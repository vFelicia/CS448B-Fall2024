00:00 - In this course, you will learn
the basics of reinforcement
00:03 - learning and how to implement it
using gymnasium. gymnasium is a
00:08 - software library by open AI that
provides a collection of pre
00:11 - built environments for
reinforcement learning agents.
00:15 - In this course, Mustafa will
show you how to use gymnasium to
00:18 - test and improve your
reinforcement learning
00:21 - algorithms.
00:22 - So what will be the objectives
of the course. So what we'll be
00:26 - doing is, we'll be learning the
basics of reinforcement learning
00:30 - and opening a gymnasium, you
will learn how to use the
00:33 - gymnasium interface to interact
with different environments.
00:36 - We'll explore various gymnastic
environments, so environments
00:40 - could be like a taxi
environment, it could be like a
00:42 - gadget environment, you can have
different Atari games as well.
00:45 - And we'll also understand their
characteristics will build and
00:49 - evaluate reinforcement learning
agents using given as
00:52 - prerequisites knowledge of
Python programming language, so
00:55 - you need to be familiar with
basic loops. What is a class
00:59 - we'll have different methods. So
I'll explain it to you while as
01:02 - we go along basic understanding
of reinforcement learning
01:06 - concepts. So whatever the topics
are that are required, I'll be
01:09 - explaining it to you. While we
are while we start the course.
01:14 - Introduction to reinforcement
learning, what is reinforcement
01:17 - learning, we will learn the RL
technologies. And also we will
01:21 - understand the reinforcement
learning workflow. My next
01:25 - section is introduction to
opening a gymnasium. So I'll
01:28 - introduce you to open a
gymnasium and also we will set
01:31 - up the gymnasium environment.
And I'll also I'll explain you
01:35 - the basic concepts that are
required for using gymnasium. So
01:39 - it could be the environment
agent observation, action and
01:43 - the word. The next section is
gymnasium environments. So we'll
01:48 - understand the types of
gymnasium environments. We will
01:54 - understand the difference
between continuous and discrete
01:56 - actions and observations
understanding the properties of
02:02 - different gymnasium environments
and also the games classic
02:06 - control Atari back to the
robotics we got different types
02:10 - of the environments available
and there are many more
02:12 - available so like Jack we have
taxi environment we have frozen
02:15 - like environment. So now let the
next question is the gymnasium
02:19 - interface will be interacting
with environments using the gym
02:23 - interface will understand the
gymnasium API. And we'll create
02:27 - some custom environments. Now
the next section is building
02:31 - reinforcement learning agents.
So we'll be implementing RL
02:35 - agents using gymnasium will
understand the algorithm Q
02:38 - learning will be training and
evaluating RL agents using
02:41 - gymnasium. And we'll be
improving RL agents with policy
02:45 - search and other techniques.
Basically, we'll be solving
02:49 - blackjack using Q learning this
will be the next section. So let
02:52 - me just write it down
03:07 - that will be the tutorial and
these are the advanced topics.
03:14 - So after you get a basic
understanding of these of their
03:18 - of reinforcement learning
gymnasium, and solve some server
03:22 - problem, then it can go and
explore the other advanced
03:25 - topics. So the these are the
using a porn AI baselines for
03:32 - benchmarking RL algorithms,
creating and using rappers to
03:35 - modify environment dynamics,
combining multiple environments
03:39 - to create more complex
scenarios. So we can have two
03:41 - taxes coming at each other. Or
you can have multiple players in
03:46 - a poker environment. So
basically, you can simulate
03:48 - those environments. Also, we'll
be integrating with other tools
03:51 - such as TensorFlow and pytorch.
Okay, so basically, this is the
03:55 - course. Now let's look at the
conclusion. So by the end of
03:58 - this course, you will have a
very strong understanding of
04:01 - reinforcement learning and open
air gymnasium, you will be able
04:04 - to use the museum to build and
evaluate RL agents. And we'll
04:08 - have a very good understanding
of the different gymnasium
04:11 - environments available. This
code is an excellent starting
04:14 - point for anyone interested in
developing or working with
04:16 - reinforcement learning system.
Let's start with the basics of
04:24 - reinforcement learning. It is a
type of machine learning in
04:28 - which an agent learns to make
decisions by interacting within
04:30 - the environment. So basically,
as you can see, I have an agent
04:34 - and I have an environment. So
basically, let's take an example
04:37 - of a robot to what will happen
is that the robot when it takes
04:43 - an action Suppose it takes a
step forward. So, we will reward
04:47 - either we will reward the robot
for that action or either we
04:50 - will penalize it for the action.
So how will you decide that? So
04:55 - what my basic policy would be
that suppose if the robot takes
04:59 - a step and it banged into wall.
So, we will have to penalize it
05:03 - and we will deduct some point
for that. Or if the robot takes
05:06 - a step, and it doesn't bang into
anything, that will be a good
05:11 - action, so we will have to give
it some points. This is
05:14 - basically that's what will
happen. So, what is an agent? An
05:18 - agent is an entity that
interacts with an environment in
05:21 - order to learn how to make
decisions that will maximize a
05:24 - specific goal or objectives. So,
basically, in our case, when we
05:28 - have a robot need it to take
something from one place to
05:32 - another place to the city, it
has to reach the particular
05:35 - endpoint. So, we will have to
train the robot to reach that
05:39 - goal. So, it will take the
decision autonomously as it
05:44 - receives input from the
environment, perform the actions
05:47 - and receive rewards or penalties
based on these actions. The
05:51 - environment on the other hand is
the external system or context
05:54 - in which the agent operates. So
basically, in our case,
05:57 - everything that is nearby to the
robot is the environment. So
06:02 - basically, it could be a
warehouse to the full warehouse
06:04 - will be the environment and it
will be interacting with the
06:08 - robot will be interacting with
the environment. So it can be a
06:11 - simulation of physical system or
any other system that the agent
06:13 - interacts with. The environment
provides feedback to the agent
06:17 - in the form of rewards or
punishments. When the agent uses
06:20 - to learn how to make better
decisions. The agent receives
06:23 - feedback in the form of rewards
or penalties. So as we discussed
06:26 - earlier in the robot kills, when
he takes a step forward, and it
06:30 - doesn't matter to wall, we will
give it a reward. And our goal
06:34 - is to maximize the total reward.
So if it takes a step forward,
06:38 - and doesn't bind it to anything,
it will eventually reach the
06:41 - goal. So it is a trial and error
process, observing the
06:46 - consequences and adjusting its
behavior based on the rewards it
06:49 - receives. Over time, the agent
learns to identify the actions
06:53 - that lead to the highest rewards
and avoid those that lead to
06:55 - penalties. So as we keep
training the robot, it will
07:00 - eventually learn what decisions
to take autonomously. So it
07:05 - adapts to new situations and
environments. And it will adjust
07:09 - its behavior based on the
feedback it receives and improve
07:13 - the performance. And it will
become efficient decision maker,
07:18 - which leads to better
performance as well. So let's
07:21 - also look at some real world use
cases of reinforcement learning.
07:24 - So basically, the example that
we covered is robotics. We could
07:28 - perform tasks such as grasping
objects navigating and
07:31 - interacting with humans. For
example, RL has been used to
07:34 - train robots to play tennis,
table tennis, where the robot
07:38 - learns to adjust its movement
based on the position of the
07:41 - ball. So let's look at another
example of which is the game
07:45 - playing. So like the most famous
example is of Atari Games. So
07:50 - like DeepMind launched Atari
game module. Reinforcement
07:58 - Learning has been applied to
game learning, particularly in
08:01 - the development of artificial
objects. Or we can also talk
08:04 - about Blackjack, we can talk
about chess, so a lot of games,
08:07 - Atari Games AlphaGo, a computer
program developed by Google
08:11 - DeepMind, which is reinforcement
learning to learn to play the
08:15 - game of go at a world class
level. Also, we can use
08:18 - reinforcement learning for
autonomous driving.
08:20 - Reinforcement learning can be
used to train autonomous
08:22 - vehicles to make decisions in
complex and dynamic
08:24 - environments. For example,
reinforcement learning can be
08:28 - used to train a self driving car
to navigate to the traffic,
08:31 - world obstacles and make safe
and efficient driving decisions.
08:35 - Also, another very good example
is of personalized
08:39 - recommendations, which is
basically Netflix or Google
08:42 - recommendations. Reinforcement
learning can be used to provide
08:45 - personalized recommendations to
user based on their preferences
08:48 - and behavior. For example, we
can use RL to optimize the
08:51 - recommendations of a video
streaming service, learning what
08:54 - content to recommend to the
users and to maximize user
08:59 - engagement and satisfaction. So
now we have learned about the
09:03 - agent and the environment. So
now let's move on to gymnasium,
09:06 - which is a toolkit that will
help us simulate these
09:09 - environments for our agent. So
basically, it is a set of a
09:15 - collection of environments, or
tasks that can be used to test
09:18 - and develop reinforcement
learning algorithms. These
09:21 - environments are typically game
like with well defined rules and
09:26 - reward structure, making them
useful for evaluating and
09:29 - comparing different
reinforcement learning
09:31 - algorithms. So what I'll do is I
head over to the documentation
09:34 - page and show you the different
types of environments that are
09:37 - available. So this is the
introduction page. So basically,
09:42 - it is just two lines of code to
set up an environment and you
09:46 - can interact with the
environment as well. So I
09:48 - encourage you all to go and
check out the documentation and
09:51 - also explore the different types
of environments that are
09:53 - available. So we have a Cardpool
environment we have a mountain
09:56 - car. Also interesting
environments. We have a car
10:01 - racing environment as well.
Basically, for gymnasium, there
10:04 - is only one single agent. But if
you want multi agents, then we
10:08 - have another library called
petting zoo. We have a bipedal
10:14 - Walker here. And another
interesting environment is
10:18 - obliging environment. So in this
tutorial, I'll be covering
10:21 - blackjack environment. And
another environment is a taxi
10:24 - environment. So basically, this
is like our cell running
10:26 - example. Also, we have a frozen
lake environment, forking, these
10:34 - are all the very interesting
environments. So it could range
10:38 - from games like Pong, or record
to more complex simulations like
10:42 - robotics or autonomous driving.
The NASM environments are
10:45 - designed to be easy to use, and
come with a standard interface
10:48 - for interacting with the
environment. Now, basically,
10:51 - what all steps do, we need to
start interacting with
10:54 - gymnasium. So let's check it
out. So we need to define the
10:57 - environment that we want to work
in P to create an instance of
11:01 - the environment. So that's what
we did here. So basically, we
11:05 - use them that make and use the
environment that we want. We
11:09 - will define the agents policy,
how it decides which action to
11:12 - take, interact with the
environment, taking actions and
11:15 - receiving rewards, update the
agents policy based on the
11:18 - reward it receives. So just to
go back, we have an example of a
11:24 - robot. When we take that example
and apply these rules, we'll be
11:28 - able to interact very easily. So
let me just to show you how we
11:31 - can do that to an environment
will be the warehouse
11:33 - environment, an instance of the
environment we'll create first,
11:37 - we'll define the policy. So in
our case, the policy was that
11:40 - once the robot takes a step
forward, and if it doesn't crash
11:46 - into a wall, or even cat class
into another word, that is a
11:50 - successful step, and then we
reward it, otherwise, we'll have
11:53 - to penalize it. So basically,
this is the fourth step, we have
11:59 - to update the agents policy. So
what we have to do is we need to
12:02 - update it that once you take a
successful tip, you need to keep
12:06 - taking successful steps and you
will in the end, and we'll keep
12:12 - repeating it until our
performance is satisfactory.
12:19 - Some of the main concepts we
will need in open AI gymnasium.
12:24 - So the first we have is
observation and action spaces.
12:27 - And our observation space is the
set of possible states that an
12:31 - agent can observe in the
environment. And Action space is
12:34 - the set of possible actions that
an agent can take in an
12:37 - environment. So let's take the
example of our robot. So what is
12:42 - the thing is that I have my
robot right here. And the
12:45 - possible actions that it can
take is either it can travel in
12:49 - the front, or it can double to
the right
12:54 - or to the left or take a step
back. So this is the possible
13:01 - action space that can take also,
in this case, the observation
13:06 - space will be an ideal place to
be 567 action space is different
13:18 - than the observation space. This
is the full action space and
13:23 - observation space will be
changed.
13:37 - This is action space. This is
the observation space. Let's
13:42 - look at episode. An episode is a
complete run through of an
13:46 - environment starting from the
initial state and continuing
13:50 - until a terminal state is
reached. So let's look at that
13:54 - in our example as well. So I
have my robot right here. And I
13:57 - wanted to reach here. So this
would be an episode and I can
14:01 - train the robot to run multiple
episodes. So it will start at a
14:04 - particular position and it will
reach this terminal position. So
14:08 - that will be one episode it will
start there and could go here as
14:11 - well. Basically, this could be a
terminal location or it could
14:14 - this could be the start location
and this could be the terminal
14:16 - location. These are all the
different episodes that we have.
14:22 - Each episode is composed of
sequence of states action and
14:25 - towards rapper rapper is a tool
in open engine that allows you
14:30 - to modify and environments
behavior without changing its
14:33 - code. So we'll look into that
later on. Wrappers can be used
14:37 - to add features such as Time
Limits, reward shaping, and
14:39 - action must be benchmark open
and Jim provides a set of
14:44 - benchmark environments, which
are standardized tests that can
14:46 - be used to evaluate and compare
reinforcement learning
14:49 - algorithms. So if we have
multiple algorithms for a
14:53 - particular environment or a
problem, we can use the
14:55 - benchmark to compare them We are
done with the basics of
15:03 - reinforcement learning. And also
we have learned the agent
15:07 - environment and everything that
is needed to get started. So now
15:11 - let's implement the game of
blackjack using gymnasium.
15:15 - First, let's get introduced to
the game of blackjack. So what
15:21 - are the basic rules, so the
basic rules is that I need I
15:24 - will have two cars and the
dealer will have two cars. Now
15:27 - in this scenario, both will be
AI players. So the thing is that
15:33 - at each turn, I will have to
decide that I need a new car or
15:36 - do I want to keep my current set
of cards, but the dealer has to
15:40 - keep playing until he until the
E reaches over 70 The sum of the
15:45 - card which is over 70 And if I
have to when I need to be a
15:50 - bigger number, then my some of
the cards have to be greater
15:54 - than the dealer's card. So let's
go over all the basic rules of
15:57 - blackjack. This game is played
with one or more decks of
16:00 - standard playing cards. Each
dealer is dealt two cards and
16:03 - the dealer is also day two cards
with one card facedown. The
16:07 - value of each card is determined
by its rank. Aces can be worth
16:10 - one or 11. face cards kings,
queens and jacks are what 10 and
16:15 - all other cards our their face
value. Players have the option
16:20 - to hit and take additional cards
to improve their hand or stand
16:23 - and keep the current trend, the
dealer must hit until their hand
16:26 - has a value of 17 or more. If a
player's hand goes over 21 The
16:31 - bust and lose the game. If the
dealer's hand goes over 21 the
16:34 - player wins the game if neither
the player nor the dealer bust
16:38 - the hand with the highest total
value that is less than or equal
16:41 - to 21 wins to basically these
are the rules now let's head
16:48 - over to the gymnasium
documentation and check out how
16:50 - blackjack is implemented. So
here we are in the gymnasium
16:54 - documentation now let's check
out the action space and
17:00 - observation space here. So
basically we need to import
17:03 - blackjack we want to create the
blackjack environment this is
17:09 - the rules that we discussed no
action space the action space is
17:13 - one comma in the range of zero
comma one indicating whether to
17:16 - stick or hit. So it's a player's
decision with a dealer has to be
17:22 - played the observation consists
of a three tuple containing the
17:26 - Players Current sum the value of
the dealers one card one to 10
17:30 - where one is is and whether the
player holds a usable is one or
17:34 - zero. Basically this will be the
observation space our starting
17:38 - space will be four between four
and 11 dealer car has to 11 and
17:44 - other usable is if I ever usable
based on what will be the
17:48 - rewards in this particular
reinforcement learning problem.
17:51 - So we'll have win game as plus
one lose game as minus one draw
17:56 - game is zero and win game with
natural blackjack plus 1.5 If
18:00 - natural is true, and this one if
natural is false. So basically
18:03 - this is another type of set that
is the natural environment. So
18:09 - basically we can understand that
with natural is true whether to
18:14 - give an additional reward for
starting with a natural backdrop
18:16 - that is starting with an ace and
10. So we can specify that when
18:20 - we set up the environment. Now
let's see how we can implement
18:25 - it in Google collab. Now we've
covered the basics of blackjack.
18:32 - Now let's understand how we can
solve this game of blackjack. So
18:37 - let's first discuss a book that
is very popular regarding to
18:40 - blank that it's called
reinforcement learning and
18:43 - introduction by Richard Sutton
and Andrew Bartow. It covers the
18:47 - basic applications of
reinforcement learning. And also
18:51 - there is a chapter on Blackjack,
and it also covers other games.
18:54 - And they have modelled it a
Markov decision process, and how
19:00 - every action influences the
outcome of the game. Now also
19:04 - they explain how reinforcement
learning algorithms can be used
19:07 - to learn optimal policies for
playing blackjack based on
19:10 - maximizing the expected return
over the long term. So now this
19:14 - seems really complicated, but
this is the beginner course you
19:17 - don't need to learn this. Once
this course is over and you
19:21 - still have a curiosity you can
go check out this book. And
19:25 - because why should you check out
this book because it presents
19:28 - several approaches to solving
the blackjack problem including
19:31 - Monte Carlo methods and
temporary difference learning.
19:35 - It also covers value functions
and policy improvement, which
19:38 - are a very advanced topics. So
maybe we can create a course on
19:42 - that but as this is a beginner
course, you do not need to know
19:45 - all these. So let's start with
the basic implementation. What I
19:54 - will be doing is I will be
importing the libraries that
19:57 - we'll be needing for us to
install I'll be copying this
20:04 - multiple times
20:13 - and I plot lib, then let's do
NumPy. See one. So basically
20:20 - NumPy is used for data
manipulation. As we import I
20:23 - just explained to you when we
bought what each library is used
20:26 - for, take UDM we are in
synchronism.
20:37 - 0.2 7.0. So basically, this is
the LT S support Long Term
20:42 - Support version. And let's do
matplotlib in line. So, as you
20:48 - all know, when accurate lib is
used, it opens a new window. But
20:51 - in collab, Google collab, there
is no window. So basically, the
20:55 - plots should be in line. So
basically we'll do this
20:58 - percentage matplotlib in line
30. So all the plots below the
21:03 - cell. So yeah, it's done, let's
do pip install. So I already
21:11 - have it installed, but in my
machine, it'll take some time to
21:13 - install by till it get
installed. Let's also import all
21:17 - the necessary materials that
we'll need. So from collections,
21:23 - I will import defaults. So why I
am using a default is because it
21:29 - allows us to access keys without
checking that if the value is
21:34 - accessed, its value exists from
input map plot lib.pi plot as
21:43 - PLT. So this is just also write
down why we'll be using
21:59 - access to you find the keys keys
that do not exist. Check for us,
22:13 - instead of us having to always
check if that keys exist, it
22:16 - will just take it for us. We'll
be using this for going clots
22:23 - from Mac lot plot lib dot
patches. So basically this is
22:28 - for creating shapes or shapes.
Let's import NumPy. So NumPy is
22:38 - very popular. Michigan NumPy is
for array manipulation and image
22:43 - manipulation. Manipulation
22:54 - there's two input C one C one is
a very popular data
22:57 - visualization library as soon as
and from TQ DM so basically this
23:04 - is for a loader. So whatever
training that happens, it will
23:07 - show it as a loader. So to be
very easy to understand cleaning
23:11 - covers. So I think we have
completed the installation of
23:16 - the basic libraries. Now let's
so let's so what I'll be doing
23:26 - is I'll be using the Jim that
make command to create a
23:29 - environment so it will be env is
equal to Jim. So I've already
23:34 - imported the gymnasium. Okay, I
forgot to input let me import it
23:39 - as well. So from import
gymnasium. Now let's do gym dot
23:48 - make. So our environment name is
Blackjack, we won. Check we
23:57 - want. And what I'll be doing is
I'll be setting the SAP
24:02 - parameter to true what is SAP
parameter it is the natural
24:06 - environment, basically the
default state of certain and the
24:10 - book that we discussed. So what
I'll be doing is and also I'll
24:13 - be setting the render mode is
equal to RGB grid because I'll
24:18 - be showing you the training
states so exactly what is the
24:22 - position of the card at that
particular moment in training?
24:27 - That will be interesting to
watch. That is why I will set
24:29 - this render mode to RGB array.
Now let's go and see how we can
24:36 - set up the environment. So I
will be observing the
24:39 - environment what I'll be doing
is I'll be setting this as Let's
24:43 - observe the environment
24:50 - so we'll be using the NV dot
start method to observe an
24:57 - episode so basically we
discussed what is an episode in
25:00 - is basically all the steps that
it takes for me to reach from
25:04 - the starting point to the
endpoint once. Basically, that
25:07 - is an episode. And for sitting
in Episode, we do it as
25:11 - observation, comma info is equal
to en v dot reset. So basically,
25:16 - we use the reset method to reset
the full board, or you said the
25:21 - particular environment. And
we'll be keeping done as far as
25:24 - because we have just started the
training, let's do let's have
25:30 - some comments. They said no
environment
25:38 - first observation So, and what
will be the output of this, so,
25:45 - observation is equal to it is a
tuple. And as we have discussed
25:50 - before, it will be 16 Nine and
false. So what is this tuple. So
25:55 - basically, this is my hand, what
I have been dealt, this is the
25:58 - dealer's hand. And this is the
number of if, do I have an ace,
26:03 - so, I don't have an ace at the
start, that is why I caught the
26:06 - output as false. So, it is a
Boolean and it is like the this
26:14 - value deposits that if I have a
as usable without busting So
26:18 - yeah, that is the thing, if I
can have an ace, but I can also
26:22 - bust as well. So, this is a an
ace which is usable, and without
26:27 - busting. So let's write that
down as well. So, what this
26:32 - output let's
26:48 - consistently when is
26:59 - the first value in the current
some, the second will be value
27:10 - of dealers sub called the third
will be Boolean whether the
27:28 - player holds, holds usable is
and what is a usable is where it
27:39 - is usable.
27:43 - Yes, it comes without busted.
Basically that is let's see how
27:54 - we can execute that. So, for
executing an action, first we'll
28:03 - receive our first Object
Observation should be basically
28:05 - that will be the starting space,
we'll be going to then after
28:09 - that we'll be using the end
third step. And we'll be giving
28:12 - the action to interact with the
environment. So in whatever
28:15 - example of the blackjack, it
will be that if I want to take a
28:19 - new cart, or do I want to keep
my current set of cards that
28:22 - will be passed in the action.
This function takes an action as
28:26 - an input and execute it in the
environment. Because that action
28:30 - changes the state of the
environment, it returns four
28:33 - useful variables to us these are
next state. This is the
28:36 - observation that the agent will
receive after taking the action.
28:39 - It will also give us the reward
the Agent will receive after
28:43 - taking the action. Either this
environment has terminated or it
28:48 - will give us truncated that if
the episode ended early, or the
28:55 - time limit is reached. This is a
dictionary that contains
28:58 - additional information about the
environment. The next reward
29:01 - terminated and truncated
variables are self explanatory,
29:04 - but the info variable requires
some additional explanation.
29:07 - This variable contains a
dictionary that might have some
29:10 - extra information about the
environment. But in blackjack v
29:13 - one environment you can ignore
it. For example, in atari
29:16 - environments, the info
dictionary has a fairly short
29:20 - lives key that tells us how many
lives the agent has left. If the
29:24 - agent has utilized then the
episode is over. So basically,
29:27 - these are the four values that
are important. After we take a
29:30 - step two let's see how we can
take a step now. So let's take a
29:35 - new cell and let's take the
action. So action equal to NV
29:44 - dot action underscore space dot
sample
29:54 - now what is this? This is a
sample action that will be
29:57 - taking sample a random action
Basically, this is the training
30:02 - loop. So that's why we are
taking a sample exam. All valid
30:07 - actions. Now we have hash action
is equal to one. And what we'll
30:14 - do is we'll execute, execute the
action, we see how we will be
30:20 - executing it. So we'll be taking
observation. So we discussed all
30:24 - the parameters here, right, so
we'll be taking all the four
30:27 - parameters, we'll be having
reward, we'll be having
30:30 - terminated, truncated, and so
forth. So we can ignore info but
30:36 - I be taking input, just in case
an action so action is the
30:43 - sample exit from here. Execute
the action.
31:00 - So we'll be receiving input and
receive info after taking it but
31:06 - taking the step. So what will be
the output if we run this so the
31:15 - sample output will be the
division is equal to So
31:18 - basically, we have learned what
this tuple means. This is a 24
31:22 - is my current sum to basically
I've gone bust. Now, also,
31:28 - first, I do not have any usable.
Basically, this is just a sample
31:32 - start. So I have some values
here. So I'll explain to you all
31:36 - these values. So and reward is
equal to minus 1.0. So
31:42 - basically, I've lost, so
terminated will be true.
31:51 - We have truncated
31:58 - and hash info, we call to
length. So yeah, this, this is
32:03 - how it's going to be for this
particular one simple step. So
32:09 - once we get the word terminated
is equal to true, what do we do
32:12 - then the current episode, we
have to reset. So if terminated,
32:19 - I had to take a text.
32:31 - needed is equal to true or we
should stop the current episodes
32:43 - and begin a new one using en
with a reset. So basically, I've
32:50 - talked about this method before
as well. And this is what is a
32:56 - single action that we take in
this particular environment. Now
32:59 - let's build our agent. So now we
have committed the basic setup
33:06 - of the environment and our
agent. Now let's understand the
33:09 - approach that we'll be using to
solve like that. So let's look
33:13 - at the epsilon greedy strategy.
This is a very optimum strategy
33:16 - to solve that. So let's
understand it. In this strategy,
33:20 - the agent takes an action that
is either the best action based
33:23 - on the current policy with a
probability of one minus epsilon
33:27 - or a random action with a
probability of epsilon. This
33:31 - approach balances the
exploitation of the current best
33:34 - policy with exploration of new
policies, which can lead to
33:37 - better rewards in the long term.
So basically, it is a very
33:41 - strict policy. Either I take the
current best policy or current
33:45 - best approach, or action, or
either take a random action. So
33:51 - how you set the policy let's
let's look at that later. But
33:54 - let's understand the epsilon
greedy strategy. In the context
33:58 - of blackjack, the epsilon greedy
strategy can be applied to
34:01 - determine whether the player
should hit or stand at each step
34:04 - of the game, the agent, that is
the player can choose to take
34:08 - the action that is either
recommended by the current
34:11 - policy, or it is a random
action. The policy is learned
34:15 - over time by updating the action
value estimates of each state
34:19 - action pair based on the rewards
received during the game. As the
34:23 - game is played repeatedly, the
agent learns the optimal policy
34:27 - that maximizes the expected
rewards. Basically, we as we
34:30 - keep on running the iterations
and the agent keeps learning, it
34:35 - will understand the optimum
policy that will maximize our
34:38 - expected reward. Initially, the
agent may explore by taking
34:42 - random actions to discover new
strategies. However, as the game
34:45 - progresses, the agent will start
to exploit the best known
34:48 - policy, which should maximize
the expected reward over time.
34:51 - So this is the approach that
we'll be using. Now, let's build
34:56 - our agent using this approach.
The epsilon greedy strategy
35:00 - Okay, so let's take a code cell
and create our class. So
35:05 - basically, we'll create a
blackjack agent class. And let's
35:11 - do def, we define the initial
constructor that is a default
35:16 - constructor. And what all
parameters do we need for this,
35:19 - so we'll take the self
betterment of the learning rate.
35:23 - Basically, this will be passed
as epsilon, this will be a
35:27 - float, stick initial epsilon be
a float as well. Let's take
35:40 - epsilon decay is a way of float
as well. Let's take final
35:48 - epsilon
35:53 - should be a float as well. And
let's take the discount factor
36:08 - and this will be 0.95 in this
case, so we have completed the
36:11 - initial Garaventa. Let's move
ahead and create our initial
36:18 - complete the initial
constructor. q underscore value
36:22 - is equal to default IC so we'll
be using default because we do
36:27 - not want to always check if that
key exists to be a lamda method,
36:31 - lambda function and P dot zero
Yeah, so we can just initialize
36:36 - it as
36:38 - np dot zeros. Let's set it at E
and V dot action space
36:47 - dot n. Now let's read some
comments as well just for
36:54 - explanation. So what are we
going to do we are going to
36:57 - initialize or also whenever I
say RL it is it is a
37:03 - reinforcement learning agent
with empty dictionary
37:11 - of state action values. This is
the key values learning rate and
37:24 - epsilon. Arguments let's
understand all the arguments as
37:31 - well. So we'll be calculating
the learning rate.
37:41 - So, I think these are pretty
self explanatory, let me just
37:46 - explain to you the discount
factor. So, the learning rate is
37:49 - the learning rate itself, the
initial epsilon value, epsilon
37:52 - decay, the decay for epsilon and
this will be the final epsilon
37:56 - value and the discount factor is
something that needs to be
37:58 - understood. Let's understand the
discount factor discount factor
38:06 - for computing the Q value so
I'll explain to you what the Q
38:08 - value is just first complete the
basic class of our agent.
38:23 - This is the approach that we are
going to be using using Q
38:26 - learning to solving blackjack
using Q learning. So, now let's
38:30 - complete our consulter
basically, this is the learning
38:36 - rate. discount factor is equal
to discount factor
38:52 - epsilon be able to initial
better know also good epsilon is
39:03 - equal to initial epsilon loop
self dot epsilon DK
39:18 - single equals
39:27 - final epsilon going to final
underscore epsilon cuts to the
39:38 - training error says training
error is equal to blank correct.
39:44 - Now let's create another method
that is the get action method
39:54 - basically, I need to complete
the integration. Let's do that.
39:58 - It may complete the end
condition yeah
40:12 - I'm just looking at
40:22 - this is a tuple that we
discussed observation tuple that
40:25 - we'll be getting, which is
intelligible and the output will
40:30 - be an integer. So this will be
the get action method where
40:35 - we'll refine the policy. So if
np dot random dot random maybe
40:42 - doing random is less than self
dot random dot epsilon
40:53 - so I think I could do return in
v dot action underscore sample.
41:03 - So we do taking the sample
action instead of taking the
41:07 - correct action. Yeah, else if
the probability is, is good
41:11 - enough. Let me take the return
of n p dot arg max dot q. So
41:29 - basically, this is the key
values that we'll be using your
41:32 - so I'll explain to you in just
some seconds. Me complete all
41:36 - the methods. So let's let's give
it some parameters here.
41:40 - Basically, this is this will
return
41:48 - return the best action be at one
minus epsilon otherwise random
42:07 - action with probability explore
for exploration. So yeah, this
42:10 - is the basically the approach
that we understood before.
42:19 - And short exploration. Let's
create another method first
42:31 - printing of what are the Q value
is, so basically, let me just
42:35 - write it down here. So the Q
value function
42:42 - is used to estimate the optimal
action to take in each state,
42:53 - the optimal action in a state is
the one it is the one
42:59 - that maximizes expected long
term rewards. So basically,
43:05 - we've understood the epsilon
approach to basically this is
43:08 - how we'll be maximizing it. Now,
this is a bit complicated the
43:12 - equation there is an arg max,
which will return the maximizing
43:16 - value. So I encourage you guys
to go and just understand the
43:21 - logic behind it. It's pretty
complicated, but it is this is
43:25 - basically the optimal action
that we have to take in each
43:28 - step. Basically, this will be
calculated for us in our in our
43:34 - game, pls. Let's move on now we
need to complete the update
43:40 - method for it's complete that
you have our def update and it
43:47 - will have self observation. You
see this is the operation tuple.
43:59 - So like you might be having some
errors. So when we compile it,
44:04 - we'll get to know all the
actions that do reward the
44:11 - workload to terminated boolean
and do next observation. Again,
44:22 - and to kill it.
44:29 - So we've understood what the cue
function is basically we'll be
44:32 - updating the cue function
44:44 - so let's understand it. Future.
value is equal to not terminated
44:54 - into np dot max self dot Q
values of next observation, we
45:03 - will be taking the Q value of
the next observation and
45:07 - multiplying it with the
terminating method. So, if it is
45:11 - terminated, then we have to
start again, if it is not, then
45:15 - we know, we know what need to
calculate this equal to. So, he
45:21 - has the reward. Basically, this
is the method that is a
45:24 - function. So, explaining to you
how we calculate it discount
45:29 - factor into future Q value minus
Admittedly, this is the literal
45:43 - formula that we use to calculate
it will explain to you, but
45:47 - let's go ahead and complete this
method. Observation of action.
45:55 - Three, and this is done the
queue method, key values done.
45:59 - Let's do the training. But now
we need to update the Q value.
46:04 - Now, we have completed the
difference now let's do the self
46:08 - dot q values of observations and
of the action 2d array is equal
46:16 - to self dot values. So,
basically this is what we have
46:22 - to do again, you score values
I'll be copying this plus self
46:33 - dot x revisit the learning rate
into the temporal difference
46:41 - and this will be self dot
training dot append temporal
46:51 - difference now, let's compare
the last method that dT d k
46:59 - epsilon.
47:18 - Lot salon minus salon decay was
ugly we have completed our class
47:27 - like the agent. So, now we've
completed the class of our agent
47:37 - three and we've defined all the
actions that the agent can
47:40 - perform. Now let's see the
training of the agent. So let's
47:46 - start the training. We'll be
taking it one episode at a time.
47:51 - So let's do the parameters for
servers we'll keep the learning
47:54 - rate 0.01 Visit the learning
rate we have the episodes equal
48:06 - to 100,000 we can change it like
just keep it 100,000 Because
48:12 - that's will give us a very good
trained agent, but I'll just
48:15 - reduce it for the tutorial. This
is the standard that you should
48:21 - keep if you want to get a real
world agent to start
48:41 - shorts by two so we reduce the
exploration what time we wanted
48:45 - to take the appropriate path
without having to explore too
48:51 - much to this will basically
reduce it. Then we have the
48:54 - final epsilon which really this
is where we need to get by the
49:00 - end of the training. Let's
create our agent this is this
49:03 - these are all the parameters the
hyper parameters doors, we have
49:12 - agent is equal to have a black
agent learning rate.
49:28 - learning rate we have the
initial which means you can just
49:34 - pop up on intubated is equal to
start. Start if you don't have
49:42 - one, and basically we need to
get to 0.1 from one
49:48 - and we have epsilon decay is
equal to epsilon decay. It's
49:51 - past that. Now also let's do it
the final epsilon. So this is
49:58 - the agent that we need Hello. So
now, let's turn the training
50:04 - let's turn to training now. So
basically we have completed the
50:07 - agent setup now. Now let's start
the training. So what I'll do is
50:11 - I'll just set up the environment
in v is equal to gym dot
50:17 - rappers, not record. So we will
be recording all the statistics
50:23 - and I will just show you after
the training, how the agent
50:26 - improves over time. So will you
plotting some charts as well. So
50:29 - let me complete the training
part first. And then we'll talk
50:33 - about what the training has, how
the training has improved over
50:38 - time, and what kind of all the
statistics would mean. We'll be
50:42 - putting charts and also I will
give you the statistics we have
50:48 - episodes and episodes okay, it
will be recording the statistics
50:56 - for episode now let's do that.
So we have under 1000 episodes,
51:00 - let's make it 10 For this
tutorial, just to that it will
51:03 - run pass. So I will also output
the environment at particular
51:09 - point so I will so every time
that a card is dealt or anything
51:13 - that is decision is made by the
agent will be plotting that
51:16 - particular environment and see
the what is happening in real
51:20 - time. So I'll be doing that as
well. So it's like watching live
51:24 - play live poker in real time.
Live blackjack and play time. So
51:30 - let's do that. Let's let's do
that. Now. We do end with a
51:34 - reset. Now why do we do in
winter reset because as we
51:37 - discussed, after every episode,
we need to reset the
51:39 - environment. Because I have
traveled from one point to
51:41 - another point I need to reset
the environment. Okay, so done
51:46 - is equal to false because we
need to complete the challenge
51:49 - need to get to the endpoint need
to clear up now I need to
51:54 - import. So what I'll be doing is
for every episode, I will be
51:58 - carrying the output because I'm
only interested in the current
52:01 - episode and what is happening
right now in real time. So I'll
52:05 - import it IPython display input
clear output. Basically, this is
52:11 - what I need to import. So
basically for every episode,
52:13 - this will clear the output. Now
let's complete third training
52:16 - for one episode. result we play
one episode as do while not
52:22 - done, so is also while we are
not done, let's run this loop.
52:28 - Let's do action equal to agent
dot get action of observation.
52:36 - Let's do next underscore OBS
comma toward comma terminated
52:42 - whistle all the information that
we'll get after making a
52:45 - particular step should be
truncated the info equal to E NV
52:52 - dot step. Let's update now let's
do agent dot update. We'll be
53:02 - passing all these parameters, we
have to pass observation action
53:08 - reward terminated and the next
position. Next observation
53:18 - frame. So basically I will be
rendering the frame so at
53:22 - particular instance what is
happening we can just output
53:26 - that as well. So I'll be doing
that we will able to end the
53:30 - Render loop PLT dot I am show
frame from Python, I'll be
53:36 - showing you the exact location
of the cards and exact position
53:40 - of the gods exactly what is
happening in real time. Now,
53:43 - what we have to do is we have to
check if that episode is
53:46 - terminated. So how will we know
that we have a parameter called
53:49 - terminated or we have truncated
so truncated is that we have a
53:53 - bust or terminated is that the
time limit is oh observation is
54:00 - equal to next observation. And
we have to do agent. So I am
54:08 - excited to follow me to agent
dot d k, epsilon. So over time,
54:14 - it will get smarter. So this is
it. So I'll be using the package
54:21 - called annotations because I'll
be using subscripts in my
54:24 - methods. So I got an error here.
Now let's run it and check
54:27 - again. If you're also getting an
error, make sure to import the
54:36 - package annotations. Now
everything's done fine. Let's do
54:40 - it. Yeah, basically. Basically
this is like running an ad is
54:46 - not defined. So it is not
defined. So I'd say okay, it
54:59 - should be brutal basically these
things happen yeah this sounds
55:04 - fine now that's tribes but
agreement equal to equal to
55:09 - where is that here let's see
55:14 - what I can do record episode so
it is en mi comma non thought
55:24 - basically these things happen so
so there is nothing in the
55:29 - gymnasium that happens so it is
rappers no rapper also has no
55:37 - attribute record episode
statistics so it is Np p Saudia
55:44 - obviously, this is episode
statistics
55:54 - now in Opie is not defined for
is not OB, this is our OBS, this
56:03 - is observation space. Now, we're
gonna continue to add episode
56:08 - when they already exist. To see
why that happens. Everything is
56:12 - done, let's run everything
through run all. Now if you get
56:18 - some errors, just make sure to
check out the integrations or I
56:22 - just link my Google colab
notebook in the description, you
56:25 - guys can go and check it out.
Because there were a lot of
56:29 - integration issues before
running it. Now that's it. So as
56:34 - you can see, for every episode,
we are able to visualize it. So
56:37 - divisibility when you 121, then
it's a drop in both are 21.
56:45 - Let's see it is 12 It's pretty
fun to watch. So now currently,
56:52 - I've said the episodes over
100,000, I can change it and
56:55 - keep it as 10. But I'm just
gonna let it train. So once that
56:58 - is done, I just come back and
let's just visualize the
57:02 - training part of them. Because
we need to understand the error
57:05 - losses and we need to understand
the policy that we have used.
57:13 - This is running, let's just
complete the visualization code.
57:16 - So what I'll be doing is rolling
land. So we will be
57:20 - understanding all the parameters
and all the data relating to our
57:24 - policy and understand what is
the appropriate amount of
57:28 - episodes that we couldn't use,
and also the loss function.
57:33 - We'll plot it and we'll
understand it that way as well.
57:36 - So let's do that. So what I've
done is I've set the rolling
57:40 - length is 500 and I've done the
finger sizer so I let me just
57:44 - complete the code and I'll
explain to you everything what
57:46 - we've done. Yeah, just give us
five subplots, not subplots.
57:57 - Just to access of you know that
set title, so we'll just give it
58:01 - a title as episode rewards. So
what how does the reward change
58:06 - in each episode we have plotted
let's plot that episode rewards.
58:15 - Now let's do reward the score
moving score average equal to NP
58:26 - dot convolve. We will plot a
colored graph in lieu np dot
58:32 - array E and V dot return Q dot
satin so we'll flatten it to dot
58:43 - Latin comma np dot once
58:54 - and let's do more is equal to
valid and let's do rolling
59:03 - length divided by rolling this
will be for every 500 iterations
59:06 - we're doing this now this is
completed let's do x of zero dot
59:11 - dot array range then
59:21 - the word moving average
59:29 - the moving average of the reward
now what I will be doing is I
59:32 - will be putting it against the
reward moving average to that
59:39 - value. The value here so we are
going to plot it now let's do
59:44 - excess of one two is the y axis
got set title. So lengths
59:57 - this is done let's do length
underscore Moving underscore
60:02 - will be our variable that can be
the same way to a carnival
60:06 - within do np dot Cornwalls. So
I'll explain to you why we use
60:11 - the Commonwealth method as well
60:23 - we need to do a blanket dot
plugin, then we need to do np
60:31 - dot once. Rolling and again and
mod is equal to same so instead
60:35 - of valid we'll do same now it's
to the window same close the
60:48 - brackets, divide this by rolling
in
61:02 - not clot length training, moving
average three out only ever
61:14 - talked about training error.
Score moving average comma,
61:26 - training underscore underscore
knowing no score. Loop PLT dot
61:37 - tight layout
61:47 - basically, I've completed the
training or visualizing of the
61:52 - training parameters and how the
graph looks like. So what I'll
61:56 - do is I'll just wait for the
training to complete and then
61:58 - I'll explain to you the graph.
So the training is complete. Now
62:02 - let's look at the different
parameters that we have plotted.
62:05 - So they have three different
types of charts. Let's
62:08 - understand each one of them. So
first one we have is episode
62:11 - rewards. So the episode rewards
in our case of solving
62:16 - Blackjack, and episode is a can
be defined as a game of
62:20 - blackjack played from start to
finish. And the episode rewards
62:24 - is a total sum of the rewards
obtained during that game, it
62:27 - could be the sum of the numbers
of wins and losses. And we can
62:31 - add any bonuses or penalties for
certain actions. So as you can
62:34 - see, for each episode, there is
a very drastic change in the
62:39 - rewards. So basically, this is
very interesting to watch that
62:42 - it is not a flatline, basically,
we do not have any rangebound
62:45 - moment between it just just too
volatile, just keep changing it
62:52 - this is very interesting to
watch. So now let's look at the
62:55 - episode lens combat which is
plotted against each episode. So
63:03 - basically, this is one complete
sequence of playing a hand
63:06 - taking a hit or a stand and
receiving the result. Now the
63:10 - length of an episode would
depend on the rules of the game,
63:12 - the number of players and other
factors. So in our game, it
63:15 - could be just two player, but
you can have more number of
63:17 - players as well. And in a game
of blackjack you could consist
63:22 - of it would consist of a single
hand or multiple hands with
63:25 - different lengths of each
episode. So basically, you can
63:28 - see, in our case, it would be
more than one only. But it
63:34 - doesn't go over two. So
basically everybody will find in
63:36 - between 1.4 1.3 basically 1.3
and 1.4. So each episode is just
63:44 - between 1.3 and 1.4. That is
also very interesting to watch.
63:48 - Let's look at the training
error. The training error is a
63:51 - measure of how well our model is
performing on the training data.
63:54 - Now in our context of solving
blackjack using Kulani. The
63:57 - training error would be
difference between the expected
63:59 - value of the Q function and the
actual Q values obtained during
64:02 - the training. The goal of Q
learning is to minimize this
64:05 - error and converge to optimal Q
values that maximize the
64:08 - expected reward. A high training
error indicates that the model
64:12 - is not learning effectively. And
adjustments may be needed to
64:15 - divert the motoring data. So as
you can see at around 3000
64:19 - around 3000 This is pretty good.
Our training error is pretty
64:25 - low. Now after 3000 goes pretty
high. So that's it now we are
64:38 - done with our training of the
blackjack agent. And we have
64:42 - successfully solved the
blackjack game using the Q
64:46 - learning algorithm. Now let's
quickly revise all the things
64:49 - that we have learned while we
solve when we started solving
64:53 - that chapter. So we started with
the reinforcement learning
64:58 - basics. Then I also explained to
you the role of an agent, what
65:04 - exactly constitutes an
environment, what each action
65:08 - is? And how do we keep track of
each each state that the agent
65:12 - is currently in. Then we also
learned about the observation
65:17 - aspect, and also how we use
rewards to keep track of a
65:22 - successful step and also give
penalty to the agent when it
65:27 - makes a wrong step. So
basically, we have covered that,
65:31 - then I'll also explain to you
the real world use cases of
65:34 - reinforcement learning. We saw
basically there is robotics game
65:38 - playing autonomous driving
personalized recommendations.
65:42 - Then we started by with the
gymnasium libraries basically,
65:46 - it gives us a collection of
environments that can be used to
65:49 - test and develop reinforcement
learning algorithms. So here we
65:52 - have made use of the blackjack
environment. Then we started
65:58 - with the setting, then we
started the setup for the
66:01 - gymnasium environment. Also,
I've walked you through all the
66:05 - processes that we need to
successfully solve any problems.
66:08 - This is true for any
reinforcement learning problem.
66:12 - Then I also explained to you the
main concepts of open air
66:15 - gymnasium. To be started with
observation and action spaces
66:21 - episode, what is the wrapper
benchmark. Now we start with the
66:27 - introduction to the blackjack
game, basically are the basic
66:29 - rules. The game is paired with
one or more decks of tender
66:32 - playing cards. Each player is
dealt two cards and the data is
66:35 - also dealt two cards with one
card facedown. The value of each
66:39 - card is determined by its rank.
It says Can we work 111 face
66:43 - cards, that is kings, queens and
jacks, what 10 and all other
66:47 - face cards are worth at face
value. And basically, we have
66:50 - the option to hit and take
additional cards to improve our
66:54 - hand or stand and give the
current time the dealer must hit
66:57 - until there were and as a value
of 17 or more. If a player's
67:01 - hand goes over 21 They bust and
they lose the game. If the
67:05 - dealer's hand goes over 21 the
player wins the game. If neither
67:09 - the player nor the dealer busts
the hand with the highest total
67:12 - value that is less than or equal
to 21 wins the game. So, this is
67:16 - an example road. Now we have
learned about the action space,
67:19 - the observation space, the
starting space, what the rewards
67:24 - and what constitutes an episode
and how does an episode end. So,
67:28 - we have the termination
conditions here. Now, we have
67:30 - started with the solving of
blackjack, I also introduced you
67:33 - to a very good book that is
reinforcement learning and
67:35 - introduction by Richard Sutton
and Andrew Bartow. Basically,
67:40 - this is the the summary of the
book. Now, we started with the
67:49 - installing of the So, basically
we have started with the
67:53 - installing of all of the all the
libraries that we need for the
67:56 - tutorial. So there is
matplotlib, NumPy, Seabourn then
68:01 - we started with our import
statements. So, we have imported
68:06 - all the libraries that we need.
Now here is where we have
68:11 - created our logic we want
environment then we have set the
68:15 - environment and started with the
first observation then we have
68:20 - observation tuple basically this
is the tuple. Now then, we start
68:27 - executing the actions, we have
the next step reward terminated
68:29 - state truncated state and the
info. Basically, this is the
68:34 - step. Now, we have used the
epsilon greedy strategy to solve
68:38 - blackjack. So, what is the
epsilon greedy strategy? In this
68:44 - strategy, the agent takes an
action that is either the best
68:47 - action based on the current
policy with a probability of one
68:50 - minus epsilon or a random action
with a probability of epsilon.
68:53 - This approach balances the
exploitation of the current base
68:56 - policy with exploration of new
policies, which can lead to
68:59 - better rewards in the long run.
Basically, this was the epsilon
69:04 - greedy strategy that we use. So
I explained to you the what is
69:09 - the Q value. So, this is the
class for our black jacket, we
69:13 - have all the methods we have to
get actually meant that date
69:15 - method. Now we started with the
training. So we have set up some
69:20 - hyper parameters here, I also
explained in detail what each
69:23 - parameter does, so you can just
play around with these. Now we
69:27 - start with the training loop, we
have the episode, we have
69:30 - created a number of episodes.
Now we what we do is while not
69:36 - done, we play the episode. And
basically this is an area where
69:40 - I got to we started with the
rolling length.
69:45 - So this is the plotting of the
graphs that we did here, too.
69:49 - This was the code for that. And
also I explained to you the
69:53 - URL, what are all the graphs
that we have plotted here? So
70:01 - now we've solved logic using key
learning. Also. Now let's look
70:05 - at some other methods that we
can solve that check. So the
70:10 - first one is Monte Carlo method.
Monte Carlo is a more model free
70:15 - method that learns from
experience. In the context of
70:18 - blackjack Monte Carlo methods
involves playing the game
70:21 - several times, and in keeping
track of the rewards obtained
70:24 - for each action, the agent then
updates its value function based
70:28 - on the average of the rewards
obtained for each state action
70:32 - pair. Monte Carlo methods are
suitable for problem like
70:36 - episode tasks like blackjack, so
you can use him NASM and try
70:40 - this approach as well. Now let's
look at the other approach.
70:44 - temporal difference method. It
is another model free method
70:49 - that learns from experience. In
the TD learning the agent
70:52 - updates its value function based
on the difference between the
70:55 - predicted and the actual reward.
TD methods are suitable for
70:59 - problems with continuous tasks
like blackjack. Q learning is
71:03 - something that we've already
solved blackjack. But let's just
71:07 - quickly revise what Q learning
you're learning is a model free
71:10 - reinforcement learning algorithm
that learns to optimal policy.
71:14 - By updating its Q values for
each state pair. The agents
71:17 - selects the action with the
highest Q value for a given
71:20 - state. Q learning is suitable
for problems with finite states
71:24 - and actions like blackjack. The
fourth method that we have is
71:28 - deep Q networks. So basically we
will be solving so in the next
71:34 - example, we will be solving card
poll problem that is like an
71:38 - Atari game where you have to
balance the card poll, and we'll
71:41 - be using deep Q networks. So I
also explained in detail what
71:45 - are the deep Q networks. So
let's look at the brief. Deep Q
71:50 - networks combined reinforcement
learning with deep neural
71:52 - networks, DQ NS learn the
optimal policy by approximating
71:56 - the Q values using a deep neural
network. DQ ns are suitable for
72:00 - problems with high dimensional
state spaces like image based
72:04 - games to it like an image from
these are suitable for image
72:07 - based games. Now, the fifth
example that we have is actor
72:11 - critic example. Actor critic is
a model based reinforcement
72:14 - learning algorithm that uses the
two networks an actor and a
72:18 - critic, the actor network
selects the actions while the
72:21 - critic network evaluates the
actions taken by the actor,
72:25 - actor critic is suitable for
problems with continuous action
72:28 - spaces like that gap. So these
are all the different approaches
72:32 - that you can use to solve
blackjack. And I highly
72:35 - encourage you all to use
gymnasium and try these
72:38 - approaches. So you will find you
will, you will have to use some
72:41 - math for that. Also, you will
find that online as well. But
72:46 - these are the different
approaches that you can use.
72:56 - So now let's go to another
example where we'll be using the
73:00 - environment of the carpool. So
now let's understand what what
73:04 - what is the carpool environment,
and then we understand about
73:07 - deep Q networks. So in the
carpool environment, we have an
73:11 - agent that can decide to actions
moving the cart, right or woman
73:15 - the cart left, right or left, so
the poll attached to it stays
73:19 - upright. So as you can see in
this clip, we have to balance
73:24 - the poll. And these are can be
random actions, so it could just
73:27 - tilt left twice, or it could go
to the left five times, and it
73:32 - could go to the right one. So we
need to have all the
73:35 - possibilities covered here. You
can find more information about
73:38 - the environment at the gymnasium
website. So I have also linked
73:41 - the website link here. Now let's
look in detail. As the agent
73:48 - observes the current state of
the environment and chooses an
73:50 - action, the environment
transitions to a new state, and
73:54 - also returns a reward that
indicates the consequences of
73:57 - the action in this task reward
plus one for every incremental
74:01 - time step. And the environment
terminates if the pole of the
74:04 - pole falls over too far, or the
cart moves over 2.4 units away
74:10 - from the center. So basically,
we do not have a lot we cannot
74:13 - allow the pole to fall 2.4 units
away from the center. And for
74:18 - every incremental time step, we
have pressed one reward, so we
74:22 - have to maximize the number of
times you can balance this
74:25 - cardboard. This means meter
performing scenarios will run
74:28 - for longer duration,
accumulating larger return. The
74:32 - Cardpool task is designed so
that the inputs to the agent are
74:35 - for real values, representing
the environmental state
74:38 - position, velocity, etc. We take
these four inputs without any
74:42 - scaling and pass them through a
small fully connected network
74:45 - with two outputs one for each
action. The network is trained
74:49 - to predict the expected value
for each action given the input
74:52 - state. The action with the
highest expected value is then
74:55 - chosen. So now let's look in
detail at how We can implement
75:00 - this network as well. Now let's
look at all the methods that
75:08 - we'll be using from Python. So
the first is the neural
75:11 - networks, which is imported
using torch.nn. Now, in general,
75:16 - this module provides tools for
building neural networks. It
75:19 - includes a wide range of layer
type, wide range of layer types,
75:23 - such as fully connected layers,
convolutional layers, and
75:26 - recurrent layers, as well as
activation functions and loss
75:30 - functions. Now let's look at in
perspective of a DQ n, the
75:35 - torch.nn model is used to define
our neural network architecture.
75:39 - Now let's look at the other
method that is the optimization
75:42 - method. Now this module provides
a range of optimization
75:45 - algorithm for training neural
networks. It includes classic
75:49 - optimization algorithms, such as
stochastic gradient descent, as
75:52 - well as more advanced algorithms
like Adam and RMS Pro. But let's
75:57 - look at in context of the DQ N,
we'll be using towards the
76:00 - optimization module to optimize
our neural network weights. Next
76:07 - is torch dot auto grad, this
model now, this model in the
76:12 - context of DQ n is used to
compute gradients during back
76:15 - propagation. This module
provides automatic
76:18 - differentiation functionality
which is essential for training
76:21 - neural networks. Why am back
propagation, it enables patterns
76:25 - to automatically compute
gradients of a loss function
76:28 - with respect to all the
parameters of the network
76:30 - allowing optimization algorithms
to adjust the parameters in
76:34 - order to minimize the loss. Now
let's import the modules that we
76:39 - need. So the first is the
gymnasium classic control. Let's
76:44 - import that. Basically that
convert that contains a model
76:50 - that we need to cut pole v1
model let's install that pip
76:54 - install Nazeem classic
underscore control this is the
77:04 - name so now let's also import
all the other methods that we'll
77:08 - need so let's import import NASM
as gym mat. So basically these
77:16 - are all the inbuilt models
method inbuilt is doing random
77:21 - randomization what method is
matplotlib this charting library
77:30 - plot lib it's used for making
charts there's import map plot
77:43 - dot pie plot as PLT I think one
of the most common code lines in
77:52 - Python code named tuple and that
so basically we'll be using
77:58 - these data structures to manage
the training maker tools input
78:04 - count meter reading this now
let's import all the torch
78:08 - models for torch torch.nn has an
78:21 - OPCOM OPCOM import.nn dot
functional functional f Now
78:44 - let's also create our
environment so we need this will
78:47 - be gym dot make card poll
78:53 - click poll new one now also
let's set the inline setting for
79:06 - matplotlib. In AD lib.net I
think I forgot to import this
79:16 - little read that quote because I
think we can directly make it in
79:21 - Leno's in collab. If we end
Google collab, we want pyplot.to
79:29 - be plotted as inline. So that
will be displayed below the cell
79:37 - if is IPython so basically this
is Google collab again from
79:41 - ipython. Input display because
we need the display to display
79:49 - the charts do PLT that ion.
Also, if you do not have a GPU,
79:58 - we can just add a condition as
well. And if you do have a GPU,
80:02 - then we can increase the
training. So let's set the
80:04 - variable. If we have a GPU, this
CUDA is the art.cuda.so.
80:14 - Basically, this is the method to
understand if we do have a GPU
80:19 - available. And you can just go
to settings in tools, you need
80:24 - to go to settings, and Google
editor in the run time, I think
80:29 - you need to send it around. So
it will change and time time.
80:33 - And you can just add a GPU or so
I will do that, I'll do that as
80:36 - well. So it will basically give
us a lot more computing power.
80:43 - So now we've completed the basic
in code where we have imported
80:48 - all the libraries that we will
need. Now let's look at another
80:53 - concept that is very important
in this in this example, or to
80:58 - solve cardboard. So now let's
look at replay memory. So what
81:01 - is replay memory, it is a
technique used in D enforcement
81:04 - learning to store and manage the
experience of an agent during
81:07 - training. The idea is to store
the agents experiences as a
81:11 - sequence of state action reward
next, state tuples. So
81:15 - basically, we have just four
values, which are collected as
81:18 - the agent interacts with the
environment. During training,
81:21 - these experiences are used to
update the agents policy and
81:24 - value function. So basically,
this is the we collect all the
81:29 - actions performed by our agent.
Now let's look at what the
81:35 - importance of replay memory. The
replay memory allows the agent
81:40 - to learn from past experiences
by randomly sampling a batch of
81:44 - experiences from the memory
buffer, rather than just
81:47 - learning from the most recent
experience. So it's like the, so
81:52 - we instead of just learning from
our recent experience, we take a
81:57 - sample of all the past
experiences and make a decision
82:01 - based on that. This helps us to
reduce the correlation between
82:04 - subsequent experiences, which
can improve the stability and
82:09 - convergence of the learning
algorithm. In addition, by
82:12 - storing experiences in a buffer,
the agent can reuse past
82:15 - experiences to update his policy
and value function multiple
82:19 - times, which can further improve
learning efficiency. The replay
82:23 - memory is typically implemented
as a fixed size buffer or queue
82:27 - that stores the most recent
experiences. When the buffer is
82:31 - full. New experiences overwrite
the oldest experiences in the
82:35 - buffer. During training, a batch
of experiences is randomly
82:40 - sampled from the buffer and used
to update the agent's policy and
82:45 - value function. This process is
repeated iteratively until the
82:49 - agent converges to an optimal
policy. So basically, this is an
82:53 - example it's a basic thing. It's
a very basic concept, we store
82:58 - all of the training data until
the queue is full. And the
83:03 - buffer memory that we have will
keep using it as we go ahead.
83:08 - Where does the concept now let's
look at how we'll use the
83:13 - concept of replay memory in
order to implement our DQ n
83:18 - algorithm. So what we'll do is,
let's understand that so we'll
83:23 - store the transition that the
agent observed, allowing us to
83:26 - reuse this iterator by sampling
from it randomly the transitions
83:30 - that build up a batch or D
correlated, it has been shown
83:34 - that this greatly stabilizes and
improve the deployment training
83:37 - procedure. Now let's also
understand how we implement this
83:41 - in code. So we'll have two
classes. One is the first will
83:45 - be transition class it is a
named tuple representing a
83:49 - single transition in our
environment. Basically, this is
83:52 - our sample input. So this will
be represent one state and
83:58 - essentially map state action
pairs to the next state reward
84:02 - result with the state being the
screen dividends image as
84:05 - described later on. Next will be
the replay class, a cyclic
84:10 - buffer of bounded size that
holds the transitions observed
84:14 - decently. It also implements a
dot sample method for selecting
84:18 - a random batch of transitions
pertaining. Let's implement our
84:25 - replay memory class. So the
first will be the transition
84:29 - state that we have which is a
tuple which is the named tuple
84:36 - by the name of transition and
then we'll have a state action
84:51 - next
85:04 - We have recently this is our
condition tuple. Now let's also
85:13 - create our class, which is the
deeply mini class. So it will be
85:17 - a class we call replay memory,
then object. Let's do def
85:28 - underscore underscore in it,
self, comma, capacity. And then
85:39 - we'll have self dot memory. So
basically this will return the
85:42 - full memory buffer. So it is
equal to that. I don't know how
85:48 - you pronounce it, but I
pronounce it as that it could be
85:50 - called as DQ also, depending on
the accent here, max length is
85:58 - equal to capacity. So basically,
this is our max length. After
86:02 - that it it will overflow and
then we'll reset it so we'll use
86:09 - one transition into this memory
was here it says no memory will
86:15 - have self comma REM sleep are
some arguments as well.
86:20 - Basically, it is also comment
86:28 - save so I think it is double
86:36 - save a transition save raishin.
86:53 - Memory node append transition
star arcs
87:07 - Okay, so basically this will
append to our memory buffer,
87:12 - let's create another method of
the sample. This method will
87:16 - help us sample our buffer
remember, before memory cells
87:22 - come up bad size. So we have a
capacity and the batch size. So
87:26 - it the capacity could be 50 and
the batch will be 10. So this is
87:31 - how the there's no return.
random sample. Okay. Still not
87:42 - memory. Let's size
87:51 - define underscore underscore
length may give us the length of
87:56 - the buffer at any point.
88:00 - Return length or size but
memory. So yeah, I think we have
88:07 - done it completed. Let's also
look through the code again. So
88:11 - we have defined the transition
tuple and replay memory class.
88:18 - Now we have fulfils your state
action next state. I think I
88:23 - need to do that this year
awards. I think it is rewarding.
88:30 - And we will have other class as
well. So we'll have the push
88:33 - method. This method takes in a
state which is state action next
88:39 - ID tuple as input and creates a
transition object from it and
88:44 - append it to the deck. If the
deck is already at maximum
88:47 - capacity, the oldest element in
the deck is removed. So this is
88:51 - how the append method works.
Let's look at the sample method.
88:55 - This method randomly samples a
batch of batch size, which is
88:58 - the batch size parameters that
were passed experiences from the
89:01 - deck and returns them as a list.
So this is the sampling method.
89:06 - Let's look at the length method.
This method returns the current
89:09 - length of the deck. Now let's
understand the DQ n algorithm in
89:15 - detail and how we use the
algorithm to solve the Cardpool
89:18 - problem. So it is a
reinforcement learning algorithm
89:22 - that uses deep neural networks
to approximate the Q function in
89:25 - a Q learning algorithm. So in
short, basically it is that but
89:29 - we need to understand the steps
involved to solve the Cardpool
89:32 - environment. Let's look at that
as well. First we need to
89:36 - initialize the Q network with
random weights. The second is
89:39 - sample and action using an
epsilon greedy policy.
89:42 - Basically, we have covered this
in our blackjack tutorial, which
89:48 - selects the action with the
highest Q value with probability
89:50 - one minus epsilon and the random
action with a probability
89:53 - epsilon. Execute the action and
observe the next state and
89:57 - reward. Store the experience to
Built in a replay buffer, so we
90:01 - have implemented the class
already sample a mini batch of
90:04 - experiences from the replay
buffer, we've already created
90:07 - the method for that, compute the
Q target values for the mini
90:10 - batch using the Bellman
equation. So basically, this is
90:13 - how we calculate the t Q value
I'll explain to you when we read
90:18 - the code, compute the Q values
for the mini batch using the
90:21 - current Q network. So we'll have
that value. And we just need to
90:25 - compute the Q value for that.
Compute the loss between the Q
90:29 - values and the Q target values,
and update the Q network
90:34 - parameters using gradient
descent. And we'll keep doing
90:38 - these steps until we reach
convergence or we reach a fixed
90:42 - number of episodes. So I will
explain in detail what what we
90:47 - are going to do to the end of
the cardboard. The DQ n
90:53 - algorithm uses a target network
to stabilize the training
90:57 - process. The target network is a
copy of the Q network that is
91:00 - updated less frequently than the
Q network. This helps to prevent
91:04 - the Q values from oscillating
during training. So that's what
91:07 - happened in our budget. When we
are solving that check. In
91:12 - Cardpool environment, the DTN
algorithm learns to balance the
91:15 - port on the cart by moving the
cart left or right. The queue
91:19 - network takes the state of the
environment as inputs and
91:22 - outputs the Q values for each
possible action, the DQ n
91:26 - algorithm learns to maximize the
Q values by updating the Q
91:29 - network parameters using
gradient descent. With enough
91:32 - training, the DQ n algorithm can
learn to balance the pole on the
91:36 - cart for extended periods of
time. So basically, this is all
91:39 - the steps in what and how we are
going to solve the Godbole.
91:46 - Example, we are going to balance
out the card pool for a very
91:50 - extended period of time. Now
let's go to our cue network. So
91:57 - let's take some notes here.
92:08 - So far model will be a
convolutional neural network. If
92:11 - you take the difference between
the current and the previous
92:14 - screen patches, it will have two
outputs representing Q, S comma
92:19 - left and q s comma right, where
s is the input to the network.
92:23 - That, in effect, the network is
trying to predict the expected
92:26 - return of taking each action
given the current input. So
92:29 - basically, this is our cue
network. This is the full
92:32 - implementation will have two
outputs. And the network is
92:37 - trying to predict the expected
return of taking each action
92:39 - given the current input. Now
let's implement our DQ N class.
92:43 - Let's add the class here. That's
right class, the Q n and n n dot
92:51 - module.
92:57 - So let's do def, underscore,
underscore init, underscore
93:03 - underscore, self comma n,
underscore observations. Comma
93:12 - and underscore actions. So I'll
explain to you in detail that
93:16 - just complete the class first,
and then we'll read the comments
93:20 - as well. So it will be very easy
to understand. We self dot
93:25 - underscore underscore in it.
Self thought we left three
93:36 - layers, let's do all the three
layers here to three, the equal
93:41 - to and then thought linear and
underscore observations 128. So
93:51 - we have sizes and 128. Let's do
that. We'll the next one will be
93:57 - 128 by 128. This is how it will
be now and the other one will be
94:06 - the exactly the opposite of this
will be 128 by the number of
94:10 - actions comma and underscore
options. So we've implemented
94:17 - the new folding in it consulted.
Let's go and define our method.
94:22 - That is the forward method. And
we have to do self dot causes
94:27 - comma x and x is equal to f dot
value self dot layer 1x And then
94:39 - we have the layer 2x. And then
we'll return the cell got layer
94:45 - 3x. So we'll basically go to
three layers what the code now
94:53 - let's understand what are we
trying to do here? So we have
94:58 - the cue network which is Mighty
perceptron, so let's just keep
95:02 - writing it down yet mighty per
mighty layer or scepter on any
95:11 - point, you can just use
perceptron with three layers.
95:18 - So, this is what we are trying
to achieve here. Now, we have
95:22 - what the input is it is a
tensor. So, the what will be the
95:27 - input, so it will be a tensor of
the size and underscore
95:30 - observation is our size
95:34 - which is single this is what we
passed here. Actions is our
95:45 - input to the network this is
basically the state input that
95:55 - is the state of the environment,
state of environment to the
96:02 - network. So, we have the fully
connected layer with 120
96:09 - neurons, followed by a value
activation function. I'll also
96:14 - explain like why we're using
that the second layer is also a
96:17 - fully connected layer with 128
neurons and a ReLU activation
96:23 - function. The final layer is a
fully connected layer with an
96:27 - actions where an action
represents the number of
96:29 - possible actions in the
environment, which represents a
96:37 - certain number of possible
actions.
96:46 - environment now, let's look at
the forward method what we have
96:52 - implemented here. So, if you
take the input as the tensor
96:55 - that is the X tensor, as it
passed through the first layer
96:59 - of the network using the value
activation function, pass, take
97:04 - input, we pass to two or three
layers on the network when.
97:18 - Basically, this is what we're
trying to do here. And what we
97:22 - are also trying to do is, so,
we'll pass it to the state to
97:26 - the network. And what we will
get this, the output is the
97:30 - corresponding neuron in the
final layer. So, when you pass
97:32 - here, we'll get an output we'll
pass it to the next year. And in
97:36 - the third layer, we finally
return. So and what will happen
97:40 - here, so during training, the Q
network is updated using the
97:43 - Bellman equation that we have
discussed already to minimize
97:46 - the mean squared error between
the target values and the Q
97:49 - values. Yeah, so let's look at
that. Now let's look at the
97:57 - training of our DQ n. So what
we'll do is we'll have a Q
98:02 - network, which is updated using
the Bellman equation to minimize
98:05 - the mean squared error between
the predicted Q values and the
98:08 - target Q values. The Target Q
values are computed using the Q
98:12 - network but with the weights
frozen and not updated during
98:15 - the current iteration of
training. This, this helps to
98:18 - stabilize the training process
and prevent the network from
98:21 - overfitting to the data. Okay,
now let's look at all the other
98:26 - parameters that we will be have.
Or I can say parameters and
98:30 - utilities as well. So first,
we'll have select action, it
98:34 - will select an action according
to the epsilon greedy policy, we
98:37 - have again covered this in the
blackjack tutorial. In the back
98:40 - when we were solving blackjack
using toolani. We simply put
98:43 - will sometimes use our model for
choosing the action. And
98:48 - sometimes we'll just sample one
uniformly. So there is no steps
98:52 - for that. But we will see how we
can do that. Now, the
98:57 - probability of choosing a random
action we start at EPS start and
99:02 - will decay exponentially towards
EPs and EPS decay controls the
99:06 - rate of decay. So, basically,
this is the set of we do not
99:10 - have any particular target
value. So as we go along, the
99:13 - values will keep changing. And
we have to decide whether we
99:16 - want to take a random action or
we want to take from the action
99:20 - that we have on the board. Next
we have plot durations. It is a
99:24 - helper for plotting the
durations of episodes, along
99:28 - with an average over the last
100 episodes. The measure used
99:31 - in the official evaluations. The
plot will be underneath the cell
99:35 - containing the main training
loop and will update after every
99:37 - episode. So we'll plot this
training durations as well.
99:41 - Let's now go to the code. So
let's implement
99:48 - all the parameters that we are
going to need so I'll have pad
99:50 - size, that size equal to 1.8
comma You can do zero point. So
100:01 - I'll also explain like this,
just complete all the parameters
100:05 - for
100:09 - your start. Start is equal to
0.9, we have EPS, and you will
100:20 - do 0.05. So basically this is
when we will end. This is our
100:24 - epsilon and lbps decay rate,
which is going to be 1000. And
100:31 - we have the tall, which is the
one who is 0.005. And our
100:36 - learning rate is going to be one
e raise to minus four. Yeah,
100:42 - this is our this is what is
going to be Yep. Now, let's read
100:47 - the comments as well, what is
going to be the batch size
100:51 - number of transitions sampled
from the labor force
101:10 - it is grandma this is what is
the discount factor as
101:21 - mentioned, we will do the
discount factor. So what is
101:28 - going to be the EPS third
quarter
101:32 - is the final value of episode
yesterday the starting value of
101:37 - the epsilon basically, we can
just write it next to it as
101:43 - well. So we don't have to waste
the time just writing now, it is
101:49 - a starting value of epsilon.
102:00 - Let's look at the EPS. And this
is the ending value of final
102:06 - value of epsilon. Let's look at
the EPS became in EPS API
102:12 - educate controls to equal
mediaspace.
102:20 - controls the rate of exponential
decay
102:29 - decay of epsilon higher means a
slower rate. So first we'll
102:34 - start with a very slow rate. And
then we'll keep improving on our
102:39 - learning rates. So there is
going to me higher means slower
102:48 - and slower decay. Let's look at
our this as well is that is the
102:59 - update rate of the target
network? What is tau? Tau is the
103:04 - update rate of the target
network. And what is going to be
103:13 - the LRT it is going to be a
learning rate. So this is going
103:16 - to be the atom optimizer. So
let's look at now what is this
103:20 - is the perfect time to
understand about the atom W
103:23 - optimizer, Atom optimizer and
atom optimizer. So basically now
103:33 - let's look at the atom
optimizer. So basically we've
103:35 - defined it you're eliminating
all these parameters and
103:39 - eliminating it for the atom
optimizer. So this is the
103:42 - learning rate for the RM
optimizer. So Adam, adaptive
103:46 - moment estimation is a popular
optimization algorithm that is
103:50 - commonly used in deep learning.
It is an extension of stochastic
103:54 - gradient descent. If you do not
know about that, it's okay, you
103:58 - can still watch the rest of the
video, which is the most basic
104:02 - optimization algorithm used to
train neural networks. The main
104:05 - idea behind Adam is to combine
the advantages of two other
104:09 - optimization techniques, add a
grad and RMS problem. So if you
104:14 - do not know about that as well,
it's okay. In the DQ n
104:17 - algorithm. We use Adam optimizer
to update the weights of our
104:22 - neural network based on the
gradients of the loss function
104:25 - with respect to the parameters,
so we'll define our loss
104:28 - function as one minus epsilon.
Specifically, we use the Adam W
104:32 - optimizer which is a variant of
the atom. So we have defined it
104:35 - here which is a variant of the
atom that also incorporates
104:41 - weight decay regularization.
weight decay helps prevent
104:45 - overfitting by adding a penalty
to the last function that is
104:49 - proportional to the magnitude of
the weights. By adding this
104:52 - penalty the optimizer encourages
the network to learn simpler and
104:56 - more generalizable
representations. The learning
105:00 - rate is a hyper parameter that
controls the step size taken
105:04 - during optimization. It is an
important parameter to tune as a
105:09 - high learning rate can cause the
optimizer to overshoot the
105:12 - optimal weights and lead to
divergence while a low learning
105:16 - rate can result in slow
convergence and getting stuck in
105:19 - a local minima. In DQ n
algorithm, we set the learning
105:23 - rate to one e raise to minus
one. So basically, this is what
105:31 - this is why we're going to use
that optimizer. So we don't get
105:35 - stuck in a local minima. So, we
need to control the learning
105:39 - rate. And for the content to the
learning rate, we need that and
105:42 - W optimizer. In summary, the
Adam W optimizer is is a widely
105:47 - is a widely used optimization
algorithm in deep learning. And
105:52 - it is used in DQ n algorithm to
update the weights of the neural
105:56 - network based on the gradients
of the loss function with
105:59 - respect to the parameters, while
also incorporating weight decay
106:02 - regularization. So this is the
very critical role played by
106:07 - Adam W optimizer. Now, let's so
now we will learn about the atom
106:12 - optimizer. Now let's learn about
now let's read the code for the
106:16 - other two other utilities that
are pending. So the first
106:22 - variable that we have is the end
actions, this is going to be NV
106:25 - dot action, the action space. So
basically, this is going to be
106:29 - the action space of the carpool
environment. So let me just keep
106:35 - it up, let's just keep writing
the number the comments as well.
106:39 - Number of actions from them
action space. Also, now let's do
106:49 - that net number of
106:57 - state observations, which is
going to be state comma info.
107:04 - So, basically, this is going to
be that tuple that we talked
107:06 - about the content all the
information related to the
107:10 - current state, the current
107:12 - state that we are in and of the
reasons is going to be length of
107:19 - state. Now let's create our
policy net, which is going to be
107:30 - a class it is going to be an
instance of the DQ N class. We
107:35 - have this Ruby and underscore
operations comma and underscore
107:42 - actions not to device specific
devices that what we have set up
107:53 - to target net as well. Targeted
net is equal to same thing. Now
108:00 - let's load the state
108:03 - target dot score net not load
underscore state underscore the
108:16 - policy this coordinate got state
I think it is policy and I
108:26 - understand the state.
108:29 - Now let's complete the optimizer
so I'll just come back and
108:32 - explain to you everything is
just complete the optimizer also
108:35 - now basically, we this is going
to be the torch method OChem but
108:42 - we added W we discussed about
atom W I also explained to you
108:46 - the atom W optimizer and our
atom optimizer is going to be
108:51 - policy underscore net dot
parameters. Okay parameters
109:00 - needed we have already defined
small error is equal to capital
109:04 - error. Total Error is defined
here.
109:08 - one equals two minus code from
AMS guard AMS grad, so like
109:16 - listening to you what this means
as well.
109:19 - Let's complete the code memory
we'll be taking the last 10,000
109:24 - episodes for our memory. Do
replay. Memory 10,000 We passed
109:32 - in to our buffer size is going
to be 10,000. Let's define the
109:36 - steps done as well. With me,
this is where we currently are.
109:43 - Okay, let's let's go ahead and
just complete all the methods
109:48 - let's do the Select action as
well. Basically this is going to
109:51 - be the method that will help us
select any action. So if I have
109:55 - a state, it will just tell us
which methods are and
110:05 - steps underscore done sample is
equal to random sample I think
110:14 - it is wrong it is random not
random
110:22 - and with the PPS on the score
threshold is equal to EPS score
110:30 - and plus EPS underscore start we
have defined all these
110:40 - parameters before VPS the scope
and into we have to divide it by
110:50 - offering when you go to the next
line so with me goes less
110:58 - mad not mad dot exponent of
minus one towards star off steps
111:12 - done by up to K yeah this is
going to be the equation
111:24 - and then let's do steps done
once we're done with this little
111:31 - loop do steps than possible will
rent payment from those sample
111:41 - to them UPS underscore threshold
and do with torch.no Grad so
111:52 - basically we'll be doing that
112:04 - we'll do T dot max one minute
and the largest column of each
112:11 - row
112:18 - this column value
112:28 - okay what does it mean this is a
row we have the second column
112:38 - second column on max
112:48 - next up where Max element was
was fun so, we pick action with
113:00 - largest yeah so, we need to like
we need to optimize for the
113:04 - larger what will basically this
is what we are going to do
113:11 - expected reward let's do return
policy score net state dot max
113:29 - not view one comma one now we
need to do the as of this as
113:37 - well is the ELS
113:49 - dot pensar en v dot actions so
we'll just return the action
113:57 - space
114:00 - action underscore space dot
sample response. So basically
114:04 - this is our we're going to do is
we'll see if we want to take a
114:09 - sample one already to keep
continuing the path that we are
114:12 - on. Do that okay, now with the
device is equal to device and
114:28 - command D type call sheet.com
Okay, so I'll explain to you all
114:39 - these utilities let's just
complete all the methods while
114:42 - we're at it. So does equal to
Episode underscore durations
114:56 - Okay, So what we'll do is just
go over all the methods that we
115:06 - completed. So, the N actions is
the number of actions in the
115:11 - environments, no environments
action space will be the gym
115:16 - environments, state comma info
are obtained by the setting the
115:20 - environment and observations is
the number of features in the
115:25 - state. Let's just drill down as
well in the state let's do the
115:35 - target net is initiate with the
same weight as policy net Okay.
115:40 - target net initialized
115:49 - with the same weight as policy
net then we have the optimizer
116:04 - which is not an optimizer Adam W
optimizer optimizer is LM W
116:13 - optimizer. Adam W. Normally
optimizer pytorch with the
116:23 - learning rate and other hyper
parameters specified earlier. So
116:27 - we use it to optimize the
weights.
116:32 - Use to optimize weights, we
already passed all the
116:37 - parameters that we need.
116:40 - Let's go to the memory. So the
memory is an instance of the
116:45 - replay memory, and it has a
capacity of 10,000. Okay, so
116:49 - let's do a site wide recall you
can store
116:57 - intense experiences which will
be for training for training
117:14 - just ever done. Because it is it
is used to keep track of
117:21 - the steps number of texts keep
track of number of texts taken
117:26 - by the date. We're going to
start with we're going to start
117:34 - with zero. Now let's understand
the step action method as well.
117:39 - So it takes the current state as
input, let's just write down and
117:44 - code this current state and
returns an action so it could be
117:56 - either that the action is it
could be selected by choosing
117:59 - the action with the highest Q
value. So this is going to be
118:01 - the ISP value. Or it could be
that we have taken a random so
118:06 - how we're going to decide so
we're going to decide by this
118:08 - expression. So we had discussed
earlier that we're going to be
118:11 - calculating the threshold value
and if there was a sample within
118:16 - the threshold value, and we have
no grad then we go to the
118:20 - maximum or too high SQL that we
have that we find or we take the
118:28 - random random approach. So
basically this is going to be
118:31 - the envy that action or sample
everything the sample and the
118:34 - device will be either a CPU or a
GPU in our game it is the GPU
118:37 - that we have defined earlier.
And last we have episode
118:41 - durations interviews to keep
track of the duration we can
118:55 - often each episode
119:05 - so now let's also complete the
plot durations method which will
119:09 - plot all the duration of each.
So net is taken by the agent we
119:19 - have shows multiple calls. Start
with the PLT dot trigger. Mainly
119:27 - we're going to plot we're going
to be using biplot here when
119:33 - durations underscore t is equal
to torch dot tensor. We have the
119:41 - episode underscore duration.
Format D type is equal to torch
119:51 - dot float. If so underscore
result The PLT dot title wizard,
120:05 - we're going to be keeping the
title and result as a rule that
120:10 - we have also basically the first
time we change the title and
120:14 - then we'll plot the crosshairs
so that now training painting
120:20 - graphs
120:30 - PLT dot x, x level is going to
be episode and then we have PLT
120:42 - dot y label, which is duration.
Then we have PLT dot plot
120:55 - durations. Obviously, we think
this munition under different
120:59 - durations might have taken not
NumPy will convert it to the
121:05 - numpy array. And then we'll take
100 Episode averages. So let's
121:11 - start it on. First, let's do
that clip length
121:19 - greater than 100 doesn't equal
200.
121:32 - Then means durations underscore
t naught unfold zero to slice it
121:44 - and we'll take the mean and then
we'll take the other viewers
121:50 - what little means is equal to
Torstar cat it's going to be
122:01 - towards our cat towards third
zeros 99 And then we have the
122:08 - means then we have PLT dot plot
122:18 - dot NumPy let's add a pause when
you have PLT dot 0.001 pause so
122:42 - that plots are updated. And then
we if we have a display so
122:50 - basically I just write it down
so that it works on all the
122:54 - environments. If we just run it
in your local VS code also it
122:57 - should work right. So it will be
show underscore results display
123:07 - dot display dot DCF and display
123:20 - display dot here or
123:28 - wait is equal to
123:35 - display dot display PLT dot GCF.
So I'll explain to you what all
123:43 - these plots mean. We're done. So
we have completed the method.
123:53 - Let's now go and write some
comments. So but it basically it
123:57 - will be very clear for all of
you. So this method is used to
124:06 - visualize visualize the training
progress of the DQ N
124:18 - basically, this is the method
here. Now first we'll create a
124:23 - tensor from Episode durations.
So basically this is the keep
124:29 - track of the lengths of
intuition. And then we'll get
124:33 - into using matplotlib. And if we
have the show result as true
124:36 - then we'll label label it as
dessert or as it will be
124:41 - training. And the x axis is
labeled as episodes the Y is
124:47 - labeled as duration. And we have
a function that is called we
124:52 - have a variable that is called
durations underscore t is a blue
124:56 - line and overlays With a red
line that shows 100 Episode
125:02 - moving average of the durations,
so let's just write it down. So
125:06 - the 100
125:15 - moving average of the durations
Yep, so while we wait for just a
125:25 - second so that the plots are
updated it'll come in real time.
125:32 - And if you have a Jupyter
Notebook, then we'll use can use
125:36 - this method as we'll just use a
normal method so now let's start
125:43 - our training loop. So, let's
create our optimized model here
125:50 - called model from spaces if
length
126:06 - match network size then return
we do not need to optimize at
126:13 - the start then we have the
transition when we take a memory
126:19 - that sample basically this is
the sampling we will do the
126:24 - sampling
126:31 - okay, then we have the batch
match is equal to transition so,
126:43 - visibly this looks a little bit
complicated, but I'll explain to
126:47 - you like why we do this
basically we transpose the batch
126:52 - and it will convert the batch
array of transitions to
126:55 - transition a batch or is it
converts batch or a batch of
127:04 - transitions to trans transition
of batteries
127:16 - now, let's compute the mask of
non final states 30 being non
127:21 - underscore final underscore mask
equal to torch dot insert then
127:28 - we have that tuple do map
iterator than the continue
127:38 - number is says not none
127:47 - match.com device
128:09 - this is done so let's write down
what this method will do what
128:14 - we're planning to do, we're
going to compute
128:18 - our mask off non final states
and the batch limits okay and
128:34 - the final state will be where
the simulation and basically
128:37 - that means that
128:42 - let's do I think we have to
provide a parameter can be D
128:46 - type SQL to towards that Boolean
129:05 - now let's do the non final next
states
129:16 - recall that cat as far as that's
not next step
129:42 - is not null. Then we have the
state underscore batch equal to
129:52 - torch dot cat batch dot state
And we have the action but the
130:04 - action bets New Order batch
Yeah, this is bad dot action
130:16 - this is bad got reward
130:22 - so now let's compute the Q value
now state score action let's
130:30 - call values equal to policy net
basically this is the policy
130:38 - that we're planning to use
policy net state school beds not
130:49 - the other one comma action
131:00 - we'll compute the key value here
131:06 - it's now compute the next state
values also. Next underscore
131:14 - state which will be the final
state if it is ending then it
131:21 - will be zero in case the finest
gets the state was the final
131:27 - state will be taught not zero
131:33 - match score size comma device is
equal to device us now with no
131:45 - no torch or that no grand
131:53 - do next underscore state
underscore values non underscore
132:02 - final mask SQL to target net it
looks a little bit complicated
132:11 - but trust me when I explained to
you after writing the code they
132:14 - were pretty clear what's missing
there looks code looks
132:21 - intimidating but what we tend to
do is pretty simple got Max one
132:30 - of zero now let's also compute
the expected value
132:40 - expected underscore state
underscore action value
132:48 - I think we I forgot that yeah we
need to do this first you can do
132:53 - this first cannot
132:56 - underscore values equal to next
state values values to gamma
133:12 - let's see what we what that
about losses well I'll explain
133:22 - to you what police teams losses
criterion
133:30 - is equal to n dot mood one loss
that's the name and also the
133:44 - call to
133:51 - state score action score I use
expected the school state school
134:02 - action expected sports score
action score values squeeze one.
134:24 - Now let's optimize
134:31 - the optimizer dot basically our
query and the way to do
134:39 - optimizer dot zero underscore
grad loss dot backward
134:54 - is in place so it must be
looking like It's pretty tough
135:01 - but trust me I'll explain to the
Medicare then dot dot utils not
135:12 - clip grad value add value
underscore policy not net that
135:24 - parameters commander
135:36 - then verse will keep the value
now let's do optimizer dot
135:45 - now, let's look at this. So,
first we check if this is a mini
135:51 - batch, so, we have the Optimize
method right. So, this is deep Q
135:56 - learning that is working what we
already know. So well let's
136:01 - check what we do. So first we
replay memory. So, first we
136:06 - check if the replay memory
contains enough samples to fill
136:09 - up a mini batch let's write it
down if check if we have enough
136:19 - samples for you if you do not
then return nothing then do
136:28 - nothing okay and then let's
transition. So, first then we
136:33 - what do we do is we take many
bytes extract
136:45 - extract extract I mean us me
myself transitions from Epson
137:00 - comma reward comma next step
from that
137:15 - now then also what we do is we
calculated calculate the
137:23 - expected Q value and it is the
sum of the immediate reward. So,
137:28 - basically we have it here
137:40 - too, we have the gamma as well I
cannot see grandma Yeah, so,
137:47 - basically just search for Yeah,
this is where we do the expected
137:55 - state calculation. So, what we
do is okay good calculate the
138:04 - expected Q value Expected Q
value you value for each
138:13 - transition using the target
network basically we have
138:20 - created our neural network here.
Now, if you do not know how to
138:25 - create a basic neural network,
no issues so, basically it is
138:32 - the sum of the immediate reward
and the discounted Q value of
138:37 - the next state. Okay, now under
this contract is controlled by
138:41 - the gamma. Okay, now, let's see
the function. Now what we'll do
138:48 - is we'll calculate the q value
predicted by the Policy Network
138:52 - for each transition. And we'll
also select the queue
138:55 - corresponding to the action
taken by the agent. So,
138:58 - basically, what we are doing
here, we are also selecting now,
139:06 - we need to calculate the hover
loss as well. The upper loss is
139:11 - a smooth approximation of the
mean squared error loss and is
139:15 - less sensitive to outliers. So,
we need to be less than smooth.
139:22 - What is our loss it is a smooth
approximation. We do not
139:26 - understand how the loss knows us
you can just look it up. But
139:29 - it's a pretty good metric for
calculating data loss. Listen
139:36 - that
139:42 - basically, this is our losses.
And now finally we back
139:48 - propagate now to the Quality
Network and update the
139:52 - parameters using the Adam W
optimizer. And we also have
139:57 - clipped it to a maximum of 100
To prevent the exploding
140:01 - gradient problem, so
140:05 - maximum value is equal to under
to prevent exploding gradient
140:18 - problem so now, let's go on to
the main loop now maintaining
140:27 - loop. So we have completed the
training part of it now we need
140:29 - to create,
140:32 - we need to start implementing
the algorithm. Let's do that
140:35 - now. So what we'll do is if
torch dot CUDA, basically, I'm
140:39 - just making the code in such a
way that if you do not have a
140:43 - GPU, you can use this code, you
don't have to make any changes.
140:48 - So then you need to make number
underscore episodes, you can
140:52 - just keep it 50.
140:58 - And also, if it is available,
then I will keep it somewhere
141:01 - else. I'll do it
141:15 - now let's loop over the episodes
for I underscore episode in
141:22 - range of number of episodes
141:31 - to date, so I don't I think you
all know what this does is
141:36 - really set the environment and
get the current state and the
141:38 - all the information that we
need. Yeah, let's just write it
141:44 - down again. Visualize the
environment. Get state to state
141:56 - is equal to torch the tensor
state comma D type is equal to
142:06 - call that float 32. Comma device
is equal to device and let's do
142:15 - an squeeze zero. I think you all
know what we're doing here. I've
142:24 - already explained it once. Let's
look let's create another loop.
142:28 - Now what do we have the
encounter? Number of counter the
142:34 - reaction is equal to cylinder
since we need to select an
142:36 - action here. Select underscore
action false was for the first
142:43 - time we do not want anything
142:45 - with by the state and we want
observation. Comma, reward comma
142:54 - terminated comma can get it any
other parameters that we get is
143:05 - equal to NV dot step is
143:16 - equal to send your items where
you could take action on items.
143:22 - And then at Maker step forward,
so we'll need a new observation
143:29 - they were terminated and you're
truncated well. Yeah, now let's
143:36 - give the final reward is well,
we've taken a step forward now
143:39 - let's
143:43 - look on the reward or the tensor
reward karma device, welcome
143:57 - device. And we have done is
equal to terminate it or
144:05 - truncated.
144:07 - And we have if Yeah, so we can
either have that we have
144:11 - completed the number of steps or
that we have the card if the
144:17 - card pool has fallen off 2.4
meters away from the center to
144:23 - easily that could be that next
underscore state is equal to
144:29 - none. As we need to do you find
the next state? We'll do that.
144:38 - One next state will be we call
it a tensor
144:50 - tensor observation variation
comma V type V Going to torch
145:02 - dot float 32 comma device, the
call to device dot and squeeze,
145:14 - zero now let's add this next
step in memory. So we can do
145:26 - that using manager push, we have
created that in our class
145:31 - action, we already have all this
action. Next, we have the next
145:39 - video. And we have the reward.
Now store, just keep that in
145:46 - comments as well store the
transition
145:54 - and then move to next, let's
move to the next as well.
146:12 - So basically, we move to the
next step, let's optimize the
146:14 - model. Now. We have already
created the model for that, the
146:19 - method for that. Now let's do
the soft update of the target
146:23 - network weights. So we already
have that new target underscore
146:27 - net underscore state. The score
did this mean we need to do it
146:34 - for both the target and the
policy next. goal to target and
146:43 - the score net dot state
underscore dict. So if you find
146:50 - it pretty difficult, it's okay.
I happen to be the first time.
146:54 - But when it comes into code, I
just go everything once again.
146:57 - So everything becomes very
clear. For Kean policy net, I
147:02 - think we have to take this one
is the policy net, that's to
147:06 - training that state.
147:11 - And then when to do key. The
call to policy net underscore
147:17 - net underscore state underscore
of key swing into to multiplied
147:26 - by the tau tau plus target net
state t. So the old value also
147:38 - into one minus tau. Basically
this is what we're planning to
147:41 - do now.
147:46 - One minus tau is what we're
doing. Now let's see if we are
147:53 - done. We are going to do a
software update. And then we are
147:56 - done. Episode dot.so We are
completed one episode two, I
148:01 - think I've explained to you what
is the difference between
148:04 - Episode and an iteration.
Basically, an episode is the one
148:11 - sequence between a start and an
end to basically in the
148:14 - cardboard instance, the start
will be when we started the
148:19 - initial state. And then if we
fall of 2.4 meters away from the
148:24 - center, or that we have
completed the game, basically we
148:27 - have achieved the maximum state
that we could we have a number
148:31 - of times we could have, we could
have played this game. So this
148:36 - is one, this is going to be one
one time that we are going to
148:39 - play the game basically. And we
are for if you have a GPU, we're
148:43 - going to do this 600 times. So
basically, we have to play this
148:45 - game 600 times. There's going
durations and there's to break
148:51 - out. Now let's complete it. So
finally, we have completed then
149:04 - we are going to plot all the
durations. Basically what I
149:08 - encourage is we can just keep
changing the number of durations
149:11 - and check the results.
Basically, that is something I
149:13 - want you to do. And I'm not
going to do that. I'm just going
149:16 - to stick to the 600 episodes,
because I have a GPU I have and
149:22 - PLT dot show
149:32 - done with the cart pole
environment as well. Let's
149:34 - quickly summarize everything
that we have learned. So what
149:39 - the agent has to decide it has
to decide between two actions
149:42 - moving the cart left or right.
So basically this is so we do a
149:47 - loop here. So as you can see,
either the carpet will move left
149:51 - or right and we need to balance
it out. So now I've explained
149:58 - all this in detail Let's look at
all the methods that we have
150:03 - imported. So, we have imported
the torch that nn which is used
150:07 - for our neural network, we have
imported that thought is that
150:10 - octane method, which is for
optimization imported towards an
150:13 - autograph, which is for
automatic differentiation. And
150:15 - also I've explained in detail
what each of this is you would
150:19 - extend each of this in detail as
well. Now, we have started with
150:23 - the import of the module that we
need also we have started. So,
150:29 - basically we have installed it
and here we have imported all
150:33 - the methods that we need, here
we have set up the environment.
150:37 - And also I have also made it
such that you can run it in the
150:41 - Google collab notebook as well
also in just any python script
150:46 - as well. So, just accordingly,
then we have detected if CUDA is
150:52 - available, which is a GPU. Now,
I have also explained to you
150:56 - what is the concept of replay
memory, basically, it is a
150:59 - technique used in reinforcement
learning to store and manage the
151:02 - experiences of an agent during
training. So, basically, we will
151:05 - make use of this, we have saved
all of the steps that the agent
151:10 - takes, and use that to make a
future decisions. So we're going
151:18 - to make two classes one
transition class and one replay
151:20 - memory class to basically we
have to transition here and then
151:23 - we have the replay memory class.
And we are going to use the DQ n
151:28 - algorithm. It is a deep Q
network, which is a
151:31 - reinforcement learning algorithm
that uses deep learning deep
151:35 - neural networks to approximate
the Q function, I will end with
151:38 - the Q function and we have also
learned about the Q learning
151:40 - algorithm when we were solving
that chapter. So, we have
151:47 - basically I have written all the
steps that we will be using to
151:51 - solve the Cardpool environment
using the DQ n algorithm. Then
151:57 - we have implemented the class
for that as well. Then we come
152:00 - to the training section. During
training the Q network is
152:03 - updated using the Bellman
equation I have explained the
152:05 - Bellman equation as well, to
minimize the mean squared error
152:08 - between the predicted Q values
and the target Q values. The
152:12 - Target Q values are computed
using the Q network but with the
152:14 - weights frozen and not updated
during the current iteration of
152:17 - training. Now, we have some
important concepts here. So
152:22 - basically, this is the Select
Action, maybe P start and we
152:25 - have plot durations, where we
paste the case what I have
152:28 - explained to you all these hyper
parameters as well. Let's go to
152:31 - a very important concept that is
Adam optimizer. So Adam adaptive
152:35 - moment estimation is a popular
optimization algorithm that is
152:38 - commonly used in deep learning.
It is an extension of stochastic
152:42 - gradient descent, which is the
most basic optimization
152:45 - algorithm used to train neural
networks. The main idea behind
152:48 - it is to combine the advantages
of two other optimization
152:51 - techniques added an RMS block.
Here I've explained in detail
152:55 - how we'll be using it in our DQ
n algorithm. Next, we get the
153:00 - number of actions from the
geometric space we set up, we
153:03 - set the environment and get the
initial state, we set up the
153:07 - number of features that we have,
we have the atom optimizer, we
153:11 - have initialized our training
neural network as well, we have
153:14 - the replay memory which is a
10,000 buffer, which is
153:18 - contained 10,000 last 10,000
episodes, then, basically this
153:24 - is a training loop, I explained
in detail as well, then we have
153:27 - the optimized model, this method
will optimize the model. So
153:31 - basically what we need to do is
we need to just take a mini
153:35 - batch. So and also we will be
using sampling as well. To have
153:40 - gone to that now you have
started with the training. So
153:46 - we'll take that if CUDA is
available that is a GPU is
153:49 - available, then we'll do six
minutes because or else we'll
153:52 - only find it episodes. So yeah,
this is the training loop. And
153:57 - then we have plotted the end
result. This chart I've also
154:00 - explained in detail how we use
this chart to understand the
154:05 - performance of our algorithm.
Now we've completed our solving
154:15 - of logic using Q learning. So
basically, let's move on to the
154:19 - advanced topics that we are
going to do go to cover and get
154:22 - you will explore after learning
this beginners tutorial. So you
154:26 - could explore deep reinforcement
learning. Deep Reinforcement
154:30 - learning is a combination of
deep learning and reinforcement
154:32 - learning that uses neural
networks to approximate the Q
154:35 - values in Q learning. Deep RL
algorithms such as deep Q
154:39 - network and actor critic have
been successful in solving
154:42 - complex tasks such as Atari
Games robotics and autonomous
154:45 - driving. So, basically, this is
what you can explore, you can
154:50 - dive into autonomous driving
now. Policy gradients policy
154:54 - gradient algorithms directly
optimize the policy function by
154:57 - adjusting the parameters of the
policy to gradient Set. This
155:01 - allows the agent to learn a
policy that maximize the that
155:04 - maximizes the expected reward
without competing the Q values.
155:09 - The advantage of policy gradient
algorithm is that they can
155:12 - handle continuous action spaces,
unlike Q learning. So this is
155:15 - pretty interesting. You can also
explore this. Now let's move on
155:19 - to multi agent reinforcement
learning. Yeah, so this is
155:21 - pretty interesting. Now you can
have two three cars in an
155:26 - environment and you can simulate
that. And you can use petting
155:30 - zoo for that. So petting zoo is
a brother of Guinness. And it
155:34 - allows it gives us an
environment for multi agent
155:37 - reinforcement learning. Multi
agent reinforcement learning
155:40 - extends reinforcement learning
to the scenario where multiple
155:43 - agents interact with each other
in a shared environment. In
155:46 - multi reinforcement learning,
each multi agent reinforcement
155:49 - learning agent learns his policy
based on the joint reward
155:53 - received by all agents. The
opening a gymnasium toolkit
155:56 - provides environments for MRL
such as the classic prisoner's
155:59 - dilemma game and cooperative
navigation tasks. So there's
156:03 - some pretty interesting
imitation learning, also known
156:06 - as learning from demonstration
is a technique that trains an
156:09 - agent to mimic the behavior of
an expert. This approach has
156:12 - been used in various
applications such as autonomous
156:14 - driving, robotics and game
learning. And genbrain Open AI
156:18 - gymnasium toolkit include
environments from imitation
156:20 - learning such as Motoko humanoid
locomotion task flow, this also
156:25 - sounds pretty interesting. Now
let's look at the last topic
156:28 - that you could cover in the
advanced topics to discuss
156:31 - transfer learning. transfer
learning is the technique of
156:34 - transferring knowledge learned
in one task to another task. In
156:38 - reinforcement learning, transfer
learning can help speed up the
156:40 - learning process and improve the
performance of the agent. The
156:43 - Albany gymnasium toolkit
provides environments for
156:46 - transfer learning, such as the
classic inverted pendulum does
156:49 - and card polls wind up just so
now we talked about multi agent
156:55 - reinforcement learning, so let's
check out putting zoo
156:57 - documentation that I mentioned.
So what we can do is here you
157:03 - can check out all the different
types of environment that are
157:05 - available. So what are the games
we have pulled up? Here we have
157:10 - Mario Bros for multi agents of
Luigi and Mario here on the
157:19 - action space is defined so I've
explained all the concepts here
157:21 - so it wouldn't be very difficult
for you to start implementing
157:24 - these. We have Space Invaders
will be basically these are
157:28 - different types of games
available. Let's check out
157:31 - classic games. We have chest
Solibri chest as well. So I
157:35 - highly encourage you to go first
Lex was when you understand the
157:39 - basics, then I highly encourage
you to go out and check out the
157:43 - petting zoo documentation and
study creating your own
157:47 - environments and your own agents
and solve some real world
157:51 - problems.