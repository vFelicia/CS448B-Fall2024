00:00 - David Clinton has written and created many
popular technical books and video courses.
00:06 - This data analytics course, along with the
accompanying website, and Jupyter Notebooks
00:11 - will help you learn data analytics in 30 days.
Welcome to my course, I'm really glad to have
00:17 - you here. And I'm even happier that you've
decided to join the data analytics party.
00:23 - Who am I, I'm the author of more than a dozen
books on Linux and AWS administration, digital
00:30 - security, and dozens of courses on Pluralsight.
I've also got a fistful of articles right
00:36 - here on the Free Code Camp news site. But
I just write about stuff. Hopefully, when
00:42 - you're done with this content, you'll be out
using data to change the world. Since you've
00:47 - already seen my claim that this will only
take you 30 days, I should explain what this
00:54 - actually is going to show you the tools you'll
need to find and manipulate raw data, and
01:00 - use various graphing tools to help you understand
and interpret it. But don't expect us to cover
01:05 - a full data science curriculum here complete
with single and multivariable calculus, algorithmic
01:12 - problem solving, or even machine learning.
That would require a whole lot more time and
01:17 - effort. If that's what you're after, check
out the new data science content that Free
01:23 - Code Camp is in the process of bringing online,
there's something else you won't get from
01:28 - these videos experience. Once you've watched
the entire course, you probably still won't
01:34 - be able to do much on your own. The value
of actual hands on experience is the mistakes
01:40 - you make, you know, Miss typing syntax, not
properly understanding what your code is doing,
01:47 - or failing to account for Environment specific
restrictions. diagnosing and working through
01:52 - those mistakes is where you'll really begin
to take charge and accomplish great things.
01:57 - So where will you get that experience? If
you're ambitious, and you've got exciting
02:02 - project ideas of your own, by all means, dive
in and try it out. But if you think you still
02:08 - need some guidance, then I've got everything
you should need on my stories dot the data
02:13 - project dotnet site, you can work through
the exercises in each of eight data stories
02:19 - that you'll find over there. If there's a
specific skill you're looking for the learning
02:25 - objectives index down here will point you
directly to the chapter where you'll find
02:30 - it all that's available for anyone and for
free. If you happen to prefer working with
02:35 - a real book, you can purchase the same content
in that format. But don't think anything's
02:40 - missing from the free website.
02:42 - But right now,
02:44 - let's talk about data analytics tools. There
are many ways to consume data, the one you
02:51 - choose will reflect your specific needs, and
your comfort with various skills. spreadsheets,
02:58 - as you probably already know, are much more
than just fancy calculators or places to keep
03:03 - your household budget numbers. They also come
with powerful functions, external integrations
03:10 - and graphing capabilities. Enterprise strength
tools like Tableau Splunk, or Microsoft's
03:16 - Power BI, are also great for crunching numbers
and visualizing insights, which you can then
03:22 - share with your team members. So then what's
the big deal with Python? Well, the Python
03:27 - ecosystem is much, much broader than those
purpose build tools. And the Python community
03:34 - makes all kinds of useful data specific libraries
and modules available. When you let Python
03:40 - loose against your data. You've got all the
resources of a full bore industrial strength
03:46 - programming language at your fingertips. It's
not what you can do with it. That's the challenge.
03:52 - It's finding something that you can't do.
OK, but what about Jupiter? Jupiter is an
03:58 - open source platform within which you can
load your data and execute your Python code.
04:04 - It's a lot like a programming Id like Microsoft
Visual Studio. And while Jupyter Notebooks
04:11 - can be used with a growing number of languages,
and for as many tasks as you can imagine,
04:16 - it's best known and loved as a host for Python
data heroics. Once upon a time, the lines
04:23 - of code you'd write for your programs would
be saved to a single text file whose name
04:28 - ended with a dot p y suffix. When you want
it to run your code to see how things went.
04:34 - You do it from the command line or a powerful
and complicated source code editor like Visual
04:39 - Studio. But for anything to work, it all had
to work. That would make it harder to troubleshoot
04:46 - when something didn't go according to spec.
But it would also make it a lot harder to
04:51 - play around with specific details just to
see what happens. And it also made it tough
04:56 - to share live versions of your code across
the internet. As we'll soon see, Jupyter Notebooks
05:03 - let you run your code a single line at a time,
or altogether. That flexibility makes it easier
05:09 - to understand your code and when things go
wrong to troubleshoot it. notebooks, by the
05:16 - way, are JSON based files that effectively
move the processing environment for just about
05:22 - any data oriented programming code from your
server or workstation to your web browser.
05:28 - You can download Jupiter to your PC or a private
server and access the interface through any
05:34 - browser with network access. Or you can run
notebooks on third party hosting services
05:40 - like Google's core laboratory, or for a cost
cloud providers like Amazon's Sage maker studio
05:47 - notebooks, or Microsoft's as your notebook.
Jupiter comes in three flavors. The two you're
05:53 - most likely to encounter are classic notebooks
and the newer Jupiter lab. Both run nicely
05:59 - within your browser. But Jupiter lab comes
with more extensions and lets you work with
06:03 - multiple notebook files and terminal access
within a single browser tab. I'll be using
06:09 - the classic notebook environment for the demos
in this course, but there's usually no problem
06:14 - transferring notebooks between versions. The
third labor just to be complete is Jupiter
06:20 - hub. A server version built to provide authenticated
notebook access to multiple users, you can
06:26 - serve up to 100 or so users from a single
cloud server using the littlest Jupiter hub.
06:33 - For larger deployments involving clusters
of servers, it probably be better off with
06:38 - a Kubernetes version known as zero to Jupiter
hub with Kubernetes. But all that's way beyond
06:44 - the scope of this course. Our next job is
to build our work environment. Assuming you've
06:51 - decided to host Jupiter on your own machine,
you'll need Python installed. The good news
06:57 - is that most operating systems come with Python
pre installed, you can confirm that you've
07:02 - got an up to date version of Python by opening
a command prompt and typing Python dash dash
07:08 - version or sometimes Python three dash dash
version, a pythons installed, you'll probably
07:16 - see something like this. Just make sure you've
got Python three installed and not the deprecated
07:22 - and insecure Python two. If you do need to
install Python manually, you're best off using
07:28 - pythons official documentation that will be
the most complete and up to date source available
07:34 - that will work with whatever operating system
you're on. It's important to note that not
07:40 - all Python versions even those from three
point x will necessarily behave quite the
07:45 - way you expect. You may for instance, find
that you need a library written for version
07:50 - 3.9. But that there's no way to get it working
on your 3.7 system.
07:56 - Upgrading your system version to 3.9 might
work out well for you. But it could also cause
08:02 - some unexpected and unpleasant consequences.
It's hard to know when a particular Python
08:07 - library might also be needed by your core
operating system. If you pull the original
08:13 - version of the library, he might end up disabling
the OS itself. And don't think it won't happen.
08:19 - I crippled my own O's that way just a few
months ago. One solution is to run Python
08:25 - for your project within a special virtual
environment that's isolated from your larger
08:30 - o 's. That way, you can install all the libraries
and versions you like without having to worry
08:36 - about damaging your work system. You can do
this using a full scale virtual container
08:42 - running a Docker or as I prefer LX D image
or on a standalone AWS cloud instance. But
08:49 - you can also use pythons own v n module, you
want to read the official documentation for
08:56 - the virtual environment instructions specific
to your host OS. Whichever version of Jupiter
09:01 - you choose, if you decide to install and run
it locally, the Jupiter project officially
09:07 - recommends doing it through the Python Anaconda
distribution and its binary package manager
09:13 - conda. Various guides to doing that are available
for various OS hosts, but this official page
09:19 - is a good place to start. As you can see though,
the Python PIP package manager is also an
09:25 - option. Once all that's done, you should be
able to open a notebook right in your browser
09:30 - and get right down to work. For me, a notebooks
most powerful feature is the way you can run
09:37 - subsets of your code within individual cells.
This makes it easier to break down long and
09:43 - complex programs into easily readable and
executable snippets. With a cell selected.
09:50 - Clicking the Run button will execute just
that cells code. Note how the box on the left
09:56 - gets a number representing the sequential
position of the execution. As you become more
10:01 - familiar with Jupiter, you'll probably get
in the habit of executing cells using Ctrl
10:07 - and enter rather than a mouse click, you can
insert a new cell right after the one that's
10:12 - currently selected by clicking the plus button.
the up and down arrows move cells as you might
10:18 - expect, up and down. cells, by default are
formatted to handle code, Python three in
10:25 - my case, but they can also be set for markdown
which can be handy for documenting your notebooks,
10:31 - or making new sections easier to find. A single
hashtag in markdown, for instance, represents
10:36 - a top level selection title, executing the
cell will print the text to match your formatting
10:43 - instruction. The precise locations and appearance
of the buttons you'll use to get this stuff
10:48 - done will vary between the different Jupiter
versions, but all the basic functions are
10:53 - universally available. Whatever values your
code creates, will remain in the kernel memory
11:00 - until the output from a particular cell or
for the entire kernel are cleared. This lets
11:05 - you rerun previous or subsequent cells to
see what impact the change might have. It
11:11 - also means resetting your environment for
a complete start over is as easy as selecting
11:15 - restart kernel and clear all outputs.
11:27 - Not all Python functionality will be available
out of the box. Sometimes, as you saw just
11:32 - before, you'll need to tell Python to load
one or more modules through declarations within
11:38 - your code. But some modules need to be installed
manually from the host command line before
11:43 - they can even be imported. For such cases,
Python recommends their installer program
11:48 - Pip, or in some cases, the conda tool from
the Anaconda distribution. You can read more
11:53 - about using PIP for the proper care and feeding
of your Python system within the helpful Python
11:59 - documentation site. Okay, here's where we
get down to real work. We're going to head
12:07 - out to the internet to find reliable data
that will help us answer a real world question.
12:12 - And we'll use a public API to get the data.
Then we'll examine the data to get a feel
12:18 - for its current formatting, and what it'll
take to fix it. After applying the necessary
12:24 - formatting, so our code can happily read it
will merge multiple data sets together so
12:29 - we can look for correlations, and then experiment
with graphing tools to find the one that represents
12:35 - our data in the most intelligible way. What's
the problem we're trying to solve? I'm curious
12:41 - to see whether wages paid to us workers over
the past 20 years have on average, gone up,
12:49 - assuming they have increased, I'd also like
to know whether the extra money in their pockets
12:53 - has also increased their actual standard of
living? To answer those questions, we're going
12:58 - to access two data sets collected and maintained
by the US government's Bureau of Labor Statistics.
13:06 - One of the many nice things about the Bureau
of Labor Statistics usually referred to as
13:11 - BLS, is that they provide an API for access
from within our Python scripts. To make that
13:18 - work, you'll need to know the BLS endpoint
address matching the specific data series
13:23 - you need the Python code to initiate the request,
and for higher volume requests a BLS API key.
13:33 - Getting the series endpoint addresses you
need may take some digging around in the BLS
13:38 - website. However, the most popular data sets
are accessible through a single page. This
13:43 - shows you what that looks like, including
the endpoint codes like lns, one one and a
13:49 - whole lot of zeros for the civilian labor
force said, You can also search for data sets
13:54 - on this page. Searching for a computer for
instance, will take you to a list that includes
14:00 - the deeply tempting average hourly wage for
level 11 computer and mathematical occupations
14:07 - in Austin, Round Rock, Texas. The information
you'll discover by expanding that selection
14:13 - will include its series ID, which is the end
point because I know you can barely contain
14:19 - your curiosity. I'll click through and show
you that it turns out that level 11 computer
14:24 - in mathematical professions in Austin, Round
Rock Texas, could expect to earn $51.76 an
14:32 - hour back in 2019. So how do you turn those
series IDs into Python friendly data? manually
14:40 - writing get and put requests can be very picky,
and it'll take a lot of tries before you get
14:45 - it exactly right. To avoid all that, I decided
to go with a third party Python library called
14:52 - BLS. that's available through all of our share
roses GitHub repo, you install the library
14:58 - on your host machine, using pip install BLS.
That's all it'll take.
15:04 - While we're here, we might as well activate
our BLS API key. You register for the API
15:10 - From this page, and they'll send you an email
with your key and a validation URL that you'll
15:15 - need to click. Once you've got your key, you
export it to your system environment on Linux,
15:21 - or Mac OS, that would mean running something
like this, where your key is substituted for
15:26 - the fake one I'm using here, I'm going to
use the API to request us consumer price index,
15:33 - CPI, and wage and salary statistics between
2002 and 2020. The CPI is a measure of the
15:40 - price of a basket of essential consumer goods.
It's an important proxy for changes in the
15:45 - cost of living, which in turn, is an indicator
of the general health of the economy. Our
15:51 - wages data will come from the BLS employment
cost index, covering wages and salaries for
15:57 - private industry workers in all industries
and occupations. A growing employment index
16:03 - would at first glance, suggests that things
are getting better for most people. However,
16:08 - seeing the average employment wage trends
in isolation isn't all that useful. After
16:13 - all, the highest salary won't do you much
good if your basic expenses are higher still.
16:19 - So the goal is to pull both the CPI and wages
data sets, and then correlate them looking
16:25 - for patterns. This will show us how wages
have been changing in relation to costs. Now,
16:32 - let me show you how it actually works. With
valid endpoints for the two data sets we are
16:37 - going to be using, we're all set to start
digging for CPI, and employment gold. Importing
16:44 - these four libraries, including BLS will give
us all the tools we'll need. pandas stands
16:50 - for Python for data analysis, which is a library
for working with data as data frames. Data
16:56 - frames are perhaps the most important structure
you'll use as you learn to process large data
17:02 - sets. NumPy is a library for executing mathematical
functions against large arrays of data. And
17:08 - map plot live is a library for plotting data
in visual graphs of various kinds. when importing
17:15 - a library, you assign it the name you'll use
to invoke it. You can choose just about anything
17:20 - but pandas is often represented by PD NumPy
as NP and matplotlib as PLT, I'm also importing
17:29 - the BLS library that we installed a bit earlier,
that will be invoked using its actual name,
17:35 - BLS. I'll execute that cell. Now I pass the
BLS endpoint for the CPI data series to the
17:42 - BLS get series command from the BLS library.
The endpoint code itself was of course, copied
17:49 - from the popular data sets page on the BLS
website, I'll assign the data series that
17:54 - comes back to a data frame using the variable
CPI, and then save the data frame to a local
18:01 - CSV file. This isn't necessary, but you might
find it easier to work with the data when
18:06 - it's saved locally. Next, I'll load the data
from the new CSV file using the pan das PD
18:13 - read CSV command against the file name. I'll
assign the variable name CPI data to the new
18:19 - data frame that comes out the other end. Running
just CPI data will print out the first and
18:25 - last five lines of the data frame. The date
column contains month and year values. And
18:32 - the second column contains our actual data.
I'd like to simplify the headers to make them
18:37 - easier to work with. So I'll use the pan das
columns attribute I definitely prefer this
18:43 - way. However, we'll need to also see the wages
data to know whether the formatted uses is
18:48 - compatible with our CPI set. So I'll pull
the wages data series using the BLS library
18:55 - and assign it to the wages data frame. Once
again, I'll save it to a local CSV file and
19:01 - read that data into a new data frame called
df. I'll clean up my column headers and use
19:06 - head to print only the first five lines of
data, you should notice two things. This data
19:13 - isn't delivered in monthly increments, but
quarterly, but only one entry for every three
19:18 - months. And the date format is different.
Instead of a month number, there's q1 or q2.
19:25 - If you want Python to sync between our two
data sets, we'll need to do some editing.
19:30 - I'll do that by replacing every March data
point meaning any date entry containing the
19:36 - string dash 03 within its date value with
the string q one June that is a string including
19:45 - dash 06 we'll get q2 and so on. As you can
see now when I print just the date column,
19:53 - some values have been updated to the new format.
But the rest of them are from this point unnecessary
19:59 - and will cause us trouble So we'll have to
get rid of them altogether. I'll do that by
20:03 - creating a new data frame called New CPI and
reading into it the contents of the old CPI
20:10 - data data frame. But I'll use the pan das
string dot contains function to identify all
20:16 - the rows in the data frame that contain a
dash. And by specifying false, dropping them,
20:22 - we'll be left with only properly formatted
quarterly data points. And I said, I'll save
20:28 - this data frame to a CSV file to notice how
we've dropped from 232 rows to just 77. Just
20:36 - because I'm paranoid, I'll create a new data
frame called New df. So the old df data frame
20:42 - will still be available to me, should I accidentally
make a mess with what we're about to do next,
20:48 - with our data all neatened up, we're ready
to begin our analysis. We've got a big problem
20:54 - here, the data in the CPI set comes in absolute
point values. While the wages are reported
21:01 - in percentages measuring growth, as is there's
no way to accurately compare them. For one
21:08 - thing, each row of our wages data is the percentage
by which wages whatever is in that quarter
21:15 - had the current rate continued for a full
12 months. So not only do those values not
21:21 - correspond to the absolute CPI price data,
they're not even technically true of their
21:26 - own timeframe. So when we're told that the
rate for the first quarter of 2002 was 3.5%.
21:34 - That means that if wages continued to rise
at the current first quarter rate, for a full
21:39 - 12 months, the annual average growth would
have been 3.5%, but not 14%, which means the
21:46 - numbers we're going to work with will have
to be adjusted. That's because the actual
21:50 - growth during say the three months of 2002,
quarter one wasn't 3.5%, but only a quarter
21:58 - of that, or 0.875%. If I don't make this adjustment,
but continue to map quarterly growth numbers
22:05 - to quarterly CPI prices that are calculated
output would lead us to think that wages are
22:11 - growing so fast, that they become detached
from reality. Now, I should warn you that
22:16 - solving this compatibility problem will require
some fake math, I'm going to divide each quarterly
22:23 - growth rate by four or in other words, I'll
pretend that the real changes to wages during
22:28 - those three months were exactly one quarter
of the reported year over year rate. I'm sure
22:34 - that's almost certainly not true. And it's
a gross simplification. However, for the big
22:39 - historical picture I'm trying to draw here,
it's probably close enough. Now that will
22:44 - still leave us with a number that's a percentage,
but the corresponding CPI number we're comparing
22:49 - it to is again a point figure. To solve this
problem. I'll apply one more piece of fakery.
22:57 - To convert those percentages to match the
CPI values, I'm going to create a function,
23:02 - I'll feed the function the starting 2002 first
quarter CPI value of 170 7.1. That'll be my
23:10 - baseline. I'll give that variable the name
new num. For each iteration, the function
23:16 - will make through the rows of my wages data
series, I'll divide the current wage value
23:21 - x by 400. Where'd I get that number 100 simply
converts the percentage to 3.5, etc, to a
23:29 - decimal 0.035. And the four will reduce the
annual or 12 month rate to a quarterly rate
23:37 - covering three months to convert that to a
usable number a multiply it by the current
23:42 - value of new num, and then add new num to
the product. That should give us an approximation
23:48 - of the original CPI value adjusted by the
related wage growth percentage. But of course,
23:55 - this won't be a number that has any direct
equivalent in the real world. Instead, it
23:59 - is, as I said, an arbitrary approximation
of what that number might have been. But again,
24:06 - I think it'll be close enough for our purposes.
Take a moment to read through the function.
24:11 - Global new num declares a variable as global.
This makes it possible for me to replace the
24:16 - original value of new num with the functions
output, so the percentage in the next row
24:21 - will be adjusted by the updated value. Note
also how any strings will be ignored. And
24:28 - finally, Note how the updated data series
will populate the new wages data variable.
24:34 - Let's check that new data looks great. Our
next task will be to merge our two data frames
24:40 - and then plot their data. Don't go away. What's
left, we need to merge our two data series
24:47 - so Python can compare them. But since we've
already done all the cleaning up and manipulation,
24:54 - this will go smoothly. I'll create a new data
frame called merge data and feed it with the
24:59 - URL Put up this PD merge function, I simply
supply the names of my two data frames and
25:05 - specify that the date column should be the
index. That wasn't hard. Let's take a look.
25:11 - Our data is all there, we can visually scan
through the CPI and wages columns and look
25:17 - for any unusual relationships. But that defeats
the point. Python data analytics is all about
25:23 - letting our code do that for us. Let's plot
the thing. Here, we'll tell plot to take our
25:29 - merge data frame merge data and create a bar
chart. Because there's an awful lot of data
25:35 - here, I'll extend the size of the chart with
a manual fixed size value, I set the x axis
25:42 - to use values in the date column. And again,
because there are so many of them, I'll rotate
25:47 - the labels by 45 degrees to make them more
readable. Finally, I'll set the labels for
25:52 - the x and y axes. This is what comes out the
other end, because of the crowding, it's not
25:58 - especially easy to read. But you can see that
the orange wages bars are for the most part
26:04 - higher than the blue CPI bars. That means
that wages are experiencing a higher growth
26:10 - rate than the CPI, we'll have a stab at analyzing
some of this a bit later, is there an easier
26:16 - way to display all this data, you bet there
is I can change the value of kind from bar
26:22 - to line, and things will instantly improve.
Here's how the new code will work as a line
26:27 - plot and with the grid.
26:30 - Python, along with its associated libraries,
gives us the ability to use a much wider variety
26:37 - of plotting tools than just bar and line graphs.
We're going to explore just two of them here,
26:44 - scatter plots, and histograms. We'll also
talk a bit about how regression lines work,
26:49 - and what kinds of insights they can show us.
We'll begin with scatter plots. This code
26:54 - is from the property rights and economic development
chapter on my Teach Yourself data analytics
27:00 - website, you can catch up on the background
over there. But the code you're looking at
27:05 - comes from two data sources, the World Bank's
measure of per capita gross domestic product
27:11 - by country and the index of economic freedom
data from the heritage.org site, I merged
27:17 - data from the two data frames into this one
called merge data, I'll create a simple scatterplot.
27:24 - With this one line command, we can clearly
see a pattern, the higher the per capita gross
27:30 - domestic product, meaning the more economic
activity a country generates, the further
27:35 - to the right on the x axis, a country's dot
is likely to fall. And the further to the
27:41 - right, the higher is the economic freedom
score. Of course, there are anomalies in our
27:46 - data. There are countries whose position appears
way out of range of all the others, it'd be
27:52 - nice if we could somehow see which countries
those are, and would also be nice if we could
27:56 - quantify the precise statistical relationship
between our two values, rather than having
28:02 - to visually guess. We'll begin by visualizing
those anomalies in our data. To make this
28:08 - happen. all important another couple of libraries
that are part of the plotly family of tools,
28:14 - you may need to manually install them on your
host using pip install plotly. Before those
28:19 - will work. From there, we can run p x scatter,
and point it to our merged data data frame
28:26 - associating the score column with the x axis
and value with the y axis. So we'll be able
28:32 - to hover over a dot and see the data it represents.
We'll add the hover data argument and tell
28:38 - it to include country and score data. This
time when you run the code, you get the same
28:44 - nice plot. But if you hover your mouse over
any dot, you'll also see its data values.
28:50 - In this example, you can see that the tiny
but rich country of Luxembourg has an economic
28:56 - freedom score of 75.9 and a per capita GDP
of more than $121,000. You can similarly pick
29:04 - out other countries at either end of the chart,
we can learn more about the statistical relationship
29:10 - between our values by adding a regression
line, a measure of the data is R squared value.
29:16 - We already saw how our plot showed a visible
trend up and to the right. But we also saw
29:21 - there were outliers. Can we be confident that
the outliers are the exceptions, and that
29:27 - the overall relationship between our two data
sources is sound. There's only so much we
29:32 - can assume based on visually viewing the graph
at some point, we'll need hard numbers to
29:37 - describe what we're looking at. A simple linear
regression analysis can give us a measure
29:42 - of the strength of the relationship between
a dependent variable and the data model. r
29:47 - squared is a number between zero and 100%.
Where 100% would indicate a perfect fit. Of
29:54 - course, in the real world, a 100% fit is next
to impossible. You'll judge The accuracy of
30:00 - your model or assumption within the context
of the data you're working with, how can you
30:06 - add a regression line to a panda's chart?
There are, as always, many ways to go about
30:11 - it, I like simple and the O LS trend line
approach is about as simple as it gets, just
30:17 - add a trendline argument to the code we've
already been using. That's it. Oh LS By the
30:22 - way, stands for ordinary least squares, which
is a, which is a type of linear regression.
30:28 - And here's how it looked with our regression
line. When I hover over the line, I'm showing
30:32 - an R squared value of 0.550451. Or in other
words around 35%. For our purposes, I consider
30:42 - that a pretty strong correlation. a histogram
is a plotting tool that breaks your data down
30:48 - into bins. A bin is actually an approximation
of a statistically appropriate interval between
30:55 - sets of your data bins attempt to guess at
the probability density function
31:01 - PDF, that will best represent the values you're
actually using. But they may not display exactly
31:07 - the way you think, especially when you use
a default value. I'll illustrate how this
31:12 - works or actually how it doesn't work. Using
data from the do birthdays make elite athletes
31:18 - chapter on the website. As you can see over
there, I'd scraped the semi official NHL API
31:25 - for the birth dates of around 1100 current
NHL players. My goal was to visualize the
31:31 - distribution of their birth dates across all
12 months to see if their births were concentrated
31:36 - within a specific yearly season. When I display
the data using a histogram, we didn't see
31:42 - the pattern we'd expected. In fact, the pattern
wasn't truly representative of the real world.
31:48 - That's because histograms are great for showing
frequency distributions by grouping data points
31:53 - together into bins. This can help us quickly
visualize the state of a very large data set
31:59 - where granular precision will get in the way,
but it can be misleading for use cases like
32:05 - ours. Since we were looking for a literal
mapping of events to calendar dates. Even
32:10 - setting the bin amount to 12. to match the
number of months won't help, because a histogram
32:16 - won't necessarily stick to those exact borders.
What we really need here is a plain old bar
32:22 - graph that incorporates our value counts numbers,
I'll pipe the results of value counts to a
32:28 - data frame called df one, and then plot that
as a simple bar graph. In the next module,
32:35 - we're going to talk about understanding our
data visualizations, and integrating what
32:40 - we see in our Jupyter Notebooks with stuff
that happens out there. In the real world.
32:46 - Stay tuned. We're supposed to be doing data
analytics here, such as staring at pretty
32:52 - graphs probably isn't the whole point. The
CPI and wages data sets we plotted in the
32:59 - previous chapter, for instance, showed us
a clear general correlation. But there were
33:04 - some visually recognizable anomalies. Unless
we can connect those anomalies with historical
33:10 - events, and explain them in a historical context,
we won't be getting the full value from our
33:16 - data. But even before going there, we should
confirm that our plots actually make sense
33:21 - in the context of their data sources. Working
with our BLS examples, let's look at graphs
33:28 - to compare CPI and wages data from both before
and after our manipulation. That way, we can
33:35 - be sure that our math and particularly our
fake math didn't skew things too badly. Here's
33:42 - what our CPI data look like when plotted using
the raw data. It's certainly a busy graph,
33:48 - but you can clearly see the gentle upward
slope punctuated by a handful of sudden jumps.
33:54 - Next, we'll see that same data after removing
three out of every four months data points,
34:00 - the same ups and downs are still visible.
Given our overall goals. I'd categorize our
34:05 - transformation as a success. Now, how about
the wages data here, because we move from
34:11 - percentages to currency, the transformation
was more intrusive, and the risks of misrepresentation
34:18 - were greater. We'll also need to take into
account the way a percentage will display
34:23 - differently from an absolute value. Here's
the original data. Note how there's no consistent
34:30 - curve, either upwards or downwards. That's
because we're measuring the rate of growth
34:35 - as it took place within each individual quarter,
not the growth itself. Now compare that with
34:41 - this line graph of that wage data now converted
to currency based values. The gentle curve,
34:48 - you see makes some sense, it's about real
growth after all, not growth rates, but it's
34:53 - also possible to recognize a few spots where
the curve steepens and others where it smooths
34:59 - out a bit more. Why are the slopes so smooth
in comparison with the percentage based data?
35:04 - Look at the y axis labels. The index graph
is measured in points between 180 and 280.
35:10 - While the percentage graph goes from zero
to 3.5. It's the scale that's different. All
35:18 - in all, I believe we're safe concluding that
what we produced is a good match with our
35:22 - source data. establishing some kind of historical
context for your data will require looking
35:28 - for anomalies and associating them with known
historical events. That's something I do at
35:33 - length in the wages and CPI Reality Check
chapter on the website. If you're interested,
35:39 - I'm sure you'll work through that material
on your own. But I think you've seen enough
35:43 - here to get a picture of how plotting the
right visualization can be helpful.
35:48 - But that brings us to the end of this particular
course, as I've mentioned, a number of times
35:53 - already, the full curriculum is available
on my the data project dotnet site, and you're
35:58 - more than welcome to join all the cool kids
over there and be in touch if you've got something
36:03 - to add to the conversation. The main thing
is to realize that the end of this course
36:08 - isn't anywhere near the end of your data analytics
education. watching me calmly execute nice,
36:15 - clean code samples isn't really learning.
Unless you're a very special breed of genius,
36:20 - you won't begin to understand how all this
really works. Until you dive in and work things
36:25 - through yourself. I say worth things through.
But what I really mean is not worth things
36:31 - through because it's mistakes and frustration
that are the best teachers. Don't imagine
36:36 - that my Python code just came to exist on
a quiet afternoon. While I was sipping nice
36:41 - hot coffee. First of all, I don't drink coffee.
But more to the point, there was nothing quiet
36:47 - about it. There were humiliating failures,
reformulations start overs and countless trips
36:52 - to stack overflow before things began to take
shape. But the more problems I faced and overcame,
36:59 - the deeper the process sank into my mind,
and the better I got at it. And so will you
37:05 - just be prepared for tough times ahead. Before
you all run off and get on with your day,
37:10 - let's spend a moment or two reviewing everything
we saw here. We spoke about the many ways
37:15 - you can work with Jupyter notebooks, including
online platforms like Google's Collaboratory,
37:22 - and locally hosting either Jupiter lab or
plastic notebooks within introduced ourselves
37:28 - to the Jupiter environment, learning about
cells kernels and the operating environment.
37:34 - We saw how we can find data through public
API's and how to integrate API credentials
37:39 - into our Python environment. Python libraries
and modules were our next focus, including
37:45 - how to import appropriate libraries to allow
us to effectively clean and manipulate our
37:51 - data. And finally, turning to some actual
data analytics. We learned some basics of
37:57 - plotting, including working with scatter plots,
regression lines and histograms. And we closed
38:02 - out the chorus with a quick discussion of
how to use our data visualizations to integrate
38:07 - our insights with the real world. I hope this
has been helpful for you and I invite you
38:11 - to check out some of my other content on my
main website, bootstrap dash it.com. Take
38:18 - care