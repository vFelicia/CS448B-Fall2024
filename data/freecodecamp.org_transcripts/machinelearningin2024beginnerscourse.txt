00:00 - this machine learning course is created
00:01 - for beginners who are learning in 2024
00:04 - the course begins with a machine
00:05 - learning road map for 2024 emphasizing
00:09 - career paths and beginner-friendly
00:11 - Theory then the course moves on to
00:13 - Hands-On practical applications and a
00:16 - comprehensive end to-end project using
00:19 - python Todd have created this course she
00:22 - is an experienced data science
00:24 - professional her aim is to demystify
00:26 - machine learning Concepts making them
00:29 - accessible and actionable for newcomers
00:31 - and to bridge the gap in existing
00:33 - educational resources setting you on a
00:36 - path to success in the evolving field of
00:38 - machine learning looking to step into
00:40 - machine learning or data science it's
00:42 - about starting somewhere practical yet
00:44 - powerful in this introductory course
00:47 - machine learning for beginners we are
00:49 - going to cover the basics of machine
00:51 - learning and we're going to put that
00:53 - into practice by implementing it in a
00:56 - real world case study I'm d founder of
01:00 - Lun Tech where we are making data
01:02 - science and AI more accessible for
01:05 - individuals and businesses if you're
01:07 - looking for machine learning deep
01:09 - learning data science or AI resources
01:12 - then check out the free resources
01:14 - section in
01:15 - lunch. or our YouTube channel where you
01:18 - can find more content and you can dive
01:20 - into machine learning and in AI we're
01:23 - going to start with machine learning
01:24 - road map we in this detailed section we
01:27 - are going to discuss the exact skill set
01:30 - that you need to get into machine
01:32 - learning we're also going to cover the
01:34 - definition of machine learning what is a
01:36 - common career path and lot of resources
01:39 - that you can use in order to get into
01:41 - machine learning then we are going to
01:43 - start with the actual Theory we are
01:45 - going to touch base the basics we're
01:47 - going to learn what are those different
01:50 - fundamentals in machine learning once we
01:52 - have learned the theory and we have also
01:54 - looked into the machine learning road
01:56 - map we're going to put our Theory into
01:58 - practice we are going to conduct an
02:01 - endtoend a basic yet powerful case study
02:05 - where we're are going to implement the
02:07 - linear aggression model we're going to
02:09 - use it both for caal analysis and for
02:12 - Predictive Analytics for Californian
02:14 - house prices we're are going to find out
02:17 - the features that drive the Californian
02:19 - house values and we are going to discuss
02:22 - the stepbystep approach for conducting a
02:25 - real world data science project at the
02:27 - end of this course you are going to to
02:30 - know the exact machine learning road map
02:32 - for
02:33 - 2024 what are the exact skill set and
02:36 - the action plan that you can use to get
02:38 - into machine learning and in data
02:41 - science you are going to learn the
02:43 - basics when it comes to machine learning
02:46 - you're going to implement it into actual
02:49 - machine learning project end to end
02:52 - including implementing pandas numai
02:56 - psychic learn touch models medal tap and
02:59 - curn in Python for a real world data
03:02 - science project dive into machine
03:04 - learning with us start Simple Start
03:06 - strong let's get
03:10 - started hi there in this video we are
03:13 - going to talk about how you can get into
03:15 - machine learning in
03:17 - 2024 first we are going to start with
03:19 - all the skills that you need in order to
03:21 - get into machine learning step by step
03:24 - what are the topics that you need to
03:25 - cover and what are the topics that you
03:27 - need to study in order to to get into
03:30 - machine learning we are going to talk
03:32 - about what is machine learning then we
03:34 - are going to cover step by step what are
03:37 - the exact topics and the skills that you
03:39 - need in order to become a machine
03:41 - learning researcher or just get into
03:43 - machine learning then we're going to
03:46 - cover the type of exact projects you can
03:48 - complete so examples of portfolio
03:50 - projects in order to put it on your
03:52 - resume and to start to apply for machine
03:55 - learning related jobs and then we are
03:58 - going to also talk about the type of
04:00 - industries that you can get into once
04:02 - you have all the skills and you want to
04:04 - get into machine learning so the exact
04:07 - career path and what kind of business
04:09 - titles are usually related to machine
04:11 - learning we are also going to talk about
04:14 - the average salary that you can expect
04:16 - for each of those different machine
04:18 - learning related positions at the end of
04:21 - this video you are going to know what
04:23 - exactly machine learning is where is it
04:26 - used what kind of skills are there that
04:28 - you need in order to get into to machine
04:30 - learning in 2024 and what kind of career
04:33 - path with what kind of compensation you
04:35 - can expect with the corresponding
04:37 - business titles when you want to start
04:39 - your career in machine learning so we
04:42 - will first start with the definition of
04:44 - machine learning what machine learning
04:46 - is and what are the different sorts of
04:48 - applications of machine learning that
04:50 - you most likely have heard of but you
04:52 - didn't know that it was based on machine
04:54 - learning so what is machine learning
04:56 - machine learning is a brand of
04:58 - artificial intelligence of AI that helps
05:01 - to uh build models based on the data and
05:05 - then learn from this data in order to
05:07 - make different decisions and it's being
05:09 - used across different Industries uh
05:12 - starting from healthare till
05:13 - entertainment in order to improve uh the
05:17 - customer uh experience custom identify
05:20 - customer behavior um improve the sales
05:23 - for the businesses uh and it also helps
05:26 - um governments to make decisions so it's
05:29 - really has a wide range of applications
05:32 - so let's start with the healthcare for
05:34 - instance machine learning is being used
05:36 - in the healthcare to help with the uh
05:38 - diagnosis of diseases it can help to uh
05:41 - diagnose cancer uh during the co it
05:44 - helped many hospitals to identify
05:47 - whether people are getting more uh
05:49 - severe side effects or they are getting
05:52 - p uh pneumonia um based on those
05:55 - pictures and that was all based on
05:57 - machine learning and specifically comp
05:59 - computer
06:00 - vision uh in the healthcare is also
06:03 - being used for drug Discovery it's being
06:05 - used for personalized medicine for
06:07 - personalizing treatment plans to improve
06:10 - the operations of the hospitals to
06:12 - understand what is the amount of uh
06:14 - people and uh patients that hospital can
06:17 - expect in each of those uh uh days per
06:20 - week and also to estimate the amount of
06:23 - doctors that need to be available the
06:25 - amount of uh people uh that the hospital
06:28 - can expect in the emergency room based
06:31 - on the day or the time of the day and
06:34 - this is basically not a machine learning
06:36 - application then we have uh machine
06:38 - learning in finance machine learning is
06:40 - being largely used in finance for
06:42 - different applications starting from
06:44 - fraud detection in credit cards or in
06:47 - other sorts of banking operations um
06:51 - it's also being used in trading uh with
06:53 - specifically in combination with
06:55 - quantitative Finance to help traders to
06:58 - make decisions with they need to go
07:00 - short or long into different stocks or
07:02 - bonds or different assets just in
07:04 - general to estimate the price that those
07:07 - talks will happen Assets in the real
07:10 - time in the most accurate way uh it's
07:13 - also being used in uh retail uh it helps
07:16 - you understand an estimated demand for
07:18 - certain products in certain warehouses
07:21 - it also helps you understand what is the
07:23 - most appropriate or closest uh uh
07:26 - warehouses that the items for that
07:28 - corresponding customer should be shipped
07:30 - so it's uh optimizing the operations
07:33 - it's also being used to build different
07:34 - direct Commander systems and search
07:36 - engines like the famous Amazon is doing
07:40 - so every time when you go to Amazon and
07:42 - you are searching for project or product
07:45 - you will most likely see many article
07:46 - recommenders and that's based on machine
07:49 - learning because Amazon is uh Gathering
07:52 - the data and comparing your behavior So
07:54 - based on what you have bought based on
07:57 - what you are searching uh to other
07:59 - customers and those items to other items
08:02 - in order to understand what are the
08:04 - items that you will most likely will be
08:06 - interested in and eventually will buy it
08:10 - and that's exactly based on machine
08:11 - learning and specifically different
08:13 - sorts of recommended system
08:15 - algorithm and then we have uh marketing
08:18 - where machine learning is being heavily
08:20 - used because this can help to understand
08:24 - uh what are these different tactics and
08:27 - specific targeting uh groups that that
08:29 - you belong and how retailers can Target
08:32 - you uh in order to reduce their
08:34 - marketing cost and to result in high
08:36 - conversion rates so to ensure that you
08:38 - buy their product then we have machine
08:41 - learning in autonomous vehicles that's
08:43 - based on machine learning and
08:45 - specifically uh deep learning
08:47 - applications uh and then we have also um
08:51 - uh natural language Pro processing which
08:54 - is highly related to the famous Chad GPT
08:57 - I'm sure you are using it and that's
08:59 - that's based on the machine learning and
09:01 - specifically the large language models
09:04 - so the Transformers large language
09:06 - models where you are going and providing
09:08 - your text and then question and the chat
09:11 - GPT will provide answer to you or in
09:13 - fact any other uh virtual assistant or
09:16 - chat boats those are all based on
09:19 - machine
09:20 - learning and then we have also uh smart
09:23 - home devices so Alexa is based on
09:26 - machine learning also in agriculture uh
09:29 - machine learning is being used heavily
09:31 - these days to estimate what the weather
09:33 - conditions will be uh to understand what
09:36 - will be the uh production of different
09:38 - plants uh what will be the um outcome of
09:42 - this uh to understand and to make
09:44 - decisions uh also how they can optimize
09:47 - those uh crop uh yields to monitor uh
09:51 - soil health and for different sorts of
09:53 - applications that can just in general uh
09:55 - improve the uh revenue for the farmer
10:00 - then we have of course in the
10:02 - entertainment so the Vivid example is
10:04 - Netflix that uses the uh data uh that
10:09 - you are providing uh related to the
10:11 - movies and also based on what kind of
10:13 - movies you are watching Netflix is uh
10:15 - building this super smart recommender
10:18 - system to recommend you movies that you
10:21 - most likely will be interested in and
10:23 - you will also like it so in all this
10:26 - machine learning is being used and it's
10:28 - actually super powerful topic and super
10:31 - powerful uh field to get into and in the
10:34 - upcoming 10 years this is only going to
10:37 - grow so if you have made that decision
10:40 - or you are about to make that decision
10:41 - to get into machine learning continue
10:43 - watching this video because I'm going to
10:45 - tell you exactly what kind of skills you
10:47 - need and what kind of uh practice
10:50 - projects you can complete in order to
10:52 - get into machine learning in
10:55 - 2024 so you first need to start with
10:57 - mathematics you Al also need to know
11:00 - python you also need to know statistics
11:03 - you will need to know machine learning
11:05 - and you will need to know some NLP to
11:08 - get into machine learning so let's now
11:11 - unpack each of those skill sets so
11:13 - independent the type of machine learning
11:15 - you are going to do you need to know
11:17 - mathematics and specifically you need to
11:20 - know linear algebra so you need to know
11:23 - what is matrix multiplication what are
11:25 - the vectors matrices dot product you
11:28 - need to know how you can uh multiply
11:30 - those different matrices Matrix with
11:33 - vectors what are these different rules
11:35 - the dimensions also what does it mean to
11:38 - transform a matrix the inverse of the
11:41 - Matrix identity Matrix diagonal matrix
11:45 - uh those are all Concepts as part of
11:47 - linear algebra that you need to know as
11:50 - part of your mathematical skill set in
11:52 - order to understand those different
11:54 - machine learning
11:55 - algorithms then as part of your
11:57 - mathematics you also need to to know
11:59 - calculus and specifically differential
12:02 - Theory so you need to know these
12:04 - different theorems such as chain rule
12:06 - the rule of uh differentiating when you
12:09 - have sum of instances when you have
12:12 - constant multiply with an instance when
12:14 - you have um uh sum but also subtraction
12:18 - division multiplication of two items and
12:21 - then you need to take the uh derivative
12:23 - of that what is this idea of derivative
12:25 - what is the idea of partial derivative
12:27 - what is the idea of Haitian so first
12:29 - order derivative second order derivative
12:32 - and it would be also great to know a
12:34 - basic integration Theory so we have
12:37 - differentiation and the opposite of it
12:39 - is integration Theory so this is kind of
12:42 - basic you don't need to know uh too much
12:45 - when it comes to calculus but those are
12:47 - basic things that you need to know uh in
12:49 - order to succeed in machine learning uh
12:53 - then the next Concepts uh such as
12:55 - discrete mathematics so you need to know
12:58 - uh what is this idea of uh graph Theory
13:02 - uh what are this uh combinations
13:05 - combinators uh what is uh this idea of
13:08 - complexity which is important when you
13:11 - want to become a machine learning
13:12 - engineer because you need to understand
13:14 - what is this Big O notation so you need
13:16 - to understand what is this complexity of
13:19 - uh n s complexity of n complexity of n
13:23 - log n um and about that you need to know
13:27 - uh some basic um mathematics when it
13:29 - comes which comes from usually high
13:31 - school so you need to know
13:33 - multiplication division you need to
13:35 - understand uh multiplying uh uh amounts
13:39 - which are within the parentheses you
13:41 - need to understand um different symbols
13:44 - that represent mathematical um values
13:48 - you need to know this idea of using X's
13:50 - y's uh and then what is X2 what is y^ 2
13:54 - What is X to ^ 3 so different exponents
13:57 - of the different VAR variables then you
14:00 - need to know what is logarithm what is
14:02 - logarithm at the base of two what is
14:03 - logarithm at the base of e and then at
14:06 - the base of 10 uh what is the idea of e
14:09 - so what is the idea of Pi uh what is
14:12 - this idea of uh exponent logarithm and
14:15 - how does those uh transform when it
14:18 - comes to taking derivative of the
14:20 - logarithm taking the derivative of the
14:23 - uh exponent those are all values and all
14:26 - uh topics that are actually quite basic
14:29 - they might sound complicated but they
14:31 - are actually not so if someone explains
14:34 - you uh clearly then you will definitely
14:36 - understand it from the first goal and uh
14:39 - for this uh to understand all those
14:42 - different mathematical Concepts so
14:44 - linear algebra calculus differential
14:46 - Theory and then discrete mathematics and
14:49 - those different symbols you need to uh
14:52 - go for instance uh and look for courses
14:55 - or um YouTube tutorials that are about
14:59 - uh basic mathematics uh for machine
15:03 - learning and AI uh don't go and look
15:06 - further you can check for instance Can
15:07 - Academy which is uh quite favorite when
15:10 - it comes to learning math uh both for
15:13 - uni students and also for just people
15:15 - who want to learn mathematics and this
15:17 - will be your guide um or you can check
15:20 - our resources at Lear tech. cuz we are
15:23 - going also to uh provide this resources
15:26 - for you uh in case you want to learn
15:29 - mathematics for your machine Learning
15:31 - Journey the next skill set that you need
15:33 - to gain in order to break into machine
15:35 - learning is the statistics so you need
15:38 - to know this is a must statistics if you
15:41 - want to get into machine learning and in
15:43 - AI in general so there are few topics
15:46 - that you must um study when it comes to
15:50 - statistics and uh those are descriptive
15:53 - statistics multivariate statistics
15:56 - inferential statistics probability
15:58 - distribution and some bial
16:01 - thinking so let's start with descriptive
16:03 - statistics when it comes to descriptive
16:05 - statistics you need to know what is side
16:07 - of mean uh median standard deviation
16:11 - variance and uh just in general how you
16:14 - can uh analyze the data with using this
16:18 - descriptive measures so distance
16:21 - measures but also variational measures
16:24 - then the next topic area that you need
16:26 - to know as part of your statistical
16:28 - Journey is the inferential statistics so
16:31 - you need to know those INF famous
16:32 - theories such as Central limit theorem
16:35 - the law of a large numbers uh and how
16:38 - you can um relate to this idea of
16:41 - population sample unbiased sample and
16:45 - also uh a hypothesis testing confidence
16:48 - interval statistical significance uh and
16:51 - uh how you can test different theories
16:54 - by using uh this idea of statistical
16:57 - significance uh what what is the power
16:59 - of the test what is type one error what
17:01 - is type two error so uh this is super
17:04 - important for understanding different
17:06 - SES of machine learning applications if
17:08 - you want to get into machine learning
17:11 - then you have probability distributions
17:13 - and this idea of probabilities so to
17:16 - understand those different machine
17:17 - learning Concepts you need to know what
17:20 - are probabilities so what is this idea
17:22 - of probability what is this idea of
17:24 - Sample versus
17:25 - population uh what is what does it mean
17:28 - to estimate probability what are those
17:31 - different rules of probability so
17:33 - conditional
17:34 - probability uh and um those uh
17:38 - probability uh values and rules that
17:41 - usually you can uh apply when you have
17:45 - uh probability of um multipliers
17:48 - probability of two sums um and then uh
17:53 - you need to understand some uh popular
17:55 - and you need to know some popular
17:56 - probability distribution function
17:59 - and those are perno distribution
18:01 - binomial distribution uh normal
18:04 - distribution uniform distribution
18:07 - exponential distribution so those are
18:09 - all super important distributions that
18:11 - you need to know in order to understand
18:14 - uh this idea of normality
18:17 - normalization uh also uh this idea of
18:20 - bare noly trials and uh relating uh
18:23 - different probability distributions to
18:25 - different uh uh higher level statistical
18:28 - concept steps so rolling a dice the
18:30 - probability of it how it is related to
18:32 - bero distribution or to binomial
18:34 - distribution and those are super
18:36 - important when it comes to hypothesis
18:38 - testing but also for uh many other
18:41 - machine learning
18:42 - applications so then we have the ban
18:46 - thinking this is super important when it
18:48 - comes to more advanced machine learning
18:50 - but also some basic machine learning you
18:52 - need to know what is the Bas theorem
18:54 - which arguably is one of the most
18:56 - popular statistical theorems out there
18:59 - comparable also to the central limit
19:00 - theorem you need to know what is
19:03 - conditional probability what is this
19:04 - bias theorem and how does it relate to
19:07 - conditional probability uh what is this
19:10 - uh bation uh statistics Ide at very high
19:14 - level you don't need to know everything
19:16 - in uh super detailed but you need to
19:19 - know um the these Concepts at least at
19:22 - high level in order to understand
19:24 - machine learning so to learn statistics
19:27 - and fundamental concepts of Statistics
19:30 - you can check out the fundamentals to
19:32 - statistics course at
19:34 - lunch. here you can learn all this
19:37 - required Concepts and topics and you can
19:39 - practice it in order to get into machine
19:42 - learning and to gain the statistical
19:44 - skills the next skill set that you must
19:47 - know is the fundamentals to machine
19:49 - learning so this covers not only the
19:51 - basics of machine learning but also the
19:54 - most popular machine learning algorithms
19:57 - so you need to know this uh different um
20:00 - mathematical side of these algorithms
20:02 - step by step how they work what are the
20:04 - benefits of them what are the demores
20:07 - and and which one to use for what type
20:09 - of applications so you need to know this
20:12 - uh categorization of supervised versus
20:15 - unsupervised versus
20:17 - semi-supervised then you need to know
20:19 - what is this idea of classification
20:21 - regression or uh clustering then you
20:25 - need to know uh also time series
20:27 - analysis uh you also need to know uh
20:30 - these different popular algorithms
20:33 - including linear
20:35 - regression also logistic regression LDA
20:38 - so linear discriminant analysis you need
20:41 - to know KNN you uh need to know uh
20:44 - decision treats both classification and
20:46 - regression case you need to know uh
20:48 - random Forest begging but also boosting
20:51 - so popular boosting algorithms like uh
20:53 - light GBM GBM uh so gradient boosting
20:57 - models and you need to know uh HG boost
21:01 - uh you uh also need to know um some
21:05 - supervised learning algorithm such as K
21:08 - means uh usually Ed for class string you
21:11 - need to know DB scan which becomes more
21:13 - and more popular in uh class string
21:15 - algorithms you also need to know
21:17 - hierarchal class string um and um for
21:21 - all this type of uh models you need to
21:24 - understand the idea behind them what are
21:26 - the advantages and disadvantages whether
21:28 - they can be applied for unsupervised
21:30 - versus supervised versus semi-supervised
21:32 - you need to know whether they are for
21:34 - regression classification or for uh
21:37 - class stre beside of this popular
21:39 - algorithms and models you also need to
21:41 - know the basics of uh training a machine
21:44 - learning model so you need to know uh
21:47 - this process behind training validating
21:50 - and testing your machine learning
21:52 - algorithms so you need to know uh what
21:55 - does it mean to uh perform
21:56 - hyperparameter tuning
21:58 - what are those different optimization
22:00 - algorithms that can be used to optimize
22:02 - your parameters such as uh GD SGD SGD
22:06 - with momentum Adam and Adam V you also
22:10 - need to know the testing process this
22:13 - idea of splitting the data into train
22:15 - validation and then test you need to
22:17 - know resampling techniques why are they
22:20 - used including the um bootstrapping and
22:23 - uh cross viation and there's different
22:25 - sorts of cross viation techniques such
22:27 - as one out cross validation kful cross
22:30 - validation validation set approach uh
22:33 - you also need to know um this uh idea of
22:37 - uh Matrix and how you can use different
22:39 - Matrix to evaluate your machine learning
22:41 - models such as uh classification type of
22:44 - metrics like F1 score
22:47 - FB Precision recall um cross entropy um
22:52 - and also you need to know some Matrix
22:54 - that can be used to evaluate regression
22:56 - type of problems like the uh me squared
22:59 - error so MC root me squared error R MC
23:03 - uh MAA so the absolute uh version of
23:07 - those different sorts of Errors um and
23:10 - um or the residual sum of squares for
23:13 - all these cases you not only need to
23:16 - know higher level what the those
23:18 - algorithms or those uh topics or
23:20 - concepts are doing but you actually need
23:22 - to know the uh mathematics behind it
23:25 - their benefits the uh disadvantage ages
23:28 - because during the interviews you can
23:30 - definitely expect questions that will
23:32 - test uh not only your high level
23:35 - understanding but also this uh
23:36 - background knowledge if you want to
23:39 - learn machine learning and you want to
23:40 - gain those skills then uh feel free to
23:43 - check out my uh fundamentals to machine
23:45 - learning course at
23:47 - lunch. or you can also check out and
23:49 - download for free the fundamentals to
23:52 - machine learning handbook that I
23:54 - published with free cord Camp then the
23:56 - next skill set that you definitely need
23:58 - to gain is a knowledge in python python
24:01 - is actually one of the most popular
24:04 - programming languages out there and it's
24:06 - being used across software Engineers uh
24:09 - AI Engineers machine learning Engineers
24:11 - data scientists so this this is the
24:14 - universal language I would say when it
24:16 - comes to programming so if you're
24:18 - considering getting into machine
24:20 - learning in 2024 then python will be
24:23 - your friend so knowing the theory is one
24:26 - thing then uh implementing it uh in in
24:29 - the actual job is another and that's
24:31 - exactly where python comes in handy so
24:34 - you need to know python in order to
24:35 - perform uh descriptive statistics in
24:38 - order to trade machine learning model or
24:41 - more advanced machine learning models so
24:43 - deep learning models you can use for
24:45 - training validation and uh for testing
24:49 - of your models and uh also for building
24:52 - different sorts of applications so
24:54 - python is super powerful therefore it's
24:56 - also gaining such a high uh popularity
24:59 - across the globe because it has so many
25:02 - uh libraries it has uh taner flow pie
25:06 - torch both that uh are must if you want
25:09 - to not only get into machine learning
25:12 - but also the advanced uh levels of
25:14 - machine learning so if you are
25:16 - considering the AI engineering jobs or
25:19 - machine learning engineering jobs and uh
25:22 - you want to train for instance deep
25:24 - learning models uh or you want to build
25:27 - large l W models or generative AI models
25:30 - then you definitely need to learn uh
25:32 - pytorch and tens flow which are
25:34 - Frameworks that I use in order to uh
25:37 - Implement different deep learning uh
25:40 - which are Advanced machine learning
25:42 - models here are few libraries that you
25:44 - need to know in order to uh get into
25:47 - machine learning so you definitely need
25:49 - to know pandas napai you need to know
25:52 - psyit learn scipi you also need to know
25:55 - uh nltk for the TX data you also need to
25:59 - know tensor flow and Pythor for bit more
26:02 - advanced machine learning and um beside
26:05 - this there are also data visualization
26:07 - libraries that I would definitely
26:09 - suggest you to practice with which are
26:11 - the Met plot lip and specifically the
26:14 - PIP plot and also the
26:16 - curn when it comes to python beside
26:18 - knowing how to use libraries you also
26:21 - need to know some basic data structures
26:23 - so you need to know what are these
26:24 - variables how you can create variables
26:27 - what what are the matrices arrays how
26:29 - the indexing works and also uh what are
26:32 - the lists what are the sets so unique
26:35 - lists uh What uh are the ways that you
26:38 - can what are the different operations
26:40 - you can perform uh how does the Sorting
26:43 - for instance work I would definitely
26:45 - suggest you know um some basic data
26:47 - structures and algorithms such as binary
26:50 - sort so in optimal way to sort your
26:52 - arrays you also need to know uh the data
26:56 - processing in Python so you need to
26:58 - understand how to identify missing data
27:00 - how to uh identify uh duplicating your
27:04 - data how to clean this how to perform
27:07 - feature engineering so how to combine uh
27:10 - multiple variables or to perform
27:11 - operations to create new
27:13 - variables um you also need to know uh
27:16 - how you can aggregate your data how you
27:18 - can filter your data how you can sort
27:20 - your data and of course you also need to
27:23 - know how you can form AB testing in your
27:26 - Python and how you can train machine
27:28 - learning models how you can test it and
27:31 - how you can evaluate them and also
27:33 - visualize the performance of it if you
27:36 - want to Learn Python then the easiest
27:38 - thing you can do is just to Google for
27:40 - uh python for data science or python for
27:43 - machine learning tutorials or blogs or
27:46 - you can even try out the python for data
27:49 - science course at Learner tech. in order
27:52 - to learn all these Basics and usage of
27:54 - these libraries and some practical
27:56 - examples when it comes to python for
27:58 - machine learning the next skill set that
28:01 - you need to gain in order to get into
28:02 - machine learning is the basic
28:05 - introduction to NLP natural language
28:08 - processing so you need to know how to
28:10 - work with text Data given that these
28:13 - days the text data is the Cornerstone of
28:15 - all these different Advanced algorithms
28:17 - such as uh gpts Transformers the
28:20 - attention mechanisms so those uh
28:23 - applications that you see as part of
28:24 - building chat boat or this uh p I uh
28:28 - applications based on Tex data they are
28:31 - all based on NLP so therefore you need
28:34 - to know this basics of NLP to just get
28:37 - started with machine learning so you
28:39 - need to know uh this idea of text Data
28:43 - what are those strings uh how you can
28:45 - clean Text data so how you can clean uh
28:48 - those um dirty data that you get and
28:52 - what are the steps involved such as
28:54 - lower casing uh removing punctuation
28:58 - tokenization uh also what is this idea
29:00 - of stemming lemmatization stop wordss
29:03 - how you can use the nltk in Pyon in
29:05 - order to perform this cleaning you also
29:08 - need to know uh this idea of embeddings
29:12 - and uh you can also learn this idea of
29:16 - uh the uh tfidf which is a basic uh NLP
29:21 - algorithm uh you also uh can learn this
29:24 - idea of word and Bings uh the sub word
29:27 - embeddings uh and the character
29:29 - embeddings if you want to learn the
29:31 - basics of NLP you can check out those
29:34 - Concepts and learn them as part of the
29:37 - blogs there are many tutorials on
29:39 - YouTube you can also try the
29:41 - introduction to uh NLP course at lunch.
29:46 - in order to learn this uh different
29:48 - Basics that form the NLP if you want to
29:51 - go beyond this uh intro till medium
29:54 - level machine learning and you also want
29:56 - to learn more advanced machine learning
29:59 - and this is something that you need to
30:00 - know after you have gained all these
30:02 - preview skills that I mentioned then you
30:05 - can gain uh this uh knowledge and the
30:07 - skill set by learning deep learning and
30:10 - also uh you can consider uh getting into
30:13 - generative AI topics so you can for
30:16 - instance learn what are the rnns what
30:19 - are the Ann what are the CNN you can
30:22 - learn what is this uh out encoder
30:24 - concept what are the variational outen
30:26 - coders what what are the uh generative
30:28 - adversarial networks so gens uh you can
30:32 - understand what is this idea of
30:33 - reconstruction error uh you can
30:36 - understand this um these different sorts
30:38 - of neural networks what is this idea of
30:40 - back propagation the optimization of
30:42 - these algorithms by using the different
30:44 - optimization algorithms such as GD HGD
30:47 - um HGD momentum Adam adamw RMS prop uh
30:52 - you uh can also go One Step Beyond and
30:55 - you can uh get into gener AI topics such
30:58 - as um uh the uh variational Auto
31:02 - encoders like I just mentioned but also
31:04 - the large language models so if you want
31:07 - to move towards the NLP side of
31:09 - generative Ai and you want to know how
31:11 - the ched GPT has been invented how the
31:14 - gpts work or the birth model uh then you
31:17 - will definitely need to uh get into this
31:20 - topic of language model so what are the
31:22 - end grams what is the attention
31:25 - mechanism what is the difference between
31:27 - the self attention and attention what is
31:30 - uh one head self attention mechanism
31:32 - what is multi-ad self attention
31:34 - mechanism you also need to know at high
31:36 - level this uh encoder decoder
31:39 - architecture of Transformers so you need
31:42 - to know the architecture of Transformers
31:44 - and how they solve different problems of
31:46 - uh reur neuron networks or RNN and
31:49 - lstms uh you can also look into uh this
31:53 - uh uh encoder based or decoder based
31:56 - algorithm such as
31:57 - uh gpts or Birch model and those all
32:02 - will help you to not only get into
32:04 - machine learning but also stand out from
32:07 - all the other candidates by having this
32:09 - Advanced knowledge let's now talk about
32:11 - different sorts of projects that you can
32:13 - complete in order to train your machine
32:16 - learning skill set that you just learned
32:18 - uh so there are few projects that I
32:20 - suggest you to complete and you can put
32:22 - it this on your resume to start to apply
32:24 - for machine learning roles the first
32:27 - application the project that I would
32:29 - suggest you to do is building a basic
32:31 - recommender system whether it's a job
32:34 - recommender system or a movie
32:37 - recommender system in this way you can
32:39 - showcase how you can use for instance
32:41 - text Data from those job advertisement
32:45 - how you can use numeric data such as the
32:47 - ratings of the movies in order to build
32:50 - a topend recommender system this will
32:54 - showcase your understanding of the
32:56 - distance measures such as cosign
32:57 - similarity this Cann algorithm idea and
33:01 - this will help you to uh uh tackle this
33:04 - specific uh area of data science and
33:08 - machine learning the next project I
33:10 - would suggest you to do will be to build
33:12 - a regression based model so in this way
33:15 - you will showcase that you understand
33:17 - this idea of regression how to work with
33:20 - a Predictive Analytics and predictive
33:22 - model that has a dependent variable
33:25 - response variable that is in the numeric
33:27 - format so here for instance you can uh
33:30 - estimate the salaries of the jobs based
33:33 - on the uh characteristics of the uh job
33:36 - based on this data which you can get for
33:38 - instance from uh open source uh web
33:41 - pages such as keegle and you can then uh
33:44 - use different sorts of regression
33:45 - algorithms to perform your predictions
33:48 - of the salaries evaluate the model and
33:51 - then compare the uh performance of the
33:53 - different machine learning regression
33:55 - based algorithms for instance you can
33:58 - use the uh linear regression you can use
34:01 - the decision trees regression version
34:03 - you can use the um uh random Forest you
34:06 - can use uh GBM xgo in order to Showcase
34:11 - and then in one uh graph to compare this
34:15 - uh performance of these different
34:16 - algorithms by using single regression uh
34:21 - ml modal metrics so for instance the
34:24 - rmsc this project will showcase that you
34:27 - understand how you can train a
34:28 - regression model how you can test it and
34:31 - validate it and it will showcase your
34:34 - understanding of optimization of this
34:35 - regression algorithm you understand this
34:38 - concept of hyperparameter unit the next
34:41 - project that I would suggest you to do
34:42 - in order to Showcase your classification
34:45 - knowledge so when it comes to uh
34:47 - predicting a class for an observation
34:50 - given uh the feature space would be uh
34:54 - to uh build a classification model that
34:56 - would classify emails being a Spam or
34:59 - not a Spam so you can use a publicly
35:02 - available data that will be uh
35:05 - describing a specific email and then you
35:07 - will have multiple emails and the idea
35:10 - is to uh build a machine learning model
35:12 - that would classify the email to the
35:15 - class zero and class one where class
35:18 - zero for instance can be your uh not
35:20 - being a Spam and one being a Spam so
35:24 - with this binary classification you will
35:25 - showcase that you know how to train a
35:28 - machine learning model for
35:30 - classification purposes and you can here
35:33 - use for instance logistic regression you
35:35 - can use also the decision Trea for
35:37 - classification case you can also use
35:40 - random Forest the uh EG she Bo for
35:43 - classification GBM for classification
35:46 - and uh with all these models you can
35:48 - then obtain the performance metrics such
35:50 - as uh F1 score or you can put the rck
35:53 - curve uh or the uh area under the Curve
35:56 - metrics and you can also compare those
35:59 - different classification models so in
36:01 - this way you will also tackle another
36:04 - area of expertise when it comes to the
36:06 - machine
36:07 - learning then a final project that I
36:10 - would suggest you to do would be uh from
36:12 - the unsupervised learning to Showcase
36:14 - another area of expertise and here you
36:17 - can for instance use data to your
36:20 - customers into good better and best
36:23 - customers based on their transaction
36:25 - history the amount of uh money that they
36:27 - are spending in the store so uh in this
36:31 - case you can for instance use K means uh
36:34 - DB scan hierarchy clustering and then
36:37 - you can evaluate your uh clustering
36:39 - algorithms and then select the one that
36:41 - performs the best so you will then in
36:44 - this case cover yet another area of
36:47 - machine learning which would be super
36:49 - important to show case that you can not
36:51 - only handle recommended systems or
36:54 - supervised learning but also
36:55 - unsupervised learning and the reason why
36:58 - I suggest you to uh cover all these
37:00 - different areas and complete this four
37:02 - different projects is because in this
37:04 - way you will be covering different
37:07 - expertise and areas of machine learning
37:10 - so you will be also putting projects on
37:12 - your uh resume that are covering
37:15 - different sorts of algorithms different
37:17 - sorts of uh Matrix and approaches and it
37:20 - will show case that you actually know a
37:22 - lot from machine
37:24 - learning now if you want to go beyond
37:27 - the basic or medium level and you want
37:29 - to be considered for medium or Advanced
37:33 - machine learning uh levels and positions
37:36 - you also need to know bit more advanced
37:38 - which means that you need to complete
37:40 - bit more advanced projects for instance
37:43 - if you want to apply for generative AI
37:45 - related or large language models related
37:48 - positions I would suggest you to
37:50 - complete a project where you are
37:51 - building a very basic uh large language
37:54 - model and specifically the pre-training
37:57 - process which is the most difficult one
38:00 - so in this case uh for instance you can
38:03 - build a baby GPT and I'll put a here
38:05 - link that you can follow where I'm
38:08 - building a baby GPT a basic pre-trained
38:11 - GPT algorithm where uh I am using a text
38:16 - Data uh publicly available data in order
38:19 - to uh uh process data in the same way
38:23 - like GPT is doing and the encoded part
38:25 - of the Transformer
38:27 - in this way you will showcase to your um
38:31 - hiring managers that you understand this
38:33 - architecture behind Transformers
38:35 - architecture behind the um uh large
38:38 - language models and the gpts and you
38:41 - understand how you can use pytorch in
38:44 - Python in order to do this Advanced NLP
38:48 - and generative AI task and finally let's
38:51 - now talk about the common career path
38:54 - and the business titles that you can
38:56 - expect from a career in machine learning
38:59 - so assuming that you have gained all the
39:01 - skills uh that are must for breaking
39:03 - into machine learning there are
39:05 - different sorts of business titles that
39:07 - you can apply in order to get into
39:09 - machine learning so when it comes to
39:11 - machine learning uh you can uh get into
39:14 - machine learning uh and there are
39:16 - different fields that are covered as
39:17 - part of this so uh first we have the
39:20 - general machine learning researcher
39:23 - machine learning researcher is basically
39:25 - doing a research so training testing
39:27 - evaluating different machine learning
39:29 - algorithms they are usually people who
39:31 - come from academic background but it
39:33 - doesn't mean that you cannot get into
39:35 - machine learning research without
39:37 - getting a degree in statistics
39:39 - mathematics or in um um machine learning
39:43 - specifically not at all so uh if you
39:46 - have this um desire and this passion for
39:49 - reading doing research uh and you don't
39:52 - mind reading uh research papers then
39:55 - machine learning res researcher job
39:57 - would be a good fit for you so machine
40:00 - learning combined with research then
40:02 - sets you uh for the machine learning
40:04 - researcher role then we have the machine
40:07 - learning engineer so machine learning
40:10 - engineer is the engineering version of
40:12 - the machine learning uh expertise which
40:15 - means that we are combining machine
40:17 - learning skills with the engineering
40:19 - skills such as
40:21 - productionizing pipelines or end to end
40:24 - robust pipeline scalability of the m
40:26 - model considering all these different
40:27 - aspects of the model not only from the
40:30 - performance side when it comes to the
40:32 - quality of the algorithm but also the uh
40:35 - scalability of it and when putting it in
40:38 - front of many users so when it comes to
40:41 - combining engineering with machine
40:44 - learning then you get machine learning
40:46 - engineering so if you are someone who is
40:49 - a software engineer and you want to get
40:50 - into machine learning then machine
40:52 - learning engineering would be the best
40:54 - fit for you so so for machine learning
40:57 - engineering you not only need to have
40:59 - all these different skills that I
41:00 - already mentioned but you also need to
41:02 - have this good grasp of uh uh
41:05 - scalability of algorithms the uh uh data
41:08 - structures and algorithms type of um
41:11 - skill set uh the uh complexity of the
41:15 - moral uh also system design so this one
41:19 - uh converges more towards and similar to
41:22 - the software engineering position
41:24 - combined with machine learning red than
41:26 - your pure machine learning or AI role
41:29 - then we have the AI research versus AI
41:32 - engineering position so uh the uh AI
41:35 - research position is similar to The
41:37 - Machine learning uh research position
41:39 - and the AI engineer position is similar
41:42 - to The Machine learning engineer
41:43 - position with only single difference
41:45 - when it comes to machine learning we are
41:47 - specifically talking about the
41:49 - traditional machine learning so linear
41:50 - regression logistic regression and also
41:53 - uh random Forest exy boost begging and
41:56 - when it comes to AI research and AI
41:59 - engineer position here we are tackling
42:01 - more the advanced machine learning so
42:03 - here we are talking about deep learning
42:05 - models such as RNN lstms grus CNN or
42:09 - computer vision applications and we are
42:12 - also talking about uh generative AI
42:15 - models large language models so uh we
42:17 - are talking about um the Transformers
42:20 - implementation of Transformers the gbts
42:23 - T5 all these different algorithms that
42:25 - are from uh more advanced uh AI topics
42:29 - rather than traditional machine learning
42:32 - uh for those you will then be applying
42:34 - for AI research and AI engineering
42:37 - positions and finally you have these
42:39 - different sorts of obervations niches
42:41 - from AI for instance NLP research NLP
42:45 - engineer or even data science positions
42:49 - for which you will need to know machine
42:50 - learning and knowing machine learning
42:53 - will set you apart for the source of
42:55 - positions so also the business titles
42:58 - such as data science or technical data
43:00 - science positions NLP researcher NLP
43:03 - engineer for this all uh you will need
43:06 - to know machine learning and knowing
43:07 - machine learning will help you to break
43:09 - into those positions and those career
43:11 - paths if you want to prepare for your
43:14 - deep learning interviews for instance
43:15 - and you want to get into AI engineering
43:18 - or AI research then I have recently
43:21 - published for free a full course with
43:24 - 100 interview questions with answers for
43:27 - a span of 7.5 hours that will help you
43:30 - to prepare for your deep learning
43:32 - interviews and for your machine learning
43:34 - interviews you can check out my uh
43:36 - fundamentals to machine learning course
43:39 - at lunch. or uh you can download the
43:42 - machine learning fundamentals handbook
43:45 - from free Cod camp and check out my
43:47 - blogs and also free resources at lunch.
43:50 - AI in order to prepare for your
43:52 - interviews and in order to get into
43:54 - machine learning let's not talk talk
43:56 - about the list of resources that you can
43:58 - use in order to get into machine
44:00 - learning in
44:02 - 2024 so to learn statistics and the
44:05 - fundamental concepts of Statistics you
44:07 - can check out the fundamental statistics
44:09 - course at
44:11 - lunch. here you can learn all this
44:14 - required Concepts and topics and you can
44:16 - practice it in order to get into machine
44:18 - learning and to gain this statistical
44:21 - skills then when you want to learn
44:23 - machine learning you can check the
44:24 - fundamentals to a learning course at
44:27 - lunch. to get all these basic concepts
44:30 - the fundamentals to machine learning and
44:33 - the list of comprehensive and the most
44:35 - comprehensive list of machine learning
44:37 - algorithms out there as part of this
44:40 - course then you can also check out the
44:42 - introduction to NLP course at the lunch.
44:45 - a in order to learn the basic concepts
44:48 - behind natural language
44:50 - preprocessing and finally if you want to
44:53 - Learn Python and specifically python for
44:56 - Ral learning you can check out the
44:58 - python for data science course at
45:00 - lunch. and if you want to get access to
45:03 - this different projects that you can
45:05 - practice your machine learning skills
45:06 - that you just learned you can either
45:08 - check out the ultimate data science boot
45:10 - camp that covers a specific course the
45:13 - uh data science uh project portfolio
45:16 - course covering multiple of these
45:18 - projects that you can train your machine
45:20 - learning skills and put on your resume
45:22 - or you can also check my GitHub account
45:25 - or my LinkedIn account where I cover
45:28 - many case studies including the baby GPT
45:32 - and I will also put the link to this
45:34 - course and to this uh case study in the
45:38 - link below and once you have gained all
45:40 - the skills you are ready to get into
45:43 - machine learning in
45:45 - 2024 in this lecture we will go through
45:47 - the basic concepts in machine learning
45:49 - that is needed to understand and follow
45:52 - conversations and solve main problems
45:54 - using machine learning strong
45:56 - understanding of machine learning Basics
45:58 - is an important step for anyone looking
46:00 - to learn more about or work with machine
46:03 - learning we'll be looking at the three
46:05 - concepts in this tutorial we will Define
46:08 - and look into the difference between
46:10 - supervised and unsupervised machine
46:12 - learning models then we will look into
46:14 - the difference between the regression
46:16 - and classification type of machine
46:17 - learning models after this we will look
46:20 - into the process of training machine
46:22 - learning models from scratch and how to
46:24 - evaluate them by introducing performance
46:27 - metrics what you can use depending on
46:29 - the type of machine learning model or
46:31 - problem you are dealing with so whether
46:33 - it's a supervised or unsupervised
46:35 - whether it's regression versus
46:37 - classification type of
46:41 - problem machine learning methods are
46:43 - categorized into two types depending on
46:45 - the existence of the label data in the
46:48 - training data set which is especially
46:50 - important in the training process so we
46:52 - are talking about the So-Cal dependent
46:54 - variable that we so in the section of
46:57 - fundamental Su statistics supervised and
47:00 - unsupervised machine learning models are
47:02 - two main type of machine learning
47:03 - algorithms one key difference between
47:06 - the two is the level of supervision
47:07 - during the training phase supervised
47:10 - machine learning algorithms are Guided
47:11 - by the labeled examples while as
47:14 - supervised algorithms are not as
47:16 - learning model is more reliable but it
47:18 - also requires a larger amount of labeled
47:20 - data which can be timec consuming and
47:23 - quite expensive to
47:24 - obtain
47:27 - examples of supervised machine learning
47:28 - models include regression and
47:30 - classification type of
47:33 - models on the other hand unsupervised
47:36 - machine learning algorithms are trained
47:38 - on unlabeled data the model must find
47:41 - patterns and relationships in the data
47:43 - without the guidance of correct outputs
47:45 - so we no longer have a dependent
47:47 - variable so unsupervised ml models
47:50 - require training data that consists only
47:53 - of independent variables or the features
47:55 - and there is no dependent variable or
47:58 - label data that can supervise the
48:00 - algorithm when learning from the
48:03 - data examples of unsupervised models are
48:06 - clust string models and outlier
48:08 - detection
48:10 - techniques supervised machine learning
48:12 - methods are categorized into two types
48:14 - depending on the type of dependent
48:16 - variable they are predicting so we have
48:18 - regression type and we have
48:19 - classification type some key differences
48:23 - between regression and classification
48:24 - include output type so the regression
48:27 - algorithms predict continuous values
48:30 - while the classification algorithms
48:31 - predict categorized values some key
48:34 - difference between regression and
48:36 - classification include the output type
48:38 - the evaluation metrics and their
48:41 - applications so with regards to the
48:44 - output type regression algorithms
48:46 - predict continuous values while
48:48 - classification algorithms predict
48:50 - categorical
48:51 - values with regard to the evaluation
48:54 - metric different evaluation metrics are
48:56 - being used for regression and
48:58 - classification tasks for example mean
49:01 - square is commonly used to evaluate
49:03 - regression models while accuracy is
49:05 - commonly used to evaluate classification
49:07 - models when it comes to Applications
49:09 - regression and classification models are
49:11 - used in entirely different types of
49:13 - applications regression models are often
49:15 - used for prediction tests while
49:17 - classifications are used for decision
49:19 - making tasks progression algorithms are
49:22 - used to predict the continuous value
49:24 - such as price or probability for example
49:28 - a regression model might be used to
49:29 - predict the price of a house based on
49:31 - its size location or other
49:38 - features examples of regression type of
49:40 - machine learning models are linear
49:42 - regression fixed effect regression exus
49:46 - regression
49:47 - Etc classification algorithms on the
49:50 - other hand are used to predict the
49:51 - categorical value these algorithms take
49:54 - an input and classify it to one of the
49:57 - several predetermined categories for
50:00 - example a classification model might be
50:02 - used to classify emails as a Spam or as
50:05 - not a Spam or to identify the type of
50:08 - animal in an
50:11 - image examples of classification type of
50:13 - machine learning models are logistic
50:16 - regression exus classification random
50:19 - Forest
50:23 - classification let us now look into
50:26 - different typee of performance metrics
50:27 - we can use in order to evaluate
50:29 - different type of machine learning
50:31 - models for aggression models common
50:33 - evaluation Matrix includes residual sum
50:35 - of squared which is the RSS mean squared
50:38 - error which is the msse the root mean
50:40 - squared error or rmsc and the mean
50:43 - absolute error which is the m AE this
50:46 - metrix measure the difference between
50:47 - the predicted values and the True Values
50:49 - with a lower value indicating a better
50:51 - feed for the model so let's go through
50:54 - this metrics one by one the first one is
50:56 - the RSS or the residual sum of squares
50:59 - this is a matrix commonly used in the
51:01 - setting of linear regression when we are
51:03 - evaluating the performance of the model
51:05 - in estimating the different coefficients
51:08 - and here the beta is a coefficient and
51:10 - the Yi is our dependent variable value
51:12 - and the Y head is the predicted value as
51:15 - you can see the RSS or the residual sum
51:17 - of square or the beta is equal to sum of
51:20 - all the squ of Y IUS y hat across all I
51:24 - is equal to 1 n where I is the index of
51:27 - the each r or the individual or the
51:30 - observation included in the data the
51:33 - second Matrix is the m or the mean
51:35 - squared error which is the average of
51:36 - the squared differences between the
51:38 - predicted values and the True Values so
51:41 - as you can see m is equal to 1 / to n
51:44 - and then sum across all i y i minus y
51:47 - head squ as you can see the RSS and the
51:50 - msse are quite similar in terms of their
51:53 - uh formulas the only difference is that
51:55 - we are adding a 1 / to n and then this
51:57 - makes it the average across all the
51:59 - square differences between the predicted
52:01 - value and the actual true valum a lower
52:04 - value of msse indicates a better fit the
52:08 - rmsc which is the root mean squared
52:09 - error is the square root of the msse so
52:12 - as you can see it has the same formula
52:14 - as msse only with the difference that we
52:16 - are adding a square roof on the top of
52:18 - that formula a lower value of rmsc
52:21 - indicates a better fit and finally the
52:25 - Mae or the mean absolute error is the
52:27 - average absolute difference between the
52:29 - predicted values so the Y hat and the
52:32 - True Values or y i a lower value of this
52:35 - indicates a better
52:37 - fit the choice of a regression metrics
52:39 - depends on the specific problem you are
52:41 - trying to solve and the nature of your
52:43 - data for instance the MSE is commonly
52:46 - used when you want to penalize large
52:48 - errors more than the small ones MSE is
52:52 - sensitive to outliers which means that
52:54 - it may not be the best choice when your
52:56 - data contains many outliers or extreme
53:00 - values rmsc on the other hand which is
53:03 - the square root of the MSC makes it
53:06 - easier to interpret so it's easier
53:08 - interpretable because it's in the same
53:10 - units as Target variable it is commonly
53:13 - used when you want to compare the
53:14 - performance of different models or when
53:17 - you want to report the error in a way
53:18 - that it's easier to understand and to
53:22 - explain the Mia is commonly used when
53:25 - you want to penalize all errors equally
53:28 - regardless of their magnitude and Mia is
53:31 - less sensitive to outliers compared to
53:36 - msse for classification models common
53:39 - evaluation metrics include accuracy
53:41 - precision recall and F1 score this
53:45 - metrics measure the ability of the
53:46 - machine learning model to correctly
53:48 - classify instances into the correct
53:51 - categories let's briefly look into this
53:53 - metrix individually so the accuracy is a
53:56 - proportion of correct predictions made
53:58 - by the model it's calculated by taking
54:01 - the correct predictions so the correct
54:02 - number of predictions and divide two all
54:04 - number of predictions which means
54:06 - correct predictions plus incorrect
54:08 - predictions next we will look into the
54:11 - Precision so Precision is the proportion
54:13 - of true positive predictions among all
54:16 - positive predictions made by the model
54:18 - and it's equal to True positive divided
54:20 - to True positive plus false positive so
54:23 - all number of positives true positives
54:26 - are cases where the model correctly
54:28 - predict a positive outcome while false
54:30 - positives are the cases where the model
54:32 - incorrectly predict a positive outcome
54:35 - next Matrix is recall recall is a
54:37 - proportion of true positive predictions
54:39 - among all actual positive instances it's
54:42 - calculated as the number of true
54:44 - positive predictions divided by the
54:46 - total number of actual positive
54:47 - instances which means dividing the true
54:50 - positive to True positive plus false
54:53 - negative so for example let's say we are
54:56 - looking into medical test a true
54:58 - positive would be a case where it has
55:00 - correctly identifies a patient as having
55:02 - a disease while a false positive would
55:05 - be a case where the test incorrectly
55:07 - identifies a healthy patient as having
55:09 - the
55:11 - disease and the final score is the F1
55:14 - score the F1 score is the harmonic mean
55:16 - or the usual mean of the Precision and
55:18 - recall with a higher value indicating a
55:21 - better balance between precision and
55:22 - recall and it's calculated as the
55:25 - two times recall times Precision divided
55:28 - to recall plus
55:34 - Precision for unsupervised models such
55:36 - as class string models whose performance
55:38 - is typically evaluated using metrics
55:40 - that measure the similarity of the data
55:42 - points within a cluster and the dis
55:44 - similarity of the data points between
55:46 - different clusters we have three type of
55:49 - metrics that we can use homogeneity is a
55:52 - measure of the degree to which all of
55:53 - the data points within a single cluster
55:55 - belong to the same class A Higher value
55:58 - indicates a more homogeneous cluster so
56:01 - as you can see homogeneity of age where
56:03 - age is the simply the short way of
56:05 - describing homogeneity is equal to one
56:08 - minus conditional entropy given cluster
56:11 - assignments divided to the entropy or
56:13 - predicted class if you wondering what
56:16 - this entropy is then stay tuned as we
56:18 - are going to discuss this entropy
56:20 - whenever we will discuss the clustering
56:22 - as well as decision trees X Matrix is
56:25 - the silid score silid score is a measure
56:28 - of the similarity of the data point to
56:29 - its own cluster compared to the other
56:32 - clusters a higher silid score indicates
56:34 - that the data point is well matched to
56:36 - its own cluster this is usually used for
56:39 - DB scan or k me so here the silhouette
56:42 - score can be represented by this formula
56:44 - so the S so or the silhouette score is
56:47 - equal to B minus AO divided to the
56:50 - maximum of AO and B where s o is The
56:53 - Silo coefficient of the data point
56:55 - characterized by o AO is the average
56:59 - distance between o and all the other
57:01 - data points in the cluster to which o
57:03 - belongs and the B is the minimum average
57:06 - distance from o to all the Clusters to
57:08 - which o does not
57:11 - belong the final metrix we look to is
57:13 - the completeness completeness is another
57:16 - measure of the degree to which all of
57:17 - the data points that belongs to a
57:19 - particular class are assigned to the
57:21 - same cluster a higher value indicates a
57:24 - more compete
57:26 - cluster let's conclude this lecture by
57:29 - going through the step-by-step process
57:31 - of evaluating a machine learning model
57:33 - at a very simplified version since there
57:36 - are many additional considerations and
57:38 - techniques that may be needed depending
57:41 - on a specific task and the
57:42 - characteristics of the data knowing how
57:45 - to properly train machine learning model
57:47 - is really important since this defines
57:49 - the accuracy of the results and
57:51 - conclusions you will
57:53 - make the training Pro process starts
57:55 - with the preparing of the data this
57:57 - includes splitting the data into
57:59 - training and test sets or if you are
58:01 - using more advanced resampling
58:03 - techniques that we will talk about later
58:05 - than splitting your data into multiple
58:07 - sets the training set of your data is
58:10 - used to feed the model if you have also
58:13 - a validation set then this validation
58:15 - set is used to optimize your
58:17 - hyperparameters and to pick the best
58:19 - model while the test set is to use to
58:22 - evaluate the model
58:23 - performance when when we will approach
58:25 - more lectures in this section we will
58:28 - talk in detail about these different
58:30 - techniques as well as what the training
58:32 - means what the test means what
58:33 - validation means as well as what the
58:36 - hyperparameter tuning
58:39 - means secondly we need to choose an
58:41 - algorithm or set of algorithms and train
58:44 - the model on the training data and save
58:47 - the fitted model there are many
58:49 - different algorithms to choose from and
58:51 - the appropriate algorithm will depend on
58:53 - the specific test task and the
58:55 - characteristics of the data as a third
58:58 - step we need to adjust the model
58:59 - parameters to minimize the error on the
59:02 - training set by performing
59:03 - hyperparameter tuning for this we need
59:06 - to use validation data and then we can
59:09 - select the best model that results in
59:11 - the least possible validation error rate
59:14 - in this step we want to look for the
59:16 - optimal set of parameters that are
59:18 - included as part of our model to end up
59:20 - with a model that has the least possible
59:22 - error so it performs in the best
59:24 - possible way in the final two steps we
59:27 - need to evaluate the model we are always
59:29 - interested in a test a rate and not the
59:32 - training or the validation error rates
59:34 - because we have not used a test set but
59:36 - we have used the training and validation
59:38 - sets so this test error rate will give
59:40 - you an idea of how well the model will
59:43 - generalize to the new unseen data we
59:46 - need to use the optimal set of
59:48 - parameters from hyperparameter tuning
59:50 - stage and the training data to train the
59:53 - model again with this hyper parameters
59:55 - and with the best
59:56 - model so we can use the best fitted
59:59 - model to get the predictions on the test
60:01 - data and this will help us to calculate
60:03 - our test error
60:05 - rate once we have calculated the test
60:08 - error rate and we have also obtained our
60:10 - best model we are ready to save the
60:12 - predictions so once we are satisfied
60:14 - with the model performance and we have
60:16 - tuned the parameters we can use it to
60:18 - make predictions on a new unseen data on
60:20 - the test data and compute the
60:22 - performance metrics for the model us the
60:24 - predictions and the real values of the
60:26 - target variable from the test data and
60:29 - this complete this lecture so in this
60:31 - lecture we have spoken about the basics
60:33 - of machine learning we have discussed
60:35 - the difference between the the
60:36 - unsupervised and supervised learning
60:38 - models as well as regression versus
60:41 - classification we have discussed in
60:43 - details the different type of
60:44 - performance metrics we can use to
60:46 - evaluate different type of machine
60:48 - learning models as well as we have
60:50 - looked into the simplified version of
60:52 - the step-by-step process to train the
60:54 - machine machine learning model in this
60:56 - lecture lecture number two we will
60:58 - discuss a very important Concepts which
61:00 - you need to know before considering and
61:02 - applying any statistical or machine
61:04 - learning model here I'm talking about
61:06 - the bias of the model and the variance
61:08 - of the model and the trade of between
61:09 - the two which we call bias various trade
61:12 - of whenever you are using a statistical
61:15 - econometrical or a machine learning
61:17 - model no matter how simple the model is
61:20 - you should always evaluate your model
61:22 - and check its error rate in all this
61:24 - cases it comes down to the trade-off you
61:26 - make between the variance of the model
61:28 - and the bias of your model because there
61:30 - is always a catch when it comes to the
61:32 - model choice and the
61:38 - performance let us firstly Define what
61:40 - bias and the variant of the machine
61:42 - learning model are the inability of the
61:45 - model to capture the true relationship
61:47 - in the data is called bias hence the
61:50 - machine learning models that are able to
61:52 - detect the true relationship in the data
61:54 - have low
61:56 - bias usually complex models or more
61:59 - flexible models tend to have a lower
62:01 - bias than simpler models so
62:03 - mathematically the bias of the model can
62:05 - be expressed as the expectation of the
62:07 - difference between the
62:09 - estimate and the True
62:11 - Value let us also Define the variance of
62:13 - the model the variance of the model is
62:15 - the inconsistency level or the
62:17 - variability of the model performance
62:20 - when applying the model to different
62:21 - data sets when the same model that is
62:24 - trained using training data performs
62:26 - entirely differently than on the test
62:28 - data this means that there is a large
62:30 - variation or variance in the model
62:33 - complex models or more flexible models
62:36 - tend to have a higher variance than
62:37 - simpler
62:40 - models in order to evaluate the
62:42 - performance of the model we need to look
62:43 - at the amount of error that the model is
62:45 - making for Simplicity let's assume we
62:48 - have the following simple regression
62:49 - model which aims to use a single
62:51 - independent variable X to model the
62:53 - numeric y dependent
62:55 - variable that is we fit our model on our
62:58 - training observations where we have a
63:00 - pair of independent and dependent
63:01 - variables X1 y1 X2 Y2 up to xn YN and we
63:07 - obtain an estimate for our training
63:09 - observations
63:11 - fhe we can then compute this let's say
63:14 - fhe X1 fhe X2 up to fhe xn which are the
63:17 - estimat for our dependent variable y1 Y2
63:20 - up to YN and if these are approximately
63:23 - equal to this actual values so one head
63:26 - is approximately equal to y1 Y2 head is
63:29 - approximately equal to Y2 head Etc then
63:32 - the training error rate would be small
63:35 - however if we are really interested in
63:37 - whether our model is predicting the
63:39 - dependent variable appropriately we want
63:42 - to instead of looking at the training
63:44 - error rate we want to look at our test
63:46 - error rate so so the error rate of the
63:49 - model is the expected Square difference
63:51 - between the real test values and their
63:53 - prediction
63:55 - where the predictions are made using the
63:56 - machine learning model we can rewrite
63:58 - this aor rate as a sum of two quantities
64:01 - where as you can see the left part is
64:03 - the amount of FX minus F hat x^ squared
64:08 - and the second entity is the variance of
64:09 - the error term so the accuracy of Y head
64:13 - as a prediction for y depends on the two
64:15 - quantities which we can call the
64:18 - reducible error and the irreducible
64:20 - error so this is the reducible error
64:22 - equal to FX minus f x s and then we have
64:26 - our irreducible error or the variance of
64:28 - Epsilon so the accuracy of Y head as a
64:31 - prediction for y depends on the two
64:33 - quantities which we can call the
64:35 - reducible error and the irreducible
64:37 - error in general The Fad will not be a
64:39 - perfect estimate for f and this
64:42 - inaccuracy will introduce some errors
64:44 - this error is reducible since we can
64:47 - potentially improve the accuracy of fad
64:49 - by using the most appropriate machine
64:51 - learning model and the best version of
64:53 - it to estimate the F however even if it
64:57 - was possible to find a model that would
64:59 - estimate F perfectly so that the
65:02 - estimated response took the form of Y
65:04 - head is equal to FX our prediction would
65:07 - still have some error in it this happens
65:10 - because Y is also a function of the
65:11 - error rate Epsilon which by definition
65:14 - cannot be predicted by using our feature
65:17 - X so there will always be some error
65:19 - that is not predictable so variability
65:23 - associated with the error Epsilon also
65:25 - affects the accuracy of the predictions
65:27 - and this is known as the irreducible
65:30 - error because no matter how well we will
65:32 - estimate F we cannot reduce the error
65:35 - introduced by the Epsilon this error
65:37 - contains all the features that are not
65:38 - included in our model so all the unknown
65:41 - factors that have an influence on our
65:43 - dependent variable but are not included
65:45 - as part of our
65:46 - data but we can't reduce the reducible
65:49 - error rate which is based on two values
65:52 - the variance of the estimate and the
65:54 - bias of the model if we were to simplify
65:57 - the mathematical expression describing
65:58 - the error rate that we got then it's
66:01 - equal to the variance of our model plus
66:03 - squared bias of our model plus the
66:06 - irreducible error so even if we cannot
66:09 - reduce the irreducible error we can
66:11 - reduce the reducible error rate which is
66:14 - based on the two values the variance and
66:16 - the squared bias so though the
66:18 - mathematical derivation is out of the
66:20 - scope of this course just keep in mind
66:22 - that the reducible error of the model
66:24 - can be described as the sum of the
66:26 - variance of the model and a squared bias
66:28 - of the
66:30 - model so mathematically the error in the
66:33 - supervised machine learning model is
66:35 - equal to the squared bias in the model
66:37 - the variance of the model and the
66:38 - irreducible error therefore in order to
66:41 - minimize the expected test error rate so
66:44 - on the Unseen data we need to select the
66:46 - machine learning meod that
66:48 - simultaneously achieves low variance and
66:50 - low
66:51 - bias and that's exactly what we call
66:54 - called bias variance tradeoff the
66:55 - problem is is that there is a negative
66:57 - correlation between the variance and the
66:59 - bias of the model another thing that is
67:02 - highly related to the bias and the
67:03 - variance of the model is the flexibility
67:05 - of the machine learning model so
67:07 - flexibility of the machine learning
67:08 - model has a direct impact on its
67:10 - variance and on its
67:13 - bias let's look at this relationships
67:15 - one by one so complex models or more
67:18 - flexible models tend to have a lower
67:20 - bias but at the same time complex models
67:23 - or flexible models tend to have higher
67:25 - variance than simpler models so as the
67:28 - flexibility of the model increases the
67:31 - model finds the true patterns in the
67:32 - data easier which reduces the bias of
67:35 - the model at the same time the variance
67:37 - of such models increases so as the
67:40 - flexibility of the model decreases model
67:43 - finds it more difficult to find the true
67:44 - parents in the data which then increases
67:47 - the bias of the morel but also decreases
67:49 - the variance of the model keep this
67:51 - topic in mind and we will continue this
67:53 - topic in the next next lecture when we
67:55 - will be discussing the topic of
67:56 - overfitting and how to solve the
67:58 - overfitting problem by using
68:00 - regularization in this lecture lecture
68:02 - number three we will talk about very
68:04 - important concept called overfitting and
68:07 - how we can solve overfitting by using
68:09 - different techniques including
68:11 - regularization this topic is related to
68:14 - the previous lecture and to the topics
68:16 - of error of the model train error rate
68:19 - test error rate bias and a variance of
68:23 - the machine learning model overfitting
68:25 - is important to know and also how to
68:27 - solve it with
68:28 - regularization because this topic can
68:31 - lead to inaccurate predictions and the
68:33 - lack of generalization of the model to
68:36 - new
68:37 - data knowing how to detect and prevent
68:40 - overfitting is crucial in building
68:42 - effective machine learning models
68:45 - questions about this topic are almost
68:47 - guaranteed to appear during every single
68:50 - data science
68:51 - interview in the previous lecture we
68:53 - discuss the relationship between model
68:55 - flexibility and the variance as well as
68:58 - the bias of the model we saw that as the
69:01 - flexibility of the model increases model
69:04 - finds the true pattern in the data
69:06 - easier which reduces the bias of the
69:08 - model but at the same time the variance
69:10 - of such models
69:12 - increases so as the flexibility of the
69:14 - model decreases model finds it more
69:17 - difficult to find the true patterns in
69:18 - the data which then increases the bias
69:21 - of the model and decreases the variance
69:23 - of the model
69:26 - let's first formally Define what the
69:28 - overfitting problem is as well as what
69:30 - the underfitting is so overfitting
69:32 - occurs when the model performs well in
69:34 - the training while the model performs
69:37 - worse on the test data so you end up
69:39 - having a low training error rate but a
69:41 - high test error rate and in the ideal
69:44 - world we want our test error rate to be
69:46 - low or at least that the training a rate
69:48 - is equal to the test error rate
69:50 - overfitting is a common problem in
69:52 - machine learning where a model learns
69:55 - the detail and noise in training data to
69:58 - the point where it negatively impacts
69:59 - the performance of the model on this new
70:01 - data so the model follows the data too
70:05 - closely closer than it should this means
70:08 - that the noise or random fluctuations of
70:10 - the training data is picked up and
70:12 - learned as concepts by the model which
70:14 - it should actually
70:16 - ignore the problem is that the noise or
70:18 - random component of the training data
70:21 - will be very different from the noise in
70:22 - the new data the model will therefore be
70:25 - less effective in making predictions on
70:26 - new data overfitting is caused by having
70:30 - too many features too complex of a model
70:33 - or too little of the
70:35 - data when the model is overfitting then
70:38 - also the model has high variance and low
70:41 - bias usually the higher is the model
70:43 - flexibility the higher is the risk of
70:46 - overfitting because then we have higher
70:48 - risk of having a model following the
70:50 - data too closely and following the noise
70:54 - so underfitting is the other way around
70:56 - underfitting occurs when our test error
70:58 - rate is much lower than our training
71:00 - error
71:02 - rate given that overfitting is much
71:05 - bigger of a problem and we want ideally
71:07 - to fix the case when our test theate is
71:10 - large we will only focus on the
71:11 - overfitting and this also the topic that
71:14 - you can expect during your data science
71:16 - interviews as well as something that you
71:18 - need to be aware of whenever you are
71:19 - training a machine learning model all
71:22 - right so now we we know what overfitting
71:24 - is we should now talk about how we can
71:26 - fix this problem there are several ways
71:29 - of fixing or preventing overfitting
71:32 - first you can reduce the complexity of
71:34 - the model we saw that higher the
71:36 - complexity of the model higher is the
71:38 - chance of the following the data
71:39 - including the noise too closely
71:41 - resulting in overfitting therefore
71:44 - reducing the flexibility of the model
71:46 - will reduce the overfitting as well this
71:49 - can be done by using a simpler model
71:51 - with fewer parameters or by applying a
71:54 - regularization techniques such as L1 or
71:56 - L2 regularization that we will talk in a
71:58 - bit kind solution is to collect more
72:00 - data the more data you have the less
72:03 - likely your model will overfit third and
72:06 - another solution is using resampling
72:08 - techniques one of which is cross
72:10 - validation this is a technique that
72:12 - allows you to train and test your model
72:15 - on different subsets of your data which
72:17 - can help you to identify if your model
72:19 - is overfitting we will discuss cross
72:21 - validation as well as other re sampling
72:24 - techniques later in the section another
72:27 - solution is to apply early stopping
72:29 - early stopping is a technique where you
72:31 - monitor the performance of the model on
72:33 - a validation set during the training
72:35 - process and stop the training when the
72:38 - performance starts to decrease another
72:40 - solution is to use assemble methods by
72:42 - combining multiple models such as
72:44 - decision trees overfitting can be
72:46 - reduced we will be covering many popular
72:49 - emble techniques in this course as
72:51 - well finally you can is what we call
72:54 - dropout dropout is a regularization
72:56 - technique for reducing overfitting in
72:58 - narrow networks by dropping out or
73:00 - setting to zero some of the neurons
73:03 - during the training process because from
73:05 - time to time Dropout related questions
73:08 - do appear during the data science
73:10 - interviews for people with no experience
73:12 - so if someone asks you about Dropout
73:15 - then at least you will remember that
73:16 - it's a technique used to solve
73:18 - overfitting in the setting of deep
73:21 - learning it's worth noting that there is
73:24 - no one solution that works for all types
73:26 - of overfitting and often a group of
73:29 - these techniques that we just talk about
73:31 - should be used to address the
73:36 - problem we saw that when the model is
73:38 - overfitting then the model has high
73:40 - variance and low bias by definition
73:44 - regularization or what we also call
73:46 - shrinkage is a method that shrinks some
73:49 - of the estimated coefficients toward
73:51 - zero to penalize unimportant variables
73:54 - for increasing the variance of the model
73:57 - this is a technique used to solve the
73:59 - overfitting problem by introducing the
74:02 - lethal bias in the model was
74:04 - significantly decreasing its
74:07 - variance there are three types of
74:09 - regularization techniques that are
74:10 - widely known in the industry the first
74:13 - one is to reach regression or L2
74:15 - regularization the second one is the ler
74:18 - regression or the L1 regularization and
74:21 - finally the third one is the Dropout
74:24 - which is a regularization technique used
74:26 - in deep learning we will cover the first
74:29 - two types in this
74:32 - lecture let's now talk about re
74:34 - regression or L2 regularization so re
74:37 - regression or L2 regularization is a
74:40 - shrinkage technique that aims to solve
74:42 - overfitting by shrinking some of the
74:44 - modor coefficients towards zero
74:46 - retrogression introduces latal bias into
74:48 - the model while significantly reducing
74:51 - the model
74:52 - variance R regression is a variation of
74:55 - linear regression but instead of trying
74:57 - to minimize the sum of squared
74:59 - residuales that linear regression does
75:01 - it aims to minimize the sum of squared
75:03 - residuales added on the top of the
75:05 - squared coefficients what we call L2
75:07 - regularization term let's look at a
75:10 - multiple linear regression example with
75:12 - P independent variables or predictors
75:15 - that are used to model the dependent
75:16 - variable
75:19 - y if you have followed the statistical
75:22 - section of this course
75:23 - you might also recall that the most
75:25 - popular estimation technique to estimate
75:27 - the parameter of the linear regression
75:29 - assuming its assumptions are satisfied
75:31 - is the ordinary Le squares or the OLS
75:34 - which finds the optimal coefficients by
75:35 - minimizing the sum of squared residuales
75:38 - or the
75:39 - RSS so re regression is pretty similar
75:42 - to the OS except that the coefficients
75:44 - are estimated by minimizing a slightly
75:47 - different cost or loss
75:50 - function this is the loss function of
75:52 - the re regession where beta J is the
75:54 - coefficient of the model for variable J
75:57 - beta0 is the intercept and x i j is the
76:00 - input value for the variable J and
76:02 - observation I Yi is a target variable or
76:05 - the dependent variable for observation Y
76:08 - and N is the number of samples and
76:10 - Lambda is what we call regularization
76:12 - parameter of the r
76:14 - regression so this is the loss function
76:16 - of OLS that you can see here and added a
76:20 - penalization term so it's combined the
76:24 - what we call RSS so if you check out the
76:26 - very initial lecture in this section
76:28 - where we spoke about different metrics
76:29 - that can be used to evaluate regression
76:31 - type of models you can see RSS and the
76:33 - definition of RSS well if you compare
76:36 - this expression then you can easily find
76:38 - that this is the exact formula for the
76:40 - RSS added with an intercept and this
76:43 - right term is what we called a penalty
76:46 - amount which basically represents the
76:48 - Lambda times the sum of the squar of the
76:50 - coefficients included in our model here
76:53 - Lambda which is always positive so it's
76:55 - always larger than equal zero is the
76:57 - tuning parameter or the penalty
76:59 - parameter this expression of the sum
77:01 - squared coefficients is called L2 Norm
77:04 - which is why we call this L2 penalty
77:06 - based regression or L2
77:09 - regularization in this way regression
77:12 - assigns a penalty by shrinking their
77:14 - coefficients towards zero reduces the
77:17 - overall model variance but this
77:19 - coefficient will never become exactly
77:21 - zero so the model parameters are never
77:24 - said to exactly zero which means that
77:26 - all P predictors of the model are still
77:30 - intact this one is a key property of
77:32 - retrogression to keep in mind that it
77:34 - shrinks the parameters towards zero but
77:37 - never exactly sets them equal to
77:42 - zero L2 Norm is a mathematical term
77:45 - coming from linear algebra and it's
77:47 - standing for alian
77:50 - Norm we spoke about the penalty
77:52 - parameter lum LDA what we also call the
77:54 - tuning parameter Lambda which serves to
77:56 - control the relative impact of the
77:58 - penalty on the regression coefficient
78:00 - estimates when the Lambda is equal to
78:03 - zero the penalty term has no effect and
78:05 - the re regression will introduce the
78:07 - ordinary Le squares estimates but as the
78:10 - Lambda increases the impact of the
78:12 - shrinkage penalty grows and the r
78:14 - regression coefficient estimates
78:16 - approach to zero what is important to
78:18 - keep in mind which you can also see from
78:20 - this graph is that in r agression large
78:23 - Lambda will assign a penalty to some
78:25 - variables by shrinking their
78:27 - coefficients towards zero but they will
78:29 - never become exactly zero which becomes
78:32 - a problem when you are dealing with a
78:34 - model that has a large number of
78:36 - features and your model has a low
78:39 - interpretability retrogressions
78:41 - advantage over ordinarily squares is
78:43 - coming from the earlier introduced bias
78:45 - Varian trade of phenomenon so as in
78:48 - Lambda the penalty parameter increases
78:50 - the flexibility of the retrogression F
78:52 - decreases leading to decreased variance
78:55 - but increased
78:56 - bias the main advantages of
78:58 - retrogression are solving overfitting
79:01 - which regression can shrink the
79:03 - regression coefficient of less important
79:05 - predictors towards zero it can improve
79:07 - the prediction accuracy as well by
79:09 - reducing the variance and increasing the
79:11 - bias of the model Rich repression is
79:15 - less sensitive to outliers in the data
79:17 - compared to linear regression Rich
79:19 - regression is computationally less
79:21 - expensive compared to class or
79:23 - regression the main disadvantage of R
79:25 - aggression is the low modal
79:28 - interpretability as the P so the number
79:30 - of features your model is
79:32 - large let's now look into another
79:34 - regularization technique called l or
79:36 - regression or L1 regularization by
79:39 - definition l or regression or L1
79:41 - regularization is a shrinkage technique
79:43 - that aims to solve overfitting by
79:45 - shrinking some of the modal coefficients
79:47 - towards zero and setting some to exactly
79:50 - zero l or regression like retrogression
79:53 - introduces later bias into the model
79:55 - while significantly reducing model
79:57 - variance there is however small
79:59 - difference between the two regression
80:01 - techniques that makes a huge difference
80:03 - in their results we saw that one of the
80:06 - biggest disadvantages of R regression is
80:08 - that it will always include all the
80:10 - predictors or all the p predictors in
80:12 - the final
80:13 - model whereas in case of lasso it
80:16 - overcomes this disadvantage so large
80:19 - Lambda or penalty parameter will assign
80:22 - a penalty to some variables by shrinking
80:24 - their coefficients towards zero in case
80:27 - of Rich aggression they will never
80:29 - become exactly zero which becomes a
80:31 - problem when your model has a large
80:33 - number of features and it has a low
80:36 - interpretability and L or regression
80:38 - overcomes this disadvantage of
80:42 - retrogression let's have a look at the
80:44 - loss function of L
80:46 - regularization so this is the loss
80:48 - function of OLS which is a left part of
80:51 - the formula called RSS combined with a
80:54 - penalty amount which is the right hand
80:55 - side of the expression the Lambda times
80:58 - some of the absolute values of the
81:00 - coefficients beta
81:02 - J as you can see this is the RSS that we
81:05 - just saw which is exactly the same as
81:07 - the loss function of the OLS and then we
81:09 - are adding the second term which
81:11 - basically is the Lambda the penalization
81:14 - parameter multiplied by the sum of the
81:16 - absolute value of the coefficient beta J
81:18 - where J goes from one till p and the p
81:21 - is number of predictors included in now
81:23 - model here once again the Lambda which
81:26 - is always positive larger than equal Z
81:29 - is a tuning parameter or the penalty
81:31 - parameter this expression of the sum of
81:34 - squared coefficients is called L1 Norm
81:37 - which is why we call this L1 penalty
81:39 - based regression or L1 regularization in
81:42 - this way L of regression assigns a
81:44 - penalty to some of the variables by
81:46 - shrinking their coefficients towards
81:48 - zero and setting some of these
81:50 - parameters to exactly zero
81:54 - so this means that some of the
81:56 - coefficients will end up being exactly
81:58 - equal to zero which is a key difference
82:00 - between the L regression versus the reg
82:04 - regression the L1 Norm is a mathematical
82:07 - term coming from the linear alra and
82:09 - it's standing for man had Norm or
82:11 - distance you might see here a key
82:13 - difference when comparing the visual
82:15 - representation of the L regression
82:17 - compared to the visual representation of
82:19 - the reg agression so if you look at this
82:21 - point you can see that there will be
82:23 - cases where our coefficients will be set
82:26 - to exactly zero this is where we have
82:28 - this intersection whereas in case of R
82:31 - regression you can recall that there was
82:33 - not a single intersection so the numbers
82:36 - where the circle was closed to the
82:39 - intersection points but there was not a
82:41 - single point when there was an
82:43 - intersection and the coefficients were
82:45 - put to zero and that's the key
82:47 - difference between two regression type
82:49 - of models between the two regularization
82:52 - Tech
82:53 - techniques the main advantages of loss
82:55 - or regression are solving overfitting so
82:58 - loss or regression can shrink the
82:59 - regression coefficient of less important
83:01 - predictors toward zero and some to
83:04 - exactly zero as the model filters some
83:07 - variables out L indirectly performs also
83:10 - what we call feature selection such that
83:12 - the resulted model is highly
83:14 - interpretable and with less features and
83:17 - much more interpretable compared to the
83:19 - reg aggression laso can also improve the
83:22 - predi accuracy of the model by reducing
83:25 - the variance and increasing the bias of
83:27 - the model but not as much as the
83:32 - retrogression earlier when speaking
83:34 - about correlation we also briefly
83:36 - discussed the concept of causation we
83:38 - discuss that correlation is not a
83:40 - causation and we also briefly spoke the
83:42 - method used to determine whether there
83:44 - is a causation or not that model is the
83:47 - infamous linear aggression and even if
83:49 - this model is recognized as a simple
83:51 - approach it's one one of the few methods
83:53 - that allows identifying features that
83:55 - have an impact or statistically
83:57 - significant impact on a variable that we
83:59 - are interested in and we want to explain
84:02 - and it also helps you identify how and
84:05 - how much there is a change in the Target
84:07 - variable when changing the independent
84:09 - variable
84:14 - values to understand the concept of
84:16 - linear aggression you should also know
84:17 - and understand the concepts of dependent
84:19 - variable independent variable linearity
84:22 - and statistical significant effect
84:25 - dependent variables are often referred
84:27 - to as response variables or explained
84:29 - variables by definition dependent
84:31 - variable is a variable that is being
84:33 - measured or tested it's called the
84:35 - dependent variable because it's thought
84:37 - to depend on the independent variables
84:40 - so you can have one or multiple
84:42 - independent variables but you can have
84:44 - only one dependent variable that you are
84:46 - interested in that is your target
84:49 - variable let's now look into the
84:51 - independent variable definition so
84:53 - independent variables are often referred
84:55 - as regressors or explanatory variables
84:58 - and by definition independent variable
85:00 - is the variable that is being
85:02 - manipulated or controlled in the
85:04 - experiment and is believed to have an
85:07 - effect on the dependent variable put it
85:09 - differently the value of the dependent
85:12 - variable is s to depend on the value of
85:14 - the independent variable for example in
85:16 - an experiment to test the effect of
85:19 - having a degree on the wage the degree
85:21 - variable would be your independent
85:23 - variable and the wage would be your
85:25 - dependent variable finally let's look
85:27 - into the very important concept of
85:29 - statistical significance we call the
85:32 - effect statistically significant if it's
85:33 - unlikely to have occurred by random
85:35 - chance in other words a statistically
85:38 - significant effect is one that is likely
85:40 - to be real and not due to a random
85:44 - chance let's now Define the linear
85:46 - regression model formally and then we
85:49 - will dive deep into the theoretical and
85:50 - practical details
85:53 - by definition V regression is a
85:55 - statistical or machine learning method
85:57 - that can help to model the impact of a
85:59 - unit change in the variable the
86:00 - independent variable on the values of
86:03 - another Target variable or the dependent
86:05 - variable when the relationship between
86:07 - the two variables is assumed to be
86:09 - linear when the linear regression model
86:12 - is based on a single independent
86:13 - variable then we call this model simple
86:15 - linear
86:17 - regression when the model is based on
86:19 - multiple independent variables we call
86:21 - it multiple linear
86:26 - regression let's look at the
86:28 - mathematical expression describing
86:29 - linear regression you can recall that
86:32 - when the linear regression model is
86:33 - based on a single independent variable
86:35 - we just call it a simple linear
86:37 - regression this expression that you see
86:39 - here is the most common mathematical
86:41 - expression describing simple linear
86:43 - regression so you can see that we are
86:45 - saying that the Yi is equal to Beta 0
86:47 - plus beta 1 x i plus
86:50 - UI in this expression the Yi is the
86:54 - dependent variable and the I that you
86:57 - see here is the index corresponding to
86:59 - the E row so whenever you are getting
87:01 - the data and you want to analyze this
87:04 - data you will have multiple rows and if
87:06 - your multiple rows describe the
87:08 - observations that you have in your data
87:10 - so it can be people it can be
87:12 - observation describing uh your data then
87:16 - the each characterizes the specific roow
87:18 - the each roow that you have in your data
87:20 - and the Yi is then variables value
87:23 - corresponding to that each show then the
87:26 - same holds for the XI so the XI is then
87:29 - the independent variable or the
87:31 - explanatory variable or the regressor
87:34 - that you have in your model which is the
87:36 - variable that we are testing so we want
87:38 - to manipulate it to see whether this
87:39 - variable has a statistically significant
87:42 - impact on the dependent variable y so we
87:45 - want to see whether the unit change in
87:47 - the X will result in a specific change
87:50 - in the Y and what kind of change is
87:53 - that so beta Z that you see here is not
87:57 - a variable and it's called intercept or
87:59 - constant something that is unknown so we
88:01 - don't have that in our data and it's one
88:03 - of the parameters of linear regression
88:05 - it's an unknown number which the linear
88:07 - regression model should estimate so we
88:10 - want to use the linear regression model
88:12 - to find out this uh unknown value as
88:15 - well as the second unknown value which
88:16 - is a beta one as well as we can estimate
88:19 - the error terms which are represented by
88:21 - the UR
88:24 - so beta one next to the XI so next to
88:27 - the independent variable is also not a
88:29 - variable so like beta zero is an unknown
88:31 - parameter in linear regression model an
88:34 - unknown number which the linear
88:35 - regression model should estimate beta
88:38 - one is often referred as a slope
88:40 - coefficient of variable X which is the
88:42 - number that quantifies how much
88:45 - dependent variable y will change if the
88:47 - independent variable X will change by
88:49 - one unit so that's EX exactly what we
88:52 - are most interested in the beta one
88:55 - because this is the coefficient and this
88:57 - is the unknown number that will help us
88:59 - to understand and answer the question
89:01 - whether our independent variable X has a
89:03 - statistically significant impact on our
89:05 - dependent variable y finally the U that
89:08 - you see here or the UI in the expression
89:11 - is the error term or the amount of
89:13 - mistake that the model makes when
89:15 - explaining the target variable we add
89:17 - this value since we know that we can
89:19 - never exactly and accurately estimate
89:21 - the Target variable so we will always
89:24 - make some amount of estimation error and
89:26 - we can never estimate the exact value of
89:28 - y hence we need to account for this
89:31 - mistake that we are going to make and we
89:32 - know in advance that we are going to
89:34 - have this mistake by adding an error
89:36 - term to our
89:38 - model let's also have a brief look at
89:41 - how multiple linear regression is
89:42 - usually expressed in mathematical terms
89:45 - so you might recall that difference
89:46 - between the simple linear regression and
89:48 - multiple linear regression is that the
89:50 - first one has a a single independent
89:53 - variable in it whereas the letter or the
89:55 - multiple linear regression like the name
89:57 - suggest has multiple independent
89:59 - variables in it so more than
90:01 - one knowing this type of Expressions is
90:04 - critical since they not only appear a
90:06 - lot in the interviews but also in
90:08 - general you will see them in the data
90:10 - science blogs in presentations in books
90:13 - and also in papers so being able to
90:15 - quickly identify and say ah I remember
90:18 - saying this at once then it will help
90:20 - you to easier understand and follow the
90:23 - process and the story
90:25 - line so uh what you see here you can
90:28 - read as Yi is equal to Beta 0 plus beta
90:31 - 1 * X1 I plus beta 2 * X2 I plus beta 3
90:35 - * X3 I plus UI so this is the most
90:39 - common mathematical expression
90:40 - describing multiple linear regression in
90:43 - this case with three independent
90:46 - variables so if you were to have more
90:48 - independent variables you should add
90:50 - them with their corresponding indices
90:52 - and coefficients so in this case the
90:54 - method will aim to estimate the model
90:56 - parameters which are beta 0 beta 1 beta
90:59 - 2 and beta Tre so like before Yi is our
91:03 - dependent variable which is always a
91:05 - single one so we only have one dependent
91:08 - variable then we have beta 0 which is
91:10 - our intercept or the constant then we
91:13 - have our first slope coefficient which
91:14 - is beta 1 corresponding to our first
91:16 - independent variable X1 then we have X1
91:19 - I which stands for the independent
91:22 - variable the first independent variable
91:24 - with an index one and the I stands for
91:26 - the index corresponding to the row so
91:29 - whenever we have multiple linear
91:30 - regression we always need to specify two
91:33 - indices and not only one like we had in
91:36 - our uh single linear regression the
91:38 - index cor that characterizes which
91:41 - independent variable we are referring to
91:43 - so whether it's independent variable one
91:45 - two or three and then we need to specify
91:48 - which row we are referring to which is
91:49 - the index I so you might notice that
91:51 - that in this case all the indices are
91:53 - the same because we are uh looking into
91:56 - one specific role and we are
91:57 - representing this role by using the
91:59 - independent variables the error term and
92:01 - dependent variable so then we are adding
92:04 - our third term which is beta 2 * x2i so
92:08 - the beta 2 is our third unknown
92:10 - parameter in the model and the second
92:13 - slope coefficient corresponding to our
92:15 - second independent variable and then we
92:17 - have our third independent variable with
92:19 - the corresponding slope coefficient beta
92:21 - 3 as well as we also add like always an
92:24 - error term to account for the error that
92:26 - we know that we are going to
92:29 - make so now when we know what the linear
92:32 - regression is and how to express it in
92:34 - the mathematical terms you might be
92:36 - asking the next logical question well we
92:39 - know that when we know what the linear
92:40 - regression is and how to express it in
92:42 - the mathematical terms you might be
92:44 - asking the next logical question how do
92:46 - we find those unknown parameters in the
92:48 - model in order to find out how the
92:50 - independent variables in impacted the
92:52 - dependent variable finding this unknown
92:55 - parameters is called estimating in data
92:58 - science and in general so we are
93:00 - interested in finding out the possible
93:03 - values or the values that the best
93:05 - approximate the unknown values in our
93:07 - model and we call this process
93:10 - estimation and one technique used to
93:12 - estimate linear regression parameters is
93:15 - called oils or ordinary Le
93:20 - squares so domain idea behind this
93:22 - approach the OLS is to find the best
93:24 - fitting straight line so the regression
93:26 - line through a set of paired X and y's
93:29 - so our independent variables and
93:31 - dependent variables values by minimizing
93:34 - the sum of squared
93:35 - errors so to minimize the sum of squares
93:39 - of the differences between the observed
93:42 - dependent variable and its values which
93:45 - are the predicted values that we are
93:47 - predicted by our model that's exactly
93:50 - what we want to do by by using this
93:52 - linear function of the independent
93:54 - variables the residuals so this is too
93:57 - much information let's go it step by
93:59 - step so in linear regression we just so
94:02 - when we are expressing our simple linear
94:04 - regression we have this error term and
94:07 - we can never know what is the actual
94:08 - error term but what we can do is to
94:11 - estimate the value of the error term
94:13 - which we call residual so we want to
94:15 - minimize the sum of squ residuales
94:18 - because we don't know the errors so we
94:20 - want to find a line
94:22 - that will best fit our data in such way
94:25 - that the error that we are making or the
94:27 - sum of squared errors is as small as
94:29 - possible and since we don't know the
94:31 - errors we can estimate the Errors By
94:34 - each time looking at the predicted value
94:36 - that is predicted by our model and the
94:38 - True Value and then we can subtract them
94:40 - from each other and we can see how good
94:42 - our model is estimating the values that
94:45 - we have so how good is our model
94:47 - estimating the unknown
94:49 - parameters so to minimize the sum of
94:52 - squar of the differences between the
94:54 - observed dependent variable and its
94:56 - values predicted by the linear function
94:59 - of the independent variables so the
95:01 - minimizing the sum of squared
95:04 - residuales so uh we Define the estimate
95:07 - of a parameters and variables by adding
95:09 - a hge on the top of the variables or
95:12 - parameters so in this case you can see
95:14 - that y I had is equal to Beta Z head
95:16 - plus beta 1 head XI so you can see that
95:20 - we no longer have a error term this and
95:22 - we say that Yi head is the estimated
95:25 - value of Yi and beta zero head is the
95:28 - estimated value of beta 0 beta 1 head is
95:32 - the estimated value of our beta 1 and
95:34 - the XI is still our data so the values
95:37 - that we have in our data and therefore
95:39 - we don't have a hat since that does not
95:42 - need to be estimated so what we want to
95:45 - do is to estimate our dependent variable
95:48 - and we want to compare our estimated
95:50 - value that we got using our OLS with the
95:53 - actual with the real value such that we
95:55 - can calculate our errors or the estimate
95:58 - of the error which is represented by the
96:00 - UI head so the UI head is equal to Yi
96:03 - minus Yi head where UI head is simply
96:06 - the estimate of the error term or the
96:11 - residual so this predicted error is
96:14 - always referred as residual so make sure
96:16 - that you do not confuse the error with
96:18 - the residual so error can never be
96:20 - observed error you can never calculate
96:23 - and you will never know but what you can
96:24 - do is to predict the error and you can
96:27 - when you predict the error then you get
96:28 - a recal and what oil is trying to do is
96:32 - to minimize the amount of airor that
96:34 - it's
96:35 - making therefore it looks at the sum of
96:38 - squared residuales across all the
96:40 - observation and it tries to find the
96:43 - line that will minimize this value
96:45 - therefore we are saying that the O tries
96:48 - to find the best fitting straight line
96:50 - such that it minimizes the sum of
96:52 - squared
96:54 - residuals we have discussed this model
96:56 - when we were talking about this model
96:58 - mainly from the perspective of causal
97:00 - analysis in order to identify features
97:03 - that have a statistically significant
97:05 - impact on the response variable but
97:07 - linear regression can also be used as a
97:09 - prediction model for modeling linear
97:12 - relationship so let's refresh our memory
97:14 - with the definition of linear regression
97:16 - model by definition linear regression is
97:18 - a statistical or a machine learning
97:20 - method that can help to modrow the
97:22 - impact of a unit change in a variable
97:24 - the independent variable on the values
97:26 - of another Target variable the dependent
97:28 - variable when the relationship between
97:30 - two variables is linear we also
97:33 - discussed how mathematically we can
97:35 - express what we call Simple linear
97:36 - regression and a multiple linear
97:38 - regression so this how the uh simple
97:41 - linear regression can be represented so
97:44 - uh in case of simple linear regression
97:45 - you might recorde that we are dealing
97:47 - with just a single independent variable
97:49 - and we always have just one dependent
97:52 - variable both in the single linear
97:54 - regression and in the multiple linear
97:55 - regression so here you can see that Yi
97:58 - is equal to Beta 0 plus beta 1 * XI plus
98:01 - UI where Y is the dependent variable and
98:04 - I is basically the index of each
98:06 - observation or the row and then the beta
98:09 - 0 is The Intercept which is also known
98:11 - as constant and then the beta 1 is the
98:14 - slope coefficient or a parameter
98:16 - corresponding to the independent
98:17 - variable X which is unnown and a
98:20 - constant which want to estimate along to
98:22 - the beta zero and then the XI is the
98:25 - independent variable corresponding to
98:26 - the observation I and then finally the
98:29 - UI is the error term corresponding to
98:31 - the observation I do keep in mind that
98:33 - this error term we are adding because we
98:35 - do know that we always are going to make
98:37 - a mistake and we can never perfectly
98:39 - estimate the dependent variable
98:41 - therefore to account for this mistake we
98:43 - are adding this
98:46 - UI so let's also recall the estimation
98:49 - technique that we use to estimate the
98:50 - parameter of the linear regression model
98:52 - so the beta 0 and beta 1 and to predict
98:55 - the response variable so we call this
98:57 - estimation technique ORS or the ordinary
99:00 - Le squares NS is an estimation technique
99:03 - for estimating the unknown parameters in
99:05 - the linear regression model to predict
99:07 - the response or the dependent variable
99:09 - so we need to estimate the beta Z so we
99:11 - need to get the beta zero head and we
99:13 - need to estimate the beta one or the
99:15 - beta 1 head in order to obtain the Y I
99:18 - head so Yi head is equal to Beta Z head
99:21 - plus beta 1 head time x i where the um
99:25 - difference between the Yi head and the
99:28 - Yi so the true value of the dependent
99:31 - variable and the predicted value they
99:33 - are different will then produce our
99:35 - estimate of the error or what we also
99:37 - call residual the main idea behind this
99:40 - approach is to find the best fitting
99:41 - straight line so the regression line
99:44 - through a set of paired X and Y values
99:46 - by minimizing the sum of squared
99:48 - residuales so we want to minimize our
99:51 - errors as much as possible therefore we
99:53 - are taking their squared version and we
99:55 - are trying to sum them up and we want to
99:57 - minimize this entire error so to
99:59 - minimize the sum of squar residual so
100:01 - the difference between the observed
100:03 - dependent variable and its values
100:05 - predicted by the linear function of the
100:07 - independent variables we need to use the
100:10 - OLS one of the most common questions
100:13 - related to linear regression that comes
100:15 - time and time again in the uh data
100:17 - science related interviews is a topic of
100:20 - the Assumption of the linear regression
100:22 - model so you need to know each of these
100:25 - five fundamental assumptions of the
100:26 - linear regression and the OLS and also
100:29 - you need to know how to test whether
100:30 - each of these assumptions are
100:33 - satisfied so the first assumption is the
100:35 - linearity Assumption which states that
100:37 - the relationship between the independent
100:39 - variables and the dependent variable is
100:41 - linear we also say that the model is
100:43 - linear in parameters you can also check
100:46 - whether the linearity assumption is
100:47 - Satisfied by plotting the residuals to
100:49 - the fitted values if the pattern is not
100:52 - linear then the estimat will be biased
100:54 - in this case we say that the linearity
100:56 - assumption is violated and we need to
100:58 - use more flexible models such as tree
101:00 - based models that we will discuss in a
101:02 - bit that are able to model these
101:04 - nonlinear
101:06 - relationships the second assumption in
101:08 - the linear regression is the Assumption
101:10 - about randomness of the sample which
101:12 - means that the data is randomly sampled
101:14 - and which basically means that the
101:15 - errors or the residuales of the
101:17 - different observations in the data are
101:19 - independent of each other you can also
101:22 - check whether the second assumption so
101:24 - this assumption about random sample is
101:26 - Satisfied by plotting the residuals you
101:29 - can then check whether the mean of this
101:31 - residuales is around zero and if not
101:33 - then the OLS estimate will be biased and
101:36 - the second assumption is violated this
101:38 - means that you are systematically over
101:40 - or under predicting the dependent
101:44 - variable the third assumption is the
101:46 - exogeneity Assumption which is a really
101:48 - important assumption often as during the
101:50 - data science interviews exogeneity means
101:53 - that each independent variable is
101:55 - uncorrelated with the error terms
101:57 - exogeneity refers to the assumption that
102:00 - the independent variables are not
102:01 - affected by the error term in the model
102:04 - in other words the independent variables
102:06 - are assumed to be determined
102:07 - independently of the erors in the model
102:10 - exogeneity is a key Assumption of the
102:11 - new regression model as it allows us to
102:13 - interpret the estimated coefficient as
102:15 - representing the true causal effect of
102:18 - the independent variables on the
102:19 - dependent variable
102:21 - if the independent variables are not
102:23 - exogeneous then the estimated
102:25 - coefficients may be biased and the
102:27 - interpretation of the results may be
102:28 - invalid in this case we call this
102:31 - problem an endogeneity problem and we
102:33 - say that the independent variable is not
102:35 - exogeneous but it's endogeneous it's
102:38 - important to carefully consider the
102:40 - exogeneity Assumption when building a
102:42 - linear regression model as violation of
102:44 - this assumption can lead to invalid or
102:47 - misleading results if this assumption is
102:49 - satisfied for an independent variable in
102:51 - the linear model we call this
102:53 - independent variable exogeneous so
102:55 - otherwise we call it endogeneous and we
102:57 - say that we have a problem of
103:00 - endogenity endogenity refers to the
103:03 - situation in which the independent
103:04 - variables in the linear regression model
103:06 - are correlated with the error terms in
103:08 - the model in other words the errors are
103:10 - not independent of the independent
103:12 - variables endogeneity is a violation of
103:15 - one of the key assumptions of the linear
103:17 - regression model which is that the
103:19 - independent variables are EX geners or
103:21 - not affected by the errors in the model
103:23 - endogenity can arise in a number of ways
103:26 - for example it can be caused by omitted
103:28 - variable bias in which an important
103:31 - predictor of the dependent variable is
103:33 - not included in the model it can also be
103:35 - caused by the reverse causality in which
103:38 - the dependent variable affects the
103:40 - independent variable so those two are a
103:43 - very popular examples of the case when
103:45 - we can get an endogenity problem and
103:48 - those are things that you should know
103:49 - whenever you are interest in for data
103:51 - science roles especially when it's
103:53 - related to machine learning because
103:55 - those questions are uh being asked to
103:57 - you in order to test whether you
103:59 - understand the concept of exogeneity
104:00 - versus endogenity and also in which
104:03 - cases you can get endogenity and also
104:05 - how you can solve it so uh in case of
104:08 - omitted variable bias let's say you are
104:10 - estimating a person's salary and you are
104:13 - using as independent variable their
104:15 - education their number of years of
104:18 - experience and uh some other factors but
104:20 - you are not including for instance in
104:22 - your model a feature that would describe
104:24 - the uh intelligence of a person or uh
104:27 - for instance IQ of the person well given
104:31 - that those are a very important
104:33 - indicator for a person in order to
104:35 - perform in their uh field and this can
104:38 - definitely have um indirect impact on
104:40 - their salary not including these
104:42 - variables will result in omitted
104:44 - variable bias because this will then be
104:47 - uh Incorporated in your um error term
104:51 - and uh this can also relate to the other
104:53 - independent variables because then your
104:56 - uh IQ is also related to the um to the
105:00 - education that you have higher is your
105:02 - IQ usually higher is your education so
105:05 - in this way you will have an error term
105:07 - that includes an important variable so
105:09 - this is the omitted variable which is
105:12 - then uh correlated with your uh one of
105:15 - your or multiple of your independent
105:16 - variables include in your model so the
105:19 - other example other cause of the
105:21 - endogenity problem is the reverse
105:23 - causality and um what reverse causality
105:25 - means is basically that not only the
105:28 - independent variable has an impact on
105:29 - the dependent variable but also the
105:31 - dependent variable has an impact on the
105:33 - independent variable so there is a
105:35 - reverse relationship which is something
105:37 - that we want to avoid we want to have
105:39 - our features that include in our model
105:41 - that have only an impact on dependent
105:44 - variable so they are explaining the
105:46 - dependent variable but not the other way
105:48 - around because if you have the um the
105:50 - other way so you have the dependent
105:51 - variable impacting your independent
105:53 - variable then you will have the error
105:55 - term being related to this independent
105:57 - variable because there are some
105:59 - components that also Define your
106:01 - dependent variable so knowing the uh few
106:04 - examples such as those that can cause uh
106:07 - endogenity so they can violate the
106:09 - exogeneity assumption is really
106:12 - important then uh you can also check for
106:14 - the exogeneity Assumption by conducting
106:17 - a formal statistical test this is called
106:20 - house one test so this is an
106:21 - econometrical test that helps to
106:24 - understand whether you have an
106:25 - exogeneity uh violation or not but this
106:28 - is out of the scope of this course I
106:30 - will however include uh many resources
106:33 - related the exogeneity endogenity the
106:36 - omitted variable bias as well as the
106:38 - reverse cality and also how the house
106:40 - one test can be conducted so for that
106:43 - check out the interation guide where you
106:45 - can also find the corresponding free
106:47 - your
106:48 - resources the fourth assumption linear
106:51 - regression is the Assumption about homos
106:53 - skes homos refers to the assumption that
106:56 - the variance of the errors is constant
106:58 - across all predicted values this
107:00 - assumption is also known as the
107:02 - homogeneity of the variance homosa is an
107:06 - important Assumption of linear
107:07 - regression model as it allows us to use
107:10 - certain statistical techniques and make
107:12 - inferences about parameters of the model
107:15 - if the errors are not homoskedastic then
107:17 - the result of these techniques may be
107:19 - invalid or misleading if this assumption
107:22 - is violated then we say that we have
107:25 - heteroscedasticity hecticity refers to
107:27 - the situation in which the variance of
107:29 - the error terms in the linear regression
107:31 - model is not constant across all the
107:34 - predicted values so we have a variating
107:37 - variant in other words the Assumption of
107:39 - homos skas testing in that case is
107:41 - violated and we say we have a problem of
107:44 - heos heteros can be a real problem in V
107:48 - regression nurses because it can lead to
107:50 - invalid or misleading results for
107:53 - example the standard estimates and the
107:55 - confidence intervals for the parameters
107:57 - may be incorrect which means that also
107:59 - the statistical test may have incorrect
108:02 - type one error rates so you might recall
108:04 - when we were discussing the linear
108:06 - regression as part of the fundamental
108:07 - statis section of this course is that we
108:10 - uh looked into the output that comes
108:12 - from a python and we saw that we are
108:14 - getting uh estimates as part of the
108:16 - output as well as standard errors then
108:18 - the T Test so the student T test and
108:21 - then the corresponding P values and the
108:23 - 95% confidence intervals so whenever
108:26 - there is a heos problem the um
108:29 - coefficient might still be accurate but
108:32 - then the corresponding standard error
108:34 - the U student T Test which is based on
108:37 - the standard error and then the P value
108:40 - as well as the uh confidence intervals
108:43 - may not be accurate so you might get the
108:46 - uh good and reasonable coefficient but
108:49 - then you don't know how to correctly
108:51 - evaluate them you might end up
108:53 - discovering that um you might end up
108:55 - stating that certain uh independent
108:57 - variables are statistically significant
108:59 - because their coefficients are
109:01 - statistically significant since their P
109:03 - values are small but in the reality
109:05 - those P values are misleading because
109:07 - they are based on the wrong statistical
109:09 - uh test and they are based on the wrong
109:12 - standard errors you can check for this
109:15 - assumption by plotting the residual and
109:17 - see whether there is a funnel like graph
109:19 - if there's Fel like gra then you have a
109:22 - a constant variance but if there is not
109:24 - then you won't see this fenel like this
109:27 - shape that indicates that your variances
109:30 - are constant and if not then we say we
109:33 - have a problem of heos skos if you have
109:36 - a heteros system you can no longer use
109:39 - the OS and the linear regression and
109:41 - instead you need to look for other more
109:44 - advanced econometrical regression
109:46 - techniques that do not make such a
109:48 - strong assumption regarding the variance
109:50 - of your um residuals so you can for
109:53 - instance use the GLS the fgs the GMM and
109:58 - this type of solutions will um help to
110:01 - solve the hoscar problem and they will
110:03 - not make a strong assumptions regarding
110:05 - the variance in your
110:08 - model the fifth and the final assumption
110:11 - in linear regression is the Assumption
110:13 - about no perfect multicolinearity this
110:16 - assumption states that there are no
110:17 - exactly new relationships between the
110:20 - independent variables multicolinearity
110:22 - refers to the case when two or more
110:25 - independent variables in your linear
110:26 - regression model are highly correlated
110:29 - with each other this can be a problem
110:31 - because it can lead to unstable and
110:33 - unreliable estimate of the parameters in
110:36 - the model perfect multicolinearity
110:38 - happens when the independent variables
110:40 - are perfectly correlated with each other
110:43 - meaning that one variable can be
110:45 - perfectly predicted from the other ones
110:47 - and this can cause the estimated
110:49 - coefficient your linear regression model
110:51 - to be infinite or undefined and can lead
110:54 - your errors to be uh entirely misleading
110:57 - when making a predictions using this
110:58 - model if perfect multicolinearity is
111:01 - detected it may be necessary to remove
111:04 - one if not more problematic variables
111:07 - such that you will avoid having
111:08 - correlated variables in your model and
111:11 - even if the perfect multicolinearity is
111:13 - not present multicolinearity at a high
111:16 - level can still be a problem if the
111:18 - correlations between the independent
111:20 - variables are high in this case the
111:22 - estimate of the parameters may be
111:24 - imprecise and the model may be uh
111:26 - entirely misleading and will results in
111:28 - less reliable uh
111:31 - predictions so uh to test for the
111:34 - multicolinearity Assumption you have
111:36 - different solutions you have different
111:38 - options the first way uh you can do that
111:40 - is by using the uh di test De test is a
111:45 - formal statistical and econometrical
111:47 - test that will help you to identify
111:50 - which variables cause a problem and
111:52 - whether you have a perfect
111:53 - multicolinearity in your linear
111:55 - regression model you can PL heat map
111:58 - which will be based on the uh
112:00 - correlation metrix corresponding to your
112:02 - features then you will have your uh
112:04 - correlations per pair of independent
112:07 - variables plotted as a part of your heat
112:10 - map and then you can identify all the um
112:13 - pair of features that are highly
112:15 - correlated with each other and those are
112:17 - problematic features one of which should
112:19 - be removed from your model and in this
112:22 - way by uh showing the heat map you can
112:25 - also showcase your stakeholders why you
112:27 - have remove certain variables from your
112:29 - model whereas explaining a Diller test
112:32 - is much more complex because it involves
112:34 - more advanced econometrics and linear uh
112:37 - regression um
112:40 - explanation so if you're wondering how
112:42 - you can perform this de FL test and you
112:45 - want to prepare the uh questions related
112:47 - to perfect multicolinearity as well as
112:49 - how you can solve the perfect
112:51 - multicolinearity problem in your linear
112:53 - regression model then head towards the
112:55 - interview preparation guide included in
112:57 - this part of of the course in order to
112:59 - answer such questions and also to see
113:02 - the 30 most popular interview questions
113:04 - you can expect from this section in the
113:06 - interview preparation guide now let's
113:09 - look into an example coming from the
113:11 - linear regression in order to see how
113:13 - all those pieces of the puzzle come
113:15 - together so let's say we have collected
113:17 - a data on a class size and a test course
113:19 - for of students and we want to model the
113:22 - linear relationship between the class
113:24 - size and the test course using the
113:26 - linear regression model so as we have
113:28 - just one independent variable we are
113:29 - dealing with a simple linear regession
113:32 - and the model equation would be as
113:33 - follows so you can see that the test
113:35 - course is equal to beta0 plus beta 1
113:38 - multip by class size plus Epsilon so
113:41 - here the class size is the single
113:43 - independent variable that we got in our
113:45 - model the test score is the dependent
113:47 - variable the beta0 is is The Intercept
113:50 - or the constant the beta one is the
113:52 - coefficient of Interest as this the
113:54 - coefficient corresponding to our
113:56 - independent variable and this will help
113:58 - us to understand what is the uh impact
114:01 - of a unit change in the class size on
114:04 - the test score and then finally we are
114:06 - including in our model our error term to
114:09 - account for the mistakes that we are
114:11 - definitely going to make when estimating
114:13 - the uh dependent variable that has
114:16 - course the goal is to estimate the
114:19 - coefficient 0 and beta 1 from the data
114:22 - and use the estimated model to predict
114:24 - the test course based on the class size
114:27 - so once we have the estimates we can
114:29 - then interpret them as follows the Y
114:31 - intercept the beta zero represents the
114:34 - expected test course when the class size
114:36 - is zero it represents the base score
114:38 - that the student would have obtained if
114:40 - the class size would have been zero then
114:42 - the coefficient for the class size the
114:45 - beta one represents the change in the
114:46 - test course associated with the one unit
114:49 - change in the class size the positive
114:52 - coefficient would imply that one unit
114:54 - change in the class size would increase
114:56 - the test course whereas the negative
114:59 - coefficient would uh imply that the one
115:02 - unit change in the class size will
115:04 - decrease the test course uh
115:06 - correspondingly we can then use this
115:08 - model with OLS estimate in order to
115:10 - predict the test course for any given
115:13 - class
115:14 - size so let's go ahead and Implement
115:17 - that in Python if you're wondering how
115:19 - this can be done then head towards the
115:21 - resources section as as well as the part
115:23 - of the Python for data science where you
115:26 - can learn more about how to work with
115:28 - pendant data frames how to import the
115:30 - data as well as how to fit a linear
115:32 - regression model so the problem is as
115:36 - follows we have collected data on the
115:38 - class size and we have this independent
115:40 - variable so as you can see here we have
115:42 - the students uncore data and then we
115:44 - have the class size and this our feature
115:47 - and then we want to estimate the Y which
115:49 - is the test SC so uh here is the code a
115:53 - sample code that will fit a linear
115:55 - regression model we are keeping here
115:57 - everything very simple we are not
115:59 - splitting our data into training test
116:01 - and then fitting the model on the
116:02 - training data and making the predictions
116:05 - with the test score but we just want to
116:07 - see how we can interpret the uh
116:09 - coefficients so keeping everything very
116:13 - simple so you can see here that we are
116:16 - getting an intercept equal to
116:18 - 63.7 and the coefficient corresponding
116:21 - to our single independent variable class
116:23 - size is equal to minus
116:25 - 0.40 what this means is that so each
116:28 - increase of the uh class size by one
116:31 - unit will result in the decrease of the
116:33 - test scores with 0.4 so there is a
116:36 - negative relationship between the two
116:39 - now the next question is whether there
116:40 - is a statistical significance whether
116:42 - the uh coefficient is actually
116:44 - significant and where the class size has
116:47 - actually statistically significant imp
116:49 - impact on the dependent variable but all
116:52 - those are things that we have discussed
116:54 - as part of the fundamental statistic
116:56 - section of this course as well as we are
116:58 - going to look into a linear regression
117:00 - example when we are going to discuss the
117:03 - hypothesis testing so I would highly
117:06 - suggest you to uh stop in here to
117:09 - revisit the fundamentals to statistic
117:11 - section of this course to refresh your
117:13 - memory in terms of linear regression and
117:16 - then um check also the hypothesis test
117:19 - uh section of the course in order to
117:22 - look into a specific example of linear
117:25 - regression when we are discussing the
117:27 - standard errors how you can evaluate
117:29 - your OLS estimation results how you can
117:32 - use the student T Test the P value and
117:34 - the confidence intervals and how you can
117:36 - estimate them in this way you will learn
117:38 - for now only the theory related to the
117:40 - coefficients and then you can um add on
117:43 - the top of this Theory once you have
117:45 - learned all the other sections and the
117:46 - other topics in this course let's
117:49 - finally discuss the advantages and the
117:51 - disadvantages of the linear regression
117:52 - model so some of the advantages of the
117:55 - linear regression model are the
117:56 - following the linear regression is
117:58 - relatively simple and easy to understand
118:01 - and to implement linear regression
118:03 - models are well suited for understanding
118:05 - the relationship between a single
118:06 - independent variable and a dependent
118:08 - variable also linear regression can help
118:11 - to handle multiple independent variables
118:14 - and can estimate the unique relationship
118:16 - between each independent variable and
118:18 - the corresponding dependent variable
118:20 - thear regression model can also be
118:22 - extended to handle more complex models
118:25 - such as pooms interaction terms allowing
118:28 - for more flexibility in the modeling the
118:30 - data also linear aggression model can be
118:33 - easily regularized to prevent
118:35 - overfitting which is a common problem in
118:37 - modeling as we saw uh in the beginning
118:39 - of this section so you can use for
118:41 - instance retrogression which is an
118:42 - extension of Vue regression you can use
118:45 - ler regression which is also an
118:46 - extension of Vue regression model and
118:49 - then finally linear regression models
118:51 - are widely supported by software
118:53 - packages and libraries making it easy to
118:55 - implement and to analyze and some of the
118:59 - disadvantages of the linear aggression
119:01 - are the following so the linear
119:03 - aggression models make a lot of strong
119:05 - assumptions regarding for instance the
119:07 - linearity between independent variables
119:09 - and independent variables while the true
119:12 - relationship can actually be also
119:14 - nonlinear so the model will not then be
119:17 - able to capture the complexity of the
119:19 - data so nonlinearity and the predictions
119:21 - will be inaccurate therefore it's really
119:24 - important to have a data that has a
119:26 - linear relationship for linear
119:28 - regression to work linear regression
119:31 - also assumes that the error terms are
119:32 - normally distributed and also
119:34 - homoskedastic error terms are
119:36 - independent across observations
119:38 - violations of the strong assumption will
119:40 - lead to bias and inefficient estimates
119:43 - linear regression is also sensitive to
119:45 - outliers which can have a
119:46 - disproportionate effect on the estimate
119:48 - of the regression coefficients linear
119:51 - regression does not easily handle
119:52 - categorical independent variables which
119:55 - often require additional data
119:56 - preparation or the use of indicator
119:58 - variables or using
120:00 - encodings finally linear regression also
120:03 - assumes that the independent variables
120:05 - are exogeneous and not affected by the
120:08 - error terms if this assumption is
120:10 - violated then the result of the model
120:12 - may be
120:18 - misleading
120:23 - in this lecture lecture number five we
120:25 - will discuss another simple machine
120:27 - learning technique called logistic
120:29 - regression which is simple but very
120:32 - important classification model useful
120:34 - when dealing with a problem where the
120:36 - output should be a probability so the
120:39 - name regression in logistic regression
120:41 - might be confusing since this is
120:43 - actually a classification model logistic
120:46 - regression is widely used in a variety
120:48 - of fields such as social sciences
120:50 - medicine and
120:54 - Engineering so let us firstly Define the
120:56 - logistic regression model the logistic
120:58 - regression is a supervised
120:59 - classification technique that models the
121:01 - conditional probability of an event
121:03 - occurring or observation belonging to a
121:05 - certain class given a data set of
121:07 - independent variables and those are our
121:09 - features the class can have two
121:11 - categories or more but later on we will
121:14 - learn that logistic regression Works
121:15 - ideally when we have just two classes
121:18 - this is is another very important and
121:20 - very popular machine learning technique
121:22 - which though named regression is
121:24 - actually a supervised classification
121:27 - technique so when the relationship
121:29 - between two variables is linear the
121:31 - dependent variable is a categorical
121:34 - variable and you want to predict a
121:36 - variable in the form of a probability so
121:39 - a number between zero and one then
121:41 - logistic regression comes in very handy
121:44 - this is because during the prediction
121:45 - process in logistic regression the
121:47 - classifier predicts the probability
121:49 - ility a value between Z and one of each
121:52 - observation belonging to a certain
121:55 - class for instance if you want to
121:57 - predict the probability or the
121:59 - likelihood of a candidate being elected
122:01 - or Not Elected during the election
122:03 - process given the set of characteristics
122:06 - that you got about your candidate let's
122:08 - say the popularity score the past
122:10 - successes and other descriptive
122:12 - variables about this candidate then
122:14 - logistic regression comes in very handy
122:16 - to model this probability
122:20 - so rather than predicting the response
122:22 - variable logistic regression models the
122:24 - probability that y belongs to a
122:27 - particular
122:28 - category similar to the linear
122:30 - regression with a difference that
122:32 - instead of Y it predicts the log odds so
122:36 - we will come about this definition of
122:37 - log odds and odds in a bit in
122:40 - statistical terminology what we are
122:42 - trying to do is to model the conditional
122:44 - distribution of the response y given the
122:47 - predictors X
122:49 - therefore logistic regression helps to
122:51 - predict the probability of Y belonging
122:53 - to a certain class given the feature
122:56 - space what we call probability of Y
122:58 - given X if you're wondering what is the
123:01 - concept of probability what is this
123:04 - conditional probability then make sure
123:06 - to head towards the section of
123:07 - fundamentals to statistics as we are
123:10 - going to in detail about this Concepts
123:12 - as well as we are looking into different
123:13 - examples these definitions and this
123:16 - Concepts will help you to better follow
123:18 - this lecture
123:20 - so here we see the probability X which
123:22 - is what we are interested in modeling
123:24 - and it's equal to e to the power beta 0
123:26 - + beta 1 * x / to 1 + e^ beta 0 + beta 1
123:31 - * X let's now look into the formulas for
123:35 - the odds and log ODS both these formulas
123:38 - are really important because you can
123:39 - expect them during your data science
123:41 - interviews so sometimes you will be
123:43 - asked to explicitly write down the odds
123:45 - and log ODS formulas and those are
123:48 - highly related to the log likelihood and
123:50 - likelihood functions which are the base
123:52 - for the estimation technique mle or the
123:54 - maximum likelihood estimation used to
123:57 - estimate the unknown parameters in the
123:59 - logistic
124:00 - agression so the log odds and the odds
124:03 - are highly related to each other and in
124:05 - logistic regression we use the odds and
124:07 - log ODS to describe the probability of
124:09 - an event occurring the odds is a ratio
124:13 - of the probability of an event occurring
124:15 - to the probability of the event not
124:17 - occurring so as you can see the odd is
124:19 - equal to PX / to 1 - PX where PX is the
124:23 - probability of event occurring and 1 -
124:26 - PX is the probability of the event not
124:28 - occurring so this formula is equal to E
124:32 - power beta 0 + beta 1 * X in our formula
124:35 - where we only have one independent
124:37 - variable and the E simply is the ERS
124:41 - number or the 2.72 which is a constant
124:44 - so we won't derive this formula by
124:46 - ourselves because that's out of the
124:48 - scope of this course but feel free to
124:51 - head out to the PX formula that we just
124:53 - saw in the previous slide and take this
124:55 - formula divide it to one minus 2 exactly
124:57 - the same expression and you can verify
125:00 - that you will end up with this
125:01 - expression that you see
125:03 - here for example if the probability of a
125:06 - person having a heart attack is 0.2 then
125:09 - the ads of having a heart attack will be
125:12 - 0.2 / to 1 - 0.2 which is equal to
125:17 - 0.25 the low OD also known as the logit
125:20 - function is a natural logarithm of the
125:22 - OD so as you can see here the log of px/
125:25 - to 1 minus PX and this is equal to Beta
125:28 - 0 plus beta 1 * X so you can see that we
125:32 - are getting rid of this e and this is
125:34 - simply because of a mathematical
125:36 - expression that says if we take the log
125:38 - of the e to the power something then we
125:41 - end up with only the exponent part in it
125:44 - though this is out of the scope of this
125:45 - course to look into the mathematical
125:47 - derivation of this formula I will
125:49 - include many resources regarding this
125:51 - logarithm the Transformations and the
125:53 - mathematics behind it just in case you
125:55 - want to look into those details and do
125:58 - some uh extra
126:02 - learning so logistic regression uses the
126:05 - log ODS as the dependent variable and
126:07 - the independent variables are used to
126:09 - predict this log ODS the coefficient of
126:11 - the independent varibles represent then
126:13 - the change in the log OD for a one unit
126:15 - change in the independent variable so
126:18 - you might that in the linear regression
126:19 - we were modeling the actual dependent
126:21 - variable in case of logistic regression
126:24 - the difference is that we are modeling
126:26 - the
126:29 - logas another important Concept in
126:31 - logistic regression is the likelihood
126:33 - function the likelihood function is used
126:35 - to estimate the parameters of the model
126:37 - given the observed data sometimes during
126:40 - the interviews you might also be asked
126:41 - to write down the exact likelihood
126:43 - formula or the log likelihood function
126:46 - so I would definitely suggest you to
126:48 - memorize this one and to understand all
126:50 - the components included in this formula
126:53 - the likelihood function describes the
126:55 - probability of The observed data given
126:57 - the parameters of the model and if you
126:59 - follow the lecture of the probability
127:01 - density functions in the section of
127:03 - fundamentals to statistics you might
127:05 - here even recognize the bar noly PDF
127:08 - since the likelihood function here is
127:10 - based on the probability Mass function
127:12 - of a Baro distribution which is a
127:15 - distribution of a binary outcome So This
127:17 - is highly applicable to the case where
127:20 - we have only two categories in our
127:22 - dependent variable and we are trying to
127:24 - estimate the probability of observation
127:26 - to belonging to one of those two classes
127:29 - so this is the L likelihood function and
127:32 - this is the likelihood function we start
127:34 - with the likelihood function and the L
127:37 - the capital letter L stands for the
127:39 - likelihood function the L is equal the
127:41 - likelihood function L is equal to
127:43 - product across all pair of these
127:46 - multipliers so we have Peak side to the
127:48 - power Yi multiplied by 1 - pxi to^ 1 - y
127:53 - i where pxi is the PX that we just so
127:57 - only for observation I and the Yi is
128:00 - simply the class so Yi will either be
128:02 - equal to zero or one so Yi is equal to 1
128:06 - then 1 minus Yi is equal to zero so we
128:10 - every time we are looking into the
128:12 - probability of observation belonging to
128:15 - the first class multiply by the
128:17 - probability of observation not belonging
128:19 - to that plus and we take this cross
128:21 - multiplications and we do that for all
128:24 - the observations that are included in
128:26 - our data and this also comes from
128:28 - mathematics so this stands for the
128:31 - product so uh given that it's harder to
128:34 - work with products compared to the sums
128:37 - we then apply the Lo likelihood uh
128:39 - transformation in order to obtain the Lo
128:42 - likelihood function instead of
128:43 - likelihood function so when we apply
128:46 - this log transformation so we take the L
128:48 - logarithm of this expression we end up
128:50 - with this log likelihood expression and
128:53 - here again one more time we are making
128:55 - use of a mathematical property which
128:57 - says that if we take the logarithm of
128:59 - the products we end up with the sum of
129:02 - the logarithms so we go from the
129:03 - products to the sums I will also include
129:06 - resources regarding that such that you
129:07 - can also learn the mathematics Behind
129:09 - These
129:10 - Transformations so the L likelihood with
129:13 - a lowercase L is equal to logarithm of
129:17 - the products p i^ y i * 1 - PX i^ 1 - Yi
129:23 - and when we apply that mathematical
129:24 - transformation then the L is equal to
129:27 - sum across all observation I is equal to
129:29 - 1 till M and then y i so the power the
129:32 - exponent comes to the front Yi *
129:36 - logarithm of the pxi plus 1 - Yi *
129:39 - logarithm of 1 -
129:42 - pxi while for linear regression we use
129:45 - OLS as estimation technique for logis
129:48 - regression in other estimation technique
129:50 - should be used the reason why we cannot
129:53 - use OLS in logistic regression to find
129:55 - the best fitting line is because the
129:57 - errors can become very large or very
130:00 - small and sometimes even negative in
130:03 - case of logistic aggression while for
130:05 - logistic regression we aim for predicted
130:07 - value between zero and
130:09 - one therefore for logistic regression we
130:13 - need to use estimation technique called
130:15 - maximum likelihood estimation or in
130:17 - short mle where the likelihood function
130:21 - calculates the probability of observing
130:23 - the data outcome given the input data in
130:25 - the model we just saw the likel function
130:28 - in the previous slide this function is
130:30 - then optimized to find the set of
130:33 - parameters that result in the largest
130:35 - sum likelihood so the maximum likelihood
130:38 - over the training data
130:41 - set logistic function will always
130:43 - produce this s-shaped curve regardless
130:46 - of the value of independent variable
130:48 - able X resulting in sensible estimation
130:51 - most of the time so value between 0 and
130:53 - one so as you can see this s-shaped cure
130:57 - is what characterizes the maximum
130:58 - likelihood estimation corresponding to
131:01 - the logistic regression and it will
131:03 - always provide not come between zero and
131:09 - one then the idea behind the maximum
131:12 - likelihood estimation is to find a set
131:14 - of estimates that would maximize the
131:16 - likelihood function
131:18 - so let's go through the maximum
131:20 - likelihood estimation step by step what
131:23 - we need to do first is to define a
131:25 - likelihood function the first step is to
131:27 - always Define this function for the
131:29 - model secondly we need to write the log
131:32 - likelihood function so the next step is
131:34 - to take the natural logarithm of the
131:36 - likelihood function to obtain the log
131:39 - likelihood
131:40 - function so I'm talking about this
131:45 - one the L likel function is a more
131:48 - convenient and computationally efficient
131:50 - function to work with and what we need
131:53 - to do next is to find the maximum of
131:55 - this L like function so this step
131:58 - consists of finding the values of the
132:00 - parameters beta 0 and beta 1 that
132:02 - maximize the L lik function there are
132:05 - many optimization algorithms that can be
132:07 - used to find the maximum but these are
132:10 - out of the scope of this course and you
132:12 - don't need to know them as part of
132:13 - becoming a data scientist and entering
132:16 - data science field in the fourth step we
132:19 - need to estimate the parameters so we
132:21 - are talking about the beta 0 and beta 1
132:24 - once the maximum of the log likel
132:26 - function is found the values of the
132:28 - parameters that correspond to the
132:30 - maximum are considered the maximum
132:32 - likelihood estimate of the
132:34 - parameters and then in the next step we
132:36 - need to check the model Feit so once the
132:39 - maximum likelihood estimates are
132:40 - obtained we can check the goodness of
132:43 - fit of the model by calculating
132:45 - information criteria such as AIC B Bic
132:48 - or R squ where AIC stands for akas
132:51 - information criteria Bic stands for bi
132:54 - information criteria and r s refers to
132:58 - the same evaluation value that we use
133:00 - for evaluating linear
133:02 - regression in the final step we need to
133:05 - make predictions and evaluate the model
133:08 - using the maximum likelihood estimates
133:10 - the model can be used to make
133:11 - predictions on a new unseen data and the
133:15 - performance of the model can be then
133:16 - evaluated using various evaluation
133:19 - metrics such as accuracy precision and
133:23 - recall those are metrics that we have
133:25 - Revisited as part of the very initial
133:27 - lecture in the section and those are
133:29 - metrics that you need to know so unlike
133:32 - the AIC Bic that we just spoke about
133:35 - that evaluates the goodness of feed of
133:37 - the very initial estimates that come
133:38 - from the maximum likelihood the accuracy
133:41 - and precision and the recall evaluate
133:43 - the final model so the values that we
133:46 - get for the nuan in data when we make
133:48 - the predictions and we get the
133:51 - classes and those are metrics that you
133:53 - need to know if you're wondering what
133:55 - this accuracy is what this Precision
133:57 - recall is as well as the F1 score make
134:00 - sure to head towards the very initial
134:02 - lecture in this section where we talked
134:04 - about the exact definition of this
134:08 - metrix let's finally discuss the
134:10 - advantages and the disadvantages of the
134:12 - logistic regression so some of the
134:15 - advantages of logistic regressions are
134:17 - that it's a simple model it has a low
134:20 - variance it has a low bias and it
134:23 - provides probabilities some of the
134:25 - disadvantages of logistic regressions
134:27 - are logistic regression is unable to
134:30 - model nonlinear relationship so one of
134:32 - the key assumptions that logistic
134:34 - regression is making is that there is a
134:36 - linear relationship between your
134:38 - independent variable and your dependent
134:41 - variable logistic regression is also
134:43 - unstable when your classes are well
134:46 - separable as well logistic agression
134:49 - becomes very unstable when you have more
134:51 - than two classes so this means whenever
134:54 - you have more than two categories in
134:56 - your dependent variable or whenever your
134:58 - classes are well separable using
135:01 - logistic regression for classification
135:03 - purposes will not be very smart so
135:05 - instead you should look for other models
135:08 - that you can use for this task and one
135:10 - of such models is linear discriminate
135:12 - analysis so the LDA that we will
135:15 - introduce in the next lecture so this is
135:18 - all for this lecture where we have
135:19 - looked into the logistic regression and
135:21 - the maximum likelihood estimation in the
135:24 - next lecture we will look into the LDA
135:27 - so stay tuned and I will see you in the
135:28 - next
135:34 - lecture looking to step into machine
135:36 - learning or data science it's about
135:39 - starting somewhere practical yet
135:41 - powerful and as the simple yet most
135:43 - popular machine learning algorithm
135:45 - linear regression linear aggression
135:48 - isn't just a jargon it's a tool that is
135:50 - used both for a finding out what are the
135:53 - most important features in your data as
135:55 - well as being used to forecast the
135:57 - future that's your starting point in the
136:00 - Journey of data science and Hands-On
136:02 - machine learning uh work embark on a
136:06 - handson data science and machine
136:08 - learning project where we are going to
136:10 - find what are the drivers of Californian
136:12 - house prices you will clean the data
136:15 - visualize the key trends you will learn
136:18 - how to process your data and how to use
136:21 - different python libraries to understand
136:23 - what are those drivers of Californian
136:25 - house values you're are going to learn
136:28 - how to implement linear regression in
136:31 - Python and learn all these fundamental
136:33 - steps that you need in order to conduct
136:35 - a proper handson data science project at
136:39 - the end of this project you will not
136:40 - only learn those different python
136:42 - libraries when it comes to data science
136:44 - and machine learning such as pandas
136:46 - psyit learn tou models medf Le curn but
136:51 - you will also be able to put this
136:53 - project on your person website and on
136:56 - your resume a point size stepbystep case
136:59 - study and approach to build your
137:01 - confidence and expertise in machine
137:04 - learning and in data science in this
137:06 - part we are going to talk about a case
137:08 - study in the field of Predictive
137:10 - Analytics and causal analysis so we are
137:13 - going to use this simple yet powerful
137:16 - regression technique called your
137:17 - regression in order to perform causal
137:20 - analysis and Predictive Analytics so by
137:23 - causal analysis I mean that we are going
137:26 - to look into this correlations clation
137:28 - and we're trying to figure out what are
137:31 - the features that have an impact on the
137:35 - housing price on the house value so what
137:37 - are these features that are describing
137:40 - the house that Define and cause the
137:43 - variation in the uh house prices the
137:46 - goal of this case study is to uh
137:49 - practice linear regression model and to
137:52 - get this first feeling of how uh you can
137:55 - use a machine learning model a simple
137:58 - machine learning model in order to
137:59 - perform uh model training model
138:02 - evaluation and also use it for causal
138:05 - analysis where you are trying to
138:07 - identify features that have a
138:09 - statistically significant impact on your
138:12 - response variable so on your dependent
138:14 - variable so here is the step-by-step
138:16 - process that we are going to follow in
138:18 - order to find out what are the features
138:21 - that Define the Californian house values
138:25 - so first we are going to understand what
138:27 - are the set of independent variables
138:29 - that we have we're also going to
138:31 - understand what is the response variable
138:33 - that we have so for our multiple linear
138:36 - regression model we are going to
138:38 - understand what are this uh techniques
138:41 - that we uh need and what are the
138:43 - libraries in Python that we need to load
138:45 - in order to be able to conduct this case
138:47 - study so first we are going to load all
138:49 - these libraries and we are going to
138:51 - understand why we need them then we are
138:53 - going to conduct data loading and data
138:55 - preprocessing this is a very important
138:58 - step and I deliberately didn't want you
139:00 - to skip this and didn't want you to give
139:03 - you the clean data cuz uh usually in
139:05 - normal real Hands-On data science job
139:08 - you won't get a clean data you will get
139:10 - a dirty data which will contain missing
139:13 - values which will contain outliers and
139:15 - those are things that you need to handle
139:18 - before you proceed to the actual and F
139:21 - part which is the modeling and the uh
139:24 - analysis so therefore we are going to do
139:27 - missing data analysis we are going to
139:29 - remove the missing data from our
139:32 - Californian house price data we are
139:34 - going to conduct outlier detection so we
139:37 - are going to identify outliers we are
139:39 - going to learn different techniques that
139:41 - you can use visualization uh techniques
139:44 - uh in Python that you can use in order
139:47 - to identify outliers and then remove
139:49 - them from your data then we are going to
139:52 - perform data visualization so we are
139:54 - going to explore the data and we are
139:56 - going to do different plots to learn
139:58 - more about the data to learn more about
140:00 - this outliers and different statistical
140:03 - techniques uh combined with python so
140:07 - then we are going to do correlation
140:08 - analysis to identify some problematic
140:11 - features which is something that I would
140:13 - suggest you to do independent the nature
140:15 - of your case study to understand
140:17 - understand what kind of variables you
140:18 - have what is the relationship between
140:20 - them and whether you are dealing with
140:22 - some potentially problematic
140:24 - variables so then we will be uh moving
140:27 - towards the fun part which is performing
140:30 - the uh multiple theine regression in
140:32 - order to perform the caal NES which
140:35 - means identifying the features in the
140:38 - Californian house blocks that Define the
140:41 - value of the Californian
140:44 - houses so uh finally we will do very
140:47 - quickly another uh implementation of the
140:50 - same multiple uh multiple linear
140:52 - regression in order to uh give you not
140:56 - only one but two different ways of
140:58 - conducting linear regression because
141:00 - linear regression can be used not only
141:02 - for caal analysis but also as a
141:04 - standalone a common machine learning
141:07 - regression type of model therefore I
141:10 - will also tell you how you can use psych
141:12 - learn as a second way of training and
141:15 - then predicting the C for house
141:18 - values so without further Ado let's get
141:21 - started once you become a DAT a
141:23 - scientist or machine learning researcher
141:25 - or machine learning engineer there will
141:27 - be some cases some Hands-On uh data
141:30 - science projects where the business will
141:32 - come to you and we'll tell you well here
141:35 - we have this data and we want to
141:37 - understand what are these features that
141:39 - have the biggest influence on this Auto
141:41 - factor in this specific case in our case
141:44 - study um let's assume we have a client
141:47 - that uh is interested in identifying
141:51 - what are the features that uh Define the
141:54 - house price so maybe it's someone who
141:56 - wants to um uh invest in uh houses so
142:02 - it's someone who is interested in buying
142:04 - houses and maybe even renovating them
142:07 - and then reselling them and making a
142:08 - profit in that way or maybe in the
142:10 - long-term uh investment Market when uh
142:14 - people are buying real estate in a way
142:16 - of uh in inting in it and then longing
142:18 - for uh holding it for a long time and
142:20 - then uh selling it later or for some
142:23 - other purposes the end goal in this
142:25 - specific case uh for a person is to
142:28 - identify what are this features of the
142:31 - house that makes this house um to be
142:36 - priced at a certain level so what are
142:38 - the features of the house that are
142:40 - causing the price and the value of the
142:44 - house so we are going to make use of
142:47 - this very popular data set that is
142:49 - available on kagal and it's originally
142:51 - coming from psyit learn and is called
142:53 - California housing prices I'll also make
142:57 - sure to put the link uh of this uh
143:00 - specific um data set uh both in my
143:03 - GitHub account uh under this repository
143:06 - that will be dedicated for this specific
143:08 - case study as well as um I will also
143:11 - point out the additional links that you
143:14 - can use to learn more about this data
143:16 - set so uh this data set is derived from
143:19 - 1990 um US Census so United uh States
143:22 - census using one row Paris sensus block
143:27 - so a Blog group or block is the smallest
143:30 - uh geographical unit for which the US
143:33 - cus Bureau publishes sample data so a
143:36 - Blog group typically has a population of
143:39 - 600 to 3,000 people who are living there
143:44 - so a household is a group of people
143:46 - residing with within a single home uh
143:49 - since the average number of rooms and
143:50 - bedrooms in this data set are provided
143:53 - per household this conss may be um May
143:56 - take surprisingly large values for blog
143:59 - groups with few households and many
144:02 - empty houses such as Vacation
144:05 - Resorts so
144:08 - um let's now look into uh the variables
144:12 - that are available in this specific data
144:14 - set so uh what we have here is the med
144:17 - Inc which is the median income in blog
144:20 - group so uh this um touches the uh
144:23 - financial side and uh Financial level of
144:27 - the uh block uh block of
144:30 - households then we have House age so
144:32 - this is the median house age in the
144:34 - block group uh then we have average
144:37 - rooms which is the average number of
144:39 - rooms uh per
144:41 - household and then we have average
144:43 - bedroom which is the average number of
144:45 - bedrooms per household then we have
144:48 - population which is the uh blog group
144:51 - population so that's basically like we
144:53 - just saw that's the number of people who
144:55 - live in that
144:56 - block then we have a uh o OU uh which is
145:01 - basically the average number of
145:03 - household
145:04 - members uh then we have latitude and
145:07 - longitude which are the latitude and
145:09 - longitude of this uh block group that we
145:12 - are looking into so as you can see here
145:15 - we are dealing with aggregate data so we
145:18 - don't have the uh the data per household
145:22 - but rather the data is calculated and
145:25 - average aggregated based on a block so
145:28 - this very common in data science uh when
145:31 - we uh want to reduce the dimension of
145:33 - the data and when we want to have some
145:35 - sensible numbers and create this
145:37 - crosssection data and uh cross-section
145:40 - data means that we have multiple
145:43 - observations for which we have data on a
145:45 - single time period period in this case
145:48 - we are using as an aggregation unit the
145:51 - block and uh we have already learned as
145:54 - part of the uh Theory lectures this idea
145:58 - of median so we have seen that there are
146:00 - different descriptive measures that we
146:02 - can use in order to aggregate our data
146:05 - one of them is the mean but the other
146:07 - one is the median and often times
146:09 - especially if we are dealing with skute
146:11 - distribution so if we have a
146:13 - distribution that is not symmetric but
146:15 - it's rather right cuute or left skewed
146:18 - then we need to use this idea of median
146:20 - because median is then better
146:21 - representation of this um uh scale of
146:25 - the data um compared to the mean and um
146:30 - in this case we will soon see when
146:32 - representing and visualizing this data
146:34 - that we are indeed dealing with a skewed
146:37 - data so um this basically a very simple
146:42 - a very basic data set with not too many
146:44 - features so great um way to uh get your
146:48 - hands uh uh on with actual machine
146:51 - learning use case uh we will be keeping
146:54 - it simple but yet we will be learning
146:56 - the basics and the fundamentals uh in a
146:59 - very good way such that uh learning more
147:03 - um difficult and more advanced machine
147:05 - learning models will be much more easier
147:06 - for you so let's now get into the actual
147:09 - coding part so uh here I will be using
147:11 - the Google clap so I will be sharing the
147:15 - link to this notebook uh combined with
147:17 - the data in my python for data science
147:20 - repository and you can make use of it in
147:22 - order to uh follow this uh tutorial uh
147:26 - with me so uh we always start with
147:29 - importing uh libraries we can run a l
147:32 - regression uh manually without using
147:35 - libraries by using matrix
147:37 - multiplication uh but I would suggest
147:40 - you not to do that you can do it for fun
147:43 - or to understand this metrix
147:44 - multiplication the linear algebra behind
147:47 - the linear regression but uh if you want
147:50 - to um get handson and uh understand how
147:54 - you can use the new regression like you
147:57 - expect to do it on your day-to-day job
147:59 - then you expect to use um instead
148:02 - libraries such as psychic learn or you
148:04 - can also use the statsmodels.api
148:08 - libraries in order to understand uh this
148:11 - topic and also to get handson I decided
148:14 - to uh showcase this example not only in
148:17 - one library in Cy thir but also the
148:20 - starts models and uh the reason for this
148:23 - is because many people use linear
148:26 - regression uh just for Predictive
148:28 - Analytics and for that using psyit learn
148:31 - this is the go-to option but um if you
148:34 - want to use linear regression for causal
148:37 - analysis so to identify and interpret
148:40 - this uh features the independent
148:42 - variables that have a statistically
148:44 - significant impact on your response
148:45 - variable and then you will need to uh
148:49 - use another Library a very handy one for
148:52 - linear regression which is called uh
148:53 - stats models. API and from there you
148:57 - need to import the SM uh functionality
149:01 - and this will help you to do exactly
149:03 - that so later on we will see how nicely
149:06 - this Library will provide you the
149:08 - outcome exactly like you will learn on
149:10 - your uh traditional econometrics or
149:13 - introduction to linear regression uh
149:15 - class so I'm going to give you all this
149:18 - background information like no one
149:20 - before and we're going to interpret and
149:22 - learn everything such that um you start
149:25 - your machine Learning Journey in a very
149:27 - proper and uh in a very um uh high
149:32 - quality way so uh in this case uh first
149:37 - thing we are going to import is the
149:38 - pendence library so we are importing
149:40 - pendis Library as PD and then non pile
149:43 - Library as NP we are going to need
149:45 - pendes uh just to uh create a pendis
149:49 - data frame to read the data and then to
149:51 - perform data wrangling to identify the
149:54 - missing data outliers so common data
149:56 - wrangling and data prosessing steps and
149:59 - then we are going to use npy and npy is
150:02 - a common way to uh use whenever you are
150:05 - visualizing data or whenever you are
150:08 - dealing with metrices or with arrays so
150:11 - pandas and nonp are being used
150:14 - interchangeably so then we are going to
150:16 - use meth plot lip and specifically the
150:19 - PIP plat from it uh and this library is
150:22 - very important um when you want to
150:25 - visualize a data uh then we have cburn
150:29 - um which uh is another handy data
150:32 - visualization library in Python so
150:34 - whenever you want to visualize data in
150:36 - Python then methot leip and Cy uh cburn
150:40 - there are two uh very handy data
150:42 - visualization techniques that you must
150:44 - know if you like this um cooler
150:48 - undertone of colors the Seaburn will be
150:50 - your go-to option because then the
150:52 - visualizations that you are creating are
150:54 - much more appealing compared to the med
150:56 - plot Le but the underlying way of
150:58 - working so plotting scatter plot or
151:01 - lines or um heat map they are the
151:05 - same so then we have the STS mods. API
151:09 - uh which is the library from which we
151:11 - will be importing the uh as uh that is
151:14 - the temple uh linear regression model
151:16 - that we will be using uh for our caal
151:19 - analysis uh here I'm also importing the
151:23 - uh from Psychic learn um linear model
151:26 - and specifically the linear regression
151:28 - model and um this one uh is basically
151:32 - similar to this one you can uh use both
151:34 - of them but um it is a common um way of
151:39 - working with machine learning model so
151:41 - whenever you are dealing with Predictive
151:42 - Analytics so we you are using the data
151:45 - not for uh identifying features that
151:47 - have a statistically significant impact
151:49 - on the response variable so features
151:51 - that have an influence and are causing
151:54 - the dependent variable but rather you
151:56 - are just interested to use the data to
151:59 - train the model on this data and then um
152:03 - test it on an unseen data then uh you
152:06 - can use pyit learn so psyit learn will
152:10 - uh will be something that you will be
152:12 - using not only for linear regression but
152:14 - also for a machine learning model I
152:16 - think of uh Canon um logistic regression
152:21 - um random Forest decision trees um
152:25 - boosting techniques such as light GBM
152:28 - GBM um also clustering techniques like K
152:32 - means DB scan anything that you can
152:34 - think of uh that fits in in this
152:36 - category of traditional machine learning
152:38 - model you will be able to find Ayler
152:41 - therefore I didn't want you to limit
152:43 - this tutorial only to the S models which
152:46 - we could do uh if we wanted to use um if
152:50 - we wanted to have this case study for uh
152:52 - specifically for linear regression which
152:54 - we are doing but instead I wanted to
152:56 - Showcase also this usage of psychic
152:58 - learn because pyic learn is something
153:00 - that you can use Beyond linear
153:02 - regression so for all these added type
153:04 - of machine learning models and given
153:07 - that this course is designed to
153:09 - introduce you to the world of machine
153:10 - learning I thought that we will combine
153:12 - this uh also with psychic learning
153:14 - something that you are going to see time
153:17 - and time again when you are uh using
153:19 - python combined with machine
153:22 - learning so then I'm also uh importing
153:26 - the uh training test plate uh from the
153:29 - psychic learn model selection such that
153:32 - we can uh split our data into train and
153:35 - test now uh before we move into uh the
153:41 - uh actual training and testing we need
153:43 - to first load our data so so therefore
153:47 - uh what I did was to uh here uh in this
153:51 - sample data so in a folder in Google
153:53 - collab I uh put it this housing. CSV
153:56 - data that's the data that you can
153:58 - download uh when you go to this specific
154:01 - uh page so uh when you go here um then
154:05 - uh you can also uh download here that
154:08 - data so download 49 kab of this uh
154:12 - housing data and that's exactly what I'm
154:15 - uh downloading and then uploading here
154:18 - in Google clap so this housing. CSV in
154:22 - this folder so I'm copying the path and
154:24 - I'm putting it here and I'm creating a
154:27 - variable that holds this um name so the
154:32 - path of the data so the file uncore path
154:35 - is the variable string variable that
154:37 - holds the path of the data and then what
154:40 - I need to do is that I need to uh take
154:44 - this file uncore path and and I need to
154:46 - put it in the pd. read CSV uh which is a
154:50 - function that we can use in order to uh
154:53 - load data so PD stands for pandas the
154:57 - short way of uh naming pandas uh PD do
155:02 - and then read uncore CSV is the function
155:04 - that we are taking from Panda's library
155:06 - and then within the parentheses we are
155:08 - putting the file uncore path if you want
155:11 - to learn more about this Basics or
155:13 - variable different data structures some
155:15 - basic python for data science then um to
155:19 - ensure that we are keeping this specific
155:21 - tutorial structured I will not be
155:23 - talking about that but feel free to
155:25 - check the python for data science course
155:28 - and I will put the link um in the
155:29 - comments below such that you can uh
155:33 - learn that if you don't know yet and
155:35 - then you can come back to this tutorial
155:37 - to learn how you can use python in
155:39 - combination with linear regression so uh
155:43 - the first thing that I tend to do before
155:45 - moving on to the the actual execution
155:47 - stage is to um look into the data to
155:50 - perform data exploration so what I tend
155:53 - to do is to look at the data field so
155:56 - the name of the variables that are
155:58 - available in the data and that you can
156:01 - do by doing data. columns so you will
156:04 - then look into the columns in your data
156:07 - this will be the name of your uh uh data
156:10 - fields so let's go ahead and do command
156:13 - enter so we see that we have longitude
156:15 - like attitude housing unor median age we
156:18 - have total rooms we have total bedrooms
156:21 - population so basically the the um
156:24 - amount of people who are living in the
156:25 - in those households and in those houses
156:28 - then we have households then we have
156:30 - median income we have median housecore
156:32 - value and we have ocean proximity now
156:37 - you might notice that the name of these
156:39 - variables are a bit different than in
156:41 - the actual um documentation of the
156:45 - California house so you see here the
156:47 - naming is different but the underlying
156:50 - uh explanation is the same so here they
156:53 - are just trying to make it uh nicer and
156:57 - uh represent it in a better uh naming
157:00 - but uh it is a common um thing to see in
157:04 - Python when we are dealing with uh data
157:06 - that uh we have this underscores in the
157:09 - name approvation so we have housing
157:12 - uncore median AG which in this case you
157:15 - can see that it says house um age so bit
157:20 - different but their meaning is the same
157:22 - this is still the median house age in
157:24 - the block group so uh one thing uh that
157:28 - you can also uh notice here is that the
157:32 - um in the official uh documentation we
157:35 - don't have this um one extra variable
157:38 - that we have here which is the ocean
157:41 - proximity and this basically uh
157:43 - describes the uh Clos cless of the house
157:47 - from the ocean which of course uh for
157:50 - some people can definitely mean a
157:52 - increase or decrease in the house price
157:55 - so I basically um we have all these
157:59 - variables and next thing that I tend to
158:02 - do is to look into the actual data and
158:05 - one thing that we can do is just to look
158:07 - at the um top 10 rows of the data
158:10 - instead of printing the entire uh data
158:13 - frame so when we go and uh execute this
158:17 - specific part of the code and the
158:19 - command you can see that here we have
158:21 - the top 10 rows uh of our data so we
158:25 - have the longitude the latitude we have
158:27 - the housing median age you can see we
158:29 - are see some 41 year 21 year 52 year
158:33 - basically the number of years that a
158:35 - house the median age of the house is 41
158:39 - 21 52 and this is per
158:42 - block then we have the number of total
158:44 - bedrooms so we see that uh we have um in
158:49 - this blog uh the total number of rooms
158:52 - that this houses have is
158:54 - 7,99 so we are already seeing a data
158:57 - that consists of these large numbers
159:00 - which is something to take into account
159:01 - when uh you are dealing with machine
159:03 - learning models and especially with line
159:05 - regression then we have total bedrooms
159:08 - um and we have then population
159:11 - households median income median house
159:14 - value and the ocean
159:16 - proximity one thing that you can see
159:18 - right of the bed is that uh we have
159:21 - longitude and latitude uh which have
159:24 - some uh unique uh
159:27 - characteristics um and longitude is with
159:30 - minuses latitude is with pluses uh but
159:34 - that's fine for the linear regression
159:35 - because what it is basically looking is
159:38 - uh whether a variation in certain
159:41 - independent variables in this case
159:43 - longitude and latitude but that will
159:45 - cause a change in the dependent variable
159:47 - so just to refresh our memory what this
159:49 - linear regression will do in this case
159:52 - um so we are dealing with multiple inine
159:54 - regression because we have more than one
159:56 - independent variables so we have as
159:59 - independent variables those different
160:00 - features that describe the house except
160:03 - of the house price because median house
160:06 - value is the dependent variable so
160:09 - that's basically what we are trying to
160:10 - figure out we want to see what are the
160:13 - features of the house that cause so
160:19 - Define the house price we want to
160:21 - identify what are um the features that
160:24 - cause a change in our dependent variable
160:27 - and specifically uh what is the uh
160:30 - change in our median house price uh
160:35 - volue if we apply a one unit change in
160:39 - our independent feature so if we have a
160:42 - multiple linear regession we have
160:44 - learned during the theory lecture that
160:46 - what linear regression tries to use
160:47 - during causal analysis is that it tries
160:50 - to keep all the independent variables
160:52 - constant and then investigate for a
160:54 - specific independent variable what is
160:56 - this one unit uh change uh increase uh
160:59 - in the specific independent variable
161:02 - will result in what kind of change in
161:04 - our dependent variable so if we for
161:07 - instance change by one unit our uh
161:11 - housing median age um then what will be
161:14 - the correspond in change in our median
161:18 - household value keeping everything else
161:21 - concent so that's basically the idea
161:24 - behind multi multiple linear regression
161:26 - and using that for this specific use
161:28 - case and in here um what we also want to
161:32 - do is to find out what are the uh data
161:35 - types and whether we can learn bit more
161:37 - about our data before proceeding to the
161:40 - next step and for that I tend to use
161:42 - this uh info uh function in panel
161:46 - so given that the data is a penis data
161:48 - frame I will just do data. info and then
161:51 - parentheses and then this will uh show
161:54 - us what is the data type and what is the
161:57 - number of new values per
162:00 - variable so um as we have already
162:03 - noticed from this header which we can
162:06 - also see here being confirmed that ocean
162:08 - proximity is a variable that is not a
162:11 - numeric value so here you can see nearby
162:14 - um also a value for that variable which
162:18 - unlike all the other values is
162:20 - represented by a string so this is
162:23 - something that we need to take into
162:24 - account because later on when we uh will
162:26 - be doing the data prop processing and we
162:28 - will actually uh
162:30 - actually run this model we will need to
162:33 - do something with this specific variable
162:35 - we need to process it so um for the rest
162:39 - we are dealing with numeric variables so
162:42 - you can see here that longitude latitude
162:45 - or all the other variables including our
162:47 - dependent variable is a numeric variable
162:50 - so float
162:51 - 64 the only variable that needs to be
162:54 - taken care of is this ocean uncore
162:57 - proximity uh which um we can actually
163:01 - later on also see that is um categorical
163:04 - string variable and what this basically
163:06 - means is that it has these different
163:08 - categories so um for instance uh let us
163:13 - actually do that in here very quickly
163:16 - so let's see what are all the unique
163:19 - values for this variable so if we take
163:22 - the name of this variable so we copied
163:26 - from this overview in here and we do
163:34 - unique then this should give us the
163:37 - unique values for this categorical
163:40 - variable so here we go so we have
163:43 - actually five different unique values
163:46 - for this categorical string variable so
163:49 - this means that this ocean proximity can
163:51 - take uh five different values and it can
163:54 - be either near Bay it can be less than 1
163:57 - hour from the ocean it can be Inland it
163:59 - can be near Ocean and it can be uh in
164:02 - the Iceland what this means is that we
164:04 - are dealing with a feature that
164:06 - describes the distance uh of the block
164:09 - from the ocean and here the underlying
164:12 - idea is that maybe this specific feature
164:16 - has a statistically significant impact
164:18 - on the house value meaning that it might
164:21 - be possible that for some people um in
164:26 - certain areas or in certain countries
164:29 - living in the uh nearby the ocean uh
164:32 - will be increasing the value of the
164:35 - house so if there is a huge demand for
164:37 - houses which are near the ocean so
164:39 - people prefer to uh leave near the ocean
164:42 - then most likely there will be a
164:44 - positive relationship
164:46 - if there is a uh negative relationship
164:49 - then it means that uh people uh if uh in
164:54 - that area in California for instance
164:56 - people do not prefer to live near the
164:58 - ocean then uh we will see this negative
165:01 - relationship so we can see that um if we
165:05 - increase uh the uh if if people uh if
165:08 - the house is in the uh um area that is
165:13 - uh not close to Ocean so further from
165:15 - the ocean then the house value will be
165:18 - higher so this is something that we want
165:19 - to figure out with this line regression
165:21 - we want to understand what are the
165:23 - features that uh Define the value of the
165:27 - house and we can say that um if the
165:30 - house has those characteristics then
165:33 - most likely the house price will be
165:35 - higher or the house price will be lower
165:38 - and uh linear aggression helps us to not
165:41 - only understand what are those features
165:43 - but also to understand how much higher
165:45 - or how much lower will be the value of
165:48 - the house if we have the certain
165:50 - characteristics and if we increase the
165:52 - certain characteristics by one unit so
165:55 - next we are going to look into uh the
165:57 - missing data in our data so in order to
166:00 - have a proper machine learning model we
166:02 - need to do some uh data processing so
166:05 - for that what we need to do is we need
166:07 - to check for the uh missing values in
166:10 - our data and we need to understand what
166:13 - is this amount of new values per data
166:17 - field and this will help us to
166:19 - understand whether uh we can uh remove
166:22 - some of those missing values or we need
166:24 - to do
166:25 - imputation so depending on the amount of
166:29 - missing data that we got in our data we
166:32 - can then understand which all those
166:34 - Solutions we need to take so here we can
166:37 - see that uh we don't have any n values
166:40 - when it comes to longitude latitude
166:41 - housing median age and all the other
166:44 - variables except of one variable one
166:47 - independent variable and that's the
166:49 - total bedrooms so we can see that um out
166:52 - of all the observations that we got the
166:55 - total uh underscore bedrooms variable
166:57 - has 207 cases when we do not have the
167:02 - corresponding uh
167:04 - information so when it comes to
167:06 - representing this numbers in percentages
167:09 - which is something that you should do as
167:11 - your next step we can see that um out of
167:14 - uh the entire data set uh for total
167:18 - underscore bedrooms variable um only 1.
167:22 - n n uh 3% is missing now this is really
167:25 - important because by simply looking at
167:28 - the number of times the uh number of
167:31 - missing uh observations perir data field
167:34 - this won't be helpful for you because
167:37 - you will not be able to understand
167:39 - relatively how much of the data is
167:41 - missing now if you have for a certain
167:44 - variable 50% missing or 80% missing then
167:47 - it means that for majority of your house
167:50 - blocks you don't have that information
167:53 - and including that will not be
167:54 - beneficial for your Morel nor will be it
167:57 - accurate to include it and it will
167:58 - result in biased uh Morel because if you
168:02 - have for the majority of observations uh
168:05 - no information and for certain
168:07 - observations you do that inform you have
168:09 - that information then you will
168:11 - automatically skew your results and you
168:13 - will have biased results
168:15 - therefore if you have uh for the
168:18 - majority of your um data set that
168:22 - specific uh variable missing then I
168:25 - would suggest you choose just to drop
168:27 - that independent variable in this case
168:30 - we have just one uh% uh of the uh house
168:34 - blocks missing that information which
168:37 - means that this gives me confidence that
168:40 - uh I would rather keep this independent
168:43 - variable and just to drop those
168:44 - observations that do not have a total uh
168:47 - underscore bedrooms uh information now
168:51 - another solution could also be is uh to
168:54 - instead of dropping that entire
168:55 - independent variable is just to uh use
168:58 - some sort of imputation technique so uh
169:02 - what this means is that uh we will uh
169:05 - try to find a way to systematically find
169:09 - a replacement for that missing value so
169:11 - we can use mean imputation median imput
169:14 - ation or more model based more advanced
169:17 - statistical or econometrical approaches
169:19 - to perform imputation so for now this
169:22 - out of the scope of this problem but I
169:24 - would say look at the uh percentage of
169:28 - uh observations that for which this uh
169:31 - independent variable has missing uh
169:32 - values if this is uh low like less than
169:35 - 10% and you have a large data set then
169:39 - uh you should uh be comfortable dropping
169:42 - those observations but if you have a
169:45 - small data set so you got only 100
169:47 - observations and for them like 20% or
169:50 - 40% is missing then consider from
169:53 - imputation so try to find the values
169:56 - that can be um used in order to replace
169:59 - those missing
170:01 - values now uh once we have this
170:04 - information and we have identified the
170:06 - missing values the next thing is to uh
170:08 - clean the data so here what I'm doing is
170:11 - that I'm using the data that we got and
170:13 - I'm using the function drop na which
170:16 - means drop the um uh observations where
170:21 - the uh value is missing so I'm dropping
170:24 - all the observations for which the total
170:26 - underscore bedrooms has a null value so
170:30 - I'm getting rid of my missing
170:32 - observations so after doing that I'm
170:34 - checking whether I got rid of my missing
170:36 - observations and you can see here that
170:38 - when I'm printing data do is n do sum so
170:42 - I'm summing up the number of uh Missing
170:46 - observations no values per uh variable
170:49 - then uh now I no longer have any missing
170:52 - observations so I successfully deleted
170:55 - all the missing
170:56 - observations now the next state is to
170:59 - describe the data uh through some
171:02 - descriptive statistics and through data
171:05 - visualization so before moving on
171:07 - towards the caal analysis or predictive
171:09 - analysis in any sort of machine learning
171:12 - traditional machine learning approach
171:14 - try to First Look Into the data try to
171:17 - understand the data and see uh whether
171:19 - you are seeing some patterns uh what is
171:21 - the mean uh of different um numeric data
171:25 - fields uh do you have certain uh
171:27 - categorical values that cause an un
171:30 - unbalanced data those are things that
171:33 - you can discover uh early on uh before
171:37 - moving on to uh the model training and
171:40 - testing and blindly believing to the
171:42 - numbers so data visualization techniques
171:45 - and data exploration are great way to
171:48 - understand uh this uh data that you got
171:51 - before using that uh in order to train
171:54 - in t machine learning model so here I'm
171:58 - using the uh traditional describe
172:00 - function of pendas so data. describe
172:03 - parentheses and then this will give me
172:06 - the descriptive statistics of my data so
172:10 - here what we can see is that in total we
172:12 - got uh 20 , 640
172:17 - observations uh and then uh we also have
172:20 - a mean of uh all the variables so you
172:24 - can see that per variable I have the
172:26 - same count which basically means that
172:28 - for all variables I have the same number
172:30 - of rows and then uh here I have the mean
172:34 - which means that um here we have the
172:36 - mean of the uh variables so per variable
172:40 - we have their mean and then we have
172:42 - their standard deviation so the square
172:44 - root of the variance we have the minimum
172:46 - we have the maximum but we also have the
172:50 - 25th percentile the 15 percentile and
172:53 - the 75th percentile so the uh percentile
172:58 - uh and quantiles those are uh
173:00 - statistical terms that we oftenly use
173:03 - and the 25th percentile is the first
173:05 - quantile the 15 percentile is the second
173:08 - quantile or the uh median and the 75th
173:12 - percentile is the
173:15 - third quantile so uh what this basically
173:18 - means is that uh this percentiles help
173:22 - us to understand what is this threshold
173:25 - when it comes to looking at the um
173:28 - observations uh that fall under the
173:32 - 25% uh and then above the 25% so when we
173:36 - look at this uh standard deviation
173:38 - standard deviation helps us to interpret
173:40 - the variation in the data at the unit so
173:44 - scale of that variable so in this case
173:46 - the variable is median house value and
173:49 - we have that the mean is equal to
173:51 - 206 ,000 approximately so more or less
173:56 - that uh range 206 K and then the
174:01 - standard deviation is
174:02 - 115k what this means is that uh in the
174:06 - data set we will find blocks that will
174:09 - have the median house value that will be
174:12 - uh 200 uh 6K 206k plus 115k which is
174:18 - around
174:19 - 321k so there will be blocks where the
174:21 - median house value is around
174:24 - 321k and there will also be blocks where
174:27 - the um median house value will be around
174:31 - uh 91k so 206,000 minus
174:36 - 115k so this the idea behind standard
174:38 - deviation this variation your data so
174:42 - next we can interpret the idea of this
174:44 - uh minimum and the maximum of your data
174:47 - in your data fields the minimum will
174:49 - help you to understand what is this
174:50 - minimum value that you have per data
174:52 - field numeric data field and what is the
174:54 - maximum value so what is the range of
174:56 - values that you are looking into in case
174:59 - of the median house value this means
175:01 - what um are the uh what is this minimum
175:04 - median house value per uh block and uh
175:08 - in case of Maximum what is this um
175:11 - highest value per block when it comes to
175:14 - Medan house value so this can uh help
175:17 - you to understand um when we look at
175:20 - this aggregated data so the median house
175:22 - value what are the blocks that have the
175:25 - uh cheapest uh houses when it comes to
175:27 - their valuation and what are the most
175:30 - expensive uh blocks of
175:32 - houses so we can see that uh the
175:35 - cheapest um block uh where in that block
175:39 - the median house value is uh 15K so
175:43 - 14,999
175:44 - and the house block with the um highest
175:48 - valuation when it comes to the median
175:50 - house value so uh the median um
175:54 - valuation of the houses is equal to
175:56 - $500,000
175:58 - And1 which means that when we look at
176:00 - our blocks of houses um that uh the
176:04 - median house value in this most
176:06 - expensive blocks will be a maximum
176:10 - 500k so uh next thing that I tend to do
176:14 - is to visualize the data I tend to start
176:16 - with the dependent variable so this is
176:18 - the variable of interest the target
176:20 - variable or the response variable which
176:23 - is in our case the median house value so
176:26 - this will serve us as our dependent
176:28 - variable and what I want you to do is to
176:31 - upload this histogram uh in order to
176:34 - understand what is the distribution of
176:35 - median house values so I want to see
176:38 - that when when looking at the data what
176:41 - are the um most frequently appearing
176:44 - median house values and uh what are this
176:49 - uh type of blocks that have um unique
176:52 - less frequently um appearing uh meded
176:56 - house
176:56 - values by plotting this type of plots
176:59 - you can see some outliers some um
177:02 - frequently appearing values but also
177:04 - some values that uh go uh and uh are
177:08 - lying outside of the range and this will
177:11 - help you to identify and learn more
177:12 - about your data and toid identify
177:14 - outliers in your data so in here I'm
177:18 - using the uh curn uh Library so given
177:21 - that earlier I already imported this
177:23 - libraries there is no need to import
177:25 - here what I'm doing is that I'm setting
177:27 - the the GD so which basically means that
177:29 - I'm saying the background should be
177:31 - white and I also want discrete so this
177:33 - means those discrete behind then I'm
177:36 - initializing the size of the figure so
177:39 - PLT this comes from met plotly P plot
177:42 - and then I'm setting the figure the
177:44 - figure size should be 10x 6 so um this
177:48 - is the 10 and this is the six then we
177:51 - have the main plot so I'm um using the
177:54 - uh his plot function from curn and then
177:58 - I'm taking from the uh clean data so
178:01 - from which we have removed the missing
178:03 - data I'm picking the uh variable of
178:06 - interest which is the median house value
178:08 - and then I'm saying upload this um
178:10 - histogram using the fors green color and
178:14 - then uh I'm saying uh the title of this
178:18 - figure is distribution of p and house
178:20 - values then um I'm also mentioning what
178:23 - is the X label which basically means
178:25 - what is the name of this variable that
178:27 - I'm putting on the xaxis which is a
178:30 - median house value and what is the Y
178:33 - label so what is the name of the
178:35 - variable that I need to put on the Y AIS
178:39 - and then I'm saying pl. show which means
178:41 - show me the figure so that's basically
178:44 - how in Python the visualization works we
178:47 - uh first need to write down the the
178:50 - actual uh figure size uh and then we
178:53 - need to uh Set uh the function uh in the
178:56 - right variable so provide data to the
178:59 - visualization then we need to put the
179:01 - title we need to put the X label y label
179:03 - and then we need to say show me the
179:06 - visualization and uh if you want to
179:09 - learn more about this visualization
179:10 - techniques uh make sure to check the
179:12 - python for data science course cuz that
179:15 - one will help you to understand slowly
179:18 - uh and in detail how you can uh
179:21 - visualize your data so in here what we
179:23 - are visualizing is the frequency of
179:26 - these median house values in the entire
179:29 - data set what this means is that we are
179:31 - looking at the um number of times each
179:35 - of those median house values appear in
179:38 - the data set so uh we want to understand
179:41 - are there uh certain uh median house
179:44 - values that appear very often and are
179:47 - there certain house values that do not
179:50 - appear that often so those can be may be
179:52 - considered
179:53 - outliers uh because we want in our data
179:56 - only to keep those uh most relevant and
180:00 - representative data points we want to
180:03 - derive conclusions that hold for the
180:04 - majority of our uh uh observations and
180:08 - not for outliers we will be then using
180:11 - that uh representative data in order to
180:15 - run our linear regression and then make
180:17 - conclusions when looking at this graph
180:19 - what we can see is that uh we have a
180:21 - certain cluster of um median house
180:24 - values that appear quite often and those
180:27 - are the cases when this frequency is
180:29 - high so you can see that uh we have for
180:32 - instance houses in here in all this
180:36 - block that appear um very often so for
180:40 - instance the median house value U of A
180:44 - about 160 170k this appears very
180:48 - frequently so you can see that the
180:49 - frequency is above 1,000 those are the
180:52 - most frequently appearing Medan house
180:55 - values and um there are cases when the
181:01 - um so you can see in here and you can
181:03 - see in here houses that uh whose median
181:08 - house value is not appearing very often
181:11 - so you can see that their frequency is
181:12 - low so um roughly speaking those houses
181:17 - they are unusual houses they can be
181:20 - considered as outliers and the same
181:22 - holds also for these houses because you
181:25 - can see that for those the frequency is
181:28 - very low which means that in our
181:30 - population of houses so California house
181:32 - prices you'll most likely see houses uh
181:35 - blocks of houses whose medium value is
181:39 - between let's say um 17K up to to uh
181:44 - let's say uh 300 or
181:47 - 350k but anything below and above this
181:50 - is considered as unusual so you don't
181:53 - often see a houses that are um so house
181:57 - blocks that have a median house value
181:59 - less than uh 70 or 60k and then uh also
182:05 - uh houses that are above um 370 or 400k
182:11 - so do consider that uh we are dealing
182:13 - with
182:14 - 1990 um a year data and not the current
182:19 - uh prices because nowadays uh
182:21 - Californian houses are much more
182:23 - expensive but this is the data coming
182:25 - from 1990 so uh do take that into
182:28 - account when interpreting this type of
182:30 - data
182:31 - visualizations so uh what we can then do
182:34 - is to use this idea of inter quantile
182:36 - range to remove this outl what this
182:38 - basically means is that we are looking
182:41 - at the lowest 25th uh% percentile so uh
182:45 - we are looking at this first quantile so
182:47 - 0.25 which is a 25th percentile and we
182:51 - are looking at this upper 25th um
182:54 - percent which means the third quantile
182:56 - or the 75th percentile and then we want
183:00 - to basically remove those uh by using
183:03 - this idea of 25th percentile and 75th
183:07 - percentile so the first quantile and the
183:09 - third quantile we can then identify what
183:12 - are the um uh observations so the blocks
183:17 - that have a median house value that is
183:19 - below the uh 25th per H and above the
183:23 - 75% he so basically we want to uh get
183:27 - the middle part of our data so we want
183:29 - to get this data for which the median
183:32 - house uh value is above the 25th
183:35 - percentile so U above all the uh median
183:39 - house values that is above the uh lowest
183:41 - 25% uh percent
183:44 - and then we also want to remove this
183:46 - very large median house
183:48 - values so we want to uh keep in our data
183:52 - the so-called normal uh and
183:54 - representative blocks blocks where the
183:57 - Medan house uh value is above the lowest
184:01 - 25% and smaller than the largest 25%
184:05 - what we are using is this statistical uh
184:08 - term called inter Quan range you don't
184:10 - need to know the name but I think it
184:12 - would be just work to understand it
184:14 - because this is a very popular way of uh
184:17 - making a datadriven uh removal of the
184:21 - outliers so I'm selecting the um 25th
184:24 - percentile by using the quantile
184:26 - function from pandas uh so I'm saying
184:29 - find for me the um value that divides my
184:35 - entire uh block of observations so block
184:38 - observations to observations for which
184:41 - the Medan house value is below the um
184:45 - the um 25th percentile and above the
184:49 - 25th percentile so what are the largest
184:52 - 75% and what are the smallest
184:54 - 25% when it comes to the median house
184:57 - value and we will then be removing this
184:59 - 25% so that I will do by using this q1
185:03 - and then uh we will be using the uh Q3
185:06 - in order to remove the very large median
185:08 - house Valu so the uh upper 25th
185:11 - percentile and then uh in order to um
185:15 - calculate the inter quanti range we need
185:17 - to uh pick the Q3 and subtract from it
185:21 - the q1 so just to understand this idea
185:24 - of q1 and Q3 so the Quantas better let's
185:27 - actually print this uh
185:30 - q1 and this uh
185:34 - Q3 so let's actually remove this part
185:37 - for now and they run
185:42 - it
185:48 - so as you can see here what we are
185:50 - finding is that the uh q1 so the 25th
185:54 - percentile or first quantile is equal to
186:01 - 19,500 so it basically is a number in
186:04 - here what it means is that um we have uh
186:10 - 25% um of the um
186:14 - observations the smallest observations
186:17 - have a median house value that is below
186:21 - the uh $119,500
186:24 - and the remaining 75 uh% of our
186:29 - observations have a meeting house value
186:32 - that is above the
186:34 - $190,500
186:36 - and then the uh Q3 which is the third
186:40 - quantile or the 75th percentile it
186:43 - describes this threshold the volume
186:46 - where we make a distinction between the
186:49 - um uh lowest median house values the
186:52 - first 75th uh% of the lowest uh median
186:56 - house values versus the uh most
186:58 - expensive so the highest median house
187:01 - values so what is this upper uh
187:04 - 25% uh when it comes to the median house
187:07 - value so we see that that distinction is
187:11 - 264,000 save
187:14 - $700 so it is somewhere in here which
187:17 - basically means that when it comes to
187:19 - this uh to this blocks of uh houses the
187:22 - most expensive ones with the highest
187:24 - valuation so the 25% top rated median
187:29 - house values they are above
187:32 - 264,000 that's something that we want to
187:35 - remove so we want to remove the
187:37 - observations that have a smallest median
187:39 - house value and the largest median house
187:42 - values and and usually it's a common
187:44 - practice when it comes to the inter
187:46 - quantile uh range approach to multiply
187:49 - the inter quantile range by 1.5 in order
187:52 - to um obtain the lower bound and the
187:55 - upper bound so to understand what are
187:57 - the um thresholds that we need to use in
188:00 - order to remove the uh blocks uh so
188:03 - observation from our data where the med
188:07 - house value is very small or very large
188:11 - so for that we will be multiply the IQR
188:14 - so inter quanti range by 1.5 and when we
188:17 - uh subtract this value from q1 then we
188:20 - will be getting our lower bound when uh
188:23 - we will be adding this value to Q3 then
188:26 - we will be using and getting this
188:27 - threshold when it comes to the uh upper
188:30 - bound and we will be seeing that um
188:33 - after we uh clean this uh outliers from
188:37 - our data we end up uh getting um smaller
188:41 - data so this means that uh previously we
188:44 - had uh
188:45 - 20K so
188:47 - 20,43 3 observations and now we have
188:52 - 9,369 observations so we have roughly
188:55 - removed um like about 1,000 or bit over
188:59 - 1,000 observations from our
189:01 - data so uh next let's look into some
189:05 - other variables for instance the median
189:07 - uh income and um one other technique
189:11 - that we can use in order to identify
189:13 - outliers in the data is by using the box
189:16 - plots so I wanted to showcase the
189:18 - different approaches that we can use in
189:20 - order to visualize the data and to
189:22 - identify outliers such that you will be
189:25 - familiar with uh different techniques so
189:28 - let's go ahead and plot the uh box plot
189:31 - and box plot is a statistical um way to
189:35 - represent your data uh the central boook
189:38 - uh represents the inter Quant range so
189:42 - um that is is the IQR uh and with the uh
189:45 - with the bottom and the top edges they
189:47 - indicate the 25th percentile so the
189:50 - first quantile and the 75% H so the
189:53 - third quantile respectively the length
189:56 - of this box that you see here uh this
189:58 - dark part is basically the 50% of your
190:01 - data for the median
190:03 - income and uh this uh median uh line
190:08 - inside this box um this is the uh the
190:11 - one with uh contrast in color that
190:14 - represents the median of the data set so
190:17 - the median is the middle value when data
190:19 - is sorted in an ascending order then we
190:22 - have this whiskers in our box Flo and
190:25 - this line of whiskers extends from the
190:28 - top and the bottom of the box and
190:31 - indicate this range for the rest of the
190:33 - data set excluding the
190:35 - outliers they are typically this 1.5 IQR
190:39 - above and 1.5 times um IQR uh below the
190:44 - q1 something that we also saw uh just
190:47 - previously when we were removing the
190:48 - outliers from the median house volum so
190:52 - in order to um identify the outliers you
190:56 - can quickly see that we have all these
190:58 - points that um lie above the 1.5 time
191:02 - IQR above the um third quantile so the
191:06 - 75% H and um that's something that you
191:10 - can also see here and this means that
191:13 - those are uh blocks of houses that have
191:16 - unusually high median income that's
191:19 - something that we want to remove from
191:20 - our data and therefore we can use the uh
191:24 - exactly the same approach that we used
191:25 - previously for the median house value so
191:28 - we will then identify the uh 25th
191:31 - percentile or the first quantile so q1
191:33 - and then Q3 so the third quantile or the
191:36 - 75th percentile then we will compute the
191:39 - IQR um and then we will be obtaining the
191:41 - lower bound and the upper upper bound
191:43 - using this
191:44 - 1.5 um as a scale and then we will be
191:47 - using that this lower bound and upper
191:49 - bound to then um use this filters in
191:54 - order to remove from the data all the
191:56 - observations where the medium income is
191:59 - above the lower bound and all the
192:02 - observations that have a median income
192:05 - below the upper bound so we are using
192:07 - lower bound and upper bound to perform
192:10 - double filtering we are using two
192:12 - filters in the same row as you can see
192:15 - and we are using this parenthesis and
192:17 - this end functionality to tell to python
192:20 - well first look that this condition is
192:22 - satisfied so the observations have a
192:24 - median income that is above this lower
192:26 - bound and at the same time it should
192:29 - hold that the observation so the block
192:32 - should have a median income that is
192:33 - below the upper bound and if this uh
192:37 - block this observation in the data
192:39 - satisfies to two of this criteria then
192:41 - we are dealing with a good point a
192:43 - normal point and we can keep this and we
192:46 - are saying that this is our new data so
192:49 - let's actually go ahead and execute this
192:51 - code in this case we can see too high as
192:55 - all our out layers lie in this part of
192:58 - the box putot and then we will end up
193:00 - with the clean data I'm taking this
193:02 - clean data and then I'm putting it under
193:05 - data just for
193:06 - Simplicity and uh this data now uh is
193:11 - much more clean and uh it's better
193:13 - representation of the population
193:15 - something that ideally we want because
193:17 - we want to find out what are the
193:19 - features that uh describe and Define the
193:23 - house value not based on this unique and
193:26 - rare houses which are too expensive or
193:29 - which are in the blogs that have uh very
193:32 - high income uh people but rather we want
193:35 - to see the uh the uh true representation
193:38 - so the most frequently appearing data
193:40 - what are the features that Define the
193:42 - house value of the prices uh for common
193:46 - uh houses and for common areas for
193:48 - people with average or with normal
193:51 - income that's what we want to uh find so
193:56 - uh the next thing that I tend to do uh
193:59 - when it comes to especially regression
194:01 - nases and caal nases is to plot the
194:04 - correlation heat map so this means that
194:08 - uh we are getting the um uh correlation
194:12 - Matrix pairwise correlation score uh for
194:16 - each of this pair of variables in our
194:19 - data when it comes to the linear
194:21 - regression one of the uh assumptions of
194:24 - the linear regression that we learned
194:26 - during the theory part is that we should
194:28 - not have a perfect multicolinearity what
194:30 - this means is that there should not be a
194:33 - high correlation between pair of
194:36 - independent variables so knowing one
194:39 - should not help us to automatically
194:41 - Define the value of the other
194:43 - independent variable and if the
194:45 - correlation between the two independent
194:48 - variables is very high it means that we
194:51 - might potentially be dealing with
194:53 - multicolinearity that's something that
194:55 - we do want to avoid so hit map is a
194:59 - great way to identify whether we have
195:01 - this type of problematic independent
195:03 - variables and whether we need to drop
195:05 - any of them or maybe multiple of them to
195:08 - ensure that we are dealing with proper
195:10 - linear regression model and the
195:11 - assumptions lession model is satisfied
195:14 - now when we look at this correlation
195:16 - heat map um and uh here we use the curn
195:20 - in order to plot this as you can see
195:22 - here the colors can be from very light
195:25 - so white from till very dark green where
195:29 - uh the light means um there is a
195:33 - negative strong negative correlation and
195:35 - very dark uh green means that there is a
195:38 - very strong
195:40 - positive correlation
195:43 - so uh we know that correlation a value
195:46 - Pearson correlation can take values
195:48 - between minus one and 1 minus one means
195:51 - uh very strong negative correlation one
195:53 - means very strong positive
195:55 - correlation and um usually when uh we
195:59 - are dealing with correlation of the
196:01 - variable with itself so a correlation
196:03 - between longitude and longitude then uh
196:06 - this correlation is equal to one so as
196:10 - you can see on the diagonal we have
196:12 - there for all the ones because those are
196:13 - the pairwise correlation of the
196:15 - variables with themselves and then um in
196:19 - here uh all the values under the
196:21 - diagonal are actually equal to the uh
196:23 - mirror of them in the upper diagonal
196:25 - because the variable between so the
196:28 - correlation between uh the same two
196:30 - variables independent of how we put it
196:33 - so which one we put first and which one
196:35 - the second is going to be the same so
196:38 - basically correlation between longitude
196:40 - and ltitude and correlation latitude and
196:43 - longitude is the same so um now we have
196:47 - refreshed our memory on this let's now
196:49 - look into the actual number and this
196:51 - heat map so as we can see here we have
196:54 - this section where we um have uh
196:58 - variables independent variables um that
197:01 - have a low uh positive correlation with
197:05 - the uh remaining independent variables
197:08 - so you can see here that we have this
197:10 - light green uh values which indicate a
197:14 - low positive relationship between pair
197:16 - of
197:17 - variables one thing that is very
197:19 - interesting here is the middle part of
197:21 - this heat map where we have this dark
197:23 - numbers so the numbers uh below the
197:26 - diagonals are something we can interpret
197:29 - and remember that below diagonal and
197:31 - above diagonal is basically the mirror
197:33 - we here already see a problem because we
197:36 - are dealing with variables which are
197:38 - going to be independent variables in our
197:40 - model that have a high correlation
197:43 - now why is this a problem because one of
197:46 - the assumptions of linear regression
197:48 - like we saw during the theory section is
197:50 - that we should not have a multiple uh
197:54 - colinearity so multicolinearity problem
197:57 - when we have perfect multicolinearity it
198:00 - means that we are dealing with
198:02 - independent variables that have a high
198:04 - correlation knowing a value one variable
198:07 - will help us to know automatically what
198:10 - is the value of the other one and when
198:12 - we have a correlation of 0.93 which is
198:16 - very high or
198:17 - 0.98 this means that those two variables
198:20 - those two independent variables they
198:23 - have a super high positive relationship
198:26 - this is a problem because this might
198:28 - cause our model to result in uh very
198:33 - large standard errors and also not
198:37 - accurate and not generalizable model
198:39 - that's something we want to avoid and
198:42 - and uh we want to ensure that the
198:43 - assumptions of our model are
198:46 - satisfied now um we are dealing with
198:49 - independent variable which is total
198:51 - underscore bedrooms and households which
198:54 - means that number of total
198:56 - bedrooms uh pair block and the uh
199:00 - households is highly correlated
199:02 - positively correlated and this a problem
199:06 - so ideally what we want to do is to drop
199:09 - one of those two independent variables
199:11 - and and uh the reason why we can't do
199:13 - that is because uh those two variables
199:17 - given that they are highly correlated
199:19 - they already uh explain similar type of
199:22 - information so they contain similar type
199:24 - of variation which means that including
199:27 - the two just it doesn't make sense on
199:30 - one hand it's uh violating the moral
199:32 - assumptions potentially and on the other
199:34 - hand it's not even adding too much volum
199:37 - because the other one already shows
199:40 - similar variation so
199:42 - um the total underscore bedrooms
199:44 - basically contains similar type of
199:47 - information as the households so we can
199:50 - as well um so we can better just drop
199:53 - one of those uh two independent
199:56 - variables now uh the question is which
199:59 - one and that's something that we can uh
200:02 - Define by also looking at other
200:04 - correlations in here because we uh have
200:08 - a total bedrooms uh having a high
200:12 - correlation with households but we can
200:14 - also see that the total underscore rooms
200:17 - has a very high correlation with our
200:21 - households so this means that there is
200:23 - yet another independent variable that
200:25 - has a high correlation with our
200:28 - households
200:30 - variable and then this total underscore
200:33 - rooms has also High uh correlation with
200:36 - the total underscore bedroom so this
200:38 - means that um we can decide which one is
200:44 - has um more frequently uh High
200:48 - correlation with the rest of independent
200:50 - variables and in this case it seems like
200:52 - that the largest two numbers in here are
200:55 - the um this one and this one so we see
200:59 - that the total bedroom has a 0.93 as
201:02 - correlation with the total underscore
201:04 - rooms and uh at the same time we also
201:07 - see that hotel bedrooms has also um very
201:12 - high correlation with the household so
201:15 - 0.98 which means that total underscore
201:18 - bedrooms has the highest correlation
201:20 - with the remaining independent variables
201:22 - so we might as well drop this
201:24 - independent variable but before you do
201:28 - that I would suggest to do one more
201:30 - quick visual check and it is to look
201:32 - into the total uncore bedroom
201:35 - correlation with the dependent variable
201:37 - to understand how strong of a
201:39 - relationship does this have on the
201:42 - response variable that we are looking
201:43 - into so we see that the uh total
201:46 - underscore
201:48 - bedroom uh has this one
201:50 - 0.05 correlation with the response
201:53 - variable so the median house value when
201:56 - it comes to the total rooms that one has
201:58 - much higher so I'm already seeing from
202:00 - here that uh we can feel comfortable uh
202:04 - excluding and dropping the total
202:06 - underscore bedroom from our data in
202:08 - order to ensure that we are not dealing
202:10 - with perfect multicolinearity
202:18 - so this exactly what I'm doing here so
202:21 - I'm dropping the um total
202:24 - bedrooms so after doing that we no
202:27 - longer have this uh total bedrooms as
202:29 - the column so before moving on to the
202:32 - actual CA analysis there is one more
202:34 - step that I wanted to uh show you uh
202:37 - which is super important when it comes
202:39 - to the POS analysis and some uh
202:41 - introductory econometrical stuff so uh
202:45 - when you have a string categorical
202:47 - variable there are a few ways that you
202:49 - can deal with them one easy way that you
202:52 - will see um on the web is to perform one
202:55 - H encoding which basically means
202:58 - transforming all this uh string values
203:00 - so um we have a near Bay less than 1
203:04 - hour ocean uh Inland near Ocean Iceland
203:07 - to transform all these values to some
203:09 - numbers such that we we have for the
203:12 - ocean proximity variable values such as
203:15 - 1 2 3 4 5 one way of doing that can be
203:19 - uh something like this but better way
203:22 - when it comes to using this type of
203:23 - variables in linear regression is to
203:26 - transform this a string uh category type
203:29 - of variable to what we're calling dami
203:32 - variables so dami variable means that
203:35 - this variable takes two possible values
203:38 - and usually uh it is a binary Boolean
203:41 - variable
203:42 - which means that it can take two
203:43 - possible values zero and one where one
203:47 - means that the condition is satisfied
203:49 - and zero means condition is not
203:51 - satisfied so let me give you an example
203:53 - in this specific case we have that the
203:55 - ocean proximity has five different
203:57 - values and ocean proximity is just a
204:00 - single
204:01 - variable then uh what we will do is we
204:04 - will use the uh get underscore D
204:07 - function in Python from pandas in order
204:10 - to uh go from this one variable to a
204:15 - five different variable per each of this
204:18 - category which means that now we will
204:20 - have new variables that uh will uh
204:23 - basically be uh whether uh it is uh
204:26 - nearby or not whether it's less than 1
204:29 - hour uh uh from the ocean uh variable
204:33 - whether it's Inland whether it's near
204:35 - Ocean or whether is an island this will
204:38 - be a separate binary variable a dummy
204:42 - variable that will take value 0 and one
204:45 - which means that we are going from one
204:47 - string categorical variable to five
204:50 - different dami variables and in this
204:53 - case um each of those dami variables
204:56 - that you can see here we are creating
204:58 - five dami variables each of each for uh
205:01 - each of those five categories and then
205:04 - uh we are combining them and uh from the
205:07 - original data we will then be dropping
205:10 - the ocean prox IM data so on one hand we
205:13 - are getting rid of this string variable
205:16 - which is a problematic variable for
205:17 - linear regression when combined with the
205:20 - pyler library because cyler cannot
205:22 - handle this type of um data when it
205:25 - comes to linear regression and B we are
205:28 - making our job easier when it comes to
205:31 - interpreting the results so uh
205:33 - interpreting linear regression for CER
205:36 - nazes uh is much more easy when we have
205:40 - dami variables then when we have a one
205:43 - string categorical variable so just to
205:46 - give you an example if we are creating
205:48 - from this string variable uh five
205:51 - different dami variables and those are
205:53 - those five different dami variables that
205:55 - you can see in here so this means that
205:57 - if we are looking at this one category
206:00 - so let's say uh ocean _ proximity under
206:03 - Inland it means that for all the rows
206:07 - where we have the value equal to zero it
206:10 - means this criteria is not satisfied
206:12 - which means that uh ocean proximity uh
206:15 - underscore Inland is equal to zero which
206:18 - means that the house blob we are dealing
206:20 - with is not from
206:23 - Inland so that criteria is not satisfied
206:26 - and otherwise if this value is equal to
206:29 - one so for all these rows when the ocean
206:31 - proximity Inland is equal to one It
206:34 - means that the criteria is satisfied and
206:36 - we are dealing with house blocks that
206:38 - are indeed in the Inland one thing thing
206:41 - to keep in mind uh when it comes to uh
206:44 - transforming a string categorical
206:46 - variable to um set of DS is that you
206:49 - always need to drop at least one of the
206:51 - categories and the reason for this is
206:54 - because we learned during the theory
206:55 - that uh we should have no perfect
206:59 - multicolinearity this means that um we
207:03 - cannot have five different variables
207:06 - that are perfectly
207:07 - correlated and if we include all these
207:10 - values and this variables it means that
207:13 - um when uh we know that the uh uh block
207:17 - of houses is not near the bay is not
207:20 - less than 1 hour ocean is not Inland is
207:22 - not near the ocean automatically we know
207:25 - that it should be the remaining category
207:27 - which is Inland so we know that for all
207:29 - those blocks the um uh ocean proximity
207:33 - underscore uh uh irand uh Iceland will
207:37 - be equal to one and that's something
207:40 - that we want to avoid because because
207:41 - that is the definition of perfect
207:43 - multicolinearity So to avoid one of the
207:45 - oils assumptions to be violated we need
207:48 - to drop one of those
207:50 - categories so uh we can see in here uh
207:54 - that's exactly uh what I'm doing I'm
207:57 - saying so let's go ahead and actually
208:00 - drop one of those variables so let's see
208:04 - first what is the set of all variables
208:06 - we got so we got less than one hour uh
208:09 - ocean Inland Iceland new bay and then uh
208:13 - new ocean let's actually drop one of
208:15 - them so let's drop the
208:18 - Iceland and uh that we can do very
208:21 - simply by let me
208:26 - see I is not allowing me to add a code
208:29 - in here so we are doing data is equal
208:33 - to uh and
208:35 - then data do drop and then the name of
208:40 - the variable we within the uh quotation
208:42 - marks and then uh X is = to 1 so in this
208:48 - way I'm basically dropping one of the uh
208:50 - daming variables that uh I created in
208:53 - order to avoid the perfect
208:55 - multicolinearity assumption to be
208:57 - violated and once I go ahead and print
209:01 - the columns now we should see uh this uh
209:05 - column uh
209:07 - disappearing here we go so we
209:10 - successfully deleted that variable let's
209:12 - go ahead and actually get the head so
209:14 - now you can see that we no longer have a
209:16 - string in our data but instead we got
209:18 - four additional binary variable out of a
209:21 - string categorical variable with five
209:24 - categories all right now we are ready to
209:27 - do the actual work uh when it comes to
209:30 - the training a machine learning model uh
209:33 - or statistical model we learn during the
209:35 - uh theory that we always need to split
209:37 - that data into train uh and test set
209:41 - that is the minimum in some cases we
209:43 - also need to do train validation and
209:45 - test such that we can train the model on
209:47 - the training data and then optimize the
209:49 - model on validation data and find out
209:52 - what is the optimal set of
209:54 - hyperparameters and then uh use this
209:57 - information to uh apply this fitted and
210:00 - optimized model on an unseen test data
210:03 - we are going to skip the validation set
210:05 - for Simplicity especially given that we
210:08 - are dealing with a very simple machine
210:09 - learning model as linear regression
210:11 - and we're going to split our data into
210:14 - train and test and here uh what I'm
210:17 - going to do is first I'm creating this
210:19 - list of the name or variables that we
210:22 - are going to use in order to um train
210:26 - our machine learning bottle so uh we
210:29 - have a set of independent variables and
210:31 - a set of dependent variable so in our
210:34 - multiple linear
210:35 - regression here is the set of uh
210:38 - independent variables that we will have
210:40 - so we have long itude latitude housing
210:43 - median Edge total rooms population
210:45 - households median income median house
210:48 - value and the four different categorical
210:50 - dami uh four different uh dami variables
210:52 - that we built from the categorical
210:56 - variable then um I am specifying that
211:00 - the uh Target variable is so the target
211:05 - so the response variable or the
211:06 - dependent variable is the um median
211:10 - house value this is the value that we
211:12 - want to uh uh Target because we want to
211:16 - see what are the features and what are
211:18 - the independent variables out of the set
211:21 - of all features that have a
211:22 - statistically significant impact on the
211:26 - uh dependent variable which is the
211:28 - median house value because we want to
211:30 - find out what are these features um
211:33 - describing the houses in the block that
211:36 - cause a change cause a variation in the
211:39 - um t Target variable such as the Medan
211:42 - house value so here we have X is equal
211:46 - to and then uh from the data we are
211:49 - taking all the features that have the
211:51 - following names and then we have the uh
211:54 - Target which is a midin house uh house
211:56 - value and that's uh the column that we
211:58 - are going to subtract and select from
212:01 - the data so we are doing data
212:04 - filtering so here we are then selecting
212:07 - and what I'm using here is the train
212:10 - test complete function from the psych
212:13 - learn so you might recall that in the
212:16 - beginning we spoke and imported this uh
212:18 - model selection um library and from the
212:22 - cyler model selection we imported the
212:24 - train _ testore Spate function now this
212:28 - is a function that you are going to need
212:30 - quite a lot in machine learning because
212:32 - this a very easy way to uh split your
212:36 - data so um in here uh the arguments of
212:40 - the this function is first the uh Matrix
212:44 - or the data frame that contains the
212:46 - independent variables in our case X so
212:49 - here you fill in X and then the second
212:52 - uh argument is the dependent variable so
212:56 - uh the Y and then we have test size
212:59 - which means um what is the uh proportion
213:03 - of um observations that you want to put
213:06 - in the test and what is the proportion
213:08 - of observation that you um don't want to
213:11 - put basically in the training if you are
213:13 - putting 0.2 it means that you want your
213:16 - test size to be uh 20% of your entire
213:19 - 100% of data and the remaining 80% will
213:22 - be your training data so if you provide
213:25 - your point two to this argument then the
213:27 - function automatically understands that
213:29 - you want this 80 20 division so 80%
213:33 - training and then 20% test size and then
213:36 - finally you can also uh add the random
213:39 - State because the split is going to be
213:41 - random so the data is going to be
213:43 - randomly selected from the entire data
213:46 - and to ensure that your results are
213:48 - reproducible and uh the next time you
213:51 - are running this um notebook you will
213:53 - get the same results and also to ensure
213:56 - that me and you get the same results we
213:58 - will be using a random State and a
214:01 - random state of 111 is just um random
214:04 - number there I liked and decided to use
214:07 - here so uh when we go in um use this and
214:11 - run this command you can see that we
214:13 - have a training set size 15K and then
214:16 - test size uh 38k so when you look at
214:19 - these numbers you will then get a
214:21 - verification that you are dealing with
214:22 - 20% versus 80%
214:25 - thresholds so then we go and we do the
214:28 - training one thing to keep in mind is
214:30 - that here we are using the SM Library uh
214:34 - NSM function that we imported from the
214:38 - uh stats model. API so this is one one
214:41 - uh function that we can use in order to
214:43 - conduct our uh Cal analysis and to train
214:46 - Le regression model so uh for that what
214:50 - we need to do so uh when we
214:53 - are using this Library uh this Library
214:56 - doesn't automatically add the uh first
215:00 - uh column of ones uh in your uh set of
215:04 - independent variables which means that
215:06 - it only goes and looks at what are the
215:08 - features that you have provided and
215:09 - those are all the independent variables
215:11 - but we learned from the theory that uh
215:14 - when it comes to linear regression we
215:16 - always are adding this intercept so the
215:18 - beta0 if you go back to the theory
215:21 - lectures you can see this beta0 to be
215:24 - added to both to the simple linear
215:25 - regression and to the multipar
215:27 - regression this ensures that we look at
215:30 - this intercept and we see what is this
215:33 - average uh in this case median house
215:36 - value if all the other features are um
215:39 - equal to
215:41 - zero so um therefore given that the this
215:46 - specific stats models. API is not adding
215:49 - this uh constant um column to the
215:53 - beginning for intercept it means that we
215:56 - need to add this manually therefore we
215:58 - are saying sm. addore constant to the
216:02 - exrain which means that U now our uh x
216:06 - uh table or X data frame uh add a column
216:10 - of ones uh to the features so let me
216:14 - actually show you uh before doing the uh
216:17 - training because I think this also
216:19 - something that you should be aware of so
216:23 - if we do here a pause so I'm going to do
216:27 - xcore train underscore uh constant and
216:31 - then I'm also going to print um the same
216:36 - um
216:37 - feature data frame before adding this
216:41 - constant such that you see what I mean
216:43 - so as you can see here this is just the
216:45 - same set of all columns that form the
216:47 - independent variables the features so
216:50 - then when we add the constant now after
216:53 - doing that you can see that now we have
216:56 - this initial column of ones this is th
216:59 - such that we can have uh uh beta Z at
217:03 - the end which is the intercept and we
217:05 - can then perform a valid multiple linear
217:07 - regression otherwise you don't have an
217:09 - intercept and this is just not what
217:11 - you're looking for now the psychic learn
217:15 - Library does this automatically
217:17 - therefore when you are using uh this tou
217:19 - models. API you should add this constant
217:22 - and then I use the pyit learn without
217:24 - adding the constant and if you're
217:26 - wondering why to use this specific model
217:29 - as uh we already discussed about this
217:32 - just to refresh your memory we are using
217:34 - the T models. API because this one has
217:37 - this nice property of visualizing the
217:39 - summary of your result results so your P
217:41 - values your test your standard errors
217:44 - something that you definitely are
217:45 - looking for when you are performing a
217:47 - proper causal analysis and you want to
217:50 - identify the features that have a
217:52 - statistically significant impact on your
217:54 - dependent variable if you are using a
217:56 - machine learning model including linear
217:58 - regression only for Predictive Analytics
218:01 - so in that case you can use the psychic
218:03 - learn without worrying about using STS
218:06 - models.
218:08 - API so this is about adding constant uh
218:11 - now we are ready to actually uh fit our
218:14 - model or train our model therefore what
218:17 - we need to do is to use sm. OLS so OS is
218:20 - the ordinar squares estimation technique
218:23 - that we also discussed as part of the
218:24 - theory and we need to provide first the
218:27 - dependent variable so Yore train and
218:30 - then the um feature set which is xcore
218:34 - train uncore constant so then what we
218:37 - need to do is to do that feed par
218:40 - paresis which means that take the OS
218:43 - model and use the Yore train as my
218:46 - dependent variable and xcore Trainor
218:49 - constant as my independent variable set
218:52 - and then fit the OLS algorithm and
218:55 - linear regression on this specific data
218:59 - if you're wondering why y train or X
219:01 - train and what is the differ between
219:03 - train and test and sure to go and
219:06 - revisit the training um Theory lectures
219:09 - because there I go in detail into this
219:11 - concept of training and testing and how
219:14 - we can divide the data into train and
219:18 - test and uh this Y and X as we have
219:21 - already discussed during this tutorial
219:22 - is simply this distinction between
219:24 - independent variables defined by X and
219:27 - the dependent variable defined by y so y
219:29 - train y test is the dependent variable
219:31 - data for the training data and test data
219:34 - and then EXT train ex uh test is simply
219:37 - the training data features so ex train
219:40 - and then test data features X test we
219:43 - need to use x train and Y train to fit
219:46 - our data to learn from the data and then
219:50 - once it comes down to evaluating the
219:52 - model we need to uh use the fitted model
219:56 - from which we have learned using both
219:58 - the dependent variable and the
220:00 - independent variable set so y train X
220:02 - train and then uh once we have this
220:04 - model uh that is fitted we can apply
220:07 - this to unseen data exore test we have
220:10 - can obtain the predictions and we can
220:12 - compare this to the true y so Yore test
220:16 - and to see how
220:18 - different the Y uh underscore test is
220:22 - from the Y predictions for this unseen
220:24 - data and to evaluate how moral uh is
220:28 - performing this prediction so how moral
220:30 - is uh managing to identify the median uh
220:35 - house values and predict median house uh
220:37 - values based on the uh um fitted model
220:41 - and on an unseen data so exore test so
220:45 - this is just a background info and some
220:47 - refreshment and now um in this case we
220:50 - are just uh fitting the data on the
220:52 - training uh dependent variable and then
220:54 - training uh independent variable edit a
220:57 - constant and then we are ready to print
220:59 - the summary now let's now interpret
221:01 - those results first thing that we can
221:04 - see is that uh all the coefficients and
221:06 - all the independent variables are
221:08 - statistically significant and how can I
221:11 - say this well um if we look in here we
221:13 - can see the column of P values this is
221:17 - the first thing that you need to look at
221:18 - when you are getting this results of a
221:20 - caal analysis in linear oppression so
221:23 - here we are seeing that the P value is
221:25 - very small and just to refresh our
221:27 - memory P value says what is this
221:30 - probability that you have obtained too
221:32 - high of a test statistics uh given that
221:35 - this is just by a random chance so you
221:37 - are seeing statistically significant
221:39 - results which is just by random chance
221:41 - and not because your uh n hypothesis is
221:44 - false and you need to reject it so
221:47 - that's one thing in here you can see you
221:50 - can see that we are getting much more so
221:52 - first thing that you can do is to verify
221:54 - that you have used the correct dependent
221:56 - variable so you can see here that the
221:58 - dependent variable is a median house
222:00 - value the model that is used to estimate
222:04 - those coefficients in your model is the
222:06 - OS the method is the Le squares so Le
222:09 - squares is simply uh the uh technique
222:11 - that is the underlying approach of
222:13 - minimizing the sum of uh uh squared
222:16 - residuals so the least squares the date
222:19 - that we are running this analysis is the
222:21 - 26th of January of
222:24 - 2024 uh so we have the number of
222:26 - observations which is the number of
222:28 - training observations so the 80% of our
222:30 - original data we have R squ which is the
222:33 - um Matrix that showcases what is the um
222:38 - goodness of fat of your model so r s is
222:42 - a matrix that is commonly used in linear
222:44 - regression specifically to identify how
222:47 - good your model is able to fit your data
222:50 - with this linear regression line and the
222:53 - r squ uh the maximum of R squ is one and
222:56 - the minimum is zero
222:59 - 0.58 uh in this case approximately 59 it
223:02 - means that uh all your data that you got
223:05 - and all your independent variables so
223:07 - those are all the independent variables
223:09 - that you have included they are able to
223:12 - explain
223:13 - 59% so
223:15 - 0.59 out of the entire set of variation
223:19 - so 59% of variation in your response
223:22 - variable which is the median house value
223:25 - you are able to explain with a set of
223:27 - independent variables that you have
223:28 - provided to the model now what does this
223:31 - mean on one hand it means that you have
223:33 - a reasonable enough information so
223:35 - anything above 0.5 is quite good which
223:38 - means that more than half of the uh
223:41 - entire variation in your median house
223:43 - value you are able to explain but on the
223:47 - other hand it means also that there is
223:49 - approximately 40% of variation so
223:52 - information about your house values that
223:55 - you don't have in your data this means
223:58 - that you might consider going and
224:00 - looking for extra additional information
224:02 - so additional independent variables to
224:05 - add on the top of the existing
224:06 - independent variables in order to
224:08 - increase this amount and to increase the
224:10 - amount of information and variation that
224:13 - you are able to explain with your model
224:18 - so the r squ this is like the best way
224:21 - to uh explain what is the quality of
224:25 - your regression model another thing that
224:28 - we have is the adjusted R squ adjusted R
224:30 - squ and R squ in this specific case as
224:33 - you can see they are the same so 0 um 59
224:36 - this usually means that uh you're fine
224:39 - when it comes amount of features that
224:41 - you are using once you overwhelm your
224:44 - model with too many features you will
224:45 - notice that the adjusted R squ will be
224:47 - different than your R squ so adjusted R
224:50 - squ helps you to understand whether your
224:52 - Motel is performing well only because
224:55 - you are adding so many of you of those
224:57 - variables or because really they contain
224:59 - some useful information CU sometimes the
225:02 - r squ it will automatically increase
225:04 - just because you are adding too many
225:06 - independent variables but in some cases
225:09 - those independ variables they are not
225:10 - useful so they are just adding to the
225:12 - complexity of the model and possibly
225:14 - overfitting your model but not providing
225:17 - any edit
225:18 - information then we have the F
225:20 - statistics here which corresponds to the
225:23 - F test and uh F test um it comes from
225:27 - statistics uh you don't need to know it
225:30 - but I would say uh check out the
225:32 - fundamentals to statistics course if you
225:34 - do want to know it because it means that
225:36 - uh you are testing whether all these
225:38 - independent variables Al together
225:40 - whether they are helping to explain your
225:44 - uh dependent variable so the median
225:46 - house value and uh if the F statistics
225:49 - is very large or the P value of your F
225:52 - statistics is very small so
225:54 - 0.0 it means that all your independent
225:58 - variables jointly are statistically
226:01 - significant which means that all of them
226:04 - together helped you explain your uh uh
226:07 - median house value and have a
226:09 - statistically significant impact or your
226:11 - median house value which means that you
226:13 - have a good set of independent
226:16 - variables so then we have the log
226:18 - likelihood not super relevant in this
226:20 - case you have the AIC Bic which stand
226:22 - for AAS information criteria and bation
226:25 - information criteria those are also not
226:28 - necessary to know for now but once you
226:31 - advance in your career in machine
226:32 - learning it might be useful to know at
226:34 - higher level for now think of it like um
226:36 - value that helps to understand this uh
226:38 - information that you gain when you are
226:41 - adding this set of independent variables
226:43 - to your model but this is just optional
226:46 - ignore it if you don't know it for now
226:49 - okay let's now go into the fun part so
226:51 - in this Mata uh part of the summary uh
226:55 - table we got first the set of uh
226:57 - independent variables so we have our
226:59 - constant which is The Intercept we have
227:01 - the longitude latitude housing median
227:03 - age total roles population households
227:06 - median income and the four dami
227:08 - variables that we have created
227:10 - then we have the coefficients
227:11 - corresponding to those independent
227:12 - variables those are basically the beta0
227:16 - beta 1 head beta 2 head Etc which are
227:19 - the um parameters of the linear
227:21 - regression model that our oils method
227:24 - has estimated based on the data that we
227:26 - have
227:28 - provided now before interpreting this
227:30 - independent variables the first thing
227:32 - you need to do as I mentioned in the
227:34 - beginning is to look at this P value
227:37 - column this showcases the set of all
227:40 - independent variables that are
227:42 - statistically significant and usually
227:45 - this table that you will get from a Sato
227:47 - API is at 5% significance level so the
227:51 - alpha the threshold of statistical
227:53 - significance is equal to 5% and any P
227:56 - value that is smaller than 0.05 it means
227:59 - you are dealing with a statistically
228:01 - significant independent variable now the
228:04 - next thing that you can see here in the
228:06 - left is the T statistics this P value is
228:09 - based on a t test so this T Test is
228:12 - simply stating as we have learned during
228:14 - the theory and you can also check the
228:16 - fundamental to statistics course from
228:18 - lunar tech for more detailed
228:20 - understanding of this test but for now
228:23 - this T Test um States a hypothesis
228:26 - whether um each of these independent
228:28 - variables individually has a
228:31 - statistically significant impact on the
228:33 - dependent variable and whenever this uh
228:37 - T Test has a p value that is smaller
228:40 - than the 0.05 it means you are dealing
228:43 - with statistically significant uh
228:45 - independent variable in this case we are
228:48 - super lucky all our independent
228:50 - variables are statistically significant
228:52 - then the question is whether we have a
228:55 - positive statistical significant or
228:56 - negative that's something that you can
228:58 - see by the signs of these numbers so you
229:01 - can see that longitude has a negative
229:04 - coefficient latitude negative
229:06 - coefficient housing median age positive
229:08 - coefficient
229:10 - Etc negative coefficient means that this
229:12 - independent variable causes a negative
229:15 - change in the dependent variable so more
229:18 - specifically when we look for instance
229:21 - the um let's say which one should we
229:25 - look uh let's say the uh total uh
229:29 - underscore rooms when we look at the
229:31 - total underscore rooms and it's minus
229:34 - 2.67 it means that when we look at this
229:36 - total number of rooms and we increase
229:39 - the number of
229:41 - rooms uh by uh one additional unit so
229:46 - one more room added to the total
229:49 - underscore rooms then the uh house value
229:53 - uh decreases by minus
229:56 - 2.67 now you might be wondering but how
229:58 - is this possible well first of all the
230:01 - value the coefficient is quite small so
230:03 - in one hand it's it's not super relevant
230:05 - as we can see the uh relationship
230:08 - between them is not super strong because
230:10 - the U margin of this um coefficient is
230:13 - quite small but on the other hand you
230:15 - can explain that at some point when you
230:17 - are adding more rooms it just doesn't
230:20 - add any value and in fact in some cases
230:22 - just decreases the value of the house
230:24 - this might be the case at least this is
230:27 - the case based on this data we can see
230:30 - that if there is a negative coefficient
230:32 - then one unit increase in that specific
230:34 - independent variables all else constant
230:37 - will result in um uh in this case for
230:41 - instance in case of the total rooms uh
230:45 - 2.67
230:46 - decrease in the median house value
230:50 - everything else constant we are also
230:52 - referring to this ass set that is parus
230:54 - in econometric which means that
230:56 - everything else constant so one more
230:59 - time let's refresh our memory on this so
231:01 - ensure that we are clear on this if we
231:04 - add one more room to the total number of
231:07 - rooms then the median house value will
231:11 - decrease by
231:14 - $267 and this when the longitude
231:17 - latitude house median age population
231:20 - households median income and all the
231:22 - other criterias are the same so if we
231:25 - have uh for instance this negative value
231:28 - this means that we are getting a
231:29 - decrease in the median house value if we
231:32 - have an increase by one unit in our uh
231:36 - total number of roles now let's look at
231:38 - the op opposite uh case when the
231:40 - coefficient is actually positive and
231:42 - large which is the hous in median age
231:45 - this means is if we have two houses they
231:48 - have uh exactly the same characteristics
231:50 - so they have the same longitude latitude
231:52 - they have the same total number of rooms
231:54 - population housing households median
231:57 - income they are uh the same in terms of
232:00 - the distance from the ocean then um if
232:04 - one of these houses has one more
232:07 - additional year added on the uh median
232:12 - age so housing median age so it's one
232:14 - year older then the house value of this
232:18 - specific house is higher by
232:24 - $846 so this house which has one more
232:28 - additional median age has $
232:33 - 846 higher median house value compared
232:36 - to the one that has all these
232:38 - characteris ICS except it has just the
232:41 - um uh house median age that uh is one
232:45 - year less so one more additional uh year
232:49 - in the median age will result in
232:53 - 846 uh increase in the mediate house
232:56 - value everything else
232:58 - constant so this is regarding this idea
233:01 - of negative and imp positive and then
233:03 - the margin of coefficient now let's look
233:06 - at one dami variable and um explain the
233:09 - idea behind it and how we can interpret
233:12 - it and uh it's it's a good way to
233:15 - understand how the dond variables can be
233:16 - interpreted in the context of linear
233:18 - regression so one of the independent
233:20 - variables is the ocean proximity Inland
233:23 - and the coefficient is equal to-
233:26 - 2108 e plus 0.5 this simply means -
233:30 - 210 K uh approximately and um what this
233:35 - means is that if we have two houses
233:39 - they have exactly the same
233:40 - characteristics so their longitude
233:42 - latitude is the same house median age is
233:44 - the same they have the same total number
233:46 - of rooms population households median
233:49 - income all these characteristics for
233:51 - this two blocks of houses is the same
233:54 - with a single difference that one block
233:57 - is located in the um Inland when it
234:00 - comes to Ocean proximity and the other
234:03 - block of houses is not located in the
234:05 - Inland so in this case the reference so
234:08 - the
234:09 - um category that we have removed from
234:12 - here was the Iceland you might recall uh
234:16 - so if the block of houses is in the
234:19 - Inland that their value is on average uh
234:25 - smaller and less by
234:29 - 210k when it comes to the median house
234:31 - value compared to the block of houses
234:33 - that has exactly the same
234:34 - characteristics but it's not in the
234:36 - Inland so for instance is in the uh
234:40 - Iceland so uh when it comes to this dumi
234:43 - variables where there is also an
234:45 - underlying reference variable which you
234:48 - have deleted as part of your string
234:50 - categorical variable then you need to
234:52 - reference your dami variable to that
234:55 - specific category this might sound
234:58 - complex it is actually not I would say
235:01 - uh it's just a matter of practicing and
235:03 - trying to understand what is this
235:05 - approach of D variable it means that you
235:08 - either have that criteria or not in this
235:10 - specific case it means that if you have
235:12 - two blocks of houses with exactly the
235:14 - same characteristics and one block of
235:16 - houses is in the Inland and the other
235:19 - one is not in the Inland for instance is
235:21 - in the Iceland then the block of houses
235:24 - in the Inland will have on average
235:27 - 210,000 less uh median house value
235:30 - compared to the block of houses that is
235:33 - the IND for instance in the Iceland uh
235:35 - when it comes to the ocean
235:37 - proximity which kind of uh makes sense
235:40 - because in California people might
235:42 - prefer living uh in the isoland location
235:44 - in the houses might have more demand
235:46 - when it comes to the Iceland location
235:48 - compared to the um Inland locations so
235:52 - the longitude uh has a statistically
235:55 - significant impact on the uh median
235:57 - house value latitude house median age
236:01 - has an impact and causes a a
236:04 - statistically significant difference in
236:06 - the Medan house value if there is a
236:08 - change in median age the total number of
236:11 - rooms have an impact on the median house
236:14 - volume and the population has an impact
236:17 - households median income as well as the
236:21 - uh proximity from the ocean and this is
236:25 - because all their P values is uh zero
236:28 - which means that they are smaller than
236:30 - 0.05 and this means that they all have a
236:33 - statistically significant impact on the
236:36 - median house value in the Californian
236:38 - house in market now when it comes to the
236:41 - uh interpretation of all of them uh we
236:43 - have interpreted just few uh for the
236:46 - sake of Simplicity and ensuring that
236:49 - this uh this entire case study doesn't
236:51 - take too long but what I would suggest
236:54 - you to do is to uh interpret all of the
236:58 - uh coefficients here because we have
237:00 - interpreted just the housing median age
237:03 - and the um the total number of rooms but
237:07 - you can also interpret the population uh
237:10 - as well as the median income and uh we
237:13 - have also interpreted one of those Dy
237:15 - variables but feel free also to
237:16 - interpret all the other ones so by doing
237:19 - this you can also uh Even build an
237:22 - entire case study paper in which you can
237:25 - explain in one or two pages the results
237:28 - that you have obtained and this will
237:29 - showcase that you have an understanding
237:31 - of how you can interpret the linear
237:34 - gressional results another thing that I
237:36 - would suggest you to do is to uh add a
237:39 - comment on the standard error so let's
237:41 - now look into the standard errors we can
237:44 - see a huge standard error that we are um
237:47 - making and this is the direct result of
237:50 - the fourth assumption that was violated
237:53 - now this case study is super important
237:55 - and useful in a way that it showcases
237:58 - what happens if some of your um
238:01 - assumptions are satisfied and if some of
238:04 - those assumptions are violated so in
238:08 - this specific specific case the
238:09 - Assumption related to the uh uh the
238:12 - errors having a constant variance is
238:14 - violated so we have a heos SK assist the
238:17 - issue and that's something that we are
238:19 - seeing back in our results and this is a
238:22 - very good example of the case that even
238:24 - without checking the assumptions you can
238:27 - already see that the standard error is
238:31 - very large and uh you can see here that
238:34 - given that the standard ER is large this
238:36 - already gives a hint that most most
238:38 - likely our
238:40 - heteroscedasticity uh is present and our
238:42 - homoscedasticity assumption is violated
238:44 - you keep in mind this um idea of um
238:49 - large standard errors that we just saw
238:51 - because we are going to see that this
238:52 - becomes a problem also for the um
238:55 - performance of the model and we will see
238:57 - that we are obtaining a large error due
239:01 - to this and uh one more comment when it
239:04 - comes to the total rooms and the housing
239:06 - median age in some cases the linear
239:09 - regression results might not seem
239:10 - logical but sometimes they actually is
239:13 - an underlying explanation that can be
239:15 - provided or maybe your model is just
239:18 - overfitting or biased that's also
239:20 - possible and uh that's
239:22 - something that uh you can do by checking
239:26 - your ois assumptions and uh before uh
239:30 - going to that stage I wanted to briefly
239:33 - showcase to you this um idea of
239:36 - predictions so we have now fitted our
239:39 - model on the uh uh training data and we
239:44 - are ready to perform the predictions so
239:46 - we can then use our fitted model and we
239:48 - can then uh use the test data so ex test
239:53 - in order to perform the predictions so
239:55 - to uh use a data to get new house
239:59 - mediate house values for the um blocks
240:03 - of houses for which we are not providing
240:05 - the uh corresponding Medan house price
240:07 - so on aning data we are uh re um
240:11 - applying our model that we have already
240:14 - fitted and we want to see what are these
240:16 - predicted median house values and then
240:19 - we can compare these predictions to the
240:22 - true median house values that we have
240:24 - but we are not yet exposing them and we
240:27 - want to see how good our model is doing
240:29 - a job of estimating and finding these
240:33 - unknown median house values for the test
240:35 - data so for all the blocks of houses for
240:37 - which we provided the characteristics in
240:39 - the X test but we are not providing the
240:42 - Y test so uh as usual like in case of
240:45 - training we are adding a constant with
240:47 - this library and then we are saying
240:49 - model. fitted model uncore fitted so the
240:51 - fitted model and then that predict and
240:54 - providing the test data and those are
240:55 - the test
240:57 - predictions now uh once we do this we
241:00 - can then get the test predictions and uh
241:03 - if we print those you can see that we
241:06 - are getting a least of house values
241:10 - those are the house values for the um um
241:14 - blocks of houses which were included as
241:16 - part of the testing data so the 20% of
241:19 - our entire data
241:21 - set uh like I mentioned just before in
241:24 - order to ensure that your model is
241:26 - performing well you need to check the OS
241:28 - assumptions so uh during the um Theory
241:32 - section we learned that there are a
241:34 - couple of assumptions that your model
241:35 - should satisfy and your data should
241:37 - satisfy for OLS to provide uh B unbiased
241:41 - and um efficient uh estimates which
241:45 - means that they are accurate their
241:48 - standard error is low something that um
241:51 - we are also seeing as part of the
241:53 - summary results and uh your estimates
241:56 - are accurate so the standard error is a
241:59 - measure that showcases how efficient
242:01 - your estimat are which means um do you
242:04 - have a high variation uh can the
242:08 - coefficients that you are showing in
242:09 - this table very a lot which means that
242:12 - you don't have accurate um coefficient
242:15 - and your coefficient can be all the way
242:17 - from one place to the other so the range
242:19 - is very L large which means that your
242:21 - standard error will be very large and
242:23 - this is a bad sign or you are dealing
242:26 - with an accurate estimation and uh it's
242:29 - more precise estimation and in that case
242:32 - the standard there will be low uh and
242:35 - unbias estimate means that your
242:37 - estimates are are a true representation
242:39 - of the pattern between each pair of
242:41 - independent variable and the response
242:43 - variable if you want to learn more about
242:45 - this IDE of bias unbias and then
242:48 - efficiency and sure to check the U
242:51 - fundamental statistics course at lunar
242:53 - Tech because it explains very clearly
242:56 - this Concepts in detail so here I'm
242:59 - assuming that you know or maybe you
243:01 - don't even need it but I would suggest
243:03 - you to know at higher level at least
243:06 - then uh let's quickly do the checking of
243:08 - oiless assumption so the first
243:10 - assumption is the linearity Assumption
243:12 - which means that your model is linear in
243:14 - parameters one way of checking that is
243:17 - by using your already fitted model and
243:21 - your uh predicted model so the Y uh uh
243:25 - test which are your true house median
243:27 - house values for your test data and then
243:30 - test predictions which are your uh
243:32 - predicted median house values for nonen
243:35 - data so you are using the uh True Values
243:38 - and the predicted values in order to um
243:41 - plot them and then to also plot the best
243:45 - fitted line in an ideal situation when
243:47 - you would make no error and your model
243:49 - would give you the exact True Values um
243:53 - and then see how well your um uh how
243:57 - linear is this relationship do we
243:58 - actually have a linear
244:00 - relationship now if the observed versus
244:03 - predicted values where the observed
244:05 - means the uh real uh test test wise and
244:09 - the predicted means the test predictions
244:12 - if this pattern is kind of linear and
244:15 - matching this perfect linear line then
244:18 - you have um assumption one that is
244:20 - satisfied your linearity assumption is
244:22 - satisfied and you can say that your uh
244:25 - data uh and your model is indeed linear
244:29 - in
244:30 - parameters then uh we have the second
244:33 - assumption which states that your uh
244:36 - sample should be random and this
244:38 - basically translates that the uh
244:41 - expectation of your error terms should
244:43 - be equal to zero and uh one way of
244:47 - checking this is by simply taking the
244:50 - residuales from your fitted model so
244:52 - model on score fitted and then that's
244:54 - residual so you take the residuales you
244:56 - obtain the average which is a good
244:59 - estimate of your expectation of errors
245:02 - and then this is the mean of residuales
245:04 - so the average uh
245:06 - residuales where the residual is the
245:09 - estimate of your true error terms and
245:12 - then uh here what I do is just I just
245:15 - round up uh to the two decimals behind
245:18 - uh the uh the point this means that uh
245:22 - we are getting uh this average amount of
245:26 - uh errors or the estimate of the errors
245:28 - which we are referring as residuales and
245:30 - if this number is equal to zero which is
245:32 - the case so the mean of the residuales
245:34 - in our model is zero it means that
245:36 - indeed the um uh expectation of the uh
245:41 - error terms at least the estimate of it
245:43 - expectation of the residuales is inde
245:46 - equal to zero another way of checking
245:48 - the um uh second assumption which is
245:51 - that the um moral uh has a is based on
245:55 - the random sample and the sample we are
245:56 - using is random which means that the
245:59 - expectation of the error terms is equal
246:01 - to zero is by plotting the residuales
246:04 - versus fitted values so uh we are taking
246:07 - the resid from the fitted model and we
246:10 - are comparing to the fitted values that
246:13 - comes from the model uh and we are
246:16 - looking at this um graph this scatter
246:20 - plot which you can see in here and we're
246:23 - looking where this um pattern is
246:29 - uh
246:30 - symmetric uh around the uh threshold of
246:34 - zero so you can see this line kind of
246:37 - comes right in the middle of this
246:39 - pattern which means that on average we
246:42 - have residuales that are across zero so
246:44 - the mean of the residuales is equal to
246:46 - zero and that's exactly what we were
246:48 - calculating also here therefore we can
246:51 - say that we are indeed dealing with a
246:52 - random
246:53 - sample this FL is also super useful when
246:56 - it comes to the fourth assumption that
246:58 - we will come a bit later so for now
247:01 - let's check the third assumption which
247:03 - is the Assumption of exogeneity so
247:05 - exogeneity means that uh each of our
247:09 - independent variables should be
247:10 - uncorrelated from the error terms so
247:13 - there is no omitted variable bias there
247:15 - is no um reverse causality which means
247:19 - that the uh independent variable has an
247:22 - impact on the dependent variable but not
247:24 - the other way around so dependent
247:26 - variable should not have an impact and
247:28 - should not cause the independent
247:30 - variable so for that there are few ways
247:33 - that we can deal with uh with this uh
247:36 - one way is just straightforward to
247:38 - compute the uh correlation coefficient
247:41 - between between each of these
247:42 - independent variables and the residuales
247:44 - that you have obtained from your fitted
247:45 - model the just simple uh technique that
247:48 - you can use in a very uh quick way to
247:51 - understand what is this uh correlation
247:54 - between each pair of independent
247:55 - variable and the residuals which are the
247:57 - best estimates of your error terms and
248:00 - in this way you can understand that
248:01 - there is a correlation between your
248:03 - independent variables and your error
248:05 - terms another way you can do that and
248:08 - this is more advanced and bit more um
248:11 - towards the econometrical side is by
248:13 - using this test which is called the
248:15 - Durban uh view housan test so this uh
248:19 - Durban view housan test is um a more
248:23 - professional more advanced way of uh
248:25 - using an econometrical test to find out
248:28 - whether you have um exogene so
248:31 - exogeneity sup is satisfied or you have
248:34 - endogenity which means that one or
248:36 - multiple of your your independent
248:38 - variables is potentially correlated with
248:41 - your error terms uh I won't go into
248:44 - detail of this test uh I'll put some
248:47 - explanation here and also feel free to
248:50 - uh check any uh introductory to
248:52 - econometrics course to understand more
248:55 - on this Duran Vu housan test for
248:58 - exogeneity assumption the fourth
249:00 - assumption that we will talk about is
249:02 - the homos skasis homosa assumption
249:05 - states that the error terms should have
249:08 - a variance that is constant which means
249:10 - that when we are looking at this
249:12 - variation that uh the model is making uh
249:15 - across uh different observations that uh
249:18 - when we look at them the variation is
249:20 - kind of constant so uh we have all these
249:24 - uh cases when the uh in observations for
249:27 - which the residuals are bit small in
249:29 - some cases bit large we have this miror
249:32 - when it comes to this figure with what
249:35 - we are calling heteros skos which means
249:37 - means that homos assumption is violated
249:40 - our error terms do not have a variation
249:42 - that is constant across all the
249:44 - observations and we have a high
249:46 - variation and different variations for
249:49 - different observations so we have the
249:51 - heteros issue we should consider a bit
249:54 - more um flexible approaches like uh GLS
249:59 - fgs GMM all bit more advanced
250:02 - econometrical
250:04 - algorithms so uh the final part of this
250:07 - case study will be to show you how you
250:09 - can do uh this all but for machine
250:13 - learning traditional machine learning
250:14 - site by using the psychic learn so uh in
250:18 - here um I'm using the um standard scaler
250:24 - function in order to uh scale my data
250:27 - because we saw uh in the summary of the
250:30 - table um that we got from the stats uh
250:33 - mos. API that our data is at a very high
250:37 - scale because the uh median house values
250:39 - are those large numbers the uh age uh
250:43 - the median age of the house is in this
250:45 - very large numbers that's something that
250:46 - you want to avoid when you are using the
250:49 - linear regression as a Predictive
250:50 - Analytics model when you are using it
250:52 - for interpreting purposes then you
250:54 - should keep the skilles because it's
250:56 - easier to interpret those values and to
250:59 - understand uh what is the difference in
251:01 - the median price uh of the house when
251:05 - you compare different characteristics of
251:06 - the box of houses but when it comes to
251:09 - using it for Predictive Analytics
251:11 - purposes which means that you really
251:13 - care about the accuracy of your
251:16 - predictions then you need to uh scale
251:19 - your data and ensure that your data is
251:21 - standardized one way of doing that is by
251:23 - using the standard scaler function uh in
251:26 - the pyit learn.
251:28 - preprocessing uh and uh the way I do it
251:30 - is that I initialize the scaler by using
251:34 - the standard scaler and then parenthesis
251:36 - which are just import from this psychic
251:38 - learn library and then uh I am uh taking
251:43 - this scaler I'm doing that fitore
251:46 - transform exrain which basically means
251:49 - take the independent variables and
251:51 - ensure that we scale and standardize the
251:54 - data and standardization simply means
251:56 - that uh we are
251:59 - standardizing the data that we have to
252:02 - ensure that um some large values do not
252:06 - wrongly influence the predictive power
252:08 - of the model so the the model is not
252:10 - confused by the large numbers and finds
252:13 - a wrong variation but instead it focuses
252:15 - on the a true variation in the data
252:18 - based on how much the change in one
252:21 - independent variable causes a change in
252:23 - the dependent
252:24 - variable here given that we are dealing
252:26 - with the supervised learning algorithm
252:29 - uh the exrain uh scaled will be then
252:32 - containing our standardized uh features
252:36 - so independent variables and then each
252:38 - test SC will contain our standardized
252:41 - test features so the Unseen data that
252:44 - the model will not see during the
252:45 - training but only during prediction and
252:48 - then what we will be doing is that we
252:50 - will also use the um y train and Y train
252:54 - uh is the dependent variable in now
252:57 - supervised model and why train
252:59 - corresponds to the training data so we
253:01 - will then first initialize the linear
253:03 - regression here so linear regression
253:05 - model from pyit learn
253:07 - and then uh we will initialize the model
253:10 - this is just the empty linear regression
253:12 - model and then we will take this
253:14 - initialized uh model and then we will
253:17 - fit them on the uh training data so
253:19 - exore trained uncore scale so this is
253:22 - the trained features and then the um uh
253:26 - dependent variable from training data so
253:28 - why
253:29 - train uh do you knowe that I'm not
253:32 - scaling the dependent variable this is a
253:33 - common practice cuz you don't want to uh
253:37 - standardize your dependent variable
253:39 - rather than you want to ensure that your
253:41 - features are standardized because what
253:43 - you care is about the variation in your
253:47 - features and to ensure that the model
253:50 - doesn't mess up when it's learning from
253:52 - those features less when it comes to
253:55 - looking into the impact of those
253:56 - features on your dependent
253:58 - variable so then uh I am fitting the uh
254:02 - model on this training data so uh
254:05 - features and independent variable and
254:07 - then I'm using this fitted uh model the
254:11 - LR which already has learned from this
254:14 - features and dependent variable during
254:15 - supervised training and then I'm using
254:19 - the X test scale so the test
254:21 - standardized uh data in order to uh
254:25 - perform the prediction so to predict the
254:28 - immediate house values for the test data
254:30 - unseen data and you can notice that here
254:33 - in no places I'm using white test white
254:36 - test I'm keeping to myself which is the
254:38 - dependent variable True Values such that
254:41 - I can then compare to this predicted
254:43 - values and see how well my motor was
254:46 - able to actually get the predictions now
254:50 - uh let's actually also do one more step
254:53 - I'm importing from the psyit learn the
254:56 - Matrix such as mean squared error uh and
254:59 - I'm using the mean squared error to find
255:01 - out how well my motel was able to
255:04 - predict those house prices so this means
255:07 - that uh we have on average we are making
255:10 - an error of
255:12 - 59,000 of dollars when it comes to the
255:14 - median house prices which uh dependent
255:18 - on what we consider as large or small
255:21 - this is something that we can look into
255:23 - so um like I mentioned in the beginning
255:26 - the uh idea behind linear regression
255:30 - using IND specific uh course is not to
255:34 - uh use it in terms of pure traditional
255:36 - machine learning but rather than to
255:38 - perform um causal analysis and to see
255:41 - how we can interpret it when it comes to
255:43 - the quality of the predictive power of
255:46 - the model then uh if you want to improve
255:50 - this model this can be considered as the
255:53 - next step you can understand whether
255:55 - your model is overfitting and then the
255:57 - next step could be to apply for instance
255:59 - the um lasso regularization so lasso
256:03 - regression which addresses the
256:05 - overfitting you can also consider going
256:07 - back and removing more outliers from the
256:10 - data Maybe the outliers that we have
256:12 - removed was not enough so you can also
256:14 - apply that factor then another thing
256:17 - that you can do is to consider bit more
256:19 - advanced machine learning algorithms
256:21 - because it can be that um although the
256:26 - um regression assumption is satisfied
256:29 - but still um using bit more flexible
256:32 - Motors like random Forest decision trees
256:34 - or boosting techniques will be bit more
256:36 - more appropriate and this will give you
256:38 - higher predictive power consider also uh
256:43 - uh working more with this uh scaled uh
256:46 - version or normalization of your data as
256:49 - the next step in your machine Learning
256:51 - Journey you can consider learning bit
256:53 - more advanced machine learning models so
256:56 - now when you know in detail what is
256:58 - linear regression and how you can use it
257:00 - how you can train and test a machine
257:03 - learning Model A simple one yet very
257:05 - popular one and you also know what is
257:08 - logistic progression and all these
257:09 - Basics you're ready to go on to the next
257:12 - step which is learning all the other
257:14 - popular traditional machine learning
257:16 - models think about learning decision
257:19 - trees for modeling nonlinear
257:21 - relationships think about learning
257:23 - bagging boosting random forest and
257:27 - different sours of optimization
257:28 - algorithms like gradi and descent HGD
257:31 - HGD with momentum Adam Adam V RMS prop
257:36 - and what is the difference between them
257:37 - and how you can Implement them and also
257:40 - consider a learning clustering
257:42 - approaches like K means uh DB skin
257:45 - hierarchial clust string doing this will
257:48 - help you uh to get more hands on and go
257:52 - to this next step when it comes to the
257:54 - machine learning once you have covered
257:56 - all these fundamentals you are ready to
257:59 - go one step further which is getting
258:01 - into deep
258:04 - Le thank you for watching this video If
258:06 - you like this content make sure to check
258:08 - all the other videos available on this
258:10 - channel and don't forget to subscribe
258:13 - like and comment to help the algorithm
258:15 - to make this content more accessible to
258:18 - everyone across the world and if you
258:20 - want to get free resources make sure to
258:23 - check the free resources section at
258:25 - lunch. and if you want to become a job
258:29 - ready data scientist and you are looking
258:31 - for this accessible boot camp that will
258:34 - help you to make your job ready data
258:35 - scientist consider enrolling to the data
258:38 - science boot camp the ultimate data
258:41 - science boot camp at
258:42 - l. you will learn all the theory the
258:45 - fundamentals to become a jbre data
258:48 - scientist you will also implement the
258:51 - learn theory into real world multiple
258:54 - data science projects beside this after
258:57 - learning the theory and practicing it
258:59 - with a real world case studies you will
259:02 - also prepare for your data science
259:04 - interviews and if you want to stay up to
259:06 - date with do recent developments in Tech
259:08 - what are the headlines that you have
259:10 - missed in the last week what are the
259:12 - open positions currently in the market
259:15 - across the globe and what are the tech
259:17 - startups that are making waves in the
259:19 - tech and sure to subscribe to the data
259:21 - science Nai newsletter from
259:29 - [Music]
259:31 - lunarch