00:00 - this is my machine and for some reason
00:03 - it's learning
00:05 - what's up code squad if you're new here
00:07 - welcome to the squad and if not welcome
00:10 - back my name is kylie ying and today i'm
00:13 - going to be talking about machine
00:16 - learning more specifically we'll dive
00:18 - into supervised learning and then we'll
00:21 - learn how to use tensorflow to create a
00:23 - neural net and then use a neural net for
00:26 - some text classification
00:28 - sound exciting let's get started
00:33 - all right jumping straight into things
00:35 - the first thing that we're going to do
00:37 - is go to
00:38 - colab.research.google.com
00:43 - and it'll pull up a page like this
00:45 - you're going to click new notebook this
00:47 - is going to create a new notebook and
00:49 - let's retitle this to free code camp
00:52 - tutorial
00:54 - okay it doesn't really matter you know
00:56 - what you actually
00:58 - rename it to as long as you have a
01:00 - notebook open and
01:02 - just in case you know you have some
01:03 - future notebooks that's why you want to
01:04 - rename all of them
01:06 - so the typical libraries that i'll
01:08 - import when i usually start a computer
01:11 - data science project
01:13 - are
01:14 - numpy so
01:16 - import numpy as np
01:18 - pandas import pandas as pd
01:22 - and then import
01:24 - matplotlib
01:28 - okay so because i'm going to be using
01:30 - tensorflow in this video i'm also going
01:32 - to import
01:34 - tensorflow
01:36 - as tf
01:37 - and then import tensorflow
01:41 - hub as hub
01:43 - okay
01:43 - now if you click shift enter it'll run
01:46 - this cell
01:47 - another thing that you can do is click
01:49 - this little play button over here that
01:51 - will also run the cell
01:53 - so
01:55 - cool
01:56 - yeah if we click that that runs the cell
01:58 - as well
02:00 - all right so the first thing that we're
02:01 - going to need to do before we can
02:02 - actually do any data analysis is upload
02:05 - a data file
02:06 - so this little folder over here this is
02:09 - where we manage our data or sorry this
02:12 - is where we manage our files
02:14 - and
02:15 - i'm just going to drag and drop the csv
02:17 - file the link is in the description
02:18 - below
02:20 - into here
02:21 - so click ok
02:23 - and you'll see that this is currently
02:26 - being uploaded
02:29 - sweet
02:33 - might take a while because this is
02:36 - a pretty big data set
02:41 - we can wait until this yellow
02:44 - thing goes all the way around
02:46 - all right so
02:48 - pause
02:50 - all right sweet now wine reviews.csv has
02:53 - uploaded to this folder which means that
02:55 - we have access to it in this notebook
02:57 - now so now what we can actually do is
02:59 - import that csv file as a data frame
03:02 - using pandas which is this library here
03:05 - that we imported
03:06 - so what i'm going to do is say df equals
03:09 - pd.readcsv
03:10 - so this is just a command that lets us
03:12 - read a csv file
03:14 - and i'm going to type in winereviews.csv
03:17 - and then also i'm going to
03:19 - use certain columns so if we actually
03:22 - took a look at this
03:26 - uh
03:28 - data frame here we can call
03:30 - df.head
03:32 - we see that we have this like unnamed
03:35 - column over here that contains a bunch
03:38 - of
03:39 - like indices and we don't really want
03:41 - that so what i'm going to do is say use
03:43 - columns
03:45 - give a list of the columns that i want
03:47 - so i want maybe the country
03:49 - the description
03:50 - [Music]
03:51 - the points
03:54 - the price
03:57 - and you know i don't really care about
03:59 - some of these things like the twitter
04:01 - handle
04:02 - i think the variety might be cool to
04:04 - check out
04:07 - maybe the winery
04:10 - all right so
04:11 - let's
04:12 - run that and then now take another look
04:15 - at the data set so now in our data set
04:17 - we have the country the description the
04:20 - points the price
04:21 - the variety and the winery something
04:24 - that i think would be really cool is to
04:26 - see if we can try to guess or ballpark
04:29 - you know whether something falls on the
04:31 - lower end of the point spectrum or the
04:33 - higher end of the point spectrum given
04:35 - the description that we have here
04:38 - so
04:39 - the first thing that we do see also is
04:40 - that we have a few none type values in
04:43 - our data frame that's what this here
04:45 - stands for it means there's no value
04:47 - recorded there
04:48 - so
04:50 - let's just focus on these two columns
04:52 - actually the description and the points
04:54 - because i think that's what we'll try to
04:57 - like align we're gonna use a description
05:00 - to predict the points
05:02 - something that we can do is a command
05:04 - called drop nah
05:06 - don't know if that's you know the right
05:08 - way to say it but in my head that's what
05:10 - i say
05:11 - and we can say subset which means that
05:13 - you know in a subset of these
05:16 - columns that's where we're going to try
05:18 - to drop the pr like the the nand column
05:21 - so here
05:22 - i'm going gonna say description
05:25 - and then points
05:27 - run that
05:28 - okay so
05:30 - don't even know if this has changed
05:33 - okay i mean we still see this because we
05:34 - didn't drop anything in that column
05:36 - because it doesn't really matter to us
05:38 - let's just quickly see let's plot like
05:41 - the points column to see the
05:42 - distribution of the points so we can use
05:45 - matplotlib for that
05:46 - so let's do plt.hist
05:50 - so this is going to be a histogram which
05:52 - shows the distribution of values
05:54 - um because it's only a one-dimensional
05:57 - variable
05:58 - so
05:59 - let's do df dot points which calls like
06:02 - this points column
06:04 - and let's just say bins equals 20.
06:07 - now if we do plt.show this should
06:09 - display our plot
06:12 - all right i didn't include the title and
06:14 - the axes because this is kind of just
06:16 - for us to
06:18 - quickly look at it if you were to
06:21 - actually plot
06:22 - you know this as
06:24 - some sort if you were to if you wanted
06:26 - to actually plot this for other people
06:29 - to view you might want to say plt.title
06:32 - is know points histogram
06:36 - and the plt label
06:38 - the y label so that label for the y axis
06:41 - would be
06:42 - you know n the number of values that lie
06:45 - in each bin
06:46 - and then the x label i would say
06:51 - uh
06:52 - it would be the point
06:54 - so if we see something like this tada
06:57 - there is our plot this is our
06:58 - distribution of points so we see that it
07:02 - seems like it's on a range from 80 to
07:04 - 100
07:05 - which means that
07:06 - let's try to classify these reviews as
07:10 - below
07:11 - 90 and then above 90. so we're splitting
07:13 - this into two different categories low
07:16 - which is over here and high which is
07:18 - over here
07:20 - now before
07:21 - we proceed with the rest of this
07:23 - tutorial
07:24 - we're going to learn a little bit about
07:26 - machine learning because you can't
07:28 - really just dive into the code without
07:30 - understanding
07:32 - what's going on or at least having you
07:33 - know a vague sense of what's going on
07:35 - which is what i'm going to try to teach
07:37 - in this video so let's hop over to some
07:40 - more
07:41 - theoretical aspects of machine learning
07:47 - so first let's talk about what is
07:49 - machine learning
07:51 - well machine learning is a subdomain of
07:53 - computer science that focuses on
07:55 - algorithms which help a computer learn
07:58 - from data without explicit programming
08:01 - for example let's say i had a bunch of
08:03 - sports articles and a bunch of recipes
08:06 - explicit programming would be if i told
08:09 - the computer hey look for these specific
08:11 - words such as
08:13 - goal
08:13 - or player or ball
08:16 - in this text and if it has any of those
08:18 - words then it's a sports article
08:20 - on the other hand if it has flour sugar
08:23 - oil eggs then it's a recipe
08:26 - that would be explicit programming but
08:28 - in machine learning what the goal is i
08:31 - instead provide the computer with some
08:33 - sort of algorithm for the computer to be
08:35 - able to decide for itself hey these are
08:38 - words associated with the sports article
08:40 - and these are words associated with a
08:42 - recipe
08:44 - sound cool
08:45 - it is so stay tuned
08:49 - now these days we've heard a lot of
08:50 - words kind of you know being thrown out
08:52 - there such as artificial intelligence
08:54 - machine learning data science cloud
08:58 - blockchain crypto et cetera et cetera et
09:00 - cetera
09:01 - now we won't talk about the cloud or
09:04 - crypto or blockchain
09:06 - but let's kind of talk about ai
09:09 - versus ml versus data science and what
09:11 - the difference between all of these is
09:14 - so artificial intelligence is an area of
09:17 - computer science where the goal is to
09:20 - actually enable computers and machines
09:22 - to perform human-like tasks and to
09:24 - simulate human behavior
09:27 - now machine learning is a subset of ai
09:30 - that tries to solve a specific problem
09:33 - and make predictions using data
09:39 - now data science is a field that
09:41 - actually attempts to
09:43 - find patterns and draw insights from the
09:45 - data and you know data scientists might
09:48 - actually use some sort of machine
09:49 - learning techniques while they're doing
09:51 - this
09:52 - and the kind of common theme is that all
09:55 - of these overlap a little bit and all of
09:58 - them might use machine learning
10:00 - so we'll be focusing on machine learning
10:04 - there are a few different types of
10:05 - machine learning so the first one is
10:08 - supervised learning which uses labeled
10:10 - inputs
10:12 - meaning that the input has a
10:13 - corresponding output label to train
10:16 - models and to learn outputs
10:20 - so for example let's say i have these
10:23 - pictures of some animals so we have a
10:25 - cat a dog and a lizard
10:29 - well in supervised learning we would
10:31 - also have access to these labels so we
10:33 - would know that this picture is
10:35 - associated with a cat
10:37 - this picture is associated with a dog
10:40 - and this picture is associated with a
10:42 - lizard
10:43 - and now because we have all of these
10:46 - input output pairings we can stick this
10:48 - data into a model and hope that the
10:50 - model is able to
10:52 - then generalize to other future pictures
10:55 - of cats or dogs or lizards and correctly
10:58 - classify them
11:01 - now there's also such thing as
11:03 - unsupervised learning and in this case
11:05 - it uses unlabeled data in order to learn
11:08 - certain patterns that might be hiding
11:10 - inside the data
11:12 - so let's go back to our pictures of our
11:15 - animals
11:16 - and now we might have multiple pictures
11:18 - of cats multiple pictures of dogs
11:20 - multiple pictures of lizards
11:22 - and also just a quick note that we would
11:24 - also have these in supervised learning
11:26 - but all of these would have
11:28 - the cat label the dog label and the
11:30 - lizard label associated with them
11:34 - but okay now going back to supervised
11:36 - learning we have all these pictures
11:38 - and what our algorithm is going to want
11:41 - to do it wants to learn hey these are
11:44 - all something you know of group a
11:46 - because they're all similar in some way
11:49 - these are all group b
11:51 - and these are all group c
11:53 - and it basically tries to learn this
11:55 - inherent structure or pattern within the
11:58 - things that you know we're feeding it
12:01 - finally there's reinforcement learning
12:04 - so in reinforcement learning there's an
12:06 - agent that's learning in an interactive
12:08 - environment and it's learning based off
12:11 - of rewards and penalties
12:14 - so let's think about a pet
12:17 - for example
12:18 - and every single time our pet does
12:20 - something that we want it to so for
12:22 - example some sort of trick we give it a
12:25 - treat
12:26 - such as in this picture
12:28 - now
12:29 - if you know if our pet does something
12:31 - that we don't want it to for example pee
12:33 - on our flowers
12:36 - then we might scold the pet and the pet
12:38 - would then
12:40 - like the pet would then start learning
12:41 - okay
12:42 - you know it's good when i do this trick
12:44 - and it's bad when i pee on the flowers
12:47 - this is kind of what reinforcement
12:49 - learning is except instead of your pet
12:52 - it's a computer or i guess an agent
12:55 - that's being stimulated by your computer
13:01 - now in this specific video we're just
13:04 - going to be focusing on supervised
13:06 - learning so that's using these labeled
13:08 - input and output pairings in order to
13:11 - make future predictions
13:15 - okay so let's talk about supervised
13:18 - learning
13:21 - so this is kind of what our machine
13:23 - learning model is we have a series of
13:27 - inputs that we're feeding into some
13:29 - model and then this model is generating
13:32 - some sort of output or prediction
13:34 - and the coolest part is that this model
13:38 - we're as we as programmers are not
13:40 - really telling this model any specifics
13:43 - we're not explicitly programming
13:45 - anything
13:46 - rather this model
13:48 - our computer is trying to learn patterns
13:51 - amongst you know these input this input
13:54 - data in order to come up with this
13:57 - prediction
13:59 - so
14:00 - a list of inputs such as the ones here
14:04 - this is what we call a feature vector
14:07 - we'll talk about that in some more
14:08 - detail later
14:10 - so let's quickly talk about the
14:11 - different types of features or inputs
14:14 - that we might be able to feed our model
14:16 - so the first type is qualitative data
14:18 - and this means it's categorical
14:21 - which means that there are finite
14:23 - numbers of categories or groups
14:26 - so one common example is actually gender
14:29 - and i know that this might seem a little
14:31 - bit outdated but
14:33 - please bear with me because i just want
14:34 - to get the point of a qualitative
14:37 - feature across
14:39 - so here in this picture we see that
14:41 - there is a girl and a boy so let's take
14:44 - these two different groups first you
14:45 - might notice that there's not exactly a
14:48 - number associated with being a girl or
14:50 - being a boy
14:52 - so that's the nature of qualitative data
14:54 - if it doesn't have some sort of number
14:56 - associated with it it's probably
14:58 - qualitative
15:00 - now let's take a look over here there's
15:01 - different types of flags like maybe
15:04 - these represent you know your
15:05 - nationality might be a qualitative
15:08 - feature qualitative features don't have
15:10 - to necessarily be exclusive
15:12 - but they just don't have a number
15:13 - associated with it and they belong in
15:16 - groups so you might have us you might
15:19 - have canada you might have mexico et
15:22 - cetera et cetera
15:23 - these two specific qualitative features
15:26 - are known as
15:28 - nominal data in other words they don't
15:30 - have any inherent ordering in it
15:34 - now our computers don't really
15:36 - understand
15:37 - like labels or english too well
15:41 - right our computers are really really
15:43 - good at understanding numbers so how in
15:46 - the world do we convey this
15:49 - in numbers well we use something called
15:52 - one hot encoding so
15:54 - suppose we have you know a
15:57 - vector that represents these four
15:59 - different nationalities usa india canada
16:02 - and france
16:04 - what we're going to do
16:05 - is we're going to market with a 1 if
16:08 - that category applies to you and 0 if it
16:11 - doesn't
16:13 - so for somebody who has us nationality
16:16 - your vector might look like 1 0 0 0. for
16:20 - india it might be 0 one zero zero
16:22 - canada zero zero one zero and france
16:25 - zero zero zero one
16:27 - so that's one hot encoding it turns
16:29 - these different groups into a vector
16:32 - and
16:33 - i guess switches on that category with a
16:36 - one
16:38 - if that category applies and zero if it
16:41 - doesn't
16:42 - now there are also other types of
16:43 - qualitative features so something like
16:46 - age even though a number might be
16:47 - associated with it if we take different
16:50 - groupings of age so for example baby
16:54 - kid
16:56 - gen z
16:58 - young adult
17:00 - boomer
17:01 - etc etc
17:03 - if we take these different categories
17:05 - then this actually becomes a qualitative
17:08 - data set because you can assign one of
17:11 - these categories to somebody and it
17:14 - doesn't necessarily map to
17:16 - a specific number
17:19 - another example of categorical data
17:21 - might be a rating system of bad to good
17:27 - and this is what we call ordinal data so
17:29 - even though it's qualitative it has some
17:31 - sort of inherent ordering to it so hence
17:34 - the name ordinal data
17:38 - now in order to encode this into numbers
17:40 - we might just use a system like one two
17:42 - three four five
17:43 - quick note the reason why we use one hot
17:46 - encoding if for our nationalities but we
17:48 - can use one two three four five
17:50 - for ordinal data
17:52 - is because
17:53 - let's think about things this way our
17:55 - computer knows that two is closer to one
17:57 - than five right and in a case like this
18:00 - it makes sense because two is slightly
18:03 - less worse than one whereas five is
18:05 - actually really good so of course two
18:07 - should be closer to one than five
18:09 - but if we go back to our nationality
18:11 - example it doesn't really make sense to
18:14 - say you know to rate usa one india to
18:18 - canada three and france four because we
18:21 - could also switch around these labels
18:23 - and
18:24 - they would still be distinct groups and
18:26 - they're just different it's not like one
18:28 - of them is closer to the other than
18:30 - something else
18:32 - they're just different i mean i guess it
18:34 - depends on the context but
18:36 - if we're talking nationality they're
18:37 - just different right so you can't
18:39 - necessarily say two which i think was i
18:43 - assigned to india is closer to one usa
18:46 - than france which is four
18:48 - like a computer would think that but
18:51 - just thinking about it logically that
18:53 - wouldn't really make sense so that's the
18:55 - real difference between these two types
18:57 - of qualitative data sets is how you want
18:59 - to encode them for your computer
19:02 - when we're talking about features we
19:03 - also have quantitative features and
19:06 - quantitative features are numerical
19:08 - valued inputs and so this could be
19:11 - discrete and it could also be continuous
19:13 - so for example if i wanted to
19:16 - measure the size of my desk that would
19:19 - be a quantitative variable
19:21 - if i wanted to
19:23 - tell how hot you know what the
19:25 - temperature of
19:27 - this fire was that's also another
19:29 - quantitative variable
19:32 - another type of quantity and these two
19:34 - are both continuous variables
19:36 - now let's say that i'm hunting for
19:39 - easter eggs and
19:41 - this is how many easter eggs i collect
19:43 - in my basket
19:45 - it probably doesn't make too much sense
19:47 - to say you know i have 7.5 but rather i
19:49 - have seven that that would make sense or
19:51 - eight you know somebody else might have
19:53 - two which means that i won but you know
19:55 - aside from that
19:57 - this is something that would be a
19:58 - discrete quantitative variable because
20:01 - we do have it's it's not continuous
20:03 - right there are discrete values integers
20:06 - positive integers
20:08 - that would be able to describe this data
20:10 - set over here this is continuous and
20:13 - over here this is discrete
20:15 - those features those are the different
20:17 - types of inputs that we might be able to
20:19 - feed into our model
20:21 - what about the different types of
20:23 - predictions that we can actually make
20:25 - with the model
20:27 - so there are a few different tasks that
20:29 - you know we have in supervised learning
20:32 - so there's classification which means
20:34 - that we're predicting discrete classes
20:38 - for example let's say we have a bunch of
20:41 - pictures of food
20:43 - so
20:43 - you know here we have a hot dog we have
20:45 - a pizza and we have an ice cream cone an
20:47 - example of classification might be
20:49 - okay well this gets mapped to a hot dog
20:52 - label and this gets mapped to pizza and
20:55 - this gets mapped to ice cream and if we
20:57 - have any additional photos of one of
20:58 - these we want to map them to one of
21:00 - these three classes
21:02 - this is known as multi-class
21:04 - classification because we have a bunch
21:06 - of different classes
21:08 - that we're trying to map it to so hence
21:10 - the name multi-class
21:13 - now
21:14 - what if instead of hot dog pizza and ice
21:17 - cream we had another model that just
21:19 - told us whether or not something was a
21:20 - hot dog so this over here is a hot dog
21:23 - and these over here are simply
21:25 - not hot dogs
21:27 - well this is known as binary
21:28 - classification because there's only two
21:31 - hence binary
21:32 - it's ready
21:34 - please god what would you say
21:37 - if i told you there is an app on the
21:40 - mind we're past that part just demo it
21:43 - okay
21:44 - let's start with a hot dog
21:55 - oh
22:00 - my beautiful little adriatic friend i'm
22:03 - going to buy you the palapa of your life
22:06 - we will have 12 posts braided palm
22:08 - leaves
22:09 - you'll never feel exposed again i'm
22:11 - gonna be rich
22:12 -  you guilfoyle do pizza let's do
22:14 - pizza yeah
22:16 - hey zach
22:23 - not hot dog
22:25 - wait what the huh
22:27 - that's that's it it only does hot dogs
22:30 - no and a nah hot dog
22:37 - now let's talk about some other examples
22:39 - of classification to kind of really
22:41 - drill this down for you
22:43 - other types of binary classification
22:45 - might be
22:46 - positive or negative so if we have
22:48 - restaurant reviews positive or negative
22:50 - two different categories
22:52 - something else might be pictures of cats
22:54 - versus pictures of dogs
22:56 - cool cats and dogs
22:57 - and then maybe we have a bunch of emails
22:59 - and we're trying to create a spam filter
23:01 - one another example of binary
23:03 - classification might be spam or not spam
23:07 - now what about multi-class
23:09 - classification
23:11 - so going back to our first example of
23:13 - the cats and dogs
23:15 - well we also had a lizard you know you
23:17 - might also have a dolphin so different
23:19 - types of animals that might be something
23:20 - that falls under multi-class
23:22 - classification
23:24 - another example might be different types
23:26 - of fruits so orange apple and pear
23:30 - another example might be different types
23:31 - of plant species but here basically you
23:34 - have different types of classes and you
23:36 - have multiple of them
23:37 - more than two
23:40 - all right there's also
23:42 - something known as regression and in
23:45 - regression what we're trying to do is
23:47 - predict continuous values
23:49 - so one example of regression might be
23:51 - you know this is the price of ethereum
23:54 - and we want to predict what the price
23:56 - will be at tomorrow
23:57 - well there are so many different values
23:59 - that we can predict for that and they
24:01 - don't necessarily fall under classes
24:03 - like classes just doesn't intuitively
24:05 - make sense instead it's just a number
24:08 - right it's a continuous number so that's
24:10 - an example of regression
24:12 - or it might be what is the temperature
24:15 - going to be tomorrow that's another
24:17 - example of regression
24:19 - might be what is the value of this house
24:21 - given you know how many stores it has
24:23 - how many garages it has what is its zip
24:25 - code et cetera et cetera
24:29 - okay
24:31 - so now that we've talked about our
24:32 - inputs now that we talked about that
24:35 - now that we've talked about our inputs
24:36 - and now that we've talked about our
24:37 - outputs that's pretty
24:40 - that's pretty much you know machine
24:41 - learning in a nutshell
24:43 - except for the model so let's talk about
24:46 - the model
24:47 - okay so before i dive into specifics
24:50 - about a model
24:51 - let's briefly discuss
24:53 - how do we actually make this model learn
24:55 - and how can we tell whether or not
24:58 - it's actually learning
25:01 - we'll actually use this data set in a
25:02 - real example but here let's just briefly
25:05 - talk about what this represents
25:08 - so this data set comes from a certain
25:11 - group of people and this outcome is
25:14 - whether or not they have diabetes and
25:16 - now all these other numbers over here
25:19 - these are metrics of you know how many
25:21 - pregnancies they've had what their
25:23 - glucose numbers are like what their
25:24 - blood pressure is like and so on
25:27 - so we can see that all of these are
25:28 - actually quantitative variables you know
25:30 - they might be
25:32 - discrete or they might be continuous
25:35 - but
25:36 - they are
25:37 - quantitative variables
25:40 - okay so each row in this data set
25:44 - represents a different sample in the
25:46 - data so in other words each row
25:48 - represents one person in our data set
25:52 - now each column is a different feature
25:54 - that we can feed into our data set and
25:56 - by feature i just i literally just mean
25:58 - like the different columns so this one
26:00 - here is blood pressure this one here is
26:02 - number of pregnancies this one here is
26:05 - insulin numbers and so on
26:08 - except for this one over here this one
26:10 - is actually the output label that
26:12 - we want our model to be able to
26:13 - recognize
26:17 - now these values that we actually plug
26:19 - into the model again this is what we
26:21 - would call our feature vector
26:24 - and this this is the target for that
26:27 - feature vector so this is the output
26:28 - that we were trying to predict
26:31 - this over here this is known as the
26:34 - features matrix which we call big x and
26:38 - all these outcomes together we call this
26:40 - the labels or the targets vector y
26:44 - let's kind of abstract this to a piece
26:46 - of chocolate or a chocolate bar like
26:48 - this
26:50 - and you know we have all the numbers
26:52 - that represent our x matrix over here
26:55 - and the values our outputs y our target
26:58 - sorry over here
27:01 - now
27:01 - each of these features so this this is
27:03 - our feature vector we're plugging this
27:05 - into the model the model is making some
27:08 - sort of prediction
27:09 - and then we're actually comparing this
27:11 - to our target in our data set
27:15 - and then whatever difference here we use
27:17 - that for training because hey if we're
27:19 - really far off we can tell our model and
27:21 - be like hey can you make this closer and
27:23 - if we're really close then
27:25 - we tell our model hey keep doing that
27:26 - that's really good
27:29 - okay so this is our entire bar of
27:32 - chocolate so let's say this and this bar
27:34 - here represents all the data that we
27:36 - have access to
27:38 - do we want to feed this entire thing
27:40 - into our model and use that to train our
27:42 - model
27:43 - i mean you might think okay the more
27:44 - data the better right like if i'm on if
27:48 - i'm trying to look for a restaurant i'd
27:50 - rather have a thousand reviews than
27:53 - 10. but when we're using but when we're
27:55 - building a model we don't want to use
27:58 - all of our data in fact we want to split
28:00 - up our data because we want some sort of
28:02 - metric to see how our model will
28:04 - generalize
28:06 - so we split up our chocolate bar into
28:08 - the training data set
28:10 - the validation data set and the testing
28:12 - data set
28:15 - our training data set is what we're
28:17 - going to feed into our model and you
28:18 - know this might give us an output
28:21 - we again we check it against the real
28:23 - output and we
28:25 - find something called the loss which
28:27 - we'll talk about in a second but you can
28:29 - think of the loss as a measure of how
28:32 - far off we are so how far off are we put
28:35 - that into a number value and then feed
28:38 - that back into the model and that's
28:40 - where we're making adjustments this
28:42 - process is called training
28:46 - now we also have this validation set
28:48 - so this validation set we also feed into
28:50 - the model and then we can actually
28:52 - assess the loss
28:54 - on this validation set because again we
28:56 - have the real answer and we have this
28:58 - prediction and we can see how far off we
29:00 - are
29:02 - but this validation set is actually used
29:04 - as kind of more of a reality check
29:06 - during or after the training to ensure
29:08 - that our model can handle unseen data
29:11 - because remember up until this point our
29:13 - model is only being trained
29:15 - with our training set data
29:19 - okay so for example if i have a bunch of
29:21 - different models
29:23 - and
29:24 - all of these are my validation data sets
29:26 - and these are the predictions well okay
29:29 - this loss over here is kind of high this
29:32 - one's a little bit closer but look this
29:34 - one is the lowest we want to actually
29:37 - decrease the difference between our
29:38 - prediction and our true target
29:42 - and so another use case for the
29:43 - validation data set is to actually say
29:45 - okay well model c seems to perform the
29:48 - best on unseen data so let's take model
29:52 - c now once we've selected model c
29:56 - then we can actually go back and use our
29:58 - test set which again is unseen data
30:01 - and we plug that into model c see how it
30:04 - performs and then we can use that metric
30:08 - compared to you know our our targets
30:11 - as a final reported performance
30:14 - this test set is used to kind of check
30:16 - how generalizable the final model is
30:22 - okay
30:22 - so i kind of touched on you know
30:25 - something called a loss function
30:27 - but
30:28 - what exactly does that mean and what
30:30 - exactly
30:31 - how do we how do we quantify how
30:34 - different things are
30:37 - well
30:37 - this would probably give us a higher
30:39 - loss
30:40 - than this right like we would we would
30:42 - want that too because it's a little bit
30:44 - further off
30:45 - and something like that should be a lot
30:48 - further off which means that our loss
30:49 - function
30:51 - the value output from the loss function
30:53 - should be a lot higher than the previous
30:55 - two that we just saw
30:57 - okay so there are a few different types
30:59 - of loss functions so let's put our
31:02 - mindset in like in terms of regression
31:04 - for a second so we're trying to predict
31:06 - a value that's on a continuous scale so
31:09 - this might be
31:10 - uh the temperature tomorrow right
31:13 - now
31:14 - if we have a bunch of different cities
31:16 - that we're trying to predict then we
31:18 - have a bunch of different points right
31:20 - so this here y real this is the actual
31:24 - value that we found in our data set and
31:27 - why predicted this is a value that our
31:29 - model has output so what we're trying to
31:31 - do is we're trying to find the
31:32 - difference between these two values and
31:35 - then use the absolute value of that
31:37 - and then add all of these up you know
31:40 - for every single point in our data set
31:41 - so all the different cities
31:44 - in order to calculate the loss so in
31:46 - other words what we're doing is we're
31:47 - literally just comparing hey for every
31:49 - single city how different is our
31:52 - predicted value and the real value
31:55 - and then sum up all of those values
31:58 - so
31:59 - as you can see you know this is
32:01 - basically just an absolute value
32:03 - function so that's what l1 loss is
32:06 - now if we're really close if our
32:08 - predictions are really good then you can
32:10 - see how this loss becomes really small
32:12 - and if our values are really far off
32:14 - well that becomes pretty large right
32:17 - there's also another type of loss called
32:19 - l2 loss which is the same idea but
32:21 - instead of using the absolute value
32:22 - function we square everything
32:26 - so this is also known as mean squared
32:28 - error which you might have heard of
32:30 - basically here instead of summing up all
32:34 - the differences we actually square all
32:37 - the differences and then we sum those up
32:39 - so again this is what a quadratic
32:41 - formula looks like so this is what the
32:43 - squares would look like
32:45 - and again as you can see if we're only
32:48 - off by a tiny bit our loss is really
32:49 - small which means that it's good and if
32:51 - we're off by a lot then our loss gets
32:53 - really really big really fast
32:57 - okay now let's think about the
32:58 - classification mindset when we're trying
33:00 - to predict let's say just two different
33:02 - classes so binary classification
33:04 - well your output is actually a
33:06 - probability value which is associated
33:08 - with how likely it is to be of class a
33:12 - so
33:13 - if it's closer to one then class a seems
33:15 - to be more likely and if it's closer to
33:17 - zero then it's probably class b
33:20 - so
33:20 - in binary cross entropy loss what
33:23 - happens is you're taking
33:24 - the real value times the log of the
33:27 - predicted value and then adding that
33:29 - with 1 minus the real value times the
33:31 - log of 1 minus the predicted value
33:34 - summing that up and using a
33:36 - normalization factor
33:38 - you don't really have to know this too
33:40 - well
33:41 - this is a little bit you know more
33:42 - involved mathematically but you just
33:44 - need to know that loss decreases as the
33:47 - performance gets better
33:50 - so one of the metrics of performance
33:53 - that we can talk about in
33:55 - classification specifically is accuracy
33:59 - so let's say that we have a bunch of
34:01 - pictures here and we want to predict
34:04 - their labels
34:05 - so here i also have the actual values so
34:07 - of course this is an apple this is
34:09 - orange apple apple etc and use your
34:11 - imagination think that you know these
34:13 - two are slightly different from the
34:14 - original
34:16 - well let's say that our model is
34:18 - predicting apple orange orange apple
34:22 - so you know we got this right we got
34:24 - this right we got this wrong and we got
34:26 - this right
34:27 - so the accuracy of this model is 3 out
34:30 - of 4 or 75 percent
34:33 - if you just think about it in english
34:34 - that makes sense right like
34:36 - how accurate our model is is how many
34:38 - predictions it correctly classifies
34:42 - up until now we've talked about what
34:44 - goes into our model the features what
34:46 - comes out of our model you know what
34:48 - type of prediction it is whether we're
34:49 - doing classification or regression but
34:52 - we haven't really talked about the model
34:54 - itself so let's start talking about the
34:57 - model
34:58 - and that brings me to
35:00 - neural nets okay so the reason why i'm
35:04 - going to cover neural nets is because
35:06 - they're very popular and they can also
35:08 - be used for classification
35:10 - and regression
35:12 - now
35:13 - something that i do have to mention
35:15 - though is that neural nets have become
35:17 - sometimes a little bit too popular
35:20 - and they are being sometimes maybe
35:22 - overused there are a lot of cases where
35:25 - you don't need to use a neural net and
35:27 - if you do use a neural net it's kind of
35:29 - like using a sledgehammer to crack an
35:31 - egg it's a little bit you know
35:34 - too much
35:35 - there are plenty of other models that
35:37 - can also do classification and
35:39 - regression and sometimes
35:42 - the simpler the model the better and the
35:46 - reason for that is because you don't
35:49 - want something that's so good at
35:50 - predicting your training data set that
35:53 - you don't that you know it's it's not
35:55 - good at generalizing and often the thing
35:57 - with neural nets is that they are very
35:59 - much a black box the people who create
36:01 - the neural nets
36:03 - don't really know what's going on inside
36:05 - the network itself when you look at some
36:07 - of these other models when you look at
36:10 - other types of models in machine
36:11 - learning oftentimes those might be a
36:13 - little bit more
36:15 - transparent than a neural net with a
36:17 - neural net you just have this network
36:19 - with a ton of parameters and sometimes
36:21 - you can't really explain why certain
36:23 - parameters are higher than others
36:26 - and so you just the whole question
36:28 - behind why
36:30 - is a little bit
36:32 - lacking sometimes
36:35 - but with that being said let that be
36:36 - your warning we're going to talk about
36:38 - neural nets anyways because
36:40 - they are a great tool for classification
36:42 - and for regression
36:45 - all right let's get started
36:47 - so as i mentioned you know there are a
36:49 - ton of different machine learning models
36:51 - this one here is called the random
36:53 - forest this one here could just be
36:55 - classic linear regression this one is
36:57 - called a support vector machine
37:00 - and
37:01 - these different types of models they
37:04 - have their pros and cons
37:06 - but we're going to be talking about
37:08 - neural networks and this is kind of what
37:10 - a neural net looks like actually this is
37:12 - exactly what a neural net looks like
37:15 - you have your inputs they go towards
37:17 - some layer of neurons and then you have
37:20 - some sort of output but let's take a
37:23 - closer look at one of these neurons and
37:26 - see exactly what's going on
37:29 - okay so as i just mentioned you have all
37:30 - of your inputs these are our features
37:33 - remember how we talked about feature
37:35 - vectors so this would be a feature
37:37 - vector with n different features
37:40 - now
37:41 - each of these values remember because we
37:44 - our computer really likes values each of
37:47 - these values is multiplied by a weight
37:50 - so that's the first thing that happens
37:51 - you multiply your input
37:53 - by some weight
37:55 - and then
37:56 - all of these weights go into a neuron
37:59 - and this neuron basically just sums up
38:02 - all these weights times the input values
38:05 - and then you add a little bias to it so
38:08 - this is just some number that you add in
38:11 - addition to
38:13 - the sum product of all of these
38:16 - and then the output of the sum of all of
38:18 - these plus the bias goes into an
38:20 - activation function and an activation
38:23 - function we'll dive into that a little
38:24 - bit later but you can think of it as
38:26 - just some function that will take this
38:28 - output and alter it a little bit before
38:32 - we pass it on
38:34 - and it could be the output but this is
38:36 - the output of a single neuron
38:39 - over here
38:41 - okay and then once you have a bunch of
38:43 - these neurons all together they form a
38:46 - neural network which kind of looks
38:48 - something like this
38:50 - this is just a cool picture that i found
38:52 - on wikipedia
38:55 - all right so let's take a step backwards
38:57 - and talk about this activation function
38:58 - because i just kind of glossed over it
39:01 - and i didn't really tell you
39:03 - exactly what it is
39:07 - so
39:08 - this is what
39:10 - another this is another example of a
39:11 - neural net this is what a neural net
39:13 - would look like you have your inputs
39:15 - okay they go into these layers and then
39:18 - you have another layer and then you have
39:19 - your output layer so the reason why we
39:22 - have a non-linear activation function
39:25 - is because if the output of all of these
39:28 - are linear
39:30 - then the input you know into this this
39:33 - would also just be
39:34 - a sum of some weights plus a bias
39:38 - what we could do is essentially
39:39 - propagate these weights into here and
39:42 - this entire network would just be a
39:44 - linear regression
39:46 - i'm not going to do the math here
39:48 - because
39:49 - you know it involves a little bit of
39:51 - some algebra
39:53 - but if that's something that you're
39:54 - interested in proving it would be a
39:56 - really good exercise to prove that if we
39:58 - don't have a non-linear activation
40:00 - function then a neural network just
40:02 - becomes a linear function
40:05 - which which is bad like that's that's
40:07 - what we're trying to avoid with the
40:08 - neural net otherwise we would literally
40:10 - just use a linear function
40:13 - without activation functions this
40:15 - becomes a linear model
40:18 - okay so these are the kinds of
40:20 - activation functions that i'm talking
40:22 - about
40:22 - there are more than these but these are
40:24 - three very common ones so here this is a
40:27 - sigmoid activation function so
40:29 - it goes from zero to one
40:33 - tange which goes from negative one to
40:35 - one
40:36 - and then relu which is probably one of
40:37 - the most popular ones
40:39 - um but basically what happens here is if
40:43 - a value is greater than zero then it's
40:46 - just that value and if it's less than
40:48 - zero then it becomes zero
40:51 - so basically what happens in your neural
40:53 - net is after each neuron calculates a
40:55 - value
40:56 - it gets altered by one of these
40:58 - functions so it basically gets projected
41:01 - into a zero or one a negative one or a
41:04 - one and then in this case zero or
41:06 - whatever the output is
41:09 - and then it goes on to the next neuron
41:11 - and on and on and on until you finally
41:13 - reach the output
41:15 - so that's the point of an activation
41:17 - function when you put it at the very end
41:19 - of a neuron it makes the output of the
41:21 - neuron non-linear and this actually
41:25 - allows the training to happen we'll talk
41:27 - about that also in a second
41:29 - we've seen this picture before how we
41:31 - have the training set that goes into our
41:33 - model and then we calculate some loss
41:36 - and then we make an adjustment
41:38 - which is called training so let's talk
41:40 - about this training process now
41:44 - okay
41:45 - so this is what our l2 loss function
41:47 - looks like if you can recall from a few
41:49 - minutes ago
41:51 - basically it's a quadratic
41:53 - function and when your real and
41:55 - predicted values are further apart
41:57 - then the difference becomes the diff the
42:00 - square of the difference
42:01 - becomes very large right and when
42:03 - they're close together then you minimize
42:05 - your loss and all is good in the world
42:09 - okay
42:10 - up here the error is really large
42:13 - and we want to decrease the loss right
42:16 - like the the smaller the loss the better
42:19 - our model
42:21 - is performing in some ways like loss is
42:23 - just a metric to assess how well our
42:24 - model is performing
42:26 - so our goal is to get somewhere down
42:29 - here
42:31 - and now up here this is the part that
42:33 - might involve a little bit of calculus
42:35 - understanding
42:36 - but because not everybody out there
42:38 - knows calculus
42:39 - i'm going to skip the numbers and just
42:41 - use diagrams
42:43 - so if we're up here this is
42:46 - the opposite of the slope right like the
42:49 - slope here i mean it's increasing it
42:51 - would it would be positive but we want
42:53 - to take
42:54 - if we want to get down here we want to
42:56 - go in the opposite direction as that the
42:58 - higher up we go
43:00 - the more steep
43:02 - we want to step right because the
43:03 - further away we are from our goal
43:05 - whereas maybe down here we want to take
43:07 - a baby step over here
43:09 - because we don't want to overshoot we
43:10 - don't want to you know pass this and
43:12 - never be able to find it
43:15 - so
43:16 - we use something called gradient descent
43:18 - in this case and gradient descent is
43:20 - basically taking
43:22 - it's measuring to some extent the slope
43:25 - at a given point and it's taking a step
43:28 - in the direction that will help us
43:31 - this is where back propagation comes in
43:33 - and back propagation is the reason why
43:36 - neural nets work
43:37 - so
43:38 - if we take a look at this l2 loss
43:41 - function
43:42 - okay you might think yeah like this
43:44 - depends on what our y values are right
43:47 - like what is our predicted value what is
43:49 - our real value okay well our real value
43:51 - is staying the same our predicted value
43:54 - is a function of all the weights that we
43:57 - just talked about and all the like
43:59 - inputs right but our inputs are also
44:00 - kind of staying the same
44:03 - so
44:03 - as we adjust the weight values then we
44:06 - are actually altering this loss function
44:08 - to some extent
44:10 - which means that we can calculate the
44:13 - gradient the slope
44:15 - of our loss function with respect to the
44:18 - weights
44:20 - okay
44:21 - got that
44:23 - so
44:25 - if we're looking at the various weights
44:27 - in our diagram we might calculate a
44:30 - slope
44:32 - that's
44:33 - you know with respect to that weight
44:35 - and each of them might be a slightly
44:37 - different value as we can see here
44:41 - so what we're going to do with gradient
44:44 - descent slash back propagation is we're
44:46 - actually going to set
44:48 - a new weight for that parameter
44:51 - and that value is going to be the old
44:53 - weight plus some alpha
44:56 - and we'll get back to that but just
44:58 - think of this as some variable
45:00 - multiplied by this value going down this
45:03 - way
45:05 - a quick side note if you're studying
45:06 - machine learning some more this might be
45:08 - a minus sign and this would be the
45:11 - gradient
45:12 - but
45:13 - for all purposes right now
45:16 - because we're using these arrows instead
45:18 - it's more intuitive to just add
45:21 - something in this direction
45:24 - if that confused you
45:26 - you can ignore it until you start
45:27 - getting into the math of back
45:29 - propagation
45:31 - but what i'm trying to say is that
45:34 - essentially calculating this gradient
45:37 - with respect to one of the weights in
45:39 - the neural network
45:41 - allows us to take a step
45:44 - in that direction in order to
45:46 - create a new weight
45:48 - and this value alpha here this is called
45:51 - our learning rate because we don't want
45:53 - to take this massive step every single
45:55 - time because then we're making a huge
45:57 - change to our neural network instead we
46:00 - want to take baby steps and if every
46:02 - single baby step is telling us to go in
46:04 - one direction then okay fine we're going
46:06 - that direction
46:07 - but taking small steps is better than
46:09 - you know overshooting and
46:12 - diverging off into the land of
46:14 - infinities
46:16 - yeah
46:17 - um
46:21 - you can think of this as for example if
46:23 - you were tailoring something it's better
46:26 - to remove
46:27 - like bits and bits of the fabric rather
46:30 - than removing an entire chunk and
46:32 - realizing oh my gosh i just took off way
46:34 - too much
46:35 - so that's what the learning rate is for
46:37 - it's so that we don't you know take off
46:39 - a huge chunk of the fabric instead we go
46:41 - bit by bit by bit
46:44 - okay
46:45 - so then going back to all these other
46:47 - weights
46:48 - we can see how each weight in the neural
46:50 - net is getting updated with respect to
46:54 - what this gradient value
46:57 - is telling us
46:58 - so basically this is how training
47:00 - happens in a neural net we calculate out
47:03 - the loss okay we see there's a massive
47:04 - loss we can calculate the gradient
47:07 - of that loss function with respect to
47:11 - each of the weight parameters in the
47:13 - neural network and now this allows us to
47:16 - have some direction
47:18 - some measure of the direction that we
47:20 - want to travel in for that weight
47:22 - whether we want to increase the weight
47:24 - or decrease the weight
47:25 - we know based on this gradient that you
47:28 - know we're finding
47:30 - so that is how back propagation works
47:32 - and that's exactly what's happening in
47:33 - this step right here
47:35 - that is our crash course on neural
47:37 - networks it is not the most
47:39 - comprehensive crash course out there if
47:42 - you are interested in neural networks i
47:44 - do recommend diving in deeper into the
47:46 - mathematics of it which i'm not going to
47:48 - cover in this class because not
47:49 - everybody has had the mathematical
47:51 - prerequisites
47:53 - but
47:54 - again if that's something you're
47:55 - interested definitely go and check it
47:57 - out let's move on to talk about how we
47:59 - would actually implement this neural net
48:01 - in code if we wanted to create a neural
48:04 - net
48:05 - so this is where machine learning
48:07 - libraries come in
48:09 - okay so in machine learning we often
48:12 - need to implement a model
48:14 - we probably always need to implement a
48:16 - model
48:17 - and if we want our model to be a neural
48:19 - net like this
48:21 - okay that's great we could go and we
48:22 - could code you know each
48:25 - neuron or we could code a neuron class
48:27 - and we could stitch them all together
48:29 - but
48:31 - we don't really want to start from
48:32 - scratch that would be a lot of work when
48:35 - we could be using that time to fine-tune
48:38 - our network itself
48:40 - so instead we want to use libraries that
48:42 - have already been developed and
48:44 - optimized to help us train these models
48:48 - so if we use a library called tensorflow
48:51 - then our neural net could look something
48:53 - just like this and that's a lot easier
48:55 - than trying to go through and
48:58 - you know develop and optimize the
49:00 - network entirely from scratch ourselves
49:04 - so straight from the website tensorflow
49:06 - is an open source library that helps you
49:08 - develop and train ml models
49:12 - okay great that sounds like exactly what
49:15 - we want so tensorflow is a library
49:17 - that's comprised of many modules that
49:19 - you know you might be able to see here
49:21 - but for example we might have
49:25 - this data module and here
49:27 - you know we have a bunch of tools that
49:29 - help us import data and keep data
49:32 - consistent and usable with the models
49:34 - that we will create
49:36 - another great part of this api is keras
49:39 - so here we actually have a bunch of the
49:43 - different modules which will help us
49:45 - you know create models
49:49 - help us optimize them
49:51 - and these are different types of
49:52 - optimizers
49:55 - et cetera so basically the whole point
49:58 - of this is that you know we want to use
50:01 - these packages we want to use these
50:03 - libraries because they help us
50:05 - maximize our efficiency where we can
50:07 - focus on training our models and they're
50:10 - also just you know really good and like
50:13 - fine-tuned already so why would we want
50:15 - to waste our time
50:16 - to code stuff from scratch
50:19 - all right so now let's move over to our
50:21 - notebook and i'm going to show you guys
50:23 - how to actually implement a neural net
50:26 - using tensorflow just you know
50:27 - straightforward
50:28 - feed forward no pun intended neural
50:31 - network
50:32 - now that we've learned a little bit
50:33 - about neural nets about tensorflow let's
50:35 - try to actually implement a neural net
50:38 - using tensorflow so
50:40 - let me go back to this collab tab and
50:43 - let's actually
50:45 - create a new notebook
50:48 - okay and this notebook again i'm going
50:50 - to call this maybe just a feed forward
50:54 - nullnet example
50:57 - all right now i'm going to again use
51:01 - these same imports
51:04 - run this
51:08 - great
51:10 - it's running
51:12 - okay so now the second thing that i'm
51:14 - going to have to import in here
51:17 - is my data set and so here i have a data
51:21 - set called diabetes.csv which is also
51:24 - in the description below
51:27 - click ok there and that was a that was a
51:29 - substantially smaller data set than the
51:30 - one that we tried to import at the
51:32 - beginning which don't worry we will get
51:34 - back to at the very end of this video
51:36 - but that's why it took significantly
51:38 - shorter to upload all right so
51:41 - this data set that we're using here in
51:44 - diabetes.csv this is a data set that was
51:46 - originally from the national institute
51:49 - of diabetes and digestive and kidney
51:52 - diseases
51:53 - and in this data set all these patients
51:56 - are females at least 21 years old
51:59 - of pima indian heritage
52:02 - and this data set has a bunch of
52:04 - different features but the final feature
52:06 - is whether or not the patient has
52:09 - diabetes
52:11 - okay
52:12 - let's take a look at this data set
52:17 - so the first thing that we can do is
52:19 - once again create a data frame
52:22 - so we're going to use read csv again and
52:24 - here we can just say diabetes.csv
52:28 - let's see what this looks like
52:32 - all right so we can see that each of
52:34 - these is one patient
52:36 - and how many pregnancies they've had
52:38 - their glucose blood pressure skin
52:40 - thickness
52:41 - etc a bunch of other measurements
52:44 - okay so it's always a good idea to do
52:46 - some sort of data visualization on
52:49 - these values in order to see if any of
52:52 - these you know have some sort of
52:54 - correlation to the outcome and there are
52:56 - many different metrics that you can run
52:58 - you can try to literally find the
53:00 - correlation between pregnancies and the
53:02 - outcome but for this purpose i think
53:04 - it's a lot easier to visualize things
53:08 - in i guess a visual way
53:10 - how else would you visualize things
53:13 - so here what i'm going to do is i'm
53:15 - going to
53:16 - plot each of these as a histogram so
53:19 - each value in the feature as a histogram
53:22 - and then compare it
53:24 - to the outcome so
53:27 - let's try to do that using a for loop so
53:29 - for i in range
53:31 - and here i'm going to use the columns
53:35 - of the data frame
53:37 - um up until the very last one because
53:39 - that's the outcome that's the one that
53:40 - we're actually comparing against
53:43 - what i'm going to do is say the label is
53:45 - equal to
53:46 - dataframe.columns
53:49 - i and actually
53:53 - let me make this screen larger for you
53:55 - guys because i know that some people
53:58 - yeah okay hopefully you can see this a
54:00 - little bit better
54:02 - so
54:04 - anyways down here uh for i in this range
54:09 - okay how about this this is better
54:12 - okay so for each
54:14 - basically this for loop here is trying
54:16 - to run through all of these columns
54:17 - except for the last one
54:19 - and the label is basically just getting
54:21 - the data frame column at that index so
54:25 - to show you guys what that actually
54:26 - looks like
54:28 - let's run df.columns
54:31 - so again this is just it's similar to
54:33 - going through a list of these items
54:36 - okay so the label we're you know
54:38 - indexing into something in this list and
54:41 - what we're going to do is
54:44 - plot the histogram
54:47 - so we can index into a data frame by
54:49 - calling the data frame and then saying
54:51 - you know where the data frame
54:53 - outcome
54:56 - is equal to one which means that they do
54:58 - have diabetes
55:00 - and then
55:01 - so this basically creates another
55:05 - let me show you
55:09 - this basically is a data frame where all
55:12 - the outcomes are equal to one
55:15 - okay cool so this is our new data frame
55:19 - where all the outcomes are one which
55:20 - means that all these patients are
55:22 - diabetes positive
55:24 - and then
55:25 - we're just going to index into the label
55:27 - that we have right here so we're just
55:29 - indexing it to the column
55:31 - and
55:33 - what i'm actually going to do is the
55:34 - same thing now but instead of one make
55:36 - this zero so now this here
55:39 - let me show this to you guys again
55:41 - this here is
55:43 - a data frame where all the outcomes are
55:45 - zero
55:47 - so that means that everybody here is
55:49 - um
55:51 - so that means everybody here is diabetes
55:53 - negative
55:55 - okay perfect
55:57 - well we want to tell the difference
55:58 - between the two so i'm also going to
56:00 - assign these colors so here let's use
56:04 - blue
56:06 - and red
56:07 - and then of course a label so this is
56:10 - no diabetes
56:12 - and this up here
56:14 - diabetes
56:17 - all right
56:20 - so plt
56:22 - now let's give this a title let's just
56:24 - use you know the name of the label
56:27 - and then the y label
56:29 - is n
56:31 - and the x label
56:34 - is
56:35 - the label
56:37 - if we call plt.legend
56:40 - like this then this actually shows us
56:42 - the legend including these labels
56:45 - and at the very end we call plt.show in
56:47 - order to see the plot
56:50 - so let's run this
56:53 - all right
56:55 - basically for each of
56:58 - the values
56:59 - we are plotting
57:02 - the measurements here and it's kind of
57:04 - hard to see
57:06 - you know diabetes versus no diabetes so
57:09 - another trick that we can do here is we
57:11 - can say alpha equals 0.7
57:15 - if we run these again you'll see that it
57:17 - makes them a little bit easier to see
57:19 - behind one another
57:21 - okay so something else that we might
57:23 - want if we take a look at
57:25 - the number of values so if we say the
57:28 - length of this versus the length of
57:31 - this so this is saying how many patients
57:34 - are diabetes positive and how many are
57:36 - diabetes negative
57:38 - we see that we actually have two
57:39 - different values one of them is only 268
57:41 - positive patients and then 500 negative
57:44 - patients
57:45 - so what we actually want to do is
57:47 - normalize this which means that we want
57:50 - to compare these histogram distributions
57:52 - to how many actual
57:54 - values there are in that data set
57:56 - otherwise you know you can clearly see
57:58 - that
58:00 - there are more no diabetes patients here
58:03 - than diabetes patients and this isn't
58:05 - really a fair
58:06 - measurement
58:08 - so i'm going to say density here equals
58:11 - true and just for kicks i'm going to say
58:13 - the number of bins that we're going to
58:15 - use in our histogram is 15.
58:17 - then here
58:18 - we would say this is probability because
58:20 - we're normalizing now using the density
58:23 - which means basically that we're just
58:25 - taking each of these values and dividing
58:27 - it by how many total values there are in
58:29 - the data set so instead it's a ratio
58:33 - for each column rather than just a
58:35 - straightforward number
58:39 - okay click enter again and now here we
58:41 - have
58:42 - more of a visualization
58:44 - so it does seem like you know people who
58:47 - have diabetes might have more
58:49 - pregnancies
58:50 - or people you know higher glucose levels
58:53 - that makes sense right
58:55 - seems like maybe slightly higher blood
58:58 - pressure but that's pretty inconclusive
59:01 - maybe skin thickness a little bit but
59:03 - also you know insulin a little bit
59:04 - inconclusive it does seem like perhaps
59:07 - people who have diabetes have a slightly
59:10 - higher bmi
59:13 - but so on so you can see that like
59:15 - there's no these values aren't separable
59:17 - which means that we can't really tell
59:19 - based on one
59:20 - value
59:21 - whether or not somebody has diabetes or
59:23 - not and this is where the power of
59:25 - machine learning comes into play is that
59:27 - we can actually learn
59:28 - whether or not or predict whether or not
59:30 - somebody has diabetes or not based on
59:33 - all of these features all together
59:36 - okay
59:38 - so
59:41 - now that we have this what we're going
59:43 - to do is split this into our x and y
59:47 - values so
59:49 - recall that the x is a matrix right
59:51 - so here i'm going to say it's just going
59:54 - to be all the columns up until the last
59:56 - value
59:57 - got values
59:58 - so let's run that and let's see oops
60:05 - and what does this give us this gives us
60:07 - now an array this is now a numpy array
60:11 - okay and we're going to do the same
60:13 - thing with y
60:16 - except y is just a single column so we
60:18 - can do this
60:21 - and i missed the s again
60:26 - okay so if we see why
60:29 - it's a huge array but it's one
60:31 - dimensional as you can see and it's just
60:33 - all the different labels in the data set
60:36 - so now we have our x and our y values we
60:39 - can actually just go ahead and split
60:41 - this up into our test and our our
60:43 - training in our test data sets
60:45 - so
60:46 - something that i'm going to import is
60:50 - from sklearn
60:54 - so this package this function here
60:57 - allows us to split arrays or matrices
60:59 - into random train and test
61:01 - subsets which you know is very useful
61:05 - so all we have to do is plug in our
61:08 - arrays and it should help us you know we
61:12 - would dictate the
61:13 - test size and the train size and then it
61:15 - would help us split them up
61:18 - so what i'm going to do is say x train
61:22 - and x i'm going to call this temp
61:25 - y train
61:26 - and y temp
61:28 - equal
61:29 - oh we have to import this so actually
61:32 - i'm going to come back up here
61:34 - and say from sklearn.model
61:38 - selection
61:39 - import this
61:42 - okay so make sure you rerun that cell
61:44 - but now we have access to that function
61:46 - so here train test split
61:49 - and i'm going to pass in my x and my y
61:52 - arrays
61:53 - and then i'm going to say the test size
61:57 - is equal to
62:00 - let's use 60 of our data for training
62:03 - and
62:04 - 20 for validation and twenty percent for
62:07 - test so what i'm going to do first is
62:08 - just split this up into zero point four
62:11 - and
62:13 - i'm just going to pass in random state
62:15 - equals zero so this allows us to get the
62:17 - same split every single time
62:20 - now the reason why i want to do this
62:21 - again but use 0.5 is now with this
62:24 - temporary
62:25 - data set which is technically you know
62:28 - the test set
62:30 - we're going to split this now into the
62:31 - validation and the test so let's do x
62:34 - valid
62:36 - test
62:40 - and instead of x and y we're going to
62:42 - pass in the temp that we just created up
62:45 - here so this is essentially x and y but
62:48 - only 40 of that data set and now we're
62:51 - breaking this down further
62:53 - into 50 50 which is going to be our test
62:56 - our validation and test data sets so
62:59 - let's run this cool checks out
63:03 - and let's build our model
63:05 - so here i'm going to say model equals tf
63:08 - which is tensorflow
63:11 - keras which is part of tensorflow that
63:13 - helps us write you know some neural nets
63:16 - and stuff like that
63:19 - called tf dot keras
63:21 - so let's check that out okay tf keras so
63:25 - basically it's this uh api
63:28 - that allows us to easily build some
63:31 - neural net models
63:34 - and here we're going to call the
63:36 - sequential
63:38 - so let's see
63:42 - basically it
63:43 - groups a linear stack of layers into a
63:46 - model which is exactly what we want
63:49 - because our neural net is
63:51 - uh
63:52 - because our neural net is exactly a
63:54 - stack of layers
63:57 - so let's pass in some fears here
64:01 - okay
64:02 - i think how i'm going to architect this
64:04 - is i'm just going to use a very simple
64:06 - model so keras
64:08 - dot layers dot dense
64:10 - 16 okay so basically what this actually
64:14 - let me
64:16 - activation equals
64:19 - okay what this is like setting up here
64:22 - is
64:23 - this is a layer of dense neural nets
64:26 - what does dense mean it just means that
64:27 - it takes input from everything and it
64:29 - outputs
64:30 - a value but densely connected is just
64:33 - you know okay it's a layer that's deeply
64:35 - connected with its preceding layer
64:38 - so it just means that every single
64:40 - like neuron here
64:42 - is receiving input from every single
64:45 - neuron
64:46 - that it sees from the past
64:50 - all right going back to here
64:52 - um this just means that this is a layer
64:53 - of 16 neurons that are densely connected
64:56 - so if we go back to here it means that
64:58 - this layer here is comprised of 16
65:01 - different neurons
65:02 - and then our activation function is relu
65:05 - which is one of the activation functions
65:07 - that we saw previously
65:09 - where
65:10 - if x is
65:12 - less than zero
65:14 - then this becomes zero i guess less than
65:17 - or equal to zero this becomes zero and
65:19 - if x is greater than zero then
65:22 - this becomes just x
65:25 - okay
65:26 - and you know i'm just going to add
65:28 - another layer in there just for kicks
65:30 - and then finally we're going to conclude
65:33 - this with a
65:34 - layer of only one node
65:38 - but instead this layer is only going to
65:40 - have one node and the activation here
65:44 - is going to be sigmoid
65:46 - and what that helps us do is this is
65:48 - this is where the binary classification
65:50 - comes in because sigmoid if you can
65:52 - remember maps to zero or one so
65:56 - the
65:56 - the point of this activation function is
65:59 - that it maps
66:00 - our input to a probability of whether or
66:03 - not you know something belongs to a
66:05 - single class
66:07 - so
66:08 - this is our neural net it was really
66:09 - that easy we can click enter
66:12 - or shift enter and then now let's
66:15 - compile this model so we can call
66:17 - model.compile
66:19 - and we're going to need a few different
66:21 - things here so the first one is going to
66:23 - be our optimizer
66:26 - so you can see here that our there are a
66:28 - bunch of different
66:29 - optimizers that tensorflow already has
66:33 - for us
66:34 - now
66:35 - which one do you choose that's kind of
66:38 - that's that's a hard question to answer
66:39 - because it's a question that people
66:41 - still don't really know the answer to
66:43 - but one of the most commonly used
66:45 - optimizers is atom so that's where we're
66:48 - going to start
66:49 - so going back to our example here let's
66:52 - do optimizer
66:54 - and let's set this equal to
66:59 - dot tensorflow.paris.org
67:00 - dot
67:01 - atom
67:03 - and we're going to set our hyper
67:05 - parameter called the learning rate
67:09 - we're going to let's start off with what
67:11 - the default is so 0.001
67:15 - now the second thing that we have to do
67:17 - is define our loss function so
67:20 - our loss is equal to tf.cara's
67:24 - dot losses so now because we're doing
67:27 - binary classification the one that we're
67:30 - going to use is called binary cross
67:32 - entropy so binary
67:34 - cross entropy like this
67:37 - and
67:41 - then the final thing that we're going to
67:42 - do is add
67:44 - a metric for accuracy
67:47 - and the reason why we're doing that is
67:48 - because we want to know you know how
67:49 - many do we actually
67:51 - get right
67:54 - so enter
67:56 - we now have a compiled model we have a
67:59 - neural net that we can actually feed
68:01 - data to and we can train
68:04 - but before we do that let's actually see
68:06 - how well this might perform on you know
68:09 - our training data and our validation
68:11 - data
68:13 - what we're going to call is model dot
68:15 - evaluate
68:16 - and here let's pass in x train and y
68:20 - train
68:22 - and see what happens
68:24 - okay
68:25 - so
68:26 - here we're getting a loss of 16 and an
68:29 - accuracy of
68:31 - 0.35 which is around like 35 so that
68:34 - that's pretty bad
68:36 - what about instead of train let's do
68:40 - validation
68:42 - okay it's around the same a loss of 11
68:44 - and accuracy of 35
68:47 - all right this is because our model
68:49 - hasn't seen our train or validation set
68:52 - yet and that's why the accuracy is so
68:54 - low we haven't really done any training
68:56 - so let's see if we can fix that
69:00 - what we're going to do is call model.fit
69:03 - and pass in the x train and the y frame
69:06 - values
69:08 - pass in something called the batch size
69:11 - and then epochs is how many times how
69:13 - many iterations through the entire data
69:15 - set we're going to train this
69:18 - and pass in the validation data
69:24 - now this validation data is just so that
69:26 - after every single epoch we can measure
69:28 - what the validation loss and the
69:30 - validation accuracy is
69:33 - so x valid y valid
69:36 - now we can run that
69:45 - all right so you can see that our neural
69:47 - net is training now going back really
69:49 - quickly to this batch size
69:51 - batch size is just a term that refers to
69:54 - the number of
69:56 - like samples or training examples in
69:58 - this case it's the number of women that
70:01 - we have like samples from
70:03 - that is used in every single iteration
70:05 - so this is how many samples we see
70:07 - before we go back and we make a weights
70:09 - update
70:11 - so you can see that here look our loss
70:13 - in our training set
70:15 - is decreasing fairly successfully
70:17 - and our inc our accuracy
70:20 - okay our accuracy seems to be increasing
70:22 - great
70:25 - now what about our validation loss okay
70:27 - our validation loss seems to be
70:28 - decreasing decreasing decreasing a
70:31 - little bit of an increase at the end but
70:32 - that's that's a good sign
70:34 - and our validation accuracy seems to be
70:37 - also increasing
70:39 - well let's see if we can do better so
70:42 - a few problems with our current data set
70:44 - we see that look all of our values here
70:48 - i mean our insulin our insulin range is
70:51 - from zero all the way out to like 800
70:54 - whereas something else might be
70:57 - on the scale of
70:59 - like
71:00 - you know skin thickness is on scale of 0
71:02 - to 100 but
71:04 - uh
71:05 - bmi is on the scale of 0 to like 2.5
71:10 - the fact that our features are so
71:12 - different in terms of their range that
71:15 - is something that could be messing up
71:16 - our results so instead what we're going
71:19 - to do is we're actually going to scale
71:21 - the results so all of them are
71:23 - on a more standardized range
71:26 - the first thing that i'm going to do is
71:29 - i'm going to
71:30 - import a package that lets us scale
71:33 - so
71:34 - just like how we imported this function
71:35 - up here i'm going to
71:38 - import from sklearn
71:40 - pre-processing
71:42 - i'm going to import standard scalar
71:47 - okay let's rerun this again
71:49 - and let's go back down to
71:53 - down here like before we split it up
71:55 - into the train and test sets
71:58 - okay
71:59 - so here let's just run this again for
72:02 - good measure but instead of splitting it
72:04 - up right here what i'm going to do is
72:06 - i'm going to scale the quantities so
72:09 - here i can define a scalar
72:11 - and set this equal to standard scalar
72:14 - and i'm going to say okay this scalar
72:17 - let's fit and then transform
72:20 - this x matrix
72:23 - all right
72:24 - so once we've done that our new data i'm
72:28 - actually so let's actually see what what
72:30 - x looks like now
72:32 - so we can see that everything is a lot
72:34 - closer in range it goes you know from
72:37 - maybe around
72:39 - like one to like negative one okay let's
72:43 - actually see let's plot these values
72:44 - again and see what exactly is going on
72:48 - here so
72:52 - here let's transform this back into a
72:55 - data frame so what i can do is say
72:58 - transformed
72:59 - data frame equals
73:02 - pandas.dataframe so this is creating a
73:04 - data frame
73:05 - and then
73:07 - here okay let's let's use a placeholder
73:09 - called data and then the columns are the
73:12 - same as our current columns
73:15 - now this data we're actually going to do
73:17 - something called a horizontal stack
73:19 - which means that we're taking our x our
73:22 - new
73:23 - transformed variable x
73:26 - along with
73:28 - this y
73:30 - but
73:30 - we have to do a little bit of reshaping
73:33 - here so we're going to call
73:34 - numpy.reshape because right now our x is
73:37 - a two-dimensional matrix but y is only a
73:40 - one-dimensional matrix
73:43 - which means that let me just show you
73:45 - real fast so x dot shape if we
73:49 - see what this is and then compare this
73:51 - to y dot shape
73:53 - see this is a two dimensional variable
73:55 - and this is only one dimensional it
73:57 - doesn't have a one here which means that
73:59 - it's not
74:00 - you know a 768 by one matrix it's just
74:03 - it's just a vector of length 768
74:06 - and if we call
74:08 - a horizontal stack of the two
74:10 - there'll be um there's going to be an
74:12 - error
74:13 - so instead we're going to reshape this
74:16 - and this just means okay
74:18 - project it into something where negative
74:20 - one means like numpy gets to decide
74:24 - this one just means in that second
74:26 - dimension it'll be comma one so here
74:29 - it's going to return an object of 768
74:32 - comma one
74:33 - all right
74:34 - so let's run this
74:36 - oops here columns
74:39 - cool
74:40 - and then instead of data frame dot
74:44 - columns
74:45 - actually all of these we can leave the
74:46 - same but here we just want to make sure
74:48 - that we're plotting the transformed data
74:51 - frame
74:53 - so if we can see now
74:55 - like most of these so what standard
74:58 - scalar is doing is it's actually mapping
75:00 - our values to a normal distribution and
75:03 - trying to calculate
75:04 - like how far off our values are from the
75:07 - norm
75:09 - so here
75:10 - you know we can see that glucose levels
75:12 - now range
75:13 - i mean it looks like it's fairly
75:15 - centered around zero or negative one
75:18 - um
75:19 - but all of these are now normalized
75:22 - okay so we can actually delete the cell
75:24 - now that we've visualized it we don't
75:26 - need it
75:27 - and what we're going to do is we are
75:30 - going to
75:31 - [Music]
75:32 - uh
75:33 - this is now our new x and we can
75:36 - directly pass that down into here but
75:38 - one other thing that i had mentioned
75:41 - is remember how
75:43 - if we take x and we say x
75:46 - for the outcome
75:49 - is equal to one
75:53 - or sorry i shouldn't say x i should say
75:55 - the data frame or the transform data
75:57 - frame
75:58 - doesn't matter
76:02 - and then if i set this equal to zero
76:05 - remember how these two
76:08 - sorry a little typo
76:13 - remember how these two values are so
76:14 - different like the number of
76:16 - non-diabetes patients is almost double
76:19 - the number of diabetes positive patients
76:21 - so this can sometimes also
76:24 - lead to
76:25 - the neural net not training so well
76:28 - instead what we're going to do is we're
76:30 - actually going to
76:31 - try to get these two to be approximately
76:34 - equal
76:35 - and we can do that with another thing
76:37 - called
76:38 - random over sampler which means that
76:40 - we're essentially trying to
76:43 - get
76:44 - like more random samples into that first
76:46 - sample so that they now balance out
76:49 - these two values balance out the lengths
76:53 - now okay we can do this by importing
76:56 - another package
76:58 - so we're going to use a package called
77:00 - imbalance learn
77:02 - dot over sampling
77:06 - and we're going to import random over
77:08 - sampler
77:09 - now watch what happens
77:12 - oh
77:13 - okay that did not give me an error but
77:15 - in the past this might give you an error
77:17 - and if it does what you're going to do
77:19 - is just come up here and type in
77:22 - uh an exclamation mark pip
77:24 - install dash u
77:27 - imbalanced
77:30 - run that and then this if there's an
77:32 - error here that says you know there's no
77:34 - library then this
77:36 - up here will solve that for you
77:39 - and yeah you you would have to restart
77:41 - the runtime but because
77:43 - this worked i'm not going to do that
77:47 - all right let's come back down here so
77:50 - right before we're splitting it into the
77:51 - test and train sets
77:53 - over here let's actually
77:55 - use this random over sampler in order to
78:00 - get both of these
78:01 - uh equal to one so here let's call
78:05 - random
78:06 - over sampler
78:10 - and then let's split x and y
78:14 - well i guess let's let's redo this x and
78:17 - y definition by calling fit resample
78:21 - and then x comma y
78:23 - then
78:24 - we run this cell
78:26 - all right
78:27 - uh we can't exactly
78:29 - we we never like rerand
78:32 - we can do this again
78:36 - all right so now we see that we have 500
78:40 - where the outcome is one and 500 where
78:42 - the outcome is zero so this is a good
78:44 - sign this means that now our data set is
78:47 - balanced in terms of the outcomes
78:49 - let's
78:50 - rerun all of these
78:55 - okay so it seems like our accuracy is
78:57 - closer to 50 now
78:59 - and let's think about like intuitively
79:01 - why that's happening well okay let's say
79:04 - that the first result it's just
79:06 - predicting random numbers zero one zero
79:07 - one zero one okay but
79:09 - our data set
79:11 - naturally before it had more
79:14 - values that were
79:16 - uh negative than positive and so
79:19 - naturally that accuracy would be skewed
79:21 - towards something closer to 30 percent
79:23 - if for example it's like a one to two
79:26 - ratio
79:27 - because just because of the fact that we
79:29 - didn't have very many values that
79:32 - were equal that were diabetes positive
79:35 - but now because we've balanced it this
79:37 - value is much closer to 50.
79:40 - all right
79:41 - now let's try to train our model once
79:43 - again
79:46 - okay so we see that again this loss is
79:49 - decreasing this is good and our accuracy
79:52 - seems to be increasing which is also
79:54 - good
79:55 - and let's check our validation because
79:58 - remember we need to see how this would
80:00 - generalize to unseen data
80:02 - our validation loss great seems to be
80:05 - decreasing validation accuracy
80:07 - seems to be increasing and now our
80:09 - validation accuracy is somewhere closer
80:11 - to
80:12 - 77
80:14 - which is a significant improvement
80:18 - all right now at the very very end what
80:20 - we're going to do is we're going to
80:22 - evaluate this model
80:24 - on our test data set
80:30 - and we can see that okay our loss is
80:32 - around 0.5 but that doesn't really mean
80:34 - anything to us
80:35 - but our accuracy is around 77.5
80:39 - which is pretty good because we've never
80:41 - seen this data before
80:43 - so that's a quick tutorial on how we
80:45 - just used tensorflow to create a neural
80:48 - net and then use that neural net to
80:50 - predict whether or not the sample of
80:53 - uh pima indian
80:55 - descent women have diabetes or not based
80:58 - on you know some data that we were given
81:01 - so that's very very cool now
81:04 - for the other tutorial that we started
81:06 - off with we will get to that in one
81:08 - second but first i wanted to talk about
81:11 - some more advanced neural network
81:13 - architectures
81:15 - let's talk about recurrent neural
81:17 - networks
81:19 - we've already seen the feed forward
81:20 - neural net we've already you know done
81:22 - an example problem with it we're very
81:24 - familiar with it right you have your
81:26 - inputs and then they feed into you know
81:29 - the hidden layers and then you get your
81:31 - outputs okay cool yeah we've got that
81:33 - you guys have got that
81:35 - but what if this data over here our
81:38 - inputs
81:39 - were some sort of series or sequence so
81:43 - for example they might be stock prices
81:45 - from the past 20 days or they might be
81:47 - values that represent different words in
81:50 - a sentence where the sentence has some
81:52 - sort of you know
81:54 - sequence to it or it might be
81:57 - temperatures from the past 10 months
82:01 - so on you you get what i mean like just
82:04 - basically if this was some sort of
82:06 - sequence if this data were a sequence a
82:10 - feed-forward neural net would not do a
82:12 - really good job at picking that up
82:14 - why because all of these different
82:16 - layers it evaluates each value as if
82:19 - they were independent so even if there
82:21 - were a series
82:23 - it's it's a lot harder for our feed for
82:24 - our neural net to pick up on that
82:28 - that's where recurrent neural networks
82:30 - come in so basically here we have our
82:32 - data points
82:34 - and you know this is our data point
82:36 - taken at t0 at t1 at t2 so we're going
82:39 - through time
82:40 - and what we can do is we can feed these
82:43 - into a layer of weights
82:45 - and then these layer of weights may or
82:47 - may not produce some sort of output
82:50 - but basically what this is doing because
82:52 - the
82:54 - you know calculation at each point takes
82:56 - into account the previous
82:59 - calculations as we move through this
83:01 - network we're essentially creating some
83:04 - sort of memory with the neural net so
83:07 - this neural net at whenever you know
83:10 - when we feed in x2 so x at t2
83:13 - we actually have some information that
83:15 - we remember about x at t 0 and x at t
83:19 - so that makes a recurrent neural net
83:21 - very powerful
83:23 - ta-da this part acts as a memory
83:28 - and now instead of just straightforward
83:29 - back prop we have to use something
83:31 - called back propagation through time in
83:34 - order to adjust these weights
83:37 - okay this is our unfolded rnn
83:40 - and this is what our folded version
83:42 - would look like so
83:43 - you see how you know our x at time t is
83:46 - just fed into this neuron which outputs
83:49 - some
83:50 - value at time t
83:52 - and this value kind of gets cycled into
83:55 - the next iteration of x and so on
83:59 - well there are a few problems right with
84:01 - this because
84:02 - this rnn if you if you imagine there are
84:04 - many many time steps this might end up
84:06 - being a really deep network
84:09 - and then two
84:10 - during back propagation we might be
84:13 - seeing the same terms like the weights
84:15 - in here
84:16 - over and over and over because of the
84:18 - recurrent nature of this rnn
84:21 - now why are those problems
84:23 - well these two kind of compound on each
84:26 - other and you might get something called
84:28 - exploding gradients where the model can
84:31 - become really unstable and then
84:32 - incapable of learning
84:34 - because all the gradients that we're
84:36 - using in back propagation are getting
84:39 - bigger and bigger and bigger until
84:40 - they've reached like infinity
84:43 - and then
84:44 - you know our model
84:46 - our model updates become literally all
84:48 - over the place and we can't really
84:49 - control our weights and then yeah
84:52 - not good stuff happens
84:55 - on the other hand there's also such
84:56 - thing as vanishing gradients so here our
85:00 - gradients get closer and closer and
85:02 - closer to zero and so at this point our
85:04 - model stops updating and then it becomes
85:07 - also incapable of learning
85:10 - so some really smart people have studied
85:11 - these problems and they've decided okay
85:14 - here are some different ways that we can
85:15 - overcome this problem
85:17 - there are a few things that you can do
85:18 - with the activation function but i'll
85:20 - let you look that up on your own time
85:23 - instead i'm going to talk about
85:24 - different sorts of cells or neurons that
85:27 - people have come up with in order to
85:29 - combat this
85:31 - so there's such thing as the great the
85:34 - sorry the gated recurrent unit
85:36 - and this unit as you can see so it still
85:39 - takes x at time t as an input but here
85:42 - just has a bunch of gates that are
85:44 - associated with it and then it has some
85:47 - sort of output so there's just more
85:49 - parameters inside the neuron itself
85:51 - rather than trying to just directly sum
85:53 - up some weights
85:55 - there's also the long short term memory
85:58 - unit which looks something like this
86:00 - it's very very similar to the unit that
86:02 - we just saw but instead of two gates it
86:04 - has three again i'm not going to dive
86:07 - too deep into these things because these
86:08 - are more advanced topics but i just
86:10 - wanted you guys to be aware that these
86:12 - exist and if we do use them an example
86:15 - this is exactly what's going on it's
86:16 - just we have a few more
86:19 - you know bells and whistles inside of
86:21 - our neuron all right so now that we've
86:23 - touched upon some of the more
86:24 - theoretical aspects of neural nets and
86:27 - machine learning
86:28 - let's walk through a different
86:29 - tensorflow example with text
86:30 - classification
86:32 - and try to see if we can classify some
86:35 - wine reviews
86:37 - so let's get started on that let's go
86:39 - back to our co-lab notebook where we
86:41 - were studying the wine reviews and let's
86:43 - continue on with that so here i have
86:46 - that notebook open this is the code that
86:48 - we typed at the very beginning of this
86:51 - class
86:52 - so here we have our wine reviews and
86:54 - we're actually actually need to restart
86:56 - um we need to rerun some cells
86:59 - and we need to re-import our data so let
87:02 - me do that over here
87:10 - okay so we see that it's imported
87:14 - let's rerun everything
87:20 - all right
87:23 - cool
87:23 - okay this is this is our uh these are
87:26 - our points is what we're trying to
87:27 - classify now let's split this up into a
87:29 - low tier and height here as we were
87:32 - saying earlier so how we can do that is
87:35 - if we type in
87:37 - quality label let's say
87:40 - so let's come up with a new label or
87:42 - actually let's just say this is the
87:43 - label
87:44 - and we say this will be if df dot points
87:48 - so basically the points column of the
87:50 - data frame if it's greater than or equal
87:53 - to 90.
87:55 - let's uh
87:57 - this basically will return a boolean so
87:59 - either every single row
88:01 - it will be false if it's less than 90 or
88:04 - true if it's greater than 90. and
88:07 - all i'm going to do at the very end here
88:09 - is
88:10 - forces as a type for an integer so that
88:12 - it gets mapped to zero or one because
88:14 - remember our computer understands
88:16 - numbers really well
88:19 - so here then i'm going to say that the
88:21 - data frame i don't need all the columns
88:23 - i know which ones i'm going to use so
88:25 - i'm just going to say it'll be the
88:27 - description
88:28 - and the label
88:30 - so remember this is the description
88:33 - and this is the label okay and you know
88:35 - what let me just add points in here
88:37 - anyways
88:40 - so now if i look at the very beginning
88:43 - of this
88:44 - okay so we have the points and we have
88:46 - the label let's look at the tail
88:48 - maybe that'll be okay so here again we
88:51 - have the points they're equal to 90 and
88:53 - then the label
88:55 - all right great
88:57 - so
89:00 - let's now split this up into the
89:03 - training validation and test data sets
89:06 - so
89:08 - i wanted to this there just to show you
89:11 - guys that you know we were mapping this
89:12 - the right thing but this is what we can
89:14 - actually
89:15 - um keep now that we have our data frame
89:18 - i want to split this up into our
89:19 - training validation and test data frames
89:23 - now we did this in a slightly different
89:24 - way before but i'm going to show you a
89:26 - different way to do it because i want
89:27 - you to realize that there's not just one
89:29 - way to do it like whenever you're
89:30 - working with a data set you want to be
89:32 - able to be flexible
89:34 - and split things up in different ways
89:37 - so here i'm going to say train val test
89:41 - i'm going to set this equal to np so
89:43 - numpy dot split which is going to split
89:45 - our data frame
89:47 - and this is the data frame that we're
89:48 - going to split and actually what i'm
89:50 - going to do is i'm going to mix things
89:52 - up a little bit so i'm going to say
89:53 - sample
89:54 - and i'm going to sample all of them
89:56 - which is going to basically draw random
89:58 - samples but sampling the entire data
90:00 - frame
90:01 - and then i'm going to pass in the
90:03 - different
90:04 - cuts where i actually want there to be
90:06 - breaks in the data frame
90:08 - so what that's going to look like is i
90:10 - want it to be 60 for the training 20 for
90:13 - the validation 20 for the test and
90:16 - actually in a data frame this size you
90:18 - could even go like 80 for training 10
90:22 - for validation 10 for test
90:25 - and the reason is just because there's
90:26 - so much data that like
90:28 - even with your validation and test sets
90:30 - even if you're using only 10 that's
90:32 - still enough data to kind of see how it
90:34 - generalizes
90:36 - so actually that's what i'm going to do
90:39 - um so i'm going to map this to an
90:41 - integer and i'm going to say okay so 0.8
90:44 - will be our first cut meaning 80 will go
90:46 - towards the train data set
90:49 - so i'm going to say 0.8 times the length
90:52 - of this data frame
90:53 - and then our second cut is going to be
90:57 - at the 90 mark which means that 10 of
90:59 - the data set will be for the validation
91:02 - here and 10 the remaining 10 will be for
91:06 - test
91:07 - so if we run that okay cool
91:10 - and we can quickly say like you know
91:12 - let's print the length of these
91:18 - great so you can see still plenty of
91:20 - samples for our validation and tests
91:22 - data sets
91:24 - all right so this next function i'm
91:26 - actually going to copy from this
91:28 - tensorflow module right here
91:30 - all right so we're going to use this
91:32 - we're going to slightly edit it and
91:34 - basically what this function does is it
91:36 - converts each training validation and
91:38 - test set data frame into a
91:41 - tf.data.dataset object and then it will
91:44 - shuffle and batch the data so
91:47 - let's copy this code and go back to ours
91:50 - right here now i'm going to make a
91:52 - slight difference because our data set
91:54 - is so big i'm actually going to change
91:56 - this to a larger batch size and then
91:58 - down here instead of using batch size
92:00 - i'm going to do tf.data.autotune
92:05 - okay great so let's run this except
92:08 - actually so
92:10 - in this data set i believe they use
92:12 - target to define
92:14 - their uh target column
92:17 - okay yeah so they use target to create
92:19 - their target variable but for us we
92:21 - already have a column that defines it
92:23 - and we've called it the label so instead
92:26 - what we're going to do is we're going to
92:28 - change this to label
92:30 - okay and then the next thing that we're
92:33 - going to do
92:34 - is instead of this part here
92:38 - with all of this we're just going to set
92:40 - the data frame equal to data dot
92:42 - description
92:44 - because that's the only part of the data
92:45 - frame that we actually care about
92:48 - alrighty
92:51 - and here we can because we change that
92:54 - we can remove this
92:57 - and if we run that this should be able
92:59 - to successfully create our train data
93:02 - our validation data and our test data so
93:05 - here i can do train data equals
93:08 - df
93:09 - to data set
93:12 - and i'm going to run this on the train
93:16 - and actually let's just copy and paste
93:17 - this a few more times and here instead
93:20 - of train
93:21 - this will be valid
93:24 - and here i will call this test and
93:26 - actually we didn't create a valid it's
93:29 - val
93:30 - now run this hopefully great there are
93:32 - no errors
93:34 - so basically what this um function is
93:36 - doing it's shuffling our data for us and
93:38 - it's formatting it in a proper you know
93:41 - format but then it's also batching it
93:43 - into our batch size that we specified
93:45 - and pre-fetching now this prefetch you
93:48 - can kind of um
93:51 - i guess
93:52 - you can think about it as
93:55 - just try to speed things up a little bit
93:57 - trying to optimize things a little bit
94:01 - so this it just reduces some friction
94:05 - so these are our data sets now let's try
94:07 - to take a look at like what's actually
94:08 - in here so something that we can do is
94:12 - called train data of zero but actually
94:14 - this train data is now going to be like
94:16 - a tensorflow data set so what you need
94:19 - to do is you actually need to convert it
94:21 - quickly so that we can see what's going
94:23 - on
94:25 - so you'll see that this is actually a
94:27 - tensor
94:29 - it's i guess it's a tuple in this case
94:32 - but you have the tensor of all the
94:34 - strings and then you also have the
94:36 - corresponding labels that are associated
94:39 - with them the zeros and ones that we
94:41 - came up with
94:44 - all right
94:48 - great so let's talk about how our model
94:51 - is now going to work one thing that we
94:54 - imported that you might have noticed up
94:55 - here was this tensorflow hub
94:58 - so what tensorflow hub is
95:00 - is tensorflow hub is a repository of
95:03 - trained machine learning models so
95:05 - basically these are models that are
95:07 - ready to use um
95:09 - they just need some fine tuning and we
95:12 - can actually use one of these to help us
95:14 - in our process of text classification
95:17 - so recall that computers don't really
95:20 - understand text that well right like
95:22 - computers understand numbers really well
95:25 - so
95:26 - we actually need a way to
95:28 - transform
95:30 - all of these sentences like this into
95:33 - like
95:34 - numbers that our computer can understand
95:36 - and that's where
95:37 - this embedding comes into play
95:40 - so
95:41 - one embedding that we will actually use
95:44 - is
95:45 - this nnlm
95:47 - this en english and then dimension 50.
95:50 - so this is token based text embedding
95:52 - trained on english google news
95:54 - using like a seven billion
95:56 - document corpus
95:58 - so it's a saved model that they already
96:00 - have for text embedding
96:02 - and let's see how we do so
96:05 - you can say embedding equals
96:08 - set that and actually you know what
96:10 - let's just label this
96:16 - okay
96:17 - so here is our embedding then we're
96:19 - going to create a variable called hub
96:21 - layer and set this equal to hub dot
96:24 - keras layer
96:26 - and this is uh you want to pass in the
96:29 - embedding link
96:31 - and the data type that we're actually
96:32 - going to use we're going to
96:34 - tell this that we're using strings
96:38 - and then finally we are going to tell
96:41 - this that you know trainable is true
96:44 - okay cool
96:47 - oops
96:55 - we can actually call this hub layer
96:57 - and we can do that by passing in
97:00 - let's uh you know do this little hack
97:02 - again train data
97:04 - and
97:06 - um
97:08 - just say zero
97:14 - oops okay so basically what we did we
97:18 - need to only pass the strings
97:21 - so basically what we've done here is
97:23 - every single
97:24 - uh sentence that we had in our data set
97:26 - we're essentially projecting it into
97:29 - a
97:30 - length of 50
97:32 - vector containing only numbers so that's
97:34 - what our embedding did it basically
97:36 - transformed our text into this vector of
97:39 - numbers
97:40 - that now our model can go and understand
97:44 - so let's build our model
97:48 - so let's again calls keras sequential so
97:51 - previously what we did was we passed in
97:53 - you know a list of all the different
97:55 - layers but
97:56 - i'm just going to show you guys another
97:57 - way that you can build a model so here
97:59 - you can also do model.add
98:01 - and the first thing that we're going to
98:02 - add is this hub layer that we
98:05 - defined up here
98:08 - so basically now the first
98:10 - transformation will be this text to
98:12 - value numerical value transformation
98:15 - then what i'm going to do
98:17 - is i'm going to add a layer
98:22 - and this is just going to be a classic
98:23 - dense layer
98:24 - and let's use 16 neurons again
98:30 - and we're going to use relu and i'm just
98:32 - going to add another layer of those and
98:34 - then finally i'm going to add
98:42 - my uh
98:46 - final output just like in the previous
98:48 - neural net that we created in our feed
98:50 - forward neural net
98:54 - okay so now that we have our model i'm
98:56 - going to actually compile it with the
98:58 - same exact compilation statement as the
99:01 - previous model the feed forward network
99:03 - that we did so let me just paste that in
99:05 - here
99:07 - so i'm saying model.compile i'm using
99:09 - atom as the optimizer again and actually
99:12 - the slanting rate let's go back to 0.001
99:15 - um i'd copy that from another example
99:18 - and then for the loss we're again going
99:21 - to use binary cross entropy and for our
99:23 - metric we're going to add accuracy now
99:26 - these are because we're doing binary
99:28 - classification
99:30 - okay so here first let's actually
99:33 - so now that we have our model and it's
99:36 - compiled
99:37 - let's try to evaluate the untrained
99:40 - model on the train data
99:44 - and let's actually do the same thing for
99:45 - the validation data
99:49 - okay so it seems like our accuracy is
99:51 - around 40 percent
99:53 - not so great our loss may be around 0.7
99:57 - okay so
99:58 - let's see what will happen if we
100:01 - do model.fit
100:03 - so here let's pass in our trained data
100:06 - and let's also
100:09 - let's do uh
100:10 - 10 epochs and let's pass in our
100:12 - validation
100:14 - data
100:17 - okay
100:20 - now we're starting to train
100:24 - okay so our accuracy is increasing it's
100:26 - over 50 now this is a good sign and our
100:30 - loss seems to be decreasing so this
100:31 - means that our model is training
100:37 - cool it's still increasing we like to
100:40 - see it
100:44 - and it's still going
100:48 - okay so i'm gonna let you guys train
100:51 - your model you can pause the video
100:54 - really quickly but
100:55 - i'll be back once our models are done
100:57 - training
101:00 - okay so our model's finished training
101:02 - let's take a look at the results
101:04 - so
101:05 - here it seems like our loss is steadily
101:09 - decreasing which is a good sign
101:11 - and our accuracy seems to be increasing
101:13 - almost to 90
101:15 - okay that's also a really good sign now
101:18 - if we go over here and look at the
101:19 - validation loss in the accuracy our
101:22 - validation loss okay it starts pretty
101:24 - high and then it decreases and then it
101:26 - seems like it starts to go back up again
101:29 - which means that
101:30 - seems like as training's going on the
101:32 - validation performance is actually worse
101:35 - and worse and if we look at the accuracy
101:38 - okay it starts off pretty decent at 80
101:41 - and it gets better and then it starts to
101:43 - kind of plateau and then it dips a
101:46 - little bit
101:47 - okay so what's going on here
101:50 - this is a classic example of overfitting
101:53 - so overfitting means that the training
101:56 - data that you do see that your model
101:58 - sees
101:59 - the model learns how to predict the
102:01 - training data really well but then it
102:02 - generalizes really poorly so what we can
102:06 - actually do is we can plot
102:09 - the model history so
102:11 - if we do history.history
102:14 - what happened
102:16 - and we look at the accuracy
102:20 - and then we also do the validation
102:22 - accuracy
102:30 - okay just some labels here
102:35 - and what we can
102:38 - alright and then let's just give this a
102:40 - title
102:45 - and then
102:48 - labeling
102:58 - what is going on here
103:02 - and then at the very end we can do
103:04 - guilty
103:05 - so this is going to plot
103:08 - our accuracy hopefully
103:13 - oh i think we have to type out accuracy
103:18 - okay so we can see that our training
103:20 - accuracy starts here and it gets better
103:22 - and better and better whereas our
103:24 - validation accuracy seems to okay it
103:27 - does well and then it kind of tapers off
103:30 - now if we look at the loss it's a
103:32 - different story so here
103:41 - let's just change this plot a little bit
103:44 - and let's look at this again okay so it
103:46 - looks like our training loss actually
103:48 - decreases very well
103:50 - whereas our validation loss seems to
103:52 - decrease and then start to go up again
103:56 - so
103:57 - basically our model is incapable of
103:59 - generalizing because we've trained it so
104:02 - much on our
104:04 - data that we're feeding it
104:06 - so how do we fix this
104:09 - one way would be to add something called
104:11 - drop out
104:12 - and drop out just means that every once
104:14 - in a while you select a few nodes that
104:16 - aren't really
104:17 - working
104:18 - so
104:19 - if i do this
104:25 - i can add a few layers like this in here
104:28 - and that should actually improve my
104:30 - training
104:32 - so the reason why this works is that
104:35 - every single time the model has to
104:36 - basically
104:37 - go through these obstacles these dropout
104:40 - layers where some nodes aren't working
104:42 - and try to figure out how to work around
104:44 - that
104:45 - so
104:46 - that helps it generalize a little bit
104:48 - we're just adding a little bit of
104:49 - randomness in there
104:53 - all right so here let's uh you know
104:56 - repeat our hub layer
104:58 - compile our model
105:01 - evaluate okay it seems like this is
105:03 - already really high
105:05 - but
105:06 - that's fine it could just be the
105:08 - opposite of what we just did
105:11 - um
105:14 - all right and then also another thing to
105:16 - do is stop earlier so let's just use
105:18 - five epochs
105:23 - all right i'll see you guys after
105:25 - training
105:28 - all right so we're back let's take a
105:30 - look at our results so our loss is
105:32 - decreasing accuracy increasing great
105:35 - okay so our accuracy our loss seems to
105:38 - increase a tiny bit by the end and our
105:40 - accuracy seems to decrease a tiny bit by
105:42 - the end but this is probably a much
105:44 - better generalizable model than our
105:47 - previous one that we had trained
105:49 - so
105:51 - finally what we can do is we can
105:53 - evaluate this
105:55 - on our test data in order to get our
105:57 - final results
106:00 - so sweet look at that our loss seems to
106:02 - be around 0.38 and our accuracy is
106:06 - around 83 percent so that seems to be
106:09 - really good
106:10 - awesome
106:11 - all right so up until this point we just
106:13 - created a
106:15 - neural network that we saw would help us
106:17 - with text classification
106:19 - however we did use part of the
106:21 - tensorflow hub which you know is not a
106:23 - bad thing but if we wanted more
106:26 - control over
106:28 - this model what would we have to do and
106:30 - so i'm just going to show you guys
106:31 - really quickly how we would recreate
106:33 - this model using a lstm
106:37 - here i'm going to label this section
106:39 - lstm
106:41 - and the first thing that we're going to
106:43 - do is to create some sort of encoder for
106:46 - our text because once again
106:48 - our computer does not understand english
106:52 - so
106:53 - what i'm going to do is create an
106:55 - encoder
106:56 - and set this equal to tf.layers.txt
107:02 - vectorization
107:04 - and then here i'm going to say the max
107:06 - tokens well this is going to be the
107:08 - maximum number of words that we're going
107:11 - to remember
107:13 - and i'm going to set this to 2000
107:16 - so then what i'm going to do is call
107:18 - encoder.adapt
107:20 - and here i'm going to pass in
107:23 - uh our train data
107:28 - and then actually because our train data
107:30 - doesn't really
107:31 - like this encoder would only need the
107:33 - sentences
107:34 - uh we're just going to also pass in a
107:36 - quick little lambda function because
107:40 - our train data is composed of the text
107:42 - and the label but we don't really care
107:44 - about the label we just want the text
107:46 - so let's run this
107:52 - okay
107:53 - and let's check out our vocab
107:58 - so we can call encoder.getvocabulary
108:05 - and then let's actually uh let's take a
108:08 - look at this let's see like the first 20
108:10 - items
108:12 - all right so here these are words that
108:15 - are encoded in our encoder or part of
108:18 - the vocabulary and this here this this
108:20 - represents any unknown tokens
108:24 - okay then we can create our model
108:27 - and again it doesn't really matter which
108:29 - way you define it but
108:31 - this is how i'm going to define it this
108:33 - time
108:34 - and the first thing that we're going to
108:36 - pass in is the encoder because this is
108:39 - the encoder is what is going to
108:41 - basically vectorize our text
108:44 - and then we need to have some sort of
108:47 - embedding for this vectorized text
108:50 - and that we are going to call
108:53 - keras.layer
108:54 - dot embedding
108:56 - so for the embedding we're going to need
108:58 - to pass in an input dimension and i'm
109:00 - going to set this equal to the length of
109:02 - the encoder's vocabulary
109:08 - and then we also need to pass in an
109:09 - output dimension
109:11 - so this output dimension i'm just going
109:12 - to set equal to 32 because i think
109:15 - that's how many lstm
109:17 - um
109:18 - that will be the size of our lstm
109:21 - and then
109:22 - i will also
109:24 - set mask 0
109:26 - equal to true and the reason why we use
109:29 - this masking is so that we can handle
109:32 - variable sequence lengths
109:35 - okay so that's our embedding layer so
109:37 - basically this turns these two together
109:40 - we'll be able to turn our sentence into
109:42 - a vector of numbers that our neural net
109:44 - will be able to comprehend
109:47 - then next i'm going to add in a
109:50 - lstm layer and it's literally as easy as
109:53 - this
109:54 - um and we just you know how many nodes
109:57 - there are i put the output dimensions 32
110:00 - so that's the value that i'm going to
110:02 - pass here
110:04 - and then let's just add a dense layer
110:09 - let's do a drop out layer because you
110:11 - know we saw previously that we might be
110:14 - prone to over training
110:16 - and then finally we
110:18 - result with a
110:21 - dense
110:22 - layer
110:24 - sorry i totally typed these wrong drop
110:27 - dropout and then dense one
110:29 - i also forgot the activation so here we
110:32 - want sigmoid because this is the output
110:35 - and up here we want
110:38 - really
110:40 - okay so this is our model
110:44 - and
110:46 - here i missed another s
110:53 - cool
110:54 - all right
110:55 - now for the compilation let's just
110:58 - borrow this right here
111:03 - and again we want to evaluate this
111:09 - on our trade
111:11 - data
111:12 - as well as our validation data
111:17 - just to see you know what happens
111:19 - so let's take a look at that
111:47 - okay so our accuracy seems to be around
111:50 - 53
111:51 - not great our loss around 0.7
111:55 - okay
111:56 - so
111:57 - now let's
111:58 - train our model
112:05 - let's try five to see what happens
112:21 - well
112:22 - i will see you guys in a bit
112:25 - all right so now we're nearing uh the
112:28 - end of the training
112:31 - and let's just take a quick look at the
112:32 - results so here our loss for our
112:35 - training data is decreasing strictly and
112:39 - our accuracy seems to be strictly
112:41 - increasing
112:43 - now let's talk about the validation law
112:45 - so it does seem like you know we cut it
112:47 - off early enough that this is still
112:49 - decreasing and waivers a little bit
112:51 - and our validation accuracy seems to be
112:55 - i mean the overall trend seems to be
112:57 - improving okay so we have 84 accuracy
113:01 - here which is pretty good
113:03 - and finally at the end of course we can
113:06 - evaluate our model once again so here
113:09 - it's called model.evaluate and use our
113:11 - test data to see what happens
113:17 - all right cool so we get an accuracy on
113:19 - our test data of 84
113:22 - which is pretty sweet
113:25 - that is the end of the jupiter notebook
113:27 - tutorial
113:29 - and
113:30 - thank you guys you know for being here
113:31 - for following along with this
113:33 - and now you've learned how to
113:36 - i mean not only implement a feed forward
113:38 - neural network with numerical data but
113:41 - you've also learned to use tensorflow
113:44 - with text classification and trying to
113:46 - figure out whether or not a wine review
113:48 - based on the review itself
113:50 - is maybe you know lower tier or higher
113:53 - tier which
113:54 - is awesome
113:56 - i hope you guys enjoyed this tutorial
113:58 - and you know give this video a thumbs up
114:01 - if you liked it leave a comment and
114:03 - don't forget to subscribe to free code
114:05 - camp and kylie
114:07 - all right see you guys next time