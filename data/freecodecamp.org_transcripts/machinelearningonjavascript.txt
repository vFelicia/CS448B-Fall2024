00:00 - a little bit about myself before we
00:01 - start i'm from argentina uh this is my
00:04 - first conference outside south america
00:06 - as both as a speaker and as an attendee
00:08 - so i'm pretty excited about that
00:11 - i work coding and researching different
00:12 - things at movies
00:14 - we heard a booth so i talked with some
00:17 - of the guys here uh if you want to find
00:19 - me on the internet those are my github
00:20 - and trio profiles
00:23 - so if you're wondering a little bit
00:25 - about movie we are a software
00:26 - consultancy firm
00:28 - we are based in north in texas uruguay
00:31 - argentina and colombia
00:33 - we work on a lot of different
00:34 - technologies languages we build a lot of
00:37 - different things from mobile apps to
00:39 - websites and everything
00:41 - we have our first node.js react.js
00:44 - apprenticeship on austin
00:47 - we do that periodically so if you are
00:48 - from austin or you know someone from
00:50 - austin that might be interested in
00:51 - learning nobgs or reijs
00:54 - just check our site and see how that's
00:56 - going um that's our trio
00:59 - so before we start i would like to make
01:02 - one disclaimer uh this was originally a
01:05 - one hour long talk because a broad topic
01:07 - and i need to feed them in 25 minutes so
01:10 - bear with me on some things
01:11 - i hopefully will get through everything
01:14 - um
01:15 - so a lot of people ask me today why not
01:18 - js for machine learning especially
01:20 - because there are a lot of languages
01:22 - that are well known for being performed
01:25 - are
01:25 - they have better libraries for data data
01:28 - science
01:30 - you know python has tensorflow cycle
01:32 - learn air has a lot of
01:34 - math libraries as well as math lab
01:39 - uh my answer like personally was that
01:42 - after es6 came out i'm using javascript
01:44 - for everything
01:45 - uh all my prototypes and everything is
01:47 - javascript but if you want to you know
01:50 - talk to your boss about using javascript
01:52 - for machine learning you should go with
01:54 - this explanation
01:57 - we do web development
01:59 - so it makes sense for us to use node
02:01 - because we already using node we know
02:03 - like the ecosystem package management
02:06 - management manager and everything
02:08 - so
02:09 - it makes sense it was easier to import
02:11 - people because they already know es6
02:13 - they already know javascript
02:15 - and they already know everything but the
02:17 - framework and a little bit about the
02:19 - theory so you don't need to explain like
02:22 - tensorflow distributed computing or
02:24 - extra things that you don't really need
02:26 - for the case we all love javascript
02:28 - because we are all here at javascript
02:30 - conference
02:31 - node.js has a lot of modules that are
02:34 - great for different things like
02:37 - normalizing your data
02:39 - working without output
02:40 - a lot of just general things we have
02:42 - module for everything so that's great i
02:44 - don't know
02:45 - um the first one the right tool for the
02:47 - job well for me work because i'm not
02:50 - ingesting the amount of data that
02:52 - facebook or instagram are using or
02:55 - ingesting so for me makes sense uh it's
02:58 - working now in production some of my
02:59 - products uh i had recommendation systems
03:02 - and a lot of things working
03:04 - they're fine don't have any cpu issues
03:08 - much so
03:09 - that's pretty cool
03:12 - so yeah what's uh machine learning
03:14 - anyway
03:14 - i was trying to find like a definition
03:16 - of something to put in here and i found
03:18 - this quote from arthur samuels he's like
03:21 - a pioneer in
03:23 - artificial intelligence intelligence
03:25 - he said that it's a field of study that
03:28 - gives the computer the ability to learn
03:30 - without being explicit program
03:32 - and this code is interesting to me
03:34 - because it draws like the difference
03:36 - between what's called weak ai and strong
03:39 - ai
03:41 - weaker guides you can think of that as
03:43 - when you code like a
03:45 - program to play chess or checkers
03:48 - as a user you get the impression that
03:49 - the computer is actually thinking of how
03:51 - to beat you or he the computer will
03:53 - learn how to play but actually it's just
03:55 - following a set of steps that you
03:57 - pre-programmed like trying out like
03:59 - foreign forcing or whatever
04:01 - uh but strong ai it's when you program
04:04 - the mechanism for the computer to learn
04:06 - to do something
04:07 - um it's up to the computer what they do
04:10 - next so that's kind of of what's
04:13 - interesting
04:14 - and hard about machine learning uh i'm
04:17 - linking here to
04:19 - a paper for from arthur samuels about
04:23 - game theory and ai and that's a pretty
04:26 - good article about the difference
04:28 - between artificial intelligence and
04:29 - machine learning so if you want to check
04:31 - those out
04:33 - there are different kinds of learning
04:36 - the first one is on supervised learning
04:39 - this type of learning is when you have a
04:41 - bunch of data that you know nothing
04:43 - about that maybe you're mining from your
04:45 - application from other places and you
04:47 - just want to try to figure out a pattern
04:50 - or some way to grow that data for
04:52 - example maybe you have a bunch of data
04:54 - from your users like their weight and
04:57 - their head and you just
04:59 - move things around and then you figure
05:01 - hey so
05:02 - all these guys are
05:04 - size s for t-shirts these are guys this
05:07 - old set of guys are medium and so forth
05:10 - so that's kind of
05:11 - what this kind of learning is about
05:14 - it's pretty
05:15 - commonly used for
05:17 - data clustering and data mining
05:21 - and then we have supervised learning
05:23 - which is more early use which is
05:25 - basically learning by examples you will
05:27 - have some a set of data
05:30 - label data so you know what that data
05:32 - means and you're going to feed that data
05:34 - into your model and your model is going
05:36 - to try to figure out a function
05:39 - that can learn
05:41 - different patterns about that data so
05:44 - basically
05:45 - that will help you to solve a lot of
05:47 - different problems some problems that
05:49 - supervised learning solve
05:51 - its classification problems which is
05:54 - basically you taking an input and you
05:56 - need to classify that input among a set
05:59 - of labels for example an input being a
06:01 - picture and a set of labels being that's
06:04 - a hot dog it's not a hotdog
06:08 - our regression problems when you try to
06:10 - predict some continuous numeric output
06:13 - based on
06:14 - an input for example trying to figure
06:16 - out the price of a house based on the
06:19 - size of the house or
06:21 - trying to break your score in a test
06:23 - base based on your previous scores uh
06:26 - that's a regression problem
06:29 - there are a lot of different types of
06:31 - algorithms there are
06:33 - there's so and those are pretty popular
06:35 - but there are a bunch more uh i have
06:38 - been using all these in javascript uh
06:40 - for the first four uh key near network
06:43 - neighbors uh
06:45 - classification regression trees support
06:47 - vector machines name bias there is this
06:49 - package called machine underscore
06:51 - learning they have implementations of
06:53 - all of them so if you want to just play
06:55 - around you can download the package and
06:57 - throw inputs and see what happens if you
06:59 - are interested in how the algorithm
07:01 - works you can just go to the source and
07:03 - check the files they are
07:05 - fairly small and it's pretty interesting
07:08 - to just research and check how those
07:11 - are built and that paper it's about
07:14 - like a brief description of everyone
07:18 - every single algorithm i think there are
07:20 - more than those
07:22 - and why would you use one over the other
07:25 - and just like use cases in general
07:28 - just pretty cool
07:30 - so
07:32 - i'm time uh
07:34 - i'm going to try to do a brief
07:36 - introduction to neural networks in
07:38 - javascript
07:40 - i'm going to use this library for the
07:43 - examples it's called synaptic.js how
07:45 - many of you have heard about synaptic.js
07:49 - how many of you are using synaptic.js
07:51 - like
07:53 - okay
07:54 - so
07:55 - this library uh it's
07:57 - it's for me it's great it works on both
08:00 - the browser and node.js it's what's
08:02 - created by this guy
08:04 - juan casala which is from argentina
08:07 - he has a lot of examples uh pretty
08:11 - wiki with a lot of examples everything's
08:14 - pretty well documented so just getting
08:16 - there was
08:17 - just a brief getting from trying to
08:20 - learn machine learning in python and i
08:22 - was racing issues like asking super new
08:25 - questions and they're well super polite
08:27 - like yeah you need to do that or they
08:30 - were explaining everything to me i was
08:32 - pregnant so go and give him stars or
08:35 - asking asking questions okay there is
08:38 - another library called knit taptic which
08:40 - started as a fork of synaptic then they
08:43 - built their own thing using back
08:45 - propagation and no revolution so this is
08:48 - more for
08:50 - kind of gaming things
08:52 - it's more performant but if you just
08:54 - want to play around with things i would
08:56 - recommend start with synaptic and then
08:57 - move to knit a big
08:59 - so
09:01 - thing uh
09:03 - every time i think of neural networks
09:05 - that's the picture that everyone that
09:07 - has in his mind like about notes and
09:10 - arrows and
09:11 - that doesn't really explain what they do
09:14 - but what they basically do is uh split
09:17 - data into groups so what you're trying
09:20 - to get the computer to do with a neural
09:22 - network is he computer could you split
09:24 - this data in different groups it's like
09:27 - sure i splits the data into groups and
09:30 - then you have a new entry point of your
09:32 - data and you can just see where it fits
09:35 - and that's like the classification of
09:37 - the regulation problem
09:40 - so starting from like the basic unit of
09:44 - a neural network
09:46 - obviously it's a neuron
09:47 - i won't draw the parallelism between
09:50 - this kind of neuron and the real neurons
09:52 - because i really don't know how neurons
09:55 - actually work but i don't know i know
09:56 - how this works so
09:58 - uh
09:59 - yeah a neuron basically is composed of
10:01 - different parts we have these arrows
10:04 - coming into the neuron which is the
10:06 - red dot right there
10:08 - these these arrows are called
10:10 - synapses
10:12 - uh they could be
10:14 - an output from another neuron or just
10:16 - your input every single one of these
10:18 - synapses or the arrows
10:20 - are a value which is your input and they
10:23 - are all multiplied by a weight that way
10:26 - at first it's random
10:28 - and the whole
10:30 - thing of machine learning especially on
10:32 - neural network it's just finding the
10:34 - weights
10:34 - uh that makes sense for your data to get
10:37 - the input the output that you want uh
10:40 - then
10:41 - all neural networks besides their inputs
10:44 - they have one extra input called a bias
10:47 - which is always one and it has its own
10:49 - weight
10:50 - uh and this bias what it does is ensure
10:54 - that even
10:55 - if all the inputs are zero you're still
10:58 - going to get some kind of output
11:00 - that's why that's a one
11:02 - it has also random weight assigned to
11:04 - that
11:05 - then the neuron has a
11:07 - an activation an activation function
11:09 - that we're going to see a couple of
11:10 - slides uh after this one
11:12 - and then you have the synapses going out
11:15 - of the neuron that could be your final
11:17 - output or it could be the input of the
11:19 - next one
11:21 - so
11:22 - the activation functions uh what they do
11:25 - basically is grabbing all the inputs
11:27 - from the neuron they
11:30 - add them together the inputs of the
11:32 - neuron are basically your inputs or the
11:34 - input of the of the or the output of the
11:36 - previous neuron multiplied by the weight
11:40 - and the activation function grabs all
11:42 - the inputs add them together and then
11:44 - use that as an input for
11:46 - for itself
11:48 - synaptic.js by default uses logistic
11:50 - sigmoid functions
11:52 - which basically
11:54 - maps every value that you use as an
11:57 - input between
11:58 - 0 and 1
12:00 - that's basically what it does if you
12:02 - want to know more about like the theory
12:04 - uh i linked some papers there and some
12:09 - an answer in quora that was pretty well
12:12 - explained
12:13 - there are some more uh activation
12:14 - functions i'm playing with rectified
12:17 - linear unit now a lot i didn't use the
12:20 - other two and i know neptopic has even
12:23 - more but
12:25 - sigma is
12:27 - pretty popular and you don't really need
12:29 - to change that
12:30 - unless you you notice that your use case
12:33 - is not like working so you can play
12:35 - around with the functions and trying to
12:37 - see what it's better for your use case
12:40 - but starting with sigma it's totally
12:42 - fine
12:44 - so
12:45 - a neuron can perform these four
12:47 - different operations
12:49 - they can project they can activate they
12:51 - can propagate and then gate project is
12:54 - basically
12:55 - setting a synapse between two neurons
12:57 - that's the red one it's projected into
13:00 - the blue one
13:01 - they can activate that's when they grab
13:03 - all the inputs add them together and
13:06 - call the sigmoid function with that
13:08 - value
13:09 - uh they can propagate which is the
13:12 - actual training of the of the network
13:15 - that's when you
13:16 - uh you said uh you get an output from
13:19 - the neuron
13:20 - that wasn't what you expected so you
13:22 - propagate back the error based on what
13:25 - you really expect though and that's
13:27 - going to change the weights of all the
13:29 - synapses
13:30 - back uh like on the uh to the first
13:33 - input neuron and that's what's called
13:36 - back propagation and that's how the
13:38 - network learns
13:39 - and finally they can gate that's an
13:41 - operation that's not pretty common it's
13:43 - just for one kind of neural network that
13:46 - if we have time we're going to see later
13:49 - but yeah
13:51 - this is a small example on synaptic how
13:54 - you can create this neural network based
13:56 - on two neurons
13:57 - we create two neurons a and b we project
13:59 - a to b
14:01 - that's the synapses between those and
14:03 - then b has an output then we activate a
14:06 - with 0.5
14:09 - then when we do that
14:11 - a sends through the synapse 0.5 times
14:15 - random weight first and then we activate
14:17 - b
14:18 - b grabs that
14:19 - output from a
14:20 - performs the sigma activation function
14:23 - and we get like a lot of random data
14:25 - because we are not really doing anything
14:28 - so if we wanted to train
14:30 - uh b to output zero every time that a
14:34 - activates with one this is how we do it
14:37 - uh we
14:39 - do the same thing that before create the
14:40 - two neurons we project one two to the
14:43 - other one we set up a learning rate that
14:45 - will be important for all propagation
14:48 - and then we have a for loop
14:50 - uh that that's our training session this
14:53 - training session has 20 000
14:56 - iterations
14:58 - and on each iterations does the same
15:00 - thing because a activate one
15:03 - then activates b we don't care what b is
15:06 - at this moment
15:07 - like what the output of b is
15:09 - because we are going to propagate that
15:12 - error so we say every time we activate a
15:15 - a uh you should propagate b with zero so
15:18 - what we are saying here is every time
15:20 - that a is one b should be zero and the
15:23 - propagate will change the way it's like
15:26 - 20 000 times until we get closer and
15:28 - closer to zero so when the training
15:30 - session session ends we activate a we
15:32 - want and we get that something pretty
15:35 - close to zero that you could do a math
15:37 - from and that's
15:39 - the training of your first two neuron
15:41 - based uh neural network
15:45 - uh obviously on real neural networks you
15:48 - will need like a bunch of different
15:50 - neurons and layers and this is all our
15:53 - real neural network looks uh you can see
15:56 - on like why um who why you should use a
16:00 - number of
16:01 - uh hidden layers and how many neutrons
16:04 - uh the hidden layers should have based
16:06 - on those there is a paper right there
16:09 - and a
16:10 - stack exchange link that explains that
16:13 - very well
16:15 - so let's
16:17 - build a small neural network
16:20 - as an example uh you are i guess all
16:23 - familiar with the xor operation uh what
16:26 - it does basically it takes two inputs if
16:29 - the two inputs are
16:30 - equal uh they output zero they are
16:33 - different they output one so we are
16:35 - going to train a neural network to
16:36 - perform this operation
16:39 - so based on this uh on this picture we
16:43 - can say okay so we have two inputs
16:47 - on our vector so it's going to be two
16:49 - neurons on the input layer
16:52 - we have one output on our vector so it's
16:56 - going to be one neuron on the output
16:58 - layer
16:59 - and then
17:00 - we usually
17:01 - have at least one
17:03 - hidden layer
17:04 - you could not have any but that's just
17:06 - for super simple classification problems
17:09 - and at that point you might not need
17:11 - neural networks i mean you can use them
17:13 - but
17:14 - it defeats the problems but yeah let's
17:16 - use one hand layer and the rule of thumb
17:19 - for selecting how many neurons should be
17:21 - in the handle layer and the hidden layer
17:23 - it's basically the mean between the
17:26 - neurons on the output
17:28 - layer and the input layer so in this
17:30 - case two
17:32 - so
17:33 - uh going on synaptic again
17:36 - we import the layer object the number of
17:38 - in the network object we create
17:41 - all three layers
17:43 - uh ignore that three right there there
17:45 - should be a two
17:46 - what yeah
17:48 - then we project one
17:51 - layer to the other one
17:52 - uh
17:54 - the input layer to the
17:55 - height and layer the hidden layer to the
17:57 - output layer
17:58 - and then we create the network using the
18:00 - network structure
18:02 - so we have network it looks something
18:04 - like this to when it runs from the input
18:07 - one of the on the output and or hidden
18:09 - layer
18:11 - and this is how our training looks
18:13 - we are going to use a learning rate of
18:15 - 0.3 this is like my starting point for
18:18 - every learning rate then i tweak that i
18:20 - play around with that until i get the
18:22 - output that i want and that's
18:24 - kind of how it works
18:26 - unless you have a phd and you know what
18:27 - you're doing
18:28 - but that's how i do it
18:30 - uh
18:31 - well again we have training session of
18:34 - uh 20 000
18:37 - iterations and we say okay every time
18:39 - that we activate our network with zero
18:42 - and zero i'm expecting a zero
18:45 - zero one i'm expecting a one and so for
18:47 - it and that will loop uh twenty thousand
18:50 - times
18:51 - and now if i can get
18:55 - console torque i can do
18:56 - [Music]
18:59 - uh this is
19:01 - for example uh how it works it's pretty
19:03 - fast uh it's a small example and you can
19:06 - just
19:08 - play around with that like saying 101
19:12 - and i will put someone uh yeah i will
19:15 - upload the code for for the examples uh
19:17 - later on my github
19:19 - but yeah this is how you can build a
19:21 - simple neural network uh one thing that
19:23 - you might notice is that all the inputs
19:26 - are zero numbers between zero and one so
19:29 - if you want to use like real data that
19:31 - you probably want to
19:33 - you need to perform normalization that
19:35 - means
19:36 - all your data should be mapped into
19:38 - values between zero and one uh i have an
19:41 - example for that but i had to put that
19:43 - up so go to synaptic they have a great
19:46 - guide about data normalization
19:48 - um
19:50 - yeah
19:51 - there are some other types of
19:53 - networks
19:54 - uh we have convolutional neural networks
19:57 - these are
19:58 - mostly used for image recognition
20:00 - uh because if you try to use a regular
20:03 - neural network like
20:05 - this one
20:06 - uh
20:07 - like with an image you will split the
20:09 - image into pixels but then
20:11 - uh the neural network won't have the
20:13 - context to make sense of those pixels
20:16 - because it needs a special con
20:19 - spatial context
20:21 - so
20:22 - that's why what this uh neural network
20:24 - brings into into the table you can see
20:27 - that link
20:28 - explains everything pretty well uh this
20:31 - is based on how the animals actually
20:34 - see pictures so it's pretty interesting
20:36 - you are into those things then you have
20:39 - recurrent neural networks these are
20:41 - neural networks that uh feeds themselves
20:44 - with their their own output so this is
20:48 - for trying to find patterns on sequence
20:51 - of data
20:52 - uh basically
20:54 - uh if you are like trying to teach the
20:57 - neural network the alphabet you will
20:58 - need to know which letter was before the
21:01 - one that you are doing now and that's
21:03 - that loop does that that gives that
21:06 - little context
21:08 - then you have uh
21:09 - another type of
21:12 - neural network that it's still recurrent
21:14 - but this one keeps uh it just that gate
21:17 - function on the neurons and it keeps uh
21:20 - track of things that happen long time
21:22 - ago this is used when you want to teach
21:25 - like a neural network to write like
21:27 - harry potter you can make the network
21:29 - like read all the books like 20 000
21:32 - times and then it's going to remember
21:34 - yeah so every
21:36 - five
21:37 - characters i use a space i say hurry
21:40 - this many times
21:41 - between each word and there are a lot of
21:44 - cool experiments with that you can just
21:46 - go into that league and see there are a
21:47 - lot of people doing little
21:49 - interesting things with this kind of
21:51 - network
21:53 - so i have a couple of final notes
21:57 - this is like my main note uh i
22:00 - experiment a lot with node.js and just
22:03 - with every model that i found and i
22:06 - really like to encourage people to do
22:07 - the same because i didn't know that we
22:10 - had so many modules for machine learning
22:12 - until i started like the nickname on npm
22:16 - so
22:17 - i think uh everyone should just go and
22:19 - play around with synaptic and you know
22:21 - make your co-workers aware that that
22:23 - exists because maybe they want to learn
22:25 - about this and they don't want to learn
22:27 - python or math or tensorflow
22:31 - so
22:31 - now it's pretty cool uh and also there
22:34 - is a quote that i heard on the ireland
22:36 - conf a couple of weeks ago saying that
22:38 - if you are employing people give you
22:41 - should give them time to experiment
22:43 - because otherwise they are going to
22:44 - experience production you don't want
22:46 - that so
22:48 - that's another note
22:49 - um
22:51 - i know i there are people that's
22:53 - concerned about the performance of
22:55 - javascript
22:56 - in all these things
22:58 - so my answer is check gpujs hamsterjs
23:02 - gpujs
23:03 - self-explanatory
23:05 - you can access the cpu the gpu with
23:08 - javascript
23:09 - that solve a lot of my problems with
23:12 - like huge amounts of training data i
23:15 - just connect the gpu and you said
23:17 - if you don't have gpu you can use
23:19 - hamster.js which is a native trading for
23:23 - node.js
23:24 - that's also solved a couple of problems
23:26 - for me
23:27 - and
23:28 - yeah if you want to play around with
23:30 - neural networks and you don't know what
23:31 - to do there are libraries for iot like
23:33 - journey five cylon and you can use that
23:36 - as inputs on your neural network and
23:38 - start doing like weird things
23:40 - you know
23:42 - and there is robot.js which is
23:43 - automation for the desktop on node.js
23:46 - you know you can just feed a recurring
23:50 - network with the movements of your mouse
23:52 - and try to regret your session i mean
23:54 - you can experiment with a lot of modules
23:56 - so that's pretty cool
23:58 - and
24:00 - like last thing
24:02 - i have a lot of meetings with clients
24:04 - that they say yeah i want this
24:07 - machine learning algorithm to predict uh
24:09 - stock trades or you know to be rich or
24:12 - something it's like yeah that's not how
24:15 - it works i mean your pro when you're
24:17 - working machine learning your product is
24:19 - not
24:19 - actually the
24:21 - machine learning algorithm because
24:22 - everything is open source
24:24 - there are papers for everything if you
24:26 - want to build a neural network that
24:28 - learns to play music there is a paper
24:29 - for that there is at least a hundred
24:31 - guys that implemented that in different
24:33 - ways
24:34 - so the product is the data if you don't
24:36 - have data then they will it's just not
24:39 - going to work and it's going to work
24:41 - poorly so if you are working on machine
24:44 - learning things
24:45 - your real product it's the
24:47 - the data
24:48 - uh my last advice follow this guy he
24:51 - posts a lot of things for machine
24:53 - learning for normal people like me so i
24:56 - understand that so you can understand
24:58 - that too so thank you for having me