00:00 - [Music]
00:18 - all right and with that i'd like to
00:20 - welcome everyone to our little debate
00:22 - panel or discussion panel where
00:23 - hopefully we will have a friendly
00:26 - conversation with one another and i
00:28 - noticed some some people have joined me
00:30 - up on the stage here but who are these
00:32 - people i have no idea can you maybe
00:34 - quickly introduce yourself sure
00:36 - my name is brian tarlston i work at
00:38 - microsoft on uh the language side of the
00:42 - chakra core engine and also on
00:44 - typescript um i also go to the tc39
00:47 - meetings and do that standards work as
00:49 - well
00:50 - i think it's a little bit of an
00:51 - understatement to say you do a little
00:53 - bit of standards work you're the editor
00:54 - of the ecmascript spec
00:56 - true i mean gonna get an applause for
00:59 - that yeah
01:01 - okay and who are you yeah my name is
01:04 - benedict i already spoke earlier
01:06 - yesterday you might have seen my talk i
01:08 - work on the v8 engine
01:10 - which is the engine in chrome
01:12 - and yeah i do a bit of performance work
01:14 - sometimes
01:17 - another understatement i guess
01:19 - uh wait that's that's an interesting
01:21 - coincidence you both work on a
01:22 - javascript engine
01:24 - wow oh surprise maybe we should talk
01:26 - about that yeah it's okay but you
01:29 - mentioned that you work on typescript
01:31 - that's one of the things you do um and
01:32 - we see a lot of developers using things
01:34 - like typescript and flow and other
01:35 - developer tools in their in their
01:37 - workflows
01:38 - and you also work on standards do you
01:40 - think
01:41 - types is something that should somehow
01:43 - be added to ecmascript as a language
01:45 - itself or should it remain at a build
01:46 - level
01:48 - um
01:49 - my personal opinion i think is that
01:52 - in um
01:54 - as far as types being in the in the
01:57 - engine and understood by the engine and
01:59 - used to produce um
02:02 - optimal machine code in sort of the same
02:04 - way that a
02:06 - c compiler might
02:08 - i'm not convinced that that is really
02:10 - possible javascript seems sort of
02:12 - fundamentally
02:14 - opposed to
02:16 - that kind of work
02:18 - i think probably benedict can share some
02:20 - actual data on that based on um strong
02:23 - mode investigations
02:24 - but i think there is some
02:26 - a space for types in in the tooling
02:28 - space
02:29 - um
02:30 - a sort of erasable type layer like like
02:33 - typescript has where
02:34 - um there can be
02:36 - uh you can
02:38 - you know maybe even see them in the dev
02:40 - tools and get feedback in your dev tools
02:42 - in the browser about uh what what's
02:44 - happening with types
02:45 - uh and you know get errors at run time
02:47 - for for type mismatches and that kind of
02:49 - stuff
02:50 - i think that's an interesting avenue to
02:52 - pursue
02:53 - but it is also very difficult to do
02:55 - because
02:56 - we have a lot of type systems now we
02:58 - have typescript we have flow there's
03:00 - also closure compiler
03:02 - and these are all kind of occupying the
03:04 - same space and they all have different
03:05 - ways of doing things so i think
03:08 - we'll see over the next few years
03:10 - whether
03:11 - there's coal there's a kind of
03:12 - coalescing of these ideas and sort of
03:15 - rallying around one sort of system to to
03:18 - rule them all or if uh as i actually
03:21 - suspect uh there's actually value in
03:23 - having type systems that make different
03:26 - trade-offs
03:27 - um
03:28 - uh so i think that might be actually an
03:30 - interesting place we might end up
03:32 - yeah i agree with many things you said
03:34 - so i don't really see types as a way to
03:37 - speed up javascript performance because
03:40 - you cannot really rely on them and the
03:42 - types don't operate on the level that
03:44 - the engine needs to operate on
03:46 - what i see though is the value for
03:48 - developers
03:50 - and i think the missing link that we
03:52 - have so far is that the browser doesn't
03:54 - give you or the engine doesn't give you
03:56 - feedback on whether you actually
03:59 - it doesn't validate the types really so
04:01 - you validate ahead of time but then at
04:03 - runtime anything can happen anyways you
04:05 - can pass objects that don't have that
04:07 - type because those are just living
04:09 - outside of the typescript world
04:11 - and we recently launched
04:15 - a new system that we call type profiling
04:18 - which you already might have seen
04:19 - already
04:20 - actually
04:21 - apple has this for some time in the web
04:23 - inspector
04:24 - and one use case that we could imagine
04:27 - is we collect the types at runtime so we
04:29 - know precisely what kind of object we
04:31 - have seen
04:32 - and we could use this information and
04:34 - combine it with the static information
04:36 - that you have in typescript or in flow
04:38 - and signal errors if you see a type
04:41 - an object of at runtime that doesn't
04:43 - match the type of the typescript
04:45 - declaration for example
04:47 - and
04:48 - doing this i think you can produce
04:49 - pretty good code because
04:52 - the engine you keep your code
04:53 - monomorphic just because you use the
04:56 - types to restrict it
04:58 - you can get
05:00 - if you subset javascript like if you get
05:02 - rid of eval
05:04 - and you get rid of the function
05:05 - constructor
05:07 - and
05:08 - a few other things um you end up with
05:11 - the sort of javascript subset that could
05:13 - be strongly typed
05:14 - um
05:16 - but that doesn't i don't know that many
05:18 - people are interested in using something
05:19 - that's not really javascript and you
05:21 - can't like use javascript libraries
05:24 - right you briefly touched on strong mode
05:27 - before is that something you have some
05:28 - more background on i don't have a lot of
05:30 - background on stronghold except that
05:32 - this experiment was discontinued because
05:35 - we found that so the idea is that we um
05:38 - [Music]
05:39 - we have a subset of sort of subset of
05:42 - the of javascript
05:44 - running in a dedicated mode similar to a
05:46 - strict mode that would be used strong
05:49 - and it would be restricted so that many
05:51 - of the things that make your program
05:52 - slow or
05:54 - behave in the wrong way
05:56 - no longer happen
05:58 - but i think that goes against what the
05:59 - standards committee is currently
06:01 - interested in and having this 1.js
06:04 - approach we also had an experiment on
06:07 - chakra core we had an awesome intern who
06:10 - um
06:11 - wanted to investigate
06:13 - uh how good how what what kind of
06:15 - performance gains we could get if we
06:17 - built in types into javascript and these
06:19 - types weren't typescript types these
06:20 - types were actual native types
06:23 - you know so he was annotating variables
06:25 - as this is an in 32 and this is you know
06:28 - an n16 and that kind of stuff
06:30 - and
06:32 - we found that as long as you just
06:33 - blindly trust the types which is not
06:36 - something that you can actually get away
06:38 - with in in practice but if you blindly
06:39 - trust the types you can get some pretty
06:41 - nice speedups across the board on the
06:42 - order of like 10 to 15
06:44 - um
06:46 - but the real trouble is we we have this
06:48 - awesome ecosystem with so much stuff in
06:51 - it and we just can't
06:52 - we can't break that we need to find a
06:53 - way to evolve from where we are and it's
06:56 - not entirely clear what that looks like
06:58 - yet
07:00 - and speaking of standardization at dc39
07:02 - there is a stage process for new
07:04 - features to be added to the language
07:05 - would you say there is a similar process
07:07 - that happens once a feature starts
07:09 - getting implemented in the engine itself
07:12 - a similar process like what kind of
07:14 - stages does a feature go through once it
07:16 - starts to get implemented on the browser
07:19 - side so outside of the dc39 process
07:22 - um
07:23 - yeah i mean at least on our team we
07:26 - usually start with some kind of
07:27 - prototype or a spike so we'll start
07:31 - doing this work um you know
07:34 - around stage three in the in the
07:36 - standards process that's that's a stage
07:38 - when
07:39 - the committee says we're pretty much
07:41 - done making changes to this and we
07:43 - really want implementer feedback
07:45 - um
07:46 - and then it's just a matter of uh
07:49 - getting all the test 262
07:51 - tests passing after that
07:53 - no small feat
07:56 - and then after that it's a an iterative
07:59 - uh cycle of you know shipping these
08:02 - features to developers and developers
08:04 - tell us hey this is great but you know
08:06 - xyz we go fix that ship it again and
08:08 - this whole cycle just kind of uh
08:11 - continue so those wouldn't be bug fixes
08:12 - anymore because you already passed the
08:14 - test suite would there be more like
08:16 - performance optimizations for common
08:18 - yeah yeah it's usually performance
08:19 - optimizations um it's really hard to
08:22 - anticipate in advance what patterns you
08:24 - guys are going to be using like um it's
08:28 - it's it's uh
08:31 - it's a hard game to play and
08:33 - if we try to guess
08:35 - what you might use we might get it wrong
08:37 - we might spend a lot of time optimizing
08:38 - something that just isn't important
08:41 - so this feedback cycle is really
08:42 - important to us that we can you know
08:44 - hear from developers how they're using a
08:45 - feature so that we can optimize it and
08:47 - improve ergonomics and other things like
08:49 - tooling and that kind of stuff what are
08:51 - your thoughts on that benedict it's
08:53 - actually very similar so we also start
08:55 - looking two new features
08:57 - once once they reach end of stage two
09:00 - early stage three
09:02 - uh we usually try nowadays we try to
09:04 - work together with babel for example to
09:06 - also make sure that
09:08 - things are aligned because it would be
09:10 - really bad if you use the feature at
09:12 - stage one already and then you rely on
09:14 - babel semantics and then the browser
09:16 - gives you completely different semantics
09:18 - once it hits the engine
09:21 - on the performance side we we we have to
09:23 - wait until
09:24 - some use appears so even before it hits
09:27 - the browser we can
09:29 - often see it
09:30 - um
09:31 - when we just look at the actual code
09:33 - that people write and then transpire
09:35 - babel
09:36 - but it's hard to estimate what is
09:38 - important so for example we just now
09:40 - start optimizing proxies
09:43 - just
09:43 - um this year we started looking into
09:45 - proxy performance and proxies have been
09:48 - there in year 6 since uh two years
09:50 - already
09:51 - so this is roughly the timeline when we
09:53 - noticed that okay maybe it's time to
09:55 - look into how people use it and then
09:57 - optimize the relevant cases so it sounds
09:59 - like engines optimized for a real world
10:01 - code which means that there's a bit of a
10:02 - chicken egg problem where if there's a
10:05 - new language feature that lands in
10:06 - browsers a lot of developers maybe they
10:09 - think oh it's not going to be fast yet
10:11 - in engine x or y so i'm going to not use
10:14 - it for now and maybe still transpile my
10:16 - code and avoid using this feature
10:18 - directly but that means that also it
10:20 - won't get optimized
10:22 - so how what is the best way to break
10:24 - this pattern and break this cycle
10:27 - so in my experience
10:29 - since we have evergreen browsers
10:31 - nowadays you should go for it as early
10:33 - as possible
10:35 - maybe just ship it to a subset of the
10:37 - users and see
10:39 - can you do this is this viable is this a
10:41 - logo
10:42 - and specifically on the node side you
10:45 - completely control the version of node
10:47 - and you completely know exactly which
10:50 - version of chakra core or which version
10:51 - of v8 is inside so it would be nice if
10:54 - you
10:56 - tried earlier and provide feedback and
10:58 - then we can look into this
11:00 - yeah i definitely agree with that um and
11:03 - also i find that um
11:05 - a lot of times you can get away with
11:07 - using these new features and areas of
11:08 - your application that aren't performance
11:10 - critical
11:11 - um
11:12 - so that's that's sort of my my approach
11:14 - to this problem is to um
11:17 - uh because like i can't wait to use
11:18 - these features after they get
11:20 - implemented but uh
11:22 - you know i'm not going to be using four
11:23 - of in a in a hot pass because it's um
11:26 - well now it's pretty fast but it wasn't
11:27 - as of six months ago um
11:30 - uh so yeah i think um there is a little
11:32 - bit of a chicken and an egg problem but
11:34 - yeah just you know really rely on
11:36 - feedback and you really can't get out of
11:38 - uh out of that
11:39 - out of that problem
11:41 - benchmarks are interesting
11:44 - um especially when they go out of their
11:46 - way to try and
11:46 - [Music]
11:48 - emulate real world code as much as
11:50 - possible but benchmarks just aren't um
11:53 - aren't enough like we really need to
11:54 - hear from
11:55 - developers
11:57 - and on top of that i see a danger
12:00 - one pattern that i see in the wild
12:02 - is
12:03 - when people use babel and try to
12:05 - optimize
12:06 - the transformation so that it produces
12:09 - the ideal
12:10 - whatever transpiled code
12:12 - and
12:14 - i think there's some danger in doing
12:15 - that because long term you don't want to
12:17 - transpire this code anymore or ideally
12:19 - you shouldn't transplant it anymore so i
12:21 - see the value in that but there's also a
12:23 - lot of danger because then
12:25 - once you stop transpiling you'll be set
12:28 - and then you go back to transpiling and
12:30 - you don't make progress
12:31 - right so the set of features to be
12:33 - transpiled should be a moving window
12:36 - and i think an easy way for developers
12:37 - to make that happen is to use bubble
12:39 - preset amp which basically it's like
12:41 - auto prefixer for bauble so you tell it
12:43 - the browsers that you explicitly support
12:45 - and it won't transpile anything that it
12:47 - doesn't need to transpile and that kind
12:49 - of solves the chicken egg problem as
12:50 - well to some extent right
12:53 - to some extent um you know like it's the
12:56 - the performance of the transpiled code
12:58 - um
13:00 - isn't particularly relevant to the the
13:02 - performance of the feature that you're
13:03 - trying to use um but you know if you're
13:06 - using something like preset m it does
13:07 - mean that
13:08 - um you know once browsers catch up
13:10 - you're gonna sort of immediately be
13:12 - um opted into this new feature
13:16 - which is certainly helpful
13:19 - yeah and on top of that there are many
13:22 - options um to even ship modern code
13:24 - already to modern browsers so one option
13:26 - that we have been discussing a lot in
13:28 - the past
13:29 - you can just ship modules to the browser
13:32 - nowadays and there's a fallback you can
13:34 - just provide the full transpired bundle
13:36 - to browsers who don't support modules
13:38 - but all the browsers that support
13:39 - modules support es6 so that is a good
13:41 - way to upgrade without breaking uh old
13:44 - browsers
13:45 - so we should be transpiring to two sets
13:47 - of two bundles basically essentially it
13:50 - produces two bundles so there's a
13:51 - webpack sample configuration for this or
13:53 - that probably also roll-up
13:54 - configurations to do that
13:56 - and i think this is a pretty safe way to
13:59 - move forward and it buys you a lot
14:01 - because the untranspired code is usually
14:04 - it's up to orders of magnitude smaller
14:06 - than the transpiled code
14:08 - okay so that's something that we all can
14:10 - do to make i mean to give better
14:13 - feedback and more data to browser
14:15 - developers so that they can make all
14:16 - these new features as fast as possible
14:19 - that'd be great cool and we have i want
14:22 - to stress this again we have evergreen
14:23 - browsers report a bug we can look into
14:26 - it if we don't know that there's a
14:27 - problem then we can never fix it unless
14:28 - we stumble over it by accident and then
14:32 - six weeks later up to 12 weeks later you
14:34 - have the new version and the feature is
14:36 - optimized
14:37 - ideally that sounds pretty cool i'm sold
14:40 - now we're here at a conference with lots
14:42 - of javascript developers and we've
14:44 - talked about standardization a little
14:45 - bit earlier
14:47 - how can the javascript community
14:49 - directly contribute to the whole
14:51 - standardization process
14:53 - um there's a
14:55 - number of ways in fact i'm giving a talk
14:57 - um at uh i believe 4 30 today um on that
15:01 - topic essentially
15:03 - but the spoiler alert yeah spoiler alert
15:06 - uh you should still attend my talk even
15:08 - after i say some things
15:10 - um so the
15:13 - the the best way
15:14 - is of course to to just
15:17 - try these features as they come out and
15:19 - get this feedback but even earlier in
15:20 - the process um
15:23 - tc39 is now
15:24 - entirely on github
15:26 - all of the standards work that we do
15:28 - plays out on github very little of it
15:31 - happens behind on closed doors it's
15:33 - usually just like administrivia and um
15:36 - you know ip related discussions and
15:38 - stuff like that uh so github.com tc39
15:42 - has i think there's like 100 repos now
15:46 - um like every proposal has its own
15:48 - github repo that you can follow so if
15:50 - you're really interested in a particular
15:52 - proposal like pipeline or the bind
15:54 - operator or whatever you can go to
15:56 - github and find that repo and you can
15:58 - watch it and
16:00 - the issues are used to discuss
16:02 - um you know
16:04 - issues and pull requests you can send
16:07 - actually pull requests and that's fine
16:08 - we can take those um so that might
16:10 - update the
16:11 - you can spend your own
16:13 - make your own spec updates
16:15 - uh that's a really great way um also
16:18 - like we're all on twitter you can talk
16:20 - to us on twitter uh we have an irc
16:23 - channel on free node i know irc is not
16:25 - the easiest uh
16:27 - technology to use um
16:30 - but uh
16:32 - we do have that as well so that uh
16:34 - that's another way i remember times when
16:37 - things were different when the
16:38 - ecmascript spec was published as like a
16:40 - word document it was literally
16:42 - maintained as a word document with
16:43 - annotations for the divs
16:45 - so for each new release you would have
16:47 - to download it with annotations enabled
16:48 - go through the annotations figure out
16:50 - what changed and then even for small
16:52 - typos people would have to report a bug
16:55 - and create a tracking issue and then the
16:56 - core editor would have to go and fix it
16:58 - in the word doc and upload it to
17:00 - somewhere and now uh
17:03 - most of my job is just accepting pull
17:05 - requests from
17:07 - various people
17:08 - so it's pretty nice that sounds pretty
17:10 - good i remember at the time people
17:12 - actually ended up writing a script to
17:14 - turn the word document or even the pdf
17:16 - version generated from the word document
17:18 - into an html version so that it could be
17:20 - posted online and linked to yeah
17:23 - that actually turned out to be uh
17:25 - transformational work because it was
17:27 - that work that enabled us to actually
17:29 - move the spec onto github because we're
17:30 - not just dumping the word doc in a
17:32 - github repo right we have a whole new uh
17:35 - html spec format and the whole tool
17:38 - chain built on node
17:40 - to make it like really easy for web
17:43 - developers especially to
17:45 - write spec text and read spec text and
17:47 - contribute
17:49 - right
17:50 - what other challenges would you say
17:52 - there are when it comes to
17:54 - measuring the real world performance
17:56 - because we mentioned before that there's
17:57 - a lot of micro benchmarks out there
17:59 - synthetic benchmarks and they're useful
18:01 - certainly but what we really want to
18:03 - optimize for is the real world
18:06 - performance of the code that people end
18:07 - up riding right
18:09 - yeah this is a very complicated topic so
18:12 - for
18:13 - for one thing there's the web
18:16 - we can
18:17 - just browse around and check web pages
18:19 - and we do this we actually take
18:21 - web page replay to
18:23 - make it reproducible and we look into
18:26 - web pages what they do like top thousand
18:28 - web pages
18:30 - but we have a lot of trouble on the note
18:32 - side because we don't have access to
18:34 - your applications and you should rather
18:36 - not give everyone access to it
18:39 - so getting useful workloads there is
18:41 - very hard plus the workloads on the web
18:43 - are just one
18:45 - aspect of the problem it's not
18:47 - it if you load a web page on your mobile
18:49 - phone then the engine does something
18:51 - completely different than you than if
18:52 - you i don't know
18:54 - use
18:55 - google maps or use google earth on your
18:58 - desktop and the same for a node and
19:01 - there's also the developer tooling site
19:03 - so we
19:04 - last week or the week before we launched
19:07 - the new benchmark suite that is the web
19:09 - tooling benchmark and we really
19:11 - literally just dropped the code that is
19:13 - shipped on npm
19:15 - into the benchmark and run exactly the
19:16 - code that runs on your machine too
19:18 - so that we make sure we don't just have
19:20 - a proxy for the application but we
19:22 - measure the application itself
19:24 - so the goal is to make the build times
19:27 - of developers tools faster like when you
19:29 - run npm run builds you're speeding that
19:31 - up
19:32 - yes
19:33 - so much tools like webpack or parts of
19:36 - webpack are included babel is in
19:38 - typescript is in ugly files and all of
19:40 - the things that take up all the time on
19:42 - your machine and
19:43 - when you deploy it to somewhere
19:45 - on the continuation continuous
19:47 - integration service for example
19:49 - benchmarks like these can be used by all
19:51 - browser and engine developers right it's
19:53 - not just specific to one engine
19:56 - yep yep we've been looking at the web
19:58 - tooling benchmark as well i certainly
20:00 - appreciate that benchmark since
20:02 - npm build is or npm run build is
20:06 - um well that's my get coffee time i
20:08 - guess
20:17 - does anyone have a question from the
20:18 - audience
20:21 - please raise your hand ask us a question
20:26 - come on asking us a question would be
20:28 - less awkward than not asking a question
20:29 - at this point so we're gonna we're just
20:30 - gonna be quiet up here and stare out web
20:34 - assembly never heard of it have you guys
20:36 - uh i what is this technology i don't
20:38 - know
20:39 - [Laughter]
20:40 - i love webassembly um so like i work on
20:43 - javascript the the language side and i'm
20:47 - kind of a language geek i guess
20:49 - um
20:50 - so like while i appreciate a bunch of
20:53 - the interesting things with webassembly
20:55 - you know regarding
20:57 - um you know compiling native
20:59 - applications to run on the web and
21:01 - and um
21:02 - you know getting really close to native
21:04 - performance and all of that really i'm
21:05 - just excited about the prospect of
21:08 - uh other languages becoming first-class
21:10 - citizens on the web i think that's going
21:12 - to open up a huge
21:14 - window for innovation from um all of you
21:18 - in the audience
21:20 - and i'm really excited to see
21:23 - what that technology enables
21:26 - yeah i i agree i
21:29 - personally i would love to see
21:31 - like next year or 2019
21:33 - having one big game publisher published
21:36 - on the web like next version of
21:38 - assassin's creed runs on the web only
21:41 - that would be awesome wow yeah how many
21:43 - megabytes would it take though yeah
21:45 - that's an unsolved problem plus all the
21:47 - drm issues that they have right but yeah
21:49 - so there are some technical details but
21:51 - otherwise it would just be awesome on
21:53 - the other hand for a game it's different
21:54 - than for downloading a website where you
21:56 - want to view some content right so you
21:58 - might be willing to to pay a little bit
22:00 - more and get all those megabytes in so i
22:02 - would be fine if i wouldn't run on my
22:04 - phone right so that would be okay the
22:06 - use case is completely different from
22:07 - desktop i think there's also a lot of
22:09 - room for
22:10 - um
22:13 - like you know these 3d engines have
22:15 - these massive asset pipelines and um i
22:17 - think there's it's we're in the very
22:19 - early days i think we're going to see
22:20 - tooling that's going to enable sort of
22:22 - streaming of game content and
22:25 - and that kind of stuff too to help
22:27 - um
22:28 - help address this problem of like hey if
22:30 - you want to play a game you probably
22:31 - don't want to download you know all of
22:33 - those assets again like even just
22:35 - checking if it's fresh could be could be
22:37 - expensive right that makes sense there
22:40 - was a question oh yeah sure
22:50 - okay so the question is about the binary
22:52 - ast proposal just so everyone hears
23:10 - okay so what is the status of the binary
23:12 - est proposal at dc39 and what is your
23:14 - stance on it for both of you so i don't
23:17 - know you can probably talk to the state
23:19 - but my personal opinion on it i think it
23:22 - would solve
23:23 - some really hard problems that we
23:25 - currently face in the javascript land
23:28 - but um for every new thing that we add
23:31 - it also adds a lot of new problems that
23:32 - we then have to solve as well
23:34 - so
23:35 - while i i'm generally in favor of it um
23:38 - i'm not the expert there so i i leave it
23:40 - to the expert to decide
23:42 - and maybe brian has a better opinion i'm
23:45 - also not an expert on on this um
23:49 - so the the basic idea of this proposal
23:51 - is
23:52 - um
23:53 - what if we instead of shipping
23:55 - javascript source code to browsers what
23:57 - if we shipped some packaged binary that
24:01 - includes that source text but also
24:03 - additional information that is sort of
24:07 - shoved up front so that engines don't
24:10 - need to
24:11 - scan over all of your code and collect
24:13 - all this information about like what
24:15 - variables have you captured and
24:17 - you know are using eval in this scope
24:20 - and all of this uh sort of information
24:22 - that our runtimes need to
24:25 - produce optimal code
24:28 - it's a great idea like
24:31 - there are i i definitely agree that this
24:33 - is an actual problem like this addresses
24:36 - like the goal of this proposal is to
24:38 - address that part-time bottleneck that
24:40 - many of you with
24:41 - massive code bases are feeling you know
24:43 - it can take seconds to parse javascript
24:46 - on mobile devices
24:48 - on some big properties so like that is
24:51 - not a good situation
24:54 - but in terms of
24:55 - difficulties
24:58 - what i what i'm really interested to see
25:01 - is how this proposal will handle
25:03 - um
25:04 - all of the different kinds of
25:05 - information that engines need to collect
25:08 - because we all actually collect slightly
25:09 - different information
25:12 - because we just have different
25:13 - implementations and different trade-offs
25:15 - so i think this proposal will only work
25:17 - if it is a sort of superset of all of
25:20 - the information that all of the
25:21 - implementations need to collect in order
25:24 - to produce optimal code
25:26 - i'm not super convinced that that's
25:28 - possible but i i think mozilla is going
25:30 - to work on a prototype and once that
25:32 - prototype
25:34 - comes out i think we'll know a lot more
25:36 - about
25:37 - how feasible this idea is and what's the
25:39 - current status of the proposal is it at
25:41 - stage zero or one um it's probably stage
25:44 - one i don't i don't remember off hand
25:46 - it's really easy to get to stage one
25:48 - stage one is yeah we agree there's some
25:49 - problem here and there's definitely some
25:51 - problem there so let me let me say one
25:53 - thing there will be a
25:55 - session in the deep track by my
25:57 - colleague maya
25:58 - on the browser so that would be an ideal
26:00 - question for that session oh yes
26:04 - okay and with that i'd like to thank you
26:06 - for your cooperation during this
26:08 - debate oh such a debate such a debate
26:11 - yeah it sounds like you agree about a
26:13 - lot of things
26:45 - you